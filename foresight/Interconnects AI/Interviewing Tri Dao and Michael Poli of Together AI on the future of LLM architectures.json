{"text": " Okay. Hey, everyone. Welcome to the first interview that we're going to post on interconnects. I'm really trying to bring more scientific voices into the AI discourse as media is covering a lot these days. I'm happy to be here with Michael Poli and Tree Dow, experts in some of these non-attention architectures that have been really blowing up in the last few weeks of December. So, Michael, do you want to introduce yourself first? Sure. Thank you. Thank you, Nathan, for inviting me. I do research at Together AI and was also a PhD student at Stanford working with Stefan Orenmann and Chris Rea. That's how I met Tree as well. If I had to choose, maybe I moved to a few different areas of research. If I had to choose one, I like to research at the intersection of signal processing, dynamical systems, and deep learning. Most recently, luckily, there's been more interest in efficient architectures that use some of these principles to improve scaling along different axes and to get new trade-offs at inference time. Great. And Tree? Hi, everyone. Thanks, Nathan, for hosting us. I'm really excited to be here. I'm Tree. I finished my PhD at Stanford. I'm an incoming assistant professor at Princeton. Right now, I'm Chief Scientist at Together AI. This is a startup working on AI infrastructure. And I've been working at the intersection of machine learning and systems, so designing algorithms that take advantage of the hardware that they run on. We're interested in long-range dependencies, how to encode that into model, designing architectures that can learn long-range dependencies. Yeah, really excited to be here. Yeah. Okay. I think I'm going to just, I have some questions to dive right into this. I think you two will kind of both answer them or someone can answer longer, whatever you want. I think to start with, we should talk about maybe why attention works and what the limitations of attention are. I think almost every person in tech, broadly, now knows that a transformer is a model built with attention. And ChatGPT does that, but why is this so good? Even how much of a transformer is built with attention? Are there other things going on? And what might be challenges there? Right. So, you know, a transformer, which is this architecture that powers most of the exciting applications that we're seeing nowadays, as you mentioned, ChatGPT and so on. Attention is kind of the core layer there. And attention actually became earlier around 2014, 2015, and then transformer came out, incorporating that focusing a lot on attention with these MLP, interleaving MLP and attention. And I think the success largely has been, they seem to be able to scale really well. So you can scale out the models with more parameters, with more data. And that has been the recipe for success. It sounds obvious now, but I think five years ago, that wasn't clear. So it seems like transformer architecture is a hugely successful one. And a couple of reasons why it's successful. I think it's general enough that it's able to learn a lot from data. And two, it's pretty friendly to hardware. There's no kind of sequential dependency like previous RNNs. So as a result, you can run it well on GPUs, TPUs. You can scale it up. It's very hardware efficient. I personally have worked on making it more hardware efficient as well. So it's just kind of the recipe for success, which is general architecture that scales well. If you're an NLP person, maybe you'll say to incorporate some kind of inductive bias for it to protect. Personally, I think it's a very general architecture that scales well and it's hardware friendly. Yeah, it's remarkable how it seems so obvious now. And it's like, I think one of the things that studying this work is the context length becomes a really interesting access to study alternatives to it. And essentially, Michael, do you want to talk about that? I could babble, but you know more. Sure. Yeah, there are several points. I'll start by saying that there's still great research trying to understand why, from first principles, why is it that the transformer can learn these interesting circuits. People can study. They pick apart the computational combination of different heads and transformers and so on. So there's work on basically understanding transformers from programming language that is encoded. But I think, as Trey mentioned, there are some very interesting design choices that have gone into the transformer. This interleaving of attention in NLP is quite important. And also, the transformer is essentially successful in the beginning because it encoded these techniques that have been developed for RNN, Celestium, so these other classical NLP models gating to modulate which information is absorbed into the model, gating to determine how quickly something is forgotten in this recurrence of the RNN into a parallel form. It is easily a bunch of jumps that can be easily, but not very easily, but can be optimized for RNN GPUs. Yeah, this stuff's all great. I guess the specific thing that I had in mind is how attention ends up being this kind of quadratic scaling in terms of cost when you have an input sequence that you have. If you have an input sequence of length L and you want to output a sequence of length L, essentially, if you zoom into the math and you look at what's happening in inference in most of these libraries, you have this upper triangular attention matrix where you say you can only look at the past entries of your text, and as you go through there then, you end up getting this L squared relationship where the first token you can only look at one, and then you end up looking at more tokens for each pass. And now we've been talking about recurrent neural networks, and how does something that isn't attention get around the fact that you want to look at all of the history of the text in your sequence? So if you write a long prompt to chat GPT, you really want all that information to be encoded, and how can doing something other than this dense attention matrix actually make that possible? Yeah. Right. Yeah, so you can go ahead. Before attention, there was RNNs, right? And then RNNs, they processed text, which is fine. And maybe they didn't scale as well, but yeah. Can you say briefly what a... They processed text by encoding text. Can you say briefly what a RNN is and how it works? Yeah, so these are recurrent neural nets that go back, I think, to the 80s. Maybe some of the more famous ones are LSDMs, GRU. So they were pretty popular around 2012 to 2016 or so. They were kind of state-of-the-art for translation, speech recognition, I think NLP, they were a state-of-the-art, and they processed text kind of sequentially. They see essentially one token, and then that changes their hidden state, and then they will update the hidden state, and every time they see a new token. So I think it's kind of, in some sense, mimicking how, for example, human brain process information, like you read the sentence or a passage, and maybe it's like you're storing some information in your brain, and by the time you finish reading the document, maybe you can answer questions about the documents without having to refer to that document again. So RNNs kind of work that way. They process the text, and then that changes the hidden state, and their hidden state is the representation that can be used to either generate new tokens or classify the documents or whatnot. These work well back in 2016 or so, but they've kind of fallen out of favor, empirically, they don't do as well as Transformer, I think, and as you touch on Transformer, because of this kind of quadratic scaling, you compare every token with every other token that comes before it, it gives you this very kind of easy way to propagate information. And I think that's for a reason why Transformer and attention does really well. But there's been more recently some of the new RNN architectures that seem to do pretty well. So RWKV is, I think, one of the earlier ones, you know, it's one, I really admire that project, you know, it's effort mostly from one person, really going against the orthodoxy of Transformer, showing that RNNs can be pretty strong. Who was the lead on that? I think it's this person, Bo Peng, I think, and you know, it's an entire project, but I think it's pioneered by Bo Peng. I think it's affiliated with Eluta AI and the Compute Sponsor by Stability and so on. Yeah, I was reading this earlier. At a technical level, they try to replicate something like the very key value lookup of attention with two linear RNNs to essentially be able to remove the specific attention scaling potential problems and with two RNNs, which have this better, like, long context behavior and different implementation rules. I think, and they also, the paper, trained up to 14 billion parameters, which kind of leads into some of the next questions. I was going to ask, I was going to ask Tree about Mamba and then Michael about Stripe Tahina. I think you could go in either order. I think these came out about a week apart and were these two language models kind of seen as being way closer than anyone would expect. Essentially, Stripe Tahina came out and the evaluations were close to models I've been training on all year, like Lama 2 and Mistral 7B, and I went in and I went to the Together API and I did like side by side of Mistral versus Stripe Tahina and it's like, it's a good language model. It answers most questions. There's no obvious failure modes. I think maybe Michael, do you want to comment on that? I know it's another big project and then we can go back to Mamba, even though it's slightly out of order in the chronological the release cycle that happened. Sure. I guess I'll start by saying that there's an interesting connection between all these new methods. There's this sort of convex set, which has a center and there's this connection between linear attention, so attention without the softmax, linear RNNs, and state space models, SSM. So at some level, the mathematical formulation of this kind of base model here, I'm not talking about the base architecture, just the fundamental model, is the same. And then you can go in different directions and each direction has its own trade-offs. You can go to the feature map direction, the kernel direction. So when you break down the softmax, you take away the softmax, you can place on queries and keys, kind of the fundamental the entities that compose your attention matrix, you can compose other kernel-like functions, other functions that you hope would approximate whatever capability of attention you like. You can do things like a Taylor approximation, Taylor expansion, for example. And you have a slightly different perspective, but you get something that, again, is very similar. You can go to time variance, so you take the RNN and you push this input dependence, so the way the computation inside the linear RNN is conditioned by the input sequence, and you can add things like gates. We've seen a lot of work, for example, modernizing linear attention with additional gates that allow you to make better use of your fixed state dimension. And then you have the third direction, at least in my mind, is the one that pushes, that uses the convolutional form, that pushes more towards other types of linear operators that are still associative, that are still allow you to train in parallel. So here are things, time invariant systems, I can elaborate on any of these points, but things that can switch between convolutions and recurrence, like this form model, with additional gates, again. Striped IEna was born as a project from the IEna architecture, which belongs to this third category that I just mentioned, and we're really trying to get the best per-flop architecture that we could. And one principle that was validated over and over again, and we're trying to understand better now, is that it seems composing, hybridizing different layers, different blocks of different categories, and even full attention yields something that is better than the individual components. So there seems to be a compositional aspect of these models that we're trying to understand better, and this gives you a better sort of pre-trained model per-flop. And with this model, we ran a whole suite of skating laws, and so on. Hybridizing also gives you, since we wanted something that would be kind of usable out of the box, it gives you a way easier time. When you're fine-tuning for longer contexts, we can apply some of these techniques that have been developed for transformers, and kind of surprisingly work for hybrids as well. So things like linear scaling for rotary embeddings and so on, you can go into the details. So it was mostly a project, what is the best, given the current landscape, what is the best we can do? Yeah, that's a great description of it. I mean, the sentence in the blog that's like, start training is optimized using a set of new model grafting techniques, enabling us to change the model architecture during training, kind of felt like, to me, that there's a ton going on there. And some of which you probably can't talk about, there's normal data. I don't think all the data that was quite explained, like what the longer context data was, but it's like, are you taking this from models, starting point from models that people would know, and can you say any of that? I think even just the summary that it's a synthesizing recent work into a strong model is great context for people. Yeah, well, that line, so we've, given this explosion of primitives that, you know, described, and given sort of the cost that it would require to evaluate all different combinations, we found ways to essentially start training with a configuration and then continuing on with another configuration. I think we'll have, we're going to have more work with paper. Yeah, there's so much cool work in that area. So one of the, someone at AI2 is working on a project where they're essentially trying to cut the Lama models in half and keep training them and things. It's just the wild west out there with people trying to take strong models and make them smaller while still getting the performance benefits of bigger models. I think that's a whole aside, but I wasn't expecting it to show up when people, like you follow the social media, I've striped my, you know, people are like, oh, non-attention models are finally good. And it's like, it covers up a lot of the details that are very interesting about it, in my opinion. So, okay, it turned back to tree. I think Mamba actually happened first among these. I did the, his reading back of social media. And it also was very surprising to me. I think the largest model in the Mamba suite is 2.8 billion parameters, if I remember correctly. And it was compared to a lot of the common benchmarks in open NLP. So things like GPTJ, Pythia model suites and the scores on the benchmarks reported were really strong. And I think if you want to start with the high level summary, and then I'm definitely going to make you talk about the awesome new CUDA kernels and stuff that you had to write for this project. Yeah, so this Mamba is a collaboration with, with Albert Gu, who's now, he was, he's just doing it at Stanford. That's where we met. And he's now a professor at CMU. And also at a startup. So it's a wonderful collaboration credit goes to him. Yeah, Albert has been working on this line of work called state space models. In some sense, as mentioned, it connects to things like linear tension, linear RNN, convolution, neural nets. And that's what his PhD thesis is about. I've also worked on state space for the past couple of projects. My, my angle is how to make state space more hardware efficient and kind of increase their expressiveness. So it's wonderful working with Albert. And there I think is more of a proof of concept, which is can state space actually do as well as transformer on language. So we've seen previous papers showing state space could be better on audio, could be better on some of the tasks on the long range arena. But language has always been the most difficult to get to do well for state space models. And language is also kind of the thing that people care about the most right now. So Mamba was more of a proof of concept, which is hey, we want to show that state space can be competitive, or maybe even be some of the transformers out there. So we validated that at the scale up to 3B, trained to 300B tokens. So in absolute terms, you know, these are not very strong models. These are not yet models that you would actually play with and expect it to do meaningful things. I think it's more of a, more of an academic comparison in terms of architecture, it's like, hey, training trained for the same amount of tokens, it does as well, or maybe slightly better than some of the transformer out there. And that's, in particular, has been very exciting to us. I think Albert's been pushing on this for a while. I've been pushing on this for a while. And I think finally, it seems to finally be kind of close to gap or even surpass some of the transformer. And it just, I think it opens up a bunch of opportunities. So inference could be way faster. Maybe we would have different ways to understand how in-contact learning happens, etc. So lots of, lots of future work, I would expect. Yeah. Can you go into some of the, like, what does it actually take to implement some of these new CUDA kernels? I just remember when this paper was announced, Sasha Rush, who's also very active in this space, I recommended me to talk with you too, was tweeting about the types of files or whatever. And in the paper, there's this discussion about how like the recurrent state needs to be sufficiently expressive, but doing so in a certain type of memory is a problem. Translate what this means to people thinking about GPUs and people thinking about these models being scaled. Like, is it now much easier to scale these models because they work on GPUs? Which GPUs were you using? Is there a bump that could come just from going to H100s or something? Any of that? Yeah. So the line of work on state space, sort of like S4 models kind of pioneered by Albert's work, they are in some sense recurrent neural network, but they have a much larger state size. So the state size is whatever kind of buffer that you're going to store information as you traverse or as you process the sequence. In some sense, you can view Transformers doing that as well, where it's keep the entire history. It's usually called the KV cache. It keeps the history and keep referring to it. For RNNs, they have a fixed size state. For Transformers state, you can think of the state size as increasing. And our intuition is that the larger the state size, the easier it is for the model to do well. So basically, you have more space to store whatever you need to remember. And so previous models like S4 and so on, they have an implicitly pretty large state size, but they use the convolutional view to avoid having to materialize the state. So that was wonderful. Michael has worked behind architecture, has used some of the same insight focusing on convolution. Mamba, on the other hand, focuses on the recurrent view. So we wanted to put more input dependency in the recurrence. We thought the thinking was that it was going to make it more expressive and the model would do better. But that prevents us from using this convolutional view that would make things efficient. So we had to figure out a different way to make things efficient. And so I focused on making that efficient on GPUs. And so our thinking was, instead of, okay, we're going to have a large state size, but we don't have to write to actual GPU memory, like the HBM, where you can just keep that large state in a faster memory called S RAM. You think of it as a cache. So if you're more familiar with CPUs, you just usually get cache and RAM. So if you have large state, you can keep it in the cache. And by avoiding having to write it down, you actually don't suffer too much if the state is large. So that's essentially the core idea. Would this be due to like input, like having to move the data around being really slow? Yes. Yes. That makes sense. That's a really common constraint in a lot of these things. And it's like, I think one of the most insightful things I've had now with GPUs versus TPUs and stuff is how mixtures of expert models doesn't work very well on TPUs just because you have to like, that essentially add a mixture of expert at a basic level. There's a routing layer that you learn and then multiple feedforward layers that you can choose from. And when you're distributing this, the feedforward layers could end up on a different TPU node and TPUs communicate with their neighbors. So TPUs take a big hit relative to GPUs where within video and video clusters, everything's connected so much more. And then it's easy to do that sort of distributed training. That's super interesting. And it's like, do you think there's going to be, I guess this is really where I want to open the conversation of like, what is this going to happen in 2024 in this space? Are bigger players going to move in and be exploring this? My take, seeing how good the long context learning could be and a fundamental limit is that systems like ChatGPT are going to use a transformer model for most tasks. And then if you need to do summarization, you might do a long context specialized architecture. And then we could even see a whole quiver of architectures behind something that you're using. But I think it's just like, is attention going to be dethroned? Is Sasha Rush somehow going to win this bet that everyone wants following in the area? What are you thinking about, either of you? I think it's a very, very strong architecture. And there's a proven recipe, right? You know, people scaling to a trillion of parameters. Right now, if you want, you say, well, I just want the best performing model that runs most efficiently on my hardware, that has the most support on the software side. The transformer is a safe bet. I think it's here to stay. But I think there are new ideas like state space, China, so the linear attention, ideas from linear attention, I think they're coming in. We've seen, as Michael mentioned, that mixing some of these components seem to improve performance. We're validated at, I think, seven B scale, but maybe it might even work at larger scale. I think people tend to be conservative and focusing too much on model architecture might not be worth their time. Like the LIMAR architecture is very, very strong. Most people are doing off of that. They're focusing on data. They're focusing on infrastructure, which makes sense. I think on my side personally, the other stuff is just plain interesting. There are more, I would say niche use cases, niche for now, where some of these alternative architectures are interesting, things like long contacts, different domains like audio and genomics. There's just plain interesting scientific questions you can ask, whether it follow instruction just as well, whether it's fine to interest as well. Does it play well with quantization and so on? There's just plain interesting research questions we can ask. Now on the production level, I think transformer is still incredibly strong, very well supported, both hardware and software. But I think some of these new ideas are coming in and people might start putting them as part of component in the transformer. Maybe we'll still call them transformer, but they'll just have more layers and just attention and LPE. Yeah, I 100% agree with you. So attention as a computational primitive is not going anywhere anytime soon. It's just a very efficient and a very convenient way to increase the effective state of your sequence processor. So at some level, if you're working with a model that only has a fixed state in each of its sequence mixers, you have an assumption and your assumption is that you only need so much information in the sequence. So there's always a tradeoff between the ratio of the state dimension, the sequence length, as you push things to the extreme, either model sizes, so as you make the model bigger, wider, effectively introduce more states and sequence length. Some of these margins, some of this is speculation, but some of these margins will disappear, some of the tradeoffs will change, especially 14, 30, some of these very fat models. But certainly, either whether that's hybridizing or some kind of new block, we're certainly going to see some more innovation. That's really exciting. My personal, if I had to make a prediction is that architecture design will get more interesting or more complex. There's going to be more to do. Yeah, I mean, this year, it's like, I got some 10 minute clock that's fine for us. I think with mixture of experts and this being popular as a state-state model, this is all just really within a few months outside of OpenAI. They've been doing mixture of experts for a lot longer than everyone, but in terms of open and academic communities, no one's really tried to do RLHF on mixture of experts. It should just work, but we have to learn all these things. And then the model grafting is becoming more of a real thing. That's super interesting. And it's just, I agree that it's just fun to follow. And hopefully, it gives academics and scientists more ways to influence the conversation where in industry, it's just about scaling and bigger models where we could maybe do specific things better, which I'm telling open source companies to do with their language models anyways, like if they want to have a business model, they need to have an edge. So this all fits into that kind of narrative pretty well with my regards. Is there anything else you guys are following in ML? It doesn't have to be about state-space models. What's exciting for you broadly for next year? Yeah, personally, I think data is still the most important thing. We're thinking a lot about how data influences the model performance, like really teasing that out, either having some of the synthetic tasks that correlates very well with model performance has been kind of the motivating examples in a lot of our papers and work has been focusing on synthetic tasks, or having maybe smaller data sets that kind of make it easier to really understand what's really going on. So I think personally, my focus is going to be on data for the next little bit. All the architecture stuff is fun. Making that hardware efficient is fun, but I think ultimately it's about data. If you look at the scaling law curve, the different monarchitectures generally have the same slope. They're just different offset. Seems like the only thing that changes the slope is the data quality. I love that point. That does seem true. I have the plot from Mamba in this blog post that I'm writing, which is just a little bit above. Same slope. Yeah, maybe add data. Data is really interesting. So miniaturizing architecture design, finding, breaking down what tasks are involved into, for example, language modeling and trying to package them into something that can be used to iterate, something that's quite exciting. That was one of the main techniques that was used for this zoology paper that also looks into some of these different behaviors. Personally, I'm also really excited about new applications, scientific applications, the genomics work, but even more engineering-focused. We're seeing a shift. Right now, language is still the domain that gets most clicks, most interest, but I think that will evolve over time. Some of these other applications offer, even just talking about architectures, they offer completely different design space. I'm excited to look into it. Yeah, everyone talks about language, but I feel like images and entertainment and videos are the things that are so obviously going to generate so much value to me. I don't know the ceiling on language, but when you could access a somewhat local text and video model at your homework station, that's tailored to your preferences. The amount of value that that creates is totally astronomical. I'm excited. I started playing around with these where I'd take text of the blog and convert it to dolly images and convert it to a video with generated audio all with one Python script. That's really easy to do, so I agree with your more than language. It's fun to have that view. These things actually do work really well in your experience when you stitch all of them together. It's not that good. The dolly images are pretty similar, but I'm doing something really naive where I literally take the text and have a system prompt. It's like, you're generating a series of images for visualizing a blog post, and it generates various all the machine learning thumbnails that you see everyone using. There are variations of that. The fun ones are whether it's about llama or mamba or something, and then they generate animals in them, which is good. I think I could get much better at it and have a better segmentation system for the paragraphs and or have chat to BT summarize them or something like that. But I just know that within a year, it's going to be a text-to-video API, and I'm just going to switch it, and it's going to be great. I'm laying the groundwork for infrastructure to have a multimodal content distribution, really, and I just expect it to become very fun. Even the text of voice is pretty good, I think. I don't have a studio, but once you have a studio, it's going to be able to generate perfect audio for whatever you want. Another one of my dreams that is bad for young students is I want to be able to give a slide deck to a script that returns the five-minute conference video that no one ever watched, just based on a GPT-4 reading the slide deck and voicing yourself. Those are the silly things that I have time to do because I'm not a professor. Yeah, I think these advances, these systems, they do generate a lot of economic value, and we're seeing that already. Lots of companies are now switching to using these things, and I think it's going to change the way we work, as you mentioned, the way we work, the way we entertain, so I'm just very exciting future. Yeah, anything else? Well, thanks for coming. Try to get you guys as much attention as I can bring. You never know, it'll go viral these days, so I think this was a great conversation. People are really hungry for basic intuitions in the area, so this is good. Yeah, thank you, Nathan. It was a pleasure. Absolutely. Thank you for inviting us, and maybe if there are more questions, is there a way to collect them to provide readers with listeners with an address or something, happy to answer anything? Yeah, I'll include contact info in the post in various ways. This will be out there. You'll get your comments on Substack, YouTube, Twitter. It's a mess. You've got to pay attention to 10 million streams of information these days, but you'll get contacted by people. Thankfully, for some reason, people read my stuff, but here we are. So thanks for listening.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.8, "text": " Okay. Hey, everyone. Welcome to the first interview that we're going to post on interconnects.", "tokens": [50364, 1033, 13, 1911, 11, 1518, 13, 4027, 281, 264, 700, 4049, 300, 321, 434, 516, 281, 2183, 322, 26253, 82, 13, 50704], "temperature": 0.0, "avg_logprob": -0.19461984454460865, "compression_ratio": 1.5575539568345325, "no_speech_prob": 0.11883795261383057}, {"id": 1, "seek": 0, "start": 6.8, "end": 12.88, "text": " I'm really trying to bring more scientific voices into the AI discourse as media is covering", "tokens": [50704, 286, 478, 534, 1382, 281, 1565, 544, 8134, 9802, 666, 264, 7318, 23938, 382, 3021, 307, 10322, 51008], "temperature": 0.0, "avg_logprob": -0.19461984454460865, "compression_ratio": 1.5575539568345325, "no_speech_prob": 0.11883795261383057}, {"id": 2, "seek": 0, "start": 12.88, "end": 18.36, "text": " a lot these days. I'm happy to be here with Michael Poli and Tree Dow, experts in some", "tokens": [51008, 257, 688, 613, 1708, 13, 286, 478, 2055, 281, 312, 510, 365, 5116, 3635, 72, 293, 22291, 20947, 11, 8572, 294, 512, 51282], "temperature": 0.0, "avg_logprob": -0.19461984454460865, "compression_ratio": 1.5575539568345325, "no_speech_prob": 0.11883795261383057}, {"id": 3, "seek": 0, "start": 18.36, "end": 22.72, "text": " of these non-attention architectures that have been really blowing up in the last few", "tokens": [51282, 295, 613, 2107, 12, 1591, 1251, 6331, 1303, 300, 362, 668, 534, 15068, 493, 294, 264, 1036, 1326, 51500], "temperature": 0.0, "avg_logprob": -0.19461984454460865, "compression_ratio": 1.5575539568345325, "no_speech_prob": 0.11883795261383057}, {"id": 4, "seek": 0, "start": 22.72, "end": 27.72, "text": " weeks of December. So, Michael, do you want to introduce yourself first?", "tokens": [51500, 3259, 295, 7687, 13, 407, 11, 5116, 11, 360, 291, 528, 281, 5366, 1803, 700, 30, 51750], "temperature": 0.0, "avg_logprob": -0.19461984454460865, "compression_ratio": 1.5575539568345325, "no_speech_prob": 0.11883795261383057}, {"id": 5, "seek": 2772, "start": 28.68, "end": 38.04, "text": " Sure. Thank you. Thank you, Nathan, for inviting me. I do research at Together AI and was also", "tokens": [50412, 4894, 13, 1044, 291, 13, 1044, 291, 11, 20634, 11, 337, 18202, 385, 13, 286, 360, 2132, 412, 15911, 7318, 293, 390, 611, 50880], "temperature": 0.0, "avg_logprob": -0.2824364197559846, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.0012783436104655266}, {"id": 6, "seek": 2772, "start": 38.04, "end": 45.72, "text": " a PhD student at Stanford working with Stefan Orenmann and Chris Rea. That's how I met Tree as well.", "tokens": [50880, 257, 14476, 3107, 412, 20374, 1364, 365, 32158, 422, 1095, 14912, 293, 6688, 1300, 64, 13, 663, 311, 577, 286, 1131, 22291, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2824364197559846, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.0012783436104655266}, {"id": 7, "seek": 2772, "start": 47.64, "end": 52.92, "text": " If I had to choose, maybe I moved to a few different areas of research. If I had to choose", "tokens": [51360, 759, 286, 632, 281, 2826, 11, 1310, 286, 4259, 281, 257, 1326, 819, 3179, 295, 2132, 13, 759, 286, 632, 281, 2826, 51624], "temperature": 0.0, "avg_logprob": -0.2824364197559846, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.0012783436104655266}, {"id": 8, "seek": 5292, "start": 52.92, "end": 59.480000000000004, "text": " one, I like to research at the intersection of signal processing, dynamical systems,", "tokens": [50364, 472, 11, 286, 411, 281, 2132, 412, 264, 15236, 295, 6358, 9007, 11, 5999, 804, 3652, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1796591842875761, "compression_ratio": 1.5, "no_speech_prob": 0.004060388542711735}, {"id": 9, "seek": 5292, "start": 59.480000000000004, "end": 65.96000000000001, "text": " and deep learning. Most recently, luckily, there's been more interest in efficient", "tokens": [50692, 293, 2452, 2539, 13, 4534, 3938, 11, 22880, 11, 456, 311, 668, 544, 1179, 294, 7148, 51016], "temperature": 0.0, "avg_logprob": -0.1796591842875761, "compression_ratio": 1.5, "no_speech_prob": 0.004060388542711735}, {"id": 10, "seek": 5292, "start": 65.96000000000001, "end": 71.96000000000001, "text": " architectures that use some of these principles to improve scaling along different axes", "tokens": [51016, 6331, 1303, 300, 764, 512, 295, 613, 9156, 281, 3470, 21589, 2051, 819, 35387, 51316], "temperature": 0.0, "avg_logprob": -0.1796591842875761, "compression_ratio": 1.5, "no_speech_prob": 0.004060388542711735}, {"id": 11, "seek": 5292, "start": 71.96000000000001, "end": 75.48, "text": " and to get new trade-offs at inference time.", "tokens": [51316, 293, 281, 483, 777, 4923, 12, 19231, 412, 38253, 565, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1796591842875761, "compression_ratio": 1.5, "no_speech_prob": 0.004060388542711735}, {"id": 12, "seek": 7548, "start": 75.96000000000001, "end": 79.24000000000001, "text": " Great. And Tree?", "tokens": [50388, 3769, 13, 400, 22291, 30, 50552], "temperature": 0.0, "avg_logprob": -0.23498984387046412, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.002628045855090022}, {"id": 13, "seek": 7548, "start": 81.08, "end": 88.92, "text": " Hi, everyone. Thanks, Nathan, for hosting us. I'm really excited to be here. I'm Tree. I", "tokens": [50644, 2421, 11, 1518, 13, 2561, 11, 20634, 11, 337, 16058, 505, 13, 286, 478, 534, 2919, 281, 312, 510, 13, 286, 478, 22291, 13, 286, 51036], "temperature": 0.0, "avg_logprob": -0.23498984387046412, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.002628045855090022}, {"id": 14, "seek": 7548, "start": 90.84, "end": 96.44, "text": " finished my PhD at Stanford. I'm an incoming assistant professor at Princeton. Right now,", "tokens": [51132, 4335, 452, 14476, 412, 20374, 13, 286, 478, 364, 22341, 10994, 8304, 412, 36592, 13, 1779, 586, 11, 51412], "temperature": 0.0, "avg_logprob": -0.23498984387046412, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.002628045855090022}, {"id": 15, "seek": 7548, "start": 96.44, "end": 101.72, "text": " I'm Chief Scientist at Together AI. This is a startup working on AI infrastructure.", "tokens": [51412, 286, 478, 10068, 18944, 468, 412, 15911, 7318, 13, 639, 307, 257, 18578, 1364, 322, 7318, 6896, 13, 51676], "temperature": 0.0, "avg_logprob": -0.23498984387046412, "compression_ratio": 1.3880597014925373, "no_speech_prob": 0.002628045855090022}, {"id": 16, "seek": 10172, "start": 102.6, "end": 107.96, "text": " And I've been working at the intersection of machine learning and systems, so designing", "tokens": [50408, 400, 286, 600, 668, 1364, 412, 264, 15236, 295, 3479, 2539, 293, 3652, 11, 370, 14685, 50676], "temperature": 0.0, "avg_logprob": -0.17633626937866212, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0018095985287800431}, {"id": 17, "seek": 10172, "start": 107.96, "end": 115.4, "text": " algorithms that take advantage of the hardware that they run on. We're interested in long-range", "tokens": [50676, 14642, 300, 747, 5002, 295, 264, 8837, 300, 436, 1190, 322, 13, 492, 434, 3102, 294, 938, 12, 14521, 51048], "temperature": 0.0, "avg_logprob": -0.17633626937866212, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0018095985287800431}, {"id": 18, "seek": 10172, "start": 116.12, "end": 119.48, "text": " dependencies, how to encode that into model, designing architectures that can", "tokens": [51084, 36606, 11, 577, 281, 2058, 1429, 300, 666, 2316, 11, 14685, 6331, 1303, 300, 393, 51252], "temperature": 0.0, "avg_logprob": -0.17633626937866212, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0018095985287800431}, {"id": 19, "seek": 10172, "start": 120.84, "end": 124.28, "text": " learn long-range dependencies. Yeah, really excited to be here.", "tokens": [51320, 1466, 938, 12, 14521, 36606, 13, 865, 11, 534, 2919, 281, 312, 510, 13, 51492], "temperature": 0.0, "avg_logprob": -0.17633626937866212, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0018095985287800431}, {"id": 20, "seek": 10172, "start": 125.88, "end": 130.04, "text": " Yeah. Okay. I think I'm going to just, I have some questions to dive right into this. I think", "tokens": [51572, 865, 13, 1033, 13, 286, 519, 286, 478, 516, 281, 445, 11, 286, 362, 512, 1651, 281, 9192, 558, 666, 341, 13, 286, 519, 51780], "temperature": 0.0, "avg_logprob": -0.17633626937866212, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0018095985287800431}, {"id": 21, "seek": 13004, "start": 130.84, "end": 135.0, "text": " you two will kind of both answer them or someone can answer longer, whatever you want. I think to", "tokens": [50404, 291, 732, 486, 733, 295, 1293, 1867, 552, 420, 1580, 393, 1867, 2854, 11, 2035, 291, 528, 13, 286, 519, 281, 50612], "temperature": 0.0, "avg_logprob": -0.14670500321821733, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.0024722046218812466}, {"id": 22, "seek": 13004, "start": 135.0, "end": 140.28, "text": " start with, we should talk about maybe why attention works and what the limitations of", "tokens": [50612, 722, 365, 11, 321, 820, 751, 466, 1310, 983, 3202, 1985, 293, 437, 264, 15705, 295, 50876], "temperature": 0.0, "avg_logprob": -0.14670500321821733, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.0024722046218812466}, {"id": 23, "seek": 13004, "start": 140.28, "end": 145.95999999999998, "text": " attention are. I think almost every person in tech, broadly, now knows that a transformer", "tokens": [50876, 3202, 366, 13, 286, 519, 1920, 633, 954, 294, 7553, 11, 19511, 11, 586, 3255, 300, 257, 31782, 51160], "temperature": 0.0, "avg_logprob": -0.14670500321821733, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.0024722046218812466}, {"id": 24, "seek": 13004, "start": 145.95999999999998, "end": 152.92, "text": " is a model built with attention. And ChatGPT does that, but why is this so good? Even how", "tokens": [51160, 307, 257, 2316, 3094, 365, 3202, 13, 400, 27503, 38, 47, 51, 775, 300, 11, 457, 983, 307, 341, 370, 665, 30, 2754, 577, 51508], "temperature": 0.0, "avg_logprob": -0.14670500321821733, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.0024722046218812466}, {"id": 25, "seek": 13004, "start": 152.92, "end": 157.72, "text": " much of a transformer is built with attention? Are there other things going on? And what might", "tokens": [51508, 709, 295, 257, 31782, 307, 3094, 365, 3202, 30, 2014, 456, 661, 721, 516, 322, 30, 400, 437, 1062, 51748], "temperature": 0.0, "avg_logprob": -0.14670500321821733, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.0024722046218812466}, {"id": 26, "seek": 15772, "start": 157.72, "end": 166.84, "text": " be challenges there? Right. So, you know, a transformer, which is this architecture that", "tokens": [50364, 312, 4759, 456, 30, 1779, 13, 407, 11, 291, 458, 11, 257, 31782, 11, 597, 307, 341, 9482, 300, 50820], "temperature": 0.0, "avg_logprob": -0.18155472366898148, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0017268492374569178}, {"id": 27, "seek": 15772, "start": 166.84, "end": 170.2, "text": " powers most of the exciting applications that we're seeing nowadays, as you mentioned,", "tokens": [50820, 8674, 881, 295, 264, 4670, 5821, 300, 321, 434, 2577, 13434, 11, 382, 291, 2835, 11, 50988], "temperature": 0.0, "avg_logprob": -0.18155472366898148, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0017268492374569178}, {"id": 28, "seek": 15772, "start": 170.2, "end": 177.8, "text": " ChatGPT and so on. Attention is kind of the core layer there. And attention actually", "tokens": [50988, 27503, 38, 47, 51, 293, 370, 322, 13, 31858, 307, 733, 295, 264, 4965, 4583, 456, 13, 400, 3202, 767, 51368], "temperature": 0.0, "avg_logprob": -0.18155472366898148, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0017268492374569178}, {"id": 29, "seek": 15772, "start": 178.6, "end": 184.6, "text": " became earlier around 2014, 2015, and then transformer came out, incorporating that focusing", "tokens": [51408, 3062, 3071, 926, 8227, 11, 7546, 11, 293, 550, 31782, 1361, 484, 11, 33613, 300, 8416, 51708], "temperature": 0.0, "avg_logprob": -0.18155472366898148, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0017268492374569178}, {"id": 30, "seek": 18460, "start": 184.6, "end": 195.88, "text": " a lot on attention with these MLP, interleaving MLP and attention. And I think the success largely", "tokens": [50364, 257, 688, 322, 3202, 365, 613, 21601, 47, 11, 728, 306, 6152, 21601, 47, 293, 3202, 13, 400, 286, 519, 264, 2245, 11611, 50928], "temperature": 0.0, "avg_logprob": -0.14839408204362198, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008424512343481183}, {"id": 31, "seek": 18460, "start": 195.88, "end": 204.2, "text": " has been, they seem to be able to scale really well. So you can scale out the models with more", "tokens": [50928, 575, 668, 11, 436, 1643, 281, 312, 1075, 281, 4373, 534, 731, 13, 407, 291, 393, 4373, 484, 264, 5245, 365, 544, 51344], "temperature": 0.0, "avg_logprob": -0.14839408204362198, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008424512343481183}, {"id": 32, "seek": 18460, "start": 204.2, "end": 209.88, "text": " parameters, with more data. And that has been the recipe for success. It sounds obvious now, but", "tokens": [51344, 9834, 11, 365, 544, 1412, 13, 400, 300, 575, 668, 264, 6782, 337, 2245, 13, 467, 3263, 6322, 586, 11, 457, 51628], "temperature": 0.0, "avg_logprob": -0.14839408204362198, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0008424512343481183}, {"id": 33, "seek": 20988, "start": 209.96, "end": 219.48, "text": " I think five years ago, that wasn't clear. So it seems like transformer architecture is a hugely", "tokens": [50368, 286, 519, 1732, 924, 2057, 11, 300, 2067, 380, 1850, 13, 407, 309, 2544, 411, 31782, 9482, 307, 257, 27417, 50844], "temperature": 0.0, "avg_logprob": -0.13934409450477278, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.0024334946647286415}, {"id": 34, "seek": 20988, "start": 219.48, "end": 228.51999999999998, "text": " successful one. And a couple of reasons why it's successful. I think it's general enough that", "tokens": [50844, 4406, 472, 13, 400, 257, 1916, 295, 4112, 983, 309, 311, 4406, 13, 286, 519, 309, 311, 2674, 1547, 300, 51296], "temperature": 0.0, "avg_logprob": -0.13934409450477278, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.0024334946647286415}, {"id": 35, "seek": 20988, "start": 228.51999999999998, "end": 235.56, "text": " it's able to learn a lot from data. And two, it's pretty friendly to hardware. There's no", "tokens": [51296, 309, 311, 1075, 281, 1466, 257, 688, 490, 1412, 13, 400, 732, 11, 309, 311, 1238, 9208, 281, 8837, 13, 821, 311, 572, 51648], "temperature": 0.0, "avg_logprob": -0.13934409450477278, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.0024334946647286415}, {"id": 36, "seek": 23556, "start": 235.56, "end": 242.92000000000002, "text": " kind of sequential dependency like previous RNNs. So as a result, you can run it well on GPUs,", "tokens": [50364, 733, 295, 42881, 33621, 411, 3894, 45702, 45, 82, 13, 407, 382, 257, 1874, 11, 291, 393, 1190, 309, 731, 322, 18407, 82, 11, 50732], "temperature": 0.0, "avg_logprob": -0.11652307709058125, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.007341568823903799}, {"id": 37, "seek": 23556, "start": 242.92000000000002, "end": 249.4, "text": " TPUs. You can scale it up. It's very hardware efficient. I personally have worked on making it", "tokens": [50732, 314, 8115, 82, 13, 509, 393, 4373, 309, 493, 13, 467, 311, 588, 8837, 7148, 13, 286, 5665, 362, 2732, 322, 1455, 309, 51056], "temperature": 0.0, "avg_logprob": -0.11652307709058125, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.007341568823903799}, {"id": 38, "seek": 23556, "start": 249.4, "end": 253.8, "text": " more hardware efficient as well. So it's just kind of the recipe for success, which is", "tokens": [51056, 544, 8837, 7148, 382, 731, 13, 407, 309, 311, 445, 733, 295, 264, 6782, 337, 2245, 11, 597, 307, 51276], "temperature": 0.0, "avg_logprob": -0.11652307709058125, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.007341568823903799}, {"id": 39, "seek": 23556, "start": 255.88, "end": 262.2, "text": " general architecture that scales well. If you're an NLP person, maybe you'll say to incorporate", "tokens": [51380, 2674, 9482, 300, 17408, 731, 13, 759, 291, 434, 364, 426, 45196, 954, 11, 1310, 291, 603, 584, 281, 16091, 51696], "temperature": 0.0, "avg_logprob": -0.11652307709058125, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.007341568823903799}, {"id": 40, "seek": 26220, "start": 262.2, "end": 267.24, "text": " some kind of inductive bias for it to protect. Personally, I think it's a very general architecture", "tokens": [50364, 512, 733, 295, 31612, 488, 12577, 337, 309, 281, 2371, 13, 21079, 11, 286, 519, 309, 311, 257, 588, 2674, 9482, 50616], "temperature": 0.0, "avg_logprob": -0.12937091995071578, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0006877667619846761}, {"id": 41, "seek": 26220, "start": 267.24, "end": 274.68, "text": " that scales well and it's hardware friendly. Yeah, it's remarkable how it seems so obvious now.", "tokens": [50616, 300, 17408, 731, 293, 309, 311, 8837, 9208, 13, 865, 11, 309, 311, 12802, 577, 309, 2544, 370, 6322, 586, 13, 50988], "temperature": 0.0, "avg_logprob": -0.12937091995071578, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0006877667619846761}, {"id": 42, "seek": 26220, "start": 274.68, "end": 281.0, "text": " And it's like, I think one of the things that studying this work is the context length becomes", "tokens": [50988, 400, 309, 311, 411, 11, 286, 519, 472, 295, 264, 721, 300, 7601, 341, 589, 307, 264, 4319, 4641, 3643, 51304], "temperature": 0.0, "avg_logprob": -0.12937091995071578, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0006877667619846761}, {"id": 43, "seek": 26220, "start": 281.0, "end": 288.91999999999996, "text": " a really interesting access to study alternatives to it. And essentially, Michael, do you want to", "tokens": [51304, 257, 534, 1880, 2105, 281, 2979, 20478, 281, 309, 13, 400, 4476, 11, 5116, 11, 360, 291, 528, 281, 51700], "temperature": 0.0, "avg_logprob": -0.12937091995071578, "compression_ratio": 1.603305785123967, "no_speech_prob": 0.0006877667619846761}, {"id": 44, "seek": 28892, "start": 288.92, "end": 298.12, "text": " talk about that? I could babble, but you know more. Sure. Yeah, there are several points. I'll", "tokens": [50364, 751, 466, 300, 30, 286, 727, 7564, 638, 11, 457, 291, 458, 544, 13, 4894, 13, 865, 11, 456, 366, 2940, 2793, 13, 286, 603, 50824], "temperature": 0.0, "avg_logprob": -0.16365883771110984, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0012839117553085089}, {"id": 45, "seek": 28892, "start": 298.12, "end": 304.84000000000003, "text": " start by saying that there's still great research trying to understand why, from first principles,", "tokens": [50824, 722, 538, 1566, 300, 456, 311, 920, 869, 2132, 1382, 281, 1223, 983, 11, 490, 700, 9156, 11, 51160], "temperature": 0.0, "avg_logprob": -0.16365883771110984, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0012839117553085089}, {"id": 46, "seek": 28892, "start": 304.84000000000003, "end": 311.64, "text": " why is it that the transformer can learn these interesting circuits. People can study. They", "tokens": [51160, 983, 307, 309, 300, 264, 31782, 393, 1466, 613, 1880, 26354, 13, 3432, 393, 2979, 13, 814, 51500], "temperature": 0.0, "avg_logprob": -0.16365883771110984, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0012839117553085089}, {"id": 47, "seek": 28892, "start": 311.64, "end": 317.24, "text": " pick apart the computational combination of different heads and transformers and so on.", "tokens": [51500, 1888, 4936, 264, 28270, 6562, 295, 819, 8050, 293, 4088, 433, 293, 370, 322, 13, 51780], "temperature": 0.0, "avg_logprob": -0.16365883771110984, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.0012839117553085089}, {"id": 48, "seek": 31724, "start": 318.2, "end": 323.72, "text": " So there's work on basically understanding transformers from programming language that is", "tokens": [50412, 407, 456, 311, 589, 322, 1936, 3701, 4088, 433, 490, 9410, 2856, 300, 307, 50688], "temperature": 0.0, "avg_logprob": -0.16577819677499625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00032793512218631804}, {"id": 49, "seek": 31724, "start": 323.72, "end": 333.48, "text": " encoded. But I think, as Trey mentioned, there are some very interesting design choices that", "tokens": [50688, 2058, 12340, 13, 583, 286, 519, 11, 382, 314, 7950, 2835, 11, 456, 366, 512, 588, 1880, 1715, 7994, 300, 51176], "temperature": 0.0, "avg_logprob": -0.16577819677499625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00032793512218631804}, {"id": 50, "seek": 31724, "start": 333.48, "end": 337.8, "text": " have gone into the transformer. This interleaving of attention in NLP is quite important.", "tokens": [51176, 362, 2780, 666, 264, 31782, 13, 639, 728, 306, 6152, 295, 3202, 294, 426, 45196, 307, 1596, 1021, 13, 51392], "temperature": 0.0, "avg_logprob": -0.16577819677499625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00032793512218631804}, {"id": 51, "seek": 31724, "start": 339.32, "end": 343.96000000000004, "text": " And also, the transformer is essentially successful in the beginning because it encoded", "tokens": [51468, 400, 611, 11, 264, 31782, 307, 4476, 4406, 294, 264, 2863, 570, 309, 2058, 12340, 51700], "temperature": 0.0, "avg_logprob": -0.16577819677499625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00032793512218631804}, {"id": 52, "seek": 34396, "start": 344.03999999999996, "end": 352.91999999999996, "text": " these techniques that have been developed for RNN, Celestium, so these other classical NLP models", "tokens": [50368, 613, 7512, 300, 362, 668, 4743, 337, 45702, 45, 11, 19967, 377, 2197, 11, 370, 613, 661, 13735, 426, 45196, 5245, 50812], "temperature": 0.0, "avg_logprob": -0.14709709001624066, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.0006382030551321805}, {"id": 53, "seek": 34396, "start": 352.91999999999996, "end": 361.0, "text": " gating to modulate which information is absorbed into the model, gating to determine how quickly", "tokens": [50812, 290, 990, 281, 1072, 5256, 597, 1589, 307, 20799, 666, 264, 2316, 11, 290, 990, 281, 6997, 577, 2661, 51216], "temperature": 0.0, "avg_logprob": -0.14709709001624066, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.0006382030551321805}, {"id": 54, "seek": 34396, "start": 361.0, "end": 369.0, "text": " something is forgotten in this recurrence of the RNN into a parallel form. It is easily a bunch", "tokens": [51216, 746, 307, 11832, 294, 341, 18680, 10760, 295, 264, 45702, 45, 666, 257, 8952, 1254, 13, 467, 307, 3612, 257, 3840, 51616], "temperature": 0.0, "avg_logprob": -0.14709709001624066, "compression_ratio": 1.4720812182741116, "no_speech_prob": 0.0006382030551321805}, {"id": 55, "seek": 36900, "start": 369.0, "end": 374.04, "text": " of jumps that can be easily, but not very easily, but can be optimized for RNN GPUs.", "tokens": [50364, 295, 16704, 300, 393, 312, 3612, 11, 457, 406, 588, 3612, 11, 457, 393, 312, 26941, 337, 45702, 45, 18407, 82, 13, 50616], "temperature": 0.0, "avg_logprob": -0.14911745275769914, "compression_ratio": 1.7192307692307693, "no_speech_prob": 0.0037643699906766415}, {"id": 56, "seek": 36900, "start": 376.52, "end": 381.56, "text": " Yeah, this stuff's all great. I guess the specific thing that I had in mind is how", "tokens": [50740, 865, 11, 341, 1507, 311, 439, 869, 13, 286, 2041, 264, 2685, 551, 300, 286, 632, 294, 1575, 307, 577, 50992], "temperature": 0.0, "avg_logprob": -0.14911745275769914, "compression_ratio": 1.7192307692307693, "no_speech_prob": 0.0037643699906766415}, {"id": 57, "seek": 36900, "start": 381.56, "end": 387.96, "text": " attention ends up being this kind of quadratic scaling in terms of cost when you have an input", "tokens": [50992, 3202, 5314, 493, 885, 341, 733, 295, 37262, 21589, 294, 2115, 295, 2063, 562, 291, 362, 364, 4846, 51312], "temperature": 0.0, "avg_logprob": -0.14911745275769914, "compression_ratio": 1.7192307692307693, "no_speech_prob": 0.0037643699906766415}, {"id": 58, "seek": 36900, "start": 387.96, "end": 391.64, "text": " sequence that you have. If you have an input sequence of length L and you want to output a", "tokens": [51312, 8310, 300, 291, 362, 13, 759, 291, 362, 364, 4846, 8310, 295, 4641, 441, 293, 291, 528, 281, 5598, 257, 51496], "temperature": 0.0, "avg_logprob": -0.14911745275769914, "compression_ratio": 1.7192307692307693, "no_speech_prob": 0.0037643699906766415}, {"id": 59, "seek": 36900, "start": 391.64, "end": 396.76, "text": " sequence of length L, essentially, if you zoom into the math and you look at what's happening", "tokens": [51496, 8310, 295, 4641, 441, 11, 4476, 11, 498, 291, 8863, 666, 264, 5221, 293, 291, 574, 412, 437, 311, 2737, 51752], "temperature": 0.0, "avg_logprob": -0.14911745275769914, "compression_ratio": 1.7192307692307693, "no_speech_prob": 0.0037643699906766415}, {"id": 60, "seek": 39676, "start": 396.92, "end": 400.84, "text": " in inference in most of these libraries, you have this upper triangular attention matrix where you", "tokens": [50372, 294, 38253, 294, 881, 295, 613, 15148, 11, 291, 362, 341, 6597, 38190, 3202, 8141, 689, 291, 50568], "temperature": 0.0, "avg_logprob": -0.11733253796895345, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.011682967655360699}, {"id": 61, "seek": 39676, "start": 400.84, "end": 406.44, "text": " say you can only look at the past entries of your text, and as you go through there then,", "tokens": [50568, 584, 291, 393, 787, 574, 412, 264, 1791, 23041, 295, 428, 2487, 11, 293, 382, 291, 352, 807, 456, 550, 11, 50848], "temperature": 0.0, "avg_logprob": -0.11733253796895345, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.011682967655360699}, {"id": 62, "seek": 39676, "start": 406.44, "end": 412.2, "text": " you end up getting this L squared relationship where the first token you can only look at one,", "tokens": [50848, 291, 917, 493, 1242, 341, 441, 8889, 2480, 689, 264, 700, 14862, 291, 393, 787, 574, 412, 472, 11, 51136], "temperature": 0.0, "avg_logprob": -0.11733253796895345, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.011682967655360699}, {"id": 63, "seek": 39676, "start": 412.2, "end": 418.59999999999997, "text": " and then you end up looking at more tokens for each pass. And now we've been talking about", "tokens": [51136, 293, 550, 291, 917, 493, 1237, 412, 544, 22667, 337, 1184, 1320, 13, 400, 586, 321, 600, 668, 1417, 466, 51456], "temperature": 0.0, "avg_logprob": -0.11733253796895345, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.011682967655360699}, {"id": 64, "seek": 39676, "start": 418.59999999999997, "end": 425.71999999999997, "text": " recurrent neural networks, and how does something that isn't attention get around the fact that you", "tokens": [51456, 18680, 1753, 18161, 9590, 11, 293, 577, 775, 746, 300, 1943, 380, 3202, 483, 926, 264, 1186, 300, 291, 51812], "temperature": 0.0, "avg_logprob": -0.11733253796895345, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.011682967655360699}, {"id": 65, "seek": 42572, "start": 425.72, "end": 431.48, "text": " want to look at all of the history of the text in your sequence? So if you write a long prompt to", "tokens": [50364, 528, 281, 574, 412, 439, 295, 264, 2503, 295, 264, 2487, 294, 428, 8310, 30, 407, 498, 291, 2464, 257, 938, 12391, 281, 50652], "temperature": 0.0, "avg_logprob": -0.16507522479907885, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.0003569133405108005}, {"id": 66, "seek": 42572, "start": 431.48, "end": 437.48, "text": " chat GPT, you really want all that information to be encoded, and how can doing something other than", "tokens": [50652, 5081, 26039, 51, 11, 291, 534, 528, 439, 300, 1589, 281, 312, 2058, 12340, 11, 293, 577, 393, 884, 746, 661, 813, 50952], "temperature": 0.0, "avg_logprob": -0.16507522479907885, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.0003569133405108005}, {"id": 67, "seek": 42572, "start": 437.48, "end": 449.40000000000003, "text": " this dense attention matrix actually make that possible? Yeah. Right. Yeah, so you can go ahead.", "tokens": [50952, 341, 18011, 3202, 8141, 767, 652, 300, 1944, 30, 865, 13, 1779, 13, 865, 11, 370, 291, 393, 352, 2286, 13, 51548], "temperature": 0.0, "avg_logprob": -0.16507522479907885, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.0003569133405108005}, {"id": 68, "seek": 44940, "start": 449.4, "end": 454.28, "text": " Before attention, there was RNNs, right? And then RNNs, they processed text, which is fine.", "tokens": [50364, 4546, 3202, 11, 456, 390, 45702, 45, 82, 11, 558, 30, 400, 550, 45702, 45, 82, 11, 436, 18846, 2487, 11, 597, 307, 2489, 13, 50608], "temperature": 0.0, "avg_logprob": -0.22260211944580077, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.15195338428020477}, {"id": 69, "seek": 44940, "start": 455.64, "end": 460.59999999999997, "text": " And maybe they didn't scale as well, but yeah. Can you say briefly what a...", "tokens": [50676, 400, 1310, 436, 994, 380, 4373, 382, 731, 11, 457, 1338, 13, 1664, 291, 584, 10515, 437, 257, 485, 50924], "temperature": 0.0, "avg_logprob": -0.22260211944580077, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.15195338428020477}, {"id": 70, "seek": 44940, "start": 460.59999999999997, "end": 464.76, "text": " They processed text by encoding text. Can you say briefly what a RNN is and how it works?", "tokens": [50924, 814, 18846, 2487, 538, 43430, 2487, 13, 1664, 291, 584, 10515, 437, 257, 45702, 45, 307, 293, 577, 309, 1985, 30, 51132], "temperature": 0.0, "avg_logprob": -0.22260211944580077, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.15195338428020477}, {"id": 71, "seek": 44940, "start": 467.08, "end": 472.28, "text": " Yeah, so these are recurrent neural nets that go back, I think, to the 80s. Maybe some of the", "tokens": [51248, 865, 11, 370, 613, 366, 18680, 1753, 18161, 36170, 300, 352, 646, 11, 286, 519, 11, 281, 264, 4688, 82, 13, 2704, 512, 295, 264, 51508], "temperature": 0.0, "avg_logprob": -0.22260211944580077, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.15195338428020477}, {"id": 72, "seek": 47228, "start": 472.28, "end": 482.84, "text": " more famous ones are LSDMs, GRU. So they were pretty popular around 2012 to 2016 or so.", "tokens": [50364, 544, 4618, 2306, 366, 441, 23969, 26386, 11, 10903, 52, 13, 407, 436, 645, 1238, 3743, 926, 9125, 281, 6549, 420, 370, 13, 50892], "temperature": 0.0, "avg_logprob": -0.20658739952191915, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.039017461240291595}, {"id": 73, "seek": 47228, "start": 483.79999999999995, "end": 486.76, "text": " They were kind of state-of-the-art for translation, speech recognition,", "tokens": [50940, 814, 645, 733, 295, 1785, 12, 2670, 12, 3322, 12, 446, 337, 12853, 11, 6218, 11150, 11, 51088], "temperature": 0.0, "avg_logprob": -0.20658739952191915, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.039017461240291595}, {"id": 74, "seek": 47228, "start": 488.84, "end": 496.28, "text": " I think NLP, they were a state-of-the-art, and they processed text kind of sequentially.", "tokens": [51192, 286, 519, 426, 45196, 11, 436, 645, 257, 1785, 12, 2670, 12, 3322, 12, 446, 11, 293, 436, 18846, 2487, 733, 295, 5123, 3137, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20658739952191915, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.039017461240291595}, {"id": 75, "seek": 49628, "start": 497.08, "end": 506.44, "text": " They see essentially one token, and then that changes their hidden state, and then they will", "tokens": [50404, 814, 536, 4476, 472, 14862, 11, 293, 550, 300, 2962, 641, 7633, 1785, 11, 293, 550, 436, 486, 50872], "temperature": 0.0, "avg_logprob": -0.21850688020947953, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.004329650662839413}, {"id": 76, "seek": 49628, "start": 506.44, "end": 513.9599999999999, "text": " update the hidden state, and every time they see a new token. So I think it's kind of, in some sense,", "tokens": [50872, 5623, 264, 7633, 1785, 11, 293, 633, 565, 436, 536, 257, 777, 14862, 13, 407, 286, 519, 309, 311, 733, 295, 11, 294, 512, 2020, 11, 51248], "temperature": 0.0, "avg_logprob": -0.21850688020947953, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.004329650662839413}, {"id": 77, "seek": 49628, "start": 513.9599999999999, "end": 521.88, "text": " mimicking how, for example, human brain process information, like you read the sentence or a", "tokens": [51248, 12247, 10401, 577, 11, 337, 1365, 11, 1952, 3567, 1399, 1589, 11, 411, 291, 1401, 264, 8174, 420, 257, 51644], "temperature": 0.0, "avg_logprob": -0.21850688020947953, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.004329650662839413}, {"id": 78, "seek": 52188, "start": 521.88, "end": 527.88, "text": " passage, and maybe it's like you're storing some information in your brain, and by the time you", "tokens": [50364, 11497, 11, 293, 1310, 309, 311, 411, 291, 434, 26085, 512, 1589, 294, 428, 3567, 11, 293, 538, 264, 565, 291, 50664], "temperature": 0.0, "avg_logprob": -0.13458826065063476, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0024720104411244392}, {"id": 79, "seek": 52188, "start": 527.88, "end": 533.32, "text": " finish reading the document, maybe you can answer questions about the documents without having to", "tokens": [50664, 2413, 3760, 264, 4166, 11, 1310, 291, 393, 1867, 1651, 466, 264, 8512, 1553, 1419, 281, 50936], "temperature": 0.0, "avg_logprob": -0.13458826065063476, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0024720104411244392}, {"id": 80, "seek": 52188, "start": 534.28, "end": 540.76, "text": " refer to that document again. So RNNs kind of work that way. They process the text,", "tokens": [50984, 2864, 281, 300, 4166, 797, 13, 407, 45702, 45, 82, 733, 295, 589, 300, 636, 13, 814, 1399, 264, 2487, 11, 51308], "temperature": 0.0, "avg_logprob": -0.13458826065063476, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0024720104411244392}, {"id": 81, "seek": 52188, "start": 541.56, "end": 544.52, "text": " and then that changes the hidden state, and their hidden state is the representation that", "tokens": [51348, 293, 550, 300, 2962, 264, 7633, 1785, 11, 293, 641, 7633, 1785, 307, 264, 10290, 300, 51496], "temperature": 0.0, "avg_logprob": -0.13458826065063476, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0024720104411244392}, {"id": 82, "seek": 52188, "start": 544.52, "end": 550.76, "text": " can be used to either generate new tokens or classify the documents or whatnot.", "tokens": [51496, 393, 312, 1143, 281, 2139, 8460, 777, 22667, 420, 33872, 264, 8512, 420, 25882, 13, 51808], "temperature": 0.0, "avg_logprob": -0.13458826065063476, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0024720104411244392}, {"id": 83, "seek": 55188, "start": 552.36, "end": 562.12, "text": " These work well back in 2016 or so, but they've kind of fallen out of favor, empirically, they don't", "tokens": [50388, 1981, 589, 731, 646, 294, 6549, 420, 370, 11, 457, 436, 600, 733, 295, 11547, 484, 295, 2294, 11, 25790, 984, 11, 436, 500, 380, 50876], "temperature": 0.0, "avg_logprob": -0.1759941445456611, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.00011771285062422976}, {"id": 84, "seek": 55188, "start": 562.12, "end": 567.32, "text": " do as well as Transformer, I think, and as you touch on Transformer, because of this kind of", "tokens": [50876, 360, 382, 731, 382, 27938, 260, 11, 286, 519, 11, 293, 382, 291, 2557, 322, 27938, 260, 11, 570, 295, 341, 733, 295, 51136], "temperature": 0.0, "avg_logprob": -0.1759941445456611, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.00011771285062422976}, {"id": 85, "seek": 55188, "start": 567.32, "end": 572.68, "text": " quadratic scaling, you compare every token with every other token that comes before it,", "tokens": [51136, 37262, 21589, 11, 291, 6794, 633, 14862, 365, 633, 661, 14862, 300, 1487, 949, 309, 11, 51404], "temperature": 0.0, "avg_logprob": -0.1759941445456611, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.00011771285062422976}, {"id": 86, "seek": 57268, "start": 573.4, "end": 583.9599999999999, "text": " it gives you this very kind of easy way to propagate information. And I think that's for a", "tokens": [50400, 309, 2709, 291, 341, 588, 733, 295, 1858, 636, 281, 48256, 1589, 13, 400, 286, 519, 300, 311, 337, 257, 50928], "temperature": 0.0, "avg_logprob": -0.20418893968736804, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.013211049139499664}, {"id": 87, "seek": 57268, "start": 583.9599999999999, "end": 592.1999999999999, "text": " reason why Transformer and attention does really well. But there's been more recently some of the", "tokens": [50928, 1778, 983, 27938, 260, 293, 3202, 775, 534, 731, 13, 583, 456, 311, 668, 544, 3938, 512, 295, 264, 51340], "temperature": 0.0, "avg_logprob": -0.20418893968736804, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.013211049139499664}, {"id": 88, "seek": 57268, "start": 592.1999999999999, "end": 601.0799999999999, "text": " new RNN architectures that seem to do pretty well. So RWKV is, I think, one of the earlier ones,", "tokens": [51340, 777, 45702, 45, 6331, 1303, 300, 1643, 281, 360, 1238, 731, 13, 407, 42513, 42, 53, 307, 11, 286, 519, 11, 472, 295, 264, 3071, 2306, 11, 51784], "temperature": 0.0, "avg_logprob": -0.20418893968736804, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.013211049139499664}, {"id": 89, "seek": 60108, "start": 602.0400000000001, "end": 610.36, "text": " you know, it's one, I really admire that project, you know, it's effort mostly from one person,", "tokens": [50412, 291, 458, 11, 309, 311, 472, 11, 286, 534, 21951, 300, 1716, 11, 291, 458, 11, 309, 311, 4630, 5240, 490, 472, 954, 11, 50828], "temperature": 0.0, "avg_logprob": -0.19491239388783774, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0074564190581440926}, {"id": 90, "seek": 60108, "start": 610.36, "end": 616.84, "text": " really going against the orthodoxy of Transformer, showing that RNNs can be pretty strong.", "tokens": [50828, 534, 516, 1970, 264, 19052, 22189, 88, 295, 27938, 260, 11, 4099, 300, 45702, 45, 82, 393, 312, 1238, 2068, 13, 51152], "temperature": 0.0, "avg_logprob": -0.19491239388783774, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0074564190581440926}, {"id": 91, "seek": 60108, "start": 617.88, "end": 620.36, "text": " Who was the lead on that? I think it's this person,", "tokens": [51204, 2102, 390, 264, 1477, 322, 300, 30, 286, 519, 309, 311, 341, 954, 11, 51328], "temperature": 0.0, "avg_logprob": -0.19491239388783774, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0074564190581440926}, {"id": 92, "seek": 60108, "start": 623.72, "end": 629.96, "text": " Bo Peng, I think, and you know, it's an entire project, but I think it's pioneered by Bo Peng.", "tokens": [51496, 3286, 25783, 11, 286, 519, 11, 293, 291, 458, 11, 309, 311, 364, 2302, 1716, 11, 457, 286, 519, 309, 311, 19761, 4073, 538, 3286, 25783, 13, 51808], "temperature": 0.0, "avg_logprob": -0.19491239388783774, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.0074564190581440926}, {"id": 93, "seek": 62996, "start": 630.0400000000001, "end": 636.84, "text": " I think it's affiliated with Eluta AI and the Compute Sponsor by Stability and so on.", "tokens": [50368, 286, 519, 309, 311, 42174, 365, 2699, 12093, 7318, 293, 264, 6620, 1169, 1738, 892, 284, 538, 745, 2310, 293, 370, 322, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1853823878548362, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.0004172631015535444}, {"id": 94, "seek": 62996, "start": 636.84, "end": 643.0, "text": " Yeah, I was reading this earlier. At a technical level, they try to replicate something like the", "tokens": [50708, 865, 11, 286, 390, 3760, 341, 3071, 13, 1711, 257, 6191, 1496, 11, 436, 853, 281, 25356, 746, 411, 264, 51016], "temperature": 0.0, "avg_logprob": -0.1853823878548362, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.0004172631015535444}, {"id": 95, "seek": 62996, "start": 643.0, "end": 651.8000000000001, "text": " very key value lookup of attention with two linear RNNs to essentially be able to remove", "tokens": [51016, 588, 2141, 2158, 574, 1010, 295, 3202, 365, 732, 8213, 45702, 45, 82, 281, 4476, 312, 1075, 281, 4159, 51456], "temperature": 0.0, "avg_logprob": -0.1853823878548362, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.0004172631015535444}, {"id": 96, "seek": 62996, "start": 651.8000000000001, "end": 658.76, "text": " the specific attention scaling potential problems and with two RNNs, which have this", "tokens": [51456, 264, 2685, 3202, 21589, 3995, 2740, 293, 365, 732, 45702, 45, 82, 11, 597, 362, 341, 51804], "temperature": 0.0, "avg_logprob": -0.1853823878548362, "compression_ratio": 1.5478260869565217, "no_speech_prob": 0.0004172631015535444}, {"id": 97, "seek": 65876, "start": 658.76, "end": 661.88, "text": " better, like, long context behavior and different implementation rules.", "tokens": [50364, 1101, 11, 411, 11, 938, 4319, 5223, 293, 819, 11420, 4474, 13, 50520], "temperature": 0.0, "avg_logprob": -0.17501551076906538, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.002630587201565504}, {"id": 98, "seek": 65876, "start": 662.4399999999999, "end": 666.6, "text": " I think, and they also, the paper, trained up to 14 billion parameters, which kind of leads", "tokens": [50548, 286, 519, 11, 293, 436, 611, 11, 264, 3035, 11, 8895, 493, 281, 3499, 5218, 9834, 11, 597, 733, 295, 6689, 50756], "temperature": 0.0, "avg_logprob": -0.17501551076906538, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.002630587201565504}, {"id": 99, "seek": 65876, "start": 666.6, "end": 672.04, "text": " into some of the next questions. I was going to ask, I was going to ask Tree about Mamba and then", "tokens": [50756, 666, 512, 295, 264, 958, 1651, 13, 286, 390, 516, 281, 1029, 11, 286, 390, 516, 281, 1029, 22291, 466, 376, 23337, 293, 550, 51028], "temperature": 0.0, "avg_logprob": -0.17501551076906538, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.002630587201565504}, {"id": 100, "seek": 65876, "start": 672.04, "end": 677.0, "text": " Michael about Stripe Tahina. I think you could go in either order. I think these came out about", "tokens": [51028, 5116, 466, 20390, 494, 31027, 1426, 13, 286, 519, 291, 727, 352, 294, 2139, 1668, 13, 286, 519, 613, 1361, 484, 466, 51276], "temperature": 0.0, "avg_logprob": -0.17501551076906538, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.002630587201565504}, {"id": 101, "seek": 65876, "start": 677.0, "end": 683.4, "text": " a week apart and were these two language models kind of seen as being way closer than anyone", "tokens": [51276, 257, 1243, 4936, 293, 645, 613, 732, 2856, 5245, 733, 295, 1612, 382, 885, 636, 4966, 813, 2878, 51596], "temperature": 0.0, "avg_logprob": -0.17501551076906538, "compression_ratio": 1.6728624535315986, "no_speech_prob": 0.002630587201565504}, {"id": 102, "seek": 68340, "start": 683.4, "end": 688.68, "text": " would expect. Essentially, Stripe Tahina came out and the evaluations were close to models", "tokens": [50364, 576, 2066, 13, 23596, 11, 20390, 494, 31027, 1426, 1361, 484, 293, 264, 43085, 645, 1998, 281, 5245, 50628], "temperature": 0.0, "avg_logprob": -0.1357111136118571, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.04335137829184532}, {"id": 103, "seek": 68340, "start": 688.68, "end": 694.52, "text": " I've been training on all year, like Lama 2 and Mistral 7B, and I went in and I went to the", "tokens": [50628, 286, 600, 668, 3097, 322, 439, 1064, 11, 411, 441, 2404, 568, 293, 20166, 2155, 1614, 33, 11, 293, 286, 1437, 294, 293, 286, 1437, 281, 264, 50920], "temperature": 0.0, "avg_logprob": -0.1357111136118571, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.04335137829184532}, {"id": 104, "seek": 68340, "start": 695.16, "end": 701.24, "text": " Together API and I did like side by side of Mistral versus Stripe Tahina and it's like,", "tokens": [50952, 15911, 9362, 293, 286, 630, 411, 1252, 538, 1252, 295, 20166, 2155, 5717, 20390, 494, 31027, 1426, 293, 309, 311, 411, 11, 51256], "temperature": 0.0, "avg_logprob": -0.1357111136118571, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.04335137829184532}, {"id": 105, "seek": 68340, "start": 701.24, "end": 705.0799999999999, "text": " it's a good language model. It answers most questions. There's no obvious failure modes.", "tokens": [51256, 309, 311, 257, 665, 2856, 2316, 13, 467, 6338, 881, 1651, 13, 821, 311, 572, 6322, 7763, 14068, 13, 51448], "temperature": 0.0, "avg_logprob": -0.1357111136118571, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.04335137829184532}, {"id": 106, "seek": 68340, "start": 705.64, "end": 709.8, "text": " I think maybe Michael, do you want to comment on that? I know it's another big project and then", "tokens": [51476, 286, 519, 1310, 5116, 11, 360, 291, 528, 281, 2871, 322, 300, 30, 286, 458, 309, 311, 1071, 955, 1716, 293, 550, 51684], "temperature": 0.0, "avg_logprob": -0.1357111136118571, "compression_ratio": 1.6192170818505338, "no_speech_prob": 0.04335137829184532}, {"id": 107, "seek": 70980, "start": 709.8, "end": 713.56, "text": " we can go back to Mamba, even though it's slightly out of order in the chronological", "tokens": [50364, 321, 393, 352, 646, 281, 376, 23337, 11, 754, 1673, 309, 311, 4748, 484, 295, 1668, 294, 264, 19393, 4383, 50552], "temperature": 0.0, "avg_logprob": -0.1651472678551307, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0002268881507916376}, {"id": 108, "seek": 70980, "start": 714.1999999999999, "end": 723.16, "text": " the release cycle that happened. Sure. I guess I'll start by saying that", "tokens": [50584, 264, 4374, 6586, 300, 2011, 13, 4894, 13, 286, 2041, 286, 603, 722, 538, 1566, 300, 51032], "temperature": 0.0, "avg_logprob": -0.1651472678551307, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0002268881507916376}, {"id": 109, "seek": 70980, "start": 724.28, "end": 730.8399999999999, "text": " there's an interesting connection between all these new methods. There's this sort of convex set,", "tokens": [51088, 456, 311, 364, 1880, 4984, 1296, 439, 613, 777, 7150, 13, 821, 311, 341, 1333, 295, 42432, 992, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1651472678551307, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0002268881507916376}, {"id": 110, "seek": 70980, "start": 731.8, "end": 736.28, "text": " which has a center and there's this connection between linear attention,", "tokens": [51464, 597, 575, 257, 3056, 293, 456, 311, 341, 4984, 1296, 8213, 3202, 11, 51688], "temperature": 0.0, "avg_logprob": -0.1651472678551307, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.0002268881507916376}, {"id": 111, "seek": 73628, "start": 737.0, "end": 743.48, "text": " so attention without the softmax, linear RNNs, and state space models, SSM.", "tokens": [50400, 370, 3202, 1553, 264, 2787, 41167, 11, 8213, 45702, 45, 82, 11, 293, 1785, 1901, 5245, 11, 12238, 44, 13, 50724], "temperature": 0.0, "avg_logprob": -0.23419387166093036, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0007433143327943981}, {"id": 112, "seek": 73628, "start": 744.28, "end": 749.16, "text": " So at some level, the mathematical formulation of this kind of base model here,", "tokens": [50764, 407, 412, 512, 1496, 11, 264, 18894, 37642, 295, 341, 733, 295, 3096, 2316, 510, 11, 51008], "temperature": 0.0, "avg_logprob": -0.23419387166093036, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0007433143327943981}, {"id": 113, "seek": 73628, "start": 750.12, "end": 754.6, "text": " I'm not talking about the base architecture, just the fundamental model, is the same.", "tokens": [51056, 286, 478, 406, 1417, 466, 264, 3096, 9482, 11, 445, 264, 8088, 2316, 11, 307, 264, 912, 13, 51280], "temperature": 0.0, "avg_logprob": -0.23419387166093036, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0007433143327943981}, {"id": 114, "seek": 73628, "start": 755.4, "end": 759.9599999999999, "text": " And then you can go in different directions and each direction has its own trade-offs.", "tokens": [51320, 400, 550, 291, 393, 352, 294, 819, 11095, 293, 1184, 3513, 575, 1080, 1065, 4923, 12, 19231, 13, 51548], "temperature": 0.0, "avg_logprob": -0.23419387166093036, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0007433143327943981}, {"id": 115, "seek": 75996, "start": 760.84, "end": 769.64, "text": " You can go to the feature map direction, the kernel direction. So when you break down the", "tokens": [50408, 509, 393, 352, 281, 264, 4111, 4471, 3513, 11, 264, 28256, 3513, 13, 407, 562, 291, 1821, 760, 264, 50848], "temperature": 0.0, "avg_logprob": -0.10817118388850515, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0005354487802833319}, {"id": 116, "seek": 75996, "start": 769.64, "end": 777.24, "text": " softmax, you take away the softmax, you can place on queries and keys, kind of the fundamental", "tokens": [50848, 2787, 41167, 11, 291, 747, 1314, 264, 2787, 41167, 11, 291, 393, 1081, 322, 24109, 293, 9317, 11, 733, 295, 264, 8088, 51228], "temperature": 0.0, "avg_logprob": -0.10817118388850515, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0005354487802833319}, {"id": 117, "seek": 75996, "start": 777.24, "end": 782.12, "text": " the entities that compose your attention matrix, you can compose other kernel-like functions,", "tokens": [51228, 264, 16667, 300, 35925, 428, 3202, 8141, 11, 291, 393, 35925, 661, 28256, 12, 4092, 6828, 11, 51472], "temperature": 0.0, "avg_logprob": -0.10817118388850515, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0005354487802833319}, {"id": 118, "seek": 75996, "start": 782.6800000000001, "end": 787.64, "text": " other functions that you hope would approximate whatever capability of attention you like.", "tokens": [51500, 661, 6828, 300, 291, 1454, 576, 30874, 2035, 13759, 295, 3202, 291, 411, 13, 51748], "temperature": 0.0, "avg_logprob": -0.10817118388850515, "compression_ratio": 1.7655502392344498, "no_speech_prob": 0.0005354487802833319}, {"id": 119, "seek": 78764, "start": 787.64, "end": 791.24, "text": " You can do things like a Taylor approximation, Taylor expansion, for example.", "tokens": [50364, 509, 393, 360, 721, 411, 257, 12060, 28023, 11, 12060, 11260, 11, 337, 1365, 13, 50544], "temperature": 0.0, "avg_logprob": -0.1334902600544255, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.0004860790795646608}, {"id": 120, "seek": 78764, "start": 792.92, "end": 798.04, "text": " And you have a slightly different perspective, but you get something that, again, is very similar.", "tokens": [50628, 400, 291, 362, 257, 4748, 819, 4585, 11, 457, 291, 483, 746, 300, 11, 797, 11, 307, 588, 2531, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1334902600544255, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.0004860790795646608}, {"id": 121, "seek": 78764, "start": 798.6, "end": 804.6, "text": " You can go to time variance, so you take the RNN and you push this input dependence,", "tokens": [50912, 509, 393, 352, 281, 565, 21977, 11, 370, 291, 747, 264, 45702, 45, 293, 291, 2944, 341, 4846, 31704, 11, 51212], "temperature": 0.0, "avg_logprob": -0.1334902600544255, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.0004860790795646608}, {"id": 122, "seek": 78764, "start": 804.6, "end": 812.52, "text": " so the way the computation inside the linear RNN is conditioned by the input sequence,", "tokens": [51212, 370, 264, 636, 264, 24903, 1854, 264, 8213, 45702, 45, 307, 35833, 538, 264, 4846, 8310, 11, 51608], "temperature": 0.0, "avg_logprob": -0.1334902600544255, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.0004860790795646608}, {"id": 123, "seek": 81252, "start": 812.52, "end": 816.28, "text": " and you can add things like gates. We've seen a lot of work, for example,", "tokens": [50364, 293, 291, 393, 909, 721, 411, 19792, 13, 492, 600, 1612, 257, 688, 295, 589, 11, 337, 1365, 11, 50552], "temperature": 0.0, "avg_logprob": -0.12673920566595873, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.0007297834963537753}, {"id": 124, "seek": 81252, "start": 816.28, "end": 821.88, "text": " modernizing linear attention with additional gates that allow you to make better use of your", "tokens": [50552, 4363, 3319, 8213, 3202, 365, 4497, 19792, 300, 2089, 291, 281, 652, 1101, 764, 295, 428, 50832], "temperature": 0.0, "avg_logprob": -0.12673920566595873, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.0007297834963537753}, {"id": 125, "seek": 81252, "start": 821.88, "end": 828.28, "text": " fixed state dimension. And then you have the third direction, at least in my mind,", "tokens": [50832, 6806, 1785, 10139, 13, 400, 550, 291, 362, 264, 2636, 3513, 11, 412, 1935, 294, 452, 1575, 11, 51152], "temperature": 0.0, "avg_logprob": -0.12673920566595873, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.0007297834963537753}, {"id": 126, "seek": 81252, "start": 828.28, "end": 833.96, "text": " is the one that pushes, that uses the convolutional form, that pushes more towards other types of", "tokens": [51152, 307, 264, 472, 300, 21020, 11, 300, 4960, 264, 45216, 304, 1254, 11, 300, 21020, 544, 3030, 661, 3467, 295, 51436], "temperature": 0.0, "avg_logprob": -0.12673920566595873, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.0007297834963537753}, {"id": 127, "seek": 81252, "start": 833.96, "end": 840.52, "text": " linear operators that are still associative, that are still allow you to train in parallel.", "tokens": [51436, 8213, 19077, 300, 366, 920, 4180, 1166, 11, 300, 366, 920, 2089, 291, 281, 3847, 294, 8952, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12673920566595873, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.0007297834963537753}, {"id": 128, "seek": 84052, "start": 840.52, "end": 845.56, "text": " So here are things, time invariant systems, I can elaborate on any of these points,", "tokens": [50364, 407, 510, 366, 721, 11, 565, 33270, 394, 3652, 11, 286, 393, 20945, 322, 604, 295, 613, 2793, 11, 50616], "temperature": 0.0, "avg_logprob": -0.21327796678864555, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.000281687593087554}, {"id": 129, "seek": 84052, "start": 845.56, "end": 848.92, "text": " but things that can switch between convolutions and recurrence, like this form model,", "tokens": [50616, 457, 721, 300, 393, 3679, 1296, 3754, 15892, 293, 18680, 10760, 11, 411, 341, 1254, 2316, 11, 50784], "temperature": 0.0, "avg_logprob": -0.21327796678864555, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.000281687593087554}, {"id": 130, "seek": 84052, "start": 849.64, "end": 859.64, "text": " with additional gates, again. Striped IEna was born as a project from the IEna architecture,", "tokens": [50820, 365, 4497, 19792, 11, 797, 13, 20390, 3452, 286, 36, 629, 390, 4232, 382, 257, 1716, 490, 264, 286, 36, 629, 9482, 11, 51320], "temperature": 0.0, "avg_logprob": -0.21327796678864555, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.000281687593087554}, {"id": 131, "seek": 84052, "start": 859.64, "end": 865.48, "text": " which belongs to this third category that I just mentioned, and we're really trying to get the best", "tokens": [51320, 597, 12953, 281, 341, 2636, 7719, 300, 286, 445, 2835, 11, 293, 321, 434, 534, 1382, 281, 483, 264, 1151, 51612], "temperature": 0.0, "avg_logprob": -0.21327796678864555, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.000281687593087554}, {"id": 132, "seek": 86548, "start": 865.48, "end": 874.36, "text": " per-flop architecture that we could. And one principle that was validated over and over again,", "tokens": [50364, 680, 12, 3423, 404, 9482, 300, 321, 727, 13, 400, 472, 8665, 300, 390, 40693, 670, 293, 670, 797, 11, 50808], "temperature": 0.0, "avg_logprob": -0.1289618730545044, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0013453029096126556}, {"id": 133, "seek": 86548, "start": 875.0, "end": 881.32, "text": " and we're trying to understand better now, is that it seems composing, hybridizing different", "tokens": [50840, 293, 321, 434, 1382, 281, 1223, 1101, 586, 11, 307, 300, 309, 2544, 715, 6110, 11, 13051, 3319, 819, 51156], "temperature": 0.0, "avg_logprob": -0.1289618730545044, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0013453029096126556}, {"id": 134, "seek": 86548, "start": 882.9200000000001, "end": 889.48, "text": " layers, different blocks of different categories, and even full attention yields something that", "tokens": [51236, 7914, 11, 819, 8474, 295, 819, 10479, 11, 293, 754, 1577, 3202, 32168, 746, 300, 51564], "temperature": 0.0, "avg_logprob": -0.1289618730545044, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0013453029096126556}, {"id": 135, "seek": 86548, "start": 889.48, "end": 894.2, "text": " is better than the individual components. So there seems to be a compositional aspect", "tokens": [51564, 307, 1101, 813, 264, 2609, 6677, 13, 407, 456, 2544, 281, 312, 257, 10199, 2628, 4171, 51800], "temperature": 0.0, "avg_logprob": -0.1289618730545044, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0013453029096126556}, {"id": 136, "seek": 89420, "start": 894.2, "end": 898.76, "text": " of these models that we're trying to understand better, and this gives you a better", "tokens": [50364, 295, 613, 5245, 300, 321, 434, 1382, 281, 1223, 1101, 11, 293, 341, 2709, 291, 257, 1101, 50592], "temperature": 0.0, "avg_logprob": -0.12378939604147887, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.00047969157458283007}, {"id": 137, "seek": 89420, "start": 899.4000000000001, "end": 907.4000000000001, "text": " sort of pre-trained model per-flop. And with this model, we ran a whole suite of skating laws,", "tokens": [50624, 1333, 295, 659, 12, 17227, 2001, 2316, 680, 12, 3423, 404, 13, 400, 365, 341, 2316, 11, 321, 5872, 257, 1379, 14205, 295, 29103, 6064, 11, 51024], "temperature": 0.0, "avg_logprob": -0.12378939604147887, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.00047969157458283007}, {"id": 138, "seek": 89420, "start": 907.4000000000001, "end": 913.0, "text": " and so on. Hybridizing also gives you, since we wanted something that would be kind of usable", "tokens": [51024, 293, 370, 322, 13, 47088, 3319, 611, 2709, 291, 11, 1670, 321, 1415, 746, 300, 576, 312, 733, 295, 29975, 51304], "temperature": 0.0, "avg_logprob": -0.12378939604147887, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.00047969157458283007}, {"id": 139, "seek": 89420, "start": 913.0, "end": 919.0, "text": " out of the box, it gives you a way easier time. When you're fine-tuning for longer contexts,", "tokens": [51304, 484, 295, 264, 2424, 11, 309, 2709, 291, 257, 636, 3571, 565, 13, 1133, 291, 434, 2489, 12, 83, 37726, 337, 2854, 30628, 11, 51604], "temperature": 0.0, "avg_logprob": -0.12378939604147887, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.00047969157458283007}, {"id": 140, "seek": 89420, "start": 919.0, "end": 923.32, "text": " we can apply some of these techniques that have been developed for transformers, and kind of", "tokens": [51604, 321, 393, 3079, 512, 295, 613, 7512, 300, 362, 668, 4743, 337, 4088, 433, 11, 293, 733, 295, 51820], "temperature": 0.0, "avg_logprob": -0.12378939604147887, "compression_ratio": 1.6654545454545455, "no_speech_prob": 0.00047969157458283007}, {"id": 141, "seek": 92332, "start": 923.32, "end": 930.5200000000001, "text": " surprisingly work for hybrids as well. So things like linear scaling for", "tokens": [50364, 17600, 589, 337, 2477, 1443, 3742, 382, 731, 13, 407, 721, 411, 8213, 21589, 337, 50724], "temperature": 0.0, "avg_logprob": -0.1746870380963466, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.00025708862813189626}, {"id": 142, "seek": 92332, "start": 930.5200000000001, "end": 934.2, "text": " rotary embeddings and so on, you can go into the details. So it was mostly a project,", "tokens": [50724, 45811, 12240, 29432, 293, 370, 322, 11, 291, 393, 352, 666, 264, 4365, 13, 407, 309, 390, 5240, 257, 1716, 11, 50908], "temperature": 0.0, "avg_logprob": -0.1746870380963466, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.00025708862813189626}, {"id": 143, "seek": 92332, "start": 934.2, "end": 937.1600000000001, "text": " what is the best, given the current landscape, what is the best we can do?", "tokens": [50908, 437, 307, 264, 1151, 11, 2212, 264, 2190, 9661, 11, 437, 307, 264, 1151, 321, 393, 360, 30, 51056], "temperature": 0.0, "avg_logprob": -0.1746870380963466, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.00025708862813189626}, {"id": 144, "seek": 92332, "start": 937.88, "end": 941.72, "text": " Yeah, that's a great description of it. I mean, the sentence in the blog that's like,", "tokens": [51092, 865, 11, 300, 311, 257, 869, 3855, 295, 309, 13, 286, 914, 11, 264, 8174, 294, 264, 6968, 300, 311, 411, 11, 51284], "temperature": 0.0, "avg_logprob": -0.1746870380963466, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.00025708862813189626}, {"id": 145, "seek": 92332, "start": 941.72, "end": 946.2, "text": " start training is optimized using a set of new model grafting techniques, enabling us to change", "tokens": [51284, 722, 3097, 307, 26941, 1228, 257, 992, 295, 777, 2316, 1295, 20930, 7512, 11, 23148, 505, 281, 1319, 51508], "temperature": 0.0, "avg_logprob": -0.1746870380963466, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.00025708862813189626}, {"id": 146, "seek": 92332, "start": 946.2, "end": 951.8000000000001, "text": " the model architecture during training, kind of felt like, to me, that there's a ton going on", "tokens": [51508, 264, 2316, 9482, 1830, 3097, 11, 733, 295, 2762, 411, 11, 281, 385, 11, 300, 456, 311, 257, 2952, 516, 322, 51788], "temperature": 0.0, "avg_logprob": -0.1746870380963466, "compression_ratio": 1.6966666666666668, "no_speech_prob": 0.00025708862813189626}, {"id": 147, "seek": 95180, "start": 951.8, "end": 956.68, "text": " there. And some of which you probably can't talk about, there's normal data. I don't think all the", "tokens": [50364, 456, 13, 400, 512, 295, 597, 291, 1391, 393, 380, 751, 466, 11, 456, 311, 2710, 1412, 13, 286, 500, 380, 519, 439, 264, 50608], "temperature": 0.0, "avg_logprob": -0.13896541428147702, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.0010984260588884354}, {"id": 148, "seek": 95180, "start": 956.68, "end": 960.8399999999999, "text": " data that was quite explained, like what the longer context data was, but it's like,", "tokens": [50608, 1412, 300, 390, 1596, 8825, 11, 411, 437, 264, 2854, 4319, 1412, 390, 11, 457, 309, 311, 411, 11, 50816], "temperature": 0.0, "avg_logprob": -0.13896541428147702, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.0010984260588884354}, {"id": 149, "seek": 95180, "start": 960.8399999999999, "end": 964.4399999999999, "text": " are you taking this from models, starting point from models that people would know,", "tokens": [50816, 366, 291, 1940, 341, 490, 5245, 11, 2891, 935, 490, 5245, 300, 561, 576, 458, 11, 50996], "temperature": 0.0, "avg_logprob": -0.13896541428147702, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.0010984260588884354}, {"id": 150, "seek": 95180, "start": 964.4399999999999, "end": 970.92, "text": " and can you say any of that? I think even just the summary that it's a synthesizing recent work", "tokens": [50996, 293, 393, 291, 584, 604, 295, 300, 30, 286, 519, 754, 445, 264, 12691, 300, 309, 311, 257, 26617, 3319, 5162, 589, 51320], "temperature": 0.0, "avg_logprob": -0.13896541428147702, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.0010984260588884354}, {"id": 151, "seek": 95180, "start": 970.92, "end": 977.0799999999999, "text": " into a strong model is great context for people. Yeah, well, that line, so we've,", "tokens": [51320, 666, 257, 2068, 2316, 307, 869, 4319, 337, 561, 13, 865, 11, 731, 11, 300, 1622, 11, 370, 321, 600, 11, 51628], "temperature": 0.0, "avg_logprob": -0.13896541428147702, "compression_ratio": 1.654275092936803, "no_speech_prob": 0.0010984260588884354}, {"id": 152, "seek": 97708, "start": 977.8000000000001, "end": 985.8000000000001, "text": " given this explosion of primitives that, you know, described, and given sort of the", "tokens": [50400, 2212, 341, 15673, 295, 2886, 38970, 300, 11, 291, 458, 11, 7619, 11, 293, 2212, 1333, 295, 264, 50800], "temperature": 0.0, "avg_logprob": -0.2262672015598842, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0004511165025178343}, {"id": 153, "seek": 97708, "start": 988.6, "end": 996.2, "text": " cost that it would require to evaluate all different combinations, we found ways to essentially", "tokens": [50940, 2063, 300, 309, 576, 3651, 281, 13059, 439, 819, 21267, 11, 321, 1352, 2098, 281, 4476, 51320], "temperature": 0.0, "avg_logprob": -0.2262672015598842, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0004511165025178343}, {"id": 154, "seek": 97708, "start": 996.84, "end": 1002.6, "text": " start training with a configuration and then continuing on with another configuration. I", "tokens": [51352, 722, 3097, 365, 257, 11694, 293, 550, 9289, 322, 365, 1071, 11694, 13, 286, 51640], "temperature": 0.0, "avg_logprob": -0.2262672015598842, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.0004511165025178343}, {"id": 155, "seek": 100260, "start": 1002.6, "end": 1007.32, "text": " think we'll have, we're going to have more work with paper. Yeah, there's so much cool work in", "tokens": [50364, 519, 321, 603, 362, 11, 321, 434, 516, 281, 362, 544, 589, 365, 3035, 13, 865, 11, 456, 311, 370, 709, 1627, 589, 294, 50600], "temperature": 0.0, "avg_logprob": -0.16902804374694824, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0006262452807277441}, {"id": 156, "seek": 100260, "start": 1007.32, "end": 1011.96, "text": " that area. So one of the, someone at AI2 is working on a project where they're essentially", "tokens": [50600, 300, 1859, 13, 407, 472, 295, 264, 11, 1580, 412, 7318, 17, 307, 1364, 322, 257, 1716, 689, 436, 434, 4476, 50832], "temperature": 0.0, "avg_logprob": -0.16902804374694824, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0006262452807277441}, {"id": 157, "seek": 100260, "start": 1011.96, "end": 1017.0, "text": " trying to cut the Lama models in half and keep training them and things. It's just the wild west", "tokens": [50832, 1382, 281, 1723, 264, 441, 2404, 5245, 294, 1922, 293, 1066, 3097, 552, 293, 721, 13, 467, 311, 445, 264, 4868, 7009, 51084], "temperature": 0.0, "avg_logprob": -0.16902804374694824, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0006262452807277441}, {"id": 158, "seek": 100260, "start": 1017.0, "end": 1022.6, "text": " out there with people trying to take strong models and make them smaller while still getting the", "tokens": [51084, 484, 456, 365, 561, 1382, 281, 747, 2068, 5245, 293, 652, 552, 4356, 1339, 920, 1242, 264, 51364], "temperature": 0.0, "avg_logprob": -0.16902804374694824, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0006262452807277441}, {"id": 159, "seek": 100260, "start": 1022.6, "end": 1027.4, "text": " performance benefits of bigger models. I think that's a whole aside, but I wasn't expecting it to", "tokens": [51364, 3389, 5311, 295, 3801, 5245, 13, 286, 519, 300, 311, 257, 1379, 7359, 11, 457, 286, 2067, 380, 9650, 309, 281, 51604], "temperature": 0.0, "avg_logprob": -0.16902804374694824, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0006262452807277441}, {"id": 160, "seek": 100260, "start": 1027.4, "end": 1032.04, "text": " show up when people, like you follow the social media, I've striped my, you know, people are like,", "tokens": [51604, 855, 493, 562, 561, 11, 411, 291, 1524, 264, 2093, 3021, 11, 286, 600, 3575, 3452, 452, 11, 291, 458, 11, 561, 366, 411, 11, 51836], "temperature": 0.0, "avg_logprob": -0.16902804374694824, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0006262452807277441}, {"id": 161, "seek": 103204, "start": 1032.04, "end": 1036.92, "text": " oh, non-attention models are finally good. And it's like, it covers up a lot of the details", "tokens": [50364, 1954, 11, 2107, 12, 1591, 1251, 5245, 366, 2721, 665, 13, 400, 309, 311, 411, 11, 309, 10538, 493, 257, 688, 295, 264, 4365, 50608], "temperature": 0.0, "avg_logprob": -0.13053326513253005, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.000324983149766922}, {"id": 162, "seek": 103204, "start": 1036.92, "end": 1044.92, "text": " that are very interesting about it, in my opinion. So, okay, it turned back to tree. I think Mamba", "tokens": [50608, 300, 366, 588, 1880, 466, 309, 11, 294, 452, 4800, 13, 407, 11, 1392, 11, 309, 3574, 646, 281, 4230, 13, 286, 519, 376, 23337, 51008], "temperature": 0.0, "avg_logprob": -0.13053326513253005, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.000324983149766922}, {"id": 163, "seek": 103204, "start": 1044.92, "end": 1051.8799999999999, "text": " actually happened first among these. I did the, his reading back of social media. And it also was", "tokens": [51008, 767, 2011, 700, 3654, 613, 13, 286, 630, 264, 11, 702, 3760, 646, 295, 2093, 3021, 13, 400, 309, 611, 390, 51356], "temperature": 0.0, "avg_logprob": -0.13053326513253005, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.000324983149766922}, {"id": 164, "seek": 103204, "start": 1051.8799999999999, "end": 1058.44, "text": " very surprising to me. I think the largest model in the Mamba suite is 2.8 billion parameters,", "tokens": [51356, 588, 8830, 281, 385, 13, 286, 519, 264, 6443, 2316, 294, 264, 376, 23337, 14205, 307, 568, 13, 23, 5218, 9834, 11, 51684], "temperature": 0.0, "avg_logprob": -0.13053326513253005, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.000324983149766922}, {"id": 165, "seek": 105844, "start": 1058.44, "end": 1064.92, "text": " if I remember correctly. And it was compared to a lot of the common benchmarks in open NLP. So", "tokens": [50364, 498, 286, 1604, 8944, 13, 400, 309, 390, 5347, 281, 257, 688, 295, 264, 2689, 43751, 294, 1269, 426, 45196, 13, 407, 50688], "temperature": 0.0, "avg_logprob": -0.11313902601903798, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0023963216226547956}, {"id": 166, "seek": 105844, "start": 1064.92, "end": 1071.48, "text": " things like GPTJ, Pythia model suites and the scores on the benchmarks reported were really", "tokens": [50688, 721, 411, 26039, 51, 41, 11, 9953, 392, 654, 2316, 459, 3324, 293, 264, 13444, 322, 264, 43751, 7055, 645, 534, 51016], "temperature": 0.0, "avg_logprob": -0.11313902601903798, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0023963216226547956}, {"id": 167, "seek": 105844, "start": 1071.48, "end": 1077.3200000000002, "text": " strong. And I think if you want to start with the high level summary, and then I'm definitely going", "tokens": [51016, 2068, 13, 400, 286, 519, 498, 291, 528, 281, 722, 365, 264, 1090, 1496, 12691, 11, 293, 550, 286, 478, 2138, 516, 51308], "temperature": 0.0, "avg_logprob": -0.11313902601903798, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0023963216226547956}, {"id": 168, "seek": 105844, "start": 1077.3200000000002, "end": 1082.2, "text": " to make you talk about the awesome new CUDA kernels and stuff that you had to write for this project.", "tokens": [51308, 281, 652, 291, 751, 466, 264, 3476, 777, 29777, 7509, 23434, 1625, 293, 1507, 300, 291, 632, 281, 2464, 337, 341, 1716, 13, 51552], "temperature": 0.0, "avg_logprob": -0.11313902601903798, "compression_ratio": 1.533596837944664, "no_speech_prob": 0.0023963216226547956}, {"id": 169, "seek": 108220, "start": 1082.76, "end": 1089.72, "text": " Yeah, so this Mamba is a collaboration with, with Albert Gu, who's now, he was,", "tokens": [50392, 865, 11, 370, 341, 376, 23337, 307, 257, 9363, 365, 11, 365, 20812, 2694, 11, 567, 311, 586, 11, 415, 390, 11, 50740], "temperature": 0.0, "avg_logprob": -0.24092095621516196, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004903583787381649}, {"id": 170, "seek": 108220, "start": 1091.16, "end": 1096.28, "text": " he's just doing it at Stanford. That's where we met. And he's now a professor at CMU.", "tokens": [50812, 415, 311, 445, 884, 309, 412, 20374, 13, 663, 311, 689, 321, 1131, 13, 400, 415, 311, 586, 257, 8304, 412, 20424, 52, 13, 51068], "temperature": 0.0, "avg_logprob": -0.24092095621516196, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004903583787381649}, {"id": 171, "seek": 108220, "start": 1098.1200000000001, "end": 1103.32, "text": " And also at a startup. So it's a wonderful collaboration credit goes to him.", "tokens": [51160, 400, 611, 412, 257, 18578, 13, 407, 309, 311, 257, 3715, 9363, 5397, 1709, 281, 796, 13, 51420], "temperature": 0.0, "avg_logprob": -0.24092095621516196, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004903583787381649}, {"id": 172, "seek": 108220, "start": 1104.8400000000001, "end": 1108.44, "text": " Yeah, Albert has been working on this line of work called state space models.", "tokens": [51496, 865, 11, 20812, 575, 668, 1364, 322, 341, 1622, 295, 589, 1219, 1785, 1901, 5245, 13, 51676], "temperature": 0.0, "avg_logprob": -0.24092095621516196, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.004903583787381649}, {"id": 173, "seek": 110844, "start": 1109.0800000000002, "end": 1114.04, "text": " In some sense, as mentioned, it connects to things like linear tension,", "tokens": [50396, 682, 512, 2020, 11, 382, 2835, 11, 309, 16967, 281, 721, 411, 8213, 8980, 11, 50644], "temperature": 0.0, "avg_logprob": -0.19352211169342495, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.008703717030584812}, {"id": 174, "seek": 110844, "start": 1114.04, "end": 1124.6000000000001, "text": " linear RNN, convolution, neural nets. And that's what his PhD thesis is about. I've also worked on", "tokens": [50644, 8213, 45702, 45, 11, 45216, 11, 18161, 36170, 13, 400, 300, 311, 437, 702, 14476, 22288, 307, 466, 13, 286, 600, 611, 2732, 322, 51172], "temperature": 0.0, "avg_logprob": -0.19352211169342495, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.008703717030584812}, {"id": 175, "seek": 110844, "start": 1126.04, "end": 1132.28, "text": " state space for the past couple of projects. My, my angle is how to make state space more", "tokens": [51244, 1785, 1901, 337, 264, 1791, 1916, 295, 4455, 13, 1222, 11, 452, 5802, 307, 577, 281, 652, 1785, 1901, 544, 51556], "temperature": 0.0, "avg_logprob": -0.19352211169342495, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.008703717030584812}, {"id": 176, "seek": 113228, "start": 1132.28, "end": 1140.52, "text": " hardware efficient and kind of increase their expressiveness. So it's wonderful working with", "tokens": [50364, 8837, 7148, 293, 733, 295, 3488, 641, 5109, 8477, 13, 407, 309, 311, 3715, 1364, 365, 50776], "temperature": 0.0, "avg_logprob": -0.18385431170463562, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.016391083598136902}, {"id": 177, "seek": 113228, "start": 1141.08, "end": 1148.28, "text": " Albert. And there I think is more of a proof of concept, which is can state space actually do", "tokens": [50804, 20812, 13, 400, 456, 286, 519, 307, 544, 295, 257, 8177, 295, 3410, 11, 597, 307, 393, 1785, 1901, 767, 360, 51164], "temperature": 0.0, "avg_logprob": -0.18385431170463562, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.016391083598136902}, {"id": 178, "seek": 113228, "start": 1148.28, "end": 1155.6399999999999, "text": " as well as transformer on language. So we've seen previous papers showing state space could be better", "tokens": [51164, 382, 731, 382, 31782, 322, 2856, 13, 407, 321, 600, 1612, 3894, 10577, 4099, 1785, 1901, 727, 312, 1101, 51532], "temperature": 0.0, "avg_logprob": -0.18385431170463562, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.016391083598136902}, {"id": 179, "seek": 115564, "start": 1155.64, "end": 1163.72, "text": " on audio, could be better on some of the tasks on the long range arena. But language has always been", "tokens": [50364, 322, 6278, 11, 727, 312, 1101, 322, 512, 295, 264, 9608, 322, 264, 938, 3613, 18451, 13, 583, 2856, 575, 1009, 668, 50768], "temperature": 0.0, "avg_logprob": -0.11596031711526113, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.009553384967148304}, {"id": 180, "seek": 115564, "start": 1164.68, "end": 1172.1200000000001, "text": " the most difficult to get to do well for state space models. And language is also kind of the", "tokens": [50816, 264, 881, 2252, 281, 483, 281, 360, 731, 337, 1785, 1901, 5245, 13, 400, 2856, 307, 611, 733, 295, 264, 51188], "temperature": 0.0, "avg_logprob": -0.11596031711526113, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.009553384967148304}, {"id": 181, "seek": 115564, "start": 1172.1200000000001, "end": 1180.2, "text": " thing that people care about the most right now. So Mamba was more of a proof of concept, which is", "tokens": [51188, 551, 300, 561, 1127, 466, 264, 881, 558, 586, 13, 407, 376, 23337, 390, 544, 295, 257, 8177, 295, 3410, 11, 597, 307, 51592], "temperature": 0.0, "avg_logprob": -0.11596031711526113, "compression_ratio": 1.5421052631578946, "no_speech_prob": 0.009553384967148304}, {"id": 182, "seek": 118020, "start": 1181.16, "end": 1186.04, "text": " hey, we want to show that state space can be competitive, or maybe even be some of the", "tokens": [50412, 4177, 11, 321, 528, 281, 855, 300, 1785, 1901, 393, 312, 10043, 11, 420, 1310, 754, 312, 512, 295, 264, 50656], "temperature": 0.0, "avg_logprob": -0.19223518371582032, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0032215025275945663}, {"id": 183, "seek": 118020, "start": 1186.04, "end": 1195.16, "text": " transformers out there. So we validated that at the scale up to 3B, trained to 300B tokens.", "tokens": [50656, 4088, 433, 484, 456, 13, 407, 321, 40693, 300, 412, 264, 4373, 493, 281, 805, 33, 11, 8895, 281, 6641, 33, 22667, 13, 51112], "temperature": 0.0, "avg_logprob": -0.19223518371582032, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0032215025275945663}, {"id": 184, "seek": 118020, "start": 1195.16, "end": 1199.8, "text": " So in absolute terms, you know, these are not very strong models. These are not yet models that you", "tokens": [51112, 407, 294, 8236, 2115, 11, 291, 458, 11, 613, 366, 406, 588, 2068, 5245, 13, 1981, 366, 406, 1939, 5245, 300, 291, 51344], "temperature": 0.0, "avg_logprob": -0.19223518371582032, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0032215025275945663}, {"id": 185, "seek": 118020, "start": 1199.8, "end": 1204.8400000000001, "text": " would actually play with and expect it to do meaningful things. I think it's more of a,", "tokens": [51344, 576, 767, 862, 365, 293, 2066, 309, 281, 360, 10995, 721, 13, 286, 519, 309, 311, 544, 295, 257, 11, 51596], "temperature": 0.0, "avg_logprob": -0.19223518371582032, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.0032215025275945663}, {"id": 186, "seek": 120484, "start": 1205.72, "end": 1209.8799999999999, "text": " more of an academic comparison in terms of architecture, it's like, hey,", "tokens": [50408, 544, 295, 364, 7778, 9660, 294, 2115, 295, 9482, 11, 309, 311, 411, 11, 4177, 11, 50616], "temperature": 0.0, "avg_logprob": -0.17175383096212868, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.007455554325133562}, {"id": 187, "seek": 120484, "start": 1209.8799999999999, "end": 1215.3999999999999, "text": " training trained for the same amount of tokens, it does as well, or maybe slightly better than", "tokens": [50616, 3097, 8895, 337, 264, 912, 2372, 295, 22667, 11, 309, 775, 382, 731, 11, 420, 1310, 4748, 1101, 813, 50892], "temperature": 0.0, "avg_logprob": -0.17175383096212868, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.007455554325133562}, {"id": 188, "seek": 120484, "start": 1215.3999999999999, "end": 1221.6399999999999, "text": " some of the transformer out there. And that's, in particular, has been very exciting to us.", "tokens": [50892, 512, 295, 264, 31782, 484, 456, 13, 400, 300, 311, 11, 294, 1729, 11, 575, 668, 588, 4670, 281, 505, 13, 51204], "temperature": 0.0, "avg_logprob": -0.17175383096212868, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.007455554325133562}, {"id": 189, "seek": 120484, "start": 1221.6399999999999, "end": 1227.24, "text": " I think Albert's been pushing on this for a while. I've been pushing on this for a while. And I think", "tokens": [51204, 286, 519, 20812, 311, 668, 7380, 322, 341, 337, 257, 1339, 13, 286, 600, 668, 7380, 322, 341, 337, 257, 1339, 13, 400, 286, 519, 51484], "temperature": 0.0, "avg_logprob": -0.17175383096212868, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.007455554325133562}, {"id": 190, "seek": 122724, "start": 1227.24, "end": 1236.04, "text": " finally, it seems to finally be kind of close to gap or even surpass some of the transformer.", "tokens": [50364, 2721, 11, 309, 2544, 281, 2721, 312, 733, 295, 1998, 281, 7417, 420, 754, 27650, 512, 295, 264, 31782, 13, 50804], "temperature": 0.0, "avg_logprob": -0.20105944106827922, "compression_ratio": 1.4598930481283423, "no_speech_prob": 0.07911529392004013}, {"id": 191, "seek": 122724, "start": 1237.56, "end": 1245.0, "text": " And it just, I think it opens up a bunch of opportunities. So inference could be way faster.", "tokens": [50880, 400, 309, 445, 11, 286, 519, 309, 9870, 493, 257, 3840, 295, 4786, 13, 407, 38253, 727, 312, 636, 4663, 13, 51252], "temperature": 0.0, "avg_logprob": -0.20105944106827922, "compression_ratio": 1.4598930481283423, "no_speech_prob": 0.07911529392004013}, {"id": 192, "seek": 122724, "start": 1245.72, "end": 1250.6, "text": " Maybe we would have different ways to understand how in-contact learning happens, etc.", "tokens": [51288, 2704, 321, 576, 362, 819, 2098, 281, 1223, 577, 294, 12, 9000, 578, 2539, 2314, 11, 5183, 13, 51532], "temperature": 0.0, "avg_logprob": -0.20105944106827922, "compression_ratio": 1.4598930481283423, "no_speech_prob": 0.07911529392004013}, {"id": 193, "seek": 125060, "start": 1250.6, "end": 1253.7199999999998, "text": " So lots of, lots of future work, I would expect.", "tokens": [50364, 407, 3195, 295, 11, 3195, 295, 2027, 589, 11, 286, 576, 2066, 13, 50520], "temperature": 0.0, "avg_logprob": -0.15548971721104213, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.01639437861740589}, {"id": 194, "seek": 125060, "start": 1254.76, "end": 1260.1999999999998, "text": " Yeah. Can you go into some of the, like, what does it actually take to implement some of these", "tokens": [50572, 865, 13, 1664, 291, 352, 666, 512, 295, 264, 11, 411, 11, 437, 775, 309, 767, 747, 281, 4445, 512, 295, 613, 50844], "temperature": 0.0, "avg_logprob": -0.15548971721104213, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.01639437861740589}, {"id": 195, "seek": 125060, "start": 1260.1999999999998, "end": 1264.52, "text": " new CUDA kernels? I just remember when this paper was announced, Sasha Rush, who's also very active", "tokens": [50844, 777, 29777, 7509, 23434, 1625, 30, 286, 445, 1604, 562, 341, 3035, 390, 7548, 11, 29276, 28389, 11, 567, 311, 611, 588, 4967, 51060], "temperature": 0.0, "avg_logprob": -0.15548971721104213, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.01639437861740589}, {"id": 196, "seek": 125060, "start": 1264.52, "end": 1269.8799999999999, "text": " in this space, I recommended me to talk with you too, was tweeting about the types of files or", "tokens": [51060, 294, 341, 1901, 11, 286, 9628, 385, 281, 751, 365, 291, 886, 11, 390, 40090, 466, 264, 3467, 295, 7098, 420, 51328], "temperature": 0.0, "avg_logprob": -0.15548971721104213, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.01639437861740589}, {"id": 197, "seek": 125060, "start": 1269.8799999999999, "end": 1276.36, "text": " whatever. And in the paper, there's this discussion about how like the recurrent state needs to be", "tokens": [51328, 2035, 13, 400, 294, 264, 3035, 11, 456, 311, 341, 5017, 466, 577, 411, 264, 18680, 1753, 1785, 2203, 281, 312, 51652], "temperature": 0.0, "avg_logprob": -0.15548971721104213, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.01639437861740589}, {"id": 198, "seek": 127636, "start": 1276.6799999999998, "end": 1281.1599999999999, "text": " sufficiently expressive, but doing so in a certain type of memory is a problem.", "tokens": [50380, 31868, 40189, 11, 457, 884, 370, 294, 257, 1629, 2010, 295, 4675, 307, 257, 1154, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09715909307653253, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00884414091706276}, {"id": 199, "seek": 127636, "start": 1283.08, "end": 1288.6799999999998, "text": " Translate what this means to people thinking about GPUs and people thinking about these models", "tokens": [50700, 6531, 17593, 437, 341, 1355, 281, 561, 1953, 466, 18407, 82, 293, 561, 1953, 466, 613, 5245, 50980], "temperature": 0.0, "avg_logprob": -0.09715909307653253, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00884414091706276}, {"id": 200, "seek": 127636, "start": 1288.6799999999998, "end": 1294.4399999999998, "text": " being scaled. Like, is it now much easier to scale these models because they work on GPUs?", "tokens": [50980, 885, 36039, 13, 1743, 11, 307, 309, 586, 709, 3571, 281, 4373, 613, 5245, 570, 436, 589, 322, 18407, 82, 30, 51268], "temperature": 0.0, "avg_logprob": -0.09715909307653253, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00884414091706276}, {"id": 201, "seek": 127636, "start": 1294.4399999999998, "end": 1299.56, "text": " Which GPUs were you using? Is there a bump that could come just from going to H100s or something?", "tokens": [51268, 3013, 18407, 82, 645, 291, 1228, 30, 1119, 456, 257, 9961, 300, 727, 808, 445, 490, 516, 281, 389, 6879, 82, 420, 746, 30, 51524], "temperature": 0.0, "avg_logprob": -0.09715909307653253, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00884414091706276}, {"id": 202, "seek": 129956, "start": 1299.56, "end": 1308.76, "text": " Any of that? Yeah. So the line of work on state space,", "tokens": [50364, 2639, 295, 300, 30, 865, 13, 407, 264, 1622, 295, 589, 322, 1785, 1901, 11, 50824], "temperature": 0.0, "avg_logprob": -0.23568895386486519, "compression_ratio": 1.2035398230088497, "no_speech_prob": 0.002979593351483345}, {"id": 203, "seek": 129956, "start": 1309.72, "end": 1318.6799999999998, "text": " sort of like S4 models kind of pioneered by Albert's work, they are in some sense", "tokens": [50872, 1333, 295, 411, 318, 19, 5245, 733, 295, 19761, 4073, 538, 20812, 311, 589, 11, 436, 366, 294, 512, 2020, 51320], "temperature": 0.0, "avg_logprob": -0.23568895386486519, "compression_ratio": 1.2035398230088497, "no_speech_prob": 0.002979593351483345}, {"id": 204, "seek": 131868, "start": 1318.68, "end": 1327.96, "text": " recurrent neural network, but they have a much larger state size. So the state size is whatever", "tokens": [50364, 18680, 1753, 18161, 3209, 11, 457, 436, 362, 257, 709, 4833, 1785, 2744, 13, 407, 264, 1785, 2744, 307, 2035, 50828], "temperature": 0.0, "avg_logprob": -0.14182775870136832, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.12412918359041214}, {"id": 205, "seek": 131868, "start": 1328.68, "end": 1334.92, "text": " kind of buffer that you're going to store information as you traverse or as you process", "tokens": [50864, 733, 295, 21762, 300, 291, 434, 516, 281, 3531, 1589, 382, 291, 45674, 420, 382, 291, 1399, 51176], "temperature": 0.0, "avg_logprob": -0.14182775870136832, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.12412918359041214}, {"id": 206, "seek": 131868, "start": 1335.72, "end": 1339.96, "text": " the sequence. In some sense, you can view Transformers doing that as well, where it's", "tokens": [51216, 264, 8310, 13, 682, 512, 2020, 11, 291, 393, 1910, 27938, 433, 884, 300, 382, 731, 11, 689, 309, 311, 51428], "temperature": 0.0, "avg_logprob": -0.14182775870136832, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.12412918359041214}, {"id": 207, "seek": 131868, "start": 1339.96, "end": 1344.52, "text": " keep the entire history. It's usually called the KV cache. It keeps the history and keep", "tokens": [51428, 1066, 264, 2302, 2503, 13, 467, 311, 2673, 1219, 264, 591, 53, 19459, 13, 467, 5965, 264, 2503, 293, 1066, 51656], "temperature": 0.0, "avg_logprob": -0.14182775870136832, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.12412918359041214}, {"id": 208, "seek": 134452, "start": 1344.52, "end": 1352.2, "text": " referring to it. For RNNs, they have a fixed size state. For Transformers state, you can think of", "tokens": [50364, 13761, 281, 309, 13, 1171, 45702, 45, 82, 11, 436, 362, 257, 6806, 2744, 1785, 13, 1171, 27938, 433, 1785, 11, 291, 393, 519, 295, 50748], "temperature": 0.0, "avg_logprob": -0.10366241378013534, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.002251350786536932}, {"id": 209, "seek": 134452, "start": 1352.2, "end": 1362.04, "text": " the state size as increasing. And our intuition is that the larger the state size, the easier it is", "tokens": [50748, 264, 1785, 2744, 382, 5662, 13, 400, 527, 24002, 307, 300, 264, 4833, 264, 1785, 2744, 11, 264, 3571, 309, 307, 51240], "temperature": 0.0, "avg_logprob": -0.10366241378013534, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.002251350786536932}, {"id": 210, "seek": 134452, "start": 1362.04, "end": 1367.32, "text": " for the model to do well. So basically, you have more space to store whatever you need to remember.", "tokens": [51240, 337, 264, 2316, 281, 360, 731, 13, 407, 1936, 11, 291, 362, 544, 1901, 281, 3531, 2035, 291, 643, 281, 1604, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10366241378013534, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.002251350786536932}, {"id": 211, "seek": 134452, "start": 1367.32, "end": 1372.6, "text": " And so previous models like S4 and so on, they have an implicitly pretty large state size,", "tokens": [51504, 400, 370, 3894, 5245, 411, 318, 19, 293, 370, 322, 11, 436, 362, 364, 26947, 356, 1238, 2416, 1785, 2744, 11, 51768], "temperature": 0.0, "avg_logprob": -0.10366241378013534, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.002251350786536932}, {"id": 212, "seek": 137260, "start": 1372.6, "end": 1377.24, "text": " but they use the convolutional view to avoid having to materialize the state. So that was", "tokens": [50364, 457, 436, 764, 264, 45216, 304, 1910, 281, 5042, 1419, 281, 2527, 1125, 264, 1785, 13, 407, 300, 390, 50596], "temperature": 0.0, "avg_logprob": -0.1140665899623524, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.000606831512413919}, {"id": 213, "seek": 137260, "start": 1377.24, "end": 1384.1999999999998, "text": " wonderful. Michael has worked behind architecture, has used some of the same insight focusing on", "tokens": [50596, 3715, 13, 5116, 575, 2732, 2261, 9482, 11, 575, 1143, 512, 295, 264, 912, 11269, 8416, 322, 50944], "temperature": 0.0, "avg_logprob": -0.1140665899623524, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.000606831512413919}, {"id": 214, "seek": 137260, "start": 1384.1999999999998, "end": 1393.8, "text": " convolution. Mamba, on the other hand, focuses on the recurrent view. So we wanted to put more", "tokens": [50944, 45216, 13, 376, 23337, 11, 322, 264, 661, 1011, 11, 16109, 322, 264, 18680, 1753, 1910, 13, 407, 321, 1415, 281, 829, 544, 51424], "temperature": 0.0, "avg_logprob": -0.1140665899623524, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.000606831512413919}, {"id": 215, "seek": 137260, "start": 1393.8, "end": 1400.6, "text": " input dependency in the recurrence. We thought the thinking was that it was going to make it", "tokens": [51424, 4846, 33621, 294, 264, 18680, 10760, 13, 492, 1194, 264, 1953, 390, 300, 309, 390, 516, 281, 652, 309, 51764], "temperature": 0.0, "avg_logprob": -0.1140665899623524, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.000606831512413919}, {"id": 216, "seek": 140060, "start": 1400.6, "end": 1405.3999999999999, "text": " more expressive and the model would do better. But that prevents us from using this convolutional", "tokens": [50364, 544, 40189, 293, 264, 2316, 576, 360, 1101, 13, 583, 300, 22367, 505, 490, 1228, 341, 45216, 304, 50604], "temperature": 0.0, "avg_logprob": -0.10869814275385259, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0005356870824471116}, {"id": 217, "seek": 140060, "start": 1405.3999999999999, "end": 1409.32, "text": " view that would make things efficient. So we had to figure out a different way to make things", "tokens": [50604, 1910, 300, 576, 652, 721, 7148, 13, 407, 321, 632, 281, 2573, 484, 257, 819, 636, 281, 652, 721, 50800], "temperature": 0.0, "avg_logprob": -0.10869814275385259, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0005356870824471116}, {"id": 218, "seek": 140060, "start": 1409.32, "end": 1418.84, "text": " efficient. And so I focused on making that efficient on GPUs. And so our thinking was,", "tokens": [50800, 7148, 13, 400, 370, 286, 5178, 322, 1455, 300, 7148, 322, 18407, 82, 13, 400, 370, 527, 1953, 390, 11, 51276], "temperature": 0.0, "avg_logprob": -0.10869814275385259, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0005356870824471116}, {"id": 219, "seek": 140060, "start": 1419.9599999999998, "end": 1424.52, "text": " instead of, okay, we're going to have a large state size, but we don't have to write to actual", "tokens": [51332, 2602, 295, 11, 1392, 11, 321, 434, 516, 281, 362, 257, 2416, 1785, 2744, 11, 457, 321, 500, 380, 362, 281, 2464, 281, 3539, 51560], "temperature": 0.0, "avg_logprob": -0.10869814275385259, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0005356870824471116}, {"id": 220, "seek": 142452, "start": 1424.52, "end": 1432.92, "text": " GPU memory, like the HBM, where you can just keep that large state in a faster memory called S RAM.", "tokens": [50364, 18407, 4675, 11, 411, 264, 389, 18345, 11, 689, 291, 393, 445, 1066, 300, 2416, 1785, 294, 257, 4663, 4675, 1219, 318, 14561, 13, 50784], "temperature": 0.0, "avg_logprob": -0.17444466155709573, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.021611027419567108}, {"id": 221, "seek": 142452, "start": 1432.92, "end": 1439.24, "text": " You think of it as a cache. So if you're more familiar with CPUs, you just usually get cache and", "tokens": [50784, 509, 519, 295, 309, 382, 257, 19459, 13, 407, 498, 291, 434, 544, 4963, 365, 13199, 82, 11, 291, 445, 2673, 483, 19459, 293, 51100], "temperature": 0.0, "avg_logprob": -0.17444466155709573, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.021611027419567108}, {"id": 222, "seek": 142452, "start": 1439.24, "end": 1446.2, "text": " RAM. So if you have large state, you can keep it in the cache. And by avoiding having to write it", "tokens": [51100, 14561, 13, 407, 498, 291, 362, 2416, 1785, 11, 291, 393, 1066, 309, 294, 264, 19459, 13, 400, 538, 20220, 1419, 281, 2464, 309, 51448], "temperature": 0.0, "avg_logprob": -0.17444466155709573, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.021611027419567108}, {"id": 223, "seek": 142452, "start": 1446.2, "end": 1451.6399999999999, "text": " down, you actually don't suffer too much if the state is large. So that's essentially the core", "tokens": [51448, 760, 11, 291, 767, 500, 380, 9753, 886, 709, 498, 264, 1785, 307, 2416, 13, 407, 300, 311, 4476, 264, 4965, 51720], "temperature": 0.0, "avg_logprob": -0.17444466155709573, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.021611027419567108}, {"id": 224, "seek": 145164, "start": 1451.72, "end": 1455.48, "text": " idea. Would this be due to like input, like having to move the data around being really slow?", "tokens": [50368, 1558, 13, 6068, 341, 312, 3462, 281, 411, 4846, 11, 411, 1419, 281, 1286, 264, 1412, 926, 885, 534, 2964, 30, 50556], "temperature": 0.0, "avg_logprob": -0.16650562599057056, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.0029806485399603844}, {"id": 225, "seek": 145164, "start": 1456.2, "end": 1462.1200000000001, "text": " Yes. Yes. That makes sense. That's a really common constraint in a lot of these things. And it's like,", "tokens": [50592, 1079, 13, 1079, 13, 663, 1669, 2020, 13, 663, 311, 257, 534, 2689, 25534, 294, 257, 688, 295, 613, 721, 13, 400, 309, 311, 411, 11, 50888], "temperature": 0.0, "avg_logprob": -0.16650562599057056, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.0029806485399603844}, {"id": 226, "seek": 145164, "start": 1463.0, "end": 1467.88, "text": " I think one of the most insightful things I've had now with GPUs versus TPUs and stuff is how", "tokens": [50932, 286, 519, 472, 295, 264, 881, 46401, 721, 286, 600, 632, 586, 365, 18407, 82, 5717, 314, 8115, 82, 293, 1507, 307, 577, 51176], "temperature": 0.0, "avg_logprob": -0.16650562599057056, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.0029806485399603844}, {"id": 227, "seek": 145164, "start": 1467.88, "end": 1472.6000000000001, "text": " mixtures of expert models doesn't work very well on TPUs just because you have to like,", "tokens": [51176, 2752, 37610, 295, 5844, 5245, 1177, 380, 589, 588, 731, 322, 314, 8115, 82, 445, 570, 291, 362, 281, 411, 11, 51412], "temperature": 0.0, "avg_logprob": -0.16650562599057056, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.0029806485399603844}, {"id": 228, "seek": 145164, "start": 1473.24, "end": 1477.48, "text": " that essentially add a mixture of expert at a basic level. There's a routing layer that you", "tokens": [51444, 300, 4476, 909, 257, 9925, 295, 5844, 412, 257, 3875, 1496, 13, 821, 311, 257, 32722, 4583, 300, 291, 51656], "temperature": 0.0, "avg_logprob": -0.16650562599057056, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.0029806485399603844}, {"id": 229, "seek": 147748, "start": 1477.48, "end": 1481.64, "text": " learn and then multiple feedforward layers that you can choose from. And when you're distributing", "tokens": [50364, 1466, 293, 550, 3866, 3154, 13305, 7914, 300, 291, 393, 2826, 490, 13, 400, 562, 291, 434, 41406, 50572], "temperature": 0.0, "avg_logprob": -0.090769282409123, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.017983105033636093}, {"id": 230, "seek": 147748, "start": 1481.64, "end": 1486.6, "text": " this, the feedforward layers could end up on a different TPU node and TPUs communicate with their", "tokens": [50572, 341, 11, 264, 3154, 13305, 7914, 727, 917, 493, 322, 257, 819, 314, 8115, 9984, 293, 314, 8115, 82, 7890, 365, 641, 50820], "temperature": 0.0, "avg_logprob": -0.090769282409123, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.017983105033636093}, {"id": 231, "seek": 147748, "start": 1486.6, "end": 1493.0, "text": " neighbors. So TPUs take a big hit relative to GPUs where within video and video clusters,", "tokens": [50820, 12512, 13, 407, 314, 8115, 82, 747, 257, 955, 2045, 4972, 281, 18407, 82, 689, 1951, 960, 293, 960, 23313, 11, 51140], "temperature": 0.0, "avg_logprob": -0.090769282409123, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.017983105033636093}, {"id": 232, "seek": 147748, "start": 1493.0, "end": 1497.88, "text": " everything's connected so much more. And then it's easy to do that sort of distributed training.", "tokens": [51140, 1203, 311, 4582, 370, 709, 544, 13, 400, 550, 309, 311, 1858, 281, 360, 300, 1333, 295, 12631, 3097, 13, 51384], "temperature": 0.0, "avg_logprob": -0.090769282409123, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.017983105033636093}, {"id": 233, "seek": 147748, "start": 1499.32, "end": 1503.16, "text": " That's super interesting. And it's like, do you think there's going to be,", "tokens": [51456, 663, 311, 1687, 1880, 13, 400, 309, 311, 411, 11, 360, 291, 519, 456, 311, 516, 281, 312, 11, 51648], "temperature": 0.0, "avg_logprob": -0.090769282409123, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.017983105033636093}, {"id": 234, "seek": 150316, "start": 1503.88, "end": 1507.96, "text": " I guess this is really where I want to open the conversation of like, what is this going to", "tokens": [50400, 286, 2041, 341, 307, 534, 689, 286, 528, 281, 1269, 264, 3761, 295, 411, 11, 437, 307, 341, 516, 281, 50604], "temperature": 0.0, "avg_logprob": -0.12891187772646054, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0013248848263174295}, {"id": 235, "seek": 150316, "start": 1507.96, "end": 1517.16, "text": " happen in 2024 in this space? Are bigger players going to move in and be exploring this? My take,", "tokens": [50604, 1051, 294, 45237, 294, 341, 1901, 30, 2014, 3801, 4150, 516, 281, 1286, 294, 293, 312, 12736, 341, 30, 1222, 747, 11, 51064], "temperature": 0.0, "avg_logprob": -0.12891187772646054, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0013248848263174295}, {"id": 236, "seek": 150316, "start": 1517.16, "end": 1522.92, "text": " seeing how good the long context learning could be and a fundamental limit is that systems like", "tokens": [51064, 2577, 577, 665, 264, 938, 4319, 2539, 727, 312, 293, 257, 8088, 4948, 307, 300, 3652, 411, 51352], "temperature": 0.0, "avg_logprob": -0.12891187772646054, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0013248848263174295}, {"id": 237, "seek": 150316, "start": 1522.92, "end": 1529.3200000000002, "text": " ChatGPT are going to use a transformer model for most tasks. And then if you need to do", "tokens": [51352, 27503, 38, 47, 51, 366, 516, 281, 764, 257, 31782, 2316, 337, 881, 9608, 13, 400, 550, 498, 291, 643, 281, 360, 51672], "temperature": 0.0, "avg_logprob": -0.12891187772646054, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.0013248848263174295}, {"id": 238, "seek": 152932, "start": 1529.32, "end": 1534.6, "text": " summarization, you might do a long context specialized architecture. And then we could even", "tokens": [50364, 14611, 2144, 11, 291, 1062, 360, 257, 938, 4319, 19813, 9482, 13, 400, 550, 321, 727, 754, 50628], "temperature": 0.0, "avg_logprob": -0.17208090773573867, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0013668036554008722}, {"id": 239, "seek": 152932, "start": 1534.6, "end": 1539.24, "text": " see a whole quiver of architectures behind something that you're using. But I think", "tokens": [50628, 536, 257, 1379, 421, 1837, 295, 6331, 1303, 2261, 746, 300, 291, 434, 1228, 13, 583, 286, 519, 50860], "temperature": 0.0, "avg_logprob": -0.17208090773573867, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0013668036554008722}, {"id": 240, "seek": 152932, "start": 1539.96, "end": 1546.36, "text": " it's just like, is attention going to be dethroned? Is Sasha Rush somehow going to win this bet that", "tokens": [50896, 309, 311, 445, 411, 11, 307, 3202, 516, 281, 312, 1141, 1703, 19009, 30, 1119, 29276, 28389, 6063, 516, 281, 1942, 341, 778, 300, 51216], "temperature": 0.0, "avg_logprob": -0.17208090773573867, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0013668036554008722}, {"id": 241, "seek": 152932, "start": 1546.36, "end": 1551.0, "text": " everyone wants following in the area? What are you thinking about, either of you?", "tokens": [51216, 1518, 2738, 3480, 294, 264, 1859, 30, 708, 366, 291, 1953, 466, 11, 2139, 295, 291, 30, 51448], "temperature": 0.0, "avg_logprob": -0.17208090773573867, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0013668036554008722}, {"id": 242, "seek": 152932, "start": 1553.48, "end": 1558.76, "text": " I think it's a very, very strong architecture. And there's a proven recipe, right? You know,", "tokens": [51572, 286, 519, 309, 311, 257, 588, 11, 588, 2068, 9482, 13, 400, 456, 311, 257, 12785, 6782, 11, 558, 30, 509, 458, 11, 51836], "temperature": 0.0, "avg_logprob": -0.17208090773573867, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0013668036554008722}, {"id": 243, "seek": 155876, "start": 1558.84, "end": 1562.12, "text": " people scaling to a trillion of parameters. Right now, if you want, you say,", "tokens": [50368, 561, 21589, 281, 257, 18723, 295, 9834, 13, 1779, 586, 11, 498, 291, 528, 11, 291, 584, 11, 50532], "temperature": 0.0, "avg_logprob": -0.19855217500166458, "compression_ratio": 1.56, "no_speech_prob": 0.0009545403881929815}, {"id": 244, "seek": 155876, "start": 1562.76, "end": 1568.76, "text": " well, I just want the best performing model that runs most efficiently on my hardware,", "tokens": [50564, 731, 11, 286, 445, 528, 264, 1151, 10205, 2316, 300, 6676, 881, 19621, 322, 452, 8837, 11, 50864], "temperature": 0.0, "avg_logprob": -0.19855217500166458, "compression_ratio": 1.56, "no_speech_prob": 0.0009545403881929815}, {"id": 245, "seek": 155876, "start": 1568.76, "end": 1573.32, "text": " that has the most support on the software side. The transformer is a safe bet. I think it's here", "tokens": [50864, 300, 575, 264, 881, 1406, 322, 264, 4722, 1252, 13, 440, 31782, 307, 257, 3273, 778, 13, 286, 519, 309, 311, 510, 51092], "temperature": 0.0, "avg_logprob": -0.19855217500166458, "compression_ratio": 1.56, "no_speech_prob": 0.0009545403881929815}, {"id": 246, "seek": 155876, "start": 1573.32, "end": 1584.12, "text": " to stay. But I think there are new ideas like state space, China, so the linear attention,", "tokens": [51092, 281, 1754, 13, 583, 286, 519, 456, 366, 777, 3487, 411, 1785, 1901, 11, 3533, 11, 370, 264, 8213, 3202, 11, 51632], "temperature": 0.0, "avg_logprob": -0.19855217500166458, "compression_ratio": 1.56, "no_speech_prob": 0.0009545403881929815}, {"id": 247, "seek": 158412, "start": 1584.12, "end": 1589.0, "text": " ideas from linear attention, I think they're coming in. We've seen, as Michael mentioned,", "tokens": [50364, 3487, 490, 8213, 3202, 11, 286, 519, 436, 434, 1348, 294, 13, 492, 600, 1612, 11, 382, 5116, 2835, 11, 50608], "temperature": 0.0, "avg_logprob": -0.20049264890338303, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006286867428570986}, {"id": 248, "seek": 158412, "start": 1589.0, "end": 1594.52, "text": " that mixing some of these components seem to improve performance. We're validated at, I think,", "tokens": [50608, 300, 11983, 512, 295, 613, 6677, 1643, 281, 3470, 3389, 13, 492, 434, 7363, 770, 412, 11, 286, 519, 11, 50884], "temperature": 0.0, "avg_logprob": -0.20049264890338303, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006286867428570986}, {"id": 249, "seek": 158412, "start": 1594.52, "end": 1601.9599999999998, "text": " seven B scale, but maybe it might even work at larger scale. I think people tend to be", "tokens": [50884, 3407, 363, 4373, 11, 457, 1310, 309, 1062, 754, 589, 412, 4833, 4373, 13, 286, 519, 561, 3928, 281, 312, 51256], "temperature": 0.0, "avg_logprob": -0.20049264890338303, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006286867428570986}, {"id": 250, "seek": 158412, "start": 1601.9599999999998, "end": 1607.8799999999999, "text": " conservative and focusing too much on model architecture might not be worth their time.", "tokens": [51256, 13780, 293, 8416, 886, 709, 322, 2316, 9482, 1062, 406, 312, 3163, 641, 565, 13, 51552], "temperature": 0.0, "avg_logprob": -0.20049264890338303, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006286867428570986}, {"id": 251, "seek": 158412, "start": 1607.8799999999999, "end": 1611.6399999999999, "text": " Like the LIMAR architecture is very, very strong. Most people are doing off of that. They're", "tokens": [51552, 1743, 264, 441, 6324, 1899, 9482, 307, 588, 11, 588, 2068, 13, 4534, 561, 366, 884, 766, 295, 300, 13, 814, 434, 51740], "temperature": 0.0, "avg_logprob": -0.20049264890338303, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.006286867428570986}, {"id": 252, "seek": 161164, "start": 1611.72, "end": 1618.6000000000001, "text": " focusing on data. They're focusing on infrastructure, which makes sense. I think on", "tokens": [50368, 8416, 322, 1412, 13, 814, 434, 8416, 322, 6896, 11, 597, 1669, 2020, 13, 286, 519, 322, 50712], "temperature": 0.0, "avg_logprob": -0.1494913472757711, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0009694744367152452}, {"id": 253, "seek": 161164, "start": 1619.5600000000002, "end": 1626.3600000000001, "text": " my side personally, the other stuff is just plain interesting. There are more, I would say niche", "tokens": [50760, 452, 1252, 5665, 11, 264, 661, 1507, 307, 445, 11121, 1880, 13, 821, 366, 544, 11, 286, 576, 584, 19956, 51100], "temperature": 0.0, "avg_logprob": -0.1494913472757711, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0009694744367152452}, {"id": 254, "seek": 161164, "start": 1626.3600000000001, "end": 1634.0400000000002, "text": " use cases, niche for now, where some of these alternative architectures are interesting,", "tokens": [51100, 764, 3331, 11, 19956, 337, 586, 11, 689, 512, 295, 613, 8535, 6331, 1303, 366, 1880, 11, 51484], "temperature": 0.0, "avg_logprob": -0.1494913472757711, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0009694744367152452}, {"id": 255, "seek": 161164, "start": 1634.0400000000002, "end": 1637.0, "text": " things like long contacts, different domains like audio and genomics.", "tokens": [51484, 721, 411, 938, 15836, 11, 819, 25514, 411, 6278, 293, 1049, 29884, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1494913472757711, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0009694744367152452}, {"id": 256, "seek": 163700, "start": 1637.56, "end": 1643.88, "text": " There's just plain interesting scientific questions you can ask, whether it follow", "tokens": [50392, 821, 311, 445, 11121, 1880, 8134, 1651, 291, 393, 1029, 11, 1968, 309, 1524, 50708], "temperature": 0.0, "avg_logprob": -0.18731604682074654, "compression_ratio": 1.7470817120622568, "no_speech_prob": 0.001454493380151689}, {"id": 257, "seek": 163700, "start": 1643.88, "end": 1647.72, "text": " instruction just as well, whether it's fine to interest as well. Does it play well with", "tokens": [50708, 10951, 445, 382, 731, 11, 1968, 309, 311, 2489, 281, 1179, 382, 731, 13, 4402, 309, 862, 731, 365, 50900], "temperature": 0.0, "avg_logprob": -0.18731604682074654, "compression_ratio": 1.7470817120622568, "no_speech_prob": 0.001454493380151689}, {"id": 258, "seek": 163700, "start": 1647.72, "end": 1652.2, "text": " quantization and so on? There's just plain interesting research questions we can ask.", "tokens": [50900, 4426, 2144, 293, 370, 322, 30, 821, 311, 445, 11121, 1880, 2132, 1651, 321, 393, 1029, 13, 51124], "temperature": 0.0, "avg_logprob": -0.18731604682074654, "compression_ratio": 1.7470817120622568, "no_speech_prob": 0.001454493380151689}, {"id": 259, "seek": 163700, "start": 1652.2, "end": 1658.36, "text": " Now on the production level, I think transformer is still incredibly strong, very well supported,", "tokens": [51124, 823, 322, 264, 4265, 1496, 11, 286, 519, 31782, 307, 920, 6252, 2068, 11, 588, 731, 8104, 11, 51432], "temperature": 0.0, "avg_logprob": -0.18731604682074654, "compression_ratio": 1.7470817120622568, "no_speech_prob": 0.001454493380151689}, {"id": 260, "seek": 163700, "start": 1659.64, "end": 1665.16, "text": " both hardware and software. But I think some of these new ideas are coming in and people might", "tokens": [51496, 1293, 8837, 293, 4722, 13, 583, 286, 519, 512, 295, 613, 777, 3487, 366, 1348, 294, 293, 561, 1062, 51772], "temperature": 0.0, "avg_logprob": -0.18731604682074654, "compression_ratio": 1.7470817120622568, "no_speech_prob": 0.001454493380151689}, {"id": 261, "seek": 166516, "start": 1665.16, "end": 1669.48, "text": " start putting them as part of component in the transformer. Maybe we'll still call them", "tokens": [50364, 722, 3372, 552, 382, 644, 295, 6542, 294, 264, 31782, 13, 2704, 321, 603, 920, 818, 552, 50580], "temperature": 0.0, "avg_logprob": -0.28605511983235676, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.00024869744083844125}, {"id": 262, "seek": 166516, "start": 1669.48, "end": 1676.68, "text": " transformer, but they'll just have more layers and just attention and LPE.", "tokens": [50580, 31782, 11, 457, 436, 603, 445, 362, 544, 7914, 293, 445, 3202, 293, 441, 5208, 13, 50940], "temperature": 0.0, "avg_logprob": -0.28605511983235676, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.00024869744083844125}, {"id": 263, "seek": 166516, "start": 1678.52, "end": 1687.0, "text": " Yeah, I 100% agree with you. So attention as a computational primitive is not going anywhere", "tokens": [51032, 865, 11, 286, 2319, 4, 3986, 365, 291, 13, 407, 3202, 382, 257, 28270, 28540, 307, 406, 516, 4992, 51456], "temperature": 0.0, "avg_logprob": -0.28605511983235676, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.00024869744083844125}, {"id": 264, "seek": 168700, "start": 1687.72, "end": 1693.8, "text": " anytime soon. It's just a very efficient and a very convenient way to increase", "tokens": [50400, 13038, 2321, 13, 467, 311, 445, 257, 588, 7148, 293, 257, 588, 10851, 636, 281, 3488, 50704], "temperature": 0.0, "avg_logprob": -0.08457955394882753, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.002630001399666071}, {"id": 265, "seek": 168700, "start": 1694.36, "end": 1703.56, "text": " the effective state of your sequence processor. So at some level, if you're working with a model", "tokens": [50732, 264, 4942, 1785, 295, 428, 8310, 15321, 13, 407, 412, 512, 1496, 11, 498, 291, 434, 1364, 365, 257, 2316, 51192], "temperature": 0.0, "avg_logprob": -0.08457955394882753, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.002630001399666071}, {"id": 266, "seek": 168700, "start": 1703.56, "end": 1710.2, "text": " that only has a fixed state in each of its sequence mixers, you have an assumption and", "tokens": [51192, 300, 787, 575, 257, 6806, 1785, 294, 1184, 295, 1080, 8310, 2890, 433, 11, 291, 362, 364, 15302, 293, 51524], "temperature": 0.0, "avg_logprob": -0.08457955394882753, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.002630001399666071}, {"id": 267, "seek": 168700, "start": 1710.2, "end": 1715.0, "text": " your assumption is that you only need so much information in the sequence. So there's always", "tokens": [51524, 428, 15302, 307, 300, 291, 787, 643, 370, 709, 1589, 294, 264, 8310, 13, 407, 456, 311, 1009, 51764], "temperature": 0.0, "avg_logprob": -0.08457955394882753, "compression_ratio": 1.6745283018867925, "no_speech_prob": 0.002630001399666071}, {"id": 268, "seek": 171500, "start": 1715.0, "end": 1721.24, "text": " a tradeoff between the ratio of the state dimension, the sequence length, as you push", "tokens": [50364, 257, 4923, 4506, 1296, 264, 8509, 295, 264, 1785, 10139, 11, 264, 8310, 4641, 11, 382, 291, 2944, 50676], "temperature": 0.0, "avg_logprob": -0.18887532990554284, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0005791697767563164}, {"id": 269, "seek": 171500, "start": 1721.24, "end": 1726.76, "text": " things to the extreme, either model sizes, so as you make the model bigger, wider, effectively", "tokens": [50676, 721, 281, 264, 8084, 11, 2139, 2316, 11602, 11, 370, 382, 291, 652, 264, 2316, 3801, 11, 11842, 11, 8659, 50952], "temperature": 0.0, "avg_logprob": -0.18887532990554284, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0005791697767563164}, {"id": 270, "seek": 171500, "start": 1726.76, "end": 1736.6, "text": " introduce more states and sequence length. Some of these margins, some of this is speculation,", "tokens": [50952, 5366, 544, 4368, 293, 8310, 4641, 13, 2188, 295, 613, 30317, 11, 512, 295, 341, 307, 27696, 11, 51444], "temperature": 0.0, "avg_logprob": -0.18887532990554284, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0005791697767563164}, {"id": 271, "seek": 171500, "start": 1736.6, "end": 1742.84, "text": " but some of these margins will disappear, some of the tradeoffs will change, especially 14, 30,", "tokens": [51444, 457, 512, 295, 613, 30317, 486, 11596, 11, 512, 295, 264, 4923, 19231, 486, 1319, 11, 2318, 3499, 11, 2217, 11, 51756], "temperature": 0.0, "avg_logprob": -0.18887532990554284, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0005791697767563164}, {"id": 272, "seek": 174284, "start": 1742.9199999999998, "end": 1751.9599999999998, "text": " some of these very fat models. But certainly, either whether that's hybridizing or some kind", "tokens": [50368, 512, 295, 613, 588, 4046, 5245, 13, 583, 3297, 11, 2139, 1968, 300, 311, 13051, 3319, 420, 512, 733, 50820], "temperature": 0.0, "avg_logprob": -0.16653223882747603, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0016479067271575332}, {"id": 273, "seek": 174284, "start": 1751.9599999999998, "end": 1757.8799999999999, "text": " of new block, we're certainly going to see some more innovation. That's really exciting.", "tokens": [50820, 295, 777, 3461, 11, 321, 434, 3297, 516, 281, 536, 512, 544, 8504, 13, 663, 311, 534, 4670, 13, 51116], "temperature": 0.0, "avg_logprob": -0.16653223882747603, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0016479067271575332}, {"id": 274, "seek": 174284, "start": 1757.8799999999999, "end": 1762.84, "text": " My personal, if I had to make a prediction is that architecture design will get more", "tokens": [51116, 1222, 2973, 11, 498, 286, 632, 281, 652, 257, 17630, 307, 300, 9482, 1715, 486, 483, 544, 51364], "temperature": 0.0, "avg_logprob": -0.16653223882747603, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0016479067271575332}, {"id": 275, "seek": 174284, "start": 1763.56, "end": 1766.9199999999998, "text": " interesting or more complex. There's going to be more to do.", "tokens": [51400, 1880, 420, 544, 3997, 13, 821, 311, 516, 281, 312, 544, 281, 360, 13, 51568], "temperature": 0.0, "avg_logprob": -0.16653223882747603, "compression_ratio": 1.5571428571428572, "no_speech_prob": 0.0016479067271575332}, {"id": 276, "seek": 176692, "start": 1767.8000000000002, "end": 1774.8400000000001, "text": " Yeah, I mean, this year, it's like, I got some 10 minute clock that's fine for us. I think with", "tokens": [50408, 865, 11, 286, 914, 11, 341, 1064, 11, 309, 311, 411, 11, 286, 658, 512, 1266, 3456, 7830, 300, 311, 2489, 337, 505, 13, 286, 519, 365, 50760], "temperature": 0.0, "avg_logprob": -0.18668050688456714, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.004198133014142513}, {"id": 277, "seek": 176692, "start": 1774.8400000000001, "end": 1783.0, "text": " mixture of experts and this being popular as a state-state model, this is all just really within", "tokens": [50760, 9925, 295, 8572, 293, 341, 885, 3743, 382, 257, 1785, 12, 15406, 2316, 11, 341, 307, 439, 445, 534, 1951, 51168], "temperature": 0.0, "avg_logprob": -0.18668050688456714, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.004198133014142513}, {"id": 278, "seek": 176692, "start": 1783.0, "end": 1786.6000000000001, "text": " a few months outside of OpenAI. They've been doing mixture of experts for a lot longer than", "tokens": [51168, 257, 1326, 2493, 2380, 295, 7238, 48698, 13, 814, 600, 668, 884, 9925, 295, 8572, 337, 257, 688, 2854, 813, 51348], "temperature": 0.0, "avg_logprob": -0.18668050688456714, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.004198133014142513}, {"id": 279, "seek": 176692, "start": 1786.6000000000001, "end": 1792.2, "text": " everyone, but in terms of open and academic communities, no one's really tried to do RLHF", "tokens": [51348, 1518, 11, 457, 294, 2115, 295, 1269, 293, 7778, 4456, 11, 572, 472, 311, 534, 3031, 281, 360, 497, 43, 39, 37, 51628], "temperature": 0.0, "avg_logprob": -0.18668050688456714, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.004198133014142513}, {"id": 280, "seek": 176692, "start": 1792.2, "end": 1795.5600000000002, "text": " on mixture of experts. It should just work, but we have to learn all these things. And then the", "tokens": [51628, 322, 9925, 295, 8572, 13, 467, 820, 445, 589, 11, 457, 321, 362, 281, 1466, 439, 613, 721, 13, 400, 550, 264, 51796], "temperature": 0.0, "avg_logprob": -0.18668050688456714, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.004198133014142513}, {"id": 281, "seek": 179556, "start": 1795.56, "end": 1802.36, "text": " model grafting is becoming more of a real thing. That's super interesting. And it's just, I agree", "tokens": [50364, 2316, 1295, 20930, 307, 5617, 544, 295, 257, 957, 551, 13, 663, 311, 1687, 1880, 13, 400, 309, 311, 445, 11, 286, 3986, 50704], "temperature": 0.0, "avg_logprob": -0.12441011775623669, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.00032500081579200923}, {"id": 282, "seek": 179556, "start": 1802.36, "end": 1809.48, "text": " that it's just fun to follow. And hopefully, it gives academics and scientists more ways to", "tokens": [50704, 300, 309, 311, 445, 1019, 281, 1524, 13, 400, 4696, 11, 309, 2709, 25695, 293, 7708, 544, 2098, 281, 51060], "temperature": 0.0, "avg_logprob": -0.12441011775623669, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.00032500081579200923}, {"id": 283, "seek": 179556, "start": 1809.48, "end": 1814.2, "text": " influence the conversation where in industry, it's just about scaling and bigger models where we", "tokens": [51060, 6503, 264, 3761, 689, 294, 3518, 11, 309, 311, 445, 466, 21589, 293, 3801, 5245, 689, 321, 51296], "temperature": 0.0, "avg_logprob": -0.12441011775623669, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.00032500081579200923}, {"id": 284, "seek": 179556, "start": 1814.2, "end": 1818.84, "text": " could maybe do specific things better, which I'm telling open source companies to do with", "tokens": [51296, 727, 1310, 360, 2685, 721, 1101, 11, 597, 286, 478, 3585, 1269, 4009, 3431, 281, 360, 365, 51528], "temperature": 0.0, "avg_logprob": -0.12441011775623669, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.00032500081579200923}, {"id": 285, "seek": 179556, "start": 1818.84, "end": 1822.12, "text": " their language models anyways, like if they want to have a business model, they need to have an edge.", "tokens": [51528, 641, 2856, 5245, 13448, 11, 411, 498, 436, 528, 281, 362, 257, 1606, 2316, 11, 436, 643, 281, 362, 364, 4691, 13, 51692], "temperature": 0.0, "avg_logprob": -0.12441011775623669, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.00032500081579200923}, {"id": 286, "seek": 182212, "start": 1822.6799999999998, "end": 1827.08, "text": " So this all fits into that kind of narrative pretty well with my regards.", "tokens": [50392, 407, 341, 439, 9001, 666, 300, 733, 295, 9977, 1238, 731, 365, 452, 14258, 13, 50612], "temperature": 0.0, "avg_logprob": -0.10022087942195844, "compression_ratio": 1.4654377880184333, "no_speech_prob": 0.0015976120484992862}, {"id": 287, "seek": 182212, "start": 1828.4399999999998, "end": 1832.12, "text": " Is there anything else you guys are following in ML? It doesn't have to be about state-space", "tokens": [50680, 1119, 456, 1340, 1646, 291, 1074, 366, 3480, 294, 21601, 30, 467, 1177, 380, 362, 281, 312, 466, 1785, 12, 24824, 50864], "temperature": 0.0, "avg_logprob": -0.10022087942195844, "compression_ratio": 1.4654377880184333, "no_speech_prob": 0.0015976120484992862}, {"id": 288, "seek": 182212, "start": 1832.12, "end": 1835.08, "text": " models. What's exciting for you broadly for next year?", "tokens": [50864, 5245, 13, 708, 311, 4670, 337, 291, 19511, 337, 958, 1064, 30, 51012], "temperature": 0.0, "avg_logprob": -0.10022087942195844, "compression_ratio": 1.4654377880184333, "no_speech_prob": 0.0015976120484992862}, {"id": 289, "seek": 182212, "start": 1838.12, "end": 1846.9199999999998, "text": " Yeah, personally, I think data is still the most important thing. We're thinking a lot about how", "tokens": [51164, 865, 11, 5665, 11, 286, 519, 1412, 307, 920, 264, 881, 1021, 551, 13, 492, 434, 1953, 257, 688, 466, 577, 51604], "temperature": 0.0, "avg_logprob": -0.10022087942195844, "compression_ratio": 1.4654377880184333, "no_speech_prob": 0.0015976120484992862}, {"id": 290, "seek": 184692, "start": 1846.92, "end": 1853.5600000000002, "text": " data influences the model performance, like really teasing that out, either having some", "tokens": [50364, 1412, 21222, 264, 2316, 3389, 11, 411, 534, 37720, 300, 484, 11, 2139, 1419, 512, 50696], "temperature": 0.0, "avg_logprob": -0.18462626139322916, "compression_ratio": 1.6770186335403727, "no_speech_prob": 0.002286504954099655}, {"id": 291, "seek": 184692, "start": 1853.5600000000002, "end": 1859.5600000000002, "text": " of the synthetic tasks that correlates very well with model performance has been kind of the", "tokens": [50696, 295, 264, 23420, 9608, 300, 13983, 1024, 588, 731, 365, 2316, 3389, 575, 668, 733, 295, 264, 50996], "temperature": 0.0, "avg_logprob": -0.18462626139322916, "compression_ratio": 1.6770186335403727, "no_speech_prob": 0.002286504954099655}, {"id": 292, "seek": 184692, "start": 1859.5600000000002, "end": 1867.0800000000002, "text": " motivating examples in a lot of our papers and work has been focusing on synthetic tasks,", "tokens": [50996, 41066, 5110, 294, 257, 688, 295, 527, 10577, 293, 589, 575, 668, 8416, 322, 23420, 9608, 11, 51372], "temperature": 0.0, "avg_logprob": -0.18462626139322916, "compression_ratio": 1.6770186335403727, "no_speech_prob": 0.002286504954099655}, {"id": 293, "seek": 186708, "start": 1868.04, "end": 1877.3999999999999, "text": " or having maybe smaller data sets that kind of make it easier to really understand what's", "tokens": [50412, 420, 1419, 1310, 4356, 1412, 6352, 300, 733, 295, 652, 309, 3571, 281, 534, 1223, 437, 311, 50880], "temperature": 0.0, "avg_logprob": -0.20217521488666534, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.02331809513270855}, {"id": 294, "seek": 186708, "start": 1877.3999999999999, "end": 1887.32, "text": " really going on. So I think personally, my focus is going to be on data for the next little bit.", "tokens": [50880, 534, 516, 322, 13, 407, 286, 519, 5665, 11, 452, 1879, 307, 516, 281, 312, 322, 1412, 337, 264, 958, 707, 857, 13, 51376], "temperature": 0.0, "avg_logprob": -0.20217521488666534, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.02331809513270855}, {"id": 295, "seek": 186708, "start": 1888.6, "end": 1895.72, "text": " All the architecture stuff is fun. Making that hardware efficient is fun, but I think", "tokens": [51440, 1057, 264, 9482, 1507, 307, 1019, 13, 14595, 300, 8837, 7148, 307, 1019, 11, 457, 286, 519, 51796], "temperature": 0.0, "avg_logprob": -0.20217521488666534, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.02331809513270855}, {"id": 296, "seek": 189708, "start": 1897.08, "end": 1903.96, "text": " ultimately it's about data. If you look at the scaling law curve, the different", "tokens": [50364, 6284, 309, 311, 466, 1412, 13, 759, 291, 574, 412, 264, 21589, 2101, 7605, 11, 264, 819, 50708], "temperature": 0.0, "avg_logprob": -0.172379352428295, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.00040443119360134006}, {"id": 297, "seek": 189708, "start": 1903.96, "end": 1907.1599999999999, "text": " monarchitectures generally have the same slope. They're just different offset.", "tokens": [50708, 1108, 1178, 5739, 1303, 5101, 362, 264, 912, 13525, 13, 814, 434, 445, 819, 18687, 13, 50868], "temperature": 0.0, "avg_logprob": -0.172379352428295, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.00040443119360134006}, {"id": 298, "seek": 189708, "start": 1908.36, "end": 1912.52, "text": " Seems like the only thing that changes the slope is the data quality.", "tokens": [50928, 22524, 411, 264, 787, 551, 300, 2962, 264, 13525, 307, 264, 1412, 3125, 13, 51136], "temperature": 0.0, "avg_logprob": -0.172379352428295, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.00040443119360134006}, {"id": 299, "seek": 189708, "start": 1913.6399999999999, "end": 1918.84, "text": " I love that point. That does seem true. I have the plot from Mamba in this blog post that I'm", "tokens": [51192, 286, 959, 300, 935, 13, 663, 775, 1643, 2074, 13, 286, 362, 264, 7542, 490, 376, 23337, 294, 341, 6968, 2183, 300, 286, 478, 51452], "temperature": 0.0, "avg_logprob": -0.172379352428295, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.00040443119360134006}, {"id": 300, "seek": 191884, "start": 1918.84, "end": 1922.9199999999998, "text": " writing, which is just a little bit above. Same slope.", "tokens": [50364, 3579, 11, 597, 307, 445, 257, 707, 857, 3673, 13, 10635, 13525, 13, 50568], "temperature": 0.0, "avg_logprob": -0.17715897498192726, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0004801918694283813}, {"id": 301, "seek": 191884, "start": 1927.56, "end": 1934.4399999999998, "text": " Yeah, maybe add data. Data is really interesting. So miniaturizing architecture design, finding,", "tokens": [50800, 865, 11, 1310, 909, 1412, 13, 11888, 307, 534, 1880, 13, 407, 923, 7676, 374, 3319, 9482, 1715, 11, 5006, 11, 51144], "temperature": 0.0, "avg_logprob": -0.17715897498192726, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0004801918694283813}, {"id": 302, "seek": 191884, "start": 1935.08, "end": 1940.9199999999998, "text": " breaking down what tasks are involved into, for example, language modeling and trying to", "tokens": [51176, 7697, 760, 437, 9608, 366, 3288, 666, 11, 337, 1365, 11, 2856, 15983, 293, 1382, 281, 51468], "temperature": 0.0, "avg_logprob": -0.17715897498192726, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0004801918694283813}, {"id": 303, "seek": 191884, "start": 1942.1999999999998, "end": 1946.12, "text": " package them into something that can be used to iterate, something that's quite exciting.", "tokens": [51532, 7372, 552, 666, 746, 300, 393, 312, 1143, 281, 44497, 11, 746, 300, 311, 1596, 4670, 13, 51728], "temperature": 0.0, "avg_logprob": -0.17715897498192726, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.0004801918694283813}, {"id": 304, "seek": 194612, "start": 1946.4399999999998, "end": 1958.04, "text": " That was one of the main techniques that was used for this zoology paper that also looks into", "tokens": [50380, 663, 390, 472, 295, 264, 2135, 7512, 300, 390, 1143, 337, 341, 710, 1092, 7794, 3035, 300, 611, 1542, 666, 50960], "temperature": 0.0, "avg_logprob": -0.20428826014200846, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.0006445248727686703}, {"id": 305, "seek": 194612, "start": 1958.04, "end": 1962.28, "text": " some of these different behaviors. Personally, I'm also really excited about new applications,", "tokens": [50960, 512, 295, 613, 819, 15501, 13, 21079, 11, 286, 478, 611, 534, 2919, 466, 777, 5821, 11, 51172], "temperature": 0.0, "avg_logprob": -0.20428826014200846, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.0006445248727686703}, {"id": 306, "seek": 194612, "start": 1964.1999999999998, "end": 1970.6, "text": " scientific applications, the genomics work, but even more engineering-focused.", "tokens": [51268, 8134, 5821, 11, 264, 1049, 29884, 589, 11, 457, 754, 544, 7043, 12, 44062, 13, 51588], "temperature": 0.0, "avg_logprob": -0.20428826014200846, "compression_ratio": 1.4916201117318435, "no_speech_prob": 0.0006445248727686703}, {"id": 307, "seek": 197060, "start": 1971.0, "end": 1980.28, "text": " We're seeing a shift. Right now, language is still the domain that gets most clicks,", "tokens": [50384, 492, 434, 2577, 257, 5513, 13, 1779, 586, 11, 2856, 307, 920, 264, 9274, 300, 2170, 881, 18521, 11, 50848], "temperature": 0.0, "avg_logprob": -0.20318690405951606, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0013247207971289754}, {"id": 308, "seek": 197060, "start": 1980.28, "end": 1982.28, "text": " most interest, but I think that will evolve over time.", "tokens": [50848, 881, 1179, 11, 457, 286, 519, 300, 486, 16693, 670, 565, 13, 50948], "temperature": 0.0, "avg_logprob": -0.20318690405951606, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0013247207971289754}, {"id": 309, "seek": 197060, "start": 1984.36, "end": 1988.6799999999998, "text": " Some of these other applications offer, even just talking about architectures,", "tokens": [51052, 2188, 295, 613, 661, 5821, 2626, 11, 754, 445, 1417, 466, 6331, 1303, 11, 51268], "temperature": 0.0, "avg_logprob": -0.20318690405951606, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0013247207971289754}, {"id": 310, "seek": 197060, "start": 1988.6799999999998, "end": 1993.32, "text": " they offer completely different design space. I'm excited to look into it.", "tokens": [51268, 436, 2626, 2584, 819, 1715, 1901, 13, 286, 478, 2919, 281, 574, 666, 309, 13, 51500], "temperature": 0.0, "avg_logprob": -0.20318690405951606, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0013247207971289754}, {"id": 311, "seek": 197060, "start": 1994.04, "end": 1999.24, "text": " Yeah, everyone talks about language, but I feel like images and entertainment and videos are the", "tokens": [51536, 865, 11, 1518, 6686, 466, 2856, 11, 457, 286, 841, 411, 5267, 293, 12393, 293, 2145, 366, 264, 51796], "temperature": 0.0, "avg_logprob": -0.20318690405951606, "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.0013247207971289754}, {"id": 312, "seek": 199924, "start": 1999.32, "end": 2003.56, "text": " things that are so obviously going to generate so much value to me. I don't know the ceiling on", "tokens": [50368, 721, 300, 366, 370, 2745, 516, 281, 8460, 370, 709, 2158, 281, 385, 13, 286, 500, 380, 458, 264, 13655, 322, 50580], "temperature": 0.0, "avg_logprob": -0.10034476485207816, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0049039991572499275}, {"id": 313, "seek": 199924, "start": 2003.56, "end": 2009.24, "text": " language, but when you could access a somewhat local text and video model at your homework", "tokens": [50580, 2856, 11, 457, 562, 291, 727, 2105, 257, 8344, 2654, 2487, 293, 960, 2316, 412, 428, 14578, 50864], "temperature": 0.0, "avg_logprob": -0.10034476485207816, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0049039991572499275}, {"id": 314, "seek": 199924, "start": 2009.24, "end": 2014.44, "text": " station, that's tailored to your preferences. The amount of value that that creates is totally", "tokens": [50864, 5214, 11, 300, 311, 34858, 281, 428, 21910, 13, 440, 2372, 295, 2158, 300, 300, 7829, 307, 3879, 51124], "temperature": 0.0, "avg_logprob": -0.10034476485207816, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0049039991572499275}, {"id": 315, "seek": 199924, "start": 2015.08, "end": 2021.24, "text": " astronomical. I'm excited. I started playing around with these where I'd take text of the", "tokens": [51156, 49035, 13, 286, 478, 2919, 13, 286, 1409, 2433, 926, 365, 613, 689, 286, 1116, 747, 2487, 295, 264, 51464], "temperature": 0.0, "avg_logprob": -0.10034476485207816, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0049039991572499275}, {"id": 316, "seek": 199924, "start": 2021.24, "end": 2027.0, "text": " blog and convert it to dolly images and convert it to a video with generated audio all with one", "tokens": [51464, 6968, 293, 7620, 309, 281, 2722, 88, 5267, 293, 7620, 309, 281, 257, 960, 365, 10833, 6278, 439, 365, 472, 51752], "temperature": 0.0, "avg_logprob": -0.10034476485207816, "compression_ratio": 1.6859205776173285, "no_speech_prob": 0.0049039991572499275}, {"id": 317, "seek": 202700, "start": 2027.0, "end": 2033.8, "text": " Python script. That's really easy to do, so I agree with your more than language. It's fun to", "tokens": [50364, 15329, 5755, 13, 663, 311, 534, 1858, 281, 360, 11, 370, 286, 3986, 365, 428, 544, 813, 2856, 13, 467, 311, 1019, 281, 50704], "temperature": 0.0, "avg_logprob": -0.12387779484624448, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0025505442172288895}, {"id": 318, "seek": 202700, "start": 2033.8, "end": 2040.36, "text": " have that view. These things actually do work really well in your experience when you stitch", "tokens": [50704, 362, 300, 1910, 13, 1981, 721, 767, 360, 589, 534, 731, 294, 428, 1752, 562, 291, 5635, 51032], "temperature": 0.0, "avg_logprob": -0.12387779484624448, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0025505442172288895}, {"id": 319, "seek": 202700, "start": 2040.36, "end": 2047.16, "text": " all of them together. It's not that good. The dolly images are pretty similar, but I'm doing", "tokens": [51032, 439, 295, 552, 1214, 13, 467, 311, 406, 300, 665, 13, 440, 2722, 88, 5267, 366, 1238, 2531, 11, 457, 286, 478, 884, 51372], "temperature": 0.0, "avg_logprob": -0.12387779484624448, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0025505442172288895}, {"id": 320, "seek": 202700, "start": 2047.16, "end": 2052.36, "text": " something really naive where I literally take the text and have a system prompt. It's like,", "tokens": [51372, 746, 534, 29052, 689, 286, 3736, 747, 264, 2487, 293, 362, 257, 1185, 12391, 13, 467, 311, 411, 11, 51632], "temperature": 0.0, "avg_logprob": -0.12387779484624448, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0025505442172288895}, {"id": 321, "seek": 205236, "start": 2052.36, "end": 2057.88, "text": " you're generating a series of images for visualizing a blog post, and it generates various", "tokens": [50364, 291, 434, 17746, 257, 2638, 295, 5267, 337, 5056, 3319, 257, 6968, 2183, 11, 293, 309, 23815, 3683, 50640], "temperature": 0.0, "avg_logprob": -0.1365631862922951, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.01853935606777668}, {"id": 322, "seek": 205236, "start": 2058.84, "end": 2062.6800000000003, "text": " all the machine learning thumbnails that you see everyone using. There are variations of that.", "tokens": [50688, 439, 264, 3479, 2539, 46987, 300, 291, 536, 1518, 1228, 13, 821, 366, 17840, 295, 300, 13, 50880], "temperature": 0.0, "avg_logprob": -0.1365631862922951, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.01853935606777668}, {"id": 323, "seek": 205236, "start": 2062.6800000000003, "end": 2066.52, "text": " The fun ones are whether it's about llama or mamba or something, and then they generate", "tokens": [50880, 440, 1019, 2306, 366, 1968, 309, 311, 466, 23272, 420, 275, 23337, 420, 746, 11, 293, 550, 436, 8460, 51072], "temperature": 0.0, "avg_logprob": -0.1365631862922951, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.01853935606777668}, {"id": 324, "seek": 205236, "start": 2066.52, "end": 2072.84, "text": " animals in them, which is good. I think I could get much better at it and have a better segmentation", "tokens": [51072, 4882, 294, 552, 11, 597, 307, 665, 13, 286, 519, 286, 727, 483, 709, 1101, 412, 309, 293, 362, 257, 1101, 9469, 399, 51388], "temperature": 0.0, "avg_logprob": -0.1365631862922951, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.01853935606777668}, {"id": 325, "seek": 205236, "start": 2072.84, "end": 2077.96, "text": " system for the paragraphs and or have chat to BT summarize them or something like that. But I", "tokens": [51388, 1185, 337, 264, 48910, 293, 420, 362, 5081, 281, 31144, 20858, 552, 420, 746, 411, 300, 13, 583, 286, 51644], "temperature": 0.0, "avg_logprob": -0.1365631862922951, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.01853935606777668}, {"id": 326, "seek": 207796, "start": 2077.96, "end": 2082.6, "text": " just know that within a year, it's going to be a text-to-video API, and I'm just going to switch", "tokens": [50364, 445, 458, 300, 1951, 257, 1064, 11, 309, 311, 516, 281, 312, 257, 2487, 12, 1353, 12, 40876, 9362, 11, 293, 286, 478, 445, 516, 281, 3679, 50596], "temperature": 0.0, "avg_logprob": -0.1091477626890648, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005059398710727692}, {"id": 327, "seek": 207796, "start": 2082.6, "end": 2087.4, "text": " it, and it's going to be great. I'm laying the groundwork for infrastructure to have a", "tokens": [50596, 309, 11, 293, 309, 311, 516, 281, 312, 869, 13, 286, 478, 14903, 264, 2727, 1902, 337, 6896, 281, 362, 257, 50836], "temperature": 0.0, "avg_logprob": -0.1091477626890648, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005059398710727692}, {"id": 328, "seek": 207796, "start": 2087.4, "end": 2095.8, "text": " multimodal content distribution, really, and I just expect it to become very fun. Even the text", "tokens": [50836, 32972, 378, 304, 2701, 7316, 11, 534, 11, 293, 286, 445, 2066, 309, 281, 1813, 588, 1019, 13, 2754, 264, 2487, 51256], "temperature": 0.0, "avg_logprob": -0.1091477626890648, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005059398710727692}, {"id": 329, "seek": 207796, "start": 2095.8, "end": 2101.48, "text": " of voice is pretty good, I think. I don't have a studio, but once you have a studio, it's going", "tokens": [51256, 295, 3177, 307, 1238, 665, 11, 286, 519, 13, 286, 500, 380, 362, 257, 6811, 11, 457, 1564, 291, 362, 257, 6811, 11, 309, 311, 516, 51540], "temperature": 0.0, "avg_logprob": -0.1091477626890648, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005059398710727692}, {"id": 330, "seek": 207796, "start": 2101.48, "end": 2106.52, "text": " to be able to generate perfect audio for whatever you want. Another one of my dreams that is bad", "tokens": [51540, 281, 312, 1075, 281, 8460, 2176, 6278, 337, 2035, 291, 528, 13, 3996, 472, 295, 452, 7505, 300, 307, 1578, 51792], "temperature": 0.0, "avg_logprob": -0.1091477626890648, "compression_ratio": 1.7352941176470589, "no_speech_prob": 0.005059398710727692}, {"id": 331, "seek": 210652, "start": 2106.52, "end": 2113.64, "text": " for young students is I want to be able to give a slide deck to a script that returns the five-minute", "tokens": [50364, 337, 2037, 1731, 307, 286, 528, 281, 312, 1075, 281, 976, 257, 4137, 9341, 281, 257, 5755, 300, 11247, 264, 1732, 12, 18256, 50720], "temperature": 0.0, "avg_logprob": -0.12545492842390732, "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.0036485239397734404}, {"id": 332, "seek": 210652, "start": 2113.64, "end": 2123.16, "text": " conference video that no one ever watched, just based on a GPT-4 reading the slide deck and voicing", "tokens": [50720, 7586, 960, 300, 572, 472, 1562, 6337, 11, 445, 2361, 322, 257, 26039, 51, 12, 19, 3760, 264, 4137, 9341, 293, 1650, 5776, 51196], "temperature": 0.0, "avg_logprob": -0.12545492842390732, "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.0036485239397734404}, {"id": 333, "seek": 210652, "start": 2123.16, "end": 2128.68, "text": " yourself. Those are the silly things that I have time to do because I'm not a professor.", "tokens": [51196, 1803, 13, 3950, 366, 264, 11774, 721, 300, 286, 362, 565, 281, 360, 570, 286, 478, 406, 257, 8304, 13, 51472], "temperature": 0.0, "avg_logprob": -0.12545492842390732, "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.0036485239397734404}, {"id": 334, "seek": 212868, "start": 2128.68, "end": 2136.7599999999998, "text": " Yeah, I think these advances, these systems, they do generate a lot of economic value,", "tokens": [50364, 865, 11, 286, 519, 613, 25297, 11, 613, 3652, 11, 436, 360, 8460, 257, 688, 295, 4836, 2158, 11, 50768], "temperature": 0.0, "avg_logprob": -0.15597998421147186, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.07691564410924911}, {"id": 335, "seek": 212868, "start": 2136.7599999999998, "end": 2141.48, "text": " and we're seeing that already. Lots of companies are now switching to using these things, and I", "tokens": [50768, 293, 321, 434, 2577, 300, 1217, 13, 15908, 295, 3431, 366, 586, 16493, 281, 1228, 613, 721, 11, 293, 286, 51004], "temperature": 0.0, "avg_logprob": -0.15597998421147186, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.07691564410924911}, {"id": 336, "seek": 212868, "start": 2141.48, "end": 2145.96, "text": " think it's going to change the way we work, as you mentioned, the way we work, the way we entertain,", "tokens": [51004, 519, 309, 311, 516, 281, 1319, 264, 636, 321, 589, 11, 382, 291, 2835, 11, 264, 636, 321, 589, 11, 264, 636, 321, 7655, 11, 51228], "temperature": 0.0, "avg_logprob": -0.15597998421147186, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.07691564410924911}, {"id": 337, "seek": 212868, "start": 2145.96, "end": 2147.8799999999997, "text": " so I'm just very exciting future.", "tokens": [51228, 370, 286, 478, 445, 588, 4670, 2027, 13, 51324], "temperature": 0.0, "avg_logprob": -0.15597998421147186, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.07691564410924911}, {"id": 338, "seek": 212868, "start": 2150.68, "end": 2156.7599999999998, "text": " Yeah, anything else? Well, thanks for coming. Try to get you guys as much attention as I can", "tokens": [51464, 865, 11, 1340, 1646, 30, 1042, 11, 3231, 337, 1348, 13, 6526, 281, 483, 291, 1074, 382, 709, 3202, 382, 286, 393, 51768], "temperature": 0.0, "avg_logprob": -0.15597998421147186, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.07691564410924911}, {"id": 339, "seek": 215676, "start": 2156.76, "end": 2161.0800000000004, "text": " bring. You never know, it'll go viral these days, so I think this was a great conversation.", "tokens": [50364, 1565, 13, 509, 1128, 458, 11, 309, 603, 352, 16132, 613, 1708, 11, 370, 286, 519, 341, 390, 257, 869, 3761, 13, 50580], "temperature": 0.0, "avg_logprob": -0.1581100488637949, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.0023196267429739237}, {"id": 340, "seek": 215676, "start": 2161.0800000000004, "end": 2164.6000000000004, "text": " People are really hungry for basic intuitions in the area, so this is good.", "tokens": [50580, 3432, 366, 534, 8067, 337, 3875, 16224, 626, 294, 264, 1859, 11, 370, 341, 307, 665, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1581100488637949, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.0023196267429739237}, {"id": 341, "seek": 215676, "start": 2166.6800000000003, "end": 2168.92, "text": " Yeah, thank you, Nathan. It was a pleasure.", "tokens": [50860, 865, 11, 1309, 291, 11, 20634, 13, 467, 390, 257, 6834, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1581100488637949, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.0023196267429739237}, {"id": 342, "seek": 215676, "start": 2170.36, "end": 2178.28, "text": " Absolutely. Thank you for inviting us, and maybe if there are more questions,", "tokens": [51044, 7021, 13, 1044, 291, 337, 18202, 505, 11, 293, 1310, 498, 456, 366, 544, 1651, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1581100488637949, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.0023196267429739237}, {"id": 343, "seek": 217828, "start": 2179.1600000000003, "end": 2187.32, "text": " is there a way to collect them to provide readers with listeners with an address or", "tokens": [50408, 307, 456, 257, 636, 281, 2500, 552, 281, 2893, 17147, 365, 23274, 365, 364, 2985, 420, 50816], "temperature": 0.0, "avg_logprob": -0.16407431088961089, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.09257794171571732}, {"id": 344, "seek": 217828, "start": 2187.32, "end": 2192.92, "text": " something, happy to answer anything? Yeah, I'll include contact info in the post in", "tokens": [50816, 746, 11, 2055, 281, 1867, 1340, 30, 865, 11, 286, 603, 4090, 3385, 13614, 294, 264, 2183, 294, 51096], "temperature": 0.0, "avg_logprob": -0.16407431088961089, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.09257794171571732}, {"id": 345, "seek": 217828, "start": 2193.8, "end": 2199.2400000000002, "text": " various ways. This will be out there. You'll get your comments on Substack, YouTube, Twitter. It's", "tokens": [51140, 3683, 2098, 13, 639, 486, 312, 484, 456, 13, 509, 603, 483, 428, 3053, 322, 8511, 372, 501, 11, 3088, 11, 5794, 13, 467, 311, 51412], "temperature": 0.0, "avg_logprob": -0.16407431088961089, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.09257794171571732}, {"id": 346, "seek": 217828, "start": 2199.2400000000002, "end": 2203.7200000000003, "text": " a mess. You've got to pay attention to 10 million streams of information these days, but you'll get", "tokens": [51412, 257, 2082, 13, 509, 600, 658, 281, 1689, 3202, 281, 1266, 2459, 15842, 295, 1589, 613, 1708, 11, 457, 291, 603, 483, 51636], "temperature": 0.0, "avg_logprob": -0.16407431088961089, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.09257794171571732}, {"id": 347, "seek": 220372, "start": 2203.72, "end": 2210.68, "text": " contacted by people. Thankfully, for some reason, people read my stuff, but here we are. So thanks for listening.", "tokens": [50364, 21546, 538, 561, 13, 28344, 11, 337, 512, 1778, 11, 561, 1401, 452, 1507, 11, 457, 510, 321, 366, 13, 407, 3231, 337, 4764, 13, 50712], "temperature": 0.0, "avg_logprob": -0.24210023880004883, "compression_ratio": 1.1770833333333333, "no_speech_prob": 0.11749746650457382}], "language": "en"}