1
00:00:00,000 --> 00:00:06,800
Okay. Hey, everyone. Welcome to the first interview that we're going to post on interconnects.

2
00:00:06,800 --> 00:00:12,880
I'm really trying to bring more scientific voices into the AI discourse as media is covering

3
00:00:12,880 --> 00:00:18,360
a lot these days. I'm happy to be here with Michael Poli and Tree Dow, experts in some

4
00:00:18,360 --> 00:00:22,720
of these non-attention architectures that have been really blowing up in the last few

5
00:00:22,720 --> 00:00:27,720
weeks of December. So, Michael, do you want to introduce yourself first?

6
00:00:28,680 --> 00:00:38,040
Sure. Thank you. Thank you, Nathan, for inviting me. I do research at Together AI and was also

7
00:00:38,040 --> 00:00:45,720
a PhD student at Stanford working with Stefan Orenmann and Chris Rea. That's how I met Tree as well.

8
00:00:47,640 --> 00:00:52,920
If I had to choose, maybe I moved to a few different areas of research. If I had to choose

9
00:00:52,920 --> 00:00:59,480
one, I like to research at the intersection of signal processing, dynamical systems,

10
00:00:59,480 --> 00:01:05,960
and deep learning. Most recently, luckily, there's been more interest in efficient

11
00:01:05,960 --> 00:01:11,960
architectures that use some of these principles to improve scaling along different axes

12
00:01:11,960 --> 00:01:15,480
and to get new trade-offs at inference time.

13
00:01:15,960 --> 00:01:19,240
Great. And Tree?

14
00:01:21,080 --> 00:01:28,920
Hi, everyone. Thanks, Nathan, for hosting us. I'm really excited to be here. I'm Tree. I

15
00:01:30,840 --> 00:01:36,440
finished my PhD at Stanford. I'm an incoming assistant professor at Princeton. Right now,

16
00:01:36,440 --> 00:01:41,720
I'm Chief Scientist at Together AI. This is a startup working on AI infrastructure.

17
00:01:42,600 --> 00:01:47,960
And I've been working at the intersection of machine learning and systems, so designing

18
00:01:47,960 --> 00:01:55,400
algorithms that take advantage of the hardware that they run on. We're interested in long-range

19
00:01:56,120 --> 00:01:59,480
dependencies, how to encode that into model, designing architectures that can

20
00:02:00,840 --> 00:02:04,280
learn long-range dependencies. Yeah, really excited to be here.

21
00:02:05,880 --> 00:02:10,040
Yeah. Okay. I think I'm going to just, I have some questions to dive right into this. I think

22
00:02:10,840 --> 00:02:15,000
you two will kind of both answer them or someone can answer longer, whatever you want. I think to

23
00:02:15,000 --> 00:02:20,280
start with, we should talk about maybe why attention works and what the limitations of

24
00:02:20,280 --> 00:02:25,960
attention are. I think almost every person in tech, broadly, now knows that a transformer

25
00:02:25,960 --> 00:02:32,920
is a model built with attention. And ChatGPT does that, but why is this so good? Even how

26
00:02:32,920 --> 00:02:37,720
much of a transformer is built with attention? Are there other things going on? And what might

27
00:02:37,720 --> 00:02:46,840
be challenges there? Right. So, you know, a transformer, which is this architecture that

28
00:02:46,840 --> 00:02:50,200
powers most of the exciting applications that we're seeing nowadays, as you mentioned,

29
00:02:50,200 --> 00:02:57,800
ChatGPT and so on. Attention is kind of the core layer there. And attention actually

30
00:02:58,600 --> 00:03:04,600
became earlier around 2014, 2015, and then transformer came out, incorporating that focusing

31
00:03:04,600 --> 00:03:15,880
a lot on attention with these MLP, interleaving MLP and attention. And I think the success largely

32
00:03:15,880 --> 00:03:24,200
has been, they seem to be able to scale really well. So you can scale out the models with more

33
00:03:24,200 --> 00:03:29,880
parameters, with more data. And that has been the recipe for success. It sounds obvious now, but

34
00:03:29,960 --> 00:03:39,480
I think five years ago, that wasn't clear. So it seems like transformer architecture is a hugely

35
00:03:39,480 --> 00:03:48,520
successful one. And a couple of reasons why it's successful. I think it's general enough that

36
00:03:48,520 --> 00:03:55,560
it's able to learn a lot from data. And two, it's pretty friendly to hardware. There's no

37
00:03:55,560 --> 00:04:02,920
kind of sequential dependency like previous RNNs. So as a result, you can run it well on GPUs,

38
00:04:02,920 --> 00:04:09,400
TPUs. You can scale it up. It's very hardware efficient. I personally have worked on making it

39
00:04:09,400 --> 00:04:13,800
more hardware efficient as well. So it's just kind of the recipe for success, which is

40
00:04:15,880 --> 00:04:22,200
general architecture that scales well. If you're an NLP person, maybe you'll say to incorporate

41
00:04:22,200 --> 00:04:27,240
some kind of inductive bias for it to protect. Personally, I think it's a very general architecture

42
00:04:27,240 --> 00:04:34,680
that scales well and it's hardware friendly. Yeah, it's remarkable how it seems so obvious now.

43
00:04:34,680 --> 00:04:41,000
And it's like, I think one of the things that studying this work is the context length becomes

44
00:04:41,000 --> 00:04:48,920
a really interesting access to study alternatives to it. And essentially, Michael, do you want to

45
00:04:48,920 --> 00:04:58,120
talk about that? I could babble, but you know more. Sure. Yeah, there are several points. I'll

46
00:04:58,120 --> 00:05:04,840
start by saying that there's still great research trying to understand why, from first principles,

47
00:05:04,840 --> 00:05:11,640
why is it that the transformer can learn these interesting circuits. People can study. They

48
00:05:11,640 --> 00:05:17,240
pick apart the computational combination of different heads and transformers and so on.

49
00:05:18,200 --> 00:05:23,720
So there's work on basically understanding transformers from programming language that is

50
00:05:23,720 --> 00:05:33,480
encoded. But I think, as Trey mentioned, there are some very interesting design choices that

51
00:05:33,480 --> 00:05:37,800
have gone into the transformer. This interleaving of attention in NLP is quite important.

52
00:05:39,320 --> 00:05:43,960
And also, the transformer is essentially successful in the beginning because it encoded

53
00:05:44,040 --> 00:05:52,920
these techniques that have been developed for RNN, Celestium, so these other classical NLP models

54
00:05:52,920 --> 00:06:01,000
gating to modulate which information is absorbed into the model, gating to determine how quickly

55
00:06:01,000 --> 00:06:09,000
something is forgotten in this recurrence of the RNN into a parallel form. It is easily a bunch

56
00:06:09,000 --> 00:06:14,040
of jumps that can be easily, but not very easily, but can be optimized for RNN GPUs.

57
00:06:16,520 --> 00:06:21,560
Yeah, this stuff's all great. I guess the specific thing that I had in mind is how

58
00:06:21,560 --> 00:06:27,960
attention ends up being this kind of quadratic scaling in terms of cost when you have an input

59
00:06:27,960 --> 00:06:31,640
sequence that you have. If you have an input sequence of length L and you want to output a

60
00:06:31,640 --> 00:06:36,760
sequence of length L, essentially, if you zoom into the math and you look at what's happening

61
00:06:36,920 --> 00:06:40,840
in inference in most of these libraries, you have this upper triangular attention matrix where you

62
00:06:40,840 --> 00:06:46,440
say you can only look at the past entries of your text, and as you go through there then,

63
00:06:46,440 --> 00:06:52,200
you end up getting this L squared relationship where the first token you can only look at one,

64
00:06:52,200 --> 00:06:58,600
and then you end up looking at more tokens for each pass. And now we've been talking about

65
00:06:58,600 --> 00:07:05,720
recurrent neural networks, and how does something that isn't attention get around the fact that you

66
00:07:05,720 --> 00:07:11,480
want to look at all of the history of the text in your sequence? So if you write a long prompt to

67
00:07:11,480 --> 00:07:17,480
chat GPT, you really want all that information to be encoded, and how can doing something other than

68
00:07:17,480 --> 00:07:29,400
this dense attention matrix actually make that possible? Yeah. Right. Yeah, so you can go ahead.

69
00:07:29,400 --> 00:07:34,280
Before attention, there was RNNs, right? And then RNNs, they processed text, which is fine.

70
00:07:35,640 --> 00:07:40,600
And maybe they didn't scale as well, but yeah. Can you say briefly what a...

71
00:07:40,600 --> 00:07:44,760
They processed text by encoding text. Can you say briefly what a RNN is and how it works?

72
00:07:47,080 --> 00:07:52,280
Yeah, so these are recurrent neural nets that go back, I think, to the 80s. Maybe some of the

73
00:07:52,280 --> 00:08:02,840
more famous ones are LSDMs, GRU. So they were pretty popular around 2012 to 2016 or so.

74
00:08:03,800 --> 00:08:06,760
They were kind of state-of-the-art for translation, speech recognition,

75
00:08:08,840 --> 00:08:16,280
I think NLP, they were a state-of-the-art, and they processed text kind of sequentially.

76
00:08:17,080 --> 00:08:26,440
They see essentially one token, and then that changes their hidden state, and then they will

77
00:08:26,440 --> 00:08:33,960
update the hidden state, and every time they see a new token. So I think it's kind of, in some sense,

78
00:08:33,960 --> 00:08:41,880
mimicking how, for example, human brain process information, like you read the sentence or a

79
00:08:41,880 --> 00:08:47,880
passage, and maybe it's like you're storing some information in your brain, and by the time you

80
00:08:47,880 --> 00:08:53,320
finish reading the document, maybe you can answer questions about the documents without having to

81
00:08:54,280 --> 00:09:00,760
refer to that document again. So RNNs kind of work that way. They process the text,

82
00:09:01,560 --> 00:09:04,520
and then that changes the hidden state, and their hidden state is the representation that

83
00:09:04,520 --> 00:09:10,760
can be used to either generate new tokens or classify the documents or whatnot.

84
00:09:12,360 --> 00:09:22,120
These work well back in 2016 or so, but they've kind of fallen out of favor, empirically, they don't

85
00:09:22,120 --> 00:09:27,320
do as well as Transformer, I think, and as you touch on Transformer, because of this kind of

86
00:09:27,320 --> 00:09:32,680
quadratic scaling, you compare every token with every other token that comes before it,

87
00:09:33,400 --> 00:09:43,960
it gives you this very kind of easy way to propagate information. And I think that's for a

88
00:09:43,960 --> 00:09:52,200
reason why Transformer and attention does really well. But there's been more recently some of the

89
00:09:52,200 --> 00:10:01,080
new RNN architectures that seem to do pretty well. So RWKV is, I think, one of the earlier ones,

90
00:10:02,040 --> 00:10:10,360
you know, it's one, I really admire that project, you know, it's effort mostly from one person,

91
00:10:10,360 --> 00:10:16,840
really going against the orthodoxy of Transformer, showing that RNNs can be pretty strong.

92
00:10:17,880 --> 00:10:20,360
Who was the lead on that? I think it's this person,

93
00:10:23,720 --> 00:10:29,960
Bo Peng, I think, and you know, it's an entire project, but I think it's pioneered by Bo Peng.

94
00:10:30,040 --> 00:10:36,840
I think it's affiliated with Eluta AI and the Compute Sponsor by Stability and so on.

95
00:10:36,840 --> 00:10:43,000
Yeah, I was reading this earlier. At a technical level, they try to replicate something like the

96
00:10:43,000 --> 00:10:51,800
very key value lookup of attention with two linear RNNs to essentially be able to remove

97
00:10:51,800 --> 00:10:58,760
the specific attention scaling potential problems and with two RNNs, which have this

98
00:10:58,760 --> 00:11:01,880
better, like, long context behavior and different implementation rules.

99
00:11:02,440 --> 00:11:06,600
I think, and they also, the paper, trained up to 14 billion parameters, which kind of leads

100
00:11:06,600 --> 00:11:12,040
into some of the next questions. I was going to ask, I was going to ask Tree about Mamba and then

101
00:11:12,040 --> 00:11:17,000
Michael about Stripe Tahina. I think you could go in either order. I think these came out about

102
00:11:17,000 --> 00:11:23,400
a week apart and were these two language models kind of seen as being way closer than anyone

103
00:11:23,400 --> 00:11:28,680
would expect. Essentially, Stripe Tahina came out and the evaluations were close to models

104
00:11:28,680 --> 00:11:34,520
I've been training on all year, like Lama 2 and Mistral 7B, and I went in and I went to the

105
00:11:35,160 --> 00:11:41,240
Together API and I did like side by side of Mistral versus Stripe Tahina and it's like,

106
00:11:41,240 --> 00:11:45,080
it's a good language model. It answers most questions. There's no obvious failure modes.

107
00:11:45,640 --> 00:11:49,800
I think maybe Michael, do you want to comment on that? I know it's another big project and then

108
00:11:49,800 --> 00:11:53,560
we can go back to Mamba, even though it's slightly out of order in the chronological

109
00:11:54,200 --> 00:12:03,160
the release cycle that happened. Sure. I guess I'll start by saying that

110
00:12:04,280 --> 00:12:10,840
there's an interesting connection between all these new methods. There's this sort of convex set,

111
00:12:11,800 --> 00:12:16,280
which has a center and there's this connection between linear attention,

112
00:12:17,000 --> 00:12:23,480
so attention without the softmax, linear RNNs, and state space models, SSM.

113
00:12:24,280 --> 00:12:29,160
So at some level, the mathematical formulation of this kind of base model here,

114
00:12:30,120 --> 00:12:34,600
I'm not talking about the base architecture, just the fundamental model, is the same.

115
00:12:35,400 --> 00:12:39,960
And then you can go in different directions and each direction has its own trade-offs.

116
00:12:40,840 --> 00:12:49,640
You can go to the feature map direction, the kernel direction. So when you break down the

117
00:12:49,640 --> 00:12:57,240
softmax, you take away the softmax, you can place on queries and keys, kind of the fundamental

118
00:12:57,240 --> 00:13:02,120
the entities that compose your attention matrix, you can compose other kernel-like functions,

119
00:13:02,680 --> 00:13:07,640
other functions that you hope would approximate whatever capability of attention you like.

120
00:13:07,640 --> 00:13:11,240
You can do things like a Taylor approximation, Taylor expansion, for example.

121
00:13:12,920 --> 00:13:18,040
And you have a slightly different perspective, but you get something that, again, is very similar.

122
00:13:18,600 --> 00:13:24,600
You can go to time variance, so you take the RNN and you push this input dependence,

123
00:13:24,600 --> 00:13:32,520
so the way the computation inside the linear RNN is conditioned by the input sequence,

124
00:13:32,520 --> 00:13:36,280
and you can add things like gates. We've seen a lot of work, for example,

125
00:13:36,280 --> 00:13:41,880
modernizing linear attention with additional gates that allow you to make better use of your

126
00:13:41,880 --> 00:13:48,280
fixed state dimension. And then you have the third direction, at least in my mind,

127
00:13:48,280 --> 00:13:53,960
is the one that pushes, that uses the convolutional form, that pushes more towards other types of

128
00:13:53,960 --> 00:14:00,520
linear operators that are still associative, that are still allow you to train in parallel.

129
00:14:00,520 --> 00:14:05,560
So here are things, time invariant systems, I can elaborate on any of these points,

130
00:14:05,560 --> 00:14:08,920
but things that can switch between convolutions and recurrence, like this form model,

131
00:14:09,640 --> 00:14:19,640
with additional gates, again. Striped IEna was born as a project from the IEna architecture,

132
00:14:19,640 --> 00:14:25,480
which belongs to this third category that I just mentioned, and we're really trying to get the best

133
00:14:25,480 --> 00:14:34,360
per-flop architecture that we could. And one principle that was validated over and over again,

134
00:14:35,000 --> 00:14:41,320
and we're trying to understand better now, is that it seems composing, hybridizing different

135
00:14:42,920 --> 00:14:49,480
layers, different blocks of different categories, and even full attention yields something that

136
00:14:49,480 --> 00:14:54,200
is better than the individual components. So there seems to be a compositional aspect

137
00:14:54,200 --> 00:14:58,760
of these models that we're trying to understand better, and this gives you a better

138
00:14:59,400 --> 00:15:07,400
sort of pre-trained model per-flop. And with this model, we ran a whole suite of skating laws,

139
00:15:07,400 --> 00:15:13,000
and so on. Hybridizing also gives you, since we wanted something that would be kind of usable

140
00:15:13,000 --> 00:15:19,000
out of the box, it gives you a way easier time. When you're fine-tuning for longer contexts,

141
00:15:19,000 --> 00:15:23,320
we can apply some of these techniques that have been developed for transformers, and kind of

142
00:15:23,320 --> 00:15:30,520
surprisingly work for hybrids as well. So things like linear scaling for

143
00:15:30,520 --> 00:15:34,200
rotary embeddings and so on, you can go into the details. So it was mostly a project,

144
00:15:34,200 --> 00:15:37,160
what is the best, given the current landscape, what is the best we can do?

145
00:15:37,880 --> 00:15:41,720
Yeah, that's a great description of it. I mean, the sentence in the blog that's like,

146
00:15:41,720 --> 00:15:46,200
start training is optimized using a set of new model grafting techniques, enabling us to change

147
00:15:46,200 --> 00:15:51,800
the model architecture during training, kind of felt like, to me, that there's a ton going on

148
00:15:51,800 --> 00:15:56,680
there. And some of which you probably can't talk about, there's normal data. I don't think all the

149
00:15:56,680 --> 00:16:00,840
data that was quite explained, like what the longer context data was, but it's like,

150
00:16:00,840 --> 00:16:04,440
are you taking this from models, starting point from models that people would know,

151
00:16:04,440 --> 00:16:10,920
and can you say any of that? I think even just the summary that it's a synthesizing recent work

152
00:16:10,920 --> 00:16:17,080
into a strong model is great context for people. Yeah, well, that line, so we've,

153
00:16:17,800 --> 00:16:25,800
given this explosion of primitives that, you know, described, and given sort of the

154
00:16:28,600 --> 00:16:36,200
cost that it would require to evaluate all different combinations, we found ways to essentially

155
00:16:36,840 --> 00:16:42,600
start training with a configuration and then continuing on with another configuration. I

156
00:16:42,600 --> 00:16:47,320
think we'll have, we're going to have more work with paper. Yeah, there's so much cool work in

157
00:16:47,320 --> 00:16:51,960
that area. So one of the, someone at AI2 is working on a project where they're essentially

158
00:16:51,960 --> 00:16:57,000
trying to cut the Lama models in half and keep training them and things. It's just the wild west

159
00:16:57,000 --> 00:17:02,600
out there with people trying to take strong models and make them smaller while still getting the

160
00:17:02,600 --> 00:17:07,400
performance benefits of bigger models. I think that's a whole aside, but I wasn't expecting it to

161
00:17:07,400 --> 00:17:12,040
show up when people, like you follow the social media, I've striped my, you know, people are like,

162
00:17:12,040 --> 00:17:16,920
oh, non-attention models are finally good. And it's like, it covers up a lot of the details

163
00:17:16,920 --> 00:17:24,920
that are very interesting about it, in my opinion. So, okay, it turned back to tree. I think Mamba

164
00:17:24,920 --> 00:17:31,880
actually happened first among these. I did the, his reading back of social media. And it also was

165
00:17:31,880 --> 00:17:38,440
very surprising to me. I think the largest model in the Mamba suite is 2.8 billion parameters,

166
00:17:38,440 --> 00:17:44,920
if I remember correctly. And it was compared to a lot of the common benchmarks in open NLP. So

167
00:17:44,920 --> 00:17:51,480
things like GPTJ, Pythia model suites and the scores on the benchmarks reported were really

168
00:17:51,480 --> 00:17:57,320
strong. And I think if you want to start with the high level summary, and then I'm definitely going

169
00:17:57,320 --> 00:18:02,200
to make you talk about the awesome new CUDA kernels and stuff that you had to write for this project.

170
00:18:02,760 --> 00:18:09,720
Yeah, so this Mamba is a collaboration with, with Albert Gu, who's now, he was,

171
00:18:11,160 --> 00:18:16,280
he's just doing it at Stanford. That's where we met. And he's now a professor at CMU.

172
00:18:18,120 --> 00:18:23,320
And also at a startup. So it's a wonderful collaboration credit goes to him.

173
00:18:24,840 --> 00:18:28,440
Yeah, Albert has been working on this line of work called state space models.

174
00:18:29,080 --> 00:18:34,040
In some sense, as mentioned, it connects to things like linear tension,

175
00:18:34,040 --> 00:18:44,600
linear RNN, convolution, neural nets. And that's what his PhD thesis is about. I've also worked on

176
00:18:46,040 --> 00:18:52,280
state space for the past couple of projects. My, my angle is how to make state space more

177
00:18:52,280 --> 00:19:00,520
hardware efficient and kind of increase their expressiveness. So it's wonderful working with

178
00:19:01,080 --> 00:19:08,280
Albert. And there I think is more of a proof of concept, which is can state space actually do

179
00:19:08,280 --> 00:19:15,640
as well as transformer on language. So we've seen previous papers showing state space could be better

180
00:19:15,640 --> 00:19:23,720
on audio, could be better on some of the tasks on the long range arena. But language has always been

181
00:19:24,680 --> 00:19:32,120
the most difficult to get to do well for state space models. And language is also kind of the

182
00:19:32,120 --> 00:19:40,200
thing that people care about the most right now. So Mamba was more of a proof of concept, which is

183
00:19:41,160 --> 00:19:46,040
hey, we want to show that state space can be competitive, or maybe even be some of the

184
00:19:46,040 --> 00:19:55,160
transformers out there. So we validated that at the scale up to 3B, trained to 300B tokens.

185
00:19:55,160 --> 00:19:59,800
So in absolute terms, you know, these are not very strong models. These are not yet models that you

186
00:19:59,800 --> 00:20:04,840
would actually play with and expect it to do meaningful things. I think it's more of a,

187
00:20:05,720 --> 00:20:09,880
more of an academic comparison in terms of architecture, it's like, hey,

188
00:20:09,880 --> 00:20:15,400
training trained for the same amount of tokens, it does as well, or maybe slightly better than

189
00:20:15,400 --> 00:20:21,640
some of the transformer out there. And that's, in particular, has been very exciting to us.

190
00:20:21,640 --> 00:20:27,240
I think Albert's been pushing on this for a while. I've been pushing on this for a while. And I think

191
00:20:27,240 --> 00:20:36,040
finally, it seems to finally be kind of close to gap or even surpass some of the transformer.

192
00:20:37,560 --> 00:20:45,000
And it just, I think it opens up a bunch of opportunities. So inference could be way faster.

193
00:20:45,720 --> 00:20:50,600
Maybe we would have different ways to understand how in-contact learning happens, etc.

194
00:20:50,600 --> 00:20:53,720
So lots of, lots of future work, I would expect.

195
00:20:54,760 --> 00:21:00,200
Yeah. Can you go into some of the, like, what does it actually take to implement some of these

196
00:21:00,200 --> 00:21:04,520
new CUDA kernels? I just remember when this paper was announced, Sasha Rush, who's also very active

197
00:21:04,520 --> 00:21:09,880
in this space, I recommended me to talk with you too, was tweeting about the types of files or

198
00:21:09,880 --> 00:21:16,360
whatever. And in the paper, there's this discussion about how like the recurrent state needs to be

199
00:21:16,680 --> 00:21:21,160
sufficiently expressive, but doing so in a certain type of memory is a problem.

200
00:21:23,080 --> 00:21:28,680
Translate what this means to people thinking about GPUs and people thinking about these models

201
00:21:28,680 --> 00:21:34,440
being scaled. Like, is it now much easier to scale these models because they work on GPUs?

202
00:21:34,440 --> 00:21:39,560
Which GPUs were you using? Is there a bump that could come just from going to H100s or something?

203
00:21:39,560 --> 00:21:48,760
Any of that? Yeah. So the line of work on state space,

204
00:21:49,720 --> 00:21:58,680
sort of like S4 models kind of pioneered by Albert's work, they are in some sense

205
00:21:58,680 --> 00:22:07,960
recurrent neural network, but they have a much larger state size. So the state size is whatever

206
00:22:08,680 --> 00:22:14,920
kind of buffer that you're going to store information as you traverse or as you process

207
00:22:15,720 --> 00:22:19,960
the sequence. In some sense, you can view Transformers doing that as well, where it's

208
00:22:19,960 --> 00:22:24,520
keep the entire history. It's usually called the KV cache. It keeps the history and keep

209
00:22:24,520 --> 00:22:32,200
referring to it. For RNNs, they have a fixed size state. For Transformers state, you can think of

210
00:22:32,200 --> 00:22:42,040
the state size as increasing. And our intuition is that the larger the state size, the easier it is

211
00:22:42,040 --> 00:22:47,320
for the model to do well. So basically, you have more space to store whatever you need to remember.

212
00:22:47,320 --> 00:22:52,600
And so previous models like S4 and so on, they have an implicitly pretty large state size,

213
00:22:52,600 --> 00:22:57,240
but they use the convolutional view to avoid having to materialize the state. So that was

214
00:22:57,240 --> 00:23:04,200
wonderful. Michael has worked behind architecture, has used some of the same insight focusing on

215
00:23:04,200 --> 00:23:13,800
convolution. Mamba, on the other hand, focuses on the recurrent view. So we wanted to put more

216
00:23:13,800 --> 00:23:20,600
input dependency in the recurrence. We thought the thinking was that it was going to make it

217
00:23:20,600 --> 00:23:25,400
more expressive and the model would do better. But that prevents us from using this convolutional

218
00:23:25,400 --> 00:23:29,320
view that would make things efficient. So we had to figure out a different way to make things

219
00:23:29,320 --> 00:23:38,840
efficient. And so I focused on making that efficient on GPUs. And so our thinking was,

220
00:23:39,960 --> 00:23:44,520
instead of, okay, we're going to have a large state size, but we don't have to write to actual

221
00:23:44,520 --> 00:23:52,920
GPU memory, like the HBM, where you can just keep that large state in a faster memory called S RAM.

222
00:23:52,920 --> 00:23:59,240
You think of it as a cache. So if you're more familiar with CPUs, you just usually get cache and

223
00:23:59,240 --> 00:24:06,200
RAM. So if you have large state, you can keep it in the cache. And by avoiding having to write it

224
00:24:06,200 --> 00:24:11,640
down, you actually don't suffer too much if the state is large. So that's essentially the core

225
00:24:11,720 --> 00:24:15,480
idea. Would this be due to like input, like having to move the data around being really slow?

226
00:24:16,200 --> 00:24:22,120
Yes. Yes. That makes sense. That's a really common constraint in a lot of these things. And it's like,

227
00:24:23,000 --> 00:24:27,880
I think one of the most insightful things I've had now with GPUs versus TPUs and stuff is how

228
00:24:27,880 --> 00:24:32,600
mixtures of expert models doesn't work very well on TPUs just because you have to like,

229
00:24:33,240 --> 00:24:37,480
that essentially add a mixture of expert at a basic level. There's a routing layer that you

230
00:24:37,480 --> 00:24:41,640
learn and then multiple feedforward layers that you can choose from. And when you're distributing

231
00:24:41,640 --> 00:24:46,600
this, the feedforward layers could end up on a different TPU node and TPUs communicate with their

232
00:24:46,600 --> 00:24:53,000
neighbors. So TPUs take a big hit relative to GPUs where within video and video clusters,

233
00:24:53,000 --> 00:24:57,880
everything's connected so much more. And then it's easy to do that sort of distributed training.

234
00:24:59,320 --> 00:25:03,160
That's super interesting. And it's like, do you think there's going to be,

235
00:25:03,880 --> 00:25:07,960
I guess this is really where I want to open the conversation of like, what is this going to

236
00:25:07,960 --> 00:25:17,160
happen in 2024 in this space? Are bigger players going to move in and be exploring this? My take,

237
00:25:17,160 --> 00:25:22,920
seeing how good the long context learning could be and a fundamental limit is that systems like

238
00:25:22,920 --> 00:25:29,320
ChatGPT are going to use a transformer model for most tasks. And then if you need to do

239
00:25:29,320 --> 00:25:34,600
summarization, you might do a long context specialized architecture. And then we could even

240
00:25:34,600 --> 00:25:39,240
see a whole quiver of architectures behind something that you're using. But I think

241
00:25:39,960 --> 00:25:46,360
it's just like, is attention going to be dethroned? Is Sasha Rush somehow going to win this bet that

242
00:25:46,360 --> 00:25:51,000
everyone wants following in the area? What are you thinking about, either of you?

243
00:25:53,480 --> 00:25:58,760
I think it's a very, very strong architecture. And there's a proven recipe, right? You know,

244
00:25:58,840 --> 00:26:02,120
people scaling to a trillion of parameters. Right now, if you want, you say,

245
00:26:02,760 --> 00:26:08,760
well, I just want the best performing model that runs most efficiently on my hardware,

246
00:26:08,760 --> 00:26:13,320
that has the most support on the software side. The transformer is a safe bet. I think it's here

247
00:26:13,320 --> 00:26:24,120
to stay. But I think there are new ideas like state space, China, so the linear attention,

248
00:26:24,120 --> 00:26:29,000
ideas from linear attention, I think they're coming in. We've seen, as Michael mentioned,

249
00:26:29,000 --> 00:26:34,520
that mixing some of these components seem to improve performance. We're validated at, I think,

250
00:26:34,520 --> 00:26:41,960
seven B scale, but maybe it might even work at larger scale. I think people tend to be

251
00:26:41,960 --> 00:26:47,880
conservative and focusing too much on model architecture might not be worth their time.

252
00:26:47,880 --> 00:26:51,640
Like the LIMAR architecture is very, very strong. Most people are doing off of that. They're

253
00:26:51,720 --> 00:26:58,600
focusing on data. They're focusing on infrastructure, which makes sense. I think on

254
00:26:59,560 --> 00:27:06,360
my side personally, the other stuff is just plain interesting. There are more, I would say niche

255
00:27:06,360 --> 00:27:14,040
use cases, niche for now, where some of these alternative architectures are interesting,

256
00:27:14,040 --> 00:27:17,000
things like long contacts, different domains like audio and genomics.

257
00:27:17,560 --> 00:27:23,880
There's just plain interesting scientific questions you can ask, whether it follow

258
00:27:23,880 --> 00:27:27,720
instruction just as well, whether it's fine to interest as well. Does it play well with

259
00:27:27,720 --> 00:27:32,200
quantization and so on? There's just plain interesting research questions we can ask.

260
00:27:32,200 --> 00:27:38,360
Now on the production level, I think transformer is still incredibly strong, very well supported,

261
00:27:39,640 --> 00:27:45,160
both hardware and software. But I think some of these new ideas are coming in and people might

262
00:27:45,160 --> 00:27:49,480
start putting them as part of component in the transformer. Maybe we'll still call them

263
00:27:49,480 --> 00:27:56,680
transformer, but they'll just have more layers and just attention and LPE.

264
00:27:58,520 --> 00:28:07,000
Yeah, I 100% agree with you. So attention as a computational primitive is not going anywhere

265
00:28:07,720 --> 00:28:13,800
anytime soon. It's just a very efficient and a very convenient way to increase

266
00:28:14,360 --> 00:28:23,560
the effective state of your sequence processor. So at some level, if you're working with a model

267
00:28:23,560 --> 00:28:30,200
that only has a fixed state in each of its sequence mixers, you have an assumption and

268
00:28:30,200 --> 00:28:35,000
your assumption is that you only need so much information in the sequence. So there's always

269
00:28:35,000 --> 00:28:41,240
a tradeoff between the ratio of the state dimension, the sequence length, as you push

270
00:28:41,240 --> 00:28:46,760
things to the extreme, either model sizes, so as you make the model bigger, wider, effectively

271
00:28:46,760 --> 00:28:56,600
introduce more states and sequence length. Some of these margins, some of this is speculation,

272
00:28:56,600 --> 00:29:02,840
but some of these margins will disappear, some of the tradeoffs will change, especially 14, 30,

273
00:29:02,920 --> 00:29:11,960
some of these very fat models. But certainly, either whether that's hybridizing or some kind

274
00:29:11,960 --> 00:29:17,880
of new block, we're certainly going to see some more innovation. That's really exciting.

275
00:29:17,880 --> 00:29:22,840
My personal, if I had to make a prediction is that architecture design will get more

276
00:29:23,560 --> 00:29:26,920
interesting or more complex. There's going to be more to do.

277
00:29:27,800 --> 00:29:34,840
Yeah, I mean, this year, it's like, I got some 10 minute clock that's fine for us. I think with

278
00:29:34,840 --> 00:29:43,000
mixture of experts and this being popular as a state-state model, this is all just really within

279
00:29:43,000 --> 00:29:46,600
a few months outside of OpenAI. They've been doing mixture of experts for a lot longer than

280
00:29:46,600 --> 00:29:52,200
everyone, but in terms of open and academic communities, no one's really tried to do RLHF

281
00:29:52,200 --> 00:29:55,560
on mixture of experts. It should just work, but we have to learn all these things. And then the

282
00:29:55,560 --> 00:30:02,360
model grafting is becoming more of a real thing. That's super interesting. And it's just, I agree

283
00:30:02,360 --> 00:30:09,480
that it's just fun to follow. And hopefully, it gives academics and scientists more ways to

284
00:30:09,480 --> 00:30:14,200
influence the conversation where in industry, it's just about scaling and bigger models where we

285
00:30:14,200 --> 00:30:18,840
could maybe do specific things better, which I'm telling open source companies to do with

286
00:30:18,840 --> 00:30:22,120
their language models anyways, like if they want to have a business model, they need to have an edge.

287
00:30:22,680 --> 00:30:27,080
So this all fits into that kind of narrative pretty well with my regards.

288
00:30:28,440 --> 00:30:32,120
Is there anything else you guys are following in ML? It doesn't have to be about state-space

289
00:30:32,120 --> 00:30:35,080
models. What's exciting for you broadly for next year?

290
00:30:38,120 --> 00:30:46,920
Yeah, personally, I think data is still the most important thing. We're thinking a lot about how

291
00:30:46,920 --> 00:30:53,560
data influences the model performance, like really teasing that out, either having some

292
00:30:53,560 --> 00:30:59,560
of the synthetic tasks that correlates very well with model performance has been kind of the

293
00:30:59,560 --> 00:31:07,080
motivating examples in a lot of our papers and work has been focusing on synthetic tasks,

294
00:31:08,040 --> 00:31:17,400
or having maybe smaller data sets that kind of make it easier to really understand what's

295
00:31:17,400 --> 00:31:27,320
really going on. So I think personally, my focus is going to be on data for the next little bit.

296
00:31:28,600 --> 00:31:35,720
All the architecture stuff is fun. Making that hardware efficient is fun, but I think

297
00:31:37,080 --> 00:31:43,960
ultimately it's about data. If you look at the scaling law curve, the different

298
00:31:43,960 --> 00:31:47,160
monarchitectures generally have the same slope. They're just different offset.

299
00:31:48,360 --> 00:31:52,520
Seems like the only thing that changes the slope is the data quality.

300
00:31:53,640 --> 00:31:58,840
I love that point. That does seem true. I have the plot from Mamba in this blog post that I'm

301
00:31:58,840 --> 00:32:02,920
writing, which is just a little bit above. Same slope.

302
00:32:07,560 --> 00:32:14,440
Yeah, maybe add data. Data is really interesting. So miniaturizing architecture design, finding,

303
00:32:15,080 --> 00:32:20,920
breaking down what tasks are involved into, for example, language modeling and trying to

304
00:32:22,200 --> 00:32:26,120
package them into something that can be used to iterate, something that's quite exciting.

305
00:32:26,440 --> 00:32:38,040
That was one of the main techniques that was used for this zoology paper that also looks into

306
00:32:38,040 --> 00:32:42,280
some of these different behaviors. Personally, I'm also really excited about new applications,

307
00:32:44,200 --> 00:32:50,600
scientific applications, the genomics work, but even more engineering-focused.

308
00:32:51,000 --> 00:33:00,280
We're seeing a shift. Right now, language is still the domain that gets most clicks,

309
00:33:00,280 --> 00:33:02,280
most interest, but I think that will evolve over time.

310
00:33:04,360 --> 00:33:08,680
Some of these other applications offer, even just talking about architectures,

311
00:33:08,680 --> 00:33:13,320
they offer completely different design space. I'm excited to look into it.

312
00:33:14,040 --> 00:33:19,240
Yeah, everyone talks about language, but I feel like images and entertainment and videos are the

313
00:33:19,320 --> 00:33:23,560
things that are so obviously going to generate so much value to me. I don't know the ceiling on

314
00:33:23,560 --> 00:33:29,240
language, but when you could access a somewhat local text and video model at your homework

315
00:33:29,240 --> 00:33:34,440
station, that's tailored to your preferences. The amount of value that that creates is totally

316
00:33:35,080 --> 00:33:41,240
astronomical. I'm excited. I started playing around with these where I'd take text of the

317
00:33:41,240 --> 00:33:47,000
blog and convert it to dolly images and convert it to a video with generated audio all with one

318
00:33:47,000 --> 00:33:53,800
Python script. That's really easy to do, so I agree with your more than language. It's fun to

319
00:33:53,800 --> 00:34:00,360
have that view. These things actually do work really well in your experience when you stitch

320
00:34:00,360 --> 00:34:07,160
all of them together. It's not that good. The dolly images are pretty similar, but I'm doing

321
00:34:07,160 --> 00:34:12,360
something really naive where I literally take the text and have a system prompt. It's like,

322
00:34:12,360 --> 00:34:17,880
you're generating a series of images for visualizing a blog post, and it generates various

323
00:34:18,840 --> 00:34:22,680
all the machine learning thumbnails that you see everyone using. There are variations of that.

324
00:34:22,680 --> 00:34:26,520
The fun ones are whether it's about llama or mamba or something, and then they generate

325
00:34:26,520 --> 00:34:32,840
animals in them, which is good. I think I could get much better at it and have a better segmentation

326
00:34:32,840 --> 00:34:37,960
system for the paragraphs and or have chat to BT summarize them or something like that. But I

327
00:34:37,960 --> 00:34:42,600
just know that within a year, it's going to be a text-to-video API, and I'm just going to switch

328
00:34:42,600 --> 00:34:47,400
it, and it's going to be great. I'm laying the groundwork for infrastructure to have a

329
00:34:47,400 --> 00:34:55,800
multimodal content distribution, really, and I just expect it to become very fun. Even the text

330
00:34:55,800 --> 00:35:01,480
of voice is pretty good, I think. I don't have a studio, but once you have a studio, it's going

331
00:35:01,480 --> 00:35:06,520
to be able to generate perfect audio for whatever you want. Another one of my dreams that is bad

332
00:35:06,520 --> 00:35:13,640
for young students is I want to be able to give a slide deck to a script that returns the five-minute

333
00:35:13,640 --> 00:35:23,160
conference video that no one ever watched, just based on a GPT-4 reading the slide deck and voicing

334
00:35:23,160 --> 00:35:28,680
yourself. Those are the silly things that I have time to do because I'm not a professor.

335
00:35:28,680 --> 00:35:36,760
Yeah, I think these advances, these systems, they do generate a lot of economic value,

336
00:35:36,760 --> 00:35:41,480
and we're seeing that already. Lots of companies are now switching to using these things, and I

337
00:35:41,480 --> 00:35:45,960
think it's going to change the way we work, as you mentioned, the way we work, the way we entertain,

338
00:35:45,960 --> 00:35:47,880
so I'm just very exciting future.

339
00:35:50,680 --> 00:35:56,760
Yeah, anything else? Well, thanks for coming. Try to get you guys as much attention as I can

340
00:35:56,760 --> 00:36:01,080
bring. You never know, it'll go viral these days, so I think this was a great conversation.

341
00:36:01,080 --> 00:36:04,600
People are really hungry for basic intuitions in the area, so this is good.

342
00:36:06,680 --> 00:36:08,920
Yeah, thank you, Nathan. It was a pleasure.

343
00:36:10,360 --> 00:36:18,280
Absolutely. Thank you for inviting us, and maybe if there are more questions,

344
00:36:19,160 --> 00:36:27,320
is there a way to collect them to provide readers with listeners with an address or

345
00:36:27,320 --> 00:36:32,920
something, happy to answer anything? Yeah, I'll include contact info in the post in

346
00:36:33,800 --> 00:36:39,240
various ways. This will be out there. You'll get your comments on Substack, YouTube, Twitter. It's

347
00:36:39,240 --> 00:36:43,720
a mess. You've got to pay attention to 10 million streams of information these days, but you'll get

348
00:36:43,720 --> 00:36:50,680
contacted by people. Thankfully, for some reason, people read my stuff, but here we are. So thanks for listening.

