start	end	text
0	6800	Okay. Hey, everyone. Welcome to the first interview that we're going to post on interconnects.
6800	12880	I'm really trying to bring more scientific voices into the AI discourse as media is covering
12880	18360	a lot these days. I'm happy to be here with Michael Poli and Tree Dow, experts in some
18360	22720	of these non-attention architectures that have been really blowing up in the last few
22720	27720	weeks of December. So, Michael, do you want to introduce yourself first?
28680	38040	Sure. Thank you. Thank you, Nathan, for inviting me. I do research at Together AI and was also
38040	45720	a PhD student at Stanford working with Stefan Orenmann and Chris Rea. That's how I met Tree as well.
47640	52920	If I had to choose, maybe I moved to a few different areas of research. If I had to choose
52920	59480	one, I like to research at the intersection of signal processing, dynamical systems,
59480	65960	and deep learning. Most recently, luckily, there's been more interest in efficient
65960	71960	architectures that use some of these principles to improve scaling along different axes
71960	75480	and to get new trade-offs at inference time.
75960	79240	Great. And Tree?
81080	88920	Hi, everyone. Thanks, Nathan, for hosting us. I'm really excited to be here. I'm Tree. I
90840	96440	finished my PhD at Stanford. I'm an incoming assistant professor at Princeton. Right now,
96440	101720	I'm Chief Scientist at Together AI. This is a startup working on AI infrastructure.
102600	107960	And I've been working at the intersection of machine learning and systems, so designing
107960	115400	algorithms that take advantage of the hardware that they run on. We're interested in long-range
116120	119480	dependencies, how to encode that into model, designing architectures that can
120840	124280	learn long-range dependencies. Yeah, really excited to be here.
125880	130040	Yeah. Okay. I think I'm going to just, I have some questions to dive right into this. I think
130840	135000	you two will kind of both answer them or someone can answer longer, whatever you want. I think to
135000	140280	start with, we should talk about maybe why attention works and what the limitations of
140280	145960	attention are. I think almost every person in tech, broadly, now knows that a transformer
145960	152920	is a model built with attention. And ChatGPT does that, but why is this so good? Even how
152920	157720	much of a transformer is built with attention? Are there other things going on? And what might
157720	166840	be challenges there? Right. So, you know, a transformer, which is this architecture that
166840	170200	powers most of the exciting applications that we're seeing nowadays, as you mentioned,
170200	177800	ChatGPT and so on. Attention is kind of the core layer there. And attention actually
178600	184600	became earlier around 2014, 2015, and then transformer came out, incorporating that focusing
184600	195880	a lot on attention with these MLP, interleaving MLP and attention. And I think the success largely
195880	204200	has been, they seem to be able to scale really well. So you can scale out the models with more
204200	209880	parameters, with more data. And that has been the recipe for success. It sounds obvious now, but
209960	219480	I think five years ago, that wasn't clear. So it seems like transformer architecture is a hugely
219480	228520	successful one. And a couple of reasons why it's successful. I think it's general enough that
228520	235560	it's able to learn a lot from data. And two, it's pretty friendly to hardware. There's no
235560	242920	kind of sequential dependency like previous RNNs. So as a result, you can run it well on GPUs,
242920	249400	TPUs. You can scale it up. It's very hardware efficient. I personally have worked on making it
249400	253800	more hardware efficient as well. So it's just kind of the recipe for success, which is
255880	262200	general architecture that scales well. If you're an NLP person, maybe you'll say to incorporate
262200	267240	some kind of inductive bias for it to protect. Personally, I think it's a very general architecture
267240	274680	that scales well and it's hardware friendly. Yeah, it's remarkable how it seems so obvious now.
274680	281000	And it's like, I think one of the things that studying this work is the context length becomes
281000	288920	a really interesting access to study alternatives to it. And essentially, Michael, do you want to
288920	298120	talk about that? I could babble, but you know more. Sure. Yeah, there are several points. I'll
298120	304840	start by saying that there's still great research trying to understand why, from first principles,
304840	311640	why is it that the transformer can learn these interesting circuits. People can study. They
311640	317240	pick apart the computational combination of different heads and transformers and so on.
318200	323720	So there's work on basically understanding transformers from programming language that is
323720	333480	encoded. But I think, as Trey mentioned, there are some very interesting design choices that
333480	337800	have gone into the transformer. This interleaving of attention in NLP is quite important.
339320	343960	And also, the transformer is essentially successful in the beginning because it encoded
344040	352920	these techniques that have been developed for RNN, Celestium, so these other classical NLP models
352920	361000	gating to modulate which information is absorbed into the model, gating to determine how quickly
361000	369000	something is forgotten in this recurrence of the RNN into a parallel form. It is easily a bunch
369000	374040	of jumps that can be easily, but not very easily, but can be optimized for RNN GPUs.
376520	381560	Yeah, this stuff's all great. I guess the specific thing that I had in mind is how
381560	387960	attention ends up being this kind of quadratic scaling in terms of cost when you have an input
387960	391640	sequence that you have. If you have an input sequence of length L and you want to output a
391640	396760	sequence of length L, essentially, if you zoom into the math and you look at what's happening
396920	400840	in inference in most of these libraries, you have this upper triangular attention matrix where you
400840	406440	say you can only look at the past entries of your text, and as you go through there then,
406440	412200	you end up getting this L squared relationship where the first token you can only look at one,
412200	418600	and then you end up looking at more tokens for each pass. And now we've been talking about
418600	425720	recurrent neural networks, and how does something that isn't attention get around the fact that you
425720	431480	want to look at all of the history of the text in your sequence? So if you write a long prompt to
431480	437480	chat GPT, you really want all that information to be encoded, and how can doing something other than
437480	449400	this dense attention matrix actually make that possible? Yeah. Right. Yeah, so you can go ahead.
449400	454280	Before attention, there was RNNs, right? And then RNNs, they processed text, which is fine.
455640	460600	And maybe they didn't scale as well, but yeah. Can you say briefly what a...
460600	464760	They processed text by encoding text. Can you say briefly what a RNN is and how it works?
467080	472280	Yeah, so these are recurrent neural nets that go back, I think, to the 80s. Maybe some of the
472280	482840	more famous ones are LSDMs, GRU. So they were pretty popular around 2012 to 2016 or so.
483800	486760	They were kind of state-of-the-art for translation, speech recognition,
488840	496280	I think NLP, they were a state-of-the-art, and they processed text kind of sequentially.
497080	506440	They see essentially one token, and then that changes their hidden state, and then they will
506440	513960	update the hidden state, and every time they see a new token. So I think it's kind of, in some sense,
513960	521880	mimicking how, for example, human brain process information, like you read the sentence or a
521880	527880	passage, and maybe it's like you're storing some information in your brain, and by the time you
527880	533320	finish reading the document, maybe you can answer questions about the documents without having to
534280	540760	refer to that document again. So RNNs kind of work that way. They process the text,
541560	544520	and then that changes the hidden state, and their hidden state is the representation that
544520	550760	can be used to either generate new tokens or classify the documents or whatnot.
552360	562120	These work well back in 2016 or so, but they've kind of fallen out of favor, empirically, they don't
562120	567320	do as well as Transformer, I think, and as you touch on Transformer, because of this kind of
567320	572680	quadratic scaling, you compare every token with every other token that comes before it,
573400	583960	it gives you this very kind of easy way to propagate information. And I think that's for a
583960	592200	reason why Transformer and attention does really well. But there's been more recently some of the
592200	601080	new RNN architectures that seem to do pretty well. So RWKV is, I think, one of the earlier ones,
602040	610360	you know, it's one, I really admire that project, you know, it's effort mostly from one person,
610360	616840	really going against the orthodoxy of Transformer, showing that RNNs can be pretty strong.
617880	620360	Who was the lead on that? I think it's this person,
623720	629960	Bo Peng, I think, and you know, it's an entire project, but I think it's pioneered by Bo Peng.
630040	636840	I think it's affiliated with Eluta AI and the Compute Sponsor by Stability and so on.
636840	643000	Yeah, I was reading this earlier. At a technical level, they try to replicate something like the
643000	651800	very key value lookup of attention with two linear RNNs to essentially be able to remove
651800	658760	the specific attention scaling potential problems and with two RNNs, which have this
658760	661880	better, like, long context behavior and different implementation rules.
662440	666600	I think, and they also, the paper, trained up to 14 billion parameters, which kind of leads
666600	672040	into some of the next questions. I was going to ask, I was going to ask Tree about Mamba and then
672040	677000	Michael about Stripe Tahina. I think you could go in either order. I think these came out about
677000	683400	a week apart and were these two language models kind of seen as being way closer than anyone
683400	688680	would expect. Essentially, Stripe Tahina came out and the evaluations were close to models
688680	694520	I've been training on all year, like Lama 2 and Mistral 7B, and I went in and I went to the
695160	701240	Together API and I did like side by side of Mistral versus Stripe Tahina and it's like,
701240	705080	it's a good language model. It answers most questions. There's no obvious failure modes.
705640	709800	I think maybe Michael, do you want to comment on that? I know it's another big project and then
709800	713560	we can go back to Mamba, even though it's slightly out of order in the chronological
714200	723160	the release cycle that happened. Sure. I guess I'll start by saying that
724280	730840	there's an interesting connection between all these new methods. There's this sort of convex set,
731800	736280	which has a center and there's this connection between linear attention,
737000	743480	so attention without the softmax, linear RNNs, and state space models, SSM.
744280	749160	So at some level, the mathematical formulation of this kind of base model here,
750120	754600	I'm not talking about the base architecture, just the fundamental model, is the same.
755400	759960	And then you can go in different directions and each direction has its own trade-offs.
760840	769640	You can go to the feature map direction, the kernel direction. So when you break down the
769640	777240	softmax, you take away the softmax, you can place on queries and keys, kind of the fundamental
777240	782120	the entities that compose your attention matrix, you can compose other kernel-like functions,
782680	787640	other functions that you hope would approximate whatever capability of attention you like.
787640	791240	You can do things like a Taylor approximation, Taylor expansion, for example.
792920	798040	And you have a slightly different perspective, but you get something that, again, is very similar.
798600	804600	You can go to time variance, so you take the RNN and you push this input dependence,
804600	812520	so the way the computation inside the linear RNN is conditioned by the input sequence,
812520	816280	and you can add things like gates. We've seen a lot of work, for example,
816280	821880	modernizing linear attention with additional gates that allow you to make better use of your
821880	828280	fixed state dimension. And then you have the third direction, at least in my mind,
828280	833960	is the one that pushes, that uses the convolutional form, that pushes more towards other types of
833960	840520	linear operators that are still associative, that are still allow you to train in parallel.
840520	845560	So here are things, time invariant systems, I can elaborate on any of these points,
845560	848920	but things that can switch between convolutions and recurrence, like this form model,
849640	859640	with additional gates, again. Striped IEna was born as a project from the IEna architecture,
859640	865480	which belongs to this third category that I just mentioned, and we're really trying to get the best
865480	874360	per-flop architecture that we could. And one principle that was validated over and over again,
875000	881320	and we're trying to understand better now, is that it seems composing, hybridizing different
882920	889480	layers, different blocks of different categories, and even full attention yields something that
889480	894200	is better than the individual components. So there seems to be a compositional aspect
894200	898760	of these models that we're trying to understand better, and this gives you a better
899400	907400	sort of pre-trained model per-flop. And with this model, we ran a whole suite of skating laws,
907400	913000	and so on. Hybridizing also gives you, since we wanted something that would be kind of usable
913000	919000	out of the box, it gives you a way easier time. When you're fine-tuning for longer contexts,
919000	923320	we can apply some of these techniques that have been developed for transformers, and kind of
923320	930520	surprisingly work for hybrids as well. So things like linear scaling for
930520	934200	rotary embeddings and so on, you can go into the details. So it was mostly a project,
934200	937160	what is the best, given the current landscape, what is the best we can do?
937880	941720	Yeah, that's a great description of it. I mean, the sentence in the blog that's like,
941720	946200	start training is optimized using a set of new model grafting techniques, enabling us to change
946200	951800	the model architecture during training, kind of felt like, to me, that there's a ton going on
951800	956680	there. And some of which you probably can't talk about, there's normal data. I don't think all the
956680	960840	data that was quite explained, like what the longer context data was, but it's like,
960840	964440	are you taking this from models, starting point from models that people would know,
964440	970920	and can you say any of that? I think even just the summary that it's a synthesizing recent work
970920	977080	into a strong model is great context for people. Yeah, well, that line, so we've,
977800	985800	given this explosion of primitives that, you know, described, and given sort of the
988600	996200	cost that it would require to evaluate all different combinations, we found ways to essentially
996840	1002600	start training with a configuration and then continuing on with another configuration. I
1002600	1007320	think we'll have, we're going to have more work with paper. Yeah, there's so much cool work in
1007320	1011960	that area. So one of the, someone at AI2 is working on a project where they're essentially
1011960	1017000	trying to cut the Lama models in half and keep training them and things. It's just the wild west
1017000	1022600	out there with people trying to take strong models and make them smaller while still getting the
1022600	1027400	performance benefits of bigger models. I think that's a whole aside, but I wasn't expecting it to
1027400	1032040	show up when people, like you follow the social media, I've striped my, you know, people are like,
1032040	1036920	oh, non-attention models are finally good. And it's like, it covers up a lot of the details
1036920	1044920	that are very interesting about it, in my opinion. So, okay, it turned back to tree. I think Mamba
1044920	1051880	actually happened first among these. I did the, his reading back of social media. And it also was
1051880	1058440	very surprising to me. I think the largest model in the Mamba suite is 2.8 billion parameters,
1058440	1064920	if I remember correctly. And it was compared to a lot of the common benchmarks in open NLP. So
1064920	1071480	things like GPTJ, Pythia model suites and the scores on the benchmarks reported were really
1071480	1077320	strong. And I think if you want to start with the high level summary, and then I'm definitely going
1077320	1082200	to make you talk about the awesome new CUDA kernels and stuff that you had to write for this project.
1082760	1089720	Yeah, so this Mamba is a collaboration with, with Albert Gu, who's now, he was,
1091160	1096280	he's just doing it at Stanford. That's where we met. And he's now a professor at CMU.
1098120	1103320	And also at a startup. So it's a wonderful collaboration credit goes to him.
1104840	1108440	Yeah, Albert has been working on this line of work called state space models.
1109080	1114040	In some sense, as mentioned, it connects to things like linear tension,
1114040	1124600	linear RNN, convolution, neural nets. And that's what his PhD thesis is about. I've also worked on
1126040	1132280	state space for the past couple of projects. My, my angle is how to make state space more
1132280	1140520	hardware efficient and kind of increase their expressiveness. So it's wonderful working with
1141080	1148280	Albert. And there I think is more of a proof of concept, which is can state space actually do
1148280	1155640	as well as transformer on language. So we've seen previous papers showing state space could be better
1155640	1163720	on audio, could be better on some of the tasks on the long range arena. But language has always been
1164680	1172120	the most difficult to get to do well for state space models. And language is also kind of the
1172120	1180200	thing that people care about the most right now. So Mamba was more of a proof of concept, which is
1181160	1186040	hey, we want to show that state space can be competitive, or maybe even be some of the
1186040	1195160	transformers out there. So we validated that at the scale up to 3B, trained to 300B tokens.
1195160	1199800	So in absolute terms, you know, these are not very strong models. These are not yet models that you
1199800	1204840	would actually play with and expect it to do meaningful things. I think it's more of a,
1205720	1209880	more of an academic comparison in terms of architecture, it's like, hey,
1209880	1215400	training trained for the same amount of tokens, it does as well, or maybe slightly better than
1215400	1221640	some of the transformer out there. And that's, in particular, has been very exciting to us.
1221640	1227240	I think Albert's been pushing on this for a while. I've been pushing on this for a while. And I think
1227240	1236040	finally, it seems to finally be kind of close to gap or even surpass some of the transformer.
1237560	1245000	And it just, I think it opens up a bunch of opportunities. So inference could be way faster.
1245720	1250600	Maybe we would have different ways to understand how in-contact learning happens, etc.
1250600	1253720	So lots of, lots of future work, I would expect.
1254760	1260200	Yeah. Can you go into some of the, like, what does it actually take to implement some of these
1260200	1264520	new CUDA kernels? I just remember when this paper was announced, Sasha Rush, who's also very active
1264520	1269880	in this space, I recommended me to talk with you too, was tweeting about the types of files or
1269880	1276360	whatever. And in the paper, there's this discussion about how like the recurrent state needs to be
1276680	1281160	sufficiently expressive, but doing so in a certain type of memory is a problem.
1283080	1288680	Translate what this means to people thinking about GPUs and people thinking about these models
1288680	1294440	being scaled. Like, is it now much easier to scale these models because they work on GPUs?
1294440	1299560	Which GPUs were you using? Is there a bump that could come just from going to H100s or something?
1299560	1308760	Any of that? Yeah. So the line of work on state space,
1309720	1318680	sort of like S4 models kind of pioneered by Albert's work, they are in some sense
1318680	1327960	recurrent neural network, but they have a much larger state size. So the state size is whatever
1328680	1334920	kind of buffer that you're going to store information as you traverse or as you process
1335720	1339960	the sequence. In some sense, you can view Transformers doing that as well, where it's
1339960	1344520	keep the entire history. It's usually called the KV cache. It keeps the history and keep
1344520	1352200	referring to it. For RNNs, they have a fixed size state. For Transformers state, you can think of
1352200	1362040	the state size as increasing. And our intuition is that the larger the state size, the easier it is
1362040	1367320	for the model to do well. So basically, you have more space to store whatever you need to remember.
1367320	1372600	And so previous models like S4 and so on, they have an implicitly pretty large state size,
1372600	1377240	but they use the convolutional view to avoid having to materialize the state. So that was
1377240	1384200	wonderful. Michael has worked behind architecture, has used some of the same insight focusing on
1384200	1393800	convolution. Mamba, on the other hand, focuses on the recurrent view. So we wanted to put more
1393800	1400600	input dependency in the recurrence. We thought the thinking was that it was going to make it
1400600	1405400	more expressive and the model would do better. But that prevents us from using this convolutional
1405400	1409320	view that would make things efficient. So we had to figure out a different way to make things
1409320	1418840	efficient. And so I focused on making that efficient on GPUs. And so our thinking was,
1419960	1424520	instead of, okay, we're going to have a large state size, but we don't have to write to actual
1424520	1432920	GPU memory, like the HBM, where you can just keep that large state in a faster memory called S RAM.
1432920	1439240	You think of it as a cache. So if you're more familiar with CPUs, you just usually get cache and
1439240	1446200	RAM. So if you have large state, you can keep it in the cache. And by avoiding having to write it
1446200	1451640	down, you actually don't suffer too much if the state is large. So that's essentially the core
1451720	1455480	idea. Would this be due to like input, like having to move the data around being really slow?
1456200	1462120	Yes. Yes. That makes sense. That's a really common constraint in a lot of these things. And it's like,
1463000	1467880	I think one of the most insightful things I've had now with GPUs versus TPUs and stuff is how
1467880	1472600	mixtures of expert models doesn't work very well on TPUs just because you have to like,
1473240	1477480	that essentially add a mixture of expert at a basic level. There's a routing layer that you
1477480	1481640	learn and then multiple feedforward layers that you can choose from. And when you're distributing
1481640	1486600	this, the feedforward layers could end up on a different TPU node and TPUs communicate with their
1486600	1493000	neighbors. So TPUs take a big hit relative to GPUs where within video and video clusters,
1493000	1497880	everything's connected so much more. And then it's easy to do that sort of distributed training.
1499320	1503160	That's super interesting. And it's like, do you think there's going to be,
1503880	1507960	I guess this is really where I want to open the conversation of like, what is this going to
1507960	1517160	happen in 2024 in this space? Are bigger players going to move in and be exploring this? My take,
1517160	1522920	seeing how good the long context learning could be and a fundamental limit is that systems like
1522920	1529320	ChatGPT are going to use a transformer model for most tasks. And then if you need to do
1529320	1534600	summarization, you might do a long context specialized architecture. And then we could even
1534600	1539240	see a whole quiver of architectures behind something that you're using. But I think
1539960	1546360	it's just like, is attention going to be dethroned? Is Sasha Rush somehow going to win this bet that
1546360	1551000	everyone wants following in the area? What are you thinking about, either of you?
1553480	1558760	I think it's a very, very strong architecture. And there's a proven recipe, right? You know,
1558840	1562120	people scaling to a trillion of parameters. Right now, if you want, you say,
1562760	1568760	well, I just want the best performing model that runs most efficiently on my hardware,
1568760	1573320	that has the most support on the software side. The transformer is a safe bet. I think it's here
1573320	1584120	to stay. But I think there are new ideas like state space, China, so the linear attention,
1584120	1589000	ideas from linear attention, I think they're coming in. We've seen, as Michael mentioned,
1589000	1594520	that mixing some of these components seem to improve performance. We're validated at, I think,
1594520	1601960	seven B scale, but maybe it might even work at larger scale. I think people tend to be
1601960	1607880	conservative and focusing too much on model architecture might not be worth their time.
1607880	1611640	Like the LIMAR architecture is very, very strong. Most people are doing off of that. They're
1611720	1618600	focusing on data. They're focusing on infrastructure, which makes sense. I think on
1619560	1626360	my side personally, the other stuff is just plain interesting. There are more, I would say niche
1626360	1634040	use cases, niche for now, where some of these alternative architectures are interesting,
1634040	1637000	things like long contacts, different domains like audio and genomics.
1637560	1643880	There's just plain interesting scientific questions you can ask, whether it follow
1643880	1647720	instruction just as well, whether it's fine to interest as well. Does it play well with
1647720	1652200	quantization and so on? There's just plain interesting research questions we can ask.
1652200	1658360	Now on the production level, I think transformer is still incredibly strong, very well supported,
1659640	1665160	both hardware and software. But I think some of these new ideas are coming in and people might
1665160	1669480	start putting them as part of component in the transformer. Maybe we'll still call them
1669480	1676680	transformer, but they'll just have more layers and just attention and LPE.
1678520	1687000	Yeah, I 100% agree with you. So attention as a computational primitive is not going anywhere
1687720	1693800	anytime soon. It's just a very efficient and a very convenient way to increase
1694360	1703560	the effective state of your sequence processor. So at some level, if you're working with a model
1703560	1710200	that only has a fixed state in each of its sequence mixers, you have an assumption and
1710200	1715000	your assumption is that you only need so much information in the sequence. So there's always
1715000	1721240	a tradeoff between the ratio of the state dimension, the sequence length, as you push
1721240	1726760	things to the extreme, either model sizes, so as you make the model bigger, wider, effectively
1726760	1736600	introduce more states and sequence length. Some of these margins, some of this is speculation,
1736600	1742840	but some of these margins will disappear, some of the tradeoffs will change, especially 14, 30,
1742920	1751960	some of these very fat models. But certainly, either whether that's hybridizing or some kind
1751960	1757880	of new block, we're certainly going to see some more innovation. That's really exciting.
1757880	1762840	My personal, if I had to make a prediction is that architecture design will get more
1763560	1766920	interesting or more complex. There's going to be more to do.
1767800	1774840	Yeah, I mean, this year, it's like, I got some 10 minute clock that's fine for us. I think with
1774840	1783000	mixture of experts and this being popular as a state-state model, this is all just really within
1783000	1786600	a few months outside of OpenAI. They've been doing mixture of experts for a lot longer than
1786600	1792200	everyone, but in terms of open and academic communities, no one's really tried to do RLHF
1792200	1795560	on mixture of experts. It should just work, but we have to learn all these things. And then the
1795560	1802360	model grafting is becoming more of a real thing. That's super interesting. And it's just, I agree
1802360	1809480	that it's just fun to follow. And hopefully, it gives academics and scientists more ways to
1809480	1814200	influence the conversation where in industry, it's just about scaling and bigger models where we
1814200	1818840	could maybe do specific things better, which I'm telling open source companies to do with
1818840	1822120	their language models anyways, like if they want to have a business model, they need to have an edge.
1822680	1827080	So this all fits into that kind of narrative pretty well with my regards.
1828440	1832120	Is there anything else you guys are following in ML? It doesn't have to be about state-space
1832120	1835080	models. What's exciting for you broadly for next year?
1838120	1846920	Yeah, personally, I think data is still the most important thing. We're thinking a lot about how
1846920	1853560	data influences the model performance, like really teasing that out, either having some
1853560	1859560	of the synthetic tasks that correlates very well with model performance has been kind of the
1859560	1867080	motivating examples in a lot of our papers and work has been focusing on synthetic tasks,
1868040	1877400	or having maybe smaller data sets that kind of make it easier to really understand what's
1877400	1887320	really going on. So I think personally, my focus is going to be on data for the next little bit.
1888600	1895720	All the architecture stuff is fun. Making that hardware efficient is fun, but I think
1897080	1903960	ultimately it's about data. If you look at the scaling law curve, the different
1903960	1907160	monarchitectures generally have the same slope. They're just different offset.
1908360	1912520	Seems like the only thing that changes the slope is the data quality.
1913640	1918840	I love that point. That does seem true. I have the plot from Mamba in this blog post that I'm
1918840	1922920	writing, which is just a little bit above. Same slope.
1927560	1934440	Yeah, maybe add data. Data is really interesting. So miniaturizing architecture design, finding,
1935080	1940920	breaking down what tasks are involved into, for example, language modeling and trying to
1942200	1946120	package them into something that can be used to iterate, something that's quite exciting.
1946440	1958040	That was one of the main techniques that was used for this zoology paper that also looks into
1958040	1962280	some of these different behaviors. Personally, I'm also really excited about new applications,
1964200	1970600	scientific applications, the genomics work, but even more engineering-focused.
1971000	1980280	We're seeing a shift. Right now, language is still the domain that gets most clicks,
1980280	1982280	most interest, but I think that will evolve over time.
1984360	1988680	Some of these other applications offer, even just talking about architectures,
1988680	1993320	they offer completely different design space. I'm excited to look into it.
1994040	1999240	Yeah, everyone talks about language, but I feel like images and entertainment and videos are the
1999320	2003560	things that are so obviously going to generate so much value to me. I don't know the ceiling on
2003560	2009240	language, but when you could access a somewhat local text and video model at your homework
2009240	2014440	station, that's tailored to your preferences. The amount of value that that creates is totally
2015080	2021240	astronomical. I'm excited. I started playing around with these where I'd take text of the
2021240	2027000	blog and convert it to dolly images and convert it to a video with generated audio all with one
2027000	2033800	Python script. That's really easy to do, so I agree with your more than language. It's fun to
2033800	2040360	have that view. These things actually do work really well in your experience when you stitch
2040360	2047160	all of them together. It's not that good. The dolly images are pretty similar, but I'm doing
2047160	2052360	something really naive where I literally take the text and have a system prompt. It's like,
2052360	2057880	you're generating a series of images for visualizing a blog post, and it generates various
2058840	2062680	all the machine learning thumbnails that you see everyone using. There are variations of that.
2062680	2066520	The fun ones are whether it's about llama or mamba or something, and then they generate
2066520	2072840	animals in them, which is good. I think I could get much better at it and have a better segmentation
2072840	2077960	system for the paragraphs and or have chat to BT summarize them or something like that. But I
2077960	2082600	just know that within a year, it's going to be a text-to-video API, and I'm just going to switch
2082600	2087400	it, and it's going to be great. I'm laying the groundwork for infrastructure to have a
2087400	2095800	multimodal content distribution, really, and I just expect it to become very fun. Even the text
2095800	2101480	of voice is pretty good, I think. I don't have a studio, but once you have a studio, it's going
2101480	2106520	to be able to generate perfect audio for whatever you want. Another one of my dreams that is bad
2106520	2113640	for young students is I want to be able to give a slide deck to a script that returns the five-minute
2113640	2123160	conference video that no one ever watched, just based on a GPT-4 reading the slide deck and voicing
2123160	2128680	yourself. Those are the silly things that I have time to do because I'm not a professor.
2128680	2136760	Yeah, I think these advances, these systems, they do generate a lot of economic value,
2136760	2141480	and we're seeing that already. Lots of companies are now switching to using these things, and I
2141480	2145960	think it's going to change the way we work, as you mentioned, the way we work, the way we entertain,
2145960	2147880	so I'm just very exciting future.
2150680	2156760	Yeah, anything else? Well, thanks for coming. Try to get you guys as much attention as I can
2156760	2161080	bring. You never know, it'll go viral these days, so I think this was a great conversation.
2161080	2164600	People are really hungry for basic intuitions in the area, so this is good.
2166680	2168920	Yeah, thank you, Nathan. It was a pleasure.
2170360	2178280	Absolutely. Thank you for inviting us, and maybe if there are more questions,
2179160	2187320	is there a way to collect them to provide readers with listeners with an address or
2187320	2192920	something, happy to answer anything? Yeah, I'll include contact info in the post in
2193800	2199240	various ways. This will be out there. You'll get your comments on Substack, YouTube, Twitter. It's
2199240	2203720	a mess. You've got to pay attention to 10 million streams of information these days, but you'll get
2203720	2210680	contacted by people. Thankfully, for some reason, people read my stuff, but here we are. So thanks for listening.
