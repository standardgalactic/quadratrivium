Alright, so we'll move on to our next speaker. Yanxun will be our next speaker
and she is a associate professor in the Department of Applied Mathematics and
Statistics at Johns Hopkins University. Yanxun has a lot of research interest in
the reinforcement learning, high-dimensional data analysis, and
non-parametric statistics. Okay, so today she is going to talk about a Bayesian
reinforcement learning framework for DPR. So you can see the screen, right? Yeah,
yeah, great. So thank you Lu for organizing this session and introducing me. So
today I will talk about a Bayesian reinforcement learning method for
optimizing treatments for HIV patients. So first I'd like to thank all my
collaborators in this project. So my peer-de-student Wei and also my
collaborators, Leah and Raha from Johns Hopkins University and Yanni from
Texas A&M and also two clinicians, Amanda and Jane from Georgetown University
and Washington University in St. Louis. Okay, so making the optimal
sequential decisions is very important in many diseases. So today I will focus on
HIV and we know the emergence of anti-retroviral drugs, ARP, has transformed
the HIV infection from a fetal disease to a chronic disease. So it's
significantly reduced HIV-related mortality and the common ARP drugs fall
into different drug classes with different mechanisms. For example, the
first one, the NRPI, it's called a nucleotide or probably can also be a mouse.
Okay, so the first drug class called NRPI is called nucleotide reverse
transcriptase inhibitor. It inhibits the reverse transcription step in viral
replication and other common drug classes include like NRTI, PI, ESTA, EI and
the booster. And the model ART drugs usually combines three or more ART drugs
from different drug classes. Since such cocktail approach is most efficient in
suppressing viral loads. For example, the clinician can prescribe two NRTIs and one
drug from instinct and all they can prescribe like two drugs from an ARTI
drug class and one from an ARPI or one from the PI. So people living with
HIV now are recommended to follow up with their physicians semi-annually by the
current HIV treatment guideline. And at each visit, there are social demographics,
lab test results, such as CD4 counts and viral loads, and also clinical
environments such as BMI and the glucose are collected. And then physician
assigns their ART regimen based on their clinical observations. So each ART
regimen is a combination of different drugs from different classes. And this
process repeats every half a year by average. So nowadays, the US Department
of Health and Human Services provides general guidelines on assigning ART
treatments. However, these guidelines are usually applied to treatment
naive subjects, meaning subjects who are recently diagnosed with HIV and who
never took ART treatments before. And for happily pretreated people with HIV,
for example, if this person has been diagnosed with HIV for 20 years and has
been taking ART drugs for a long time, and there's no consensus. And also the
current guideline didn't take into account the potential adverse effects
caused by the long term use of ART drugs, because HIV now is more like a
chronic disease. And those patients needs to take ART drugs every day. And
indefinitely. So you need to take every day to take the drugs every day to
suppress the viral loads. And they may cause some long term adverse effects.
But for example, like the kidney disease, weight gain or depression, and the
current guideline didn't take into account all those side effects. So our
goal is to determine the personalized ART regimen to optimize the long term
health. That means not only the drugs can supply the viral loads, but can also
control the side effects. So large scale HIV studies such as maximize
cohort studies. And it provides us opportunities to achieve this goal. And
they collect data from participants semi annual visits. And here is the ART
treatment history for one randomly selected subject with seven visits. Here's
the X axis shows the calendar dates of their visits. And the Y axis shows the
ART drug combinations they have this person has taken. And different colors
here represent different drug classes and the drug name are marked. So I use a
three letter to represent each drug. And also at each visit, the house related
environments are recorded. For example, this figure shows the subjects,
longitudinal depression scores, viral load, EGFR for kidney function and the
BMI. So there are many challenges to optimize the sequential ART regimens
in a data driven manner. First, we need to estimate the drug effects. Like before
we assign them, we need to understand their drug effects from a high dimensional
and unbalanced space. So when I say high dimensional, it means with more than 30
ART drugs, they're all FDA approved on the market. There are a large number of
possible drug combinations. So that could be millions of drug combinations you
can choose. So even like the feasible drug combinations about like thousands,
there's still there are a large number of possible drug combinations. And also
when I say the unbalanced, that means some drug combinations are firmly used, but
others are wrong. So for example, we can see for this drug combination is two
ART drugs and one PR drug. It was the using our database for almost 1000
times, but another similar drug combination. So the same two ART drug, but
a different the PR drug, it was only used for 12 times. And the second challenge
is how to generate a realistic ART regimen from a large discreet space in the
optimization procedure. So after we understand the treatment effects of
these drug combinations, we need to assign the drug regimen to patients. That
means we need to generate a realistic ART regimen. So then the problem is how to
represent each ART regimen. A naive method would be say, okay, I can use a battery
vector. So say I have 30 ART, I have 30 ART drugs on the market, and then I can use
the battery vector is indimensional battery vector to represent. So if this drug
combination contains drug one, then it's one, otherwise it's zero. However, it will
cause two to the N possible drug combinations. And many of them will not
be feasible. So like I said, usually, you know, people combine the drugs from
different drug combinations, but usually we assign like, for example, two or three
drugs from ARTI, but like really we assign one drug from PI. So not all these
possible two to the N combinations are possible. And then we need the efficient
way to represent the drug combination. And lastly, we aim to optimize sequential
treatments from observational data. So we have all those data collected from
observational study, but we want to assign them to the patients in the future. So
the fundamental challenge of optimizing treatments from observational study is
this distribution shift issue. So that means the training data may be collected
on the different policies from the one we try to evaluate. So we need to address
the distribution shift issue. So to address these challenges, we develop a
two-step Bayesian reinforcement learning framework. And here is an overview. In the
first step, we propose a Bayesian dynamics model for individuals'
longitudinal observations using a microverte Gaussian process. And these estimate
dynamics describe how individuals' health-related variables evolve over time,
condition on their historical states and the treatment histories, with uncertainty
quantification. And in the second step, we create a pessimistic environment with
uncertainty penalization to mitigate the distribution shift issue and also use
an offline reinforcement learning method to optimize the sequential ART regimen.
So it's a two-step approach. Okay, so now let's go to the model details.
So first, I introduce the problem formulation. Assume for each individual eye,
we use XI0 to denote the baseline covariates, such as a race. And the TRI records the
visit times. So assume each one has GI visits, and we have PI1 to TIGI to denote the time of each
visit. And also assume we have M time varying health state variables. So here we call state
variables because they change over time. For example, like H, BMI, EGFR, those clinical
variables or demographics, they are collected in each visit. And also the I to represent the ART
drug combination used by individual eye during the time period, TIGI minus 1 to TIGI.
And then the data can be summarized as D. So for each subject, so we have a total of I subjects,
and we have baseline covariates, the time of each visit, and the time varying state variables,
and also the drug information. And then we use the YIG bar. So the bar is a common fun we really
use to represent the history. So the YIG bar means all the state variables before the current
visit J. And the ZIG bar means all the treatments this person has been taking on pure the time J.
And also we use the dynamic, we use the after dynamic model. So that means condition on the YIG
bar and the ZIG plus one, we update the state variables from YIG to YG plus one. So remember
the first step of our method to learn the dynamic model of how to transform from YG to YG plus one.
Of course, the YG plus one condition on the whole history of the YG and the treatments the IJ.
Okay, so our goal is to optimize the ART assignments to maximize the individual's long-term
house outcome. So because we want to maximize the house outcome, essentially we can transform the
problem to an optimization problem. So this optimization problem is like we first define
for each individual I, we suppose she already has GI visits. So if this person is a new patient,
so the GI will be equal zero. So the GI can be zero, or if this person already has GI visits,
and then we want to predict the next few visits, for example. And then we let the Y new and the
GI new to denote her future longitudinal states and the treatments, because our reward may depend on
her future states. For example, we want her less than the next two years, the depression is minimal.
And assume for any future visited J, the ART regimen is assigned through a policy function
PI. So that means if we can learn, if we can prioritize this function PI, and we can optimize
data and the equivalently we can optimize the treatment. And let's say we assign a stochastic
reward function RI that depends on the house states. So we can depend, we can define the reward as
for example, this person, their, their very loaded is low, and the pressure set is low, and the BMI is
in the normal range, and the kidney function is a normal range. Okay, so for example, if our goal
is to select the sequential ART regimen that leads to lowest accumulated very low in the next two
years, and it is coming an active sum of the very loads. Okay, so they notice the expected reward
for any individual I to be the following. Because we, in the first step, we learn a Bayesian model.
So essentially, you can generate their future states. And also, so we can generate the future
states, why are you from the learned dynamic model, and we generate the ZI new from their
the PI, the function. So we learned the PI from their parameter as the function PI.
And also, because we learned the dynamic model, and then we can integrate out certain things,
optimization procedure. So that's kind of the benefits of using the Bayesian framework in the
first step. Because in the decision making step, we can integrate out uncertainty, we can adjust
for this uncertainty quantification from their uncertainty of their dynamic, their dynamic model.
And our goal is to find as optimized optimal policy function PI that parameterized by data
i star. So we want to find as an optimal parameter that can maximize the r i theta.
So that's a problem. Okay, so now we have already defined our reward function r i. And we want to
find as a parameter theta that can maximize the r i. And essentially, it's a classical
reinforcement learning problem. And we can use the policy gradient method to solve the problem.
So essentially, if you can calculate the gradient of r i, and then you can use the policy gradient
method, essentially, you update the theta by this formula. So it's also classical results,
it's that you update the theta by the previous theta, and then plus some step size s i, q,
and then times their gradient. So the essential problem is how to calculate the gradient of our
reward function. So we can see our reward function is relatively complex, right? You take the expected
value of the reward function r i, and r is a function of y mu, and y mu you need to generate
from the predict distribution of your dynamical model. And besides all of that, we also need to
integrate out the uncertainty from the dynamical model by the P5 condition on D, that's a posterior
distribution of the phi. So it's not easy to calculate the gradient of this r i theta. Okay,
so luckily for the policy gradient method, there's a way to calculate that. So if you're
interested in details, you can find the details in the paper, but we can represent the derivative
of r i theta as follows. And this formula looks very complex, but we can't decompose this into
three separate steps. The first step is this step. So it's about the log of our policy function. So
essentially, if you can parameterize the ART assignment function, and then you can optimize
that. So that's our first challenge. We need to parameterize the policy function. And the second
step is how to generate the future states based on our dynamical model. So that's the second step,
is we want to sample the future states. And the last thing is if we can define the reward function.
So essentially, you decompose the calculating this gradient by three
sub steps. If we can sample future states, if we can define the reward function,
if we can parameterize the policy function, and then we can calculate the gradient of the reward.
And then we can plug into policy gradient method and then get the optimal theta.
Okay, so now I will introduce each part. So how to sample the future states? So if we want to
sample the future states, basically, we need a model, and then we do the predictive inference.
And in this case, we have multiple longitudinal states. And we use a multivariate Gaussian
process. The reason we use a multivariate Gaussian process because it's a popular choice for modeling
irregular space multivariate longitudinal data with great flexibility and also natural
uncertainty quantification. And here's irregular, it comes from the missing data because sometimes
maybe some visits and some measurements may be missing. Okay, so the multivariate Gaussian process,
the whole framework of this multivariate Gaussian process is relatively standard. We use yimt to
denote the nth longitudinal variable for treat for individual i at the time t. And we have a mean
function ft and the answer id residue epsilon. So for this f, we assume the multivariate Gaussian
process is distributed. So we will have the mean function. So the next slide I'll introduce how we
model the mean function. And for the various covariance parts, we assume they're the separable
covariance function. So they're here the cm to denote the correlation, represent the correlation
among different state variables because they could be correlated. And the ct to represent their
correlation among the time. So this separable covariance function adjusts for the correlation
among variables and also along the time. Okay, and here for the ct, the correlation kernel that
for the temporal correlation, we use the oil kernel because we expect the curve that's not too
smooth. Okay, so for the mean parts, that's kind of the key country, one of the key contributions
of this model is that for the mean parts, the first two parts are kind of standard. We use a
baseline, it's like linear Mase effect model, we have fixed effects and random effects. But
the how to model the drug combination effects, it's a key thing. So remember I said, for the
drug combination, it's a high dimensional space. So how to represent the drug combinations.
Here we borrow like the kernel idea. So the way we model that is we assume
there exists. So we assume there there's a okay, okay, so we assume there's a
we define a drug similarity regimen function kappa here. So because the z itself is a high
dimensional space. So to reduce the dimensionality, we introduce kappa. So it's like borrow the
kernel idea, we reduce the dimension from the original all the drug combinations to a manageable
size as capital D here. So we introduce a bunch of representative drug regimens ZD. And then we
calculate the similarity between each drug, possible drug combination with ZD. And then as
long as we can estimate the parameter gamma and D here, and then we can represent the drug effects
for each drug combination. And the way we define their similarity function, it depends on two
properties. The first one is we want sharing of information because the similar drugs from
because the drugs from the same drug class, they have similar drug effects because we share the
same mechanisms. So we want to share information from different drug combinations. And also from
this kernel, the introduction of this similarity function, it can reduce the high dimensionality.
So let me briefly introduce the idea of this kernel. Because of time limit, I will not see
the detail. So consider a straight way to compute the drug combination similarity.
And the one straight forward idea is we use linear kernel. So the linear kernel, they can
compute the similarity between regimens based on the proportion of common drugs that two regimens
share. So for example, here, we have three drug combinations. And all of them use two same drugs
from the NRTI class. So D4T plus LAM. And assume the third drug, the first two regimens, they
choose one PI drug, but different PI drugs. And another one is choose NRTI. So you can use a
linear kernel. That means the pairwise similarity among these three kernels will be two over three,
right? Because they have three drugs, and they share two same drugs. However, there are some
disadvantages. For example, the first two drug combinations. So to both of them, they use two
same NRTI drugs. And the third drug, they belong to the same drug class. Because the same drug class,
they share the same madness. So we would expect the similarity between the first two drug combinations
would be larger or would be higher compared to the similarity between there, between them and the
third drug. Because the third, because the third drug combination, they have the drug from a different
drug class. And another example, for example, if you have these two drug combinations, both of
them have two drugs from NRTI drug class, and one drug from the PI drug class. If we use a linear
kernel, and they would share zero similarity, because they don't share any of common drugs.
However, we know the same drug class will share some similarity. So the good method,
we should borrow this clinical information and share some similarity for these two drug combinations.
So the way we set up the, we define the drug similarity is we use the sub subject kernel.
So the sub subject kernel is the idea was to represent the sentences in natural language
processing literature. And here we represent our drug combination by a tree structure. And the
subject kernel can represent the similarity at all levels of the tree representation. So essentially,
the upper level is ART. And then we have the second level to represent which drug class
we draw the drugs. And the third level represents how many drugs from each drug class. And the
third level represents the each drug from each drug class. And then the sub subject kernel can
represent the whole similarity. For example, like regimen A and B, they can adjust for their similarity
is a blue box. And for regimen A and C, they can adjust for their drug similarity is a yellow box.
Even, you know, they don't share any common drugs, but you can still incorporate their similarity.
Okay, so now I have introduced this Markov-Berica Gaussian process to model the
longitudinal states. And then if we have a model and we have our own parameters, and then we can
write down the likelihood, and you can assign the price to all unknown parameters, you can
obtain the posterior distribution from the MCMC. So I will skip all the computational
details here. But essentially, now we finish the first step. So we have the Markov-Berica
normal model, we can sample future states. Okay, so the second one is how to define the reward
function. And the reward function, it depends on the clinical goal, right? So here,
it depends on how we define the long term house for each individual. So here, after consultation
with the clinicians, we determine that we define the reward that depends on the barrel load,
kidney function and the depression. So we want to care about first, you know, whether it can
successfully suppress the barrel load, and also maintain a good kidney function and also the good
mental health. So let's see, our goal is we will, so here we can say we want to maximize
the overall house in the next two years. So remember, the visits are semi-annual visits.
So that's why here the sum is from the next visit, next four visits, because next four visits means
the overall good health in the next two years. And then we want the depression, this as small as
possible. And also, oh yeah, here is the next four visits. And also for depression, it's as small as
possible. And for the barrel load and the EGFR, because as long as they are normal threshold,
it will be fine. So we define this kind of step function, as long as they are in the normal range,
it'll be fine. And if it's outside of the normal range, and we give certain penalty. And also here,
we have to personalize the weights, WI. So for example, if some person, they more care about
the depression, and then the WI1 can have a higher weight. So it's personalized and determined by
the physician and also patient himself. And also to mitigate the distribution shift
issue, we use certainly penalized reward. That's another advantage of using the Bayesian method
in the first step, because we can easily quantify the uncertainty. So this idea is by this paper by
you from UC Berkeley's group. And essentially, we define a pessimistic environment by introducing
a penalized reward. So the RA is defined by the previous slide. But now we penalize the uncertainty
of the, it's the predictive variability of the state and their treatments. And it's a
tuning parameter we need to learn. Okay. And then we use a posterior predict distribution to
quantify the uncertainty again, because we have a Bayesian model, so that's very straightforward.
Okay, so now we define a reward function. And the last step is how to parameterize the policy
function. So to prioritize the policy function, we make this also the three types of decision
after talking with clinicians. So essentially, we decompose this to several steps. So first,
if this person has been using ART drugs for a long time, and we will see if this person needs
to switch the regimen or not. So if the older drug works, and we can just keep using the older drug.
So this is what we will represent as a logistic regression method in the logistic regression
model. As long as all the health measurements are in within the normal range, and then we will
decide to just, you know, keep the old drug. And if one of them is not in the normal range,
we will switch. If this person needs to switch, and then we will need to generate a new regimen.
And because we used the three representation, and then we can now decide, you know, if we need to
switch how to generate a new regimen, essentially, we need to decide like which drug class and how
many drugs use the initial class and also which individual drugs at each class. So this essentially
it's a non-central hypergeometric distribution. Again, I skipped all the details. It's kind of
a little bit complex. So we have these three levels of three decisions. Okay, so now we have already
finished these three steps. So we have multivariate Gaussian process to some whole future states,
and we define reward function. And then we have ways to prioritize a policy function. We can use
a policy gradient method to optimize a print. Okay. Okay, so now, so here I finish all the
matter introduction. Last part of the slides is I will introduce real data analysis results.
So for the real data, we got about 300 women from the Washington DC site from the white study.
And also now we get four state variables at each visit, depression, viral load, EGFR, and BMI.
And there are about 8% missing data. And the baseline covariates we consider include age,
smoking status, substance use, employment status, hypertension, diabetes. And in this study,
we have 31 ART drugs and six drug classes. And we choose 105 representative drug regiments. So
those regiments based on the popularity of the drug combinations, if they have been used a lot
of times for the from the patients, and then we would know that as representative ART regimen.
Okay, so here is one hypothetical patient. So we'll use this example to demonstrate the precision
medicine, the, you know, the utility of the clinical utility of the proposed methods. Okay,
so this person has been has been had 31 visits. And here is their history of treatments. And for
these patients, we assume their weights as one third, one third, one third. Okay, and then we run
our optimization method. And here we can see the expected reward versus the SGD iteration. So it
became relatively stable after 1,000 iterations. And here is optimal regimen is the next two years.
So we can see at the visit to there are two ART drugs when you see one PR and one poster,
and then it changes one new regimen for visits 33 to 35. Okay, and also here I want to show under
their estimated optimal regiments, that's the predicted depression stores. And we can see for
their visits from 32 to 36, they're about 23% improvement of depression. So that also shows
the clinical utility of our, you know, newly assigned optimal drug combinations. Okay, I will
skip the next example due to the time limit. Oh, yeah. Okay, to summarize, we, we propose a like
a Bayesian reforcing learning approach is a two step approach. And it can learn their dynamics
with uncertainty quantification, it can also assign the long term optimal drug combinations to
optimize each individual's health. Okay, yeah, thank you.
Thank you so much, Yanxun. Any questions from the audience?
Yanxun, those are very exciting work. I actually have some questions because you touched a very
good point where you need to balance the priority like competing priorities when you are in the
clinical practice. But since we are a little bit over time, so I probably will talk to you later
about that. I was wondering like how the uncertainty will be impacted by how you
define your reward function. Oh, yeah. So the uncertainty part, you know, how the uncertainty
affects the final decision depending on how you tune the parameter. So like here, and yeah, I skip
that part, but here you can show if we have different tooling parameters like lambda equal
zero, you don't penalize at all. And then you have this drug combination. And if you use like
increase the lambda, and then it will penalize the uncertainty, it's kind of uncertainties
reflected by the sample size in their data. If this drug combination has has been used a lot of
times, it has relatively narrow uncertainty, it had never been used, then it has a lot of
uncertainty. So for example, here with lambda equal zero, they actually recommend this drug
combination is the first recommendation. So it actually never been used in the in the data.
So that's kind of create a trade off, like we need to discuss the clinician, like if this drug
combination has never been used, do you want to try this new drug, or you want more conservative
choices, like, you know, these two drug combinations, it right, it has been used more times. Yeah,
I mean, this this tuning parameter definitely plays a role. But you know, actually, when you
define your reward function, there is another part with the personalized weights. So I was wondering,
like, no, we also have similar problems. So we also have like, for example, the survival or
quality of life to balance. But then when we provide the personalized weights, and if you
change a little bit of the weights, actually, the rules or the decision you will make, or you learn
from the data will change. So yeah, that's a good question. Yeah, we can we can discuss more
details. Yeah. Thank you so much. I'm going to share my screen.
