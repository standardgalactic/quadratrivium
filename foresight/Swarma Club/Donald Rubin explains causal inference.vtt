WEBVTT

00:00.000 --> 00:04.480
各位老师同学大家好,欢迎大家参加今天的讲座活动。

00:04.480 --> 00:08.200
我是主持人杨二茶,是清华大学的再度博士生。

00:08.200 --> 00:14.720
本次活动有集致和志愿社区附画的英国科学社会成员自发组织,

00:14.720 --> 00:17.160
并且有集致提供应用支持。

00:17.160 --> 00:26.960
今天的活动我们非常非常地荣幸邀请到了当今世界上最具影响力的统计学家 Donald Ruffin教授来为大家介绍英国推荡的工作。

00:26.960 --> 00:32.520
然后为了方便这个讲座,接下来我都会用英语再给大家进行成熟。

00:32.520 --> 00:36.960
Dear students and teachers from all over the world and from the community,

00:36.960 --> 00:38.960
welcome to today's event.

00:38.960 --> 00:45.360
We are very honored to have Donald Ruffin, one of the world's most influential scholar in statistics,

00:45.360 --> 00:54.960
to be with us and to introduce the essential concepts for causal inference in randomized experiments and observational studies, a remarkable history.

00:54.960 --> 00:59.960
First, let me briefly introduce Professor Donald Ruffin.

00:59.960 --> 01:11.960
He is the John Litt Professor Emeritus of Statistics from Harvard University, where he has been professor since 1983 and also department chair for 13 years.

01:11.960 --> 01:23.960
He also holds professorship at the Young Mathematical Sciences Center in Tsinghua University and Murray Schusterman Senior Research Fellow at Fox School of Business Temple University.

01:23.960 --> 01:31.960
He is the winner of Wilkes Medal, Pausen Prize for Statistics Innovation, George W. Snedekor Award.

01:31.960 --> 01:51.960
He has been elected to be a fellow member, honorary member of American Statistical Association, Institute of Mathematics Statistics, International Statistical Institute, American Association for the Advancement of Science, American Academy of Arts and Science, European Association of Methodology,

01:51.960 --> 01:55.960
British Academy and the US National Academy of Sciences.

01:55.960 --> 02:04.960
As of 2021, he has authored and co-authored over 400 publications, including 10 books, and has four joint patents.

02:04.960 --> 02:17.960
And for many years, he has been one of the most highly cited scholars in the world, with currently over 35,000, 350,000 citations from Google Scholar.

02:17.960 --> 02:23.960
And we're also very honored to have three discussant speakers with us.

02:23.960 --> 02:28.960
They are Professor Zhou Xiaohua and Zhu Zhou from Peking University.

02:28.960 --> 02:43.960
He's the PKIU Chair, Professor and the Chair of Department of Biostatistics in Peking University, and Xi Liang Zhao, Professor from the Wang Yan'an Institute for Studies in Economics and from Xia Men University.

02:43.960 --> 02:50.960
And also Professor Tui Pong from Association Professor with tenure in Qinghai University.

02:50.960 --> 03:03.960
Now without further ado, let us give a warm applause and welcome to Donald Ruby, and to have him introduce for us the causal inference.

03:03.960 --> 03:05.960
Okay.

03:05.960 --> 03:11.960
So these, these slides were created a few months ago.

03:11.960 --> 03:19.960
I want to thank the organizers for inviting me.

03:19.960 --> 03:25.960
And I'm very pleased to be able to give this introductory lecture.

03:25.960 --> 03:49.960
Another comment that that's relevant today, but that is, is not relevant with was not relevant when I made up these slides is that within the last week, my co authors on on a causal paper, just got the share the Nobel Prize in economics.

03:50.960 --> 03:53.960
And Guido in bends.

03:53.960 --> 04:00.960
Just about a week ago, got a Nobel Prize in economics.

04:00.960 --> 04:09.960
And I'm very happy for work that's completely consistent with this framework that I'm going to describe here.

04:09.960 --> 04:25.960
And actually both of them gave me gave me phone calls the day that I got the Nobel Prize because it's an impressive event so in, in addition to being this interpretation of essential concepts and causal inference.

04:25.960 --> 04:43.960
I think now it's pretty much become the standard approach, at least in economics and parts of medicine. So I think it's the talk is even more relevant than it would have been of even a few weeks ago.

04:43.960 --> 05:00.960
The title is essential concepts for causal inference and randomized experiments and observational studies are remarkable history, and a couple comments about the title, what I mean by essential concepts is ones that you need, and you get rid of the clutter.

05:00.960 --> 05:22.960
So for example, I'm often asked about the graphical approach to to to causal inference. And in many cases, other approaches than the one I'll be describing here can be useful for communicating ideas but they're not essential.

05:23.960 --> 05:38.960
And the sense of essential here is like the mathematical sense of of essential. So if you leave out a condition in the theorem that makes the condition false without that condition, then that's essential.

05:38.960 --> 05:42.960
And if you if you add stuff, that's not essential.

05:42.960 --> 05:59.960
So for example, if you say you have a right triangle, and with two sides equals so it's an isosceles right triangle, and you say the twice is the square of the short sides equals the long side.

05:59.960 --> 06:13.960
Well, that's true. But it has clutter in it because you're putting something in that you don't need because it's any, it's any right triangle for which that's that's true.

06:13.960 --> 06:28.960
So, and why do I call it a remarkable history. Well, as the idea will be developed here, which you'll, which you'll see at least from my perspective, is the history of the formal history is very recent.

06:28.960 --> 06:44.960
You can trace the intuitive idea of causal inference back for thousands of years, but the formal mathematical history is really, in my mind, remarkably recent, meaning it's a 20th century idea.

06:44.960 --> 07:03.960
That's related in some ways to another set of remarkable ideas, having to do with quantum mechanics, basically the idea that you can define two things, each of which can be measurable, you can have to observe, but you can't observe them simultaneously.

07:03.960 --> 07:16.960
And that's what I regard as one of the essential ideas of causal inference, as reflected in, especially in the, in the mathematics of the 20th century. Okay, next slide.

07:16.960 --> 07:25.960
Well, a pro-leg to causal inference is my sort of exposure to what I regard as science.

07:25.960 --> 07:42.960
As a kid, like many kids in high school, I was interested in math and physics, but far more physics than math. So when I went to college, I went to Princeton University, which I entered 1961.

07:42.960 --> 07:52.960
And the first physics course I took with a relatively famous physicist named John Wheeler.

07:52.960 --> 08:02.960
John Wheeler was a colleague of Albert Einstein's, and so they were in the physics department at the same time.

08:02.960 --> 08:22.960
And he was also well known for his attributed to having made up the name Black Holes. And in fact, he denied that he was giving a lecture at one point in time as describing something that has so much mass, so much gravity, that even light cannot escape.

08:22.960 --> 08:36.960
And somebody in the audience, this is an audience of about 200 people, shouted out, it's a black hole. And according to Wheeler, he tried to find the guy afterwards, but never could.

08:36.960 --> 08:43.960
And so he, he started using the expression became attributed to him.

08:43.960 --> 08:50.960
Another thing that Wheeler, he was a really kind person and outstanding teacher.

08:51.960 --> 09:00.960
And his first problem that he assigned us in 1961 was how far can a wild goose fly.

09:00.960 --> 09:07.960
And obviously he was not looking for a precise answer, but he wanted to know how you would try to solve it.

09:07.960 --> 09:22.960
What are the principles that, that even when you're really in physics understand, what are the principles of conservation conservation of energy, momentum, these other basic concepts and so the energy was had to be conserved.

09:22.960 --> 09:31.960
So how does, how does a wild goose say up in the air and fly. Well, what he's doing is he's converting energy.

09:32.960 --> 09:45.960
That potential energy into kinetic energy to fight gravity pulling him down. So he wanted you to think about those those principles.

09:45.960 --> 09:56.960
Another thing that Wheeler was known for is he was the PhD advisor of Richard Feynman, the famous US physicist Feynman.

09:57.960 --> 10:08.960
But these were complicated years in 1961, especially for a male in the United States because there was a draft lottery.

10:08.960 --> 10:35.960
In order to stay out of the draft, to avoid tromping through the mud, thousands of miles away, you had to stay in school. And so, at that time, if you were going to be in getting a PhD in physics, pretty much the job market looked like you even if you got a PhD, you had to had to be in nuclear physics.

10:35.960 --> 10:40.960
And so, instead, I switched to psychology.

10:40.960 --> 10:54.960
And I went when I went into psychology at Princeton, there was a gun, Sylvan Tompkins, who was a wonderful professor is also a character named Julian James.

10:55.960 --> 11:03.960
In 1964, and became actually very good friends with especially later when I returned to live in Princeton.

11:03.960 --> 11:17.960
And it was became very close friends and talked about lots of ideas. And one of his his critical ideas was consciousness. What makes human beings different from other animals.

11:17.960 --> 11:26.960
He wrote a book that was published in the 1970s, called the origin of consciousness in the breakdown of the bicameral mind.

11:26.960 --> 11:32.960
And this book actually advanced a thesis that consciousness was special to humans.

11:32.960 --> 11:47.960
It's not possessed by animals, because they had two hemispheres, so all brains have have two hemispheres, but humans developed the art of communicating between the two hemispheres, which are really independent.

11:47.960 --> 11:55.960
They're only connected with a gang of nerves at the base of the of the brain at the top of the spine for the corpus callosum.

11:55.960 --> 12:06.960
And they, these two hemispheres communicated almost independently through language at the zap messages back and forth.

12:06.960 --> 12:19.960
And so his, his version of consciousness was that people would, we had these two hemispheres talking to each other, almost as if they were.

12:19.960 --> 12:27.960
And in fact that pre conscious people would hear these messages as messages of God's.

12:27.960 --> 12:42.960
And this was very influential on my on my thinking and psychology became very interesting to me is all science everything we math mathematics even formal logic is obviously filtered through our brain.

12:42.960 --> 12:48.960
So could you go back to the, yes, I'm not done with that yet.

12:49.960 --> 13:07.960
So I went when I went to graduate school at Harvard, I started out in psychology, and then I realized that I really wasn't that serious about psychology, and I switched to computer science where I got a master's degree.

13:07.960 --> 13:28.960
And then I actually eventually switched to department of statistics right from my eventual PhD, where my advisor was William Cochran, this wonderful Scottish statistician, very, very down to earth guy, who taught me experimental design in 1968.

13:28.960 --> 13:41.960
But this was a was a great course, because it had formal experimental design how you learn about the world how you learn about what works and what doesn't work in terms of interventions.

13:41.960 --> 13:53.960
And the other thing about Cochran's course that was I opening for me, it was completely consistent with the science that I learned when I was in physics.

13:53.960 --> 14:03.960
And one thing about it is there was a clear separation between science, which is the object of inference, and what you do to learn about the science.

14:03.960 --> 14:15.960
So what you do to learn about science is you intervene in the real world in some way to intervene to measure aspects of science of the world at one point in time.

14:15.960 --> 14:35.960
To intervene you have to change something you have to either observe something which you throw photons at it, or something. So the, which you try to learn about is, is the object at a point in time at one instance, and you learn about it through what you observe at a, at a subsequent

14:35.960 --> 14:46.960
instance. But the, the notation that you use to represent the science at the previous point in time is the same.

14:46.960 --> 14:53.960
So the science notation for the science doesn't change as you try to learn about it, or measure it.

14:53.960 --> 15:02.960
But the, but the fact of measurement changes the science from the earlier point in time to a later point in time.

15:02.960 --> 15:22.960
And that way of thinking about it is, is missing data always exist. We cannot go back in time and undo what we did to, to measure the science. And this was completely consistent with, with two principles in, in, in physics, the well known

15:23.960 --> 15:33.960
uncertainty principle, for example, which says you cannot measure position and momentum at the same point in time you have to take your choice they both exist.

15:33.960 --> 15:48.960
But you can only measure one with this with certainty. And there's another principle that's sometimes confused which is the observer effect that the active observation actually changes the world has to do with time.

15:48.960 --> 15:54.960
And the state of science at time to is different from the state of science prior to that.

15:54.960 --> 16:07.960
And where's the Heisenberg uncertainty principle is has to do with with quantum mechanics and uncertainty of being able to measure both things at the same point in time.

16:07.960 --> 16:11.960
Okay, next slide.

16:11.960 --> 16:23.960
So, so my exposure to statistics when I entered Harvard again was to Cochran, and, and he was a protege of Ronald Fisher.

16:23.960 --> 16:39.960
And Ronald Fisher had really revolutionized the field of statistics in fact started in some real sense, and in 1925 with his book statistical methods to research workers.

16:39.960 --> 16:59.960
And in the last chapter in that 1925 book, he proposed randomization. So if you're trying to learn about what an intervention does meaning is fertilizer one better than fertilizer to control fertilizer versus a new fertilizer, or for a variety of

16:59.960 --> 17:15.960
seeds of a plant is a new variety better than the old variety, you actually should randomize turns out that the idea of randomization, mathematically, was in the air at the time.

17:15.960 --> 17:29.960
If you read some old student gossip articles in the early 20s, you would say well if you had randomized, then the distribution would be this way.

17:29.960 --> 17:44.960
This sort of was in the air at Rotham study at the Rotham study experimental station, where they were doing studies of different fertilizers and different breeding of plants and animals.

17:45.960 --> 18:05.960
And the idea of actually randomizing this issue in thereby you created balance and all treat pre treatment variables in expectation. So the fact that it created balance was in these articles, in various expressions, but the Fisher said that I know that's what the mathematics

18:05.960 --> 18:16.960
is, but I'm saying you should actually do that. You should actually toss coins to assign treatments, and no one had actually done that, but as far as I can find.

18:16.960 --> 18:20.960
Before there's some debate about this.

18:20.960 --> 18:25.960
But it will say a little about that in the subsequent slide.

18:25.960 --> 18:40.960
Now there was recondite advice at the time that if you had unbalanced covariates, so an important pre treatment variable that was unbalanced between treatment group and a control group.

18:41.960 --> 18:56.960
Fisher actually said he would re randomize. Now he didn't write that. And I can't find that advice anywhere. So you got a bad randomization, meaning for example, in the study of hypertensive drugs, where they have males and females.

18:56.960 --> 19:06.960
If you if you did complete randomization, it's possible that all the males would get an active treatment, and all the females would get the control treatment.

19:07.960 --> 19:09.960
Bad advice.

19:09.960 --> 19:24.960
Bad randomization so the advice was from from what would Fisher wrote is live with the randomization, because that's what underlie all the mathematics like the f, f distributions and t distributions.

19:24.960 --> 19:30.960
If you talked to Cochran about what Fisher actually would do, he would re randomize.

19:30.960 --> 19:39.960
He would throw away the bad randomized allocation and look for a better one at the, at the time.

19:39.960 --> 19:55.960
The theory was was was complicated because you didn't have the usual symmetry arguments working for you. They were actually led to truncated distributions, which you which for the, which the computation was back then impossible.

19:55.960 --> 20:08.960
But the theory and application is now being pursued because the computing is much better than it was. That's being pursued in a series of articles I wrote with a former PhD students.

20:08.960 --> 20:17.960
And it's being pursued in other words, I'm doing with other people.

20:17.960 --> 20:22.960
And that's it's being pursued because the computations is, is possible.

20:22.960 --> 20:36.960
Now, if you, if you did get an acceptable randomization you were living with it. How would you do the assessment of the was there a causal effect.

20:36.960 --> 20:50.960
And in here he had this very deep idea that's related to Carol Poppers of the philosophers idea of science advances by rejecting myths.

20:50.960 --> 21:05.960
So what, what, what, what Fisher did is he had this, this method of inference, where you hypothetically re randomized, you look at the data, put down a null hypothesis and hypothesis that that you don't that you want to prove is wrong.

21:05.960 --> 21:23.960
And so in this re randomization was a stochastic proof by contradiction so it's not really a proof by contradiction, but you get a probability that you would observe data, which you did observe this extreme, if the null hypothesis were true.

21:23.960 --> 21:37.960
I think it's a brilliant idea that that led to a sort of a deceptive version of it, because the name and Pearson ideas which are which I think are not nearly as sharp.

21:37.960 --> 21:56.960
And I, and I think that this proof by contradiction, it can be embedded within a Bayesian framework. And I wrote about this in an annals of statistics paper 1984, where I call that a posterior predictive check or price, where the p value in

21:56.960 --> 22:10.960
a check is a posterior predictive p value, or instead of having a sharp null hypothesis, what I mean by sharp, if there's no missing data anymore. So you can fill in all, all missing values.

22:10.960 --> 22:23.960
And, but instead of having a sharp null hypothesis you can have a stochastic null hypothesis. So it becomes a Bayesian positive predictive p value.

22:23.960 --> 22:42.960
Even though he was never formal about the, this idea of an alternative hypothesis, because he said, I have never, I have no problem in choosing a test statistic, just do it. He clearly understood this site, a non null causal effect, because in this 1918 paper, years

22:43.960 --> 22:57.960
from the 1925 book, he has this direct quote, if we say this boy has grown tall, he has, because he has been well fed, we are not merely tracing out the cause and effect in the individual instance.

22:57.960 --> 23:08.960
In this particular case, we're suggesting that he might have quite probably not very well written, might quite probably have been worse fed.

23:08.960 --> 23:19.960
In that case, he would have been shorter. So he's comparing the boy's height, being well fed with his hypothetical height, had he been poorly fed.

23:19.960 --> 23:31.960
So he clearly has this idea of, of the, of an alternative hypothesis here the alternative is being well poorly fed, because the, what's observed as being well fed.

23:31.960 --> 23:44.960
But he never had any explicit notation, never any mathematical notation for formalizing non null causal effects, despite, I mean, it's very difficult to criticize.

23:44.960 --> 23:56.960
Fisher's work with respect to distributions under this sharp null hypothesis, tremendous geometric insights based on symmetry arguments.

23:56.960 --> 24:09.960
Fisher was was legally blind. And, and so according to Cochran, he would, he would, when he had a problem, he would, you know, sit down, close his eyes and think about things.

24:09.960 --> 24:25.960
And we have these blasted insights, I guess that, that came into his mind. So where do we get the notation that that that was arose in the textbooks in the mid 20th century like, like in Kempthal next slide.

24:25.960 --> 24:38.960
That's okay. I mean, these things happen with with with zoom, especially when the when participants are thousands of miles away.

24:39.960 --> 25:03.960
So the notation that was used in in subsequent years to describe in mathematically the situation with about causal effects was actually due to Neyman Jersey Neyman in his 1923 master's thesis, where he defined written in in in Polish.

25:03.960 --> 25:27.960
And this thesis was not translated to English until 1990, although because Neyman by before 1990 was at Berkeley, the notation had had a big influence because Berkeley was the outstanding department of statistics at the time Neyman was was there.

25:27.960 --> 25:37.960
And so what he defined the estimates the quantities to be estimated in randomized experiments as functions of potential outcomes for N units.

25:37.960 --> 25:50.960
In other words, why of zero is the array of potential outcomes under treatment zero, and why one is the array of potential outcomes under an active treatment, labeled here treatment one.

25:50.960 --> 26:15.960
And Neyman at 23 was written in Polish, as I said, translated to English in 1990. And I, I made up the phrase potential outcomes, really to stay close to this 1923 paper, where at least the translation of into English of this, of this paper in 1990, called the potential yields, where the units of the

26:15.960 --> 26:31.960
experiment were the N units were plots of land, and the outcomes there were yields of of of a particular variety of a of a plant under different fertilizers.

26:31.960 --> 26:40.960
And so the treatments there would be the control fertilizer and treatment one for example would be the active fertilizer.

26:40.960 --> 26:52.960
And Neyman actually use this notation, although later he denigrated he said I was I was just fooling around I really didn't know what I was doing.

26:52.960 --> 27:15.960
And there are there are other examples of that in science we can see people using notation that they really didn't understand the full meaning of fact actually you can you can see that in some of the work on in relativity as well.

27:15.960 --> 27:31.960
I don't want to get off onto that topic, although I could answer some questions about it. And so, namely put down this notation, and implicitly assuming something that I later called, so to stable unit treatment value assumption.

27:31.960 --> 27:50.960
What this means is that if I give if I have a Y which is the outcome for plot I so I indexes the plots, and w indexes the different treatments. So that's a function of I and w. That's all that's that's it's a meaning it's a well defined function.

27:50.960 --> 27:53.960
So you have that notation.

27:53.960 --> 28:10.960
So all that the outcome for particular treatment on a particular plot. It's just a function of that plot, and of that treatment. It's not affected by the fertilizers that the other plots got. So for example,

28:11.960 --> 28:33.960
excluded interference between units for example, if you're doing an agricultural experiment where rain would move one fertilizer on one plot to an adjacent plot. So the adjacent plots, not only were affected by the fertilizer they receive, but by fertilizers adjacent plots got.

28:34.960 --> 28:55.960
And so agronomists knew this for hundreds of years, but, but this assumption wasn't really formalized until the combination of of naming and this explicit assumption that I made up and in the quote said for 1968.

28:55.960 --> 29:06.960
So the point is that you cannot observe both of these on any one unit cannot observe both the outcome under under active treatment and the outcome under control treatment.

29:06.960 --> 29:18.960
And so this idea was kind of natural to me, because I grew up with with Heisenberg that you that just existed, but the name and contribution went beyond this notation.

29:18.960 --> 29:26.960
And he evaluated the operating characteristics of procedure procedures, such as estimators over the randomization distribution.

29:26.960 --> 29:45.960
So he said, Okay, I observe this under the, what would be the expectation of the of a statistic, an estimator such as the observed outcomes, the observed potential outcomes under active treatment, minus the observed potential outcomes under control treatment.

29:45.960 --> 30:02.960
So this is a statistic being a function of observed values. But now we'll see what I would have observed under all possible values, and I can derive pro operating characters for such as unbiasedness of this difference in observe sample needs.

30:02.960 --> 30:08.960
Very important idea which eventually led to all the name and Pearson stuff.

30:08.960 --> 30:24.960
He also worried about the role of non additive unit level causal effects, an additive unit level causal effect is the difference between why I have zero and why I have one is the same for all I.

30:24.960 --> 30:33.960
So that would say that the new fertilizer is better than the old fertilizer by the same amount and all plots.

30:33.960 --> 30:51.960
And name and word about that in 1923, because he he not only defined unbiasedness, but but he he eventually defined confident would later was called confidence intervals.

30:51.960 --> 31:06.960
Now, in, in, in later years, he denied the lack of understanding in 1923, pointing out that the, that the definition of the potential outcomes was an important idea.

31:06.960 --> 31:25.960
And he said, I really didn't understand the real depth of randomization, because they never advocated it is Fisher, who advocated randomization actually randomizing plots, not, not name it. Next slide please.

31:25.960 --> 31:48.960
So once on these insights. Well, these are 20th century insights at least as, as, as far as, as, as I can see. I've looked a little bit I'm not a really in depth historian like Steve Stigler, my former colleague at the University of Chicago, but I think I can't find any, any corresponding insights in

31:48.960 --> 32:06.960
these insights where you define an estimate quantities you want to estimate in terms of measurable quantities, which, but these are individually measure, but they cannot be simultaneously measured, measurable even theoretically.

32:06.960 --> 32:27.960
So I really think these are really 20th century insights that were seem to be floating around Northern Europe at, at the time was was was Fisher really the, the, the, the first to have these well Steve Stigler likes to point to an American psychologist,

32:28.960 --> 32:51.960
who's whose father was a mathematician at Harvard in the late 19th century, who was writing about unbiasedness from, from taking representative samples, very, very close but as far as I can see, there's no use of the idea of randomization as a base of inference the way,

32:52.960 --> 33:13.960
the way Fisher did. But anyway, after Fisher proposed randomization in this last chapter of statistical methods for research workers, randomized trials quickly dominated agricultural and animal, animal breeding in the United Kingdom, and became even dominant in the United States in

33:13.960 --> 33:18.960
industrial work, more applied work was was was done.

33:18.960 --> 33:22.960
For example, by Oscar Kempthorne,

33:22.960 --> 33:33.960
Bill Cochran and Gertrude Cox, George box wonder wonderful work by by by box in industrial experiments, David Cox

33:33.960 --> 33:35.960
wrote a wonderful textbook.

33:36.960 --> 34:01.960
I'm planning experiments and supporting more mathematical work and almost pure mathematical work was done in the Indian statistics Institute, which is founded by my Helen obis work was done there by RC bows, Nair, see a row who is still going strong in his 100th birthday he had his 100th birthday.

34:01.960 --> 34:09.960
Are we 100 first I think a few weeks ago.

34:09.960 --> 34:30.960
And subsequently randomized controlled trials entered Western industry they started doing industrial experiments and at the end of the Second World War, when the, the, the, the West, the allies had completely sort of destroyed.

34:31.960 --> 34:35.960
Japan at the end of World War two.

34:35.960 --> 34:52.960
We sent Edward Deming, the West and Edward Deming to help rebuild Japanese industry, and through the use of quality control in experiments and in randomized experiments.

34:52.960 --> 35:05.960
It was a combination of randomized experiments, and especially with the idea of quality control. Japan has given a Deming medal for quality control since 1951.

35:05.960 --> 35:30.960
But the insights that that were exposed in the in this work with randomized trials were really limited at this time to non conscious units plants, animals, or industrial object like widgets you know various mechanical things that we built or paints that

35:30.960 --> 35:42.960
we used to do experiments on paints for painting the dividing lines in in in roads, for example. Next slide.

35:42.960 --> 35:57.960
The transportation of these insights to randomize control trials with conscious units came before the transportation well before the transportation of these insights to non randomized studies.

35:57.960 --> 36:15.960
And in medicine my understanding is the first randomized experiments were done in the United Kingdom in 1946, the Medical Research Center, and a guy named Sir Bradford Hill on Streptococcus,

36:15.960 --> 36:22.960
where they have antibiotics study of antibiotics they actually did randomized experiment.

36:23.960 --> 36:37.960
Early big randomized experiment with salt vaccine, whereas like thousands of people that was done in 1954, actually randomized experiment with conscious units, and they actually use blinding them.

36:37.960 --> 36:51.960
The idea that you don't tell the patient, which you're, or the kid in this case, which vaccine you're getting the the active vaccine or placebo vaccine.

36:52.960 --> 37:03.960
In the in the 50s and middle eight 50s of the United States Food and Drug Administration, it became almost wedded to the use of randomized experiments.

37:03.960 --> 37:20.960
And so randomized experiments entered pharmacology. And before this before the 1950s and Paul Meyer was my colleague at the University of Chicago was the main instigator this he really sort of fell in love with with random device experiments.

37:20.960 --> 37:36.960
But when I say falling in love. I think there was an over-resilience in adherence to the intention to treat principle to estimate the effect of the assignment to treatment, rather the effect of assignment and receipt of treatment, which I don't think was very wise.

37:36.960 --> 38:00.960
But the idea intention to treat is you analyze the data, the way the units got assigned, not what they took. And I think this is a very relevant comment now, because I view the recent Nobel Prize in economics, which is given to like co authors, angriest and

38:01.960 --> 38:11.960
David Carr, for actually implementing a study in the non randomized interesting inference in the non randomized study.

38:11.960 --> 38:24.960
This the idea of non compliance because that conscious units have had a habit of not agreeing with what they were randomized to not agreeing to doing what they were randomized to do.

38:25.960 --> 38:32.960
And the interesting thing is, but there's no use of these Fisher name and insights, or the notation and non randomized trials.

38:32.960 --> 38:41.960
It wasn't used at all, I think, until I proposed using it in 1974 paper.

38:41.960 --> 38:52.960
So for example if you look at the 1964 US Surgeon General support report on cigarette smoking and lung cancer, very important study.

38:52.960 --> 38:56.960
Everything was done by regression.

38:56.960 --> 39:00.960
In fact, ordinary least squares regression.

39:01.960 --> 39:16.960
Nothing. No, no use of the insights from from randomized experiments in this giant observational studies. In fact, regression was used everywhere epidemiology economics, social science psychology sociology.

39:16.960 --> 39:26.960
They all used regression with the potential outcomes replaced by the observed outcome with an indicator w i for each unit for which treatment they got.

39:26.960 --> 39:40.960
So they would do a regression of the observed value is why I ops as the observed value for each unit on an indicator variable w i and covariance.

39:40.960 --> 39:50.960
But this notation violates this this principle that I learned when I was a kid that you separate the science from what we do to learn about the science.

39:50.960 --> 40:04.960
So this why obs notation would became completely prevalent for dealing with with non randomized data, starting actually in the at the end of the 19th century, we're doing regression.

40:04.960 --> 40:17.960
There's an old paper by you on how you do this, using this observational notation and regression.

40:17.960 --> 40:21.960
Next slide please.

40:21.960 --> 40:40.960
Well, it turns out this sequence of papers came to be called the Ruben causal model to the name Paul Holland made up in his 1986 Journal of the Wilson of the jazz American statistical association paper, Ruben causal model, which I sort of the proposed this model

40:41.960 --> 40:56.960
and wrote 1974. There's a follow up paper and proceedings and 75 paper and missing data where I, I, I called missing data in a randomized experiment should be treated as missing data.

40:56.960 --> 41:08.960
Even from a Bayesian point of view, if I frequent this point of view, talked about how randomization inferences basically based on missing data followed up with another publication 1977.

41:09.960 --> 41:20.960
Probably the, the, the, in some sense the most complete paper was in 1978 in the annals of statistics was on Bayesian difference for causal effects.

41:20.960 --> 41:40.960
The contributions of this paper, I think some of it were, were, were beyond what, what Neyman did but certainly built on, on, on Neyman's notation to say the notation of potential outcomes, define causal effects in all situations, not just in randomized experiments.

41:40.960 --> 41:47.960
The idea that you can separate the science from what you do to learn about the science says you first define what you're trying to estimate.

41:47.960 --> 41:56.960
And in an observational study you're trying to estimate the same thing you're trying to estimate in a randomized experiment, you just intervening in different ways.

41:56.960 --> 42:09.960
Actually Neyman when I, when I had an office next to his at Berkeley disagreed, he said no, you can't talk about causal effects without randomization.

42:09.960 --> 42:20.960
You have to have randomization even talk about causal effects, otherwise it's too speculative. So instead, Neyman said let's talk about the stars.

42:20.960 --> 42:32.960
Another confusion of his paper was that you needed an assignment mechanism for causal inference. So an assignment mechanism is just the generalization of randomization.

42:32.960 --> 42:52.960
So it's the probability distribution for the treatment indicator, given the covariates and the potential outcomes. So that's probability of w, given covariates x, and potential outcomes y0, y1 with the general dependence on y0 and y1.

42:52.960 --> 43:04.960
And I defined unconfounded in, I guess, 74 I think, to be when this assignment mechanism doesn't depend upon the potential outcomes.

43:04.960 --> 43:11.960
And you know it doesn't depend upon the potential outcomes in randomized controlled trials, because you did the randomization.

43:11.960 --> 43:22.960
And it was, you couldn't see the potential outcomes, and you didn't use them, and you didn't use any unobserved predictors of potential outcomes.

43:22.960 --> 43:40.960
The confusion inference I defined this concept called the ignorable, where the assignment mechanism just depend upon observed values, for example any sequential randomized controlled tiles, so that in this potential dependence on why

43:40.960 --> 43:54.960
the observed value as in you doing a play the winner randomization. So you'd randomize the first unit, but after the after the first unit you change the odds and getting active treatment versus control treatment.

43:54.960 --> 44:06.960
So it, the odds are in favor of the more successful treatment based on the earlier units. So here why are you first all the observed values of y0 and y1.

44:06.960 --> 44:15.960
And of course in Bayesian inference, you have to model the science, in addition to the assignment mechanism.

44:15.960 --> 44:29.960
Now, and I called the ignorable because that factor in the likelihood for the posterior distribution can be ignored to get the same inference and get the same answer.

44:29.960 --> 44:42.960
So the, so the assignment mechanism creates missing and absurd potential outcomes. I say an artistic touch is needed here, because all models are wrong.

44:42.960 --> 45:01.960
There's a question that goes back to George box, or before that von Neumann even even said that the real world is much too complex for anything but simplified models. So if, if I know if the math was too hard for von Neumann, and who's obviously too hard for

45:02.960 --> 45:21.960
So that the idea, the idea of, of models that we use in science are always wrong, they're always too, too, too simple. I say an artistic touch is needed here, and there's a great Picasso quote I don't know if I have it here but it's

45:21.960 --> 45:29.960
he's talking about computers and Picasso said computers are worthless. They only give answers.

45:29.960 --> 45:42.960
Picasso's point I think is, is a great expression of the field of statistics thing that that makes statistics artistic is you have to posit models.

45:42.960 --> 45:53.960
And, and you know these models are wrong, but they're, they're, they're working hypotheses, and that they have to be discarded as, as you go along.

45:53.960 --> 45:57.960
Please.

45:57.960 --> 46:05.960
So the fundamental problem facing causal inference which is an expression that made up in the 1975 paper.

46:05.960 --> 46:08.960
It's a missing data problem.

46:08.960 --> 46:25.960
And, you know, Paul Holland had had a more eloquent way of saying this in his jazz of paper 1986, but it's an expression that that I, well, that was in this 75 paper.

46:25.960 --> 46:38.960
So for each unit, and here we have an example with any units, and either a control treatment zero, or an active treatment one, you get to observe either why one or why zero, you can observe both of it.

46:38.960 --> 46:48.960
And the, and the random assignment of active or control means a representative sample of why I one will be compared to represents that of why I zero obvious.

46:48.960 --> 47:05.960
And once you put down the notation, it's obvious notation with the question marks and the checks. It's obvious, but you have to have names notation to do that, and you have to have the idea. This notation applies beyond randomized experiments.

47:05.960 --> 47:17.960
And, and then you have to have the ideas you need a model on this assignment mechanism that creates missing and observed data, in order to draw inference about the missing potential outcomes.

47:17.960 --> 47:41.960
I think that that was an important insight. And in fact, I think it's the basic fundamental idea behind this year's Nobel Prize in economics, because David card was actually doing cause inference in a study where an observational

47:42.960 --> 47:52.960
study where he had hypothetical randomization he can describe it that way, hypothetical randomization of fast food restaurants in New Jersey versus Pennsylvania.

47:52.960 --> 48:10.960
And he the hypothetical experiment, again not said this way but had to be on his mind was a coin was tossed to see whether the, with the restaurant was situated in New Jersey, or Pennsylvania, because New Jersey and Pennsylvania had different laws

48:10.960 --> 48:26.960
and different state laws on the minimum wage. And so the units in that experiment with a different fast food restaurants, and the hypothetical randomization was were they in Pennsylvania, or New Jersey, where there were different minimum wage laws.

48:26.960 --> 48:42.960
And that has important economic implications, and which eventually led to David cards Nobel Prize, and English and in Ben's use of these potential outcomes to describe the methods.

48:42.960 --> 48:58.960
You're both both very generous. I mean, I'm, I'm, I'm sort of jealous of it, but, but it was not I was not any more mistake for the Nobel Prize to do that, then a mistake and lots of the Nobel Prize had mistakes in that in that same sense.

48:58.960 --> 49:02.960
Okay, next slide.

49:02.960 --> 49:18.960
It's a mistake to do this for regression use of this regression goes back to the 19th century for causal inference, but it's a mistake to regress its observe value on an indicator for which treatment you got in and covariates.

49:18.960 --> 49:27.960
Why is it a mistake. Look at this notation. It loses the potential outcomes and the key Fisher naming concepts and using the observe value notation.

49:27.960 --> 49:33.960
Mixes up this why habs mixes up the assignment mechanism and the in science.

49:33.960 --> 49:49.960
And here, my insights, which built on Namens insight from the notation is, you want to separate the science here the, the, the wise from what you did to learn about the science the W's.

49:49.960 --> 50:10.960
This was, was critical to the 20th century insights, and here this mixed it all up again suppresses these key insights there are no missing data. So once you write this down, and you're regressing the observe value of why on the observed W's and the, and the observed covariates.

50:11.960 --> 50:29.960
Hmm, must be parameters well parameters are always missing because they're they're hypothetical. So but this this notation for observational its use and observational became standard and biostatistics economics, epidemiology everywhere, and even great statisticians and

50:29.960 --> 50:47.960
epidemiologists, for example, Fisher, I can't talk about these pay I don't have time, but I can find mistakes that Fisher made in dealing with secondary outcomes in randomized experiments.

50:48.960 --> 51:07.960
We made mistakes, Cochran made mistakes cornfield made mistakes, they confuse themselves in observational studies, because they're always in this observe value notation, and talking about doing regressions, the cornfield quotation that's on the next slide, I think it's particularly revealing.

51:07.960 --> 51:11.960
So next slide please.

51:11.960 --> 51:25.960
So the results on the way observational studies were quote handle, because there was no design phase, and they're instead confused analysis, and they, and they mixed up association versus causation came muddled.

51:25.960 --> 51:37.960
I think one of the real examples of that is it was called now a case control study and cases referred to people with a disease controls referred to people without the disease.

51:37.960 --> 51:54.960
And I much prefer the terminology case non case, because the control in a case control study is completely different than a control in a randomized experiment control in a randomized experiment is a person who's exposed to the control treatment.

51:54.960 --> 52:06.960
And in this case controls that a case control is somebody without the disease. So it's very confusing terminology.

52:06.960 --> 52:21.960
Because with case control studies, use the sampling mechanism is how you drew samples, but they're confounded sampling mechanisms in a case control study are confounded because they by definition depend upon the observed potential outcomes.

52:21.960 --> 52:37.960
In other words, what they did they typically, they looked at all the cases, all the people with the disease, and they compared them to non cases, people without the disease.

52:37.960 --> 52:47.960
So this is what you had to do when you're looking at a rare outcome, you almost had to do it because they're otherwise became too expensive to collect data.

52:47.960 --> 52:57.960
This is a quote from cornfield, who's the main epidemiologist on the smoking lung cancer.

52:57.960 --> 53:12.960
A direct quote from a paper that he wrote in 1959, we now consider the distinction between the kinds of inferences that can be supported by observational studies is non randomized studies, whether prospective or retrospective.

53:12.960 --> 53:24.960
And this case control prospective means you go forward by by looking at smokers and non smokers.

53:24.960 --> 53:31.960
Whereas a case control looks at lung cancer versus nonline cancer.

53:31.960 --> 53:44.960
And so the prospective studies, or retrospective and those but they're observational, and those that can be supported by experimental studies that there is, is a distinction seems undeniable.

53:44.960 --> 54:05.960
But it's exact nature is elusive. And I agree with it at that time it was elusive, because it didn't become formalized until I did it in 1977 by putting down an assignment mechanism, which embedded these two kinds of studies within one framework and could define what the benefit of

54:05.960 --> 54:16.960
the assumption was. It's, it's unconfoundedness is that the assignment mechanism cannot depend upon potential outcomes, because you did the assignment.

54:16.960 --> 54:26.960
But in a, an observational study where the prospective retrospective is unconfoundedness of the assignment mechanism is an assumption.

54:26.960 --> 54:38.960
There's a big difference between how well justified the assumption in a randomized experiment is risk the assumption in an observational study.

54:38.960 --> 54:41.960
Next slide please.

54:41.960 --> 54:51.960
So the starting conclusions of I mean the starting conclusions on causality. And that's why I hope this was a useful introductory lecture.

54:51.960 --> 55:08.960
So let's retain the key insights from the past key insights meaning insights from name and Fisher, but issue, get rid of confusion from the past, get rid of the fact that everybody's doing everything by regression, and they're replacing the potential outcomes by the observed

55:08.960 --> 55:11.960
outcome and trying to do the squares regression.

55:11.960 --> 55:22.960
So the important conclusion is to realize that the ideas between randomized controlled trials are extremely recent people have been talking about causality for thousands of years.

55:22.960 --> 55:31.960
Now which direction should I hunt for food, should I plant this variety or plant that variety, when we came out more an agrarian society.

55:31.960 --> 55:36.960
The ideas are extremely recent. These are 20th century ideas.

55:36.960 --> 55:54.960
Now we should certainly update statistical methods from in both design and analysis to take advantage of modern computing. So what I mean by design and modern, you know, take advantage of modern computing to take the advantage of modern computing to throw away bad randomized

55:54.960 --> 56:15.960
allocations, use machine learning ideas to draw thousands of randomized randomized allocations, and then compare balance on on zillions of covariates and understand the geometry behind that, and then throw away the bad allocations.

56:15.960 --> 56:29.960
I encourage mathematical precision, especially in notation and logical flow. So I don't care about mathematical precision in these in these stupid theorems about asymptotic balance and stuff like that.

56:29.960 --> 56:36.960
Take advantage of modern computing and stay with with finite sample methods as much as possible.

56:36.960 --> 56:57.960
And this precision in in thinking and logical flow can have critical consequences in in in current in challenging applications. For example, now in placebo effects. I think one of the really important areas of advancement can be can built on some

56:57.960 --> 57:09.960
ideas of placebo effects using this notation of potential outcomes, and for which I think the this this Nobel Prize was recently given.

57:09.960 --> 57:23.960
Okay, so I think I probably have run out of time if not more. It's four minutes after 10 is I get so I will let our host say how we should proceed.

57:23.960 --> 57:26.960
I thought we have the next slide.

57:26.960 --> 57:46.960
The last slide is newer general ideas. Okay, so this in here is really this idea propensity score for for for designing studies both experimental and observational studies, this idea that you, you, you summarize all zillions of covariates by a low

57:46.960 --> 57:57.960
three dimensional summary. So this is really is tied to machine learning ideas and use these both for designing experimental and observational studies.

57:57.960 --> 58:13.960
So this propensity score idea is due to this paper by Paul Rosenbaum, and I wrote in 1983 that you can design observational studies and propensity scores because they do not they are not a function of potential outcomes, they can be used in

58:13.960 --> 58:21.960
a much more important idea, then, then its use as an analysis tool.

58:21.960 --> 58:34.960
And re randomization experiments to avoid unlucky allocations, and this is an expanded template for observational studies.

58:34.960 --> 58:41.960
Another joint paper with us with a sequence of papers.

58:41.960 --> 58:55.960
Another really important idea is what I with Paul wasn't that Paul, constantly for a caucus and I called principal stratification, which generalize this idea of instrumental variables from economics.

58:55.960 --> 59:08.960
So I, I think this is with principal stratification. I think as as a basic idea, that's yet sort of got the Nobel Prize and I mean I'll make the tie there, even though formerly it didn't.

59:08.960 --> 59:28.960
I think it's a combination with with complications to deal with with not only non compliance, which is the instrumental variables, setting, but basically, is ruben and and, and, and, and Thomas paper and placebo effects.

59:28.960 --> 59:39.960
I recently wrote where the that that generalization is is is made using.

59:39.960 --> 59:54.960
Actually, this paper says that when you're doing randomized experiments is currently done for a new drug. For example, the way it's done the placebo control trials.

59:54.960 --> 01:00:02.960
It affects the placebo effects meaning that people think they're getting the active treatment, and that affects the outcome.

01:00:02.960 --> 01:00:16.960
And if you do a placebo controlled randomized trial, and now you prove a drug, when people have a are taking an approved drug, they know they're getting an active treatment when you fill a prescription farm.

01:00:17.960 --> 01:00:41.960
Retreated in practice, are getting the placebo effect. In addition, and what that leads to in practice is higher doses of drug that you then what you need for the outcome effect because the effect on the outcome you're already getting with the

01:00:41.960 --> 01:00:46.960
when you go to a drug store in in in you in you fill a prescription.

01:00:46.960 --> 01:01:06.960
So in practice, I think 80 or something like 50 to 60% of drugs as approved by FDA from placebo controlled trials are ratcheted down in the future based on observational data, saying the doses were too high, led to too many side effects.

01:01:06.960 --> 01:01:17.960
And that's approved for practice is the dose that was used in the placebo controlled trials, when giving that dose.

01:01:17.960 --> 01:01:27.960
The units the the patients with you observe is it in addition to the drug effect, you're getting from placebo effect.

01:01:27.960 --> 01:01:42.960
So, the, the, the drug that's being assigned is too high, because it's, it's, it's being assigned to get the effect, the real effect of the drug plus the placebo placebo effect.

01:01:42.960 --> 01:01:50.960
So this, this, this 2020 paper that I wrote that that's published in Andrews journal, I believe.

01:01:50.960 --> 01:02:08.960
So to me, it's the Japanese Journal, Kazuma Shumatsu's journal, actually, is about that, that, that the topic, have other things to submit to Andrews journal on on this topic, I think it's a very important topic.

01:02:08.960 --> 01:02:11.960
Okay, I think I'm finally done.

01:02:11.960 --> 01:02:12.960
Yes.

01:02:12.960 --> 01:02:16.960
And thank you for your patience, as we struggle with the slides.

01:02:16.960 --> 01:02:22.960
Thank you so much and sorry for the slides is being too excited to see you today.

01:02:22.960 --> 01:02:25.960
Very kind. Thank you.

01:02:25.960 --> 01:02:31.960
Okay, so the next part of the, of the seminar will be the photo session.

01:02:31.960 --> 01:02:39.960
Please all the guests to open your camera and then we're going to take a group photo.

01:02:39.960 --> 01:02:52.960
And then we're going to take a group photo and then please all the students and teachers to open your computer and then we're going to show you the camera and then we're going to take a group photo.

01:03:09.960 --> 01:03:24.960
We have like 17 pages of people who are here today, listening to your talk.

01:03:24.960 --> 01:03:25.960
That's nice.

01:03:25.960 --> 01:03:29.960
Yeah, we have like 500 people today.

01:03:29.960 --> 01:03:31.960
That's a lot of people.

01:03:31.960 --> 01:03:35.960
It is, we have crowds of guests here.

01:03:35.960 --> 01:03:41.960
Thank you so much for the wonderful talk.

01:03:41.960 --> 01:03:46.960
And also, sorry for the slides being stumbling.

01:03:46.960 --> 01:03:48.960
That's okay.

01:03:48.960 --> 01:03:55.960
But, but it's very, very nice to have you here. And now let's move on to the discussion session.

01:03:55.960 --> 01:03:58.960
Please turn off all of your cameras.

01:03:58.960 --> 01:04:04.960
I saw a lot of friendly and familiar faces here.

01:04:04.960 --> 01:04:07.960
Please turn off the camera.

01:04:07.960 --> 01:04:14.960
We're going to send the photo to the UK.

01:04:14.960 --> 01:04:16.960
Okay, thank you.

01:04:16.960 --> 01:04:17.960
Thank you.

01:04:17.960 --> 01:04:19.960
Please turn off your camera.

01:04:19.960 --> 01:04:30.960
And then we can continue the next part.

01:04:30.960 --> 01:04:36.960
Oops.

01:04:36.960 --> 01:04:39.960
Can you see my share?

01:04:39.960 --> 01:04:45.960
You can't see it.

01:04:45.960 --> 01:04:48.960
And now it's the discussion session.

01:04:48.960 --> 01:04:54.960
I'm sorry if my computer is not working really well today.

01:04:54.960 --> 01:05:04.960
And let us welcome the three speakers.

01:05:04.960 --> 01:05:09.960
Let us welcome all three important guests for today's session.

01:05:09.960 --> 01:05:13.960
There will be Professor Zhou Xiaohua.

01:05:13.960 --> 01:05:15.960
I saw your camera is on.

01:05:15.960 --> 01:05:17.960
Andrew Joe.

01:05:17.960 --> 01:05:27.960
And Professor Zhao Xiliang.

01:05:27.960 --> 01:05:32.960
And Professor Cui Po.

01:05:32.960 --> 01:05:35.960
Okay.

01:05:35.960 --> 01:05:49.960
I'm going to comment at Rubin's talk and we are going to have an open discussion session. So you may, you may each one ask questions to Professor Donald Rubin.

01:05:49.960 --> 01:05:58.960
And now I'll pass the mic to Professor Zhou Xiaohua from Peking University.

01:05:58.960 --> 01:06:00.960
Yeah, hi, Don.

01:06:00.960 --> 01:06:04.960
Nice to hear you talk again.

01:06:04.960 --> 01:06:09.960
I've never heard this talk before, but every time I will gain insight from your talk.

01:06:09.960 --> 01:06:21.960
Yeah, yeah, I, I think one of the reasons is that I, I use a limited number of slides. And so the talk is always slightly different.

01:06:21.960 --> 01:06:35.960
Anyway, this is very great talk or just common few sense and also maybe just discuss a little bit further work actually based on principle strata ideas.

01:06:35.960 --> 01:06:42.960
I think it's a very important idea. And by the way, so I have a note down for like over 25 years old friend of mine.

01:06:42.960 --> 01:06:54.960
And actually my research in causal inference actually introduced by Don to me about, I think I can remember when I was doing post that hover or when that started.

01:06:54.960 --> 01:07:00.960
But, but one work I had done was done is about encouragement design. I don't know if you remember.

01:07:00.960 --> 01:07:03.960
Andrew, your, your voice is breaking up.

01:07:03.960 --> 01:07:05.960
Oh, can you hear me.

01:07:05.960 --> 01:07:06.960
Yeah, I can hear you now.

01:07:06.960 --> 01:07:07.960
Okay.

01:07:07.960 --> 01:07:10.960
Yeah, I just, yeah, I just said look.

01:07:10.960 --> 01:07:11.960
Okay.

01:07:11.960 --> 01:07:12.960
Yeah, go ahead.

01:07:12.960 --> 01:07:20.960
So what I say that my research in causal inference actually introduced you to me about 25 years ago.

01:07:20.960 --> 01:07:25.960
Yes, about a study about the randomized encouragement design for flu shot.

01:07:25.960 --> 01:07:29.960
Actually, that's a paper we did with Emmons and his students.

01:07:29.960 --> 01:07:30.960
That's right.

01:07:30.960 --> 01:07:31.960
You remember that.

01:07:31.960 --> 01:07:32.960
Yes.

01:07:32.960 --> 01:07:40.960
And then actually that's a great work and then got the award from international Bayesian society.

01:07:40.960 --> 01:07:50.960
Yeah, actually, so I very grateful actually you are introducing me to the field of the causal inference and then, and then we'll have done some work so I want to mention a little bit.

01:07:50.960 --> 01:08:13.960
The, the, the Ruben framework for you call the Ruben causal model, I think I like that model, because I think one thing I don't probably dimension maybe to pride in his talk is I think is a Ruben causal model actually clearly distinguish between estimate and estimation, I think that's actually very crucial.

01:08:13.960 --> 01:08:24.960
I mean, bio statistic in the medical field. So what I mean is the estimate is quantity is determined by science. So scientific question you want to ask.

01:08:24.960 --> 01:08:41.960
So that is given by estimate, which has nothing to do with the model with assumptions I think that should be very clear, because, because particularly the causal inference became more popular in computer science, in artificial intelligence, and in other fields.

01:08:41.960 --> 01:08:48.960
I feel like when I read those papers, it's not clear to me what their estimate with what they're trying to do.

01:08:48.960 --> 01:09:00.960
They're just through all the jargon which we talk about the mathematics stuff and then the, the, the location really get confused, because our audience mostly actually from the computer science I think the field.

01:09:00.960 --> 01:09:13.960
I think it's very glad that actually down you give a talk in this audience is good. Yeah, because I think this audience most familiar with the pearls, the graphic model, I think my understanding is, but I think it's good.

01:09:13.960 --> 01:09:29.960
I mean, the pearls graph model I think has their own usage, has own usage. But on the other hand, I think it's a woman, we need to clear what are the scientific questions, and what are the estimates with estimate is the parameter, which defines scientific questions.

01:09:29.960 --> 01:09:41.960
I think if you don't make that clear everything well, well, well, I think we'll go around afterwards. Then after you have an estimate so clearly defined and everybody understand, then you talk about estimations.

01:09:41.960 --> 01:09:54.960
So where does assumption come in, because the, the estimation whether you can estimate estimate dependent on the data you have, you have a, if you have a randomized trial data or you have a long compliance, or you have truncation by death.

01:09:55.960 --> 01:10:09.960
So, so you have to very clear, clear for a study the data you have to estimate estimate, and then also clearly to make your assumptions, which to make estimate estimate more.

01:10:09.960 --> 01:10:16.960
So that's a crucial I think that part of the causal inference to say the parameter you have is whether it's estimable.

01:10:16.960 --> 01:10:33.960
So, is the parameter you have can be estimated consistently based on the data you have. I think that part I think is part of the, I think the key, I mean maybe don't agree in the causal inference, compared with other field, the, as an ability issues.

01:10:33.960 --> 01:10:43.960
And then the third I think is the important is to check your assumptions you made in your causal inference, but some something make sense some something doesn't.

01:10:43.960 --> 01:10:50.960
So if the assumption doesn't make no sense in the in practice, in practice, then I don't think your causal inference make any sense.

01:10:51.960 --> 01:11:04.960
So that's I learned from down actually to work on those the medical field, the problem. So I want to also mention a little bit about the, the some extension us actually some application of the principle certification down mentioned just a little bit.

01:11:04.960 --> 01:11:09.960
So actually that's the he mentioned about this dude.

01:11:10.960 --> 01:11:20.960
So that's one of the students actually work with us with work with me and down on the also around my encouragement design. So we have actually two paper on that area.

01:11:20.960 --> 01:11:34.960
So I want to mention, based on the principle certification that I have done some work actually also motivated by the dance, the principle. So why is truncation by that's how do you make causal inference when you have a truncation by that.

01:11:34.960 --> 01:11:48.960
So what's our problem is, is this if you interested in some parameters, like quality of life in five years after you take the treatment by the people might die one year after receive the treatment.

01:11:48.960 --> 01:11:56.960
So, so for the people who die, what are their outcomes. So actually that's a very interesting issue right now.

01:11:57.960 --> 01:12:06.960
Can you still treat as a missing data, which some people did to say, even they die this is still missing the quality of life five year after the die.

01:12:06.960 --> 01:12:12.960
So that's actually might be a problem because I don't know what the quality of life for that people.

01:12:12.960 --> 01:12:29.960
So that's actually the big issue. So that's actually show the principle of the stratification really work in the cities. So, so we have done some work to how do you make causal inference when when when you have a truncation by death.

01:12:29.960 --> 01:12:43.960
This is very important because that one is commonly occur in medical research, because for most of the cohort study large cohort study you always have some people who died during the study.

01:12:43.960 --> 01:12:53.960
So how do you deal with us and and then they don't maybe have a time to talk about that truncation by that but that's actually the very important applications in medical research.

01:12:53.960 --> 01:13:02.960
But another area I think I also get a lot of attention recently is the precision medicine to say how do you do the causal inference in precision medicine.

01:13:02.960 --> 01:13:14.960
So you might say what is a precision medicine probably in this audience may not familiar. So precision medicine is like this, as we know in the in the medicine, there's no one drug can treat all patient well.

01:13:14.960 --> 01:13:27.960
So, so you're so the precision medicine sometimes also called personalized medicine. So that means you may want to tailor your treatment based on characteristic of the patients.

01:13:27.960 --> 01:13:42.960
So maybe let's say for the cancer patients, you may have the chemotherapy or you may have surgery, but for some people is better to use chemo than than other people based on the genetic information of the subject.

01:13:42.960 --> 01:13:56.960
So that area actually is called a heterogeneous treatment fact. So that means, so the in the literature, the talk down mentioned, mostly focused on, I think the overall average treatment fact to the whole population.

01:13:56.960 --> 01:14:01.960
But sometimes you might might be interested actually is the sub population.

01:14:01.960 --> 01:14:18.960
So maybe you want to condition on covariates and then look at the causal effect in the sub group. So, so they call the heterogeneous treatment fact, but the principle down just mentioned about the Ruben causal model can still apply, but you have to do more work to make work.

01:14:18.960 --> 01:14:25.960
So that's called a heterogeneous treatment fact. Actually, I think this is still right open area in medicine.

01:14:26.960 --> 01:14:35.960
And then particularly when you have observational data, which have so one of the difficulty with observational data in causal inferences is our major confounders.

01:14:35.960 --> 01:14:49.960
So the reason I think the card and then even got lower prices, they actually try to use the instrument variable to crack for the major confounders, which is they call the late.

01:14:49.960 --> 01:14:55.960
So I think, later on I think we call the complier average cause of that right, right.

01:14:55.960 --> 01:15:01.960
Yeah, but so maybe the late is the first proposed by ambulance and the angry.

01:15:01.960 --> 01:15:13.960
Yes. Yeah, okay. And then later on I think we actually in our medical field we, we, we prefer the CSE, the complier average cause of that.

01:15:13.960 --> 01:15:32.960
Okay, I think that the causal inference is getting more and more popular. And I think the current research in causal inference is how do you apply the clear principle like Ruben model to more complicated problem, like heterogeneous treatment

01:15:32.960 --> 01:15:41.960
like big data, like observational data, like communication by death, or like right now is artificial intelligence.

01:15:41.960 --> 01:15:59.960
Like, like, like, like in this section of the most like say how do you apply causal inference to, to the reinforcement learning to recommend, recommend, recommending systems recommendation systems, which is can be applied, because actually they're doing that down I

01:15:59.960 --> 01:16:07.960
wasn't aware about that the in computer science they apply causal inference to almost all field of the computer science.

01:16:07.960 --> 01:16:24.960
But I mean we look closely is I think the framework is not still not very clear actually one of the paper I'm working on with my student is how do you currently define the cause of framework for recommend recommend dating system system, like the how do you recommend

01:16:24.960 --> 01:16:37.960
the causal inference principle, which is a doing that all the time, because they try to introduce causal inference in, in that, in that framework to make more stable or more.

01:16:37.960 --> 01:16:47.960
They call the robust predictions. So, so as the third field I think is in the causal inference is that is a causal prediction. So that's a little bit different from the causal effect.

01:16:47.960 --> 01:17:01.960
It's very popular those days in the science to say, how can you make prediction, which has some causal property, I think, and no one I think needs a lot of work to be done, I think that field. Yeah.

01:17:01.960 --> 01:17:05.960
So, so, Professor Joe, Professor Joe we have limited time.

01:17:05.960 --> 01:17:14.960
So I will more maybe once I don't comment a little bit is what do you think about the causal predictions, do you have any thought on that.

01:17:14.960 --> 01:17:21.960
I, I should do, I can say, one or two minutes now.

01:17:21.960 --> 01:17:39.960
I think that's probably easier for the audience and for me as well, instead of trying to remember everybody if I so if I make a comment. So I very much appreciate Andrew's comments and as, as he said we've known each other for decades.

01:17:39.960 --> 01:17:41.960
We've written papers together.

01:17:41.960 --> 01:18:01.960
And I think that that's your point about the, the, the being precision medicine, and the problem of censoring by, by, by death, think the, it was the censoring by death I think has been underappreciated for for years.

01:18:01.960 --> 01:18:15.960
And I think the idea of thinking about sensing, sensing due due due to death as creating a particular principle straight up where they can always survivors, the subgroup of people who would survive.

01:18:15.960 --> 01:18:27.960
And so you try to do it do a study, and you want to try to separate the outcome of death from the outcome of quality of life, I think with many serious diseases. That's, that's an important question.

01:18:27.960 --> 01:18:31.960
These otherwise you sort of get the wrong answer.

01:18:31.960 --> 01:18:41.960
Because the, so how if you want to look at the effect of an intervention on quality of life. What do you do with people who died.

01:18:41.960 --> 01:18:58.960
Well, if you just throw them out, then you can get completely the wrong answer. If, especially if the, if, if one treatment kills people who are weak, and then the other treatment looks better because it lets them survive.

01:18:58.960 --> 01:19:11.960
And it's just, you've got to worry about, about both outcomes, the death outcome, and the quality of life outcome. And I think that there's a lot of work to be done there.

01:19:11.960 --> 01:19:26.960
I, I, I second Andrew's comment about that, that this perspective has, has a lot of, of things to offer current studies and in, in medicine where with those are critical attributes.

01:19:26.960 --> 01:19:40.960
Also the comment about precision medicine. Certainly, I'm on the word that the idea of precision medicine is especially important when treating patients, you don't want to, when it when it treat everybody the same.

01:19:40.960 --> 01:19:56.960
But one of the ideas of precision medicine is you want to obviously have a different treatment for different people. And the, and there the complication is principal straighter are only partially identified.

01:19:56.960 --> 01:20:05.960
If you, if, if, but there, but here's where, where, where the role of covariates can be really important.

01:20:05.960 --> 01:20:23.960
And, and I think there's a, probably the most, maybe the, maybe the most relevant paper is, is one that on this topic is one that I wrote with for me to me Ellie, and some of her students, her Italian students.

01:20:23.960 --> 01:20:39.960
Because here the, the words is published, I don't, I don't remember right, right, right now. But the, but the idea is that you can, the covariates help to predict which principal straighter you're in.

01:20:39.960 --> 01:20:51.960
And obviously the medicine, the principal straighter help define which treatments work for which sub subgroup of patients.

01:20:51.960 --> 01:20:57.960
And I think that's a terribly important idea.

01:20:57.960 --> 01:21:03.960
And maybe I'll stop there to make sure we have time, because I can always come back at the end of it.

01:21:04.960 --> 01:21:12.960
Actually, Professor safe home is also very interested in this question. Professors, say, do you want to make a comment.

01:21:12.960 --> 01:21:14.960
Yes, hi.

01:21:14.960 --> 01:21:22.960
Hi, this is punctually here I'm in the computer science department in Tsinghua University. Actually, we are in the same university now.

01:21:22.960 --> 01:21:24.960
Yes.

01:21:24.960 --> 01:21:33.960
There is some problem with my camera so next time I hope that we can, we can meet face to face.

01:21:33.960 --> 01:21:42.960
My question is that actually, in the recent years, you know that there is a very obvious trend that in the machine learning community.

01:21:42.960 --> 01:21:56.960
There are many researchers, especially very, I mean, famous researchers in this field to start to talk about causality, right, we need to embrace causality into the machine learning.

01:21:56.960 --> 01:22:04.960
Actually, from the 2016 we just start to, to think about so what's the limitation.

01:22:04.960 --> 01:22:22.960
There are some problems that today's machine learning framework cannot solve in the end. Actually, we find that there are several problems like explainability and also the stability problem, I mean the generalization to out of distribution and also fairness issues.

01:22:23.960 --> 01:22:39.960
We think that when we choose back, all of these problems are the root cause of these problems is, is just that in the learning framework we, we didn't even try to differentiate the causality and also the correlation.

01:22:39.960 --> 01:22:49.960
So, because in the, in the big data we just absorb a lot of spurious correlations in our model so that directly lead to these problems.

01:22:49.960 --> 01:23:06.960
So, I just want to listen to your thoughts and comments on this trend. And also, the second question is, when we try to do this when we try to embrace the causality into the learning.

01:23:06.960 --> 01:23:19.960
Some researchers just use, for example, the structure causal model, and we, our group actually borrow a lot of ideas from the Rubens causal model the potential outcome framework.

01:23:19.960 --> 01:23:27.960
So, actually, we are not that confident because we are just a halfway.

01:23:27.960 --> 01:23:48.960
We stand in the, in the machine learning community we just a halfway with respect to the causal inference community so we just want to listen to your advice from your perspective so how can the potential Rubens causal model help in the machine learning tasks.

01:23:48.960 --> 01:23:49.960
Thank you.

01:23:49.960 --> 01:24:18.960
Sure. Yeah, I'll say just a couple of sentences now about this. I think one of the critical things and I've over the years I've done a variety of consulting and very often the main effort is at the front, meaning at the beginning, when trying to understand exactly what the, what the question is being asked and try to get

01:24:18.960 --> 01:24:45.960
investigators away from talking about very theoretical things and get them to address what they can do. So, if you're talking about causal inference in machine learning, computer science, real world, what are the collections of interventions

01:24:45.960 --> 01:25:01.960
that you can implement. I don't want to talk about the theory, because the theory will will will will follow from the question that you're trying to address. If, if your eventual answer can't help doing what you want to do.

01:25:01.960 --> 01:25:03.960
It's worthless.

01:25:03.960 --> 01:25:17.960
I think that that's one of the, the important ideas from this 20th century ideas of experimentation is you should try to experiment on things that you can actually do.

01:25:17.960 --> 01:25:26.960
So, in the, in the computer science world, what can you actually do, what are the intro and interventions that you can implement.

01:25:26.960 --> 01:25:29.960
Now,

01:25:29.960 --> 01:25:46.960
I'm thinking about that implement in the context of a randomized experiment. What are the, what are the interventions that you can actually create and, and do, and then what are the outcomes that you want to achieve.

01:25:47.960 --> 01:26:04.960
Now, I think the internet kind of experimentation is more far more complicated than the classical ones for one primary reason. Everybody's interfering with everybody else.

01:26:04.960 --> 01:26:20.960
And it's, it's, it's very hard to get discrete non interfering units in, in, in, even in, even if you can do the randomization.

01:26:20.960 --> 01:26:34.960
And there's, there's only one real study that I've been actively involved in, which is something that I didn't refer to here at all but it has to do with.

01:26:35.960 --> 01:26:54.960
Actually, it's, I guess it's, it's partially confidential, so I probably shouldn't say too much about, but it has, it worries about the second order causal effects that you get on the internet, where, where the units of the quote experimentation are not very independent.

01:26:54.960 --> 01:27:11.960
And so I guess my general advice to people who are thinking about in terms of internet experiments is to decide, first of all, what the interventions that are possible to do, and then how to deal with the interference.

01:27:11.960 --> 01:27:27.960
And I think that's a wide open area and how you, how you deal with interfering units units who are communicating, because the, the network on on the internet is, is so incredibly vast.

01:27:27.960 --> 01:27:46.960
And you, and you don't, and depending upon how what the links are between the different nodes in the network, you get, you have to get a different type of, of, of inference and a lot of work has to be done there.

01:27:47.960 --> 01:27:53.960
I, I don't know whether that as you question at all, but those are my fast thoughts on it.

01:27:53.960 --> 01:28:04.960
Thank you, thank you. And if the time, time allows, we may have more discussions, just the host. Yeah.

01:28:04.960 --> 01:28:24.960
You just mentioned about the treatment and how it is important in the causal inference. We actually have Professor Joe Silan who has written, written the most one of the most popular textbooks in econometrics and talking about the policy treatment and policy impact evaluation.

01:28:24.960 --> 01:28:31.960
So let us welcome John Silan to give some comments.

01:28:32.960 --> 01:28:42.960
Actually, can you send me a title for this book, if, especially if it's in English.

01:28:42.960 --> 01:28:45.960
Okay, I'm going to send you right now.

01:28:45.960 --> 01:28:47.960
Okay, good.

01:28:47.960 --> 01:28:52.960
Yeah, we're like, we're like, I also like to see you too.

01:28:52.960 --> 01:29:03.960
I prefer Ruben and thank you very much for the great lecture about the history of the Ruben called model.

01:29:03.960 --> 01:29:10.960
The following poll model is actually the basis for calling for the empirical economics.

01:29:10.960 --> 01:29:37.960
Economics usually use random control trial and a natural experiment, natural experiment, trying to identify color effects in economics, especially in liberal economics, which so called the credible revolution in empirical economics by anguish.

01:29:37.960 --> 01:29:48.960
Especially Ruben called model is, is especially suited to do policy evaluation in economics.

01:29:48.960 --> 01:29:55.960
After implanting on government policy, governments usually want to know the effects of the policy.

01:29:55.960 --> 01:30:07.960
So Ruben Paul model played a great role in those kinds of research questions. Ruben called model can perform the research question and make the evaluation.

01:30:08.960 --> 01:30:25.960
There is another kind of problem in economics. That is, we saw the economic phenomena, and we want to find out the causes of the phenomena or the problem.

01:30:25.960 --> 01:30:39.960
For example, in 2008, you know, it's happened a world economic crisis. So economies want to know what's the reason called this crisis.

01:30:39.960 --> 01:30:57.960
So in this case, I have a question for Ruben is that how Ruben called model play a role in those kind of problems. This is just like another direction from

01:30:58.960 --> 01:31:20.960
from policy evaluation is from manipulation to find out the color effects. And this probably is a try to reverse we will see effects, how to find out the reasons can you give

01:31:20.960 --> 01:31:28.960
us some sort of problem.

01:31:28.960 --> 01:31:45.960
Sure. Yeah, I think what you're pointing out is a really critical point that actually arises, especially in philosophy.

01:31:45.960 --> 01:32:00.960
If you look at sort of historically at at some of the literature on causal inference, a lot of it is devoted to finding the cause of an effect.

01:32:00.960 --> 01:32:21.960
So you got sick what caused that. And there is an important distinction between searching for the cause of an effect, which is I think, really a descriptive question, versus the effects of causes in other words.

01:32:21.960 --> 01:32:35.960
So think about randomized experiments. What you're randomizing are the causes, and then what you can observe are what the effects of those causes, those interventions are.

01:32:35.960 --> 01:32:55.960
As, as we look at data, what we see are a state, you know, the current state of affairs. And here we were constantly drawn to wondering, ah, what caused that effect.

01:32:55.960 --> 01:33:10.960
And in fact, in my mind, one of the revolutions of the 20th century, this experimental revolution, and the cause of effect revolution is that's almost an unanswerable question what the cause of an effect is.

01:33:10.960 --> 01:33:21.960
Let me give you a trivial example that they've used for years. So somebody died dies of lung cancer.

01:33:21.960 --> 01:33:32.960
First person is ah, he died of lung cancer because he smoked two packs of cigarettes a day from the time he was 14 years old.

01:33:32.960 --> 01:33:43.960
And the next person says, well, that's true. But the reason the reason he spoke, he spoke two packs of cigarettes a day was his parents were both heavy smokers.

01:33:43.960 --> 01:33:53.960
And each of them smoked two packs of cigarettes a day. And there are always cigarettes around the house, even when he was a young teenager.

01:33:53.960 --> 01:34:03.960
Other person says, ah, well, that's true. But the reason both of his parents smoked is that his, their, their grandparents smoked.

01:34:03.960 --> 01:34:13.960
And they came from the old country and the old country, everybody was a smoker. So they were smoking all the time. They smoked three packs a day, four packs a day.

01:34:13.960 --> 01:34:22.960
They, they're like eating cigarettes. In fact, one of the relatives came from Turkey, where everyone smoked unfiltered cigarettes all the time.

01:34:22.960 --> 01:34:34.960
So the real cause was that, that his grandparents smoked cigarettes, and therefore his parents were doomed to smoke cigarettes. So the real cause was his grandparents.

01:34:34.960 --> 01:34:38.960
And of course that can go on indefinitely.

01:34:39.960 --> 01:34:57.960
I think one of the revolutions of the 20th century and the idea of experimentation was a change in focus from looking for the cause of something that exists to the effect of causes, the effect of interventions, where you can formulate the

01:34:58.960 --> 01:35:06.960
question and the answer in terms of actual interventions that you can do in terms of an assignment mechanism.

01:35:06.960 --> 01:35:19.960
Ideally a randomized assignment so that it's unconfounded or some other, but the, but these other, these other questions, the effects, the causes of effects of things that is descriptive.

01:35:20.960 --> 01:35:36.960
They may be hypothetical, maybe able to think, and I've written about that in various places. For example, there was one discussion I wrote, I think with the title, which ifs have causal answers.

01:35:36.960 --> 01:35:46.960
So what kind of question that would be is, if I, if I remove the sun from the solar system with the planets still go in their orbits.

01:35:46.960 --> 01:35:53.960
There's no causal like, like, like questions, but the intervention of removing the sun, we have no idea what that even means.

01:35:53.960 --> 01:36:10.960
So I think it's important when formulating questions in practice for things that you really want to understand is you have to formulate them in the, in the context of what interventions that you can do, because then you try to choose the

01:36:10.960 --> 01:36:17.960
one that's most favorable to you.

01:36:17.960 --> 01:36:20.960
Does that help in any way, I hope so.

01:36:20.960 --> 01:36:39.960
So that means you have come to some ideas first, then you can manipulate is or interface data to find some correct and very often the manipulation that you can do, you will find that you want to do is you really can't do it.

01:36:39.960 --> 01:36:51.960
And then I think the focus should turn to interventions that you can do, because if you're a practical person, why should you spend time thinking about something you can't do.

01:36:51.960 --> 01:37:03.960
Right, it's like saying, ah, if I live to 2000 years old, when I'm 1000 years old, I will, I will start in the study of medicine.

01:37:04.960 --> 01:37:15.960
Why spend time thinking about that you're not going to live to 1000 years old. So why so why have a debate with other people on what you're going to do when you're 1000 years old.

01:37:15.960 --> 01:37:17.960
It's not worth the effort.

01:37:17.960 --> 01:37:24.960
So you should, you should expand effort thinking about things that can be done.

01:37:24.960 --> 01:37:41.960
Maybe maybe that's too applied for, for some philosophers, but it's, I guess, I've, I've come to the conclusion that people spend too much time worrying about things that they can't change.

01:37:41.960 --> 01:37:47.960
Okay, thank you.

01:37:48.960 --> 01:38:01.960
We actually have also a few questions from the audience that they want to see, Professor Rubins comments, then, then we'll first answer the audience questions and then we'll come back to discussion if we have more time.

01:38:01.960 --> 01:38:03.960
Okay, that's fine.

01:38:03.960 --> 01:38:14.960
Okay, so the first question is, what's your opinion about the future development of the potential outcome framework. What's your expectation.

01:38:15.960 --> 01:38:31.960
Ah, well, my expectation is, it's the, it's the correct basic framework, but they're all sorts of aspects of the real world that it hasn't been modified to address.

01:38:32.960 --> 01:38:52.960
Because it, so for example, this idea that Andrew talked about of precision medicine, I think is is an important one, because the, and the role of covariates, I think is underappreciated there.

01:38:53.960 --> 01:39:06.960
If you do randomized experiments one attitude used to be, you should pretty much ignore covariates, you should do large simple, simple trials, for example, and I think that's wrong.

01:39:06.960 --> 01:39:17.960
Because in order to do anything precise with being precise means at least to me is to be able to to condition on more and more descriptors of individuals.

01:39:18.960 --> 01:39:36.960
And so the idea there is you can't just do large simple trials. And actually I'd be interested in hearing what would Andrew has to say about that because certainly there's been an emphasis in in in recent years on doing this large simple

01:39:37.960 --> 01:39:56.960
where you don't worry about enforcing balance on many covariates, you just the simple refers to just complete randomization, because the basic theory of Fisher and Neyman is averaging over everything, but when you actually going down to making decisions, decisions

01:39:56.960 --> 01:40:00.960
have to be very conditional.

01:40:00.960 --> 01:40:03.960
What do you have to say about that.

01:40:03.960 --> 01:40:08.960
Well, I think I will pull wait because because the audience probably want to hear from you.

01:40:08.960 --> 01:40:09.960
Okay.

01:40:09.960 --> 01:40:12.960
And now if the time allow, I will see if you will.

01:40:12.960 --> 01:40:15.960
Okay, fine.

01:40:15.960 --> 01:40:31.960
So, so this basic answer is that even in design, we now have the computational tools to make designs far more precise than then can be achieved just by complete randomization.

01:40:31.960 --> 01:40:43.960
And an important aspect of that is this idea of rerandomization randomization. So you look at at zillions of possible randomized allocations.

01:40:43.960 --> 01:40:59.960
And then you have measures of marriage, like the difference between covariate distributions in the treatment group and the control group, and you throw out all bad allocations by using the computer to do that for you.

01:40:59.960 --> 01:41:11.960
And the resulting distributions that you're using are not the simple t distributions and f distributions and chi squared distributions.

01:41:11.960 --> 01:41:24.960
Because they this throwing out bad randomizations means you're truncating those distributions. And so the distributions are complex mixtures of truncated distributions, even asymptotically.

01:41:24.960 --> 01:41:31.960
And so using this general framework of potential outcomes and Ruben causal model.

01:41:31.960 --> 01:41:49.960
Then you can then you can try to use the power of modern computing to do the really tedious job of throwing out bad allocations, but you first have to define bad allocations and bad allocations will be defined in terms of covariate

01:41:49.960 --> 01:42:02.960
distributions and multivariate covariate distributions. They're really tedious to compute, but that's what computers are great at doing in these really tedious computations. They're terrible at thinking.

01:42:02.960 --> 01:42:17.960
I agree. I think the realization could be a potential future research area. But on the other hand, I think that the realization can solve the bias, some bias issues.

01:42:17.960 --> 01:42:28.960
On the other hand, they, they create some difficulty in analysis, because of the use throughout some data, and then you have to just for I know you have done some work in that.

01:42:28.960 --> 01:42:34.960
But I think, but when the data structure complicated. So they are going to complicate analysis further.

01:42:34.960 --> 01:42:45.960
Like if you have, let's say non digital data, and then you have a missing data in your outcomes, and then you have delay in correct collect outcomes.

01:42:45.960 --> 01:42:56.960
So I'm kind of worried about that part to say, maybe the outcome is survival time. So you can't just wait until they die. That's too long.

01:42:57.960 --> 01:43:08.960
And, but I think by the idea is I like the idea of a randomization but just need to think about the design issue and analysis the issue how to balance those two.

01:43:08.960 --> 01:43:27.960
Yes. And when I think about why it's this re randomization has become a more recent idea is again I said this briefly in the talk. I mean, Fisher would would throw out a obviously bad randomized allocation.

01:43:27.960 --> 01:43:45.960
He would never live with one where all the good plants got got a treat one treatment and all the bad plants got got the other other treatment. In fact, that's why he invented the analysis of covariance in 1935 I think he invented it.

01:43:45.960 --> 01:44:06.960
And he was had an experiment experiment where, according to the complete randomization, the tea bushes in India, and all the, the, all the bushes with low yield in the previous year, we're assigned one treatment and the ones with good yield previous year, got assigned the other

01:44:06.960 --> 01:44:13.960
treatment. And so we have to adjust for that, using the analysis of covariance.

01:44:13.960 --> 01:44:25.960
And I wrote down details actually that's a paper where he, where he, he made some mistakes, not mathematical mistakes but mental mistakes, fundamental thinking about.

01:44:25.960 --> 01:44:42.960
And I think the reason why he made those mistakes is he didn't, he wasn't using Naaman's notation for these intermediate potential outcomes, which he was saying should be adjusted for.

01:44:42.960 --> 01:44:54.960
Okay, thank you. This is really, really helpful, especially in regards to the hot topic of a real normalization recently. It's been discussed. So it's very helpful to hear your comments.

01:44:54.960 --> 01:45:14.960
So the next question is from Deng Wu from Waseda University. He was asking, now the COVID-19 pandemic is getting down worldwide. However, it is reported that the reasons for getting down are not clear enough, even for epidemiological

01:45:14.960 --> 01:45:26.960
experts. So how causal inference methods can do some more intelligence analysis in epidemiology that, sorry.

01:45:26.960 --> 01:45:54.960
Okay, well, I guess there are now a variety of vaccines being proposed and the rules for who gets them vary not only by country, but within a country like the United States, the rules vary by state.

01:45:54.960 --> 01:46:18.960
So the 50 states, each of them has somewhat different versions of that. And so I think this gives an opportunity because of the variety of interventions being proposed to actually learn about and compare the different rules.

01:46:18.960 --> 01:46:23.960
So some of the rules have age limitations.

01:46:23.960 --> 01:46:41.960
You know, they have other rules have you, you have to have boosters with it with the same vaccine. There's recent stuff in the United States at least that said that that seems to suggest that a booster from any of the vaccines works, no matter what vaccine you got earlier.

01:46:41.960 --> 01:46:51.960
So there I think there are a variety of, I guess what would be called natural experiments taking place all over the world with respect to COVID.

01:46:52.960 --> 01:47:11.960
And so I'm, I'm surely I'm not an expert on following and tracking what what what these interventions are. There's certainly is a variety of them which leads to, I think what would be called natural experiments where there's some

01:47:11.960 --> 01:47:25.960
the assignment mechanism is probably not that related to the assignment mechanism is not really related to potential outcomes, which is the critical assumption. I think realizing that that's the assumption.

01:47:25.960 --> 01:47:38.960
That's the critical assumption to do inference in natural experiments is is really a critical idea to keep in mind.

01:47:39.960 --> 01:47:49.960
Okay, so the next next question from the audience is from her your way.

01:47:49.960 --> 01:48:02.960
His question is, is the output set of causality included in the correlation output set of vice versa. What are the relationships.

01:48:03.960 --> 01:48:24.960
Okay, I think I understood the words, but I may not have understood the big idea. So read again please is the output set of causality included in the correlation output set or vice versa. What are the relationships.

01:48:24.960 --> 01:48:43.960
So I guess by output set that terminology is, is not familiar to me. But, but the I assume what it means is the output set meaning the, the scientific conclusions that are reached at the end of the causal analysis.

01:48:43.960 --> 01:48:47.960
Is, is that how I should interpret that.

01:48:47.960 --> 01:48:53.960
I just wonder if it is outcome set.

01:48:53.960 --> 01:49:02.960
So is the outcome set me that the outcome set. I don't see the outcome set output. I think they're probably this person from computer science.

01:49:02.960 --> 01:49:17.960
So the computer think about input output. So this is the output. So that means the results probably right now. So he took he talked about if you if I interpret correctly is if you do the causal inference you get the results, and then you do the correlation.

01:49:17.960 --> 01:49:20.960
Not as you get results how those results.

01:49:20.960 --> 01:49:24.960
What's the relationship between those two results is all right.

01:49:24.960 --> 01:49:29.960
Maybe I suggest the organizer move on to the next question. I think I see us.

01:49:29.960 --> 01:49:35.960
So, so if you want to rephrase that in the comments, so please do that be fine. Yeah.

01:49:35.960 --> 01:49:46.960
Yeah, yeah, yeah, that'll be helpful. So the next question is that, do you think causal learning has the potential to be applied in more complex control problems like robotic arm control.

01:49:46.960 --> 01:49:53.960
If that how to apply causal learning to the high dimension space.

01:49:53.960 --> 01:50:08.960
Okay, so first with respect to the high, high dimensional space I, I think that when you're dealing with it with with a high dimensional space. The, the first thing to do is to understand Euclidean geometry.

01:50:08.960 --> 01:50:25.960
And, and, and because there are many sort of examples where people think that if you have a high dimensional covariates that that that creates complications, or in some sense it does.

01:50:25.960 --> 01:50:41.960
So if you look at the Euclidean space, and you, and you understand eigenvectors and eigenvalues. If you, if you have a high, a high dimensional space of covariates, but only a few treatments.

01:50:41.960 --> 01:50:52.960
And then what happens automatically is after looking at the first principal components, there are no differences in the, in the, in the lower principal components.

01:50:52.960 --> 01:50:56.960
So you don't have to worry about balancing things that are already balanced.

01:50:56.960 --> 01:51:11.960
It doesn't, it doesn't create the same kind of problem as if you have a high dimensional outcome space, which leads to like multiple comparisons problems when, when, when trying to look at p values or significance levels.

01:51:11.960 --> 01:51:30.960
I, I'm not, I'm a little bit that I, I know about the work being being done in in sort of more than machine learning computer science is it starts with something that's not a complication.

01:51:30.960 --> 01:51:36.960
So we're worrying about eigenspaces that where there's no difference.

01:51:36.960 --> 01:51:55.960
If, if you have high, you know, if you have high dimensional covariates, and you sort of ordered them, you know, ranked them in importance, and then you use look at the icon structure, you find out in the unimportant spaces.

01:51:55.960 --> 01:51:57.960
There are no differences.

01:51:57.960 --> 01:52:07.960
And that's because you don't have enough units. But, but, but still you don't have to, I mean, mechanically, it's, it's not an issue to be worried about.

01:52:07.960 --> 01:52:11.960
I hope that address the question to some extent at least.

01:52:11.960 --> 01:52:26.960
Thank you. I, I think it will be helpful for him to. Okay, so then, the next question is that if potential outcome framework has same function in causal explainable AI with SCM.

01:52:27.960 --> 01:52:30.960
SCM structural causal model.

01:52:30.960 --> 01:52:33.960
Okay.

01:52:33.960 --> 01:52:38.960
And so we the sense with the question again please.

01:52:38.960 --> 01:52:51.960
If potential outcome framework have the same function in causal explainable AI with structural causal model.

01:52:51.960 --> 01:52:57.960
Well, I think he's comparing with POM with SCM.

01:52:57.960 --> 01:53:04.960
The structural causal model. I mean, I, I, I don't really understand what the with those words mean.

01:53:04.960 --> 01:53:14.960
If you, if you have the question that you're addressing formulated in terms of potential outcomes.

01:53:15.960 --> 01:53:30.960
Then the next step is to design a study to try to address that, that, that, that causal question. And the study will, will obviously depend upon whether it's randomized on the assignment mechanism.

01:53:31.960 --> 01:53:48.960
Are you, are you just observing observational data. Are you actually doing an experiment and depending upon which of those you're doing the method of analysis will follow from the, from the design that, that you used.

01:53:48.960 --> 01:53:58.960
I basically like Bayesian models, I think Bayesian models are where everything's a random variable are the right way to derive statistics.

01:53:58.960 --> 01:54:14.960
The fundamental way to do a basic analysis is uses use the basic idea the essential idea of a fisher doing basing it on the randomization distribution, or hypothetical distribution if you if you didn't do a randomized experiment.

01:54:14.960 --> 01:54:24.960
That's the first step, and then more complex answers are given by the by the posterior distribution.

01:54:24.960 --> 01:54:33.960
I don't understand all these other words that are sometime added, like structural or graphical. I mean those.

01:54:33.960 --> 01:54:45.960
Those don't mean anything to me. So I, I just, I guess I view a lot of that as clutter.

01:54:45.960 --> 01:55:00.960
I have often self serving clutter, which is put in in papers to just for self serving reasons.

01:55:00.960 --> 01:55:20.960
The answers are very important when you're expressing either the question or the theory. Okay, let's go to the next question is bad allocation cannot truly be measured by covariates. Is there a way to measure randomness beyond using big data covariates.

01:55:20.960 --> 01:55:24.960
So the first part of the question was what we did again. I'm not sure I understood that.

01:55:25.960 --> 01:55:31.960
Bad allocation, bad allocation cannot truly be measured by covariates.

01:55:31.960 --> 01:55:41.960
Okay, let me respond with a question why not. How else, how else would you measure it.

01:55:41.960 --> 01:55:50.960
And he was asking is there a way to measure randomness beyond using big data covariates.

01:55:50.960 --> 01:55:58.960
So, because randomness refers to a property of what you did.

01:55:58.960 --> 01:56:07.960
You did random allocation, and everything's an approximation. I mean, you know, how do you draw random numbers.

01:56:07.960 --> 01:56:22.960
You know, in the old days, you took out ran table of random numbers and grabbed a haphazard graduate student asked him to close his eyes, turn to a page in the round table of random numbers and put his finger down.

01:56:22.960 --> 01:56:30.960
If you looked at which pages were, were drawn a random page was always in the middle.

01:56:30.960 --> 01:56:43.960
It was never the first page or the last page. Similarly, the, the number that was drawn on a particular page was never at the very top, or the very bottom, or in the very end some corner.

01:56:43.960 --> 01:57:02.960
So, random was always, you mean, sort of uniform in some sense, but you have to describe the process by which it's done, and then it's a mathematical assumption after that.

01:57:03.960 --> 01:57:10.960
I'm not sure I addressed at all the, the end of that question, could you read the, the end again.

01:57:10.960 --> 01:57:12.960
With correlation.

01:57:12.960 --> 01:57:21.960
The end is, is there a way to measure randomness beyond using big data covariates.

01:57:21.960 --> 01:57:38.960
And randomness again is a property of a procedure which is all hypothetical. It's a mathematical structure, and the way it should be certain implications of a mathematical process which are mathematical, which are from covariates I have.

01:57:38.960 --> 01:57:42.960
If I have no covariates at all.

01:57:42.960 --> 01:57:47.960
How do I even assess for something looks random.

01:57:48.960 --> 01:58:04.960
Okay, so the next question is how to understand the success of COVID-19 containment policies in relationship to public health leadership roles through causality statistics.

01:58:04.960 --> 01:58:10.960
Yeah, well, I guess this is like the an earlier question.

01:58:10.960 --> 01:58:30.960
Yeah, because it's, it's, you have, like, different countries have have different policies being implemented at, at different times, presumably, the many of the policies are not being decided based on the actual potential outcomes.

01:58:30.960 --> 01:58:51.960
I think that probably the policies and the implementation heaven have a lot of natural experimentation in them right right now, especially as implemented with various different states or regions of countries having modifications of policies.

01:58:51.960 --> 01:58:57.960
Okay, so this is a very similar question from the first question we responded earlier.

01:58:57.960 --> 01:59:10.960
So, so, so let's go to the next question. Okay, this will be the last question. What's the role of assignment mechanism in potential outcome, and how to design a good RCT.

01:59:10.960 --> 01:59:31.960
Okay, so the potential outcomes are how you define the science and the covariates. So it's, it's completely the potential outcomes framework is completely separate from the assignment mechanism, whether it's randomized or natural experiment or completely

01:59:31.960 --> 01:59:48.960
observational. So that's that that's what I think of the the real contributions that I made in the 1970s was over and over again emphasizing the distinction between the framework for defining causal effects, which is potential

01:59:48.960 --> 02:00:01.960
outcomes, and thinking about them as potential outcomes of the bit of interventions, often in trying to make the interventions ones that you could possibly do.

02:00:01.960 --> 02:00:15.960
And then the assignment mechanism is what you actually did with how you implemented the interventions on different different units. I think one of the things that I think, and you emphasize that that's

02:00:15.960 --> 02:00:37.960
important are these complex designs we have nesting and clustered randomization. And we wrote several papers about those kinds of designs. So, but again, the critical first step is to think what you want to learn about, then formulate

02:00:38.960 --> 02:00:56.960
that question in terms of potential outcomes. And then once it formulated in terms of potential outcomes, then think very hard about how you do the intervention or hypothetical intervention that would yield some observed potential outcomes and some missing potential outcomes.

02:00:56.960 --> 02:01:17.960
And my own preference with at that point, to be build Bayesian models where you get statistics functions of observed data. And then at that point, do inference, starting with to the extent possible for sharing an inference based on randomization distributions

02:01:17.960 --> 02:01:31.960
or hypothetical randomization distributions, and then get more refinement from from the Bayesian models and more precise answers to more complex questions.

02:01:31.960 --> 02:01:36.960
Okay. Thank you so much for attending this event.

02:01:36.960 --> 02:01:44.960
If we don't have any more questions. I thought Professor two from earlier had a question he wanted to ask.

02:01:44.960 --> 02:01:47.960
Professor say do you want to propose your question.

02:01:47.960 --> 02:01:51.960
Do I still have a time.

02:01:51.960 --> 02:02:08.960
Okay, okay, I just want to take two minutes to echo to dance question on how, how, how can we do intervention in the machine learning and how can I do with interference in our problem.

02:02:08.960 --> 02:02:19.960
Yeah, actually, this is my understanding. So the goal of the machine learning is actually just to do prediction. Right. And in the class of my understanding is to.

02:02:20.960 --> 02:02:36.960
Yeah. And in the classical setting actually we made some quite ideal hypothesis. The hypothesis is the ID right the sample that the training and testing they are just from, I don't from the identical distribution.

02:02:36.960 --> 02:02:51.960
So, and another setting is just that the domain adaptive adaptation that okay we train from one distribution and we can test from on the other distribution but the testing distribution should be known.

02:02:51.960 --> 02:03:03.960
So, either the ID, we call the ID learning or the transfer learning or domain adaptation actually, we just assume that the testing distribution is known.

02:03:03.960 --> 02:03:19.960
Okay, so this is a classical setting. So under this setting, actually, what's the machine learning models trying to do is just to do the distribution feeding or data feeding, because we know the testing distribution, right.

02:03:19.960 --> 02:03:35.960
So, now this is actually quite ideal. I mean, assumption because in the real in the real applications we cannot assume that okay the testing distribution is known because there are a lot of interventions.

02:03:35.960 --> 02:03:51.960
There are a lot of distribution shifted in the real applications right. So, so now actually, I think a very fundamental problem in the machine learning community is trying to to relax assumption that the testing distribution is known.

02:03:51.960 --> 02:04:05.960
Okay, so if we don't have the testing distribution then how can we optimize the model, how can we learn the parameters. So, then we need a new paradigm for the learning.

02:04:06.960 --> 02:04:27.960
I think, from our understanding, if we don't know the testing distribution, then we need to pursue the true model, the true model that generate the data, right, if we can, we can gather two model, then of course, no matter how the covariate shift and how the distribution

02:04:27.960 --> 02:04:31.960
change we can make a reasonable prediction, right.

02:04:31.960 --> 02:04:32.960
Correct.

02:04:32.960 --> 02:04:51.960
And the logic is, if we have the true model, actually for any samples, for any sample, we can perform uniformly good, right, because we can model the true signal and only the noise cannot be modeled if we have the true model.

02:04:51.960 --> 02:04:52.960
Correct.

02:04:52.960 --> 02:05:01.960
So, we, we just want to do a reverse engineering that means, okay, if you change the data distribution.

02:05:01.960 --> 02:05:18.960
Okay, you're just a change that there's a kind of intervention, for example, you can raise, reweight all of the training samples to another distribution, if a model can can have, I mean, the uniformly good performance on all of the

02:05:19.960 --> 02:05:39.960
distributions, for example, you have a multiple sample evading strategy, right, and under each strategy, actually you can get one distribution, but across different distributions, if we can have a good, I mean, a prediction performance, that means, okay, the model actually captured the true model.

02:05:39.960 --> 02:05:47.960
So, I think that in the machine learning, maybe one way of intervention is just the sample evading.

02:05:47.960 --> 02:05:57.960
So, we're going to train the distributions, and then under these distributions, whether we can find out the invariant, I mean, model or invariant structure.

02:05:57.960 --> 02:06:08.960
And this invariance, I think it's somewhat related to a causal effect, a causal relationship or causal structure.

02:06:09.960 --> 02:06:19.960
Our idea, but I'm not just very sure whether this is correct from the causal inference perspective, especially from the perspective of

02:06:19.960 --> 02:06:22.960
Rubin causal model.

02:06:22.960 --> 02:06:28.960
Right, well, I think what you, what you mean by the

02:06:28.960 --> 02:06:36.960
true causal model is the true distribution of outcomes given covariates.

02:06:36.960 --> 02:06:47.960
When the covariates are simple enough, let's say just define a few straighter, then we waiting works.

02:06:47.960 --> 02:06:58.960
But if there is, if the covariates are complex or complicated, I don't want to use complex in the, in the mathematical sense.

02:06:59.960 --> 02:07:10.960
But if they're complicated, and you have many outcome variables, I think you have to rely on modeling the y, the outcome, given the covariates.

02:07:10.960 --> 02:07:25.960
And the modeling can, and then what you do, because if you have a good model for outcomes given covariates for y given x, then you can predict y at lots of different x values.

02:07:25.960 --> 02:07:39.960
And I think that that's the, the bigger picture, whereas waiting is, you have to, you have to have the continuous version of waiting to have the general idea.

02:07:39.960 --> 02:07:42.960
Yeah, correctly formulated.

02:07:42.960 --> 02:07:46.960
So I, I think we're saying the same thing.

02:07:46.960 --> 02:07:49.960
Basically, although the words may be slightly different.

02:07:49.960 --> 02:07:51.960
Yeah, yeah.

02:07:51.960 --> 02:08:02.960
But, but there has been work done starting in the probably mid 1930s in in survey work.

02:08:02.960 --> 02:08:14.960
I think the probably the work that's closest to the work that that you're talking about in computer science is probably the work on missing data.

02:08:14.960 --> 02:08:31.960
Because here, you'd worry about if the, if the guys who are, who are missing have very different covariate values, you know that you're relying on extrapolation, relying more and more in some hypothetical model.

02:08:31.960 --> 02:08:47.960
And the waiting, the idea of just just waiting units only works when the support for the data are the same for the people who are observed, and the people who are missing, or otherwise the, the imputation of the missing data won't be very good.

02:08:48.960 --> 02:09:09.960
In fact, there, there was a three volume collection of books that were published around I think 1984 or three by the, under the National Academy of Sciences, because there was a committee on, on, on handling missing data.

02:09:09.960 --> 02:09:26.960
And the, there are various authors to the, to the three volumes, I think I'm on, I'm one of the co authors on one of the volumes, but a guy named William Maddow and adow with author on all.

02:09:26.960 --> 02:09:42.960
I mean, it's actually because it's true during a problem. You're discussing right now, except it was done on text of, of large US surveys mostly.

02:09:42.960 --> 02:10:00.960
The Medical Examination Survey, which was 20 or 30,000, and they, and because this was a design survey, it had certain structures that like it had different sampling units were states and has a hierarchical level to it.

02:10:00.960 --> 02:10:14.960
So that, that, that literature, I think is, is quite relevant, although not the techniques are being used because, again, the techniques are far more simple minded than the ones that can be used today.

02:10:14.960 --> 02:10:25.960
But I think it's worth looking at server, it's historical interest, and there probably are a variety of interesting ideas.

02:10:25.960 --> 02:10:40.960
Yeah, thank you. Thank you very much. I think this is very constructive comments and inspiring, but just now that the signal is not very good. So I will write an email to you and could you send the survey title to me and I will.

02:10:40.960 --> 02:10:41.960
Fine, I will do so.

02:10:41.960 --> 02:10:43.960
Thank you. Thank you. Thank you very much.

02:10:43.960 --> 02:10:51.960
Thank you so much. So it's actually getting very late, almost like 1118 in China.

02:10:51.960 --> 02:11:04.960
Yeah, I know you need time to grab lunch as well. Yes. Thank you so much for, for the wonderful, wonderful talk that we're very, very honored and very lucky to have you here tonight.

02:11:04.960 --> 02:11:20.960
And thank you all for the speakers because I was reading the comments from the Billy Billy because this room is full. So we have a lot of also broadcasting guests from the website of Billy Billy and they were commenting on how useful and how helpful those

02:11:20.960 --> 02:11:30.960
responses are. So I guess they contribute a lot to the knowledge and we are very, very happy and learn a lot tonight.

02:11:30.960 --> 02:11:46.960
I really appreciate the formal discussions and the informal questions. They are very helpful generally for trying to clarify these important ideas. Thanks again for the invitation.

02:11:46.960 --> 02:11:56.960
So last section, I'm going to give a brief introduction about Professor Don's book.

02:11:56.960 --> 02:11:58.960
One second my.

02:11:58.960 --> 02:11:59.960
Okay.

02:11:59.960 --> 02:12:04.960
Here's what happens every single time I try to share something on my screen.

02:12:04.960 --> 02:12:08.960
I don't have to say goodbye because I have a meeting tomorrow.

02:12:08.960 --> 02:12:09.960
Okay.

02:12:09.960 --> 02:12:13.960
Okay, you have to get up or hope say back back in China.

02:12:13.960 --> 02:12:16.960
I hope I hope so too.

02:12:16.960 --> 02:12:19.960
Good seeing you again. Bye bye.

02:12:19.960 --> 02:12:21.960
Bye bye.

02:12:21.960 --> 02:12:25.960
Okay, so can everybody see my screen.

02:12:25.960 --> 02:12:28.960
I can't see your screen I can see your face.

02:12:28.960 --> 02:12:30.960
You can see me in my screen.

02:12:30.960 --> 02:12:34.960
I can't I can see your face but I can't see your screen.

02:12:34.960 --> 02:12:36.960
You can't see my screen.

02:12:36.960 --> 02:12:38.960
Okay, let's try again.

02:12:38.960 --> 02:12:42.960
That's what happens every single time I try to share something.

02:12:42.960 --> 02:12:43.960
I see it.

02:12:43.960 --> 02:12:44.960
It comes.

02:12:44.960 --> 02:12:49.960
Yes, there's a very long time of that time lack.

02:12:49.960 --> 02:12:52.960
Yes.

02:12:52.960 --> 02:12:53.960
Okay.

02:12:53.960 --> 02:12:56.960
Well, the speed of light is only so fast.

02:12:56.960 --> 02:13:03.960
Okay, yes, depending on how many circuits, it has to go through even the speed of light can take time.

02:13:04.960 --> 02:13:08.960
Okay, so I'm going to finish this really fast.

02:13:08.960 --> 02:13:19.960
We are going to read Professor Donna Rubins book on causal inference for statistics, social and biomedical sciences, the end introduction.

02:13:19.960 --> 02:13:24.960
This will be a, this is a very classical book to learn about causal inference.

02:13:24.960 --> 02:13:29.960
So the reading club will start next Sunday every morning.

02:13:29.960 --> 02:13:38.960
And then we will have students and also scholars sharing the contents of each chapter every single week.

02:13:38.960 --> 02:13:44.960
And then we are also having the coding session to help to replicate the studies in the book.

02:13:44.960 --> 02:13:47.960
So, on the right is the QR code.

02:13:47.960 --> 02:13:52.960
So if you're interested, please register through the QR code on the right.

02:13:52.960 --> 02:13:55.960
And again, thank you very much for ruby.

02:13:55.960 --> 02:13:56.960
Okay.

02:13:56.960 --> 02:13:58.960
I'm going to use Chinese for a moment.

02:13:58.960 --> 02:14:00.960
This is our background.

02:14:00.960 --> 02:14:01.960
It's really ruby.

02:14:01.960 --> 02:14:03.960
This book has a reading club.

02:14:03.960 --> 02:14:04.960
We call it English Science.

02:14:04.960 --> 02:14:06.960
The reading club is translated by English.

02:14:06.960 --> 02:14:11.960
And then we will, if you're interested, you can scan the QR code on the right.

02:14:11.960 --> 02:14:18.960
And then come to our reading club to participate in our new series of English teaching.

02:14:18.960 --> 02:14:21.960
Okay, so that's all for tonight.

02:14:21.960 --> 02:14:26.960
Thank you so much. Stay safe and have a good day.

02:14:26.960 --> 02:14:30.960
Thank you very much. And again, thanks for the invitation.

02:14:30.960 --> 02:14:32.960
Thank you so much.

02:14:32.960 --> 02:14:33.960
Bye bye.

02:14:33.960 --> 02:14:34.960
Bye bye.

02:14:34.960 --> 02:14:35.960
Have a good night.

