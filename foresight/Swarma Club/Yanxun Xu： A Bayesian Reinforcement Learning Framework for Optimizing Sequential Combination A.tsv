start	end	text
0	7280	Alright, so we'll move on to our next speaker. Yanxun will be our next speaker
7280	12720	and she is a associate professor in the Department of Applied Mathematics and
12720	19080	Statistics at Johns Hopkins University. Yanxun has a lot of research interest in
19080	23360	the reinforcement learning, high-dimensional data analysis, and
23360	29360	non-parametric statistics. Okay, so today she is going to talk about a Bayesian
29360	36040	reinforcement learning framework for DPR. So you can see the screen, right? Yeah,
36040	41800	yeah, great. So thank you Lu for organizing this session and introducing me. So
41800	44880	today I will talk about a Bayesian reinforcement learning method for
44880	51440	optimizing treatments for HIV patients. So first I'd like to thank all my
51440	56320	collaborators in this project. So my peer-de-student Wei and also my
56320	60640	collaborators, Leah and Raha from Johns Hopkins University and Yanni from
60640	65800	Texas A&M and also two clinicians, Amanda and Jane from Georgetown University
65800	74160	and Washington University in St. Louis. Okay, so making the optimal
74160	79320	sequential decisions is very important in many diseases. So today I will focus on
79320	85960	HIV and we know the emergence of anti-retroviral drugs, ARP, has transformed
85960	90840	the HIV infection from a fetal disease to a chronic disease. So it's
90840	97440	significantly reduced HIV-related mortality and the common ARP drugs fall
97440	102760	into different drug classes with different mechanisms. For example, the
102760	108040	first one, the NRPI, it's called a nucleotide or probably can also be a mouse.
108040	112560	Okay, so the first drug class called NRPI is called nucleotide reverse
112560	117800	transcriptase inhibitor. It inhibits the reverse transcription step in viral
117800	123560	replication and other common drug classes include like NRTI, PI, ESTA, EI and
123560	129720	the booster. And the model ART drugs usually combines three or more ART drugs
129720	135120	from different drug classes. Since such cocktail approach is most efficient in
135120	141640	suppressing viral loads. For example, the clinician can prescribe two NRTIs and one
141920	146480	drug from instinct and all they can prescribe like two drugs from an ARTI
146480	155000	drug class and one from an ARPI or one from the PI. So people living with
155000	159560	HIV now are recommended to follow up with their physicians semi-annually by the
159560	165440	current HIV treatment guideline. And at each visit, there are social demographics,
165440	170320	lab test results, such as CD4 counts and viral loads, and also clinical
170320	175240	environments such as BMI and the glucose are collected. And then physician
175240	180120	assigns their ART regimen based on their clinical observations. So each ART
180120	185240	regimen is a combination of different drugs from different classes. And this
185240	191560	process repeats every half a year by average. So nowadays, the US Department
191560	197080	of Health and Human Services provides general guidelines on assigning ART
197080	201880	treatments. However, these guidelines are usually applied to treatment
201880	207440	naive subjects, meaning subjects who are recently diagnosed with HIV and who
207440	213440	never took ART treatments before. And for happily pretreated people with HIV,
213440	218800	for example, if this person has been diagnosed with HIV for 20 years and has
218800	224560	been taking ART drugs for a long time, and there's no consensus. And also the
224560	229240	current guideline didn't take into account the potential adverse effects
229280	235000	caused by the long term use of ART drugs, because HIV now is more like a
235040	241120	chronic disease. And those patients needs to take ART drugs every day. And
241160	245320	indefinitely. So you need to take every day to take the drugs every day to
245320	250400	suppress the viral loads. And they may cause some long term adverse effects.
250560	254800	But for example, like the kidney disease, weight gain or depression, and the
254800	258840	current guideline didn't take into account all those side effects. So our
258840	264400	goal is to determine the personalized ART regimen to optimize the long term
264400	269920	health. That means not only the drugs can supply the viral loads, but can also
269920	277000	control the side effects. So large scale HIV studies such as maximize
277000	282040	cohort studies. And it provides us opportunities to achieve this goal. And
282040	287440	they collect data from participants semi annual visits. And here is the ART
287600	292480	treatment history for one randomly selected subject with seven visits. Here's
292480	297720	the X axis shows the calendar dates of their visits. And the Y axis shows the
297720	303120	ART drug combinations they have this person has taken. And different colors
303120	307280	here represent different drug classes and the drug name are marked. So I use a
307280	314000	three letter to represent each drug. And also at each visit, the house related
314000	318520	environments are recorded. For example, this figure shows the subjects,
318520	323080	longitudinal depression scores, viral load, EGFR for kidney function and the
323080	329720	BMI. So there are many challenges to optimize the sequential ART regimens
329720	334760	in a data driven manner. First, we need to estimate the drug effects. Like before
334760	338960	we assign them, we need to understand their drug effects from a high dimensional
338960	343760	and unbalanced space. So when I say high dimensional, it means with more than 30
343760	348800	ART drugs, they're all FDA approved on the market. There are a large number of
348800	353440	possible drug combinations. So that could be millions of drug combinations you
353440	357040	can choose. So even like the feasible drug combinations about like thousands,
357040	360760	there's still there are a large number of possible drug combinations. And also
360760	365400	when I say the unbalanced, that means some drug combinations are firmly used, but
365400	369120	others are wrong. So for example, we can see for this drug combination is two
369120	374560	ART drugs and one PR drug. It was the using our database for almost 1000
374560	379960	times, but another similar drug combination. So the same two ART drug, but
379960	386440	a different the PR drug, it was only used for 12 times. And the second challenge
386440	391880	is how to generate a realistic ART regimen from a large discreet space in the
391880	396240	optimization procedure. So after we understand the treatment effects of
396240	401360	these drug combinations, we need to assign the drug regimen to patients. That
401360	406320	means we need to generate a realistic ART regimen. So then the problem is how to
406320	411640	represent each ART regimen. A naive method would be say, okay, I can use a battery
411640	418040	vector. So say I have 30 ART, I have 30 ART drugs on the market, and then I can use
418040	422680	the battery vector is indimensional battery vector to represent. So if this drug
422680	428680	combination contains drug one, then it's one, otherwise it's zero. However, it will
428680	433880	cause two to the N possible drug combinations. And many of them will not
433880	437760	be feasible. So like I said, usually, you know, people combine the drugs from
437760	441440	different drug combinations, but usually we assign like, for example, two or three
441640	447160	drugs from ARTI, but like really we assign one drug from PI. So not all these
447160	452720	possible two to the N combinations are possible. And then we need the efficient
452720	458440	way to represent the drug combination. And lastly, we aim to optimize sequential
458440	462680	treatments from observational data. So we have all those data collected from
462680	468360	observational study, but we want to assign them to the patients in the future. So
468360	473560	the fundamental challenge of optimizing treatments from observational study is
473560	477880	this distribution shift issue. So that means the training data may be collected
477880	481560	on the different policies from the one we try to evaluate. So we need to address
481560	487320	the distribution shift issue. So to address these challenges, we develop a
487320	492280	two-step Bayesian reinforcement learning framework. And here is an overview. In the
492280	496440	first step, we propose a Bayesian dynamics model for individuals'
496520	501000	longitudinal observations using a microverte Gaussian process. And these estimate
501000	505560	dynamics describe how individuals' health-related variables evolve over time,
505560	509560	condition on their historical states and the treatment histories, with uncertainty
509560	515720	quantification. And in the second step, we create a pessimistic environment with
515720	521720	uncertainty penalization to mitigate the distribution shift issue and also use
521720	525560	an offline reinforcement learning method to optimize the sequential ART regimen.
526200	532520	So it's a two-step approach. Okay, so now let's go to the model details.
533240	540680	So first, I introduce the problem formulation. Assume for each individual eye,
540680	547960	we use XI0 to denote the baseline covariates, such as a race. And the TRI records the
547960	555080	visit times. So assume each one has GI visits, and we have PI1 to TIGI to denote the time of each
555080	561240	visit. And also assume we have M time varying health state variables. So here we call state
561240	566840	variables because they change over time. For example, like H, BMI, EGFR, those clinical
566840	573880	variables or demographics, they are collected in each visit. And also the I to represent the ART
573880	579320	drug combination used by individual eye during the time period, TIGI minus 1 to TIGI.
581000	588360	And then the data can be summarized as D. So for each subject, so we have a total of I subjects,
588360	593480	and we have baseline covariates, the time of each visit, and the time varying state variables,
593480	602200	and also the drug information. And then we use the YIG bar. So the bar is a common fun we really
602280	608840	use to represent the history. So the YIG bar means all the state variables before the current
608840	616040	visit J. And the ZIG bar means all the treatments this person has been taking on pure the time J.
616680	624680	And also we use the dynamic, we use the after dynamic model. So that means condition on the YIG
624680	633720	bar and the ZIG plus one, we update the state variables from YIG to YG plus one. So remember
633720	640680	the first step of our method to learn the dynamic model of how to transform from YG to YG plus one.
640680	646440	Of course, the YG plus one condition on the whole history of the YG and the treatments the IJ.
647800	653080	Okay, so our goal is to optimize the ART assignments to maximize the individual's long-term
653080	659480	house outcome. So because we want to maximize the house outcome, essentially we can transform the
659480	665400	problem to an optimization problem. So this optimization problem is like we first define
666200	671960	for each individual I, we suppose she already has GI visits. So if this person is a new patient,
671960	678120	so the GI will be equal zero. So the GI can be zero, or if this person already has GI visits,
678120	684280	and then we want to predict the next few visits, for example. And then we let the Y new and the
684280	691480	GI new to denote her future longitudinal states and the treatments, because our reward may depend on
691480	698520	her future states. For example, we want her less than the next two years, the depression is minimal.
700040	705800	And assume for any future visited J, the ART regimen is assigned through a policy function
705800	712920	PI. So that means if we can learn, if we can prioritize this function PI, and we can optimize
712920	720760	data and the equivalently we can optimize the treatment. And let's say we assign a stochastic
720760	727640	reward function RI that depends on the house states. So we can depend, we can define the reward as
727720	735160	for example, this person, their, their very loaded is low, and the pressure set is low, and the BMI is
735160	741000	in the normal range, and the kidney function is a normal range. Okay, so for example, if our goal
741000	747080	is to select the sequential ART regimen that leads to lowest accumulated very low in the next two
747080	753560	years, and it is coming an active sum of the very loads. Okay, so they notice the expected reward
753560	758920	for any individual I to be the following. Because we, in the first step, we learn a Bayesian model.
758920	765560	So essentially, you can generate their future states. And also, so we can generate the future
765560	771400	states, why are you from the learned dynamic model, and we generate the ZI new from their
772280	776360	the PI, the function. So we learned the PI from their parameter as the function PI.
777000	782360	And also, because we learned the dynamic model, and then we can integrate out certain things,
782440	787480	optimization procedure. So that's kind of the benefits of using the Bayesian framework in the
787480	792840	first step. Because in the decision making step, we can integrate out uncertainty, we can adjust
792840	799560	for this uncertainty quantification from their uncertainty of their dynamic, their dynamic model.
801000	807720	And our goal is to find as optimized optimal policy function PI that parameterized by data
807720	813080	i star. So we want to find as an optimal parameter that can maximize the r i theta.
814200	820440	So that's a problem. Okay, so now we have already defined our reward function r i. And we want to
820440	825960	find as a parameter theta that can maximize the r i. And essentially, it's a classical
825960	831400	reinforcement learning problem. And we can use the policy gradient method to solve the problem.
831400	837320	So essentially, if you can calculate the gradient of r i, and then you can use the policy gradient
837320	843160	method, essentially, you update the theta by this formula. So it's also classical results,
843160	850280	it's that you update the theta by the previous theta, and then plus some step size s i, q,
850280	856760	and then times their gradient. So the essential problem is how to calculate the gradient of our
856760	862200	reward function. So we can see our reward function is relatively complex, right? You take the expected
862200	869400	value of the reward function r i, and r is a function of y mu, and y mu you need to generate
869400	875960	from the predict distribution of your dynamical model. And besides all of that, we also need to
875960	881080	integrate out the uncertainty from the dynamical model by the P5 condition on D, that's a posterior
881080	889320	distribution of the phi. So it's not easy to calculate the gradient of this r i theta. Okay,
889400	895000	so luckily for the policy gradient method, there's a way to calculate that. So if you're
895000	899400	interested in details, you can find the details in the paper, but we can represent the derivative
899400	906760	of r i theta as follows. And this formula looks very complex, but we can't decompose this into
906760	915560	three separate steps. The first step is this step. So it's about the log of our policy function. So
915560	922360	essentially, if you can parameterize the ART assignment function, and then you can optimize
922360	927560	that. So that's our first challenge. We need to parameterize the policy function. And the second
927560	933960	step is how to generate the future states based on our dynamical model. So that's the second step,
933960	939960	is we want to sample the future states. And the last thing is if we can define the reward function.
939960	944680	So essentially, you decompose the calculating this gradient by three
945480	950520	sub steps. If we can sample future states, if we can define the reward function,
950520	955560	if we can parameterize the policy function, and then we can calculate the gradient of the reward.
955560	959720	And then we can plug into policy gradient method and then get the optimal theta.
960600	967080	Okay, so now I will introduce each part. So how to sample the future states? So if we want to
967080	971800	sample the future states, basically, we need a model, and then we do the predictive inference.
971800	977400	And in this case, we have multiple longitudinal states. And we use a multivariate Gaussian
977400	982680	process. The reason we use a multivariate Gaussian process because it's a popular choice for modeling
982680	989240	irregular space multivariate longitudinal data with great flexibility and also natural
989240	995320	uncertainty quantification. And here's irregular, it comes from the missing data because sometimes
995320	1002040	maybe some visits and some measurements may be missing. Okay, so the multivariate Gaussian process,
1002040	1009080	the whole framework of this multivariate Gaussian process is relatively standard. We use yimt to
1009080	1016760	denote the nth longitudinal variable for treat for individual i at the time t. And we have a mean
1016760	1026680	function ft and the answer id residue epsilon. So for this f, we assume the multivariate Gaussian
1026680	1031480	process is distributed. So we will have the mean function. So the next slide I'll introduce how we
1031480	1038120	model the mean function. And for the various covariance parts, we assume they're the separable
1038120	1043720	covariance function. So they're here the cm to denote the correlation, represent the correlation
1043720	1049720	among different state variables because they could be correlated. And the ct to represent their
1049720	1054520	correlation among the time. So this separable covariance function adjusts for the correlation
1054520	1062280	among variables and also along the time. Okay, and here for the ct, the correlation kernel that
1062840	1068760	for the temporal correlation, we use the oil kernel because we expect the curve that's not too
1068760	1075080	smooth. Okay, so for the mean parts, that's kind of the key country, one of the key contributions
1075080	1080840	of this model is that for the mean parts, the first two parts are kind of standard. We use a
1080840	1085880	baseline, it's like linear Mase effect model, we have fixed effects and random effects. But
1085880	1093080	the how to model the drug combination effects, it's a key thing. So remember I said, for the
1093080	1098280	drug combination, it's a high dimensional space. So how to represent the drug combinations.
1098280	1103480	Here we borrow like the kernel idea. So the way we model that is we assume
1104840	1111240	there exists. So we assume there there's a okay, okay, so we assume there's a
1112040	1119480	we define a drug similarity regimen function kappa here. So because the z itself is a high
1119480	1124920	dimensional space. So to reduce the dimensionality, we introduce kappa. So it's like borrow the
1124920	1130920	kernel idea, we reduce the dimension from the original all the drug combinations to a manageable
1130920	1137720	size as capital D here. So we introduce a bunch of representative drug regimens ZD. And then we
1137720	1143320	calculate the similarity between each drug, possible drug combination with ZD. And then as
1143320	1150280	long as we can estimate the parameter gamma and D here, and then we can represent the drug effects
1150280	1159480	for each drug combination. And the way we define their similarity function, it depends on two
1159480	1164360	properties. The first one is we want sharing of information because the similar drugs from
1164360	1169240	because the drugs from the same drug class, they have similar drug effects because we share the
1169240	1174760	same mechanisms. So we want to share information from different drug combinations. And also from
1174760	1179960	this kernel, the introduction of this similarity function, it can reduce the high dimensionality.
1180280	1188760	So let me briefly introduce the idea of this kernel. Because of time limit, I will not see
1188760	1193560	the detail. So consider a straight way to compute the drug combination similarity.
1194440	1198840	And the one straight forward idea is we use linear kernel. So the linear kernel, they can
1198840	1204120	compute the similarity between regimens based on the proportion of common drugs that two regimens
1204120	1210520	share. So for example, here, we have three drug combinations. And all of them use two same drugs
1210520	1218200	from the NRTI class. So D4T plus LAM. And assume the third drug, the first two regimens, they
1218200	1224120	choose one PI drug, but different PI drugs. And another one is choose NRTI. So you can use a
1224120	1229080	linear kernel. That means the pairwise similarity among these three kernels will be two over three,
1229080	1235560	right? Because they have three drugs, and they share two same drugs. However, there are some
1236280	1242840	disadvantages. For example, the first two drug combinations. So to both of them, they use two
1242840	1247960	same NRTI drugs. And the third drug, they belong to the same drug class. Because the same drug class,
1247960	1253640	they share the same madness. So we would expect the similarity between the first two drug combinations
1253640	1259160	would be larger or would be higher compared to the similarity between there, between them and the
1259160	1263320	third drug. Because the third, because the third drug combination, they have the drug from a different
1263320	1268760	drug class. And another example, for example, if you have these two drug combinations, both of
1268760	1275800	them have two drugs from NRTI drug class, and one drug from the PI drug class. If we use a linear
1275800	1280840	kernel, and they would share zero similarity, because they don't share any of common drugs.
1280840	1288120	However, we know the same drug class will share some similarity. So the good method,
1288120	1293560	we should borrow this clinical information and share some similarity for these two drug combinations.
1293560	1299720	So the way we set up the, we define the drug similarity is we use the sub subject kernel.
1299720	1304360	So the sub subject kernel is the idea was to represent the sentences in natural language
1304360	1311080	processing literature. And here we represent our drug combination by a tree structure. And the
1311080	1318280	subject kernel can represent the similarity at all levels of the tree representation. So essentially,
1318280	1325720	the upper level is ART. And then we have the second level to represent which drug class
1325720	1331000	we draw the drugs. And the third level represents how many drugs from each drug class. And the
1331000	1338200	third level represents the each drug from each drug class. And then the sub subject kernel can
1338200	1343560	represent the whole similarity. For example, like regimen A and B, they can adjust for their similarity
1343560	1349400	is a blue box. And for regimen A and C, they can adjust for their drug similarity is a yellow box.
1349400	1354600	Even, you know, they don't share any common drugs, but you can still incorporate their similarity.
1355320	1359880	Okay, so now I have introduced this Markov-Berica Gaussian process to model the
1359880	1364760	longitudinal states. And then if we have a model and we have our own parameters, and then we can
1364760	1368920	write down the likelihood, and you can assign the price to all unknown parameters, you can
1368920	1373480	obtain the posterior distribution from the MCMC. So I will skip all the computational
1373480	1377880	details here. But essentially, now we finish the first step. So we have the Markov-Berica
1377880	1383000	normal model, we can sample future states. Okay, so the second one is how to define the reward
1383000	1388360	function. And the reward function, it depends on the clinical goal, right? So here,
1389720	1397080	it depends on how we define the long term house for each individual. So here, after consultation
1397080	1402600	with the clinicians, we determine that we define the reward that depends on the barrel load,
1402600	1407240	kidney function and the depression. So we want to care about first, you know, whether it can
1407240	1412040	successfully suppress the barrel load, and also maintain a good kidney function and also the good
1412040	1419960	mental health. So let's see, our goal is we will, so here we can say we want to maximize
1421160	1427800	the overall house in the next two years. So remember, the visits are semi-annual visits.
1427800	1434280	So that's why here the sum is from the next visit, next four visits, because next four visits means
1434280	1441080	the overall good health in the next two years. And then we want the depression, this as small as
1441080	1448760	possible. And also, oh yeah, here is the next four visits. And also for depression, it's as small as
1448760	1454200	possible. And for the barrel load and the EGFR, because as long as they are normal threshold,
1454200	1459320	it will be fine. So we define this kind of step function, as long as they are in the normal range,
1459320	1465640	it'll be fine. And if it's outside of the normal range, and we give certain penalty. And also here,
1465640	1473000	we have to personalize the weights, WI. So for example, if some person, they more care about
1473000	1478600	the depression, and then the WI1 can have a higher weight. So it's personalized and determined by
1478600	1485240	the physician and also patient himself. And also to mitigate the distribution shift
1485240	1491640	issue, we use certainly penalized reward. That's another advantage of using the Bayesian method
1491640	1497080	in the first step, because we can easily quantify the uncertainty. So this idea is by this paper by
1497080	1503240	you from UC Berkeley's group. And essentially, we define a pessimistic environment by introducing
1503240	1511240	a penalized reward. So the RA is defined by the previous slide. But now we penalize the uncertainty
1511240	1517800	of the, it's the predictive variability of the state and their treatments. And it's a
1517800	1525000	tuning parameter we need to learn. Okay. And then we use a posterior predict distribution to
1525000	1529080	quantify the uncertainty again, because we have a Bayesian model, so that's very straightforward.
1529080	1534600	Okay, so now we define a reward function. And the last step is how to parameterize the policy
1534600	1542200	function. So to prioritize the policy function, we make this also the three types of decision
1542200	1547800	after talking with clinicians. So essentially, we decompose this to several steps. So first,
1548440	1553880	if this person has been using ART drugs for a long time, and we will see if this person needs
1553880	1559960	to switch the regimen or not. So if the older drug works, and we can just keep using the older drug.
1559960	1564440	So this is what we will represent as a logistic regression method in the logistic regression
1564440	1570600	model. As long as all the health measurements are in within the normal range, and then we will
1570600	1575160	decide to just, you know, keep the old drug. And if one of them is not in the normal range,
1575160	1581720	we will switch. If this person needs to switch, and then we will need to generate a new regimen.
1582280	1588520	And because we used the three representation, and then we can now decide, you know, if we need to
1588520	1593240	switch how to generate a new regimen, essentially, we need to decide like which drug class and how
1593240	1598600	many drugs use the initial class and also which individual drugs at each class. So this essentially
1598600	1603080	it's a non-central hypergeometric distribution. Again, I skipped all the details. It's kind of
1603080	1611560	a little bit complex. So we have these three levels of three decisions. Okay, so now we have already
1611560	1615880	finished these three steps. So we have multivariate Gaussian process to some whole future states,
1615880	1620760	and we define reward function. And then we have ways to prioritize a policy function. We can use
1620760	1628040	a policy gradient method to optimize a print. Okay. Okay, so now, so here I finish all the
1628040	1634680	matter introduction. Last part of the slides is I will introduce real data analysis results.
1635720	1642120	So for the real data, we got about 300 women from the Washington DC site from the white study.
1643000	1649400	And also now we get four state variables at each visit, depression, viral load, EGFR, and BMI.
1649400	1654360	And there are about 8% missing data. And the baseline covariates we consider include age,
1654360	1660680	smoking status, substance use, employment status, hypertension, diabetes. And in this study,
1660680	1668280	we have 31 ART drugs and six drug classes. And we choose 105 representative drug regiments. So
1668280	1673400	those regiments based on the popularity of the drug combinations, if they have been used a lot
1673400	1678600	of times for the from the patients, and then we would know that as representative ART regimen.
1679160	1685960	Okay, so here is one hypothetical patient. So we'll use this example to demonstrate the precision
1685960	1693000	medicine, the, you know, the utility of the clinical utility of the proposed methods. Okay,
1693000	1701000	so this person has been has been had 31 visits. And here is their history of treatments. And for
1701000	1708920	these patients, we assume their weights as one third, one third, one third. Okay, and then we run
1708920	1715480	our optimization method. And here we can see the expected reward versus the SGD iteration. So it
1715480	1722440	became relatively stable after 1,000 iterations. And here is optimal regimen is the next two years.
1722440	1727800	So we can see at the visit to there are two ART drugs when you see one PR and one poster,
1727800	1735720	and then it changes one new regimen for visits 33 to 35. Okay, and also here I want to show under
1735720	1741480	their estimated optimal regiments, that's the predicted depression stores. And we can see for
1741480	1749320	their visits from 32 to 36, they're about 23% improvement of depression. So that also shows
1749320	1754680	the clinical utility of our, you know, newly assigned optimal drug combinations. Okay, I will
1754680	1765480	skip the next example due to the time limit. Oh, yeah. Okay, to summarize, we, we propose a like
1765480	1770680	a Bayesian reforcing learning approach is a two step approach. And it can learn their dynamics
1770680	1776120	with uncertainty quantification, it can also assign the long term optimal drug combinations to
1777320	1781000	optimize each individual's health. Okay, yeah, thank you.
1781240	1787880	Thank you so much, Yanxun. Any questions from the audience?
1788920	1794840	Yanxun, those are very exciting work. I actually have some questions because you touched a very
1794840	1802520	good point where you need to balance the priority like competing priorities when you are in the
1802520	1806840	clinical practice. But since we are a little bit over time, so I probably will talk to you later
1806840	1814440	about that. I was wondering like how the uncertainty will be impacted by how you
1814440	1823320	define your reward function. Oh, yeah. So the uncertainty part, you know, how the uncertainty
1823320	1828600	affects the final decision depending on how you tune the parameter. So like here, and yeah, I skip
1828600	1832840	that part, but here you can show if we have different tooling parameters like lambda equal
1832840	1838440	zero, you don't penalize at all. And then you have this drug combination. And if you use like
1838440	1843240	increase the lambda, and then it will penalize the uncertainty, it's kind of uncertainties
1843240	1848600	reflected by the sample size in their data. If this drug combination has has been used a lot of
1848600	1854040	times, it has relatively narrow uncertainty, it had never been used, then it has a lot of
1854040	1857960	uncertainty. So for example, here with lambda equal zero, they actually recommend this drug
1857960	1863640	combination is the first recommendation. So it actually never been used in the in the data.
1863640	1868440	So that's kind of create a trade off, like we need to discuss the clinician, like if this drug
1868440	1873160	combination has never been used, do you want to try this new drug, or you want more conservative
1873160	1878760	choices, like, you know, these two drug combinations, it right, it has been used more times. Yeah,
1878760	1882920	I mean, this this tuning parameter definitely plays a role. But you know, actually, when you
1882920	1888680	define your reward function, there is another part with the personalized weights. So I was wondering,
1888680	1895240	like, no, we also have similar problems. So we also have like, for example, the survival or
1895240	1900440	quality of life to balance. But then when we provide the personalized weights, and if you
1900440	1906520	change a little bit of the weights, actually, the rules or the decision you will make, or you learn
1906520	1914760	from the data will change. So yeah, that's a good question. Yeah, we can we can discuss more
1914760	1920920	details. Yeah. Thank you so much. I'm going to share my screen.
