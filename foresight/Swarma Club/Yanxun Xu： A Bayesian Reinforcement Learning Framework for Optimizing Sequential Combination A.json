{"text": " Alright, so we'll move on to our next speaker. Yanxun will be our next speaker and she is a associate professor in the Department of Applied Mathematics and Statistics at Johns Hopkins University. Yanxun has a lot of research interest in the reinforcement learning, high-dimensional data analysis, and non-parametric statistics. Okay, so today she is going to talk about a Bayesian reinforcement learning framework for DPR. So you can see the screen, right? Yeah, yeah, great. So thank you Lu for organizing this session and introducing me. So today I will talk about a Bayesian reinforcement learning method for optimizing treatments for HIV patients. So first I'd like to thank all my collaborators in this project. So my peer-de-student Wei and also my collaborators, Leah and Raha from Johns Hopkins University and Yanni from Texas A&M and also two clinicians, Amanda and Jane from Georgetown University and Washington University in St. Louis. Okay, so making the optimal sequential decisions is very important in many diseases. So today I will focus on HIV and we know the emergence of anti-retroviral drugs, ARP, has transformed the HIV infection from a fetal disease to a chronic disease. So it's significantly reduced HIV-related mortality and the common ARP drugs fall into different drug classes with different mechanisms. For example, the first one, the NRPI, it's called a nucleotide or probably can also be a mouse. Okay, so the first drug class called NRPI is called nucleotide reverse transcriptase inhibitor. It inhibits the reverse transcription step in viral replication and other common drug classes include like NRTI, PI, ESTA, EI and the booster. And the model ART drugs usually combines three or more ART drugs from different drug classes. Since such cocktail approach is most efficient in suppressing viral loads. For example, the clinician can prescribe two NRTIs and one drug from instinct and all they can prescribe like two drugs from an ARTI drug class and one from an ARPI or one from the PI. So people living with HIV now are recommended to follow up with their physicians semi-annually by the current HIV treatment guideline. And at each visit, there are social demographics, lab test results, such as CD4 counts and viral loads, and also clinical environments such as BMI and the glucose are collected. And then physician assigns their ART regimen based on their clinical observations. So each ART regimen is a combination of different drugs from different classes. And this process repeats every half a year by average. So nowadays, the US Department of Health and Human Services provides general guidelines on assigning ART treatments. However, these guidelines are usually applied to treatment naive subjects, meaning subjects who are recently diagnosed with HIV and who never took ART treatments before. And for happily pretreated people with HIV, for example, if this person has been diagnosed with HIV for 20 years and has been taking ART drugs for a long time, and there's no consensus. And also the current guideline didn't take into account the potential adverse effects caused by the long term use of ART drugs, because HIV now is more like a chronic disease. And those patients needs to take ART drugs every day. And indefinitely. So you need to take every day to take the drugs every day to suppress the viral loads. And they may cause some long term adverse effects. But for example, like the kidney disease, weight gain or depression, and the current guideline didn't take into account all those side effects. So our goal is to determine the personalized ART regimen to optimize the long term health. That means not only the drugs can supply the viral loads, but can also control the side effects. So large scale HIV studies such as maximize cohort studies. And it provides us opportunities to achieve this goal. And they collect data from participants semi annual visits. And here is the ART treatment history for one randomly selected subject with seven visits. Here's the X axis shows the calendar dates of their visits. And the Y axis shows the ART drug combinations they have this person has taken. And different colors here represent different drug classes and the drug name are marked. So I use a three letter to represent each drug. And also at each visit, the house related environments are recorded. For example, this figure shows the subjects, longitudinal depression scores, viral load, EGFR for kidney function and the BMI. So there are many challenges to optimize the sequential ART regimens in a data driven manner. First, we need to estimate the drug effects. Like before we assign them, we need to understand their drug effects from a high dimensional and unbalanced space. So when I say high dimensional, it means with more than 30 ART drugs, they're all FDA approved on the market. There are a large number of possible drug combinations. So that could be millions of drug combinations you can choose. So even like the feasible drug combinations about like thousands, there's still there are a large number of possible drug combinations. And also when I say the unbalanced, that means some drug combinations are firmly used, but others are wrong. So for example, we can see for this drug combination is two ART drugs and one PR drug. It was the using our database for almost 1000 times, but another similar drug combination. So the same two ART drug, but a different the PR drug, it was only used for 12 times. And the second challenge is how to generate a realistic ART regimen from a large discreet space in the optimization procedure. So after we understand the treatment effects of these drug combinations, we need to assign the drug regimen to patients. That means we need to generate a realistic ART regimen. So then the problem is how to represent each ART regimen. A naive method would be say, okay, I can use a battery vector. So say I have 30 ART, I have 30 ART drugs on the market, and then I can use the battery vector is indimensional battery vector to represent. So if this drug combination contains drug one, then it's one, otherwise it's zero. However, it will cause two to the N possible drug combinations. And many of them will not be feasible. So like I said, usually, you know, people combine the drugs from different drug combinations, but usually we assign like, for example, two or three drugs from ARTI, but like really we assign one drug from PI. So not all these possible two to the N combinations are possible. And then we need the efficient way to represent the drug combination. And lastly, we aim to optimize sequential treatments from observational data. So we have all those data collected from observational study, but we want to assign them to the patients in the future. So the fundamental challenge of optimizing treatments from observational study is this distribution shift issue. So that means the training data may be collected on the different policies from the one we try to evaluate. So we need to address the distribution shift issue. So to address these challenges, we develop a two-step Bayesian reinforcement learning framework. And here is an overview. In the first step, we propose a Bayesian dynamics model for individuals' longitudinal observations using a microverte Gaussian process. And these estimate dynamics describe how individuals' health-related variables evolve over time, condition on their historical states and the treatment histories, with uncertainty quantification. And in the second step, we create a pessimistic environment with uncertainty penalization to mitigate the distribution shift issue and also use an offline reinforcement learning method to optimize the sequential ART regimen. So it's a two-step approach. Okay, so now let's go to the model details. So first, I introduce the problem formulation. Assume for each individual eye, we use XI0 to denote the baseline covariates, such as a race. And the TRI records the visit times. So assume each one has GI visits, and we have PI1 to TIGI to denote the time of each visit. And also assume we have M time varying health state variables. So here we call state variables because they change over time. For example, like H, BMI, EGFR, those clinical variables or demographics, they are collected in each visit. And also the I to represent the ART drug combination used by individual eye during the time period, TIGI minus 1 to TIGI. And then the data can be summarized as D. So for each subject, so we have a total of I subjects, and we have baseline covariates, the time of each visit, and the time varying state variables, and also the drug information. And then we use the YIG bar. So the bar is a common fun we really use to represent the history. So the YIG bar means all the state variables before the current visit J. And the ZIG bar means all the treatments this person has been taking on pure the time J. And also we use the dynamic, we use the after dynamic model. So that means condition on the YIG bar and the ZIG plus one, we update the state variables from YIG to YG plus one. So remember the first step of our method to learn the dynamic model of how to transform from YG to YG plus one. Of course, the YG plus one condition on the whole history of the YG and the treatments the IJ. Okay, so our goal is to optimize the ART assignments to maximize the individual's long-term house outcome. So because we want to maximize the house outcome, essentially we can transform the problem to an optimization problem. So this optimization problem is like we first define for each individual I, we suppose she already has GI visits. So if this person is a new patient, so the GI will be equal zero. So the GI can be zero, or if this person already has GI visits, and then we want to predict the next few visits, for example. And then we let the Y new and the GI new to denote her future longitudinal states and the treatments, because our reward may depend on her future states. For example, we want her less than the next two years, the depression is minimal. And assume for any future visited J, the ART regimen is assigned through a policy function PI. So that means if we can learn, if we can prioritize this function PI, and we can optimize data and the equivalently we can optimize the treatment. And let's say we assign a stochastic reward function RI that depends on the house states. So we can depend, we can define the reward as for example, this person, their, their very loaded is low, and the pressure set is low, and the BMI is in the normal range, and the kidney function is a normal range. Okay, so for example, if our goal is to select the sequential ART regimen that leads to lowest accumulated very low in the next two years, and it is coming an active sum of the very loads. Okay, so they notice the expected reward for any individual I to be the following. Because we, in the first step, we learn a Bayesian model. So essentially, you can generate their future states. And also, so we can generate the future states, why are you from the learned dynamic model, and we generate the ZI new from their the PI, the function. So we learned the PI from their parameter as the function PI. And also, because we learned the dynamic model, and then we can integrate out certain things, optimization procedure. So that's kind of the benefits of using the Bayesian framework in the first step. Because in the decision making step, we can integrate out uncertainty, we can adjust for this uncertainty quantification from their uncertainty of their dynamic, their dynamic model. And our goal is to find as optimized optimal policy function PI that parameterized by data i star. So we want to find as an optimal parameter that can maximize the r i theta. So that's a problem. Okay, so now we have already defined our reward function r i. And we want to find as a parameter theta that can maximize the r i. And essentially, it's a classical reinforcement learning problem. And we can use the policy gradient method to solve the problem. So essentially, if you can calculate the gradient of r i, and then you can use the policy gradient method, essentially, you update the theta by this formula. So it's also classical results, it's that you update the theta by the previous theta, and then plus some step size s i, q, and then times their gradient. So the essential problem is how to calculate the gradient of our reward function. So we can see our reward function is relatively complex, right? You take the expected value of the reward function r i, and r is a function of y mu, and y mu you need to generate from the predict distribution of your dynamical model. And besides all of that, we also need to integrate out the uncertainty from the dynamical model by the P5 condition on D, that's a posterior distribution of the phi. So it's not easy to calculate the gradient of this r i theta. Okay, so luckily for the policy gradient method, there's a way to calculate that. So if you're interested in details, you can find the details in the paper, but we can represent the derivative of r i theta as follows. And this formula looks very complex, but we can't decompose this into three separate steps. The first step is this step. So it's about the log of our policy function. So essentially, if you can parameterize the ART assignment function, and then you can optimize that. So that's our first challenge. We need to parameterize the policy function. And the second step is how to generate the future states based on our dynamical model. So that's the second step, is we want to sample the future states. And the last thing is if we can define the reward function. So essentially, you decompose the calculating this gradient by three sub steps. If we can sample future states, if we can define the reward function, if we can parameterize the policy function, and then we can calculate the gradient of the reward. And then we can plug into policy gradient method and then get the optimal theta. Okay, so now I will introduce each part. So how to sample the future states? So if we want to sample the future states, basically, we need a model, and then we do the predictive inference. And in this case, we have multiple longitudinal states. And we use a multivariate Gaussian process. The reason we use a multivariate Gaussian process because it's a popular choice for modeling irregular space multivariate longitudinal data with great flexibility and also natural uncertainty quantification. And here's irregular, it comes from the missing data because sometimes maybe some visits and some measurements may be missing. Okay, so the multivariate Gaussian process, the whole framework of this multivariate Gaussian process is relatively standard. We use yimt to denote the nth longitudinal variable for treat for individual i at the time t. And we have a mean function ft and the answer id residue epsilon. So for this f, we assume the multivariate Gaussian process is distributed. So we will have the mean function. So the next slide I'll introduce how we model the mean function. And for the various covariance parts, we assume they're the separable covariance function. So they're here the cm to denote the correlation, represent the correlation among different state variables because they could be correlated. And the ct to represent their correlation among the time. So this separable covariance function adjusts for the correlation among variables and also along the time. Okay, and here for the ct, the correlation kernel that for the temporal correlation, we use the oil kernel because we expect the curve that's not too smooth. Okay, so for the mean parts, that's kind of the key country, one of the key contributions of this model is that for the mean parts, the first two parts are kind of standard. We use a baseline, it's like linear Mase effect model, we have fixed effects and random effects. But the how to model the drug combination effects, it's a key thing. So remember I said, for the drug combination, it's a high dimensional space. So how to represent the drug combinations. Here we borrow like the kernel idea. So the way we model that is we assume there exists. So we assume there there's a okay, okay, so we assume there's a we define a drug similarity regimen function kappa here. So because the z itself is a high dimensional space. So to reduce the dimensionality, we introduce kappa. So it's like borrow the kernel idea, we reduce the dimension from the original all the drug combinations to a manageable size as capital D here. So we introduce a bunch of representative drug regimens ZD. And then we calculate the similarity between each drug, possible drug combination with ZD. And then as long as we can estimate the parameter gamma and D here, and then we can represent the drug effects for each drug combination. And the way we define their similarity function, it depends on two properties. The first one is we want sharing of information because the similar drugs from because the drugs from the same drug class, they have similar drug effects because we share the same mechanisms. So we want to share information from different drug combinations. And also from this kernel, the introduction of this similarity function, it can reduce the high dimensionality. So let me briefly introduce the idea of this kernel. Because of time limit, I will not see the detail. So consider a straight way to compute the drug combination similarity. And the one straight forward idea is we use linear kernel. So the linear kernel, they can compute the similarity between regimens based on the proportion of common drugs that two regimens share. So for example, here, we have three drug combinations. And all of them use two same drugs from the NRTI class. So D4T plus LAM. And assume the third drug, the first two regimens, they choose one PI drug, but different PI drugs. And another one is choose NRTI. So you can use a linear kernel. That means the pairwise similarity among these three kernels will be two over three, right? Because they have three drugs, and they share two same drugs. However, there are some disadvantages. For example, the first two drug combinations. So to both of them, they use two same NRTI drugs. And the third drug, they belong to the same drug class. Because the same drug class, they share the same madness. So we would expect the similarity between the first two drug combinations would be larger or would be higher compared to the similarity between there, between them and the third drug. Because the third, because the third drug combination, they have the drug from a different drug class. And another example, for example, if you have these two drug combinations, both of them have two drugs from NRTI drug class, and one drug from the PI drug class. If we use a linear kernel, and they would share zero similarity, because they don't share any of common drugs. However, we know the same drug class will share some similarity. So the good method, we should borrow this clinical information and share some similarity for these two drug combinations. So the way we set up the, we define the drug similarity is we use the sub subject kernel. So the sub subject kernel is the idea was to represent the sentences in natural language processing literature. And here we represent our drug combination by a tree structure. And the subject kernel can represent the similarity at all levels of the tree representation. So essentially, the upper level is ART. And then we have the second level to represent which drug class we draw the drugs. And the third level represents how many drugs from each drug class. And the third level represents the each drug from each drug class. And then the sub subject kernel can represent the whole similarity. For example, like regimen A and B, they can adjust for their similarity is a blue box. And for regimen A and C, they can adjust for their drug similarity is a yellow box. Even, you know, they don't share any common drugs, but you can still incorporate their similarity. Okay, so now I have introduced this Markov-Berica Gaussian process to model the longitudinal states. And then if we have a model and we have our own parameters, and then we can write down the likelihood, and you can assign the price to all unknown parameters, you can obtain the posterior distribution from the MCMC. So I will skip all the computational details here. But essentially, now we finish the first step. So we have the Markov-Berica normal model, we can sample future states. Okay, so the second one is how to define the reward function. And the reward function, it depends on the clinical goal, right? So here, it depends on how we define the long term house for each individual. So here, after consultation with the clinicians, we determine that we define the reward that depends on the barrel load, kidney function and the depression. So we want to care about first, you know, whether it can successfully suppress the barrel load, and also maintain a good kidney function and also the good mental health. So let's see, our goal is we will, so here we can say we want to maximize the overall house in the next two years. So remember, the visits are semi-annual visits. So that's why here the sum is from the next visit, next four visits, because next four visits means the overall good health in the next two years. And then we want the depression, this as small as possible. And also, oh yeah, here is the next four visits. And also for depression, it's as small as possible. And for the barrel load and the EGFR, because as long as they are normal threshold, it will be fine. So we define this kind of step function, as long as they are in the normal range, it'll be fine. And if it's outside of the normal range, and we give certain penalty. And also here, we have to personalize the weights, WI. So for example, if some person, they more care about the depression, and then the WI1 can have a higher weight. So it's personalized and determined by the physician and also patient himself. And also to mitigate the distribution shift issue, we use certainly penalized reward. That's another advantage of using the Bayesian method in the first step, because we can easily quantify the uncertainty. So this idea is by this paper by you from UC Berkeley's group. And essentially, we define a pessimistic environment by introducing a penalized reward. So the RA is defined by the previous slide. But now we penalize the uncertainty of the, it's the predictive variability of the state and their treatments. And it's a tuning parameter we need to learn. Okay. And then we use a posterior predict distribution to quantify the uncertainty again, because we have a Bayesian model, so that's very straightforward. Okay, so now we define a reward function. And the last step is how to parameterize the policy function. So to prioritize the policy function, we make this also the three types of decision after talking with clinicians. So essentially, we decompose this to several steps. So first, if this person has been using ART drugs for a long time, and we will see if this person needs to switch the regimen or not. So if the older drug works, and we can just keep using the older drug. So this is what we will represent as a logistic regression method in the logistic regression model. As long as all the health measurements are in within the normal range, and then we will decide to just, you know, keep the old drug. And if one of them is not in the normal range, we will switch. If this person needs to switch, and then we will need to generate a new regimen. And because we used the three representation, and then we can now decide, you know, if we need to switch how to generate a new regimen, essentially, we need to decide like which drug class and how many drugs use the initial class and also which individual drugs at each class. So this essentially it's a non-central hypergeometric distribution. Again, I skipped all the details. It's kind of a little bit complex. So we have these three levels of three decisions. Okay, so now we have already finished these three steps. So we have multivariate Gaussian process to some whole future states, and we define reward function. And then we have ways to prioritize a policy function. We can use a policy gradient method to optimize a print. Okay. Okay, so now, so here I finish all the matter introduction. Last part of the slides is I will introduce real data analysis results. So for the real data, we got about 300 women from the Washington DC site from the white study. And also now we get four state variables at each visit, depression, viral load, EGFR, and BMI. And there are about 8% missing data. And the baseline covariates we consider include age, smoking status, substance use, employment status, hypertension, diabetes. And in this study, we have 31 ART drugs and six drug classes. And we choose 105 representative drug regiments. So those regiments based on the popularity of the drug combinations, if they have been used a lot of times for the from the patients, and then we would know that as representative ART regimen. Okay, so here is one hypothetical patient. So we'll use this example to demonstrate the precision medicine, the, you know, the utility of the clinical utility of the proposed methods. Okay, so this person has been has been had 31 visits. And here is their history of treatments. And for these patients, we assume their weights as one third, one third, one third. Okay, and then we run our optimization method. And here we can see the expected reward versus the SGD iteration. So it became relatively stable after 1,000 iterations. And here is optimal regimen is the next two years. So we can see at the visit to there are two ART drugs when you see one PR and one poster, and then it changes one new regimen for visits 33 to 35. Okay, and also here I want to show under their estimated optimal regiments, that's the predicted depression stores. And we can see for their visits from 32 to 36, they're about 23% improvement of depression. So that also shows the clinical utility of our, you know, newly assigned optimal drug combinations. Okay, I will skip the next example due to the time limit. Oh, yeah. Okay, to summarize, we, we propose a like a Bayesian reforcing learning approach is a two step approach. And it can learn their dynamics with uncertainty quantification, it can also assign the long term optimal drug combinations to optimize each individual's health. Okay, yeah, thank you. Thank you so much, Yanxun. Any questions from the audience? Yanxun, those are very exciting work. I actually have some questions because you touched a very good point where you need to balance the priority like competing priorities when you are in the clinical practice. But since we are a little bit over time, so I probably will talk to you later about that. I was wondering like how the uncertainty will be impacted by how you define your reward function. Oh, yeah. So the uncertainty part, you know, how the uncertainty affects the final decision depending on how you tune the parameter. So like here, and yeah, I skip that part, but here you can show if we have different tooling parameters like lambda equal zero, you don't penalize at all. And then you have this drug combination. And if you use like increase the lambda, and then it will penalize the uncertainty, it's kind of uncertainties reflected by the sample size in their data. If this drug combination has has been used a lot of times, it has relatively narrow uncertainty, it had never been used, then it has a lot of uncertainty. So for example, here with lambda equal zero, they actually recommend this drug combination is the first recommendation. So it actually never been used in the in the data. So that's kind of create a trade off, like we need to discuss the clinician, like if this drug combination has never been used, do you want to try this new drug, or you want more conservative choices, like, you know, these two drug combinations, it right, it has been used more times. Yeah, I mean, this this tuning parameter definitely plays a role. But you know, actually, when you define your reward function, there is another part with the personalized weights. So I was wondering, like, no, we also have similar problems. So we also have like, for example, the survival or quality of life to balance. But then when we provide the personalized weights, and if you change a little bit of the weights, actually, the rules or the decision you will make, or you learn from the data will change. So yeah, that's a good question. Yeah, we can we can discuss more details. Yeah. Thank you so much. I'm going to share my screen.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.28, "text": " Alright, so we'll move on to our next speaker. Yanxun will be our next speaker", "tokens": [50364, 2798, 11, 370, 321, 603, 1286, 322, 281, 527, 958, 8145, 13, 13633, 87, 409, 486, 312, 527, 958, 8145, 50728], "temperature": 0.0, "avg_logprob": -0.21801552873976687, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.019342411309480667}, {"id": 1, "seek": 0, "start": 7.28, "end": 12.72, "text": " and she is a associate professor in the Department of Applied Mathematics and", "tokens": [50728, 293, 750, 307, 257, 14644, 8304, 294, 264, 5982, 295, 3132, 39459, 15776, 37541, 293, 51000], "temperature": 0.0, "avg_logprob": -0.21801552873976687, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.019342411309480667}, {"id": 2, "seek": 0, "start": 12.72, "end": 19.080000000000002, "text": " Statistics at Johns Hopkins University. Yanxun has a lot of research interest in", "tokens": [51000, 49226, 412, 37016, 29999, 3535, 13, 13633, 87, 409, 575, 257, 688, 295, 2132, 1179, 294, 51318], "temperature": 0.0, "avg_logprob": -0.21801552873976687, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.019342411309480667}, {"id": 3, "seek": 0, "start": 19.080000000000002, "end": 23.36, "text": " the reinforcement learning, high-dimensional data analysis, and", "tokens": [51318, 264, 29280, 2539, 11, 1090, 12, 18759, 1412, 5215, 11, 293, 51532], "temperature": 0.0, "avg_logprob": -0.21801552873976687, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.019342411309480667}, {"id": 4, "seek": 0, "start": 23.36, "end": 29.36, "text": " non-parametric statistics. Okay, so today she is going to talk about a Bayesian", "tokens": [51532, 2107, 12, 2181, 335, 17475, 12523, 13, 1033, 11, 370, 965, 750, 307, 516, 281, 751, 466, 257, 7840, 42434, 51832], "temperature": 0.0, "avg_logprob": -0.21801552873976687, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.019342411309480667}, {"id": 5, "seek": 2936, "start": 29.36, "end": 36.04, "text": " reinforcement learning framework for DPR. So you can see the screen, right? Yeah,", "tokens": [50364, 29280, 2539, 8388, 337, 413, 15958, 13, 407, 291, 393, 536, 264, 2568, 11, 558, 30, 865, 11, 50698], "temperature": 0.0, "avg_logprob": -0.2785179431621845, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0018651043064892292}, {"id": 6, "seek": 2936, "start": 36.04, "end": 41.8, "text": " yeah, great. So thank you Lu for organizing this session and introducing me. So", "tokens": [50698, 1338, 11, 869, 13, 407, 1309, 291, 5047, 337, 17608, 341, 5481, 293, 15424, 385, 13, 407, 50986], "temperature": 0.0, "avg_logprob": -0.2785179431621845, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0018651043064892292}, {"id": 7, "seek": 2936, "start": 41.8, "end": 44.879999999999995, "text": " today I will talk about a Bayesian reinforcement learning method for", "tokens": [50986, 965, 286, 486, 751, 466, 257, 7840, 42434, 29280, 2539, 3170, 337, 51140], "temperature": 0.0, "avg_logprob": -0.2785179431621845, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0018651043064892292}, {"id": 8, "seek": 2936, "start": 44.879999999999995, "end": 51.44, "text": " optimizing treatments for HIV patients. So first I'd like to thank all my", "tokens": [51140, 40425, 15795, 337, 15907, 4209, 13, 407, 700, 286, 1116, 411, 281, 1309, 439, 452, 51468], "temperature": 0.0, "avg_logprob": -0.2785179431621845, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0018651043064892292}, {"id": 9, "seek": 2936, "start": 51.44, "end": 56.32, "text": " collaborators in this project. So my peer-de-student Wei and also my", "tokens": [51468, 39789, 294, 341, 1716, 13, 407, 452, 15108, 12, 1479, 12, 372, 24064, 21174, 293, 611, 452, 51712], "temperature": 0.0, "avg_logprob": -0.2785179431621845, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.0018651043064892292}, {"id": 10, "seek": 5632, "start": 56.32, "end": 60.64, "text": " collaborators, Leah and Raha from Johns Hopkins University and Yanni from", "tokens": [50364, 39789, 11, 38591, 293, 497, 4408, 490, 37016, 29999, 3535, 293, 398, 35832, 490, 50580], "temperature": 0.0, "avg_logprob": -0.3111161221279187, "compression_ratio": 1.5059760956175299, "no_speech_prob": 0.0020810607820749283}, {"id": 11, "seek": 5632, "start": 60.64, "end": 65.8, "text": " Texas A&M and also two clinicians, Amanda and Jane from Georgetown University", "tokens": [50580, 7885, 316, 5, 44, 293, 611, 732, 32862, 11, 20431, 293, 13048, 490, 34848, 3535, 50838], "temperature": 0.0, "avg_logprob": -0.3111161221279187, "compression_ratio": 1.5059760956175299, "no_speech_prob": 0.0020810607820749283}, {"id": 12, "seek": 5632, "start": 65.8, "end": 74.16, "text": " and Washington University in St. Louis. Okay, so making the optimal", "tokens": [50838, 293, 6149, 3535, 294, 745, 13, 9763, 13, 1033, 11, 370, 1455, 264, 16252, 51256], "temperature": 0.0, "avg_logprob": -0.3111161221279187, "compression_ratio": 1.5059760956175299, "no_speech_prob": 0.0020810607820749283}, {"id": 13, "seek": 5632, "start": 74.16, "end": 79.32, "text": " sequential decisions is very important in many diseases. So today I will focus on", "tokens": [51256, 42881, 5327, 307, 588, 1021, 294, 867, 11044, 13, 407, 965, 286, 486, 1879, 322, 51514], "temperature": 0.0, "avg_logprob": -0.3111161221279187, "compression_ratio": 1.5059760956175299, "no_speech_prob": 0.0020810607820749283}, {"id": 14, "seek": 5632, "start": 79.32, "end": 85.96000000000001, "text": " HIV and we know the emergence of anti-retroviral drugs, ARP, has transformed", "tokens": [51514, 15907, 293, 321, 458, 264, 36211, 295, 6061, 12, 1505, 24088, 35083, 7766, 11, 8943, 47, 11, 575, 16894, 51846], "temperature": 0.0, "avg_logprob": -0.3111161221279187, "compression_ratio": 1.5059760956175299, "no_speech_prob": 0.0020810607820749283}, {"id": 15, "seek": 8596, "start": 85.96, "end": 90.83999999999999, "text": " the HIV infection from a fetal disease to a chronic disease. So it's", "tokens": [50364, 264, 15907, 11764, 490, 257, 15136, 304, 4752, 281, 257, 14493, 4752, 13, 407, 309, 311, 50608], "temperature": 0.0, "avg_logprob": -0.22785886827405993, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0016223657876253128}, {"id": 16, "seek": 8596, "start": 90.83999999999999, "end": 97.44, "text": " significantly reduced HIV-related mortality and the common ARP drugs fall", "tokens": [50608, 10591, 9212, 15907, 12, 12004, 23330, 293, 264, 2689, 8943, 47, 7766, 2100, 50938], "temperature": 0.0, "avg_logprob": -0.22785886827405993, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0016223657876253128}, {"id": 17, "seek": 8596, "start": 97.44, "end": 102.75999999999999, "text": " into different drug classes with different mechanisms. For example, the", "tokens": [50938, 666, 819, 4110, 5359, 365, 819, 15902, 13, 1171, 1365, 11, 264, 51204], "temperature": 0.0, "avg_logprob": -0.22785886827405993, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0016223657876253128}, {"id": 18, "seek": 8596, "start": 102.75999999999999, "end": 108.03999999999999, "text": " first one, the NRPI, it's called a nucleotide or probably can also be a mouse.", "tokens": [51204, 700, 472, 11, 264, 38399, 31701, 11, 309, 311, 1219, 257, 14962, 310, 482, 420, 1391, 393, 611, 312, 257, 9719, 13, 51468], "temperature": 0.0, "avg_logprob": -0.22785886827405993, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0016223657876253128}, {"id": 19, "seek": 8596, "start": 108.03999999999999, "end": 112.56, "text": " Okay, so the first drug class called NRPI is called nucleotide reverse", "tokens": [51468, 1033, 11, 370, 264, 700, 4110, 1508, 1219, 38399, 31701, 307, 1219, 14962, 310, 482, 9943, 51694], "temperature": 0.0, "avg_logprob": -0.22785886827405993, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.0016223657876253128}, {"id": 20, "seek": 11256, "start": 112.56, "end": 117.8, "text": " transcriptase inhibitor. It inhibits the reverse transcription step in viral", "tokens": [50364, 24444, 651, 20406, 3029, 13, 467, 20406, 1208, 264, 9943, 35288, 1823, 294, 16132, 50626], "temperature": 0.0, "avg_logprob": -0.2988312013687626, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.001115772989578545}, {"id": 21, "seek": 11256, "start": 117.8, "end": 123.56, "text": " replication and other common drug classes include like NRTI, PI, ESTA, EI and", "tokens": [50626, 39911, 293, 661, 2689, 4110, 5359, 4090, 411, 38399, 5422, 11, 27176, 11, 462, 28075, 11, 462, 40, 293, 50914], "temperature": 0.0, "avg_logprob": -0.2988312013687626, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.001115772989578545}, {"id": 22, "seek": 11256, "start": 123.56, "end": 129.72, "text": " the booster. And the model ART drugs usually combines three or more ART drugs", "tokens": [50914, 264, 29275, 13, 400, 264, 2316, 8943, 51, 7766, 2673, 29520, 1045, 420, 544, 8943, 51, 7766, 51222], "temperature": 0.0, "avg_logprob": -0.2988312013687626, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.001115772989578545}, {"id": 23, "seek": 11256, "start": 129.72, "end": 135.12, "text": " from different drug classes. Since such cocktail approach is most efficient in", "tokens": [51222, 490, 819, 4110, 5359, 13, 4162, 1270, 26382, 3109, 307, 881, 7148, 294, 51492], "temperature": 0.0, "avg_logprob": -0.2988312013687626, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.001115772989578545}, {"id": 24, "seek": 11256, "start": 135.12, "end": 141.64000000000001, "text": " suppressing viral loads. For example, the clinician can prescribe two NRTIs and one", "tokens": [51492, 1003, 18605, 16132, 12668, 13, 1171, 1365, 11, 264, 45962, 393, 49292, 732, 38399, 51, 6802, 293, 472, 51818], "temperature": 0.0, "avg_logprob": -0.2988312013687626, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.001115772989578545}, {"id": 25, "seek": 14164, "start": 141.92, "end": 146.48, "text": " drug from instinct and all they can prescribe like two drugs from an ARTI", "tokens": [50378, 4110, 490, 16556, 293, 439, 436, 393, 49292, 411, 732, 7766, 490, 364, 8943, 5422, 50606], "temperature": 0.0, "avg_logprob": -0.27432362929634424, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0009397180401720107}, {"id": 26, "seek": 14164, "start": 146.48, "end": 155.0, "text": " drug class and one from an ARPI or one from the PI. So people living with", "tokens": [50606, 4110, 1508, 293, 472, 490, 364, 8943, 31701, 420, 472, 490, 264, 27176, 13, 407, 561, 2647, 365, 51032], "temperature": 0.0, "avg_logprob": -0.27432362929634424, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0009397180401720107}, {"id": 27, "seek": 14164, "start": 155.0, "end": 159.56, "text": " HIV now are recommended to follow up with their physicians semi-annually by the", "tokens": [51032, 15907, 586, 366, 9628, 281, 1524, 493, 365, 641, 21966, 12909, 12, 969, 671, 538, 264, 51260], "temperature": 0.0, "avg_logprob": -0.27432362929634424, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0009397180401720107}, {"id": 28, "seek": 14164, "start": 159.56, "end": 165.44, "text": " current HIV treatment guideline. And at each visit, there are social demographics,", "tokens": [51260, 2190, 15907, 5032, 41653, 13, 400, 412, 1184, 3441, 11, 456, 366, 2093, 36884, 11, 51554], "temperature": 0.0, "avg_logprob": -0.27432362929634424, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0009397180401720107}, {"id": 29, "seek": 14164, "start": 165.44, "end": 170.32, "text": " lab test results, such as CD4 counts and viral loads, and also clinical", "tokens": [51554, 2715, 1500, 3542, 11, 1270, 382, 6743, 19, 14893, 293, 16132, 12668, 11, 293, 611, 9115, 51798], "temperature": 0.0, "avg_logprob": -0.27432362929634424, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0009397180401720107}, {"id": 30, "seek": 17032, "start": 170.32, "end": 175.23999999999998, "text": " environments such as BMI and the glucose are collected. And then physician", "tokens": [50364, 12388, 1270, 382, 363, 13808, 293, 264, 23997, 366, 11087, 13, 400, 550, 16456, 50610], "temperature": 0.0, "avg_logprob": -0.12318195615495954, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.000910851638764143}, {"id": 31, "seek": 17032, "start": 175.23999999999998, "end": 180.12, "text": " assigns their ART regimen based on their clinical observations. So each ART", "tokens": [50610, 6269, 82, 641, 8943, 51, 1121, 19676, 2361, 322, 641, 9115, 18163, 13, 407, 1184, 8943, 51, 50854], "temperature": 0.0, "avg_logprob": -0.12318195615495954, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.000910851638764143}, {"id": 32, "seek": 17032, "start": 180.12, "end": 185.23999999999998, "text": " regimen is a combination of different drugs from different classes. And this", "tokens": [50854, 1121, 19676, 307, 257, 6562, 295, 819, 7766, 490, 819, 5359, 13, 400, 341, 51110], "temperature": 0.0, "avg_logprob": -0.12318195615495954, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.000910851638764143}, {"id": 33, "seek": 17032, "start": 185.23999999999998, "end": 191.56, "text": " process repeats every half a year by average. So nowadays, the US Department", "tokens": [51110, 1399, 35038, 633, 1922, 257, 1064, 538, 4274, 13, 407, 13434, 11, 264, 2546, 5982, 51426], "temperature": 0.0, "avg_logprob": -0.12318195615495954, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.000910851638764143}, {"id": 34, "seek": 17032, "start": 191.56, "end": 197.07999999999998, "text": " of Health and Human Services provides general guidelines on assigning ART", "tokens": [51426, 295, 5912, 293, 10294, 12124, 6417, 2674, 12470, 322, 49602, 8943, 51, 51702], "temperature": 0.0, "avg_logprob": -0.12318195615495954, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.000910851638764143}, {"id": 35, "seek": 19708, "start": 197.08, "end": 201.88000000000002, "text": " treatments. However, these guidelines are usually applied to treatment", "tokens": [50364, 15795, 13, 2908, 11, 613, 12470, 366, 2673, 6456, 281, 5032, 50604], "temperature": 0.0, "avg_logprob": -0.13688651994727125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002672529313713312}, {"id": 36, "seek": 19708, "start": 201.88000000000002, "end": 207.44, "text": " naive subjects, meaning subjects who are recently diagnosed with HIV and who", "tokens": [50604, 29052, 13066, 11, 3620, 13066, 567, 366, 3938, 16899, 365, 15907, 293, 567, 50882], "temperature": 0.0, "avg_logprob": -0.13688651994727125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002672529313713312}, {"id": 37, "seek": 19708, "start": 207.44, "end": 213.44, "text": " never took ART treatments before. And for happily pretreated people with HIV,", "tokens": [50882, 1128, 1890, 8943, 51, 15795, 949, 13, 400, 337, 19909, 1162, 26559, 561, 365, 15907, 11, 51182], "temperature": 0.0, "avg_logprob": -0.13688651994727125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002672529313713312}, {"id": 38, "seek": 19708, "start": 213.44, "end": 218.8, "text": " for example, if this person has been diagnosed with HIV for 20 years and has", "tokens": [51182, 337, 1365, 11, 498, 341, 954, 575, 668, 16899, 365, 15907, 337, 945, 924, 293, 575, 51450], "temperature": 0.0, "avg_logprob": -0.13688651994727125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002672529313713312}, {"id": 39, "seek": 19708, "start": 218.8, "end": 224.56, "text": " been taking ART drugs for a long time, and there's no consensus. And also the", "tokens": [51450, 668, 1940, 8943, 51, 7766, 337, 257, 938, 565, 11, 293, 456, 311, 572, 19115, 13, 400, 611, 264, 51738], "temperature": 0.0, "avg_logprob": -0.13688651994727125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002672529313713312}, {"id": 40, "seek": 22456, "start": 224.56, "end": 229.24, "text": " current guideline didn't take into account the potential adverse effects", "tokens": [50364, 2190, 41653, 994, 380, 747, 666, 2696, 264, 3995, 27590, 5065, 50598], "temperature": 0.0, "avg_logprob": -0.19223468223314608, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0004954640753567219}, {"id": 41, "seek": 22456, "start": 229.28, "end": 235.0, "text": " caused by the long term use of ART drugs, because HIV now is more like a", "tokens": [50600, 7008, 538, 264, 938, 1433, 764, 295, 8943, 51, 7766, 11, 570, 15907, 586, 307, 544, 411, 257, 50886], "temperature": 0.0, "avg_logprob": -0.19223468223314608, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0004954640753567219}, {"id": 42, "seek": 22456, "start": 235.04, "end": 241.12, "text": " chronic disease. And those patients needs to take ART drugs every day. And", "tokens": [50888, 14493, 4752, 13, 400, 729, 4209, 2203, 281, 747, 8943, 51, 7766, 633, 786, 13, 400, 51192], "temperature": 0.0, "avg_logprob": -0.19223468223314608, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0004954640753567219}, {"id": 43, "seek": 22456, "start": 241.16, "end": 245.32, "text": " indefinitely. So you need to take every day to take the drugs every day to", "tokens": [51194, 24162, 10925, 13, 407, 291, 643, 281, 747, 633, 786, 281, 747, 264, 7766, 633, 786, 281, 51402], "temperature": 0.0, "avg_logprob": -0.19223468223314608, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0004954640753567219}, {"id": 44, "seek": 22456, "start": 245.32, "end": 250.4, "text": " suppress the viral loads. And they may cause some long term adverse effects.", "tokens": [51402, 26835, 264, 16132, 12668, 13, 400, 436, 815, 3082, 512, 938, 1433, 27590, 5065, 13, 51656], "temperature": 0.0, "avg_logprob": -0.19223468223314608, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0004954640753567219}, {"id": 45, "seek": 25040, "start": 250.56, "end": 254.8, "text": " But for example, like the kidney disease, weight gain or depression, and the", "tokens": [50372, 583, 337, 1365, 11, 411, 264, 19000, 4752, 11, 3364, 6052, 420, 10799, 11, 293, 264, 50584], "temperature": 0.0, "avg_logprob": -0.2476628840654746, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0006985185900703073}, {"id": 46, "seek": 25040, "start": 254.8, "end": 258.84000000000003, "text": " current guideline didn't take into account all those side effects. So our", "tokens": [50584, 2190, 41653, 994, 380, 747, 666, 2696, 439, 729, 1252, 5065, 13, 407, 527, 50786], "temperature": 0.0, "avg_logprob": -0.2476628840654746, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0006985185900703073}, {"id": 47, "seek": 25040, "start": 258.84000000000003, "end": 264.4, "text": " goal is to determine the personalized ART regimen to optimize the long term", "tokens": [50786, 3387, 307, 281, 6997, 264, 28415, 8943, 51, 1121, 19676, 281, 19719, 264, 938, 1433, 51064], "temperature": 0.0, "avg_logprob": -0.2476628840654746, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0006985185900703073}, {"id": 48, "seek": 25040, "start": 264.4, "end": 269.92, "text": " health. That means not only the drugs can supply the viral loads, but can also", "tokens": [51064, 1585, 13, 663, 1355, 406, 787, 264, 7766, 393, 5847, 264, 16132, 12668, 11, 457, 393, 611, 51340], "temperature": 0.0, "avg_logprob": -0.2476628840654746, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0006985185900703073}, {"id": 49, "seek": 25040, "start": 269.92, "end": 277.0, "text": " control the side effects. So large scale HIV studies such as maximize", "tokens": [51340, 1969, 264, 1252, 5065, 13, 407, 2416, 4373, 15907, 5313, 1270, 382, 19874, 51694], "temperature": 0.0, "avg_logprob": -0.2476628840654746, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0006985185900703073}, {"id": 50, "seek": 27700, "start": 277.0, "end": 282.04, "text": " cohort studies. And it provides us opportunities to achieve this goal. And", "tokens": [50364, 28902, 5313, 13, 400, 309, 6417, 505, 4786, 281, 4584, 341, 3387, 13, 400, 50616], "temperature": 0.0, "avg_logprob": -0.18755707460291246, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0055534010753035545}, {"id": 51, "seek": 27700, "start": 282.04, "end": 287.44, "text": " they collect data from participants semi annual visits. And here is the ART", "tokens": [50616, 436, 2500, 1412, 490, 10503, 12909, 9784, 17753, 13, 400, 510, 307, 264, 8943, 51, 50886], "temperature": 0.0, "avg_logprob": -0.18755707460291246, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0055534010753035545}, {"id": 52, "seek": 27700, "start": 287.6, "end": 292.48, "text": " treatment history for one randomly selected subject with seven visits. Here's", "tokens": [50894, 5032, 2503, 337, 472, 16979, 8209, 3983, 365, 3407, 17753, 13, 1692, 311, 51138], "temperature": 0.0, "avg_logprob": -0.18755707460291246, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0055534010753035545}, {"id": 53, "seek": 27700, "start": 292.48, "end": 297.72, "text": " the X axis shows the calendar dates of their visits. And the Y axis shows the", "tokens": [51138, 264, 1783, 10298, 3110, 264, 12183, 11691, 295, 641, 17753, 13, 400, 264, 398, 10298, 3110, 264, 51400], "temperature": 0.0, "avg_logprob": -0.18755707460291246, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0055534010753035545}, {"id": 54, "seek": 27700, "start": 297.72, "end": 303.12, "text": " ART drug combinations they have this person has taken. And different colors", "tokens": [51400, 8943, 51, 4110, 21267, 436, 362, 341, 954, 575, 2726, 13, 400, 819, 4577, 51670], "temperature": 0.0, "avg_logprob": -0.18755707460291246, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0055534010753035545}, {"id": 55, "seek": 30312, "start": 303.12, "end": 307.28000000000003, "text": " here represent different drug classes and the drug name are marked. So I use a", "tokens": [50364, 510, 2906, 819, 4110, 5359, 293, 264, 4110, 1315, 366, 12658, 13, 407, 286, 764, 257, 50572], "temperature": 0.0, "avg_logprob": -0.17211725495078348, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0011334455339238048}, {"id": 56, "seek": 30312, "start": 307.28000000000003, "end": 314.0, "text": " three letter to represent each drug. And also at each visit, the house related", "tokens": [50572, 1045, 5063, 281, 2906, 1184, 4110, 13, 400, 611, 412, 1184, 3441, 11, 264, 1782, 4077, 50908], "temperature": 0.0, "avg_logprob": -0.17211725495078348, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0011334455339238048}, {"id": 57, "seek": 30312, "start": 314.0, "end": 318.52, "text": " environments are recorded. For example, this figure shows the subjects,", "tokens": [50908, 12388, 366, 8287, 13, 1171, 1365, 11, 341, 2573, 3110, 264, 13066, 11, 51134], "temperature": 0.0, "avg_logprob": -0.17211725495078348, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0011334455339238048}, {"id": 58, "seek": 30312, "start": 318.52, "end": 323.08, "text": " longitudinal depression scores, viral load, EGFR for kidney function and the", "tokens": [51134, 48250, 10799, 13444, 11, 16132, 3677, 11, 462, 38, 34658, 337, 19000, 2445, 293, 264, 51362], "temperature": 0.0, "avg_logprob": -0.17211725495078348, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0011334455339238048}, {"id": 59, "seek": 30312, "start": 323.08, "end": 329.72, "text": " BMI. So there are many challenges to optimize the sequential ART regimens", "tokens": [51362, 363, 13808, 13, 407, 456, 366, 867, 4759, 281, 19719, 264, 42881, 8943, 51, 1121, 37294, 51694], "temperature": 0.0, "avg_logprob": -0.17211725495078348, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0011334455339238048}, {"id": 60, "seek": 32972, "start": 329.72, "end": 334.76000000000005, "text": " in a data driven manner. First, we need to estimate the drug effects. Like before", "tokens": [50364, 294, 257, 1412, 9555, 9060, 13, 2386, 11, 321, 643, 281, 12539, 264, 4110, 5065, 13, 1743, 949, 50616], "temperature": 0.0, "avg_logprob": -0.14087658959466057, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.004981261678040028}, {"id": 61, "seek": 32972, "start": 334.76000000000005, "end": 338.96000000000004, "text": " we assign them, we need to understand their drug effects from a high dimensional", "tokens": [50616, 321, 6269, 552, 11, 321, 643, 281, 1223, 641, 4110, 5065, 490, 257, 1090, 18795, 50826], "temperature": 0.0, "avg_logprob": -0.14087658959466057, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.004981261678040028}, {"id": 62, "seek": 32972, "start": 338.96000000000004, "end": 343.76000000000005, "text": " and unbalanced space. So when I say high dimensional, it means with more than 30", "tokens": [50826, 293, 517, 40251, 1901, 13, 407, 562, 286, 584, 1090, 18795, 11, 309, 1355, 365, 544, 813, 2217, 51066], "temperature": 0.0, "avg_logprob": -0.14087658959466057, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.004981261678040028}, {"id": 63, "seek": 32972, "start": 343.76000000000005, "end": 348.8, "text": " ART drugs, they're all FDA approved on the market. There are a large number of", "tokens": [51066, 8943, 51, 7766, 11, 436, 434, 439, 18933, 10826, 322, 264, 2142, 13, 821, 366, 257, 2416, 1230, 295, 51318], "temperature": 0.0, "avg_logprob": -0.14087658959466057, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.004981261678040028}, {"id": 64, "seek": 32972, "start": 348.8, "end": 353.44000000000005, "text": " possible drug combinations. So that could be millions of drug combinations you", "tokens": [51318, 1944, 4110, 21267, 13, 407, 300, 727, 312, 6803, 295, 4110, 21267, 291, 51550], "temperature": 0.0, "avg_logprob": -0.14087658959466057, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.004981261678040028}, {"id": 65, "seek": 32972, "start": 353.44000000000005, "end": 357.04, "text": " can choose. So even like the feasible drug combinations about like thousands,", "tokens": [51550, 393, 2826, 13, 407, 754, 411, 264, 26648, 4110, 21267, 466, 411, 5383, 11, 51730], "temperature": 0.0, "avg_logprob": -0.14087658959466057, "compression_ratio": 1.723021582733813, "no_speech_prob": 0.004981261678040028}, {"id": 66, "seek": 35704, "start": 357.04, "end": 360.76000000000005, "text": " there's still there are a large number of possible drug combinations. And also", "tokens": [50364, 456, 311, 920, 456, 366, 257, 2416, 1230, 295, 1944, 4110, 21267, 13, 400, 611, 50550], "temperature": 0.0, "avg_logprob": -0.23010530886442765, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.00024154194397851825}, {"id": 67, "seek": 35704, "start": 360.76000000000005, "end": 365.40000000000003, "text": " when I say the unbalanced, that means some drug combinations are firmly used, but", "tokens": [50550, 562, 286, 584, 264, 517, 40251, 11, 300, 1355, 512, 4110, 21267, 366, 20031, 1143, 11, 457, 50782], "temperature": 0.0, "avg_logprob": -0.23010530886442765, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.00024154194397851825}, {"id": 68, "seek": 35704, "start": 365.40000000000003, "end": 369.12, "text": " others are wrong. So for example, we can see for this drug combination is two", "tokens": [50782, 2357, 366, 2085, 13, 407, 337, 1365, 11, 321, 393, 536, 337, 341, 4110, 6562, 307, 732, 50968], "temperature": 0.0, "avg_logprob": -0.23010530886442765, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.00024154194397851825}, {"id": 69, "seek": 35704, "start": 369.12, "end": 374.56, "text": " ART drugs and one PR drug. It was the using our database for almost 1000", "tokens": [50968, 8943, 51, 7766, 293, 472, 11568, 4110, 13, 467, 390, 264, 1228, 527, 8149, 337, 1920, 9714, 51240], "temperature": 0.0, "avg_logprob": -0.23010530886442765, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.00024154194397851825}, {"id": 70, "seek": 35704, "start": 374.56, "end": 379.96000000000004, "text": " times, but another similar drug combination. So the same two ART drug, but", "tokens": [51240, 1413, 11, 457, 1071, 2531, 4110, 6562, 13, 407, 264, 912, 732, 8943, 51, 4110, 11, 457, 51510], "temperature": 0.0, "avg_logprob": -0.23010530886442765, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.00024154194397851825}, {"id": 71, "seek": 35704, "start": 379.96000000000004, "end": 386.44, "text": " a different the PR drug, it was only used for 12 times. And the second challenge", "tokens": [51510, 257, 819, 264, 11568, 4110, 11, 309, 390, 787, 1143, 337, 2272, 1413, 13, 400, 264, 1150, 3430, 51834], "temperature": 0.0, "avg_logprob": -0.23010530886442765, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.00024154194397851825}, {"id": 72, "seek": 38644, "start": 386.44, "end": 391.88, "text": " is how to generate a realistic ART regimen from a large discreet space in the", "tokens": [50364, 307, 577, 281, 8460, 257, 12465, 8943, 51, 1121, 19676, 490, 257, 2416, 2983, 4751, 1901, 294, 264, 50636], "temperature": 0.0, "avg_logprob": -0.13077062429841033, "compression_ratio": 1.7300884955752212, "no_speech_prob": 0.0005883305566385388}, {"id": 73, "seek": 38644, "start": 391.88, "end": 396.24, "text": " optimization procedure. So after we understand the treatment effects of", "tokens": [50636, 19618, 10747, 13, 407, 934, 321, 1223, 264, 5032, 5065, 295, 50854], "temperature": 0.0, "avg_logprob": -0.13077062429841033, "compression_ratio": 1.7300884955752212, "no_speech_prob": 0.0005883305566385388}, {"id": 74, "seek": 38644, "start": 396.24, "end": 401.36, "text": " these drug combinations, we need to assign the drug regimen to patients. That", "tokens": [50854, 613, 4110, 21267, 11, 321, 643, 281, 6269, 264, 4110, 1121, 19676, 281, 4209, 13, 663, 51110], "temperature": 0.0, "avg_logprob": -0.13077062429841033, "compression_ratio": 1.7300884955752212, "no_speech_prob": 0.0005883305566385388}, {"id": 75, "seek": 38644, "start": 401.36, "end": 406.32, "text": " means we need to generate a realistic ART regimen. So then the problem is how to", "tokens": [51110, 1355, 321, 643, 281, 8460, 257, 12465, 8943, 51, 1121, 19676, 13, 407, 550, 264, 1154, 307, 577, 281, 51358], "temperature": 0.0, "avg_logprob": -0.13077062429841033, "compression_ratio": 1.7300884955752212, "no_speech_prob": 0.0005883305566385388}, {"id": 76, "seek": 38644, "start": 406.32, "end": 411.64, "text": " represent each ART regimen. A naive method would be say, okay, I can use a battery", "tokens": [51358, 2906, 1184, 8943, 51, 1121, 19676, 13, 316, 29052, 3170, 576, 312, 584, 11, 1392, 11, 286, 393, 764, 257, 5809, 51624], "temperature": 0.0, "avg_logprob": -0.13077062429841033, "compression_ratio": 1.7300884955752212, "no_speech_prob": 0.0005883305566385388}, {"id": 77, "seek": 41164, "start": 411.64, "end": 418.03999999999996, "text": " vector. So say I have 30 ART, I have 30 ART drugs on the market, and then I can use", "tokens": [50364, 8062, 13, 407, 584, 286, 362, 2217, 8943, 51, 11, 286, 362, 2217, 8943, 51, 7766, 322, 264, 2142, 11, 293, 550, 286, 393, 764, 50684], "temperature": 0.0, "avg_logprob": -0.21018792762130986, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0035931558813899755}, {"id": 78, "seek": 41164, "start": 418.03999999999996, "end": 422.68, "text": " the battery vector is indimensional battery vector to represent. So if this drug", "tokens": [50684, 264, 5809, 8062, 307, 1016, 332, 11075, 5809, 8062, 281, 2906, 13, 407, 498, 341, 4110, 50916], "temperature": 0.0, "avg_logprob": -0.21018792762130986, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0035931558813899755}, {"id": 79, "seek": 41164, "start": 422.68, "end": 428.68, "text": " combination contains drug one, then it's one, otherwise it's zero. However, it will", "tokens": [50916, 6562, 8306, 4110, 472, 11, 550, 309, 311, 472, 11, 5911, 309, 311, 4018, 13, 2908, 11, 309, 486, 51216], "temperature": 0.0, "avg_logprob": -0.21018792762130986, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0035931558813899755}, {"id": 80, "seek": 41164, "start": 428.68, "end": 433.88, "text": " cause two to the N possible drug combinations. And many of them will not", "tokens": [51216, 3082, 732, 281, 264, 426, 1944, 4110, 21267, 13, 400, 867, 295, 552, 486, 406, 51476], "temperature": 0.0, "avg_logprob": -0.21018792762130986, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0035931558813899755}, {"id": 81, "seek": 41164, "start": 433.88, "end": 437.76, "text": " be feasible. So like I said, usually, you know, people combine the drugs from", "tokens": [51476, 312, 26648, 13, 407, 411, 286, 848, 11, 2673, 11, 291, 458, 11, 561, 10432, 264, 7766, 490, 51670], "temperature": 0.0, "avg_logprob": -0.21018792762130986, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0035931558813899755}, {"id": 82, "seek": 41164, "start": 437.76, "end": 441.44, "text": " different drug combinations, but usually we assign like, for example, two or three", "tokens": [51670, 819, 4110, 21267, 11, 457, 2673, 321, 6269, 411, 11, 337, 1365, 11, 732, 420, 1045, 51854], "temperature": 0.0, "avg_logprob": -0.21018792762130986, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0035931558813899755}, {"id": 83, "seek": 44164, "start": 441.64, "end": 447.15999999999997, "text": " drugs from ARTI, but like really we assign one drug from PI. So not all these", "tokens": [50364, 7766, 490, 8943, 5422, 11, 457, 411, 534, 321, 6269, 472, 4110, 490, 27176, 13, 407, 406, 439, 613, 50640], "temperature": 0.0, "avg_logprob": -0.18698872802078084, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00010719495185185224}, {"id": 84, "seek": 44164, "start": 447.15999999999997, "end": 452.71999999999997, "text": " possible two to the N combinations are possible. And then we need the efficient", "tokens": [50640, 1944, 732, 281, 264, 426, 21267, 366, 1944, 13, 400, 550, 321, 643, 264, 7148, 50918], "temperature": 0.0, "avg_logprob": -0.18698872802078084, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00010719495185185224}, {"id": 85, "seek": 44164, "start": 452.71999999999997, "end": 458.44, "text": " way to represent the drug combination. And lastly, we aim to optimize sequential", "tokens": [50918, 636, 281, 2906, 264, 4110, 6562, 13, 400, 16386, 11, 321, 5939, 281, 19719, 42881, 51204], "temperature": 0.0, "avg_logprob": -0.18698872802078084, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00010719495185185224}, {"id": 86, "seek": 44164, "start": 458.44, "end": 462.68, "text": " treatments from observational data. So we have all those data collected from", "tokens": [51204, 15795, 490, 9951, 1478, 1412, 13, 407, 321, 362, 439, 729, 1412, 11087, 490, 51416], "temperature": 0.0, "avg_logprob": -0.18698872802078084, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00010719495185185224}, {"id": 87, "seek": 44164, "start": 462.68, "end": 468.36, "text": " observational study, but we want to assign them to the patients in the future. So", "tokens": [51416, 9951, 1478, 2979, 11, 457, 321, 528, 281, 6269, 552, 281, 264, 4209, 294, 264, 2027, 13, 407, 51700], "temperature": 0.0, "avg_logprob": -0.18698872802078084, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.00010719495185185224}, {"id": 88, "seek": 46836, "start": 468.36, "end": 473.56, "text": " the fundamental challenge of optimizing treatments from observational study is", "tokens": [50364, 264, 8088, 3430, 295, 40425, 15795, 490, 9951, 1478, 2979, 307, 50624], "temperature": 0.0, "avg_logprob": -0.15225391387939452, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.00020986543677281588}, {"id": 89, "seek": 46836, "start": 473.56, "end": 477.88, "text": " this distribution shift issue. So that means the training data may be collected", "tokens": [50624, 341, 7316, 5513, 2734, 13, 407, 300, 1355, 264, 3097, 1412, 815, 312, 11087, 50840], "temperature": 0.0, "avg_logprob": -0.15225391387939452, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.00020986543677281588}, {"id": 90, "seek": 46836, "start": 477.88, "end": 481.56, "text": " on the different policies from the one we try to evaluate. So we need to address", "tokens": [50840, 322, 264, 819, 7657, 490, 264, 472, 321, 853, 281, 13059, 13, 407, 321, 643, 281, 2985, 51024], "temperature": 0.0, "avg_logprob": -0.15225391387939452, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.00020986543677281588}, {"id": 91, "seek": 46836, "start": 481.56, "end": 487.32, "text": " the distribution shift issue. So to address these challenges, we develop a", "tokens": [51024, 264, 7316, 5513, 2734, 13, 407, 281, 2985, 613, 4759, 11, 321, 1499, 257, 51312], "temperature": 0.0, "avg_logprob": -0.15225391387939452, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.00020986543677281588}, {"id": 92, "seek": 46836, "start": 487.32, "end": 492.28000000000003, "text": " two-step Bayesian reinforcement learning framework. And here is an overview. In the", "tokens": [51312, 732, 12, 16792, 7840, 42434, 29280, 2539, 8388, 13, 400, 510, 307, 364, 12492, 13, 682, 264, 51560], "temperature": 0.0, "avg_logprob": -0.15225391387939452, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.00020986543677281588}, {"id": 93, "seek": 46836, "start": 492.28000000000003, "end": 496.44, "text": " first step, we propose a Bayesian dynamics model for individuals'", "tokens": [51560, 700, 1823, 11, 321, 17421, 257, 7840, 42434, 15679, 2316, 337, 5346, 6, 51768], "temperature": 0.0, "avg_logprob": -0.15225391387939452, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.00020986543677281588}, {"id": 94, "seek": 49644, "start": 496.52, "end": 501.0, "text": " longitudinal observations using a microverte Gaussian process. And these estimate", "tokens": [50368, 48250, 18163, 1228, 257, 4532, 331, 975, 39148, 1399, 13, 400, 613, 12539, 50592], "temperature": 0.0, "avg_logprob": -0.2702927711682442, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0013041816418990493}, {"id": 95, "seek": 49644, "start": 501.0, "end": 505.56, "text": " dynamics describe how individuals' health-related variables evolve over time,", "tokens": [50592, 15679, 6786, 577, 5346, 6, 1585, 12, 12004, 9102, 16693, 670, 565, 11, 50820], "temperature": 0.0, "avg_logprob": -0.2702927711682442, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0013041816418990493}, {"id": 96, "seek": 49644, "start": 505.56, "end": 509.56, "text": " condition on their historical states and the treatment histories, with uncertainty", "tokens": [50820, 4188, 322, 641, 8584, 4368, 293, 264, 5032, 30631, 11, 365, 15697, 51020], "temperature": 0.0, "avg_logprob": -0.2702927711682442, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0013041816418990493}, {"id": 97, "seek": 49644, "start": 509.56, "end": 515.72, "text": " quantification. And in the second step, we create a pessimistic environment with", "tokens": [51020, 4426, 3774, 13, 400, 294, 264, 1150, 1823, 11, 321, 1884, 257, 37399, 3142, 2823, 365, 51328], "temperature": 0.0, "avg_logprob": -0.2702927711682442, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0013041816418990493}, {"id": 98, "seek": 49644, "start": 515.72, "end": 521.72, "text": " uncertainty penalization to mitigate the distribution shift issue and also use", "tokens": [51328, 15697, 13661, 2144, 281, 27336, 264, 7316, 5513, 2734, 293, 611, 764, 51628], "temperature": 0.0, "avg_logprob": -0.2702927711682442, "compression_ratio": 1.6475409836065573, "no_speech_prob": 0.0013041816418990493}, {"id": 99, "seek": 52172, "start": 521.72, "end": 525.5600000000001, "text": " an offline reinforcement learning method to optimize the sequential ART regimen.", "tokens": [50364, 364, 21857, 29280, 2539, 3170, 281, 19719, 264, 42881, 8943, 51, 1121, 19676, 13, 50556], "temperature": 0.0, "avg_logprob": -0.18290501549130395, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.0005441011744551361}, {"id": 100, "seek": 52172, "start": 526.2, "end": 532.52, "text": " So it's a two-step approach. Okay, so now let's go to the model details.", "tokens": [50588, 407, 309, 311, 257, 732, 12, 16792, 3109, 13, 1033, 11, 370, 586, 718, 311, 352, 281, 264, 2316, 4365, 13, 50904], "temperature": 0.0, "avg_logprob": -0.18290501549130395, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.0005441011744551361}, {"id": 101, "seek": 52172, "start": 533.24, "end": 540.6800000000001, "text": " So first, I introduce the problem formulation. Assume for each individual eye,", "tokens": [50940, 407, 700, 11, 286, 5366, 264, 1154, 37642, 13, 6281, 2540, 337, 1184, 2609, 3313, 11, 51312], "temperature": 0.0, "avg_logprob": -0.18290501549130395, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.0005441011744551361}, {"id": 102, "seek": 52172, "start": 540.6800000000001, "end": 547.96, "text": " we use XI0 to denote the baseline covariates, such as a race. And the TRI records the", "tokens": [51312, 321, 764, 1783, 40, 15, 281, 45708, 264, 20518, 49851, 1024, 11, 1270, 382, 257, 4569, 13, 400, 264, 314, 5577, 7724, 264, 51676], "temperature": 0.0, "avg_logprob": -0.18290501549130395, "compression_ratio": 1.452054794520548, "no_speech_prob": 0.0005441011744551361}, {"id": 103, "seek": 54796, "start": 547.96, "end": 555.08, "text": " visit times. So assume each one has GI visits, and we have PI1 to TIGI to denote the time of each", "tokens": [50364, 3441, 1413, 13, 407, 6552, 1184, 472, 575, 26634, 17753, 11, 293, 321, 362, 27176, 16, 281, 28819, 26252, 281, 45708, 264, 565, 295, 1184, 50720], "temperature": 0.0, "avg_logprob": -0.1988738775253296, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0028440917376428843}, {"id": 104, "seek": 54796, "start": 555.08, "end": 561.24, "text": " visit. And also assume we have M time varying health state variables. So here we call state", "tokens": [50720, 3441, 13, 400, 611, 6552, 321, 362, 376, 565, 22984, 1585, 1785, 9102, 13, 407, 510, 321, 818, 1785, 51028], "temperature": 0.0, "avg_logprob": -0.1988738775253296, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0028440917376428843}, {"id": 105, "seek": 54796, "start": 561.24, "end": 566.84, "text": " variables because they change over time. For example, like H, BMI, EGFR, those clinical", "tokens": [51028, 9102, 570, 436, 1319, 670, 565, 13, 1171, 1365, 11, 411, 389, 11, 363, 13808, 11, 462, 38, 34658, 11, 729, 9115, 51308], "temperature": 0.0, "avg_logprob": -0.1988738775253296, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0028440917376428843}, {"id": 106, "seek": 54796, "start": 566.84, "end": 573.88, "text": " variables or demographics, they are collected in each visit. And also the I to represent the ART", "tokens": [51308, 9102, 420, 36884, 11, 436, 366, 11087, 294, 1184, 3441, 13, 400, 611, 264, 286, 281, 2906, 264, 8943, 51, 51660], "temperature": 0.0, "avg_logprob": -0.1988738775253296, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.0028440917376428843}, {"id": 107, "seek": 57388, "start": 573.88, "end": 579.32, "text": " drug combination used by individual eye during the time period, TIGI minus 1 to TIGI.", "tokens": [50364, 4110, 6562, 1143, 538, 2609, 3313, 1830, 264, 565, 2896, 11, 28819, 26252, 3175, 502, 281, 28819, 26252, 13, 50636], "temperature": 0.0, "avg_logprob": -0.17268527283960458, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004441761411726475}, {"id": 108, "seek": 57388, "start": 581.0, "end": 588.36, "text": " And then the data can be summarized as D. So for each subject, so we have a total of I subjects,", "tokens": [50720, 400, 550, 264, 1412, 393, 312, 14611, 1602, 382, 413, 13, 407, 337, 1184, 3983, 11, 370, 321, 362, 257, 3217, 295, 286, 13066, 11, 51088], "temperature": 0.0, "avg_logprob": -0.17268527283960458, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004441761411726475}, {"id": 109, "seek": 57388, "start": 588.36, "end": 593.48, "text": " and we have baseline covariates, the time of each visit, and the time varying state variables,", "tokens": [51088, 293, 321, 362, 20518, 49851, 1024, 11, 264, 565, 295, 1184, 3441, 11, 293, 264, 565, 22984, 1785, 9102, 11, 51344], "temperature": 0.0, "avg_logprob": -0.17268527283960458, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004441761411726475}, {"id": 110, "seek": 57388, "start": 593.48, "end": 602.2, "text": " and also the drug information. And then we use the YIG bar. So the bar is a common fun we really", "tokens": [51344, 293, 611, 264, 4110, 1589, 13, 400, 550, 321, 764, 264, 398, 10489, 2159, 13, 407, 264, 2159, 307, 257, 2689, 1019, 321, 534, 51780], "temperature": 0.0, "avg_logprob": -0.17268527283960458, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004441761411726475}, {"id": 111, "seek": 60220, "start": 602.2800000000001, "end": 608.84, "text": " use to represent the history. So the YIG bar means all the state variables before the current", "tokens": [50368, 764, 281, 2906, 264, 2503, 13, 407, 264, 398, 10489, 2159, 1355, 439, 264, 1785, 9102, 949, 264, 2190, 50696], "temperature": 0.0, "avg_logprob": -0.19497161441379124, "compression_ratio": 1.6686046511627908, "no_speech_prob": 0.0018669363344088197}, {"id": 112, "seek": 60220, "start": 608.84, "end": 616.0400000000001, "text": " visit J. And the ZIG bar means all the treatments this person has been taking on pure the time J.", "tokens": [50696, 3441, 508, 13, 400, 264, 1176, 10489, 2159, 1355, 439, 264, 15795, 341, 954, 575, 668, 1940, 322, 6075, 264, 565, 508, 13, 51056], "temperature": 0.0, "avg_logprob": -0.19497161441379124, "compression_ratio": 1.6686046511627908, "no_speech_prob": 0.0018669363344088197}, {"id": 113, "seek": 60220, "start": 616.6800000000001, "end": 624.6800000000001, "text": " And also we use the dynamic, we use the after dynamic model. So that means condition on the YIG", "tokens": [51088, 400, 611, 321, 764, 264, 8546, 11, 321, 764, 264, 934, 8546, 2316, 13, 407, 300, 1355, 4188, 322, 264, 398, 10489, 51488], "temperature": 0.0, "avg_logprob": -0.19497161441379124, "compression_ratio": 1.6686046511627908, "no_speech_prob": 0.0018669363344088197}, {"id": 114, "seek": 62468, "start": 624.68, "end": 633.7199999999999, "text": " bar and the ZIG plus one, we update the state variables from YIG to YG plus one. So remember", "tokens": [50364, 2159, 293, 264, 1176, 10489, 1804, 472, 11, 321, 5623, 264, 1785, 9102, 490, 398, 10489, 281, 398, 38, 1804, 472, 13, 407, 1604, 50816], "temperature": 0.0, "avg_logprob": -0.17741700714709713, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.008709750138223171}, {"id": 115, "seek": 62468, "start": 633.7199999999999, "end": 640.68, "text": " the first step of our method to learn the dynamic model of how to transform from YG to YG plus one.", "tokens": [50816, 264, 700, 1823, 295, 527, 3170, 281, 1466, 264, 8546, 2316, 295, 577, 281, 4088, 490, 398, 38, 281, 398, 38, 1804, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17741700714709713, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.008709750138223171}, {"id": 116, "seek": 62468, "start": 640.68, "end": 646.4399999999999, "text": " Of course, the YG plus one condition on the whole history of the YG and the treatments the IJ.", "tokens": [51164, 2720, 1164, 11, 264, 398, 38, 1804, 472, 4188, 322, 264, 1379, 2503, 295, 264, 398, 38, 293, 264, 15795, 264, 286, 41, 13, 51452], "temperature": 0.0, "avg_logprob": -0.17741700714709713, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.008709750138223171}, {"id": 117, "seek": 62468, "start": 647.8, "end": 653.0799999999999, "text": " Okay, so our goal is to optimize the ART assignments to maximize the individual's long-term", "tokens": [51520, 1033, 11, 370, 527, 3387, 307, 281, 19719, 264, 8943, 51, 22546, 281, 19874, 264, 2609, 311, 938, 12, 7039, 51784], "temperature": 0.0, "avg_logprob": -0.17741700714709713, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.008709750138223171}, {"id": 118, "seek": 65308, "start": 653.08, "end": 659.48, "text": " house outcome. So because we want to maximize the house outcome, essentially we can transform the", "tokens": [50364, 1782, 9700, 13, 407, 570, 321, 528, 281, 19874, 264, 1782, 9700, 11, 4476, 321, 393, 4088, 264, 50684], "temperature": 0.0, "avg_logprob": -0.12895750195792552, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0006164942751638591}, {"id": 119, "seek": 65308, "start": 659.48, "end": 665.4000000000001, "text": " problem to an optimization problem. So this optimization problem is like we first define", "tokens": [50684, 1154, 281, 364, 19618, 1154, 13, 407, 341, 19618, 1154, 307, 411, 321, 700, 6964, 50980], "temperature": 0.0, "avg_logprob": -0.12895750195792552, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0006164942751638591}, {"id": 120, "seek": 65308, "start": 666.2, "end": 671.96, "text": " for each individual I, we suppose she already has GI visits. So if this person is a new patient,", "tokens": [51020, 337, 1184, 2609, 286, 11, 321, 7297, 750, 1217, 575, 26634, 17753, 13, 407, 498, 341, 954, 307, 257, 777, 4537, 11, 51308], "temperature": 0.0, "avg_logprob": -0.12895750195792552, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0006164942751638591}, {"id": 121, "seek": 65308, "start": 671.96, "end": 678.12, "text": " so the GI will be equal zero. So the GI can be zero, or if this person already has GI visits,", "tokens": [51308, 370, 264, 26634, 486, 312, 2681, 4018, 13, 407, 264, 26634, 393, 312, 4018, 11, 420, 498, 341, 954, 1217, 575, 26634, 17753, 11, 51616], "temperature": 0.0, "avg_logprob": -0.12895750195792552, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0006164942751638591}, {"id": 122, "seek": 67812, "start": 678.12, "end": 684.28, "text": " and then we want to predict the next few visits, for example. And then we let the Y new and the", "tokens": [50364, 293, 550, 321, 528, 281, 6069, 264, 958, 1326, 17753, 11, 337, 1365, 13, 400, 550, 321, 718, 264, 398, 777, 293, 264, 50672], "temperature": 0.0, "avg_logprob": -0.18363146159959876, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0005792173906229436}, {"id": 123, "seek": 67812, "start": 684.28, "end": 691.48, "text": " GI new to denote her future longitudinal states and the treatments, because our reward may depend on", "tokens": [50672, 26634, 777, 281, 45708, 720, 2027, 48250, 4368, 293, 264, 15795, 11, 570, 527, 7782, 815, 5672, 322, 51032], "temperature": 0.0, "avg_logprob": -0.18363146159959876, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0005792173906229436}, {"id": 124, "seek": 67812, "start": 691.48, "end": 698.52, "text": " her future states. For example, we want her less than the next two years, the depression is minimal.", "tokens": [51032, 720, 2027, 4368, 13, 1171, 1365, 11, 321, 528, 720, 1570, 813, 264, 958, 732, 924, 11, 264, 10799, 307, 13206, 13, 51384], "temperature": 0.0, "avg_logprob": -0.18363146159959876, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0005792173906229436}, {"id": 125, "seek": 67812, "start": 700.04, "end": 705.8, "text": " And assume for any future visited J, the ART regimen is assigned through a policy function", "tokens": [51460, 400, 6552, 337, 604, 2027, 11220, 508, 11, 264, 8943, 51, 1121, 19676, 307, 13279, 807, 257, 3897, 2445, 51748], "temperature": 0.0, "avg_logprob": -0.18363146159959876, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0005792173906229436}, {"id": 126, "seek": 70580, "start": 705.8, "end": 712.92, "text": " PI. So that means if we can learn, if we can prioritize this function PI, and we can optimize", "tokens": [50364, 27176, 13, 407, 300, 1355, 498, 321, 393, 1466, 11, 498, 321, 393, 25164, 341, 2445, 27176, 11, 293, 321, 393, 19719, 50720], "temperature": 0.0, "avg_logprob": -0.2026337914996677, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.0006665740511380136}, {"id": 127, "seek": 70580, "start": 712.92, "end": 720.76, "text": " data and the equivalently we can optimize the treatment. And let's say we assign a stochastic", "tokens": [50720, 1412, 293, 264, 9052, 2276, 321, 393, 19719, 264, 5032, 13, 400, 718, 311, 584, 321, 6269, 257, 342, 8997, 2750, 51112], "temperature": 0.0, "avg_logprob": -0.2026337914996677, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.0006665740511380136}, {"id": 128, "seek": 70580, "start": 720.76, "end": 727.64, "text": " reward function RI that depends on the house states. So we can depend, we can define the reward as", "tokens": [51112, 7782, 2445, 30474, 300, 5946, 322, 264, 1782, 4368, 13, 407, 321, 393, 5672, 11, 321, 393, 6964, 264, 7782, 382, 51456], "temperature": 0.0, "avg_logprob": -0.2026337914996677, "compression_ratio": 1.672514619883041, "no_speech_prob": 0.0006665740511380136}, {"id": 129, "seek": 72764, "start": 727.72, "end": 735.16, "text": " for example, this person, their, their very loaded is low, and the pressure set is low, and the BMI is", "tokens": [50368, 337, 1365, 11, 341, 954, 11, 641, 11, 641, 588, 13210, 307, 2295, 11, 293, 264, 3321, 992, 307, 2295, 11, 293, 264, 363, 13808, 307, 50740], "temperature": 0.0, "avg_logprob": -0.24686246815294322, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0016227230662479997}, {"id": 130, "seek": 72764, "start": 735.16, "end": 741.0, "text": " in the normal range, and the kidney function is a normal range. Okay, so for example, if our goal", "tokens": [50740, 294, 264, 2710, 3613, 11, 293, 264, 19000, 2445, 307, 257, 2710, 3613, 13, 1033, 11, 370, 337, 1365, 11, 498, 527, 3387, 51032], "temperature": 0.0, "avg_logprob": -0.24686246815294322, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0016227230662479997}, {"id": 131, "seek": 72764, "start": 741.0, "end": 747.08, "text": " is to select the sequential ART regimen that leads to lowest accumulated very low in the next two", "tokens": [51032, 307, 281, 3048, 264, 42881, 8943, 51, 1121, 19676, 300, 6689, 281, 12437, 31346, 588, 2295, 294, 264, 958, 732, 51336], "temperature": 0.0, "avg_logprob": -0.24686246815294322, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0016227230662479997}, {"id": 132, "seek": 72764, "start": 747.08, "end": 753.56, "text": " years, and it is coming an active sum of the very loads. Okay, so they notice the expected reward", "tokens": [51336, 924, 11, 293, 309, 307, 1348, 364, 4967, 2408, 295, 264, 588, 12668, 13, 1033, 11, 370, 436, 3449, 264, 5176, 7782, 51660], "temperature": 0.0, "avg_logprob": -0.24686246815294322, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0016227230662479997}, {"id": 133, "seek": 75356, "start": 753.56, "end": 758.92, "text": " for any individual I to be the following. Because we, in the first step, we learn a Bayesian model.", "tokens": [50364, 337, 604, 2609, 286, 281, 312, 264, 3480, 13, 1436, 321, 11, 294, 264, 700, 1823, 11, 321, 1466, 257, 7840, 42434, 2316, 13, 50632], "temperature": 0.0, "avg_logprob": -0.2524379291365632, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.0017543660942465067}, {"id": 134, "seek": 75356, "start": 758.92, "end": 765.56, "text": " So essentially, you can generate their future states. And also, so we can generate the future", "tokens": [50632, 407, 4476, 11, 291, 393, 8460, 641, 2027, 4368, 13, 400, 611, 11, 370, 321, 393, 8460, 264, 2027, 50964], "temperature": 0.0, "avg_logprob": -0.2524379291365632, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.0017543660942465067}, {"id": 135, "seek": 75356, "start": 765.56, "end": 771.4, "text": " states, why are you from the learned dynamic model, and we generate the ZI new from their", "tokens": [50964, 4368, 11, 983, 366, 291, 490, 264, 3264, 8546, 2316, 11, 293, 321, 8460, 264, 1176, 40, 777, 490, 641, 51256], "temperature": 0.0, "avg_logprob": -0.2524379291365632, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.0017543660942465067}, {"id": 136, "seek": 75356, "start": 772.28, "end": 776.3599999999999, "text": " the PI, the function. So we learned the PI from their parameter as the function PI.", "tokens": [51300, 264, 27176, 11, 264, 2445, 13, 407, 321, 3264, 264, 27176, 490, 641, 13075, 382, 264, 2445, 27176, 13, 51504], "temperature": 0.0, "avg_logprob": -0.2524379291365632, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.0017543660942465067}, {"id": 137, "seek": 75356, "start": 777.0, "end": 782.3599999999999, "text": " And also, because we learned the dynamic model, and then we can integrate out certain things,", "tokens": [51536, 400, 611, 11, 570, 321, 3264, 264, 8546, 2316, 11, 293, 550, 321, 393, 13365, 484, 1629, 721, 11, 51804], "temperature": 0.0, "avg_logprob": -0.2524379291365632, "compression_ratio": 1.9533898305084745, "no_speech_prob": 0.0017543660942465067}, {"id": 138, "seek": 78236, "start": 782.44, "end": 787.48, "text": " optimization procedure. So that's kind of the benefits of using the Bayesian framework in the", "tokens": [50368, 19618, 10747, 13, 407, 300, 311, 733, 295, 264, 5311, 295, 1228, 264, 7840, 42434, 8388, 294, 264, 50620], "temperature": 0.0, "avg_logprob": -0.16952394247055053, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.00028239018865861}, {"id": 139, "seek": 78236, "start": 787.48, "end": 792.84, "text": " first step. Because in the decision making step, we can integrate out uncertainty, we can adjust", "tokens": [50620, 700, 1823, 13, 1436, 294, 264, 3537, 1455, 1823, 11, 321, 393, 13365, 484, 15697, 11, 321, 393, 4369, 50888], "temperature": 0.0, "avg_logprob": -0.16952394247055053, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.00028239018865861}, {"id": 140, "seek": 78236, "start": 792.84, "end": 799.5600000000001, "text": " for this uncertainty quantification from their uncertainty of their dynamic, their dynamic model.", "tokens": [50888, 337, 341, 15697, 4426, 3774, 490, 641, 15697, 295, 641, 8546, 11, 641, 8546, 2316, 13, 51224], "temperature": 0.0, "avg_logprob": -0.16952394247055053, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.00028239018865861}, {"id": 141, "seek": 78236, "start": 801.0, "end": 807.72, "text": " And our goal is to find as optimized optimal policy function PI that parameterized by data", "tokens": [51296, 400, 527, 3387, 307, 281, 915, 382, 26941, 16252, 3897, 2445, 27176, 300, 13075, 1602, 538, 1412, 51632], "temperature": 0.0, "avg_logprob": -0.16952394247055053, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.00028239018865861}, {"id": 142, "seek": 80772, "start": 807.72, "end": 813.08, "text": " i star. So we want to find as an optimal parameter that can maximize the r i theta.", "tokens": [50364, 741, 3543, 13, 407, 321, 528, 281, 915, 382, 364, 16252, 13075, 300, 393, 19874, 264, 367, 741, 9725, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1562681957683732, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0007436373853124678}, {"id": 143, "seek": 80772, "start": 814.2, "end": 820.44, "text": " So that's a problem. Okay, so now we have already defined our reward function r i. And we want to", "tokens": [50688, 407, 300, 311, 257, 1154, 13, 1033, 11, 370, 586, 321, 362, 1217, 7642, 527, 7782, 2445, 367, 741, 13, 400, 321, 528, 281, 51000], "temperature": 0.0, "avg_logprob": -0.1562681957683732, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0007436373853124678}, {"id": 144, "seek": 80772, "start": 820.44, "end": 825.96, "text": " find as a parameter theta that can maximize the r i. And essentially, it's a classical", "tokens": [51000, 915, 382, 257, 13075, 9725, 300, 393, 19874, 264, 367, 741, 13, 400, 4476, 11, 309, 311, 257, 13735, 51276], "temperature": 0.0, "avg_logprob": -0.1562681957683732, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0007436373853124678}, {"id": 145, "seek": 80772, "start": 825.96, "end": 831.4, "text": " reinforcement learning problem. And we can use the policy gradient method to solve the problem.", "tokens": [51276, 29280, 2539, 1154, 13, 400, 321, 393, 764, 264, 3897, 16235, 3170, 281, 5039, 264, 1154, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1562681957683732, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0007436373853124678}, {"id": 146, "seek": 80772, "start": 831.4, "end": 837.32, "text": " So essentially, if you can calculate the gradient of r i, and then you can use the policy gradient", "tokens": [51548, 407, 4476, 11, 498, 291, 393, 8873, 264, 16235, 295, 367, 741, 11, 293, 550, 291, 393, 764, 264, 3897, 16235, 51844], "temperature": 0.0, "avg_logprob": -0.1562681957683732, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.0007436373853124678}, {"id": 147, "seek": 83732, "start": 837.32, "end": 843.1600000000001, "text": " method, essentially, you update the theta by this formula. So it's also classical results,", "tokens": [50364, 3170, 11, 4476, 11, 291, 5623, 264, 9725, 538, 341, 8513, 13, 407, 309, 311, 611, 13735, 3542, 11, 50656], "temperature": 0.0, "avg_logprob": -0.18160023159450955, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0011511236662045121}, {"id": 148, "seek": 83732, "start": 843.1600000000001, "end": 850.2800000000001, "text": " it's that you update the theta by the previous theta, and then plus some step size s i, q,", "tokens": [50656, 309, 311, 300, 291, 5623, 264, 9725, 538, 264, 3894, 9725, 11, 293, 550, 1804, 512, 1823, 2744, 262, 741, 11, 9505, 11, 51012], "temperature": 0.0, "avg_logprob": -0.18160023159450955, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0011511236662045121}, {"id": 149, "seek": 83732, "start": 850.2800000000001, "end": 856.7600000000001, "text": " and then times their gradient. So the essential problem is how to calculate the gradient of our", "tokens": [51012, 293, 550, 1413, 641, 16235, 13, 407, 264, 7115, 1154, 307, 577, 281, 8873, 264, 16235, 295, 527, 51336], "temperature": 0.0, "avg_logprob": -0.18160023159450955, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0011511236662045121}, {"id": 150, "seek": 83732, "start": 856.7600000000001, "end": 862.2, "text": " reward function. So we can see our reward function is relatively complex, right? You take the expected", "tokens": [51336, 7782, 2445, 13, 407, 321, 393, 536, 527, 7782, 2445, 307, 7226, 3997, 11, 558, 30, 509, 747, 264, 5176, 51608], "temperature": 0.0, "avg_logprob": -0.18160023159450955, "compression_ratio": 1.735159817351598, "no_speech_prob": 0.0011511236662045121}, {"id": 151, "seek": 86220, "start": 862.2, "end": 869.4000000000001, "text": " value of the reward function r i, and r is a function of y mu, and y mu you need to generate", "tokens": [50364, 2158, 295, 264, 7782, 2445, 367, 741, 11, 293, 367, 307, 257, 2445, 295, 288, 2992, 11, 293, 288, 2992, 291, 643, 281, 8460, 50724], "temperature": 0.0, "avg_logprob": -0.14913504885644027, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0008040173561312258}, {"id": 152, "seek": 86220, "start": 869.4000000000001, "end": 875.96, "text": " from the predict distribution of your dynamical model. And besides all of that, we also need to", "tokens": [50724, 490, 264, 6069, 7316, 295, 428, 5999, 804, 2316, 13, 400, 11868, 439, 295, 300, 11, 321, 611, 643, 281, 51052], "temperature": 0.0, "avg_logprob": -0.14913504885644027, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0008040173561312258}, {"id": 153, "seek": 86220, "start": 875.96, "end": 881.08, "text": " integrate out the uncertainty from the dynamical model by the P5 condition on D, that's a posterior", "tokens": [51052, 13365, 484, 264, 15697, 490, 264, 5999, 804, 2316, 538, 264, 430, 20, 4188, 322, 413, 11, 300, 311, 257, 33529, 51308], "temperature": 0.0, "avg_logprob": -0.14913504885644027, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0008040173561312258}, {"id": 154, "seek": 86220, "start": 881.08, "end": 889.32, "text": " distribution of the phi. So it's not easy to calculate the gradient of this r i theta. Okay,", "tokens": [51308, 7316, 295, 264, 13107, 13, 407, 309, 311, 406, 1858, 281, 8873, 264, 16235, 295, 341, 367, 741, 9725, 13, 1033, 11, 51720], "temperature": 0.0, "avg_logprob": -0.14913504885644027, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.0008040173561312258}, {"id": 155, "seek": 88932, "start": 889.4000000000001, "end": 895.0, "text": " so luckily for the policy gradient method, there's a way to calculate that. So if you're", "tokens": [50368, 370, 22880, 337, 264, 3897, 16235, 3170, 11, 456, 311, 257, 636, 281, 8873, 300, 13, 407, 498, 291, 434, 50648], "temperature": 0.0, "avg_logprob": -0.12704404871514502, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.00026115731452591717}, {"id": 156, "seek": 88932, "start": 895.0, "end": 899.4000000000001, "text": " interested in details, you can find the details in the paper, but we can represent the derivative", "tokens": [50648, 3102, 294, 4365, 11, 291, 393, 915, 264, 4365, 294, 264, 3035, 11, 457, 321, 393, 2906, 264, 13760, 50868], "temperature": 0.0, "avg_logprob": -0.12704404871514502, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.00026115731452591717}, {"id": 157, "seek": 88932, "start": 899.4000000000001, "end": 906.7600000000001, "text": " of r i theta as follows. And this formula looks very complex, but we can't decompose this into", "tokens": [50868, 295, 367, 741, 9725, 382, 10002, 13, 400, 341, 8513, 1542, 588, 3997, 11, 457, 321, 393, 380, 22867, 541, 341, 666, 51236], "temperature": 0.0, "avg_logprob": -0.12704404871514502, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.00026115731452591717}, {"id": 158, "seek": 88932, "start": 906.7600000000001, "end": 915.5600000000001, "text": " three separate steps. The first step is this step. So it's about the log of our policy function. So", "tokens": [51236, 1045, 4994, 4439, 13, 440, 700, 1823, 307, 341, 1823, 13, 407, 309, 311, 466, 264, 3565, 295, 527, 3897, 2445, 13, 407, 51676], "temperature": 0.0, "avg_logprob": -0.12704404871514502, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.00026115731452591717}, {"id": 159, "seek": 91556, "start": 915.56, "end": 922.3599999999999, "text": " essentially, if you can parameterize the ART assignment function, and then you can optimize", "tokens": [50364, 4476, 11, 498, 291, 393, 13075, 1125, 264, 8943, 51, 15187, 2445, 11, 293, 550, 291, 393, 19719, 50704], "temperature": 0.0, "avg_logprob": -0.09205505412112, "compression_ratio": 1.808411214953271, "no_speech_prob": 8.749762491788715e-05}, {"id": 160, "seek": 91556, "start": 922.3599999999999, "end": 927.56, "text": " that. So that's our first challenge. We need to parameterize the policy function. And the second", "tokens": [50704, 300, 13, 407, 300, 311, 527, 700, 3430, 13, 492, 643, 281, 13075, 1125, 264, 3897, 2445, 13, 400, 264, 1150, 50964], "temperature": 0.0, "avg_logprob": -0.09205505412112, "compression_ratio": 1.808411214953271, "no_speech_prob": 8.749762491788715e-05}, {"id": 161, "seek": 91556, "start": 927.56, "end": 933.9599999999999, "text": " step is how to generate the future states based on our dynamical model. So that's the second step,", "tokens": [50964, 1823, 307, 577, 281, 8460, 264, 2027, 4368, 2361, 322, 527, 5999, 804, 2316, 13, 407, 300, 311, 264, 1150, 1823, 11, 51284], "temperature": 0.0, "avg_logprob": -0.09205505412112, "compression_ratio": 1.808411214953271, "no_speech_prob": 8.749762491788715e-05}, {"id": 162, "seek": 91556, "start": 933.9599999999999, "end": 939.9599999999999, "text": " is we want to sample the future states. And the last thing is if we can define the reward function.", "tokens": [51284, 307, 321, 528, 281, 6889, 264, 2027, 4368, 13, 400, 264, 1036, 551, 307, 498, 321, 393, 6964, 264, 7782, 2445, 13, 51584], "temperature": 0.0, "avg_logprob": -0.09205505412112, "compression_ratio": 1.808411214953271, "no_speech_prob": 8.749762491788715e-05}, {"id": 163, "seek": 93996, "start": 939.96, "end": 944.6800000000001, "text": " So essentially, you decompose the calculating this gradient by three", "tokens": [50364, 407, 4476, 11, 291, 22867, 541, 264, 28258, 341, 16235, 538, 1045, 50600], "temperature": 0.0, "avg_logprob": -0.12505964713521522, "compression_ratio": 1.8427947598253276, "no_speech_prob": 0.00020025292178615928}, {"id": 164, "seek": 93996, "start": 945.48, "end": 950.52, "text": " sub steps. If we can sample future states, if we can define the reward function,", "tokens": [50640, 1422, 4439, 13, 759, 321, 393, 6889, 2027, 4368, 11, 498, 321, 393, 6964, 264, 7782, 2445, 11, 50892], "temperature": 0.0, "avg_logprob": -0.12505964713521522, "compression_ratio": 1.8427947598253276, "no_speech_prob": 0.00020025292178615928}, {"id": 165, "seek": 93996, "start": 950.52, "end": 955.5600000000001, "text": " if we can parameterize the policy function, and then we can calculate the gradient of the reward.", "tokens": [50892, 498, 321, 393, 13075, 1125, 264, 3897, 2445, 11, 293, 550, 321, 393, 8873, 264, 16235, 295, 264, 7782, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12505964713521522, "compression_ratio": 1.8427947598253276, "no_speech_prob": 0.00020025292178615928}, {"id": 166, "seek": 93996, "start": 955.5600000000001, "end": 959.72, "text": " And then we can plug into policy gradient method and then get the optimal theta.", "tokens": [51144, 400, 550, 321, 393, 5452, 666, 3897, 16235, 3170, 293, 550, 483, 264, 16252, 9725, 13, 51352], "temperature": 0.0, "avg_logprob": -0.12505964713521522, "compression_ratio": 1.8427947598253276, "no_speech_prob": 0.00020025292178615928}, {"id": 167, "seek": 93996, "start": 960.6, "end": 967.08, "text": " Okay, so now I will introduce each part. So how to sample the future states? So if we want to", "tokens": [51396, 1033, 11, 370, 586, 286, 486, 5366, 1184, 644, 13, 407, 577, 281, 6889, 264, 2027, 4368, 30, 407, 498, 321, 528, 281, 51720], "temperature": 0.0, "avg_logprob": -0.12505964713521522, "compression_ratio": 1.8427947598253276, "no_speech_prob": 0.00020025292178615928}, {"id": 168, "seek": 96708, "start": 967.08, "end": 971.8000000000001, "text": " sample the future states, basically, we need a model, and then we do the predictive inference.", "tokens": [50364, 6889, 264, 2027, 4368, 11, 1936, 11, 321, 643, 257, 2316, 11, 293, 550, 321, 360, 264, 35521, 38253, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1656206754537729, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.000677113130223006}, {"id": 169, "seek": 96708, "start": 971.8000000000001, "end": 977.4000000000001, "text": " And in this case, we have multiple longitudinal states. And we use a multivariate Gaussian", "tokens": [50600, 400, 294, 341, 1389, 11, 321, 362, 3866, 48250, 4368, 13, 400, 321, 764, 257, 2120, 592, 3504, 473, 39148, 50880], "temperature": 0.0, "avg_logprob": -0.1656206754537729, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.000677113130223006}, {"id": 170, "seek": 96708, "start": 977.4000000000001, "end": 982.6800000000001, "text": " process. The reason we use a multivariate Gaussian process because it's a popular choice for modeling", "tokens": [50880, 1399, 13, 440, 1778, 321, 764, 257, 2120, 592, 3504, 473, 39148, 1399, 570, 309, 311, 257, 3743, 3922, 337, 15983, 51144], "temperature": 0.0, "avg_logprob": -0.1656206754537729, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.000677113130223006}, {"id": 171, "seek": 96708, "start": 982.6800000000001, "end": 989.24, "text": " irregular space multivariate longitudinal data with great flexibility and also natural", "tokens": [51144, 29349, 1901, 2120, 592, 3504, 473, 48250, 1412, 365, 869, 12635, 293, 611, 3303, 51472], "temperature": 0.0, "avg_logprob": -0.1656206754537729, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.000677113130223006}, {"id": 172, "seek": 96708, "start": 989.24, "end": 995.32, "text": " uncertainty quantification. And here's irregular, it comes from the missing data because sometimes", "tokens": [51472, 15697, 4426, 3774, 13, 400, 510, 311, 29349, 11, 309, 1487, 490, 264, 5361, 1412, 570, 2171, 51776], "temperature": 0.0, "avg_logprob": -0.1656206754537729, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.000677113130223006}, {"id": 173, "seek": 99532, "start": 995.32, "end": 1002.0400000000001, "text": " maybe some visits and some measurements may be missing. Okay, so the multivariate Gaussian process,", "tokens": [50364, 1310, 512, 17753, 293, 512, 15383, 815, 312, 5361, 13, 1033, 11, 370, 264, 2120, 592, 3504, 473, 39148, 1399, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1873527983544578, "compression_ratio": 1.5891891891891892, "no_speech_prob": 0.000791506958194077}, {"id": 174, "seek": 99532, "start": 1002.0400000000001, "end": 1009.08, "text": " the whole framework of this multivariate Gaussian process is relatively standard. We use yimt to", "tokens": [50700, 264, 1379, 8388, 295, 341, 2120, 592, 3504, 473, 39148, 1399, 307, 7226, 3832, 13, 492, 764, 288, 332, 83, 281, 51052], "temperature": 0.0, "avg_logprob": -0.1873527983544578, "compression_ratio": 1.5891891891891892, "no_speech_prob": 0.000791506958194077}, {"id": 175, "seek": 99532, "start": 1009.08, "end": 1016.7600000000001, "text": " denote the nth longitudinal variable for treat for individual i at the time t. And we have a mean", "tokens": [51052, 45708, 264, 297, 392, 48250, 7006, 337, 2387, 337, 2609, 741, 412, 264, 565, 256, 13, 400, 321, 362, 257, 914, 51436], "temperature": 0.0, "avg_logprob": -0.1873527983544578, "compression_ratio": 1.5891891891891892, "no_speech_prob": 0.000791506958194077}, {"id": 176, "seek": 101676, "start": 1016.76, "end": 1026.68, "text": " function ft and the answer id residue epsilon. So for this f, we assume the multivariate Gaussian", "tokens": [50364, 2445, 283, 83, 293, 264, 1867, 4496, 34799, 17889, 13, 407, 337, 341, 283, 11, 321, 6552, 264, 2120, 592, 3504, 473, 39148, 50860], "temperature": 0.0, "avg_logprob": -0.2241070788839589, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001809948356822133}, {"id": 177, "seek": 101676, "start": 1026.68, "end": 1031.48, "text": " process is distributed. So we will have the mean function. So the next slide I'll introduce how we", "tokens": [50860, 1399, 307, 12631, 13, 407, 321, 486, 362, 264, 914, 2445, 13, 407, 264, 958, 4137, 286, 603, 5366, 577, 321, 51100], "temperature": 0.0, "avg_logprob": -0.2241070788839589, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001809948356822133}, {"id": 178, "seek": 101676, "start": 1031.48, "end": 1038.12, "text": " model the mean function. And for the various covariance parts, we assume they're the separable", "tokens": [51100, 2316, 264, 914, 2445, 13, 400, 337, 264, 3683, 49851, 719, 3166, 11, 321, 6552, 436, 434, 264, 3128, 712, 51432], "temperature": 0.0, "avg_logprob": -0.2241070788839589, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001809948356822133}, {"id": 179, "seek": 101676, "start": 1038.12, "end": 1043.72, "text": " covariance function. So they're here the cm to denote the correlation, represent the correlation", "tokens": [51432, 49851, 719, 2445, 13, 407, 436, 434, 510, 264, 14668, 281, 45708, 264, 20009, 11, 2906, 264, 20009, 51712], "temperature": 0.0, "avg_logprob": -0.2241070788839589, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.001809948356822133}, {"id": 180, "seek": 104372, "start": 1043.72, "end": 1049.72, "text": " among different state variables because they could be correlated. And the ct to represent their", "tokens": [50364, 3654, 819, 1785, 9102, 570, 436, 727, 312, 38574, 13, 400, 264, 269, 83, 281, 2906, 641, 50664], "temperature": 0.0, "avg_logprob": -0.15276436578659786, "compression_ratio": 1.8719211822660098, "no_speech_prob": 0.0003682615060824901}, {"id": 181, "seek": 104372, "start": 1049.72, "end": 1054.52, "text": " correlation among the time. So this separable covariance function adjusts for the correlation", "tokens": [50664, 20009, 3654, 264, 565, 13, 407, 341, 3128, 712, 49851, 719, 2445, 4369, 82, 337, 264, 20009, 50904], "temperature": 0.0, "avg_logprob": -0.15276436578659786, "compression_ratio": 1.8719211822660098, "no_speech_prob": 0.0003682615060824901}, {"id": 182, "seek": 104372, "start": 1054.52, "end": 1062.28, "text": " among variables and also along the time. Okay, and here for the ct, the correlation kernel that", "tokens": [50904, 3654, 9102, 293, 611, 2051, 264, 565, 13, 1033, 11, 293, 510, 337, 264, 269, 83, 11, 264, 20009, 28256, 300, 51292], "temperature": 0.0, "avg_logprob": -0.15276436578659786, "compression_ratio": 1.8719211822660098, "no_speech_prob": 0.0003682615060824901}, {"id": 183, "seek": 104372, "start": 1062.84, "end": 1068.76, "text": " for the temporal correlation, we use the oil kernel because we expect the curve that's not too", "tokens": [51320, 337, 264, 30881, 20009, 11, 321, 764, 264, 3184, 28256, 570, 321, 2066, 264, 7605, 300, 311, 406, 886, 51616], "temperature": 0.0, "avg_logprob": -0.15276436578659786, "compression_ratio": 1.8719211822660098, "no_speech_prob": 0.0003682615060824901}, {"id": 184, "seek": 106876, "start": 1068.76, "end": 1075.08, "text": " smooth. Okay, so for the mean parts, that's kind of the key country, one of the key contributions", "tokens": [50364, 5508, 13, 1033, 11, 370, 337, 264, 914, 3166, 11, 300, 311, 733, 295, 264, 2141, 1941, 11, 472, 295, 264, 2141, 15725, 50680], "temperature": 0.0, "avg_logprob": -0.18402547917814335, "compression_ratio": 1.9139344262295082, "no_speech_prob": 0.0007321041193790734}, {"id": 185, "seek": 106876, "start": 1075.08, "end": 1080.84, "text": " of this model is that for the mean parts, the first two parts are kind of standard. We use a", "tokens": [50680, 295, 341, 2316, 307, 300, 337, 264, 914, 3166, 11, 264, 700, 732, 3166, 366, 733, 295, 3832, 13, 492, 764, 257, 50968], "temperature": 0.0, "avg_logprob": -0.18402547917814335, "compression_ratio": 1.9139344262295082, "no_speech_prob": 0.0007321041193790734}, {"id": 186, "seek": 106876, "start": 1080.84, "end": 1085.8799999999999, "text": " baseline, it's like linear Mase effect model, we have fixed effects and random effects. But", "tokens": [50968, 20518, 11, 309, 311, 411, 8213, 376, 651, 1802, 2316, 11, 321, 362, 6806, 5065, 293, 4974, 5065, 13, 583, 51220], "temperature": 0.0, "avg_logprob": -0.18402547917814335, "compression_ratio": 1.9139344262295082, "no_speech_prob": 0.0007321041193790734}, {"id": 187, "seek": 106876, "start": 1085.8799999999999, "end": 1093.08, "text": " the how to model the drug combination effects, it's a key thing. So remember I said, for the", "tokens": [51220, 264, 577, 281, 2316, 264, 4110, 6562, 5065, 11, 309, 311, 257, 2141, 551, 13, 407, 1604, 286, 848, 11, 337, 264, 51580], "temperature": 0.0, "avg_logprob": -0.18402547917814335, "compression_ratio": 1.9139344262295082, "no_speech_prob": 0.0007321041193790734}, {"id": 188, "seek": 106876, "start": 1093.08, "end": 1098.28, "text": " drug combination, it's a high dimensional space. So how to represent the drug combinations.", "tokens": [51580, 4110, 6562, 11, 309, 311, 257, 1090, 18795, 1901, 13, 407, 577, 281, 2906, 264, 4110, 21267, 13, 51840], "temperature": 0.0, "avg_logprob": -0.18402547917814335, "compression_ratio": 1.9139344262295082, "no_speech_prob": 0.0007321041193790734}, {"id": 189, "seek": 109828, "start": 1098.28, "end": 1103.48, "text": " Here we borrow like the kernel idea. So the way we model that is we assume", "tokens": [50364, 1692, 321, 11172, 411, 264, 28256, 1558, 13, 407, 264, 636, 321, 2316, 300, 307, 321, 6552, 50624], "temperature": 0.0, "avg_logprob": -0.15963944521817294, "compression_ratio": 1.765625, "no_speech_prob": 0.00036824081325903535}, {"id": 190, "seek": 109828, "start": 1104.84, "end": 1111.24, "text": " there exists. So we assume there there's a okay, okay, so we assume there's a", "tokens": [50692, 456, 8198, 13, 407, 321, 6552, 456, 456, 311, 257, 1392, 11, 1392, 11, 370, 321, 6552, 456, 311, 257, 51012], "temperature": 0.0, "avg_logprob": -0.15963944521817294, "compression_ratio": 1.765625, "no_speech_prob": 0.00036824081325903535}, {"id": 191, "seek": 109828, "start": 1112.04, "end": 1119.48, "text": " we define a drug similarity regimen function kappa here. So because the z itself is a high", "tokens": [51052, 321, 6964, 257, 4110, 32194, 1121, 19676, 2445, 350, 25637, 510, 13, 407, 570, 264, 710, 2564, 307, 257, 1090, 51424], "temperature": 0.0, "avg_logprob": -0.15963944521817294, "compression_ratio": 1.765625, "no_speech_prob": 0.00036824081325903535}, {"id": 192, "seek": 109828, "start": 1119.48, "end": 1124.92, "text": " dimensional space. So to reduce the dimensionality, we introduce kappa. So it's like borrow the", "tokens": [51424, 18795, 1901, 13, 407, 281, 5407, 264, 10139, 1860, 11, 321, 5366, 350, 25637, 13, 407, 309, 311, 411, 11172, 264, 51696], "temperature": 0.0, "avg_logprob": -0.15963944521817294, "compression_ratio": 1.765625, "no_speech_prob": 0.00036824081325903535}, {"id": 193, "seek": 112492, "start": 1124.92, "end": 1130.92, "text": " kernel idea, we reduce the dimension from the original all the drug combinations to a manageable", "tokens": [50364, 28256, 1558, 11, 321, 5407, 264, 10139, 490, 264, 3380, 439, 264, 4110, 21267, 281, 257, 38798, 50664], "temperature": 0.0, "avg_logprob": -0.19118803600932277, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0004654636431951076}, {"id": 194, "seek": 112492, "start": 1130.92, "end": 1137.72, "text": " size as capital D here. So we introduce a bunch of representative drug regimens ZD. And then we", "tokens": [50664, 2744, 382, 4238, 413, 510, 13, 407, 321, 5366, 257, 3840, 295, 12424, 4110, 1121, 37294, 1176, 35, 13, 400, 550, 321, 51004], "temperature": 0.0, "avg_logprob": -0.19118803600932277, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0004654636431951076}, {"id": 195, "seek": 112492, "start": 1137.72, "end": 1143.3200000000002, "text": " calculate the similarity between each drug, possible drug combination with ZD. And then as", "tokens": [51004, 8873, 264, 32194, 1296, 1184, 4110, 11, 1944, 4110, 6562, 365, 1176, 35, 13, 400, 550, 382, 51284], "temperature": 0.0, "avg_logprob": -0.19118803600932277, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0004654636431951076}, {"id": 196, "seek": 112492, "start": 1143.3200000000002, "end": 1150.28, "text": " long as we can estimate the parameter gamma and D here, and then we can represent the drug effects", "tokens": [51284, 938, 382, 321, 393, 12539, 264, 13075, 15546, 293, 413, 510, 11, 293, 550, 321, 393, 2906, 264, 4110, 5065, 51632], "temperature": 0.0, "avg_logprob": -0.19118803600932277, "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0004654636431951076}, {"id": 197, "seek": 115028, "start": 1150.28, "end": 1159.48, "text": " for each drug combination. And the way we define their similarity function, it depends on two", "tokens": [50364, 337, 1184, 4110, 6562, 13, 400, 264, 636, 321, 6964, 641, 32194, 2445, 11, 309, 5946, 322, 732, 50824], "temperature": 0.0, "avg_logprob": -0.14324016960299746, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.000666567764710635}, {"id": 198, "seek": 115028, "start": 1159.48, "end": 1164.36, "text": " properties. The first one is we want sharing of information because the similar drugs from", "tokens": [50824, 7221, 13, 440, 700, 472, 307, 321, 528, 5414, 295, 1589, 570, 264, 2531, 7766, 490, 51068], "temperature": 0.0, "avg_logprob": -0.14324016960299746, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.000666567764710635}, {"id": 199, "seek": 115028, "start": 1164.36, "end": 1169.24, "text": " because the drugs from the same drug class, they have similar drug effects because we share the", "tokens": [51068, 570, 264, 7766, 490, 264, 912, 4110, 1508, 11, 436, 362, 2531, 4110, 5065, 570, 321, 2073, 264, 51312], "temperature": 0.0, "avg_logprob": -0.14324016960299746, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.000666567764710635}, {"id": 200, "seek": 115028, "start": 1169.24, "end": 1174.76, "text": " same mechanisms. So we want to share information from different drug combinations. And also from", "tokens": [51312, 912, 15902, 13, 407, 321, 528, 281, 2073, 1589, 490, 819, 4110, 21267, 13, 400, 611, 490, 51588], "temperature": 0.0, "avg_logprob": -0.14324016960299746, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.000666567764710635}, {"id": 201, "seek": 115028, "start": 1174.76, "end": 1179.96, "text": " this kernel, the introduction of this similarity function, it can reduce the high dimensionality.", "tokens": [51588, 341, 28256, 11, 264, 9339, 295, 341, 32194, 2445, 11, 309, 393, 5407, 264, 1090, 10139, 1860, 13, 51848], "temperature": 0.0, "avg_logprob": -0.14324016960299746, "compression_ratio": 1.962809917355372, "no_speech_prob": 0.000666567764710635}, {"id": 202, "seek": 118028, "start": 1180.28, "end": 1188.76, "text": " So let me briefly introduce the idea of this kernel. Because of time limit, I will not see", "tokens": [50364, 407, 718, 385, 10515, 5366, 264, 1558, 295, 341, 28256, 13, 1436, 295, 565, 4948, 11, 286, 486, 406, 536, 50788], "temperature": 0.0, "avg_logprob": -0.18827133414186079, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00012146168592153117}, {"id": 203, "seek": 118028, "start": 1188.76, "end": 1193.56, "text": " the detail. So consider a straight way to compute the drug combination similarity.", "tokens": [50788, 264, 2607, 13, 407, 1949, 257, 2997, 636, 281, 14722, 264, 4110, 6562, 32194, 13, 51028], "temperature": 0.0, "avg_logprob": -0.18827133414186079, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00012146168592153117}, {"id": 204, "seek": 118028, "start": 1194.44, "end": 1198.84, "text": " And the one straight forward idea is we use linear kernel. So the linear kernel, they can", "tokens": [51072, 400, 264, 472, 2997, 2128, 1558, 307, 321, 764, 8213, 28256, 13, 407, 264, 8213, 28256, 11, 436, 393, 51292], "temperature": 0.0, "avg_logprob": -0.18827133414186079, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00012146168592153117}, {"id": 205, "seek": 118028, "start": 1198.84, "end": 1204.12, "text": " compute the similarity between regimens based on the proportion of common drugs that two regimens", "tokens": [51292, 14722, 264, 32194, 1296, 1121, 37294, 2361, 322, 264, 16068, 295, 2689, 7766, 300, 732, 1121, 37294, 51556], "temperature": 0.0, "avg_logprob": -0.18827133414186079, "compression_ratio": 1.7355769230769231, "no_speech_prob": 0.00012146168592153117}, {"id": 206, "seek": 120412, "start": 1204.12, "end": 1210.52, "text": " share. So for example, here, we have three drug combinations. And all of them use two same drugs", "tokens": [50364, 2073, 13, 407, 337, 1365, 11, 510, 11, 321, 362, 1045, 4110, 21267, 13, 400, 439, 295, 552, 764, 732, 912, 7766, 50684], "temperature": 0.0, "avg_logprob": -0.14879881166944317, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0039446474984288216}, {"id": 207, "seek": 120412, "start": 1210.52, "end": 1218.1999999999998, "text": " from the NRTI class. So D4T plus LAM. And assume the third drug, the first two regimens, they", "tokens": [50684, 490, 264, 38399, 5422, 1508, 13, 407, 413, 19, 51, 1804, 441, 2865, 13, 400, 6552, 264, 2636, 4110, 11, 264, 700, 732, 1121, 37294, 11, 436, 51068], "temperature": 0.0, "avg_logprob": -0.14879881166944317, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0039446474984288216}, {"id": 208, "seek": 120412, "start": 1218.1999999999998, "end": 1224.12, "text": " choose one PI drug, but different PI drugs. And another one is choose NRTI. So you can use a", "tokens": [51068, 2826, 472, 27176, 4110, 11, 457, 819, 27176, 7766, 13, 400, 1071, 472, 307, 2826, 38399, 5422, 13, 407, 291, 393, 764, 257, 51364], "temperature": 0.0, "avg_logprob": -0.14879881166944317, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0039446474984288216}, {"id": 209, "seek": 120412, "start": 1224.12, "end": 1229.08, "text": " linear kernel. That means the pairwise similarity among these three kernels will be two over three,", "tokens": [51364, 8213, 28256, 13, 663, 1355, 264, 6119, 3711, 32194, 3654, 613, 1045, 23434, 1625, 486, 312, 732, 670, 1045, 11, 51612], "temperature": 0.0, "avg_logprob": -0.14879881166944317, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.0039446474984288216}, {"id": 210, "seek": 122908, "start": 1229.08, "end": 1235.56, "text": " right? Because they have three drugs, and they share two same drugs. However, there are some", "tokens": [50364, 558, 30, 1436, 436, 362, 1045, 7766, 11, 293, 436, 2073, 732, 912, 7766, 13, 2908, 11, 456, 366, 512, 50688], "temperature": 0.0, "avg_logprob": -0.11165519382642664, "compression_ratio": 1.861904761904762, "no_speech_prob": 0.0017818610649555922}, {"id": 211, "seek": 122908, "start": 1236.28, "end": 1242.84, "text": " disadvantages. For example, the first two drug combinations. So to both of them, they use two", "tokens": [50724, 37431, 13, 1171, 1365, 11, 264, 700, 732, 4110, 21267, 13, 407, 281, 1293, 295, 552, 11, 436, 764, 732, 51052], "temperature": 0.0, "avg_logprob": -0.11165519382642664, "compression_ratio": 1.861904761904762, "no_speech_prob": 0.0017818610649555922}, {"id": 212, "seek": 122908, "start": 1242.84, "end": 1247.96, "text": " same NRTI drugs. And the third drug, they belong to the same drug class. Because the same drug class,", "tokens": [51052, 912, 38399, 5422, 7766, 13, 400, 264, 2636, 4110, 11, 436, 5784, 281, 264, 912, 4110, 1508, 13, 1436, 264, 912, 4110, 1508, 11, 51308], "temperature": 0.0, "avg_logprob": -0.11165519382642664, "compression_ratio": 1.861904761904762, "no_speech_prob": 0.0017818610649555922}, {"id": 213, "seek": 122908, "start": 1247.96, "end": 1253.6399999999999, "text": " they share the same madness. So we would expect the similarity between the first two drug combinations", "tokens": [51308, 436, 2073, 264, 912, 28736, 13, 407, 321, 576, 2066, 264, 32194, 1296, 264, 700, 732, 4110, 21267, 51592], "temperature": 0.0, "avg_logprob": -0.11165519382642664, "compression_ratio": 1.861904761904762, "no_speech_prob": 0.0017818610649555922}, {"id": 214, "seek": 125364, "start": 1253.64, "end": 1259.16, "text": " would be larger or would be higher compared to the similarity between there, between them and the", "tokens": [50364, 576, 312, 4833, 420, 576, 312, 2946, 5347, 281, 264, 32194, 1296, 456, 11, 1296, 552, 293, 264, 50640], "temperature": 0.0, "avg_logprob": -0.10904458138795026, "compression_ratio": 1.9795918367346939, "no_speech_prob": 0.0013667986495420337}, {"id": 215, "seek": 125364, "start": 1259.16, "end": 1263.3200000000002, "text": " third drug. Because the third, because the third drug combination, they have the drug from a different", "tokens": [50640, 2636, 4110, 13, 1436, 264, 2636, 11, 570, 264, 2636, 4110, 6562, 11, 436, 362, 264, 4110, 490, 257, 819, 50848], "temperature": 0.0, "avg_logprob": -0.10904458138795026, "compression_ratio": 1.9795918367346939, "no_speech_prob": 0.0013667986495420337}, {"id": 216, "seek": 125364, "start": 1263.3200000000002, "end": 1268.76, "text": " drug class. And another example, for example, if you have these two drug combinations, both of", "tokens": [50848, 4110, 1508, 13, 400, 1071, 1365, 11, 337, 1365, 11, 498, 291, 362, 613, 732, 4110, 21267, 11, 1293, 295, 51120], "temperature": 0.0, "avg_logprob": -0.10904458138795026, "compression_ratio": 1.9795918367346939, "no_speech_prob": 0.0013667986495420337}, {"id": 217, "seek": 125364, "start": 1268.76, "end": 1275.8000000000002, "text": " them have two drugs from NRTI drug class, and one drug from the PI drug class. If we use a linear", "tokens": [51120, 552, 362, 732, 7766, 490, 38399, 5422, 4110, 1508, 11, 293, 472, 4110, 490, 264, 27176, 4110, 1508, 13, 759, 321, 764, 257, 8213, 51472], "temperature": 0.0, "avg_logprob": -0.10904458138795026, "compression_ratio": 1.9795918367346939, "no_speech_prob": 0.0013667986495420337}, {"id": 218, "seek": 125364, "start": 1275.8000000000002, "end": 1280.8400000000001, "text": " kernel, and they would share zero similarity, because they don't share any of common drugs.", "tokens": [51472, 28256, 11, 293, 436, 576, 2073, 4018, 32194, 11, 570, 436, 500, 380, 2073, 604, 295, 2689, 7766, 13, 51724], "temperature": 0.0, "avg_logprob": -0.10904458138795026, "compression_ratio": 1.9795918367346939, "no_speech_prob": 0.0013667986495420337}, {"id": 219, "seek": 128084, "start": 1280.84, "end": 1288.12, "text": " However, we know the same drug class will share some similarity. So the good method,", "tokens": [50364, 2908, 11, 321, 458, 264, 912, 4110, 1508, 486, 2073, 512, 32194, 13, 407, 264, 665, 3170, 11, 50728], "temperature": 0.0, "avg_logprob": -0.16981810110586662, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0002611676463857293}, {"id": 220, "seek": 128084, "start": 1288.12, "end": 1293.56, "text": " we should borrow this clinical information and share some similarity for these two drug combinations.", "tokens": [50728, 321, 820, 11172, 341, 9115, 1589, 293, 2073, 512, 32194, 337, 613, 732, 4110, 21267, 13, 51000], "temperature": 0.0, "avg_logprob": -0.16981810110586662, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0002611676463857293}, {"id": 221, "seek": 128084, "start": 1293.56, "end": 1299.72, "text": " So the way we set up the, we define the drug similarity is we use the sub subject kernel.", "tokens": [51000, 407, 264, 636, 321, 992, 493, 264, 11, 321, 6964, 264, 4110, 32194, 307, 321, 764, 264, 1422, 3983, 28256, 13, 51308], "temperature": 0.0, "avg_logprob": -0.16981810110586662, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0002611676463857293}, {"id": 222, "seek": 128084, "start": 1299.72, "end": 1304.36, "text": " So the sub subject kernel is the idea was to represent the sentences in natural language", "tokens": [51308, 407, 264, 1422, 3983, 28256, 307, 264, 1558, 390, 281, 2906, 264, 16579, 294, 3303, 2856, 51540], "temperature": 0.0, "avg_logprob": -0.16981810110586662, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0002611676463857293}, {"id": 223, "seek": 130436, "start": 1304.36, "end": 1311.08, "text": " processing literature. And here we represent our drug combination by a tree structure. And the", "tokens": [50364, 9007, 10394, 13, 400, 510, 321, 2906, 527, 4110, 6562, 538, 257, 4230, 3877, 13, 400, 264, 50700], "temperature": 0.0, "avg_logprob": -0.12550159822027368, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.0014777041506022215}, {"id": 224, "seek": 130436, "start": 1311.08, "end": 1318.28, "text": " subject kernel can represent the similarity at all levels of the tree representation. So essentially,", "tokens": [50700, 3983, 28256, 393, 2906, 264, 32194, 412, 439, 4358, 295, 264, 4230, 10290, 13, 407, 4476, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12550159822027368, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.0014777041506022215}, {"id": 225, "seek": 130436, "start": 1318.28, "end": 1325.7199999999998, "text": " the upper level is ART. And then we have the second level to represent which drug class", "tokens": [51060, 264, 6597, 1496, 307, 8943, 51, 13, 400, 550, 321, 362, 264, 1150, 1496, 281, 2906, 597, 4110, 1508, 51432], "temperature": 0.0, "avg_logprob": -0.12550159822027368, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.0014777041506022215}, {"id": 226, "seek": 130436, "start": 1325.7199999999998, "end": 1331.0, "text": " we draw the drugs. And the third level represents how many drugs from each drug class. And the", "tokens": [51432, 321, 2642, 264, 7766, 13, 400, 264, 2636, 1496, 8855, 577, 867, 7766, 490, 1184, 4110, 1508, 13, 400, 264, 51696], "temperature": 0.0, "avg_logprob": -0.12550159822027368, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.0014777041506022215}, {"id": 227, "seek": 133100, "start": 1331.0, "end": 1338.2, "text": " third level represents the each drug from each drug class. And then the sub subject kernel can", "tokens": [50364, 2636, 1496, 8855, 264, 1184, 4110, 490, 1184, 4110, 1508, 13, 400, 550, 264, 1422, 3983, 28256, 393, 50724], "temperature": 0.0, "avg_logprob": -0.16041765714946546, "compression_ratio": 1.8504672897196262, "no_speech_prob": 0.00027369745657779276}, {"id": 228, "seek": 133100, "start": 1338.2, "end": 1343.56, "text": " represent the whole similarity. For example, like regimen A and B, they can adjust for their similarity", "tokens": [50724, 2906, 264, 1379, 32194, 13, 1171, 1365, 11, 411, 1121, 19676, 316, 293, 363, 11, 436, 393, 4369, 337, 641, 32194, 50992], "temperature": 0.0, "avg_logprob": -0.16041765714946546, "compression_ratio": 1.8504672897196262, "no_speech_prob": 0.00027369745657779276}, {"id": 229, "seek": 133100, "start": 1343.56, "end": 1349.4, "text": " is a blue box. And for regimen A and C, they can adjust for their drug similarity is a yellow box.", "tokens": [50992, 307, 257, 3344, 2424, 13, 400, 337, 1121, 19676, 316, 293, 383, 11, 436, 393, 4369, 337, 641, 4110, 32194, 307, 257, 5566, 2424, 13, 51284], "temperature": 0.0, "avg_logprob": -0.16041765714946546, "compression_ratio": 1.8504672897196262, "no_speech_prob": 0.00027369745657779276}, {"id": 230, "seek": 133100, "start": 1349.4, "end": 1354.6, "text": " Even, you know, they don't share any common drugs, but you can still incorporate their similarity.", "tokens": [51284, 2754, 11, 291, 458, 11, 436, 500, 380, 2073, 604, 2689, 7766, 11, 457, 291, 393, 920, 16091, 641, 32194, 13, 51544], "temperature": 0.0, "avg_logprob": -0.16041765714946546, "compression_ratio": 1.8504672897196262, "no_speech_prob": 0.00027369745657779276}, {"id": 231, "seek": 135460, "start": 1355.32, "end": 1359.8799999999999, "text": " Okay, so now I have introduced this Markov-Berica Gaussian process to model the", "tokens": [50400, 1033, 11, 370, 586, 286, 362, 7268, 341, 3934, 5179, 12, 33, 260, 2262, 39148, 1399, 281, 2316, 264, 50628], "temperature": 0.0, "avg_logprob": -0.21927106741702918, "compression_ratio": 1.781456953642384, "no_speech_prob": 0.0010161431273445487}, {"id": 232, "seek": 135460, "start": 1359.8799999999999, "end": 1364.76, "text": " longitudinal states. And then if we have a model and we have our own parameters, and then we can", "tokens": [50628, 48250, 4368, 13, 400, 550, 498, 321, 362, 257, 2316, 293, 321, 362, 527, 1065, 9834, 11, 293, 550, 321, 393, 50872], "temperature": 0.0, "avg_logprob": -0.21927106741702918, "compression_ratio": 1.781456953642384, "no_speech_prob": 0.0010161431273445487}, {"id": 233, "seek": 135460, "start": 1364.76, "end": 1368.9199999999998, "text": " write down the likelihood, and you can assign the price to all unknown parameters, you can", "tokens": [50872, 2464, 760, 264, 22119, 11, 293, 291, 393, 6269, 264, 3218, 281, 439, 9841, 9834, 11, 291, 393, 51080], "temperature": 0.0, "avg_logprob": -0.21927106741702918, "compression_ratio": 1.781456953642384, "no_speech_prob": 0.0010161431273445487}, {"id": 234, "seek": 135460, "start": 1368.9199999999998, "end": 1373.48, "text": " obtain the posterior distribution from the MCMC. So I will skip all the computational", "tokens": [51080, 12701, 264, 33529, 7316, 490, 264, 8797, 39261, 13, 407, 286, 486, 10023, 439, 264, 28270, 51308], "temperature": 0.0, "avg_logprob": -0.21927106741702918, "compression_ratio": 1.781456953642384, "no_speech_prob": 0.0010161431273445487}, {"id": 235, "seek": 135460, "start": 1373.48, "end": 1377.8799999999999, "text": " details here. But essentially, now we finish the first step. So we have the Markov-Berica", "tokens": [51308, 4365, 510, 13, 583, 4476, 11, 586, 321, 2413, 264, 700, 1823, 13, 407, 321, 362, 264, 3934, 5179, 12, 33, 260, 2262, 51528], "temperature": 0.0, "avg_logprob": -0.21927106741702918, "compression_ratio": 1.781456953642384, "no_speech_prob": 0.0010161431273445487}, {"id": 236, "seek": 135460, "start": 1377.8799999999999, "end": 1383.0, "text": " normal model, we can sample future states. Okay, so the second one is how to define the reward", "tokens": [51528, 2710, 2316, 11, 321, 393, 6889, 2027, 4368, 13, 1033, 11, 370, 264, 1150, 472, 307, 577, 281, 6964, 264, 7782, 51784], "temperature": 0.0, "avg_logprob": -0.21927106741702918, "compression_ratio": 1.781456953642384, "no_speech_prob": 0.0010161431273445487}, {"id": 237, "seek": 138300, "start": 1383.0, "end": 1388.36, "text": " function. And the reward function, it depends on the clinical goal, right? So here,", "tokens": [50364, 2445, 13, 400, 264, 7782, 2445, 11, 309, 5946, 322, 264, 9115, 3387, 11, 558, 30, 407, 510, 11, 50632], "temperature": 0.0, "avg_logprob": -0.1672876448858352, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.001781874569132924}, {"id": 238, "seek": 138300, "start": 1389.72, "end": 1397.08, "text": " it depends on how we define the long term house for each individual. So here, after consultation", "tokens": [50700, 309, 5946, 322, 577, 321, 6964, 264, 938, 1433, 1782, 337, 1184, 2609, 13, 407, 510, 11, 934, 20932, 51068], "temperature": 0.0, "avg_logprob": -0.1672876448858352, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.001781874569132924}, {"id": 239, "seek": 138300, "start": 1397.08, "end": 1402.6, "text": " with the clinicians, we determine that we define the reward that depends on the barrel load,", "tokens": [51068, 365, 264, 32862, 11, 321, 6997, 300, 321, 6964, 264, 7782, 300, 5946, 322, 264, 13257, 3677, 11, 51344], "temperature": 0.0, "avg_logprob": -0.1672876448858352, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.001781874569132924}, {"id": 240, "seek": 138300, "start": 1402.6, "end": 1407.24, "text": " kidney function and the depression. So we want to care about first, you know, whether it can", "tokens": [51344, 19000, 2445, 293, 264, 10799, 13, 407, 321, 528, 281, 1127, 466, 700, 11, 291, 458, 11, 1968, 309, 393, 51576], "temperature": 0.0, "avg_logprob": -0.1672876448858352, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.001781874569132924}, {"id": 241, "seek": 138300, "start": 1407.24, "end": 1412.04, "text": " successfully suppress the barrel load, and also maintain a good kidney function and also the good", "tokens": [51576, 10727, 26835, 264, 13257, 3677, 11, 293, 611, 6909, 257, 665, 19000, 2445, 293, 611, 264, 665, 51816], "temperature": 0.0, "avg_logprob": -0.1672876448858352, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.001781874569132924}, {"id": 242, "seek": 141204, "start": 1412.04, "end": 1419.96, "text": " mental health. So let's see, our goal is we will, so here we can say we want to maximize", "tokens": [50364, 4973, 1585, 13, 407, 718, 311, 536, 11, 527, 3387, 307, 321, 486, 11, 370, 510, 321, 393, 584, 321, 528, 281, 19874, 50760], "temperature": 0.0, "avg_logprob": -0.140709365766073, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.001622369745746255}, {"id": 243, "seek": 141204, "start": 1421.1599999999999, "end": 1427.8, "text": " the overall house in the next two years. So remember, the visits are semi-annual visits.", "tokens": [50820, 264, 4787, 1782, 294, 264, 958, 732, 924, 13, 407, 1604, 11, 264, 17753, 366, 12909, 12, 969, 901, 17753, 13, 51152], "temperature": 0.0, "avg_logprob": -0.140709365766073, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.001622369745746255}, {"id": 244, "seek": 141204, "start": 1427.8, "end": 1434.28, "text": " So that's why here the sum is from the next visit, next four visits, because next four visits means", "tokens": [51152, 407, 300, 311, 983, 510, 264, 2408, 307, 490, 264, 958, 3441, 11, 958, 1451, 17753, 11, 570, 958, 1451, 17753, 1355, 51476], "temperature": 0.0, "avg_logprob": -0.140709365766073, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.001622369745746255}, {"id": 245, "seek": 141204, "start": 1434.28, "end": 1441.08, "text": " the overall good health in the next two years. And then we want the depression, this as small as", "tokens": [51476, 264, 4787, 665, 1585, 294, 264, 958, 732, 924, 13, 400, 550, 321, 528, 264, 10799, 11, 341, 382, 1359, 382, 51816], "temperature": 0.0, "avg_logprob": -0.140709365766073, "compression_ratio": 1.8067632850241546, "no_speech_prob": 0.001622369745746255}, {"id": 246, "seek": 144108, "start": 1441.08, "end": 1448.76, "text": " possible. And also, oh yeah, here is the next four visits. And also for depression, it's as small as", "tokens": [50364, 1944, 13, 400, 611, 11, 1954, 1338, 11, 510, 307, 264, 958, 1451, 17753, 13, 400, 611, 337, 10799, 11, 309, 311, 382, 1359, 382, 50748], "temperature": 0.0, "avg_logprob": -0.13500103860531212, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.0012252875603735447}, {"id": 247, "seek": 144108, "start": 1448.76, "end": 1454.1999999999998, "text": " possible. And for the barrel load and the EGFR, because as long as they are normal threshold,", "tokens": [50748, 1944, 13, 400, 337, 264, 13257, 3677, 293, 264, 462, 38, 34658, 11, 570, 382, 938, 382, 436, 366, 2710, 14678, 11, 51020], "temperature": 0.0, "avg_logprob": -0.13500103860531212, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.0012252875603735447}, {"id": 248, "seek": 144108, "start": 1454.1999999999998, "end": 1459.32, "text": " it will be fine. So we define this kind of step function, as long as they are in the normal range,", "tokens": [51020, 309, 486, 312, 2489, 13, 407, 321, 6964, 341, 733, 295, 1823, 2445, 11, 382, 938, 382, 436, 366, 294, 264, 2710, 3613, 11, 51276], "temperature": 0.0, "avg_logprob": -0.13500103860531212, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.0012252875603735447}, {"id": 249, "seek": 144108, "start": 1459.32, "end": 1465.6399999999999, "text": " it'll be fine. And if it's outside of the normal range, and we give certain penalty. And also here,", "tokens": [51276, 309, 603, 312, 2489, 13, 400, 498, 309, 311, 2380, 295, 264, 2710, 3613, 11, 293, 321, 976, 1629, 16263, 13, 400, 611, 510, 11, 51592], "temperature": 0.0, "avg_logprob": -0.13500103860531212, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.0012252875603735447}, {"id": 250, "seek": 146564, "start": 1465.64, "end": 1473.0, "text": " we have to personalize the weights, WI. So for example, if some person, they more care about", "tokens": [50364, 321, 362, 281, 2973, 1125, 264, 17443, 11, 343, 40, 13, 407, 337, 1365, 11, 498, 512, 954, 11, 436, 544, 1127, 466, 50732], "temperature": 0.0, "avg_logprob": -0.17153764854777942, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0008424434345215559}, {"id": 251, "seek": 146564, "start": 1473.0, "end": 1478.6000000000001, "text": " the depression, and then the WI1 can have a higher weight. So it's personalized and determined by", "tokens": [50732, 264, 10799, 11, 293, 550, 264, 343, 40, 16, 393, 362, 257, 2946, 3364, 13, 407, 309, 311, 28415, 293, 9540, 538, 51012], "temperature": 0.0, "avg_logprob": -0.17153764854777942, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0008424434345215559}, {"id": 252, "seek": 146564, "start": 1478.6000000000001, "end": 1485.24, "text": " the physician and also patient himself. And also to mitigate the distribution shift", "tokens": [51012, 264, 16456, 293, 611, 4537, 3647, 13, 400, 611, 281, 27336, 264, 7316, 5513, 51344], "temperature": 0.0, "avg_logprob": -0.17153764854777942, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0008424434345215559}, {"id": 253, "seek": 146564, "start": 1485.24, "end": 1491.64, "text": " issue, we use certainly penalized reward. That's another advantage of using the Bayesian method", "tokens": [51344, 2734, 11, 321, 764, 3297, 13661, 1602, 7782, 13, 663, 311, 1071, 5002, 295, 1228, 264, 7840, 42434, 3170, 51664], "temperature": 0.0, "avg_logprob": -0.17153764854777942, "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.0008424434345215559}, {"id": 254, "seek": 149164, "start": 1491.64, "end": 1497.0800000000002, "text": " in the first step, because we can easily quantify the uncertainty. So this idea is by this paper by", "tokens": [50364, 294, 264, 700, 1823, 11, 570, 321, 393, 3612, 40421, 264, 15697, 13, 407, 341, 1558, 307, 538, 341, 3035, 538, 50636], "temperature": 0.0, "avg_logprob": -0.17764566209581162, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0009694343898445368}, {"id": 255, "seek": 149164, "start": 1497.0800000000002, "end": 1503.24, "text": " you from UC Berkeley's group. And essentially, we define a pessimistic environment by introducing", "tokens": [50636, 291, 490, 14079, 23684, 311, 1594, 13, 400, 4476, 11, 321, 6964, 257, 37399, 3142, 2823, 538, 15424, 50944], "temperature": 0.0, "avg_logprob": -0.17764566209581162, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0009694343898445368}, {"id": 256, "seek": 149164, "start": 1503.24, "end": 1511.24, "text": " a penalized reward. So the RA is defined by the previous slide. But now we penalize the uncertainty", "tokens": [50944, 257, 13661, 1602, 7782, 13, 407, 264, 14626, 307, 7642, 538, 264, 3894, 4137, 13, 583, 586, 321, 13661, 1125, 264, 15697, 51344], "temperature": 0.0, "avg_logprob": -0.17764566209581162, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0009694343898445368}, {"id": 257, "seek": 149164, "start": 1511.24, "end": 1517.8000000000002, "text": " of the, it's the predictive variability of the state and their treatments. And it's a", "tokens": [51344, 295, 264, 11, 309, 311, 264, 35521, 35709, 295, 264, 1785, 293, 641, 15795, 13, 400, 309, 311, 257, 51672], "temperature": 0.0, "avg_logprob": -0.17764566209581162, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0009694343898445368}, {"id": 258, "seek": 151780, "start": 1517.8, "end": 1525.0, "text": " tuning parameter we need to learn. Okay. And then we use a posterior predict distribution to", "tokens": [50364, 15164, 13075, 321, 643, 281, 1466, 13, 1033, 13, 400, 550, 321, 764, 257, 33529, 6069, 7316, 281, 50724], "temperature": 0.0, "avg_logprob": -0.17658842441647551, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00033531617373228073}, {"id": 259, "seek": 151780, "start": 1525.0, "end": 1529.08, "text": " quantify the uncertainty again, because we have a Bayesian model, so that's very straightforward.", "tokens": [50724, 40421, 264, 15697, 797, 11, 570, 321, 362, 257, 7840, 42434, 2316, 11, 370, 300, 311, 588, 15325, 13, 50928], "temperature": 0.0, "avg_logprob": -0.17658842441647551, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00033531617373228073}, {"id": 260, "seek": 151780, "start": 1529.08, "end": 1534.6, "text": " Okay, so now we define a reward function. And the last step is how to parameterize the policy", "tokens": [50928, 1033, 11, 370, 586, 321, 6964, 257, 7782, 2445, 13, 400, 264, 1036, 1823, 307, 577, 281, 13075, 1125, 264, 3897, 51204], "temperature": 0.0, "avg_logprob": -0.17658842441647551, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00033531617373228073}, {"id": 261, "seek": 151780, "start": 1534.6, "end": 1542.2, "text": " function. So to prioritize the policy function, we make this also the three types of decision", "tokens": [51204, 2445, 13, 407, 281, 25164, 264, 3897, 2445, 11, 321, 652, 341, 611, 264, 1045, 3467, 295, 3537, 51584], "temperature": 0.0, "avg_logprob": -0.17658842441647551, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00033531617373228073}, {"id": 262, "seek": 154220, "start": 1542.2, "end": 1547.8, "text": " after talking with clinicians. So essentially, we decompose this to several steps. So first,", "tokens": [50364, 934, 1417, 365, 32862, 13, 407, 4476, 11, 321, 22867, 541, 341, 281, 2940, 4439, 13, 407, 700, 11, 50644], "temperature": 0.0, "avg_logprob": -0.13763746721991177, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.007342452649027109}, {"id": 263, "seek": 154220, "start": 1548.44, "end": 1553.88, "text": " if this person has been using ART drugs for a long time, and we will see if this person needs", "tokens": [50676, 498, 341, 954, 575, 668, 1228, 8943, 51, 7766, 337, 257, 938, 565, 11, 293, 321, 486, 536, 498, 341, 954, 2203, 50948], "temperature": 0.0, "avg_logprob": -0.13763746721991177, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.007342452649027109}, {"id": 264, "seek": 154220, "start": 1553.88, "end": 1559.96, "text": " to switch the regimen or not. So if the older drug works, and we can just keep using the older drug.", "tokens": [50948, 281, 3679, 264, 1121, 19676, 420, 406, 13, 407, 498, 264, 4906, 4110, 1985, 11, 293, 321, 393, 445, 1066, 1228, 264, 4906, 4110, 13, 51252], "temperature": 0.0, "avg_logprob": -0.13763746721991177, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.007342452649027109}, {"id": 265, "seek": 154220, "start": 1559.96, "end": 1564.44, "text": " So this is what we will represent as a logistic regression method in the logistic regression", "tokens": [51252, 407, 341, 307, 437, 321, 486, 2906, 382, 257, 3565, 3142, 24590, 3170, 294, 264, 3565, 3142, 24590, 51476], "temperature": 0.0, "avg_logprob": -0.13763746721991177, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.007342452649027109}, {"id": 266, "seek": 154220, "start": 1564.44, "end": 1570.6000000000001, "text": " model. As long as all the health measurements are in within the normal range, and then we will", "tokens": [51476, 2316, 13, 1018, 938, 382, 439, 264, 1585, 15383, 366, 294, 1951, 264, 2710, 3613, 11, 293, 550, 321, 486, 51784], "temperature": 0.0, "avg_logprob": -0.13763746721991177, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.007342452649027109}, {"id": 267, "seek": 157060, "start": 1570.6, "end": 1575.1599999999999, "text": " decide to just, you know, keep the old drug. And if one of them is not in the normal range,", "tokens": [50364, 4536, 281, 445, 11, 291, 458, 11, 1066, 264, 1331, 4110, 13, 400, 498, 472, 295, 552, 307, 406, 294, 264, 2710, 3613, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11015356381734212, "compression_ratio": 1.9322709163346614, "no_speech_prob": 0.0011157785775139928}, {"id": 268, "seek": 157060, "start": 1575.1599999999999, "end": 1581.7199999999998, "text": " we will switch. If this person needs to switch, and then we will need to generate a new regimen.", "tokens": [50592, 321, 486, 3679, 13, 759, 341, 954, 2203, 281, 3679, 11, 293, 550, 321, 486, 643, 281, 8460, 257, 777, 1121, 19676, 13, 50920], "temperature": 0.0, "avg_logprob": -0.11015356381734212, "compression_ratio": 1.9322709163346614, "no_speech_prob": 0.0011157785775139928}, {"id": 269, "seek": 157060, "start": 1582.28, "end": 1588.52, "text": " And because we used the three representation, and then we can now decide, you know, if we need to", "tokens": [50948, 400, 570, 321, 1143, 264, 1045, 10290, 11, 293, 550, 321, 393, 586, 4536, 11, 291, 458, 11, 498, 321, 643, 281, 51260], "temperature": 0.0, "avg_logprob": -0.11015356381734212, "compression_ratio": 1.9322709163346614, "no_speech_prob": 0.0011157785775139928}, {"id": 270, "seek": 157060, "start": 1588.52, "end": 1593.24, "text": " switch how to generate a new regimen, essentially, we need to decide like which drug class and how", "tokens": [51260, 3679, 577, 281, 8460, 257, 777, 1121, 19676, 11, 4476, 11, 321, 643, 281, 4536, 411, 597, 4110, 1508, 293, 577, 51496], "temperature": 0.0, "avg_logprob": -0.11015356381734212, "compression_ratio": 1.9322709163346614, "no_speech_prob": 0.0011157785775139928}, {"id": 271, "seek": 157060, "start": 1593.24, "end": 1598.6, "text": " many drugs use the initial class and also which individual drugs at each class. So this essentially", "tokens": [51496, 867, 7766, 764, 264, 5883, 1508, 293, 611, 597, 2609, 7766, 412, 1184, 1508, 13, 407, 341, 4476, 51764], "temperature": 0.0, "avg_logprob": -0.11015356381734212, "compression_ratio": 1.9322709163346614, "no_speech_prob": 0.0011157785775139928}, {"id": 272, "seek": 159860, "start": 1598.6, "end": 1603.08, "text": " it's a non-central hypergeometric distribution. Again, I skipped all the details. It's kind of", "tokens": [50364, 309, 311, 257, 2107, 12, 2207, 2155, 9848, 432, 29470, 7316, 13, 3764, 11, 286, 30193, 439, 264, 4365, 13, 467, 311, 733, 295, 50588], "temperature": 0.0, "avg_logprob": -0.1972076416015625, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008425716659985483}, {"id": 273, "seek": 159860, "start": 1603.08, "end": 1611.56, "text": " a little bit complex. So we have these three levels of three decisions. Okay, so now we have already", "tokens": [50588, 257, 707, 857, 3997, 13, 407, 321, 362, 613, 1045, 4358, 295, 1045, 5327, 13, 1033, 11, 370, 586, 321, 362, 1217, 51012], "temperature": 0.0, "avg_logprob": -0.1972076416015625, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008425716659985483}, {"id": 274, "seek": 159860, "start": 1611.56, "end": 1615.8799999999999, "text": " finished these three steps. So we have multivariate Gaussian process to some whole future states,", "tokens": [51012, 4335, 613, 1045, 4439, 13, 407, 321, 362, 2120, 592, 3504, 473, 39148, 1399, 281, 512, 1379, 2027, 4368, 11, 51228], "temperature": 0.0, "avg_logprob": -0.1972076416015625, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008425716659985483}, {"id": 275, "seek": 159860, "start": 1615.8799999999999, "end": 1620.76, "text": " and we define reward function. And then we have ways to prioritize a policy function. We can use", "tokens": [51228, 293, 321, 6964, 7782, 2445, 13, 400, 550, 321, 362, 2098, 281, 25164, 257, 3897, 2445, 13, 492, 393, 764, 51472], "temperature": 0.0, "avg_logprob": -0.1972076416015625, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008425716659985483}, {"id": 276, "seek": 159860, "start": 1620.76, "end": 1628.04, "text": " a policy gradient method to optimize a print. Okay. Okay, so now, so here I finish all the", "tokens": [51472, 257, 3897, 16235, 3170, 281, 19719, 257, 4482, 13, 1033, 13, 1033, 11, 370, 586, 11, 370, 510, 286, 2413, 439, 264, 51836], "temperature": 0.0, "avg_logprob": -0.1972076416015625, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.0008425716659985483}, {"id": 277, "seek": 162804, "start": 1628.04, "end": 1634.68, "text": " matter introduction. Last part of the slides is I will introduce real data analysis results.", "tokens": [50364, 1871, 9339, 13, 5264, 644, 295, 264, 9788, 307, 286, 486, 5366, 957, 1412, 5215, 3542, 13, 50696], "temperature": 0.0, "avg_logprob": -0.21139688077180283, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.00026115443324670196}, {"id": 278, "seek": 162804, "start": 1635.72, "end": 1642.12, "text": " So for the real data, we got about 300 women from the Washington DC site from the white study.", "tokens": [50748, 407, 337, 264, 957, 1412, 11, 321, 658, 466, 6641, 2266, 490, 264, 6149, 9114, 3621, 490, 264, 2418, 2979, 13, 51068], "temperature": 0.0, "avg_logprob": -0.21139688077180283, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.00026115443324670196}, {"id": 279, "seek": 162804, "start": 1643.0, "end": 1649.3999999999999, "text": " And also now we get four state variables at each visit, depression, viral load, EGFR, and BMI.", "tokens": [51112, 400, 611, 586, 321, 483, 1451, 1785, 9102, 412, 1184, 3441, 11, 10799, 11, 16132, 3677, 11, 462, 38, 34658, 11, 293, 363, 13808, 13, 51432], "temperature": 0.0, "avg_logprob": -0.21139688077180283, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.00026115443324670196}, {"id": 280, "seek": 162804, "start": 1649.3999999999999, "end": 1654.36, "text": " And there are about 8% missing data. And the baseline covariates we consider include age,", "tokens": [51432, 400, 456, 366, 466, 1649, 4, 5361, 1412, 13, 400, 264, 20518, 49851, 1024, 321, 1949, 4090, 3205, 11, 51680], "temperature": 0.0, "avg_logprob": -0.21139688077180283, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.00026115443324670196}, {"id": 281, "seek": 165436, "start": 1654.36, "end": 1660.6799999999998, "text": " smoking status, substance use, employment status, hypertension, diabetes. And in this study,", "tokens": [50364, 14055, 6558, 11, 12961, 764, 11, 11949, 6558, 11, 46172, 11, 13881, 13, 400, 294, 341, 2979, 11, 50680], "temperature": 0.0, "avg_logprob": -0.164980234724752, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0006986063672229648}, {"id": 282, "seek": 165436, "start": 1660.6799999999998, "end": 1668.28, "text": " we have 31 ART drugs and six drug classes. And we choose 105 representative drug regiments. So", "tokens": [50680, 321, 362, 10353, 8943, 51, 7766, 293, 2309, 4110, 5359, 13, 400, 321, 2826, 33705, 12424, 4110, 1121, 8321, 13, 407, 51060], "temperature": 0.0, "avg_logprob": -0.164980234724752, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0006986063672229648}, {"id": 283, "seek": 165436, "start": 1668.28, "end": 1673.3999999999999, "text": " those regiments based on the popularity of the drug combinations, if they have been used a lot", "tokens": [51060, 729, 1121, 8321, 2361, 322, 264, 19301, 295, 264, 4110, 21267, 11, 498, 436, 362, 668, 1143, 257, 688, 51316], "temperature": 0.0, "avg_logprob": -0.164980234724752, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0006986063672229648}, {"id": 284, "seek": 165436, "start": 1673.3999999999999, "end": 1678.6, "text": " of times for the from the patients, and then we would know that as representative ART regimen.", "tokens": [51316, 295, 1413, 337, 264, 490, 264, 4209, 11, 293, 550, 321, 576, 458, 300, 382, 12424, 8943, 51, 1121, 19676, 13, 51576], "temperature": 0.0, "avg_logprob": -0.164980234724752, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0006986063672229648}, {"id": 285, "seek": 167860, "start": 1679.1599999999999, "end": 1685.9599999999998, "text": " Okay, so here is one hypothetical patient. So we'll use this example to demonstrate the precision", "tokens": [50392, 1033, 11, 370, 510, 307, 472, 33053, 4537, 13, 407, 321, 603, 764, 341, 1365, 281, 11698, 264, 18356, 50732], "temperature": 0.0, "avg_logprob": -0.13617748372695027, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0009695317130535841}, {"id": 286, "seek": 167860, "start": 1685.9599999999998, "end": 1693.0, "text": " medicine, the, you know, the utility of the clinical utility of the proposed methods. Okay,", "tokens": [50732, 7195, 11, 264, 11, 291, 458, 11, 264, 14877, 295, 264, 9115, 14877, 295, 264, 10348, 7150, 13, 1033, 11, 51084], "temperature": 0.0, "avg_logprob": -0.13617748372695027, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0009695317130535841}, {"id": 287, "seek": 167860, "start": 1693.0, "end": 1701.0, "text": " so this person has been has been had 31 visits. And here is their history of treatments. And for", "tokens": [51084, 370, 341, 954, 575, 668, 575, 668, 632, 10353, 17753, 13, 400, 510, 307, 641, 2503, 295, 15795, 13, 400, 337, 51484], "temperature": 0.0, "avg_logprob": -0.13617748372695027, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0009695317130535841}, {"id": 288, "seek": 170100, "start": 1701.0, "end": 1708.92, "text": " these patients, we assume their weights as one third, one third, one third. Okay, and then we run", "tokens": [50364, 613, 4209, 11, 321, 6552, 641, 17443, 382, 472, 2636, 11, 472, 2636, 11, 472, 2636, 13, 1033, 11, 293, 550, 321, 1190, 50760], "temperature": 0.0, "avg_logprob": -0.2498031891498369, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0005976282409392297}, {"id": 289, "seek": 170100, "start": 1708.92, "end": 1715.48, "text": " our optimization method. And here we can see the expected reward versus the SGD iteration. So it", "tokens": [50760, 527, 19618, 3170, 13, 400, 510, 321, 393, 536, 264, 5176, 7782, 5717, 264, 34520, 35, 24784, 13, 407, 309, 51088], "temperature": 0.0, "avg_logprob": -0.2498031891498369, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0005976282409392297}, {"id": 290, "seek": 170100, "start": 1715.48, "end": 1722.44, "text": " became relatively stable after 1,000 iterations. And here is optimal regimen is the next two years.", "tokens": [51088, 3062, 7226, 8351, 934, 502, 11, 1360, 36540, 13, 400, 510, 307, 16252, 1121, 19676, 307, 264, 958, 732, 924, 13, 51436], "temperature": 0.0, "avg_logprob": -0.2498031891498369, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0005976282409392297}, {"id": 291, "seek": 170100, "start": 1722.44, "end": 1727.8, "text": " So we can see at the visit to there are two ART drugs when you see one PR and one poster,", "tokens": [51436, 407, 321, 393, 536, 412, 264, 3441, 281, 456, 366, 732, 8943, 51, 7766, 562, 291, 536, 472, 11568, 293, 472, 17171, 11, 51704], "temperature": 0.0, "avg_logprob": -0.2498031891498369, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0005976282409392297}, {"id": 292, "seek": 172780, "start": 1727.8, "end": 1735.72, "text": " and then it changes one new regimen for visits 33 to 35. Okay, and also here I want to show under", "tokens": [50364, 293, 550, 309, 2962, 472, 777, 1121, 19676, 337, 17753, 11816, 281, 6976, 13, 1033, 11, 293, 611, 510, 286, 528, 281, 855, 833, 50760], "temperature": 0.0, "avg_logprob": -0.13548608926626352, "compression_ratio": 1.5774058577405858, "no_speech_prob": 0.0004441564960870892}, {"id": 293, "seek": 172780, "start": 1735.72, "end": 1741.48, "text": " their estimated optimal regiments, that's the predicted depression stores. And we can see for", "tokens": [50760, 641, 14109, 16252, 1121, 8321, 11, 300, 311, 264, 19147, 10799, 9512, 13, 400, 321, 393, 536, 337, 51048], "temperature": 0.0, "avg_logprob": -0.13548608926626352, "compression_ratio": 1.5774058577405858, "no_speech_prob": 0.0004441564960870892}, {"id": 294, "seek": 172780, "start": 1741.48, "end": 1749.32, "text": " their visits from 32 to 36, they're about 23% improvement of depression. So that also shows", "tokens": [51048, 641, 17753, 490, 8858, 281, 8652, 11, 436, 434, 466, 6673, 4, 10444, 295, 10799, 13, 407, 300, 611, 3110, 51440], "temperature": 0.0, "avg_logprob": -0.13548608926626352, "compression_ratio": 1.5774058577405858, "no_speech_prob": 0.0004441564960870892}, {"id": 295, "seek": 172780, "start": 1749.32, "end": 1754.68, "text": " the clinical utility of our, you know, newly assigned optimal drug combinations. Okay, I will", "tokens": [51440, 264, 9115, 14877, 295, 527, 11, 291, 458, 11, 15109, 13279, 16252, 4110, 21267, 13, 1033, 11, 286, 486, 51708], "temperature": 0.0, "avg_logprob": -0.13548608926626352, "compression_ratio": 1.5774058577405858, "no_speech_prob": 0.0004441564960870892}, {"id": 296, "seek": 175468, "start": 1754.68, "end": 1765.48, "text": " skip the next example due to the time limit. Oh, yeah. Okay, to summarize, we, we propose a like", "tokens": [50364, 10023, 264, 958, 1365, 3462, 281, 264, 565, 4948, 13, 876, 11, 1338, 13, 1033, 11, 281, 20858, 11, 321, 11, 321, 17421, 257, 411, 50904], "temperature": 0.0, "avg_logprob": -0.2188525881086077, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0008967819740064442}, {"id": 297, "seek": 175468, "start": 1765.48, "end": 1770.68, "text": " a Bayesian reforcing learning approach is a two step approach. And it can learn their dynamics", "tokens": [50904, 257, 7840, 42434, 1895, 284, 2175, 2539, 3109, 307, 257, 732, 1823, 3109, 13, 400, 309, 393, 1466, 641, 15679, 51164], "temperature": 0.0, "avg_logprob": -0.2188525881086077, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0008967819740064442}, {"id": 298, "seek": 175468, "start": 1770.68, "end": 1776.1200000000001, "text": " with uncertainty quantification, it can also assign the long term optimal drug combinations to", "tokens": [51164, 365, 15697, 4426, 3774, 11, 309, 393, 611, 6269, 264, 938, 1433, 16252, 4110, 21267, 281, 51436], "temperature": 0.0, "avg_logprob": -0.2188525881086077, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0008967819740064442}, {"id": 299, "seek": 175468, "start": 1777.3200000000002, "end": 1781.0, "text": " optimize each individual's health. Okay, yeah, thank you.", "tokens": [51496, 19719, 1184, 2609, 311, 1585, 13, 1033, 11, 1338, 11, 1309, 291, 13, 51680], "temperature": 0.0, "avg_logprob": -0.2188525881086077, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.0008967819740064442}, {"id": 300, "seek": 178100, "start": 1781.24, "end": 1787.88, "text": " Thank you so much, Yanxun. Any questions from the audience?", "tokens": [50376, 1044, 291, 370, 709, 11, 13633, 87, 409, 13, 2639, 1651, 490, 264, 4034, 30, 50708], "temperature": 0.0, "avg_logprob": -0.15429642688797182, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013449678663164377}, {"id": 301, "seek": 178100, "start": 1788.92, "end": 1794.84, "text": " Yanxun, those are very exciting work. I actually have some questions because you touched a very", "tokens": [50760, 13633, 87, 409, 11, 729, 366, 588, 4670, 589, 13, 286, 767, 362, 512, 1651, 570, 291, 9828, 257, 588, 51056], "temperature": 0.0, "avg_logprob": -0.15429642688797182, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013449678663164377}, {"id": 302, "seek": 178100, "start": 1794.84, "end": 1802.52, "text": " good point where you need to balance the priority like competing priorities when you are in the", "tokens": [51056, 665, 935, 689, 291, 643, 281, 4772, 264, 9365, 411, 15439, 15503, 562, 291, 366, 294, 264, 51440], "temperature": 0.0, "avg_logprob": -0.15429642688797182, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013449678663164377}, {"id": 303, "seek": 178100, "start": 1802.52, "end": 1806.84, "text": " clinical practice. But since we are a little bit over time, so I probably will talk to you later", "tokens": [51440, 9115, 3124, 13, 583, 1670, 321, 366, 257, 707, 857, 670, 565, 11, 370, 286, 1391, 486, 751, 281, 291, 1780, 51656], "temperature": 0.0, "avg_logprob": -0.15429642688797182, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.0013449678663164377}, {"id": 304, "seek": 180684, "start": 1806.84, "end": 1814.4399999999998, "text": " about that. I was wondering like how the uncertainty will be impacted by how you", "tokens": [50364, 466, 300, 13, 286, 390, 6359, 411, 577, 264, 15697, 486, 312, 15653, 538, 577, 291, 50744], "temperature": 0.0, "avg_logprob": -0.18194309870402017, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.00970417633652687}, {"id": 305, "seek": 180684, "start": 1814.4399999999998, "end": 1823.32, "text": " define your reward function. Oh, yeah. So the uncertainty part, you know, how the uncertainty", "tokens": [50744, 6964, 428, 7782, 2445, 13, 876, 11, 1338, 13, 407, 264, 15697, 644, 11, 291, 458, 11, 577, 264, 15697, 51188], "temperature": 0.0, "avg_logprob": -0.18194309870402017, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.00970417633652687}, {"id": 306, "seek": 180684, "start": 1823.32, "end": 1828.6, "text": " affects the final decision depending on how you tune the parameter. So like here, and yeah, I skip", "tokens": [51188, 11807, 264, 2572, 3537, 5413, 322, 577, 291, 10864, 264, 13075, 13, 407, 411, 510, 11, 293, 1338, 11, 286, 10023, 51452], "temperature": 0.0, "avg_logprob": -0.18194309870402017, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.00970417633652687}, {"id": 307, "seek": 180684, "start": 1828.6, "end": 1832.84, "text": " that part, but here you can show if we have different tooling parameters like lambda equal", "tokens": [51452, 300, 644, 11, 457, 510, 291, 393, 855, 498, 321, 362, 819, 46593, 9834, 411, 13607, 2681, 51664], "temperature": 0.0, "avg_logprob": -0.18194309870402017, "compression_ratio": 1.7089201877934272, "no_speech_prob": 0.00970417633652687}, {"id": 308, "seek": 183284, "start": 1832.84, "end": 1838.4399999999998, "text": " zero, you don't penalize at all. And then you have this drug combination. And if you use like", "tokens": [50364, 4018, 11, 291, 500, 380, 13661, 1125, 412, 439, 13, 400, 550, 291, 362, 341, 4110, 6562, 13, 400, 498, 291, 764, 411, 50644], "temperature": 0.0, "avg_logprob": -0.12709884815387898, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.007934254594147205}, {"id": 309, "seek": 183284, "start": 1838.4399999999998, "end": 1843.24, "text": " increase the lambda, and then it will penalize the uncertainty, it's kind of uncertainties", "tokens": [50644, 3488, 264, 13607, 11, 293, 550, 309, 486, 13661, 1125, 264, 15697, 11, 309, 311, 733, 295, 11308, 6097, 50884], "temperature": 0.0, "avg_logprob": -0.12709884815387898, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.007934254594147205}, {"id": 310, "seek": 183284, "start": 1843.24, "end": 1848.6, "text": " reflected by the sample size in their data. If this drug combination has has been used a lot of", "tokens": [50884, 15502, 538, 264, 6889, 2744, 294, 641, 1412, 13, 759, 341, 4110, 6562, 575, 575, 668, 1143, 257, 688, 295, 51152], "temperature": 0.0, "avg_logprob": -0.12709884815387898, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.007934254594147205}, {"id": 311, "seek": 183284, "start": 1848.6, "end": 1854.04, "text": " times, it has relatively narrow uncertainty, it had never been used, then it has a lot of", "tokens": [51152, 1413, 11, 309, 575, 7226, 9432, 15697, 11, 309, 632, 1128, 668, 1143, 11, 550, 309, 575, 257, 688, 295, 51424], "temperature": 0.0, "avg_logprob": -0.12709884815387898, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.007934254594147205}, {"id": 312, "seek": 183284, "start": 1854.04, "end": 1857.9599999999998, "text": " uncertainty. So for example, here with lambda equal zero, they actually recommend this drug", "tokens": [51424, 15697, 13, 407, 337, 1365, 11, 510, 365, 13607, 2681, 4018, 11, 436, 767, 2748, 341, 4110, 51620], "temperature": 0.0, "avg_logprob": -0.12709884815387898, "compression_ratio": 1.840637450199203, "no_speech_prob": 0.007934254594147205}, {"id": 313, "seek": 185796, "start": 1857.96, "end": 1863.64, "text": " combination is the first recommendation. So it actually never been used in the in the data.", "tokens": [50364, 6562, 307, 264, 700, 11879, 13, 407, 309, 767, 1128, 668, 1143, 294, 264, 294, 264, 1412, 13, 50648], "temperature": 0.0, "avg_logprob": -0.20851755142211914, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0036493262741714716}, {"id": 314, "seek": 185796, "start": 1863.64, "end": 1868.44, "text": " So that's kind of create a trade off, like we need to discuss the clinician, like if this drug", "tokens": [50648, 407, 300, 311, 733, 295, 1884, 257, 4923, 766, 11, 411, 321, 643, 281, 2248, 264, 45962, 11, 411, 498, 341, 4110, 50888], "temperature": 0.0, "avg_logprob": -0.20851755142211914, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0036493262741714716}, {"id": 315, "seek": 185796, "start": 1868.44, "end": 1873.16, "text": " combination has never been used, do you want to try this new drug, or you want more conservative", "tokens": [50888, 6562, 575, 1128, 668, 1143, 11, 360, 291, 528, 281, 853, 341, 777, 4110, 11, 420, 291, 528, 544, 13780, 51124], "temperature": 0.0, "avg_logprob": -0.20851755142211914, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0036493262741714716}, {"id": 316, "seek": 185796, "start": 1873.16, "end": 1878.76, "text": " choices, like, you know, these two drug combinations, it right, it has been used more times. Yeah,", "tokens": [51124, 7994, 11, 411, 11, 291, 458, 11, 613, 732, 4110, 21267, 11, 309, 558, 11, 309, 575, 668, 1143, 544, 1413, 13, 865, 11, 51404], "temperature": 0.0, "avg_logprob": -0.20851755142211914, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0036493262741714716}, {"id": 317, "seek": 185796, "start": 1878.76, "end": 1882.92, "text": " I mean, this this tuning parameter definitely plays a role. But you know, actually, when you", "tokens": [51404, 286, 914, 11, 341, 341, 15164, 13075, 2138, 5749, 257, 3090, 13, 583, 291, 458, 11, 767, 11, 562, 291, 51612], "temperature": 0.0, "avg_logprob": -0.20851755142211914, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.0036493262741714716}, {"id": 318, "seek": 188292, "start": 1882.92, "end": 1888.68, "text": " define your reward function, there is another part with the personalized weights. So I was wondering,", "tokens": [50364, 6964, 428, 7782, 2445, 11, 456, 307, 1071, 644, 365, 264, 28415, 17443, 13, 407, 286, 390, 6359, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10815533438881675, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.014495112001895905}, {"id": 319, "seek": 188292, "start": 1888.68, "end": 1895.24, "text": " like, no, we also have similar problems. So we also have like, for example, the survival or", "tokens": [50652, 411, 11, 572, 11, 321, 611, 362, 2531, 2740, 13, 407, 321, 611, 362, 411, 11, 337, 1365, 11, 264, 12559, 420, 50980], "temperature": 0.0, "avg_logprob": -0.10815533438881675, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.014495112001895905}, {"id": 320, "seek": 188292, "start": 1895.24, "end": 1900.44, "text": " quality of life to balance. But then when we provide the personalized weights, and if you", "tokens": [50980, 3125, 295, 993, 281, 4772, 13, 583, 550, 562, 321, 2893, 264, 28415, 17443, 11, 293, 498, 291, 51240], "temperature": 0.0, "avg_logprob": -0.10815533438881675, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.014495112001895905}, {"id": 321, "seek": 188292, "start": 1900.44, "end": 1906.52, "text": " change a little bit of the weights, actually, the rules or the decision you will make, or you learn", "tokens": [51240, 1319, 257, 707, 857, 295, 264, 17443, 11, 767, 11, 264, 4474, 420, 264, 3537, 291, 486, 652, 11, 420, 291, 1466, 51544], "temperature": 0.0, "avg_logprob": -0.10815533438881675, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.014495112001895905}, {"id": 322, "seek": 190652, "start": 1906.52, "end": 1914.76, "text": " from the data will change. So yeah, that's a good question. Yeah, we can we can discuss more", "tokens": [50364, 490, 264, 1412, 486, 1319, 13, 407, 1338, 11, 300, 311, 257, 665, 1168, 13, 865, 11, 321, 393, 321, 393, 2248, 544, 50776], "temperature": 0.0, "avg_logprob": -0.21831529405381944, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.0767640620470047}, {"id": 323, "seek": 190652, "start": 1914.76, "end": 1920.92, "text": " details. Yeah. Thank you so much. I'm going to share my screen.", "tokens": [50776, 4365, 13, 865, 13, 1044, 291, 370, 709, 13, 286, 478, 516, 281, 2073, 452, 2568, 13, 51084], "temperature": 0.0, "avg_logprob": -0.21831529405381944, "compression_ratio": 1.2892561983471074, "no_speech_prob": 0.0767640620470047}], "language": "English"}