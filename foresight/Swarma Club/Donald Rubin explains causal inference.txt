各位老师同学大家好,欢迎大家参加今天的讲座活动。
我是主持人杨二茶,是清华大学的再度博士生。
本次活动有集致和志愿社区附画的英国科学社会成员自发组织,
并且有集致提供应用支持。
今天的活动我们非常非常地荣幸邀请到了当今世界上最具影响力的统计学家 Donald Ruffin教授来为大家介绍英国推荡的工作。
然后为了方便这个讲座,接下来我都会用英语再给大家进行成熟。
Dear students and teachers from all over the world and from the community,
welcome to today's event.
We are very honored to have Donald Ruffin, one of the world's most influential scholar in statistics,
to be with us and to introduce the essential concepts for causal inference in randomized experiments and observational studies, a remarkable history.
First, let me briefly introduce Professor Donald Ruffin.
He is the John Litt Professor Emeritus of Statistics from Harvard University, where he has been professor since 1983 and also department chair for 13 years.
He also holds professorship at the Young Mathematical Sciences Center in Tsinghua University and Murray Schusterman Senior Research Fellow at Fox School of Business Temple University.
He is the winner of Wilkes Medal, Pausen Prize for Statistics Innovation, George W. Snedekor Award.
He has been elected to be a fellow member, honorary member of American Statistical Association, Institute of Mathematics Statistics, International Statistical Institute, American Association for the Advancement of Science, American Academy of Arts and Science, European Association of Methodology,
British Academy and the US National Academy of Sciences.
As of 2021, he has authored and co-authored over 400 publications, including 10 books, and has four joint patents.
And for many years, he has been one of the most highly cited scholars in the world, with currently over 35,000, 350,000 citations from Google Scholar.
And we're also very honored to have three discussant speakers with us.
They are Professor Zhou Xiaohua and Zhu Zhou from Peking University.
He's the PKIU Chair, Professor and the Chair of Department of Biostatistics in Peking University, and Xi Liang Zhao, Professor from the Wang Yan'an Institute for Studies in Economics and from Xia Men University.
And also Professor Tui Pong from Association Professor with tenure in Qinghai University.
Now without further ado, let us give a warm applause and welcome to Donald Ruby, and to have him introduce for us the causal inference.
Okay.
So these, these slides were created a few months ago.
I want to thank the organizers for inviting me.
And I'm very pleased to be able to give this introductory lecture.
Another comment that that's relevant today, but that is, is not relevant with was not relevant when I made up these slides is that within the last week, my co authors on on a causal paper, just got the share the Nobel Prize in economics.
And Guido in bends.
Just about a week ago, got a Nobel Prize in economics.
And I'm very happy for work that's completely consistent with this framework that I'm going to describe here.
And actually both of them gave me gave me phone calls the day that I got the Nobel Prize because it's an impressive event so in, in addition to being this interpretation of essential concepts and causal inference.
I think now it's pretty much become the standard approach, at least in economics and parts of medicine. So I think it's the talk is even more relevant than it would have been of even a few weeks ago.
The title is essential concepts for causal inference and randomized experiments and observational studies are remarkable history, and a couple comments about the title, what I mean by essential concepts is ones that you need, and you get rid of the clutter.
So for example, I'm often asked about the graphical approach to to to causal inference. And in many cases, other approaches than the one I'll be describing here can be useful for communicating ideas but they're not essential.
And the sense of essential here is like the mathematical sense of of essential. So if you leave out a condition in the theorem that makes the condition false without that condition, then that's essential.
And if you if you add stuff, that's not essential.
So for example, if you say you have a right triangle, and with two sides equals so it's an isosceles right triangle, and you say the twice is the square of the short sides equals the long side.
Well, that's true. But it has clutter in it because you're putting something in that you don't need because it's any, it's any right triangle for which that's that's true.
So, and why do I call it a remarkable history. Well, as the idea will be developed here, which you'll, which you'll see at least from my perspective, is the history of the formal history is very recent.
You can trace the intuitive idea of causal inference back for thousands of years, but the formal mathematical history is really, in my mind, remarkably recent, meaning it's a 20th century idea.
That's related in some ways to another set of remarkable ideas, having to do with quantum mechanics, basically the idea that you can define two things, each of which can be measurable, you can have to observe, but you can't observe them simultaneously.
And that's what I regard as one of the essential ideas of causal inference, as reflected in, especially in the, in the mathematics of the 20th century. Okay, next slide.
Well, a pro-leg to causal inference is my sort of exposure to what I regard as science.
As a kid, like many kids in high school, I was interested in math and physics, but far more physics than math. So when I went to college, I went to Princeton University, which I entered 1961.
And the first physics course I took with a relatively famous physicist named John Wheeler.
John Wheeler was a colleague of Albert Einstein's, and so they were in the physics department at the same time.
And he was also well known for his attributed to having made up the name Black Holes. And in fact, he denied that he was giving a lecture at one point in time as describing something that has so much mass, so much gravity, that even light cannot escape.
And somebody in the audience, this is an audience of about 200 people, shouted out, it's a black hole. And according to Wheeler, he tried to find the guy afterwards, but never could.
And so he, he started using the expression became attributed to him.
Another thing that Wheeler, he was a really kind person and outstanding teacher.
And his first problem that he assigned us in 1961 was how far can a wild goose fly.
And obviously he was not looking for a precise answer, but he wanted to know how you would try to solve it.
What are the principles that, that even when you're really in physics understand, what are the principles of conservation conservation of energy, momentum, these other basic concepts and so the energy was had to be conserved.
So how does, how does a wild goose say up in the air and fly. Well, what he's doing is he's converting energy.
That potential energy into kinetic energy to fight gravity pulling him down. So he wanted you to think about those those principles.
Another thing that Wheeler was known for is he was the PhD advisor of Richard Feynman, the famous US physicist Feynman.
But these were complicated years in 1961, especially for a male in the United States because there was a draft lottery.
In order to stay out of the draft, to avoid tromping through the mud, thousands of miles away, you had to stay in school. And so, at that time, if you were going to be in getting a PhD in physics, pretty much the job market looked like you even if you got a PhD, you had to had to be in nuclear physics.
And so, instead, I switched to psychology.
And I went when I went into psychology at Princeton, there was a gun, Sylvan Tompkins, who was a wonderful professor is also a character named Julian James.
In 1964, and became actually very good friends with especially later when I returned to live in Princeton.
And it was became very close friends and talked about lots of ideas. And one of his his critical ideas was consciousness. What makes human beings different from other animals.
He wrote a book that was published in the 1970s, called the origin of consciousness in the breakdown of the bicameral mind.
And this book actually advanced a thesis that consciousness was special to humans.
It's not possessed by animals, because they had two hemispheres, so all brains have have two hemispheres, but humans developed the art of communicating between the two hemispheres, which are really independent.
They're only connected with a gang of nerves at the base of the of the brain at the top of the spine for the corpus callosum.
And they, these two hemispheres communicated almost independently through language at the zap messages back and forth.
And so his, his version of consciousness was that people would, we had these two hemispheres talking to each other, almost as if they were.
And in fact that pre conscious people would hear these messages as messages of God's.
And this was very influential on my on my thinking and psychology became very interesting to me is all science everything we math mathematics even formal logic is obviously filtered through our brain.
So could you go back to the, yes, I'm not done with that yet.
So I went when I went to graduate school at Harvard, I started out in psychology, and then I realized that I really wasn't that serious about psychology, and I switched to computer science where I got a master's degree.
And then I actually eventually switched to department of statistics right from my eventual PhD, where my advisor was William Cochran, this wonderful Scottish statistician, very, very down to earth guy, who taught me experimental design in 1968.
But this was a was a great course, because it had formal experimental design how you learn about the world how you learn about what works and what doesn't work in terms of interventions.
And the other thing about Cochran's course that was I opening for me, it was completely consistent with the science that I learned when I was in physics.
And one thing about it is there was a clear separation between science, which is the object of inference, and what you do to learn about the science.
So what you do to learn about science is you intervene in the real world in some way to intervene to measure aspects of science of the world at one point in time.
To intervene you have to change something you have to either observe something which you throw photons at it, or something. So the, which you try to learn about is, is the object at a point in time at one instance, and you learn about it through what you observe at a, at a subsequent
instance. But the, the notation that you use to represent the science at the previous point in time is the same.
So the science notation for the science doesn't change as you try to learn about it, or measure it.
But the, but the fact of measurement changes the science from the earlier point in time to a later point in time.
And that way of thinking about it is, is missing data always exist. We cannot go back in time and undo what we did to, to measure the science. And this was completely consistent with, with two principles in, in, in physics, the well known
uncertainty principle, for example, which says you cannot measure position and momentum at the same point in time you have to take your choice they both exist.
But you can only measure one with this with certainty. And there's another principle that's sometimes confused which is the observer effect that the active observation actually changes the world has to do with time.
And the state of science at time to is different from the state of science prior to that.
And where's the Heisenberg uncertainty principle is has to do with with quantum mechanics and uncertainty of being able to measure both things at the same point in time.
Okay, next slide.
So, so my exposure to statistics when I entered Harvard again was to Cochran, and, and he was a protege of Ronald Fisher.
And Ronald Fisher had really revolutionized the field of statistics in fact started in some real sense, and in 1925 with his book statistical methods to research workers.
And in the last chapter in that 1925 book, he proposed randomization. So if you're trying to learn about what an intervention does meaning is fertilizer one better than fertilizer to control fertilizer versus a new fertilizer, or for a variety of
seeds of a plant is a new variety better than the old variety, you actually should randomize turns out that the idea of randomization, mathematically, was in the air at the time.
If you read some old student gossip articles in the early 20s, you would say well if you had randomized, then the distribution would be this way.
This sort of was in the air at Rotham study at the Rotham study experimental station, where they were doing studies of different fertilizers and different breeding of plants and animals.
And the idea of actually randomizing this issue in thereby you created balance and all treat pre treatment variables in expectation. So the fact that it created balance was in these articles, in various expressions, but the Fisher said that I know that's what the mathematics
is, but I'm saying you should actually do that. You should actually toss coins to assign treatments, and no one had actually done that, but as far as I can find.
Before there's some debate about this.
But it will say a little about that in the subsequent slide.
Now there was recondite advice at the time that if you had unbalanced covariates, so an important pre treatment variable that was unbalanced between treatment group and a control group.
Fisher actually said he would re randomize. Now he didn't write that. And I can't find that advice anywhere. So you got a bad randomization, meaning for example, in the study of hypertensive drugs, where they have males and females.
If you if you did complete randomization, it's possible that all the males would get an active treatment, and all the females would get the control treatment.
Bad advice.
Bad randomization so the advice was from from what would Fisher wrote is live with the randomization, because that's what underlie all the mathematics like the f, f distributions and t distributions.
If you talked to Cochran about what Fisher actually would do, he would re randomize.
He would throw away the bad randomized allocation and look for a better one at the, at the time.
The theory was was was complicated because you didn't have the usual symmetry arguments working for you. They were actually led to truncated distributions, which you which for the, which the computation was back then impossible.
But the theory and application is now being pursued because the computing is much better than it was. That's being pursued in a series of articles I wrote with a former PhD students.
And it's being pursued in other words, I'm doing with other people.
And that's it's being pursued because the computations is, is possible.
Now, if you, if you did get an acceptable randomization you were living with it. How would you do the assessment of the was there a causal effect.
And in here he had this very deep idea that's related to Carol Poppers of the philosophers idea of science advances by rejecting myths.
So what, what, what, what Fisher did is he had this, this method of inference, where you hypothetically re randomized, you look at the data, put down a null hypothesis and hypothesis that that you don't that you want to prove is wrong.
And so in this re randomization was a stochastic proof by contradiction so it's not really a proof by contradiction, but you get a probability that you would observe data, which you did observe this extreme, if the null hypothesis were true.
I think it's a brilliant idea that that led to a sort of a deceptive version of it, because the name and Pearson ideas which are which I think are not nearly as sharp.
And I, and I think that this proof by contradiction, it can be embedded within a Bayesian framework. And I wrote about this in an annals of statistics paper 1984, where I call that a posterior predictive check or price, where the p value in
a check is a posterior predictive p value, or instead of having a sharp null hypothesis, what I mean by sharp, if there's no missing data anymore. So you can fill in all, all missing values.
And, but instead of having a sharp null hypothesis you can have a stochastic null hypothesis. So it becomes a Bayesian positive predictive p value.
Even though he was never formal about the, this idea of an alternative hypothesis, because he said, I have never, I have no problem in choosing a test statistic, just do it. He clearly understood this site, a non null causal effect, because in this 1918 paper, years
from the 1925 book, he has this direct quote, if we say this boy has grown tall, he has, because he has been well fed, we are not merely tracing out the cause and effect in the individual instance.
In this particular case, we're suggesting that he might have quite probably not very well written, might quite probably have been worse fed.
In that case, he would have been shorter. So he's comparing the boy's height, being well fed with his hypothetical height, had he been poorly fed.
So he clearly has this idea of, of the, of an alternative hypothesis here the alternative is being well poorly fed, because the, what's observed as being well fed.
But he never had any explicit notation, never any mathematical notation for formalizing non null causal effects, despite, I mean, it's very difficult to criticize.
Fisher's work with respect to distributions under this sharp null hypothesis, tremendous geometric insights based on symmetry arguments.
Fisher was was legally blind. And, and so according to Cochran, he would, he would, when he had a problem, he would, you know, sit down, close his eyes and think about things.
And we have these blasted insights, I guess that, that came into his mind. So where do we get the notation that that that was arose in the textbooks in the mid 20th century like, like in Kempthal next slide.
That's okay. I mean, these things happen with with with zoom, especially when the when participants are thousands of miles away.
So the notation that was used in in subsequent years to describe in mathematically the situation with about causal effects was actually due to Neyman Jersey Neyman in his 1923 master's thesis, where he defined written in in in Polish.
And this thesis was not translated to English until 1990, although because Neyman by before 1990 was at Berkeley, the notation had had a big influence because Berkeley was the outstanding department of statistics at the time Neyman was was there.
And so what he defined the estimates the quantities to be estimated in randomized experiments as functions of potential outcomes for N units.
In other words, why of zero is the array of potential outcomes under treatment zero, and why one is the array of potential outcomes under an active treatment, labeled here treatment one.
And Neyman at 23 was written in Polish, as I said, translated to English in 1990. And I, I made up the phrase potential outcomes, really to stay close to this 1923 paper, where at least the translation of into English of this, of this paper in 1990, called the potential yields, where the units of the
experiment were the N units were plots of land, and the outcomes there were yields of of of a particular variety of a of a plant under different fertilizers.
And so the treatments there would be the control fertilizer and treatment one for example would be the active fertilizer.
And Neyman actually use this notation, although later he denigrated he said I was I was just fooling around I really didn't know what I was doing.
And there are there are other examples of that in science we can see people using notation that they really didn't understand the full meaning of fact actually you can you can see that in some of the work on in relativity as well.
I don't want to get off onto that topic, although I could answer some questions about it. And so, namely put down this notation, and implicitly assuming something that I later called, so to stable unit treatment value assumption.
What this means is that if I give if I have a Y which is the outcome for plot I so I indexes the plots, and w indexes the different treatments. So that's a function of I and w. That's all that's that's it's a meaning it's a well defined function.
So you have that notation.
So all that the outcome for particular treatment on a particular plot. It's just a function of that plot, and of that treatment. It's not affected by the fertilizers that the other plots got. So for example,
excluded interference between units for example, if you're doing an agricultural experiment where rain would move one fertilizer on one plot to an adjacent plot. So the adjacent plots, not only were affected by the fertilizer they receive, but by fertilizers adjacent plots got.
And so agronomists knew this for hundreds of years, but, but this assumption wasn't really formalized until the combination of of naming and this explicit assumption that I made up and in the quote said for 1968.
So the point is that you cannot observe both of these on any one unit cannot observe both the outcome under under active treatment and the outcome under control treatment.
And so this idea was kind of natural to me, because I grew up with with Heisenberg that you that just existed, but the name and contribution went beyond this notation.
And he evaluated the operating characteristics of procedure procedures, such as estimators over the randomization distribution.
So he said, Okay, I observe this under the, what would be the expectation of the of a statistic, an estimator such as the observed outcomes, the observed potential outcomes under active treatment, minus the observed potential outcomes under control treatment.
So this is a statistic being a function of observed values. But now we'll see what I would have observed under all possible values, and I can derive pro operating characters for such as unbiasedness of this difference in observe sample needs.
Very important idea which eventually led to all the name and Pearson stuff.
He also worried about the role of non additive unit level causal effects, an additive unit level causal effect is the difference between why I have zero and why I have one is the same for all I.
So that would say that the new fertilizer is better than the old fertilizer by the same amount and all plots.
And name and word about that in 1923, because he he not only defined unbiasedness, but but he he eventually defined confident would later was called confidence intervals.
Now, in, in, in later years, he denied the lack of understanding in 1923, pointing out that the, that the definition of the potential outcomes was an important idea.
And he said, I really didn't understand the real depth of randomization, because they never advocated it is Fisher, who advocated randomization actually randomizing plots, not, not name it. Next slide please.
So once on these insights. Well, these are 20th century insights at least as, as, as far as, as, as I can see. I've looked a little bit I'm not a really in depth historian like Steve Stigler, my former colleague at the University of Chicago, but I think I can't find any, any corresponding insights in
these insights where you define an estimate quantities you want to estimate in terms of measurable quantities, which, but these are individually measure, but they cannot be simultaneously measured, measurable even theoretically.
So I really think these are really 20th century insights that were seem to be floating around Northern Europe at, at the time was was was Fisher really the, the, the, the first to have these well Steve Stigler likes to point to an American psychologist,
who's whose father was a mathematician at Harvard in the late 19th century, who was writing about unbiasedness from, from taking representative samples, very, very close but as far as I can see, there's no use of the idea of randomization as a base of inference the way,
the way Fisher did. But anyway, after Fisher proposed randomization in this last chapter of statistical methods for research workers, randomized trials quickly dominated agricultural and animal, animal breeding in the United Kingdom, and became even dominant in the United States in
industrial work, more applied work was was was done.
For example, by Oscar Kempthorne,
Bill Cochran and Gertrude Cox, George box wonder wonderful work by by by box in industrial experiments, David Cox
wrote a wonderful textbook.
I'm planning experiments and supporting more mathematical work and almost pure mathematical work was done in the Indian statistics Institute, which is founded by my Helen obis work was done there by RC bows, Nair, see a row who is still going strong in his 100th birthday he had his 100th birthday.
Are we 100 first I think a few weeks ago.
And subsequently randomized controlled trials entered Western industry they started doing industrial experiments and at the end of the Second World War, when the, the, the, the West, the allies had completely sort of destroyed.
Japan at the end of World War two.
We sent Edward Deming, the West and Edward Deming to help rebuild Japanese industry, and through the use of quality control in experiments and in randomized experiments.
It was a combination of randomized experiments, and especially with the idea of quality control. Japan has given a Deming medal for quality control since 1951.
But the insights that that were exposed in the in this work with randomized trials were really limited at this time to non conscious units plants, animals, or industrial object like widgets you know various mechanical things that we built or paints that
we used to do experiments on paints for painting the dividing lines in in in roads, for example. Next slide.
The transportation of these insights to randomize control trials with conscious units came before the transportation well before the transportation of these insights to non randomized studies.
And in medicine my understanding is the first randomized experiments were done in the United Kingdom in 1946, the Medical Research Center, and a guy named Sir Bradford Hill on Streptococcus,
where they have antibiotics study of antibiotics they actually did randomized experiment.
Early big randomized experiment with salt vaccine, whereas like thousands of people that was done in 1954, actually randomized experiment with conscious units, and they actually use blinding them.
The idea that you don't tell the patient, which you're, or the kid in this case, which vaccine you're getting the the active vaccine or placebo vaccine.
In the in the 50s and middle eight 50s of the United States Food and Drug Administration, it became almost wedded to the use of randomized experiments.
And so randomized experiments entered pharmacology. And before this before the 1950s and Paul Meyer was my colleague at the University of Chicago was the main instigator this he really sort of fell in love with with random device experiments.
But when I say falling in love. I think there was an over-resilience in adherence to the intention to treat principle to estimate the effect of the assignment to treatment, rather the effect of assignment and receipt of treatment, which I don't think was very wise.
But the idea intention to treat is you analyze the data, the way the units got assigned, not what they took. And I think this is a very relevant comment now, because I view the recent Nobel Prize in economics, which is given to like co authors, angriest and
David Carr, for actually implementing a study in the non randomized interesting inference in the non randomized study.
This the idea of non compliance because that conscious units have had a habit of not agreeing with what they were randomized to not agreeing to doing what they were randomized to do.
And the interesting thing is, but there's no use of these Fisher name and insights, or the notation and non randomized trials.
It wasn't used at all, I think, until I proposed using it in 1974 paper.
So for example if you look at the 1964 US Surgeon General support report on cigarette smoking and lung cancer, very important study.
Everything was done by regression.
In fact, ordinary least squares regression.
Nothing. No, no use of the insights from from randomized experiments in this giant observational studies. In fact, regression was used everywhere epidemiology economics, social science psychology sociology.
They all used regression with the potential outcomes replaced by the observed outcome with an indicator w i for each unit for which treatment they got.
So they would do a regression of the observed value is why I ops as the observed value for each unit on an indicator variable w i and covariance.
But this notation violates this this principle that I learned when I was a kid that you separate the science from what we do to learn about the science.
So this why obs notation would became completely prevalent for dealing with with non randomized data, starting actually in the at the end of the 19th century, we're doing regression.
There's an old paper by you on how you do this, using this observational notation and regression.
Next slide please.
Well, it turns out this sequence of papers came to be called the Ruben causal model to the name Paul Holland made up in his 1986 Journal of the Wilson of the jazz American statistical association paper, Ruben causal model, which I sort of the proposed this model
and wrote 1974. There's a follow up paper and proceedings and 75 paper and missing data where I, I, I called missing data in a randomized experiment should be treated as missing data.
Even from a Bayesian point of view, if I frequent this point of view, talked about how randomization inferences basically based on missing data followed up with another publication 1977.
Probably the, the, the, in some sense the most complete paper was in 1978 in the annals of statistics was on Bayesian difference for causal effects.
The contributions of this paper, I think some of it were, were, were beyond what, what Neyman did but certainly built on, on, on Neyman's notation to say the notation of potential outcomes, define causal effects in all situations, not just in randomized experiments.
The idea that you can separate the science from what you do to learn about the science says you first define what you're trying to estimate.
And in an observational study you're trying to estimate the same thing you're trying to estimate in a randomized experiment, you just intervening in different ways.
Actually Neyman when I, when I had an office next to his at Berkeley disagreed, he said no, you can't talk about causal effects without randomization.
You have to have randomization even talk about causal effects, otherwise it's too speculative. So instead, Neyman said let's talk about the stars.
Another confusion of his paper was that you needed an assignment mechanism for causal inference. So an assignment mechanism is just the generalization of randomization.
So it's the probability distribution for the treatment indicator, given the covariates and the potential outcomes. So that's probability of w, given covariates x, and potential outcomes y0, y1 with the general dependence on y0 and y1.
And I defined unconfounded in, I guess, 74 I think, to be when this assignment mechanism doesn't depend upon the potential outcomes.
And you know it doesn't depend upon the potential outcomes in randomized controlled trials, because you did the randomization.
And it was, you couldn't see the potential outcomes, and you didn't use them, and you didn't use any unobserved predictors of potential outcomes.
The confusion inference I defined this concept called the ignorable, where the assignment mechanism just depend upon observed values, for example any sequential randomized controlled tiles, so that in this potential dependence on why
the observed value as in you doing a play the winner randomization. So you'd randomize the first unit, but after the after the first unit you change the odds and getting active treatment versus control treatment.
So it, the odds are in favor of the more successful treatment based on the earlier units. So here why are you first all the observed values of y0 and y1.
And of course in Bayesian inference, you have to model the science, in addition to the assignment mechanism.
Now, and I called the ignorable because that factor in the likelihood for the posterior distribution can be ignored to get the same inference and get the same answer.
So the, so the assignment mechanism creates missing and absurd potential outcomes. I say an artistic touch is needed here, because all models are wrong.
There's a question that goes back to George box, or before that von Neumann even even said that the real world is much too complex for anything but simplified models. So if, if I know if the math was too hard for von Neumann, and who's obviously too hard for
So that the idea, the idea of, of models that we use in science are always wrong, they're always too, too, too simple. I say an artistic touch is needed here, and there's a great Picasso quote I don't know if I have it here but it's
he's talking about computers and Picasso said computers are worthless. They only give answers.
Picasso's point I think is, is a great expression of the field of statistics thing that that makes statistics artistic is you have to posit models.
And, and you know these models are wrong, but they're, they're, they're working hypotheses, and that they have to be discarded as, as you go along.
Please.
So the fundamental problem facing causal inference which is an expression that made up in the 1975 paper.
It's a missing data problem.
And, you know, Paul Holland had had a more eloquent way of saying this in his jazz of paper 1986, but it's an expression that that I, well, that was in this 75 paper.
So for each unit, and here we have an example with any units, and either a control treatment zero, or an active treatment one, you get to observe either why one or why zero, you can observe both of it.
And the, and the random assignment of active or control means a representative sample of why I one will be compared to represents that of why I zero obvious.
And once you put down the notation, it's obvious notation with the question marks and the checks. It's obvious, but you have to have names notation to do that, and you have to have the idea. This notation applies beyond randomized experiments.
And, and then you have to have the ideas you need a model on this assignment mechanism that creates missing and observed data, in order to draw inference about the missing potential outcomes.
I think that that was an important insight. And in fact, I think it's the basic fundamental idea behind this year's Nobel Prize in economics, because David card was actually doing cause inference in a study where an observational
study where he had hypothetical randomization he can describe it that way, hypothetical randomization of fast food restaurants in New Jersey versus Pennsylvania.
And he the hypothetical experiment, again not said this way but had to be on his mind was a coin was tossed to see whether the, with the restaurant was situated in New Jersey, or Pennsylvania, because New Jersey and Pennsylvania had different laws
and different state laws on the minimum wage. And so the units in that experiment with a different fast food restaurants, and the hypothetical randomization was were they in Pennsylvania, or New Jersey, where there were different minimum wage laws.
And that has important economic implications, and which eventually led to David cards Nobel Prize, and English and in Ben's use of these potential outcomes to describe the methods.
You're both both very generous. I mean, I'm, I'm, I'm sort of jealous of it, but, but it was not I was not any more mistake for the Nobel Prize to do that, then a mistake and lots of the Nobel Prize had mistakes in that in that same sense.
Okay, next slide.
It's a mistake to do this for regression use of this regression goes back to the 19th century for causal inference, but it's a mistake to regress its observe value on an indicator for which treatment you got in and covariates.
Why is it a mistake. Look at this notation. It loses the potential outcomes and the key Fisher naming concepts and using the observe value notation.
Mixes up this why habs mixes up the assignment mechanism and the in science.
And here, my insights, which built on Namens insight from the notation is, you want to separate the science here the, the, the wise from what you did to learn about the science the W's.
This was, was critical to the 20th century insights, and here this mixed it all up again suppresses these key insights there are no missing data. So once you write this down, and you're regressing the observe value of why on the observed W's and the, and the observed covariates.
Hmm, must be parameters well parameters are always missing because they're they're hypothetical. So but this this notation for observational its use and observational became standard and biostatistics economics, epidemiology everywhere, and even great statisticians and
epidemiologists, for example, Fisher, I can't talk about these pay I don't have time, but I can find mistakes that Fisher made in dealing with secondary outcomes in randomized experiments.
We made mistakes, Cochran made mistakes cornfield made mistakes, they confuse themselves in observational studies, because they're always in this observe value notation, and talking about doing regressions, the cornfield quotation that's on the next slide, I think it's particularly revealing.
So next slide please.
So the results on the way observational studies were quote handle, because there was no design phase, and they're instead confused analysis, and they, and they mixed up association versus causation came muddled.
I think one of the real examples of that is it was called now a case control study and cases referred to people with a disease controls referred to people without the disease.
And I much prefer the terminology case non case, because the control in a case control study is completely different than a control in a randomized experiment control in a randomized experiment is a person who's exposed to the control treatment.
And in this case controls that a case control is somebody without the disease. So it's very confusing terminology.
Because with case control studies, use the sampling mechanism is how you drew samples, but they're confounded sampling mechanisms in a case control study are confounded because they by definition depend upon the observed potential outcomes.
In other words, what they did they typically, they looked at all the cases, all the people with the disease, and they compared them to non cases, people without the disease.
So this is what you had to do when you're looking at a rare outcome, you almost had to do it because they're otherwise became too expensive to collect data.
This is a quote from cornfield, who's the main epidemiologist on the smoking lung cancer.
A direct quote from a paper that he wrote in 1959, we now consider the distinction between the kinds of inferences that can be supported by observational studies is non randomized studies, whether prospective or retrospective.
And this case control prospective means you go forward by by looking at smokers and non smokers.
Whereas a case control looks at lung cancer versus nonline cancer.
And so the prospective studies, or retrospective and those but they're observational, and those that can be supported by experimental studies that there is, is a distinction seems undeniable.
But it's exact nature is elusive. And I agree with it at that time it was elusive, because it didn't become formalized until I did it in 1977 by putting down an assignment mechanism, which embedded these two kinds of studies within one framework and could define what the benefit of
the assumption was. It's, it's unconfoundedness is that the assignment mechanism cannot depend upon potential outcomes, because you did the assignment.
But in a, an observational study where the prospective retrospective is unconfoundedness of the assignment mechanism is an assumption.
There's a big difference between how well justified the assumption in a randomized experiment is risk the assumption in an observational study.
Next slide please.
So the starting conclusions of I mean the starting conclusions on causality. And that's why I hope this was a useful introductory lecture.
So let's retain the key insights from the past key insights meaning insights from name and Fisher, but issue, get rid of confusion from the past, get rid of the fact that everybody's doing everything by regression, and they're replacing the potential outcomes by the observed
outcome and trying to do the squares regression.
So the important conclusion is to realize that the ideas between randomized controlled trials are extremely recent people have been talking about causality for thousands of years.
Now which direction should I hunt for food, should I plant this variety or plant that variety, when we came out more an agrarian society.
The ideas are extremely recent. These are 20th century ideas.
Now we should certainly update statistical methods from in both design and analysis to take advantage of modern computing. So what I mean by design and modern, you know, take advantage of modern computing to take the advantage of modern computing to throw away bad randomized
allocations, use machine learning ideas to draw thousands of randomized randomized allocations, and then compare balance on on zillions of covariates and understand the geometry behind that, and then throw away the bad allocations.
I encourage mathematical precision, especially in notation and logical flow. So I don't care about mathematical precision in these in these stupid theorems about asymptotic balance and stuff like that.
Take advantage of modern computing and stay with with finite sample methods as much as possible.
And this precision in in thinking and logical flow can have critical consequences in in in current in challenging applications. For example, now in placebo effects. I think one of the really important areas of advancement can be can built on some
ideas of placebo effects using this notation of potential outcomes, and for which I think the this this Nobel Prize was recently given.
Okay, so I think I probably have run out of time if not more. It's four minutes after 10 is I get so I will let our host say how we should proceed.
I thought we have the next slide.
The last slide is newer general ideas. Okay, so this in here is really this idea propensity score for for for designing studies both experimental and observational studies, this idea that you, you, you summarize all zillions of covariates by a low
three dimensional summary. So this is really is tied to machine learning ideas and use these both for designing experimental and observational studies.
So this propensity score idea is due to this paper by Paul Rosenbaum, and I wrote in 1983 that you can design observational studies and propensity scores because they do not they are not a function of potential outcomes, they can be used in
a much more important idea, then, then its use as an analysis tool.
And re randomization experiments to avoid unlucky allocations, and this is an expanded template for observational studies.
Another joint paper with us with a sequence of papers.
Another really important idea is what I with Paul wasn't that Paul, constantly for a caucus and I called principal stratification, which generalize this idea of instrumental variables from economics.
So I, I think this is with principal stratification. I think as as a basic idea, that's yet sort of got the Nobel Prize and I mean I'll make the tie there, even though formerly it didn't.
I think it's a combination with with complications to deal with with not only non compliance, which is the instrumental variables, setting, but basically, is ruben and and, and, and, and Thomas paper and placebo effects.
I recently wrote where the that that generalization is is is made using.
Actually, this paper says that when you're doing randomized experiments is currently done for a new drug. For example, the way it's done the placebo control trials.
It affects the placebo effects meaning that people think they're getting the active treatment, and that affects the outcome.
And if you do a placebo controlled randomized trial, and now you prove a drug, when people have a are taking an approved drug, they know they're getting an active treatment when you fill a prescription farm.
Retreated in practice, are getting the placebo effect. In addition, and what that leads to in practice is higher doses of drug that you then what you need for the outcome effect because the effect on the outcome you're already getting with the
when you go to a drug store in in in you in you fill a prescription.
So in practice, I think 80 or something like 50 to 60% of drugs as approved by FDA from placebo controlled trials are ratcheted down in the future based on observational data, saying the doses were too high, led to too many side effects.
And that's approved for practice is the dose that was used in the placebo controlled trials, when giving that dose.
The units the the patients with you observe is it in addition to the drug effect, you're getting from placebo effect.
So, the, the, the drug that's being assigned is too high, because it's, it's, it's being assigned to get the effect, the real effect of the drug plus the placebo placebo effect.
So this, this, this 2020 paper that I wrote that that's published in Andrews journal, I believe.
So to me, it's the Japanese Journal, Kazuma Shumatsu's journal, actually, is about that, that, that the topic, have other things to submit to Andrews journal on on this topic, I think it's a very important topic.
Okay, I think I'm finally done.
Yes.
And thank you for your patience, as we struggle with the slides.
Thank you so much and sorry for the slides is being too excited to see you today.
Very kind. Thank you.
Okay, so the next part of the, of the seminar will be the photo session.
Please all the guests to open your camera and then we're going to take a group photo.
And then we're going to take a group photo and then please all the students and teachers to open your computer and then we're going to show you the camera and then we're going to take a group photo.
We have like 17 pages of people who are here today, listening to your talk.
That's nice.
Yeah, we have like 500 people today.
That's a lot of people.
It is, we have crowds of guests here.
Thank you so much for the wonderful talk.
And also, sorry for the slides being stumbling.
That's okay.
But, but it's very, very nice to have you here. And now let's move on to the discussion session.
Please turn off all of your cameras.
I saw a lot of friendly and familiar faces here.
Please turn off the camera.
We're going to send the photo to the UK.
Okay, thank you.
Thank you.
Please turn off your camera.
And then we can continue the next part.
Oops.
Can you see my share?
You can't see it.
And now it's the discussion session.
I'm sorry if my computer is not working really well today.
And let us welcome the three speakers.
Let us welcome all three important guests for today's session.
There will be Professor Zhou Xiaohua.
I saw your camera is on.
Andrew Joe.
And Professor Zhao Xiliang.
And Professor Cui Po.
Okay.
I'm going to comment at Rubin's talk and we are going to have an open discussion session. So you may, you may each one ask questions to Professor Donald Rubin.
And now I'll pass the mic to Professor Zhou Xiaohua from Peking University.
Yeah, hi, Don.
Nice to hear you talk again.
I've never heard this talk before, but every time I will gain insight from your talk.
Yeah, yeah, I, I think one of the reasons is that I, I use a limited number of slides. And so the talk is always slightly different.
Anyway, this is very great talk or just common few sense and also maybe just discuss a little bit further work actually based on principle strata ideas.
I think it's a very important idea. And by the way, so I have a note down for like over 25 years old friend of mine.
And actually my research in causal inference actually introduced by Don to me about, I think I can remember when I was doing post that hover or when that started.
But, but one work I had done was done is about encouragement design. I don't know if you remember.
Andrew, your, your voice is breaking up.
Oh, can you hear me.
Yeah, I can hear you now.
Okay.
Yeah, I just, yeah, I just said look.
Okay.
Yeah, go ahead.
So what I say that my research in causal inference actually introduced you to me about 25 years ago.
Yes, about a study about the randomized encouragement design for flu shot.
Actually, that's a paper we did with Emmons and his students.
That's right.
You remember that.
Yes.
And then actually that's a great work and then got the award from international Bayesian society.
Yeah, actually, so I very grateful actually you are introducing me to the field of the causal inference and then, and then we'll have done some work so I want to mention a little bit.
The, the, the Ruben framework for you call the Ruben causal model, I think I like that model, because I think one thing I don't probably dimension maybe to pride in his talk is I think is a Ruben causal model actually clearly distinguish between estimate and estimation, I think that's actually very crucial.
I mean, bio statistic in the medical field. So what I mean is the estimate is quantity is determined by science. So scientific question you want to ask.
So that is given by estimate, which has nothing to do with the model with assumptions I think that should be very clear, because, because particularly the causal inference became more popular in computer science, in artificial intelligence, and in other fields.
I feel like when I read those papers, it's not clear to me what their estimate with what they're trying to do.
They're just through all the jargon which we talk about the mathematics stuff and then the, the, the location really get confused, because our audience mostly actually from the computer science I think the field.
I think it's very glad that actually down you give a talk in this audience is good. Yeah, because I think this audience most familiar with the pearls, the graphic model, I think my understanding is, but I think it's good.
I mean, the pearls graph model I think has their own usage, has own usage. But on the other hand, I think it's a woman, we need to clear what are the scientific questions, and what are the estimates with estimate is the parameter, which defines scientific questions.
I think if you don't make that clear everything well, well, well, I think we'll go around afterwards. Then after you have an estimate so clearly defined and everybody understand, then you talk about estimations.
So where does assumption come in, because the, the estimation whether you can estimate estimate dependent on the data you have, you have a, if you have a randomized trial data or you have a long compliance, or you have truncation by death.
So, so you have to very clear, clear for a study the data you have to estimate estimate, and then also clearly to make your assumptions, which to make estimate estimate more.
So that's a crucial I think that part of the causal inference to say the parameter you have is whether it's estimable.
So, is the parameter you have can be estimated consistently based on the data you have. I think that part I think is part of the, I think the key, I mean maybe don't agree in the causal inference, compared with other field, the, as an ability issues.
And then the third I think is the important is to check your assumptions you made in your causal inference, but some something make sense some something doesn't.
So if the assumption doesn't make no sense in the in practice, in practice, then I don't think your causal inference make any sense.
So that's I learned from down actually to work on those the medical field, the problem. So I want to also mention a little bit about the, the some extension us actually some application of the principle certification down mentioned just a little bit.
So actually that's the he mentioned about this dude.
So that's one of the students actually work with us with work with me and down on the also around my encouragement design. So we have actually two paper on that area.
So I want to mention, based on the principle certification that I have done some work actually also motivated by the dance, the principle. So why is truncation by that's how do you make causal inference when you have a truncation by that.
So what's our problem is, is this if you interested in some parameters, like quality of life in five years after you take the treatment by the people might die one year after receive the treatment.
So, so for the people who die, what are their outcomes. So actually that's a very interesting issue right now.
Can you still treat as a missing data, which some people did to say, even they die this is still missing the quality of life five year after the die.
So that's actually might be a problem because I don't know what the quality of life for that people.
So that's actually the big issue. So that's actually show the principle of the stratification really work in the cities. So, so we have done some work to how do you make causal inference when when when you have a truncation by death.
This is very important because that one is commonly occur in medical research, because for most of the cohort study large cohort study you always have some people who died during the study.
So how do you deal with us and and then they don't maybe have a time to talk about that truncation by that but that's actually the very important applications in medical research.
But another area I think I also get a lot of attention recently is the precision medicine to say how do you do the causal inference in precision medicine.
So you might say what is a precision medicine probably in this audience may not familiar. So precision medicine is like this, as we know in the in the medicine, there's no one drug can treat all patient well.
So, so you're so the precision medicine sometimes also called personalized medicine. So that means you may want to tailor your treatment based on characteristic of the patients.
So maybe let's say for the cancer patients, you may have the chemotherapy or you may have surgery, but for some people is better to use chemo than than other people based on the genetic information of the subject.
So that area actually is called a heterogeneous treatment fact. So that means, so the in the literature, the talk down mentioned, mostly focused on, I think the overall average treatment fact to the whole population.
But sometimes you might might be interested actually is the sub population.
So maybe you want to condition on covariates and then look at the causal effect in the sub group. So, so they call the heterogeneous treatment fact, but the principle down just mentioned about the Ruben causal model can still apply, but you have to do more work to make work.
So that's called a heterogeneous treatment fact. Actually, I think this is still right open area in medicine.
And then particularly when you have observational data, which have so one of the difficulty with observational data in causal inferences is our major confounders.
So the reason I think the card and then even got lower prices, they actually try to use the instrument variable to crack for the major confounders, which is they call the late.
So I think, later on I think we call the complier average cause of that right, right.
Yeah, but so maybe the late is the first proposed by ambulance and the angry.
Yes. Yeah, okay. And then later on I think we actually in our medical field we, we, we prefer the CSE, the complier average cause of that.
Okay, I think that the causal inference is getting more and more popular. And I think the current research in causal inference is how do you apply the clear principle like Ruben model to more complicated problem, like heterogeneous treatment
like big data, like observational data, like communication by death, or like right now is artificial intelligence.
Like, like, like, like in this section of the most like say how do you apply causal inference to, to the reinforcement learning to recommend, recommend, recommending systems recommendation systems, which is can be applied, because actually they're doing that down I
wasn't aware about that the in computer science they apply causal inference to almost all field of the computer science.
But I mean we look closely is I think the framework is not still not very clear actually one of the paper I'm working on with my student is how do you currently define the cause of framework for recommend recommend dating system system, like the how do you recommend
the causal inference principle, which is a doing that all the time, because they try to introduce causal inference in, in that, in that framework to make more stable or more.
They call the robust predictions. So, so as the third field I think is in the causal inference is that is a causal prediction. So that's a little bit different from the causal effect.
It's very popular those days in the science to say, how can you make prediction, which has some causal property, I think, and no one I think needs a lot of work to be done, I think that field. Yeah.
So, so, Professor Joe, Professor Joe we have limited time.
So I will more maybe once I don't comment a little bit is what do you think about the causal predictions, do you have any thought on that.
I, I should do, I can say, one or two minutes now.
I think that's probably easier for the audience and for me as well, instead of trying to remember everybody if I so if I make a comment. So I very much appreciate Andrew's comments and as, as he said we've known each other for decades.
We've written papers together.
And I think that that's your point about the, the, the being precision medicine, and the problem of censoring by, by, by death, think the, it was the censoring by death I think has been underappreciated for for years.
And I think the idea of thinking about sensing, sensing due due due to death as creating a particular principle straight up where they can always survivors, the subgroup of people who would survive.
And so you try to do it do a study, and you want to try to separate the outcome of death from the outcome of quality of life, I think with many serious diseases. That's, that's an important question.
These otherwise you sort of get the wrong answer.
Because the, so how if you want to look at the effect of an intervention on quality of life. What do you do with people who died.
Well, if you just throw them out, then you can get completely the wrong answer. If, especially if the, if, if one treatment kills people who are weak, and then the other treatment looks better because it lets them survive.
And it's just, you've got to worry about, about both outcomes, the death outcome, and the quality of life outcome. And I think that there's a lot of work to be done there.
I, I, I second Andrew's comment about that, that this perspective has, has a lot of, of things to offer current studies and in, in medicine where with those are critical attributes.
Also the comment about precision medicine. Certainly, I'm on the word that the idea of precision medicine is especially important when treating patients, you don't want to, when it when it treat everybody the same.
But one of the ideas of precision medicine is you want to obviously have a different treatment for different people. And the, and there the complication is principal straighter are only partially identified.
If you, if, if, but there, but here's where, where, where the role of covariates can be really important.
And, and I think there's a, probably the most, maybe the, maybe the most relevant paper is, is one that on this topic is one that I wrote with for me to me Ellie, and some of her students, her Italian students.
Because here the, the words is published, I don't, I don't remember right, right, right now. But the, but the idea is that you can, the covariates help to predict which principal straighter you're in.
And obviously the medicine, the principal straighter help define which treatments work for which sub subgroup of patients.
And I think that's a terribly important idea.
And maybe I'll stop there to make sure we have time, because I can always come back at the end of it.
Actually, Professor safe home is also very interested in this question. Professors, say, do you want to make a comment.
Yes, hi.
Hi, this is punctually here I'm in the computer science department in Tsinghua University. Actually, we are in the same university now.
Yes.
There is some problem with my camera so next time I hope that we can, we can meet face to face.
My question is that actually, in the recent years, you know that there is a very obvious trend that in the machine learning community.
There are many researchers, especially very, I mean, famous researchers in this field to start to talk about causality, right, we need to embrace causality into the machine learning.
Actually, from the 2016 we just start to, to think about so what's the limitation.
There are some problems that today's machine learning framework cannot solve in the end. Actually, we find that there are several problems like explainability and also the stability problem, I mean the generalization to out of distribution and also fairness issues.
We think that when we choose back, all of these problems are the root cause of these problems is, is just that in the learning framework we, we didn't even try to differentiate the causality and also the correlation.
So, because in the, in the big data we just absorb a lot of spurious correlations in our model so that directly lead to these problems.
So, I just want to listen to your thoughts and comments on this trend. And also, the second question is, when we try to do this when we try to embrace the causality into the learning.
Some researchers just use, for example, the structure causal model, and we, our group actually borrow a lot of ideas from the Rubens causal model the potential outcome framework.
So, actually, we are not that confident because we are just a halfway.
We stand in the, in the machine learning community we just a halfway with respect to the causal inference community so we just want to listen to your advice from your perspective so how can the potential Rubens causal model help in the machine learning tasks.
Thank you.
Sure. Yeah, I'll say just a couple of sentences now about this. I think one of the critical things and I've over the years I've done a variety of consulting and very often the main effort is at the front, meaning at the beginning, when trying to understand exactly what the, what the question is being asked and try to get
investigators away from talking about very theoretical things and get them to address what they can do. So, if you're talking about causal inference in machine learning, computer science, real world, what are the collections of interventions
that you can implement. I don't want to talk about the theory, because the theory will will will will follow from the question that you're trying to address. If, if your eventual answer can't help doing what you want to do.
It's worthless.
I think that that's one of the, the important ideas from this 20th century ideas of experimentation is you should try to experiment on things that you can actually do.
So, in the, in the computer science world, what can you actually do, what are the intro and interventions that you can implement.
Now,
I'm thinking about that implement in the context of a randomized experiment. What are the, what are the interventions that you can actually create and, and do, and then what are the outcomes that you want to achieve.
Now, I think the internet kind of experimentation is more far more complicated than the classical ones for one primary reason. Everybody's interfering with everybody else.
And it's, it's, it's very hard to get discrete non interfering units in, in, in, even in, even if you can do the randomization.
And there's, there's only one real study that I've been actively involved in, which is something that I didn't refer to here at all but it has to do with.
Actually, it's, I guess it's, it's partially confidential, so I probably shouldn't say too much about, but it has, it worries about the second order causal effects that you get on the internet, where, where the units of the quote experimentation are not very independent.
And so I guess my general advice to people who are thinking about in terms of internet experiments is to decide, first of all, what the interventions that are possible to do, and then how to deal with the interference.
And I think that's a wide open area and how you, how you deal with interfering units units who are communicating, because the, the network on on the internet is, is so incredibly vast.
And you, and you don't, and depending upon how what the links are between the different nodes in the network, you get, you have to get a different type of, of, of inference and a lot of work has to be done there.
I, I don't know whether that as you question at all, but those are my fast thoughts on it.
Thank you, thank you. And if the time, time allows, we may have more discussions, just the host. Yeah.
You just mentioned about the treatment and how it is important in the causal inference. We actually have Professor Joe Silan who has written, written the most one of the most popular textbooks in econometrics and talking about the policy treatment and policy impact evaluation.
So let us welcome John Silan to give some comments.
Actually, can you send me a title for this book, if, especially if it's in English.
Okay, I'm going to send you right now.
Okay, good.
Yeah, we're like, we're like, I also like to see you too.
I prefer Ruben and thank you very much for the great lecture about the history of the Ruben called model.
The following poll model is actually the basis for calling for the empirical economics.
Economics usually use random control trial and a natural experiment, natural experiment, trying to identify color effects in economics, especially in liberal economics, which so called the credible revolution in empirical economics by anguish.
Especially Ruben called model is, is especially suited to do policy evaluation in economics.
After implanting on government policy, governments usually want to know the effects of the policy.
So Ruben Paul model played a great role in those kinds of research questions. Ruben called model can perform the research question and make the evaluation.
There is another kind of problem in economics. That is, we saw the economic phenomena, and we want to find out the causes of the phenomena or the problem.
For example, in 2008, you know, it's happened a world economic crisis. So economies want to know what's the reason called this crisis.
So in this case, I have a question for Ruben is that how Ruben called model play a role in those kind of problems. This is just like another direction from
from policy evaluation is from manipulation to find out the color effects. And this probably is a try to reverse we will see effects, how to find out the reasons can you give
us some sort of problem.
Sure. Yeah, I think what you're pointing out is a really critical point that actually arises, especially in philosophy.
If you look at sort of historically at at some of the literature on causal inference, a lot of it is devoted to finding the cause of an effect.
So you got sick what caused that. And there is an important distinction between searching for the cause of an effect, which is I think, really a descriptive question, versus the effects of causes in other words.
So think about randomized experiments. What you're randomizing are the causes, and then what you can observe are what the effects of those causes, those interventions are.
As, as we look at data, what we see are a state, you know, the current state of affairs. And here we were constantly drawn to wondering, ah, what caused that effect.
And in fact, in my mind, one of the revolutions of the 20th century, this experimental revolution, and the cause of effect revolution is that's almost an unanswerable question what the cause of an effect is.
Let me give you a trivial example that they've used for years. So somebody died dies of lung cancer.
First person is ah, he died of lung cancer because he smoked two packs of cigarettes a day from the time he was 14 years old.
And the next person says, well, that's true. But the reason the reason he spoke, he spoke two packs of cigarettes a day was his parents were both heavy smokers.
And each of them smoked two packs of cigarettes a day. And there are always cigarettes around the house, even when he was a young teenager.
Other person says, ah, well, that's true. But the reason both of his parents smoked is that his, their, their grandparents smoked.
And they came from the old country and the old country, everybody was a smoker. So they were smoking all the time. They smoked three packs a day, four packs a day.
They, they're like eating cigarettes. In fact, one of the relatives came from Turkey, where everyone smoked unfiltered cigarettes all the time.
So the real cause was that, that his grandparents smoked cigarettes, and therefore his parents were doomed to smoke cigarettes. So the real cause was his grandparents.
And of course that can go on indefinitely.
I think one of the revolutions of the 20th century and the idea of experimentation was a change in focus from looking for the cause of something that exists to the effect of causes, the effect of interventions, where you can formulate the
question and the answer in terms of actual interventions that you can do in terms of an assignment mechanism.
Ideally a randomized assignment so that it's unconfounded or some other, but the, but these other, these other questions, the effects, the causes of effects of things that is descriptive.
They may be hypothetical, maybe able to think, and I've written about that in various places. For example, there was one discussion I wrote, I think with the title, which ifs have causal answers.
So what kind of question that would be is, if I, if I remove the sun from the solar system with the planets still go in their orbits.
There's no causal like, like, like questions, but the intervention of removing the sun, we have no idea what that even means.
So I think it's important when formulating questions in practice for things that you really want to understand is you have to formulate them in the, in the context of what interventions that you can do, because then you try to choose the
one that's most favorable to you.
Does that help in any way, I hope so.
So that means you have come to some ideas first, then you can manipulate is or interface data to find some correct and very often the manipulation that you can do, you will find that you want to do is you really can't do it.
And then I think the focus should turn to interventions that you can do, because if you're a practical person, why should you spend time thinking about something you can't do.
Right, it's like saying, ah, if I live to 2000 years old, when I'm 1000 years old, I will, I will start in the study of medicine.
Why spend time thinking about that you're not going to live to 1000 years old. So why so why have a debate with other people on what you're going to do when you're 1000 years old.
It's not worth the effort.
So you should, you should expand effort thinking about things that can be done.
Maybe maybe that's too applied for, for some philosophers, but it's, I guess, I've, I've come to the conclusion that people spend too much time worrying about things that they can't change.
Okay, thank you.
We actually have also a few questions from the audience that they want to see, Professor Rubins comments, then, then we'll first answer the audience questions and then we'll come back to discussion if we have more time.
Okay, that's fine.
Okay, so the first question is, what's your opinion about the future development of the potential outcome framework. What's your expectation.
Ah, well, my expectation is, it's the, it's the correct basic framework, but they're all sorts of aspects of the real world that it hasn't been modified to address.
Because it, so for example, this idea that Andrew talked about of precision medicine, I think is is an important one, because the, and the role of covariates, I think is underappreciated there.
If you do randomized experiments one attitude used to be, you should pretty much ignore covariates, you should do large simple, simple trials, for example, and I think that's wrong.
Because in order to do anything precise with being precise means at least to me is to be able to to condition on more and more descriptors of individuals.
And so the idea there is you can't just do large simple trials. And actually I'd be interested in hearing what would Andrew has to say about that because certainly there's been an emphasis in in in recent years on doing this large simple
where you don't worry about enforcing balance on many covariates, you just the simple refers to just complete randomization, because the basic theory of Fisher and Neyman is averaging over everything, but when you actually going down to making decisions, decisions
have to be very conditional.
What do you have to say about that.
Well, I think I will pull wait because because the audience probably want to hear from you.
Okay.
And now if the time allow, I will see if you will.
Okay, fine.
So, so this basic answer is that even in design, we now have the computational tools to make designs far more precise than then can be achieved just by complete randomization.
And an important aspect of that is this idea of rerandomization randomization. So you look at at zillions of possible randomized allocations.
And then you have measures of marriage, like the difference between covariate distributions in the treatment group and the control group, and you throw out all bad allocations by using the computer to do that for you.
And the resulting distributions that you're using are not the simple t distributions and f distributions and chi squared distributions.
Because they this throwing out bad randomizations means you're truncating those distributions. And so the distributions are complex mixtures of truncated distributions, even asymptotically.
And so using this general framework of potential outcomes and Ruben causal model.
Then you can then you can try to use the power of modern computing to do the really tedious job of throwing out bad allocations, but you first have to define bad allocations and bad allocations will be defined in terms of covariate
distributions and multivariate covariate distributions. They're really tedious to compute, but that's what computers are great at doing in these really tedious computations. They're terrible at thinking.
I agree. I think the realization could be a potential future research area. But on the other hand, I think that the realization can solve the bias, some bias issues.
On the other hand, they, they create some difficulty in analysis, because of the use throughout some data, and then you have to just for I know you have done some work in that.
But I think, but when the data structure complicated. So they are going to complicate analysis further.
Like if you have, let's say non digital data, and then you have a missing data in your outcomes, and then you have delay in correct collect outcomes.
So I'm kind of worried about that part to say, maybe the outcome is survival time. So you can't just wait until they die. That's too long.
And, but I think by the idea is I like the idea of a randomization but just need to think about the design issue and analysis the issue how to balance those two.
Yes. And when I think about why it's this re randomization has become a more recent idea is again I said this briefly in the talk. I mean, Fisher would would throw out a obviously bad randomized allocation.
He would never live with one where all the good plants got got a treat one treatment and all the bad plants got got the other other treatment. In fact, that's why he invented the analysis of covariance in 1935 I think he invented it.
And he was had an experiment experiment where, according to the complete randomization, the tea bushes in India, and all the, the, all the bushes with low yield in the previous year, we're assigned one treatment and the ones with good yield previous year, got assigned the other
treatment. And so we have to adjust for that, using the analysis of covariance.
And I wrote down details actually that's a paper where he, where he, he made some mistakes, not mathematical mistakes but mental mistakes, fundamental thinking about.
And I think the reason why he made those mistakes is he didn't, he wasn't using Naaman's notation for these intermediate potential outcomes, which he was saying should be adjusted for.
Okay, thank you. This is really, really helpful, especially in regards to the hot topic of a real normalization recently. It's been discussed. So it's very helpful to hear your comments.
So the next question is from Deng Wu from Waseda University. He was asking, now the COVID-19 pandemic is getting down worldwide. However, it is reported that the reasons for getting down are not clear enough, even for epidemiological
experts. So how causal inference methods can do some more intelligence analysis in epidemiology that, sorry.
Okay, well, I guess there are now a variety of vaccines being proposed and the rules for who gets them vary not only by country, but within a country like the United States, the rules vary by state.
So the 50 states, each of them has somewhat different versions of that. And so I think this gives an opportunity because of the variety of interventions being proposed to actually learn about and compare the different rules.
So some of the rules have age limitations.
You know, they have other rules have you, you have to have boosters with it with the same vaccine. There's recent stuff in the United States at least that said that that seems to suggest that a booster from any of the vaccines works, no matter what vaccine you got earlier.
So there I think there are a variety of, I guess what would be called natural experiments taking place all over the world with respect to COVID.
And so I'm, I'm surely I'm not an expert on following and tracking what what what these interventions are. There's certainly is a variety of them which leads to, I think what would be called natural experiments where there's some
the assignment mechanism is probably not that related to the assignment mechanism is not really related to potential outcomes, which is the critical assumption. I think realizing that that's the assumption.
That's the critical assumption to do inference in natural experiments is is really a critical idea to keep in mind.
Okay, so the next next question from the audience is from her your way.
His question is, is the output set of causality included in the correlation output set of vice versa. What are the relationships.
Okay, I think I understood the words, but I may not have understood the big idea. So read again please is the output set of causality included in the correlation output set or vice versa. What are the relationships.
So I guess by output set that terminology is, is not familiar to me. But, but the I assume what it means is the output set meaning the, the scientific conclusions that are reached at the end of the causal analysis.
Is, is that how I should interpret that.
I just wonder if it is outcome set.
So is the outcome set me that the outcome set. I don't see the outcome set output. I think they're probably this person from computer science.
So the computer think about input output. So this is the output. So that means the results probably right now. So he took he talked about if you if I interpret correctly is if you do the causal inference you get the results, and then you do the correlation.
Not as you get results how those results.
What's the relationship between those two results is all right.
Maybe I suggest the organizer move on to the next question. I think I see us.
So, so if you want to rephrase that in the comments, so please do that be fine. Yeah.
Yeah, yeah, yeah, that'll be helpful. So the next question is that, do you think causal learning has the potential to be applied in more complex control problems like robotic arm control.
If that how to apply causal learning to the high dimension space.
Okay, so first with respect to the high, high dimensional space I, I think that when you're dealing with it with with a high dimensional space. The, the first thing to do is to understand Euclidean geometry.
And, and, and because there are many sort of examples where people think that if you have a high dimensional covariates that that that creates complications, or in some sense it does.
So if you look at the Euclidean space, and you, and you understand eigenvectors and eigenvalues. If you, if you have a high, a high dimensional space of covariates, but only a few treatments.
And then what happens automatically is after looking at the first principal components, there are no differences in the, in the, in the lower principal components.
So you don't have to worry about balancing things that are already balanced.
It doesn't, it doesn't create the same kind of problem as if you have a high dimensional outcome space, which leads to like multiple comparisons problems when, when, when trying to look at p values or significance levels.
I, I'm not, I'm a little bit that I, I know about the work being being done in in sort of more than machine learning computer science is it starts with something that's not a complication.
So we're worrying about eigenspaces that where there's no difference.
If, if you have high, you know, if you have high dimensional covariates, and you sort of ordered them, you know, ranked them in importance, and then you use look at the icon structure, you find out in the unimportant spaces.
There are no differences.
And that's because you don't have enough units. But, but, but still you don't have to, I mean, mechanically, it's, it's not an issue to be worried about.
I hope that address the question to some extent at least.
Thank you. I, I think it will be helpful for him to. Okay, so then, the next question is that if potential outcome framework has same function in causal explainable AI with SCM.
SCM structural causal model.
Okay.
And so we the sense with the question again please.
If potential outcome framework have the same function in causal explainable AI with structural causal model.
Well, I think he's comparing with POM with SCM.
The structural causal model. I mean, I, I, I don't really understand what the with those words mean.
If you, if you have the question that you're addressing formulated in terms of potential outcomes.
Then the next step is to design a study to try to address that, that, that, that causal question. And the study will, will obviously depend upon whether it's randomized on the assignment mechanism.
Are you, are you just observing observational data. Are you actually doing an experiment and depending upon which of those you're doing the method of analysis will follow from the, from the design that, that you used.
I basically like Bayesian models, I think Bayesian models are where everything's a random variable are the right way to derive statistics.
The fundamental way to do a basic analysis is uses use the basic idea the essential idea of a fisher doing basing it on the randomization distribution, or hypothetical distribution if you if you didn't do a randomized experiment.
That's the first step, and then more complex answers are given by the by the posterior distribution.
I don't understand all these other words that are sometime added, like structural or graphical. I mean those.
Those don't mean anything to me. So I, I just, I guess I view a lot of that as clutter.
I have often self serving clutter, which is put in in papers to just for self serving reasons.
The answers are very important when you're expressing either the question or the theory. Okay, let's go to the next question is bad allocation cannot truly be measured by covariates. Is there a way to measure randomness beyond using big data covariates.
So the first part of the question was what we did again. I'm not sure I understood that.
Bad allocation, bad allocation cannot truly be measured by covariates.
Okay, let me respond with a question why not. How else, how else would you measure it.
And he was asking is there a way to measure randomness beyond using big data covariates.
So, because randomness refers to a property of what you did.
You did random allocation, and everything's an approximation. I mean, you know, how do you draw random numbers.
You know, in the old days, you took out ran table of random numbers and grabbed a haphazard graduate student asked him to close his eyes, turn to a page in the round table of random numbers and put his finger down.
If you looked at which pages were, were drawn a random page was always in the middle.
It was never the first page or the last page. Similarly, the, the number that was drawn on a particular page was never at the very top, or the very bottom, or in the very end some corner.
So, random was always, you mean, sort of uniform in some sense, but you have to describe the process by which it's done, and then it's a mathematical assumption after that.
I'm not sure I addressed at all the, the end of that question, could you read the, the end again.
With correlation.
The end is, is there a way to measure randomness beyond using big data covariates.
And randomness again is a property of a procedure which is all hypothetical. It's a mathematical structure, and the way it should be certain implications of a mathematical process which are mathematical, which are from covariates I have.
If I have no covariates at all.
How do I even assess for something looks random.
Okay, so the next question is how to understand the success of COVID-19 containment policies in relationship to public health leadership roles through causality statistics.
Yeah, well, I guess this is like the an earlier question.
Yeah, because it's, it's, you have, like, different countries have have different policies being implemented at, at different times, presumably, the many of the policies are not being decided based on the actual potential outcomes.
I think that probably the policies and the implementation heaven have a lot of natural experimentation in them right right now, especially as implemented with various different states or regions of countries having modifications of policies.
Okay, so this is a very similar question from the first question we responded earlier.
So, so, so let's go to the next question. Okay, this will be the last question. What's the role of assignment mechanism in potential outcome, and how to design a good RCT.
Okay, so the potential outcomes are how you define the science and the covariates. So it's, it's completely the potential outcomes framework is completely separate from the assignment mechanism, whether it's randomized or natural experiment or completely
observational. So that's that that's what I think of the the real contributions that I made in the 1970s was over and over again emphasizing the distinction between the framework for defining causal effects, which is potential
outcomes, and thinking about them as potential outcomes of the bit of interventions, often in trying to make the interventions ones that you could possibly do.
And then the assignment mechanism is what you actually did with how you implemented the interventions on different different units. I think one of the things that I think, and you emphasize that that's
important are these complex designs we have nesting and clustered randomization. And we wrote several papers about those kinds of designs. So, but again, the critical first step is to think what you want to learn about, then formulate
that question in terms of potential outcomes. And then once it formulated in terms of potential outcomes, then think very hard about how you do the intervention or hypothetical intervention that would yield some observed potential outcomes and some missing potential outcomes.
And my own preference with at that point, to be build Bayesian models where you get statistics functions of observed data. And then at that point, do inference, starting with to the extent possible for sharing an inference based on randomization distributions
or hypothetical randomization distributions, and then get more refinement from from the Bayesian models and more precise answers to more complex questions.
Okay. Thank you so much for attending this event.
If we don't have any more questions. I thought Professor two from earlier had a question he wanted to ask.
Professor say do you want to propose your question.
Do I still have a time.
Okay, okay, I just want to take two minutes to echo to dance question on how, how, how can we do intervention in the machine learning and how can I do with interference in our problem.
Yeah, actually, this is my understanding. So the goal of the machine learning is actually just to do prediction. Right. And in the class of my understanding is to.
Yeah. And in the classical setting actually we made some quite ideal hypothesis. The hypothesis is the ID right the sample that the training and testing they are just from, I don't from the identical distribution.
So, and another setting is just that the domain adaptive adaptation that okay we train from one distribution and we can test from on the other distribution but the testing distribution should be known.
So, either the ID, we call the ID learning or the transfer learning or domain adaptation actually, we just assume that the testing distribution is known.
Okay, so this is a classical setting. So under this setting, actually, what's the machine learning models trying to do is just to do the distribution feeding or data feeding, because we know the testing distribution, right.
So, now this is actually quite ideal. I mean, assumption because in the real in the real applications we cannot assume that okay the testing distribution is known because there are a lot of interventions.
There are a lot of distribution shifted in the real applications right. So, so now actually, I think a very fundamental problem in the machine learning community is trying to to relax assumption that the testing distribution is known.
Okay, so if we don't have the testing distribution then how can we optimize the model, how can we learn the parameters. So, then we need a new paradigm for the learning.
I think, from our understanding, if we don't know the testing distribution, then we need to pursue the true model, the true model that generate the data, right, if we can, we can gather two model, then of course, no matter how the covariate shift and how the distribution
change we can make a reasonable prediction, right.
Correct.
And the logic is, if we have the true model, actually for any samples, for any sample, we can perform uniformly good, right, because we can model the true signal and only the noise cannot be modeled if we have the true model.
Correct.
So, we, we just want to do a reverse engineering that means, okay, if you change the data distribution.
Okay, you're just a change that there's a kind of intervention, for example, you can raise, reweight all of the training samples to another distribution, if a model can can have, I mean, the uniformly good performance on all of the
distributions, for example, you have a multiple sample evading strategy, right, and under each strategy, actually you can get one distribution, but across different distributions, if we can have a good, I mean, a prediction performance, that means, okay, the model actually captured the true model.
So, I think that in the machine learning, maybe one way of intervention is just the sample evading.
So, we're going to train the distributions, and then under these distributions, whether we can find out the invariant, I mean, model or invariant structure.
And this invariance, I think it's somewhat related to a causal effect, a causal relationship or causal structure.
Our idea, but I'm not just very sure whether this is correct from the causal inference perspective, especially from the perspective of
Rubin causal model.
Right, well, I think what you, what you mean by the
true causal model is the true distribution of outcomes given covariates.
When the covariates are simple enough, let's say just define a few straighter, then we waiting works.
But if there is, if the covariates are complex or complicated, I don't want to use complex in the, in the mathematical sense.
But if they're complicated, and you have many outcome variables, I think you have to rely on modeling the y, the outcome, given the covariates.
And the modeling can, and then what you do, because if you have a good model for outcomes given covariates for y given x, then you can predict y at lots of different x values.
And I think that that's the, the bigger picture, whereas waiting is, you have to, you have to have the continuous version of waiting to have the general idea.
Yeah, correctly formulated.
So I, I think we're saying the same thing.
Basically, although the words may be slightly different.
Yeah, yeah.
But, but there has been work done starting in the probably mid 1930s in in survey work.
I think the probably the work that's closest to the work that that you're talking about in computer science is probably the work on missing data.
Because here, you'd worry about if the, if the guys who are, who are missing have very different covariate values, you know that you're relying on extrapolation, relying more and more in some hypothetical model.
And the waiting, the idea of just just waiting units only works when the support for the data are the same for the people who are observed, and the people who are missing, or otherwise the, the imputation of the missing data won't be very good.
In fact, there, there was a three volume collection of books that were published around I think 1984 or three by the, under the National Academy of Sciences, because there was a committee on, on, on handling missing data.
And the, there are various authors to the, to the three volumes, I think I'm on, I'm one of the co authors on one of the volumes, but a guy named William Maddow and adow with author on all.
I mean, it's actually because it's true during a problem. You're discussing right now, except it was done on text of, of large US surveys mostly.
The Medical Examination Survey, which was 20 or 30,000, and they, and because this was a design survey, it had certain structures that like it had different sampling units were states and has a hierarchical level to it.
So that, that, that literature, I think is, is quite relevant, although not the techniques are being used because, again, the techniques are far more simple minded than the ones that can be used today.
But I think it's worth looking at server, it's historical interest, and there probably are a variety of interesting ideas.
Yeah, thank you. Thank you very much. I think this is very constructive comments and inspiring, but just now that the signal is not very good. So I will write an email to you and could you send the survey title to me and I will.
Fine, I will do so.
Thank you. Thank you. Thank you very much.
Thank you so much. So it's actually getting very late, almost like 1118 in China.
Yeah, I know you need time to grab lunch as well. Yes. Thank you so much for, for the wonderful, wonderful talk that we're very, very honored and very lucky to have you here tonight.
And thank you all for the speakers because I was reading the comments from the Billy Billy because this room is full. So we have a lot of also broadcasting guests from the website of Billy Billy and they were commenting on how useful and how helpful those
responses are. So I guess they contribute a lot to the knowledge and we are very, very happy and learn a lot tonight.
I really appreciate the formal discussions and the informal questions. They are very helpful generally for trying to clarify these important ideas. Thanks again for the invitation.
So last section, I'm going to give a brief introduction about Professor Don's book.
One second my.
Okay.
Here's what happens every single time I try to share something on my screen.
I don't have to say goodbye because I have a meeting tomorrow.
Okay.
Okay, you have to get up or hope say back back in China.
I hope I hope so too.
Good seeing you again. Bye bye.
Bye bye.
Okay, so can everybody see my screen.
I can't see your screen I can see your face.
You can see me in my screen.
I can't I can see your face but I can't see your screen.
You can't see my screen.
Okay, let's try again.
That's what happens every single time I try to share something.
I see it.
It comes.
Yes, there's a very long time of that time lack.
Yes.
Okay.
Well, the speed of light is only so fast.
Okay, yes, depending on how many circuits, it has to go through even the speed of light can take time.
Okay, so I'm going to finish this really fast.
We are going to read Professor Donna Rubins book on causal inference for statistics, social and biomedical sciences, the end introduction.
This will be a, this is a very classical book to learn about causal inference.
So the reading club will start next Sunday every morning.
And then we will have students and also scholars sharing the contents of each chapter every single week.
And then we are also having the coding session to help to replicate the studies in the book.
So, on the right is the QR code.
So if you're interested, please register through the QR code on the right.
And again, thank you very much for ruby.
Okay.
I'm going to use Chinese for a moment.
This is our background.
It's really ruby.
This book has a reading club.
We call it English Science.
The reading club is translated by English.
And then we will, if you're interested, you can scan the QR code on the right.
And then come to our reading club to participate in our new series of English teaching.
Okay, so that's all for tonight.
Thank you so much. Stay safe and have a good day.
Thank you very much. And again, thanks for the invitation.
Thank you so much.
Bye bye.
Bye bye.
Have a good night.
