1
00:00:00,000 --> 00:00:07,280
Alright, so we'll move on to our next speaker. Yanxun will be our next speaker

2
00:00:07,280 --> 00:00:12,720
and she is a associate professor in the Department of Applied Mathematics and

3
00:00:12,720 --> 00:00:19,080
Statistics at Johns Hopkins University. Yanxun has a lot of research interest in

4
00:00:19,080 --> 00:00:23,360
the reinforcement learning, high-dimensional data analysis, and

5
00:00:23,360 --> 00:00:29,360
non-parametric statistics. Okay, so today she is going to talk about a Bayesian

6
00:00:29,360 --> 00:00:36,040
reinforcement learning framework for DPR. So you can see the screen, right? Yeah,

7
00:00:36,040 --> 00:00:41,800
yeah, great. So thank you Lu for organizing this session and introducing me. So

8
00:00:41,800 --> 00:00:44,880
today I will talk about a Bayesian reinforcement learning method for

9
00:00:44,880 --> 00:00:51,440
optimizing treatments for HIV patients. So first I'd like to thank all my

10
00:00:51,440 --> 00:00:56,320
collaborators in this project. So my peer-de-student Wei and also my

11
00:00:56,320 --> 00:01:00,640
collaborators, Leah and Raha from Johns Hopkins University and Yanni from

12
00:01:00,640 --> 00:01:05,800
Texas A&M and also two clinicians, Amanda and Jane from Georgetown University

13
00:01:05,800 --> 00:01:14,160
and Washington University in St. Louis. Okay, so making the optimal

14
00:01:14,160 --> 00:01:19,320
sequential decisions is very important in many diseases. So today I will focus on

15
00:01:19,320 --> 00:01:25,960
HIV and we know the emergence of anti-retroviral drugs, ARP, has transformed

16
00:01:25,960 --> 00:01:30,840
the HIV infection from a fetal disease to a chronic disease. So it's

17
00:01:30,840 --> 00:01:37,440
significantly reduced HIV-related mortality and the common ARP drugs fall

18
00:01:37,440 --> 00:01:42,760
into different drug classes with different mechanisms. For example, the

19
00:01:42,760 --> 00:01:48,040
first one, the NRPI, it's called a nucleotide or probably can also be a mouse.

20
00:01:48,040 --> 00:01:52,560
Okay, so the first drug class called NRPI is called nucleotide reverse

21
00:01:52,560 --> 00:01:57,800
transcriptase inhibitor. It inhibits the reverse transcription step in viral

22
00:01:57,800 --> 00:02:03,560
replication and other common drug classes include like NRTI, PI, ESTA, EI and

23
00:02:03,560 --> 00:02:09,720
the booster. And the model ART drugs usually combines three or more ART drugs

24
00:02:09,720 --> 00:02:15,120
from different drug classes. Since such cocktail approach is most efficient in

25
00:02:15,120 --> 00:02:21,640
suppressing viral loads. For example, the clinician can prescribe two NRTIs and one

26
00:02:21,920 --> 00:02:26,480
drug from instinct and all they can prescribe like two drugs from an ARTI

27
00:02:26,480 --> 00:02:35,000
drug class and one from an ARPI or one from the PI. So people living with

28
00:02:35,000 --> 00:02:39,560
HIV now are recommended to follow up with their physicians semi-annually by the

29
00:02:39,560 --> 00:02:45,440
current HIV treatment guideline. And at each visit, there are social demographics,

30
00:02:45,440 --> 00:02:50,320
lab test results, such as CD4 counts and viral loads, and also clinical

31
00:02:50,320 --> 00:02:55,240
environments such as BMI and the glucose are collected. And then physician

32
00:02:55,240 --> 00:03:00,120
assigns their ART regimen based on their clinical observations. So each ART

33
00:03:00,120 --> 00:03:05,240
regimen is a combination of different drugs from different classes. And this

34
00:03:05,240 --> 00:03:11,560
process repeats every half a year by average. So nowadays, the US Department

35
00:03:11,560 --> 00:03:17,080
of Health and Human Services provides general guidelines on assigning ART

36
00:03:17,080 --> 00:03:21,880
treatments. However, these guidelines are usually applied to treatment

37
00:03:21,880 --> 00:03:27,440
naive subjects, meaning subjects who are recently diagnosed with HIV and who

38
00:03:27,440 --> 00:03:33,440
never took ART treatments before. And for happily pretreated people with HIV,

39
00:03:33,440 --> 00:03:38,800
for example, if this person has been diagnosed with HIV for 20 years and has

40
00:03:38,800 --> 00:03:44,560
been taking ART drugs for a long time, and there's no consensus. And also the

41
00:03:44,560 --> 00:03:49,240
current guideline didn't take into account the potential adverse effects

42
00:03:49,280 --> 00:03:55,000
caused by the long term use of ART drugs, because HIV now is more like a

43
00:03:55,040 --> 00:04:01,120
chronic disease. And those patients needs to take ART drugs every day. And

44
00:04:01,160 --> 00:04:05,320
indefinitely. So you need to take every day to take the drugs every day to

45
00:04:05,320 --> 00:04:10,400
suppress the viral loads. And they may cause some long term adverse effects.

46
00:04:10,560 --> 00:04:14,800
But for example, like the kidney disease, weight gain or depression, and the

47
00:04:14,800 --> 00:04:18,840
current guideline didn't take into account all those side effects. So our

48
00:04:18,840 --> 00:04:24,400
goal is to determine the personalized ART regimen to optimize the long term

49
00:04:24,400 --> 00:04:29,920
health. That means not only the drugs can supply the viral loads, but can also

50
00:04:29,920 --> 00:04:37,000
control the side effects. So large scale HIV studies such as maximize

51
00:04:37,000 --> 00:04:42,040
cohort studies. And it provides us opportunities to achieve this goal. And

52
00:04:42,040 --> 00:04:47,440
they collect data from participants semi annual visits. And here is the ART

53
00:04:47,600 --> 00:04:52,480
treatment history for one randomly selected subject with seven visits. Here's

54
00:04:52,480 --> 00:04:57,720
the X axis shows the calendar dates of their visits. And the Y axis shows the

55
00:04:57,720 --> 00:05:03,120
ART drug combinations they have this person has taken. And different colors

56
00:05:03,120 --> 00:05:07,280
here represent different drug classes and the drug name are marked. So I use a

57
00:05:07,280 --> 00:05:14,000
three letter to represent each drug. And also at each visit, the house related

58
00:05:14,000 --> 00:05:18,520
environments are recorded. For example, this figure shows the subjects,

59
00:05:18,520 --> 00:05:23,080
longitudinal depression scores, viral load, EGFR for kidney function and the

60
00:05:23,080 --> 00:05:29,720
BMI. So there are many challenges to optimize the sequential ART regimens

61
00:05:29,720 --> 00:05:34,760
in a data driven manner. First, we need to estimate the drug effects. Like before

62
00:05:34,760 --> 00:05:38,960
we assign them, we need to understand their drug effects from a high dimensional

63
00:05:38,960 --> 00:05:43,760
and unbalanced space. So when I say high dimensional, it means with more than 30

64
00:05:43,760 --> 00:05:48,800
ART drugs, they're all FDA approved on the market. There are a large number of

65
00:05:48,800 --> 00:05:53,440
possible drug combinations. So that could be millions of drug combinations you

66
00:05:53,440 --> 00:05:57,040
can choose. So even like the feasible drug combinations about like thousands,

67
00:05:57,040 --> 00:06:00,760
there's still there are a large number of possible drug combinations. And also

68
00:06:00,760 --> 00:06:05,400
when I say the unbalanced, that means some drug combinations are firmly used, but

69
00:06:05,400 --> 00:06:09,120
others are wrong. So for example, we can see for this drug combination is two

70
00:06:09,120 --> 00:06:14,560
ART drugs and one PR drug. It was the using our database for almost 1000

71
00:06:14,560 --> 00:06:19,960
times, but another similar drug combination. So the same two ART drug, but

72
00:06:19,960 --> 00:06:26,440
a different the PR drug, it was only used for 12 times. And the second challenge

73
00:06:26,440 --> 00:06:31,880
is how to generate a realistic ART regimen from a large discreet space in the

74
00:06:31,880 --> 00:06:36,240
optimization procedure. So after we understand the treatment effects of

75
00:06:36,240 --> 00:06:41,360
these drug combinations, we need to assign the drug regimen to patients. That

76
00:06:41,360 --> 00:06:46,320
means we need to generate a realistic ART regimen. So then the problem is how to

77
00:06:46,320 --> 00:06:51,640
represent each ART regimen. A naive method would be say, okay, I can use a battery

78
00:06:51,640 --> 00:06:58,040
vector. So say I have 30 ART, I have 30 ART drugs on the market, and then I can use

79
00:06:58,040 --> 00:07:02,680
the battery vector is indimensional battery vector to represent. So if this drug

80
00:07:02,680 --> 00:07:08,680
combination contains drug one, then it's one, otherwise it's zero. However, it will

81
00:07:08,680 --> 00:07:13,880
cause two to the N possible drug combinations. And many of them will not

82
00:07:13,880 --> 00:07:17,760
be feasible. So like I said, usually, you know, people combine the drugs from

83
00:07:17,760 --> 00:07:21,440
different drug combinations, but usually we assign like, for example, two or three

84
00:07:21,640 --> 00:07:27,160
drugs from ARTI, but like really we assign one drug from PI. So not all these

85
00:07:27,160 --> 00:07:32,720
possible two to the N combinations are possible. And then we need the efficient

86
00:07:32,720 --> 00:07:38,440
way to represent the drug combination. And lastly, we aim to optimize sequential

87
00:07:38,440 --> 00:07:42,680
treatments from observational data. So we have all those data collected from

88
00:07:42,680 --> 00:07:48,360
observational study, but we want to assign them to the patients in the future. So

89
00:07:48,360 --> 00:07:53,560
the fundamental challenge of optimizing treatments from observational study is

90
00:07:53,560 --> 00:07:57,880
this distribution shift issue. So that means the training data may be collected

91
00:07:57,880 --> 00:08:01,560
on the different policies from the one we try to evaluate. So we need to address

92
00:08:01,560 --> 00:08:07,320
the distribution shift issue. So to address these challenges, we develop a

93
00:08:07,320 --> 00:08:12,280
two-step Bayesian reinforcement learning framework. And here is an overview. In the

94
00:08:12,280 --> 00:08:16,440
first step, we propose a Bayesian dynamics model for individuals'

95
00:08:16,520 --> 00:08:21,000
longitudinal observations using a microverte Gaussian process. And these estimate

96
00:08:21,000 --> 00:08:25,560
dynamics describe how individuals' health-related variables evolve over time,

97
00:08:25,560 --> 00:08:29,560
condition on their historical states and the treatment histories, with uncertainty

98
00:08:29,560 --> 00:08:35,720
quantification. And in the second step, we create a pessimistic environment with

99
00:08:35,720 --> 00:08:41,720
uncertainty penalization to mitigate the distribution shift issue and also use

100
00:08:41,720 --> 00:08:45,560
an offline reinforcement learning method to optimize the sequential ART regimen.

101
00:08:46,200 --> 00:08:52,520
So it's a two-step approach. Okay, so now let's go to the model details.

102
00:08:53,240 --> 00:09:00,680
So first, I introduce the problem formulation. Assume for each individual eye,

103
00:09:00,680 --> 00:09:07,960
we use XI0 to denote the baseline covariates, such as a race. And the TRI records the

104
00:09:07,960 --> 00:09:15,080
visit times. So assume each one has GI visits, and we have PI1 to TIGI to denote the time of each

105
00:09:15,080 --> 00:09:21,240
visit. And also assume we have M time varying health state variables. So here we call state

106
00:09:21,240 --> 00:09:26,840
variables because they change over time. For example, like H, BMI, EGFR, those clinical

107
00:09:26,840 --> 00:09:33,880
variables or demographics, they are collected in each visit. And also the I to represent the ART

108
00:09:33,880 --> 00:09:39,320
drug combination used by individual eye during the time period, TIGI minus 1 to TIGI.

109
00:09:41,000 --> 00:09:48,360
And then the data can be summarized as D. So for each subject, so we have a total of I subjects,

110
00:09:48,360 --> 00:09:53,480
and we have baseline covariates, the time of each visit, and the time varying state variables,

111
00:09:53,480 --> 00:10:02,200
and also the drug information. And then we use the YIG bar. So the bar is a common fun we really

112
00:10:02,280 --> 00:10:08,840
use to represent the history. So the YIG bar means all the state variables before the current

113
00:10:08,840 --> 00:10:16,040
visit J. And the ZIG bar means all the treatments this person has been taking on pure the time J.

114
00:10:16,680 --> 00:10:24,680
And also we use the dynamic, we use the after dynamic model. So that means condition on the YIG

115
00:10:24,680 --> 00:10:33,720
bar and the ZIG plus one, we update the state variables from YIG to YG plus one. So remember

116
00:10:33,720 --> 00:10:40,680
the first step of our method to learn the dynamic model of how to transform from YG to YG plus one.

117
00:10:40,680 --> 00:10:46,440
Of course, the YG plus one condition on the whole history of the YG and the treatments the IJ.

118
00:10:47,800 --> 00:10:53,080
Okay, so our goal is to optimize the ART assignments to maximize the individual's long-term

119
00:10:53,080 --> 00:10:59,480
house outcome. So because we want to maximize the house outcome, essentially we can transform the

120
00:10:59,480 --> 00:11:05,400
problem to an optimization problem. So this optimization problem is like we first define

121
00:11:06,200 --> 00:11:11,960
for each individual I, we suppose she already has GI visits. So if this person is a new patient,

122
00:11:11,960 --> 00:11:18,120
so the GI will be equal zero. So the GI can be zero, or if this person already has GI visits,

123
00:11:18,120 --> 00:11:24,280
and then we want to predict the next few visits, for example. And then we let the Y new and the

124
00:11:24,280 --> 00:11:31,480
GI new to denote her future longitudinal states and the treatments, because our reward may depend on

125
00:11:31,480 --> 00:11:38,520
her future states. For example, we want her less than the next two years, the depression is minimal.

126
00:11:40,040 --> 00:11:45,800
And assume for any future visited J, the ART regimen is assigned through a policy function

127
00:11:45,800 --> 00:11:52,920
PI. So that means if we can learn, if we can prioritize this function PI, and we can optimize

128
00:11:52,920 --> 00:12:00,760
data and the equivalently we can optimize the treatment. And let's say we assign a stochastic

129
00:12:00,760 --> 00:12:07,640
reward function RI that depends on the house states. So we can depend, we can define the reward as

130
00:12:07,720 --> 00:12:15,160
for example, this person, their, their very loaded is low, and the pressure set is low, and the BMI is

131
00:12:15,160 --> 00:12:21,000
in the normal range, and the kidney function is a normal range. Okay, so for example, if our goal

132
00:12:21,000 --> 00:12:27,080
is to select the sequential ART regimen that leads to lowest accumulated very low in the next two

133
00:12:27,080 --> 00:12:33,560
years, and it is coming an active sum of the very loads. Okay, so they notice the expected reward

134
00:12:33,560 --> 00:12:38,920
for any individual I to be the following. Because we, in the first step, we learn a Bayesian model.

135
00:12:38,920 --> 00:12:45,560
So essentially, you can generate their future states. And also, so we can generate the future

136
00:12:45,560 --> 00:12:51,400
states, why are you from the learned dynamic model, and we generate the ZI new from their

137
00:12:52,280 --> 00:12:56,360
the PI, the function. So we learned the PI from their parameter as the function PI.

138
00:12:57,000 --> 00:13:02,360
And also, because we learned the dynamic model, and then we can integrate out certain things,

139
00:13:02,440 --> 00:13:07,480
optimization procedure. So that's kind of the benefits of using the Bayesian framework in the

140
00:13:07,480 --> 00:13:12,840
first step. Because in the decision making step, we can integrate out uncertainty, we can adjust

141
00:13:12,840 --> 00:13:19,560
for this uncertainty quantification from their uncertainty of their dynamic, their dynamic model.

142
00:13:21,000 --> 00:13:27,720
And our goal is to find as optimized optimal policy function PI that parameterized by data

143
00:13:27,720 --> 00:13:33,080
i star. So we want to find as an optimal parameter that can maximize the r i theta.

144
00:13:34,200 --> 00:13:40,440
So that's a problem. Okay, so now we have already defined our reward function r i. And we want to

145
00:13:40,440 --> 00:13:45,960
find as a parameter theta that can maximize the r i. And essentially, it's a classical

146
00:13:45,960 --> 00:13:51,400
reinforcement learning problem. And we can use the policy gradient method to solve the problem.

147
00:13:51,400 --> 00:13:57,320
So essentially, if you can calculate the gradient of r i, and then you can use the policy gradient

148
00:13:57,320 --> 00:14:03,160
method, essentially, you update the theta by this formula. So it's also classical results,

149
00:14:03,160 --> 00:14:10,280
it's that you update the theta by the previous theta, and then plus some step size s i, q,

150
00:14:10,280 --> 00:14:16,760
and then times their gradient. So the essential problem is how to calculate the gradient of our

151
00:14:16,760 --> 00:14:22,200
reward function. So we can see our reward function is relatively complex, right? You take the expected

152
00:14:22,200 --> 00:14:29,400
value of the reward function r i, and r is a function of y mu, and y mu you need to generate

153
00:14:29,400 --> 00:14:35,960
from the predict distribution of your dynamical model. And besides all of that, we also need to

154
00:14:35,960 --> 00:14:41,080
integrate out the uncertainty from the dynamical model by the P5 condition on D, that's a posterior

155
00:14:41,080 --> 00:14:49,320
distribution of the phi. So it's not easy to calculate the gradient of this r i theta. Okay,

156
00:14:49,400 --> 00:14:55,000
so luckily for the policy gradient method, there's a way to calculate that. So if you're

157
00:14:55,000 --> 00:14:59,400
interested in details, you can find the details in the paper, but we can represent the derivative

158
00:14:59,400 --> 00:15:06,760
of r i theta as follows. And this formula looks very complex, but we can't decompose this into

159
00:15:06,760 --> 00:15:15,560
three separate steps. The first step is this step. So it's about the log of our policy function. So

160
00:15:15,560 --> 00:15:22,360
essentially, if you can parameterize the ART assignment function, and then you can optimize

161
00:15:22,360 --> 00:15:27,560
that. So that's our first challenge. We need to parameterize the policy function. And the second

162
00:15:27,560 --> 00:15:33,960
step is how to generate the future states based on our dynamical model. So that's the second step,

163
00:15:33,960 --> 00:15:39,960
is we want to sample the future states. And the last thing is if we can define the reward function.

164
00:15:39,960 --> 00:15:44,680
So essentially, you decompose the calculating this gradient by three

165
00:15:45,480 --> 00:15:50,520
sub steps. If we can sample future states, if we can define the reward function,

166
00:15:50,520 --> 00:15:55,560
if we can parameterize the policy function, and then we can calculate the gradient of the reward.

167
00:15:55,560 --> 00:15:59,720
And then we can plug into policy gradient method and then get the optimal theta.

168
00:16:00,600 --> 00:16:07,080
Okay, so now I will introduce each part. So how to sample the future states? So if we want to

169
00:16:07,080 --> 00:16:11,800
sample the future states, basically, we need a model, and then we do the predictive inference.

170
00:16:11,800 --> 00:16:17,400
And in this case, we have multiple longitudinal states. And we use a multivariate Gaussian

171
00:16:17,400 --> 00:16:22,680
process. The reason we use a multivariate Gaussian process because it's a popular choice for modeling

172
00:16:22,680 --> 00:16:29,240
irregular space multivariate longitudinal data with great flexibility and also natural

173
00:16:29,240 --> 00:16:35,320
uncertainty quantification. And here's irregular, it comes from the missing data because sometimes

174
00:16:35,320 --> 00:16:42,040
maybe some visits and some measurements may be missing. Okay, so the multivariate Gaussian process,

175
00:16:42,040 --> 00:16:49,080
the whole framework of this multivariate Gaussian process is relatively standard. We use yimt to

176
00:16:49,080 --> 00:16:56,760
denote the nth longitudinal variable for treat for individual i at the time t. And we have a mean

177
00:16:56,760 --> 00:17:06,680
function ft and the answer id residue epsilon. So for this f, we assume the multivariate Gaussian

178
00:17:06,680 --> 00:17:11,480
process is distributed. So we will have the mean function. So the next slide I'll introduce how we

179
00:17:11,480 --> 00:17:18,120
model the mean function. And for the various covariance parts, we assume they're the separable

180
00:17:18,120 --> 00:17:23,720
covariance function. So they're here the cm to denote the correlation, represent the correlation

181
00:17:23,720 --> 00:17:29,720
among different state variables because they could be correlated. And the ct to represent their

182
00:17:29,720 --> 00:17:34,520
correlation among the time. So this separable covariance function adjusts for the correlation

183
00:17:34,520 --> 00:17:42,280
among variables and also along the time. Okay, and here for the ct, the correlation kernel that

184
00:17:42,840 --> 00:17:48,760
for the temporal correlation, we use the oil kernel because we expect the curve that's not too

185
00:17:48,760 --> 00:17:55,080
smooth. Okay, so for the mean parts, that's kind of the key country, one of the key contributions

186
00:17:55,080 --> 00:18:00,840
of this model is that for the mean parts, the first two parts are kind of standard. We use a

187
00:18:00,840 --> 00:18:05,880
baseline, it's like linear Mase effect model, we have fixed effects and random effects. But

188
00:18:05,880 --> 00:18:13,080
the how to model the drug combination effects, it's a key thing. So remember I said, for the

189
00:18:13,080 --> 00:18:18,280
drug combination, it's a high dimensional space. So how to represent the drug combinations.

190
00:18:18,280 --> 00:18:23,480
Here we borrow like the kernel idea. So the way we model that is we assume

191
00:18:24,840 --> 00:18:31,240
there exists. So we assume there there's a okay, okay, so we assume there's a

192
00:18:32,040 --> 00:18:39,480
we define a drug similarity regimen function kappa here. So because the z itself is a high

193
00:18:39,480 --> 00:18:44,920
dimensional space. So to reduce the dimensionality, we introduce kappa. So it's like borrow the

194
00:18:44,920 --> 00:18:50,920
kernel idea, we reduce the dimension from the original all the drug combinations to a manageable

195
00:18:50,920 --> 00:18:57,720
size as capital D here. So we introduce a bunch of representative drug regimens ZD. And then we

196
00:18:57,720 --> 00:19:03,320
calculate the similarity between each drug, possible drug combination with ZD. And then as

197
00:19:03,320 --> 00:19:10,280
long as we can estimate the parameter gamma and D here, and then we can represent the drug effects

198
00:19:10,280 --> 00:19:19,480
for each drug combination. And the way we define their similarity function, it depends on two

199
00:19:19,480 --> 00:19:24,360
properties. The first one is we want sharing of information because the similar drugs from

200
00:19:24,360 --> 00:19:29,240
because the drugs from the same drug class, they have similar drug effects because we share the

201
00:19:29,240 --> 00:19:34,760
same mechanisms. So we want to share information from different drug combinations. And also from

202
00:19:34,760 --> 00:19:39,960
this kernel, the introduction of this similarity function, it can reduce the high dimensionality.

203
00:19:40,280 --> 00:19:48,760
So let me briefly introduce the idea of this kernel. Because of time limit, I will not see

204
00:19:48,760 --> 00:19:53,560
the detail. So consider a straight way to compute the drug combination similarity.

205
00:19:54,440 --> 00:19:58,840
And the one straight forward idea is we use linear kernel. So the linear kernel, they can

206
00:19:58,840 --> 00:20:04,120
compute the similarity between regimens based on the proportion of common drugs that two regimens

207
00:20:04,120 --> 00:20:10,520
share. So for example, here, we have three drug combinations. And all of them use two same drugs

208
00:20:10,520 --> 00:20:18,200
from the NRTI class. So D4T plus LAM. And assume the third drug, the first two regimens, they

209
00:20:18,200 --> 00:20:24,120
choose one PI drug, but different PI drugs. And another one is choose NRTI. So you can use a

210
00:20:24,120 --> 00:20:29,080
linear kernel. That means the pairwise similarity among these three kernels will be two over three,

211
00:20:29,080 --> 00:20:35,560
right? Because they have three drugs, and they share two same drugs. However, there are some

212
00:20:36,280 --> 00:20:42,840
disadvantages. For example, the first two drug combinations. So to both of them, they use two

213
00:20:42,840 --> 00:20:47,960
same NRTI drugs. And the third drug, they belong to the same drug class. Because the same drug class,

214
00:20:47,960 --> 00:20:53,640
they share the same madness. So we would expect the similarity between the first two drug combinations

215
00:20:53,640 --> 00:20:59,160
would be larger or would be higher compared to the similarity between there, between them and the

216
00:20:59,160 --> 00:21:03,320
third drug. Because the third, because the third drug combination, they have the drug from a different

217
00:21:03,320 --> 00:21:08,760
drug class. And another example, for example, if you have these two drug combinations, both of

218
00:21:08,760 --> 00:21:15,800
them have two drugs from NRTI drug class, and one drug from the PI drug class. If we use a linear

219
00:21:15,800 --> 00:21:20,840
kernel, and they would share zero similarity, because they don't share any of common drugs.

220
00:21:20,840 --> 00:21:28,120
However, we know the same drug class will share some similarity. So the good method,

221
00:21:28,120 --> 00:21:33,560
we should borrow this clinical information and share some similarity for these two drug combinations.

222
00:21:33,560 --> 00:21:39,720
So the way we set up the, we define the drug similarity is we use the sub subject kernel.

223
00:21:39,720 --> 00:21:44,360
So the sub subject kernel is the idea was to represent the sentences in natural language

224
00:21:44,360 --> 00:21:51,080
processing literature. And here we represent our drug combination by a tree structure. And the

225
00:21:51,080 --> 00:21:58,280
subject kernel can represent the similarity at all levels of the tree representation. So essentially,

226
00:21:58,280 --> 00:22:05,720
the upper level is ART. And then we have the second level to represent which drug class

227
00:22:05,720 --> 00:22:11,000
we draw the drugs. And the third level represents how many drugs from each drug class. And the

228
00:22:11,000 --> 00:22:18,200
third level represents the each drug from each drug class. And then the sub subject kernel can

229
00:22:18,200 --> 00:22:23,560
represent the whole similarity. For example, like regimen A and B, they can adjust for their similarity

230
00:22:23,560 --> 00:22:29,400
is a blue box. And for regimen A and C, they can adjust for their drug similarity is a yellow box.

231
00:22:29,400 --> 00:22:34,600
Even, you know, they don't share any common drugs, but you can still incorporate their similarity.

232
00:22:35,320 --> 00:22:39,880
Okay, so now I have introduced this Markov-Berica Gaussian process to model the

233
00:22:39,880 --> 00:22:44,760
longitudinal states. And then if we have a model and we have our own parameters, and then we can

234
00:22:44,760 --> 00:22:48,920
write down the likelihood, and you can assign the price to all unknown parameters, you can

235
00:22:48,920 --> 00:22:53,480
obtain the posterior distribution from the MCMC. So I will skip all the computational

236
00:22:53,480 --> 00:22:57,880
details here. But essentially, now we finish the first step. So we have the Markov-Berica

237
00:22:57,880 --> 00:23:03,000
normal model, we can sample future states. Okay, so the second one is how to define the reward

238
00:23:03,000 --> 00:23:08,360
function. And the reward function, it depends on the clinical goal, right? So here,

239
00:23:09,720 --> 00:23:17,080
it depends on how we define the long term house for each individual. So here, after consultation

240
00:23:17,080 --> 00:23:22,600
with the clinicians, we determine that we define the reward that depends on the barrel load,

241
00:23:22,600 --> 00:23:27,240
kidney function and the depression. So we want to care about first, you know, whether it can

242
00:23:27,240 --> 00:23:32,040
successfully suppress the barrel load, and also maintain a good kidney function and also the good

243
00:23:32,040 --> 00:23:39,960
mental health. So let's see, our goal is we will, so here we can say we want to maximize

244
00:23:41,160 --> 00:23:47,800
the overall house in the next two years. So remember, the visits are semi-annual visits.

245
00:23:47,800 --> 00:23:54,280
So that's why here the sum is from the next visit, next four visits, because next four visits means

246
00:23:54,280 --> 00:24:01,080
the overall good health in the next two years. And then we want the depression, this as small as

247
00:24:01,080 --> 00:24:08,760
possible. And also, oh yeah, here is the next four visits. And also for depression, it's as small as

248
00:24:08,760 --> 00:24:14,200
possible. And for the barrel load and the EGFR, because as long as they are normal threshold,

249
00:24:14,200 --> 00:24:19,320
it will be fine. So we define this kind of step function, as long as they are in the normal range,

250
00:24:19,320 --> 00:24:25,640
it'll be fine. And if it's outside of the normal range, and we give certain penalty. And also here,

251
00:24:25,640 --> 00:24:33,000
we have to personalize the weights, WI. So for example, if some person, they more care about

252
00:24:33,000 --> 00:24:38,600
the depression, and then the WI1 can have a higher weight. So it's personalized and determined by

253
00:24:38,600 --> 00:24:45,240
the physician and also patient himself. And also to mitigate the distribution shift

254
00:24:45,240 --> 00:24:51,640
issue, we use certainly penalized reward. That's another advantage of using the Bayesian method

255
00:24:51,640 --> 00:24:57,080
in the first step, because we can easily quantify the uncertainty. So this idea is by this paper by

256
00:24:57,080 --> 00:25:03,240
you from UC Berkeley's group. And essentially, we define a pessimistic environment by introducing

257
00:25:03,240 --> 00:25:11,240
a penalized reward. So the RA is defined by the previous slide. But now we penalize the uncertainty

258
00:25:11,240 --> 00:25:17,800
of the, it's the predictive variability of the state and their treatments. And it's a

259
00:25:17,800 --> 00:25:25,000
tuning parameter we need to learn. Okay. And then we use a posterior predict distribution to

260
00:25:25,000 --> 00:25:29,080
quantify the uncertainty again, because we have a Bayesian model, so that's very straightforward.

261
00:25:29,080 --> 00:25:34,600
Okay, so now we define a reward function. And the last step is how to parameterize the policy

262
00:25:34,600 --> 00:25:42,200
function. So to prioritize the policy function, we make this also the three types of decision

263
00:25:42,200 --> 00:25:47,800
after talking with clinicians. So essentially, we decompose this to several steps. So first,

264
00:25:48,440 --> 00:25:53,880
if this person has been using ART drugs for a long time, and we will see if this person needs

265
00:25:53,880 --> 00:25:59,960
to switch the regimen or not. So if the older drug works, and we can just keep using the older drug.

266
00:25:59,960 --> 00:26:04,440
So this is what we will represent as a logistic regression method in the logistic regression

267
00:26:04,440 --> 00:26:10,600
model. As long as all the health measurements are in within the normal range, and then we will

268
00:26:10,600 --> 00:26:15,160
decide to just, you know, keep the old drug. And if one of them is not in the normal range,

269
00:26:15,160 --> 00:26:21,720
we will switch. If this person needs to switch, and then we will need to generate a new regimen.

270
00:26:22,280 --> 00:26:28,520
And because we used the three representation, and then we can now decide, you know, if we need to

271
00:26:28,520 --> 00:26:33,240
switch how to generate a new regimen, essentially, we need to decide like which drug class and how

272
00:26:33,240 --> 00:26:38,600
many drugs use the initial class and also which individual drugs at each class. So this essentially

273
00:26:38,600 --> 00:26:43,080
it's a non-central hypergeometric distribution. Again, I skipped all the details. It's kind of

274
00:26:43,080 --> 00:26:51,560
a little bit complex. So we have these three levels of three decisions. Okay, so now we have already

275
00:26:51,560 --> 00:26:55,880
finished these three steps. So we have multivariate Gaussian process to some whole future states,

276
00:26:55,880 --> 00:27:00,760
and we define reward function. And then we have ways to prioritize a policy function. We can use

277
00:27:00,760 --> 00:27:08,040
a policy gradient method to optimize a print. Okay. Okay, so now, so here I finish all the

278
00:27:08,040 --> 00:27:14,680
matter introduction. Last part of the slides is I will introduce real data analysis results.

279
00:27:15,720 --> 00:27:22,120
So for the real data, we got about 300 women from the Washington DC site from the white study.

280
00:27:23,000 --> 00:27:29,400
And also now we get four state variables at each visit, depression, viral load, EGFR, and BMI.

281
00:27:29,400 --> 00:27:34,360
And there are about 8% missing data. And the baseline covariates we consider include age,

282
00:27:34,360 --> 00:27:40,680
smoking status, substance use, employment status, hypertension, diabetes. And in this study,

283
00:27:40,680 --> 00:27:48,280
we have 31 ART drugs and six drug classes. And we choose 105 representative drug regiments. So

284
00:27:48,280 --> 00:27:53,400
those regiments based on the popularity of the drug combinations, if they have been used a lot

285
00:27:53,400 --> 00:27:58,600
of times for the from the patients, and then we would know that as representative ART regimen.

286
00:27:59,160 --> 00:28:05,960
Okay, so here is one hypothetical patient. So we'll use this example to demonstrate the precision

287
00:28:05,960 --> 00:28:13,000
medicine, the, you know, the utility of the clinical utility of the proposed methods. Okay,

288
00:28:13,000 --> 00:28:21,000
so this person has been has been had 31 visits. And here is their history of treatments. And for

289
00:28:21,000 --> 00:28:28,920
these patients, we assume their weights as one third, one third, one third. Okay, and then we run

290
00:28:28,920 --> 00:28:35,480
our optimization method. And here we can see the expected reward versus the SGD iteration. So it

291
00:28:35,480 --> 00:28:42,440
became relatively stable after 1,000 iterations. And here is optimal regimen is the next two years.

292
00:28:42,440 --> 00:28:47,800
So we can see at the visit to there are two ART drugs when you see one PR and one poster,

293
00:28:47,800 --> 00:28:55,720
and then it changes one new regimen for visits 33 to 35. Okay, and also here I want to show under

294
00:28:55,720 --> 00:29:01,480
their estimated optimal regiments, that's the predicted depression stores. And we can see for

295
00:29:01,480 --> 00:29:09,320
their visits from 32 to 36, they're about 23% improvement of depression. So that also shows

296
00:29:09,320 --> 00:29:14,680
the clinical utility of our, you know, newly assigned optimal drug combinations. Okay, I will

297
00:29:14,680 --> 00:29:25,480
skip the next example due to the time limit. Oh, yeah. Okay, to summarize, we, we propose a like

298
00:29:25,480 --> 00:29:30,680
a Bayesian reforcing learning approach is a two step approach. And it can learn their dynamics

299
00:29:30,680 --> 00:29:36,120
with uncertainty quantification, it can also assign the long term optimal drug combinations to

300
00:29:37,320 --> 00:29:41,000
optimize each individual's health. Okay, yeah, thank you.

301
00:29:41,240 --> 00:29:47,880
Thank you so much, Yanxun. Any questions from the audience?

302
00:29:48,920 --> 00:29:54,840
Yanxun, those are very exciting work. I actually have some questions because you touched a very

303
00:29:54,840 --> 00:30:02,520
good point where you need to balance the priority like competing priorities when you are in the

304
00:30:02,520 --> 00:30:06,840
clinical practice. But since we are a little bit over time, so I probably will talk to you later

305
00:30:06,840 --> 00:30:14,440
about that. I was wondering like how the uncertainty will be impacted by how you

306
00:30:14,440 --> 00:30:23,320
define your reward function. Oh, yeah. So the uncertainty part, you know, how the uncertainty

307
00:30:23,320 --> 00:30:28,600
affects the final decision depending on how you tune the parameter. So like here, and yeah, I skip

308
00:30:28,600 --> 00:30:32,840
that part, but here you can show if we have different tooling parameters like lambda equal

309
00:30:32,840 --> 00:30:38,440
zero, you don't penalize at all. And then you have this drug combination. And if you use like

310
00:30:38,440 --> 00:30:43,240
increase the lambda, and then it will penalize the uncertainty, it's kind of uncertainties

311
00:30:43,240 --> 00:30:48,600
reflected by the sample size in their data. If this drug combination has has been used a lot of

312
00:30:48,600 --> 00:30:54,040
times, it has relatively narrow uncertainty, it had never been used, then it has a lot of

313
00:30:54,040 --> 00:30:57,960
uncertainty. So for example, here with lambda equal zero, they actually recommend this drug

314
00:30:57,960 --> 00:31:03,640
combination is the first recommendation. So it actually never been used in the in the data.

315
00:31:03,640 --> 00:31:08,440
So that's kind of create a trade off, like we need to discuss the clinician, like if this drug

316
00:31:08,440 --> 00:31:13,160
combination has never been used, do you want to try this new drug, or you want more conservative

317
00:31:13,160 --> 00:31:18,760
choices, like, you know, these two drug combinations, it right, it has been used more times. Yeah,

318
00:31:18,760 --> 00:31:22,920
I mean, this this tuning parameter definitely plays a role. But you know, actually, when you

319
00:31:22,920 --> 00:31:28,680
define your reward function, there is another part with the personalized weights. So I was wondering,

320
00:31:28,680 --> 00:31:35,240
like, no, we also have similar problems. So we also have like, for example, the survival or

321
00:31:35,240 --> 00:31:40,440
quality of life to balance. But then when we provide the personalized weights, and if you

322
00:31:40,440 --> 00:31:46,520
change a little bit of the weights, actually, the rules or the decision you will make, or you learn

323
00:31:46,520 --> 00:31:54,760
from the data will change. So yeah, that's a good question. Yeah, we can we can discuss more

324
00:31:54,760 --> 00:32:00,920
details. Yeah. Thank you so much. I'm going to share my screen.

