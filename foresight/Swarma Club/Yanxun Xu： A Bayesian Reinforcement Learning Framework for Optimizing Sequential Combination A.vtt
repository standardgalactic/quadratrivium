WEBVTT

00:00.000 --> 00:07.280
Alright, so we'll move on to our next speaker. Yanxun will be our next speaker

00:07.280 --> 00:12.720
and she is a associate professor in the Department of Applied Mathematics and

00:12.720 --> 00:19.080
Statistics at Johns Hopkins University. Yanxun has a lot of research interest in

00:19.080 --> 00:23.360
the reinforcement learning, high-dimensional data analysis, and

00:23.360 --> 00:29.360
non-parametric statistics. Okay, so today she is going to talk about a Bayesian

00:29.360 --> 00:36.040
reinforcement learning framework for DPR. So you can see the screen, right? Yeah,

00:36.040 --> 00:41.800
yeah, great. So thank you Lu for organizing this session and introducing me. So

00:41.800 --> 00:44.880
today I will talk about a Bayesian reinforcement learning method for

00:44.880 --> 00:51.440
optimizing treatments for HIV patients. So first I'd like to thank all my

00:51.440 --> 00:56.320
collaborators in this project. So my peer-de-student Wei and also my

00:56.320 --> 01:00.640
collaborators, Leah and Raha from Johns Hopkins University and Yanni from

01:00.640 --> 01:05.800
Texas A&M and also two clinicians, Amanda and Jane from Georgetown University

01:05.800 --> 01:14.160
and Washington University in St. Louis. Okay, so making the optimal

01:14.160 --> 01:19.320
sequential decisions is very important in many diseases. So today I will focus on

01:19.320 --> 01:25.960
HIV and we know the emergence of anti-retroviral drugs, ARP, has transformed

01:25.960 --> 01:30.840
the HIV infection from a fetal disease to a chronic disease. So it's

01:30.840 --> 01:37.440
significantly reduced HIV-related mortality and the common ARP drugs fall

01:37.440 --> 01:42.760
into different drug classes with different mechanisms. For example, the

01:42.760 --> 01:48.040
first one, the NRPI, it's called a nucleotide or probably can also be a mouse.

01:48.040 --> 01:52.560
Okay, so the first drug class called NRPI is called nucleotide reverse

01:52.560 --> 01:57.800
transcriptase inhibitor. It inhibits the reverse transcription step in viral

01:57.800 --> 02:03.560
replication and other common drug classes include like NRTI, PI, ESTA, EI and

02:03.560 --> 02:09.720
the booster. And the model ART drugs usually combines three or more ART drugs

02:09.720 --> 02:15.120
from different drug classes. Since such cocktail approach is most efficient in

02:15.120 --> 02:21.640
suppressing viral loads. For example, the clinician can prescribe two NRTIs and one

02:21.920 --> 02:26.480
drug from instinct and all they can prescribe like two drugs from an ARTI

02:26.480 --> 02:35.000
drug class and one from an ARPI or one from the PI. So people living with

02:35.000 --> 02:39.560
HIV now are recommended to follow up with their physicians semi-annually by the

02:39.560 --> 02:45.440
current HIV treatment guideline. And at each visit, there are social demographics,

02:45.440 --> 02:50.320
lab test results, such as CD4 counts and viral loads, and also clinical

02:50.320 --> 02:55.240
environments such as BMI and the glucose are collected. And then physician

02:55.240 --> 03:00.120
assigns their ART regimen based on their clinical observations. So each ART

03:00.120 --> 03:05.240
regimen is a combination of different drugs from different classes. And this

03:05.240 --> 03:11.560
process repeats every half a year by average. So nowadays, the US Department

03:11.560 --> 03:17.080
of Health and Human Services provides general guidelines on assigning ART

03:17.080 --> 03:21.880
treatments. However, these guidelines are usually applied to treatment

03:21.880 --> 03:27.440
naive subjects, meaning subjects who are recently diagnosed with HIV and who

03:27.440 --> 03:33.440
never took ART treatments before. And for happily pretreated people with HIV,

03:33.440 --> 03:38.800
for example, if this person has been diagnosed with HIV for 20 years and has

03:38.800 --> 03:44.560
been taking ART drugs for a long time, and there's no consensus. And also the

03:44.560 --> 03:49.240
current guideline didn't take into account the potential adverse effects

03:49.280 --> 03:55.000
caused by the long term use of ART drugs, because HIV now is more like a

03:55.040 --> 04:01.120
chronic disease. And those patients needs to take ART drugs every day. And

04:01.160 --> 04:05.320
indefinitely. So you need to take every day to take the drugs every day to

04:05.320 --> 04:10.400
suppress the viral loads. And they may cause some long term adverse effects.

04:10.560 --> 04:14.800
But for example, like the kidney disease, weight gain or depression, and the

04:14.800 --> 04:18.840
current guideline didn't take into account all those side effects. So our

04:18.840 --> 04:24.400
goal is to determine the personalized ART regimen to optimize the long term

04:24.400 --> 04:29.920
health. That means not only the drugs can supply the viral loads, but can also

04:29.920 --> 04:37.000
control the side effects. So large scale HIV studies such as maximize

04:37.000 --> 04:42.040
cohort studies. And it provides us opportunities to achieve this goal. And

04:42.040 --> 04:47.440
they collect data from participants semi annual visits. And here is the ART

04:47.600 --> 04:52.480
treatment history for one randomly selected subject with seven visits. Here's

04:52.480 --> 04:57.720
the X axis shows the calendar dates of their visits. And the Y axis shows the

04:57.720 --> 05:03.120
ART drug combinations they have this person has taken. And different colors

05:03.120 --> 05:07.280
here represent different drug classes and the drug name are marked. So I use a

05:07.280 --> 05:14.000
three letter to represent each drug. And also at each visit, the house related

05:14.000 --> 05:18.520
environments are recorded. For example, this figure shows the subjects,

05:18.520 --> 05:23.080
longitudinal depression scores, viral load, EGFR for kidney function and the

05:23.080 --> 05:29.720
BMI. So there are many challenges to optimize the sequential ART regimens

05:29.720 --> 05:34.760
in a data driven manner. First, we need to estimate the drug effects. Like before

05:34.760 --> 05:38.960
we assign them, we need to understand their drug effects from a high dimensional

05:38.960 --> 05:43.760
and unbalanced space. So when I say high dimensional, it means with more than 30

05:43.760 --> 05:48.800
ART drugs, they're all FDA approved on the market. There are a large number of

05:48.800 --> 05:53.440
possible drug combinations. So that could be millions of drug combinations you

05:53.440 --> 05:57.040
can choose. So even like the feasible drug combinations about like thousands,

05:57.040 --> 06:00.760
there's still there are a large number of possible drug combinations. And also

06:00.760 --> 06:05.400
when I say the unbalanced, that means some drug combinations are firmly used, but

06:05.400 --> 06:09.120
others are wrong. So for example, we can see for this drug combination is two

06:09.120 --> 06:14.560
ART drugs and one PR drug. It was the using our database for almost 1000

06:14.560 --> 06:19.960
times, but another similar drug combination. So the same two ART drug, but

06:19.960 --> 06:26.440
a different the PR drug, it was only used for 12 times. And the second challenge

06:26.440 --> 06:31.880
is how to generate a realistic ART regimen from a large discreet space in the

06:31.880 --> 06:36.240
optimization procedure. So after we understand the treatment effects of

06:36.240 --> 06:41.360
these drug combinations, we need to assign the drug regimen to patients. That

06:41.360 --> 06:46.320
means we need to generate a realistic ART regimen. So then the problem is how to

06:46.320 --> 06:51.640
represent each ART regimen. A naive method would be say, okay, I can use a battery

06:51.640 --> 06:58.040
vector. So say I have 30 ART, I have 30 ART drugs on the market, and then I can use

06:58.040 --> 07:02.680
the battery vector is indimensional battery vector to represent. So if this drug

07:02.680 --> 07:08.680
combination contains drug one, then it's one, otherwise it's zero. However, it will

07:08.680 --> 07:13.880
cause two to the N possible drug combinations. And many of them will not

07:13.880 --> 07:17.760
be feasible. So like I said, usually, you know, people combine the drugs from

07:17.760 --> 07:21.440
different drug combinations, but usually we assign like, for example, two or three

07:21.640 --> 07:27.160
drugs from ARTI, but like really we assign one drug from PI. So not all these

07:27.160 --> 07:32.720
possible two to the N combinations are possible. And then we need the efficient

07:32.720 --> 07:38.440
way to represent the drug combination. And lastly, we aim to optimize sequential

07:38.440 --> 07:42.680
treatments from observational data. So we have all those data collected from

07:42.680 --> 07:48.360
observational study, but we want to assign them to the patients in the future. So

07:48.360 --> 07:53.560
the fundamental challenge of optimizing treatments from observational study is

07:53.560 --> 07:57.880
this distribution shift issue. So that means the training data may be collected

07:57.880 --> 08:01.560
on the different policies from the one we try to evaluate. So we need to address

08:01.560 --> 08:07.320
the distribution shift issue. So to address these challenges, we develop a

08:07.320 --> 08:12.280
two-step Bayesian reinforcement learning framework. And here is an overview. In the

08:12.280 --> 08:16.440
first step, we propose a Bayesian dynamics model for individuals'

08:16.520 --> 08:21.000
longitudinal observations using a microverte Gaussian process. And these estimate

08:21.000 --> 08:25.560
dynamics describe how individuals' health-related variables evolve over time,

08:25.560 --> 08:29.560
condition on their historical states and the treatment histories, with uncertainty

08:29.560 --> 08:35.720
quantification. And in the second step, we create a pessimistic environment with

08:35.720 --> 08:41.720
uncertainty penalization to mitigate the distribution shift issue and also use

08:41.720 --> 08:45.560
an offline reinforcement learning method to optimize the sequential ART regimen.

08:46.200 --> 08:52.520
So it's a two-step approach. Okay, so now let's go to the model details.

08:53.240 --> 09:00.680
So first, I introduce the problem formulation. Assume for each individual eye,

09:00.680 --> 09:07.960
we use XI0 to denote the baseline covariates, such as a race. And the TRI records the

09:07.960 --> 09:15.080
visit times. So assume each one has GI visits, and we have PI1 to TIGI to denote the time of each

09:15.080 --> 09:21.240
visit. And also assume we have M time varying health state variables. So here we call state

09:21.240 --> 09:26.840
variables because they change over time. For example, like H, BMI, EGFR, those clinical

09:26.840 --> 09:33.880
variables or demographics, they are collected in each visit. And also the I to represent the ART

09:33.880 --> 09:39.320
drug combination used by individual eye during the time period, TIGI minus 1 to TIGI.

09:41.000 --> 09:48.360
And then the data can be summarized as D. So for each subject, so we have a total of I subjects,

09:48.360 --> 09:53.480
and we have baseline covariates, the time of each visit, and the time varying state variables,

09:53.480 --> 10:02.200
and also the drug information. And then we use the YIG bar. So the bar is a common fun we really

10:02.280 --> 10:08.840
use to represent the history. So the YIG bar means all the state variables before the current

10:08.840 --> 10:16.040
visit J. And the ZIG bar means all the treatments this person has been taking on pure the time J.

10:16.680 --> 10:24.680
And also we use the dynamic, we use the after dynamic model. So that means condition on the YIG

10:24.680 --> 10:33.720
bar and the ZIG plus one, we update the state variables from YIG to YG plus one. So remember

10:33.720 --> 10:40.680
the first step of our method to learn the dynamic model of how to transform from YG to YG plus one.

10:40.680 --> 10:46.440
Of course, the YG plus one condition on the whole history of the YG and the treatments the IJ.

10:47.800 --> 10:53.080
Okay, so our goal is to optimize the ART assignments to maximize the individual's long-term

10:53.080 --> 10:59.480
house outcome. So because we want to maximize the house outcome, essentially we can transform the

10:59.480 --> 11:05.400
problem to an optimization problem. So this optimization problem is like we first define

11:06.200 --> 11:11.960
for each individual I, we suppose she already has GI visits. So if this person is a new patient,

11:11.960 --> 11:18.120
so the GI will be equal zero. So the GI can be zero, or if this person already has GI visits,

11:18.120 --> 11:24.280
and then we want to predict the next few visits, for example. And then we let the Y new and the

11:24.280 --> 11:31.480
GI new to denote her future longitudinal states and the treatments, because our reward may depend on

11:31.480 --> 11:38.520
her future states. For example, we want her less than the next two years, the depression is minimal.

11:40.040 --> 11:45.800
And assume for any future visited J, the ART regimen is assigned through a policy function

11:45.800 --> 11:52.920
PI. So that means if we can learn, if we can prioritize this function PI, and we can optimize

11:52.920 --> 12:00.760
data and the equivalently we can optimize the treatment. And let's say we assign a stochastic

12:00.760 --> 12:07.640
reward function RI that depends on the house states. So we can depend, we can define the reward as

12:07.720 --> 12:15.160
for example, this person, their, their very loaded is low, and the pressure set is low, and the BMI is

12:15.160 --> 12:21.000
in the normal range, and the kidney function is a normal range. Okay, so for example, if our goal

12:21.000 --> 12:27.080
is to select the sequential ART regimen that leads to lowest accumulated very low in the next two

12:27.080 --> 12:33.560
years, and it is coming an active sum of the very loads. Okay, so they notice the expected reward

12:33.560 --> 12:38.920
for any individual I to be the following. Because we, in the first step, we learn a Bayesian model.

12:38.920 --> 12:45.560
So essentially, you can generate their future states. And also, so we can generate the future

12:45.560 --> 12:51.400
states, why are you from the learned dynamic model, and we generate the ZI new from their

12:52.280 --> 12:56.360
the PI, the function. So we learned the PI from their parameter as the function PI.

12:57.000 --> 13:02.360
And also, because we learned the dynamic model, and then we can integrate out certain things,

13:02.440 --> 13:07.480
optimization procedure. So that's kind of the benefits of using the Bayesian framework in the

13:07.480 --> 13:12.840
first step. Because in the decision making step, we can integrate out uncertainty, we can adjust

13:12.840 --> 13:19.560
for this uncertainty quantification from their uncertainty of their dynamic, their dynamic model.

13:21.000 --> 13:27.720
And our goal is to find as optimized optimal policy function PI that parameterized by data

13:27.720 --> 13:33.080
i star. So we want to find as an optimal parameter that can maximize the r i theta.

13:34.200 --> 13:40.440
So that's a problem. Okay, so now we have already defined our reward function r i. And we want to

13:40.440 --> 13:45.960
find as a parameter theta that can maximize the r i. And essentially, it's a classical

13:45.960 --> 13:51.400
reinforcement learning problem. And we can use the policy gradient method to solve the problem.

13:51.400 --> 13:57.320
So essentially, if you can calculate the gradient of r i, and then you can use the policy gradient

13:57.320 --> 14:03.160
method, essentially, you update the theta by this formula. So it's also classical results,

14:03.160 --> 14:10.280
it's that you update the theta by the previous theta, and then plus some step size s i, q,

14:10.280 --> 14:16.760
and then times their gradient. So the essential problem is how to calculate the gradient of our

14:16.760 --> 14:22.200
reward function. So we can see our reward function is relatively complex, right? You take the expected

14:22.200 --> 14:29.400
value of the reward function r i, and r is a function of y mu, and y mu you need to generate

14:29.400 --> 14:35.960
from the predict distribution of your dynamical model. And besides all of that, we also need to

14:35.960 --> 14:41.080
integrate out the uncertainty from the dynamical model by the P5 condition on D, that's a posterior

14:41.080 --> 14:49.320
distribution of the phi. So it's not easy to calculate the gradient of this r i theta. Okay,

14:49.400 --> 14:55.000
so luckily for the policy gradient method, there's a way to calculate that. So if you're

14:55.000 --> 14:59.400
interested in details, you can find the details in the paper, but we can represent the derivative

14:59.400 --> 15:06.760
of r i theta as follows. And this formula looks very complex, but we can't decompose this into

15:06.760 --> 15:15.560
three separate steps. The first step is this step. So it's about the log of our policy function. So

15:15.560 --> 15:22.360
essentially, if you can parameterize the ART assignment function, and then you can optimize

15:22.360 --> 15:27.560
that. So that's our first challenge. We need to parameterize the policy function. And the second

15:27.560 --> 15:33.960
step is how to generate the future states based on our dynamical model. So that's the second step,

15:33.960 --> 15:39.960
is we want to sample the future states. And the last thing is if we can define the reward function.

15:39.960 --> 15:44.680
So essentially, you decompose the calculating this gradient by three

15:45.480 --> 15:50.520
sub steps. If we can sample future states, if we can define the reward function,

15:50.520 --> 15:55.560
if we can parameterize the policy function, and then we can calculate the gradient of the reward.

15:55.560 --> 15:59.720
And then we can plug into policy gradient method and then get the optimal theta.

16:00.600 --> 16:07.080
Okay, so now I will introduce each part. So how to sample the future states? So if we want to

16:07.080 --> 16:11.800
sample the future states, basically, we need a model, and then we do the predictive inference.

16:11.800 --> 16:17.400
And in this case, we have multiple longitudinal states. And we use a multivariate Gaussian

16:17.400 --> 16:22.680
process. The reason we use a multivariate Gaussian process because it's a popular choice for modeling

16:22.680 --> 16:29.240
irregular space multivariate longitudinal data with great flexibility and also natural

16:29.240 --> 16:35.320
uncertainty quantification. And here's irregular, it comes from the missing data because sometimes

16:35.320 --> 16:42.040
maybe some visits and some measurements may be missing. Okay, so the multivariate Gaussian process,

16:42.040 --> 16:49.080
the whole framework of this multivariate Gaussian process is relatively standard. We use yimt to

16:49.080 --> 16:56.760
denote the nth longitudinal variable for treat for individual i at the time t. And we have a mean

16:56.760 --> 17:06.680
function ft and the answer id residue epsilon. So for this f, we assume the multivariate Gaussian

17:06.680 --> 17:11.480
process is distributed. So we will have the mean function. So the next slide I'll introduce how we

17:11.480 --> 17:18.120
model the mean function. And for the various covariance parts, we assume they're the separable

17:18.120 --> 17:23.720
covariance function. So they're here the cm to denote the correlation, represent the correlation

17:23.720 --> 17:29.720
among different state variables because they could be correlated. And the ct to represent their

17:29.720 --> 17:34.520
correlation among the time. So this separable covariance function adjusts for the correlation

17:34.520 --> 17:42.280
among variables and also along the time. Okay, and here for the ct, the correlation kernel that

17:42.840 --> 17:48.760
for the temporal correlation, we use the oil kernel because we expect the curve that's not too

17:48.760 --> 17:55.080
smooth. Okay, so for the mean parts, that's kind of the key country, one of the key contributions

17:55.080 --> 18:00.840
of this model is that for the mean parts, the first two parts are kind of standard. We use a

18:00.840 --> 18:05.880
baseline, it's like linear Mase effect model, we have fixed effects and random effects. But

18:05.880 --> 18:13.080
the how to model the drug combination effects, it's a key thing. So remember I said, for the

18:13.080 --> 18:18.280
drug combination, it's a high dimensional space. So how to represent the drug combinations.

18:18.280 --> 18:23.480
Here we borrow like the kernel idea. So the way we model that is we assume

18:24.840 --> 18:31.240
there exists. So we assume there there's a okay, okay, so we assume there's a

18:32.040 --> 18:39.480
we define a drug similarity regimen function kappa here. So because the z itself is a high

18:39.480 --> 18:44.920
dimensional space. So to reduce the dimensionality, we introduce kappa. So it's like borrow the

18:44.920 --> 18:50.920
kernel idea, we reduce the dimension from the original all the drug combinations to a manageable

18:50.920 --> 18:57.720
size as capital D here. So we introduce a bunch of representative drug regimens ZD. And then we

18:57.720 --> 19:03.320
calculate the similarity between each drug, possible drug combination with ZD. And then as

19:03.320 --> 19:10.280
long as we can estimate the parameter gamma and D here, and then we can represent the drug effects

19:10.280 --> 19:19.480
for each drug combination. And the way we define their similarity function, it depends on two

19:19.480 --> 19:24.360
properties. The first one is we want sharing of information because the similar drugs from

19:24.360 --> 19:29.240
because the drugs from the same drug class, they have similar drug effects because we share the

19:29.240 --> 19:34.760
same mechanisms. So we want to share information from different drug combinations. And also from

19:34.760 --> 19:39.960
this kernel, the introduction of this similarity function, it can reduce the high dimensionality.

19:40.280 --> 19:48.760
So let me briefly introduce the idea of this kernel. Because of time limit, I will not see

19:48.760 --> 19:53.560
the detail. So consider a straight way to compute the drug combination similarity.

19:54.440 --> 19:58.840
And the one straight forward idea is we use linear kernel. So the linear kernel, they can

19:58.840 --> 20:04.120
compute the similarity between regimens based on the proportion of common drugs that two regimens

20:04.120 --> 20:10.520
share. So for example, here, we have three drug combinations. And all of them use two same drugs

20:10.520 --> 20:18.200
from the NRTI class. So D4T plus LAM. And assume the third drug, the first two regimens, they

20:18.200 --> 20:24.120
choose one PI drug, but different PI drugs. And another one is choose NRTI. So you can use a

20:24.120 --> 20:29.080
linear kernel. That means the pairwise similarity among these three kernels will be two over three,

20:29.080 --> 20:35.560
right? Because they have three drugs, and they share two same drugs. However, there are some

20:36.280 --> 20:42.840
disadvantages. For example, the first two drug combinations. So to both of them, they use two

20:42.840 --> 20:47.960
same NRTI drugs. And the third drug, they belong to the same drug class. Because the same drug class,

20:47.960 --> 20:53.640
they share the same madness. So we would expect the similarity between the first two drug combinations

20:53.640 --> 20:59.160
would be larger or would be higher compared to the similarity between there, between them and the

20:59.160 --> 21:03.320
third drug. Because the third, because the third drug combination, they have the drug from a different

21:03.320 --> 21:08.760
drug class. And another example, for example, if you have these two drug combinations, both of

21:08.760 --> 21:15.800
them have two drugs from NRTI drug class, and one drug from the PI drug class. If we use a linear

21:15.800 --> 21:20.840
kernel, and they would share zero similarity, because they don't share any of common drugs.

21:20.840 --> 21:28.120
However, we know the same drug class will share some similarity. So the good method,

21:28.120 --> 21:33.560
we should borrow this clinical information and share some similarity for these two drug combinations.

21:33.560 --> 21:39.720
So the way we set up the, we define the drug similarity is we use the sub subject kernel.

21:39.720 --> 21:44.360
So the sub subject kernel is the idea was to represent the sentences in natural language

21:44.360 --> 21:51.080
processing literature. And here we represent our drug combination by a tree structure. And the

21:51.080 --> 21:58.280
subject kernel can represent the similarity at all levels of the tree representation. So essentially,

21:58.280 --> 22:05.720
the upper level is ART. And then we have the second level to represent which drug class

22:05.720 --> 22:11.000
we draw the drugs. And the third level represents how many drugs from each drug class. And the

22:11.000 --> 22:18.200
third level represents the each drug from each drug class. And then the sub subject kernel can

22:18.200 --> 22:23.560
represent the whole similarity. For example, like regimen A and B, they can adjust for their similarity

22:23.560 --> 22:29.400
is a blue box. And for regimen A and C, they can adjust for their drug similarity is a yellow box.

22:29.400 --> 22:34.600
Even, you know, they don't share any common drugs, but you can still incorporate their similarity.

22:35.320 --> 22:39.880
Okay, so now I have introduced this Markov-Berica Gaussian process to model the

22:39.880 --> 22:44.760
longitudinal states. And then if we have a model and we have our own parameters, and then we can

22:44.760 --> 22:48.920
write down the likelihood, and you can assign the price to all unknown parameters, you can

22:48.920 --> 22:53.480
obtain the posterior distribution from the MCMC. So I will skip all the computational

22:53.480 --> 22:57.880
details here. But essentially, now we finish the first step. So we have the Markov-Berica

22:57.880 --> 23:03.000
normal model, we can sample future states. Okay, so the second one is how to define the reward

23:03.000 --> 23:08.360
function. And the reward function, it depends on the clinical goal, right? So here,

23:09.720 --> 23:17.080
it depends on how we define the long term house for each individual. So here, after consultation

23:17.080 --> 23:22.600
with the clinicians, we determine that we define the reward that depends on the barrel load,

23:22.600 --> 23:27.240
kidney function and the depression. So we want to care about first, you know, whether it can

23:27.240 --> 23:32.040
successfully suppress the barrel load, and also maintain a good kidney function and also the good

23:32.040 --> 23:39.960
mental health. So let's see, our goal is we will, so here we can say we want to maximize

23:41.160 --> 23:47.800
the overall house in the next two years. So remember, the visits are semi-annual visits.

23:47.800 --> 23:54.280
So that's why here the sum is from the next visit, next four visits, because next four visits means

23:54.280 --> 24:01.080
the overall good health in the next two years. And then we want the depression, this as small as

24:01.080 --> 24:08.760
possible. And also, oh yeah, here is the next four visits. And also for depression, it's as small as

24:08.760 --> 24:14.200
possible. And for the barrel load and the EGFR, because as long as they are normal threshold,

24:14.200 --> 24:19.320
it will be fine. So we define this kind of step function, as long as they are in the normal range,

24:19.320 --> 24:25.640
it'll be fine. And if it's outside of the normal range, and we give certain penalty. And also here,

24:25.640 --> 24:33.000
we have to personalize the weights, WI. So for example, if some person, they more care about

24:33.000 --> 24:38.600
the depression, and then the WI1 can have a higher weight. So it's personalized and determined by

24:38.600 --> 24:45.240
the physician and also patient himself. And also to mitigate the distribution shift

24:45.240 --> 24:51.640
issue, we use certainly penalized reward. That's another advantage of using the Bayesian method

24:51.640 --> 24:57.080
in the first step, because we can easily quantify the uncertainty. So this idea is by this paper by

24:57.080 --> 25:03.240
you from UC Berkeley's group. And essentially, we define a pessimistic environment by introducing

25:03.240 --> 25:11.240
a penalized reward. So the RA is defined by the previous slide. But now we penalize the uncertainty

25:11.240 --> 25:17.800
of the, it's the predictive variability of the state and their treatments. And it's a

25:17.800 --> 25:25.000
tuning parameter we need to learn. Okay. And then we use a posterior predict distribution to

25:25.000 --> 25:29.080
quantify the uncertainty again, because we have a Bayesian model, so that's very straightforward.

25:29.080 --> 25:34.600
Okay, so now we define a reward function. And the last step is how to parameterize the policy

25:34.600 --> 25:42.200
function. So to prioritize the policy function, we make this also the three types of decision

25:42.200 --> 25:47.800
after talking with clinicians. So essentially, we decompose this to several steps. So first,

25:48.440 --> 25:53.880
if this person has been using ART drugs for a long time, and we will see if this person needs

25:53.880 --> 25:59.960
to switch the regimen or not. So if the older drug works, and we can just keep using the older drug.

25:59.960 --> 26:04.440
So this is what we will represent as a logistic regression method in the logistic regression

26:04.440 --> 26:10.600
model. As long as all the health measurements are in within the normal range, and then we will

26:10.600 --> 26:15.160
decide to just, you know, keep the old drug. And if one of them is not in the normal range,

26:15.160 --> 26:21.720
we will switch. If this person needs to switch, and then we will need to generate a new regimen.

26:22.280 --> 26:28.520
And because we used the three representation, and then we can now decide, you know, if we need to

26:28.520 --> 26:33.240
switch how to generate a new regimen, essentially, we need to decide like which drug class and how

26:33.240 --> 26:38.600
many drugs use the initial class and also which individual drugs at each class. So this essentially

26:38.600 --> 26:43.080
it's a non-central hypergeometric distribution. Again, I skipped all the details. It's kind of

26:43.080 --> 26:51.560
a little bit complex. So we have these three levels of three decisions. Okay, so now we have already

26:51.560 --> 26:55.880
finished these three steps. So we have multivariate Gaussian process to some whole future states,

26:55.880 --> 27:00.760
and we define reward function. And then we have ways to prioritize a policy function. We can use

27:00.760 --> 27:08.040
a policy gradient method to optimize a print. Okay. Okay, so now, so here I finish all the

27:08.040 --> 27:14.680
matter introduction. Last part of the slides is I will introduce real data analysis results.

27:15.720 --> 27:22.120
So for the real data, we got about 300 women from the Washington DC site from the white study.

27:23.000 --> 27:29.400
And also now we get four state variables at each visit, depression, viral load, EGFR, and BMI.

27:29.400 --> 27:34.360
And there are about 8% missing data. And the baseline covariates we consider include age,

27:34.360 --> 27:40.680
smoking status, substance use, employment status, hypertension, diabetes. And in this study,

27:40.680 --> 27:48.280
we have 31 ART drugs and six drug classes. And we choose 105 representative drug regiments. So

27:48.280 --> 27:53.400
those regiments based on the popularity of the drug combinations, if they have been used a lot

27:53.400 --> 27:58.600
of times for the from the patients, and then we would know that as representative ART regimen.

27:59.160 --> 28:05.960
Okay, so here is one hypothetical patient. So we'll use this example to demonstrate the precision

28:05.960 --> 28:13.000
medicine, the, you know, the utility of the clinical utility of the proposed methods. Okay,

28:13.000 --> 28:21.000
so this person has been has been had 31 visits. And here is their history of treatments. And for

28:21.000 --> 28:28.920
these patients, we assume their weights as one third, one third, one third. Okay, and then we run

28:28.920 --> 28:35.480
our optimization method. And here we can see the expected reward versus the SGD iteration. So it

28:35.480 --> 28:42.440
became relatively stable after 1,000 iterations. And here is optimal regimen is the next two years.

28:42.440 --> 28:47.800
So we can see at the visit to there are two ART drugs when you see one PR and one poster,

28:47.800 --> 28:55.720
and then it changes one new regimen for visits 33 to 35. Okay, and also here I want to show under

28:55.720 --> 29:01.480
their estimated optimal regiments, that's the predicted depression stores. And we can see for

29:01.480 --> 29:09.320
their visits from 32 to 36, they're about 23% improvement of depression. So that also shows

29:09.320 --> 29:14.680
the clinical utility of our, you know, newly assigned optimal drug combinations. Okay, I will

29:14.680 --> 29:25.480
skip the next example due to the time limit. Oh, yeah. Okay, to summarize, we, we propose a like

29:25.480 --> 29:30.680
a Bayesian reforcing learning approach is a two step approach. And it can learn their dynamics

29:30.680 --> 29:36.120
with uncertainty quantification, it can also assign the long term optimal drug combinations to

29:37.320 --> 29:41.000
optimize each individual's health. Okay, yeah, thank you.

29:41.240 --> 29:47.880
Thank you so much, Yanxun. Any questions from the audience?

29:48.920 --> 29:54.840
Yanxun, those are very exciting work. I actually have some questions because you touched a very

29:54.840 --> 30:02.520
good point where you need to balance the priority like competing priorities when you are in the

30:02.520 --> 30:06.840
clinical practice. But since we are a little bit over time, so I probably will talk to you later

30:06.840 --> 30:14.440
about that. I was wondering like how the uncertainty will be impacted by how you

30:14.440 --> 30:23.320
define your reward function. Oh, yeah. So the uncertainty part, you know, how the uncertainty

30:23.320 --> 30:28.600
affects the final decision depending on how you tune the parameter. So like here, and yeah, I skip

30:28.600 --> 30:32.840
that part, but here you can show if we have different tooling parameters like lambda equal

30:32.840 --> 30:38.440
zero, you don't penalize at all. And then you have this drug combination. And if you use like

30:38.440 --> 30:43.240
increase the lambda, and then it will penalize the uncertainty, it's kind of uncertainties

30:43.240 --> 30:48.600
reflected by the sample size in their data. If this drug combination has has been used a lot of

30:48.600 --> 30:54.040
times, it has relatively narrow uncertainty, it had never been used, then it has a lot of

30:54.040 --> 30:57.960
uncertainty. So for example, here with lambda equal zero, they actually recommend this drug

30:57.960 --> 31:03.640
combination is the first recommendation. So it actually never been used in the in the data.

31:03.640 --> 31:08.440
So that's kind of create a trade off, like we need to discuss the clinician, like if this drug

31:08.440 --> 31:13.160
combination has never been used, do you want to try this new drug, or you want more conservative

31:13.160 --> 31:18.760
choices, like, you know, these two drug combinations, it right, it has been used more times. Yeah,

31:18.760 --> 31:22.920
I mean, this this tuning parameter definitely plays a role. But you know, actually, when you

31:22.920 --> 31:28.680
define your reward function, there is another part with the personalized weights. So I was wondering,

31:28.680 --> 31:35.240
like, no, we also have similar problems. So we also have like, for example, the survival or

31:35.240 --> 31:40.440
quality of life to balance. But then when we provide the personalized weights, and if you

31:40.440 --> 31:46.520
change a little bit of the weights, actually, the rules or the decision you will make, or you learn

31:46.520 --> 31:54.760
from the data will change. So yeah, that's a good question. Yeah, we can we can discuss more

31:54.760 --> 32:00.920
details. Yeah. Thank you so much. I'm going to share my screen.

