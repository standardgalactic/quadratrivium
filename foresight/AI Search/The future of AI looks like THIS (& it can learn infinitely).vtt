WEBVTT

00:00.000 --> 00:03.840
AI, as we know it today, is actually quite dumb.

00:03.840 --> 00:10.320
Yes, this includes chatGPT, stable diffusion, Sora, and all the other state-of-the-art models

00:10.320 --> 00:12.200
that we have right now.

00:12.200 --> 00:16.880
They're still very incapable and inefficient, and the future generation of AI will look

00:16.880 --> 00:19.560
very different from what we have now.

00:19.560 --> 00:24.840
So in this video, I'm going to explain why the current generation is so limited and

00:24.840 --> 00:28.560
what the future generation of AI will look like.

00:28.560 --> 00:31.440
First we need to understand the mechanics of AI.

00:31.440 --> 00:37.400
As we know today, all AI is based on the neural network, which is designed based on the human

00:37.400 --> 00:38.400
brain.

00:38.400 --> 00:44.000
This is basically a network of nodes in which information flows through from one end to

00:44.000 --> 00:45.000
the other.

00:45.000 --> 00:50.880
Now, this is going to be a very simplified explanation of how a neural network works.

00:50.880 --> 00:56.400
I'm explaining this for people without a technical background in AI, so if you do have

00:56.400 --> 00:59.680
experience in AI, feel free to skip this section.

00:59.680 --> 01:05.320
Each dot in a neural network is called a node or neuron, and each line of nodes is

01:05.320 --> 01:06.880
called a layer.

01:06.880 --> 01:11.700
You might have heard of the term deep learning, or deep neural networks.

01:11.700 --> 01:16.760
This is basically a neural network with many layers, hence it is very deep.

01:16.760 --> 01:21.200
Each node determines how much information flows through to the next layer.

01:21.200 --> 01:23.400
Now again, this is an oversimplification.

01:23.400 --> 01:28.080
There are a lot of settings like weights and biases and activation functions, but basically

01:28.080 --> 01:33.600
just think of this neural network as a series of dials and knobs, which determine how much

01:33.600 --> 01:37.120
information flows through to the next layer.

01:37.120 --> 01:38.720
Here's a simple example.

01:38.720 --> 01:44.240
Let's say we have this neural network, which is designed to determine whether an image

01:44.240 --> 01:46.160
is a cat or a dog.

01:46.160 --> 01:50.960
For its input, we would feed it an image of a cat or a dog, and this image would be broken

01:50.960 --> 01:57.000
down into data, also known as tokens, which are then fed through this neural network.

01:57.000 --> 02:02.360
Eventually after the data flows through all these layers, it reaches the end layer, which

02:02.360 --> 02:06.560
would conclude whether the image is a cat or a dog.

02:06.560 --> 02:08.960
Now what about training a model?

02:08.960 --> 02:10.240
How does that work?

02:10.240 --> 02:15.280
Well, a neural network needs to undergo usually millions of rounds of training to learn how

02:15.280 --> 02:16.280
to do something.

02:16.280 --> 02:20.840
Here's an example of how one round of training would look like.

02:20.840 --> 02:26.320
Let's say you input an image of a dog, and then this image would be broken down into data,

02:26.320 --> 02:30.120
which flows through this neural network, and it spits out the answer.

02:30.120 --> 02:31.120
This is a dog.

02:31.120 --> 02:36.080
Well, in that case, since it got the answer correct, it's likely that these dials and

02:36.080 --> 02:40.560
knobs, which we can also refer to as weights, are set correctly.

02:40.560 --> 02:45.320
If it gets the answer right, well, we don't really need to tweak these weights further.

02:45.320 --> 02:47.440
However, what if it gets it wrong?

02:47.440 --> 02:49.680
What if it says that this is a cat?

02:49.680 --> 02:53.000
Well, in that case, it would incur a penalty.

02:53.000 --> 02:57.720
And this penalty would cause the weights in this neural network to be updated so that

02:57.720 --> 03:00.760
this penalty would be minimized in the future.

03:00.760 --> 03:05.440
Specifically, the weights would be updated from the last layer to the next layer back

03:05.440 --> 03:10.280
to the next layer back in a process which is called back propagation all the way until

03:10.280 --> 03:13.080
it reaches the first layer of nodes.

03:13.080 --> 03:16.240
And usually one round of training isn't good enough.

03:16.240 --> 03:20.840
So the network would undergo millions of rounds of training where the weights would

03:20.840 --> 03:26.280
be slightly tweaked to minimize the penalty incurred from any errors.

03:26.280 --> 03:31.760
And this goes on and on until finally, we reach the right configuration of dials and

03:31.760 --> 03:37.240
knobs so that this neural network can very accurately determine whether any image is

03:37.240 --> 03:39.000
a cat or a dog.

03:39.000 --> 03:43.280
And this is how AI models that we know today are trained as well.

03:43.280 --> 03:49.960
So for example, GPT is basically a neural network, but these dials and knobs are optimized

03:49.960 --> 03:52.720
for understanding natural language.

03:52.720 --> 03:57.160
Stable diffusion is another neural network where the dials and knobs are optimized for

03:57.160 --> 03:58.960
image generation.

03:58.960 --> 04:05.460
Now again, this is very much an oversimplification and the architecture or basically the design

04:05.460 --> 04:08.560
of the neural network is also very important.

04:08.560 --> 04:11.880
For example, how many layers should we have?

04:11.880 --> 04:15.400
How many nodes in each layer should we have?

04:15.400 --> 04:21.680
There are also many different architectures, such as the transformer model for large language

04:21.680 --> 04:29.080
models or LSTM for time series data or convolutional neural networks for object detection and image

04:29.080 --> 04:30.640
classification.

04:30.640 --> 04:36.120
But in a nutshell, the backbone behind all these AI models is just a neural network,

04:36.120 --> 04:41.160
which has a preconfigured set of dials and knobs to do the job accurately.

04:41.160 --> 04:45.520
So now that you understand how the current generation of AI works, let's look at the

04:45.520 --> 04:48.200
biggest limitations of this.

04:48.200 --> 04:53.400
First of all, once the model is finished training, the weights or basically these dials and knobs

04:53.400 --> 04:55.120
are fixed in value.

04:55.120 --> 05:00.760
When the user asks chat GPT something or when the user uses stable diffusion to generate

05:00.760 --> 05:04.320
an image, these dials and knobs do not change in value.

05:04.320 --> 05:08.360
In other words, all the AI models that we have today are fixed.

05:08.360 --> 05:13.080
Think of this as a brain that cannot learn or get any smarter.

05:13.080 --> 05:19.160
For example, GPT-4 cannot continue learning and become smarter and smarter with time.

05:19.160 --> 05:26.160
If we want a smarter model, well, we need to train a new generation of GPT such as GPT-4.0

05:26.160 --> 05:29.120
or GPT-5 or whatever you want to call it.

05:29.120 --> 05:30.720
Same with stable diffusion.

05:30.720 --> 05:36.040
For example, stable diffusion 2 cannot get smarter and generate better images as we use

05:36.040 --> 05:37.180
it more and more.

05:37.180 --> 05:42.780
In order for it to improve, we currently need to train a new generation, also known as stable

05:42.780 --> 05:44.280
diffusion 3.

05:44.280 --> 05:49.540
And once stable diffusion 3 is finished training, well, that's as smart as it gets.

05:49.540 --> 05:53.540
And if you don't think it's good enough, well, you need to train a new model.

05:53.540 --> 05:59.220
So basically all the AI models that we have today are fixed in their intelligence and

05:59.220 --> 06:00.420
their capabilities.

06:00.420 --> 06:06.420
Again, think of this as a brain that has stopped growing and cannot learn or get smarter.

06:06.420 --> 06:08.900
But this is not how the human brain works.

06:08.900 --> 06:14.540
There's a term called neuroplasticity, which refers to how the brain can reorganize or

06:14.540 --> 06:20.820
reconfigure itself by forming new neural connections over time in order to adapt to new environments

06:20.820 --> 06:22.140
or learn new things.

06:22.140 --> 06:26.780
And that's exactly what the next generation of AI can do, which we'll talk about in a

06:26.780 --> 06:27.780
second.

06:27.780 --> 06:31.220
But there's another huge limitation of current AI models.

06:31.220 --> 06:35.500
They are extremely inefficient and compute intensive.

06:35.500 --> 06:40.100
As you may know, AI is designed based on the architecture of the human brain.

06:40.100 --> 06:44.540
So let's compare it to the efficiency of the human brain right now.

06:44.540 --> 06:49.920
GPT-3 has 175 billion parameters.

06:49.920 --> 06:56.300
This was trained using thousands of GPUs over several weeks or several months.

06:56.300 --> 07:05.060
The total power required for training GPT-3 was estimated to be around 1287 megawatt hours

07:05.460 --> 07:06.580
of electricity.

07:06.580 --> 07:12.660
This is roughly equivalent to the monthly electricity consumption of 1500 homes in the

07:12.660 --> 07:13.660
USA.

07:13.660 --> 07:18.140
Now, keep in mind GPT-3 was completed in 2020.

07:18.140 --> 07:19.700
That's four years ago.

07:19.700 --> 07:23.140
The latest version, GPT-4, is closed source.

07:23.140 --> 07:27.980
So we don't actually know its architecture or how long it took to train.

07:27.980 --> 07:35.020
But we do know that it has around 1.76 trillion parameters, 10 times more than GPT-3.

07:35.020 --> 07:40.860
Keep in mind that the amount of computations required scales exponentially as the parameter

07:40.860 --> 07:42.620
size gets larger.

07:42.620 --> 07:49.940
So from a rough calculation, GPT-4 could have taken around 41,000 megawatt hours of

07:49.940 --> 07:51.780
energy to train.

07:51.780 --> 07:57.820
That's enough energy to power around 47,000 homes in the US for a month.

07:57.820 --> 08:04.220
The compute used to create these state-of-the-art models that we know today, such as GPT-4

08:04.220 --> 08:11.020
Clawd-3 or Gemini 1.5 Pro, requires massive data centers and a lot of energy.

08:11.020 --> 08:16.140
That's why tech giants are scrambling to invest and build even bigger data centers,

08:16.140 --> 08:19.900
because they know that compute is the main limitation here.

08:19.900 --> 08:26.220
And that's exactly why Microsoft and OpenAI are planning a $100 billion Stargate project

08:26.220 --> 08:29.060
to build the biggest data center in the world.

08:29.060 --> 08:31.740
All of this is for more compute.

08:31.740 --> 08:34.500
Now contrast this to the human brain.

08:34.500 --> 08:40.420
Some might say the human brain is still more intelligent than GPT-4, at least in some regards.

08:40.420 --> 08:48.020
The human brain only uses 175 kilowatt hours in an entire year, and it gets this energy

08:48.020 --> 08:50.980
in the form of calories from the food we eat.

08:50.980 --> 08:58.980
So training GPT-4 is estimated to require approximately 234,000 times more energy than

08:58.980 --> 09:02.460
what the human brain uses in an entire year.

09:02.460 --> 09:08.100
In other words, the energy required to train GPT-4 once could power the human brain for

09:08.100 --> 09:12.060
over 234,000 years.

09:12.060 --> 09:16.300
Now I gave this comparison to show you that there's something fundamentally wrong with

09:16.300 --> 09:18.020
AI models today.

09:18.020 --> 09:23.100
They are very energy inefficient, and they take up a lot of compute.

09:23.100 --> 09:26.660
It's not even close to the efficiency of the human brain.

09:26.660 --> 09:31.500
So the next generation of AI has to solve this inefficiency problem as well, otherwise

09:31.500 --> 09:33.180
it will not be sustainable.

09:33.180 --> 09:39.020
So to summarize the major limitations of current AI models is number one, they are fixed and

09:39.020 --> 09:44.780
unable to improve or learn further after being trained, and number two, they're also very

09:44.780 --> 09:47.580
energy intensive and inefficient.

09:47.580 --> 09:51.900
These are the two biggest problems of the current generation of AI.

09:51.900 --> 09:54.300
Now let's enter the next generation.

09:54.300 --> 09:58.900
We aren't there yet, but there are a few possible architectures that are being discussed

09:58.900 --> 10:01.020
and developed as we speak.

10:01.020 --> 10:04.660
The first architecture is called liquid neural networks.

10:04.660 --> 10:10.460
Now liquid neural networks are designed to mimic the flexibility or the plasticity of

10:10.460 --> 10:11.740
the human brain.

10:11.740 --> 10:17.620
The human brain is very flexible and can reorganize or reconfigure itself over time,

10:17.620 --> 10:23.420
and this ability allows the brain to adapt to new situations or learn new skills or compensate

10:23.420 --> 10:25.540
for injury and disease.

10:25.540 --> 10:30.900
For example, when you learn something new, your brain changes structurally and functionally

10:30.900 --> 10:33.220
to accommodate the new information.

10:33.220 --> 10:37.820
Learning a new language can lead to changes in the brain's structure and function, such

10:37.820 --> 10:41.780
as increased density of gray matter in the left hemisphere.

10:41.780 --> 10:46.020
The brain can also reconfigure itself to recover from injury.

10:46.020 --> 10:51.820
For example, after a traumatic brain injury, physical therapy and cognitive exercises can

10:51.820 --> 10:56.140
help rewire the brain to regain lost functions.

10:56.140 --> 11:00.860
And for people who've lost a sense, like sight or hearing, the brain will reorganize

11:00.860 --> 11:06.140
itself to compensate for the loss and make other senses become more acute.

11:06.140 --> 11:12.140
So this flexibility, this plasticity is exactly what liquid neural networks are designed to

11:12.140 --> 11:13.460
have.

11:13.460 --> 11:17.300
Liquid neural networks can adapt in real time to new data.

11:17.300 --> 11:23.060
This means that the configuration of the neural network can change as it receives new inputs,

11:23.060 --> 11:24.980
and that's why it's called liquid.

11:24.980 --> 11:30.220
These connections in the network and these dials and knobs are fluid, so they can change

11:30.220 --> 11:32.780
dynamically over time.

11:32.780 --> 11:38.260
Liquid neural networks also retain what they have learned while incorporating new information.

11:38.260 --> 11:43.660
This is similar to how our brains can remember old information while learning new things.

11:43.660 --> 11:46.860
So here's how liquid neural networks work.

11:46.860 --> 11:51.420
They have three main components, much like a traditional neural network.

11:51.420 --> 11:54.980
It has an input layer, which receives the input data.

11:54.980 --> 11:59.940
But then in the middle, we have this liquid layer, otherwise known as a reservoir.

11:59.940 --> 12:03.100
This is the core component of a liquid neural network.

12:03.100 --> 12:06.620
And it's basically a large recurrent neural network.

12:06.620 --> 12:12.460
Think of this as a big bowl of water in which each splash creates a ripple.

12:12.460 --> 12:17.540
These ripples are basically the neurons in this network reacting to inputs.

12:17.540 --> 12:23.700
The reservoir acts as a dynamic system that transforms the input data into a high dimensional

12:23.700 --> 12:26.700
representation called reservoir states.

12:26.700 --> 12:33.060
And this reservoir's rich dynamics and transformations capture the complex temporal patterns in the

12:33.060 --> 12:34.460
input data.

12:34.460 --> 12:37.140
And then finally, we have the output layer.

12:37.140 --> 12:42.380
This layer receives the reservoir states and maps them to the desired output using what

12:42.380 --> 12:44.620
is called a readout function.

12:44.620 --> 12:49.580
In layman terms, this is a layer that looks at the ripples in the reservoir and tries

12:49.580 --> 12:52.060
to understand what it all means.

12:52.060 --> 12:57.780
It takes the dynamic patterns from the reservoir and makes predictions or decisions from it.

12:57.780 --> 13:04.620
The key aspect of liquid neural networks is this reservoir layer, which remains untrained

13:04.620 --> 13:07.340
during the entire learning process.

13:07.340 --> 13:12.900
Finally the output layer is trained to map the reservoir states to the target outputs.

13:12.900 --> 13:15.540
In other words, to understand what these ripples mean.

13:15.540 --> 13:20.660
And because this reservoir remains fluid and flexible throughout time, it's not fixed

13:20.660 --> 13:25.740
in value, that allows this liquid neural network to basically adapt to new data and learn

13:25.740 --> 13:27.220
new things.

13:27.220 --> 13:30.260
Here's how you would train a liquid neural network.

13:30.260 --> 13:35.240
The connections between neurons in the reservoirs are set up randomly at the start.

13:35.240 --> 13:40.000
These connections typically stay the same and don't change during training.

13:40.000 --> 13:42.400
Next you would feed the input layer some data.

13:42.400 --> 13:47.380
And when this data is broken down into tokens and it reaches the reservoir layer, it causes

13:47.380 --> 13:52.640
the neurons in the reservoir to react and create complex patterns, much like ripples

13:52.640 --> 13:53.640
in water.

13:53.640 --> 13:58.680
So as this input data creates ripples, you basically observe and analyze the patterns

13:58.680 --> 14:01.040
created in the reservoir over time.

14:01.040 --> 14:03.360
And that's exactly what the readout layer does.

14:03.360 --> 14:05.720
It learns to recognize these patterns.

14:05.720 --> 14:10.440
It's like learning, aha, this is what caused this type of ripple and that is what caused

14:10.440 --> 14:11.720
this other type of ripple.

14:11.720 --> 14:16.180
And eventually after lots and lots of rounds of training, the readout layer can make accurate

14:16.180 --> 14:19.200
predictions based on observed patterns.

14:19.200 --> 14:24.440
Again note that only the readout layer is trained, which is simpler and faster because

14:24.440 --> 14:27.200
you're not adjusting anything in the reservoir layer.

14:27.200 --> 14:32.960
This is much quicker and needs less compute compared to traditional neural networks.

14:32.960 --> 14:37.240
That's because in neural networks that we know today, all the weights including those

14:37.240 --> 14:39.520
in the hidden layers are trainable.

14:39.520 --> 14:44.680
This means more parameters to optimize leading to longer training times and higher computational

14:44.680 --> 14:45.880
requirements.

14:45.880 --> 14:52.280
But in liquid neural networks, you don't adjust the weights of the reservoir during training.

14:52.280 --> 14:54.620
Only the readout layer is trained.

14:54.620 --> 15:00.480
And this significantly reduces the computational burden during training since fewer parameters

15:00.480 --> 15:02.240
need to be optimized.

15:02.240 --> 15:04.680
Plus it's a lot faster to train.

15:04.680 --> 15:07.280
Thanks to our sponsor Bright Data.

15:07.280 --> 15:12.560
Bright Data is an all-in-one platform designed to help businesses collect high quality web

15:12.560 --> 15:14.520
data at scale.

15:14.520 --> 15:19.360
This is especially useful for AI companies which require huge amounts of diverse and

15:19.360 --> 15:25.400
high quality training data to build robust and unbiased AI models.

15:25.400 --> 15:30.400
Collecting this training data manually can be time consuming and prone to errors.

15:30.400 --> 15:32.520
And that's where Bright Data comes in.

15:32.520 --> 15:37.640
With Bright Data, you can access high volume, high quality web data effortlessly.

15:37.640 --> 15:44.040
From parsed validated data sets to custom scraping solutions, they've got you covered.

15:44.040 --> 15:48.720
Get parsed and cleaned data sets ready to use on demand.

15:48.720 --> 15:54.200
Customize any data set to fit your specific needs and benefit from reliable delivery and

15:54.200 --> 15:56.080
full compliance.

15:56.080 --> 16:03.160
In fact every 15 minutes their customers scrape enough data to train chat GPT from scratch.

16:03.160 --> 16:05.360
That's a lot of data, to say the least.

16:05.360 --> 16:11.560
They have many tools like the web scraper API, the proxy manager, and unblocking technologies

16:11.560 --> 16:17.520
to help automate your data scraping at scale, allowing you to build reliable data sets to

16:17.520 --> 16:20.720
train any AI or LLM.

16:20.720 --> 16:23.840
Visit the link in the description below to learn more.

16:23.840 --> 16:28.240
It's a lot faster for these liquid neural networks to converge at an optimum.

16:28.240 --> 16:33.660
And because of this reservoir where the weights and configurations can change dynamically depending

16:33.660 --> 16:38.240
on the data that you feed it, liquid neural networks can potentially be much smaller than

16:38.240 --> 16:42.320
traditional neural networks which have fixed weights and connections.

16:42.320 --> 16:45.560
And this offers a lot more efficient learning and inference.

16:45.560 --> 16:51.280
So for example, researchers at MIT were able to pilot a drone using a liquid neural network

16:51.360 --> 16:57.200
with only 20,000 parameters, which is very tiny compared to state-of-the-art AI models

16:57.200 --> 17:01.800
such as GPT4, which often have over a trillion parameters.

17:01.800 --> 17:02.800
Just think about that.

17:02.800 --> 17:06.880
20,000 parameters versus over a trillion parameters.

17:06.880 --> 17:12.600
So these smaller sizes generally translate to faster inference and lower computational

17:12.600 --> 17:14.200
requirements.

17:14.200 --> 17:17.560
Liquid neural networks are also way less memory intensive.

17:17.560 --> 17:23.240
Again, since you don't train the reservoir's weights, memory usage is much lower during

17:23.240 --> 17:27.880
training compared to traditional neural networks where the gradients and the parameters for

17:27.880 --> 17:30.840
all layers must be stored in memory.

17:30.840 --> 17:36.200
Liquid neural networks are particularly good at processing temporal data due to their dynamic

17:36.200 --> 17:37.200
reservoir.

17:37.200 --> 17:41.680
So they excel in tasks that involve complex time series data.

17:41.680 --> 17:46.440
Now you might be wondering, well, how can these liquid neural networks actually be applied

17:46.440 --> 17:47.800
in the real world?

17:47.800 --> 17:49.840
So here are some use cases.

17:49.840 --> 17:55.280
As we race to build fully autonomous AI robots, these robots will be deployed in the real

17:55.280 --> 18:00.200
world and oftentimes they might encounter situations that they've never seen before

18:00.200 --> 18:01.600
during training.

18:01.600 --> 18:06.720
For example, there could be unpredictable environments in search and rescue missions,

18:06.720 --> 18:11.320
but with liquid neural networks, these robots can adapt to changing conditions and learn

18:11.320 --> 18:13.200
new tasks on the fly.

18:13.200 --> 18:17.640
And eventually we're going to have these autonomous robots in our houses helping us do chores

18:17.640 --> 18:18.880
and other tasks.

18:18.880 --> 18:23.240
But maybe you have a certain way of folding clothes or doing the laundry or cooking that

18:23.240 --> 18:25.080
the robot was never trained on.

18:25.080 --> 18:29.820
So with a traditional neural network, these robots aren't able to learn new skills after

18:29.820 --> 18:31.200
being deployed.

18:31.200 --> 18:36.360
But with liquid neural networks built into a humanoid robot, it can learn new tasks that

18:36.360 --> 18:37.360
you teach it.

18:37.360 --> 18:40.720
And this robot will become a lot more personalized for you.

18:40.720 --> 18:42.960
And then we have autonomous driving.

18:42.960 --> 18:47.200
There's no doubt that self-driving cars will eventually become the future.

18:47.200 --> 18:52.040
But current technologies still do not perform well, especially in challenging environments

18:52.040 --> 18:53.040
or new conditions.

18:53.040 --> 18:58.680
Again, this is because traditional neural networks can only do well on data that they

18:58.680 --> 18:59.680
were trained on.

18:59.680 --> 19:02.200
They're not able to adapt to new environments.

19:02.200 --> 19:07.000
But with liquid neural networks, autonomous vehicles can navigate complex and dynamic

19:07.000 --> 19:12.400
environments by continuously learning and training from sensor data and adjusting their

19:12.400 --> 19:14.200
behavior accordingly.

19:14.200 --> 19:17.320
It's constantly training and improving over time.

19:17.320 --> 19:23.120
Now as I've mentioned before, liquid neural networks often incorporate recurrent connections,

19:23.120 --> 19:27.040
making them suitable for processing time series data.

19:27.040 --> 19:31.440
So it's great for things like weather prediction and of course, stock trading.

19:31.440 --> 19:37.180
The stock market is filled with ever-changing trends and cycles, so it's close to impossible

19:37.180 --> 19:41.280
for one fixed algorithm or formula to beat the market.

19:41.280 --> 19:46.680
However, because liquid neural networks can adapt to ever-changing data, it can optimize

19:46.680 --> 19:50.560
trading strategies in real time to maximize profits.

19:50.560 --> 19:55.280
In other words, you could be constantly streaming the latest market data to this liquid neural

19:55.280 --> 20:00.240
network, which would change its configuration to adapt to this data in real time to help

20:00.240 --> 20:02.400
you maximize profits.

20:02.400 --> 20:04.800
Another use case would be healthcare.

20:04.800 --> 20:10.360
Liquid neural networks can be used in wearable devices to monitor patients in real time,

20:10.360 --> 20:15.360
adapting to changes in the patient's conditions and predicting potential health issues before

20:15.360 --> 20:17.040
they become critical.

20:17.040 --> 20:22.480
In cybersecurity, liquid neural networks can continuously learn from network traffic and

20:22.480 --> 20:28.960
user behavior to adapt access control policies and detect anomalies or unauthorized access

20:28.960 --> 20:30.400
attempts.

20:30.400 --> 20:34.840
Yet another use case would be streaming services such as Netflix.

20:34.840 --> 20:40.280
They can use liquid neural networks to adapt to each user's viewing habits and preferences,

20:40.280 --> 20:43.720
providing more personalized content recommendations.

20:43.720 --> 20:46.600
Another use case would be smart city management.

20:46.600 --> 20:51.720
For example, liquid neural networks can optimize traffic flow in real time by learning from

20:51.720 --> 20:57.840
traffic patterns and changing traffic lights accordingly to reduce congestion and improve

20:57.840 --> 20:59.200
efficiency.

20:59.200 --> 21:01.960
Energy management is also very relevant.

21:01.960 --> 21:06.680
Smart grids can use liquid neural networks to balance power, supply, and demand in real

21:06.680 --> 21:12.440
time, improving efficiency and reducing costs by adapting to consumption patterns.

21:12.440 --> 21:18.000
However, although liquid neural networks seem promising, it does have its limitations.

21:18.000 --> 21:23.160
This is still a relatively new concept in the field of neural networks and research on

21:23.160 --> 21:28.560
them is still in its early stages compared to more traditional architectures.

21:28.560 --> 21:33.800
While liquid neural networks show promising theoretical benefits such as the ability to

21:33.800 --> 21:39.160
process continuous data streams and adapt on the fly, there is still a lack of real

21:39.160 --> 21:43.720
world results demonstrating their superiority on a large scale.

21:43.720 --> 21:48.800
Many researchers are likely waiting for more compelling benchmark results before investing

21:48.800 --> 21:52.160
significant effort into liquid neural networks.

21:52.160 --> 21:58.240
Also as I mentioned previously, they're particularly suited for temporal or sequence data.

21:58.240 --> 22:04.080
Also for tasks that do not involve time such as identifying images of cats or dogs, traditional

22:04.080 --> 22:08.960
neural networks might actually be more effective and straightforward to implement.

22:08.960 --> 22:14.760
Also the dynamics within this reservoir layer can be very complex and difficult to interpret

22:14.760 --> 22:19.680
and this makes it challenging to understand how the reservoir processes these inputs.

22:19.680 --> 22:22.960
It would be quite hard to fine tune it for optimal performance.

22:22.960 --> 22:29.040
Finally, there is a lack of standardized support and fewer established frameworks for liquid

22:29.040 --> 22:32.000
neural networks compared to traditional neural networks.

22:32.000 --> 22:36.440
And this can make implementation and experimentation more challenging.

22:36.440 --> 22:41.800
So all in all, liquid neural networks are still a very early concept and an area of active

22:41.800 --> 22:42.800
research.

22:42.800 --> 22:47.520
Unlike traditional neural networks that are fixed and need to be retrained with a large

22:47.520 --> 22:52.400
data set to learn new information, liquid neural networks can update their knowledge

22:52.400 --> 22:55.400
incrementally with each new piece of data.

22:55.400 --> 23:00.560
This offers a flexible and adaptive model which could potentially become infinitely

23:00.560 --> 23:02.720
smarter over time.

23:02.720 --> 23:08.000
Now liquid neural networks aren't the only possibility that could become the next generation

23:08.000 --> 23:09.000
of AI.

23:09.000 --> 23:13.540
We have another type of neural network which is designed to mimic the human brain even

23:13.540 --> 23:16.400
more than traditional neural networks.

23:16.400 --> 23:19.640
And this brings us to spiking neural networks.

23:19.640 --> 23:26.120
These are closely inspired by the way neurons in our brains communicate using discrete spikes

23:26.120 --> 23:28.080
or action potentials.

23:28.080 --> 23:33.520
You see, in the human brain, which is basically a network of neurons, each neuron doesn't

23:33.520 --> 23:37.800
immediately fire to the next set of neurons when it receives input.

23:37.800 --> 23:43.840
Instead, the input has to build up to a certain threshold and once it passes this threshold,

23:43.840 --> 23:46.000
then it fires to the next set of neurons.

23:46.000 --> 23:49.520
And after it fires, it goes back to its resting state.

23:49.520 --> 23:53.920
Well, spiking neural networks are designed to mimic this behavior.

23:53.920 --> 23:55.920
So here's how it works.

23:55.920 --> 24:00.000
The architecture is quite similar to traditional neural networks.

24:00.000 --> 24:06.800
However, for each neuron, it waits to receive signals or spikes from other neurons.

24:06.800 --> 24:09.800
Think of these spikes as like little electric pulses.

24:09.800 --> 24:15.240
The input data, such as an image or a sound, is turned into the spikes that move through

24:15.280 --> 24:16.760
this neural network.

24:16.760 --> 24:22.120
For example, if it's a loud sound, it might generate more spikes while a quiet sound might

24:22.120 --> 24:24.000
generate fewer spikes.

24:24.000 --> 24:28.480
Now each neuron in the network collects incoming spikes.

24:28.480 --> 24:30.840
Imagine a bucket collecting drops of water.

24:30.840 --> 24:33.440
As more spikes come in, the bucket fills up.

24:33.440 --> 24:38.240
And when the neuron gets enough spikes, in other words, when it reaches a certain threshold,

24:38.240 --> 24:41.000
it fires a spike to the next set of neurons.

24:41.000 --> 24:45.600
And after firing, it resets and starts collecting again from zero.

24:45.600 --> 24:51.160
So instead of using continuous signals like traditional neural networks, spiking neural

24:51.160 --> 24:57.040
networks uses spikes, which are basically bursts of activity at discrete time points

24:57.040 --> 24:59.120
to process information.

24:59.120 --> 25:05.000
In other words, spiking neural networks incorporate time into their processing, with neurons firing

25:05.000 --> 25:08.560
only when their potential exceeds a certain threshold.

25:08.560 --> 25:13.200
Now there are different methods and algorithms to train a spiking neural network, and there

25:13.200 --> 25:16.520
currently isn't a standard wave that's set in stone.

25:16.520 --> 25:19.400
So this is still an active field of research.

25:19.400 --> 25:25.240
One common method is called spike timing dependent plasticity, or STDP.

25:25.240 --> 25:30.520
This method is inspired by how the brain strengthens or weakens connections between neurons.

25:30.520 --> 25:36.920
So if one neuron spikes just before another, then the connection between them gets stronger.

25:36.920 --> 25:40.800
If it spikes just after, then the connection gets weaker.

25:40.800 --> 25:45.840
It's like learning which connections are important based on the timing of the spikes.

25:45.840 --> 25:51.120
And speaking of timing, it's the exact timing of spikes that matters.

25:51.120 --> 25:55.440
It's not just about how many spikes there are, but when they happen.

25:55.440 --> 25:59.960
Now STDP is only one method to train the spiking neural networks.

25:59.960 --> 26:03.440
There are a few other ones, which are beyond the scope of this video.

26:03.440 --> 26:08.000
But like traditional neural networks, spiking neural networks have to undergo millions of

26:08.000 --> 26:12.880
rounds of training with a lot of data, and eventually the configuration of the network

26:12.880 --> 26:16.640
and all its parameters will reach an optimum state.

26:16.640 --> 26:22.400
Now again, I'd like to remind you that this is a very simplified explanation of spiking

26:22.400 --> 26:26.760
neural networks, and I've left out a lot of mathematical details.

26:26.760 --> 26:29.240
But in a nutshell, that's how it works.

26:29.240 --> 26:33.840
So you might be wondering, well, what are the benefits of spiking neural networks?

26:33.840 --> 26:38.800
First of all, it's designed to mimic the human brain even more by implementing this

26:38.800 --> 26:40.160
spiking mechanism.

26:40.160 --> 26:46.160
So in theory, maybe we could reach a superior level of intelligence compared to the current

26:46.160 --> 26:50.240
generation of AI if we mimicked the human brain even more.

26:50.240 --> 26:54.600
But the biggest benefit of spiking neural networks is their efficiency.

26:54.600 --> 26:59.680
If you remember at the beginning of the video, I compared the energy consumption of the human

26:59.680 --> 27:06.200
brain versus a current state of the art model like GPT-4, which requires huge data centers

27:06.200 --> 27:08.160
and huge amounts of compute.

27:08.160 --> 27:12.200
That's because traditional neural networks are always active.

27:12.200 --> 27:16.640
Each input of data activates the entire neural network.

27:16.640 --> 27:22.680
So you have to do an insane amount of matrix multiplications across the entire network

27:22.760 --> 27:25.680
just to do one round of training or inference.

27:25.680 --> 27:31.080
However, for spiking neural networks, they only use energy where spikes occur, while

27:31.080 --> 27:34.240
the rest of the neural network remains inactive.

27:34.240 --> 27:37.040
This makes it a lot more energy efficient.

27:37.040 --> 27:42.680
Plus, spiking neural networks are particularly suitable for neuromorphic chips which are

27:42.680 --> 27:45.400
designed to mimic the human brain.

27:45.400 --> 27:51.200
Now, neuromorphic chips are a huge topic and deserves its own full video.

27:51.200 --> 27:55.440
So let me know in the comments if you'd like me to make a video on this as well.

27:55.440 --> 28:01.240
So how can these spiking neural networks actually be applied to the real world?

28:01.240 --> 28:08.080
Well, because these neural networks can encode and process information in the timing of spikes,

28:08.080 --> 28:11.320
this is great for processing temporal data.

28:11.320 --> 28:18.000
This makes them great for adaptive and autonomous systems, plus this spike timing-dependent

28:18.000 --> 28:23.880
plasticity, which I mentioned before, where the timing of the spikes influences the strength

28:23.880 --> 28:26.000
of the connections in the network.

28:26.000 --> 28:29.840
This can lead to more robust and adaptive learning capability.

28:29.840 --> 28:35.880
So this dynamic learning can make spiking neural networks suitable for autonomous systems

28:35.880 --> 28:41.280
such as self-driving, where the AI has to learn and adapt to changing environments.

28:41.280 --> 28:46.160
Or it can be used in real-time processing like predicting the stock market or patient

28:46.160 --> 28:50.560
monitoring and personalized medicine, and of course, autonomous robots.

28:50.560 --> 28:56.160
Now, although spiking neural networks offer some huge benefits, especially regarding energy

28:56.160 --> 28:59.480
efficiency, they do have some limitations.

28:59.480 --> 29:04.440
Setting up and programming spiking neural networks is more complicated compared to

29:04.440 --> 29:06.040
traditional neural networks.

29:06.040 --> 29:11.600
This spiking behavior, of course, adds a layer of complexity, making them harder to design

29:11.600 --> 29:13.240
and understand.

29:13.240 --> 29:16.760
Studying spiking neural networks is also quite difficult.

29:16.760 --> 29:21.040
Current neural networks use methods like backpropagation to adjust their parameters,

29:21.040 --> 29:26.360
but this process doesn't work well with these discrete time-based spikes.

29:26.360 --> 29:32.400
Researchers are still trying to find an effective training algorithm for spiking neural networks.

29:32.400 --> 29:37.240
Also given this additional dimension of time, spiking neural networks might actually require

29:37.240 --> 29:40.360
more computational resources to simulate.

29:40.360 --> 29:46.080
This is because they need to track and process spikes over time, which can be computationally

29:46.080 --> 29:47.080
expensive.

29:47.080 --> 29:53.680
Yet, another limitation is that running spiking neural networks efficiently often requires

29:53.680 --> 30:00.040
specialized hardware such as neuromorphic chips, which are not widely available or standardized

30:00.040 --> 30:03.520
compared to conventional computing hardware.

30:03.520 --> 30:08.160
Neuromorphic chips are optimized for this spike-based processing and are still being

30:08.160 --> 30:09.160
developed.

30:09.160 --> 30:15.100
And that's why, for example, Sam Altman is investing millions of dollars into a neuromorphic

30:15.100 --> 30:17.080
chip company called RAIN.

30:17.080 --> 30:22.360
Finally, while spiking neural networks show promising results, especially for time-based

30:22.360 --> 30:28.200
data, they often lag behind current neural networks for non-time-based data.

30:28.200 --> 30:34.840
They often underperform compared with current AI models, particularly for complex tasks.

30:34.840 --> 30:40.160
This is partly due to the challenges in training spiking neural networks effectively.

30:40.160 --> 30:45.320
And as with liquid neural networks, spiking neural networks are also relatively new.

30:45.320 --> 30:51.000
So there are fewer tools and frameworks available for developing spiking neural networks compared

30:51.000 --> 30:53.400
to current AI models.

30:53.400 --> 30:58.160
This makes experimentation and development slower and more difficult.

30:58.160 --> 31:03.520
But anyways, that sums up what could potentially be the next generation of AI.

31:03.520 --> 31:08.960
To bring it all back, the current generation of AI is very energy inefficient, requiring

31:08.960 --> 31:10.920
huge amounts of compute.

31:10.920 --> 31:14.560
Plus, it can't learn new things after being trained.

31:14.560 --> 31:21.160
If we want to achieve AGI or ASI, we need to essentially create something as efficient

31:21.160 --> 31:26.720
and as fluid as the human brain, which can constantly learn new things and adapt to

31:26.720 --> 31:28.800
changing environments.

31:28.800 --> 31:33.520
These are the two essential things that new types of neural networks, such as liquid neural

31:33.520 --> 31:38.200
networks and spiking neural networks can solve, at least in theory.

31:38.200 --> 31:43.720
However, these are still relatively new and they are still being developed, but the potential

31:43.720 --> 31:45.480
could be massive.

31:45.480 --> 31:49.880
Imagine an AI that can keep learning and get infinitely smarter.

31:49.880 --> 31:54.000
Let me know what you think about these neural networks in the comments below.

31:54.000 --> 31:58.160
Things are happening so fast in the world of AI, it's quite hard to keep up with all

31:58.160 --> 32:01.280
the technological innovations that are happening right now.

32:01.280 --> 32:06.000
So if I've missed any other groundbreaking architectures that are worth mentioning, please

32:06.000 --> 32:10.560
let me know in the comments below and I'll try to do a video on that as well.

32:10.560 --> 32:15.520
As always, if you enjoyed this video, remember to like, share, subscribe and stay tuned for

32:15.520 --> 32:17.160
more content.

32:17.160 --> 32:22.000
Also we built a site where you can find all the AI tools out there as well as find jobs

32:22.000 --> 32:24.840
in machine learning, data science and more.

32:24.840 --> 32:27.720
Check it out at ai-search.io.

32:27.720 --> 32:30.320
Thanks for watching and I'll see you in the next one.

