1
00:00:00,000 --> 00:00:03,840
AI, as we know it today, is actually quite dumb.

2
00:00:03,840 --> 00:00:10,320
Yes, this includes chatGPT, stable diffusion, Sora, and all the other state-of-the-art models

3
00:00:10,320 --> 00:00:12,200
that we have right now.

4
00:00:12,200 --> 00:00:16,880
They're still very incapable and inefficient, and the future generation of AI will look

5
00:00:16,880 --> 00:00:19,560
very different from what we have now.

6
00:00:19,560 --> 00:00:24,840
So in this video, I'm going to explain why the current generation is so limited and

7
00:00:24,840 --> 00:00:28,560
what the future generation of AI will look like.

8
00:00:28,560 --> 00:00:31,440
First we need to understand the mechanics of AI.

9
00:00:31,440 --> 00:00:37,400
As we know today, all AI is based on the neural network, which is designed based on the human

10
00:00:37,400 --> 00:00:38,400
brain.

11
00:00:38,400 --> 00:00:44,000
This is basically a network of nodes in which information flows through from one end to

12
00:00:44,000 --> 00:00:45,000
the other.

13
00:00:45,000 --> 00:00:50,880
Now, this is going to be a very simplified explanation of how a neural network works.

14
00:00:50,880 --> 00:00:56,400
I'm explaining this for people without a technical background in AI, so if you do have

15
00:00:56,400 --> 00:00:59,680
experience in AI, feel free to skip this section.

16
00:00:59,680 --> 00:01:05,320
Each dot in a neural network is called a node or neuron, and each line of nodes is

17
00:01:05,320 --> 00:01:06,880
called a layer.

18
00:01:06,880 --> 00:01:11,700
You might have heard of the term deep learning, or deep neural networks.

19
00:01:11,700 --> 00:01:16,760
This is basically a neural network with many layers, hence it is very deep.

20
00:01:16,760 --> 00:01:21,200
Each node determines how much information flows through to the next layer.

21
00:01:21,200 --> 00:01:23,400
Now again, this is an oversimplification.

22
00:01:23,400 --> 00:01:28,080
There are a lot of settings like weights and biases and activation functions, but basically

23
00:01:28,080 --> 00:01:33,600
just think of this neural network as a series of dials and knobs, which determine how much

24
00:01:33,600 --> 00:01:37,120
information flows through to the next layer.

25
00:01:37,120 --> 00:01:38,720
Here's a simple example.

26
00:01:38,720 --> 00:01:44,240
Let's say we have this neural network, which is designed to determine whether an image

27
00:01:44,240 --> 00:01:46,160
is a cat or a dog.

28
00:01:46,160 --> 00:01:50,960
For its input, we would feed it an image of a cat or a dog, and this image would be broken

29
00:01:50,960 --> 00:01:57,000
down into data, also known as tokens, which are then fed through this neural network.

30
00:01:57,000 --> 00:02:02,360
Eventually after the data flows through all these layers, it reaches the end layer, which

31
00:02:02,360 --> 00:02:06,560
would conclude whether the image is a cat or a dog.

32
00:02:06,560 --> 00:02:08,960
Now what about training a model?

33
00:02:08,960 --> 00:02:10,240
How does that work?

34
00:02:10,240 --> 00:02:15,280
Well, a neural network needs to undergo usually millions of rounds of training to learn how

35
00:02:15,280 --> 00:02:16,280
to do something.

36
00:02:16,280 --> 00:02:20,840
Here's an example of how one round of training would look like.

37
00:02:20,840 --> 00:02:26,320
Let's say you input an image of a dog, and then this image would be broken down into data,

38
00:02:26,320 --> 00:02:30,120
which flows through this neural network, and it spits out the answer.

39
00:02:30,120 --> 00:02:31,120
This is a dog.

40
00:02:31,120 --> 00:02:36,080
Well, in that case, since it got the answer correct, it's likely that these dials and

41
00:02:36,080 --> 00:02:40,560
knobs, which we can also refer to as weights, are set correctly.

42
00:02:40,560 --> 00:02:45,320
If it gets the answer right, well, we don't really need to tweak these weights further.

43
00:02:45,320 --> 00:02:47,440
However, what if it gets it wrong?

44
00:02:47,440 --> 00:02:49,680
What if it says that this is a cat?

45
00:02:49,680 --> 00:02:53,000
Well, in that case, it would incur a penalty.

46
00:02:53,000 --> 00:02:57,720
And this penalty would cause the weights in this neural network to be updated so that

47
00:02:57,720 --> 00:03:00,760
this penalty would be minimized in the future.

48
00:03:00,760 --> 00:03:05,440
Specifically, the weights would be updated from the last layer to the next layer back

49
00:03:05,440 --> 00:03:10,280
to the next layer back in a process which is called back propagation all the way until

50
00:03:10,280 --> 00:03:13,080
it reaches the first layer of nodes.

51
00:03:13,080 --> 00:03:16,240
And usually one round of training isn't good enough.

52
00:03:16,240 --> 00:03:20,840
So the network would undergo millions of rounds of training where the weights would

53
00:03:20,840 --> 00:03:26,280
be slightly tweaked to minimize the penalty incurred from any errors.

54
00:03:26,280 --> 00:03:31,760
And this goes on and on until finally, we reach the right configuration of dials and

55
00:03:31,760 --> 00:03:37,240
knobs so that this neural network can very accurately determine whether any image is

56
00:03:37,240 --> 00:03:39,000
a cat or a dog.

57
00:03:39,000 --> 00:03:43,280
And this is how AI models that we know today are trained as well.

58
00:03:43,280 --> 00:03:49,960
So for example, GPT is basically a neural network, but these dials and knobs are optimized

59
00:03:49,960 --> 00:03:52,720
for understanding natural language.

60
00:03:52,720 --> 00:03:57,160
Stable diffusion is another neural network where the dials and knobs are optimized for

61
00:03:57,160 --> 00:03:58,960
image generation.

62
00:03:58,960 --> 00:04:05,460
Now again, this is very much an oversimplification and the architecture or basically the design

63
00:04:05,460 --> 00:04:08,560
of the neural network is also very important.

64
00:04:08,560 --> 00:04:11,880
For example, how many layers should we have?

65
00:04:11,880 --> 00:04:15,400
How many nodes in each layer should we have?

66
00:04:15,400 --> 00:04:21,680
There are also many different architectures, such as the transformer model for large language

67
00:04:21,680 --> 00:04:29,080
models or LSTM for time series data or convolutional neural networks for object detection and image

68
00:04:29,080 --> 00:04:30,640
classification.

69
00:04:30,640 --> 00:04:36,120
But in a nutshell, the backbone behind all these AI models is just a neural network,

70
00:04:36,120 --> 00:04:41,160
which has a preconfigured set of dials and knobs to do the job accurately.

71
00:04:41,160 --> 00:04:45,520
So now that you understand how the current generation of AI works, let's look at the

72
00:04:45,520 --> 00:04:48,200
biggest limitations of this.

73
00:04:48,200 --> 00:04:53,400
First of all, once the model is finished training, the weights or basically these dials and knobs

74
00:04:53,400 --> 00:04:55,120
are fixed in value.

75
00:04:55,120 --> 00:05:00,760
When the user asks chat GPT something or when the user uses stable diffusion to generate

76
00:05:00,760 --> 00:05:04,320
an image, these dials and knobs do not change in value.

77
00:05:04,320 --> 00:05:08,360
In other words, all the AI models that we have today are fixed.

78
00:05:08,360 --> 00:05:13,080
Think of this as a brain that cannot learn or get any smarter.

79
00:05:13,080 --> 00:05:19,160
For example, GPT-4 cannot continue learning and become smarter and smarter with time.

80
00:05:19,160 --> 00:05:26,160
If we want a smarter model, well, we need to train a new generation of GPT such as GPT-4.0

81
00:05:26,160 --> 00:05:29,120
or GPT-5 or whatever you want to call it.

82
00:05:29,120 --> 00:05:30,720
Same with stable diffusion.

83
00:05:30,720 --> 00:05:36,040
For example, stable diffusion 2 cannot get smarter and generate better images as we use

84
00:05:36,040 --> 00:05:37,180
it more and more.

85
00:05:37,180 --> 00:05:42,780
In order for it to improve, we currently need to train a new generation, also known as stable

86
00:05:42,780 --> 00:05:44,280
diffusion 3.

87
00:05:44,280 --> 00:05:49,540
And once stable diffusion 3 is finished training, well, that's as smart as it gets.

88
00:05:49,540 --> 00:05:53,540
And if you don't think it's good enough, well, you need to train a new model.

89
00:05:53,540 --> 00:05:59,220
So basically all the AI models that we have today are fixed in their intelligence and

90
00:05:59,220 --> 00:06:00,420
their capabilities.

91
00:06:00,420 --> 00:06:06,420
Again, think of this as a brain that has stopped growing and cannot learn or get smarter.

92
00:06:06,420 --> 00:06:08,900
But this is not how the human brain works.

93
00:06:08,900 --> 00:06:14,540
There's a term called neuroplasticity, which refers to how the brain can reorganize or

94
00:06:14,540 --> 00:06:20,820
reconfigure itself by forming new neural connections over time in order to adapt to new environments

95
00:06:20,820 --> 00:06:22,140
or learn new things.

96
00:06:22,140 --> 00:06:26,780
And that's exactly what the next generation of AI can do, which we'll talk about in a

97
00:06:26,780 --> 00:06:27,780
second.

98
00:06:27,780 --> 00:06:31,220
But there's another huge limitation of current AI models.

99
00:06:31,220 --> 00:06:35,500
They are extremely inefficient and compute intensive.

100
00:06:35,500 --> 00:06:40,100
As you may know, AI is designed based on the architecture of the human brain.

101
00:06:40,100 --> 00:06:44,540
So let's compare it to the efficiency of the human brain right now.

102
00:06:44,540 --> 00:06:49,920
GPT-3 has 175 billion parameters.

103
00:06:49,920 --> 00:06:56,300
This was trained using thousands of GPUs over several weeks or several months.

104
00:06:56,300 --> 00:07:05,060
The total power required for training GPT-3 was estimated to be around 1287 megawatt hours

105
00:07:05,460 --> 00:07:06,580
of electricity.

106
00:07:06,580 --> 00:07:12,660
This is roughly equivalent to the monthly electricity consumption of 1500 homes in the

107
00:07:12,660 --> 00:07:13,660
USA.

108
00:07:13,660 --> 00:07:18,140
Now, keep in mind GPT-3 was completed in 2020.

109
00:07:18,140 --> 00:07:19,700
That's four years ago.

110
00:07:19,700 --> 00:07:23,140
The latest version, GPT-4, is closed source.

111
00:07:23,140 --> 00:07:27,980
So we don't actually know its architecture or how long it took to train.

112
00:07:27,980 --> 00:07:35,020
But we do know that it has around 1.76 trillion parameters, 10 times more than GPT-3.

113
00:07:35,020 --> 00:07:40,860
Keep in mind that the amount of computations required scales exponentially as the parameter

114
00:07:40,860 --> 00:07:42,620
size gets larger.

115
00:07:42,620 --> 00:07:49,940
So from a rough calculation, GPT-4 could have taken around 41,000 megawatt hours of

116
00:07:49,940 --> 00:07:51,780
energy to train.

117
00:07:51,780 --> 00:07:57,820
That's enough energy to power around 47,000 homes in the US for a month.

118
00:07:57,820 --> 00:08:04,220
The compute used to create these state-of-the-art models that we know today, such as GPT-4

119
00:08:04,220 --> 00:08:11,020
Clawd-3 or Gemini 1.5 Pro, requires massive data centers and a lot of energy.

120
00:08:11,020 --> 00:08:16,140
That's why tech giants are scrambling to invest and build even bigger data centers,

121
00:08:16,140 --> 00:08:19,900
because they know that compute is the main limitation here.

122
00:08:19,900 --> 00:08:26,220
And that's exactly why Microsoft and OpenAI are planning a $100 billion Stargate project

123
00:08:26,220 --> 00:08:29,060
to build the biggest data center in the world.

124
00:08:29,060 --> 00:08:31,740
All of this is for more compute.

125
00:08:31,740 --> 00:08:34,500
Now contrast this to the human brain.

126
00:08:34,500 --> 00:08:40,420
Some might say the human brain is still more intelligent than GPT-4, at least in some regards.

127
00:08:40,420 --> 00:08:48,020
The human brain only uses 175 kilowatt hours in an entire year, and it gets this energy

128
00:08:48,020 --> 00:08:50,980
in the form of calories from the food we eat.

129
00:08:50,980 --> 00:08:58,980
So training GPT-4 is estimated to require approximately 234,000 times more energy than

130
00:08:58,980 --> 00:09:02,460
what the human brain uses in an entire year.

131
00:09:02,460 --> 00:09:08,100
In other words, the energy required to train GPT-4 once could power the human brain for

132
00:09:08,100 --> 00:09:12,060
over 234,000 years.

133
00:09:12,060 --> 00:09:16,300
Now I gave this comparison to show you that there's something fundamentally wrong with

134
00:09:16,300 --> 00:09:18,020
AI models today.

135
00:09:18,020 --> 00:09:23,100
They are very energy inefficient, and they take up a lot of compute.

136
00:09:23,100 --> 00:09:26,660
It's not even close to the efficiency of the human brain.

137
00:09:26,660 --> 00:09:31,500
So the next generation of AI has to solve this inefficiency problem as well, otherwise

138
00:09:31,500 --> 00:09:33,180
it will not be sustainable.

139
00:09:33,180 --> 00:09:39,020
So to summarize the major limitations of current AI models is number one, they are fixed and

140
00:09:39,020 --> 00:09:44,780
unable to improve or learn further after being trained, and number two, they're also very

141
00:09:44,780 --> 00:09:47,580
energy intensive and inefficient.

142
00:09:47,580 --> 00:09:51,900
These are the two biggest problems of the current generation of AI.

143
00:09:51,900 --> 00:09:54,300
Now let's enter the next generation.

144
00:09:54,300 --> 00:09:58,900
We aren't there yet, but there are a few possible architectures that are being discussed

145
00:09:58,900 --> 00:10:01,020
and developed as we speak.

146
00:10:01,020 --> 00:10:04,660
The first architecture is called liquid neural networks.

147
00:10:04,660 --> 00:10:10,460
Now liquid neural networks are designed to mimic the flexibility or the plasticity of

148
00:10:10,460 --> 00:10:11,740
the human brain.

149
00:10:11,740 --> 00:10:17,620
The human brain is very flexible and can reorganize or reconfigure itself over time,

150
00:10:17,620 --> 00:10:23,420
and this ability allows the brain to adapt to new situations or learn new skills or compensate

151
00:10:23,420 --> 00:10:25,540
for injury and disease.

152
00:10:25,540 --> 00:10:30,900
For example, when you learn something new, your brain changes structurally and functionally

153
00:10:30,900 --> 00:10:33,220
to accommodate the new information.

154
00:10:33,220 --> 00:10:37,820
Learning a new language can lead to changes in the brain's structure and function, such

155
00:10:37,820 --> 00:10:41,780
as increased density of gray matter in the left hemisphere.

156
00:10:41,780 --> 00:10:46,020
The brain can also reconfigure itself to recover from injury.

157
00:10:46,020 --> 00:10:51,820
For example, after a traumatic brain injury, physical therapy and cognitive exercises can

158
00:10:51,820 --> 00:10:56,140
help rewire the brain to regain lost functions.

159
00:10:56,140 --> 00:11:00,860
And for people who've lost a sense, like sight or hearing, the brain will reorganize

160
00:11:00,860 --> 00:11:06,140
itself to compensate for the loss and make other senses become more acute.

161
00:11:06,140 --> 00:11:12,140
So this flexibility, this plasticity is exactly what liquid neural networks are designed to

162
00:11:12,140 --> 00:11:13,460
have.

163
00:11:13,460 --> 00:11:17,300
Liquid neural networks can adapt in real time to new data.

164
00:11:17,300 --> 00:11:23,060
This means that the configuration of the neural network can change as it receives new inputs,

165
00:11:23,060 --> 00:11:24,980
and that's why it's called liquid.

166
00:11:24,980 --> 00:11:30,220
These connections in the network and these dials and knobs are fluid, so they can change

167
00:11:30,220 --> 00:11:32,780
dynamically over time.

168
00:11:32,780 --> 00:11:38,260
Liquid neural networks also retain what they have learned while incorporating new information.

169
00:11:38,260 --> 00:11:43,660
This is similar to how our brains can remember old information while learning new things.

170
00:11:43,660 --> 00:11:46,860
So here's how liquid neural networks work.

171
00:11:46,860 --> 00:11:51,420
They have three main components, much like a traditional neural network.

172
00:11:51,420 --> 00:11:54,980
It has an input layer, which receives the input data.

173
00:11:54,980 --> 00:11:59,940
But then in the middle, we have this liquid layer, otherwise known as a reservoir.

174
00:11:59,940 --> 00:12:03,100
This is the core component of a liquid neural network.

175
00:12:03,100 --> 00:12:06,620
And it's basically a large recurrent neural network.

176
00:12:06,620 --> 00:12:12,460
Think of this as a big bowl of water in which each splash creates a ripple.

177
00:12:12,460 --> 00:12:17,540
These ripples are basically the neurons in this network reacting to inputs.

178
00:12:17,540 --> 00:12:23,700
The reservoir acts as a dynamic system that transforms the input data into a high dimensional

179
00:12:23,700 --> 00:12:26,700
representation called reservoir states.

180
00:12:26,700 --> 00:12:33,060
And this reservoir's rich dynamics and transformations capture the complex temporal patterns in the

181
00:12:33,060 --> 00:12:34,460
input data.

182
00:12:34,460 --> 00:12:37,140
And then finally, we have the output layer.

183
00:12:37,140 --> 00:12:42,380
This layer receives the reservoir states and maps them to the desired output using what

184
00:12:42,380 --> 00:12:44,620
is called a readout function.

185
00:12:44,620 --> 00:12:49,580
In layman terms, this is a layer that looks at the ripples in the reservoir and tries

186
00:12:49,580 --> 00:12:52,060
to understand what it all means.

187
00:12:52,060 --> 00:12:57,780
It takes the dynamic patterns from the reservoir and makes predictions or decisions from it.

188
00:12:57,780 --> 00:13:04,620
The key aspect of liquid neural networks is this reservoir layer, which remains untrained

189
00:13:04,620 --> 00:13:07,340
during the entire learning process.

190
00:13:07,340 --> 00:13:12,900
Finally the output layer is trained to map the reservoir states to the target outputs.

191
00:13:12,900 --> 00:13:15,540
In other words, to understand what these ripples mean.

192
00:13:15,540 --> 00:13:20,660
And because this reservoir remains fluid and flexible throughout time, it's not fixed

193
00:13:20,660 --> 00:13:25,740
in value, that allows this liquid neural network to basically adapt to new data and learn

194
00:13:25,740 --> 00:13:27,220
new things.

195
00:13:27,220 --> 00:13:30,260
Here's how you would train a liquid neural network.

196
00:13:30,260 --> 00:13:35,240
The connections between neurons in the reservoirs are set up randomly at the start.

197
00:13:35,240 --> 00:13:40,000
These connections typically stay the same and don't change during training.

198
00:13:40,000 --> 00:13:42,400
Next you would feed the input layer some data.

199
00:13:42,400 --> 00:13:47,380
And when this data is broken down into tokens and it reaches the reservoir layer, it causes

200
00:13:47,380 --> 00:13:52,640
the neurons in the reservoir to react and create complex patterns, much like ripples

201
00:13:52,640 --> 00:13:53,640
in water.

202
00:13:53,640 --> 00:13:58,680
So as this input data creates ripples, you basically observe and analyze the patterns

203
00:13:58,680 --> 00:14:01,040
created in the reservoir over time.

204
00:14:01,040 --> 00:14:03,360
And that's exactly what the readout layer does.

205
00:14:03,360 --> 00:14:05,720
It learns to recognize these patterns.

206
00:14:05,720 --> 00:14:10,440
It's like learning, aha, this is what caused this type of ripple and that is what caused

207
00:14:10,440 --> 00:14:11,720
this other type of ripple.

208
00:14:11,720 --> 00:14:16,180
And eventually after lots and lots of rounds of training, the readout layer can make accurate

209
00:14:16,180 --> 00:14:19,200
predictions based on observed patterns.

210
00:14:19,200 --> 00:14:24,440
Again note that only the readout layer is trained, which is simpler and faster because

211
00:14:24,440 --> 00:14:27,200
you're not adjusting anything in the reservoir layer.

212
00:14:27,200 --> 00:14:32,960
This is much quicker and needs less compute compared to traditional neural networks.

213
00:14:32,960 --> 00:14:37,240
That's because in neural networks that we know today, all the weights including those

214
00:14:37,240 --> 00:14:39,520
in the hidden layers are trainable.

215
00:14:39,520 --> 00:14:44,680
This means more parameters to optimize leading to longer training times and higher computational

216
00:14:44,680 --> 00:14:45,880
requirements.

217
00:14:45,880 --> 00:14:52,280
But in liquid neural networks, you don't adjust the weights of the reservoir during training.

218
00:14:52,280 --> 00:14:54,620
Only the readout layer is trained.

219
00:14:54,620 --> 00:15:00,480
And this significantly reduces the computational burden during training since fewer parameters

220
00:15:00,480 --> 00:15:02,240
need to be optimized.

221
00:15:02,240 --> 00:15:04,680
Plus it's a lot faster to train.

222
00:15:04,680 --> 00:15:07,280
Thanks to our sponsor Bright Data.

223
00:15:07,280 --> 00:15:12,560
Bright Data is an all-in-one platform designed to help businesses collect high quality web

224
00:15:12,560 --> 00:15:14,520
data at scale.

225
00:15:14,520 --> 00:15:19,360
This is especially useful for AI companies which require huge amounts of diverse and

226
00:15:19,360 --> 00:15:25,400
high quality training data to build robust and unbiased AI models.

227
00:15:25,400 --> 00:15:30,400
Collecting this training data manually can be time consuming and prone to errors.

228
00:15:30,400 --> 00:15:32,520
And that's where Bright Data comes in.

229
00:15:32,520 --> 00:15:37,640
With Bright Data, you can access high volume, high quality web data effortlessly.

230
00:15:37,640 --> 00:15:44,040
From parsed validated data sets to custom scraping solutions, they've got you covered.

231
00:15:44,040 --> 00:15:48,720
Get parsed and cleaned data sets ready to use on demand.

232
00:15:48,720 --> 00:15:54,200
Customize any data set to fit your specific needs and benefit from reliable delivery and

233
00:15:54,200 --> 00:15:56,080
full compliance.

234
00:15:56,080 --> 00:16:03,160
In fact every 15 minutes their customers scrape enough data to train chat GPT from scratch.

235
00:16:03,160 --> 00:16:05,360
That's a lot of data, to say the least.

236
00:16:05,360 --> 00:16:11,560
They have many tools like the web scraper API, the proxy manager, and unblocking technologies

237
00:16:11,560 --> 00:16:17,520
to help automate your data scraping at scale, allowing you to build reliable data sets to

238
00:16:17,520 --> 00:16:20,720
train any AI or LLM.

239
00:16:20,720 --> 00:16:23,840
Visit the link in the description below to learn more.

240
00:16:23,840 --> 00:16:28,240
It's a lot faster for these liquid neural networks to converge at an optimum.

241
00:16:28,240 --> 00:16:33,660
And because of this reservoir where the weights and configurations can change dynamically depending

242
00:16:33,660 --> 00:16:38,240
on the data that you feed it, liquid neural networks can potentially be much smaller than

243
00:16:38,240 --> 00:16:42,320
traditional neural networks which have fixed weights and connections.

244
00:16:42,320 --> 00:16:45,560
And this offers a lot more efficient learning and inference.

245
00:16:45,560 --> 00:16:51,280
So for example, researchers at MIT were able to pilot a drone using a liquid neural network

246
00:16:51,360 --> 00:16:57,200
with only 20,000 parameters, which is very tiny compared to state-of-the-art AI models

247
00:16:57,200 --> 00:17:01,800
such as GPT4, which often have over a trillion parameters.

248
00:17:01,800 --> 00:17:02,800
Just think about that.

249
00:17:02,800 --> 00:17:06,880
20,000 parameters versus over a trillion parameters.

250
00:17:06,880 --> 00:17:12,600
So these smaller sizes generally translate to faster inference and lower computational

251
00:17:12,600 --> 00:17:14,200
requirements.

252
00:17:14,200 --> 00:17:17,560
Liquid neural networks are also way less memory intensive.

253
00:17:17,560 --> 00:17:23,240
Again, since you don't train the reservoir's weights, memory usage is much lower during

254
00:17:23,240 --> 00:17:27,880
training compared to traditional neural networks where the gradients and the parameters for

255
00:17:27,880 --> 00:17:30,840
all layers must be stored in memory.

256
00:17:30,840 --> 00:17:36,200
Liquid neural networks are particularly good at processing temporal data due to their dynamic

257
00:17:36,200 --> 00:17:37,200
reservoir.

258
00:17:37,200 --> 00:17:41,680
So they excel in tasks that involve complex time series data.

259
00:17:41,680 --> 00:17:46,440
Now you might be wondering, well, how can these liquid neural networks actually be applied

260
00:17:46,440 --> 00:17:47,800
in the real world?

261
00:17:47,800 --> 00:17:49,840
So here are some use cases.

262
00:17:49,840 --> 00:17:55,280
As we race to build fully autonomous AI robots, these robots will be deployed in the real

263
00:17:55,280 --> 00:18:00,200
world and oftentimes they might encounter situations that they've never seen before

264
00:18:00,200 --> 00:18:01,600
during training.

265
00:18:01,600 --> 00:18:06,720
For example, there could be unpredictable environments in search and rescue missions,

266
00:18:06,720 --> 00:18:11,320
but with liquid neural networks, these robots can adapt to changing conditions and learn

267
00:18:11,320 --> 00:18:13,200
new tasks on the fly.

268
00:18:13,200 --> 00:18:17,640
And eventually we're going to have these autonomous robots in our houses helping us do chores

269
00:18:17,640 --> 00:18:18,880
and other tasks.

270
00:18:18,880 --> 00:18:23,240
But maybe you have a certain way of folding clothes or doing the laundry or cooking that

271
00:18:23,240 --> 00:18:25,080
the robot was never trained on.

272
00:18:25,080 --> 00:18:29,820
So with a traditional neural network, these robots aren't able to learn new skills after

273
00:18:29,820 --> 00:18:31,200
being deployed.

274
00:18:31,200 --> 00:18:36,360
But with liquid neural networks built into a humanoid robot, it can learn new tasks that

275
00:18:36,360 --> 00:18:37,360
you teach it.

276
00:18:37,360 --> 00:18:40,720
And this robot will become a lot more personalized for you.

277
00:18:40,720 --> 00:18:42,960
And then we have autonomous driving.

278
00:18:42,960 --> 00:18:47,200
There's no doubt that self-driving cars will eventually become the future.

279
00:18:47,200 --> 00:18:52,040
But current technologies still do not perform well, especially in challenging environments

280
00:18:52,040 --> 00:18:53,040
or new conditions.

281
00:18:53,040 --> 00:18:58,680
Again, this is because traditional neural networks can only do well on data that they

282
00:18:58,680 --> 00:18:59,680
were trained on.

283
00:18:59,680 --> 00:19:02,200
They're not able to adapt to new environments.

284
00:19:02,200 --> 00:19:07,000
But with liquid neural networks, autonomous vehicles can navigate complex and dynamic

285
00:19:07,000 --> 00:19:12,400
environments by continuously learning and training from sensor data and adjusting their

286
00:19:12,400 --> 00:19:14,200
behavior accordingly.

287
00:19:14,200 --> 00:19:17,320
It's constantly training and improving over time.

288
00:19:17,320 --> 00:19:23,120
Now as I've mentioned before, liquid neural networks often incorporate recurrent connections,

289
00:19:23,120 --> 00:19:27,040
making them suitable for processing time series data.

290
00:19:27,040 --> 00:19:31,440
So it's great for things like weather prediction and of course, stock trading.

291
00:19:31,440 --> 00:19:37,180
The stock market is filled with ever-changing trends and cycles, so it's close to impossible

292
00:19:37,180 --> 00:19:41,280
for one fixed algorithm or formula to beat the market.

293
00:19:41,280 --> 00:19:46,680
However, because liquid neural networks can adapt to ever-changing data, it can optimize

294
00:19:46,680 --> 00:19:50,560
trading strategies in real time to maximize profits.

295
00:19:50,560 --> 00:19:55,280
In other words, you could be constantly streaming the latest market data to this liquid neural

296
00:19:55,280 --> 00:20:00,240
network, which would change its configuration to adapt to this data in real time to help

297
00:20:00,240 --> 00:20:02,400
you maximize profits.

298
00:20:02,400 --> 00:20:04,800
Another use case would be healthcare.

299
00:20:04,800 --> 00:20:10,360
Liquid neural networks can be used in wearable devices to monitor patients in real time,

300
00:20:10,360 --> 00:20:15,360
adapting to changes in the patient's conditions and predicting potential health issues before

301
00:20:15,360 --> 00:20:17,040
they become critical.

302
00:20:17,040 --> 00:20:22,480
In cybersecurity, liquid neural networks can continuously learn from network traffic and

303
00:20:22,480 --> 00:20:28,960
user behavior to adapt access control policies and detect anomalies or unauthorized access

304
00:20:28,960 --> 00:20:30,400
attempts.

305
00:20:30,400 --> 00:20:34,840
Yet another use case would be streaming services such as Netflix.

306
00:20:34,840 --> 00:20:40,280
They can use liquid neural networks to adapt to each user's viewing habits and preferences,

307
00:20:40,280 --> 00:20:43,720
providing more personalized content recommendations.

308
00:20:43,720 --> 00:20:46,600
Another use case would be smart city management.

309
00:20:46,600 --> 00:20:51,720
For example, liquid neural networks can optimize traffic flow in real time by learning from

310
00:20:51,720 --> 00:20:57,840
traffic patterns and changing traffic lights accordingly to reduce congestion and improve

311
00:20:57,840 --> 00:20:59,200
efficiency.

312
00:20:59,200 --> 00:21:01,960
Energy management is also very relevant.

313
00:21:01,960 --> 00:21:06,680
Smart grids can use liquid neural networks to balance power, supply, and demand in real

314
00:21:06,680 --> 00:21:12,440
time, improving efficiency and reducing costs by adapting to consumption patterns.

315
00:21:12,440 --> 00:21:18,000
However, although liquid neural networks seem promising, it does have its limitations.

316
00:21:18,000 --> 00:21:23,160
This is still a relatively new concept in the field of neural networks and research on

317
00:21:23,160 --> 00:21:28,560
them is still in its early stages compared to more traditional architectures.

318
00:21:28,560 --> 00:21:33,800
While liquid neural networks show promising theoretical benefits such as the ability to

319
00:21:33,800 --> 00:21:39,160
process continuous data streams and adapt on the fly, there is still a lack of real

320
00:21:39,160 --> 00:21:43,720
world results demonstrating their superiority on a large scale.

321
00:21:43,720 --> 00:21:48,800
Many researchers are likely waiting for more compelling benchmark results before investing

322
00:21:48,800 --> 00:21:52,160
significant effort into liquid neural networks.

323
00:21:52,160 --> 00:21:58,240
Also as I mentioned previously, they're particularly suited for temporal or sequence data.

324
00:21:58,240 --> 00:22:04,080
Also for tasks that do not involve time such as identifying images of cats or dogs, traditional

325
00:22:04,080 --> 00:22:08,960
neural networks might actually be more effective and straightforward to implement.

326
00:22:08,960 --> 00:22:14,760
Also the dynamics within this reservoir layer can be very complex and difficult to interpret

327
00:22:14,760 --> 00:22:19,680
and this makes it challenging to understand how the reservoir processes these inputs.

328
00:22:19,680 --> 00:22:22,960
It would be quite hard to fine tune it for optimal performance.

329
00:22:22,960 --> 00:22:29,040
Finally, there is a lack of standardized support and fewer established frameworks for liquid

330
00:22:29,040 --> 00:22:32,000
neural networks compared to traditional neural networks.

331
00:22:32,000 --> 00:22:36,440
And this can make implementation and experimentation more challenging.

332
00:22:36,440 --> 00:22:41,800
So all in all, liquid neural networks are still a very early concept and an area of active

333
00:22:41,800 --> 00:22:42,800
research.

334
00:22:42,800 --> 00:22:47,520
Unlike traditional neural networks that are fixed and need to be retrained with a large

335
00:22:47,520 --> 00:22:52,400
data set to learn new information, liquid neural networks can update their knowledge

336
00:22:52,400 --> 00:22:55,400
incrementally with each new piece of data.

337
00:22:55,400 --> 00:23:00,560
This offers a flexible and adaptive model which could potentially become infinitely

338
00:23:00,560 --> 00:23:02,720
smarter over time.

339
00:23:02,720 --> 00:23:08,000
Now liquid neural networks aren't the only possibility that could become the next generation

340
00:23:08,000 --> 00:23:09,000
of AI.

341
00:23:09,000 --> 00:23:13,540
We have another type of neural network which is designed to mimic the human brain even

342
00:23:13,540 --> 00:23:16,400
more than traditional neural networks.

343
00:23:16,400 --> 00:23:19,640
And this brings us to spiking neural networks.

344
00:23:19,640 --> 00:23:26,120
These are closely inspired by the way neurons in our brains communicate using discrete spikes

345
00:23:26,120 --> 00:23:28,080
or action potentials.

346
00:23:28,080 --> 00:23:33,520
You see, in the human brain, which is basically a network of neurons, each neuron doesn't

347
00:23:33,520 --> 00:23:37,800
immediately fire to the next set of neurons when it receives input.

348
00:23:37,800 --> 00:23:43,840
Instead, the input has to build up to a certain threshold and once it passes this threshold,

349
00:23:43,840 --> 00:23:46,000
then it fires to the next set of neurons.

350
00:23:46,000 --> 00:23:49,520
And after it fires, it goes back to its resting state.

351
00:23:49,520 --> 00:23:53,920
Well, spiking neural networks are designed to mimic this behavior.

352
00:23:53,920 --> 00:23:55,920
So here's how it works.

353
00:23:55,920 --> 00:24:00,000
The architecture is quite similar to traditional neural networks.

354
00:24:00,000 --> 00:24:06,800
However, for each neuron, it waits to receive signals or spikes from other neurons.

355
00:24:06,800 --> 00:24:09,800
Think of these spikes as like little electric pulses.

356
00:24:09,800 --> 00:24:15,240
The input data, such as an image or a sound, is turned into the spikes that move through

357
00:24:15,280 --> 00:24:16,760
this neural network.

358
00:24:16,760 --> 00:24:22,120
For example, if it's a loud sound, it might generate more spikes while a quiet sound might

359
00:24:22,120 --> 00:24:24,000
generate fewer spikes.

360
00:24:24,000 --> 00:24:28,480
Now each neuron in the network collects incoming spikes.

361
00:24:28,480 --> 00:24:30,840
Imagine a bucket collecting drops of water.

362
00:24:30,840 --> 00:24:33,440
As more spikes come in, the bucket fills up.

363
00:24:33,440 --> 00:24:38,240
And when the neuron gets enough spikes, in other words, when it reaches a certain threshold,

364
00:24:38,240 --> 00:24:41,000
it fires a spike to the next set of neurons.

365
00:24:41,000 --> 00:24:45,600
And after firing, it resets and starts collecting again from zero.

366
00:24:45,600 --> 00:24:51,160
So instead of using continuous signals like traditional neural networks, spiking neural

367
00:24:51,160 --> 00:24:57,040
networks uses spikes, which are basically bursts of activity at discrete time points

368
00:24:57,040 --> 00:24:59,120
to process information.

369
00:24:59,120 --> 00:25:05,000
In other words, spiking neural networks incorporate time into their processing, with neurons firing

370
00:25:05,000 --> 00:25:08,560
only when their potential exceeds a certain threshold.

371
00:25:08,560 --> 00:25:13,200
Now there are different methods and algorithms to train a spiking neural network, and there

372
00:25:13,200 --> 00:25:16,520
currently isn't a standard wave that's set in stone.

373
00:25:16,520 --> 00:25:19,400
So this is still an active field of research.

374
00:25:19,400 --> 00:25:25,240
One common method is called spike timing dependent plasticity, or STDP.

375
00:25:25,240 --> 00:25:30,520
This method is inspired by how the brain strengthens or weakens connections between neurons.

376
00:25:30,520 --> 00:25:36,920
So if one neuron spikes just before another, then the connection between them gets stronger.

377
00:25:36,920 --> 00:25:40,800
If it spikes just after, then the connection gets weaker.

378
00:25:40,800 --> 00:25:45,840
It's like learning which connections are important based on the timing of the spikes.

379
00:25:45,840 --> 00:25:51,120
And speaking of timing, it's the exact timing of spikes that matters.

380
00:25:51,120 --> 00:25:55,440
It's not just about how many spikes there are, but when they happen.

381
00:25:55,440 --> 00:25:59,960
Now STDP is only one method to train the spiking neural networks.

382
00:25:59,960 --> 00:26:03,440
There are a few other ones, which are beyond the scope of this video.

383
00:26:03,440 --> 00:26:08,000
But like traditional neural networks, spiking neural networks have to undergo millions of

384
00:26:08,000 --> 00:26:12,880
rounds of training with a lot of data, and eventually the configuration of the network

385
00:26:12,880 --> 00:26:16,640
and all its parameters will reach an optimum state.

386
00:26:16,640 --> 00:26:22,400
Now again, I'd like to remind you that this is a very simplified explanation of spiking

387
00:26:22,400 --> 00:26:26,760
neural networks, and I've left out a lot of mathematical details.

388
00:26:26,760 --> 00:26:29,240
But in a nutshell, that's how it works.

389
00:26:29,240 --> 00:26:33,840
So you might be wondering, well, what are the benefits of spiking neural networks?

390
00:26:33,840 --> 00:26:38,800
First of all, it's designed to mimic the human brain even more by implementing this

391
00:26:38,800 --> 00:26:40,160
spiking mechanism.

392
00:26:40,160 --> 00:26:46,160
So in theory, maybe we could reach a superior level of intelligence compared to the current

393
00:26:46,160 --> 00:26:50,240
generation of AI if we mimicked the human brain even more.

394
00:26:50,240 --> 00:26:54,600
But the biggest benefit of spiking neural networks is their efficiency.

395
00:26:54,600 --> 00:26:59,680
If you remember at the beginning of the video, I compared the energy consumption of the human

396
00:26:59,680 --> 00:27:06,200
brain versus a current state of the art model like GPT-4, which requires huge data centers

397
00:27:06,200 --> 00:27:08,160
and huge amounts of compute.

398
00:27:08,160 --> 00:27:12,200
That's because traditional neural networks are always active.

399
00:27:12,200 --> 00:27:16,640
Each input of data activates the entire neural network.

400
00:27:16,640 --> 00:27:22,680
So you have to do an insane amount of matrix multiplications across the entire network

401
00:27:22,760 --> 00:27:25,680
just to do one round of training or inference.

402
00:27:25,680 --> 00:27:31,080
However, for spiking neural networks, they only use energy where spikes occur, while

403
00:27:31,080 --> 00:27:34,240
the rest of the neural network remains inactive.

404
00:27:34,240 --> 00:27:37,040
This makes it a lot more energy efficient.

405
00:27:37,040 --> 00:27:42,680
Plus, spiking neural networks are particularly suitable for neuromorphic chips which are

406
00:27:42,680 --> 00:27:45,400
designed to mimic the human brain.

407
00:27:45,400 --> 00:27:51,200
Now, neuromorphic chips are a huge topic and deserves its own full video.

408
00:27:51,200 --> 00:27:55,440
So let me know in the comments if you'd like me to make a video on this as well.

409
00:27:55,440 --> 00:28:01,240
So how can these spiking neural networks actually be applied to the real world?

410
00:28:01,240 --> 00:28:08,080
Well, because these neural networks can encode and process information in the timing of spikes,

411
00:28:08,080 --> 00:28:11,320
this is great for processing temporal data.

412
00:28:11,320 --> 00:28:18,000
This makes them great for adaptive and autonomous systems, plus this spike timing-dependent

413
00:28:18,000 --> 00:28:23,880
plasticity, which I mentioned before, where the timing of the spikes influences the strength

414
00:28:23,880 --> 00:28:26,000
of the connections in the network.

415
00:28:26,000 --> 00:28:29,840
This can lead to more robust and adaptive learning capability.

416
00:28:29,840 --> 00:28:35,880
So this dynamic learning can make spiking neural networks suitable for autonomous systems

417
00:28:35,880 --> 00:28:41,280
such as self-driving, where the AI has to learn and adapt to changing environments.

418
00:28:41,280 --> 00:28:46,160
Or it can be used in real-time processing like predicting the stock market or patient

419
00:28:46,160 --> 00:28:50,560
monitoring and personalized medicine, and of course, autonomous robots.

420
00:28:50,560 --> 00:28:56,160
Now, although spiking neural networks offer some huge benefits, especially regarding energy

421
00:28:56,160 --> 00:28:59,480
efficiency, they do have some limitations.

422
00:28:59,480 --> 00:29:04,440
Setting up and programming spiking neural networks is more complicated compared to

423
00:29:04,440 --> 00:29:06,040
traditional neural networks.

424
00:29:06,040 --> 00:29:11,600
This spiking behavior, of course, adds a layer of complexity, making them harder to design

425
00:29:11,600 --> 00:29:13,240
and understand.

426
00:29:13,240 --> 00:29:16,760
Studying spiking neural networks is also quite difficult.

427
00:29:16,760 --> 00:29:21,040
Current neural networks use methods like backpropagation to adjust their parameters,

428
00:29:21,040 --> 00:29:26,360
but this process doesn't work well with these discrete time-based spikes.

429
00:29:26,360 --> 00:29:32,400
Researchers are still trying to find an effective training algorithm for spiking neural networks.

430
00:29:32,400 --> 00:29:37,240
Also given this additional dimension of time, spiking neural networks might actually require

431
00:29:37,240 --> 00:29:40,360
more computational resources to simulate.

432
00:29:40,360 --> 00:29:46,080
This is because they need to track and process spikes over time, which can be computationally

433
00:29:46,080 --> 00:29:47,080
expensive.

434
00:29:47,080 --> 00:29:53,680
Yet, another limitation is that running spiking neural networks efficiently often requires

435
00:29:53,680 --> 00:30:00,040
specialized hardware such as neuromorphic chips, which are not widely available or standardized

436
00:30:00,040 --> 00:30:03,520
compared to conventional computing hardware.

437
00:30:03,520 --> 00:30:08,160
Neuromorphic chips are optimized for this spike-based processing and are still being

438
00:30:08,160 --> 00:30:09,160
developed.

439
00:30:09,160 --> 00:30:15,100
And that's why, for example, Sam Altman is investing millions of dollars into a neuromorphic

440
00:30:15,100 --> 00:30:17,080
chip company called RAIN.

441
00:30:17,080 --> 00:30:22,360
Finally, while spiking neural networks show promising results, especially for time-based

442
00:30:22,360 --> 00:30:28,200
data, they often lag behind current neural networks for non-time-based data.

443
00:30:28,200 --> 00:30:34,840
They often underperform compared with current AI models, particularly for complex tasks.

444
00:30:34,840 --> 00:30:40,160
This is partly due to the challenges in training spiking neural networks effectively.

445
00:30:40,160 --> 00:30:45,320
And as with liquid neural networks, spiking neural networks are also relatively new.

446
00:30:45,320 --> 00:30:51,000
So there are fewer tools and frameworks available for developing spiking neural networks compared

447
00:30:51,000 --> 00:30:53,400
to current AI models.

448
00:30:53,400 --> 00:30:58,160
This makes experimentation and development slower and more difficult.

449
00:30:58,160 --> 00:31:03,520
But anyways, that sums up what could potentially be the next generation of AI.

450
00:31:03,520 --> 00:31:08,960
To bring it all back, the current generation of AI is very energy inefficient, requiring

451
00:31:08,960 --> 00:31:10,920
huge amounts of compute.

452
00:31:10,920 --> 00:31:14,560
Plus, it can't learn new things after being trained.

453
00:31:14,560 --> 00:31:21,160
If we want to achieve AGI or ASI, we need to essentially create something as efficient

454
00:31:21,160 --> 00:31:26,720
and as fluid as the human brain, which can constantly learn new things and adapt to

455
00:31:26,720 --> 00:31:28,800
changing environments.

456
00:31:28,800 --> 00:31:33,520
These are the two essential things that new types of neural networks, such as liquid neural

457
00:31:33,520 --> 00:31:38,200
networks and spiking neural networks can solve, at least in theory.

458
00:31:38,200 --> 00:31:43,720
However, these are still relatively new and they are still being developed, but the potential

459
00:31:43,720 --> 00:31:45,480
could be massive.

460
00:31:45,480 --> 00:31:49,880
Imagine an AI that can keep learning and get infinitely smarter.

461
00:31:49,880 --> 00:31:54,000
Let me know what you think about these neural networks in the comments below.

462
00:31:54,000 --> 00:31:58,160
Things are happening so fast in the world of AI, it's quite hard to keep up with all

463
00:31:58,160 --> 00:32:01,280
the technological innovations that are happening right now.

464
00:32:01,280 --> 00:32:06,000
So if I've missed any other groundbreaking architectures that are worth mentioning, please

465
00:32:06,000 --> 00:32:10,560
let me know in the comments below and I'll try to do a video on that as well.

466
00:32:10,560 --> 00:32:15,520
As always, if you enjoyed this video, remember to like, share, subscribe and stay tuned for

467
00:32:15,520 --> 00:32:17,160
more content.

468
00:32:17,160 --> 00:32:22,000
Also we built a site where you can find all the AI tools out there as well as find jobs

469
00:32:22,000 --> 00:32:24,840
in machine learning, data science and more.

470
00:32:24,840 --> 00:32:27,720
Check it out at ai-search.io.

471
00:32:27,720 --> 00:32:30,320
Thanks for watching and I'll see you in the next one.

