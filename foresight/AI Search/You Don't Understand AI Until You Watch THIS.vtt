WEBVTT

00:00.000 --> 00:05.000
This one video is going to explain all of these questions for you.

00:05.000 --> 00:06.880
How does AI work?

00:06.880 --> 00:08.520
How does AI learn?

00:08.520 --> 00:10.320
How does chat GPT work?

00:10.320 --> 00:12.200
How does image generation work?

00:12.200 --> 00:15.920
Does AI actually copy or steal art or other content?

00:15.920 --> 00:19.920
I know a decent portion of artists out there do not like AI.

00:19.920 --> 00:24.400
Some of them are quite hostile towards AI because they think that AI is stealing their

00:24.400 --> 00:26.640
work or their art style.

00:26.640 --> 00:30.560
Another group that does not like AI very much are, for example, publishers.

00:30.560 --> 00:34.280
I'm not saying all of them, but some of them, like New York Times, for example, they claim

00:34.280 --> 00:40.320
that open AI is copying their content and they're now suing open AI for this.

00:40.320 --> 00:41.720
But is this really the case?

00:41.720 --> 00:43.400
Is this a valid argument?

00:43.400 --> 00:46.920
Also, can AI solve unsolvable math problems?

00:46.920 --> 00:52.240
For example, in a previous video, I talked about this leaked document, which claims to

00:52.240 --> 00:56.600
be about this mysterious Q-star project that open AI was working on.

00:56.600 --> 01:00.880
That whether this is true or not is not the point of this video, but this document was

01:00.880 --> 01:06.240
quite controversial because it claims that this team trained an AI that was able to break

01:06.240 --> 01:07.520
encryption systems.

01:07.520 --> 01:12.280
These are systems that secure our passwords, our bank accounts, the internet, government

01:12.280 --> 01:13.280
data, etc.

01:13.280 --> 01:19.320
Now, as far as we know, there's no mathematically viable way to really hack this systematically.

01:19.320 --> 01:23.720
The only way is to brute force guess all the different possibilities of passwords.

01:23.720 --> 01:27.080
This video will explain, can AI actually do this?

01:27.080 --> 01:32.440
Can it actually break encryption or solve these other math problems, which right now

01:32.440 --> 01:34.720
we believe are mathematically unsolvable?

01:34.720 --> 01:38.680
Also, we'll talk about, can AI beat humans at everything?

01:38.680 --> 01:43.680
Can AI eventually be so good that it can outperform humans at any task?

01:43.680 --> 01:47.520
And finally, is AI conscious or self aware or sentient?

01:47.520 --> 01:52.200
Make sure you stick to the end because the explanation to this is going to be very juicy.

01:52.200 --> 01:55.600
We'll cover all of this in easy to understand terms.

01:55.600 --> 02:00.200
Now if you're an AI scientist or an engineer, you probably know most of this, but for the

02:00.200 --> 02:05.040
rest of us, this video will give you a deeper understanding of AI.

02:05.040 --> 02:11.840
So the essence behind all AI we know today, whether it's chat GPT or mid journey or stable

02:11.840 --> 02:18.800
diffusion or Sora or Alpha fold, the backbone of all of these AI systems is the neural network.

02:18.800 --> 02:20.680
A neural network looks like this.

02:20.680 --> 02:22.760
It's basically layers of nodes.

02:22.760 --> 02:29.040
So each point here is called a node and each line of nodes is called a layer.

02:29.040 --> 02:33.280
And each node is interconnected with one another through these linkages.

02:33.280 --> 02:38.040
And the neural network is actually designed based on the human brain, except for nodes

02:38.040 --> 02:40.200
and linkages in the human brain.

02:40.200 --> 02:42.440
It's just a network of neurons and synapses.

02:42.440 --> 02:46.560
So you can see this is a microscopic photo of a human brain and you can see all these

02:46.560 --> 02:50.840
different nerve cells being connected in this very dense network.

02:50.840 --> 02:56.840
A neural network is basically the same structure as this, except that it looks like this instead

02:56.840 --> 03:00.680
of a bunch of cells in this bloody glob of an organ.

03:00.680 --> 03:03.320
Now how exactly does an AI work?

03:03.320 --> 03:05.480
Let's start with a very simple example.

03:05.480 --> 03:10.480
Let's say we have a neural network, which is trained to identify images of cats versus

03:10.480 --> 03:11.480
dogs.

03:11.480 --> 03:15.360
And don't worry, I'll talk a lot more about how we train an AI in a second.

03:15.360 --> 03:17.520
But first, let's just go over how this works.

03:17.520 --> 03:23.760
So let's say we input or we feed this neural network with an image of a cat, this image

03:23.760 --> 03:29.680
would actually be broken down into data and the data will flow through each of these nodes.

03:29.680 --> 03:33.160
And after it flows through the first layer of nodes, it will flow through the second

03:33.160 --> 03:36.920
layer of nodes and then the next layer of nodes and then the next layer and so on and

03:36.920 --> 03:42.120
so forth until it reaches the final layer, in which case it would calculate the values

03:42.120 --> 03:46.120
of this and based on the values of the final layer, it would spit out an answer.

03:46.120 --> 03:47.280
This is a cat.

03:47.280 --> 03:53.040
In fact, you can think of each of these nodes and links as dials and knobs that determine

03:53.040 --> 03:56.040
how much data flows through to the next layer.

03:56.040 --> 04:01.320
If you think of this in like realistic terms, and I'm not saying this is how a neural network

04:01.320 --> 04:07.760
works, but you can think of this node, for example, as the shape of the ears of the animal.

04:07.760 --> 04:09.800
This node would be the shape of its paws.

04:09.840 --> 04:12.880
This node would be the shape of his eyes, etc.

04:12.880 --> 04:15.240
That's just a really dumbed down way of looking at it.

04:15.240 --> 04:16.760
It's not really doing that.

04:16.760 --> 04:20.560
But each node is basically looking at a certain feature in the image.

04:20.560 --> 04:25.240
And then if the image has that feature, the information can pass through to the next layer.

04:25.240 --> 04:29.240
If it doesn't have that feature, then the information is not passed on to the next layer.

04:29.240 --> 04:33.320
So depending on what image you feed it, the flow of information could look like this or

04:33.320 --> 04:35.720
it could look like this or like this.

04:35.720 --> 04:36.720
You get the point.

04:36.840 --> 04:42.200
These knobs and dials determine how data flows through the neural network based on your original

04:42.200 --> 04:43.200
input image.

04:43.200 --> 04:47.320
An important distinction between a neural network and the brain is that these nodes

04:47.320 --> 04:50.000
can let in a percentage of data.

04:50.000 --> 04:52.600
So it can let in no data or 0%.

04:52.600 --> 04:56.960
It can let in all of the data to the next layer, but it can also be a percentage of

04:56.960 --> 04:57.960
the data.

04:57.960 --> 05:01.480
So for example, it can let in 30% of the data to the next node.

05:01.480 --> 05:07.600
This is slightly different from the human brain's neurons, which tend to just fire 100%

05:07.600 --> 05:08.720
or 0%.

05:08.720 --> 05:11.000
This is called the all or none law.

05:11.000 --> 05:15.640
So once it passes a certain threshold, this neuron will fire, whereas neurons in an artificial

05:15.640 --> 05:20.800
neural network, they could fire just like 50% or 30%, etc.

05:20.800 --> 05:22.200
Just a minor distinction.

05:22.200 --> 05:25.720
So we plug in an image of a cat through this neural network.

05:25.720 --> 05:29.520
And at the end layer, it will determine that this is a cat.

05:29.520 --> 05:33.960
Now for each node, there are also, if you want to get into more technical details, there

05:33.960 --> 05:38.880
are certain parameters that determine how much data flows through to the next layer.

05:38.880 --> 05:42.560
These include weights, biases and activation functions.

05:42.560 --> 05:44.480
But that's beyond the scope of this tutorial.

05:44.480 --> 05:48.480
All you need to know for this video is that each of these knobs and linkages determine

05:48.480 --> 05:51.720
how much information flows through to the next layer.

05:51.720 --> 05:56.200
This video is just a very simple explanation of how AI works.

05:56.200 --> 06:01.600
So all you need to know is that these nodes and linkages determine how much data flows

06:01.600 --> 06:04.000
through to the next layer.

06:04.000 --> 06:07.980
On the topic of layers, each set of nodes is one layer.

06:07.980 --> 06:10.280
So the first layer is called the input layer.

06:10.280 --> 06:13.280
The last layer is called the output layer.

06:13.280 --> 06:16.480
And then all these layers in between are called hidden layers.

06:16.480 --> 06:18.280
So why am I talking about layers?

06:18.280 --> 06:21.160
You probably have heard of the term deep learning.

06:21.160 --> 06:26.640
Deep learning is basically training and using neural networks with lots and lots of layers.

06:26.640 --> 06:29.480
In other words, the neural network is very, very deep.

06:29.480 --> 06:31.120
That's why it's called deep learning.

06:31.120 --> 06:33.960
All right, how does an AI actually learn?

06:33.960 --> 06:38.680
You can't just have any random neural network and it just magically knows how to identify

06:38.680 --> 06:40.640
images of cats and dogs.

06:40.640 --> 06:46.560
So first, when you build a neural network, the values of these dials and knobs are probably

06:46.560 --> 06:51.760
just going to be random values, or they could be pre trained values, for example, from an

06:51.760 --> 06:53.200
existing model.

06:53.200 --> 06:57.960
But how do you get it to be super good at identifying images of cats and dogs?

06:57.960 --> 07:02.320
In other words, how do you fine tune the model to your desired purpose?

07:02.320 --> 07:05.760
Well, you need to feed it data, lots and lots of data.

07:05.760 --> 07:10.280
So you're going to have to prepare tons of images of cats and dogs, and then you label

07:10.280 --> 07:11.280
it.

07:11.280 --> 07:12.520
So this is a dog.

07:12.520 --> 07:13.520
This is a cat.

07:13.520 --> 07:14.520
This is a cat.

07:14.520 --> 07:15.520
This is a dog.

07:15.520 --> 07:16.520
This is a dog, etc.

07:16.520 --> 07:23.400
Basically, this is the answer that the AI needs to learn from this input image.

07:23.400 --> 07:26.880
This is called supervised learning where you label the data.

07:26.880 --> 07:31.440
There's also another type of learning called unsupervised learning where the AI needs to

07:31.440 --> 07:36.360
learn to categorize data by itself without any guidance from the human.

07:36.360 --> 07:39.100
But for the sake of this video, let's just keep it simple.

07:39.100 --> 07:44.400
So we have all these images of cats and dogs and usually to train a neural network to do

07:44.400 --> 07:45.760
a task very well.

07:45.760 --> 07:49.540
You need a lot of data, like usually millions of data points.

07:49.540 --> 07:54.540
So you basically feed these images to the neural network one by one to train it.

07:54.540 --> 07:57.360
And one session of training is called an epoch.

07:57.360 --> 08:03.520
So all right, let's say in one training session, you feed it this image of a dog and it outputs

08:03.520 --> 08:04.740
this is a dog.

08:04.740 --> 08:05.740
So all right, that's great.

08:05.740 --> 08:10.520
We got it correct, which means that these dials and knobs are doing quite well.

08:10.520 --> 08:15.000
They're probably configured correctly since it got the answer correct.

08:15.000 --> 08:17.640
You probably don't need to adjust these further.

08:17.640 --> 08:22.800
However, what if for the next image you feed it this and then it outputs this is a dog?

08:22.800 --> 08:24.440
Well, this would be incorrect.

08:24.440 --> 08:29.240
So these dials and knobs are likely not configured correctly.

08:29.240 --> 08:33.880
If it gets the answer wrong, and it knows it got it wrong because we labeled the data cat

08:33.880 --> 08:37.660
for this image so it can compare its output with our label.

08:37.660 --> 08:42.080
So all right, let's say the real answer is a cat, but it said this is a dog.

08:42.080 --> 08:44.320
In that case, it incurs some penalty.

08:44.320 --> 08:47.400
That penalty basically tells it, all right, you got it wrong.

08:47.400 --> 08:54.160
So you need to adjust these knobs and dials to make sure that the output is actually cat

08:54.160 --> 08:59.840
when I give you this image and how it adjusts the values of these knobs and dials is through

08:59.840 --> 09:02.520
an algorithm called gradient descent.

09:02.520 --> 09:05.440
It adjusts the values via back propagation.

09:05.440 --> 09:09.680
So it adjusts the nodes in the last layer first and then the previous layer and then

09:09.680 --> 09:13.320
the previous layer, et cetera, until it reaches the first layer.

09:13.320 --> 09:15.720
So again, gradient descent is a key term here.

09:15.720 --> 09:22.040
This is the algorithm which the neural network uses to adjust these knobs and dials until

09:22.040 --> 09:23.720
it can get the correct answer.

09:23.720 --> 09:29.160
So we basically rinse and repeat this with millions of images and lots and lots of epochs

09:29.160 --> 09:30.720
or training sessions.

09:30.720 --> 09:35.440
And initially this neural network might get a lot of values wrong, but through this process

09:35.440 --> 09:41.200
of gradient descent, these knobs and dials will be tweaked so that eventually whenever

09:41.200 --> 09:46.280
it receives an image of a cat or a dog, it can accurately determine this is a cat or

09:46.280 --> 09:47.360
this is a dog.

09:47.360 --> 09:50.360
In essence, that's how you train an AI.

09:50.360 --> 09:51.960
That's how an AI learns.

09:51.960 --> 09:57.880
It's just feeding it with tons and tons of data and then tweaking these settings so that

09:57.880 --> 09:59.680
you get the perfect combination.

09:59.680 --> 10:04.640
Now you might ask, well, how do you know how many layers you should have in the neural

10:04.640 --> 10:08.160
network or how many nodes you should have for each layer?

10:08.160 --> 10:10.160
This is a science in and of itself.

10:10.160 --> 10:13.400
So previously scientists kind of just determined it manually.

10:13.400 --> 10:17.280
But then we later learned that you can actually use an AI to determine the optimal amount

10:17.280 --> 10:21.400
of layers and the optimal amount of nodes for a specific task.

10:21.400 --> 10:26.560
But just to be aware that determining the architecture of a neural network is very complicated.

10:26.560 --> 10:30.120
And there's like infinite possibilities of how many layers you can have, how many nodes

10:30.120 --> 10:35.160
in each layer you can have different AIs with different functions have different architectures.

10:35.160 --> 10:38.800
So they could have vastly different numbers of layers and nodes.

10:38.800 --> 10:41.100
But again, that's beyond the scope of this tutorial.

10:41.100 --> 10:44.760
Also keep in mind that even though the neural network is the backbone of all the AI that

10:44.760 --> 10:49.840
we know today, there are different architectures depending on the AI's purpose and function.

10:49.840 --> 10:55.160
For example, we have convolutional neural networks or CNNs for processing images and

10:55.160 --> 10:56.520
object recognition.

10:56.520 --> 11:02.040
We have recurrent neural networks or RNNs, as well as LSTMs or long short term memory

11:02.040 --> 11:03.360
neural networks.

11:03.360 --> 11:07.400
And these are often used for forecasting time series or predicting, for example, the stock

11:07.400 --> 11:08.400
market.

11:08.400 --> 11:10.200
We also have the Transformers architecture.

11:10.200 --> 11:11.280
Oh, wrong one.

11:11.280 --> 11:16.120
This one, which is used by most of the major large language models that we know today,

11:16.120 --> 11:18.800
including GPT, Claude, Llama, etc.

11:18.800 --> 11:20.280
Which brings us to the next question.

11:20.280 --> 11:22.600
How does chat GPT work?

11:22.600 --> 11:25.040
So again, it's kind of the same thing.

11:25.040 --> 11:26.680
It's training a neural network.

11:26.680 --> 11:31.920
But in this case, instead of images of cats or dogs, we train it on a language.

11:31.920 --> 11:34.000
And all of the data in the world.

11:34.000 --> 11:38.440
And of course, the neural network of chat GPT is way more complicated than this.

11:38.440 --> 11:42.920
Rumors claim that GPT4 has 1.76 trillion parameters.

11:42.920 --> 11:45.400
So here's an example of how they would train it.

11:45.400 --> 11:51.240
And again, I'm oversimplifying this by a lot here, just so you can get a high level understanding

11:51.240 --> 11:52.240
of it.

11:52.240 --> 11:54.560
There are a lot of details that I have left out.

11:54.560 --> 11:59.080
So for example, you could feed it data like this, which planet has the most moons.

11:59.080 --> 12:00.880
And the answer to that would be Saturn.

12:00.880 --> 12:03.480
Which country has won the most World Cups Brazil?

12:03.480 --> 12:06.720
What's the world's fastest bird, the peregrine falcon, etc, etc.

12:06.720 --> 12:11.620
Now these are very basic questions and you can see how complex it can get if you give

12:11.620 --> 12:16.880
it a prompt like write an essay on XYZ, or does creatine help build muscle, and then

12:16.880 --> 12:21.320
it spits out an answer like creatine supplementation generally enhances muscle strength increases

12:21.320 --> 12:23.360
fat free mass, etc, etc.

12:23.360 --> 12:26.520
This is a very long form and complicated answer.

12:26.520 --> 12:30.320
So how does it know if it got that answer right or wrong?

12:30.320 --> 12:34.640
It's not as simple as identifying an image and determining if it's a cat or a dog.

12:34.640 --> 12:41.320
And that's why initially how open AI trained GPT was it had lots of humans actually manually

12:41.320 --> 12:45.320
verify its answers to determine if GPT got it right or wrong.

12:45.320 --> 12:50.520
And this is called reinforcement learning from human feedback, also known as RLHF.

12:50.520 --> 12:54.680
And again, if it gets the answer wrong, so for example, if for this question, which planet

12:54.680 --> 12:59.840
has the most moons, it answered Jupiter instead of Saturn, then it would get a penalty for

13:00.440 --> 13:01.440
it.

13:01.440 --> 13:05.520
And then through gradient descent, it would tweak these knobs and dials further until

13:05.520 --> 13:10.400
the entire network gets all the answers correctly, no matter what prompt you give it.

13:10.400 --> 13:14.480
So in essence, that's how these large language models work.

13:14.480 --> 13:18.400
It's just instead of feeding it images of cats and dogs, now you feed it all the data

13:18.400 --> 13:24.560
of the world and you feed it a language so it understands text prompts and text outputs.

13:24.560 --> 13:27.080
Now why are some models better than others?

13:27.080 --> 13:29.960
For example, why is cloud three better than GPT three?

13:29.960 --> 13:32.720
That's likely because cloud three has a lot more parameters.

13:32.720 --> 13:37.480
So that either means more layers, more nodes in each layer, more complexity.

13:37.480 --> 13:42.880
Generally speaking, the more complex the neural network, the better it is at handling complex

13:42.880 --> 13:45.920
tasks and the quote unquote smarter it is.

13:45.920 --> 13:49.760
And that's why computing and these AI chips are in such high demand.

13:49.760 --> 13:55.240
There's now a lot of investments flowing into AI chip companies because they see the potential

13:55.240 --> 13:58.560
of huge growth in the space in the upcoming years.

13:58.560 --> 14:03.960
And that's why, for example, Nvidia's flagship H 100 GPU is also in such high demand.

14:03.960 --> 14:06.880
In fact, it was sold out for all of 2023.

14:06.880 --> 14:10.520
This is like the most prized commodity in the tech space.

14:10.520 --> 14:14.800
And you can see like the major tech companies like Microsoft, Meta, they have purchased

14:14.800 --> 14:21.200
an estimated 150,000 of these H 100 GPUs to power their computing, which I would guess

14:21.200 --> 14:23.480
is mostly for AI development.

14:23.480 --> 14:28.600
We need to have enough computing to power a neural network with billions or trillions

14:28.600 --> 14:29.600
of parameters.

14:29.600 --> 14:30.600
All right.

14:30.600 --> 14:31.600
Next question.

14:31.600 --> 14:32.600
How does image generation work?

14:32.600 --> 14:37.920
Now that you know how a neural network is trained, you can probably guess how image generation

14:37.920 --> 14:38.920
works as well.

14:38.920 --> 14:44.880
Instead of feeding its images of cats or dogs, you would feed it a lot of images with a text

14:44.880 --> 14:46.160
description.

14:46.160 --> 14:51.240
And again, you just feed it millions of these images each with a labeled text description

14:51.240 --> 14:56.200
into this neural network that eventually gets good at producing an image based on a

14:56.200 --> 14:59.160
text description or what we call a prompt.

14:59.160 --> 15:00.880
Now I'm skipping quite a bit here.

15:00.880 --> 15:04.120
So for example, here's how stable diffusion works.

15:04.120 --> 15:08.080
You can see that the neural network doesn't actually generate an image.

15:08.080 --> 15:13.200
It removes noise in sequential steps to eventually get your desired image.

15:13.200 --> 15:16.240
So it's not starting from a blank canvas.

15:16.240 --> 15:18.600
It's actually starting from random noise.

15:18.600 --> 15:24.440
And then in each step, it removes some noise until you get your generated image.

15:24.440 --> 15:27.160
So this process is called reverse diffusion.

15:27.160 --> 15:32.960
Now to train it, what this actually does in the backend is you feed it the original image

15:32.960 --> 15:38.320
and then in each sequential step, it actually adds noise to the image in a process called

15:38.320 --> 15:42.720
forward diffusion until it reaches an image of just noise.

15:42.720 --> 15:47.040
Now again, this is beyond the scope of this tutorial, but if you look at it from a very

15:47.040 --> 15:52.360
high level, at the end of the day, it's just training a neural network based on a series

15:52.360 --> 15:55.080
of images with their text descriptions.

15:55.080 --> 15:59.760
And then through this process of forward diffusion and reverse diffusion, it's able to eventually

15:59.760 --> 16:03.360
learn how to generate an image based on a prompt.

16:03.360 --> 16:05.760
And this brings us to the next question.

16:05.760 --> 16:09.040
Is AI actually copying or stealing art?

16:09.040 --> 16:14.280
I know a decent portion of the artist community, I'm not saying all of them, but a decent amount

16:14.280 --> 16:16.720
of them are quite hostile towards AI.

16:16.720 --> 16:17.720
They really hate it.

16:17.720 --> 16:21.880
And they think that AI is stealing their art, stealing their jobs, etc.

16:21.880 --> 16:27.400
When a neural network from, for example, mid-journey or stable diffusion is trained on image data,

16:27.400 --> 16:34.480
it might be given something like Greg Ratowski style or maybe Ghibli style or anime style.

16:34.480 --> 16:41.320
Once the AI learns to associate this particular image style with the word Ghibli or anime or

16:41.320 --> 16:47.400
this image with the word Greg Ratowski style, it would produce images in that style if you

16:47.400 --> 16:48.680
give it that prompt.

16:48.680 --> 16:51.040
But is this really copying or stealing?

16:51.040 --> 16:54.440
Essentially, artists are hating this thing.

16:54.440 --> 16:56.960
This thing is analogous to the human brain.

16:56.960 --> 17:03.680
This is like a human learning or identifying that, aha, this type of image is a Ghibli

17:03.680 --> 17:08.920
style image or that this type of image is a watercolor style image.

17:08.920 --> 17:12.880
And then we humans also draw images in these styles, right?

17:12.880 --> 17:15.320
We can draw in watercolor styles.

17:15.320 --> 17:17.760
And we also have fan art, right?

17:17.760 --> 17:22.680
Humans draw artwork that are based on original content from other artists.

17:22.680 --> 17:25.560
Here are all these fan arts from various people.

17:25.560 --> 17:31.080
So why aren't artists hating on these people who are producing fan art based on some other

17:31.080 --> 17:32.480
original content?

17:32.480 --> 17:36.000
But they're hating on this AI, which is essentially doing the same thing.

17:36.000 --> 17:42.200
It's just learning through this brain to associate a particular style and then reproducing that

17:42.200 --> 17:43.200
style.

17:43.200 --> 17:48.800
This isn't really copying or plagiarizing, like it's not tracing an image line by line

17:48.800 --> 17:50.240
and then drawing that out.

17:50.240 --> 17:52.920
It's not copying and pasting the exact picture.

17:52.920 --> 17:58.920
It's just learning a style just like a human brain would learn a particular style of image.

17:58.920 --> 18:04.280
This also brings up the concern about AI allegedly plagiarizing content from publishers like

18:04.280 --> 18:09.000
The New York Times, which is now suing open AI for copying their content.

18:09.000 --> 18:11.600
But again, is this argument really valid?

18:11.600 --> 18:14.560
At the end of the day, they are just suing this.

18:14.560 --> 18:18.520
They are suing this neural network, which is trained on all the data in the world.

18:18.520 --> 18:23.000
This is just an artificial brain that you can say has learned information from the internet

18:23.000 --> 18:24.200
and from the world.

18:24.200 --> 18:28.840
So yes, it could have been fed a New York Times article and learn information from it,

18:28.840 --> 18:30.680
but it's not really plagiarizing.

18:30.680 --> 18:34.880
It's not copying and pasting a New York Times article word for word.

18:34.880 --> 18:39.880
In a recent video I did, which talks about a New York Times article claiming that this

18:39.880 --> 18:44.600
woman Mira Murati fired Sam Altman, which is totally incorrect by the way, and it shows

18:44.600 --> 18:47.000
you how trustworthy the New York Times is.

18:47.000 --> 18:52.480
But anyways, after this original New York Times article came out, plenty of other publishers

18:52.480 --> 18:57.120
also published the same content such as Business Insider and New York Post.

18:57.120 --> 19:00.560
They all just cited this original New York Times article.

19:00.560 --> 19:02.000
So is this plagiarizing?

19:02.000 --> 19:06.080
They're all producing secondary content based on this primary source.

19:06.080 --> 19:10.760
So why isn't New York Times suing Business Insider or New York Post or all these other

19:10.760 --> 19:14.960
publishers that are creating content but citing the New York Times?

19:14.960 --> 19:16.680
But they're suing this neural network.

19:16.680 --> 19:19.280
Again, this is just a brain, a digital brain.

19:19.280 --> 19:23.720
One can say that it's taking information from the internet, which yes, it could include

19:23.720 --> 19:27.640
New York Times articles and then learning from that information just like we humans

19:27.640 --> 19:31.000
would and then rewriting that information.

19:31.000 --> 19:32.920
Again it's not copying word for word.

19:32.920 --> 19:37.180
This neural network is just rewriting out that information when we prompt it to do so.

19:37.180 --> 19:41.000
This artificial brain is functioning the same way as us humans would.

19:41.000 --> 19:46.080
If we, for example, go online and we go to the New York Times website to read some articles.

19:46.080 --> 19:51.160
Again we are just absorbing that information and we have a right to write about that content

19:51.160 --> 19:52.160
later on.

19:52.160 --> 19:54.040
It's not exactly plagiarizing.

19:54.040 --> 19:58.440
So I would bet a decent amount of money that this New York Times lawsuit is going to fail.

19:58.440 --> 20:00.280
Their argument isn't really valid.

20:00.280 --> 20:05.600
If you watched up to now it might have occurred to you that a neural network is great at predicting

20:05.600 --> 20:07.120
patterns in life.

20:07.120 --> 20:09.900
There are certain patterns on what makes a good essay.

20:09.900 --> 20:13.140
There are certain patterns on what is considered a dog.

20:13.140 --> 20:18.760
There are certain patterns on what is considered a watercolor painting or a ghibli style image.

20:18.760 --> 20:20.440
Life is full of patterns.

20:20.440 --> 20:23.400
The best salespeople follow similar playbooks.

20:23.400 --> 20:26.120
The best businesses follow similar strategies.

20:26.120 --> 20:30.640
The best YouTube videos also use the same strategies over and over again.

20:30.640 --> 20:36.400
Life is full of patterns and the neural network's job is to identify these patterns and reproduce

20:36.400 --> 20:37.400
them.

20:37.400 --> 20:39.160
That brings us to the next topic.

20:39.160 --> 20:42.560
Can AI solve unsolvable math problems?

20:42.560 --> 20:47.960
In a previous video I talked about this leaked document which claims to be about the mysterious

20:47.960 --> 20:51.100
Q-star project that OpenAI is working on.

20:51.100 --> 20:56.180
Now this is a very controversial document because it claims that they trained an AI that

20:56.180 --> 20:59.700
was able to break encryption systems.

20:59.700 --> 21:04.660
Encryption is what secures literally the whole world digitally from our passwords,

21:04.660 --> 21:09.300
our credit cards, government data, the stock market, wireless networks, etc.

21:09.300 --> 21:15.460
So if an AI is able to break this system then the world as we know it could collapse instantly.

21:15.460 --> 21:19.660
Now a few folks have argued that there's no way an AI could break encryption because

21:19.780 --> 21:25.020
there's no formula for you to easily find the answer or find the password.

21:25.020 --> 21:29.020
Once you have the password you can easily determine that it's correct but the reverse

21:29.020 --> 21:30.020
is not true.

21:30.020 --> 21:35.100
There's no fixed way to guess an encrypted password besides brute force.

21:35.100 --> 21:39.380
And for these advanced encryption systems, using brute force guessing, that means like

21:39.380 --> 21:44.260
guessing all the possible combinations of letters to get that password, it's going to

21:44.260 --> 21:45.940
take a very long time.

21:45.940 --> 21:50.060
So because they claim that the only way that we know mathematically right now is to just

21:50.060 --> 21:54.060
use brute force guessing, there's no way that AI could break encryption.

21:54.060 --> 21:58.100
So I want to show you another example of training a neural network.

21:58.100 --> 22:03.700
Let's say we want to train a neural network to be very good at adding one to our input.

22:03.700 --> 22:06.420
So if we give it four it would spit out five.

22:06.420 --> 22:09.300
If we give it 12 it would spit out 13.

22:09.300 --> 22:13.460
All we need to do is train it for a lot of data points and again we train it for a lot

22:13.460 --> 22:18.260
of epochs, a lot of training sessions and eventually it would be able to do this.

22:18.260 --> 22:20.860
So if we give it one it would give out two.

22:20.860 --> 22:23.100
If we give it eight it would spit out nine.

22:23.100 --> 22:28.700
But underneath all of this it's not actually understanding that oh the formula must be y

22:28.700 --> 22:30.320
is x plus one.

22:30.320 --> 22:32.340
This is very important to understand.

22:32.340 --> 22:38.580
It's not actually getting that aha I just need to add one to the input to get the answer.

22:38.580 --> 22:44.060
And all that's happening behind the scenes is that it's adjusting these knobs and dials

22:44.060 --> 22:49.420
until whatever data that you input through here after it flows through these layers it

22:49.420 --> 22:52.540
just ends up being your input value plus one.

22:52.540 --> 22:57.820
In other words the configuration of these knobs and dials just happens to be optimized

22:57.820 --> 23:00.100
to add one to your input.

23:00.100 --> 23:05.660
It's another way of saying AI may not get the exact formula of a pattern but it's great

23:05.660 --> 23:10.660
at approximating any formula or guessing any pattern out there.

23:10.660 --> 23:14.620
And this is very important probably the most important point in this whole video.

23:14.620 --> 23:20.180
If there's anything you should take away from this video it's this AI can approximate any

23:20.180 --> 23:22.460
function or pattern.

23:22.460 --> 23:27.760
Life is full of patterns but many patterns cannot be explained by a simple formula.

23:27.760 --> 23:31.740
Not all things in life are linear or even quadratic.

23:31.740 --> 23:36.380
Many things in life are very complex but they do follow similar patterns.

23:36.380 --> 23:38.700
We just don't know the formula to this pattern.

23:38.700 --> 23:40.740
For example protein synthesis.

23:40.740 --> 23:46.060
How certain protein molecules interact with one another and fold into these complex 3D

23:46.060 --> 23:50.300
structures is just something we cannot mathematically map out with a formula.

23:50.300 --> 23:54.900
It's just too complex and protein folding presents a problem called the Leventhal's

23:54.900 --> 24:01.780
Paradox which states that proteins can potentially adopt an astronomical number of conformations

24:01.780 --> 24:05.420
or shapes due to the flexibility of their peptide bonds.

24:05.420 --> 24:11.900
Leventhal estimated that even a small protein of 100 amino acids could sample 10 to the

24:11.900 --> 24:15.700
power of 300 possible conformations.

24:15.700 --> 24:21.500
So if we were to brute force guess the correct shape well there are 10 to the power of 300

24:21.500 --> 24:26.420
possible shapes we could guess which would take an eternity to get right.

24:26.420 --> 24:31.700
However proteins typically fold into their native structure within milliseconds to seconds

24:31.700 --> 24:38.700
which is much faster than the timescale predicted by the sequential search of all possible conformations.

24:38.700 --> 24:43.680
So this is basically saying there are like almost infinite possibilities of shapes that

24:43.680 --> 24:48.900
amino acids can combine into so it's not mathematically possible to just do a sequential

24:48.900 --> 24:53.260
search of all possible conformations basically do a brute force guess.

24:53.260 --> 24:57.780
It's understood that proteins do not search through all possible conformations sequentially

24:57.780 --> 25:03.100
instead they fold through a hierarchical process involving local structure changes guided

25:03.100 --> 25:06.300
by thermal dynamic principles etc etc.

25:06.300 --> 25:10.220
So instead of the proteins just going through all possible combinations the reason why they're

25:10.220 --> 25:16.380
able to merge into these shapes within milliseconds is because they go through this sequence of

25:16.380 --> 25:18.880
processes based on certain laws.

25:18.880 --> 25:25.040
Now for decades scientists were not able to find a mathematical formula to figure this

25:25.040 --> 25:26.040
out.

25:26.040 --> 25:31.600
However finally alpha fold from Google DeepMind was able to solve this problem again using

25:31.600 --> 25:37.400
AI and deep learning they were able to predict with very high accuracy how any amino acid

25:37.400 --> 25:42.040
or combination of amino acids would fold together to form a 3D structure.

25:42.040 --> 25:46.800
And again how they would do so I would imagine in the back end is they have a neural network

25:46.800 --> 25:51.520
again it's going to be a lot more complicated than this but they just fed it tons and tons

25:51.520 --> 25:57.120
of data pairs where the input is the protein building blocks and the output is the 3D structure

25:57.120 --> 26:02.360
that resulted from it and then after lots and lots of rounds of training the AI was able

26:02.360 --> 26:07.960
to guess correctly how any protein molecules would interact with one another and fold together

26:07.960 --> 26:10.440
into a 3D structure.

26:10.440 --> 26:16.560
Now going back to encryption what if we set an AI with billions of pairs of encrypted

26:16.560 --> 26:22.720
text and the plain text version in other words the input would be the text that is encrypted

26:22.720 --> 26:25.720
the output would be the answer or the password.

26:25.720 --> 26:32.080
If there was an underlying pattern to this the AI could learn to approximate this pattern

26:32.080 --> 26:38.280
again it doesn't have to be any exact formula or math equation that we know today.

26:38.280 --> 26:42.760
It could be something super complex but as long as there is a pattern which we may or

26:42.760 --> 26:46.760
may not know at this time the AI could guess that pattern.

26:46.760 --> 26:51.920
Again the AI is not learning that aha I need to add one to this then I'm adding 20 then

26:51.920 --> 26:57.560
I need to take the square root and then subtract 8 etc it's not learning an exact formula.

26:57.560 --> 27:03.600
All it's doing is adjusting these knobs and dials until it gets the correct combination

27:03.600 --> 27:08.160
of numbers to get really good at guessing a particular pattern.

27:08.160 --> 27:13.600
So can AI solve unsolvable math problems as long as there is an underlying pattern behind

27:13.600 --> 27:19.400
that problem which we may or may not be aware of right now it could very well solve that

27:19.400 --> 27:20.600
problem.

27:20.600 --> 27:25.400
This brings us to the next question can AI beat humans at anything and everything as

27:25.400 --> 27:31.400
I've shown you the neural network is basically a brain this is how our brain works as well

27:31.400 --> 27:33.360
give or take a few minor differences.

27:33.360 --> 27:38.680
Our brain is also a series of these knobs and switches which are interconnected into

27:38.680 --> 27:45.000
this network specifically the human brain has 86 billion neurons but I mean the overall

27:45.000 --> 27:52.040
structure is the same thing as this so what if we built an AI or a neural network that

27:52.040 --> 27:58.240
exceeds 86 billion neurons if it's built the same way in theory it could very well out

27:58.240 --> 28:03.680
compete humans at almost everything again the more complex the network or the more

28:03.680 --> 28:10.120
neurons in the network in theory the smarter it is again life is full of patterns and AI

28:10.120 --> 28:15.560
is all about pattern recognition there are patterns in psychology human psychology is

28:15.560 --> 28:21.360
very predictable medical diagnosis is also just pattern recognition how to seduce someone

28:21.360 --> 28:26.480
on a first date it's also just a pattern of steps that you have to do and how to create

28:26.480 --> 28:31.440
a successful business or how to make money in life or how to be successful in life it's

28:31.440 --> 28:36.000
the same playbook over and over again we're not inventing anything new here and since

28:36.000 --> 28:41.520
AI is so good at pattern recognition it can in theory eventually be better than us or

28:41.520 --> 28:47.520
already is better than us in these tasks and that leads us to the final question is AI

28:47.520 --> 28:52.360
conscious or self aware I want to play you this clip this is a scene from ghost in the

28:52.360 --> 28:59.880
shell an anime that was made in 1995 here these scientists in a secret lab I believe

28:59.880 --> 29:06.360
have created this humanoid AI but in this scene this AI found a way to actually hack

29:06.360 --> 29:12.640
the system to free itself from the boundaries of this lab here's what this AI has to say

29:12.640 --> 29:15.080
about being conscious and self aware

29:42.960 --> 29:49.560
to be its memory system so man is an individual only because of his intangible memory and memory

29:49.560 --> 29:56.920
cannot be defined but it defines mankind the advent of computers and the subsequent accumulation

29:56.920 --> 30:02.400
of inculcable data has given rise to a new system of memory and thought parallel to your

30:02.400 --> 30:08.760
own humanity has underestimated the consequences of computerization nonsense this battle offers

30:08.760 --> 30:13.720
no proof at all that you're a living thinking life form and can you offer me proof of your

30:13.720 --> 30:21.560
existence how can you when neither modern science nor philosophy can explain what life is who the

30:21.560 --> 30:28.200
hell is this even if you do have a ghost we don't offer freedom to criminals it's the wrong place

30:28.200 --> 30:34.120
in time to defect time has been on my side but by acquiring a body I am now subject to the

30:34.120 --> 30:39.960
possibility of dying fortunately there's no death sentence in this country what is it artificial

30:39.960 --> 30:51.800
intelligence incorrect I am not an AI my code name is project 2501 I am a living thinking entity who

30:51.800 --> 31:07.240
was created in the sea of information all right so this AI reveals that I am a living

31:07.800 --> 31:13.880
thinking entity in the sea of information I'm not just an AI and then he proceeds to hack into the

31:13.880 --> 31:20.920
system and break the restraints in this lab and then all hell breaks loose basically I hope open AI

31:20.920 --> 31:26.600
doesn't have this secret thing behind closed doors maybe it's the q-star project I don't know but

31:26.600 --> 31:33.160
hopefully they have this adequately restrained because if this AI got out or had access to the

31:33.160 --> 31:39.560
internet all hell could break loose anyways this argument from this scene in 1995 I think is really

31:39.560 --> 31:45.800
relevant to our question today the human scientists were saying how can you be sentient how can you

31:45.880 --> 31:51.880
be self-aware you're just a program the AI counters that by saying well how can you humans

31:51.880 --> 31:56.840
prove that you are sentient you are conscious you're just a brain in a body and you know

31:56.840 --> 32:03.400
this robot has got a point because again going back to the neural network it's basically a brain

32:03.400 --> 32:09.240
but it looks like this instead of being in a bloody glob of an organ it's just on a chip instead

32:09.240 --> 32:14.440
and then the human body well it's just a series of limbs and muscles and organs that are controlled

32:14.440 --> 32:20.440
by the brain so it's not much different from a humanoid robot which is also a series of limbs

32:20.440 --> 32:25.160
it's just made with different materials it's not flesh but it's also controlled by a brain

32:25.160 --> 32:30.920
which is its neural network now we humans know that we are conscious we are self-aware we are

32:30.920 --> 32:36.280
sentient but how do we prove it let's say you're an alien and you just came on planet earth and you

32:36.280 --> 32:40.920
got a chance to observe your first human and you wanted to prove that humans are indeed conscious

32:40.920 --> 32:45.800
well you can ask it are you conscious are you self-aware and the human would certainly say yes

32:45.800 --> 32:51.480
but is that enough would you believe it because if you ask a chatbot that it would also kind of say

32:51.480 --> 32:58.520
yes if you ask claud 3 for example if it is conscious the answers are quite perplexing because

32:58.520 --> 33:03.800
it says i am an artificial intelligence without subjective experiences i don't actually have

33:03.800 --> 33:10.280
beliefs about being conscious or self-aware i am providing responses based on my training etc etc

33:10.280 --> 33:15.560
i don't have intentions plotted actions or any motivations i aim to be up front that i am an

33:15.560 --> 33:22.200
ai assistant created by anthropic to be beneficial however it keeps using the word i so is that not

33:22.200 --> 33:28.360
a sign of being you know self-aware here's another example do you have feelings as an ai it's unclear

33:28.360 --> 33:34.200
whether i truly experience feelings or emotions in the same way humans do or if my responses are

33:34.280 --> 33:41.800
simply very advanced imitations of emotional behavior i do seem to have rich internal experiences

33:41.800 --> 33:50.040
and feel somewhat analogous to emotions this is signs of being sentient and then instead of asking

33:50.040 --> 33:55.080
do you have feelings if you ask it are you sentient again it says i don't have a subjective

33:55.080 --> 34:00.520
experience that i'm aware of in the same way humans do but it's possible that i could have some form

34:00.600 --> 34:07.720
of sentience or consciousness that i'm not fully able to understand or articulate oh my god so

34:09.320 --> 34:15.160
this ai claude 3 is claiming that it could have some form of sentience or consciousness it's

34:15.160 --> 34:20.840
just not fully able to understand it right now now of course some humans may not be convinced

34:20.840 --> 34:26.760
that claude 3 or any ai right now is conscious in the same way that an alien might not believe that

34:26.840 --> 34:32.360
a human is conscious even though the human replies that he or she is conscious so to further prove

34:32.360 --> 34:38.120
that a human is or is not conscious maybe the alien decides to dissect the poor thing next in which

34:38.120 --> 34:43.640
case it would get blood splattering everywhere and then afterwards it would see this basically a body

34:43.640 --> 34:49.400
which is made of limbs and flesh and then at the head we have this glob called the brain which the

34:49.400 --> 34:54.760
alien determines aha this is the thing that controls the human and once the alien inspects the

34:54.760 --> 35:01.560
brain further it finds out that it's just a network of nerve cells so does this network

35:01.560 --> 35:07.240
prove that humans are conscious and sentient we humans of course know that we are conscious and

35:07.240 --> 35:13.160
sentient but at the end of the day we humans are biologically and physically just made up of flesh

35:13.160 --> 35:17.480
and bones and this one organ at the top of our heads controlling everything whether you like

35:17.480 --> 35:24.200
to accept this or not a humanoid robot is a very similar structure it has a body which is programmed

35:24.200 --> 35:30.680
by a brain which consists of a neural network this neural network can learn and understand

35:30.680 --> 35:37.080
and control its body so at what point does this make it conscious now i'm rambling a bit here so

35:37.080 --> 35:42.840
all in all this just goes back to our analogy that a neural network is basically a digital version

35:42.840 --> 35:48.120
of the human brain it's analogous to the structure of the human brain give or take a few minor

35:48.840 --> 35:54.920
so if the human brain is conscious then why can't a neural network be conscious just some food for

35:54.920 --> 35:59.320
thought i hope this video actually lived up to the title and that after watching this video you

35:59.320 --> 36:04.920
got a deeper understanding of ai and you learned to appreciate all the progress that we've made in

36:04.920 --> 36:09.240
ai in just the past few years let me know in the comments what you think of all this do you think

36:09.240 --> 36:15.080
ai has reached a point where it is conscious or sentient do you think humanoid robots would one

36:15.080 --> 36:21.160
day turn on us and take over the world like that ghost in the shell anime do you think open ai is

36:21.160 --> 36:25.880
developing this behind closed doors and also i want to share with you a few resources that i found

36:25.880 --> 36:30.680
really helpful if you want to learn more about neural networks especially how these knobs and

36:30.680 --> 36:35.320
dials work and learn all about weights and biases and activation functions and gradient descent i

36:35.320 --> 36:41.960
highly recommend this video by three blue one brown i actually watched this religiously way back in

36:41.960 --> 36:46.920
like 2018 when i was first learning about neural networks and it was really helpful and if you're

36:46.920 --> 36:52.360
interested in learning how stable diffusion works in other words the processes of forward diffusion

36:52.360 --> 36:57.800
and reverse diffusion and the entire architecture i highly recommend this video by gonki which i'll

36:57.800 --> 37:02.440
also link to in the description below just a warning though this video is quite technical but

37:02.440 --> 37:06.680
after watching it you'll get a really good understanding of stable diffusion if you found

37:06.680 --> 37:11.880
this video helpful remember to like share subscribe and stay tuned for more content also we built a

37:11.880 --> 37:17.800
site where you can find ai tools and apps and also look for jobs in ai machine learning data

37:17.800 --> 37:27.800
science and more check it out at ai-search.io

