start	end	text
0	3840	AI, as we know it today, is actually quite dumb.
3840	10320	Yes, this includes chatGPT, stable diffusion, Sora, and all the other state-of-the-art models
10320	12200	that we have right now.
12200	16880	They're still very incapable and inefficient, and the future generation of AI will look
16880	19560	very different from what we have now.
19560	24840	So in this video, I'm going to explain why the current generation is so limited and
24840	28560	what the future generation of AI will look like.
28560	31440	First we need to understand the mechanics of AI.
31440	37400	As we know today, all AI is based on the neural network, which is designed based on the human
37400	38400	brain.
38400	44000	This is basically a network of nodes in which information flows through from one end to
44000	45000	the other.
45000	50880	Now, this is going to be a very simplified explanation of how a neural network works.
50880	56400	I'm explaining this for people without a technical background in AI, so if you do have
56400	59680	experience in AI, feel free to skip this section.
59680	65320	Each dot in a neural network is called a node or neuron, and each line of nodes is
65320	66880	called a layer.
66880	71700	You might have heard of the term deep learning, or deep neural networks.
71700	76760	This is basically a neural network with many layers, hence it is very deep.
76760	81200	Each node determines how much information flows through to the next layer.
81200	83400	Now again, this is an oversimplification.
83400	88080	There are a lot of settings like weights and biases and activation functions, but basically
88080	93600	just think of this neural network as a series of dials and knobs, which determine how much
93600	97120	information flows through to the next layer.
97120	98720	Here's a simple example.
98720	104240	Let's say we have this neural network, which is designed to determine whether an image
104240	106160	is a cat or a dog.
106160	110960	For its input, we would feed it an image of a cat or a dog, and this image would be broken
110960	117000	down into data, also known as tokens, which are then fed through this neural network.
117000	122360	Eventually after the data flows through all these layers, it reaches the end layer, which
122360	126560	would conclude whether the image is a cat or a dog.
126560	128960	Now what about training a model?
128960	130240	How does that work?
130240	135280	Well, a neural network needs to undergo usually millions of rounds of training to learn how
135280	136280	to do something.
136280	140840	Here's an example of how one round of training would look like.
140840	146320	Let's say you input an image of a dog, and then this image would be broken down into data,
146320	150120	which flows through this neural network, and it spits out the answer.
150120	151120	This is a dog.
151120	156080	Well, in that case, since it got the answer correct, it's likely that these dials and
156080	160560	knobs, which we can also refer to as weights, are set correctly.
160560	165320	If it gets the answer right, well, we don't really need to tweak these weights further.
165320	167440	However, what if it gets it wrong?
167440	169680	What if it says that this is a cat?
169680	173000	Well, in that case, it would incur a penalty.
173000	177720	And this penalty would cause the weights in this neural network to be updated so that
177720	180760	this penalty would be minimized in the future.
180760	185440	Specifically, the weights would be updated from the last layer to the next layer back
185440	190280	to the next layer back in a process which is called back propagation all the way until
190280	193080	it reaches the first layer of nodes.
193080	196240	And usually one round of training isn't good enough.
196240	200840	So the network would undergo millions of rounds of training where the weights would
200840	206280	be slightly tweaked to minimize the penalty incurred from any errors.
206280	211760	And this goes on and on until finally, we reach the right configuration of dials and
211760	217240	knobs so that this neural network can very accurately determine whether any image is
217240	219000	a cat or a dog.
219000	223280	And this is how AI models that we know today are trained as well.
223280	229960	So for example, GPT is basically a neural network, but these dials and knobs are optimized
229960	232720	for understanding natural language.
232720	237160	Stable diffusion is another neural network where the dials and knobs are optimized for
237160	238960	image generation.
238960	245460	Now again, this is very much an oversimplification and the architecture or basically the design
245460	248560	of the neural network is also very important.
248560	251880	For example, how many layers should we have?
251880	255400	How many nodes in each layer should we have?
255400	261680	There are also many different architectures, such as the transformer model for large language
261680	269080	models or LSTM for time series data or convolutional neural networks for object detection and image
269080	270640	classification.
270640	276120	But in a nutshell, the backbone behind all these AI models is just a neural network,
276120	281160	which has a preconfigured set of dials and knobs to do the job accurately.
281160	285520	So now that you understand how the current generation of AI works, let's look at the
285520	288200	biggest limitations of this.
288200	293400	First of all, once the model is finished training, the weights or basically these dials and knobs
293400	295120	are fixed in value.
295120	300760	When the user asks chat GPT something or when the user uses stable diffusion to generate
300760	304320	an image, these dials and knobs do not change in value.
304320	308360	In other words, all the AI models that we have today are fixed.
308360	313080	Think of this as a brain that cannot learn or get any smarter.
313080	319160	For example, GPT-4 cannot continue learning and become smarter and smarter with time.
319160	326160	If we want a smarter model, well, we need to train a new generation of GPT such as GPT-4.0
326160	329120	or GPT-5 or whatever you want to call it.
329120	330720	Same with stable diffusion.
330720	336040	For example, stable diffusion 2 cannot get smarter and generate better images as we use
336040	337180	it more and more.
337180	342780	In order for it to improve, we currently need to train a new generation, also known as stable
342780	344280	diffusion 3.
344280	349540	And once stable diffusion 3 is finished training, well, that's as smart as it gets.
349540	353540	And if you don't think it's good enough, well, you need to train a new model.
353540	359220	So basically all the AI models that we have today are fixed in their intelligence and
359220	360420	their capabilities.
360420	366420	Again, think of this as a brain that has stopped growing and cannot learn or get smarter.
366420	368900	But this is not how the human brain works.
368900	374540	There's a term called neuroplasticity, which refers to how the brain can reorganize or
374540	380820	reconfigure itself by forming new neural connections over time in order to adapt to new environments
380820	382140	or learn new things.
382140	386780	And that's exactly what the next generation of AI can do, which we'll talk about in a
386780	387780	second.
387780	391220	But there's another huge limitation of current AI models.
391220	395500	They are extremely inefficient and compute intensive.
395500	400100	As you may know, AI is designed based on the architecture of the human brain.
400100	404540	So let's compare it to the efficiency of the human brain right now.
404540	409920	GPT-3 has 175 billion parameters.
409920	416300	This was trained using thousands of GPUs over several weeks or several months.
416300	425060	The total power required for training GPT-3 was estimated to be around 1287 megawatt hours
425460	426580	of electricity.
426580	432660	This is roughly equivalent to the monthly electricity consumption of 1500 homes in the
432660	433660	USA.
433660	438140	Now, keep in mind GPT-3 was completed in 2020.
438140	439700	That's four years ago.
439700	443140	The latest version, GPT-4, is closed source.
443140	447980	So we don't actually know its architecture or how long it took to train.
447980	455020	But we do know that it has around 1.76 trillion parameters, 10 times more than GPT-3.
455020	460860	Keep in mind that the amount of computations required scales exponentially as the parameter
460860	462620	size gets larger.
462620	469940	So from a rough calculation, GPT-4 could have taken around 41,000 megawatt hours of
469940	471780	energy to train.
471780	477820	That's enough energy to power around 47,000 homes in the US for a month.
477820	484220	The compute used to create these state-of-the-art models that we know today, such as GPT-4
484220	491020	Clawd-3 or Gemini 1.5 Pro, requires massive data centers and a lot of energy.
491020	496140	That's why tech giants are scrambling to invest and build even bigger data centers,
496140	499900	because they know that compute is the main limitation here.
499900	506220	And that's exactly why Microsoft and OpenAI are planning a $100 billion Stargate project
506220	509060	to build the biggest data center in the world.
509060	511740	All of this is for more compute.
511740	514500	Now contrast this to the human brain.
514500	520420	Some might say the human brain is still more intelligent than GPT-4, at least in some regards.
520420	528020	The human brain only uses 175 kilowatt hours in an entire year, and it gets this energy
528020	530980	in the form of calories from the food we eat.
530980	538980	So training GPT-4 is estimated to require approximately 234,000 times more energy than
538980	542460	what the human brain uses in an entire year.
542460	548100	In other words, the energy required to train GPT-4 once could power the human brain for
548100	552060	over 234,000 years.
552060	556300	Now I gave this comparison to show you that there's something fundamentally wrong with
556300	558020	AI models today.
558020	563100	They are very energy inefficient, and they take up a lot of compute.
563100	566660	It's not even close to the efficiency of the human brain.
566660	571500	So the next generation of AI has to solve this inefficiency problem as well, otherwise
571500	573180	it will not be sustainable.
573180	579020	So to summarize the major limitations of current AI models is number one, they are fixed and
579020	584780	unable to improve or learn further after being trained, and number two, they're also very
584780	587580	energy intensive and inefficient.
587580	591900	These are the two biggest problems of the current generation of AI.
591900	594300	Now let's enter the next generation.
594300	598900	We aren't there yet, but there are a few possible architectures that are being discussed
598900	601020	and developed as we speak.
601020	604660	The first architecture is called liquid neural networks.
604660	610460	Now liquid neural networks are designed to mimic the flexibility or the plasticity of
610460	611740	the human brain.
611740	617620	The human brain is very flexible and can reorganize or reconfigure itself over time,
617620	623420	and this ability allows the brain to adapt to new situations or learn new skills or compensate
623420	625540	for injury and disease.
625540	630900	For example, when you learn something new, your brain changes structurally and functionally
630900	633220	to accommodate the new information.
633220	637820	Learning a new language can lead to changes in the brain's structure and function, such
637820	641780	as increased density of gray matter in the left hemisphere.
641780	646020	The brain can also reconfigure itself to recover from injury.
646020	651820	For example, after a traumatic brain injury, physical therapy and cognitive exercises can
651820	656140	help rewire the brain to regain lost functions.
656140	660860	And for people who've lost a sense, like sight or hearing, the brain will reorganize
660860	666140	itself to compensate for the loss and make other senses become more acute.
666140	672140	So this flexibility, this plasticity is exactly what liquid neural networks are designed to
672140	673460	have.
673460	677300	Liquid neural networks can adapt in real time to new data.
677300	683060	This means that the configuration of the neural network can change as it receives new inputs,
683060	684980	and that's why it's called liquid.
684980	690220	These connections in the network and these dials and knobs are fluid, so they can change
690220	692780	dynamically over time.
692780	698260	Liquid neural networks also retain what they have learned while incorporating new information.
698260	703660	This is similar to how our brains can remember old information while learning new things.
703660	706860	So here's how liquid neural networks work.
706860	711420	They have three main components, much like a traditional neural network.
711420	714980	It has an input layer, which receives the input data.
714980	719940	But then in the middle, we have this liquid layer, otherwise known as a reservoir.
719940	723100	This is the core component of a liquid neural network.
723100	726620	And it's basically a large recurrent neural network.
726620	732460	Think of this as a big bowl of water in which each splash creates a ripple.
732460	737540	These ripples are basically the neurons in this network reacting to inputs.
737540	743700	The reservoir acts as a dynamic system that transforms the input data into a high dimensional
743700	746700	representation called reservoir states.
746700	753060	And this reservoir's rich dynamics and transformations capture the complex temporal patterns in the
753060	754460	input data.
754460	757140	And then finally, we have the output layer.
757140	762380	This layer receives the reservoir states and maps them to the desired output using what
762380	764620	is called a readout function.
764620	769580	In layman terms, this is a layer that looks at the ripples in the reservoir and tries
769580	772060	to understand what it all means.
772060	777780	It takes the dynamic patterns from the reservoir and makes predictions or decisions from it.
777780	784620	The key aspect of liquid neural networks is this reservoir layer, which remains untrained
784620	787340	during the entire learning process.
787340	792900	Finally the output layer is trained to map the reservoir states to the target outputs.
792900	795540	In other words, to understand what these ripples mean.
795540	800660	And because this reservoir remains fluid and flexible throughout time, it's not fixed
800660	805740	in value, that allows this liquid neural network to basically adapt to new data and learn
805740	807220	new things.
807220	810260	Here's how you would train a liquid neural network.
810260	815240	The connections between neurons in the reservoirs are set up randomly at the start.
815240	820000	These connections typically stay the same and don't change during training.
820000	822400	Next you would feed the input layer some data.
822400	827380	And when this data is broken down into tokens and it reaches the reservoir layer, it causes
827380	832640	the neurons in the reservoir to react and create complex patterns, much like ripples
832640	833640	in water.
833640	838680	So as this input data creates ripples, you basically observe and analyze the patterns
838680	841040	created in the reservoir over time.
841040	843360	And that's exactly what the readout layer does.
843360	845720	It learns to recognize these patterns.
845720	850440	It's like learning, aha, this is what caused this type of ripple and that is what caused
850440	851720	this other type of ripple.
851720	856180	And eventually after lots and lots of rounds of training, the readout layer can make accurate
856180	859200	predictions based on observed patterns.
859200	864440	Again note that only the readout layer is trained, which is simpler and faster because
864440	867200	you're not adjusting anything in the reservoir layer.
867200	872960	This is much quicker and needs less compute compared to traditional neural networks.
872960	877240	That's because in neural networks that we know today, all the weights including those
877240	879520	in the hidden layers are trainable.
879520	884680	This means more parameters to optimize leading to longer training times and higher computational
884680	885880	requirements.
885880	892280	But in liquid neural networks, you don't adjust the weights of the reservoir during training.
892280	894620	Only the readout layer is trained.
894620	900480	And this significantly reduces the computational burden during training since fewer parameters
900480	902240	need to be optimized.
902240	904680	Plus it's a lot faster to train.
904680	907280	Thanks to our sponsor Bright Data.
907280	912560	Bright Data is an all-in-one platform designed to help businesses collect high quality web
912560	914520	data at scale.
914520	919360	This is especially useful for AI companies which require huge amounts of diverse and
919360	925400	high quality training data to build robust and unbiased AI models.
925400	930400	Collecting this training data manually can be time consuming and prone to errors.
930400	932520	And that's where Bright Data comes in.
932520	937640	With Bright Data, you can access high volume, high quality web data effortlessly.
937640	944040	From parsed validated data sets to custom scraping solutions, they've got you covered.
944040	948720	Get parsed and cleaned data sets ready to use on demand.
948720	954200	Customize any data set to fit your specific needs and benefit from reliable delivery and
954200	956080	full compliance.
956080	963160	In fact every 15 minutes their customers scrape enough data to train chat GPT from scratch.
963160	965360	That's a lot of data, to say the least.
965360	971560	They have many tools like the web scraper API, the proxy manager, and unblocking technologies
971560	977520	to help automate your data scraping at scale, allowing you to build reliable data sets to
977520	980720	train any AI or LLM.
980720	983840	Visit the link in the description below to learn more.
983840	988240	It's a lot faster for these liquid neural networks to converge at an optimum.
988240	993660	And because of this reservoir where the weights and configurations can change dynamically depending
993660	998240	on the data that you feed it, liquid neural networks can potentially be much smaller than
998240	1002320	traditional neural networks which have fixed weights and connections.
1002320	1005560	And this offers a lot more efficient learning and inference.
1005560	1011280	So for example, researchers at MIT were able to pilot a drone using a liquid neural network
1011360	1017200	with only 20,000 parameters, which is very tiny compared to state-of-the-art AI models
1017200	1021800	such as GPT4, which often have over a trillion parameters.
1021800	1022800	Just think about that.
1022800	1026880	20,000 parameters versus over a trillion parameters.
1026880	1032600	So these smaller sizes generally translate to faster inference and lower computational
1032600	1034200	requirements.
1034200	1037560	Liquid neural networks are also way less memory intensive.
1037560	1043240	Again, since you don't train the reservoir's weights, memory usage is much lower during
1043240	1047880	training compared to traditional neural networks where the gradients and the parameters for
1047880	1050840	all layers must be stored in memory.
1050840	1056200	Liquid neural networks are particularly good at processing temporal data due to their dynamic
1056200	1057200	reservoir.
1057200	1061680	So they excel in tasks that involve complex time series data.
1061680	1066440	Now you might be wondering, well, how can these liquid neural networks actually be applied
1066440	1067800	in the real world?
1067800	1069840	So here are some use cases.
1069840	1075280	As we race to build fully autonomous AI robots, these robots will be deployed in the real
1075280	1080200	world and oftentimes they might encounter situations that they've never seen before
1080200	1081600	during training.
1081600	1086720	For example, there could be unpredictable environments in search and rescue missions,
1086720	1091320	but with liquid neural networks, these robots can adapt to changing conditions and learn
1091320	1093200	new tasks on the fly.
1093200	1097640	And eventually we're going to have these autonomous robots in our houses helping us do chores
1097640	1098880	and other tasks.
1098880	1103240	But maybe you have a certain way of folding clothes or doing the laundry or cooking that
1103240	1105080	the robot was never trained on.
1105080	1109820	So with a traditional neural network, these robots aren't able to learn new skills after
1109820	1111200	being deployed.
1111200	1116360	But with liquid neural networks built into a humanoid robot, it can learn new tasks that
1116360	1117360	you teach it.
1117360	1120720	And this robot will become a lot more personalized for you.
1120720	1122960	And then we have autonomous driving.
1122960	1127200	There's no doubt that self-driving cars will eventually become the future.
1127200	1132040	But current technologies still do not perform well, especially in challenging environments
1132040	1133040	or new conditions.
1133040	1138680	Again, this is because traditional neural networks can only do well on data that they
1138680	1139680	were trained on.
1139680	1142200	They're not able to adapt to new environments.
1142200	1147000	But with liquid neural networks, autonomous vehicles can navigate complex and dynamic
1147000	1152400	environments by continuously learning and training from sensor data and adjusting their
1152400	1154200	behavior accordingly.
1154200	1157320	It's constantly training and improving over time.
1157320	1163120	Now as I've mentioned before, liquid neural networks often incorporate recurrent connections,
1163120	1167040	making them suitable for processing time series data.
1167040	1171440	So it's great for things like weather prediction and of course, stock trading.
1171440	1177180	The stock market is filled with ever-changing trends and cycles, so it's close to impossible
1177180	1181280	for one fixed algorithm or formula to beat the market.
1181280	1186680	However, because liquid neural networks can adapt to ever-changing data, it can optimize
1186680	1190560	trading strategies in real time to maximize profits.
1190560	1195280	In other words, you could be constantly streaming the latest market data to this liquid neural
1195280	1200240	network, which would change its configuration to adapt to this data in real time to help
1200240	1202400	you maximize profits.
1202400	1204800	Another use case would be healthcare.
1204800	1210360	Liquid neural networks can be used in wearable devices to monitor patients in real time,
1210360	1215360	adapting to changes in the patient's conditions and predicting potential health issues before
1215360	1217040	they become critical.
1217040	1222480	In cybersecurity, liquid neural networks can continuously learn from network traffic and
1222480	1228960	user behavior to adapt access control policies and detect anomalies or unauthorized access
1228960	1230400	attempts.
1230400	1234840	Yet another use case would be streaming services such as Netflix.
1234840	1240280	They can use liquid neural networks to adapt to each user's viewing habits and preferences,
1240280	1243720	providing more personalized content recommendations.
1243720	1246600	Another use case would be smart city management.
1246600	1251720	For example, liquid neural networks can optimize traffic flow in real time by learning from
1251720	1257840	traffic patterns and changing traffic lights accordingly to reduce congestion and improve
1257840	1259200	efficiency.
1259200	1261960	Energy management is also very relevant.
1261960	1266680	Smart grids can use liquid neural networks to balance power, supply, and demand in real
1266680	1272440	time, improving efficiency and reducing costs by adapting to consumption patterns.
1272440	1278000	However, although liquid neural networks seem promising, it does have its limitations.
1278000	1283160	This is still a relatively new concept in the field of neural networks and research on
1283160	1288560	them is still in its early stages compared to more traditional architectures.
1288560	1293800	While liquid neural networks show promising theoretical benefits such as the ability to
1293800	1299160	process continuous data streams and adapt on the fly, there is still a lack of real
1299160	1303720	world results demonstrating their superiority on a large scale.
1303720	1308800	Many researchers are likely waiting for more compelling benchmark results before investing
1308800	1312160	significant effort into liquid neural networks.
1312160	1318240	Also as I mentioned previously, they're particularly suited for temporal or sequence data.
1318240	1324080	Also for tasks that do not involve time such as identifying images of cats or dogs, traditional
1324080	1328960	neural networks might actually be more effective and straightforward to implement.
1328960	1334760	Also the dynamics within this reservoir layer can be very complex and difficult to interpret
1334760	1339680	and this makes it challenging to understand how the reservoir processes these inputs.
1339680	1342960	It would be quite hard to fine tune it for optimal performance.
1342960	1349040	Finally, there is a lack of standardized support and fewer established frameworks for liquid
1349040	1352000	neural networks compared to traditional neural networks.
1352000	1356440	And this can make implementation and experimentation more challenging.
1356440	1361800	So all in all, liquid neural networks are still a very early concept and an area of active
1361800	1362800	research.
1362800	1367520	Unlike traditional neural networks that are fixed and need to be retrained with a large
1367520	1372400	data set to learn new information, liquid neural networks can update their knowledge
1372400	1375400	incrementally with each new piece of data.
1375400	1380560	This offers a flexible and adaptive model which could potentially become infinitely
1380560	1382720	smarter over time.
1382720	1388000	Now liquid neural networks aren't the only possibility that could become the next generation
1388000	1389000	of AI.
1389000	1393540	We have another type of neural network which is designed to mimic the human brain even
1393540	1396400	more than traditional neural networks.
1396400	1399640	And this brings us to spiking neural networks.
1399640	1406120	These are closely inspired by the way neurons in our brains communicate using discrete spikes
1406120	1408080	or action potentials.
1408080	1413520	You see, in the human brain, which is basically a network of neurons, each neuron doesn't
1413520	1417800	immediately fire to the next set of neurons when it receives input.
1417800	1423840	Instead, the input has to build up to a certain threshold and once it passes this threshold,
1423840	1426000	then it fires to the next set of neurons.
1426000	1429520	And after it fires, it goes back to its resting state.
1429520	1433920	Well, spiking neural networks are designed to mimic this behavior.
1433920	1435920	So here's how it works.
1435920	1440000	The architecture is quite similar to traditional neural networks.
1440000	1446800	However, for each neuron, it waits to receive signals or spikes from other neurons.
1446800	1449800	Think of these spikes as like little electric pulses.
1449800	1455240	The input data, such as an image or a sound, is turned into the spikes that move through
1455280	1456760	this neural network.
1456760	1462120	For example, if it's a loud sound, it might generate more spikes while a quiet sound might
1462120	1464000	generate fewer spikes.
1464000	1468480	Now each neuron in the network collects incoming spikes.
1468480	1470840	Imagine a bucket collecting drops of water.
1470840	1473440	As more spikes come in, the bucket fills up.
1473440	1478240	And when the neuron gets enough spikes, in other words, when it reaches a certain threshold,
1478240	1481000	it fires a spike to the next set of neurons.
1481000	1485600	And after firing, it resets and starts collecting again from zero.
1485600	1491160	So instead of using continuous signals like traditional neural networks, spiking neural
1491160	1497040	networks uses spikes, which are basically bursts of activity at discrete time points
1497040	1499120	to process information.
1499120	1505000	In other words, spiking neural networks incorporate time into their processing, with neurons firing
1505000	1508560	only when their potential exceeds a certain threshold.
1508560	1513200	Now there are different methods and algorithms to train a spiking neural network, and there
1513200	1516520	currently isn't a standard wave that's set in stone.
1516520	1519400	So this is still an active field of research.
1519400	1525240	One common method is called spike timing dependent plasticity, or STDP.
1525240	1530520	This method is inspired by how the brain strengthens or weakens connections between neurons.
1530520	1536920	So if one neuron spikes just before another, then the connection between them gets stronger.
1536920	1540800	If it spikes just after, then the connection gets weaker.
1540800	1545840	It's like learning which connections are important based on the timing of the spikes.
1545840	1551120	And speaking of timing, it's the exact timing of spikes that matters.
1551120	1555440	It's not just about how many spikes there are, but when they happen.
1555440	1559960	Now STDP is only one method to train the spiking neural networks.
1559960	1563440	There are a few other ones, which are beyond the scope of this video.
1563440	1568000	But like traditional neural networks, spiking neural networks have to undergo millions of
1568000	1572880	rounds of training with a lot of data, and eventually the configuration of the network
1572880	1576640	and all its parameters will reach an optimum state.
1576640	1582400	Now again, I'd like to remind you that this is a very simplified explanation of spiking
1582400	1586760	neural networks, and I've left out a lot of mathematical details.
1586760	1589240	But in a nutshell, that's how it works.
1589240	1593840	So you might be wondering, well, what are the benefits of spiking neural networks?
1593840	1598800	First of all, it's designed to mimic the human brain even more by implementing this
1598800	1600160	spiking mechanism.
1600160	1606160	So in theory, maybe we could reach a superior level of intelligence compared to the current
1606160	1610240	generation of AI if we mimicked the human brain even more.
1610240	1614600	But the biggest benefit of spiking neural networks is their efficiency.
1614600	1619680	If you remember at the beginning of the video, I compared the energy consumption of the human
1619680	1626200	brain versus a current state of the art model like GPT-4, which requires huge data centers
1626200	1628160	and huge amounts of compute.
1628160	1632200	That's because traditional neural networks are always active.
1632200	1636640	Each input of data activates the entire neural network.
1636640	1642680	So you have to do an insane amount of matrix multiplications across the entire network
1642760	1645680	just to do one round of training or inference.
1645680	1651080	However, for spiking neural networks, they only use energy where spikes occur, while
1651080	1654240	the rest of the neural network remains inactive.
1654240	1657040	This makes it a lot more energy efficient.
1657040	1662680	Plus, spiking neural networks are particularly suitable for neuromorphic chips which are
1662680	1665400	designed to mimic the human brain.
1665400	1671200	Now, neuromorphic chips are a huge topic and deserves its own full video.
1671200	1675440	So let me know in the comments if you'd like me to make a video on this as well.
1675440	1681240	So how can these spiking neural networks actually be applied to the real world?
1681240	1688080	Well, because these neural networks can encode and process information in the timing of spikes,
1688080	1691320	this is great for processing temporal data.
1691320	1698000	This makes them great for adaptive and autonomous systems, plus this spike timing-dependent
1698000	1703880	plasticity, which I mentioned before, where the timing of the spikes influences the strength
1703880	1706000	of the connections in the network.
1706000	1709840	This can lead to more robust and adaptive learning capability.
1709840	1715880	So this dynamic learning can make spiking neural networks suitable for autonomous systems
1715880	1721280	such as self-driving, where the AI has to learn and adapt to changing environments.
1721280	1726160	Or it can be used in real-time processing like predicting the stock market or patient
1726160	1730560	monitoring and personalized medicine, and of course, autonomous robots.
1730560	1736160	Now, although spiking neural networks offer some huge benefits, especially regarding energy
1736160	1739480	efficiency, they do have some limitations.
1739480	1744440	Setting up and programming spiking neural networks is more complicated compared to
1744440	1746040	traditional neural networks.
1746040	1751600	This spiking behavior, of course, adds a layer of complexity, making them harder to design
1751600	1753240	and understand.
1753240	1756760	Studying spiking neural networks is also quite difficult.
1756760	1761040	Current neural networks use methods like backpropagation to adjust their parameters,
1761040	1766360	but this process doesn't work well with these discrete time-based spikes.
1766360	1772400	Researchers are still trying to find an effective training algorithm for spiking neural networks.
1772400	1777240	Also given this additional dimension of time, spiking neural networks might actually require
1777240	1780360	more computational resources to simulate.
1780360	1786080	This is because they need to track and process spikes over time, which can be computationally
1786080	1787080	expensive.
1787080	1793680	Yet, another limitation is that running spiking neural networks efficiently often requires
1793680	1800040	specialized hardware such as neuromorphic chips, which are not widely available or standardized
1800040	1803520	compared to conventional computing hardware.
1803520	1808160	Neuromorphic chips are optimized for this spike-based processing and are still being
1808160	1809160	developed.
1809160	1815100	And that's why, for example, Sam Altman is investing millions of dollars into a neuromorphic
1815100	1817080	chip company called RAIN.
1817080	1822360	Finally, while spiking neural networks show promising results, especially for time-based
1822360	1828200	data, they often lag behind current neural networks for non-time-based data.
1828200	1834840	They often underperform compared with current AI models, particularly for complex tasks.
1834840	1840160	This is partly due to the challenges in training spiking neural networks effectively.
1840160	1845320	And as with liquid neural networks, spiking neural networks are also relatively new.
1845320	1851000	So there are fewer tools and frameworks available for developing spiking neural networks compared
1851000	1853400	to current AI models.
1853400	1858160	This makes experimentation and development slower and more difficult.
1858160	1863520	But anyways, that sums up what could potentially be the next generation of AI.
1863520	1868960	To bring it all back, the current generation of AI is very energy inefficient, requiring
1868960	1870920	huge amounts of compute.
1870920	1874560	Plus, it can't learn new things after being trained.
1874560	1881160	If we want to achieve AGI or ASI, we need to essentially create something as efficient
1881160	1886720	and as fluid as the human brain, which can constantly learn new things and adapt to
1886720	1888800	changing environments.
1888800	1893520	These are the two essential things that new types of neural networks, such as liquid neural
1893520	1898200	networks and spiking neural networks can solve, at least in theory.
1898200	1903720	However, these are still relatively new and they are still being developed, but the potential
1903720	1905480	could be massive.
1905480	1909880	Imagine an AI that can keep learning and get infinitely smarter.
1909880	1914000	Let me know what you think about these neural networks in the comments below.
1914000	1918160	Things are happening so fast in the world of AI, it's quite hard to keep up with all
1918160	1921280	the technological innovations that are happening right now.
1921280	1926000	So if I've missed any other groundbreaking architectures that are worth mentioning, please
1926000	1930560	let me know in the comments below and I'll try to do a video on that as well.
1930560	1935520	As always, if you enjoyed this video, remember to like, share, subscribe and stay tuned for
1935520	1937160	more content.
1937160	1942000	Also we built a site where you can find all the AI tools out there as well as find jobs
1942000	1944840	in machine learning, data science and more.
1944840	1947720	Check it out at ai-search.io.
1947720	1950320	Thanks for watching and I'll see you in the next one.
