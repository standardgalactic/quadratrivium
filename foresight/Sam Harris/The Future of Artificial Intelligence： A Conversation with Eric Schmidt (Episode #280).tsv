start	end	text
0	12200	Welcome to the Making Sense podcast. This is Sam Harris. Just a note to say that if
12200	16400	you're hearing this, you are not currently on our subscriber feed and will only be hearing
16400	20880	the first part of this conversation. In order to access full episodes of the Making Sense
20880	26440	podcast, you'll need to subscribe at SamHarris.org. There you'll find our private RSS feed to
26440	31080	add to your favorite podcatcher, along with other subscriber-only content. We don't run
31080	35000	ads on the podcast, and therefore it's made possible entirely through the support of our
35000	38840	subscribers. So if you enjoy what we're doing here, please consider becoming one.
46600	53640	Okay, jumping right into it today. Today I'm speaking with Eric Schmidt. Eric is a technologist,
53640	62200	entrepreneur, and philanthropist. He joined Google in 2001 and served as its CEO and chairman from
62200	71080	2001 to 2011, and as executive chairman and technical advisor thereafter. In 2017, he co-founded
71080	76280	Schmidt Futures, a philanthropic initiative that bets early on exceptional people who are helping
76280	81960	to make the world better. He is the host of Reimagine with Eric Schmidt, his own podcast,
82920	88520	and most recently, he's the co-author of a new book, The Age of AI and Our Human Future.
89320	95720	And that is the topic of today's conversation. We cover how AI is affecting the foundations of
95720	101400	our knowledge and how it raises questions of existential risk. So we talk about the good and
101400	109400	the bad of AI, both narrow AI and ultimately, AGI, Artificial General Intelligence.
110360	116440	We discuss breakthroughs in pharmaceuticals and other good things, but we also talk about
117320	124040	cyber war and autonomous weapons and how our thinking about containing the risk here
124040	130440	by analogy to the proliferation of nuclear weapons probably needs to be revised.
131320	134840	Anyway, an important conversation, which I hope you find useful.
135800	137560	And I bring you Eric Schmidt.
144200	147000	I am here with Eric Schmidt. Eric, thanks for joining me.
147560	148440	Glad to be with you.
150440	155080	I think we have a hard hour here, so amazingly, that's a short podcast for me,
155080	159800	so there's going to be a spirit of urgency hanging over the place, and we will be
160440	166680	efficient in covering the fascinating book that you have written with Henry Kissinger and Daniel
166680	171080	Huttenlocher. That's right. And Dr. Kissinger, of course, is the former Secretary of State,
171080	177080	and Dan Huttenlocher is now the Dean of Artificial Intelligence and Computer Science
177080	181000	at the Schwartzman Center at MIT. He's a proper computer scientist.
181000	186360	Yeah, and that book is The Age of AI and Our Human Future, where you cover,
187080	193640	most of what I have said about AI thus far and every case where I have worried about
193640	200280	our possible AI future has been focused on the topic of AGI, Artificial General Intelligence,
200280	205080	which you discuss briefly in the book, but it's not your main focus. So I thought maybe we could
205080	210920	save that for the end, because I would love to get your take on AGI, but there are far more near
210920	218200	term concerns here and considerations that we could cover, and you are quite well placed
218200	224360	to cover them, because if I'm not mistaken, you ran Google for what was it, 10 years?
224360	230280	That's correct. What was your background before that? How did you come to be the CEO of Google?
230280	236440	Well, I'm a computer scientist. I have a PhD in the area, and I worked for 45 years in tech,
236440	240680	in one way or the other, a whole bunch of companies. Larry and Sergey brought me in
240680	246840	as the early CEO of the company, and we built it together. After a decade, I became chairman, Larry
246840	251960	became CEO, and then he replaced himself with Sundar, who's now doing a phenomenal job at Google.
252600	257320	So I'd say collectively, this group of which I'm a member built one of the great companies,
257320	263960	and I'm really proud of that. Yeah, and obviously, Google is quite involved in developing AI. I just
263960	271000	saw just the other day that there's a new, I think it's a 540 billion parameter language model
271000	278760	that is beating the average human at something like 150 cognitive tests now, and it seems like
279560	282760	the light is at the end of the tunnel there. I mean, it's just going to be a larger model
282760	288120	that's going to beat every human at those same tasks. But before we get into some of the details
288120	294920	here, I just wanted to organize our general approach to this. There are three questions that
294920	301480	Kant asked, and his critique of pure reason, I think it was, which seem unusually relevant
302120	309080	to the development of AI. The first is, what can we know? The second is, what should we do?
310280	317720	And the third is, what is it reasonable to hope for? And I think those are really do capture
318600	325960	almost every aspect of concern here, because as you point out in the book, AI really promises to,
325960	331640	and it has already begun to shift the foundations of human knowledge. So the question of what we
331640	337000	can know and how we can know it is enormously salient now, and maybe we can talk about some
337000	341720	of those examples. But obviously, this question of what should we do and what can we reasonably
341720	349320	hope for captures the risks we're running in developing these systems. And we're running
349320	355720	these risks well short of producing anything like artificial general intelligence. And it's
355720	362520	interesting that we're on a path now where we're really not free to decline to produce this technology.
362520	368680	I mean, to my eye, there's really no break to pull. I mean, we're in a kind of AI arms race now.
369400	376200	And the question is how to put that race for more intelligence on a footing that is not running
376200	382360	cataclysmic risk for us. So before we jump into the details, I guess I'd like to get
382360	389160	your general thoughts on how you view the stakes here and where you view the field to be at the
389160	395560	moment. Well, of course, we wrote the book, Age of AI, precisely to help answer the questions
395560	402200	you're describing, which are perfectly cast. And what's happened in the book, which is written
402200	408600	roughly a year ago and then published, we described a number of examples to illustrate the point. One
408600	415640	is the development of new moves in the game of Go, which is 2,500 years, which were discovered by
415640	421240	computer, which humans had never discovered. It's hard to imagine that humans wouldn't have
421240	425720	discovered these strategies, but they didn't. And that calls the question of are there
426280	433400	things which AI can learn that humans cannot master? That's a question. The second example that we use
433400	439000	is the development of a new drug called Hallison, which is a broad spectrum antibiotic, which could
439000	445640	not be done by humans, but a set of neuroscientists, biologists, and computer scientists put together
445720	450040	a set of programs that ultimately searched through 100 million different compounds
450760	456200	and came up with candidates that were then subsequently tested, advancing drugs at an
456200	461720	enormous rate. That's another category of success in AI. And then the third is what you've already
461720	466920	mentioned, which is large language models. And we profile in the book, GPT-3, which is the
466920	473000	predecessor of the one you described. And it's eerie. On the back cover of our book, we say,
473000	479960	to the GPT-3 computer, are you capable of human reasoning? And it answers, oh, I am not.
481000	487080	You may wonder why I give you that answer. And the answer is that you are a human reasoning
487080	493560	machine, whereas I am a language model that's not been taught how to do that. Now, is that
493560	501240	awareness or is that clever mimicry? We don't know. But each of these three examples show the
501240	507960	potential to answer consquestions. What can we know? What will happen? What can we do about it?
507960	512840	Since then, this past few weeks, we've seen the announcement that you mentioned of this
512840	519160	enormous large language model, which can beat humans on many things. And we've also seen something
519160	524600	called DAAL-E, which is a text to art program. You describe roughly what you want, and it can
524600	530280	generate art for you. Now, these are the beginnings of the impact of artificial intelligence on us
530280	536760	as humans. So Dr. Kissinger, Dan, and myself, when we looked at those, we thought, what happens to
536760	542920	society when you have these kinds of intelligence? Now, they're not human intelligence. They're
542920	548600	different kinds of intelligence in everyday life. And we talk about all the positives of which
548600	555000	they're incredible positives, better materials, better drugs, more efficient systems, better
555000	560520	understanding, better monitoring of the earth, additional solutions for climate change. There's
560520	565800	a long, long list, which I can go through. Very, very exciting. And indeed, in my personal
565800	572200	philanthropy, we are working really hard to fund AI-enabled science discoveries. We recently announced
572760	579320	a grant, a structure with a guy named James Manika, who's a friend of mine, to $125 million
580280	587320	to actually go and fund research on the really hard problems in AI, the ones that you're mentioning
587320	591800	and others, and also the economic impacts and so forth. Because I think people don't really know.
591800	597240	The real question is, what happens when these systems become more commonplace?
598120	603800	Dr. Kissinger says, if you look at history, when a system that is not understandable
604440	610120	is imposed on people, they do one of two things. They either invent it as a religion,
611240	617880	or they fight it with guns. So my concern, and I'll say it very directly, is we're playing with
618760	625800	the information space of humans. We're experimenting at scale without a set of principles
625800	630840	as to what we want to do. Do we care more about freedom? Do we care more about efficiency? Do we
630840	637400	care more about education and so forth? And Dr. Kissinger would say, the problem is that these
637400	642680	decisions are being made by technical people who are ignorant of the philosophical questions that
642680	650840	you so ably asked. And I agree with him, speaking as an example of that. So we recommend, and indeed
650840	657480	I'm trying to now fund, that people begin in a multidisciplinary way to discuss the implications
657480	663640	of this. What happens to national security? What happens to military intelligence? What happens to
663640	669320	social media? What happens to your children when your child's best friend is a computer?
670440	677000	And for the audience who might be still thinking about the killer robot, we're not building killer
677000	683320	robots, and I hope we never do. This is really about information systems that are human-like,
683400	687880	that are learning, they're dynamic, and they're emergent, and they're imprecise,
688600	697080	being used and imposed on humans around the world. That process is unstoppable. It's simply too many
697080	702040	people working on it, too many ways in which people are going to manipulate it, including
702040	708200	for hostile reasons, too many businesses being built, and too much success for some of the early
708200	714920	work. Yeah, I guess if I can just emphasize that point, the unstoppable is pretty interesting,
714920	722520	because it's just anchored to this basic fact that intelligence is, almost by definition,
722520	728920	the most valuable thing on earth. And if we can get more of it, we're going to. And we clearly can,
729560	735320	and all of these narrow intelligences we've built thus far, all that are effective, that come to
735400	742760	market that we pour resources into, are superhuman more or less right out of the gate. It's not a
742760	749560	question of human level intelligence, it's a bit of a mirage, because the moment we get something
749560	754920	that's general, it's going to be superhuman. And so we can leave the generality aside. All of these
754920	763320	piecemeal intelligences are superhuman. And the example you give of the new antibiotic,
763320	770520	Hallison, it's fascinating because it's not just a matter of doing human work faster. If I understand
770520	778600	what happened in that case, this is an AI detecting patterns and relationships in molecules already
778600	787880	known to be safe and efficacious as antibiotics, and detecting new properties that human beings
787960	795400	very likely would never have conceived of and may in fact be opaque to the people who built the AI
795400	799960	and may remain opaque. I mean, one of the issues you just raised is the issue of transparency.
800520	807640	Many of these systems are built in such a way as to be black boxes. And we don't know how the AI is
807640	814920	doing what it's doing in any specific way. It's just training against data and against its own
814920	821720	performance so as to produce a better and better result, which qualifies as intelligent and even
821720	827720	superhumanly so. And yet it may remain a black box. Maybe we can just close the loop on that
827720	837720	specific problem here. Are you concerned that transparency is a necessity when decision making
837720	843560	is important? I mean, just imagine the case where we have something like an AI oracle
844280	849960	that we are convinced makes better decisions than any person or even any group of people,
850600	854360	but we don't actually know the details of how it's making those decisions.
856520	862280	I mean, you can just multiply examples as you like, but just questions of who should get out
862280	869480	of prison, the likelihood of recidivism in the case of any person or who's likely to be more
869480	876040	violent at the level of conviction. What should the prison sentence be? It's very easy to see that
876040	884520	if we're shunting that to a black box, people are going to get fairly alarmed that in any
884520	890520	differences in outcome that are not transparent. Perhaps you have other examples of concern,
890520	896680	but do you think transparency is something that... I mean, one question is technically feasible to
896680	903720	render black boxes transparent when it matters, and two, is transparency as important as we
903720	909320	intuitively may think it is? Well, I wonder how important transparency is for the simple fact
909320	915320	that we have teenagers among our midst and the teenagers cannot explain themselves at all,
916040	920040	and yet we tolerate their behavior with some restrictions because they're not full adults.
920440	927800	So, but we wouldn't let a teenager fly an airplane or operate on a patient. So, I think a pretty simple
927800	933080	model is that at the moment, these systems cannot explain how they became to their decision. There
933080	938280	are many people working on the explainability problem. Until then, I think it's going to be really
938280	944040	important that these systems not be used in what I'm going to call life safety situations, and this
944040	949240	creates all sorts of problems. For example, in automated war, automated conflict, cyber war,
949320	954120	those sorts of things where the speed of decision making is faster than what humans can,
954120	960200	what happens if it makes a mistake? And so, again, we're at the beginning of this process,
960200	965480	and most people, including myself, believe that the explainability problem and the bias problems
966040	970520	will get resolved because there's just too much money, too many people working on it.
970520	974200	Maybe at some cost, but we'll get there. That's historically how these things work. You start
974200	978280	off with stuff that works well enough, but it shows a hint of the future, and then it gets
978280	985400	industrialized. I'm actually much more focused on what's it like to be human when you have these
985400	991960	specialized systems floating around. My favorite example here is Facebook, where they change their
991960	1000200	feed to amp it using AI, and the AI that they built was around engagement, and we know from a
1000200	1004680	great deal of social science that outrage creates more engagement, and so therefore,
1004680	1011400	there's more outrage on your feed. Now, that was a clearly deliberate decision on part of Facebook,
1011400	1014760	presumably thought it was a good product idea, but it also maximized their revenue.
1015400	1020120	That's a pretty big social experiment, given the number of users that they have,
1020120	1025640	which is not done with an understanding, in my view, of the impact of political polarization.
1026440	1030280	Now, you sit there and you go, okay, well, he doesn't work at Facebook, he doesn't really
1030280	1037720	understand, but many, many people have commented on this problem. This is an image of what happens
1037720	1044280	in a world where all of the information around you can be boosted or manipulated by AI to sell
1044280	1049560	to you, to anchor you, to change your opinion, and so forth. We're going to face some interesting
1049560	1056040	questions in the information space that television and movies and things you see online, and so forth.
1056520	1061960	Do there need to be restrictions on how AI uses the information it has about you
1062600	1067880	to pitch to you, to market to you, to entertain you? These are questions we don't have answers,
1068600	1074600	but it makes perfect sense that in the industrialization of these tools, the tools that I'm describing,
1074600	1079480	which were invented in places like Google and Facebook, will become available to everyone
1079480	1087000	and every government. Another example is a simple one, which is the kid is a two-year-old
1087000	1092520	and gets a toy, and the toy gets upgraded every year and the kid gets smarter, the toy is now,
1092520	1098760	the kid is now 12, and there's 10 years from now, there's a great toy, and this toy is smart enough
1099240	1106760	in non-human terms to be able to watch television and decide if the kid likes the show. The toy
1106760	1111640	is watching the television and the kid, the toy says to the kid, I don't like this show,
1112520	1118120	knowing that the kid's not going to like it, and the kid goes, I agree with you. Now, is that okay?
1118920	1126120	Probably. Well, what happens if that same system that's also learning learns something
1126120	1134600	that's not true, and it goes, you know, kid, I have a secret, and the kid goes, tell me, tell me,
1135400	1139320	and the secret is something which is prejudicial or false or bad or something like that.
1140040	1146280	We don't know how to describe, especially for young people, the impact of these systems on their
1146280	1153160	cognitive development. Now, we have a long history in America of having school boards and textbooks
1153160	1159000	which are approved at the state level. Are the states going to monitor this? And you sit there
1159080	1164600	and you say, well, no parent would allow that, but let's say that the normal behavior of this toy
1164600	1169480	is smart enough, understands the kid well enough to know the kid's not good at multiplication.
1170280	1176360	So the kid's bored, and the toy says, I think we should play a game. The kid goes great. And,
1176360	1181960	of course, it's a game which strengthens his or her multiplication capability. So on the one hand,
1181960	1188280	you want these systems to make people smarter, make them develop, make them more serious adults,
1188280	1193000	make the adults more productive. Another example would be my physics friends. They just want a
1193000	1198280	system to read all the physics books every night and make suggestions to them. Well, the physicists
1198280	1203400	are adults who can deal with this. But what about kids? So you're going to end up in a situation,
1203400	1209400	at least with kids and with elderly who are isolated, where these tools are going to have
1209400	1216040	an out of proportion impact on society as they perceive it. We've never run that experiment,
1216040	1223480	dynamic, emergent, and not precise. I'm not worried about airplanes being flown by AI because
1223480	1228440	they're not going to be reliable enough to do it for a while. Now, we should also say for the
1228440	1234520	listeners here that we're talking about a term which is generally known as narrow AI.
1235400	1241240	It's very specific, and we're using specific examples, drug discovery, education,
1241320	1247640	entertainment. But the eventual state of AI is called general intelligence, where you get
1247640	1254760	human kind of reasoning. In the book, what we describe that as the point where the computer
1254760	1260840	can set its own objective. And today, the good news is, the computer can't choose its objective.
1262200	1266920	At some point, that will not be true. Yeah. Yeah. Well, hopefully we'll get to AGI
1267720	1275080	at the end of this hour. But I think we should talk about the good and the bad in that order.
1275080	1280440	And maybe just spend a few minutes on the good because the good is all too obvious. Again,
1280440	1287000	and intelligence is the most valuable thing on earth. It's the thing that gives us every other
1287000	1293320	thing we want. And it's the thing that safeguards everything we have. And if there are problems,
1294040	1298360	we can't solve, well, then we can't solve them. But if there are problems that can be solved,
1298360	1305480	the way we will solve them is through greater uses of our intelligence. And if, you know,
1305480	1311080	insofar as we can leverage artificial intelligence to solve those problems, we will do that more
1311080	1316520	or less regardless of the attendant risks. And that's the problem because the attendant risks are
1317080	1323640	increasingly obvious and it seems not at all trivial. And what we've already proven, we're
1323640	1331640	capable of implementing massive technological change without really thinking about the consequences
1331640	1337640	at all. You cite the massive psychological experiment we've performed on all of humanity
1337640	1343880	with no one really consenting that is social media. And it's, you know, the effects are
1344680	1349560	ambiguous at best. I mean, there's some obviously bad effects. And it's not even
1349560	1355960	straightforward to say that democracy or even civilization can survive contact with social
1355960	1361880	media. I mean, that remains to be seen given how divisive some of its effects are. I consider
1361880	1369160	social media to be far less alarming than the prospect of having an ongoing nuclear doctrine
1369800	1378360	anchored to a proliferating regime of cyber espionage, cyber terrorism, cyber war,
1379000	1385080	all of which will be improved massively by layering AI onto all of that. So
1386040	1390040	before we jump into the bad, which is, you know, really capturing my attention,
1390920	1395880	is there anything specifically you want to say about the good here? I mean, if this goes well,
1396760	1399080	what are you hoping for? What are you expecting?
1399960	1405240	Well, there are so many positive examples that we honestly just don't have time to make a list.
1405240	1411960	We give you a few. In physics and math, the physicists and mathematicians have worked out
1411960	1417720	the formulas for how the world works, at least at the scientific level. But many of their calculations
1417720	1424200	are not computable by modern computers. They're just too complicated. An example is how do clouds
1424280	1429400	actually work is a function of something called the Navier-Stokes equations, which for a normal
1429400	1435720	sized cloud would take 100 million years for a computer to figure out. But using an AI system,
1435720	1441000	and there's a group at Caltech doing this, they can come up with a simulation of the things that
1441000	1449720	they care about. In other words, the AI provides enough accuracy in order to solve the more general
1449720	1456360	climate modeling problem. If you look at quantum chemistry, which is sort of how does, how do chemical
1456360	1463160	bonds work together? Not computable by modern methods. However, AI can provide enough of a
1463160	1468760	simulation that we can figure out how these molecules bind, which is the Hallisyn example.
1469880	1477160	In drug discovery, we know enough about biology that we can basically predict that if you do
1477800	1484120	these compounds with, you know, this antibody, we can make it stronger, we can make it weaker,
1484120	1489960	and so forth, in the computer, and then you go reproduce it in the lab. There's example after
1489960	1499080	example, where AI is being used from existing data to simulate a non-computable function in science.
1499080	1503800	And you say, what's he talking about? I'm talking about the fact that the scientists have been stuck
1504520	1509560	for decades because they know what they want to do, but they couldn't get through this barrier.
1510280	1517080	That unleashes new materials, new drugs, new forms of steel, new forms of concrete, and so forth and
1517080	1522520	so on. It also helps us with climate change, for example, because climate change is really about
1522520	1527240	energy and CO2 emission and so forth. These new surfaces, discoveries, and so forth will make a
1527240	1533400	material difference. And I'm talking about really significant numbers. So that's an example. Another
1533400	1537320	example is what's happening with these large language models that you mentioned earlier,
1538120	1541880	that people are figuring out a way to put a conversational system in front of it so that
1541880	1546760	you can talk to it. And the conversational system has enough state that it can remember what it's
1546760	1551560	talking about. It's not like a question-answer-question-answer, and it doesn't remember. It actually
1551560	1555720	remembers the context of, oh, we're talking about the Oscars, and we're talking about what happened
1555720	1561160	at the Oscars, and what do I think? And then it sort of goes and it gives you a thoughtful answer
1561240	1566760	as to what happened and what is possible. In my case, I was playing with one of them
1567880	1574920	a few months ago, and this one, I asked the question, what is the device that's in 2001
1574920	1580360	a space odyssey that I'm using today? There's something from 1969 that I'm using today that was
1580360	1586600	foreshadowed in the movie, and it comes right back and says the iPad. Now, that's a question
1586600	1594360	that Google won't answer if you ask it the way I did. So I believe that the biggest positive impact
1595160	1600360	will be that you'll have a system that you can verbally or by writing ask it questions,
1600920	1607000	and it will make you incredibly smarter. It'll give you the nuance and the understanding in the
1607000	1611560	context, and you can ask it another question, and you can refine your question. Now, if you think
1611560	1616760	about it in the work you do, or that I do, or that a scientist does, or a politician, or an artist,
1616760	1625640	this is enormously transformative. So example after example, these systems are going to build
1625640	1631160	scientific breakthroughs, scalable breakthroughs. Another example was that a group at DeepMind
1631960	1637560	figured out the folding structure of proteins, and proteins are the way in which biology works,
1637560	1641320	and the way they fold determines their effectiveness, what they actually do.
1642040	1647000	And it was thought to be not really computable, and using these techniques in a very complicated way
1647000	1652600	with a whole bunch of protein scientists, they managed to do it, and their result was replicated
1652600	1657000	in a different mechanism with different AI from something called the Baker Lab in University of
1657000	1663480	Washington. The two together have given us a map of how proteins work, which in my view is worthy
1663480	1668440	of a Nobel Prize. That's how big a discovery that is. All of a sudden, we are unlocking the way
1668440	1674040	biology works, and it affects us directly. But those are some positive examples. I think the
1674040	1681160	negative examples... Well, let's wait because I'm chock full of negative examples. But I'm interested
1681160	1690600	in how even the positive can disclose a surprisingly negative possibility, or at least it becomes
1690600	1697960	negative if we haven't planned for it ethically, politically, economically. So you imagine this
1697960	1705160	success, you imagine that more and more... So what you've just pictured was a future of machine and
1705160	1713400	human cooperation and facilitation, where people just get smarter by being able to have access
1713400	1720440	to these tools, or they get effectively smarter. But you can imagine just in the limit, more and
1720440	1726200	more getting seated to AI, because AI is just better at doing these things. It's better at
1726200	1730920	proving theorems. It's better at designing software. It's better. It's better. It's better. And
1730920	1736920	all of a sudden, the need for human developers at all, or human mathematicians at all, or you
1736920	1746440	just make the list as long as you want, it seems like some of the highest status jobs cognitively
1747240	1752760	might be among the first to fall, which is to say, I certainly expect at this point
1753560	1763000	to have an AI radiologist, certainly, before I have an AI plumber. And there's a lot more
1763640	1768760	above and beyond the radiology side of that comparison that I think is going to fall before
1769720	1777880	the basic manual tasks fall to robots. And this is a picture of real success, right? Because
1778520	1781880	in the end, all we're going to care about is performance. We're not going to care about
1782600	1789960	keeping a monkey in the loop just for reasons of sentimentality. If you're telling me that my car
1789960	1796600	can drive a thousand times better than I can, which is to say that it's going to reduce my risk of
1796600	1799800	getting in a fatal accident, you know, killing myself or killing someone else
1800440	1805960	by a factor of a thousand, if I just flip on autopilot, well, then not only am I going to flip
1805960	1812360	it on, I'm going to consider anyone who declines to do that to be negligent to the point of
1812360	1816760	criminality. And that's never going to change. Everything is going to be in the position
1817480	1824600	of a current chess master who knows that the best player on earth is never going to be a person
1824600	1832040	ever again, right? Because of alpha zero. So I disagree a little bit, and I'll tell you why.
1832680	1838200	I think you're correct in about 30 years, but I don't think that argument is true in the short term.
1839080	1843240	Yeah, no, I was not just to be clear. I'm not suggesting any time frame there. I'm just saying
1843240	1849720	ultimately, if we continue to make progress, something like this seems bound to happen.
1849720	1857400	Yes, but what I want to say is, I defy you to argue with me that making people smarter
1857960	1866120	is a bad thing. Okay, right. So let's start with the premise of the human assistant that is the
1866120	1873720	thing that you're using will make humans smarter. It'll make it deeper, better analysis, better
1873720	1881880	choices. But at least the current technology cannot replace the essentially the free will of
1881880	1886920	humans. They sort of wake up in the morning, you have a new idea, you decide something, you say,
1886920	1892120	that's a bad idea, so forth and so on. We don't know how to do that yet. And I have some speculation
1892120	1897960	on how that will happen. But in the next decade, we're going to not be solving that problem,
1897960	1902680	we'll be solving a different problem, which is how do we get the existing people doing existing
1902680	1910200	jobs to do them more efficiently that is smarter, better, faster? When we looked at the funding
1910200	1915880	for this AI program that I've since announced, the funding 125 million, a fair chunk of it is
1915880	1921000	going to really hard computer science problems. Some of them include, we don't really understand
1921000	1926280	how to explain what they're doing. As I mentioned, they're also brittle. When they fail, they can
1926280	1931560	fail catastrophically, like why did it fail? And no one can explain. They're hardening,
1931560	1935080	there are resistance to attack problems, there are a number of problems of this kind.
1935960	1938760	These are hard computer science problems, which I think we will get through.
1939480	1944120	They use a lot of power, the algorithms are expensive, that sort of thing. But we have also
1944120	1950680	focusing around the impact on jobs and employment and economics. We're also focusing on national
1950680	1955880	security. And we're focusing on the question that you're asking, which is, what's our identity?
1955880	1962520	What does it mean to be human? Before general intelligence comes, we have to deal with the
1962520	1968680	fact that these systems are not capable of choosing their own outcome, but they can be
1968680	1975320	applied to you as a citizen by somebody else against your own satisfaction. So the negatives
1975320	1983960	before AGI are all of the form, misinformation, misleading information, creating dangerous
1983960	1990600	tools, and for example, dangerous viruses. For the same reason that we built a fantastic new
1990600	1997560	antibiotic drug, it looks like you could also imagine a similar evil team producing an incredible
1997560	2002520	number of bad viruses, things that would hurt people. And you could imagine in that scenario,
2002520	2007240	they might be clever enough to be able to hurt a particular race or a particular sex or something
2007240	2012600	like that, which would be totally evil and obviously a very bad thing. We don't have a way
2012600	2018120	of discussing that today. So when I look at the positives and negatives right now, I think the
2018120	2024680	positives, as with many technologies, really overwhelm the negatives, but the negatives need
2024680	2031240	to be looked at. And we need to have the conversation right now about, let's use social media, which
2031240	2037400	is an easy whipping boy here. I would like, so I'm clear what my political position is,
2037400	2044760	I'm a very strong proponent of freedom of speech for humans. I am not in favor of freedom of speech
2044760	2051000	for computers, robots, bots, so forth and so on. I want an option with social media, which says,
2051000	2057480	I only want to see things that a human has actually communicated from themselves. I want to know that
2057480	2062840	it wasn't snuck in by some Russian agent. I want proof of providence. And I want to know that it's
2062840	2069080	a human. And if it's a real human who's, in fact, an idiot or crazy or whatever, I want to be able
2069080	2073960	to hear their voice and I want to be able to decide I don't agree with it. What's happening instead
2073960	2081720	is these systems are being boosted. They're being pitched. They're being sold by AI. And I think
2081720	2087480	that's got to be limited in some way. I'm in favor of free speech, but I don't want only some people
2087480	2092920	to have microphones. And if you talk to politicians and you look at the political structure in the
2092920	2100600	country, this isn't of completely unintended effect of getting everyone wired. Now, is it a human?
2100600	2107320	Or is it a computer? Is it a Russian? A Russian Compromat Plan? Or is it an American? Those things
2107320	2111320	need to get resolved. You cannot run a democracy without some level of trust.
2112120	2119000	Yeah. Well, let's take that piece here and obviously it extends beyond the problem of AI's
2119000	2125800	involvement in it. But the misinformation problem is enormous. What are your thoughts about it?
2125800	2132520	Because I'm just imagining we've been spared thus far the worst possible case of this, which is
2132520	2140520	just imagine under conditions of where we had something like perfect deep fakes,
2141000	2146680	that were truly difficult to tell apart from real video, what would the controversy around
2146680	2152840	the 2020 election have looked like? Or the war in Ukraine and our dealings with Putin at this
2152840	2161640	moment? Just imagine a perfect deep fake of Putin declaring a nuclear first strike on the U.S.
2161640	2168360	or whatever. Just imagine essentially a writer's room from hell where you have smart creative
2168360	2176760	people spending their waking hours figuring out how to produce media that is shattering to every
2176760	2185720	open society and conducive to provoking international conflict. That is clearly coming in some form.
2186520	2192920	I guess my first question is, are you hopeful that the moment that arrives, we will have the same
2193000	2198920	level of technology that can spot deep fakes? Or is there going to be a lag there of
2199720	2203000	months, years that are going to be difficult to navigate?
2203560	2208680	We don't know. There are people working really hard on generating deep fakes and there are people
2208680	2214760	working really hard on detecting deep fakes. And one of the general problems with misinformation
2214760	2220920	is we don't have enough training data. The term here is in order to get an AI system to
2220920	2225720	know something. You have to give it enough examples of good, bad, good, bad. And eventually you
2225720	2230840	can say, oh, here's something new and I know if it's good or bad. And one of the core problems
2230840	2235640	in misinformation is we don't have enough agreement on what is misinformation or what have you.
2236680	2242520	The thought experiment I would offer is President Putin in Russia has already shut down the internet
2242520	2248440	and free speech and controls the media and so forth. So let's imagine that he was furthering
2248440	2254200	it. If you'd like to continue listening to this conversation, you'll need to subscribe
2254200	2259080	at SamHarris.org. Once you do, you'll get access to all full-length episodes of the Making Sense
2259080	2265000	podcast, along with other subscriber-only content, including bonus episodes and AMAs
2265000	2270120	and the conversations I've been having on the Waking Up app. The Making Sense podcast is ad-free
2270120	2275800	and relies entirely on listener support. And you can subscribe now at SamHarris.org.
