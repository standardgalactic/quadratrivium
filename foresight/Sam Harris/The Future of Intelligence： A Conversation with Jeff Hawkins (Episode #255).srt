1
00:00:00,000 --> 00:00:12,160
Welcome to the Making Sense podcast. This is Sam Harris. Just a note to say that if

2
00:00:12,160 --> 00:00:16,360
you're hearing this, you are not currently on our subscriber feed and will only be hearing

3
00:00:16,360 --> 00:00:20,800
the first part of this conversation. In order to access full episodes of the Making Sense

4
00:00:20,800 --> 00:00:26,400
podcast, you'll need to subscribe at SamHarris.org. There you'll find our private RSS feed to

5
00:00:26,400 --> 00:00:31,040
add to your favorite podcatcher, along with other subscriber-only content. We don't run

6
00:00:31,040 --> 00:00:34,960
ads on the podcast, and therefore it's made possible entirely through the support of our

7
00:00:34,960 --> 00:00:38,800
subscribers. So if you enjoy what we're doing here, please consider becoming one.

8
00:00:46,960 --> 00:00:52,080
Today I'm speaking with Jeff Hawkins. Jeff is the co-founder of Numenta,

9
00:00:52,720 --> 00:00:57,680
a neuroscience research company, and also the founder of the Redwood Neuroscience Institute.

10
00:00:58,480 --> 00:01:04,640
And before that, he was one of the founders of the field of handheld computing, starting Palm

11
00:01:05,360 --> 00:01:10,000
and Handspring. He's also a member of the National Academy of Engineering,

12
00:01:10,800 --> 00:01:18,000
and he's the author of two books. The first is On Intelligence, and the second and most recent is

13
00:01:18,000 --> 00:01:24,560
A Thousand Brains, A New Theory of Intelligence. And Jeff and I talk about intelligence

14
00:01:25,200 --> 00:01:31,600
from a few different sides here. We start with the brain. We talk about how the cortex creates

15
00:01:31,600 --> 00:01:39,440
models of the world, the role of prediction in experience. We discuss the idea that thought

16
00:01:39,440 --> 00:01:46,160
is analogous to movement in conceptual space. But for the bulk of the conversation,

17
00:01:46,160 --> 00:01:52,160
we have a debate about the future of artificial intelligence, and in particular the alignment

18
00:01:52,160 --> 00:02:00,240
problem and the prospect that AI could pose some kind of existential risk to us. As you'll hear,

19
00:02:01,040 --> 00:02:08,240
Jeff and I have very different takes on that problem. Our intuitions divide fairly sharply,

20
00:02:08,960 --> 00:02:15,920
and as a consequence we have a very spirited exchange. Anyway, it was a lot of fun. I hope

21
00:02:16,000 --> 00:02:19,440
you enjoy it, and now I bring you Jeff Hawkins.

22
00:02:26,640 --> 00:02:29,360
I'm here with Jeff Hawkins. Jeff, thanks for joining me.

23
00:02:30,560 --> 00:02:32,000
Thanks for having me, Sam. It's a pleasure.

24
00:02:32,720 --> 00:02:39,280
I think we met probably just once, but I feel like we met about 15 years ago at one of those

25
00:02:39,280 --> 00:02:42,720
Beyond Belief conferences at the Salk Institute. Does that ring a bell?

26
00:02:43,280 --> 00:02:48,480
I was at one of the Beyond Belief conferences, and I don't recall meeting you there, but it's

27
00:02:48,480 --> 00:02:55,280
totally possible. It's possible we didn't meet, but I remember, I think we had an exchange where

28
00:02:55,280 --> 00:03:00,640
one of us was in the audience, and the other was in exchange over 50 feet or whatever.

29
00:03:00,640 --> 00:03:04,320
Oh, that makes sense. Yeah, I was in the audience, and I was speaking up.

30
00:03:04,880 --> 00:03:10,240
Yeah, okay, and I was probably on stage defending some Kakamemi conviction. Well,

31
00:03:10,240 --> 00:03:17,200
anyway, nice to almost meet you once again, and you have a new book which we'll cover part of,

32
00:03:17,200 --> 00:03:23,360
not by no means exhausting its topics of interest, but the new book is A Thousand Brains,

33
00:03:23,360 --> 00:03:30,960
and it's a work of neuroscience and also a discussion about the frontiers of AI and where

34
00:03:30,960 --> 00:03:38,240
all this is heading, but maybe we should start with the brain part of it and start with the

35
00:03:38,960 --> 00:03:47,200
really novel and circuitous and entrepreneurial route you've taken to get into neuroscience.

36
00:03:47,760 --> 00:03:54,720
This is the non-standard course to becoming a neuroscientist. Give us your brief biography

37
00:03:54,720 --> 00:04:00,560
here. How did you get into these topics? Well, I fell in love with brains when I

38
00:04:00,560 --> 00:04:06,080
just got out of college, so I studied electrical engineering in college, and right after I started

39
00:04:06,080 --> 00:04:12,960
my first job at Intel, I read an article by Francis Crick about brains and how we don't

40
00:04:12,960 --> 00:04:17,280
understand their work, and I just became enamored. I said, oh my god, we should understand this.

41
00:04:17,280 --> 00:04:22,720
This is me. I am my brain, and no one seems to know how this thing is working, and I just couldn't

42
00:04:22,720 --> 00:04:28,400
accept that, and so I decided to dedicate my life to figuring out what's going on when I'm thinking

43
00:04:28,480 --> 00:04:37,840
and who we are basically as a species. It was a difficult path, so I quit my job. I

44
00:04:37,840 --> 00:04:45,120
essentially applied to become a graduate student first at MIT and AI, but then I settled at Berkeley

45
00:04:45,680 --> 00:04:50,320
in neuroscience, and I said, okay, we're going to spend my life figuring out how the

46
00:04:50,320 --> 00:04:56,080
neocortex works, and I found out very quickly that that was a very difficult thing to do

47
00:04:56,080 --> 00:05:01,200
scientifically, but difficult to do from the practical aspects of science, that you couldn't

48
00:05:01,200 --> 00:05:05,680
get funding for that. It was considered too ambitious. It was theoretical work, and people

49
00:05:05,680 --> 00:05:10,640
didn't fund theoretical work, so after a couple of years as a graduate student at Berkeley,

50
00:05:10,640 --> 00:05:15,360
I set a different path. I said, okay, I'm going to go back to work in industry for a few years

51
00:05:15,920 --> 00:05:20,960
to mature, to figure out how to make institutional change because I was up against an institutional

52
00:05:20,960 --> 00:05:26,880
problem, not just a scientific problem, and that turned into a series of successful businesses

53
00:05:26,880 --> 00:05:31,840
that I was involved with and started, including Palm and HandSpring. These are some of the early

54
00:05:32,400 --> 00:05:37,040
HandTel computing companies, and we were having a tremendous amount of success with that,

55
00:05:37,040 --> 00:05:43,040
but it was never my mission to stay in the HandTel computing industry. I wanted to get back

56
00:05:43,040 --> 00:05:48,800
to neuroscience, and everybody who worked for me knew this. In fact, I told the investors,

57
00:05:48,800 --> 00:05:52,080
I'm only going to do this for four years, and they said, what? I said, yeah, that's it,

58
00:05:52,640 --> 00:05:56,400
but it turned out to be a lot longer than that because all the success we had, but eventually,

59
00:05:56,400 --> 00:06:01,520
I just extracted myself from it, and I said, I'm going to go and I have so many years left in my

60
00:06:01,520 --> 00:06:06,560
life, so after having all that success in the mobile computing space, I started a neuroscience

61
00:06:06,560 --> 00:06:09,920
institute. This is at the recommendation of some neuroscience friends of mine,

62
00:06:09,920 --> 00:06:14,000
so they helped me do that, and I ran that for three years, and now I've been running sort of a

63
00:06:14,000 --> 00:06:17,920
private lab just doing pure neuroscience for the last 17 years.

64
00:06:19,280 --> 00:06:21,040
That's Numenta, right?

65
00:06:21,040 --> 00:06:28,000
That's Numenta, yeah, and we've made some really significant progress in our goals,

66
00:06:28,560 --> 00:06:33,520
and the book documents some of the recent really significant discoveries we've made.

67
00:06:34,080 --> 00:06:40,480
So am I right in thinking that you made enough money at Palm and HandSpring that you could

68
00:06:40,480 --> 00:06:45,840
self-fund your first neuroscience institute, or is that not the case? Did you have to go raise

69
00:06:45,840 --> 00:06:52,080
money? Well, it was a bit of both. Certainly, I was a major contributor. I wasn't the only one,

70
00:06:52,640 --> 00:06:58,000
but I didn't want the funding to be the driver of what we did and how we spent all our time,

71
00:06:58,000 --> 00:07:03,920
so at the institute, we had collaborations with both Berkeley and Stanford. We didn't get funds

72
00:07:03,920 --> 00:07:10,160
from them, but we did work with them on various things, and then we had, but that was mostly

73
00:07:10,160 --> 00:07:15,200
funded by myself. Numenta is still, I'm a major contributor to it, but there are other people

74
00:07:15,200 --> 00:07:20,800
who've invested in Numenta, and we have one outside venture capitalist, and several people,

75
00:07:20,800 --> 00:07:27,440
but I'm still a major contributor to it. I just view that as a sort of a necessary thing to get

76
00:07:27,440 --> 00:07:32,560
on to the science and not have to worry about it, because when I was at Berkeley, what I was told

77
00:07:32,560 --> 00:07:39,040
over and over again, I really came to understand this. In fact, eventually, after that, when I was

78
00:07:39,040 --> 00:07:44,240
running the Redwood Neuroscience Institute, I went to Washington to talk about the National

79
00:07:44,240 --> 00:07:49,040
Science Foundation and National Institute of Health, and also to DARPA, who were the funders

80
00:07:49,040 --> 00:07:53,680
of neuroscience, and everyone thought what we were doing, which is sort of big theory, large-scale

81
00:07:53,680 --> 00:07:58,240
theories of cortical function, that this was like the most important problem to work on,

82
00:07:58,240 --> 00:08:04,080
but everyone said they can't fund it for various reasons. Over the years, I've come to appreciate

83
00:08:04,080 --> 00:08:09,520
that it's very difficult to be a scientist doing what we do with traditional funding sources,

84
00:08:10,080 --> 00:08:15,520
but we don't work outside of science. We partner with labs, and we go to conferences,

85
00:08:15,520 --> 00:08:21,520
we publish papers, we do all the regular stuff. Right. It's amazing how much comes down to funding

86
00:08:21,520 --> 00:08:27,120
or lack of funding and the incentives that would dictate whether something gets funded in the first

87
00:08:27,120 --> 00:08:33,440
place. It's by no means a perfect system. It's a kind of an intellectual market failure.

88
00:08:34,000 --> 00:08:38,320
Yeah. It is fascinating, and we could have a whole conversation about that sometimes, perhaps,

89
00:08:38,960 --> 00:08:43,840
because I ask myself, why is it so hard? Why do people can't fund this? There's reasons for it,

90
00:08:43,840 --> 00:08:49,200
and it's a complex, strange thing. When people were telling me, this is the most important thing

91
00:08:49,200 --> 00:08:54,320
anyone could be working on, and we think your approaches are great, but we can't fund that,

92
00:08:54,960 --> 00:09:00,880
why is that? I just accepted the way it was. I said, okay, this is the world I'm living in.

93
00:09:00,880 --> 00:09:07,360
I'm going to get one chance here. If I can't do this through working my way as a graduate

94
00:09:07,360 --> 00:09:12,400
student to getting a position at a university, how am I going to do it? I said, okay, it's

95
00:09:12,400 --> 00:09:18,960
not what I thought, but this is what's going to be. Nice. Well, let's jump into the neuroscience

96
00:09:18,960 --> 00:09:25,120
side of it. Generally speaking, we're going to be talking about intelligence and how it's

97
00:09:25,120 --> 00:09:35,520
accomplished in physical systems. Let's start with a definition, however loose. What is intelligence

98
00:09:35,520 --> 00:09:41,840
in your view? I didn't know and didn't have any pre-ideas about what this would be. It was a mystery

99
00:09:41,840 --> 00:09:47,440
to me, but we've learned what a good portion of your brain is doing. We started the New York

100
00:09:47,440 --> 00:09:53,600
Cortex, which is about 70% of the volume of a human brain. I now know what that does, and so

101
00:09:53,600 --> 00:09:59,120
I'm going to take that as my definition for intelligence here. What's going on in your

102
00:09:59,120 --> 00:10:05,360
New York Cortex is it's learning a model of the world, an internal recreation of all the things

103
00:10:05,360 --> 00:10:10,720
in the world that you know of and how it does. That's the key and what we've discovered,

104
00:10:10,720 --> 00:10:16,720
but it's this internal model. Intelligence requires having an internal model of the world

105
00:10:16,720 --> 00:10:21,360
in your head. It allows you to recognize where you are. It allows you to act on things. It allows

106
00:10:21,360 --> 00:10:25,520
you to plan and think about the future. If I'm going to say, what happens when I do this, the

107
00:10:25,520 --> 00:10:31,360
model tells you that. To me, intelligence is just about having a model in your head and using that

108
00:10:31,360 --> 00:10:36,960
for planning and action. It's not about doing anything in particular. It's about understanding

109
00:10:36,960 --> 00:10:41,600
the world. Yeah, that's interesting. I think most people would, that's kind of an internal

110
00:10:42,480 --> 00:10:48,560
definition of intelligence, but I think most people would reach for an external one or a

111
00:10:49,680 --> 00:10:54,800
functional one that has to take in the environment. It's something about being able to

112
00:10:55,920 --> 00:11:02,640
flexibly meet your goals under a range of conditions, more flexibly than rigidly. I

113
00:11:02,640 --> 00:11:07,520
guess there's rigid forms of intelligence, but when we're talking about anything like

114
00:11:08,240 --> 00:11:14,560
general intelligence, we're talking about something that is not merely hardwired and

115
00:11:14,560 --> 00:11:19,440
reflexive, but flexible. Yes, but if you have an internal model of the world,

116
00:11:19,440 --> 00:11:23,360
you had to learn it, at least from a human point of view. There's some things we have built in

117
00:11:23,360 --> 00:11:29,760
when we're born, but the vast majority of what you and I know, Sam, is learned. We didn't know

118
00:11:29,760 --> 00:11:33,200
what a computer was when you're born. You don't know what a coffee cup is. You don't know what

119
00:11:33,200 --> 00:11:36,880
building is. You don't know what doors are. You don't know what computer codes are. None of this

120
00:11:36,960 --> 00:11:42,960
stuff. Almost everything we interact with in the world today, in language, we don't know any

121
00:11:42,960 --> 00:11:47,520
particular language when we're born. We don't know mathematics, so we had to learn all these things.

122
00:11:47,520 --> 00:11:51,440
So if you want to say there might be an internal model that wasn't learned, well, that's pretty

123
00:11:51,440 --> 00:11:55,520
trivial, but I'm talking about models that are learned and you have to interact with the world

124
00:11:55,520 --> 00:11:59,920
to learn it. You can't learn it without being present in the world, without having an embodiment,

125
00:11:59,920 --> 00:12:04,640
without moving about, touching and seeing and hearing things. So a large part of what people

126
00:12:04,640 --> 00:12:10,560
think about, like you brought up, is, okay, we are able to solve a goal, but that's what a model

127
00:12:10,560 --> 00:12:16,000
lets you to do. That is not what intelligence itself is. Intelligence is having this ability

128
00:12:16,000 --> 00:12:21,360
to solve any goal, right? Because if your model covers that part of the world, you can figure out

129
00:12:21,360 --> 00:12:26,560
how to manipulate that part of the world and achieve what you want. So I'll give you a little

130
00:12:26,560 --> 00:12:30,800
further analogy. It's a little bit like computers. When we talk about a universal turning machine

131
00:12:30,880 --> 00:12:37,200
or what a computer is, it's not defined by what the computer is applied to do. It's like a computer

132
00:12:37,200 --> 00:12:40,320
isn't something that solves a particular problem. A computer is something that works on a set of

133
00:12:40,320 --> 00:12:46,000
principles, and that's how I think about intelligence. It's a modeling system that works on a set of

134
00:12:46,000 --> 00:12:51,840
principles. Those principles can exist in a mouse, in a dog, in a cat, in a human, and probably birds,

135
00:12:52,480 --> 00:12:55,600
but don't focus on what those animals are doing.

136
00:12:56,080 --> 00:13:01,920
Hmm. Yeah, it's important to point out that a model need not be a conscious model. In fact,

137
00:13:01,920 --> 00:13:09,680
most of our models are not conscious and might not even be, in principle, available to consciousness,

138
00:13:09,680 --> 00:13:16,160
although I think at the boundary, something that you'd say is happening entirely in the dark,

139
00:13:16,160 --> 00:13:22,160
does have a kind of, or can have a kind of liminal conscious aspect. So I mean, to take,

140
00:13:22,160 --> 00:13:28,000
you know, the coffee cup example, this leads us into a more granular discussion of what it means

141
00:13:28,000 --> 00:13:33,360
to have a model of anything at the level of the cortex. But, you know, if I reach for my coffee cup

142
00:13:34,080 --> 00:13:39,600
and grasp it, the ordinary experience of doing that is something I'm conscious of.

143
00:13:40,320 --> 00:13:47,840
I'm not conscious of all of the prediction that is built into my accomplishing that and

144
00:13:48,800 --> 00:13:55,120
experiencing what I experience when I touch a coffee cup. And yet, it's prediction that is

145
00:13:55,120 --> 00:14:00,560
required having some ongoing expectation of what's going to happen there when I, you know, when each

146
00:14:00,560 --> 00:14:08,960
finger touches the surface of the cup that allows for me to detect any error there or to be surprised

147
00:14:08,960 --> 00:14:13,840
by something truly anomalous. So if I reach for a coffee cup, and it turns out that's, you know,

148
00:14:13,840 --> 00:14:19,520
it's a hologram of a coffee cup and my hand passes right through it, the element of surprise

149
00:14:19,520 --> 00:14:27,600
there seems predicated on some ongoing prediction processing to which the results of my behavior

150
00:14:27,600 --> 00:14:34,000
is being compared. So maybe you can talk about what you mean by having a model at the level

151
00:14:34,000 --> 00:14:41,040
of the cortex and how prediction is built into that. Yeah. Well, my first book, which I published

152
00:14:41,280 --> 00:14:47,520
14 years ago called Long Intelligence, was just about that topic. It was about how it is the brain

153
00:14:47,520 --> 00:14:51,840
is making all these predictions all the time and all your sensory modalities and you're not aware of

154
00:14:51,840 --> 00:14:57,120
it. And so that's sort of the foundation. And you can't make a prediction without a model. I mean,

155
00:14:57,120 --> 00:15:01,520
a model, to make a prediction, you had to have some expectation, the expectation whether you're

156
00:15:01,520 --> 00:15:06,640
not aware of it or not, but they have an expectation. And that has to be driven from some internal

157
00:15:06,640 --> 00:15:10,960
representation of the world that says, hey, this, they're about to touch this thing, I know what it

158
00:15:10,960 --> 00:15:17,040
is, it's supposed to feel this way. And even if you're not aware that you're doing that. One of the

159
00:15:17,040 --> 00:15:23,200
key discoveries we made, and this was maybe about eight years ago, we, we had to get to the bottom

160
00:15:23,200 --> 00:15:28,800
like how do neurons make predictions? What is the physical manifestation of a prediction in the brain?

161
00:15:29,680 --> 00:15:33,440
And most of these predictions, as you point out, are not conscious, you're not aware of them.

162
00:15:33,440 --> 00:15:37,440
They're just happening. And if something, if something is wrong, then your attention is drawn

163
00:15:37,440 --> 00:15:41,680
to it. So if you felt the coffee cup and there's a little burr on the side or a crack and you didn't

164
00:15:41,680 --> 00:15:46,880
know that was expected that you'd say, Oh, there's a crack. I mean, what was the brain doing when it

165
00:15:46,880 --> 00:15:51,760
was making that prediction? And we have a, we have a theory about this. And I wrote about it in the

166
00:15:51,760 --> 00:15:58,800
book a bit. And it's, it's a beautiful, I think it's a beautiful theory. But it, it's, it's

167
00:15:58,800 --> 00:16:02,400
basically most of the predictions that are going on in your brain, most of them, not all of them,

168
00:16:02,400 --> 00:16:08,880
but most of them happen inside individual neurons. They are, it is a internal to the

169
00:16:08,880 --> 00:16:14,160
individual neurons. Now, not a single neuron can predict something, but an ensemble of neurons do

170
00:16:14,160 --> 00:16:22,240
this. But it's an internal state. And we have, we wrote a paper that came out in 1990, 2016, excuse

171
00:16:22,240 --> 00:16:30,000
me, 2016, which is, it's called wider neurons have so many synapses. And we, what we posit in

172
00:16:30,000 --> 00:16:34,880
that paper, and I'm pretty sure this is correct is that, you know, neurons have these thousands of

173
00:16:34,880 --> 00:16:40,240
synapses. Most of those synapses are being used for prediction. And when a neuron recognizes a

174
00:16:40,240 --> 00:16:45,840
pattern and says, okay, I'm supposed to be active soon, I should be, I should be becoming active soon.

175
00:16:45,840 --> 00:16:50,240
If everything is according to our model here, I should be coming active soon. And it goes into

176
00:16:50,240 --> 00:16:56,480
this internal state, the neuron itself is saying, okay, I'm expecting to become active. And you

177
00:16:56,480 --> 00:17:01,200
can't detect that consciously. It's internal to the, it's essentially just a depolarization or

178
00:17:01,200 --> 00:17:08,000
change of the voltage of the neuron. And so when, but it, we showed how the network of these neurons,

179
00:17:08,000 --> 00:17:13,680
what'll happen is, if your prediction is correct, then a small subset of the neurons become active.

180
00:17:13,680 --> 00:17:17,840
But if the prediction is incorrect, a whole bunch of neurons become active at the same time.

181
00:17:18,480 --> 00:17:23,120
And then that draws your attention to the problem. So it's a fascinating problem. But most of

182
00:17:23,120 --> 00:17:27,600
the predictions going on in your brain are not accessible outside of individual neurons.

183
00:17:27,600 --> 00:17:29,200
So there's no way you could be conscious about it.

184
00:17:31,280 --> 00:17:35,520
I guess most people are familiar with the general anatomy of a neuron where you have a,

185
00:17:35,520 --> 00:17:41,840
this spindly looking thing where there's a cell body and there's a long process,

186
00:17:41,840 --> 00:17:48,720
the axon leading away, which carries the action potential, if that neuron fires

187
00:17:48,800 --> 00:17:55,760
to the synapse and communicates neurotransmitters to other neurons. But on the other side of,

188
00:17:56,960 --> 00:18:01,920
in the standard case, on the other side of the cell body, there's this really,

189
00:18:02,800 --> 00:18:09,440
often really profuse arborization of dendrites, which is kind of the mad tangle of processes,

190
00:18:09,440 --> 00:18:16,880
which receive information from other neurons to which this neuron is connected. And

191
00:18:17,440 --> 00:18:23,760
it's the integration of information on that side. But before that neuron fires,

192
00:18:23,760 --> 00:18:28,720
the change, the probability of its firing, that that's the place you are

193
00:18:29,440 --> 00:18:35,280
locating this, the full set of predictive changes or the full set of changes that constitute

194
00:18:35,280 --> 00:18:38,320
prediction in the case of a system of neurons.

195
00:18:38,320 --> 00:18:44,240
Yeah. It's interesting. For many years, people looked at those, the connections on the dendrites,

196
00:18:44,240 --> 00:18:50,480
on that bushy part called synapses. And when they activated a synapse, most of the synapses

197
00:18:51,040 --> 00:18:55,440
were so far from the cell body that they didn't really have much of an effect.

198
00:18:55,440 --> 00:19:00,640
They didn't seem like they could make anything happen. And so, but there are thousands and

199
00:19:00,640 --> 00:19:05,440
thousands of them out there, but they don't seem powerful enough to make anything occur.

200
00:19:06,080 --> 00:19:11,120
And what was discovered basically over the last 20 years, that there are,

201
00:19:11,120 --> 00:19:14,960
there's a second type of spike. So you mentioned the one that goes down the axon,

202
00:19:14,960 --> 00:19:19,520
that's the action potential. But there are spikes that travel along the dendrites.

203
00:19:20,320 --> 00:19:25,760
And so basically what happens is the individual sections of the dendrite, like little branches

204
00:19:25,760 --> 00:19:31,520
of this tree, each one of them can recognize patterns on their own. They can recognize hundreds of

205
00:19:31,520 --> 00:19:36,160
separate patterns on these different branches. And they can cause this spike to travel along

206
00:19:36,160 --> 00:19:42,880
the dendrite. And that lowers the, changes the voltage of the cell body a little bit.

207
00:19:42,880 --> 00:19:48,160
And that is what we call the predictive state. The cell is like prime. It says, oh, I, I'm,

208
00:19:48,160 --> 00:19:54,720
if I fire, I'm ready to fire. And it's not actually a probability change. It's the timing.

209
00:19:55,360 --> 00:19:59,360
And so a cell that's in this predictive state that says, I think I should be firing now,

210
00:20:00,240 --> 00:20:05,680
or very shortly, if it does generate the regular spike, the action potential, it does it a little

211
00:20:05,680 --> 00:20:10,000
bit sooner than it would have otherwise. And it's timing that is the key to making the whole

212
00:20:10,000 --> 00:20:14,080
circuit work. We're getting pretty down in the weeds here about the science. But I hope, I don't

213
00:20:14,080 --> 00:20:18,880
know if you're, all your readers, listeners will appreciate that. Yeah. No, I think it's useful

214
00:20:18,880 --> 00:20:25,280
though. More weeds here. But I mean, one of the novel things about your argument is that

215
00:20:26,160 --> 00:20:33,440
it was inspired by some much earlier theorizing. You mark your debt to Vernon Mountcastle. But

216
00:20:33,520 --> 00:20:40,560
the idea is that there's a, a common algorithm operating more or less everywhere at the level

217
00:20:40,560 --> 00:20:46,880
of the cortex. That is, it's more or less the, you know, the cortex is doing essentially the same

218
00:20:46,880 --> 00:20:55,520
thing, whether it's producing language or vision or, you know, any other sensory channel or motor

219
00:20:55,520 --> 00:21:02,800
behavior. So talk about the, the general principle that you spend a lot of time on in the book of

220
00:21:02,800 --> 00:21:10,080
just the organization of the neocortex into cortical columns and the implications this has for

221
00:21:10,960 --> 00:21:17,680
how we view what the brain is doing in terms of sensory and motor learning and, you know,

222
00:21:17,680 --> 00:21:22,880
all of its consequences. This is, Vernon Mountcastle made this proposal back in the 70s.

223
00:21:23,520 --> 00:21:28,800
And it's just a dramatic idea. And it's an incredible idea and so incredible that some

224
00:21:28,800 --> 00:21:34,080
people just refuse to believe it, but other people really think it's a tremendous discovery.

225
00:21:34,080 --> 00:21:38,640
But what he noticed was if you look at the neocortex, if you could take one out of your head

226
00:21:38,640 --> 00:21:43,600
or out of a human's head, it's like a sheet. It's about two and a half millimeters thick.

227
00:21:43,600 --> 00:21:50,240
It is about the size of a large dinner napkin or 1500 square centimeters. And if you could fold it,

228
00:21:50,240 --> 00:21:55,680
lay it flat. And the different parts of it, like they do different things as parts that do vision,

229
00:21:55,680 --> 00:22:02,320
as parts that do language and parts that do hearing and so on. But if you cut into it and you look at

230
00:22:02,320 --> 00:22:08,880
the structure in one of these areas, it's very complicated. There are dozens of different cell

231
00:22:08,880 --> 00:22:14,400
types, but they're very prototypically connected and they're arranged in certain patterns and

232
00:22:14,400 --> 00:22:19,760
layers and different types of things. So it's a very complex structure, but it's almost the same

233
00:22:19,760 --> 00:22:24,560
everywhere. It's not the same everywhere, but almost the same everywhere. And so this is not

234
00:22:24,560 --> 00:22:29,680
just true in a human neocortex, but if you look at a rat's neocortex or a dog's neocortex or a cat

235
00:22:29,680 --> 00:22:37,280
or a monkey, this same basic structure is there. And what Vernon Malkus said is that all the parts

236
00:22:37,280 --> 00:22:41,840
of the neocortex are actually, we think of them as doing things, different things, but they're

237
00:22:41,840 --> 00:22:46,880
actually all doing some fundamental algorithm, which is the same. So hearing and touch and vision

238
00:22:46,880 --> 00:22:51,200
are really the same thing. He says, if you took part of the cortex and you hook it up to your eyes,

239
00:22:51,200 --> 00:22:54,560
you'll get vision. If you hook it up to your ears, you'll get hearing. If you hook it up to

240
00:22:54,560 --> 00:23:01,120
other parts of the neocortex, you'll get language. And so he spent many years giving the evidence

241
00:23:01,120 --> 00:23:07,840
for this. He proposed further that this algorithm was contained in what's called a column. And so

242
00:23:07,840 --> 00:23:14,240
if you would take a small area of this neocortex, remember it's like two and a half millimeters

243
00:23:14,240 --> 00:23:20,880
thick, you take a very sort of skinny little one millimeter column out of it, that that is the

244
00:23:20,880 --> 00:23:28,560
processing element. And so our human neocortex, we have about 150,000 of these columns. Other

245
00:23:28,560 --> 00:23:33,360
animals have more or less. People should picture something resembling a grain of rice in terms

246
00:23:33,360 --> 00:23:37,840
of scale here. Yeah, yeah. I sometimes say take a piece of skinny spaghetti like, you know, angel

247
00:23:37,840 --> 00:23:41,920
have pasta or something like that and cut it into two little two and a half millimeter links

248
00:23:41,920 --> 00:23:46,720
and stack them side by side. Now the funny thing about columns is you can't see them. They're not

249
00:23:46,720 --> 00:23:51,680
visual things. You can't look under a microscope, you won't see it. But he pointed out why

250
00:23:53,200 --> 00:23:57,840
they're there. It has to do with how they're connected. So all the cells in one of these

251
00:23:57,840 --> 00:24:02,880
little millimeter pieces of rice or spaghetti, if you will, are all processing the same thing. And

252
00:24:02,880 --> 00:24:07,520
the next piece of rice over processing something different and the next piece of rice over processing

253
00:24:07,520 --> 00:24:13,920
something different. And so he didn't know what was going on in the cortical column. He

254
00:24:15,040 --> 00:24:20,480
articulated the architecture. He talked about the evidence that this exists. He said, here's the

255
00:24:20,480 --> 00:24:26,720
evidence why these things are all doing the same thing. But he didn't know what it was. And it's

256
00:24:26,720 --> 00:24:32,000
kind of hard to imagine what it is that this algorithm could be doing. But that was essentially

257
00:24:32,000 --> 00:24:35,600
the core of our research. That's what we've been focused on for close to 20 years.

258
00:24:35,920 --> 00:24:41,760
It's also hard to imagine the microanatomy here because in each one of these little columns,

259
00:24:42,400 --> 00:24:48,000
there's something like 150,000 neurons on average. And if you could just unravel

260
00:24:48,720 --> 00:24:57,440
all of the connections there, the tiny filaments of nerve endings, what you would have there is

261
00:24:57,440 --> 00:25:04,560
on the order of kilometers in length, all wound up into that tiny structure. So this is a

262
00:25:05,280 --> 00:25:11,520
strange juxtaposition of simplicity and complexity, but there's certainly a

263
00:25:11,520 --> 00:25:16,320
mad tangle of processes in there. Yeah, this is why brains are so hard to study. If you look at

264
00:25:16,320 --> 00:25:20,000
another organ in the body, whether it's the heart or the liver or something like that,

265
00:25:20,640 --> 00:25:26,320
and you take a little section of it, it's pretty uniform. But here, if you take a teeny piece of

266
00:25:26,320 --> 00:25:34,480
the cortex, it's got this incredible complexity in it, which is not random. It's very specific.

267
00:25:35,440 --> 00:25:40,800
So yeah, it's hard to get wrapper your heads around how complex it is. But we need to be

268
00:25:40,800 --> 00:25:46,880
complex because what we do as humans is extremely complex. And we shouldn't be fooled that we're

269
00:25:46,880 --> 00:25:51,680
just a bunch of neurons that are doing some mass action. No, there's a very complex processing

270
00:25:51,680 --> 00:25:58,160
going on in your brain that it's not just a blob of neurons that are pulsating,

271
00:25:58,320 --> 00:26:03,680
you know, very detailed mechanisms that are undergoing it. And we figured out what some of

272
00:26:03,680 --> 00:26:12,400
those are. So describe to me what you mean by this phrase, a reference frame. What does that mean at

273
00:26:12,400 --> 00:26:19,760
the level of the cortex and cortical columns? Yeah. So we're jumping to the end point,

274
00:26:19,760 --> 00:26:23,680
because that's not where we started. We were trying to figure out how the cortical columns work.

275
00:26:24,400 --> 00:26:30,080
And what we realized is that they're little modeling engines. Each one of these cortical

276
00:26:30,080 --> 00:26:36,240
columns is able to build a model of its input. And that model is what we would call a sensory

277
00:26:36,240 --> 00:26:42,640
motor model. Let's assume it's getting in from your finger, a tip of your finger. One of the

278
00:26:42,640 --> 00:26:47,120
columns is getting input from the tip of your finger. And as your finger moves and touches

279
00:26:47,120 --> 00:26:52,800
something, the input changes. But it's not just sufficient to know how the input changes. For

280
00:26:52,880 --> 00:26:57,200
you to build a model of the object you're touching. And I use the coffee cup example quite a bit,

281
00:26:57,200 --> 00:27:00,560
because that's how we did it. If you move your finger over the coffee cup,

282
00:27:00,560 --> 00:27:03,520
and you're not even looking at the coffee cup, you could learn a model of the coffee cup. You

283
00:27:03,520 --> 00:27:07,280
could feel it just with one finger, you could feel like, oh, what this is what its shape is.

284
00:27:07,920 --> 00:27:11,840
But to do that, your brain, that cortical column, your brain as a whole, but that

285
00:27:11,840 --> 00:27:16,240
cortical column on individually has to know something about where your finger is relative

286
00:27:16,240 --> 00:27:21,200
to the cup. It's not just a changing pattern that's coming in. It has to know how your finger's

287
00:27:21,200 --> 00:27:26,640
moving and where your finger is as it touches it. So the idea of a reference frame is a way of

288
00:27:27,200 --> 00:27:31,440
noting a location. You have to have a location signal. You have to have some knowledge about

289
00:27:32,000 --> 00:27:36,480
where things are in the world relative to other things. In this case, where's your finger relative

290
00:27:36,480 --> 00:27:42,080
to the object you're trying to touch, the coffee cup. And we realize that for you to, your brain

291
00:27:42,080 --> 00:27:46,400
to make a prediction of what you're going to feel when you touch the edge of the cup. And again,

292
00:27:46,400 --> 00:27:49,440
you used to mention earlier, you're not conscious of this, you'd reach the cup and you just,

293
00:27:49,440 --> 00:27:52,080
but your brain's predicting what all your fingers are going to feel.

294
00:27:53,200 --> 00:27:57,120
It needs to know where the finger's going to be. And I have to know what the object is,

295
00:27:57,120 --> 00:28:01,040
it's a cup and these know where it's going to be. And that requires a reference frame.

296
00:28:01,040 --> 00:28:06,480
A reference frame is just a way of noting a location. It's saying relative to this cup,

297
00:28:06,480 --> 00:28:11,200
your finger is over here, not over there, not on the handle, you know, up at the top, whatever it is.

298
00:28:11,920 --> 00:28:16,400
And, and this is a deduced property, we can say for certainty that this has to exist.

299
00:28:16,480 --> 00:28:19,280
If your finger is going to make a prediction when it reaches and touches the coffee cup,

300
00:28:19,280 --> 00:28:22,960
it needs to know where the finger is, that the location has to be relative to the cup.

301
00:28:23,520 --> 00:28:28,080
So we can just say for certainty that there needs to be reference frames in the brain. And this is

302
00:28:28,080 --> 00:28:33,280
not a controversial idea. Well, we, perhaps this novel is that we realize that these reference

303
00:28:33,280 --> 00:28:38,400
frames exist in every cortical column. And it's the structure of knowledge. It applies to not just

304
00:28:38,400 --> 00:28:42,720
what your finger feels on a coffee cup and what you see when you look at it, but also how you

305
00:28:42,720 --> 00:28:48,480
arrange all your knowledge in the world is stored in these reference frames. And so we're jumping

306
00:28:48,480 --> 00:28:55,520
ahead here many steps. But when we think and when we posit, when we try to, you know, reason in our

307
00:28:55,520 --> 00:29:02,000
head, when even my language right now is, is where the neurons are walking through locations in

308
00:29:02,000 --> 00:29:06,400
reference frames, recalling the information stored there. And that's what comes into your head or

309
00:29:06,400 --> 00:29:11,360
that's what you say. So it becomes the core reference, the reference name becomes the core

310
00:29:11,360 --> 00:29:15,360
structure for the entire, everything you do. It's knowledge about the world is in these reference

311
00:29:15,360 --> 00:29:21,920
frames. Yeah, you make a strong claim about the, the primacy of motion, right? Because there's,

312
00:29:21,920 --> 00:29:28,080
everyone knows that there's part of the cortex devoted to motor action. We refer to it as the

313
00:29:28,080 --> 00:29:34,480
motor cortex and distinguish it from sensory cortex in that way. But it's also true that

314
00:29:34,480 --> 00:29:40,080
other regions of the cortex and, and perhaps every region of the cortex does have some

315
00:29:40,800 --> 00:29:47,680
connection to lower structures that can affect motion, right? So it's not, it's not that it's

316
00:29:47,680 --> 00:29:54,320
just motor cortex that's in the, in the motion game. And by analogy or by direct implication,

317
00:29:54,320 --> 00:30:02,480
you think of thought as itself being a kind of movement in conceptual space, right? So there's

318
00:30:02,480 --> 00:30:08,320
a mapping of the, the sensory world that can really only be accomplished by acting on it,

319
00:30:08,320 --> 00:30:13,840
you know, and therefore moving, right? So the other way to map the cup, you know, is to touch

320
00:30:13,840 --> 00:30:20,960
it with your fingers in the end. There is a, an analogous kind of motion in conceptual space

321
00:30:20,960 --> 00:30:26,240
and, you know, even, you know, abstract ideas like, I think some of the examples even in the book

322
00:30:26,240 --> 00:30:31,040
are like, you know, democracy, right? You know, or, or money or what, just how, how we understand

323
00:30:31,040 --> 00:30:36,240
these things. So let's go back to the first thing you said there. The idea that there's motor cortex

324
00:30:36,240 --> 00:30:43,760
and sensory cortex is sort of no longer considered right. As you mentioned, we, the neurons that,

325
00:30:43,760 --> 00:30:47,920
in these cortical columns, there are certain neurons that are the motor output neurons.

326
00:30:47,920 --> 00:30:52,880
These are in a particular layer five, as they're called. Until in the motor cortex,

327
00:30:52,880 --> 00:30:57,280
they were really big and they project to the spinal cord and say, oh, that's how you move your

328
00:30:57,280 --> 00:31:02,640
fingers. But if you look at the neurons, the columns in the visual cortex, the parts that get

329
00:31:02,640 --> 00:31:08,320
input from the eyes, they have the same layer five cells. And these cells project to a part

330
00:31:08,320 --> 00:31:13,360
of the brain called the superior colliculus, which is what controls eye motion. So this goes

331
00:31:13,360 --> 00:31:17,200
against the original idea of, oh, there's sensory cortex and motor cortex. No one believes that,

332
00:31:17,200 --> 00:31:21,680
well, I don't know, buddy, but very few people believe that anymore. It's, as far as we know,

333
00:31:21,680 --> 00:31:26,240
every part of the cortex has a motor output. And so every part of the cortex is getting some sort

334
00:31:26,240 --> 00:31:32,160
of input and it has some motor output. And so the basic algorithm of cortex is a sensory motor

335
00:31:32,160 --> 00:31:37,440
system. It's not divided. It's not like we have sensory areas and motor areas. As far as we know,

336
00:31:37,440 --> 00:31:42,320
ever it's been seen, there's these motor cells everywhere. So we can put that aside.

337
00:31:43,040 --> 00:31:52,160
Now, I can very clearly walk you through, in some sense, prove from logic that when you're

338
00:31:52,160 --> 00:31:57,200
learning what a coffee cup feels like, and I can even do this for vision, that you have to have this

339
00:31:57,200 --> 00:32:01,120
idea of a reference frame, that the finger, you have to know where your finger is relative to the

340
00:32:01,120 --> 00:32:05,680
cup. And that's how you build a model of it. And so we can build out this cortical column that

341
00:32:05,680 --> 00:32:10,080
explains how it does that. How does your, how does your parts of your cortex that representing

342
00:32:10,080 --> 00:32:15,120
your fingers are able to learn to structure a coffee cup? Now, Mount Castle, go back to him,

343
00:32:15,120 --> 00:32:20,320
he said, look, it's the same algorithm everywhere. And he says, it looks the same everywhere. So it's

344
00:32:20,320 --> 00:32:24,720
the same algorithm everywhere. So that should sort of say, hmm, well, if I'm thinking about something

345
00:32:24,720 --> 00:32:29,040
that doesn't seem like a sensory motor system, like I'm not touching something or looking,

346
00:32:29,040 --> 00:32:33,760
I'm just thinking about something, that would, if Mount Castle was right, then the same basic

347
00:32:33,760 --> 00:32:38,480
algorithm would be applying there. So that was one constraint like, well, that, you know, and the

348
00:32:38,480 --> 00:32:42,560
evidence is that Mount Castle is right. I mean, the physical evidence suggests he's right. We just,

349
00:32:42,560 --> 00:32:46,320
it just comes a little bit odd to think like, well, how is language like this and how is

350
00:32:46,320 --> 00:32:51,200
mathematics like, you know, touching a coffee cup. But then we realize that it's just,

351
00:32:51,200 --> 00:32:56,400
reference frames are a way of storing everything. And, and in the way we move through a reference

352
00:32:56,880 --> 00:32:59,520
frame, it's like, how do you move from one location? How do the neurons

353
00:33:00,160 --> 00:33:05,200
activate one location after another location after another location? We do that to this

354
00:33:05,200 --> 00:33:09,840
idea of movement. So I'm moving, if I want to access the locations on a coffee cup, I move my

355
00:33:09,840 --> 00:33:15,920
finger. But the same concept could apply to mathematics or to politics, but you're not actually

356
00:33:15,920 --> 00:33:21,440
physically moving something, but you're still walking through a structure. A good, a good bridge

357
00:33:21,440 --> 00:33:26,800
example is if I say to you, you know, imagine your house. And I ask you to walk, you know,

358
00:33:26,800 --> 00:33:31,840
tell me about your house. What you'll do is you'll mentally imagine walking through your house.

359
00:33:31,840 --> 00:33:36,000
It won't be random. You just won't have random thoughts come to your head. But you will mentally

360
00:33:36,000 --> 00:33:39,840
imagine walking through your house. And as you walk through your house, you'll recall what is

361
00:33:39,840 --> 00:33:42,880
supposed to be seen in different directions. You can say, oh, I'll walk in the front door and I'll

362
00:33:42,880 --> 00:33:47,120
look to the right. What do I see? I'll look to the left. What do I see? This is sort of a,

363
00:33:47,120 --> 00:33:50,960
an example you could relate it to something physically you could move to. But that's pretty

364
00:33:50,960 --> 00:33:54,880
much what's going on when you're thinking about anything. If you're thinking about your podcast

365
00:33:54,880 --> 00:33:59,120
and how you can get more subscribers, you have a model of that in your head. And you're, you are

366
00:34:00,320 --> 00:34:04,480
trying it out thinking about different aspects by literally invoking these different locations

367
00:34:04,480 --> 00:34:08,640
and reference frames. And so that's sort of the core of all knowledge.

368
00:34:08,640 --> 00:34:13,600
Yeah, it's interesting. I guess back to Mount Castle for a second. One piece of evidence in

369
00:34:13,600 --> 00:34:20,240
favor of this view of a common cortical algorithm is the fact that adjacent areas of cortex can be

370
00:34:21,200 --> 00:34:30,080
appropriated by various functions. If you lose your vision, say, classical visual cortex can be

371
00:34:30,080 --> 00:34:37,040
appropriated by other senses. And there's this plasticity that can ignore some of the previous

372
00:34:37,040 --> 00:34:43,120
boundaries between separate senses in the cortex. Yeah, that's right. There's this tremendous

373
00:34:43,120 --> 00:34:48,640
plasticity and you can also recover from various sorts of trauma and so on. I mean, there's some

374
00:34:48,640 --> 00:34:53,360
rewiring has to occur, but it does show that that whatever is going, whatever the circuitry in the

375
00:34:53,360 --> 00:34:59,520
visual cortex was, you know, quote, if you were a sighted person, what it would do, if you're not

376
00:34:59,520 --> 00:35:04,880
a sighted person, well, it'll just do something else. And so it's not, and so that is a very,

377
00:35:04,880 --> 00:35:10,960
very strong argument for that. There's a famous scientist, Bakurita, who did an experiment where

378
00:35:10,960 --> 00:35:16,480
he, I'm trying to remember the animal he used, maybe even recall it. But anyway, it'll come to me.

379
00:35:16,560 --> 00:35:22,560
A ferret, I think it was a ferret. Before the animal was born, he took the optic nerve

380
00:35:22,560 --> 00:35:26,400
and ran it over to one part of the, a different part of the neocortex and took the auditory nerve

381
00:35:26,400 --> 00:35:31,040
and ran it to a different part of the neocortex. Basically rewired the animal. I'm not sure we

382
00:35:31,040 --> 00:35:35,920
do these experiments today, but, and, you know, and the argument was that the animals, you know,

383
00:35:35,920 --> 00:35:41,920
still saw and still heard and so on, maybe not as well as an unaltered one, but the evidence was

384
00:35:41,920 --> 00:35:49,040
that, yeah, that really works. Yeah, so what is genetically determined and what is learned here?

385
00:35:49,040 --> 00:35:56,880
I mean, it seems that the genetics at minimum are determining what is hooked up to what initially,

386
00:35:56,880 --> 00:36:01,600
right, you know, barn? Yeah, roughly, roughly, that's right. I think, you know, like, where do the

387
00:36:01,600 --> 00:36:05,760
eyes, the optic nerve from the eyes, where do they project? And where do the regions that get

388
00:36:05,760 --> 00:36:11,520
the input from the eyes, where do they project? And so this rough sort of overall architecture

389
00:36:11,520 --> 00:36:16,640
is specified. And as we just talked through trauma and other reasons, sometimes that architecture

390
00:36:16,640 --> 00:36:23,040
can get rewired. I think also the, the, the basic algorithm that goes on in each of these

391
00:36:23,040 --> 00:36:27,840
cortical columns, the, the circuitry in the, in, inside the neocortex is pretty well determined by,

392
00:36:29,040 --> 00:36:34,640
by genetics. And in fact, what one of my guess's arguments was that humans, the human neocortex

393
00:36:34,640 --> 00:36:39,600
got large, and we have a very large one relative to our body size, just because all it had,

394
00:36:39,600 --> 00:36:43,200
all evolution had to do is discover, just make more copies of these columns, you know, you

395
00:36:43,200 --> 00:36:46,800
don't have to, you don't have to do anything new, just make more copies. And that's something easy

396
00:36:46,800 --> 00:36:52,640
for genes to specify. And so human brains got large quickly in evolutionary time,

397
00:36:52,640 --> 00:37:00,400
by that just replicate more of it type of thing. Okay, so let's go beyond the human now and talk

398
00:37:00,400 --> 00:37:07,840
about artificial intelligence. And before we talk about the risks or the imagined risks,

399
00:37:08,800 --> 00:37:14,240
tell me what you think the path looks like going forward. I mean, what are we doing now?

400
00:37:14,240 --> 00:37:20,800
And what do you think we need to do to have our dreams of true artificial general intelligence

401
00:37:20,800 --> 00:37:27,760
realized? Well, you know, today's AI is powerful as it is and successful as it is.

402
00:37:28,800 --> 00:37:35,520
I think most senior AI practitioners will admit, and many of them have, that they don't really

403
00:37:35,520 --> 00:37:39,840
think they're intelligent. You know, they're really wonderful pattern classifiers, and they

404
00:37:39,840 --> 00:37:45,040
can do all kinds of clever things. But there are very few practitioners would say, hey, this AI

405
00:37:45,040 --> 00:37:49,760
system that's recognizing faces is really intelligent. And there's a sort of a lack of

406
00:37:49,760 --> 00:37:54,960
understanding what intelligence is and how to go forward. And how do you make a system that could

407
00:37:54,960 --> 00:38:01,440
solve general problems, could do more than one thing, right? And so in the second part of my book,

408
00:38:01,440 --> 00:38:06,880
I lay out what I believe are the requirements to do that. And my approach has always been,

409
00:38:06,880 --> 00:38:10,880
for 40 years, has been like, well, I think we need to first figure out what brains do

410
00:38:11,920 --> 00:38:16,560
and how they do them. And then we'll know how to build intelligent machines, because we just

411
00:38:16,560 --> 00:38:23,680
don't seem able to intuit what an intelligent machine is. So I think the way I look at this

412
00:38:23,680 --> 00:38:27,840
problem, if we want to make, you know, what's the recipe for making an intelligent machine,

413
00:38:28,640 --> 00:38:32,640
is you have to say, what are the principles by which the brain works that we need to replicate

414
00:38:32,640 --> 00:38:37,520
and which principles don't we need to replicate? And so I made a list of these in the book, but

415
00:38:38,720 --> 00:38:42,080
if you can think of a very high level, they have to have some sort of embodiment,

416
00:38:42,080 --> 00:38:47,200
they have to have the ability to move their sensors somehow in the world. You know, you can't

417
00:38:47,200 --> 00:38:54,080
really learn how to use tools and how to, you know, run factories and how to do things unless

418
00:38:54,160 --> 00:38:58,960
you can move in the world. And it requires these reference frames I was talking about,

419
00:38:58,960 --> 00:39:02,800
because movement requires reference frames. But that's not a controversial statement,

420
00:39:02,800 --> 00:39:07,360
it's just a fact. You're going to have to have no where things are in the world.

421
00:39:08,000 --> 00:39:13,680
And then the final, there's a set of things, but one of the other big ones, which we haven't

422
00:39:13,680 --> 00:39:18,080
talked about yet, and which is where the title of the book comes from, A Thousand Brains, is that

423
00:39:18,640 --> 00:39:23,520
the way to think about our near cortex, it has 150,000 of these columns,

424
00:39:23,520 --> 00:39:29,120
we have essentially 150,000 separate modeling systems going on in our brain. And they work

425
00:39:29,120 --> 00:39:36,240
together by voting. And so that concept of a distributed intelligence system is important.

426
00:39:36,960 --> 00:39:41,520
We're not just one thing, we, it feels like we're one thing, but we're really 150,000 of these

427
00:39:41,520 --> 00:39:45,680
things. And we're only conscious of being one thing, but that's not really what's happening

428
00:39:45,680 --> 00:39:50,800
under the covers. So those are some of the key ideas. I've just picked a very, very high idea.

429
00:39:50,800 --> 00:39:54,880
It has to have an embodiment, it has to be able to move its sensors, it has to be able to

430
00:39:54,880 --> 00:40:00,320
organize information and reference frames, and it has to be distributed. And that's how we can do

431
00:40:00,960 --> 00:40:03,680
multiple sensors and sensory integration and things like that.

432
00:40:04,960 --> 00:40:13,840
I guess I question the criteria of embodiment and movement, right? I mean, I understand that

433
00:40:14,560 --> 00:40:20,480
practically speaking, that's how a useful intelligence can get trained up in our world

434
00:40:20,480 --> 00:40:27,520
to do things physically in our world. But it seems like you could have a perfectly intelligent

435
00:40:27,520 --> 00:40:37,440
system, i.e. a mind that is turned loose on simulated worlds and are capable of solving

436
00:40:37,440 --> 00:40:45,040
problems that don't require effectors of any kind. I mean, chess is obviously a very low-level

437
00:40:45,040 --> 00:40:50,240
analogy, but just imagine a thousand things like chess that represent real

438
00:40:51,680 --> 00:40:57,520
theory building or cognition in a box. Yeah, I think you're right. And so

439
00:40:58,160 --> 00:41:02,720
when I use the word movement or embodiment, and I'm careful to define it in the book because

440
00:41:03,520 --> 00:41:10,480
it doesn't have to be physical. An example I gave, you can imagine an intelligent agent that

441
00:41:10,480 --> 00:41:16,240
lives in the Internet, and movement is following links. It's not a physical thing,

442
00:41:16,800 --> 00:41:22,880
but there's still this conceptual mathematical idea of what it means to move. And so you're

443
00:41:22,880 --> 00:41:29,520
changing the location of some representation, and that could be virtual. It doesn't have to

444
00:41:29,520 --> 00:41:35,520
have a physical embodiment, but in the end, you can't learn about the world just by looking at

445
00:41:35,520 --> 00:41:42,720
a set of pictures. That's not going to happen. You can learn to classify pictures, but so some

446
00:41:42,720 --> 00:41:49,040
AI systems will have to be physically embodied like a robot, if I guess you want. Many will not

447
00:41:49,040 --> 00:41:54,880
be, many will be virtual, but they all have this internal process which I could point to the thing

448
00:41:54,880 --> 00:41:58,560
that says, here's where the reference frame is, here's where your current location is, here's

449
00:41:58,560 --> 00:42:03,920
how it's moving to a new location based on some movement vector. Like a verb, a word, you can

450
00:42:03,920 --> 00:42:08,800
think of that as like an action. And so you can have an action that's not physical, but it's

451
00:42:08,800 --> 00:42:12,480
still an action, and it moves to a new location in this internal representation.

452
00:42:12,480 --> 00:42:17,680
Right, right. Okay, well, let's talk about risk, because this is the place where I think you and

453
00:42:17,680 --> 00:42:25,760
I have very different intuitions. You are, as far as I can tell from your book, you seem very

454
00:42:25,760 --> 00:42:35,200
sanguine about AI risk. And really, you seem to think that the only real risk, the serious risk

455
00:42:35,200 --> 00:42:42,080
of things going very badly for us is that bad people will do bad things with much more powerful

456
00:42:42,080 --> 00:42:47,760
tools. So the heuristic here would be, you know, don't give your super intelligent AI to the next

457
00:42:47,760 --> 00:42:54,400
Hitler, because that would be bad. But other than that, the generic problem of self-replication,

458
00:42:54,400 --> 00:42:59,920
which you talk about briefly, and you point out, we face that on other fronts, like with,

459
00:42:59,920 --> 00:43:03,920
you know, with the pandemic, where we've been dealing with, I mean, so natural viruses and

460
00:43:03,920 --> 00:43:09,840
bacteria or computer viruses, I mean, there's anything that can self-replicate can be dangerous.

461
00:43:10,480 --> 00:43:17,840
But that aside, you seem quite confident that AI will not get away from us. There won't be an

462
00:43:17,840 --> 00:43:23,920
intelligence explosion. And we don't have to worry too much about the so-called alignment problem.

463
00:43:24,960 --> 00:43:29,600
And at one point, you even question whether it makes sense to expect that we'll produce

464
00:43:30,560 --> 00:43:35,360
something that can be appropriately called superhuman intelligence. So Brett, perhaps you

465
00:43:35,360 --> 00:43:42,960
can explain the basis for your optimism here. So I think what most people, and perhaps yourself,

466
00:43:43,600 --> 00:43:51,200
have fears about is they use humans as an example of how things can go wrong.

467
00:43:51,200 --> 00:43:56,080
And so we think about the alignment problem, or we think about, you know, motivations of an AI

468
00:43:56,080 --> 00:44:01,680
system. Well, okay, does the AI system have motivations or not? Does it have a desire to do

469
00:44:01,680 --> 00:44:09,440
anything? Now, as a human, an animal, we all have desires, right? But if you take apart what parts

470
00:44:09,440 --> 00:44:15,040
of the human brain are doing, different parts, there's some parts that are just building this

471
00:44:15,040 --> 00:44:20,560
model of the world. And this is the core of our intelligence. This is what it means to be intelligent.

472
00:44:20,560 --> 00:44:27,040
That part itself is benign. It has no motivations on its own. It doesn't desire to do anything.

473
00:44:27,040 --> 00:44:33,920
I use an example of a map. You know, a map is a model of the world. And my map can be

474
00:44:34,640 --> 00:44:40,960
very powerful tool for some to do good or to do bad. But on its own, the map doesn't do anything.

475
00:44:41,520 --> 00:44:46,240
So if you think about the neocortex on its own, it sits on top of the rest of your brain.

476
00:44:46,880 --> 00:44:51,760
And the rest of your brain is really what makes us motivated. It gets us, you know, we have our

477
00:44:52,880 --> 00:44:58,160
good sides and our bad sides, you know, our desire to maintain our life and have sex and

478
00:44:58,160 --> 00:45:01,840
aggression and all these stuff. The neocortex is just sitting there. It's like a map. It says,

479
00:45:01,840 --> 00:45:06,800
you know, I understand the world and you can use me as how you want. So when we build intelligent

480
00:45:06,800 --> 00:45:13,120
machines, we have the option and, and I think almost imperative not to build the old parts

481
00:45:13,120 --> 00:45:18,720
of the brain, too. You know, why do that? We just have this thing, which is inherently smart,

482
00:45:18,720 --> 00:45:23,680
but on its own doesn't really want to do anything. And so there's some of the some of the risks that

483
00:45:23,680 --> 00:45:31,120
come about from the people's fears about the alignment problem, specifically, is that the

484
00:45:32,000 --> 00:45:36,800
intelligent agent will decide on its own or decide for some reason to do things that are

485
00:45:36,800 --> 00:45:41,120
in its best interest, not in our best interest, or maybe it'll listen to us, but then not listen

486
00:45:41,120 --> 00:45:47,120
to us or something like this. I just don't see how that can physically happen. And for people,

487
00:45:47,120 --> 00:45:51,360
most people don't understand the separation. They just assume that this intelligence is wrapped up

488
00:45:51,360 --> 00:45:55,920
in these, all these, all the things that make us human. The intelligence explosion problem is a

489
00:45:55,920 --> 00:46:01,040
separate issue. I'm not sure which one of those you're more worried about. Yeah, well, let's,

490
00:46:01,040 --> 00:46:07,040
let's deal with the alignment issue first. And I do think that's more critical. But

491
00:46:08,000 --> 00:46:13,440
let me see if I can capture what troubles me about this picture you've painted here. It seems

492
00:46:13,440 --> 00:46:23,280
that you're, to my mind, you're being strangely anthropomorphic on one side, but not anthropomorphic

493
00:46:23,280 --> 00:46:29,920
enough on the other. I mean, so like, you know, you think that to understand intelligence and

494
00:46:30,000 --> 00:46:37,120
actually truly implement it in machines, we really have to be focused on ourselves first. And we

495
00:46:37,120 --> 00:46:43,840
have to understand how the human brain works and then emulate those principles pretty directly in

496
00:46:43,840 --> 00:46:50,320
machines. That strikes me as possibly true, but possibly not true. And if, if I had to bet, I

497
00:46:50,320 --> 00:46:58,720
think I would probably bet against it. Although even here, you seem to be not taking full account of

498
00:46:59,280 --> 00:47:04,240
what the human brain is doing. I mean, like, we, you know, we can't partition reason and emotion

499
00:47:05,040 --> 00:47:10,480
as clearly as we thought we could hundreds of years ago. And in fact, you know, certain emotions,

500
00:47:10,480 --> 00:47:15,840
you know, certain drives are built into our being able to reason effectively.

501
00:47:15,840 --> 00:47:20,720
I think that's, you know, I'll take an exception to that. I know, I know this is an opinion that

502
00:47:21,360 --> 00:47:24,160
you had Lisa Barrett on your program recently.

503
00:47:24,800 --> 00:47:27,920
Antonio Demasio is the person who's banged on about this the most.

504
00:47:27,920 --> 00:47:33,120
Yeah, I know. And I just disagree. I just, it's, you know, you can separate these two.

505
00:47:33,680 --> 00:47:40,320
And I can say this because I understand actually what's going on in the New York Cortex.

506
00:47:40,320 --> 00:47:44,720
And I can see what I have a very good sense of what these actual neurons are actually doing

507
00:47:44,720 --> 00:47:51,120
when it's modeling the world and so on. And you do not, it does not require this emotional component.

508
00:47:51,200 --> 00:47:56,640
A human does. Now, you say, you know, on one hand, I don't argue we should replicate the brain.

509
00:47:56,640 --> 00:48:01,040
I say we should replicate the structures of the New York Cortex, which is not replicating the brain.

510
00:48:01,760 --> 00:48:06,800
It's just one part of the brain. And so I'm specifically saying, you know, I don't really

511
00:48:06,800 --> 00:48:11,920
care too much about how this spinal cord works or how, you know, the brainstem does this or that.

512
00:48:11,920 --> 00:48:15,920
It's interesting. Maybe I know a little bit about it, but that's not important. The cortex sits on

513
00:48:15,920 --> 00:48:20,160
top of another structure and the cortex does its own thing and they interact. Of course,

514
00:48:20,240 --> 00:48:24,560
they interact. And our emotions affect what we learn and what we don't learn.

515
00:48:24,560 --> 00:48:29,600
But it doesn't have to be that way in a system, another system that we build.

516
00:48:29,600 --> 00:48:30,800
That's the way humans are structured.

517
00:48:30,800 --> 00:48:34,640
Yeah. Okay. So I would agree with that except the boundary between

518
00:48:35,520 --> 00:48:43,920
what is an emotion or a drive or a motivation or a goal and what is a value neutral mapping of

519
00:48:43,920 --> 00:48:51,680
reality. You know, I think that boundary is perhaps harder to specify than you think it is.

520
00:48:51,680 --> 00:48:57,200
And that certain of these things are connected, right? Which is to, I mean, here's an example.

521
00:48:57,200 --> 00:49:03,120
This is probably not a perfect analogy, but this gets at some of the surprising features of cognition

522
00:49:03,120 --> 00:49:12,160
that may await us. So we think intuitively that understanding a proposition is cognitively quite

523
00:49:12,240 --> 00:49:18,000
distinct from believing it, right? So I can give you a statement that you can believe or

524
00:49:18,000 --> 00:49:22,560
disbelieve or be uncertain about. I can say, you know, there's two plus two equals four,

525
00:49:22,560 --> 00:49:28,160
two plus two equals five, and that can give you some gigantic number and say this number is prime.

526
00:49:28,160 --> 00:49:33,120
And presumably in the first condition, you'll say, yes, I believe that. In the second, you'll say,

527
00:49:33,120 --> 00:49:38,240
no, that's false. And in the third, you won't know whether or not it's prime or not.

528
00:49:38,720 --> 00:49:43,600
So those are distinct states that we can intuitively differentiate. But there's also evidence

529
00:49:44,240 --> 00:49:49,440
to suggest that merely comprehending a statement, if I give you a statement and you

530
00:49:49,440 --> 00:49:57,520
parse it successfully, the parsing itself contains an actual default acceptance of it as true.

531
00:49:58,240 --> 00:50:05,120
And rejecting it as false is a separate operation added to that. I mean, there's not a ton of

532
00:50:05,120 --> 00:50:10,160
evidence for this, but there's certainly some behavioral evidence. So if I put you in a paradigm

533
00:50:10,160 --> 00:50:15,040
where we gave you statements that were true and false, and all you had to do was judge them true

534
00:50:15,040 --> 00:50:21,680
and false, and they were all matched for complexity. So, you know, two plus two equals four is no more

535
00:50:21,680 --> 00:50:27,360
or less complex than two plus two equals five. But it'll take you longer, systematically longer,

536
00:50:27,360 --> 00:50:32,000
to judge very simple statements to be false than to judge them to be true,

537
00:50:32,000 --> 00:50:37,280
suggesting that you're doing a further operation. Now, we can remain agnostic as to whether or

538
00:50:37,280 --> 00:50:43,120
not that's actually true. But if true, it's counterintuitive that merely understanding

539
00:50:43,120 --> 00:50:49,840
something entails some credence, epistemic credence given to it by default, and that

540
00:50:49,840 --> 00:50:56,320
to reject it as false represents a subsequent act. But that's the kind of thing that already

541
00:50:56,400 --> 00:51:03,360
were on territory that is not coldly rational. Some of the all too apish appetites have kind

542
00:51:03,360 --> 00:51:10,800
of crept into cognition here in ways that we didn't really budget for. And so the question is,

543
00:51:10,800 --> 00:51:15,200
just how much of that is avoidable in building a new type of mind?

544
00:51:16,160 --> 00:51:21,600
Well, you know, I'm not familiar with that specific research. And so I haven't heard of that. But

545
00:51:22,320 --> 00:51:29,520
to me, none of these things are surprising in any way. Just if you start thinking about the brain

546
00:51:29,520 --> 00:51:33,440
is basically trying to build models, it's constantly trying to build models. In fact,

547
00:51:34,640 --> 00:51:39,280
as you walk around your life, day to day, moment to moment, and you see things,

548
00:51:39,280 --> 00:51:42,480
you're building the model, the model is being constructed, even like where are things in the

549
00:51:42,480 --> 00:51:46,160
refrigerator right now, your brain will update, you open the refrigerator, oh, the milk's on the

550
00:51:46,160 --> 00:51:50,480
left today, whatever. And so if someone gives you a proposition like two plus two equals five,

551
00:51:51,040 --> 00:51:54,560
you know, I don't know what the evidence that you believe it and then falsify it.

552
00:51:54,560 --> 00:51:58,720
But I certainly imagine you can imagine it trying to see if it's right. It'd be like me saying to

553
00:51:59,360 --> 00:52:02,720
you, hey, you know, Sam, the milk was on the right in your refrigerator. And you'd have to

554
00:52:02,720 --> 00:52:06,800
think about it for a second. You say, well, let me think. No, last time I saw it was on the left.

555
00:52:06,800 --> 00:52:11,360
You know, no, that's wrong. But you would walk through the process of trying to imagine it

556
00:52:11,360 --> 00:52:17,360
and trying to see, does that fit my model? And yes or no. And I don't, it's not surprising to me

557
00:52:17,360 --> 00:52:22,800
that you would have to process it the way as if it was true. It's just matters saying,

558
00:52:22,800 --> 00:52:27,520
can you imagine this? Go imagine it. Do you think it's right? It's not like I believe that now I

559
00:52:27,520 --> 00:52:32,480
falsified it. It's more likely. Well, actually, I'll just give you one other datum here because

560
00:52:32,480 --> 00:52:38,560
it's just intellectually interesting and socially all too consequential. This effect goes by

561
00:52:38,560 --> 00:52:43,760
several names, I think. But one is the illusory truth effect, which is even in the act of

562
00:52:43,760 --> 00:52:49,680
disconfirming something to be false, you know, some specious rumor or conspiracy theory,

563
00:52:50,320 --> 00:52:55,680
merely having to invoke it, I mean, to have people entertain the concept again, even in the context

564
00:52:55,680 --> 00:53:04,000
of debunking it, ramifies a belief in it in many, many people. It's just, it becomes harder to

565
00:53:04,000 --> 00:53:06,640
discredit things because you have to talk about them in the first place.

566
00:53:06,640 --> 00:53:12,320
Yeah. I mean, so look, we're talking about language here, right? And in language,

567
00:53:12,400 --> 00:53:17,360
so much of what we humans know is via language. And we have no idea if it's true when someone says

568
00:53:17,360 --> 00:53:24,240
something to you, right? How do you know? And so you'd have to, so I mean, I gave an example like,

569
00:53:24,240 --> 00:53:28,880
I've never been to the city of Havana. Well, I believe it's there. I believe it's true. I don't

570
00:53:28,880 --> 00:53:33,120
know. I've never been there. I've never actually touched or smelled it or saw it. So maybe it's

571
00:53:33,120 --> 00:53:38,720
false. So I just, I mean, this is one of the issues we have. I have a whole chapter on false

572
00:53:38,720 --> 00:53:45,040
beliefs because so much of our knowledge of the world is built up on language. And the default

573
00:53:45,040 --> 00:53:50,480
assumption under language that if someone says something, it's true. It's like, it's a pattern

574
00:53:50,480 --> 00:53:54,800
in the world. You're going to accept it. If I touch a coffee cup, I accept that that's what it feels.

575
00:53:54,800 --> 00:53:59,280
Right. And if I look at something, I accept that's what it looks like. Well, if someone says

576
00:53:59,280 --> 00:54:04,160
something, my initial acceptance is, okay, that's what it is. So, you know, and then I'm going to

577
00:54:04,160 --> 00:54:08,240
say, in fact, well, if someone says something that's false, of course, well, that's a problem

578
00:54:08,240 --> 00:54:14,080
because just by the fact that I've experienced it, it's now part of my world model. And that's

579
00:54:14,080 --> 00:54:19,120
what you're referring to. I can see this is really a problem of language we face. And this is the

580
00:54:19,120 --> 00:54:23,840
root cause of almost all of our false beliefs, is that someone just says something enough times.

581
00:54:24,640 --> 00:54:30,240
And that's good enough. And you have to seek out contrary evidence for it.

582
00:54:30,720 --> 00:54:34,000
Yeah, sometimes it's good enough. Even when you're the one saying it, you just overhear

583
00:54:35,280 --> 00:54:42,080
the voice of your own mind saying it. And no, I know. That's been proven that everyone is

584
00:54:42,080 --> 00:54:46,800
susceptible to that kind of distortion of our beliefs, especially our memories,

585
00:54:46,800 --> 00:54:48,800
just remembering something over and over again changes it.

586
00:54:48,800 --> 00:54:55,200
Yeah. Okay, so let's get back to AI risk here because here's where I think you and I

587
00:54:55,200 --> 00:54:59,520
have very different intuitions. I mean, the intuition that many of us have,

588
00:55:00,320 --> 00:55:05,120
you know, that the people who have informed my views here, people like Stuart Russell,

589
00:55:05,120 --> 00:55:11,280
who you probably know at Berkeley, and Nick Bostrom, and Eleazar Yudkowski, and just lots of

590
00:55:11,280 --> 00:55:18,400
people in this spot worrying about the same thing to one another degree. The intuition is that

591
00:55:18,960 --> 00:55:27,920
you don't get a second chance to create a truly autonomous superintelligence. It seems that in

592
00:55:27,920 --> 00:55:33,360
principle, this is the kind of thing you have to get right on the first try. And having to get

593
00:55:33,360 --> 00:55:39,280
anything right on the first try just seems extraordinarily dangerous because we rarely,

594
00:55:39,280 --> 00:55:45,280
if ever, do that when doing something complicated. And another way of putting this is that it seems

595
00:55:45,280 --> 00:55:53,120
like in the space of all possible superintelligent minds, there are more ways to build one that

596
00:55:53,120 --> 00:56:00,720
isn't perfectly aligned with our long-term well-being than there are ways to build one

597
00:56:00,720 --> 00:56:07,360
that is perfectly aligned with our long-term well-being. And from my point of view, what

598
00:56:08,400 --> 00:56:13,280
your optimism and the optimism of many other people who take your side of this debate

599
00:56:14,000 --> 00:56:22,960
is based on is not really taking the prospect of intelligence seriously enough and the autonomy

600
00:56:22,960 --> 00:56:31,440
that is intrinsic to it. I mean, if we actually built a true general intelligence, what that means

601
00:56:31,440 --> 00:56:37,920
is that we would suddenly find ourselves in relationship to something that we actually

602
00:56:38,480 --> 00:56:45,760
can't perfectly understand. It's like it will be analogous to a strange person walking into the room,

603
00:56:45,760 --> 00:56:52,400
you know, you're in relationship. And if this person can think a thousand times or a million

604
00:56:52,400 --> 00:56:59,920
times faster than you can and has goals that are less than perfectly aligned with your own,

605
00:57:01,040 --> 00:57:06,560
that's going to be a problem eventually. We can't find ourselves in a state of perpetual

606
00:57:06,560 --> 00:57:12,240
negotiation with systems that are more competent and powerful and intelligent.

607
00:57:12,240 --> 00:57:17,280
I think there's two mistakes in your argument. The first one is you say

608
00:57:17,280 --> 00:57:22,800
my intuition and your intuition. I think most of the people who have this fear have an intuition

609
00:57:22,800 --> 00:57:33,280
about what happened. If you'd like to continue listening to this conversation,

610
00:57:33,280 --> 00:57:38,160
you'll need to subscribe at SamHarris.org. Once you do, you'll get access to all full-length

611
00:57:38,160 --> 00:57:43,280
episodes of the Making Sense podcast, along with other subscriber-only content, including bonus

612
00:57:43,280 --> 00:57:48,640
episodes and AMAs and the conversations I've been having on the Waking Up app. The Making Sense

613
00:57:48,640 --> 00:57:56,000
podcast is ad-free and relies entirely on listener support, and you can subscribe now at SamHarris.org.

