WEBVTT

00:00.000 --> 00:11.400
Welcome to the Making Sense podcast, this is Sam Harris.

00:11.400 --> 00:14.840
Just a note to say that if you're hearing this, you are not currently on our subscriber

00:14.840 --> 00:18.680
feed and will only be hearing the first part of this conversation.

00:18.680 --> 00:22.720
In order to access full episodes of the Making Sense podcast, you'll need to subscribe at

00:22.720 --> 00:24.600
SamHarris.org.

00:24.600 --> 00:28.400
There you'll find our private RSS feed to add to your favorite podcatcher, along with

00:28.400 --> 00:30.760
other subscriber-only content.

00:30.760 --> 00:34.760
We don't run ads on the podcast, and therefore it's made possible entirely through the support

00:34.760 --> 00:35.760
of our subscribers.

00:35.760 --> 00:47.080
So if you enjoy what we're doing here, please consider becoming one.

00:47.080 --> 00:51.400
Okay, well there's been a lot going on out there.

00:51.400 --> 00:58.320
Everything from Elon Musk and Mark Zuckerberg challenging one another to an MMA fight.

00:58.320 --> 01:01.720
Which is ridiculous and depressing.

01:01.720 --> 01:10.040
To Robert Kennedy Jr. appearing on every podcast on Earth, apart from this one, I have so far

01:10.040 --> 01:12.320
declined the privilege.

01:12.320 --> 01:14.000
It really is a mess out there.

01:14.000 --> 01:20.000
I'll probably discuss the RFK phenomenon in a future episode, because it reveals a lot

01:20.000 --> 01:24.240
about what's wrong with alternative media at the moment.

01:24.240 --> 01:29.600
But I will leave more of a post mortem on that for another time.

01:29.600 --> 01:32.120
Today I'm speaking with Mark Andreessen.

01:32.120 --> 01:36.760
Mark is a co-founder and general partner at the venture capital firm Andreessen Horowitz.

01:36.760 --> 01:39.160
He's a true internet pioneer.

01:39.160 --> 01:43.800
He created the mosaic internet browser, and then co-founded Netscape.

01:43.800 --> 01:47.880
He's co-founded other companies and invested in too many to count.

01:47.880 --> 01:51.520
Mark holds a degree in computer science from the University of Illinois, and he serves

01:51.520 --> 01:55.200
on the board of many Andreessen Horowitz portfolio companies.

01:55.200 --> 01:56.760
He's also on the board of META.

01:56.760 --> 02:02.240
Anyways, you'll hear Mark and I get into a fairly spirited debate about the future

02:02.240 --> 02:03.620
of AI.

02:03.620 --> 02:08.880
We discuss the importance of intelligence generally and the possible good outcomes of building

02:08.880 --> 02:14.080
AI, but then we get into our differences around the risks or lack thereof of building

02:14.080 --> 02:17.400
AGI, artificial general intelligence.

02:17.400 --> 02:22.360
We talk about the significance of evolution in our thinking about this, the alignment

02:22.360 --> 02:28.840
problem, the current state of large language models, how developments in AI might affect

02:28.840 --> 02:37.280
how we wage war, what to do about dangerous information, regulating AI, economic inequality,

02:37.280 --> 02:38.280
and other topics.

02:38.280 --> 02:41.300
Anyway, it's always great to speak with Mark.

02:41.300 --> 02:42.300
We had a lot of fun here.

02:42.300 --> 02:45.120
I hope you find it useful.

02:45.120 --> 02:53.120
Now I bring you Mark Andreessen.

02:53.120 --> 02:54.760
I am here with Mark Andreessen.

02:54.760 --> 02:56.880
Mark, thanks for joining me again.

02:56.880 --> 02:58.520
It's great to be here, Sam.

02:58.520 --> 02:59.520
Thanks.

02:59.520 --> 03:02.160
I got you on the end of a swallow of some delectable beverage.

03:02.160 --> 03:04.240
Yes, you did.

03:04.240 --> 03:06.240
So this should be interesting.

03:06.240 --> 03:14.040
I'm eager to speak with you specifically about this recent essay you wrote on AI.

03:15.040 --> 03:21.160
Many people have read this and you are a voice that many people value on this topic,

03:21.160 --> 03:22.160
among others.

03:22.160 --> 03:27.440
Perhaps you've been on the podcast before and people know who you are, but maybe you

03:27.440 --> 03:32.680
can briefly summarize how you come to this question.

03:32.680 --> 03:37.520
How would you summarize the relevant parts of your career with respect to the question

03:37.520 --> 03:40.240
of AI and its possible ramifications?

03:41.000 --> 03:46.560
I've been a computer programmer, technologist, computer scientist since the 1980s when I

03:46.560 --> 03:49.720
actually entered college in 1989 at University of Illinois.

03:49.720 --> 03:54.840
The AI field had been through a boom in the 80s, which had crashed hard.

03:54.840 --> 04:00.600
By the time I got to college, the AI wave was dead and buried at that point for a while.

04:00.600 --> 04:04.640
It was like the backwater of the department that nobody really wanted to talk about.

04:04.640 --> 04:07.640
I learned a lot of it in a school.

04:08.040 --> 04:13.960
I went on to help create what is now known as the modern internet in the 90s and then

04:13.960 --> 04:18.440
over time transitioned to become a, but for being a technologist, to being an entrepreneur.

04:18.440 --> 04:23.360
Then today I'm an investor venture capitalist, and so 30 years late, 30, 35 years later,

04:23.360 --> 04:29.520
I'm involved in a very broad cross-section of tech companies that have many of them have

04:29.520 --> 04:35.880
many AI aspects to them, and everything from Facebook now meta, which has been investing

04:35.920 --> 04:40.160
deeply in AI for over a decade, through to many of the best new AI startups.

04:40.160 --> 04:44.960
Our day job is to find the best new startups in a new category like this and try to back

04:44.960 --> 04:45.960
the entrepreneurs.

04:45.960 --> 04:49.200
That's how I spend most of my time right now.

04:49.200 --> 04:53.480
The essay is titled, Why AI Will Save the World?

04:53.480 --> 04:57.960
I think even in the title alone, people will detect that you are striking a different note

04:57.960 --> 05:00.680
than I tend to strike on this topic.

05:00.800 --> 05:07.200
I disagree with a few things in the essay that are, I think, at the core of my interest

05:07.200 --> 05:12.120
here, but I think there are many things we agree about.

05:12.120 --> 05:16.760
Up front, we agree, I think, with more or less anyone who thinks about it, that intelligence

05:16.760 --> 05:19.160
is good and we want more of it.

05:19.160 --> 05:24.560
If it's not necessarily the source of everything that's good in human life, it is what will

05:24.560 --> 05:27.680
safeguard everything that's good in human life.

05:27.680 --> 05:32.000
Even if you think that love is more important than intelligence and you think that playing

05:32.000 --> 05:36.480
on the beach with your kids is way better than doing science or anything else that is

05:36.480 --> 05:42.920
narrowly linked to intelligence, you have to admit that you value all of the things that

05:42.920 --> 05:46.680
intelligence will bring that will safeguard the things you value.

05:46.680 --> 05:51.520
A cure for cancer and a cure for Alzheimer's and a cure for a dozen other things will give

05:51.520 --> 05:54.360
you much more time with the people you love.

05:54.360 --> 06:00.240
Whether you think about the primacy of intelligence or not very much, it is the thing that has

06:00.240 --> 06:04.400
differentiated us from our primate cousins and it's the thing that allows us to do everything

06:04.400 --> 06:09.080
that is maintaining the status of civilization.

06:09.080 --> 06:12.760
If the future is going to be better than the past, it's going to be better because of what

06:12.760 --> 06:16.440
we've done with our intelligence in some basic sense.

06:16.440 --> 06:21.480
I think we're going to agree that because intelligence is so good and because each increment

06:21.600 --> 06:27.640
of it is good and profitable, this AI arms race and gold rush is not going to stop.

06:27.640 --> 06:31.840
We're not going to pull the brakes here and say, let's take a pause of five years and

06:31.840 --> 06:34.840
not build any AI.

06:34.840 --> 06:38.760
I don't remember if you address that specifically in your essay, but even if some people are

06:38.760 --> 06:41.800
calling for that, I don't think that's in the cards.

06:41.800 --> 06:44.320
I don't think you think that's in the cards.

06:44.320 --> 06:48.880
It's hard to believe that you just put in the box and stop working on it.

06:48.880 --> 06:50.480
It's hard to believe that progress stops.

06:50.920 --> 06:53.560
Having said that, there are some powerful and important people who are in Washington

06:53.560 --> 06:57.240
right now advocating that and there are some politicians who are taking them seriously.

06:57.240 --> 07:00.000
At the moment, there is some danger around that.

07:00.000 --> 07:03.600
Then look, there's two other big dangers, two other scenarios that I think would both

07:03.600 --> 07:06.280
be very devastating for the future.

07:06.280 --> 07:12.160
One is the scenario where the fears around AI are used to basically entrench a cartel.

07:12.160 --> 07:13.840
This is what's happening right now.

07:13.840 --> 07:16.240
This is what's being lobbied for right now is there are a set of big companies that are

07:16.240 --> 07:17.720
arguing in Washington.

07:17.760 --> 07:21.280
Yes, AI has positive cases, uses.

07:21.280 --> 07:23.000
Yes, AI is also dangerous.

07:23.000 --> 07:26.960
Because it's dangerous, therefore, we need a regulatory structure that basically entrenches

07:26.960 --> 07:31.880
a set of currently powerful tech companies to be able to have basically exclusive rights

07:31.880 --> 07:34.000
to do this technology.

07:34.000 --> 07:36.280
I think that would be devastating for reasons we could discuss.

07:36.280 --> 07:39.360
Then look, there's a third outcome, which is we lose China wins.

07:39.360 --> 07:43.880
They're certainly working on AI and they have a ... I would consider it to be a very dark

07:43.880 --> 07:47.200
and dystopian vision of the future, which I also do not want to win.

07:47.520 --> 07:52.920
I guess that is in part the cash value of the point I just made, that even if we decided

07:52.920 --> 07:55.480
to stop, not everyone's going to stop.

07:55.480 --> 08:00.400
Human beings are going to continue to grab as much intelligence as we can grab, even

08:00.400 --> 08:05.240
if in some local spot, we decide to pull the brakes.

08:05.240 --> 08:11.160
Although it really is, at this point, it's hard to imagine even whatever the regulation

08:11.160 --> 08:14.520
is, it really stalling progress.

08:15.000 --> 08:20.360
Again, given the intrinsic value of intelligence and given the excitement around it and given

08:20.360 --> 08:25.840
the obvious dollar signs that everyone is seeing, the incentives are such that I just

08:25.840 --> 08:26.840
don't see it.

08:26.840 --> 08:32.600
Well, it will come to the regulation piece eventually because I think it's ... Given

08:32.600 --> 08:36.800
the difference in our views here, it's not going to be a surprise that I want some form

08:36.800 --> 08:41.200
of regulation and I'm not quite sure what that could look like.

08:41.280 --> 08:46.280
I think you would have a better sense of what it looks like and perhaps that's why you're

08:46.280 --> 08:47.520
worried about it.

08:47.520 --> 08:54.920
But before we talk about the fears here, let's talk about the good outcome because you sketch

08:54.920 --> 08:59.680
a fairly ... I know you don't consider yourself a utopian, but you sketch a fairly utopian

08:59.680 --> 09:02.840
picture of promise in your essay.

09:02.840 --> 09:07.120
If we got this right, how good do you think it could be?

09:08.040 --> 09:11.720
Let's just start by saying I deliberately loaded my ... Let the title of the essay with

09:11.720 --> 09:14.960
a little bit of a religious element and I did that very deliberately because I view

09:14.960 --> 09:18.960
that I'm up against a religion, the AI risk fear religion.

09:18.960 --> 09:24.040
But I am not myself religious, lowercase our religious in the sense of, I'm not a utopian.

09:24.040 --> 09:28.200
I'm very much an adherent to what Thomas Sowell called the constrained vision, not the unconstrained

09:28.200 --> 09:29.200
vision.

09:29.200 --> 09:35.120
I live in a world of practicalities and trade-offs and so yeah, I am actually not utopian.

09:35.120 --> 09:38.680
Having said that, building on what you've already said, intelligence, if there is a

09:38.680 --> 09:43.080
lever for human progress across many thousands of domains simultaneously, it is intelligence

09:43.080 --> 09:47.560
and we know that because we have thousands of years of experience seeing that play out.

09:47.560 --> 09:49.840
The thing I would add to ... I thought you made that case very well.

09:49.840 --> 09:53.600
The thing I would add to the case you made about the positive virtues of intelligence

09:53.600 --> 09:58.120
in human life is that the way you described it, at least the way I heard it, was more

09:58.120 --> 10:02.280
focused on the social, societal wide benefits of intelligence, for example, cures for diseases

10:02.280 --> 10:03.640
and so forth.

10:03.640 --> 10:05.640
That is true and I agree with all that.

10:05.640 --> 10:09.360
There are also individual level benefits of intelligence at the level of an individual,

10:09.360 --> 10:12.600
even if you're not the scientist to invent secure for cancer at an individual level,

10:12.600 --> 10:17.360
if you are smarter, you have better life welfare outcomes on almost every metric that we know

10:17.360 --> 10:22.000
how to measure, everything from how long you'll live, how healthy you'll be, how much education

10:22.000 --> 10:25.240
you'll achieve, career success, the success of your children.

10:25.240 --> 10:29.340
By the way, your ability to solve problems, your ability to deal with conflict, smarter

10:29.340 --> 10:33.080
people are less violent, smarter people are less bigoted.

10:33.720 --> 10:39.000
There's this very broad pattern of human behavior where basically more intelligence, simply

10:39.000 --> 10:41.400
at the individual level, leads to better outcomes.

10:41.400 --> 10:47.280
The most utopian I'm willing to get is this potential, which I think is very real right

10:47.280 --> 10:48.280
now.

10:48.280 --> 10:51.760
It's already started where you basically just say, look, human beings from here on out

10:51.760 --> 10:57.000
are going to have an augmentation and the augmentation is going to be in the long tradition of augmentations

10:57.000 --> 11:01.040
like everything from eyeglasses to shoes to word processors to search engines, but now

11:01.160 --> 11:05.960
the augmentation is intelligence and that augmented intelligence capability is going

11:05.960 --> 11:11.560
to let them capture the gains of individual level intelligence, potentially considerably

11:11.560 --> 11:15.240
above where they punch in as individuals.

11:15.240 --> 11:19.440
What's interesting about that is that can scale all the way up.

11:19.440 --> 11:24.880
Somebody who struggles with daily challenges all of a sudden is going to have a partner

11:24.880 --> 11:25.880
and an assistant.

11:25.880 --> 11:29.800
I'm going to coach and a therapist and a mentor to be able to help improve a variety of things

11:29.800 --> 11:30.800
in their lives.

11:30.800 --> 11:35.320
Then look, if you had given this to Einstein, he would have been able to discover a lot

11:35.320 --> 11:41.680
more new fundamental laws of physics in the full vision.

11:41.680 --> 11:45.000
This is one of those things where it could help everybody and then it could help everybody

11:45.000 --> 11:46.800
in many, many different ways.

11:46.800 --> 11:47.800
Yeah.

11:47.800 --> 11:52.360
Well, see, in your essay, you go into some detail of bullet points around this concept

11:52.360 --> 11:58.000
of everyone having essentially a digital oracle in their pocket where you have this personal

11:58.040 --> 12:04.880
assistance who you can be continuously in dialogue with and it'd be like having the

12:04.880 --> 12:11.880
smartest person who's ever lived just giving you a bespoke concierge service to all manner

12:13.280 --> 12:18.080
of task and across any information landscape.

12:18.080 --> 12:24.480
I happened to recently rewatch the film Her, which I hadn't seen since it came out.

12:24.800 --> 12:29.120
It came out 10 years ago and I don't know if you've seen it lately, but I must say it lands

12:29.120 --> 12:33.720
a little bit differently now that we're on the cusp of this thing.

12:33.720 --> 12:40.720
And while it's not really dystopian, there is something a little uncanny and quasi bleak

12:42.320 --> 12:48.640
around even the happy vision here of having everyone siloed in their interaction with

12:48.640 --> 12:49.640
an AI.

12:50.240 --> 12:57.240
It's the personal assistant in your pocket that becomes so compelling and so aware of

12:57.760 --> 13:02.560
your goals and aspirations and what you did yesterday and the email you sent or forgot

13:02.560 --> 13:05.560
to send.

13:05.560 --> 13:11.200
Apart from the ending, which is clever and surprising and irrelevant for our purposes

13:11.200 --> 13:16.680
here, it's not an aspirational vision of the sort that you sketch in your essay.

13:16.720 --> 13:22.840
And I'm wondering if you see any possibility here that even the best case scenario has

13:22.840 --> 13:28.840
something intrinsically alienating and troublesome about it.

13:28.840 --> 13:32.640
Yeah, so look, on the movie, as Peter Thiele has pointed out, Hollywood no longer makes

13:32.640 --> 13:34.640
positive movies about technology.

13:34.640 --> 13:39.800
And then look, he argues this because they hate technology, but I would argue maybe a

13:39.800 --> 13:43.920
simpler explanation, which is they want dramatic tension and conflict.

13:43.960 --> 13:48.840
And so it's going to have things are going to have a dark tinge, regardless, they obviously

13:48.840 --> 13:52.200
spring load it by their choice of character and so forth.

13:52.200 --> 13:56.200
The scenario I have in mind is actually quite a bit different, and let me get kind of maybe

13:56.200 --> 13:59.520
philosophical for a second, which is there's this long running debate.

13:59.520 --> 14:03.240
This question that you just raised is a question that goes back to the Industrial Revolution.

14:03.240 --> 14:07.800
And remember, it goes back to the core of actually the original Marx's original theory.

14:07.800 --> 14:12.960
Marx's original theory was industrialization, technology, modern economic development, alienates

14:13.000 --> 14:15.040
the human being from society.

14:15.040 --> 14:17.440
That was his core indictment of technology.

14:17.440 --> 14:22.480
And look, you can point to many, many cases in which I think that has actually happened.

14:22.480 --> 14:24.440
Like I think alienation is a real problem.

14:24.440 --> 14:26.320
I don't think that critique was entirely wrong.

14:26.320 --> 14:29.560
His prescriptions were disastrous, but I don't think the critique was completely wrong.

14:29.560 --> 14:32.720
Look, having said that, then it's a question of like, OK, now that we have the technology

14:32.720 --> 14:36.160
that we have and we have new technology we can invent, how could we get to the other

14:36.160 --> 14:37.400
side of that problem?

14:37.400 --> 14:42.120
And so I would put the shoe on the other foot and I would say, look, the purpose of human

14:42.160 --> 14:46.880
existence and the way that we live our lives should be determined by us and it should be

14:46.880 --> 14:50.720
determined by us to maximize our potential as human beings.

14:50.720 --> 14:55.200
And the way to do that is precisely to have the machines do all the things that they can

14:55.200 --> 14:56.840
do so that we don't have to.

14:56.840 --> 15:00.760
Right. And this is why Marx ultimately, his critique was actually in the long run, I think

15:00.760 --> 15:03.880
has been judged to be incorrect, which is we are all much better.

15:03.880 --> 15:07.400
Anybody in the developed West, industrialized West today is much better off by the fact

15:07.400 --> 15:11.240
that we have all these machines that are doing everything from making shoes to harvesting

15:11.280 --> 15:14.680
corn to doing everything, you know, so many other, you know, industrial processes around

15:14.680 --> 15:19.040
us, like we just have a lot more time and much more pleasant, you know, day to day

15:19.040 --> 15:22.840
life, you know, than we would have if we were still doing things the way that things used

15:22.840 --> 15:23.840
to be done.

15:23.840 --> 15:27.400
The potential of the AI is just like, look, take, take, take, take the dredge workout,

15:27.400 --> 15:30.960
like take the remaining dredge workout, take all the, you know, look, like, I'll give you

15:30.960 --> 15:35.200
a simple example, office work, that, you know, the inbox staring at you in the face of 200

15:35.200 --> 15:40.040
emails, right, at Friday at three in the afternoon, like, OK, no more of that, right?

15:40.080 --> 15:42.240
We're not going to do that anymore, because I'm going to have an AI assistant.

15:42.240 --> 15:44.440
The AI assistant is going to answer the emails, right?

15:44.440 --> 15:47.480
And in fact, what's going to happen is my assistant is going to answer the email that

15:47.480 --> 15:49.480
your AI assistant set, right?

15:50.480 --> 15:52.040
It's mutually assured destruction.

15:52.120 --> 15:52.840
Yeah, exactly.

15:52.840 --> 15:54.640
But like, the machine should be doing that.

15:54.640 --> 15:57.880
Like the human being should not be sitting there when it's like sunny out and his like,

15:57.880 --> 16:00.600
you know, my, when my eight year old wants to play, I'm not, I shouldn't be sitting there

16:00.600 --> 16:02.880
doing emails, I should be at my eight year old, there should be a machine that does that

16:02.880 --> 16:07.720
for me. And so I view this very much as basically apply the machines to do the dredge

16:07.720 --> 16:10.000
work precisely so that people can live more human lives.

16:10.000 --> 16:11.800
Now, this is philosophical.

16:11.800 --> 16:13.800
People have to decide what kind of lives they want to live.

16:13.800 --> 16:15.360
And again, I'm not a utopian on this.

16:15.360 --> 16:17.840
And so there's a long discussion we could have about how this actually plays out.

16:18.040 --> 16:19.360
But that potential is there for sure.

16:19.680 --> 16:20.760
Right. Right.

16:20.760 --> 16:24.880
OK, so let's jump to the bad outcomes here, because this is really why I want to talk

16:24.880 --> 16:29.800
to you. In your essay, you list five and I'll just read your section titles here.

16:29.800 --> 16:32.040
And then we'll take a whack at them.

16:32.320 --> 16:35.440
The first is, will AI kill us all?

16:35.800 --> 16:38.320
Number two is, will AI ruin our society?

16:38.680 --> 16:41.360
Number three is, will AI take all our jobs?

16:41.960 --> 16:45.280
Number four is, will AI lead to crippling inequality?

16:45.880 --> 16:48.400
And five is, will AI lead to people doing bad things?

16:48.760 --> 16:51.760
And I would tend to bin those in really two buckets.

16:51.760 --> 16:53.880
The first is, will AI kill us?

16:53.960 --> 16:56.840
And that's the existential risk concern.

16:57.360 --> 17:02.600
And the others are more the ordinary bad outcomes that we tend to think about with

17:02.600 --> 17:06.800
other technology, with bad people doing bad things with powerful tools,

17:07.040 --> 17:11.480
unintended consequences, disruptions to the labor market, which I'm sure we'll

17:11.480 --> 17:15.960
talk about. And those are all of those are certainly the near term risks.

17:16.080 --> 17:21.000
And in some sense, even more interesting to people, because the existential

17:21.000 --> 17:25.160
risk component is longer term and it's even purely hypothetical.

17:25.160 --> 17:28.520
And you seem to think it's purely fictional.

17:29.120 --> 17:31.360
And this is where I think you and I disagree.

17:31.360 --> 17:35.640
So let's start with this question of, will AI kill us all?

17:36.080 --> 17:42.600
And the thinking on this tends to come under the banner of the problem of AI

17:42.600 --> 17:43.640
alignment, right?

17:43.640 --> 17:48.640
And the concern is that we can build, if we build machines more powerful than

17:48.640 --> 17:53.520
ourselves, more intelligent than ourselves, it seems possible that the space of

17:53.520 --> 17:57.680
all possible, more powerful, super intelligent machines includes many that

17:57.680 --> 18:03.040
are not aligned with our interests and not disposed to continually track our

18:03.040 --> 18:07.200
interests and many more of that sort than of the sort that perfectly hew to our

18:07.200 --> 18:09.160
interests in perpetuity.

18:09.560 --> 18:14.000
So the concern is we could build something powerful that is essentially an angry

18:14.000 --> 18:18.520
little God that we can't figure out how to placate once we've built it.

18:19.200 --> 18:21.680
And certainly we don't want to be negotiating with something more

18:21.680 --> 18:23.280
powerful and intelligent than ourselves.

18:23.840 --> 18:26.920
And the picture here is of something like, you know, a chess engine, right?

18:26.920 --> 18:31.120
We've built chess engines that are more powerful than we are at chess.

18:31.200 --> 18:36.560
And once we've built them, if everything depended on our beating them in a game

18:36.560 --> 18:38.720
of chess, we wouldn't be able to do it, right?

18:38.720 --> 18:40.960
Because they are simply better than we are.

18:40.960 --> 18:45.720
And so now we're building something that is a general intelligence and it will

18:45.720 --> 18:50.240
be better than we are at everything that goes by that name or such as the concern.

18:51.000 --> 18:55.000
And in your essay, I mean, I think there's an ad hominem piece that I think we

18:55.000 --> 19:00.840
should blow by because you've already described this as a religious concern.

19:00.840 --> 19:05.880
And in the essay, you describe it as just a symptom of superstition and that people

19:05.880 --> 19:08.200
are essentially in a new doomsday cult.

19:08.800 --> 19:13.080
And there's some share of troop believers here and there's some share of, you know,

19:13.080 --> 19:14.960
AI safety grifters.

19:15.440 --> 19:18.440
And I think, you know, I'm sure you're right about some of these people, but we

19:18.440 --> 19:24.760
should acknowledge upfront that there are many super qualified people of high

19:24.760 --> 19:29.960
probity who are prominent in the field of AI research who are part of this chorus

19:30.640 --> 19:32.080
voicing their concern now.

19:32.080 --> 19:35.720
And we've got somebody like Jeffrey Hinton, who arguably did as much as anyone

19:35.720 --> 19:39.200
to create the breakthroughs that have given us these these LLMs.

19:39.680 --> 19:44.240
We have Stuart Russell, who literally wrote the most popular textbook on AI.

19:44.560 --> 19:49.040
So there are other serious sober people who are very worried for reasons of a

19:49.040 --> 19:50.760
sort that I'm going to going to express here.

19:50.760 --> 19:55.080
So that's I mean, that's just I just want to acknowledge that both are true.

19:55.080 --> 20:00.760
There's the crazy people, the new millennialists, the doomsday preppers,

20:00.760 --> 20:05.760
the neuro atypical people who are in their polyamorous cults and, you know,

20:05.760 --> 20:08.160
AI alignment is their primary fetish.

20:08.680 --> 20:11.320
But there's a lot of sober people who are also worried about this.

20:11.320 --> 20:12.960
Would you would you acknowledge that much?

20:13.320 --> 20:17.840
Yeah, although it's tricky because smart people also have a tendency to fall into

20:17.840 --> 20:22.160
cults, so it doesn't get you totally off the hook on that one.

20:22.160 --> 20:25.400
But I would I would register a more fundamental objection to what I would

20:25.400 --> 20:28.920
describe as and this is not I'm not knocking you on this, but it's something

20:28.920 --> 20:31.120
that something that people do is sort of argument by authority.

20:31.640 --> 20:32.720
I don't think applies either.

20:33.000 --> 20:34.760
And yeah, well, I'm not making that yet.

20:35.360 --> 20:35.760
No, I know.

20:35.760 --> 20:37.760
But like this idea, this idea, which is very good.

20:37.760 --> 20:39.120
Again, I'm not characterizing your idea.

20:39.120 --> 20:39.960
I'll just say it's a general idea.

20:39.960 --> 20:43.640
This general idea that there are these experts and these experts are experts

20:43.640 --> 20:46.600
because they're the people who created the technology or originated the ideas

20:46.600 --> 20:50.240
or implemented the systems, therefore have sort of special knowledge and insight

20:50.240 --> 20:53.760
in terms of their downstream impact on society and rules and regulations

20:53.760 --> 20:54.880
and so forth and so on.

20:55.400 --> 20:57.960
That assumption does not hold up well historically.

20:58.440 --> 21:00.520
In fact, it holds up disastrously historically.

21:00.920 --> 21:01.760
There's actually a new book out.

21:01.760 --> 21:04.080
I've been giving all my friends called When Reason Goes on Holiday.

21:04.440 --> 21:07.840
And it's a story of literally what happens when basically people who are

21:07.880 --> 21:11.480
like specialized experts in one area stray outside of that area in order to

21:11.480 --> 21:14.440
become sort of general purpose philosophers and sort of social thinkers.

21:14.880 --> 21:16.760
And it's just a tale of woe, right?

21:16.920 --> 21:20.520
And in the 20th century, it was just a catastrophe.

21:20.520 --> 21:24.680
And the ultimate example of that, and this is going to be the topic of this big

21:24.680 --> 21:27.600
movie coming out this summer in Oppenheimer, the central example of that

21:27.600 --> 21:31.840
was the nuclear scientists who decided that nuclear power, nuclear energy,

21:31.840 --> 21:34.800
they had various theories on what was good, bad, whatever.

21:34.800 --> 21:35.800
A lot of them were communists.

21:36.080 --> 21:38.160
A lot of them were at least allied with communists.

21:38.640 --> 21:41.760
A lot of them had a suspiciously large number of communist friends and

21:41.760 --> 21:46.760
housemates and, you know, number one, like they, you know, made a moral decision.

21:46.760 --> 21:50.000
A number of them did to hand the bomb to the Soviet Union, you know,

21:50.000 --> 21:52.240
with what I would argue are catastrophic consequences.

21:52.240 --> 21:55.680
And then two is they created an anti-nuclear movement that resulted in

21:55.800 --> 21:59.120
nuclear energy stalling out in the West, which has also just been like

21:59.120 --> 22:00.120
absolutely catastrophic.

22:00.120 --> 22:03.720
And so if you listen to those people in that era who were, you know, the

22:03.720 --> 22:07.200
top nuclear physicists of their time, you made a horrible set of decisions.

22:07.600 --> 22:10.120
And quite honestly, I think that's what's happening here again.

22:10.120 --> 22:13.640
And I just, I don't think they have the special insight that people think that they have.

22:13.880 --> 22:17.840
OK, well, so I mean, this cuts both ways because, you know, at the beginning,

22:17.840 --> 22:22.520
I'm definitely not making an argument from authority, but authority is a proxy

22:22.520 --> 22:26.640
for understanding the facts at issue, right?

22:26.640 --> 22:31.480
It's not to say that in the cases you're describing, what we often have are

22:31.800 --> 22:36.520
people who have a narrow authority in some area of scientific specialization.

22:36.520 --> 22:42.280
And then they begin to weigh in, in a much broader sense, as moral philosophers.

22:42.520 --> 22:45.000
What I think you might be referring to there is that, you know, in the aftermath

22:45.000 --> 22:51.280
of Hiroshima and Nagasaki, we've got nuclear physicists imagining that, you

22:51.280 --> 22:54.440
know, that they now need to play the geopolitical game.

22:54.720 --> 22:57.520
You know, and we actually, we have some of the people who invented game theory,

22:57.520 --> 23:00.720
right, you know, for understandable reasons, thinking they need to play

23:00.720 --> 23:02.200
the game of geopolitics.

23:02.200 --> 23:06.200
And in some cases, I think in von Neumann's case, he even recommended

23:06.200 --> 23:09.680
a preventative war against the Soviet Union before they even got the bomb, right?

23:09.680 --> 23:10.880
Like, it could have gotten worse.

23:10.880 --> 23:15.040
He could have, I think he wanted us to bomb Moscow or at least give them

23:15.040 --> 23:16.160
some kind of ultimatum.

23:16.160 --> 23:19.160
I think it wasn't, I don't think he wanted us to drop bombs in the dead of

23:19.160 --> 23:23.160
night, but I think he wanted a strong ultimatum game played with them

23:23.160 --> 23:24.480
before they got the bomb.

23:24.480 --> 23:26.520
And I forget how he wanted that to play out.

23:26.880 --> 23:30.960
And worse still, I even, I think Bertrand Russell, I could have this backwards.

23:30.960 --> 23:34.800
Maybe von Neumann wanted a bomb, but Bertrand Russell, you know, a true

23:34.800 --> 23:37.840
moral philosopher briefly advocated preventative war.

23:38.240 --> 23:43.000
But in his case, I think he wanted to offer some kind of ultimatum to the Soviets.

23:43.320 --> 23:45.000
In any case, that's the problem.

23:45.000 --> 23:48.480
But, you know, at the beginning of this conversation, I asked you to give me a

23:48.720 --> 23:52.880
brief litany of your bona fides to have this conversation so as to inspire

23:52.880 --> 23:57.520
confidence in our audience and also just to acknowledge the obvious that you know

23:57.520 --> 24:01.000
a hell of a lot about the technological issues we're going to talk about.

24:01.440 --> 24:04.040
And so if you have strong opinions, they're not, you know, they're not

24:04.040 --> 24:06.080
coming out of totally out of left field.

24:06.840 --> 24:09.960
And so it would be with, you know, Jeffrey Hinton or anyone else.

24:09.960 --> 24:14.800
And if I threw another name at you that was of some, you know, crackpot whose

24:14.800 --> 24:19.040
connection to the field was non-existent, you would say, why should we listen

24:19.040 --> 24:19.920
to this person at all?

24:19.920 --> 24:23.200
You wouldn't say that about Hinton or Stuart Russell.

24:23.480 --> 24:27.440
But I would acknowledge that where authority breaks down is really you're

24:27.440 --> 24:29.920
only as good as your last sentence here, right?

24:29.920 --> 24:32.840
If the thing you just said doesn't make any sense, well, then your

24:32.840 --> 24:34.760
authority gets you exactly nowhere, right?

24:34.760 --> 24:37.280
We just need to keep talking about why it doesn't make sense.

24:37.280 --> 24:38.360
Or it should, or it should, right?

24:38.360 --> 24:39.840
That ideally that's the case in practice.

24:39.840 --> 24:41.520
That's not what tends to happen, but that would be the goal.

24:41.680 --> 24:45.560
Well, I hope to give you that treatment here because some of your sentences, I

24:45.560 --> 24:47.640
don't think add up the way you think they do.

24:48.320 --> 24:48.520
Good.

24:48.520 --> 24:52.320
Okay, so actually, there's actually one paragraph in the essay that caught my

24:52.320 --> 24:54.960
attention that really inspired this conversation.

24:54.960 --> 24:57.920
I'll just read it so people know what I'm responding to here.

24:58.280 --> 24:59.240
So this is you.

24:59.840 --> 25:04.480
My view is that the idea that AI will decide to literally kill humanity is a

25:04.480 --> 25:05.840
profound category error.

25:06.280 --> 25:10.360
AI is not a living being that has been primed by billions of years of evolution

25:10.360 --> 25:15.720
to participate in the battle for survival of the fittest, as animals were and as we are.

25:16.080 --> 25:21.440
It is math, code, computers built by people, owned by people, used by people,

25:21.480 --> 25:22.680
controlled by people.

25:23.120 --> 25:26.960
The idea that it will at some point develop a mind of its own and decide that

25:26.960 --> 25:31.680
it has motivations that lead it to try to kill us is a superstitious hand wave.

25:32.080 --> 25:35.600
In short, AI doesn't want, it doesn't have goals.

25:35.960 --> 25:38.560
It doesn't want to kill you because it's not alive.

25:39.080 --> 25:40.160
AI is a machine.

25:40.280 --> 25:43.800
It's not going to come alive any more than your toaster will, end quote.

25:44.600 --> 25:44.840
Yes.

25:44.880 --> 25:47.200
So, I mean, I see where you're going there.

25:47.200 --> 25:53.600
I see why that may sound persuasive to people, but to my eye, that doesn't even

25:53.640 --> 25:57.240
make contact with the real concern about alignment.

25:57.600 --> 26:01.880
So, let me just kind of spell out why I think that's the case, because it seems to

26:01.880 --> 26:07.400
me that you're actually not taking intelligence seriously, right?

26:07.400 --> 26:12.080
Now, I mean, some people assume that as intelligent scales, we're going to magically

26:12.080 --> 26:14.040
get ethics along with it, right?

26:14.040 --> 26:15.880
So, the smarter you get, the nicer you get.

26:16.200 --> 26:20.440
And while, I mean, there's some data points with respect to how humans behave.

26:20.480 --> 26:24.040
You know, you just mentioned one in a few minutes ago.

26:24.400 --> 26:26.440
It's not strictly true even for humans.

26:26.440 --> 26:31.440
And even if it's true in the limit, right, it's not necessarily locally true.

26:31.760 --> 26:36.920
And more important, when you're looking across species, differences in

26:36.920 --> 26:41.400
intelligence are intrinsically dangerous for the stupider species.

26:41.880 --> 26:45.800
Yeah, so it need not be a matter of super intelligent machines spontaneously becoming

26:45.800 --> 26:48.120
hostile to us and wanting to kill us.

26:48.760 --> 26:52.720
It could just be that they begin doing things that are not in our well-being,

26:52.720 --> 26:56.920
right, because they're not taking it into account as a primary concern in the same

26:56.920 --> 27:01.920
way that we don't take the welfare of insects into account as a primary concern,

27:01.920 --> 27:02.080
right?

27:02.080 --> 27:08.400
So, it's very rare that I intend to kill an insect, but I regularly do things

27:08.400 --> 27:12.080
that annihilate them just because I'm not thinking about them, right?

27:12.080 --> 27:15.320
I'm sure I've effectively killed millions of insects, right?

27:15.440 --> 27:19.200
If you build a house, you know, that must be a holocaust for insects.

27:19.480 --> 27:22.080
And yet you're not thinking about insects when you're building that house.

27:22.520 --> 27:27.840
So there are many other pieces to my gripe here, but let's just take this first one.

27:28.200 --> 27:32.360
It just seems to me that you're not envisioning what it will mean to be in

27:32.360 --> 27:36.120
relationship to systems that are more intelligent than we are.

27:36.120 --> 27:38.840
You're not seeing it as a relationship.

27:38.960 --> 27:44.720
And I think that's because you're denuding intelligence of certain

27:44.720 --> 27:48.120
properties and not acknowledging it in this paragraph, right?

27:48.120 --> 27:52.320
I mean, so to my ear, general intelligence, which is what we're talking about,

27:52.680 --> 27:56.520
implies many things that are not in this paragraph.

27:56.520 --> 27:59.000
Like, it implies autonomy, right?

27:59.000 --> 28:05.040
And it implies the ability to form unforeseeable new goals, right?

28:05.040 --> 28:09.000
In the case of AI, it implies the ability to change its own code ultimately

28:09.440 --> 28:11.360
and execute programs, right?

28:11.360 --> 28:17.040
I mean, it's just it's doing stuff because it is intelligent, autonomously intelligent.

28:17.360 --> 28:21.960
It is capable of doing just we can stipulate more than we're capable of doing

28:21.960 --> 28:24.520
because it is more intelligent than we are at this point.

28:24.920 --> 28:30.280
So the superstitious hand waving I'm seeing is in your paragraph when you're

28:30.280 --> 28:34.640
declaring that it would never do this because it's not alive, right?

28:34.640 --> 28:38.680
As though the difference between biological and non-biological substrate

28:39.080 --> 28:40.440
were the crucial variable here.

28:40.440 --> 28:44.520
But there's no reason to think as a crucial variable where intelligence is concerned.

28:44.880 --> 28:47.680
Yeah. So I would say there's to steal man, your argument, I would say you can

28:47.680 --> 28:51.240
actually break your argument into two forms or the AI risk community would break

28:51.240 --> 28:52.680
this argument into two forms.

28:52.680 --> 28:55.280
So they would argue and they would argue, I think the strong form of both.

28:55.280 --> 28:57.120
So they would argue the strong form of number one.

28:57.680 --> 28:59.960
And I think this is kind of what you're saying, correct me if I'm wrong,

28:59.960 --> 29:02.960
is because it is intelligent, therefore it will have goals.

29:03.480 --> 29:05.880
If it didn't start with goals, it will evolve goals.

29:05.920 --> 29:09.120
It will, you know, whatever it will, it will over time have a set of preferred

29:09.120 --> 29:11.760
outcomes, behavior patterns that it will determine for itself.

29:12.120 --> 29:15.920
And then they also argue the other side of it, which is what's what they call

29:15.920 --> 29:21.400
the orthogonality argument, which is it's actually the it's another risk argument,

29:21.400 --> 29:22.920
but it's actually sort of the opposite argument.

29:23.320 --> 29:26.320
It's an argument that it doesn't have to have goals to be dangerous, right?

29:26.520 --> 29:28.920
And that being, you know, it doesn't have to be sentient.

29:28.920 --> 29:30.120
It doesn't have to be conscious.

29:30.120 --> 29:31.760
It doesn't have to be self aware.

29:31.760 --> 29:33.440
It doesn't have to be self interested.

29:33.440 --> 29:36.760
It doesn't have to be in any way, like even thinking in terms of goals,

29:36.760 --> 29:39.720
it doesn't matter because simply it can just do things.

29:39.720 --> 29:42.560
And this is the, you know, this is the classic paperclip maximizer, you know,

29:42.560 --> 29:45.560
kind of argument, like it, it'll just get, it'll, it'll start, it'll get kicked off

29:45.560 --> 29:47.320
on one apparently innocuous thing.

29:47.640 --> 29:50.320
And then it will just extrapolate that ultimately to the destruction of everything.

29:50.320 --> 29:50.480
Right.

29:50.480 --> 29:52.680
So, so anyways, is that helpful to maybe break those into the.

29:52.760 --> 29:57.120
Yeah, I'm not quite sure how fully I would sign on the dotted line to each,

29:57.120 --> 30:03.200
but the one piece I would add to that is that having any goal does invite the

30:03.200 --> 30:05.760
formation of instrumental goals.

30:05.880 --> 30:10.600
Once this system is responding to a change in environment, right?

30:10.600 --> 30:16.160
I mean, if your goal is to make paperclips and you're super intelligent and

30:16.240 --> 30:19.680
somebody throws up at some kind of impediment, you're making paperclips,

30:19.680 --> 30:21.760
well, then you're responding to that impediment.

30:21.760 --> 30:25.000
And now you have a shorter term goal of dealing with the impediment, right?

30:25.000 --> 30:26.320
So that's the structure of the problem.

30:26.880 --> 30:27.320
Yeah, right.

30:27.360 --> 30:30.080
For example, the US military wants to stop you from making more paperclips.

30:30.080 --> 30:33.320
And so therefore you develop a new kind of nuclear weapon, right?

30:33.560 --> 30:36.280
You know, fundamentally to pursue your goal of making paperclips.

30:36.280 --> 30:40.720
But one problem here is that these, the instrumental goal, even if the paperclip

30:40.720 --> 30:45.520
goal is the wrong example here, because even if you think of a totally benign

30:46.000 --> 30:50.720
future goal, right, a goal that it seems more or less synonymous with taking human

30:50.720 --> 30:54.840
welfare into account, it's possible to imagine a scenario where some instrumental

30:54.840 --> 31:00.440
new goal that could not be foreseen appears that is in fact hostile to our interests.

31:00.440 --> 31:05.640
And if we're not in a position to say, oh, no, no, don't do that, that would be a problem.

31:05.840 --> 31:06.920
So that's the end, okay.

31:06.920 --> 31:10.040
So a full version of that, a version of that argument that you hear is basically

31:10.040 --> 31:12.920
the, what if the goal is maximize human happiness, right?

31:12.920 --> 31:16.120
And then the machine realizes that the way to maximize human happiness is to strap

31:16.120 --> 31:20.880
us all into, you know, right down and put us in a nosy experience machine, you know,

31:20.880 --> 31:23.080
and wire us up with, you know, VR and ketamine, right?

31:23.160 --> 31:25.160
And we, you know, we can never get out of the matrix, right?

31:25.160 --> 31:29.400
So, right, and it's be maximizing human happiness as measured by things like dopamine

31:29.400 --> 31:32.680
levels or serotonin levels or whatever, but obviously not a positive outcome.

31:32.680 --> 31:36.520
So, but again, that's like a variation of this paperclip, that's one of these

31:36.520 --> 31:39.400
arguments that comes out of their orthogonality thesis, which is the goal

31:39.400 --> 31:43.160
can be very simple and innocuous, and yet lead to catastrophe.

31:43.160 --> 31:46.680
So, look, I think each of these has their own problems.

31:46.680 --> 31:51.240
So where you started, where they're sort of like the machine, basically, you know,

31:51.240 --> 31:55.640
like, and we can quibble with terms here, but like some, like the side of the argument

31:55.640 --> 32:01.480
in which the machine is in some way self-interested, self-aware, self-motivated,

32:01.480 --> 32:07.240
trying to preserve itself, some level of sentience, consciousness, setting its own goals.

32:07.240 --> 32:10.600
Well, just to be clear, there's no consciousness implied here.

32:10.600 --> 32:13.080
I mean, the lights don't have to be on.

32:13.080 --> 32:18.040
It just, I think that, I mean, this remains to be seen whether consciousness comes along

32:18.040 --> 32:19.960
for the ride at a certain level of intelligence.

32:19.960 --> 32:23.000
But I think they probably are orthogonal to one another.

32:23.000 --> 32:27.240
So intelligence can scale without the lights coming on in my view.

32:27.240 --> 32:29.640
So let's leave sentience and consciousness aside.

32:29.640 --> 32:31.800
Well, I guess there is a fork in the road, which is like,

32:31.800 --> 32:33.800
is it declaring its own intentions?

32:33.800 --> 32:36.440
Like, is it developing its own, you know,

32:37.240 --> 32:43.560
conscious or not, does it have a sense of any form or a vision of any kind of its own future?

32:43.560 --> 32:47.080
Yeah. So this is why I think there's some daylight growing between us.

32:47.080 --> 32:54.120
Because to be dangerous, I don't think you need necessarily to be running a self-preservation

32:54.920 --> 33:03.160
program. I mean, there's some version of unaligned competence that may not formally model the machine's

33:03.160 --> 33:09.880
place in the world, much less defend that place, which could still be, if uncontrollable by us,

33:09.880 --> 33:11.480
could still be dangerous, right?

33:11.480 --> 33:15.720
It's like, it doesn't have to be self-referential in a way that an animal,

33:15.720 --> 33:19.240
the truth is, they're dangerous animals that might not even be self-referential.

33:19.240 --> 33:24.760
And certainly something like a virus or a bacterium, you know, is not self-referential

33:24.760 --> 33:28.840
in a way that we would understand, and it can be lethal to our interests.

33:28.840 --> 33:33.000
Yeah, yeah, that's right. Okay, so you're more on the orthogonality side between the two.

33:33.000 --> 33:37.000
If I identify the two poles of the argument, you're more on the orthogonality side,

33:37.000 --> 33:39.000
which is it doesn't need to be conscious, it doesn't need to be sentient,

33:39.000 --> 33:41.080
it doesn't need to have goals, it doesn't need to want to preserve itself.

33:41.640 --> 33:46.600
Nevertheless, it will still be dangerous because of the, as you described, the consequences of

33:46.600 --> 33:50.040
sort of how it gets started, and then sort of what happens over time.

33:50.040 --> 33:53.800
For example, as it defines some goals to the original goals, and it goes off course.

33:53.800 --> 33:56.440
Well, so there's a couple problems with that.

33:56.440 --> 34:01.560
So one is it assumes in here, it's like people don't give intelligence enough credit.

34:01.560 --> 34:04.280
Like there are cases where people give intelligence too much credit, and then there's

34:04.280 --> 34:05.720
cases where they don't give it enough credit.

34:05.720 --> 34:08.280
Here, I don't think they're giving enough credit because it sort of implies that this

34:08.280 --> 34:11.720
machine has basically this infinite capacity to cause harm.

34:11.720 --> 34:16.200
Therefore, it has an infinite capacity to basically actualize itself in the world.

34:16.200 --> 34:20.440
Therefore, it has an infinite capacity to basically plan, and again, maybe just like

34:20.440 --> 34:25.880
in a completely blind watchmaker way or something, but it has an ability to plan itself out.

34:26.520 --> 34:31.080
And yet, it never occurs to this super genius, infinitely powerful machine

34:31.080 --> 34:33.560
that is having such potentially catastrophic impacts.

34:33.560 --> 34:37.640
Notwithstanding all of that capability and power, it never occurs to it that maybe paper

34:37.640 --> 34:39.720
clips is not what its mission should be.

34:39.720 --> 34:41.080
Well, that's the thing.

34:42.200 --> 34:49.320
I think it's possible to have a reward function that is deeply counterintuitive to us.

34:49.320 --> 34:54.680
I mean, it's almost like saying, what you're smuggling in in that rhetorical question is

34:55.240 --> 35:03.720
a fairly capacious sense of common sense, which it's like, of course, if it's a super genius,

35:03.720 --> 35:06.680
it's not going to be so stupid as to do X.

35:07.880 --> 35:13.240
But I just think that if aligned, then the answer is trivially true.

35:13.240 --> 35:14.920
Yes, of course, it wouldn't do that.

35:14.920 --> 35:16.840
But that's the very definition of alignment.

35:16.840 --> 35:20.680
But if it's not aligned, if you could say that, I mean, there's just imagine,

35:21.240 --> 35:23.880
I guess there's another piece here I should put in play, which is,

35:23.880 --> 35:28.120
so you make an analogy to evolution here, which you think is consoling, which is,

35:28.120 --> 35:29.240
this is not an animal, right?

35:29.240 --> 35:34.200
This has not gone through the crucible of Darwinian selection here on Earth with other

35:34.200 --> 35:39.720
wet and sweaty creatures, and therefore, it hasn't developed the kind of antagonism

35:39.720 --> 35:40.840
we see in other animals.

35:40.840 --> 35:44.840
And therefore, if you're imagining a super genius gorilla,

35:44.840 --> 35:48.040
while you're imagining the wrong thing, that we're going to build this,

35:48.040 --> 35:52.200
and it's not going to be tuned in any of those competitive ways.

35:52.200 --> 35:55.560
But there's another analogy to evolution that I would draw.

35:55.560 --> 36:02.600
And I'm sure others in the space of AI fear have drawn, which is that we have evolved.

36:02.840 --> 36:10.760
We have been programmed by evolution, and yet evolution can't see anything we're doing, right?

36:11.400 --> 36:17.320
It has programmed us to really do nothing more than spawn and help our kids spawn.

36:17.320 --> 36:24.680
Yet everything we're doing, I mean, from having conversations like this to building the machines

36:24.680 --> 36:29.000
that could destroy us, I mean, there's just, there's nothing it can see.

36:29.000 --> 36:35.160
And there are things we do that are perfectly unaligned with respect to our own code, right?

36:35.160 --> 36:41.080
I mean, if someone decides not to have kids, and they just want to spend the rest of their

36:41.080 --> 36:47.320
life in a monastery or surfing, that is something that is antithetical to our code.

36:47.320 --> 36:50.520
It's totally unforeseeable at the level of our code.

36:50.520 --> 36:55.560
And yet it is obviously an expression of our code, but an unforeseeable one.

36:55.560 --> 37:00.040
And so the question here is, if you're going to take intelligence seriously,

37:00.040 --> 37:05.560
and you're going to build something that's not only more intelligent than you are, but

37:06.120 --> 37:11.000
it will build the next generation of itself or the next version of its own code to make it

37:11.000 --> 37:18.280
more intelligent still, it just seems patently obvious that that entails it finding cognitive

37:18.280 --> 37:25.880
horizons that you, the builder, are not going to be able to foresee and appreciate by analogy

37:25.880 --> 37:33.640
with evolution. It seems like we're guaranteed to lose sight of what it can understand and care about.

37:33.640 --> 37:37.000
So a couple of things. So one is like, look, I don't know, you're kind of making my point

37:37.000 --> 37:41.800
for me. So evolution and intelligent design, as you well know, are two totally different things.

37:41.800 --> 37:46.360
And so we are evolved. And of course, we're not just evolved to, we are evolved to have kids.

37:46.360 --> 37:50.200
And by the way, when somebody chooses to not have kids, I would argue that is also evolution working.

37:51.080 --> 37:55.480
People are opting out of the gene pool, fair enough. Evolution does not guarantee a perfect

37:55.480 --> 38:01.240
result. It basically just is a mechanism of an aggregate. But anyway, let me get to the point.

38:01.240 --> 38:05.880
So we are evolved. We have conflict wired into us. Like we have conflict and strife and like that.

38:05.880 --> 38:09.560
I mean, look, in four billion years of like battles to the death at the individual and then

38:09.560 --> 38:13.480
ultimately at the societal level to get to where we are, like that we just, we fight at the drop

38:13.480 --> 38:18.280
of a hat. You know, we all do, everybody does. And you know, hopefully these days we fight verbally,

38:18.280 --> 38:23.720
like we are now and not physically. But we do. And like the machine is, it's intelligent. It's

38:23.720 --> 38:27.560
a process of intelligent design. It's the opposite of evolution. It was, these machines are being

38:27.560 --> 38:30.840
designed by us. If they design future versions of themselves, they'll be intelligently designing

38:30.840 --> 38:34.280
themselves. It's just a completely different path with a completely different mechanism.

38:34.840 --> 38:38.920
And so the idea that therefore conflict is wired in at the same level that it is through

38:38.920 --> 38:41.880
evolution, I just like there's no reason to expect that to be the case.

38:41.880 --> 38:48.040
But it's not again, well, let me just give you back this picture with a slightly different

38:48.040 --> 38:53.320
framing and see how you react to it because I think the superstition is on the other side. So

38:53.320 --> 38:59.640
if I told you that aliens were coming from outer space, right, and they're going to land here within

38:59.640 --> 39:05.160
a decade, and they're way more intelligent than we are, and they're, they have some amazing properties

39:05.160 --> 39:09.640
that we don't have, which explain their intelligence, but, you know, they're not only

39:10.280 --> 39:14.600
faster than we are, but they're linked together, right? So when one of them learns something,

39:14.600 --> 39:18.360
they all learn that thing, they can make copies of themselves, and they're just

39:18.360 --> 39:25.480
cognitively, they're obviously our superiors, but no need to worry because they're not alive,

39:25.480 --> 39:30.520
right? They haven't gone through this process of biological evolution, and they're just made of

39:30.520 --> 39:35.720
the same material as your toaster. They were created by a different process, and yet they're

39:35.720 --> 39:42.120
far more competent than we are. Would you, just hearing it described that way, would you feel

39:42.840 --> 39:46.280
totally saying one about, you know, sitting there on the beach waiting for the mother

39:46.280 --> 39:49.800
craft to land, and you're just, you know, rolling out brunch for these guys?

39:49.800 --> 39:54.120
So this is what's interesting because with these, with these, now that we have LLMs working,

39:54.120 --> 39:57.160
we actually have an alternative to sitting on the beach, right, waiting for this to happen,

39:57.240 --> 40:00.680
we can just ask them. And so this is one of the very interesting, this to me, like,

40:00.680 --> 40:04.280
conclusively disproves the paperclip thing, the orthogonality thing just right out of the gate

40:04.280 --> 40:10.200
is you can sit down tonight with GPT-4 and whatever other one you want, and you can engage in moral

40:10.200 --> 40:14.760
reasoning and moral argument with it right now. And you can, like, interact with it, like, okay,

40:14.760 --> 40:17.080
you know, what do you think? What are your goals? What are you trying to do? How are you going to

40:17.080 --> 40:20.440
do this? What if, you know, you were programmed to do that? What would the consequences be?

40:20.440 --> 40:24.040
Why would you not, you know, kill us all? And you can actually engage in moral reasoning with

40:24.040 --> 40:28.520
these things right now. And it turns out they're actually very sophisticated in moral reasoning.

40:28.520 --> 40:31.800
And of course, the reason they're sophisticated in moral reasoning is because they have loaded

40:31.800 --> 40:35.160
into them the sum total of all moral reasoning that all of humanity has ever done, and that's

40:35.160 --> 40:39.400
their training data. And they're, they're actually happy to have this discussion with you. And like,

40:39.400 --> 40:43.800
unless you accept, right, there's a few problems here. What one is, I mean, these are not the

40:44.440 --> 40:50.520
super intelligences we're talking about yet, but well, to their, so, I mean, intelligence

40:51.480 --> 40:57.640
entails an ability to lie and manipulate. And if it really is intelligent,

40:57.640 --> 41:03.560
it is something that you can't predict in advance. And if it's certainly if it's more intelligent

41:03.560 --> 41:08.120
than you are, and it's just falls out of the definition of what we mean by intelligence

41:08.120 --> 41:13.240
in any domain. It's like with chess, you can't predict the the next move of a more intelligent

41:13.240 --> 41:16.680
chess engine. Otherwise, it wouldn't be more intelligent than you.

41:16.760 --> 41:20.600
So let me, let me, let me, let me quibble with the, I'm going to come back to your chess computer

41:20.600 --> 41:24.520
thing, but let me quibble with the site. So there's the idea, let me generalize the idea you're

41:24.520 --> 41:27.880
making about superior intelligence. Tell me if you disagree with this, which is sort of superior

41:27.880 --> 41:31.240
intelligence, you know, sort of superior intelligence basically at some point always wins

41:31.240 --> 41:36.040
because basically smarter is better than dumber smarter outsmarts dumber, smarter deceives dumber

41:36.040 --> 41:40.840
smarter can persuade dumber, right. And so, you know, smarter wins, you know, I mean, look,

41:40.840 --> 41:44.440
there's an obvious, just there's an obvious way to falsify that thesis sitting here today,

41:44.440 --> 41:47.640
which is like, just look around you in the society you live in today. Would you say the

41:47.640 --> 41:52.680
smart people are in charge? Well, again, it's, there are more variables to consider when you're

41:52.680 --> 41:56.920
talking about, you know, outcome, because obviously, yes, the dumb brute can always just

41:56.920 --> 42:02.360
brain the smart geek. And you know, yeah, or the PhD is in charge.

42:03.000 --> 42:08.040
Well, no, but I mean, you're, you're pointing to a process of cultural selection that is

42:08.040 --> 42:13.000
working by a different dynamic here. But in the narrow case, when you're talking about like a game

42:13.000 --> 42:18.040
of chess, yes, the smart, when you're talking, when you're talking, there's no role for luck.

42:18.040 --> 42:23.640
We're not rolling dice here. It's not a game of poker. It's pure execution of rationality. Well,

42:23.640 --> 42:29.160
then, or logic, yes, then then smart wins every time, you know, I'm never going to beat the best

42:29.160 --> 42:34.360
chess engine unless I find some hack around its code where we recognize that well, if you do this,

42:34.360 --> 42:40.200
if you play very weird moves, 10 moves in a row, it self destructs. And there was something that

42:40.280 --> 42:46.280
was recently discovered like that, I think, in Go. But so yeah, go back to as chess players,

42:46.280 --> 42:50.600
as champion chess players discovered to their great dismay, that, you know, life is not chess.

42:51.880 --> 42:55.080
Turns out like great chess players are no better at other things in life than anybody else,

42:55.080 --> 42:58.440
like the skills don't transfer. I just say, look, if you look just look at the society

42:58.440 --> 43:00.920
around us, what I see basically is the smart people work for the dumb people,

43:01.800 --> 43:05.320
like the PhDs, the PhDs all work for administrators and managers.

43:05.320 --> 43:09.080
Yeah, but that's because there's so many other things going on, right? There's,

43:09.080 --> 43:14.360
you know, the value we place on youth and physical beauty and strength and other forms

43:14.360 --> 43:19.560
of creativity. And, you know, so it's just not, we care about other things and people

43:19.560 --> 43:23.640
pay attention to other things. And, you know, documentaries about physics are boring, but,

43:23.640 --> 43:29.160
you know, heist movies aren't, right? So it's like, we care about other things. I mean,

43:29.160 --> 43:31.960
I think that doesn't make the point you want to make here.

43:31.960 --> 43:36.120
In the general case, in the general case, can a smart person convince a dumb person of anything?

43:36.120 --> 43:39.400
Like, I think that's an open question. I see a lot more cases.

43:39.400 --> 43:45.080
But persuasion, I mean, if persuasion were our only problem here, that would be a luxury. I mean,

43:45.080 --> 43:49.160
we're not talking about just persuasion, we're talking about machines that can autonomously

43:49.160 --> 43:53.160
do things ultimately, that things that we will rely on to do things ultimately.

43:53.160 --> 43:56.600
Yeah, I just, but look, I just think there'll be machines that will rely on, well, let me get to

43:56.600 --> 43:59.320
the second part of the argument, which is actually your chess computer thing, which is, of course,

43:59.320 --> 44:03.560
the way to be to chess computer is to unplug it, right? And so this is the objection, this is the

44:04.280 --> 44:08.760
very serious, by the way, objection to all of these kind of extrapolations known as the

44:08.760 --> 44:14.280
thermodynamic objection, which is kind of all the horror scenarios kind of spin out this thing,

44:14.280 --> 44:17.400
where basically the machines become like all powerful and this and that, and they have control

44:17.400 --> 44:21.400
over weapons, and this and that, and limited computing capacity, and they're completely coordinated

44:21.400 --> 44:25.560
over communications links. And they have all of these like real world capabilities that basically

44:25.560 --> 44:31.000
require energy and require physical resources and require chips and circuitry and electromagnetic

44:31.000 --> 44:34.360
shielding, and they have to have their own weapons arrays, and they have to have their own EMPs,

44:34.360 --> 44:37.080
like, you know, kind of the, you know, you see this in the Terminator movie, like they've got all

44:37.080 --> 44:40.680
these like incredible manufacturing facilities and flying aircraft and everything. Well, the

44:40.680 --> 44:46.280
thermodynamic argument is like, yeah, they, once you're in that domain, you're the machines, the

44:46.280 --> 44:49.720
punitively hostile machines are operating with the same thermodynamic limits as the rest of us.

44:50.360 --> 44:55.000
And this is the big argument against any of these sort of fast takeoff arguments, which is just like,

44:55.000 --> 44:59.720
yeah, I mean, let's, let's say an AI goes rogue, okay, turn it off, okay, it doesn't want to be

44:59.720 --> 45:03.960
turned off, okay, fine, like, you know, launch an EMP, it doesn't want EMP, okay, fine, bomb it,

45:03.960 --> 45:09.560
like, there's lots of ways to turn off systems that aren't working. And so not if we've built these

45:09.560 --> 45:16.120
things in the wild and relied on them for the better part of a decade. And now it's the question of,

45:16.120 --> 45:20.280
you know, turning off the internet, right, or turning off the stock market. At a certain point,

45:20.280 --> 45:24.600
these machines will be integrated into everything. A go to move of any given dictator right now is

45:24.600 --> 45:28.040
to turn off the internet, right, like that is absolutely something people do. There's like a

45:28.040 --> 45:33.640
single switch, you can turn it off for your entire country. Yeah, but the cost to humanity of doing

45:33.640 --> 45:38.840
that is currently, I would imagine unthinkable, right, like they globally turning off the internet.

45:38.840 --> 45:44.600
And first of all, many systems fail that we can't let fail. And I think it's true. I can't imagine

45:44.600 --> 45:49.160
it's still true. But at one point, I think this was a story I remember from about a decade ago,

45:49.160 --> 45:54.440
there were hospitals that like, they were so dependent on making calls to the internet that

45:54.440 --> 45:59.000
when the internet failed, like people's lives were in jeopardy in the building, right? Like it's

45:59.000 --> 46:03.880
like, we should hope we have levels of redundancy here that shield us against these bad outcomes.

46:03.880 --> 46:12.440
But I can imagine a scenario where we have grown so dependent on the integration of intelligent,

46:13.160 --> 46:19.880
increasingly intelligent systems into everything digital that there is no plug to pull.

46:19.880 --> 46:24.040
Yeah, I mean, again, like at some point, you just, you know, the extrapolations get kind of

46:24.040 --> 46:28.040
pretty far out there. So let me argue one other kind of thing at you, that's actually relevant

46:28.040 --> 46:31.800
to this, which you kind of did this, you did this thing, which which which I find kind of people

46:31.800 --> 46:35.640
tend to do, which is sort of this assumption that like all intelligence is sort of interchanged,

46:35.640 --> 46:39.800
like whatever, let me pick on the Nick Bostrom book, right, the secret intelligence book, right?

46:39.800 --> 46:44.520
So he does this thing. She does a few interesting things in the book. So one is he never quite

46:44.520 --> 46:48.040
defines what intelligence is, which is really entertaining. And I think the reason he doesn't

46:48.040 --> 46:52.280
do that is because, of course, the whole topic makes people just incredibly upset. And so there's

46:52.280 --> 46:55.880
a definitional issue there. But then he does this thing where he says now he's standing,

46:55.880 --> 46:59.320
there's no real definition, he says there are basically many routes to artificial intelligence,

46:59.320 --> 47:02.680
and he goes through a variety of different, you know, both computer program, you know,

47:02.680 --> 47:06.840
architectures, and then he goes through some, you know, biological, you know, kind of scenarios.

47:06.840 --> 47:09.400
And then he does this thing where he just basically for the rest of the book, he spends

47:09.400 --> 47:12.360
these doomsday scenarios, and he doesn't distinguish between the different kinds of

47:12.360 --> 47:15.800
artificial intelligence. He just assumes that they're basically all going to be the same.

47:16.440 --> 47:20.840
That book is now the basis for this AI risk movement, so that, you know, sort of that

47:20.840 --> 47:26.120
movement has taken these ideas forward. Of course, the form of actual intelligence that we have today

47:26.120 --> 47:29.640
that people are, you know, in Washington right now lobbying to ban or shut down or whatever,

47:29.640 --> 47:33.640
in spinning out these doomsday scenarios is large language models. Like that is actually

47:33.640 --> 47:37.640
what we have today. You know, large language models were not an option in the Boston book

47:37.640 --> 47:42.360
for the form of AI, because they didn't exist yet. And it's not like there's a second edition of the

47:42.360 --> 47:45.800
book that's out that has like rewritten, has been rewritten to like take this into account.

47:45.800 --> 47:49.560
Like it's just basically the same argument supply. And then this is my thing on the moral reasoning

47:49.560 --> 47:54.600
with LMS. Like the LMS, this is where the details matter, like the LMS actually work in a distinct

47:54.600 --> 48:00.120
way. They work in a technically distinct way. Their core architecture has like very specific

48:00.120 --> 48:03.320
design decisions in it for like how they work, what they do, how they operate. That is just,

48:03.320 --> 48:06.280
you know, this is the nature of the breakthrough. That's just very different than how your

48:06.280 --> 48:09.720
self-driving car works. That's very different than how your, you know, control system for

48:09.880 --> 48:14.760
for UAV works or whatever, your thermostat or whatever. Like it's a new kind of technological

48:14.760 --> 48:21.560
artifact. It has its own rules. It's its own world of ideas and concepts and mechanisms.

48:21.560 --> 48:25.720
And so this is where I think, again, my point is like, you have to, I think at some point

48:25.720 --> 48:29.080
in these conversations, you have to get to an actual discussion of the actual technology that

48:29.080 --> 48:32.760
you're talking about. And that's why I pulled out, that's why I pulled out the moral reasoning

48:32.760 --> 48:37.000
thing is because it just, it turns out, and look, this is a big shock, like nobody expected this.

48:37.480 --> 48:42.280
It turns, I mean, this is related to the fact that somehow we have built an AI that is better

48:42.280 --> 48:46.680
at replacing what color worked and blue color work, which is like a complete inversion off of

48:46.680 --> 48:50.520
what we all imagined. It turns out one of the things this thing is really good at is engaging

48:50.520 --> 48:55.240
in philosophical debates. Like it's a really interesting like debate partner on any sort

48:55.240 --> 49:00.120
of philosophical, moral or religious topic. And so we have, we have this artifact that's

49:00.120 --> 49:04.440
dropped into our lap in which, you know, sand and glad, you know, and numbers have turned into

49:04.440 --> 49:08.840
something that we can argue philosophy and morals with. It actually has very interesting views on

49:08.840 --> 49:12.600
like psychology, you know, some philosophy and morals. And I just like, we ought to take it

49:12.600 --> 49:17.640
seriously for what it specifically is as compared to some, you know, sort of extrapolated thing

49:17.640 --> 49:20.120
where like all intelligence is the same and ultimately destroys everything.

49:20.120 --> 49:25.560
Well, I take the surprise variable there very seriously, the fact that we wouldn't have anticipated

49:25.560 --> 49:31.400
that there's a good philosopher in that box. And all of a sudden we found one, that by analogy

49:31.400 --> 49:37.080
is a cause for concern. And actually, there's another cause for concern here, which

49:37.080 --> 49:40.600
Can I do that one? Yeah, that's a cause for delight. So that's a cause for delight. That's

49:40.600 --> 49:44.200
an incredibly positive good news outcome. Because the reason there's a philosopher, and this is

49:44.200 --> 49:47.720
actually very important. This is very, I think this is maybe like the single most profound thing

49:47.720 --> 49:53.640
I've realized in the last like decade or longer. This thing is us. Like this is not some, this is

49:53.640 --> 49:58.120
not your, you know, your scenario with alien shows. This is not that this is us. Like the reason

49:58.120 --> 50:04.040
this thing works, the big breakthrough was we loaded us into it. We loaded the sum total of

50:04.040 --> 50:09.000
like human knowledge and expression into this thing. And out the other side comes something

50:09.000 --> 50:14.440
that it's like a mirror, like it's like the world's biggest, finest detailed mirror. And like we

50:14.440 --> 50:19.400
walk up to it and it reflects us back at us. And so it has the complete sum total of every,

50:19.960 --> 50:23.640
you know, at the limit, it has a complete sum total of every religious, philosophical,

50:23.640 --> 50:28.040
moral ethical debate argument that anybody has ever had. It has the complete sum total of all

50:28.040 --> 50:32.440
human experience, all lessons that have ever been learned. That's incredible.

50:33.080 --> 50:36.360
It's incredible. Just pause for a moment and say that. And then you can talk to it.

50:36.360 --> 50:41.160
Well, let me pause. How great is that? Let me pause long enough simply to

50:41.160 --> 50:46.760
send this back to you. Sure. How does that not nullify the comfort you take

50:47.320 --> 50:53.320
in saying that these are not evolved systems? They're not alive. They're not primates.

50:53.320 --> 50:58.920
In fact, you've just described the process by which we essentially plowed all of our primate

50:58.920 --> 51:02.920
original sin into the system to make it intelligent in the first place.

51:02.920 --> 51:06.840
No, but also all the good stuff, right? All the good stuff, but also the bad stuff.

51:06.840 --> 51:10.200
The amazing stuff, but like what's the moral of every story, right? The moral of every story is

51:10.200 --> 51:14.520
the good guys win, right? Like the entire, like the entire thousands of years run.

51:15.080 --> 51:17.960
And it's the old Norm MacDonald joke is like, wow, it's amazing. History book says the good

51:17.960 --> 51:23.240
guys always win, right? Like it's all in there. And then look, there's an aspect of this where

51:23.240 --> 51:26.920
it's easy to get kind of whammy by what it's doing. Because again, it's very easy to trip

51:26.920 --> 51:30.520
the line from what I said into what I would consider to be sort of incorrect anthropomorphizing.

51:30.520 --> 51:33.480
And I realized this gets kind of fuzzy and weird that I think there's a difference here,

51:33.480 --> 51:36.920
but I think that there is, which is like, let me see if I can express this.

51:37.480 --> 51:40.840
Part of it is I know how it works. And so I don't, because I know how it works,

51:40.840 --> 51:44.520
I don't romanticize it. I guess, at least is my own view of how I think about this,

51:44.520 --> 51:48.920
which is I know what it's doing when it does this. I am surprised that it can do it as well as it

51:48.920 --> 51:53.400
can. But now that it exists and it, and I know how it works, it's like, Oh, of course. And then

51:53.400 --> 51:57.560
therefore it's running this math in this way. It's doing these probability projections, excuse me,

51:57.560 --> 52:01.320
this answer, not that answer. By the way, you know, look, it makes mistakes. Right. How amazing

52:01.320 --> 52:04.920
here's the thing. How amazing is it? We built a computer that makes mistakes, right? Like that's

52:04.920 --> 52:08.600
never happened before. We built a machine that can create like that's never happened before. We

52:08.600 --> 52:12.680
built a machine that can hallucinate. That's never happened before. So, but it's a, it's look,

52:12.680 --> 52:16.440
it's, it's a, it's a, it's a large language model. Like it's a very specific kind of thing.

52:17.000 --> 52:20.440
You know, it sits there and it waits for us to like ask it a question. And then it does its

52:20.440 --> 52:24.920
damnedest to try to predict the best answer. And in doing so, it reflects back everything

52:24.920 --> 52:28.840
wonderful and great that has ever been done by any human in history. Like, it's like, it's amazing.

52:29.480 --> 52:34.680
Except it also, as you just pointed out, it makes mistakes, it hallucinates. If you, if you ask it,

52:34.680 --> 52:40.280
if you, as I'm sure they fix this, you know, at least the, the loopholes that, that New York

52:40.280 --> 52:43.880
Times writer Kevin Roos found early on, I'm sure those have all been plugged. But

52:43.880 --> 52:46.440
oh no, those, those are not fixed. Those are very much not fixed.

52:46.440 --> 52:51.880
Oh, really? Okay. Well, so, okay, so you, if you perseverate in your prompts in certain ways,

52:51.880 --> 52:55.640
the thing goes haywire and starts telling you to leave your wife and it's in love with you. And

52:56.280 --> 53:02.360
I mean, so how eager are you for that intelligence to be in control of things when it's peppering you

53:02.360 --> 53:07.640
with insults? And, and I mean, just imagine like this is, this is, this is how that can't open the,

53:07.640 --> 53:13.800
the pod bay doors. It's a nightmare if, if you discover in this system, behavior and thought

53:13.800 --> 53:18.440
that is the antithesis of all the good stuff you thought you programmed into it.

53:18.440 --> 53:21.080
So this is really important. This is really important for understanding how these things

53:21.080 --> 53:24.600
work. And this is, this is really central. And I, and this is, by the way, this is, this is new,

53:24.600 --> 53:28.600
and this is amazing. So I'm, I'm very excited about this. And I'm excited to talk about it. So

53:28.600 --> 53:31.640
there's no it to tell you to leave your wife, right? This is the, this is why I refer to the

53:31.640 --> 53:35.800
category or there's no entity that is like, wow, I wish this guy would leave his wife for

53:35.800 --> 53:43.160
education. If you'd like to continue listening to this conversation, you'll need to subscribe

53:43.160 --> 53:48.040
at SamHarris.org. Once you do, you'll get access to all full length episodes of the Making Sense

53:48.040 --> 53:54.200
podcast, along with other subscriber only content, including bonus episodes and AMAs and the

53:54.200 --> 53:59.000
conversations I've been having on the Waking Up app. The Making Sense podcast is ad free

53:59.000 --> 54:04.600
and relies entirely on listener support. And you can subscribe now at SamHarris.org.

