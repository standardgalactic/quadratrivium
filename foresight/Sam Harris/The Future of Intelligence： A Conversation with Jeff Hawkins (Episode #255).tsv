start	end	text
0	12160	Welcome to the Making Sense podcast. This is Sam Harris. Just a note to say that if
12160	16360	you're hearing this, you are not currently on our subscriber feed and will only be hearing
16360	20800	the first part of this conversation. In order to access full episodes of the Making Sense
20800	26400	podcast, you'll need to subscribe at SamHarris.org. There you'll find our private RSS feed to
26400	31040	add to your favorite podcatcher, along with other subscriber-only content. We don't run
31040	34960	ads on the podcast, and therefore it's made possible entirely through the support of our
34960	38800	subscribers. So if you enjoy what we're doing here, please consider becoming one.
46960	52080	Today I'm speaking with Jeff Hawkins. Jeff is the co-founder of Numenta,
52720	57680	a neuroscience research company, and also the founder of the Redwood Neuroscience Institute.
58480	64640	And before that, he was one of the founders of the field of handheld computing, starting Palm
65360	70000	and Handspring. He's also a member of the National Academy of Engineering,
70800	78000	and he's the author of two books. The first is On Intelligence, and the second and most recent is
78000	84560	A Thousand Brains, A New Theory of Intelligence. And Jeff and I talk about intelligence
85200	91600	from a few different sides here. We start with the brain. We talk about how the cortex creates
91600	99440	models of the world, the role of prediction in experience. We discuss the idea that thought
99440	106160	is analogous to movement in conceptual space. But for the bulk of the conversation,
106160	112160	we have a debate about the future of artificial intelligence, and in particular the alignment
112160	120240	problem and the prospect that AI could pose some kind of existential risk to us. As you'll hear,
121040	128240	Jeff and I have very different takes on that problem. Our intuitions divide fairly sharply,
128960	135920	and as a consequence we have a very spirited exchange. Anyway, it was a lot of fun. I hope
136000	139440	you enjoy it, and now I bring you Jeff Hawkins.
146640	149360	I'm here with Jeff Hawkins. Jeff, thanks for joining me.
150560	152000	Thanks for having me, Sam. It's a pleasure.
152720	159280	I think we met probably just once, but I feel like we met about 15 years ago at one of those
159280	162720	Beyond Belief conferences at the Salk Institute. Does that ring a bell?
163280	168480	I was at one of the Beyond Belief conferences, and I don't recall meeting you there, but it's
168480	175280	totally possible. It's possible we didn't meet, but I remember, I think we had an exchange where
175280	180640	one of us was in the audience, and the other was in exchange over 50 feet or whatever.
180640	184320	Oh, that makes sense. Yeah, I was in the audience, and I was speaking up.
184880	190240	Yeah, okay, and I was probably on stage defending some Kakamemi conviction. Well,
190240	197200	anyway, nice to almost meet you once again, and you have a new book which we'll cover part of,
197200	203360	not by no means exhausting its topics of interest, but the new book is A Thousand Brains,
203360	210960	and it's a work of neuroscience and also a discussion about the frontiers of AI and where
210960	218240	all this is heading, but maybe we should start with the brain part of it and start with the
218960	227200	really novel and circuitous and entrepreneurial route you've taken to get into neuroscience.
227760	234720	This is the non-standard course to becoming a neuroscientist. Give us your brief biography
234720	240560	here. How did you get into these topics? Well, I fell in love with brains when I
240560	246080	just got out of college, so I studied electrical engineering in college, and right after I started
246080	252960	my first job at Intel, I read an article by Francis Crick about brains and how we don't
252960	257280	understand their work, and I just became enamored. I said, oh my god, we should understand this.
257280	262720	This is me. I am my brain, and no one seems to know how this thing is working, and I just couldn't
262720	268400	accept that, and so I decided to dedicate my life to figuring out what's going on when I'm thinking
268480	277840	and who we are basically as a species. It was a difficult path, so I quit my job. I
277840	285120	essentially applied to become a graduate student first at MIT and AI, but then I settled at Berkeley
285680	290320	in neuroscience, and I said, okay, we're going to spend my life figuring out how the
290320	296080	neocortex works, and I found out very quickly that that was a very difficult thing to do
296080	301200	scientifically, but difficult to do from the practical aspects of science, that you couldn't
301200	305680	get funding for that. It was considered too ambitious. It was theoretical work, and people
305680	310640	didn't fund theoretical work, so after a couple of years as a graduate student at Berkeley,
310640	315360	I set a different path. I said, okay, I'm going to go back to work in industry for a few years
315920	320960	to mature, to figure out how to make institutional change because I was up against an institutional
320960	326880	problem, not just a scientific problem, and that turned into a series of successful businesses
326880	331840	that I was involved with and started, including Palm and HandSpring. These are some of the early
332400	337040	HandTel computing companies, and we were having a tremendous amount of success with that,
337040	343040	but it was never my mission to stay in the HandTel computing industry. I wanted to get back
343040	348800	to neuroscience, and everybody who worked for me knew this. In fact, I told the investors,
348800	352080	I'm only going to do this for four years, and they said, what? I said, yeah, that's it,
352640	356400	but it turned out to be a lot longer than that because all the success we had, but eventually,
356400	361520	I just extracted myself from it, and I said, I'm going to go and I have so many years left in my
361520	366560	life, so after having all that success in the mobile computing space, I started a neuroscience
366560	369920	institute. This is at the recommendation of some neuroscience friends of mine,
369920	374000	so they helped me do that, and I ran that for three years, and now I've been running sort of a
374000	377920	private lab just doing pure neuroscience for the last 17 years.
379280	381040	That's Numenta, right?
381040	388000	That's Numenta, yeah, and we've made some really significant progress in our goals,
388560	393520	and the book documents some of the recent really significant discoveries we've made.
394080	400480	So am I right in thinking that you made enough money at Palm and HandSpring that you could
400480	405840	self-fund your first neuroscience institute, or is that not the case? Did you have to go raise
405840	412080	money? Well, it was a bit of both. Certainly, I was a major contributor. I wasn't the only one,
412640	418000	but I didn't want the funding to be the driver of what we did and how we spent all our time,
418000	423920	so at the institute, we had collaborations with both Berkeley and Stanford. We didn't get funds
423920	430160	from them, but we did work with them on various things, and then we had, but that was mostly
430160	435200	funded by myself. Numenta is still, I'm a major contributor to it, but there are other people
435200	440800	who've invested in Numenta, and we have one outside venture capitalist, and several people,
440800	447440	but I'm still a major contributor to it. I just view that as a sort of a necessary thing to get
447440	452560	on to the science and not have to worry about it, because when I was at Berkeley, what I was told
452560	459040	over and over again, I really came to understand this. In fact, eventually, after that, when I was
459040	464240	running the Redwood Neuroscience Institute, I went to Washington to talk about the National
464240	469040	Science Foundation and National Institute of Health, and also to DARPA, who were the funders
469040	473680	of neuroscience, and everyone thought what we were doing, which is sort of big theory, large-scale
473680	478240	theories of cortical function, that this was like the most important problem to work on,
478240	484080	but everyone said they can't fund it for various reasons. Over the years, I've come to appreciate
484080	489520	that it's very difficult to be a scientist doing what we do with traditional funding sources,
490080	495520	but we don't work outside of science. We partner with labs, and we go to conferences,
495520	501520	we publish papers, we do all the regular stuff. Right. It's amazing how much comes down to funding
501520	507120	or lack of funding and the incentives that would dictate whether something gets funded in the first
507120	513440	place. It's by no means a perfect system. It's a kind of an intellectual market failure.
514000	518320	Yeah. It is fascinating, and we could have a whole conversation about that sometimes, perhaps,
518960	523840	because I ask myself, why is it so hard? Why do people can't fund this? There's reasons for it,
523840	529200	and it's a complex, strange thing. When people were telling me, this is the most important thing
529200	534320	anyone could be working on, and we think your approaches are great, but we can't fund that,
534960	540880	why is that? I just accepted the way it was. I said, okay, this is the world I'm living in.
540880	547360	I'm going to get one chance here. If I can't do this through working my way as a graduate
547360	552400	student to getting a position at a university, how am I going to do it? I said, okay, it's
552400	558960	not what I thought, but this is what's going to be. Nice. Well, let's jump into the neuroscience
558960	565120	side of it. Generally speaking, we're going to be talking about intelligence and how it's
565120	575520	accomplished in physical systems. Let's start with a definition, however loose. What is intelligence
575520	581840	in your view? I didn't know and didn't have any pre-ideas about what this would be. It was a mystery
581840	587440	to me, but we've learned what a good portion of your brain is doing. We started the New York
587440	593600	Cortex, which is about 70% of the volume of a human brain. I now know what that does, and so
593600	599120	I'm going to take that as my definition for intelligence here. What's going on in your
599120	605360	New York Cortex is it's learning a model of the world, an internal recreation of all the things
605360	610720	in the world that you know of and how it does. That's the key and what we've discovered,
610720	616720	but it's this internal model. Intelligence requires having an internal model of the world
616720	621360	in your head. It allows you to recognize where you are. It allows you to act on things. It allows
621360	625520	you to plan and think about the future. If I'm going to say, what happens when I do this, the
625520	631360	model tells you that. To me, intelligence is just about having a model in your head and using that
631360	636960	for planning and action. It's not about doing anything in particular. It's about understanding
636960	641600	the world. Yeah, that's interesting. I think most people would, that's kind of an internal
642480	648560	definition of intelligence, but I think most people would reach for an external one or a
649680	654800	functional one that has to take in the environment. It's something about being able to
655920	662640	flexibly meet your goals under a range of conditions, more flexibly than rigidly. I
662640	667520	guess there's rigid forms of intelligence, but when we're talking about anything like
668240	674560	general intelligence, we're talking about something that is not merely hardwired and
674560	679440	reflexive, but flexible. Yes, but if you have an internal model of the world,
679440	683360	you had to learn it, at least from a human point of view. There's some things we have built in
683360	689760	when we're born, but the vast majority of what you and I know, Sam, is learned. We didn't know
689760	693200	what a computer was when you're born. You don't know what a coffee cup is. You don't know what
693200	696880	building is. You don't know what doors are. You don't know what computer codes are. None of this
696960	702960	stuff. Almost everything we interact with in the world today, in language, we don't know any
702960	707520	particular language when we're born. We don't know mathematics, so we had to learn all these things.
707520	711440	So if you want to say there might be an internal model that wasn't learned, well, that's pretty
711440	715520	trivial, but I'm talking about models that are learned and you have to interact with the world
715520	719920	to learn it. You can't learn it without being present in the world, without having an embodiment,
719920	724640	without moving about, touching and seeing and hearing things. So a large part of what people
724640	730560	think about, like you brought up, is, okay, we are able to solve a goal, but that's what a model
730560	736000	lets you to do. That is not what intelligence itself is. Intelligence is having this ability
736000	741360	to solve any goal, right? Because if your model covers that part of the world, you can figure out
741360	746560	how to manipulate that part of the world and achieve what you want. So I'll give you a little
746560	750800	further analogy. It's a little bit like computers. When we talk about a universal turning machine
750880	757200	or what a computer is, it's not defined by what the computer is applied to do. It's like a computer
757200	760320	isn't something that solves a particular problem. A computer is something that works on a set of
760320	766000	principles, and that's how I think about intelligence. It's a modeling system that works on a set of
766000	771840	principles. Those principles can exist in a mouse, in a dog, in a cat, in a human, and probably birds,
772480	775600	but don't focus on what those animals are doing.
776080	781920	Hmm. Yeah, it's important to point out that a model need not be a conscious model. In fact,
781920	789680	most of our models are not conscious and might not even be, in principle, available to consciousness,
789680	796160	although I think at the boundary, something that you'd say is happening entirely in the dark,
796160	802160	does have a kind of, or can have a kind of liminal conscious aspect. So I mean, to take,
802160	808000	you know, the coffee cup example, this leads us into a more granular discussion of what it means
808000	813360	to have a model of anything at the level of the cortex. But, you know, if I reach for my coffee cup
814080	819600	and grasp it, the ordinary experience of doing that is something I'm conscious of.
820320	827840	I'm not conscious of all of the prediction that is built into my accomplishing that and
828800	835120	experiencing what I experience when I touch a coffee cup. And yet, it's prediction that is
835120	840560	required having some ongoing expectation of what's going to happen there when I, you know, when each
840560	848960	finger touches the surface of the cup that allows for me to detect any error there or to be surprised
848960	853840	by something truly anomalous. So if I reach for a coffee cup, and it turns out that's, you know,
853840	859520	it's a hologram of a coffee cup and my hand passes right through it, the element of surprise
859520	867600	there seems predicated on some ongoing prediction processing to which the results of my behavior
867600	874000	is being compared. So maybe you can talk about what you mean by having a model at the level
874000	881040	of the cortex and how prediction is built into that. Yeah. Well, my first book, which I published
881280	887520	14 years ago called Long Intelligence, was just about that topic. It was about how it is the brain
887520	891840	is making all these predictions all the time and all your sensory modalities and you're not aware of
891840	897120	it. And so that's sort of the foundation. And you can't make a prediction without a model. I mean,
897120	901520	a model, to make a prediction, you had to have some expectation, the expectation whether you're
901520	906640	not aware of it or not, but they have an expectation. And that has to be driven from some internal
906640	910960	representation of the world that says, hey, this, they're about to touch this thing, I know what it
910960	917040	is, it's supposed to feel this way. And even if you're not aware that you're doing that. One of the
917040	923200	key discoveries we made, and this was maybe about eight years ago, we, we had to get to the bottom
923200	928800	like how do neurons make predictions? What is the physical manifestation of a prediction in the brain?
929680	933440	And most of these predictions, as you point out, are not conscious, you're not aware of them.
933440	937440	They're just happening. And if something, if something is wrong, then your attention is drawn
937440	941680	to it. So if you felt the coffee cup and there's a little burr on the side or a crack and you didn't
941680	946880	know that was expected that you'd say, Oh, there's a crack. I mean, what was the brain doing when it
946880	951760	was making that prediction? And we have a, we have a theory about this. And I wrote about it in the
951760	958800	book a bit. And it's, it's a beautiful, I think it's a beautiful theory. But it, it's, it's
958800	962400	basically most of the predictions that are going on in your brain, most of them, not all of them,
962400	968880	but most of them happen inside individual neurons. They are, it is a internal to the
968880	974160	individual neurons. Now, not a single neuron can predict something, but an ensemble of neurons do
974160	982240	this. But it's an internal state. And we have, we wrote a paper that came out in 1990, 2016, excuse
982240	990000	me, 2016, which is, it's called wider neurons have so many synapses. And we, what we posit in
990000	994880	that paper, and I'm pretty sure this is correct is that, you know, neurons have these thousands of
994880	1000240	synapses. Most of those synapses are being used for prediction. And when a neuron recognizes a
1000240	1005840	pattern and says, okay, I'm supposed to be active soon, I should be, I should be becoming active soon.
1005840	1010240	If everything is according to our model here, I should be coming active soon. And it goes into
1010240	1016480	this internal state, the neuron itself is saying, okay, I'm expecting to become active. And you
1016480	1021200	can't detect that consciously. It's internal to the, it's essentially just a depolarization or
1021200	1028000	change of the voltage of the neuron. And so when, but it, we showed how the network of these neurons,
1028000	1033680	what'll happen is, if your prediction is correct, then a small subset of the neurons become active.
1033680	1037840	But if the prediction is incorrect, a whole bunch of neurons become active at the same time.
1038480	1043120	And then that draws your attention to the problem. So it's a fascinating problem. But most of
1043120	1047600	the predictions going on in your brain are not accessible outside of individual neurons.
1047600	1049200	So there's no way you could be conscious about it.
1051280	1055520	I guess most people are familiar with the general anatomy of a neuron where you have a,
1055520	1061840	this spindly looking thing where there's a cell body and there's a long process,
1061840	1068720	the axon leading away, which carries the action potential, if that neuron fires
1068800	1075760	to the synapse and communicates neurotransmitters to other neurons. But on the other side of,
1076960	1081920	in the standard case, on the other side of the cell body, there's this really,
1082800	1089440	often really profuse arborization of dendrites, which is kind of the mad tangle of processes,
1089440	1096880	which receive information from other neurons to which this neuron is connected. And
1097440	1103760	it's the integration of information on that side. But before that neuron fires,
1103760	1108720	the change, the probability of its firing, that that's the place you are
1109440	1115280	locating this, the full set of predictive changes or the full set of changes that constitute
1115280	1118320	prediction in the case of a system of neurons.
1118320	1124240	Yeah. It's interesting. For many years, people looked at those, the connections on the dendrites,
1124240	1130480	on that bushy part called synapses. And when they activated a synapse, most of the synapses
1131040	1135440	were so far from the cell body that they didn't really have much of an effect.
1135440	1140640	They didn't seem like they could make anything happen. And so, but there are thousands and
1140640	1145440	thousands of them out there, but they don't seem powerful enough to make anything occur.
1146080	1151120	And what was discovered basically over the last 20 years, that there are,
1151120	1154960	there's a second type of spike. So you mentioned the one that goes down the axon,
1154960	1159520	that's the action potential. But there are spikes that travel along the dendrites.
1160320	1165760	And so basically what happens is the individual sections of the dendrite, like little branches
1165760	1171520	of this tree, each one of them can recognize patterns on their own. They can recognize hundreds of
1171520	1176160	separate patterns on these different branches. And they can cause this spike to travel along
1176160	1182880	the dendrite. And that lowers the, changes the voltage of the cell body a little bit.
1182880	1188160	And that is what we call the predictive state. The cell is like prime. It says, oh, I, I'm,
1188160	1194720	if I fire, I'm ready to fire. And it's not actually a probability change. It's the timing.
1195360	1199360	And so a cell that's in this predictive state that says, I think I should be firing now,
1200240	1205680	or very shortly, if it does generate the regular spike, the action potential, it does it a little
1205680	1210000	bit sooner than it would have otherwise. And it's timing that is the key to making the whole
1210000	1214080	circuit work. We're getting pretty down in the weeds here about the science. But I hope, I don't
1214080	1218880	know if you're, all your readers, listeners will appreciate that. Yeah. No, I think it's useful
1218880	1225280	though. More weeds here. But I mean, one of the novel things about your argument is that
1226160	1233440	it was inspired by some much earlier theorizing. You mark your debt to Vernon Mountcastle. But
1233520	1240560	the idea is that there's a, a common algorithm operating more or less everywhere at the level
1240560	1246880	of the cortex. That is, it's more or less the, you know, the cortex is doing essentially the same
1246880	1255520	thing, whether it's producing language or vision or, you know, any other sensory channel or motor
1255520	1262800	behavior. So talk about the, the general principle that you spend a lot of time on in the book of
1262800	1270080	just the organization of the neocortex into cortical columns and the implications this has for
1270960	1277680	how we view what the brain is doing in terms of sensory and motor learning and, you know,
1277680	1282880	all of its consequences. This is, Vernon Mountcastle made this proposal back in the 70s.
1283520	1288800	And it's just a dramatic idea. And it's an incredible idea and so incredible that some
1288800	1294080	people just refuse to believe it, but other people really think it's a tremendous discovery.
1294080	1298640	But what he noticed was if you look at the neocortex, if you could take one out of your head
1298640	1303600	or out of a human's head, it's like a sheet. It's about two and a half millimeters thick.
1303600	1310240	It is about the size of a large dinner napkin or 1500 square centimeters. And if you could fold it,
1310240	1315680	lay it flat. And the different parts of it, like they do different things as parts that do vision,
1315680	1322320	as parts that do language and parts that do hearing and so on. But if you cut into it and you look at
1322320	1328880	the structure in one of these areas, it's very complicated. There are dozens of different cell
1328880	1334400	types, but they're very prototypically connected and they're arranged in certain patterns and
1334400	1339760	layers and different types of things. So it's a very complex structure, but it's almost the same
1339760	1344560	everywhere. It's not the same everywhere, but almost the same everywhere. And so this is not
1344560	1349680	just true in a human neocortex, but if you look at a rat's neocortex or a dog's neocortex or a cat
1349680	1357280	or a monkey, this same basic structure is there. And what Vernon Malkus said is that all the parts
1357280	1361840	of the neocortex are actually, we think of them as doing things, different things, but they're
1361840	1366880	actually all doing some fundamental algorithm, which is the same. So hearing and touch and vision
1366880	1371200	are really the same thing. He says, if you took part of the cortex and you hook it up to your eyes,
1371200	1374560	you'll get vision. If you hook it up to your ears, you'll get hearing. If you hook it up to
1374560	1381120	other parts of the neocortex, you'll get language. And so he spent many years giving the evidence
1381120	1387840	for this. He proposed further that this algorithm was contained in what's called a column. And so
1387840	1394240	if you would take a small area of this neocortex, remember it's like two and a half millimeters
1394240	1400880	thick, you take a very sort of skinny little one millimeter column out of it, that that is the
1400880	1408560	processing element. And so our human neocortex, we have about 150,000 of these columns. Other
1408560	1413360	animals have more or less. People should picture something resembling a grain of rice in terms
1413360	1417840	of scale here. Yeah, yeah. I sometimes say take a piece of skinny spaghetti like, you know, angel
1417840	1421920	have pasta or something like that and cut it into two little two and a half millimeter links
1421920	1426720	and stack them side by side. Now the funny thing about columns is you can't see them. They're not
1426720	1431680	visual things. You can't look under a microscope, you won't see it. But he pointed out why
1433200	1437840	they're there. It has to do with how they're connected. So all the cells in one of these
1437840	1442880	little millimeter pieces of rice or spaghetti, if you will, are all processing the same thing. And
1442880	1447520	the next piece of rice over processing something different and the next piece of rice over processing
1447520	1453920	something different. And so he didn't know what was going on in the cortical column. He
1455040	1460480	articulated the architecture. He talked about the evidence that this exists. He said, here's the
1460480	1466720	evidence why these things are all doing the same thing. But he didn't know what it was. And it's
1466720	1472000	kind of hard to imagine what it is that this algorithm could be doing. But that was essentially
1472000	1475600	the core of our research. That's what we've been focused on for close to 20 years.
1475920	1481760	It's also hard to imagine the microanatomy here because in each one of these little columns,
1482400	1488000	there's something like 150,000 neurons on average. And if you could just unravel
1488720	1497440	all of the connections there, the tiny filaments of nerve endings, what you would have there is
1497440	1504560	on the order of kilometers in length, all wound up into that tiny structure. So this is a
1505280	1511520	strange juxtaposition of simplicity and complexity, but there's certainly a
1511520	1516320	mad tangle of processes in there. Yeah, this is why brains are so hard to study. If you look at
1516320	1520000	another organ in the body, whether it's the heart or the liver or something like that,
1520640	1526320	and you take a little section of it, it's pretty uniform. But here, if you take a teeny piece of
1526320	1534480	the cortex, it's got this incredible complexity in it, which is not random. It's very specific.
1535440	1540800	So yeah, it's hard to get wrapper your heads around how complex it is. But we need to be
1540800	1546880	complex because what we do as humans is extremely complex. And we shouldn't be fooled that we're
1546880	1551680	just a bunch of neurons that are doing some mass action. No, there's a very complex processing
1551680	1558160	going on in your brain that it's not just a blob of neurons that are pulsating,
1558320	1563680	you know, very detailed mechanisms that are undergoing it. And we figured out what some of
1563680	1572400	those are. So describe to me what you mean by this phrase, a reference frame. What does that mean at
1572400	1579760	the level of the cortex and cortical columns? Yeah. So we're jumping to the end point,
1579760	1583680	because that's not where we started. We were trying to figure out how the cortical columns work.
1584400	1590080	And what we realized is that they're little modeling engines. Each one of these cortical
1590080	1596240	columns is able to build a model of its input. And that model is what we would call a sensory
1596240	1602640	motor model. Let's assume it's getting in from your finger, a tip of your finger. One of the
1602640	1607120	columns is getting input from the tip of your finger. And as your finger moves and touches
1607120	1612800	something, the input changes. But it's not just sufficient to know how the input changes. For
1612880	1617200	you to build a model of the object you're touching. And I use the coffee cup example quite a bit,
1617200	1620560	because that's how we did it. If you move your finger over the coffee cup,
1620560	1623520	and you're not even looking at the coffee cup, you could learn a model of the coffee cup. You
1623520	1627280	could feel it just with one finger, you could feel like, oh, what this is what its shape is.
1627920	1631840	But to do that, your brain, that cortical column, your brain as a whole, but that
1631840	1636240	cortical column on individually has to know something about where your finger is relative
1636240	1641200	to the cup. It's not just a changing pattern that's coming in. It has to know how your finger's
1641200	1646640	moving and where your finger is as it touches it. So the idea of a reference frame is a way of
1647200	1651440	noting a location. You have to have a location signal. You have to have some knowledge about
1652000	1656480	where things are in the world relative to other things. In this case, where's your finger relative
1656480	1662080	to the object you're trying to touch, the coffee cup. And we realize that for you to, your brain
1662080	1666400	to make a prediction of what you're going to feel when you touch the edge of the cup. And again,
1666400	1669440	you used to mention earlier, you're not conscious of this, you'd reach the cup and you just,
1669440	1672080	but your brain's predicting what all your fingers are going to feel.
1673200	1677120	It needs to know where the finger's going to be. And I have to know what the object is,
1677120	1681040	it's a cup and these know where it's going to be. And that requires a reference frame.
1681040	1686480	A reference frame is just a way of noting a location. It's saying relative to this cup,
1686480	1691200	your finger is over here, not over there, not on the handle, you know, up at the top, whatever it is.
1691920	1696400	And, and this is a deduced property, we can say for certainty that this has to exist.
1696480	1699280	If your finger is going to make a prediction when it reaches and touches the coffee cup,
1699280	1702960	it needs to know where the finger is, that the location has to be relative to the cup.
1703520	1708080	So we can just say for certainty that there needs to be reference frames in the brain. And this is
1708080	1713280	not a controversial idea. Well, we, perhaps this novel is that we realize that these reference
1713280	1718400	frames exist in every cortical column. And it's the structure of knowledge. It applies to not just
1718400	1722720	what your finger feels on a coffee cup and what you see when you look at it, but also how you
1722720	1728480	arrange all your knowledge in the world is stored in these reference frames. And so we're jumping
1728480	1735520	ahead here many steps. But when we think and when we posit, when we try to, you know, reason in our
1735520	1742000	head, when even my language right now is, is where the neurons are walking through locations in
1742000	1746400	reference frames, recalling the information stored there. And that's what comes into your head or
1746400	1751360	that's what you say. So it becomes the core reference, the reference name becomes the core
1751360	1755360	structure for the entire, everything you do. It's knowledge about the world is in these reference
1755360	1761920	frames. Yeah, you make a strong claim about the, the primacy of motion, right? Because there's,
1761920	1768080	everyone knows that there's part of the cortex devoted to motor action. We refer to it as the
1768080	1774480	motor cortex and distinguish it from sensory cortex in that way. But it's also true that
1774480	1780080	other regions of the cortex and, and perhaps every region of the cortex does have some
1780800	1787680	connection to lower structures that can affect motion, right? So it's not, it's not that it's
1787680	1794320	just motor cortex that's in the, in the motion game. And by analogy or by direct implication,
1794320	1802480	you think of thought as itself being a kind of movement in conceptual space, right? So there's
1802480	1808320	a mapping of the, the sensory world that can really only be accomplished by acting on it,
1808320	1813840	you know, and therefore moving, right? So the other way to map the cup, you know, is to touch
1813840	1820960	it with your fingers in the end. There is a, an analogous kind of motion in conceptual space
1820960	1826240	and, you know, even, you know, abstract ideas like, I think some of the examples even in the book
1826240	1831040	are like, you know, democracy, right? You know, or, or money or what, just how, how we understand
1831040	1836240	these things. So let's go back to the first thing you said there. The idea that there's motor cortex
1836240	1843760	and sensory cortex is sort of no longer considered right. As you mentioned, we, the neurons that,
1843760	1847920	in these cortical columns, there are certain neurons that are the motor output neurons.
1847920	1852880	These are in a particular layer five, as they're called. Until in the motor cortex,
1852880	1857280	they were really big and they project to the spinal cord and say, oh, that's how you move your
1857280	1862640	fingers. But if you look at the neurons, the columns in the visual cortex, the parts that get
1862640	1868320	input from the eyes, they have the same layer five cells. And these cells project to a part
1868320	1873360	of the brain called the superior colliculus, which is what controls eye motion. So this goes
1873360	1877200	against the original idea of, oh, there's sensory cortex and motor cortex. No one believes that,
1877200	1881680	well, I don't know, buddy, but very few people believe that anymore. It's, as far as we know,
1881680	1886240	every part of the cortex has a motor output. And so every part of the cortex is getting some sort
1886240	1892160	of input and it has some motor output. And so the basic algorithm of cortex is a sensory motor
1892160	1897440	system. It's not divided. It's not like we have sensory areas and motor areas. As far as we know,
1897440	1902320	ever it's been seen, there's these motor cells everywhere. So we can put that aside.
1903040	1912160	Now, I can very clearly walk you through, in some sense, prove from logic that when you're
1912160	1917200	learning what a coffee cup feels like, and I can even do this for vision, that you have to have this
1917200	1921120	idea of a reference frame, that the finger, you have to know where your finger is relative to the
1921120	1925680	cup. And that's how you build a model of it. And so we can build out this cortical column that
1925680	1930080	explains how it does that. How does your, how does your parts of your cortex that representing
1930080	1935120	your fingers are able to learn to structure a coffee cup? Now, Mount Castle, go back to him,
1935120	1940320	he said, look, it's the same algorithm everywhere. And he says, it looks the same everywhere. So it's
1940320	1944720	the same algorithm everywhere. So that should sort of say, hmm, well, if I'm thinking about something
1944720	1949040	that doesn't seem like a sensory motor system, like I'm not touching something or looking,
1949040	1953760	I'm just thinking about something, that would, if Mount Castle was right, then the same basic
1953760	1958480	algorithm would be applying there. So that was one constraint like, well, that, you know, and the
1958480	1962560	evidence is that Mount Castle is right. I mean, the physical evidence suggests he's right. We just,
1962560	1966320	it just comes a little bit odd to think like, well, how is language like this and how is
1966320	1971200	mathematics like, you know, touching a coffee cup. But then we realize that it's just,
1971200	1976400	reference frames are a way of storing everything. And, and in the way we move through a reference
1976880	1979520	frame, it's like, how do you move from one location? How do the neurons
1980160	1985200	activate one location after another location after another location? We do that to this
1985200	1989840	idea of movement. So I'm moving, if I want to access the locations on a coffee cup, I move my
1989840	1995920	finger. But the same concept could apply to mathematics or to politics, but you're not actually
1995920	2001440	physically moving something, but you're still walking through a structure. A good, a good bridge
2001440	2006800	example is if I say to you, you know, imagine your house. And I ask you to walk, you know,
2006800	2011840	tell me about your house. What you'll do is you'll mentally imagine walking through your house.
2011840	2016000	It won't be random. You just won't have random thoughts come to your head. But you will mentally
2016000	2019840	imagine walking through your house. And as you walk through your house, you'll recall what is
2019840	2022880	supposed to be seen in different directions. You can say, oh, I'll walk in the front door and I'll
2022880	2027120	look to the right. What do I see? I'll look to the left. What do I see? This is sort of a,
2027120	2030960	an example you could relate it to something physically you could move to. But that's pretty
2030960	2034880	much what's going on when you're thinking about anything. If you're thinking about your podcast
2034880	2039120	and how you can get more subscribers, you have a model of that in your head. And you're, you are
2040320	2044480	trying it out thinking about different aspects by literally invoking these different locations
2044480	2048640	and reference frames. And so that's sort of the core of all knowledge.
2048640	2053600	Yeah, it's interesting. I guess back to Mount Castle for a second. One piece of evidence in
2053600	2060240	favor of this view of a common cortical algorithm is the fact that adjacent areas of cortex can be
2061200	2070080	appropriated by various functions. If you lose your vision, say, classical visual cortex can be
2070080	2077040	appropriated by other senses. And there's this plasticity that can ignore some of the previous
2077040	2083120	boundaries between separate senses in the cortex. Yeah, that's right. There's this tremendous
2083120	2088640	plasticity and you can also recover from various sorts of trauma and so on. I mean, there's some
2088640	2093360	rewiring has to occur, but it does show that that whatever is going, whatever the circuitry in the
2093360	2099520	visual cortex was, you know, quote, if you were a sighted person, what it would do, if you're not
2099520	2104880	a sighted person, well, it'll just do something else. And so it's not, and so that is a very,
2104880	2110960	very strong argument for that. There's a famous scientist, Bakurita, who did an experiment where
2110960	2116480	he, I'm trying to remember the animal he used, maybe even recall it. But anyway, it'll come to me.
2116560	2122560	A ferret, I think it was a ferret. Before the animal was born, he took the optic nerve
2122560	2126400	and ran it over to one part of the, a different part of the neocortex and took the auditory nerve
2126400	2131040	and ran it to a different part of the neocortex. Basically rewired the animal. I'm not sure we
2131040	2135920	do these experiments today, but, and, you know, and the argument was that the animals, you know,
2135920	2141920	still saw and still heard and so on, maybe not as well as an unaltered one, but the evidence was
2141920	2149040	that, yeah, that really works. Yeah, so what is genetically determined and what is learned here?
2149040	2156880	I mean, it seems that the genetics at minimum are determining what is hooked up to what initially,
2156880	2161600	right, you know, barn? Yeah, roughly, roughly, that's right. I think, you know, like, where do the
2161600	2165760	eyes, the optic nerve from the eyes, where do they project? And where do the regions that get
2165760	2171520	the input from the eyes, where do they project? And so this rough sort of overall architecture
2171520	2176640	is specified. And as we just talked through trauma and other reasons, sometimes that architecture
2176640	2183040	can get rewired. I think also the, the, the basic algorithm that goes on in each of these
2183040	2187840	cortical columns, the, the circuitry in the, in, inside the neocortex is pretty well determined by,
2189040	2194640	by genetics. And in fact, what one of my guess's arguments was that humans, the human neocortex
2194640	2199600	got large, and we have a very large one relative to our body size, just because all it had,
2199600	2203200	all evolution had to do is discover, just make more copies of these columns, you know, you
2203200	2206800	don't have to, you don't have to do anything new, just make more copies. And that's something easy
2206800	2212640	for genes to specify. And so human brains got large quickly in evolutionary time,
2212640	2220400	by that just replicate more of it type of thing. Okay, so let's go beyond the human now and talk
2220400	2227840	about artificial intelligence. And before we talk about the risks or the imagined risks,
2228800	2234240	tell me what you think the path looks like going forward. I mean, what are we doing now?
2234240	2240800	And what do you think we need to do to have our dreams of true artificial general intelligence
2240800	2247760	realized? Well, you know, today's AI is powerful as it is and successful as it is.
2248800	2255520	I think most senior AI practitioners will admit, and many of them have, that they don't really
2255520	2259840	think they're intelligent. You know, they're really wonderful pattern classifiers, and they
2259840	2265040	can do all kinds of clever things. But there are very few practitioners would say, hey, this AI
2265040	2269760	system that's recognizing faces is really intelligent. And there's a sort of a lack of
2269760	2274960	understanding what intelligence is and how to go forward. And how do you make a system that could
2274960	2281440	solve general problems, could do more than one thing, right? And so in the second part of my book,
2281440	2286880	I lay out what I believe are the requirements to do that. And my approach has always been,
2286880	2290880	for 40 years, has been like, well, I think we need to first figure out what brains do
2291920	2296560	and how they do them. And then we'll know how to build intelligent machines, because we just
2296560	2303680	don't seem able to intuit what an intelligent machine is. So I think the way I look at this
2303680	2307840	problem, if we want to make, you know, what's the recipe for making an intelligent machine,
2308640	2312640	is you have to say, what are the principles by which the brain works that we need to replicate
2312640	2317520	and which principles don't we need to replicate? And so I made a list of these in the book, but
2318720	2322080	if you can think of a very high level, they have to have some sort of embodiment,
2322080	2327200	they have to have the ability to move their sensors somehow in the world. You know, you can't
2327200	2334080	really learn how to use tools and how to, you know, run factories and how to do things unless
2334160	2338960	you can move in the world. And it requires these reference frames I was talking about,
2338960	2342800	because movement requires reference frames. But that's not a controversial statement,
2342800	2347360	it's just a fact. You're going to have to have no where things are in the world.
2348000	2353680	And then the final, there's a set of things, but one of the other big ones, which we haven't
2353680	2358080	talked about yet, and which is where the title of the book comes from, A Thousand Brains, is that
2358640	2363520	the way to think about our near cortex, it has 150,000 of these columns,
2363520	2369120	we have essentially 150,000 separate modeling systems going on in our brain. And they work
2369120	2376240	together by voting. And so that concept of a distributed intelligence system is important.
2376960	2381520	We're not just one thing, we, it feels like we're one thing, but we're really 150,000 of these
2381520	2385680	things. And we're only conscious of being one thing, but that's not really what's happening
2385680	2390800	under the covers. So those are some of the key ideas. I've just picked a very, very high idea.
2390800	2394880	It has to have an embodiment, it has to be able to move its sensors, it has to be able to
2394880	2400320	organize information and reference frames, and it has to be distributed. And that's how we can do
2400960	2403680	multiple sensors and sensory integration and things like that.
2404960	2413840	I guess I question the criteria of embodiment and movement, right? I mean, I understand that
2414560	2420480	practically speaking, that's how a useful intelligence can get trained up in our world
2420480	2427520	to do things physically in our world. But it seems like you could have a perfectly intelligent
2427520	2437440	system, i.e. a mind that is turned loose on simulated worlds and are capable of solving
2437440	2445040	problems that don't require effectors of any kind. I mean, chess is obviously a very low-level
2445040	2450240	analogy, but just imagine a thousand things like chess that represent real
2451680	2457520	theory building or cognition in a box. Yeah, I think you're right. And so
2458160	2462720	when I use the word movement or embodiment, and I'm careful to define it in the book because
2463520	2470480	it doesn't have to be physical. An example I gave, you can imagine an intelligent agent that
2470480	2476240	lives in the Internet, and movement is following links. It's not a physical thing,
2476800	2482880	but there's still this conceptual mathematical idea of what it means to move. And so you're
2482880	2489520	changing the location of some representation, and that could be virtual. It doesn't have to
2489520	2495520	have a physical embodiment, but in the end, you can't learn about the world just by looking at
2495520	2502720	a set of pictures. That's not going to happen. You can learn to classify pictures, but so some
2502720	2509040	AI systems will have to be physically embodied like a robot, if I guess you want. Many will not
2509040	2514880	be, many will be virtual, but they all have this internal process which I could point to the thing
2514880	2518560	that says, here's where the reference frame is, here's where your current location is, here's
2518560	2523920	how it's moving to a new location based on some movement vector. Like a verb, a word, you can
2523920	2528800	think of that as like an action. And so you can have an action that's not physical, but it's
2528800	2532480	still an action, and it moves to a new location in this internal representation.
2532480	2537680	Right, right. Okay, well, let's talk about risk, because this is the place where I think you and
2537680	2545760	I have very different intuitions. You are, as far as I can tell from your book, you seem very
2545760	2555200	sanguine about AI risk. And really, you seem to think that the only real risk, the serious risk
2555200	2562080	of things going very badly for us is that bad people will do bad things with much more powerful
2562080	2567760	tools. So the heuristic here would be, you know, don't give your super intelligent AI to the next
2567760	2574400	Hitler, because that would be bad. But other than that, the generic problem of self-replication,
2574400	2579920	which you talk about briefly, and you point out, we face that on other fronts, like with,
2579920	2583920	you know, with the pandemic, where we've been dealing with, I mean, so natural viruses and
2583920	2589840	bacteria or computer viruses, I mean, there's anything that can self-replicate can be dangerous.
2590480	2597840	But that aside, you seem quite confident that AI will not get away from us. There won't be an
2597840	2603920	intelligence explosion. And we don't have to worry too much about the so-called alignment problem.
2604960	2609600	And at one point, you even question whether it makes sense to expect that we'll produce
2610560	2615360	something that can be appropriately called superhuman intelligence. So Brett, perhaps you
2615360	2622960	can explain the basis for your optimism here. So I think what most people, and perhaps yourself,
2623600	2631200	have fears about is they use humans as an example of how things can go wrong.
2631200	2636080	And so we think about the alignment problem, or we think about, you know, motivations of an AI
2636080	2641680	system. Well, okay, does the AI system have motivations or not? Does it have a desire to do
2641680	2649440	anything? Now, as a human, an animal, we all have desires, right? But if you take apart what parts
2649440	2655040	of the human brain are doing, different parts, there's some parts that are just building this
2655040	2660560	model of the world. And this is the core of our intelligence. This is what it means to be intelligent.
2660560	2667040	That part itself is benign. It has no motivations on its own. It doesn't desire to do anything.
2667040	2673920	I use an example of a map. You know, a map is a model of the world. And my map can be
2674640	2680960	very powerful tool for some to do good or to do bad. But on its own, the map doesn't do anything.
2681520	2686240	So if you think about the neocortex on its own, it sits on top of the rest of your brain.
2686880	2691760	And the rest of your brain is really what makes us motivated. It gets us, you know, we have our
2692880	2698160	good sides and our bad sides, you know, our desire to maintain our life and have sex and
2698160	2701840	aggression and all these stuff. The neocortex is just sitting there. It's like a map. It says,
2701840	2706800	you know, I understand the world and you can use me as how you want. So when we build intelligent
2706800	2713120	machines, we have the option and, and I think almost imperative not to build the old parts
2713120	2718720	of the brain, too. You know, why do that? We just have this thing, which is inherently smart,
2718720	2723680	but on its own doesn't really want to do anything. And so there's some of the some of the risks that
2723680	2731120	come about from the people's fears about the alignment problem, specifically, is that the
2732000	2736800	intelligent agent will decide on its own or decide for some reason to do things that are
2736800	2741120	in its best interest, not in our best interest, or maybe it'll listen to us, but then not listen
2741120	2747120	to us or something like this. I just don't see how that can physically happen. And for people,
2747120	2751360	most people don't understand the separation. They just assume that this intelligence is wrapped up
2751360	2755920	in these, all these, all the things that make us human. The intelligence explosion problem is a
2755920	2761040	separate issue. I'm not sure which one of those you're more worried about. Yeah, well, let's,
2761040	2767040	let's deal with the alignment issue first. And I do think that's more critical. But
2768000	2773440	let me see if I can capture what troubles me about this picture you've painted here. It seems
2773440	2783280	that you're, to my mind, you're being strangely anthropomorphic on one side, but not anthropomorphic
2783280	2789920	enough on the other. I mean, so like, you know, you think that to understand intelligence and
2790000	2797120	actually truly implement it in machines, we really have to be focused on ourselves first. And we
2797120	2803840	have to understand how the human brain works and then emulate those principles pretty directly in
2803840	2810320	machines. That strikes me as possibly true, but possibly not true. And if, if I had to bet, I
2810320	2818720	think I would probably bet against it. Although even here, you seem to be not taking full account of
2819280	2824240	what the human brain is doing. I mean, like, we, you know, we can't partition reason and emotion
2825040	2830480	as clearly as we thought we could hundreds of years ago. And in fact, you know, certain emotions,
2830480	2835840	you know, certain drives are built into our being able to reason effectively.
2835840	2840720	I think that's, you know, I'll take an exception to that. I know, I know this is an opinion that
2841360	2844160	you had Lisa Barrett on your program recently.
2844800	2847920	Antonio Demasio is the person who's banged on about this the most.
2847920	2853120	Yeah, I know. And I just disagree. I just, it's, you know, you can separate these two.
2853680	2860320	And I can say this because I understand actually what's going on in the New York Cortex.
2860320	2864720	And I can see what I have a very good sense of what these actual neurons are actually doing
2864720	2871120	when it's modeling the world and so on. And you do not, it does not require this emotional component.
2871200	2876640	A human does. Now, you say, you know, on one hand, I don't argue we should replicate the brain.
2876640	2881040	I say we should replicate the structures of the New York Cortex, which is not replicating the brain.
2881760	2886800	It's just one part of the brain. And so I'm specifically saying, you know, I don't really
2886800	2891920	care too much about how this spinal cord works or how, you know, the brainstem does this or that.
2891920	2895920	It's interesting. Maybe I know a little bit about it, but that's not important. The cortex sits on
2895920	2900160	top of another structure and the cortex does its own thing and they interact. Of course,
2900240	2904560	they interact. And our emotions affect what we learn and what we don't learn.
2904560	2909600	But it doesn't have to be that way in a system, another system that we build.
2909600	2910800	That's the way humans are structured.
2910800	2914640	Yeah. Okay. So I would agree with that except the boundary between
2915520	2923920	what is an emotion or a drive or a motivation or a goal and what is a value neutral mapping of
2923920	2931680	reality. You know, I think that boundary is perhaps harder to specify than you think it is.
2931680	2937200	And that certain of these things are connected, right? Which is to, I mean, here's an example.
2937200	2943120	This is probably not a perfect analogy, but this gets at some of the surprising features of cognition
2943120	2952160	that may await us. So we think intuitively that understanding a proposition is cognitively quite
2952240	2958000	distinct from believing it, right? So I can give you a statement that you can believe or
2958000	2962560	disbelieve or be uncertain about. I can say, you know, there's two plus two equals four,
2962560	2968160	two plus two equals five, and that can give you some gigantic number and say this number is prime.
2968160	2973120	And presumably in the first condition, you'll say, yes, I believe that. In the second, you'll say,
2973120	2978240	no, that's false. And in the third, you won't know whether or not it's prime or not.
2978720	2983600	So those are distinct states that we can intuitively differentiate. But there's also evidence
2984240	2989440	to suggest that merely comprehending a statement, if I give you a statement and you
2989440	2997520	parse it successfully, the parsing itself contains an actual default acceptance of it as true.
2998240	3005120	And rejecting it as false is a separate operation added to that. I mean, there's not a ton of
3005120	3010160	evidence for this, but there's certainly some behavioral evidence. So if I put you in a paradigm
3010160	3015040	where we gave you statements that were true and false, and all you had to do was judge them true
3015040	3021680	and false, and they were all matched for complexity. So, you know, two plus two equals four is no more
3021680	3027360	or less complex than two plus two equals five. But it'll take you longer, systematically longer,
3027360	3032000	to judge very simple statements to be false than to judge them to be true,
3032000	3037280	suggesting that you're doing a further operation. Now, we can remain agnostic as to whether or
3037280	3043120	not that's actually true. But if true, it's counterintuitive that merely understanding
3043120	3049840	something entails some credence, epistemic credence given to it by default, and that
3049840	3056320	to reject it as false represents a subsequent act. But that's the kind of thing that already
3056400	3063360	were on territory that is not coldly rational. Some of the all too apish appetites have kind
3063360	3070800	of crept into cognition here in ways that we didn't really budget for. And so the question is,
3070800	3075200	just how much of that is avoidable in building a new type of mind?
3076160	3081600	Well, you know, I'm not familiar with that specific research. And so I haven't heard of that. But
3082320	3089520	to me, none of these things are surprising in any way. Just if you start thinking about the brain
3089520	3093440	is basically trying to build models, it's constantly trying to build models. In fact,
3094640	3099280	as you walk around your life, day to day, moment to moment, and you see things,
3099280	3102480	you're building the model, the model is being constructed, even like where are things in the
3102480	3106160	refrigerator right now, your brain will update, you open the refrigerator, oh, the milk's on the
3106160	3110480	left today, whatever. And so if someone gives you a proposition like two plus two equals five,
3111040	3114560	you know, I don't know what the evidence that you believe it and then falsify it.
3114560	3118720	But I certainly imagine you can imagine it trying to see if it's right. It'd be like me saying to
3119360	3122720	you, hey, you know, Sam, the milk was on the right in your refrigerator. And you'd have to
3122720	3126800	think about it for a second. You say, well, let me think. No, last time I saw it was on the left.
3126800	3131360	You know, no, that's wrong. But you would walk through the process of trying to imagine it
3131360	3137360	and trying to see, does that fit my model? And yes or no. And I don't, it's not surprising to me
3137360	3142800	that you would have to process it the way as if it was true. It's just matters saying,
3142800	3147520	can you imagine this? Go imagine it. Do you think it's right? It's not like I believe that now I
3147520	3152480	falsified it. It's more likely. Well, actually, I'll just give you one other datum here because
3152480	3158560	it's just intellectually interesting and socially all too consequential. This effect goes by
3158560	3163760	several names, I think. But one is the illusory truth effect, which is even in the act of
3163760	3169680	disconfirming something to be false, you know, some specious rumor or conspiracy theory,
3170320	3175680	merely having to invoke it, I mean, to have people entertain the concept again, even in the context
3175680	3184000	of debunking it, ramifies a belief in it in many, many people. It's just, it becomes harder to
3184000	3186640	discredit things because you have to talk about them in the first place.
3186640	3192320	Yeah. I mean, so look, we're talking about language here, right? And in language,
3192400	3197360	so much of what we humans know is via language. And we have no idea if it's true when someone says
3197360	3204240	something to you, right? How do you know? And so you'd have to, so I mean, I gave an example like,
3204240	3208880	I've never been to the city of Havana. Well, I believe it's there. I believe it's true. I don't
3208880	3213120	know. I've never been there. I've never actually touched or smelled it or saw it. So maybe it's
3213120	3218720	false. So I just, I mean, this is one of the issues we have. I have a whole chapter on false
3218720	3225040	beliefs because so much of our knowledge of the world is built up on language. And the default
3225040	3230480	assumption under language that if someone says something, it's true. It's like, it's a pattern
3230480	3234800	in the world. You're going to accept it. If I touch a coffee cup, I accept that that's what it feels.
3234800	3239280	Right. And if I look at something, I accept that's what it looks like. Well, if someone says
3239280	3244160	something, my initial acceptance is, okay, that's what it is. So, you know, and then I'm going to
3244160	3248240	say, in fact, well, if someone says something that's false, of course, well, that's a problem
3248240	3254080	because just by the fact that I've experienced it, it's now part of my world model. And that's
3254080	3259120	what you're referring to. I can see this is really a problem of language we face. And this is the
3259120	3263840	root cause of almost all of our false beliefs, is that someone just says something enough times.
3264640	3270240	And that's good enough. And you have to seek out contrary evidence for it.
3270720	3274000	Yeah, sometimes it's good enough. Even when you're the one saying it, you just overhear
3275280	3282080	the voice of your own mind saying it. And no, I know. That's been proven that everyone is
3282080	3286800	susceptible to that kind of distortion of our beliefs, especially our memories,
3286800	3288800	just remembering something over and over again changes it.
3288800	3295200	Yeah. Okay, so let's get back to AI risk here because here's where I think you and I
3295200	3299520	have very different intuitions. I mean, the intuition that many of us have,
3300320	3305120	you know, that the people who have informed my views here, people like Stuart Russell,
3305120	3311280	who you probably know at Berkeley, and Nick Bostrom, and Eleazar Yudkowski, and just lots of
3311280	3318400	people in this spot worrying about the same thing to one another degree. The intuition is that
3318960	3327920	you don't get a second chance to create a truly autonomous superintelligence. It seems that in
3327920	3333360	principle, this is the kind of thing you have to get right on the first try. And having to get
3333360	3339280	anything right on the first try just seems extraordinarily dangerous because we rarely,
3339280	3345280	if ever, do that when doing something complicated. And another way of putting this is that it seems
3345280	3353120	like in the space of all possible superintelligent minds, there are more ways to build one that
3353120	3360720	isn't perfectly aligned with our long-term well-being than there are ways to build one
3360720	3367360	that is perfectly aligned with our long-term well-being. And from my point of view, what
3368400	3373280	your optimism and the optimism of many other people who take your side of this debate
3374000	3382960	is based on is not really taking the prospect of intelligence seriously enough and the autonomy
3382960	3391440	that is intrinsic to it. I mean, if we actually built a true general intelligence, what that means
3391440	3397920	is that we would suddenly find ourselves in relationship to something that we actually
3398480	3405760	can't perfectly understand. It's like it will be analogous to a strange person walking into the room,
3405760	3412400	you know, you're in relationship. And if this person can think a thousand times or a million
3412400	3419920	times faster than you can and has goals that are less than perfectly aligned with your own,
3421040	3426560	that's going to be a problem eventually. We can't find ourselves in a state of perpetual
3426560	3432240	negotiation with systems that are more competent and powerful and intelligent.
3432240	3437280	I think there's two mistakes in your argument. The first one is you say
3437280	3442800	my intuition and your intuition. I think most of the people who have this fear have an intuition
3442800	3453280	about what happened. If you'd like to continue listening to this conversation,
3453280	3458160	you'll need to subscribe at SamHarris.org. Once you do, you'll get access to all full-length
3458160	3463280	episodes of the Making Sense podcast, along with other subscriber-only content, including bonus
3463280	3468640	episodes and AMAs and the conversations I've been having on the Waking Up app. The Making Sense
3468640	3476000	podcast is ad-free and relies entirely on listener support, and you can subscribe now at SamHarris.org.
