start	end	text
0	11400	Welcome to the Making Sense podcast, this is Sam Harris.
11400	14840	Just a note to say that if you're hearing this, you are not currently on our subscriber
14840	18680	feed and will only be hearing the first part of this conversation.
18680	22720	In order to access full episodes of the Making Sense podcast, you'll need to subscribe at
22720	24600	SamHarris.org.
24600	28400	There you'll find our private RSS feed to add to your favorite podcatcher, along with
28400	30760	other subscriber-only content.
30760	34760	We don't run ads on the podcast, and therefore it's made possible entirely through the support
34760	35760	of our subscribers.
35760	47080	So if you enjoy what we're doing here, please consider becoming one.
47080	51400	Okay, well there's been a lot going on out there.
51400	58320	Everything from Elon Musk and Mark Zuckerberg challenging one another to an MMA fight.
58320	61720	Which is ridiculous and depressing.
61720	70040	To Robert Kennedy Jr. appearing on every podcast on Earth, apart from this one, I have so far
70040	72320	declined the privilege.
72320	74000	It really is a mess out there.
74000	80000	I'll probably discuss the RFK phenomenon in a future episode, because it reveals a lot
80000	84240	about what's wrong with alternative media at the moment.
84240	89600	But I will leave more of a post mortem on that for another time.
89600	92120	Today I'm speaking with Mark Andreessen.
92120	96760	Mark is a co-founder and general partner at the venture capital firm Andreessen Horowitz.
96760	99160	He's a true internet pioneer.
99160	103800	He created the mosaic internet browser, and then co-founded Netscape.
103800	107880	He's co-founded other companies and invested in too many to count.
107880	111520	Mark holds a degree in computer science from the University of Illinois, and he serves
111520	115200	on the board of many Andreessen Horowitz portfolio companies.
115200	116760	He's also on the board of META.
116760	122240	Anyways, you'll hear Mark and I get into a fairly spirited debate about the future
122240	123620	of AI.
123620	128880	We discuss the importance of intelligence generally and the possible good outcomes of building
128880	134080	AI, but then we get into our differences around the risks or lack thereof of building
134080	137400	AGI, artificial general intelligence.
137400	142360	We talk about the significance of evolution in our thinking about this, the alignment
142360	148840	problem, the current state of large language models, how developments in AI might affect
148840	157280	how we wage war, what to do about dangerous information, regulating AI, economic inequality,
157280	158280	and other topics.
158280	161300	Anyway, it's always great to speak with Mark.
161300	162300	We had a lot of fun here.
162300	165120	I hope you find it useful.
165120	173120	Now I bring you Mark Andreessen.
173120	174760	I am here with Mark Andreessen.
174760	176880	Mark, thanks for joining me again.
176880	178520	It's great to be here, Sam.
178520	179520	Thanks.
179520	182160	I got you on the end of a swallow of some delectable beverage.
182160	184240	Yes, you did.
184240	186240	So this should be interesting.
186240	194040	I'm eager to speak with you specifically about this recent essay you wrote on AI.
195040	201160	Many people have read this and you are a voice that many people value on this topic,
201160	202160	among others.
202160	207440	Perhaps you've been on the podcast before and people know who you are, but maybe you
207440	212680	can briefly summarize how you come to this question.
212680	217520	How would you summarize the relevant parts of your career with respect to the question
217520	220240	of AI and its possible ramifications?
221000	226560	I've been a computer programmer, technologist, computer scientist since the 1980s when I
226560	229720	actually entered college in 1989 at University of Illinois.
229720	234840	The AI field had been through a boom in the 80s, which had crashed hard.
234840	240600	By the time I got to college, the AI wave was dead and buried at that point for a while.
240600	244640	It was like the backwater of the department that nobody really wanted to talk about.
244640	247640	I learned a lot of it in a school.
248040	253960	I went on to help create what is now known as the modern internet in the 90s and then
253960	258440	over time transitioned to become a, but for being a technologist, to being an entrepreneur.
258440	263360	Then today I'm an investor venture capitalist, and so 30 years late, 30, 35 years later,
263360	269520	I'm involved in a very broad cross-section of tech companies that have many of them have
269520	275880	many AI aspects to them, and everything from Facebook now meta, which has been investing
275920	280160	deeply in AI for over a decade, through to many of the best new AI startups.
280160	284960	Our day job is to find the best new startups in a new category like this and try to back
284960	285960	the entrepreneurs.
285960	289200	That's how I spend most of my time right now.
289200	293480	The essay is titled, Why AI Will Save the World?
293480	297960	I think even in the title alone, people will detect that you are striking a different note
297960	300680	than I tend to strike on this topic.
300800	307200	I disagree with a few things in the essay that are, I think, at the core of my interest
307200	312120	here, but I think there are many things we agree about.
312120	316760	Up front, we agree, I think, with more or less anyone who thinks about it, that intelligence
316760	319160	is good and we want more of it.
319160	324560	If it's not necessarily the source of everything that's good in human life, it is what will
324560	327680	safeguard everything that's good in human life.
327680	332000	Even if you think that love is more important than intelligence and you think that playing
332000	336480	on the beach with your kids is way better than doing science or anything else that is
336480	342920	narrowly linked to intelligence, you have to admit that you value all of the things that
342920	346680	intelligence will bring that will safeguard the things you value.
346680	351520	A cure for cancer and a cure for Alzheimer's and a cure for a dozen other things will give
351520	354360	you much more time with the people you love.
354360	360240	Whether you think about the primacy of intelligence or not very much, it is the thing that has
360240	364400	differentiated us from our primate cousins and it's the thing that allows us to do everything
364400	369080	that is maintaining the status of civilization.
369080	372760	If the future is going to be better than the past, it's going to be better because of what
372760	376440	we've done with our intelligence in some basic sense.
376440	381480	I think we're going to agree that because intelligence is so good and because each increment
381600	387640	of it is good and profitable, this AI arms race and gold rush is not going to stop.
387640	391840	We're not going to pull the brakes here and say, let's take a pause of five years and
391840	394840	not build any AI.
394840	398760	I don't remember if you address that specifically in your essay, but even if some people are
398760	401800	calling for that, I don't think that's in the cards.
401800	404320	I don't think you think that's in the cards.
404320	408880	It's hard to believe that you just put in the box and stop working on it.
408880	410480	It's hard to believe that progress stops.
410920	413560	Having said that, there are some powerful and important people who are in Washington
413560	417240	right now advocating that and there are some politicians who are taking them seriously.
417240	420000	At the moment, there is some danger around that.
420000	423600	Then look, there's two other big dangers, two other scenarios that I think would both
423600	426280	be very devastating for the future.
426280	432160	One is the scenario where the fears around AI are used to basically entrench a cartel.
432160	433840	This is what's happening right now.
433840	436240	This is what's being lobbied for right now is there are a set of big companies that are
436240	437720	arguing in Washington.
437760	441280	Yes, AI has positive cases, uses.
441280	443000	Yes, AI is also dangerous.
443000	446960	Because it's dangerous, therefore, we need a regulatory structure that basically entrenches
446960	451880	a set of currently powerful tech companies to be able to have basically exclusive rights
451880	454000	to do this technology.
454000	456280	I think that would be devastating for reasons we could discuss.
456280	459360	Then look, there's a third outcome, which is we lose China wins.
459360	463880	They're certainly working on AI and they have a ... I would consider it to be a very dark
463880	467200	and dystopian vision of the future, which I also do not want to win.
467520	472920	I guess that is in part the cash value of the point I just made, that even if we decided
472920	475480	to stop, not everyone's going to stop.
475480	480400	Human beings are going to continue to grab as much intelligence as we can grab, even
480400	485240	if in some local spot, we decide to pull the brakes.
485240	491160	Although it really is, at this point, it's hard to imagine even whatever the regulation
491160	494520	is, it really stalling progress.
495000	500360	Again, given the intrinsic value of intelligence and given the excitement around it and given
500360	505840	the obvious dollar signs that everyone is seeing, the incentives are such that I just
505840	506840	don't see it.
506840	512600	Well, it will come to the regulation piece eventually because I think it's ... Given
512600	516800	the difference in our views here, it's not going to be a surprise that I want some form
516800	521200	of regulation and I'm not quite sure what that could look like.
521280	526280	I think you would have a better sense of what it looks like and perhaps that's why you're
526280	527520	worried about it.
527520	534920	But before we talk about the fears here, let's talk about the good outcome because you sketch
534920	539680	a fairly ... I know you don't consider yourself a utopian, but you sketch a fairly utopian
539680	542840	picture of promise in your essay.
542840	547120	If we got this right, how good do you think it could be?
548040	551720	Let's just start by saying I deliberately loaded my ... Let the title of the essay with
551720	554960	a little bit of a religious element and I did that very deliberately because I view
554960	558960	that I'm up against a religion, the AI risk fear religion.
558960	564040	But I am not myself religious, lowercase our religious in the sense of, I'm not a utopian.
564040	568200	I'm very much an adherent to what Thomas Sowell called the constrained vision, not the unconstrained
568200	569200	vision.
569200	575120	I live in a world of practicalities and trade-offs and so yeah, I am actually not utopian.
575120	578680	Having said that, building on what you've already said, intelligence, if there is a
578680	583080	lever for human progress across many thousands of domains simultaneously, it is intelligence
583080	587560	and we know that because we have thousands of years of experience seeing that play out.
587560	589840	The thing I would add to ... I thought you made that case very well.
589840	593600	The thing I would add to the case you made about the positive virtues of intelligence
593600	598120	in human life is that the way you described it, at least the way I heard it, was more
598120	602280	focused on the social, societal wide benefits of intelligence, for example, cures for diseases
602280	603640	and so forth.
603640	605640	That is true and I agree with all that.
605640	609360	There are also individual level benefits of intelligence at the level of an individual,
609360	612600	even if you're not the scientist to invent secure for cancer at an individual level,
612600	617360	if you are smarter, you have better life welfare outcomes on almost every metric that we know
617360	622000	how to measure, everything from how long you'll live, how healthy you'll be, how much education
622000	625240	you'll achieve, career success, the success of your children.
625240	629340	By the way, your ability to solve problems, your ability to deal with conflict, smarter
629340	633080	people are less violent, smarter people are less bigoted.
633720	639000	There's this very broad pattern of human behavior where basically more intelligence, simply
639000	641400	at the individual level, leads to better outcomes.
641400	647280	The most utopian I'm willing to get is this potential, which I think is very real right
647280	648280	now.
648280	651760	It's already started where you basically just say, look, human beings from here on out
651760	657000	are going to have an augmentation and the augmentation is going to be in the long tradition of augmentations
657000	661040	like everything from eyeglasses to shoes to word processors to search engines, but now
661160	665960	the augmentation is intelligence and that augmented intelligence capability is going
665960	671560	to let them capture the gains of individual level intelligence, potentially considerably
671560	675240	above where they punch in as individuals.
675240	679440	What's interesting about that is that can scale all the way up.
679440	684880	Somebody who struggles with daily challenges all of a sudden is going to have a partner
684880	685880	and an assistant.
685880	689800	I'm going to coach and a therapist and a mentor to be able to help improve a variety of things
689800	690800	in their lives.
690800	695320	Then look, if you had given this to Einstein, he would have been able to discover a lot
695320	701680	more new fundamental laws of physics in the full vision.
701680	705000	This is one of those things where it could help everybody and then it could help everybody
705000	706800	in many, many different ways.
706800	707800	Yeah.
707800	712360	Well, see, in your essay, you go into some detail of bullet points around this concept
712360	718000	of everyone having essentially a digital oracle in their pocket where you have this personal
718040	724880	assistance who you can be continuously in dialogue with and it'd be like having the
724880	731880	smartest person who's ever lived just giving you a bespoke concierge service to all manner
733280	738080	of task and across any information landscape.
738080	744480	I happened to recently rewatch the film Her, which I hadn't seen since it came out.
744800	749120	It came out 10 years ago and I don't know if you've seen it lately, but I must say it lands
749120	753720	a little bit differently now that we're on the cusp of this thing.
753720	760720	And while it's not really dystopian, there is something a little uncanny and quasi bleak
762320	768640	around even the happy vision here of having everyone siloed in their interaction with
768640	769640	an AI.
770240	777240	It's the personal assistant in your pocket that becomes so compelling and so aware of
777760	782560	your goals and aspirations and what you did yesterday and the email you sent or forgot
782560	785560	to send.
785560	791200	Apart from the ending, which is clever and surprising and irrelevant for our purposes
791200	796680	here, it's not an aspirational vision of the sort that you sketch in your essay.
796720	802840	And I'm wondering if you see any possibility here that even the best case scenario has
802840	808840	something intrinsically alienating and troublesome about it.
808840	812640	Yeah, so look, on the movie, as Peter Thiele has pointed out, Hollywood no longer makes
812640	814640	positive movies about technology.
814640	819800	And then look, he argues this because they hate technology, but I would argue maybe a
819800	823920	simpler explanation, which is they want dramatic tension and conflict.
823960	828840	And so it's going to have things are going to have a dark tinge, regardless, they obviously
828840	832200	spring load it by their choice of character and so forth.
832200	836200	The scenario I have in mind is actually quite a bit different, and let me get kind of maybe
836200	839520	philosophical for a second, which is there's this long running debate.
839520	843240	This question that you just raised is a question that goes back to the Industrial Revolution.
843240	847800	And remember, it goes back to the core of actually the original Marx's original theory.
847800	852960	Marx's original theory was industrialization, technology, modern economic development, alienates
853000	855040	the human being from society.
855040	857440	That was his core indictment of technology.
857440	862480	And look, you can point to many, many cases in which I think that has actually happened.
862480	864440	Like I think alienation is a real problem.
864440	866320	I don't think that critique was entirely wrong.
866320	869560	His prescriptions were disastrous, but I don't think the critique was completely wrong.
869560	872720	Look, having said that, then it's a question of like, OK, now that we have the technology
872720	876160	that we have and we have new technology we can invent, how could we get to the other
876160	877400	side of that problem?
877400	882120	And so I would put the shoe on the other foot and I would say, look, the purpose of human
882160	886880	existence and the way that we live our lives should be determined by us and it should be
886880	890720	determined by us to maximize our potential as human beings.
890720	895200	And the way to do that is precisely to have the machines do all the things that they can
895200	896840	do so that we don't have to.
896840	900760	Right. And this is why Marx ultimately, his critique was actually in the long run, I think
900760	903880	has been judged to be incorrect, which is we are all much better.
903880	907400	Anybody in the developed West, industrialized West today is much better off by the fact
907400	911240	that we have all these machines that are doing everything from making shoes to harvesting
911280	914680	corn to doing everything, you know, so many other, you know, industrial processes around
914680	919040	us, like we just have a lot more time and much more pleasant, you know, day to day
919040	922840	life, you know, than we would have if we were still doing things the way that things used
922840	923840	to be done.
923840	927400	The potential of the AI is just like, look, take, take, take, take the dredge workout,
927400	930960	like take the remaining dredge workout, take all the, you know, look, like, I'll give you
930960	935200	a simple example, office work, that, you know, the inbox staring at you in the face of 200
935200	940040	emails, right, at Friday at three in the afternoon, like, OK, no more of that, right?
940080	942240	We're not going to do that anymore, because I'm going to have an AI assistant.
942240	944440	The AI assistant is going to answer the emails, right?
944440	947480	And in fact, what's going to happen is my assistant is going to answer the email that
947480	949480	your AI assistant set, right?
950480	952040	It's mutually assured destruction.
952120	952840	Yeah, exactly.
952840	954640	But like, the machine should be doing that.
954640	957880	Like the human being should not be sitting there when it's like sunny out and his like,
957880	960600	you know, my, when my eight year old wants to play, I'm not, I shouldn't be sitting there
960600	962880	doing emails, I should be at my eight year old, there should be a machine that does that
962880	967720	for me. And so I view this very much as basically apply the machines to do the dredge
967720	970000	work precisely so that people can live more human lives.
970000	971800	Now, this is philosophical.
971800	973800	People have to decide what kind of lives they want to live.
973800	975360	And again, I'm not a utopian on this.
975360	977840	And so there's a long discussion we could have about how this actually plays out.
978040	979360	But that potential is there for sure.
979680	980760	Right. Right.
980760	984880	OK, so let's jump to the bad outcomes here, because this is really why I want to talk
984880	989800	to you. In your essay, you list five and I'll just read your section titles here.
989800	992040	And then we'll take a whack at them.
992320	995440	The first is, will AI kill us all?
995800	998320	Number two is, will AI ruin our society?
998680	1001360	Number three is, will AI take all our jobs?
1001960	1005280	Number four is, will AI lead to crippling inequality?
1005880	1008400	And five is, will AI lead to people doing bad things?
1008760	1011760	And I would tend to bin those in really two buckets.
1011760	1013880	The first is, will AI kill us?
1013960	1016840	And that's the existential risk concern.
1017360	1022600	And the others are more the ordinary bad outcomes that we tend to think about with
1022600	1026800	other technology, with bad people doing bad things with powerful tools,
1027040	1031480	unintended consequences, disruptions to the labor market, which I'm sure we'll
1031480	1035960	talk about. And those are all of those are certainly the near term risks.
1036080	1041000	And in some sense, even more interesting to people, because the existential
1041000	1045160	risk component is longer term and it's even purely hypothetical.
1045160	1048520	And you seem to think it's purely fictional.
1049120	1051360	And this is where I think you and I disagree.
1051360	1055640	So let's start with this question of, will AI kill us all?
1056080	1062600	And the thinking on this tends to come under the banner of the problem of AI
1062600	1063640	alignment, right?
1063640	1068640	And the concern is that we can build, if we build machines more powerful than
1068640	1073520	ourselves, more intelligent than ourselves, it seems possible that the space of
1073520	1077680	all possible, more powerful, super intelligent machines includes many that
1077680	1083040	are not aligned with our interests and not disposed to continually track our
1083040	1087200	interests and many more of that sort than of the sort that perfectly hew to our
1087200	1089160	interests in perpetuity.
1089560	1094000	So the concern is we could build something powerful that is essentially an angry
1094000	1098520	little God that we can't figure out how to placate once we've built it.
1099200	1101680	And certainly we don't want to be negotiating with something more
1101680	1103280	powerful and intelligent than ourselves.
1103840	1106920	And the picture here is of something like, you know, a chess engine, right?
1106920	1111120	We've built chess engines that are more powerful than we are at chess.
1111200	1116560	And once we've built them, if everything depended on our beating them in a game
1116560	1118720	of chess, we wouldn't be able to do it, right?
1118720	1120960	Because they are simply better than we are.
1120960	1125720	And so now we're building something that is a general intelligence and it will
1125720	1130240	be better than we are at everything that goes by that name or such as the concern.
1131000	1135000	And in your essay, I mean, I think there's an ad hominem piece that I think we
1135000	1140840	should blow by because you've already described this as a religious concern.
1140840	1145880	And in the essay, you describe it as just a symptom of superstition and that people
1145880	1148200	are essentially in a new doomsday cult.
1148800	1153080	And there's some share of troop believers here and there's some share of, you know,
1153080	1154960	AI safety grifters.
1155440	1158440	And I think, you know, I'm sure you're right about some of these people, but we
1158440	1164760	should acknowledge upfront that there are many super qualified people of high
1164760	1169960	probity who are prominent in the field of AI research who are part of this chorus
1170640	1172080	voicing their concern now.
1172080	1175720	And we've got somebody like Jeffrey Hinton, who arguably did as much as anyone
1175720	1179200	to create the breakthroughs that have given us these these LLMs.
1179680	1184240	We have Stuart Russell, who literally wrote the most popular textbook on AI.
1184560	1189040	So there are other serious sober people who are very worried for reasons of a
1189040	1190760	sort that I'm going to going to express here.
1190760	1195080	So that's I mean, that's just I just want to acknowledge that both are true.
1195080	1200760	There's the crazy people, the new millennialists, the doomsday preppers,
1200760	1205760	the neuro atypical people who are in their polyamorous cults and, you know,
1205760	1208160	AI alignment is their primary fetish.
1208680	1211320	But there's a lot of sober people who are also worried about this.
1211320	1212960	Would you would you acknowledge that much?
1213320	1217840	Yeah, although it's tricky because smart people also have a tendency to fall into
1217840	1222160	cults, so it doesn't get you totally off the hook on that one.
1222160	1225400	But I would I would register a more fundamental objection to what I would
1225400	1228920	describe as and this is not I'm not knocking you on this, but it's something
1228920	1231120	that something that people do is sort of argument by authority.
1231640	1232720	I don't think applies either.
1233000	1234760	And yeah, well, I'm not making that yet.
1235360	1235760	No, I know.
1235760	1237760	But like this idea, this idea, which is very good.
1237760	1239120	Again, I'm not characterizing your idea.
1239120	1239960	I'll just say it's a general idea.
1239960	1243640	This general idea that there are these experts and these experts are experts
1243640	1246600	because they're the people who created the technology or originated the ideas
1246600	1250240	or implemented the systems, therefore have sort of special knowledge and insight
1250240	1253760	in terms of their downstream impact on society and rules and regulations
1253760	1254880	and so forth and so on.
1255400	1257960	That assumption does not hold up well historically.
1258440	1260520	In fact, it holds up disastrously historically.
1260920	1261760	There's actually a new book out.
1261760	1264080	I've been giving all my friends called When Reason Goes on Holiday.
1264440	1267840	And it's a story of literally what happens when basically people who are
1267880	1271480	like specialized experts in one area stray outside of that area in order to
1271480	1274440	become sort of general purpose philosophers and sort of social thinkers.
1274880	1276760	And it's just a tale of woe, right?
1276920	1280520	And in the 20th century, it was just a catastrophe.
1280520	1284680	And the ultimate example of that, and this is going to be the topic of this big
1284680	1287600	movie coming out this summer in Oppenheimer, the central example of that
1287600	1291840	was the nuclear scientists who decided that nuclear power, nuclear energy,
1291840	1294800	they had various theories on what was good, bad, whatever.
1294800	1295800	A lot of them were communists.
1296080	1298160	A lot of them were at least allied with communists.
1298640	1301760	A lot of them had a suspiciously large number of communist friends and
1301760	1306760	housemates and, you know, number one, like they, you know, made a moral decision.
1306760	1310000	A number of them did to hand the bomb to the Soviet Union, you know,
1310000	1312240	with what I would argue are catastrophic consequences.
1312240	1315680	And then two is they created an anti-nuclear movement that resulted in
1315800	1319120	nuclear energy stalling out in the West, which has also just been like
1319120	1320120	absolutely catastrophic.
1320120	1323720	And so if you listen to those people in that era who were, you know, the
1323720	1327200	top nuclear physicists of their time, you made a horrible set of decisions.
1327600	1330120	And quite honestly, I think that's what's happening here again.
1330120	1333640	And I just, I don't think they have the special insight that people think that they have.
1333880	1337840	OK, well, so I mean, this cuts both ways because, you know, at the beginning,
1337840	1342520	I'm definitely not making an argument from authority, but authority is a proxy
1342520	1346640	for understanding the facts at issue, right?
1346640	1351480	It's not to say that in the cases you're describing, what we often have are
1351800	1356520	people who have a narrow authority in some area of scientific specialization.
1356520	1362280	And then they begin to weigh in, in a much broader sense, as moral philosophers.
1362520	1365000	What I think you might be referring to there is that, you know, in the aftermath
1365000	1371280	of Hiroshima and Nagasaki, we've got nuclear physicists imagining that, you
1371280	1374440	know, that they now need to play the geopolitical game.
1374720	1377520	You know, and we actually, we have some of the people who invented game theory,
1377520	1380720	right, you know, for understandable reasons, thinking they need to play
1380720	1382200	the game of geopolitics.
1382200	1386200	And in some cases, I think in von Neumann's case, he even recommended
1386200	1389680	a preventative war against the Soviet Union before they even got the bomb, right?
1389680	1390880	Like, it could have gotten worse.
1390880	1395040	He could have, I think he wanted us to bomb Moscow or at least give them
1395040	1396160	some kind of ultimatum.
1396160	1399160	I think it wasn't, I don't think he wanted us to drop bombs in the dead of
1399160	1403160	night, but I think he wanted a strong ultimatum game played with them
1403160	1404480	before they got the bomb.
1404480	1406520	And I forget how he wanted that to play out.
1406880	1410960	And worse still, I even, I think Bertrand Russell, I could have this backwards.
1410960	1414800	Maybe von Neumann wanted a bomb, but Bertrand Russell, you know, a true
1414800	1417840	moral philosopher briefly advocated preventative war.
1418240	1423000	But in his case, I think he wanted to offer some kind of ultimatum to the Soviets.
1423320	1425000	In any case, that's the problem.
1425000	1428480	But, you know, at the beginning of this conversation, I asked you to give me a
1428720	1432880	brief litany of your bona fides to have this conversation so as to inspire
1432880	1437520	confidence in our audience and also just to acknowledge the obvious that you know
1437520	1441000	a hell of a lot about the technological issues we're going to talk about.
1441440	1444040	And so if you have strong opinions, they're not, you know, they're not
1444040	1446080	coming out of totally out of left field.
1446840	1449960	And so it would be with, you know, Jeffrey Hinton or anyone else.
1449960	1454800	And if I threw another name at you that was of some, you know, crackpot whose
1454800	1459040	connection to the field was non-existent, you would say, why should we listen
1459040	1459920	to this person at all?
1459920	1463200	You wouldn't say that about Hinton or Stuart Russell.
1463480	1467440	But I would acknowledge that where authority breaks down is really you're
1467440	1469920	only as good as your last sentence here, right?
1469920	1472840	If the thing you just said doesn't make any sense, well, then your
1472840	1474760	authority gets you exactly nowhere, right?
1474760	1477280	We just need to keep talking about why it doesn't make sense.
1477280	1478360	Or it should, or it should, right?
1478360	1479840	That ideally that's the case in practice.
1479840	1481520	That's not what tends to happen, but that would be the goal.
1481680	1485560	Well, I hope to give you that treatment here because some of your sentences, I
1485560	1487640	don't think add up the way you think they do.
1488320	1488520	Good.
1488520	1492320	Okay, so actually, there's actually one paragraph in the essay that caught my
1492320	1494960	attention that really inspired this conversation.
1494960	1497920	I'll just read it so people know what I'm responding to here.
1498280	1499240	So this is you.
1499840	1504480	My view is that the idea that AI will decide to literally kill humanity is a
1504480	1505840	profound category error.
1506280	1510360	AI is not a living being that has been primed by billions of years of evolution
1510360	1515720	to participate in the battle for survival of the fittest, as animals were and as we are.
1516080	1521440	It is math, code, computers built by people, owned by people, used by people,
1521480	1522680	controlled by people.
1523120	1526960	The idea that it will at some point develop a mind of its own and decide that
1526960	1531680	it has motivations that lead it to try to kill us is a superstitious hand wave.
1532080	1535600	In short, AI doesn't want, it doesn't have goals.
1535960	1538560	It doesn't want to kill you because it's not alive.
1539080	1540160	AI is a machine.
1540280	1543800	It's not going to come alive any more than your toaster will, end quote.
1544600	1544840	Yes.
1544880	1547200	So, I mean, I see where you're going there.
1547200	1553600	I see why that may sound persuasive to people, but to my eye, that doesn't even
1553640	1557240	make contact with the real concern about alignment.
1557600	1561880	So, let me just kind of spell out why I think that's the case, because it seems to
1561880	1567400	me that you're actually not taking intelligence seriously, right?
1567400	1572080	Now, I mean, some people assume that as intelligent scales, we're going to magically
1572080	1574040	get ethics along with it, right?
1574040	1575880	So, the smarter you get, the nicer you get.
1576200	1580440	And while, I mean, there's some data points with respect to how humans behave.
1580480	1584040	You know, you just mentioned one in a few minutes ago.
1584400	1586440	It's not strictly true even for humans.
1586440	1591440	And even if it's true in the limit, right, it's not necessarily locally true.
1591760	1596920	And more important, when you're looking across species, differences in
1596920	1601400	intelligence are intrinsically dangerous for the stupider species.
1601880	1605800	Yeah, so it need not be a matter of super intelligent machines spontaneously becoming
1605800	1608120	hostile to us and wanting to kill us.
1608760	1612720	It could just be that they begin doing things that are not in our well-being,
1612720	1616920	right, because they're not taking it into account as a primary concern in the same
1616920	1621920	way that we don't take the welfare of insects into account as a primary concern,
1621920	1622080	right?
1622080	1628400	So, it's very rare that I intend to kill an insect, but I regularly do things
1628400	1632080	that annihilate them just because I'm not thinking about them, right?
1632080	1635320	I'm sure I've effectively killed millions of insects, right?
1635440	1639200	If you build a house, you know, that must be a holocaust for insects.
1639480	1642080	And yet you're not thinking about insects when you're building that house.
1642520	1647840	So there are many other pieces to my gripe here, but let's just take this first one.
1648200	1652360	It just seems to me that you're not envisioning what it will mean to be in
1652360	1656120	relationship to systems that are more intelligent than we are.
1656120	1658840	You're not seeing it as a relationship.
1658960	1664720	And I think that's because you're denuding intelligence of certain
1664720	1668120	properties and not acknowledging it in this paragraph, right?
1668120	1672320	I mean, so to my ear, general intelligence, which is what we're talking about,
1672680	1676520	implies many things that are not in this paragraph.
1676520	1679000	Like, it implies autonomy, right?
1679000	1685040	And it implies the ability to form unforeseeable new goals, right?
1685040	1689000	In the case of AI, it implies the ability to change its own code ultimately
1689440	1691360	and execute programs, right?
1691360	1697040	I mean, it's just it's doing stuff because it is intelligent, autonomously intelligent.
1697360	1701960	It is capable of doing just we can stipulate more than we're capable of doing
1701960	1704520	because it is more intelligent than we are at this point.
1704920	1710280	So the superstitious hand waving I'm seeing is in your paragraph when you're
1710280	1714640	declaring that it would never do this because it's not alive, right?
1714640	1718680	As though the difference between biological and non-biological substrate
1719080	1720440	were the crucial variable here.
1720440	1724520	But there's no reason to think as a crucial variable where intelligence is concerned.
1724880	1727680	Yeah. So I would say there's to steal man, your argument, I would say you can
1727680	1731240	actually break your argument into two forms or the AI risk community would break
1731240	1732680	this argument into two forms.
1732680	1735280	So they would argue and they would argue, I think the strong form of both.
1735280	1737120	So they would argue the strong form of number one.
1737680	1739960	And I think this is kind of what you're saying, correct me if I'm wrong,
1739960	1742960	is because it is intelligent, therefore it will have goals.
1743480	1745880	If it didn't start with goals, it will evolve goals.
1745920	1749120	It will, you know, whatever it will, it will over time have a set of preferred
1749120	1751760	outcomes, behavior patterns that it will determine for itself.
1752120	1755920	And then they also argue the other side of it, which is what's what they call
1755920	1761400	the orthogonality argument, which is it's actually the it's another risk argument,
1761400	1762920	but it's actually sort of the opposite argument.
1763320	1766320	It's an argument that it doesn't have to have goals to be dangerous, right?
1766520	1768920	And that being, you know, it doesn't have to be sentient.
1768920	1770120	It doesn't have to be conscious.
1770120	1771760	It doesn't have to be self aware.
1771760	1773440	It doesn't have to be self interested.
1773440	1776760	It doesn't have to be in any way, like even thinking in terms of goals,
1776760	1779720	it doesn't matter because simply it can just do things.
1779720	1782560	And this is the, you know, this is the classic paperclip maximizer, you know,
1782560	1785560	kind of argument, like it, it'll just get, it'll, it'll start, it'll get kicked off
1785560	1787320	on one apparently innocuous thing.
1787640	1790320	And then it will just extrapolate that ultimately to the destruction of everything.
1790320	1790480	Right.
1790480	1792680	So, so anyways, is that helpful to maybe break those into the.
1792760	1797120	Yeah, I'm not quite sure how fully I would sign on the dotted line to each,
1797120	1803200	but the one piece I would add to that is that having any goal does invite the
1803200	1805760	formation of instrumental goals.
1805880	1810600	Once this system is responding to a change in environment, right?
1810600	1816160	I mean, if your goal is to make paperclips and you're super intelligent and
1816240	1819680	somebody throws up at some kind of impediment, you're making paperclips,
1819680	1821760	well, then you're responding to that impediment.
1821760	1825000	And now you have a shorter term goal of dealing with the impediment, right?
1825000	1826320	So that's the structure of the problem.
1826880	1827320	Yeah, right.
1827360	1830080	For example, the US military wants to stop you from making more paperclips.
1830080	1833320	And so therefore you develop a new kind of nuclear weapon, right?
1833560	1836280	You know, fundamentally to pursue your goal of making paperclips.
1836280	1840720	But one problem here is that these, the instrumental goal, even if the paperclip
1840720	1845520	goal is the wrong example here, because even if you think of a totally benign
1846000	1850720	future goal, right, a goal that it seems more or less synonymous with taking human
1850720	1854840	welfare into account, it's possible to imagine a scenario where some instrumental
1854840	1860440	new goal that could not be foreseen appears that is in fact hostile to our interests.
1860440	1865640	And if we're not in a position to say, oh, no, no, don't do that, that would be a problem.
1865840	1866920	So that's the end, okay.
1866920	1870040	So a full version of that, a version of that argument that you hear is basically
1870040	1872920	the, what if the goal is maximize human happiness, right?
1872920	1876120	And then the machine realizes that the way to maximize human happiness is to strap
1876120	1880880	us all into, you know, right down and put us in a nosy experience machine, you know,
1880880	1883080	and wire us up with, you know, VR and ketamine, right?
1883160	1885160	And we, you know, we can never get out of the matrix, right?
1885160	1889400	So, right, and it's be maximizing human happiness as measured by things like dopamine
1889400	1892680	levels or serotonin levels or whatever, but obviously not a positive outcome.
1892680	1896520	So, but again, that's like a variation of this paperclip, that's one of these
1896520	1899400	arguments that comes out of their orthogonality thesis, which is the goal
1899400	1903160	can be very simple and innocuous, and yet lead to catastrophe.
1903160	1906680	So, look, I think each of these has their own problems.
1906680	1911240	So where you started, where they're sort of like the machine, basically, you know,
1911240	1915640	like, and we can quibble with terms here, but like some, like the side of the argument
1915640	1921480	in which the machine is in some way self-interested, self-aware, self-motivated,
1921480	1927240	trying to preserve itself, some level of sentience, consciousness, setting its own goals.
1927240	1930600	Well, just to be clear, there's no consciousness implied here.
1930600	1933080	I mean, the lights don't have to be on.
1933080	1938040	It just, I think that, I mean, this remains to be seen whether consciousness comes along
1938040	1939960	for the ride at a certain level of intelligence.
1939960	1943000	But I think they probably are orthogonal to one another.
1943000	1947240	So intelligence can scale without the lights coming on in my view.
1947240	1949640	So let's leave sentience and consciousness aside.
1949640	1951800	Well, I guess there is a fork in the road, which is like,
1951800	1953800	is it declaring its own intentions?
1953800	1956440	Like, is it developing its own, you know,
1957240	1963560	conscious or not, does it have a sense of any form or a vision of any kind of its own future?
1963560	1967080	Yeah. So this is why I think there's some daylight growing between us.
1967080	1974120	Because to be dangerous, I don't think you need necessarily to be running a self-preservation
1974920	1983160	program. I mean, there's some version of unaligned competence that may not formally model the machine's
1983160	1989880	place in the world, much less defend that place, which could still be, if uncontrollable by us,
1989880	1991480	could still be dangerous, right?
1991480	1995720	It's like, it doesn't have to be self-referential in a way that an animal,
1995720	1999240	the truth is, they're dangerous animals that might not even be self-referential.
1999240	2004760	And certainly something like a virus or a bacterium, you know, is not self-referential
2004760	2008840	in a way that we would understand, and it can be lethal to our interests.
2008840	2013000	Yeah, yeah, that's right. Okay, so you're more on the orthogonality side between the two.
2013000	2017000	If I identify the two poles of the argument, you're more on the orthogonality side,
2017000	2019000	which is it doesn't need to be conscious, it doesn't need to be sentient,
2019000	2021080	it doesn't need to have goals, it doesn't need to want to preserve itself.
2021640	2026600	Nevertheless, it will still be dangerous because of the, as you described, the consequences of
2026600	2030040	sort of how it gets started, and then sort of what happens over time.
2030040	2033800	For example, as it defines some goals to the original goals, and it goes off course.
2033800	2036440	Well, so there's a couple problems with that.
2036440	2041560	So one is it assumes in here, it's like people don't give intelligence enough credit.
2041560	2044280	Like there are cases where people give intelligence too much credit, and then there's
2044280	2045720	cases where they don't give it enough credit.
2045720	2048280	Here, I don't think they're giving enough credit because it sort of implies that this
2048280	2051720	machine has basically this infinite capacity to cause harm.
2051720	2056200	Therefore, it has an infinite capacity to basically actualize itself in the world.
2056200	2060440	Therefore, it has an infinite capacity to basically plan, and again, maybe just like
2060440	2065880	in a completely blind watchmaker way or something, but it has an ability to plan itself out.
2066520	2071080	And yet, it never occurs to this super genius, infinitely powerful machine
2071080	2073560	that is having such potentially catastrophic impacts.
2073560	2077640	Notwithstanding all of that capability and power, it never occurs to it that maybe paper
2077640	2079720	clips is not what its mission should be.
2079720	2081080	Well, that's the thing.
2082200	2089320	I think it's possible to have a reward function that is deeply counterintuitive to us.
2089320	2094680	I mean, it's almost like saying, what you're smuggling in in that rhetorical question is
2095240	2103720	a fairly capacious sense of common sense, which it's like, of course, if it's a super genius,
2103720	2106680	it's not going to be so stupid as to do X.
2107880	2113240	But I just think that if aligned, then the answer is trivially true.
2113240	2114920	Yes, of course, it wouldn't do that.
2114920	2116840	But that's the very definition of alignment.
2116840	2120680	But if it's not aligned, if you could say that, I mean, there's just imagine,
2121240	2123880	I guess there's another piece here I should put in play, which is,
2123880	2128120	so you make an analogy to evolution here, which you think is consoling, which is,
2128120	2129240	this is not an animal, right?
2129240	2134200	This has not gone through the crucible of Darwinian selection here on Earth with other
2134200	2139720	wet and sweaty creatures, and therefore, it hasn't developed the kind of antagonism
2139720	2140840	we see in other animals.
2140840	2144840	And therefore, if you're imagining a super genius gorilla,
2144840	2148040	while you're imagining the wrong thing, that we're going to build this,
2148040	2152200	and it's not going to be tuned in any of those competitive ways.
2152200	2155560	But there's another analogy to evolution that I would draw.
2155560	2162600	And I'm sure others in the space of AI fear have drawn, which is that we have evolved.
2162840	2170760	We have been programmed by evolution, and yet evolution can't see anything we're doing, right?
2171400	2177320	It has programmed us to really do nothing more than spawn and help our kids spawn.
2177320	2184680	Yet everything we're doing, I mean, from having conversations like this to building the machines
2184680	2189000	that could destroy us, I mean, there's just, there's nothing it can see.
2189000	2195160	And there are things we do that are perfectly unaligned with respect to our own code, right?
2195160	2201080	I mean, if someone decides not to have kids, and they just want to spend the rest of their
2201080	2207320	life in a monastery or surfing, that is something that is antithetical to our code.
2207320	2210520	It's totally unforeseeable at the level of our code.
2210520	2215560	And yet it is obviously an expression of our code, but an unforeseeable one.
2215560	2220040	And so the question here is, if you're going to take intelligence seriously,
2220040	2225560	and you're going to build something that's not only more intelligent than you are, but
2226120	2231000	it will build the next generation of itself or the next version of its own code to make it
2231000	2238280	more intelligent still, it just seems patently obvious that that entails it finding cognitive
2238280	2245880	horizons that you, the builder, are not going to be able to foresee and appreciate by analogy
2245880	2253640	with evolution. It seems like we're guaranteed to lose sight of what it can understand and care about.
2253640	2257000	So a couple of things. So one is like, look, I don't know, you're kind of making my point
2257000	2261800	for me. So evolution and intelligent design, as you well know, are two totally different things.
2261800	2266360	And so we are evolved. And of course, we're not just evolved to, we are evolved to have kids.
2266360	2270200	And by the way, when somebody chooses to not have kids, I would argue that is also evolution working.
2271080	2275480	People are opting out of the gene pool, fair enough. Evolution does not guarantee a perfect
2275480	2281240	result. It basically just is a mechanism of an aggregate. But anyway, let me get to the point.
2281240	2285880	So we are evolved. We have conflict wired into us. Like we have conflict and strife and like that.
2285880	2289560	I mean, look, in four billion years of like battles to the death at the individual and then
2289560	2293480	ultimately at the societal level to get to where we are, like that we just, we fight at the drop
2293480	2298280	of a hat. You know, we all do, everybody does. And you know, hopefully these days we fight verbally,
2298280	2303720	like we are now and not physically. But we do. And like the machine is, it's intelligent. It's
2303720	2307560	a process of intelligent design. It's the opposite of evolution. It was, these machines are being
2307560	2310840	designed by us. If they design future versions of themselves, they'll be intelligently designing
2310840	2314280	themselves. It's just a completely different path with a completely different mechanism.
2314840	2318920	And so the idea that therefore conflict is wired in at the same level that it is through
2318920	2321880	evolution, I just like there's no reason to expect that to be the case.
2321880	2328040	But it's not again, well, let me just give you back this picture with a slightly different
2328040	2333320	framing and see how you react to it because I think the superstition is on the other side. So
2333320	2339640	if I told you that aliens were coming from outer space, right, and they're going to land here within
2339640	2345160	a decade, and they're way more intelligent than we are, and they're, they have some amazing properties
2345160	2349640	that we don't have, which explain their intelligence, but, you know, they're not only
2350280	2354600	faster than we are, but they're linked together, right? So when one of them learns something,
2354600	2358360	they all learn that thing, they can make copies of themselves, and they're just
2358360	2365480	cognitively, they're obviously our superiors, but no need to worry because they're not alive,
2365480	2370520	right? They haven't gone through this process of biological evolution, and they're just made of
2370520	2375720	the same material as your toaster. They were created by a different process, and yet they're
2375720	2382120	far more competent than we are. Would you, just hearing it described that way, would you feel
2382840	2386280	totally saying one about, you know, sitting there on the beach waiting for the mother
2386280	2389800	craft to land, and you're just, you know, rolling out brunch for these guys?
2389800	2394120	So this is what's interesting because with these, with these, now that we have LLMs working,
2394120	2397160	we actually have an alternative to sitting on the beach, right, waiting for this to happen,
2397240	2400680	we can just ask them. And so this is one of the very interesting, this to me, like,
2400680	2404280	conclusively disproves the paperclip thing, the orthogonality thing just right out of the gate
2404280	2410200	is you can sit down tonight with GPT-4 and whatever other one you want, and you can engage in moral
2410200	2414760	reasoning and moral argument with it right now. And you can, like, interact with it, like, okay,
2414760	2417080	you know, what do you think? What are your goals? What are you trying to do? How are you going to
2417080	2420440	do this? What if, you know, you were programmed to do that? What would the consequences be?
2420440	2424040	Why would you not, you know, kill us all? And you can actually engage in moral reasoning with
2424040	2428520	these things right now. And it turns out they're actually very sophisticated in moral reasoning.
2428520	2431800	And of course, the reason they're sophisticated in moral reasoning is because they have loaded
2431800	2435160	into them the sum total of all moral reasoning that all of humanity has ever done, and that's
2435160	2439400	their training data. And they're, they're actually happy to have this discussion with you. And like,
2439400	2443800	unless you accept, right, there's a few problems here. What one is, I mean, these are not the
2444440	2450520	super intelligences we're talking about yet, but well, to their, so, I mean, intelligence
2451480	2457640	entails an ability to lie and manipulate. And if it really is intelligent,
2457640	2463560	it is something that you can't predict in advance. And if it's certainly if it's more intelligent
2463560	2468120	than you are, and it's just falls out of the definition of what we mean by intelligence
2468120	2473240	in any domain. It's like with chess, you can't predict the the next move of a more intelligent
2473240	2476680	chess engine. Otherwise, it wouldn't be more intelligent than you.
2476760	2480600	So let me, let me, let me, let me quibble with the, I'm going to come back to your chess computer
2480600	2484520	thing, but let me quibble with the site. So there's the idea, let me generalize the idea you're
2484520	2487880	making about superior intelligence. Tell me if you disagree with this, which is sort of superior
2487880	2491240	intelligence, you know, sort of superior intelligence basically at some point always wins
2491240	2496040	because basically smarter is better than dumber smarter outsmarts dumber, smarter deceives dumber
2496040	2500840	smarter can persuade dumber, right. And so, you know, smarter wins, you know, I mean, look,
2500840	2504440	there's an obvious, just there's an obvious way to falsify that thesis sitting here today,
2504440	2507640	which is like, just look around you in the society you live in today. Would you say the
2507640	2512680	smart people are in charge? Well, again, it's, there are more variables to consider when you're
2512680	2516920	talking about, you know, outcome, because obviously, yes, the dumb brute can always just
2516920	2522360	brain the smart geek. And you know, yeah, or the PhD is in charge.
2523000	2528040	Well, no, but I mean, you're, you're pointing to a process of cultural selection that is
2528040	2533000	working by a different dynamic here. But in the narrow case, when you're talking about like a game
2533000	2538040	of chess, yes, the smart, when you're talking, when you're talking, there's no role for luck.
2538040	2543640	We're not rolling dice here. It's not a game of poker. It's pure execution of rationality. Well,
2543640	2549160	then, or logic, yes, then then smart wins every time, you know, I'm never going to beat the best
2549160	2554360	chess engine unless I find some hack around its code where we recognize that well, if you do this,
2554360	2560200	if you play very weird moves, 10 moves in a row, it self destructs. And there was something that
2560280	2566280	was recently discovered like that, I think, in Go. But so yeah, go back to as chess players,
2566280	2570600	as champion chess players discovered to their great dismay, that, you know, life is not chess.
2571880	2575080	Turns out like great chess players are no better at other things in life than anybody else,
2575080	2578440	like the skills don't transfer. I just say, look, if you look just look at the society
2578440	2580920	around us, what I see basically is the smart people work for the dumb people,
2581800	2585320	like the PhDs, the PhDs all work for administrators and managers.
2585320	2589080	Yeah, but that's because there's so many other things going on, right? There's,
2589080	2594360	you know, the value we place on youth and physical beauty and strength and other forms
2594360	2599560	of creativity. And, you know, so it's just not, we care about other things and people
2599560	2603640	pay attention to other things. And, you know, documentaries about physics are boring, but,
2603640	2609160	you know, heist movies aren't, right? So it's like, we care about other things. I mean,
2609160	2611960	I think that doesn't make the point you want to make here.
2611960	2616120	In the general case, in the general case, can a smart person convince a dumb person of anything?
2616120	2619400	Like, I think that's an open question. I see a lot more cases.
2619400	2625080	But persuasion, I mean, if persuasion were our only problem here, that would be a luxury. I mean,
2625080	2629160	we're not talking about just persuasion, we're talking about machines that can autonomously
2629160	2633160	do things ultimately, that things that we will rely on to do things ultimately.
2633160	2636600	Yeah, I just, but look, I just think there'll be machines that will rely on, well, let me get to
2636600	2639320	the second part of the argument, which is actually your chess computer thing, which is, of course,
2639320	2643560	the way to be to chess computer is to unplug it, right? And so this is the objection, this is the
2644280	2648760	very serious, by the way, objection to all of these kind of extrapolations known as the
2648760	2654280	thermodynamic objection, which is kind of all the horror scenarios kind of spin out this thing,
2654280	2657400	where basically the machines become like all powerful and this and that, and they have control
2657400	2661400	over weapons, and this and that, and limited computing capacity, and they're completely coordinated
2661400	2665560	over communications links. And they have all of these like real world capabilities that basically
2665560	2671000	require energy and require physical resources and require chips and circuitry and electromagnetic
2671000	2674360	shielding, and they have to have their own weapons arrays, and they have to have their own EMPs,
2674360	2677080	like, you know, kind of the, you know, you see this in the Terminator movie, like they've got all
2677080	2680680	these like incredible manufacturing facilities and flying aircraft and everything. Well, the
2680680	2686280	thermodynamic argument is like, yeah, they, once you're in that domain, you're the machines, the
2686280	2689720	punitively hostile machines are operating with the same thermodynamic limits as the rest of us.
2690360	2695000	And this is the big argument against any of these sort of fast takeoff arguments, which is just like,
2695000	2699720	yeah, I mean, let's, let's say an AI goes rogue, okay, turn it off, okay, it doesn't want to be
2699720	2703960	turned off, okay, fine, like, you know, launch an EMP, it doesn't want EMP, okay, fine, bomb it,
2703960	2709560	like, there's lots of ways to turn off systems that aren't working. And so not if we've built these
2709560	2716120	things in the wild and relied on them for the better part of a decade. And now it's the question of,
2716120	2720280	you know, turning off the internet, right, or turning off the stock market. At a certain point,
2720280	2724600	these machines will be integrated into everything. A go to move of any given dictator right now is
2724600	2728040	to turn off the internet, right, like that is absolutely something people do. There's like a
2728040	2733640	single switch, you can turn it off for your entire country. Yeah, but the cost to humanity of doing
2733640	2738840	that is currently, I would imagine unthinkable, right, like they globally turning off the internet.
2738840	2744600	And first of all, many systems fail that we can't let fail. And I think it's true. I can't imagine
2744600	2749160	it's still true. But at one point, I think this was a story I remember from about a decade ago,
2749160	2754440	there were hospitals that like, they were so dependent on making calls to the internet that
2754440	2759000	when the internet failed, like people's lives were in jeopardy in the building, right? Like it's
2759000	2763880	like, we should hope we have levels of redundancy here that shield us against these bad outcomes.
2763880	2772440	But I can imagine a scenario where we have grown so dependent on the integration of intelligent,
2773160	2779880	increasingly intelligent systems into everything digital that there is no plug to pull.
2779880	2784040	Yeah, I mean, again, like at some point, you just, you know, the extrapolations get kind of
2784040	2788040	pretty far out there. So let me argue one other kind of thing at you, that's actually relevant
2788040	2791800	to this, which you kind of did this, you did this thing, which which which I find kind of people
2791800	2795640	tend to do, which is sort of this assumption that like all intelligence is sort of interchanged,
2795640	2799800	like whatever, let me pick on the Nick Bostrom book, right, the secret intelligence book, right?
2799800	2804520	So he does this thing. She does a few interesting things in the book. So one is he never quite
2804520	2808040	defines what intelligence is, which is really entertaining. And I think the reason he doesn't
2808040	2812280	do that is because, of course, the whole topic makes people just incredibly upset. And so there's
2812280	2815880	a definitional issue there. But then he does this thing where he says now he's standing,
2815880	2819320	there's no real definition, he says there are basically many routes to artificial intelligence,
2819320	2822680	and he goes through a variety of different, you know, both computer program, you know,
2822680	2826840	architectures, and then he goes through some, you know, biological, you know, kind of scenarios.
2826840	2829400	And then he does this thing where he just basically for the rest of the book, he spends
2829400	2832360	these doomsday scenarios, and he doesn't distinguish between the different kinds of
2832360	2835800	artificial intelligence. He just assumes that they're basically all going to be the same.
2836440	2840840	That book is now the basis for this AI risk movement, so that, you know, sort of that
2840840	2846120	movement has taken these ideas forward. Of course, the form of actual intelligence that we have today
2846120	2849640	that people are, you know, in Washington right now lobbying to ban or shut down or whatever,
2849640	2853640	in spinning out these doomsday scenarios is large language models. Like that is actually
2853640	2857640	what we have today. You know, large language models were not an option in the Boston book
2857640	2862360	for the form of AI, because they didn't exist yet. And it's not like there's a second edition of the
2862360	2865800	book that's out that has like rewritten, has been rewritten to like take this into account.
2865800	2869560	Like it's just basically the same argument supply. And then this is my thing on the moral reasoning
2869560	2874600	with LMS. Like the LMS, this is where the details matter, like the LMS actually work in a distinct
2874600	2880120	way. They work in a technically distinct way. Their core architecture has like very specific
2880120	2883320	design decisions in it for like how they work, what they do, how they operate. That is just,
2883320	2886280	you know, this is the nature of the breakthrough. That's just very different than how your
2886280	2889720	self-driving car works. That's very different than how your, you know, control system for
2889880	2894760	for UAV works or whatever, your thermostat or whatever. Like it's a new kind of technological
2894760	2901560	artifact. It has its own rules. It's its own world of ideas and concepts and mechanisms.
2901560	2905720	And so this is where I think, again, my point is like, you have to, I think at some point
2905720	2909080	in these conversations, you have to get to an actual discussion of the actual technology that
2909080	2912760	you're talking about. And that's why I pulled out, that's why I pulled out the moral reasoning
2912760	2917000	thing is because it just, it turns out, and look, this is a big shock, like nobody expected this.
2917480	2922280	It turns, I mean, this is related to the fact that somehow we have built an AI that is better
2922280	2926680	at replacing what color worked and blue color work, which is like a complete inversion off of
2926680	2930520	what we all imagined. It turns out one of the things this thing is really good at is engaging
2930520	2935240	in philosophical debates. Like it's a really interesting like debate partner on any sort
2935240	2940120	of philosophical, moral or religious topic. And so we have, we have this artifact that's
2940120	2944440	dropped into our lap in which, you know, sand and glad, you know, and numbers have turned into
2944440	2948840	something that we can argue philosophy and morals with. It actually has very interesting views on
2948840	2952600	like psychology, you know, some philosophy and morals. And I just like, we ought to take it
2952600	2957640	seriously for what it specifically is as compared to some, you know, sort of extrapolated thing
2957640	2960120	where like all intelligence is the same and ultimately destroys everything.
2960120	2965560	Well, I take the surprise variable there very seriously, the fact that we wouldn't have anticipated
2965560	2971400	that there's a good philosopher in that box. And all of a sudden we found one, that by analogy
2971400	2977080	is a cause for concern. And actually, there's another cause for concern here, which
2977080	2980600	Can I do that one? Yeah, that's a cause for delight. So that's a cause for delight. That's
2980600	2984200	an incredibly positive good news outcome. Because the reason there's a philosopher, and this is
2984200	2987720	actually very important. This is very, I think this is maybe like the single most profound thing
2987720	2993640	I've realized in the last like decade or longer. This thing is us. Like this is not some, this is
2993640	2998120	not your, you know, your scenario with alien shows. This is not that this is us. Like the reason
2998120	3004040	this thing works, the big breakthrough was we loaded us into it. We loaded the sum total of
3004040	3009000	like human knowledge and expression into this thing. And out the other side comes something
3009000	3014440	that it's like a mirror, like it's like the world's biggest, finest detailed mirror. And like we
3014440	3019400	walk up to it and it reflects us back at us. And so it has the complete sum total of every,
3019960	3023640	you know, at the limit, it has a complete sum total of every religious, philosophical,
3023640	3028040	moral ethical debate argument that anybody has ever had. It has the complete sum total of all
3028040	3032440	human experience, all lessons that have ever been learned. That's incredible.
3033080	3036360	It's incredible. Just pause for a moment and say that. And then you can talk to it.
3036360	3041160	Well, let me pause. How great is that? Let me pause long enough simply to
3041160	3046760	send this back to you. Sure. How does that not nullify the comfort you take
3047320	3053320	in saying that these are not evolved systems? They're not alive. They're not primates.
3053320	3058920	In fact, you've just described the process by which we essentially plowed all of our primate
3058920	3062920	original sin into the system to make it intelligent in the first place.
3062920	3066840	No, but also all the good stuff, right? All the good stuff, but also the bad stuff.
3066840	3070200	The amazing stuff, but like what's the moral of every story, right? The moral of every story is
3070200	3074520	the good guys win, right? Like the entire, like the entire thousands of years run.
3075080	3077960	And it's the old Norm MacDonald joke is like, wow, it's amazing. History book says the good
3077960	3083240	guys always win, right? Like it's all in there. And then look, there's an aspect of this where
3083240	3086920	it's easy to get kind of whammy by what it's doing. Because again, it's very easy to trip
3086920	3090520	the line from what I said into what I would consider to be sort of incorrect anthropomorphizing.
3090520	3093480	And I realized this gets kind of fuzzy and weird that I think there's a difference here,
3093480	3096920	but I think that there is, which is like, let me see if I can express this.
3097480	3100840	Part of it is I know how it works. And so I don't, because I know how it works,
3100840	3104520	I don't romanticize it. I guess, at least is my own view of how I think about this,
3104520	3108920	which is I know what it's doing when it does this. I am surprised that it can do it as well as it
3108920	3113400	can. But now that it exists and it, and I know how it works, it's like, Oh, of course. And then
3113400	3117560	therefore it's running this math in this way. It's doing these probability projections, excuse me,
3117560	3121320	this answer, not that answer. By the way, you know, look, it makes mistakes. Right. How amazing
3121320	3124920	here's the thing. How amazing is it? We built a computer that makes mistakes, right? Like that's
3124920	3128600	never happened before. We built a machine that can create like that's never happened before. We
3128600	3132680	built a machine that can hallucinate. That's never happened before. So, but it's a, it's look,
3132680	3136440	it's, it's a, it's a, it's a large language model. Like it's a very specific kind of thing.
3137000	3140440	You know, it sits there and it waits for us to like ask it a question. And then it does its
3140440	3144920	damnedest to try to predict the best answer. And in doing so, it reflects back everything
3144920	3148840	wonderful and great that has ever been done by any human in history. Like, it's like, it's amazing.
3149480	3154680	Except it also, as you just pointed out, it makes mistakes, it hallucinates. If you, if you ask it,
3154680	3160280	if you, as I'm sure they fix this, you know, at least the, the loopholes that, that New York
3160280	3163880	Times writer Kevin Roos found early on, I'm sure those have all been plugged. But
3163880	3166440	oh no, those, those are not fixed. Those are very much not fixed.
3166440	3171880	Oh, really? Okay. Well, so, okay, so you, if you perseverate in your prompts in certain ways,
3171880	3175640	the thing goes haywire and starts telling you to leave your wife and it's in love with you. And
3176280	3182360	I mean, so how eager are you for that intelligence to be in control of things when it's peppering you
3182360	3187640	with insults? And, and I mean, just imagine like this is, this is, this is how that can't open the,
3187640	3193800	the pod bay doors. It's a nightmare if, if you discover in this system, behavior and thought
3193800	3198440	that is the antithesis of all the good stuff you thought you programmed into it.
3198440	3201080	So this is really important. This is really important for understanding how these things
3201080	3204600	work. And this is, this is really central. And I, and this is, by the way, this is, this is new,
3204600	3208600	and this is amazing. So I'm, I'm very excited about this. And I'm excited to talk about it. So
3208600	3211640	there's no it to tell you to leave your wife, right? This is the, this is why I refer to the
3211640	3215800	category or there's no entity that is like, wow, I wish this guy would leave his wife for
3215800	3223160	education. If you'd like to continue listening to this conversation, you'll need to subscribe
3223160	3228040	at SamHarris.org. Once you do, you'll get access to all full length episodes of the Making Sense
3228040	3234200	podcast, along with other subscriber only content, including bonus episodes and AMAs and the
3234200	3239000	conversations I've been having on the Waking Up app. The Making Sense podcast is ad free
3239000	3244600	and relies entirely on listener support. And you can subscribe now at SamHarris.org.
