WEBVTT

00:00.000 --> 00:12.200
Welcome to the Making Sense podcast. This is Sam Harris. Just a note to say that if

00:12.200 --> 00:16.400
you're hearing this, you are not currently on our subscriber feed and will only be hearing

00:16.400 --> 00:20.880
the first part of this conversation. In order to access full episodes of the Making Sense

00:20.880 --> 00:26.440
podcast, you'll need to subscribe at SamHarris.org. There you'll find our private RSS feed to

00:26.440 --> 00:31.080
add to your favorite podcatcher, along with other subscriber-only content. We don't run

00:31.080 --> 00:35.000
ads on the podcast, and therefore it's made possible entirely through the support of our

00:35.000 --> 00:38.840
subscribers. So if you enjoy what we're doing here, please consider becoming one.

00:46.600 --> 00:53.640
Okay, jumping right into it today. Today I'm speaking with Eric Schmidt. Eric is a technologist,

00:53.640 --> 01:02.200
entrepreneur, and philanthropist. He joined Google in 2001 and served as its CEO and chairman from

01:02.200 --> 01:11.080
2001 to 2011, and as executive chairman and technical advisor thereafter. In 2017, he co-founded

01:11.080 --> 01:16.280
Schmidt Futures, a philanthropic initiative that bets early on exceptional people who are helping

01:16.280 --> 01:21.960
to make the world better. He is the host of Reimagine with Eric Schmidt, his own podcast,

01:22.920 --> 01:28.520
and most recently, he's the co-author of a new book, The Age of AI and Our Human Future.

01:29.320 --> 01:35.720
And that is the topic of today's conversation. We cover how AI is affecting the foundations of

01:35.720 --> 01:41.400
our knowledge and how it raises questions of existential risk. So we talk about the good and

01:41.400 --> 01:49.400
the bad of AI, both narrow AI and ultimately, AGI, Artificial General Intelligence.

01:50.360 --> 01:56.440
We discuss breakthroughs in pharmaceuticals and other good things, but we also talk about

01:57.320 --> 02:04.040
cyber war and autonomous weapons and how our thinking about containing the risk here

02:04.040 --> 02:10.440
by analogy to the proliferation of nuclear weapons probably needs to be revised.

02:11.320 --> 02:14.840
Anyway, an important conversation, which I hope you find useful.

02:15.800 --> 02:17.560
And I bring you Eric Schmidt.

02:24.200 --> 02:27.000
I am here with Eric Schmidt. Eric, thanks for joining me.

02:27.560 --> 02:28.440
Glad to be with you.

02:30.440 --> 02:35.080
I think we have a hard hour here, so amazingly, that's a short podcast for me,

02:35.080 --> 02:39.800
so there's going to be a spirit of urgency hanging over the place, and we will be

02:40.440 --> 02:46.680
efficient in covering the fascinating book that you have written with Henry Kissinger and Daniel

02:46.680 --> 02:51.080
Huttenlocher. That's right. And Dr. Kissinger, of course, is the former Secretary of State,

02:51.080 --> 02:57.080
and Dan Huttenlocher is now the Dean of Artificial Intelligence and Computer Science

02:57.080 --> 03:01.000
at the Schwartzman Center at MIT. He's a proper computer scientist.

03:01.000 --> 03:06.360
Yeah, and that book is The Age of AI and Our Human Future, where you cover,

03:07.080 --> 03:13.640
most of what I have said about AI thus far and every case where I have worried about

03:13.640 --> 03:20.280
our possible AI future has been focused on the topic of AGI, Artificial General Intelligence,

03:20.280 --> 03:25.080
which you discuss briefly in the book, but it's not your main focus. So I thought maybe we could

03:25.080 --> 03:30.920
save that for the end, because I would love to get your take on AGI, but there are far more near

03:30.920 --> 03:38.200
term concerns here and considerations that we could cover, and you are quite well placed

03:38.200 --> 03:44.360
to cover them, because if I'm not mistaken, you ran Google for what was it, 10 years?

03:44.360 --> 03:50.280
That's correct. What was your background before that? How did you come to be the CEO of Google?

03:50.280 --> 03:56.440
Well, I'm a computer scientist. I have a PhD in the area, and I worked for 45 years in tech,

03:56.440 --> 04:00.680
in one way or the other, a whole bunch of companies. Larry and Sergey brought me in

04:00.680 --> 04:06.840
as the early CEO of the company, and we built it together. After a decade, I became chairman, Larry

04:06.840 --> 04:11.960
became CEO, and then he replaced himself with Sundar, who's now doing a phenomenal job at Google.

04:12.600 --> 04:17.320
So I'd say collectively, this group of which I'm a member built one of the great companies,

04:17.320 --> 04:23.960
and I'm really proud of that. Yeah, and obviously, Google is quite involved in developing AI. I just

04:23.960 --> 04:31.000
saw just the other day that there's a new, I think it's a 540 billion parameter language model

04:31.000 --> 04:38.760
that is beating the average human at something like 150 cognitive tests now, and it seems like

04:39.560 --> 04:42.760
the light is at the end of the tunnel there. I mean, it's just going to be a larger model

04:42.760 --> 04:48.120
that's going to beat every human at those same tasks. But before we get into some of the details

04:48.120 --> 04:54.920
here, I just wanted to organize our general approach to this. There are three questions that

04:54.920 --> 05:01.480
Kant asked, and his critique of pure reason, I think it was, which seem unusually relevant

05:02.120 --> 05:09.080
to the development of AI. The first is, what can we know? The second is, what should we do?

05:10.280 --> 05:17.720
And the third is, what is it reasonable to hope for? And I think those are really do capture

05:18.600 --> 05:25.960
almost every aspect of concern here, because as you point out in the book, AI really promises to,

05:25.960 --> 05:31.640
and it has already begun to shift the foundations of human knowledge. So the question of what we

05:31.640 --> 05:37.000
can know and how we can know it is enormously salient now, and maybe we can talk about some

05:37.000 --> 05:41.720
of those examples. But obviously, this question of what should we do and what can we reasonably

05:41.720 --> 05:49.320
hope for captures the risks we're running in developing these systems. And we're running

05:49.320 --> 05:55.720
these risks well short of producing anything like artificial general intelligence. And it's

05:55.720 --> 06:02.520
interesting that we're on a path now where we're really not free to decline to produce this technology.

06:02.520 --> 06:08.680
I mean, to my eye, there's really no break to pull. I mean, we're in a kind of AI arms race now.

06:09.400 --> 06:16.200
And the question is how to put that race for more intelligence on a footing that is not running

06:16.200 --> 06:22.360
cataclysmic risk for us. So before we jump into the details, I guess I'd like to get

06:22.360 --> 06:29.160
your general thoughts on how you view the stakes here and where you view the field to be at the

06:29.160 --> 06:35.560
moment. Well, of course, we wrote the book, Age of AI, precisely to help answer the questions

06:35.560 --> 06:42.200
you're describing, which are perfectly cast. And what's happened in the book, which is written

06:42.200 --> 06:48.600
roughly a year ago and then published, we described a number of examples to illustrate the point. One

06:48.600 --> 06:55.640
is the development of new moves in the game of Go, which is 2,500 years, which were discovered by

06:55.640 --> 07:01.240
computer, which humans had never discovered. It's hard to imagine that humans wouldn't have

07:01.240 --> 07:05.720
discovered these strategies, but they didn't. And that calls the question of are there

07:06.280 --> 07:13.400
things which AI can learn that humans cannot master? That's a question. The second example that we use

07:13.400 --> 07:19.000
is the development of a new drug called Hallison, which is a broad spectrum antibiotic, which could

07:19.000 --> 07:25.640
not be done by humans, but a set of neuroscientists, biologists, and computer scientists put together

07:25.720 --> 07:30.040
a set of programs that ultimately searched through 100 million different compounds

07:30.760 --> 07:36.200
and came up with candidates that were then subsequently tested, advancing drugs at an

07:36.200 --> 07:41.720
enormous rate. That's another category of success in AI. And then the third is what you've already

07:41.720 --> 07:46.920
mentioned, which is large language models. And we profile in the book, GPT-3, which is the

07:46.920 --> 07:53.000
predecessor of the one you described. And it's eerie. On the back cover of our book, we say,

07:53.000 --> 07:59.960
to the GPT-3 computer, are you capable of human reasoning? And it answers, oh, I am not.

08:01.000 --> 08:07.080
You may wonder why I give you that answer. And the answer is that you are a human reasoning

08:07.080 --> 08:13.560
machine, whereas I am a language model that's not been taught how to do that. Now, is that

08:13.560 --> 08:21.240
awareness or is that clever mimicry? We don't know. But each of these three examples show the

08:21.240 --> 08:27.960
potential to answer consquestions. What can we know? What will happen? What can we do about it?

08:27.960 --> 08:32.840
Since then, this past few weeks, we've seen the announcement that you mentioned of this

08:32.840 --> 08:39.160
enormous large language model, which can beat humans on many things. And we've also seen something

08:39.160 --> 08:44.600
called DAAL-E, which is a text to art program. You describe roughly what you want, and it can

08:44.600 --> 08:50.280
generate art for you. Now, these are the beginnings of the impact of artificial intelligence on us

08:50.280 --> 08:56.760
as humans. So Dr. Kissinger, Dan, and myself, when we looked at those, we thought, what happens to

08:56.760 --> 09:02.920
society when you have these kinds of intelligence? Now, they're not human intelligence. They're

09:02.920 --> 09:08.600
different kinds of intelligence in everyday life. And we talk about all the positives of which

09:08.600 --> 09:15.000
they're incredible positives, better materials, better drugs, more efficient systems, better

09:15.000 --> 09:20.520
understanding, better monitoring of the earth, additional solutions for climate change. There's

09:20.520 --> 09:25.800
a long, long list, which I can go through. Very, very exciting. And indeed, in my personal

09:25.800 --> 09:32.200
philanthropy, we are working really hard to fund AI-enabled science discoveries. We recently announced

09:32.760 --> 09:39.320
a grant, a structure with a guy named James Manika, who's a friend of mine, to $125 million

09:40.280 --> 09:47.320
to actually go and fund research on the really hard problems in AI, the ones that you're mentioning

09:47.320 --> 09:51.800
and others, and also the economic impacts and so forth. Because I think people don't really know.

09:51.800 --> 09:57.240
The real question is, what happens when these systems become more commonplace?

09:58.120 --> 10:03.800
Dr. Kissinger says, if you look at history, when a system that is not understandable

10:04.440 --> 10:10.120
is imposed on people, they do one of two things. They either invent it as a religion,

10:11.240 --> 10:17.880
or they fight it with guns. So my concern, and I'll say it very directly, is we're playing with

10:18.760 --> 10:25.800
the information space of humans. We're experimenting at scale without a set of principles

10:25.800 --> 10:30.840
as to what we want to do. Do we care more about freedom? Do we care more about efficiency? Do we

10:30.840 --> 10:37.400
care more about education and so forth? And Dr. Kissinger would say, the problem is that these

10:37.400 --> 10:42.680
decisions are being made by technical people who are ignorant of the philosophical questions that

10:42.680 --> 10:50.840
you so ably asked. And I agree with him, speaking as an example of that. So we recommend, and indeed

10:50.840 --> 10:57.480
I'm trying to now fund, that people begin in a multidisciplinary way to discuss the implications

10:57.480 --> 11:03.640
of this. What happens to national security? What happens to military intelligence? What happens to

11:03.640 --> 11:09.320
social media? What happens to your children when your child's best friend is a computer?

11:10.440 --> 11:17.000
And for the audience who might be still thinking about the killer robot, we're not building killer

11:17.000 --> 11:23.320
robots, and I hope we never do. This is really about information systems that are human-like,

11:23.400 --> 11:27.880
that are learning, they're dynamic, and they're emergent, and they're imprecise,

11:28.600 --> 11:37.080
being used and imposed on humans around the world. That process is unstoppable. It's simply too many

11:37.080 --> 11:42.040
people working on it, too many ways in which people are going to manipulate it, including

11:42.040 --> 11:48.200
for hostile reasons, too many businesses being built, and too much success for some of the early

11:48.200 --> 11:54.920
work. Yeah, I guess if I can just emphasize that point, the unstoppable is pretty interesting,

11:54.920 --> 12:02.520
because it's just anchored to this basic fact that intelligence is, almost by definition,

12:02.520 --> 12:08.920
the most valuable thing on earth. And if we can get more of it, we're going to. And we clearly can,

12:09.560 --> 12:15.320
and all of these narrow intelligences we've built thus far, all that are effective, that come to

12:15.400 --> 12:22.760
market that we pour resources into, are superhuman more or less right out of the gate. It's not a

12:22.760 --> 12:29.560
question of human level intelligence, it's a bit of a mirage, because the moment we get something

12:29.560 --> 12:34.920
that's general, it's going to be superhuman. And so we can leave the generality aside. All of these

12:34.920 --> 12:43.320
piecemeal intelligences are superhuman. And the example you give of the new antibiotic,

12:43.320 --> 12:50.520
Hallison, it's fascinating because it's not just a matter of doing human work faster. If I understand

12:50.520 --> 12:58.600
what happened in that case, this is an AI detecting patterns and relationships in molecules already

12:58.600 --> 13:07.880
known to be safe and efficacious as antibiotics, and detecting new properties that human beings

13:07.960 --> 13:15.400
very likely would never have conceived of and may in fact be opaque to the people who built the AI

13:15.400 --> 13:19.960
and may remain opaque. I mean, one of the issues you just raised is the issue of transparency.

13:20.520 --> 13:27.640
Many of these systems are built in such a way as to be black boxes. And we don't know how the AI is

13:27.640 --> 13:34.920
doing what it's doing in any specific way. It's just training against data and against its own

13:34.920 --> 13:41.720
performance so as to produce a better and better result, which qualifies as intelligent and even

13:41.720 --> 13:47.720
superhumanly so. And yet it may remain a black box. Maybe we can just close the loop on that

13:47.720 --> 13:57.720
specific problem here. Are you concerned that transparency is a necessity when decision making

13:57.720 --> 14:03.560
is important? I mean, just imagine the case where we have something like an AI oracle

14:04.280 --> 14:09.960
that we are convinced makes better decisions than any person or even any group of people,

14:10.600 --> 14:14.360
but we don't actually know the details of how it's making those decisions.

14:16.520 --> 14:22.280
I mean, you can just multiply examples as you like, but just questions of who should get out

14:22.280 --> 14:29.480
of prison, the likelihood of recidivism in the case of any person or who's likely to be more

14:29.480 --> 14:36.040
violent at the level of conviction. What should the prison sentence be? It's very easy to see that

14:36.040 --> 14:44.520
if we're shunting that to a black box, people are going to get fairly alarmed that in any

14:44.520 --> 14:50.520
differences in outcome that are not transparent. Perhaps you have other examples of concern,

14:50.520 --> 14:56.680
but do you think transparency is something that... I mean, one question is technically feasible to

14:56.680 --> 15:03.720
render black boxes transparent when it matters, and two, is transparency as important as we

15:03.720 --> 15:09.320
intuitively may think it is? Well, I wonder how important transparency is for the simple fact

15:09.320 --> 15:15.320
that we have teenagers among our midst and the teenagers cannot explain themselves at all,

15:16.040 --> 15:20.040
and yet we tolerate their behavior with some restrictions because they're not full adults.

15:20.440 --> 15:27.800
So, but we wouldn't let a teenager fly an airplane or operate on a patient. So, I think a pretty simple

15:27.800 --> 15:33.080
model is that at the moment, these systems cannot explain how they became to their decision. There

15:33.080 --> 15:38.280
are many people working on the explainability problem. Until then, I think it's going to be really

15:38.280 --> 15:44.040
important that these systems not be used in what I'm going to call life safety situations, and this

15:44.040 --> 15:49.240
creates all sorts of problems. For example, in automated war, automated conflict, cyber war,

15:49.320 --> 15:54.120
those sorts of things where the speed of decision making is faster than what humans can,

15:54.120 --> 16:00.200
what happens if it makes a mistake? And so, again, we're at the beginning of this process,

16:00.200 --> 16:05.480
and most people, including myself, believe that the explainability problem and the bias problems

16:06.040 --> 16:10.520
will get resolved because there's just too much money, too many people working on it.

16:10.520 --> 16:14.200
Maybe at some cost, but we'll get there. That's historically how these things work. You start

16:14.200 --> 16:18.280
off with stuff that works well enough, but it shows a hint of the future, and then it gets

16:18.280 --> 16:25.400
industrialized. I'm actually much more focused on what's it like to be human when you have these

16:25.400 --> 16:31.960
specialized systems floating around. My favorite example here is Facebook, where they change their

16:31.960 --> 16:40.200
feed to amp it using AI, and the AI that they built was around engagement, and we know from a

16:40.200 --> 16:44.680
great deal of social science that outrage creates more engagement, and so therefore,

16:44.680 --> 16:51.400
there's more outrage on your feed. Now, that was a clearly deliberate decision on part of Facebook,

16:51.400 --> 16:54.760
presumably thought it was a good product idea, but it also maximized their revenue.

16:55.400 --> 17:00.120
That's a pretty big social experiment, given the number of users that they have,

17:00.120 --> 17:05.640
which is not done with an understanding, in my view, of the impact of political polarization.

17:06.440 --> 17:10.280
Now, you sit there and you go, okay, well, he doesn't work at Facebook, he doesn't really

17:10.280 --> 17:17.720
understand, but many, many people have commented on this problem. This is an image of what happens

17:17.720 --> 17:24.280
in a world where all of the information around you can be boosted or manipulated by AI to sell

17:24.280 --> 17:29.560
to you, to anchor you, to change your opinion, and so forth. We're going to face some interesting

17:29.560 --> 17:36.040
questions in the information space that television and movies and things you see online, and so forth.

17:36.520 --> 17:41.960
Do there need to be restrictions on how AI uses the information it has about you

17:42.600 --> 17:47.880
to pitch to you, to market to you, to entertain you? These are questions we don't have answers,

17:48.600 --> 17:54.600
but it makes perfect sense that in the industrialization of these tools, the tools that I'm describing,

17:54.600 --> 17:59.480
which were invented in places like Google and Facebook, will become available to everyone

17:59.480 --> 18:07.000
and every government. Another example is a simple one, which is the kid is a two-year-old

18:07.000 --> 18:12.520
and gets a toy, and the toy gets upgraded every year and the kid gets smarter, the toy is now,

18:12.520 --> 18:18.760
the kid is now 12, and there's 10 years from now, there's a great toy, and this toy is smart enough

18:19.240 --> 18:26.760
in non-human terms to be able to watch television and decide if the kid likes the show. The toy

18:26.760 --> 18:31.640
is watching the television and the kid, the toy says to the kid, I don't like this show,

18:32.520 --> 18:38.120
knowing that the kid's not going to like it, and the kid goes, I agree with you. Now, is that okay?

18:38.920 --> 18:46.120
Probably. Well, what happens if that same system that's also learning learns something

18:46.120 --> 18:54.600
that's not true, and it goes, you know, kid, I have a secret, and the kid goes, tell me, tell me,

18:55.400 --> 18:59.320
and the secret is something which is prejudicial or false or bad or something like that.

19:00.040 --> 19:06.280
We don't know how to describe, especially for young people, the impact of these systems on their

19:06.280 --> 19:13.160
cognitive development. Now, we have a long history in America of having school boards and textbooks

19:13.160 --> 19:19.000
which are approved at the state level. Are the states going to monitor this? And you sit there

19:19.080 --> 19:24.600
and you say, well, no parent would allow that, but let's say that the normal behavior of this toy

19:24.600 --> 19:29.480
is smart enough, understands the kid well enough to know the kid's not good at multiplication.

19:30.280 --> 19:36.360
So the kid's bored, and the toy says, I think we should play a game. The kid goes great. And,

19:36.360 --> 19:41.960
of course, it's a game which strengthens his or her multiplication capability. So on the one hand,

19:41.960 --> 19:48.280
you want these systems to make people smarter, make them develop, make them more serious adults,

19:48.280 --> 19:53.000
make the adults more productive. Another example would be my physics friends. They just want a

19:53.000 --> 19:58.280
system to read all the physics books every night and make suggestions to them. Well, the physicists

19:58.280 --> 20:03.400
are adults who can deal with this. But what about kids? So you're going to end up in a situation,

20:03.400 --> 20:09.400
at least with kids and with elderly who are isolated, where these tools are going to have

20:09.400 --> 20:16.040
an out of proportion impact on society as they perceive it. We've never run that experiment,

20:16.040 --> 20:23.480
dynamic, emergent, and not precise. I'm not worried about airplanes being flown by AI because

20:23.480 --> 20:28.440
they're not going to be reliable enough to do it for a while. Now, we should also say for the

20:28.440 --> 20:34.520
listeners here that we're talking about a term which is generally known as narrow AI.

20:35.400 --> 20:41.240
It's very specific, and we're using specific examples, drug discovery, education,

20:41.320 --> 20:47.640
entertainment. But the eventual state of AI is called general intelligence, where you get

20:47.640 --> 20:54.760
human kind of reasoning. In the book, what we describe that as the point where the computer

20:54.760 --> 21:00.840
can set its own objective. And today, the good news is, the computer can't choose its objective.

21:02.200 --> 21:06.920
At some point, that will not be true. Yeah. Yeah. Well, hopefully we'll get to AGI

21:07.720 --> 21:15.080
at the end of this hour. But I think we should talk about the good and the bad in that order.

21:15.080 --> 21:20.440
And maybe just spend a few minutes on the good because the good is all too obvious. Again,

21:20.440 --> 21:27.000
and intelligence is the most valuable thing on earth. It's the thing that gives us every other

21:27.000 --> 21:33.320
thing we want. And it's the thing that safeguards everything we have. And if there are problems,

21:34.040 --> 21:38.360
we can't solve, well, then we can't solve them. But if there are problems that can be solved,

21:38.360 --> 21:45.480
the way we will solve them is through greater uses of our intelligence. And if, you know,

21:45.480 --> 21:51.080
insofar as we can leverage artificial intelligence to solve those problems, we will do that more

21:51.080 --> 21:56.520
or less regardless of the attendant risks. And that's the problem because the attendant risks are

21:57.080 --> 22:03.640
increasingly obvious and it seems not at all trivial. And what we've already proven, we're

22:03.640 --> 22:11.640
capable of implementing massive technological change without really thinking about the consequences

22:11.640 --> 22:17.640
at all. You cite the massive psychological experiment we've performed on all of humanity

22:17.640 --> 22:23.880
with no one really consenting that is social media. And it's, you know, the effects are

22:24.680 --> 22:29.560
ambiguous at best. I mean, there's some obviously bad effects. And it's not even

22:29.560 --> 22:35.960
straightforward to say that democracy or even civilization can survive contact with social

22:35.960 --> 22:41.880
media. I mean, that remains to be seen given how divisive some of its effects are. I consider

22:41.880 --> 22:49.160
social media to be far less alarming than the prospect of having an ongoing nuclear doctrine

22:49.800 --> 22:58.360
anchored to a proliferating regime of cyber espionage, cyber terrorism, cyber war,

22:59.000 --> 23:05.080
all of which will be improved massively by layering AI onto all of that. So

23:06.040 --> 23:10.040
before we jump into the bad, which is, you know, really capturing my attention,

23:10.920 --> 23:15.880
is there anything specifically you want to say about the good here? I mean, if this goes well,

23:16.760 --> 23:19.080
what are you hoping for? What are you expecting?

23:19.960 --> 23:25.240
Well, there are so many positive examples that we honestly just don't have time to make a list.

23:25.240 --> 23:31.960
We give you a few. In physics and math, the physicists and mathematicians have worked out

23:31.960 --> 23:37.720
the formulas for how the world works, at least at the scientific level. But many of their calculations

23:37.720 --> 23:44.200
are not computable by modern computers. They're just too complicated. An example is how do clouds

23:44.280 --> 23:49.400
actually work is a function of something called the Navier-Stokes equations, which for a normal

23:49.400 --> 23:55.720
sized cloud would take 100 million years for a computer to figure out. But using an AI system,

23:55.720 --> 24:01.000
and there's a group at Caltech doing this, they can come up with a simulation of the things that

24:01.000 --> 24:09.720
they care about. In other words, the AI provides enough accuracy in order to solve the more general

24:09.720 --> 24:16.360
climate modeling problem. If you look at quantum chemistry, which is sort of how does, how do chemical

24:16.360 --> 24:23.160
bonds work together? Not computable by modern methods. However, AI can provide enough of a

24:23.160 --> 24:28.760
simulation that we can figure out how these molecules bind, which is the Hallisyn example.

24:29.880 --> 24:37.160
In drug discovery, we know enough about biology that we can basically predict that if you do

24:37.800 --> 24:44.120
these compounds with, you know, this antibody, we can make it stronger, we can make it weaker,

24:44.120 --> 24:49.960
and so forth, in the computer, and then you go reproduce it in the lab. There's example after

24:49.960 --> 24:59.080
example, where AI is being used from existing data to simulate a non-computable function in science.

24:59.080 --> 25:03.800
And you say, what's he talking about? I'm talking about the fact that the scientists have been stuck

25:04.520 --> 25:09.560
for decades because they know what they want to do, but they couldn't get through this barrier.

25:10.280 --> 25:17.080
That unleashes new materials, new drugs, new forms of steel, new forms of concrete, and so forth and

25:17.080 --> 25:22.520
so on. It also helps us with climate change, for example, because climate change is really about

25:22.520 --> 25:27.240
energy and CO2 emission and so forth. These new surfaces, discoveries, and so forth will make a

25:27.240 --> 25:33.400
material difference. And I'm talking about really significant numbers. So that's an example. Another

25:33.400 --> 25:37.320
example is what's happening with these large language models that you mentioned earlier,

25:38.120 --> 25:41.880
that people are figuring out a way to put a conversational system in front of it so that

25:41.880 --> 25:46.760
you can talk to it. And the conversational system has enough state that it can remember what it's

25:46.760 --> 25:51.560
talking about. It's not like a question-answer-question-answer, and it doesn't remember. It actually

25:51.560 --> 25:55.720
remembers the context of, oh, we're talking about the Oscars, and we're talking about what happened

25:55.720 --> 26:01.160
at the Oscars, and what do I think? And then it sort of goes and it gives you a thoughtful answer

26:01.240 --> 26:06.760
as to what happened and what is possible. In my case, I was playing with one of them

26:07.880 --> 26:14.920
a few months ago, and this one, I asked the question, what is the device that's in 2001

26:14.920 --> 26:20.360
a space odyssey that I'm using today? There's something from 1969 that I'm using today that was

26:20.360 --> 26:26.600
foreshadowed in the movie, and it comes right back and says the iPad. Now, that's a question

26:26.600 --> 26:34.360
that Google won't answer if you ask it the way I did. So I believe that the biggest positive impact

26:35.160 --> 26:40.360
will be that you'll have a system that you can verbally or by writing ask it questions,

26:40.920 --> 26:47.000
and it will make you incredibly smarter. It'll give you the nuance and the understanding in the

26:47.000 --> 26:51.560
context, and you can ask it another question, and you can refine your question. Now, if you think

26:51.560 --> 26:56.760
about it in the work you do, or that I do, or that a scientist does, or a politician, or an artist,

26:56.760 --> 27:05.640
this is enormously transformative. So example after example, these systems are going to build

27:05.640 --> 27:11.160
scientific breakthroughs, scalable breakthroughs. Another example was that a group at DeepMind

27:11.960 --> 27:17.560
figured out the folding structure of proteins, and proteins are the way in which biology works,

27:17.560 --> 27:21.320
and the way they fold determines their effectiveness, what they actually do.

27:22.040 --> 27:27.000
And it was thought to be not really computable, and using these techniques in a very complicated way

27:27.000 --> 27:32.600
with a whole bunch of protein scientists, they managed to do it, and their result was replicated

27:32.600 --> 27:37.000
in a different mechanism with different AI from something called the Baker Lab in University of

27:37.000 --> 27:43.480
Washington. The two together have given us a map of how proteins work, which in my view is worthy

27:43.480 --> 27:48.440
of a Nobel Prize. That's how big a discovery that is. All of a sudden, we are unlocking the way

27:48.440 --> 27:54.040
biology works, and it affects us directly. But those are some positive examples. I think the

27:54.040 --> 28:01.160
negative examples... Well, let's wait because I'm chock full of negative examples. But I'm interested

28:01.160 --> 28:10.600
in how even the positive can disclose a surprisingly negative possibility, or at least it becomes

28:10.600 --> 28:17.960
negative if we haven't planned for it ethically, politically, economically. So you imagine this

28:17.960 --> 28:25.160
success, you imagine that more and more... So what you've just pictured was a future of machine and

28:25.160 --> 28:33.400
human cooperation and facilitation, where people just get smarter by being able to have access

28:33.400 --> 28:40.440
to these tools, or they get effectively smarter. But you can imagine just in the limit, more and

28:40.440 --> 28:46.200
more getting seated to AI, because AI is just better at doing these things. It's better at

28:46.200 --> 28:50.920
proving theorems. It's better at designing software. It's better. It's better. It's better. And

28:50.920 --> 28:56.920
all of a sudden, the need for human developers at all, or human mathematicians at all, or you

28:56.920 --> 29:06.440
just make the list as long as you want, it seems like some of the highest status jobs cognitively

29:07.240 --> 29:12.760
might be among the first to fall, which is to say, I certainly expect at this point

29:13.560 --> 29:23.000
to have an AI radiologist, certainly, before I have an AI plumber. And there's a lot more

29:23.640 --> 29:28.760
above and beyond the radiology side of that comparison that I think is going to fall before

29:29.720 --> 29:37.880
the basic manual tasks fall to robots. And this is a picture of real success, right? Because

29:38.520 --> 29:41.880
in the end, all we're going to care about is performance. We're not going to care about

29:42.600 --> 29:49.960
keeping a monkey in the loop just for reasons of sentimentality. If you're telling me that my car

29:49.960 --> 29:56.600
can drive a thousand times better than I can, which is to say that it's going to reduce my risk of

29:56.600 --> 29:59.800
getting in a fatal accident, you know, killing myself or killing someone else

30:00.440 --> 30:05.960
by a factor of a thousand, if I just flip on autopilot, well, then not only am I going to flip

30:05.960 --> 30:12.360
it on, I'm going to consider anyone who declines to do that to be negligent to the point of

30:12.360 --> 30:16.760
criminality. And that's never going to change. Everything is going to be in the position

30:17.480 --> 30:24.600
of a current chess master who knows that the best player on earth is never going to be a person

30:24.600 --> 30:32.040
ever again, right? Because of alpha zero. So I disagree a little bit, and I'll tell you why.

30:32.680 --> 30:38.200
I think you're correct in about 30 years, but I don't think that argument is true in the short term.

30:39.080 --> 30:43.240
Yeah, no, I was not just to be clear. I'm not suggesting any time frame there. I'm just saying

30:43.240 --> 30:49.720
ultimately, if we continue to make progress, something like this seems bound to happen.

30:49.720 --> 30:57.400
Yes, but what I want to say is, I defy you to argue with me that making people smarter

30:57.960 --> 31:06.120
is a bad thing. Okay, right. So let's start with the premise of the human assistant that is the

31:06.120 --> 31:13.720
thing that you're using will make humans smarter. It'll make it deeper, better analysis, better

31:13.720 --> 31:21.880
choices. But at least the current technology cannot replace the essentially the free will of

31:21.880 --> 31:26.920
humans. They sort of wake up in the morning, you have a new idea, you decide something, you say,

31:26.920 --> 31:32.120
that's a bad idea, so forth and so on. We don't know how to do that yet. And I have some speculation

31:32.120 --> 31:37.960
on how that will happen. But in the next decade, we're going to not be solving that problem,

31:37.960 --> 31:42.680
we'll be solving a different problem, which is how do we get the existing people doing existing

31:42.680 --> 31:50.200
jobs to do them more efficiently that is smarter, better, faster? When we looked at the funding

31:50.200 --> 31:55.880
for this AI program that I've since announced, the funding 125 million, a fair chunk of it is

31:55.880 --> 32:01.000
going to really hard computer science problems. Some of them include, we don't really understand

32:01.000 --> 32:06.280
how to explain what they're doing. As I mentioned, they're also brittle. When they fail, they can

32:06.280 --> 32:11.560
fail catastrophically, like why did it fail? And no one can explain. They're hardening,

32:11.560 --> 32:15.080
there are resistance to attack problems, there are a number of problems of this kind.

32:15.960 --> 32:18.760
These are hard computer science problems, which I think we will get through.

32:19.480 --> 32:24.120
They use a lot of power, the algorithms are expensive, that sort of thing. But we have also

32:24.120 --> 32:30.680
focusing around the impact on jobs and employment and economics. We're also focusing on national

32:30.680 --> 32:35.880
security. And we're focusing on the question that you're asking, which is, what's our identity?

32:35.880 --> 32:42.520
What does it mean to be human? Before general intelligence comes, we have to deal with the

32:42.520 --> 32:48.680
fact that these systems are not capable of choosing their own outcome, but they can be

32:48.680 --> 32:55.320
applied to you as a citizen by somebody else against your own satisfaction. So the negatives

32:55.320 --> 33:03.960
before AGI are all of the form, misinformation, misleading information, creating dangerous

33:03.960 --> 33:10.600
tools, and for example, dangerous viruses. For the same reason that we built a fantastic new

33:10.600 --> 33:17.560
antibiotic drug, it looks like you could also imagine a similar evil team producing an incredible

33:17.560 --> 33:22.520
number of bad viruses, things that would hurt people. And you could imagine in that scenario,

33:22.520 --> 33:27.240
they might be clever enough to be able to hurt a particular race or a particular sex or something

33:27.240 --> 33:32.600
like that, which would be totally evil and obviously a very bad thing. We don't have a way

33:32.600 --> 33:38.120
of discussing that today. So when I look at the positives and negatives right now, I think the

33:38.120 --> 33:44.680
positives, as with many technologies, really overwhelm the negatives, but the negatives need

33:44.680 --> 33:51.240
to be looked at. And we need to have the conversation right now about, let's use social media, which

33:51.240 --> 33:57.400
is an easy whipping boy here. I would like, so I'm clear what my political position is,

33:57.400 --> 34:04.760
I'm a very strong proponent of freedom of speech for humans. I am not in favor of freedom of speech

34:04.760 --> 34:11.000
for computers, robots, bots, so forth and so on. I want an option with social media, which says,

34:11.000 --> 34:17.480
I only want to see things that a human has actually communicated from themselves. I want to know that

34:17.480 --> 34:22.840
it wasn't snuck in by some Russian agent. I want proof of providence. And I want to know that it's

34:22.840 --> 34:29.080
a human. And if it's a real human who's, in fact, an idiot or crazy or whatever, I want to be able

34:29.080 --> 34:33.960
to hear their voice and I want to be able to decide I don't agree with it. What's happening instead

34:33.960 --> 34:41.720
is these systems are being boosted. They're being pitched. They're being sold by AI. And I think

34:41.720 --> 34:47.480
that's got to be limited in some way. I'm in favor of free speech, but I don't want only some people

34:47.480 --> 34:52.920
to have microphones. And if you talk to politicians and you look at the political structure in the

34:52.920 --> 35:00.600
country, this isn't of completely unintended effect of getting everyone wired. Now, is it a human?

35:00.600 --> 35:07.320
Or is it a computer? Is it a Russian? A Russian Compromat Plan? Or is it an American? Those things

35:07.320 --> 35:11.320
need to get resolved. You cannot run a democracy without some level of trust.

35:12.120 --> 35:19.000
Yeah. Well, let's take that piece here and obviously it extends beyond the problem of AI's

35:19.000 --> 35:25.800
involvement in it. But the misinformation problem is enormous. What are your thoughts about it?

35:25.800 --> 35:32.520
Because I'm just imagining we've been spared thus far the worst possible case of this, which is

35:32.520 --> 35:40.520
just imagine under conditions of where we had something like perfect deep fakes,

35:41.000 --> 35:46.680
that were truly difficult to tell apart from real video, what would the controversy around

35:46.680 --> 35:52.840
the 2020 election have looked like? Or the war in Ukraine and our dealings with Putin at this

35:52.840 --> 36:01.640
moment? Just imagine a perfect deep fake of Putin declaring a nuclear first strike on the U.S.

36:01.640 --> 36:08.360
or whatever. Just imagine essentially a writer's room from hell where you have smart creative

36:08.360 --> 36:16.760
people spending their waking hours figuring out how to produce media that is shattering to every

36:16.760 --> 36:25.720
open society and conducive to provoking international conflict. That is clearly coming in some form.

36:26.520 --> 36:32.920
I guess my first question is, are you hopeful that the moment that arrives, we will have the same

36:33.000 --> 36:38.920
level of technology that can spot deep fakes? Or is there going to be a lag there of

36:39.720 --> 36:43.000
months, years that are going to be difficult to navigate?

36:43.560 --> 36:48.680
We don't know. There are people working really hard on generating deep fakes and there are people

36:48.680 --> 36:54.760
working really hard on detecting deep fakes. And one of the general problems with misinformation

36:54.760 --> 37:00.920
is we don't have enough training data. The term here is in order to get an AI system to

37:00.920 --> 37:05.720
know something. You have to give it enough examples of good, bad, good, bad. And eventually you

37:05.720 --> 37:10.840
can say, oh, here's something new and I know if it's good or bad. And one of the core problems

37:10.840 --> 37:15.640
in misinformation is we don't have enough agreement on what is misinformation or what have you.

37:16.680 --> 37:22.520
The thought experiment I would offer is President Putin in Russia has already shut down the internet

37:22.520 --> 37:28.440
and free speech and controls the media and so forth. So let's imagine that he was furthering

37:28.440 --> 37:34.200
it. If you'd like to continue listening to this conversation, you'll need to subscribe

37:34.200 --> 37:39.080
at SamHarris.org. Once you do, you'll get access to all full-length episodes of the Making Sense

37:39.080 --> 37:45.000
podcast, along with other subscriber-only content, including bonus episodes and AMAs

37:45.000 --> 37:50.120
and the conversations I've been having on the Waking Up app. The Making Sense podcast is ad-free

37:50.120 --> 37:55.800
and relies entirely on listener support. And you can subscribe now at SamHarris.org.

