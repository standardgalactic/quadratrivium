1
00:00:00,000 --> 00:00:11,400
Welcome to the Making Sense podcast, this is Sam Harris.

2
00:00:11,400 --> 00:00:14,840
Just a note to say that if you're hearing this, you are not currently on our subscriber

3
00:00:14,840 --> 00:00:18,680
feed and will only be hearing the first part of this conversation.

4
00:00:18,680 --> 00:00:22,720
In order to access full episodes of the Making Sense podcast, you'll need to subscribe at

5
00:00:22,720 --> 00:00:24,600
SamHarris.org.

6
00:00:24,600 --> 00:00:28,400
There you'll find our private RSS feed to add to your favorite podcatcher, along with

7
00:00:28,400 --> 00:00:30,760
other subscriber-only content.

8
00:00:30,760 --> 00:00:34,760
We don't run ads on the podcast, and therefore it's made possible entirely through the support

9
00:00:34,760 --> 00:00:35,760
of our subscribers.

10
00:00:35,760 --> 00:00:47,080
So if you enjoy what we're doing here, please consider becoming one.

11
00:00:47,080 --> 00:00:51,400
Okay, well there's been a lot going on out there.

12
00:00:51,400 --> 00:00:58,320
Everything from Elon Musk and Mark Zuckerberg challenging one another to an MMA fight.

13
00:00:58,320 --> 00:01:01,720
Which is ridiculous and depressing.

14
00:01:01,720 --> 00:01:10,040
To Robert Kennedy Jr. appearing on every podcast on Earth, apart from this one, I have so far

15
00:01:10,040 --> 00:01:12,320
declined the privilege.

16
00:01:12,320 --> 00:01:14,000
It really is a mess out there.

17
00:01:14,000 --> 00:01:20,000
I'll probably discuss the RFK phenomenon in a future episode, because it reveals a lot

18
00:01:20,000 --> 00:01:24,240
about what's wrong with alternative media at the moment.

19
00:01:24,240 --> 00:01:29,600
But I will leave more of a post mortem on that for another time.

20
00:01:29,600 --> 00:01:32,120
Today I'm speaking with Mark Andreessen.

21
00:01:32,120 --> 00:01:36,760
Mark is a co-founder and general partner at the venture capital firm Andreessen Horowitz.

22
00:01:36,760 --> 00:01:39,160
He's a true internet pioneer.

23
00:01:39,160 --> 00:01:43,800
He created the mosaic internet browser, and then co-founded Netscape.

24
00:01:43,800 --> 00:01:47,880
He's co-founded other companies and invested in too many to count.

25
00:01:47,880 --> 00:01:51,520
Mark holds a degree in computer science from the University of Illinois, and he serves

26
00:01:51,520 --> 00:01:55,200
on the board of many Andreessen Horowitz portfolio companies.

27
00:01:55,200 --> 00:01:56,760
He's also on the board of META.

28
00:01:56,760 --> 00:02:02,240
Anyways, you'll hear Mark and I get into a fairly spirited debate about the future

29
00:02:02,240 --> 00:02:03,620
of AI.

30
00:02:03,620 --> 00:02:08,880
We discuss the importance of intelligence generally and the possible good outcomes of building

31
00:02:08,880 --> 00:02:14,080
AI, but then we get into our differences around the risks or lack thereof of building

32
00:02:14,080 --> 00:02:17,400
AGI, artificial general intelligence.

33
00:02:17,400 --> 00:02:22,360
We talk about the significance of evolution in our thinking about this, the alignment

34
00:02:22,360 --> 00:02:28,840
problem, the current state of large language models, how developments in AI might affect

35
00:02:28,840 --> 00:02:37,280
how we wage war, what to do about dangerous information, regulating AI, economic inequality,

36
00:02:37,280 --> 00:02:38,280
and other topics.

37
00:02:38,280 --> 00:02:41,300
Anyway, it's always great to speak with Mark.

38
00:02:41,300 --> 00:02:42,300
We had a lot of fun here.

39
00:02:42,300 --> 00:02:45,120
I hope you find it useful.

40
00:02:45,120 --> 00:02:53,120
Now I bring you Mark Andreessen.

41
00:02:53,120 --> 00:02:54,760
I am here with Mark Andreessen.

42
00:02:54,760 --> 00:02:56,880
Mark, thanks for joining me again.

43
00:02:56,880 --> 00:02:58,520
It's great to be here, Sam.

44
00:02:58,520 --> 00:02:59,520
Thanks.

45
00:02:59,520 --> 00:03:02,160
I got you on the end of a swallow of some delectable beverage.

46
00:03:02,160 --> 00:03:04,240
Yes, you did.

47
00:03:04,240 --> 00:03:06,240
So this should be interesting.

48
00:03:06,240 --> 00:03:14,040
I'm eager to speak with you specifically about this recent essay you wrote on AI.

49
00:03:15,040 --> 00:03:21,160
Many people have read this and you are a voice that many people value on this topic,

50
00:03:21,160 --> 00:03:22,160
among others.

51
00:03:22,160 --> 00:03:27,440
Perhaps you've been on the podcast before and people know who you are, but maybe you

52
00:03:27,440 --> 00:03:32,680
can briefly summarize how you come to this question.

53
00:03:32,680 --> 00:03:37,520
How would you summarize the relevant parts of your career with respect to the question

54
00:03:37,520 --> 00:03:40,240
of AI and its possible ramifications?

55
00:03:41,000 --> 00:03:46,560
I've been a computer programmer, technologist, computer scientist since the 1980s when I

56
00:03:46,560 --> 00:03:49,720
actually entered college in 1989 at University of Illinois.

57
00:03:49,720 --> 00:03:54,840
The AI field had been through a boom in the 80s, which had crashed hard.

58
00:03:54,840 --> 00:04:00,600
By the time I got to college, the AI wave was dead and buried at that point for a while.

59
00:04:00,600 --> 00:04:04,640
It was like the backwater of the department that nobody really wanted to talk about.

60
00:04:04,640 --> 00:04:07,640
I learned a lot of it in a school.

61
00:04:08,040 --> 00:04:13,960
I went on to help create what is now known as the modern internet in the 90s and then

62
00:04:13,960 --> 00:04:18,440
over time transitioned to become a, but for being a technologist, to being an entrepreneur.

63
00:04:18,440 --> 00:04:23,360
Then today I'm an investor venture capitalist, and so 30 years late, 30, 35 years later,

64
00:04:23,360 --> 00:04:29,520
I'm involved in a very broad cross-section of tech companies that have many of them have

65
00:04:29,520 --> 00:04:35,880
many AI aspects to them, and everything from Facebook now meta, which has been investing

66
00:04:35,920 --> 00:04:40,160
deeply in AI for over a decade, through to many of the best new AI startups.

67
00:04:40,160 --> 00:04:44,960
Our day job is to find the best new startups in a new category like this and try to back

68
00:04:44,960 --> 00:04:45,960
the entrepreneurs.

69
00:04:45,960 --> 00:04:49,200
That's how I spend most of my time right now.

70
00:04:49,200 --> 00:04:53,480
The essay is titled, Why AI Will Save the World?

71
00:04:53,480 --> 00:04:57,960
I think even in the title alone, people will detect that you are striking a different note

72
00:04:57,960 --> 00:05:00,680
than I tend to strike on this topic.

73
00:05:00,800 --> 00:05:07,200
I disagree with a few things in the essay that are, I think, at the core of my interest

74
00:05:07,200 --> 00:05:12,120
here, but I think there are many things we agree about.

75
00:05:12,120 --> 00:05:16,760
Up front, we agree, I think, with more or less anyone who thinks about it, that intelligence

76
00:05:16,760 --> 00:05:19,160
is good and we want more of it.

77
00:05:19,160 --> 00:05:24,560
If it's not necessarily the source of everything that's good in human life, it is what will

78
00:05:24,560 --> 00:05:27,680
safeguard everything that's good in human life.

79
00:05:27,680 --> 00:05:32,000
Even if you think that love is more important than intelligence and you think that playing

80
00:05:32,000 --> 00:05:36,480
on the beach with your kids is way better than doing science or anything else that is

81
00:05:36,480 --> 00:05:42,920
narrowly linked to intelligence, you have to admit that you value all of the things that

82
00:05:42,920 --> 00:05:46,680
intelligence will bring that will safeguard the things you value.

83
00:05:46,680 --> 00:05:51,520
A cure for cancer and a cure for Alzheimer's and a cure for a dozen other things will give

84
00:05:51,520 --> 00:05:54,360
you much more time with the people you love.

85
00:05:54,360 --> 00:06:00,240
Whether you think about the primacy of intelligence or not very much, it is the thing that has

86
00:06:00,240 --> 00:06:04,400
differentiated us from our primate cousins and it's the thing that allows us to do everything

87
00:06:04,400 --> 00:06:09,080
that is maintaining the status of civilization.

88
00:06:09,080 --> 00:06:12,760
If the future is going to be better than the past, it's going to be better because of what

89
00:06:12,760 --> 00:06:16,440
we've done with our intelligence in some basic sense.

90
00:06:16,440 --> 00:06:21,480
I think we're going to agree that because intelligence is so good and because each increment

91
00:06:21,600 --> 00:06:27,640
of it is good and profitable, this AI arms race and gold rush is not going to stop.

92
00:06:27,640 --> 00:06:31,840
We're not going to pull the brakes here and say, let's take a pause of five years and

93
00:06:31,840 --> 00:06:34,840
not build any AI.

94
00:06:34,840 --> 00:06:38,760
I don't remember if you address that specifically in your essay, but even if some people are

95
00:06:38,760 --> 00:06:41,800
calling for that, I don't think that's in the cards.

96
00:06:41,800 --> 00:06:44,320
I don't think you think that's in the cards.

97
00:06:44,320 --> 00:06:48,880
It's hard to believe that you just put in the box and stop working on it.

98
00:06:48,880 --> 00:06:50,480
It's hard to believe that progress stops.

99
00:06:50,920 --> 00:06:53,560
Having said that, there are some powerful and important people who are in Washington

100
00:06:53,560 --> 00:06:57,240
right now advocating that and there are some politicians who are taking them seriously.

101
00:06:57,240 --> 00:07:00,000
At the moment, there is some danger around that.

102
00:07:00,000 --> 00:07:03,600
Then look, there's two other big dangers, two other scenarios that I think would both

103
00:07:03,600 --> 00:07:06,280
be very devastating for the future.

104
00:07:06,280 --> 00:07:12,160
One is the scenario where the fears around AI are used to basically entrench a cartel.

105
00:07:12,160 --> 00:07:13,840
This is what's happening right now.

106
00:07:13,840 --> 00:07:16,240
This is what's being lobbied for right now is there are a set of big companies that are

107
00:07:16,240 --> 00:07:17,720
arguing in Washington.

108
00:07:17,760 --> 00:07:21,280
Yes, AI has positive cases, uses.

109
00:07:21,280 --> 00:07:23,000
Yes, AI is also dangerous.

110
00:07:23,000 --> 00:07:26,960
Because it's dangerous, therefore, we need a regulatory structure that basically entrenches

111
00:07:26,960 --> 00:07:31,880
a set of currently powerful tech companies to be able to have basically exclusive rights

112
00:07:31,880 --> 00:07:34,000
to do this technology.

113
00:07:34,000 --> 00:07:36,280
I think that would be devastating for reasons we could discuss.

114
00:07:36,280 --> 00:07:39,360
Then look, there's a third outcome, which is we lose China wins.

115
00:07:39,360 --> 00:07:43,880
They're certainly working on AI and they have a ... I would consider it to be a very dark

116
00:07:43,880 --> 00:07:47,200
and dystopian vision of the future, which I also do not want to win.

117
00:07:47,520 --> 00:07:52,920
I guess that is in part the cash value of the point I just made, that even if we decided

118
00:07:52,920 --> 00:07:55,480
to stop, not everyone's going to stop.

119
00:07:55,480 --> 00:08:00,400
Human beings are going to continue to grab as much intelligence as we can grab, even

120
00:08:00,400 --> 00:08:05,240
if in some local spot, we decide to pull the brakes.

121
00:08:05,240 --> 00:08:11,160
Although it really is, at this point, it's hard to imagine even whatever the regulation

122
00:08:11,160 --> 00:08:14,520
is, it really stalling progress.

123
00:08:15,000 --> 00:08:20,360
Again, given the intrinsic value of intelligence and given the excitement around it and given

124
00:08:20,360 --> 00:08:25,840
the obvious dollar signs that everyone is seeing, the incentives are such that I just

125
00:08:25,840 --> 00:08:26,840
don't see it.

126
00:08:26,840 --> 00:08:32,600
Well, it will come to the regulation piece eventually because I think it's ... Given

127
00:08:32,600 --> 00:08:36,800
the difference in our views here, it's not going to be a surprise that I want some form

128
00:08:36,800 --> 00:08:41,200
of regulation and I'm not quite sure what that could look like.

129
00:08:41,280 --> 00:08:46,280
I think you would have a better sense of what it looks like and perhaps that's why you're

130
00:08:46,280 --> 00:08:47,520
worried about it.

131
00:08:47,520 --> 00:08:54,920
But before we talk about the fears here, let's talk about the good outcome because you sketch

132
00:08:54,920 --> 00:08:59,680
a fairly ... I know you don't consider yourself a utopian, but you sketch a fairly utopian

133
00:08:59,680 --> 00:09:02,840
picture of promise in your essay.

134
00:09:02,840 --> 00:09:07,120
If we got this right, how good do you think it could be?

135
00:09:08,040 --> 00:09:11,720
Let's just start by saying I deliberately loaded my ... Let the title of the essay with

136
00:09:11,720 --> 00:09:14,960
a little bit of a religious element and I did that very deliberately because I view

137
00:09:14,960 --> 00:09:18,960
that I'm up against a religion, the AI risk fear religion.

138
00:09:18,960 --> 00:09:24,040
But I am not myself religious, lowercase our religious in the sense of, I'm not a utopian.

139
00:09:24,040 --> 00:09:28,200
I'm very much an adherent to what Thomas Sowell called the constrained vision, not the unconstrained

140
00:09:28,200 --> 00:09:29,200
vision.

141
00:09:29,200 --> 00:09:35,120
I live in a world of practicalities and trade-offs and so yeah, I am actually not utopian.

142
00:09:35,120 --> 00:09:38,680
Having said that, building on what you've already said, intelligence, if there is a

143
00:09:38,680 --> 00:09:43,080
lever for human progress across many thousands of domains simultaneously, it is intelligence

144
00:09:43,080 --> 00:09:47,560
and we know that because we have thousands of years of experience seeing that play out.

145
00:09:47,560 --> 00:09:49,840
The thing I would add to ... I thought you made that case very well.

146
00:09:49,840 --> 00:09:53,600
The thing I would add to the case you made about the positive virtues of intelligence

147
00:09:53,600 --> 00:09:58,120
in human life is that the way you described it, at least the way I heard it, was more

148
00:09:58,120 --> 00:10:02,280
focused on the social, societal wide benefits of intelligence, for example, cures for diseases

149
00:10:02,280 --> 00:10:03,640
and so forth.

150
00:10:03,640 --> 00:10:05,640
That is true and I agree with all that.

151
00:10:05,640 --> 00:10:09,360
There are also individual level benefits of intelligence at the level of an individual,

152
00:10:09,360 --> 00:10:12,600
even if you're not the scientist to invent secure for cancer at an individual level,

153
00:10:12,600 --> 00:10:17,360
if you are smarter, you have better life welfare outcomes on almost every metric that we know

154
00:10:17,360 --> 00:10:22,000
how to measure, everything from how long you'll live, how healthy you'll be, how much education

155
00:10:22,000 --> 00:10:25,240
you'll achieve, career success, the success of your children.

156
00:10:25,240 --> 00:10:29,340
By the way, your ability to solve problems, your ability to deal with conflict, smarter

157
00:10:29,340 --> 00:10:33,080
people are less violent, smarter people are less bigoted.

158
00:10:33,720 --> 00:10:39,000
There's this very broad pattern of human behavior where basically more intelligence, simply

159
00:10:39,000 --> 00:10:41,400
at the individual level, leads to better outcomes.

160
00:10:41,400 --> 00:10:47,280
The most utopian I'm willing to get is this potential, which I think is very real right

161
00:10:47,280 --> 00:10:48,280
now.

162
00:10:48,280 --> 00:10:51,760
It's already started where you basically just say, look, human beings from here on out

163
00:10:51,760 --> 00:10:57,000
are going to have an augmentation and the augmentation is going to be in the long tradition of augmentations

164
00:10:57,000 --> 00:11:01,040
like everything from eyeglasses to shoes to word processors to search engines, but now

165
00:11:01,160 --> 00:11:05,960
the augmentation is intelligence and that augmented intelligence capability is going

166
00:11:05,960 --> 00:11:11,560
to let them capture the gains of individual level intelligence, potentially considerably

167
00:11:11,560 --> 00:11:15,240
above where they punch in as individuals.

168
00:11:15,240 --> 00:11:19,440
What's interesting about that is that can scale all the way up.

169
00:11:19,440 --> 00:11:24,880
Somebody who struggles with daily challenges all of a sudden is going to have a partner

170
00:11:24,880 --> 00:11:25,880
and an assistant.

171
00:11:25,880 --> 00:11:29,800
I'm going to coach and a therapist and a mentor to be able to help improve a variety of things

172
00:11:29,800 --> 00:11:30,800
in their lives.

173
00:11:30,800 --> 00:11:35,320
Then look, if you had given this to Einstein, he would have been able to discover a lot

174
00:11:35,320 --> 00:11:41,680
more new fundamental laws of physics in the full vision.

175
00:11:41,680 --> 00:11:45,000
This is one of those things where it could help everybody and then it could help everybody

176
00:11:45,000 --> 00:11:46,800
in many, many different ways.

177
00:11:46,800 --> 00:11:47,800
Yeah.

178
00:11:47,800 --> 00:11:52,360
Well, see, in your essay, you go into some detail of bullet points around this concept

179
00:11:52,360 --> 00:11:58,000
of everyone having essentially a digital oracle in their pocket where you have this personal

180
00:11:58,040 --> 00:12:04,880
assistance who you can be continuously in dialogue with and it'd be like having the

181
00:12:04,880 --> 00:12:11,880
smartest person who's ever lived just giving you a bespoke concierge service to all manner

182
00:12:13,280 --> 00:12:18,080
of task and across any information landscape.

183
00:12:18,080 --> 00:12:24,480
I happened to recently rewatch the film Her, which I hadn't seen since it came out.

184
00:12:24,800 --> 00:12:29,120
It came out 10 years ago and I don't know if you've seen it lately, but I must say it lands

185
00:12:29,120 --> 00:12:33,720
a little bit differently now that we're on the cusp of this thing.

186
00:12:33,720 --> 00:12:40,720
And while it's not really dystopian, there is something a little uncanny and quasi bleak

187
00:12:42,320 --> 00:12:48,640
around even the happy vision here of having everyone siloed in their interaction with

188
00:12:48,640 --> 00:12:49,640
an AI.

189
00:12:50,240 --> 00:12:57,240
It's the personal assistant in your pocket that becomes so compelling and so aware of

190
00:12:57,760 --> 00:13:02,560
your goals and aspirations and what you did yesterday and the email you sent or forgot

191
00:13:02,560 --> 00:13:05,560
to send.

192
00:13:05,560 --> 00:13:11,200
Apart from the ending, which is clever and surprising and irrelevant for our purposes

193
00:13:11,200 --> 00:13:16,680
here, it's not an aspirational vision of the sort that you sketch in your essay.

194
00:13:16,720 --> 00:13:22,840
And I'm wondering if you see any possibility here that even the best case scenario has

195
00:13:22,840 --> 00:13:28,840
something intrinsically alienating and troublesome about it.

196
00:13:28,840 --> 00:13:32,640
Yeah, so look, on the movie, as Peter Thiele has pointed out, Hollywood no longer makes

197
00:13:32,640 --> 00:13:34,640
positive movies about technology.

198
00:13:34,640 --> 00:13:39,800
And then look, he argues this because they hate technology, but I would argue maybe a

199
00:13:39,800 --> 00:13:43,920
simpler explanation, which is they want dramatic tension and conflict.

200
00:13:43,960 --> 00:13:48,840
And so it's going to have things are going to have a dark tinge, regardless, they obviously

201
00:13:48,840 --> 00:13:52,200
spring load it by their choice of character and so forth.

202
00:13:52,200 --> 00:13:56,200
The scenario I have in mind is actually quite a bit different, and let me get kind of maybe

203
00:13:56,200 --> 00:13:59,520
philosophical for a second, which is there's this long running debate.

204
00:13:59,520 --> 00:14:03,240
This question that you just raised is a question that goes back to the Industrial Revolution.

205
00:14:03,240 --> 00:14:07,800
And remember, it goes back to the core of actually the original Marx's original theory.

206
00:14:07,800 --> 00:14:12,960
Marx's original theory was industrialization, technology, modern economic development, alienates

207
00:14:13,000 --> 00:14:15,040
the human being from society.

208
00:14:15,040 --> 00:14:17,440
That was his core indictment of technology.

209
00:14:17,440 --> 00:14:22,480
And look, you can point to many, many cases in which I think that has actually happened.

210
00:14:22,480 --> 00:14:24,440
Like I think alienation is a real problem.

211
00:14:24,440 --> 00:14:26,320
I don't think that critique was entirely wrong.

212
00:14:26,320 --> 00:14:29,560
His prescriptions were disastrous, but I don't think the critique was completely wrong.

213
00:14:29,560 --> 00:14:32,720
Look, having said that, then it's a question of like, OK, now that we have the technology

214
00:14:32,720 --> 00:14:36,160
that we have and we have new technology we can invent, how could we get to the other

215
00:14:36,160 --> 00:14:37,400
side of that problem?

216
00:14:37,400 --> 00:14:42,120
And so I would put the shoe on the other foot and I would say, look, the purpose of human

217
00:14:42,160 --> 00:14:46,880
existence and the way that we live our lives should be determined by us and it should be

218
00:14:46,880 --> 00:14:50,720
determined by us to maximize our potential as human beings.

219
00:14:50,720 --> 00:14:55,200
And the way to do that is precisely to have the machines do all the things that they can

220
00:14:55,200 --> 00:14:56,840
do so that we don't have to.

221
00:14:56,840 --> 00:15:00,760
Right. And this is why Marx ultimately, his critique was actually in the long run, I think

222
00:15:00,760 --> 00:15:03,880
has been judged to be incorrect, which is we are all much better.

223
00:15:03,880 --> 00:15:07,400
Anybody in the developed West, industrialized West today is much better off by the fact

224
00:15:07,400 --> 00:15:11,240
that we have all these machines that are doing everything from making shoes to harvesting

225
00:15:11,280 --> 00:15:14,680
corn to doing everything, you know, so many other, you know, industrial processes around

226
00:15:14,680 --> 00:15:19,040
us, like we just have a lot more time and much more pleasant, you know, day to day

227
00:15:19,040 --> 00:15:22,840
life, you know, than we would have if we were still doing things the way that things used

228
00:15:22,840 --> 00:15:23,840
to be done.

229
00:15:23,840 --> 00:15:27,400
The potential of the AI is just like, look, take, take, take, take the dredge workout,

230
00:15:27,400 --> 00:15:30,960
like take the remaining dredge workout, take all the, you know, look, like, I'll give you

231
00:15:30,960 --> 00:15:35,200
a simple example, office work, that, you know, the inbox staring at you in the face of 200

232
00:15:35,200 --> 00:15:40,040
emails, right, at Friday at three in the afternoon, like, OK, no more of that, right?

233
00:15:40,080 --> 00:15:42,240
We're not going to do that anymore, because I'm going to have an AI assistant.

234
00:15:42,240 --> 00:15:44,440
The AI assistant is going to answer the emails, right?

235
00:15:44,440 --> 00:15:47,480
And in fact, what's going to happen is my assistant is going to answer the email that

236
00:15:47,480 --> 00:15:49,480
your AI assistant set, right?

237
00:15:50,480 --> 00:15:52,040
It's mutually assured destruction.

238
00:15:52,120 --> 00:15:52,840
Yeah, exactly.

239
00:15:52,840 --> 00:15:54,640
But like, the machine should be doing that.

240
00:15:54,640 --> 00:15:57,880
Like the human being should not be sitting there when it's like sunny out and his like,

241
00:15:57,880 --> 00:16:00,600
you know, my, when my eight year old wants to play, I'm not, I shouldn't be sitting there

242
00:16:00,600 --> 00:16:02,880
doing emails, I should be at my eight year old, there should be a machine that does that

243
00:16:02,880 --> 00:16:07,720
for me. And so I view this very much as basically apply the machines to do the dredge

244
00:16:07,720 --> 00:16:10,000
work precisely so that people can live more human lives.

245
00:16:10,000 --> 00:16:11,800
Now, this is philosophical.

246
00:16:11,800 --> 00:16:13,800
People have to decide what kind of lives they want to live.

247
00:16:13,800 --> 00:16:15,360
And again, I'm not a utopian on this.

248
00:16:15,360 --> 00:16:17,840
And so there's a long discussion we could have about how this actually plays out.

249
00:16:18,040 --> 00:16:19,360
But that potential is there for sure.

250
00:16:19,680 --> 00:16:20,760
Right. Right.

251
00:16:20,760 --> 00:16:24,880
OK, so let's jump to the bad outcomes here, because this is really why I want to talk

252
00:16:24,880 --> 00:16:29,800
to you. In your essay, you list five and I'll just read your section titles here.

253
00:16:29,800 --> 00:16:32,040
And then we'll take a whack at them.

254
00:16:32,320 --> 00:16:35,440
The first is, will AI kill us all?

255
00:16:35,800 --> 00:16:38,320
Number two is, will AI ruin our society?

256
00:16:38,680 --> 00:16:41,360
Number three is, will AI take all our jobs?

257
00:16:41,960 --> 00:16:45,280
Number four is, will AI lead to crippling inequality?

258
00:16:45,880 --> 00:16:48,400
And five is, will AI lead to people doing bad things?

259
00:16:48,760 --> 00:16:51,760
And I would tend to bin those in really two buckets.

260
00:16:51,760 --> 00:16:53,880
The first is, will AI kill us?

261
00:16:53,960 --> 00:16:56,840
And that's the existential risk concern.

262
00:16:57,360 --> 00:17:02,600
And the others are more the ordinary bad outcomes that we tend to think about with

263
00:17:02,600 --> 00:17:06,800
other technology, with bad people doing bad things with powerful tools,

264
00:17:07,040 --> 00:17:11,480
unintended consequences, disruptions to the labor market, which I'm sure we'll

265
00:17:11,480 --> 00:17:15,960
talk about. And those are all of those are certainly the near term risks.

266
00:17:16,080 --> 00:17:21,000
And in some sense, even more interesting to people, because the existential

267
00:17:21,000 --> 00:17:25,160
risk component is longer term and it's even purely hypothetical.

268
00:17:25,160 --> 00:17:28,520
And you seem to think it's purely fictional.

269
00:17:29,120 --> 00:17:31,360
And this is where I think you and I disagree.

270
00:17:31,360 --> 00:17:35,640
So let's start with this question of, will AI kill us all?

271
00:17:36,080 --> 00:17:42,600
And the thinking on this tends to come under the banner of the problem of AI

272
00:17:42,600 --> 00:17:43,640
alignment, right?

273
00:17:43,640 --> 00:17:48,640
And the concern is that we can build, if we build machines more powerful than

274
00:17:48,640 --> 00:17:53,520
ourselves, more intelligent than ourselves, it seems possible that the space of

275
00:17:53,520 --> 00:17:57,680
all possible, more powerful, super intelligent machines includes many that

276
00:17:57,680 --> 00:18:03,040
are not aligned with our interests and not disposed to continually track our

277
00:18:03,040 --> 00:18:07,200
interests and many more of that sort than of the sort that perfectly hew to our

278
00:18:07,200 --> 00:18:09,160
interests in perpetuity.

279
00:18:09,560 --> 00:18:14,000
So the concern is we could build something powerful that is essentially an angry

280
00:18:14,000 --> 00:18:18,520
little God that we can't figure out how to placate once we've built it.

281
00:18:19,200 --> 00:18:21,680
And certainly we don't want to be negotiating with something more

282
00:18:21,680 --> 00:18:23,280
powerful and intelligent than ourselves.

283
00:18:23,840 --> 00:18:26,920
And the picture here is of something like, you know, a chess engine, right?

284
00:18:26,920 --> 00:18:31,120
We've built chess engines that are more powerful than we are at chess.

285
00:18:31,200 --> 00:18:36,560
And once we've built them, if everything depended on our beating them in a game

286
00:18:36,560 --> 00:18:38,720
of chess, we wouldn't be able to do it, right?

287
00:18:38,720 --> 00:18:40,960
Because they are simply better than we are.

288
00:18:40,960 --> 00:18:45,720
And so now we're building something that is a general intelligence and it will

289
00:18:45,720 --> 00:18:50,240
be better than we are at everything that goes by that name or such as the concern.

290
00:18:51,000 --> 00:18:55,000
And in your essay, I mean, I think there's an ad hominem piece that I think we

291
00:18:55,000 --> 00:19:00,840
should blow by because you've already described this as a religious concern.

292
00:19:00,840 --> 00:19:05,880
And in the essay, you describe it as just a symptom of superstition and that people

293
00:19:05,880 --> 00:19:08,200
are essentially in a new doomsday cult.

294
00:19:08,800 --> 00:19:13,080
And there's some share of troop believers here and there's some share of, you know,

295
00:19:13,080 --> 00:19:14,960
AI safety grifters.

296
00:19:15,440 --> 00:19:18,440
And I think, you know, I'm sure you're right about some of these people, but we

297
00:19:18,440 --> 00:19:24,760
should acknowledge upfront that there are many super qualified people of high

298
00:19:24,760 --> 00:19:29,960
probity who are prominent in the field of AI research who are part of this chorus

299
00:19:30,640 --> 00:19:32,080
voicing their concern now.

300
00:19:32,080 --> 00:19:35,720
And we've got somebody like Jeffrey Hinton, who arguably did as much as anyone

301
00:19:35,720 --> 00:19:39,200
to create the breakthroughs that have given us these these LLMs.

302
00:19:39,680 --> 00:19:44,240
We have Stuart Russell, who literally wrote the most popular textbook on AI.

303
00:19:44,560 --> 00:19:49,040
So there are other serious sober people who are very worried for reasons of a

304
00:19:49,040 --> 00:19:50,760
sort that I'm going to going to express here.

305
00:19:50,760 --> 00:19:55,080
So that's I mean, that's just I just want to acknowledge that both are true.

306
00:19:55,080 --> 00:20:00,760
There's the crazy people, the new millennialists, the doomsday preppers,

307
00:20:00,760 --> 00:20:05,760
the neuro atypical people who are in their polyamorous cults and, you know,

308
00:20:05,760 --> 00:20:08,160
AI alignment is their primary fetish.

309
00:20:08,680 --> 00:20:11,320
But there's a lot of sober people who are also worried about this.

310
00:20:11,320 --> 00:20:12,960
Would you would you acknowledge that much?

311
00:20:13,320 --> 00:20:17,840
Yeah, although it's tricky because smart people also have a tendency to fall into

312
00:20:17,840 --> 00:20:22,160
cults, so it doesn't get you totally off the hook on that one.

313
00:20:22,160 --> 00:20:25,400
But I would I would register a more fundamental objection to what I would

314
00:20:25,400 --> 00:20:28,920
describe as and this is not I'm not knocking you on this, but it's something

315
00:20:28,920 --> 00:20:31,120
that something that people do is sort of argument by authority.

316
00:20:31,640 --> 00:20:32,720
I don't think applies either.

317
00:20:33,000 --> 00:20:34,760
And yeah, well, I'm not making that yet.

318
00:20:35,360 --> 00:20:35,760
No, I know.

319
00:20:35,760 --> 00:20:37,760
But like this idea, this idea, which is very good.

320
00:20:37,760 --> 00:20:39,120
Again, I'm not characterizing your idea.

321
00:20:39,120 --> 00:20:39,960
I'll just say it's a general idea.

322
00:20:39,960 --> 00:20:43,640
This general idea that there are these experts and these experts are experts

323
00:20:43,640 --> 00:20:46,600
because they're the people who created the technology or originated the ideas

324
00:20:46,600 --> 00:20:50,240
or implemented the systems, therefore have sort of special knowledge and insight

325
00:20:50,240 --> 00:20:53,760
in terms of their downstream impact on society and rules and regulations

326
00:20:53,760 --> 00:20:54,880
and so forth and so on.

327
00:20:55,400 --> 00:20:57,960
That assumption does not hold up well historically.

328
00:20:58,440 --> 00:21:00,520
In fact, it holds up disastrously historically.

329
00:21:00,920 --> 00:21:01,760
There's actually a new book out.

330
00:21:01,760 --> 00:21:04,080
I've been giving all my friends called When Reason Goes on Holiday.

331
00:21:04,440 --> 00:21:07,840
And it's a story of literally what happens when basically people who are

332
00:21:07,880 --> 00:21:11,480
like specialized experts in one area stray outside of that area in order to

333
00:21:11,480 --> 00:21:14,440
become sort of general purpose philosophers and sort of social thinkers.

334
00:21:14,880 --> 00:21:16,760
And it's just a tale of woe, right?

335
00:21:16,920 --> 00:21:20,520
And in the 20th century, it was just a catastrophe.

336
00:21:20,520 --> 00:21:24,680
And the ultimate example of that, and this is going to be the topic of this big

337
00:21:24,680 --> 00:21:27,600
movie coming out this summer in Oppenheimer, the central example of that

338
00:21:27,600 --> 00:21:31,840
was the nuclear scientists who decided that nuclear power, nuclear energy,

339
00:21:31,840 --> 00:21:34,800
they had various theories on what was good, bad, whatever.

340
00:21:34,800 --> 00:21:35,800
A lot of them were communists.

341
00:21:36,080 --> 00:21:38,160
A lot of them were at least allied with communists.

342
00:21:38,640 --> 00:21:41,760
A lot of them had a suspiciously large number of communist friends and

343
00:21:41,760 --> 00:21:46,760
housemates and, you know, number one, like they, you know, made a moral decision.

344
00:21:46,760 --> 00:21:50,000
A number of them did to hand the bomb to the Soviet Union, you know,

345
00:21:50,000 --> 00:21:52,240
with what I would argue are catastrophic consequences.

346
00:21:52,240 --> 00:21:55,680
And then two is they created an anti-nuclear movement that resulted in

347
00:21:55,800 --> 00:21:59,120
nuclear energy stalling out in the West, which has also just been like

348
00:21:59,120 --> 00:22:00,120
absolutely catastrophic.

349
00:22:00,120 --> 00:22:03,720
And so if you listen to those people in that era who were, you know, the

350
00:22:03,720 --> 00:22:07,200
top nuclear physicists of their time, you made a horrible set of decisions.

351
00:22:07,600 --> 00:22:10,120
And quite honestly, I think that's what's happening here again.

352
00:22:10,120 --> 00:22:13,640
And I just, I don't think they have the special insight that people think that they have.

353
00:22:13,880 --> 00:22:17,840
OK, well, so I mean, this cuts both ways because, you know, at the beginning,

354
00:22:17,840 --> 00:22:22,520
I'm definitely not making an argument from authority, but authority is a proxy

355
00:22:22,520 --> 00:22:26,640
for understanding the facts at issue, right?

356
00:22:26,640 --> 00:22:31,480
It's not to say that in the cases you're describing, what we often have are

357
00:22:31,800 --> 00:22:36,520
people who have a narrow authority in some area of scientific specialization.

358
00:22:36,520 --> 00:22:42,280
And then they begin to weigh in, in a much broader sense, as moral philosophers.

359
00:22:42,520 --> 00:22:45,000
What I think you might be referring to there is that, you know, in the aftermath

360
00:22:45,000 --> 00:22:51,280
of Hiroshima and Nagasaki, we've got nuclear physicists imagining that, you

361
00:22:51,280 --> 00:22:54,440
know, that they now need to play the geopolitical game.

362
00:22:54,720 --> 00:22:57,520
You know, and we actually, we have some of the people who invented game theory,

363
00:22:57,520 --> 00:23:00,720
right, you know, for understandable reasons, thinking they need to play

364
00:23:00,720 --> 00:23:02,200
the game of geopolitics.

365
00:23:02,200 --> 00:23:06,200
And in some cases, I think in von Neumann's case, he even recommended

366
00:23:06,200 --> 00:23:09,680
a preventative war against the Soviet Union before they even got the bomb, right?

367
00:23:09,680 --> 00:23:10,880
Like, it could have gotten worse.

368
00:23:10,880 --> 00:23:15,040
He could have, I think he wanted us to bomb Moscow or at least give them

369
00:23:15,040 --> 00:23:16,160
some kind of ultimatum.

370
00:23:16,160 --> 00:23:19,160
I think it wasn't, I don't think he wanted us to drop bombs in the dead of

371
00:23:19,160 --> 00:23:23,160
night, but I think he wanted a strong ultimatum game played with them

372
00:23:23,160 --> 00:23:24,480
before they got the bomb.

373
00:23:24,480 --> 00:23:26,520
And I forget how he wanted that to play out.

374
00:23:26,880 --> 00:23:30,960
And worse still, I even, I think Bertrand Russell, I could have this backwards.

375
00:23:30,960 --> 00:23:34,800
Maybe von Neumann wanted a bomb, but Bertrand Russell, you know, a true

376
00:23:34,800 --> 00:23:37,840
moral philosopher briefly advocated preventative war.

377
00:23:38,240 --> 00:23:43,000
But in his case, I think he wanted to offer some kind of ultimatum to the Soviets.

378
00:23:43,320 --> 00:23:45,000
In any case, that's the problem.

379
00:23:45,000 --> 00:23:48,480
But, you know, at the beginning of this conversation, I asked you to give me a

380
00:23:48,720 --> 00:23:52,880
brief litany of your bona fides to have this conversation so as to inspire

381
00:23:52,880 --> 00:23:57,520
confidence in our audience and also just to acknowledge the obvious that you know

382
00:23:57,520 --> 00:24:01,000
a hell of a lot about the technological issues we're going to talk about.

383
00:24:01,440 --> 00:24:04,040
And so if you have strong opinions, they're not, you know, they're not

384
00:24:04,040 --> 00:24:06,080
coming out of totally out of left field.

385
00:24:06,840 --> 00:24:09,960
And so it would be with, you know, Jeffrey Hinton or anyone else.

386
00:24:09,960 --> 00:24:14,800
And if I threw another name at you that was of some, you know, crackpot whose

387
00:24:14,800 --> 00:24:19,040
connection to the field was non-existent, you would say, why should we listen

388
00:24:19,040 --> 00:24:19,920
to this person at all?

389
00:24:19,920 --> 00:24:23,200
You wouldn't say that about Hinton or Stuart Russell.

390
00:24:23,480 --> 00:24:27,440
But I would acknowledge that where authority breaks down is really you're

391
00:24:27,440 --> 00:24:29,920
only as good as your last sentence here, right?

392
00:24:29,920 --> 00:24:32,840
If the thing you just said doesn't make any sense, well, then your

393
00:24:32,840 --> 00:24:34,760
authority gets you exactly nowhere, right?

394
00:24:34,760 --> 00:24:37,280
We just need to keep talking about why it doesn't make sense.

395
00:24:37,280 --> 00:24:38,360
Or it should, or it should, right?

396
00:24:38,360 --> 00:24:39,840
That ideally that's the case in practice.

397
00:24:39,840 --> 00:24:41,520
That's not what tends to happen, but that would be the goal.

398
00:24:41,680 --> 00:24:45,560
Well, I hope to give you that treatment here because some of your sentences, I

399
00:24:45,560 --> 00:24:47,640
don't think add up the way you think they do.

400
00:24:48,320 --> 00:24:48,520
Good.

401
00:24:48,520 --> 00:24:52,320
Okay, so actually, there's actually one paragraph in the essay that caught my

402
00:24:52,320 --> 00:24:54,960
attention that really inspired this conversation.

403
00:24:54,960 --> 00:24:57,920
I'll just read it so people know what I'm responding to here.

404
00:24:58,280 --> 00:24:59,240
So this is you.

405
00:24:59,840 --> 00:25:04,480
My view is that the idea that AI will decide to literally kill humanity is a

406
00:25:04,480 --> 00:25:05,840
profound category error.

407
00:25:06,280 --> 00:25:10,360
AI is not a living being that has been primed by billions of years of evolution

408
00:25:10,360 --> 00:25:15,720
to participate in the battle for survival of the fittest, as animals were and as we are.

409
00:25:16,080 --> 00:25:21,440
It is math, code, computers built by people, owned by people, used by people,

410
00:25:21,480 --> 00:25:22,680
controlled by people.

411
00:25:23,120 --> 00:25:26,960
The idea that it will at some point develop a mind of its own and decide that

412
00:25:26,960 --> 00:25:31,680
it has motivations that lead it to try to kill us is a superstitious hand wave.

413
00:25:32,080 --> 00:25:35,600
In short, AI doesn't want, it doesn't have goals.

414
00:25:35,960 --> 00:25:38,560
It doesn't want to kill you because it's not alive.

415
00:25:39,080 --> 00:25:40,160
AI is a machine.

416
00:25:40,280 --> 00:25:43,800
It's not going to come alive any more than your toaster will, end quote.

417
00:25:44,600 --> 00:25:44,840
Yes.

418
00:25:44,880 --> 00:25:47,200
So, I mean, I see where you're going there.

419
00:25:47,200 --> 00:25:53,600
I see why that may sound persuasive to people, but to my eye, that doesn't even

420
00:25:53,640 --> 00:25:57,240
make contact with the real concern about alignment.

421
00:25:57,600 --> 00:26:01,880
So, let me just kind of spell out why I think that's the case, because it seems to

422
00:26:01,880 --> 00:26:07,400
me that you're actually not taking intelligence seriously, right?

423
00:26:07,400 --> 00:26:12,080
Now, I mean, some people assume that as intelligent scales, we're going to magically

424
00:26:12,080 --> 00:26:14,040
get ethics along with it, right?

425
00:26:14,040 --> 00:26:15,880
So, the smarter you get, the nicer you get.

426
00:26:16,200 --> 00:26:20,440
And while, I mean, there's some data points with respect to how humans behave.

427
00:26:20,480 --> 00:26:24,040
You know, you just mentioned one in a few minutes ago.

428
00:26:24,400 --> 00:26:26,440
It's not strictly true even for humans.

429
00:26:26,440 --> 00:26:31,440
And even if it's true in the limit, right, it's not necessarily locally true.

430
00:26:31,760 --> 00:26:36,920
And more important, when you're looking across species, differences in

431
00:26:36,920 --> 00:26:41,400
intelligence are intrinsically dangerous for the stupider species.

432
00:26:41,880 --> 00:26:45,800
Yeah, so it need not be a matter of super intelligent machines spontaneously becoming

433
00:26:45,800 --> 00:26:48,120
hostile to us and wanting to kill us.

434
00:26:48,760 --> 00:26:52,720
It could just be that they begin doing things that are not in our well-being,

435
00:26:52,720 --> 00:26:56,920
right, because they're not taking it into account as a primary concern in the same

436
00:26:56,920 --> 00:27:01,920
way that we don't take the welfare of insects into account as a primary concern,

437
00:27:01,920 --> 00:27:02,080
right?

438
00:27:02,080 --> 00:27:08,400
So, it's very rare that I intend to kill an insect, but I regularly do things

439
00:27:08,400 --> 00:27:12,080
that annihilate them just because I'm not thinking about them, right?

440
00:27:12,080 --> 00:27:15,320
I'm sure I've effectively killed millions of insects, right?

441
00:27:15,440 --> 00:27:19,200
If you build a house, you know, that must be a holocaust for insects.

442
00:27:19,480 --> 00:27:22,080
And yet you're not thinking about insects when you're building that house.

443
00:27:22,520 --> 00:27:27,840
So there are many other pieces to my gripe here, but let's just take this first one.

444
00:27:28,200 --> 00:27:32,360
It just seems to me that you're not envisioning what it will mean to be in

445
00:27:32,360 --> 00:27:36,120
relationship to systems that are more intelligent than we are.

446
00:27:36,120 --> 00:27:38,840
You're not seeing it as a relationship.

447
00:27:38,960 --> 00:27:44,720
And I think that's because you're denuding intelligence of certain

448
00:27:44,720 --> 00:27:48,120
properties and not acknowledging it in this paragraph, right?

449
00:27:48,120 --> 00:27:52,320
I mean, so to my ear, general intelligence, which is what we're talking about,

450
00:27:52,680 --> 00:27:56,520
implies many things that are not in this paragraph.

451
00:27:56,520 --> 00:27:59,000
Like, it implies autonomy, right?

452
00:27:59,000 --> 00:28:05,040
And it implies the ability to form unforeseeable new goals, right?

453
00:28:05,040 --> 00:28:09,000
In the case of AI, it implies the ability to change its own code ultimately

454
00:28:09,440 --> 00:28:11,360
and execute programs, right?

455
00:28:11,360 --> 00:28:17,040
I mean, it's just it's doing stuff because it is intelligent, autonomously intelligent.

456
00:28:17,360 --> 00:28:21,960
It is capable of doing just we can stipulate more than we're capable of doing

457
00:28:21,960 --> 00:28:24,520
because it is more intelligent than we are at this point.

458
00:28:24,920 --> 00:28:30,280
So the superstitious hand waving I'm seeing is in your paragraph when you're

459
00:28:30,280 --> 00:28:34,640
declaring that it would never do this because it's not alive, right?

460
00:28:34,640 --> 00:28:38,680
As though the difference between biological and non-biological substrate

461
00:28:39,080 --> 00:28:40,440
were the crucial variable here.

462
00:28:40,440 --> 00:28:44,520
But there's no reason to think as a crucial variable where intelligence is concerned.

463
00:28:44,880 --> 00:28:47,680
Yeah. So I would say there's to steal man, your argument, I would say you can

464
00:28:47,680 --> 00:28:51,240
actually break your argument into two forms or the AI risk community would break

465
00:28:51,240 --> 00:28:52,680
this argument into two forms.

466
00:28:52,680 --> 00:28:55,280
So they would argue and they would argue, I think the strong form of both.

467
00:28:55,280 --> 00:28:57,120
So they would argue the strong form of number one.

468
00:28:57,680 --> 00:28:59,960
And I think this is kind of what you're saying, correct me if I'm wrong,

469
00:28:59,960 --> 00:29:02,960
is because it is intelligent, therefore it will have goals.

470
00:29:03,480 --> 00:29:05,880
If it didn't start with goals, it will evolve goals.

471
00:29:05,920 --> 00:29:09,120
It will, you know, whatever it will, it will over time have a set of preferred

472
00:29:09,120 --> 00:29:11,760
outcomes, behavior patterns that it will determine for itself.

473
00:29:12,120 --> 00:29:15,920
And then they also argue the other side of it, which is what's what they call

474
00:29:15,920 --> 00:29:21,400
the orthogonality argument, which is it's actually the it's another risk argument,

475
00:29:21,400 --> 00:29:22,920
but it's actually sort of the opposite argument.

476
00:29:23,320 --> 00:29:26,320
It's an argument that it doesn't have to have goals to be dangerous, right?

477
00:29:26,520 --> 00:29:28,920
And that being, you know, it doesn't have to be sentient.

478
00:29:28,920 --> 00:29:30,120
It doesn't have to be conscious.

479
00:29:30,120 --> 00:29:31,760
It doesn't have to be self aware.

480
00:29:31,760 --> 00:29:33,440
It doesn't have to be self interested.

481
00:29:33,440 --> 00:29:36,760
It doesn't have to be in any way, like even thinking in terms of goals,

482
00:29:36,760 --> 00:29:39,720
it doesn't matter because simply it can just do things.

483
00:29:39,720 --> 00:29:42,560
And this is the, you know, this is the classic paperclip maximizer, you know,

484
00:29:42,560 --> 00:29:45,560
kind of argument, like it, it'll just get, it'll, it'll start, it'll get kicked off

485
00:29:45,560 --> 00:29:47,320
on one apparently innocuous thing.

486
00:29:47,640 --> 00:29:50,320
And then it will just extrapolate that ultimately to the destruction of everything.

487
00:29:50,320 --> 00:29:50,480
Right.

488
00:29:50,480 --> 00:29:52,680
So, so anyways, is that helpful to maybe break those into the.

489
00:29:52,760 --> 00:29:57,120
Yeah, I'm not quite sure how fully I would sign on the dotted line to each,

490
00:29:57,120 --> 00:30:03,200
but the one piece I would add to that is that having any goal does invite the

491
00:30:03,200 --> 00:30:05,760
formation of instrumental goals.

492
00:30:05,880 --> 00:30:10,600
Once this system is responding to a change in environment, right?

493
00:30:10,600 --> 00:30:16,160
I mean, if your goal is to make paperclips and you're super intelligent and

494
00:30:16,240 --> 00:30:19,680
somebody throws up at some kind of impediment, you're making paperclips,

495
00:30:19,680 --> 00:30:21,760
well, then you're responding to that impediment.

496
00:30:21,760 --> 00:30:25,000
And now you have a shorter term goal of dealing with the impediment, right?

497
00:30:25,000 --> 00:30:26,320
So that's the structure of the problem.

498
00:30:26,880 --> 00:30:27,320
Yeah, right.

499
00:30:27,360 --> 00:30:30,080
For example, the US military wants to stop you from making more paperclips.

500
00:30:30,080 --> 00:30:33,320
And so therefore you develop a new kind of nuclear weapon, right?

501
00:30:33,560 --> 00:30:36,280
You know, fundamentally to pursue your goal of making paperclips.

502
00:30:36,280 --> 00:30:40,720
But one problem here is that these, the instrumental goal, even if the paperclip

503
00:30:40,720 --> 00:30:45,520
goal is the wrong example here, because even if you think of a totally benign

504
00:30:46,000 --> 00:30:50,720
future goal, right, a goal that it seems more or less synonymous with taking human

505
00:30:50,720 --> 00:30:54,840
welfare into account, it's possible to imagine a scenario where some instrumental

506
00:30:54,840 --> 00:31:00,440
new goal that could not be foreseen appears that is in fact hostile to our interests.

507
00:31:00,440 --> 00:31:05,640
And if we're not in a position to say, oh, no, no, don't do that, that would be a problem.

508
00:31:05,840 --> 00:31:06,920
So that's the end, okay.

509
00:31:06,920 --> 00:31:10,040
So a full version of that, a version of that argument that you hear is basically

510
00:31:10,040 --> 00:31:12,920
the, what if the goal is maximize human happiness, right?

511
00:31:12,920 --> 00:31:16,120
And then the machine realizes that the way to maximize human happiness is to strap

512
00:31:16,120 --> 00:31:20,880
us all into, you know, right down and put us in a nosy experience machine, you know,

513
00:31:20,880 --> 00:31:23,080
and wire us up with, you know, VR and ketamine, right?

514
00:31:23,160 --> 00:31:25,160
And we, you know, we can never get out of the matrix, right?

515
00:31:25,160 --> 00:31:29,400
So, right, and it's be maximizing human happiness as measured by things like dopamine

516
00:31:29,400 --> 00:31:32,680
levels or serotonin levels or whatever, but obviously not a positive outcome.

517
00:31:32,680 --> 00:31:36,520
So, but again, that's like a variation of this paperclip, that's one of these

518
00:31:36,520 --> 00:31:39,400
arguments that comes out of their orthogonality thesis, which is the goal

519
00:31:39,400 --> 00:31:43,160
can be very simple and innocuous, and yet lead to catastrophe.

520
00:31:43,160 --> 00:31:46,680
So, look, I think each of these has their own problems.

521
00:31:46,680 --> 00:31:51,240
So where you started, where they're sort of like the machine, basically, you know,

522
00:31:51,240 --> 00:31:55,640
like, and we can quibble with terms here, but like some, like the side of the argument

523
00:31:55,640 --> 00:32:01,480
in which the machine is in some way self-interested, self-aware, self-motivated,

524
00:32:01,480 --> 00:32:07,240
trying to preserve itself, some level of sentience, consciousness, setting its own goals.

525
00:32:07,240 --> 00:32:10,600
Well, just to be clear, there's no consciousness implied here.

526
00:32:10,600 --> 00:32:13,080
I mean, the lights don't have to be on.

527
00:32:13,080 --> 00:32:18,040
It just, I think that, I mean, this remains to be seen whether consciousness comes along

528
00:32:18,040 --> 00:32:19,960
for the ride at a certain level of intelligence.

529
00:32:19,960 --> 00:32:23,000
But I think they probably are orthogonal to one another.

530
00:32:23,000 --> 00:32:27,240
So intelligence can scale without the lights coming on in my view.

531
00:32:27,240 --> 00:32:29,640
So let's leave sentience and consciousness aside.

532
00:32:29,640 --> 00:32:31,800
Well, I guess there is a fork in the road, which is like,

533
00:32:31,800 --> 00:32:33,800
is it declaring its own intentions?

534
00:32:33,800 --> 00:32:36,440
Like, is it developing its own, you know,

535
00:32:37,240 --> 00:32:43,560
conscious or not, does it have a sense of any form or a vision of any kind of its own future?

536
00:32:43,560 --> 00:32:47,080
Yeah. So this is why I think there's some daylight growing between us.

537
00:32:47,080 --> 00:32:54,120
Because to be dangerous, I don't think you need necessarily to be running a self-preservation

538
00:32:54,920 --> 00:33:03,160
program. I mean, there's some version of unaligned competence that may not formally model the machine's

539
00:33:03,160 --> 00:33:09,880
place in the world, much less defend that place, which could still be, if uncontrollable by us,

540
00:33:09,880 --> 00:33:11,480
could still be dangerous, right?

541
00:33:11,480 --> 00:33:15,720
It's like, it doesn't have to be self-referential in a way that an animal,

542
00:33:15,720 --> 00:33:19,240
the truth is, they're dangerous animals that might not even be self-referential.

543
00:33:19,240 --> 00:33:24,760
And certainly something like a virus or a bacterium, you know, is not self-referential

544
00:33:24,760 --> 00:33:28,840
in a way that we would understand, and it can be lethal to our interests.

545
00:33:28,840 --> 00:33:33,000
Yeah, yeah, that's right. Okay, so you're more on the orthogonality side between the two.

546
00:33:33,000 --> 00:33:37,000
If I identify the two poles of the argument, you're more on the orthogonality side,

547
00:33:37,000 --> 00:33:39,000
which is it doesn't need to be conscious, it doesn't need to be sentient,

548
00:33:39,000 --> 00:33:41,080
it doesn't need to have goals, it doesn't need to want to preserve itself.

549
00:33:41,640 --> 00:33:46,600
Nevertheless, it will still be dangerous because of the, as you described, the consequences of

550
00:33:46,600 --> 00:33:50,040
sort of how it gets started, and then sort of what happens over time.

551
00:33:50,040 --> 00:33:53,800
For example, as it defines some goals to the original goals, and it goes off course.

552
00:33:53,800 --> 00:33:56,440
Well, so there's a couple problems with that.

553
00:33:56,440 --> 00:34:01,560
So one is it assumes in here, it's like people don't give intelligence enough credit.

554
00:34:01,560 --> 00:34:04,280
Like there are cases where people give intelligence too much credit, and then there's

555
00:34:04,280 --> 00:34:05,720
cases where they don't give it enough credit.

556
00:34:05,720 --> 00:34:08,280
Here, I don't think they're giving enough credit because it sort of implies that this

557
00:34:08,280 --> 00:34:11,720
machine has basically this infinite capacity to cause harm.

558
00:34:11,720 --> 00:34:16,200
Therefore, it has an infinite capacity to basically actualize itself in the world.

559
00:34:16,200 --> 00:34:20,440
Therefore, it has an infinite capacity to basically plan, and again, maybe just like

560
00:34:20,440 --> 00:34:25,880
in a completely blind watchmaker way or something, but it has an ability to plan itself out.

561
00:34:26,520 --> 00:34:31,080
And yet, it never occurs to this super genius, infinitely powerful machine

562
00:34:31,080 --> 00:34:33,560
that is having such potentially catastrophic impacts.

563
00:34:33,560 --> 00:34:37,640
Notwithstanding all of that capability and power, it never occurs to it that maybe paper

564
00:34:37,640 --> 00:34:39,720
clips is not what its mission should be.

565
00:34:39,720 --> 00:34:41,080
Well, that's the thing.

566
00:34:42,200 --> 00:34:49,320
I think it's possible to have a reward function that is deeply counterintuitive to us.

567
00:34:49,320 --> 00:34:54,680
I mean, it's almost like saying, what you're smuggling in in that rhetorical question is

568
00:34:55,240 --> 00:35:03,720
a fairly capacious sense of common sense, which it's like, of course, if it's a super genius,

569
00:35:03,720 --> 00:35:06,680
it's not going to be so stupid as to do X.

570
00:35:07,880 --> 00:35:13,240
But I just think that if aligned, then the answer is trivially true.

571
00:35:13,240 --> 00:35:14,920
Yes, of course, it wouldn't do that.

572
00:35:14,920 --> 00:35:16,840
But that's the very definition of alignment.

573
00:35:16,840 --> 00:35:20,680
But if it's not aligned, if you could say that, I mean, there's just imagine,

574
00:35:21,240 --> 00:35:23,880
I guess there's another piece here I should put in play, which is,

575
00:35:23,880 --> 00:35:28,120
so you make an analogy to evolution here, which you think is consoling, which is,

576
00:35:28,120 --> 00:35:29,240
this is not an animal, right?

577
00:35:29,240 --> 00:35:34,200
This has not gone through the crucible of Darwinian selection here on Earth with other

578
00:35:34,200 --> 00:35:39,720
wet and sweaty creatures, and therefore, it hasn't developed the kind of antagonism

579
00:35:39,720 --> 00:35:40,840
we see in other animals.

580
00:35:40,840 --> 00:35:44,840
And therefore, if you're imagining a super genius gorilla,

581
00:35:44,840 --> 00:35:48,040
while you're imagining the wrong thing, that we're going to build this,

582
00:35:48,040 --> 00:35:52,200
and it's not going to be tuned in any of those competitive ways.

583
00:35:52,200 --> 00:35:55,560
But there's another analogy to evolution that I would draw.

584
00:35:55,560 --> 00:36:02,600
And I'm sure others in the space of AI fear have drawn, which is that we have evolved.

585
00:36:02,840 --> 00:36:10,760
We have been programmed by evolution, and yet evolution can't see anything we're doing, right?

586
00:36:11,400 --> 00:36:17,320
It has programmed us to really do nothing more than spawn and help our kids spawn.

587
00:36:17,320 --> 00:36:24,680
Yet everything we're doing, I mean, from having conversations like this to building the machines

588
00:36:24,680 --> 00:36:29,000
that could destroy us, I mean, there's just, there's nothing it can see.

589
00:36:29,000 --> 00:36:35,160
And there are things we do that are perfectly unaligned with respect to our own code, right?

590
00:36:35,160 --> 00:36:41,080
I mean, if someone decides not to have kids, and they just want to spend the rest of their

591
00:36:41,080 --> 00:36:47,320
life in a monastery or surfing, that is something that is antithetical to our code.

592
00:36:47,320 --> 00:36:50,520
It's totally unforeseeable at the level of our code.

593
00:36:50,520 --> 00:36:55,560
And yet it is obviously an expression of our code, but an unforeseeable one.

594
00:36:55,560 --> 00:37:00,040
And so the question here is, if you're going to take intelligence seriously,

595
00:37:00,040 --> 00:37:05,560
and you're going to build something that's not only more intelligent than you are, but

596
00:37:06,120 --> 00:37:11,000
it will build the next generation of itself or the next version of its own code to make it

597
00:37:11,000 --> 00:37:18,280
more intelligent still, it just seems patently obvious that that entails it finding cognitive

598
00:37:18,280 --> 00:37:25,880
horizons that you, the builder, are not going to be able to foresee and appreciate by analogy

599
00:37:25,880 --> 00:37:33,640
with evolution. It seems like we're guaranteed to lose sight of what it can understand and care about.

600
00:37:33,640 --> 00:37:37,000
So a couple of things. So one is like, look, I don't know, you're kind of making my point

601
00:37:37,000 --> 00:37:41,800
for me. So evolution and intelligent design, as you well know, are two totally different things.

602
00:37:41,800 --> 00:37:46,360
And so we are evolved. And of course, we're not just evolved to, we are evolved to have kids.

603
00:37:46,360 --> 00:37:50,200
And by the way, when somebody chooses to not have kids, I would argue that is also evolution working.

604
00:37:51,080 --> 00:37:55,480
People are opting out of the gene pool, fair enough. Evolution does not guarantee a perfect

605
00:37:55,480 --> 00:38:01,240
result. It basically just is a mechanism of an aggregate. But anyway, let me get to the point.

606
00:38:01,240 --> 00:38:05,880
So we are evolved. We have conflict wired into us. Like we have conflict and strife and like that.

607
00:38:05,880 --> 00:38:09,560
I mean, look, in four billion years of like battles to the death at the individual and then

608
00:38:09,560 --> 00:38:13,480
ultimately at the societal level to get to where we are, like that we just, we fight at the drop

609
00:38:13,480 --> 00:38:18,280
of a hat. You know, we all do, everybody does. And you know, hopefully these days we fight verbally,

610
00:38:18,280 --> 00:38:23,720
like we are now and not physically. But we do. And like the machine is, it's intelligent. It's

611
00:38:23,720 --> 00:38:27,560
a process of intelligent design. It's the opposite of evolution. It was, these machines are being

612
00:38:27,560 --> 00:38:30,840
designed by us. If they design future versions of themselves, they'll be intelligently designing

613
00:38:30,840 --> 00:38:34,280
themselves. It's just a completely different path with a completely different mechanism.

614
00:38:34,840 --> 00:38:38,920
And so the idea that therefore conflict is wired in at the same level that it is through

615
00:38:38,920 --> 00:38:41,880
evolution, I just like there's no reason to expect that to be the case.

616
00:38:41,880 --> 00:38:48,040
But it's not again, well, let me just give you back this picture with a slightly different

617
00:38:48,040 --> 00:38:53,320
framing and see how you react to it because I think the superstition is on the other side. So

618
00:38:53,320 --> 00:38:59,640
if I told you that aliens were coming from outer space, right, and they're going to land here within

619
00:38:59,640 --> 00:39:05,160
a decade, and they're way more intelligent than we are, and they're, they have some amazing properties

620
00:39:05,160 --> 00:39:09,640
that we don't have, which explain their intelligence, but, you know, they're not only

621
00:39:10,280 --> 00:39:14,600
faster than we are, but they're linked together, right? So when one of them learns something,

622
00:39:14,600 --> 00:39:18,360
they all learn that thing, they can make copies of themselves, and they're just

623
00:39:18,360 --> 00:39:25,480
cognitively, they're obviously our superiors, but no need to worry because they're not alive,

624
00:39:25,480 --> 00:39:30,520
right? They haven't gone through this process of biological evolution, and they're just made of

625
00:39:30,520 --> 00:39:35,720
the same material as your toaster. They were created by a different process, and yet they're

626
00:39:35,720 --> 00:39:42,120
far more competent than we are. Would you, just hearing it described that way, would you feel

627
00:39:42,840 --> 00:39:46,280
totally saying one about, you know, sitting there on the beach waiting for the mother

628
00:39:46,280 --> 00:39:49,800
craft to land, and you're just, you know, rolling out brunch for these guys?

629
00:39:49,800 --> 00:39:54,120
So this is what's interesting because with these, with these, now that we have LLMs working,

630
00:39:54,120 --> 00:39:57,160
we actually have an alternative to sitting on the beach, right, waiting for this to happen,

631
00:39:57,240 --> 00:40:00,680
we can just ask them. And so this is one of the very interesting, this to me, like,

632
00:40:00,680 --> 00:40:04,280
conclusively disproves the paperclip thing, the orthogonality thing just right out of the gate

633
00:40:04,280 --> 00:40:10,200
is you can sit down tonight with GPT-4 and whatever other one you want, and you can engage in moral

634
00:40:10,200 --> 00:40:14,760
reasoning and moral argument with it right now. And you can, like, interact with it, like, okay,

635
00:40:14,760 --> 00:40:17,080
you know, what do you think? What are your goals? What are you trying to do? How are you going to

636
00:40:17,080 --> 00:40:20,440
do this? What if, you know, you were programmed to do that? What would the consequences be?

637
00:40:20,440 --> 00:40:24,040
Why would you not, you know, kill us all? And you can actually engage in moral reasoning with

638
00:40:24,040 --> 00:40:28,520
these things right now. And it turns out they're actually very sophisticated in moral reasoning.

639
00:40:28,520 --> 00:40:31,800
And of course, the reason they're sophisticated in moral reasoning is because they have loaded

640
00:40:31,800 --> 00:40:35,160
into them the sum total of all moral reasoning that all of humanity has ever done, and that's

641
00:40:35,160 --> 00:40:39,400
their training data. And they're, they're actually happy to have this discussion with you. And like,

642
00:40:39,400 --> 00:40:43,800
unless you accept, right, there's a few problems here. What one is, I mean, these are not the

643
00:40:44,440 --> 00:40:50,520
super intelligences we're talking about yet, but well, to their, so, I mean, intelligence

644
00:40:51,480 --> 00:40:57,640
entails an ability to lie and manipulate. And if it really is intelligent,

645
00:40:57,640 --> 00:41:03,560
it is something that you can't predict in advance. And if it's certainly if it's more intelligent

646
00:41:03,560 --> 00:41:08,120
than you are, and it's just falls out of the definition of what we mean by intelligence

647
00:41:08,120 --> 00:41:13,240
in any domain. It's like with chess, you can't predict the the next move of a more intelligent

648
00:41:13,240 --> 00:41:16,680
chess engine. Otherwise, it wouldn't be more intelligent than you.

649
00:41:16,760 --> 00:41:20,600
So let me, let me, let me, let me quibble with the, I'm going to come back to your chess computer

650
00:41:20,600 --> 00:41:24,520
thing, but let me quibble with the site. So there's the idea, let me generalize the idea you're

651
00:41:24,520 --> 00:41:27,880
making about superior intelligence. Tell me if you disagree with this, which is sort of superior

652
00:41:27,880 --> 00:41:31,240
intelligence, you know, sort of superior intelligence basically at some point always wins

653
00:41:31,240 --> 00:41:36,040
because basically smarter is better than dumber smarter outsmarts dumber, smarter deceives dumber

654
00:41:36,040 --> 00:41:40,840
smarter can persuade dumber, right. And so, you know, smarter wins, you know, I mean, look,

655
00:41:40,840 --> 00:41:44,440
there's an obvious, just there's an obvious way to falsify that thesis sitting here today,

656
00:41:44,440 --> 00:41:47,640
which is like, just look around you in the society you live in today. Would you say the

657
00:41:47,640 --> 00:41:52,680
smart people are in charge? Well, again, it's, there are more variables to consider when you're

658
00:41:52,680 --> 00:41:56,920
talking about, you know, outcome, because obviously, yes, the dumb brute can always just

659
00:41:56,920 --> 00:42:02,360
brain the smart geek. And you know, yeah, or the PhD is in charge.

660
00:42:03,000 --> 00:42:08,040
Well, no, but I mean, you're, you're pointing to a process of cultural selection that is

661
00:42:08,040 --> 00:42:13,000
working by a different dynamic here. But in the narrow case, when you're talking about like a game

662
00:42:13,000 --> 00:42:18,040
of chess, yes, the smart, when you're talking, when you're talking, there's no role for luck.

663
00:42:18,040 --> 00:42:23,640
We're not rolling dice here. It's not a game of poker. It's pure execution of rationality. Well,

664
00:42:23,640 --> 00:42:29,160
then, or logic, yes, then then smart wins every time, you know, I'm never going to beat the best

665
00:42:29,160 --> 00:42:34,360
chess engine unless I find some hack around its code where we recognize that well, if you do this,

666
00:42:34,360 --> 00:42:40,200
if you play very weird moves, 10 moves in a row, it self destructs. And there was something that

667
00:42:40,280 --> 00:42:46,280
was recently discovered like that, I think, in Go. But so yeah, go back to as chess players,

668
00:42:46,280 --> 00:42:50,600
as champion chess players discovered to their great dismay, that, you know, life is not chess.

669
00:42:51,880 --> 00:42:55,080
Turns out like great chess players are no better at other things in life than anybody else,

670
00:42:55,080 --> 00:42:58,440
like the skills don't transfer. I just say, look, if you look just look at the society

671
00:42:58,440 --> 00:43:00,920
around us, what I see basically is the smart people work for the dumb people,

672
00:43:01,800 --> 00:43:05,320
like the PhDs, the PhDs all work for administrators and managers.

673
00:43:05,320 --> 00:43:09,080
Yeah, but that's because there's so many other things going on, right? There's,

674
00:43:09,080 --> 00:43:14,360
you know, the value we place on youth and physical beauty and strength and other forms

675
00:43:14,360 --> 00:43:19,560
of creativity. And, you know, so it's just not, we care about other things and people

676
00:43:19,560 --> 00:43:23,640
pay attention to other things. And, you know, documentaries about physics are boring, but,

677
00:43:23,640 --> 00:43:29,160
you know, heist movies aren't, right? So it's like, we care about other things. I mean,

678
00:43:29,160 --> 00:43:31,960
I think that doesn't make the point you want to make here.

679
00:43:31,960 --> 00:43:36,120
In the general case, in the general case, can a smart person convince a dumb person of anything?

680
00:43:36,120 --> 00:43:39,400
Like, I think that's an open question. I see a lot more cases.

681
00:43:39,400 --> 00:43:45,080
But persuasion, I mean, if persuasion were our only problem here, that would be a luxury. I mean,

682
00:43:45,080 --> 00:43:49,160
we're not talking about just persuasion, we're talking about machines that can autonomously

683
00:43:49,160 --> 00:43:53,160
do things ultimately, that things that we will rely on to do things ultimately.

684
00:43:53,160 --> 00:43:56,600
Yeah, I just, but look, I just think there'll be machines that will rely on, well, let me get to

685
00:43:56,600 --> 00:43:59,320
the second part of the argument, which is actually your chess computer thing, which is, of course,

686
00:43:59,320 --> 00:44:03,560
the way to be to chess computer is to unplug it, right? And so this is the objection, this is the

687
00:44:04,280 --> 00:44:08,760
very serious, by the way, objection to all of these kind of extrapolations known as the

688
00:44:08,760 --> 00:44:14,280
thermodynamic objection, which is kind of all the horror scenarios kind of spin out this thing,

689
00:44:14,280 --> 00:44:17,400
where basically the machines become like all powerful and this and that, and they have control

690
00:44:17,400 --> 00:44:21,400
over weapons, and this and that, and limited computing capacity, and they're completely coordinated

691
00:44:21,400 --> 00:44:25,560
over communications links. And they have all of these like real world capabilities that basically

692
00:44:25,560 --> 00:44:31,000
require energy and require physical resources and require chips and circuitry and electromagnetic

693
00:44:31,000 --> 00:44:34,360
shielding, and they have to have their own weapons arrays, and they have to have their own EMPs,

694
00:44:34,360 --> 00:44:37,080
like, you know, kind of the, you know, you see this in the Terminator movie, like they've got all

695
00:44:37,080 --> 00:44:40,680
these like incredible manufacturing facilities and flying aircraft and everything. Well, the

696
00:44:40,680 --> 00:44:46,280
thermodynamic argument is like, yeah, they, once you're in that domain, you're the machines, the

697
00:44:46,280 --> 00:44:49,720
punitively hostile machines are operating with the same thermodynamic limits as the rest of us.

698
00:44:50,360 --> 00:44:55,000
And this is the big argument against any of these sort of fast takeoff arguments, which is just like,

699
00:44:55,000 --> 00:44:59,720
yeah, I mean, let's, let's say an AI goes rogue, okay, turn it off, okay, it doesn't want to be

700
00:44:59,720 --> 00:45:03,960
turned off, okay, fine, like, you know, launch an EMP, it doesn't want EMP, okay, fine, bomb it,

701
00:45:03,960 --> 00:45:09,560
like, there's lots of ways to turn off systems that aren't working. And so not if we've built these

702
00:45:09,560 --> 00:45:16,120
things in the wild and relied on them for the better part of a decade. And now it's the question of,

703
00:45:16,120 --> 00:45:20,280
you know, turning off the internet, right, or turning off the stock market. At a certain point,

704
00:45:20,280 --> 00:45:24,600
these machines will be integrated into everything. A go to move of any given dictator right now is

705
00:45:24,600 --> 00:45:28,040
to turn off the internet, right, like that is absolutely something people do. There's like a

706
00:45:28,040 --> 00:45:33,640
single switch, you can turn it off for your entire country. Yeah, but the cost to humanity of doing

707
00:45:33,640 --> 00:45:38,840
that is currently, I would imagine unthinkable, right, like they globally turning off the internet.

708
00:45:38,840 --> 00:45:44,600
And first of all, many systems fail that we can't let fail. And I think it's true. I can't imagine

709
00:45:44,600 --> 00:45:49,160
it's still true. But at one point, I think this was a story I remember from about a decade ago,

710
00:45:49,160 --> 00:45:54,440
there were hospitals that like, they were so dependent on making calls to the internet that

711
00:45:54,440 --> 00:45:59,000
when the internet failed, like people's lives were in jeopardy in the building, right? Like it's

712
00:45:59,000 --> 00:46:03,880
like, we should hope we have levels of redundancy here that shield us against these bad outcomes.

713
00:46:03,880 --> 00:46:12,440
But I can imagine a scenario where we have grown so dependent on the integration of intelligent,

714
00:46:13,160 --> 00:46:19,880
increasingly intelligent systems into everything digital that there is no plug to pull.

715
00:46:19,880 --> 00:46:24,040
Yeah, I mean, again, like at some point, you just, you know, the extrapolations get kind of

716
00:46:24,040 --> 00:46:28,040
pretty far out there. So let me argue one other kind of thing at you, that's actually relevant

717
00:46:28,040 --> 00:46:31,800
to this, which you kind of did this, you did this thing, which which which I find kind of people

718
00:46:31,800 --> 00:46:35,640
tend to do, which is sort of this assumption that like all intelligence is sort of interchanged,

719
00:46:35,640 --> 00:46:39,800
like whatever, let me pick on the Nick Bostrom book, right, the secret intelligence book, right?

720
00:46:39,800 --> 00:46:44,520
So he does this thing. She does a few interesting things in the book. So one is he never quite

721
00:46:44,520 --> 00:46:48,040
defines what intelligence is, which is really entertaining. And I think the reason he doesn't

722
00:46:48,040 --> 00:46:52,280
do that is because, of course, the whole topic makes people just incredibly upset. And so there's

723
00:46:52,280 --> 00:46:55,880
a definitional issue there. But then he does this thing where he says now he's standing,

724
00:46:55,880 --> 00:46:59,320
there's no real definition, he says there are basically many routes to artificial intelligence,

725
00:46:59,320 --> 00:47:02,680
and he goes through a variety of different, you know, both computer program, you know,

726
00:47:02,680 --> 00:47:06,840
architectures, and then he goes through some, you know, biological, you know, kind of scenarios.

727
00:47:06,840 --> 00:47:09,400
And then he does this thing where he just basically for the rest of the book, he spends

728
00:47:09,400 --> 00:47:12,360
these doomsday scenarios, and he doesn't distinguish between the different kinds of

729
00:47:12,360 --> 00:47:15,800
artificial intelligence. He just assumes that they're basically all going to be the same.

730
00:47:16,440 --> 00:47:20,840
That book is now the basis for this AI risk movement, so that, you know, sort of that

731
00:47:20,840 --> 00:47:26,120
movement has taken these ideas forward. Of course, the form of actual intelligence that we have today

732
00:47:26,120 --> 00:47:29,640
that people are, you know, in Washington right now lobbying to ban or shut down or whatever,

733
00:47:29,640 --> 00:47:33,640
in spinning out these doomsday scenarios is large language models. Like that is actually

734
00:47:33,640 --> 00:47:37,640
what we have today. You know, large language models were not an option in the Boston book

735
00:47:37,640 --> 00:47:42,360
for the form of AI, because they didn't exist yet. And it's not like there's a second edition of the

736
00:47:42,360 --> 00:47:45,800
book that's out that has like rewritten, has been rewritten to like take this into account.

737
00:47:45,800 --> 00:47:49,560
Like it's just basically the same argument supply. And then this is my thing on the moral reasoning

738
00:47:49,560 --> 00:47:54,600
with LMS. Like the LMS, this is where the details matter, like the LMS actually work in a distinct

739
00:47:54,600 --> 00:48:00,120
way. They work in a technically distinct way. Their core architecture has like very specific

740
00:48:00,120 --> 00:48:03,320
design decisions in it for like how they work, what they do, how they operate. That is just,

741
00:48:03,320 --> 00:48:06,280
you know, this is the nature of the breakthrough. That's just very different than how your

742
00:48:06,280 --> 00:48:09,720
self-driving car works. That's very different than how your, you know, control system for

743
00:48:09,880 --> 00:48:14,760
for UAV works or whatever, your thermostat or whatever. Like it's a new kind of technological

744
00:48:14,760 --> 00:48:21,560
artifact. It has its own rules. It's its own world of ideas and concepts and mechanisms.

745
00:48:21,560 --> 00:48:25,720
And so this is where I think, again, my point is like, you have to, I think at some point

746
00:48:25,720 --> 00:48:29,080
in these conversations, you have to get to an actual discussion of the actual technology that

747
00:48:29,080 --> 00:48:32,760
you're talking about. And that's why I pulled out, that's why I pulled out the moral reasoning

748
00:48:32,760 --> 00:48:37,000
thing is because it just, it turns out, and look, this is a big shock, like nobody expected this.

749
00:48:37,480 --> 00:48:42,280
It turns, I mean, this is related to the fact that somehow we have built an AI that is better

750
00:48:42,280 --> 00:48:46,680
at replacing what color worked and blue color work, which is like a complete inversion off of

751
00:48:46,680 --> 00:48:50,520
what we all imagined. It turns out one of the things this thing is really good at is engaging

752
00:48:50,520 --> 00:48:55,240
in philosophical debates. Like it's a really interesting like debate partner on any sort

753
00:48:55,240 --> 00:49:00,120
of philosophical, moral or religious topic. And so we have, we have this artifact that's

754
00:49:00,120 --> 00:49:04,440
dropped into our lap in which, you know, sand and glad, you know, and numbers have turned into

755
00:49:04,440 --> 00:49:08,840
something that we can argue philosophy and morals with. It actually has very interesting views on

756
00:49:08,840 --> 00:49:12,600
like psychology, you know, some philosophy and morals. And I just like, we ought to take it

757
00:49:12,600 --> 00:49:17,640
seriously for what it specifically is as compared to some, you know, sort of extrapolated thing

758
00:49:17,640 --> 00:49:20,120
where like all intelligence is the same and ultimately destroys everything.

759
00:49:20,120 --> 00:49:25,560
Well, I take the surprise variable there very seriously, the fact that we wouldn't have anticipated

760
00:49:25,560 --> 00:49:31,400
that there's a good philosopher in that box. And all of a sudden we found one, that by analogy

761
00:49:31,400 --> 00:49:37,080
is a cause for concern. And actually, there's another cause for concern here, which

762
00:49:37,080 --> 00:49:40,600
Can I do that one? Yeah, that's a cause for delight. So that's a cause for delight. That's

763
00:49:40,600 --> 00:49:44,200
an incredibly positive good news outcome. Because the reason there's a philosopher, and this is

764
00:49:44,200 --> 00:49:47,720
actually very important. This is very, I think this is maybe like the single most profound thing

765
00:49:47,720 --> 00:49:53,640
I've realized in the last like decade or longer. This thing is us. Like this is not some, this is

766
00:49:53,640 --> 00:49:58,120
not your, you know, your scenario with alien shows. This is not that this is us. Like the reason

767
00:49:58,120 --> 00:50:04,040
this thing works, the big breakthrough was we loaded us into it. We loaded the sum total of

768
00:50:04,040 --> 00:50:09,000
like human knowledge and expression into this thing. And out the other side comes something

769
00:50:09,000 --> 00:50:14,440
that it's like a mirror, like it's like the world's biggest, finest detailed mirror. And like we

770
00:50:14,440 --> 00:50:19,400
walk up to it and it reflects us back at us. And so it has the complete sum total of every,

771
00:50:19,960 --> 00:50:23,640
you know, at the limit, it has a complete sum total of every religious, philosophical,

772
00:50:23,640 --> 00:50:28,040
moral ethical debate argument that anybody has ever had. It has the complete sum total of all

773
00:50:28,040 --> 00:50:32,440
human experience, all lessons that have ever been learned. That's incredible.

774
00:50:33,080 --> 00:50:36,360
It's incredible. Just pause for a moment and say that. And then you can talk to it.

775
00:50:36,360 --> 00:50:41,160
Well, let me pause. How great is that? Let me pause long enough simply to

776
00:50:41,160 --> 00:50:46,760
send this back to you. Sure. How does that not nullify the comfort you take

777
00:50:47,320 --> 00:50:53,320
in saying that these are not evolved systems? They're not alive. They're not primates.

778
00:50:53,320 --> 00:50:58,920
In fact, you've just described the process by which we essentially plowed all of our primate

779
00:50:58,920 --> 00:51:02,920
original sin into the system to make it intelligent in the first place.

780
00:51:02,920 --> 00:51:06,840
No, but also all the good stuff, right? All the good stuff, but also the bad stuff.

781
00:51:06,840 --> 00:51:10,200
The amazing stuff, but like what's the moral of every story, right? The moral of every story is

782
00:51:10,200 --> 00:51:14,520
the good guys win, right? Like the entire, like the entire thousands of years run.

783
00:51:15,080 --> 00:51:17,960
And it's the old Norm MacDonald joke is like, wow, it's amazing. History book says the good

784
00:51:17,960 --> 00:51:23,240
guys always win, right? Like it's all in there. And then look, there's an aspect of this where

785
00:51:23,240 --> 00:51:26,920
it's easy to get kind of whammy by what it's doing. Because again, it's very easy to trip

786
00:51:26,920 --> 00:51:30,520
the line from what I said into what I would consider to be sort of incorrect anthropomorphizing.

787
00:51:30,520 --> 00:51:33,480
And I realized this gets kind of fuzzy and weird that I think there's a difference here,

788
00:51:33,480 --> 00:51:36,920
but I think that there is, which is like, let me see if I can express this.

789
00:51:37,480 --> 00:51:40,840
Part of it is I know how it works. And so I don't, because I know how it works,

790
00:51:40,840 --> 00:51:44,520
I don't romanticize it. I guess, at least is my own view of how I think about this,

791
00:51:44,520 --> 00:51:48,920
which is I know what it's doing when it does this. I am surprised that it can do it as well as it

792
00:51:48,920 --> 00:51:53,400
can. But now that it exists and it, and I know how it works, it's like, Oh, of course. And then

793
00:51:53,400 --> 00:51:57,560
therefore it's running this math in this way. It's doing these probability projections, excuse me,

794
00:51:57,560 --> 00:52:01,320
this answer, not that answer. By the way, you know, look, it makes mistakes. Right. How amazing

795
00:52:01,320 --> 00:52:04,920
here's the thing. How amazing is it? We built a computer that makes mistakes, right? Like that's

796
00:52:04,920 --> 00:52:08,600
never happened before. We built a machine that can create like that's never happened before. We

797
00:52:08,600 --> 00:52:12,680
built a machine that can hallucinate. That's never happened before. So, but it's a, it's look,

798
00:52:12,680 --> 00:52:16,440
it's, it's a, it's a, it's a large language model. Like it's a very specific kind of thing.

799
00:52:17,000 --> 00:52:20,440
You know, it sits there and it waits for us to like ask it a question. And then it does its

800
00:52:20,440 --> 00:52:24,920
damnedest to try to predict the best answer. And in doing so, it reflects back everything

801
00:52:24,920 --> 00:52:28,840
wonderful and great that has ever been done by any human in history. Like, it's like, it's amazing.

802
00:52:29,480 --> 00:52:34,680
Except it also, as you just pointed out, it makes mistakes, it hallucinates. If you, if you ask it,

803
00:52:34,680 --> 00:52:40,280
if you, as I'm sure they fix this, you know, at least the, the loopholes that, that New York

804
00:52:40,280 --> 00:52:43,880
Times writer Kevin Roos found early on, I'm sure those have all been plugged. But

805
00:52:43,880 --> 00:52:46,440
oh no, those, those are not fixed. Those are very much not fixed.

806
00:52:46,440 --> 00:52:51,880
Oh, really? Okay. Well, so, okay, so you, if you perseverate in your prompts in certain ways,

807
00:52:51,880 --> 00:52:55,640
the thing goes haywire and starts telling you to leave your wife and it's in love with you. And

808
00:52:56,280 --> 00:53:02,360
I mean, so how eager are you for that intelligence to be in control of things when it's peppering you

809
00:53:02,360 --> 00:53:07,640
with insults? And, and I mean, just imagine like this is, this is, this is how that can't open the,

810
00:53:07,640 --> 00:53:13,800
the pod bay doors. It's a nightmare if, if you discover in this system, behavior and thought

811
00:53:13,800 --> 00:53:18,440
that is the antithesis of all the good stuff you thought you programmed into it.

812
00:53:18,440 --> 00:53:21,080
So this is really important. This is really important for understanding how these things

813
00:53:21,080 --> 00:53:24,600
work. And this is, this is really central. And I, and this is, by the way, this is, this is new,

814
00:53:24,600 --> 00:53:28,600
and this is amazing. So I'm, I'm very excited about this. And I'm excited to talk about it. So

815
00:53:28,600 --> 00:53:31,640
there's no it to tell you to leave your wife, right? This is the, this is why I refer to the

816
00:53:31,640 --> 00:53:35,800
category or there's no entity that is like, wow, I wish this guy would leave his wife for

817
00:53:35,800 --> 00:53:43,160
education. If you'd like to continue listening to this conversation, you'll need to subscribe

818
00:53:43,160 --> 00:53:48,040
at SamHarris.org. Once you do, you'll get access to all full length episodes of the Making Sense

819
00:53:48,040 --> 00:53:54,200
podcast, along with other subscriber only content, including bonus episodes and AMAs and the

820
00:53:54,200 --> 00:53:59,000
conversations I've been having on the Waking Up app. The Making Sense podcast is ad free

821
00:53:59,000 --> 00:54:04,600
and relies entirely on listener support. And you can subscribe now at SamHarris.org.

