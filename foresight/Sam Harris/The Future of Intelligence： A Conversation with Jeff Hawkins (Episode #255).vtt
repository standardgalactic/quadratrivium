WEBVTT

00:00.000 --> 00:12.160
Welcome to the Making Sense podcast. This is Sam Harris. Just a note to say that if

00:12.160 --> 00:16.360
you're hearing this, you are not currently on our subscriber feed and will only be hearing

00:16.360 --> 00:20.800
the first part of this conversation. In order to access full episodes of the Making Sense

00:20.800 --> 00:26.400
podcast, you'll need to subscribe at SamHarris.org. There you'll find our private RSS feed to

00:26.400 --> 00:31.040
add to your favorite podcatcher, along with other subscriber-only content. We don't run

00:31.040 --> 00:34.960
ads on the podcast, and therefore it's made possible entirely through the support of our

00:34.960 --> 00:38.800
subscribers. So if you enjoy what we're doing here, please consider becoming one.

00:46.960 --> 00:52.080
Today I'm speaking with Jeff Hawkins. Jeff is the co-founder of Numenta,

00:52.720 --> 00:57.680
a neuroscience research company, and also the founder of the Redwood Neuroscience Institute.

00:58.480 --> 01:04.640
And before that, he was one of the founders of the field of handheld computing, starting Palm

01:05.360 --> 01:10.000
and Handspring. He's also a member of the National Academy of Engineering,

01:10.800 --> 01:18.000
and he's the author of two books. The first is On Intelligence, and the second and most recent is

01:18.000 --> 01:24.560
A Thousand Brains, A New Theory of Intelligence. And Jeff and I talk about intelligence

01:25.200 --> 01:31.600
from a few different sides here. We start with the brain. We talk about how the cortex creates

01:31.600 --> 01:39.440
models of the world, the role of prediction in experience. We discuss the idea that thought

01:39.440 --> 01:46.160
is analogous to movement in conceptual space. But for the bulk of the conversation,

01:46.160 --> 01:52.160
we have a debate about the future of artificial intelligence, and in particular the alignment

01:52.160 --> 02:00.240
problem and the prospect that AI could pose some kind of existential risk to us. As you'll hear,

02:01.040 --> 02:08.240
Jeff and I have very different takes on that problem. Our intuitions divide fairly sharply,

02:08.960 --> 02:15.920
and as a consequence we have a very spirited exchange. Anyway, it was a lot of fun. I hope

02:16.000 --> 02:19.440
you enjoy it, and now I bring you Jeff Hawkins.

02:26.640 --> 02:29.360
I'm here with Jeff Hawkins. Jeff, thanks for joining me.

02:30.560 --> 02:32.000
Thanks for having me, Sam. It's a pleasure.

02:32.720 --> 02:39.280
I think we met probably just once, but I feel like we met about 15 years ago at one of those

02:39.280 --> 02:42.720
Beyond Belief conferences at the Salk Institute. Does that ring a bell?

02:43.280 --> 02:48.480
I was at one of the Beyond Belief conferences, and I don't recall meeting you there, but it's

02:48.480 --> 02:55.280
totally possible. It's possible we didn't meet, but I remember, I think we had an exchange where

02:55.280 --> 03:00.640
one of us was in the audience, and the other was in exchange over 50 feet or whatever.

03:00.640 --> 03:04.320
Oh, that makes sense. Yeah, I was in the audience, and I was speaking up.

03:04.880 --> 03:10.240
Yeah, okay, and I was probably on stage defending some Kakamemi conviction. Well,

03:10.240 --> 03:17.200
anyway, nice to almost meet you once again, and you have a new book which we'll cover part of,

03:17.200 --> 03:23.360
not by no means exhausting its topics of interest, but the new book is A Thousand Brains,

03:23.360 --> 03:30.960
and it's a work of neuroscience and also a discussion about the frontiers of AI and where

03:30.960 --> 03:38.240
all this is heading, but maybe we should start with the brain part of it and start with the

03:38.960 --> 03:47.200
really novel and circuitous and entrepreneurial route you've taken to get into neuroscience.

03:47.760 --> 03:54.720
This is the non-standard course to becoming a neuroscientist. Give us your brief biography

03:54.720 --> 04:00.560
here. How did you get into these topics? Well, I fell in love with brains when I

04:00.560 --> 04:06.080
just got out of college, so I studied electrical engineering in college, and right after I started

04:06.080 --> 04:12.960
my first job at Intel, I read an article by Francis Crick about brains and how we don't

04:12.960 --> 04:17.280
understand their work, and I just became enamored. I said, oh my god, we should understand this.

04:17.280 --> 04:22.720
This is me. I am my brain, and no one seems to know how this thing is working, and I just couldn't

04:22.720 --> 04:28.400
accept that, and so I decided to dedicate my life to figuring out what's going on when I'm thinking

04:28.480 --> 04:37.840
and who we are basically as a species. It was a difficult path, so I quit my job. I

04:37.840 --> 04:45.120
essentially applied to become a graduate student first at MIT and AI, but then I settled at Berkeley

04:45.680 --> 04:50.320
in neuroscience, and I said, okay, we're going to spend my life figuring out how the

04:50.320 --> 04:56.080
neocortex works, and I found out very quickly that that was a very difficult thing to do

04:56.080 --> 05:01.200
scientifically, but difficult to do from the practical aspects of science, that you couldn't

05:01.200 --> 05:05.680
get funding for that. It was considered too ambitious. It was theoretical work, and people

05:05.680 --> 05:10.640
didn't fund theoretical work, so after a couple of years as a graduate student at Berkeley,

05:10.640 --> 05:15.360
I set a different path. I said, okay, I'm going to go back to work in industry for a few years

05:15.920 --> 05:20.960
to mature, to figure out how to make institutional change because I was up against an institutional

05:20.960 --> 05:26.880
problem, not just a scientific problem, and that turned into a series of successful businesses

05:26.880 --> 05:31.840
that I was involved with and started, including Palm and HandSpring. These are some of the early

05:32.400 --> 05:37.040
HandTel computing companies, and we were having a tremendous amount of success with that,

05:37.040 --> 05:43.040
but it was never my mission to stay in the HandTel computing industry. I wanted to get back

05:43.040 --> 05:48.800
to neuroscience, and everybody who worked for me knew this. In fact, I told the investors,

05:48.800 --> 05:52.080
I'm only going to do this for four years, and they said, what? I said, yeah, that's it,

05:52.640 --> 05:56.400
but it turned out to be a lot longer than that because all the success we had, but eventually,

05:56.400 --> 06:01.520
I just extracted myself from it, and I said, I'm going to go and I have so many years left in my

06:01.520 --> 06:06.560
life, so after having all that success in the mobile computing space, I started a neuroscience

06:06.560 --> 06:09.920
institute. This is at the recommendation of some neuroscience friends of mine,

06:09.920 --> 06:14.000
so they helped me do that, and I ran that for three years, and now I've been running sort of a

06:14.000 --> 06:17.920
private lab just doing pure neuroscience for the last 17 years.

06:19.280 --> 06:21.040
That's Numenta, right?

06:21.040 --> 06:28.000
That's Numenta, yeah, and we've made some really significant progress in our goals,

06:28.560 --> 06:33.520
and the book documents some of the recent really significant discoveries we've made.

06:34.080 --> 06:40.480
So am I right in thinking that you made enough money at Palm and HandSpring that you could

06:40.480 --> 06:45.840
self-fund your first neuroscience institute, or is that not the case? Did you have to go raise

06:45.840 --> 06:52.080
money? Well, it was a bit of both. Certainly, I was a major contributor. I wasn't the only one,

06:52.640 --> 06:58.000
but I didn't want the funding to be the driver of what we did and how we spent all our time,

06:58.000 --> 07:03.920
so at the institute, we had collaborations with both Berkeley and Stanford. We didn't get funds

07:03.920 --> 07:10.160
from them, but we did work with them on various things, and then we had, but that was mostly

07:10.160 --> 07:15.200
funded by myself. Numenta is still, I'm a major contributor to it, but there are other people

07:15.200 --> 07:20.800
who've invested in Numenta, and we have one outside venture capitalist, and several people,

07:20.800 --> 07:27.440
but I'm still a major contributor to it. I just view that as a sort of a necessary thing to get

07:27.440 --> 07:32.560
on to the science and not have to worry about it, because when I was at Berkeley, what I was told

07:32.560 --> 07:39.040
over and over again, I really came to understand this. In fact, eventually, after that, when I was

07:39.040 --> 07:44.240
running the Redwood Neuroscience Institute, I went to Washington to talk about the National

07:44.240 --> 07:49.040
Science Foundation and National Institute of Health, and also to DARPA, who were the funders

07:49.040 --> 07:53.680
of neuroscience, and everyone thought what we were doing, which is sort of big theory, large-scale

07:53.680 --> 07:58.240
theories of cortical function, that this was like the most important problem to work on,

07:58.240 --> 08:04.080
but everyone said they can't fund it for various reasons. Over the years, I've come to appreciate

08:04.080 --> 08:09.520
that it's very difficult to be a scientist doing what we do with traditional funding sources,

08:10.080 --> 08:15.520
but we don't work outside of science. We partner with labs, and we go to conferences,

08:15.520 --> 08:21.520
we publish papers, we do all the regular stuff. Right. It's amazing how much comes down to funding

08:21.520 --> 08:27.120
or lack of funding and the incentives that would dictate whether something gets funded in the first

08:27.120 --> 08:33.440
place. It's by no means a perfect system. It's a kind of an intellectual market failure.

08:34.000 --> 08:38.320
Yeah. It is fascinating, and we could have a whole conversation about that sometimes, perhaps,

08:38.960 --> 08:43.840
because I ask myself, why is it so hard? Why do people can't fund this? There's reasons for it,

08:43.840 --> 08:49.200
and it's a complex, strange thing. When people were telling me, this is the most important thing

08:49.200 --> 08:54.320
anyone could be working on, and we think your approaches are great, but we can't fund that,

08:54.960 --> 09:00.880
why is that? I just accepted the way it was. I said, okay, this is the world I'm living in.

09:00.880 --> 09:07.360
I'm going to get one chance here. If I can't do this through working my way as a graduate

09:07.360 --> 09:12.400
student to getting a position at a university, how am I going to do it? I said, okay, it's

09:12.400 --> 09:18.960
not what I thought, but this is what's going to be. Nice. Well, let's jump into the neuroscience

09:18.960 --> 09:25.120
side of it. Generally speaking, we're going to be talking about intelligence and how it's

09:25.120 --> 09:35.520
accomplished in physical systems. Let's start with a definition, however loose. What is intelligence

09:35.520 --> 09:41.840
in your view? I didn't know and didn't have any pre-ideas about what this would be. It was a mystery

09:41.840 --> 09:47.440
to me, but we've learned what a good portion of your brain is doing. We started the New York

09:47.440 --> 09:53.600
Cortex, which is about 70% of the volume of a human brain. I now know what that does, and so

09:53.600 --> 09:59.120
I'm going to take that as my definition for intelligence here. What's going on in your

09:59.120 --> 10:05.360
New York Cortex is it's learning a model of the world, an internal recreation of all the things

10:05.360 --> 10:10.720
in the world that you know of and how it does. That's the key and what we've discovered,

10:10.720 --> 10:16.720
but it's this internal model. Intelligence requires having an internal model of the world

10:16.720 --> 10:21.360
in your head. It allows you to recognize where you are. It allows you to act on things. It allows

10:21.360 --> 10:25.520
you to plan and think about the future. If I'm going to say, what happens when I do this, the

10:25.520 --> 10:31.360
model tells you that. To me, intelligence is just about having a model in your head and using that

10:31.360 --> 10:36.960
for planning and action. It's not about doing anything in particular. It's about understanding

10:36.960 --> 10:41.600
the world. Yeah, that's interesting. I think most people would, that's kind of an internal

10:42.480 --> 10:48.560
definition of intelligence, but I think most people would reach for an external one or a

10:49.680 --> 10:54.800
functional one that has to take in the environment. It's something about being able to

10:55.920 --> 11:02.640
flexibly meet your goals under a range of conditions, more flexibly than rigidly. I

11:02.640 --> 11:07.520
guess there's rigid forms of intelligence, but when we're talking about anything like

11:08.240 --> 11:14.560
general intelligence, we're talking about something that is not merely hardwired and

11:14.560 --> 11:19.440
reflexive, but flexible. Yes, but if you have an internal model of the world,

11:19.440 --> 11:23.360
you had to learn it, at least from a human point of view. There's some things we have built in

11:23.360 --> 11:29.760
when we're born, but the vast majority of what you and I know, Sam, is learned. We didn't know

11:29.760 --> 11:33.200
what a computer was when you're born. You don't know what a coffee cup is. You don't know what

11:33.200 --> 11:36.880
building is. You don't know what doors are. You don't know what computer codes are. None of this

11:36.960 --> 11:42.960
stuff. Almost everything we interact with in the world today, in language, we don't know any

11:42.960 --> 11:47.520
particular language when we're born. We don't know mathematics, so we had to learn all these things.

11:47.520 --> 11:51.440
So if you want to say there might be an internal model that wasn't learned, well, that's pretty

11:51.440 --> 11:55.520
trivial, but I'm talking about models that are learned and you have to interact with the world

11:55.520 --> 11:59.920
to learn it. You can't learn it without being present in the world, without having an embodiment,

11:59.920 --> 12:04.640
without moving about, touching and seeing and hearing things. So a large part of what people

12:04.640 --> 12:10.560
think about, like you brought up, is, okay, we are able to solve a goal, but that's what a model

12:10.560 --> 12:16.000
lets you to do. That is not what intelligence itself is. Intelligence is having this ability

12:16.000 --> 12:21.360
to solve any goal, right? Because if your model covers that part of the world, you can figure out

12:21.360 --> 12:26.560
how to manipulate that part of the world and achieve what you want. So I'll give you a little

12:26.560 --> 12:30.800
further analogy. It's a little bit like computers. When we talk about a universal turning machine

12:30.880 --> 12:37.200
or what a computer is, it's not defined by what the computer is applied to do. It's like a computer

12:37.200 --> 12:40.320
isn't something that solves a particular problem. A computer is something that works on a set of

12:40.320 --> 12:46.000
principles, and that's how I think about intelligence. It's a modeling system that works on a set of

12:46.000 --> 12:51.840
principles. Those principles can exist in a mouse, in a dog, in a cat, in a human, and probably birds,

12:52.480 --> 12:55.600
but don't focus on what those animals are doing.

12:56.080 --> 13:01.920
Hmm. Yeah, it's important to point out that a model need not be a conscious model. In fact,

13:01.920 --> 13:09.680
most of our models are not conscious and might not even be, in principle, available to consciousness,

13:09.680 --> 13:16.160
although I think at the boundary, something that you'd say is happening entirely in the dark,

13:16.160 --> 13:22.160
does have a kind of, or can have a kind of liminal conscious aspect. So I mean, to take,

13:22.160 --> 13:28.000
you know, the coffee cup example, this leads us into a more granular discussion of what it means

13:28.000 --> 13:33.360
to have a model of anything at the level of the cortex. But, you know, if I reach for my coffee cup

13:34.080 --> 13:39.600
and grasp it, the ordinary experience of doing that is something I'm conscious of.

13:40.320 --> 13:47.840
I'm not conscious of all of the prediction that is built into my accomplishing that and

13:48.800 --> 13:55.120
experiencing what I experience when I touch a coffee cup. And yet, it's prediction that is

13:55.120 --> 14:00.560
required having some ongoing expectation of what's going to happen there when I, you know, when each

14:00.560 --> 14:08.960
finger touches the surface of the cup that allows for me to detect any error there or to be surprised

14:08.960 --> 14:13.840
by something truly anomalous. So if I reach for a coffee cup, and it turns out that's, you know,

14:13.840 --> 14:19.520
it's a hologram of a coffee cup and my hand passes right through it, the element of surprise

14:19.520 --> 14:27.600
there seems predicated on some ongoing prediction processing to which the results of my behavior

14:27.600 --> 14:34.000
is being compared. So maybe you can talk about what you mean by having a model at the level

14:34.000 --> 14:41.040
of the cortex and how prediction is built into that. Yeah. Well, my first book, which I published

14:41.280 --> 14:47.520
14 years ago called Long Intelligence, was just about that topic. It was about how it is the brain

14:47.520 --> 14:51.840
is making all these predictions all the time and all your sensory modalities and you're not aware of

14:51.840 --> 14:57.120
it. And so that's sort of the foundation. And you can't make a prediction without a model. I mean,

14:57.120 --> 15:01.520
a model, to make a prediction, you had to have some expectation, the expectation whether you're

15:01.520 --> 15:06.640
not aware of it or not, but they have an expectation. And that has to be driven from some internal

15:06.640 --> 15:10.960
representation of the world that says, hey, this, they're about to touch this thing, I know what it

15:10.960 --> 15:17.040
is, it's supposed to feel this way. And even if you're not aware that you're doing that. One of the

15:17.040 --> 15:23.200
key discoveries we made, and this was maybe about eight years ago, we, we had to get to the bottom

15:23.200 --> 15:28.800
like how do neurons make predictions? What is the physical manifestation of a prediction in the brain?

15:29.680 --> 15:33.440
And most of these predictions, as you point out, are not conscious, you're not aware of them.

15:33.440 --> 15:37.440
They're just happening. And if something, if something is wrong, then your attention is drawn

15:37.440 --> 15:41.680
to it. So if you felt the coffee cup and there's a little burr on the side or a crack and you didn't

15:41.680 --> 15:46.880
know that was expected that you'd say, Oh, there's a crack. I mean, what was the brain doing when it

15:46.880 --> 15:51.760
was making that prediction? And we have a, we have a theory about this. And I wrote about it in the

15:51.760 --> 15:58.800
book a bit. And it's, it's a beautiful, I think it's a beautiful theory. But it, it's, it's

15:58.800 --> 16:02.400
basically most of the predictions that are going on in your brain, most of them, not all of them,

16:02.400 --> 16:08.880
but most of them happen inside individual neurons. They are, it is a internal to the

16:08.880 --> 16:14.160
individual neurons. Now, not a single neuron can predict something, but an ensemble of neurons do

16:14.160 --> 16:22.240
this. But it's an internal state. And we have, we wrote a paper that came out in 1990, 2016, excuse

16:22.240 --> 16:30.000
me, 2016, which is, it's called wider neurons have so many synapses. And we, what we posit in

16:30.000 --> 16:34.880
that paper, and I'm pretty sure this is correct is that, you know, neurons have these thousands of

16:34.880 --> 16:40.240
synapses. Most of those synapses are being used for prediction. And when a neuron recognizes a

16:40.240 --> 16:45.840
pattern and says, okay, I'm supposed to be active soon, I should be, I should be becoming active soon.

16:45.840 --> 16:50.240
If everything is according to our model here, I should be coming active soon. And it goes into

16:50.240 --> 16:56.480
this internal state, the neuron itself is saying, okay, I'm expecting to become active. And you

16:56.480 --> 17:01.200
can't detect that consciously. It's internal to the, it's essentially just a depolarization or

17:01.200 --> 17:08.000
change of the voltage of the neuron. And so when, but it, we showed how the network of these neurons,

17:08.000 --> 17:13.680
what'll happen is, if your prediction is correct, then a small subset of the neurons become active.

17:13.680 --> 17:17.840
But if the prediction is incorrect, a whole bunch of neurons become active at the same time.

17:18.480 --> 17:23.120
And then that draws your attention to the problem. So it's a fascinating problem. But most of

17:23.120 --> 17:27.600
the predictions going on in your brain are not accessible outside of individual neurons.

17:27.600 --> 17:29.200
So there's no way you could be conscious about it.

17:31.280 --> 17:35.520
I guess most people are familiar with the general anatomy of a neuron where you have a,

17:35.520 --> 17:41.840
this spindly looking thing where there's a cell body and there's a long process,

17:41.840 --> 17:48.720
the axon leading away, which carries the action potential, if that neuron fires

17:48.800 --> 17:55.760
to the synapse and communicates neurotransmitters to other neurons. But on the other side of,

17:56.960 --> 18:01.920
in the standard case, on the other side of the cell body, there's this really,

18:02.800 --> 18:09.440
often really profuse arborization of dendrites, which is kind of the mad tangle of processes,

18:09.440 --> 18:16.880
which receive information from other neurons to which this neuron is connected. And

18:17.440 --> 18:23.760
it's the integration of information on that side. But before that neuron fires,

18:23.760 --> 18:28.720
the change, the probability of its firing, that that's the place you are

18:29.440 --> 18:35.280
locating this, the full set of predictive changes or the full set of changes that constitute

18:35.280 --> 18:38.320
prediction in the case of a system of neurons.

18:38.320 --> 18:44.240
Yeah. It's interesting. For many years, people looked at those, the connections on the dendrites,

18:44.240 --> 18:50.480
on that bushy part called synapses. And when they activated a synapse, most of the synapses

18:51.040 --> 18:55.440
were so far from the cell body that they didn't really have much of an effect.

18:55.440 --> 19:00.640
They didn't seem like they could make anything happen. And so, but there are thousands and

19:00.640 --> 19:05.440
thousands of them out there, but they don't seem powerful enough to make anything occur.

19:06.080 --> 19:11.120
And what was discovered basically over the last 20 years, that there are,

19:11.120 --> 19:14.960
there's a second type of spike. So you mentioned the one that goes down the axon,

19:14.960 --> 19:19.520
that's the action potential. But there are spikes that travel along the dendrites.

19:20.320 --> 19:25.760
And so basically what happens is the individual sections of the dendrite, like little branches

19:25.760 --> 19:31.520
of this tree, each one of them can recognize patterns on their own. They can recognize hundreds of

19:31.520 --> 19:36.160
separate patterns on these different branches. And they can cause this spike to travel along

19:36.160 --> 19:42.880
the dendrite. And that lowers the, changes the voltage of the cell body a little bit.

19:42.880 --> 19:48.160
And that is what we call the predictive state. The cell is like prime. It says, oh, I, I'm,

19:48.160 --> 19:54.720
if I fire, I'm ready to fire. And it's not actually a probability change. It's the timing.

19:55.360 --> 19:59.360
And so a cell that's in this predictive state that says, I think I should be firing now,

20:00.240 --> 20:05.680
or very shortly, if it does generate the regular spike, the action potential, it does it a little

20:05.680 --> 20:10.000
bit sooner than it would have otherwise. And it's timing that is the key to making the whole

20:10.000 --> 20:14.080
circuit work. We're getting pretty down in the weeds here about the science. But I hope, I don't

20:14.080 --> 20:18.880
know if you're, all your readers, listeners will appreciate that. Yeah. No, I think it's useful

20:18.880 --> 20:25.280
though. More weeds here. But I mean, one of the novel things about your argument is that

20:26.160 --> 20:33.440
it was inspired by some much earlier theorizing. You mark your debt to Vernon Mountcastle. But

20:33.520 --> 20:40.560
the idea is that there's a, a common algorithm operating more or less everywhere at the level

20:40.560 --> 20:46.880
of the cortex. That is, it's more or less the, you know, the cortex is doing essentially the same

20:46.880 --> 20:55.520
thing, whether it's producing language or vision or, you know, any other sensory channel or motor

20:55.520 --> 21:02.800
behavior. So talk about the, the general principle that you spend a lot of time on in the book of

21:02.800 --> 21:10.080
just the organization of the neocortex into cortical columns and the implications this has for

21:10.960 --> 21:17.680
how we view what the brain is doing in terms of sensory and motor learning and, you know,

21:17.680 --> 21:22.880
all of its consequences. This is, Vernon Mountcastle made this proposal back in the 70s.

21:23.520 --> 21:28.800
And it's just a dramatic idea. And it's an incredible idea and so incredible that some

21:28.800 --> 21:34.080
people just refuse to believe it, but other people really think it's a tremendous discovery.

21:34.080 --> 21:38.640
But what he noticed was if you look at the neocortex, if you could take one out of your head

21:38.640 --> 21:43.600
or out of a human's head, it's like a sheet. It's about two and a half millimeters thick.

21:43.600 --> 21:50.240
It is about the size of a large dinner napkin or 1500 square centimeters. And if you could fold it,

21:50.240 --> 21:55.680
lay it flat. And the different parts of it, like they do different things as parts that do vision,

21:55.680 --> 22:02.320
as parts that do language and parts that do hearing and so on. But if you cut into it and you look at

22:02.320 --> 22:08.880
the structure in one of these areas, it's very complicated. There are dozens of different cell

22:08.880 --> 22:14.400
types, but they're very prototypically connected and they're arranged in certain patterns and

22:14.400 --> 22:19.760
layers and different types of things. So it's a very complex structure, but it's almost the same

22:19.760 --> 22:24.560
everywhere. It's not the same everywhere, but almost the same everywhere. And so this is not

22:24.560 --> 22:29.680
just true in a human neocortex, but if you look at a rat's neocortex or a dog's neocortex or a cat

22:29.680 --> 22:37.280
or a monkey, this same basic structure is there. And what Vernon Malkus said is that all the parts

22:37.280 --> 22:41.840
of the neocortex are actually, we think of them as doing things, different things, but they're

22:41.840 --> 22:46.880
actually all doing some fundamental algorithm, which is the same. So hearing and touch and vision

22:46.880 --> 22:51.200
are really the same thing. He says, if you took part of the cortex and you hook it up to your eyes,

22:51.200 --> 22:54.560
you'll get vision. If you hook it up to your ears, you'll get hearing. If you hook it up to

22:54.560 --> 23:01.120
other parts of the neocortex, you'll get language. And so he spent many years giving the evidence

23:01.120 --> 23:07.840
for this. He proposed further that this algorithm was contained in what's called a column. And so

23:07.840 --> 23:14.240
if you would take a small area of this neocortex, remember it's like two and a half millimeters

23:14.240 --> 23:20.880
thick, you take a very sort of skinny little one millimeter column out of it, that that is the

23:20.880 --> 23:28.560
processing element. And so our human neocortex, we have about 150,000 of these columns. Other

23:28.560 --> 23:33.360
animals have more or less. People should picture something resembling a grain of rice in terms

23:33.360 --> 23:37.840
of scale here. Yeah, yeah. I sometimes say take a piece of skinny spaghetti like, you know, angel

23:37.840 --> 23:41.920
have pasta or something like that and cut it into two little two and a half millimeter links

23:41.920 --> 23:46.720
and stack them side by side. Now the funny thing about columns is you can't see them. They're not

23:46.720 --> 23:51.680
visual things. You can't look under a microscope, you won't see it. But he pointed out why

23:53.200 --> 23:57.840
they're there. It has to do with how they're connected. So all the cells in one of these

23:57.840 --> 24:02.880
little millimeter pieces of rice or spaghetti, if you will, are all processing the same thing. And

24:02.880 --> 24:07.520
the next piece of rice over processing something different and the next piece of rice over processing

24:07.520 --> 24:13.920
something different. And so he didn't know what was going on in the cortical column. He

24:15.040 --> 24:20.480
articulated the architecture. He talked about the evidence that this exists. He said, here's the

24:20.480 --> 24:26.720
evidence why these things are all doing the same thing. But he didn't know what it was. And it's

24:26.720 --> 24:32.000
kind of hard to imagine what it is that this algorithm could be doing. But that was essentially

24:32.000 --> 24:35.600
the core of our research. That's what we've been focused on for close to 20 years.

24:35.920 --> 24:41.760
It's also hard to imagine the microanatomy here because in each one of these little columns,

24:42.400 --> 24:48.000
there's something like 150,000 neurons on average. And if you could just unravel

24:48.720 --> 24:57.440
all of the connections there, the tiny filaments of nerve endings, what you would have there is

24:57.440 --> 25:04.560
on the order of kilometers in length, all wound up into that tiny structure. So this is a

25:05.280 --> 25:11.520
strange juxtaposition of simplicity and complexity, but there's certainly a

25:11.520 --> 25:16.320
mad tangle of processes in there. Yeah, this is why brains are so hard to study. If you look at

25:16.320 --> 25:20.000
another organ in the body, whether it's the heart or the liver or something like that,

25:20.640 --> 25:26.320
and you take a little section of it, it's pretty uniform. But here, if you take a teeny piece of

25:26.320 --> 25:34.480
the cortex, it's got this incredible complexity in it, which is not random. It's very specific.

25:35.440 --> 25:40.800
So yeah, it's hard to get wrapper your heads around how complex it is. But we need to be

25:40.800 --> 25:46.880
complex because what we do as humans is extremely complex. And we shouldn't be fooled that we're

25:46.880 --> 25:51.680
just a bunch of neurons that are doing some mass action. No, there's a very complex processing

25:51.680 --> 25:58.160
going on in your brain that it's not just a blob of neurons that are pulsating,

25:58.320 --> 26:03.680
you know, very detailed mechanisms that are undergoing it. And we figured out what some of

26:03.680 --> 26:12.400
those are. So describe to me what you mean by this phrase, a reference frame. What does that mean at

26:12.400 --> 26:19.760
the level of the cortex and cortical columns? Yeah. So we're jumping to the end point,

26:19.760 --> 26:23.680
because that's not where we started. We were trying to figure out how the cortical columns work.

26:24.400 --> 26:30.080
And what we realized is that they're little modeling engines. Each one of these cortical

26:30.080 --> 26:36.240
columns is able to build a model of its input. And that model is what we would call a sensory

26:36.240 --> 26:42.640
motor model. Let's assume it's getting in from your finger, a tip of your finger. One of the

26:42.640 --> 26:47.120
columns is getting input from the tip of your finger. And as your finger moves and touches

26:47.120 --> 26:52.800
something, the input changes. But it's not just sufficient to know how the input changes. For

26:52.880 --> 26:57.200
you to build a model of the object you're touching. And I use the coffee cup example quite a bit,

26:57.200 --> 27:00.560
because that's how we did it. If you move your finger over the coffee cup,

27:00.560 --> 27:03.520
and you're not even looking at the coffee cup, you could learn a model of the coffee cup. You

27:03.520 --> 27:07.280
could feel it just with one finger, you could feel like, oh, what this is what its shape is.

27:07.920 --> 27:11.840
But to do that, your brain, that cortical column, your brain as a whole, but that

27:11.840 --> 27:16.240
cortical column on individually has to know something about where your finger is relative

27:16.240 --> 27:21.200
to the cup. It's not just a changing pattern that's coming in. It has to know how your finger's

27:21.200 --> 27:26.640
moving and where your finger is as it touches it. So the idea of a reference frame is a way of

27:27.200 --> 27:31.440
noting a location. You have to have a location signal. You have to have some knowledge about

27:32.000 --> 27:36.480
where things are in the world relative to other things. In this case, where's your finger relative

27:36.480 --> 27:42.080
to the object you're trying to touch, the coffee cup. And we realize that for you to, your brain

27:42.080 --> 27:46.400
to make a prediction of what you're going to feel when you touch the edge of the cup. And again,

27:46.400 --> 27:49.440
you used to mention earlier, you're not conscious of this, you'd reach the cup and you just,

27:49.440 --> 27:52.080
but your brain's predicting what all your fingers are going to feel.

27:53.200 --> 27:57.120
It needs to know where the finger's going to be. And I have to know what the object is,

27:57.120 --> 28:01.040
it's a cup and these know where it's going to be. And that requires a reference frame.

28:01.040 --> 28:06.480
A reference frame is just a way of noting a location. It's saying relative to this cup,

28:06.480 --> 28:11.200
your finger is over here, not over there, not on the handle, you know, up at the top, whatever it is.

28:11.920 --> 28:16.400
And, and this is a deduced property, we can say for certainty that this has to exist.

28:16.480 --> 28:19.280
If your finger is going to make a prediction when it reaches and touches the coffee cup,

28:19.280 --> 28:22.960
it needs to know where the finger is, that the location has to be relative to the cup.

28:23.520 --> 28:28.080
So we can just say for certainty that there needs to be reference frames in the brain. And this is

28:28.080 --> 28:33.280
not a controversial idea. Well, we, perhaps this novel is that we realize that these reference

28:33.280 --> 28:38.400
frames exist in every cortical column. And it's the structure of knowledge. It applies to not just

28:38.400 --> 28:42.720
what your finger feels on a coffee cup and what you see when you look at it, but also how you

28:42.720 --> 28:48.480
arrange all your knowledge in the world is stored in these reference frames. And so we're jumping

28:48.480 --> 28:55.520
ahead here many steps. But when we think and when we posit, when we try to, you know, reason in our

28:55.520 --> 29:02.000
head, when even my language right now is, is where the neurons are walking through locations in

29:02.000 --> 29:06.400
reference frames, recalling the information stored there. And that's what comes into your head or

29:06.400 --> 29:11.360
that's what you say. So it becomes the core reference, the reference name becomes the core

29:11.360 --> 29:15.360
structure for the entire, everything you do. It's knowledge about the world is in these reference

29:15.360 --> 29:21.920
frames. Yeah, you make a strong claim about the, the primacy of motion, right? Because there's,

29:21.920 --> 29:28.080
everyone knows that there's part of the cortex devoted to motor action. We refer to it as the

29:28.080 --> 29:34.480
motor cortex and distinguish it from sensory cortex in that way. But it's also true that

29:34.480 --> 29:40.080
other regions of the cortex and, and perhaps every region of the cortex does have some

29:40.800 --> 29:47.680
connection to lower structures that can affect motion, right? So it's not, it's not that it's

29:47.680 --> 29:54.320
just motor cortex that's in the, in the motion game. And by analogy or by direct implication,

29:54.320 --> 30:02.480
you think of thought as itself being a kind of movement in conceptual space, right? So there's

30:02.480 --> 30:08.320
a mapping of the, the sensory world that can really only be accomplished by acting on it,

30:08.320 --> 30:13.840
you know, and therefore moving, right? So the other way to map the cup, you know, is to touch

30:13.840 --> 30:20.960
it with your fingers in the end. There is a, an analogous kind of motion in conceptual space

30:20.960 --> 30:26.240
and, you know, even, you know, abstract ideas like, I think some of the examples even in the book

30:26.240 --> 30:31.040
are like, you know, democracy, right? You know, or, or money or what, just how, how we understand

30:31.040 --> 30:36.240
these things. So let's go back to the first thing you said there. The idea that there's motor cortex

30:36.240 --> 30:43.760
and sensory cortex is sort of no longer considered right. As you mentioned, we, the neurons that,

30:43.760 --> 30:47.920
in these cortical columns, there are certain neurons that are the motor output neurons.

30:47.920 --> 30:52.880
These are in a particular layer five, as they're called. Until in the motor cortex,

30:52.880 --> 30:57.280
they were really big and they project to the spinal cord and say, oh, that's how you move your

30:57.280 --> 31:02.640
fingers. But if you look at the neurons, the columns in the visual cortex, the parts that get

31:02.640 --> 31:08.320
input from the eyes, they have the same layer five cells. And these cells project to a part

31:08.320 --> 31:13.360
of the brain called the superior colliculus, which is what controls eye motion. So this goes

31:13.360 --> 31:17.200
against the original idea of, oh, there's sensory cortex and motor cortex. No one believes that,

31:17.200 --> 31:21.680
well, I don't know, buddy, but very few people believe that anymore. It's, as far as we know,

31:21.680 --> 31:26.240
every part of the cortex has a motor output. And so every part of the cortex is getting some sort

31:26.240 --> 31:32.160
of input and it has some motor output. And so the basic algorithm of cortex is a sensory motor

31:32.160 --> 31:37.440
system. It's not divided. It's not like we have sensory areas and motor areas. As far as we know,

31:37.440 --> 31:42.320
ever it's been seen, there's these motor cells everywhere. So we can put that aside.

31:43.040 --> 31:52.160
Now, I can very clearly walk you through, in some sense, prove from logic that when you're

31:52.160 --> 31:57.200
learning what a coffee cup feels like, and I can even do this for vision, that you have to have this

31:57.200 --> 32:01.120
idea of a reference frame, that the finger, you have to know where your finger is relative to the

32:01.120 --> 32:05.680
cup. And that's how you build a model of it. And so we can build out this cortical column that

32:05.680 --> 32:10.080
explains how it does that. How does your, how does your parts of your cortex that representing

32:10.080 --> 32:15.120
your fingers are able to learn to structure a coffee cup? Now, Mount Castle, go back to him,

32:15.120 --> 32:20.320
he said, look, it's the same algorithm everywhere. And he says, it looks the same everywhere. So it's

32:20.320 --> 32:24.720
the same algorithm everywhere. So that should sort of say, hmm, well, if I'm thinking about something

32:24.720 --> 32:29.040
that doesn't seem like a sensory motor system, like I'm not touching something or looking,

32:29.040 --> 32:33.760
I'm just thinking about something, that would, if Mount Castle was right, then the same basic

32:33.760 --> 32:38.480
algorithm would be applying there. So that was one constraint like, well, that, you know, and the

32:38.480 --> 32:42.560
evidence is that Mount Castle is right. I mean, the physical evidence suggests he's right. We just,

32:42.560 --> 32:46.320
it just comes a little bit odd to think like, well, how is language like this and how is

32:46.320 --> 32:51.200
mathematics like, you know, touching a coffee cup. But then we realize that it's just,

32:51.200 --> 32:56.400
reference frames are a way of storing everything. And, and in the way we move through a reference

32:56.880 --> 32:59.520
frame, it's like, how do you move from one location? How do the neurons

33:00.160 --> 33:05.200
activate one location after another location after another location? We do that to this

33:05.200 --> 33:09.840
idea of movement. So I'm moving, if I want to access the locations on a coffee cup, I move my

33:09.840 --> 33:15.920
finger. But the same concept could apply to mathematics or to politics, but you're not actually

33:15.920 --> 33:21.440
physically moving something, but you're still walking through a structure. A good, a good bridge

33:21.440 --> 33:26.800
example is if I say to you, you know, imagine your house. And I ask you to walk, you know,

33:26.800 --> 33:31.840
tell me about your house. What you'll do is you'll mentally imagine walking through your house.

33:31.840 --> 33:36.000
It won't be random. You just won't have random thoughts come to your head. But you will mentally

33:36.000 --> 33:39.840
imagine walking through your house. And as you walk through your house, you'll recall what is

33:39.840 --> 33:42.880
supposed to be seen in different directions. You can say, oh, I'll walk in the front door and I'll

33:42.880 --> 33:47.120
look to the right. What do I see? I'll look to the left. What do I see? This is sort of a,

33:47.120 --> 33:50.960
an example you could relate it to something physically you could move to. But that's pretty

33:50.960 --> 33:54.880
much what's going on when you're thinking about anything. If you're thinking about your podcast

33:54.880 --> 33:59.120
and how you can get more subscribers, you have a model of that in your head. And you're, you are

34:00.320 --> 34:04.480
trying it out thinking about different aspects by literally invoking these different locations

34:04.480 --> 34:08.640
and reference frames. And so that's sort of the core of all knowledge.

34:08.640 --> 34:13.600
Yeah, it's interesting. I guess back to Mount Castle for a second. One piece of evidence in

34:13.600 --> 34:20.240
favor of this view of a common cortical algorithm is the fact that adjacent areas of cortex can be

34:21.200 --> 34:30.080
appropriated by various functions. If you lose your vision, say, classical visual cortex can be

34:30.080 --> 34:37.040
appropriated by other senses. And there's this plasticity that can ignore some of the previous

34:37.040 --> 34:43.120
boundaries between separate senses in the cortex. Yeah, that's right. There's this tremendous

34:43.120 --> 34:48.640
plasticity and you can also recover from various sorts of trauma and so on. I mean, there's some

34:48.640 --> 34:53.360
rewiring has to occur, but it does show that that whatever is going, whatever the circuitry in the

34:53.360 --> 34:59.520
visual cortex was, you know, quote, if you were a sighted person, what it would do, if you're not

34:59.520 --> 35:04.880
a sighted person, well, it'll just do something else. And so it's not, and so that is a very,

35:04.880 --> 35:10.960
very strong argument for that. There's a famous scientist, Bakurita, who did an experiment where

35:10.960 --> 35:16.480
he, I'm trying to remember the animal he used, maybe even recall it. But anyway, it'll come to me.

35:16.560 --> 35:22.560
A ferret, I think it was a ferret. Before the animal was born, he took the optic nerve

35:22.560 --> 35:26.400
and ran it over to one part of the, a different part of the neocortex and took the auditory nerve

35:26.400 --> 35:31.040
and ran it to a different part of the neocortex. Basically rewired the animal. I'm not sure we

35:31.040 --> 35:35.920
do these experiments today, but, and, you know, and the argument was that the animals, you know,

35:35.920 --> 35:41.920
still saw and still heard and so on, maybe not as well as an unaltered one, but the evidence was

35:41.920 --> 35:49.040
that, yeah, that really works. Yeah, so what is genetically determined and what is learned here?

35:49.040 --> 35:56.880
I mean, it seems that the genetics at minimum are determining what is hooked up to what initially,

35:56.880 --> 36:01.600
right, you know, barn? Yeah, roughly, roughly, that's right. I think, you know, like, where do the

36:01.600 --> 36:05.760
eyes, the optic nerve from the eyes, where do they project? And where do the regions that get

36:05.760 --> 36:11.520
the input from the eyes, where do they project? And so this rough sort of overall architecture

36:11.520 --> 36:16.640
is specified. And as we just talked through trauma and other reasons, sometimes that architecture

36:16.640 --> 36:23.040
can get rewired. I think also the, the, the basic algorithm that goes on in each of these

36:23.040 --> 36:27.840
cortical columns, the, the circuitry in the, in, inside the neocortex is pretty well determined by,

36:29.040 --> 36:34.640
by genetics. And in fact, what one of my guess's arguments was that humans, the human neocortex

36:34.640 --> 36:39.600
got large, and we have a very large one relative to our body size, just because all it had,

36:39.600 --> 36:43.200
all evolution had to do is discover, just make more copies of these columns, you know, you

36:43.200 --> 36:46.800
don't have to, you don't have to do anything new, just make more copies. And that's something easy

36:46.800 --> 36:52.640
for genes to specify. And so human brains got large quickly in evolutionary time,

36:52.640 --> 37:00.400
by that just replicate more of it type of thing. Okay, so let's go beyond the human now and talk

37:00.400 --> 37:07.840
about artificial intelligence. And before we talk about the risks or the imagined risks,

37:08.800 --> 37:14.240
tell me what you think the path looks like going forward. I mean, what are we doing now?

37:14.240 --> 37:20.800
And what do you think we need to do to have our dreams of true artificial general intelligence

37:20.800 --> 37:27.760
realized? Well, you know, today's AI is powerful as it is and successful as it is.

37:28.800 --> 37:35.520
I think most senior AI practitioners will admit, and many of them have, that they don't really

37:35.520 --> 37:39.840
think they're intelligent. You know, they're really wonderful pattern classifiers, and they

37:39.840 --> 37:45.040
can do all kinds of clever things. But there are very few practitioners would say, hey, this AI

37:45.040 --> 37:49.760
system that's recognizing faces is really intelligent. And there's a sort of a lack of

37:49.760 --> 37:54.960
understanding what intelligence is and how to go forward. And how do you make a system that could

37:54.960 --> 38:01.440
solve general problems, could do more than one thing, right? And so in the second part of my book,

38:01.440 --> 38:06.880
I lay out what I believe are the requirements to do that. And my approach has always been,

38:06.880 --> 38:10.880
for 40 years, has been like, well, I think we need to first figure out what brains do

38:11.920 --> 38:16.560
and how they do them. And then we'll know how to build intelligent machines, because we just

38:16.560 --> 38:23.680
don't seem able to intuit what an intelligent machine is. So I think the way I look at this

38:23.680 --> 38:27.840
problem, if we want to make, you know, what's the recipe for making an intelligent machine,

38:28.640 --> 38:32.640
is you have to say, what are the principles by which the brain works that we need to replicate

38:32.640 --> 38:37.520
and which principles don't we need to replicate? And so I made a list of these in the book, but

38:38.720 --> 38:42.080
if you can think of a very high level, they have to have some sort of embodiment,

38:42.080 --> 38:47.200
they have to have the ability to move their sensors somehow in the world. You know, you can't

38:47.200 --> 38:54.080
really learn how to use tools and how to, you know, run factories and how to do things unless

38:54.160 --> 38:58.960
you can move in the world. And it requires these reference frames I was talking about,

38:58.960 --> 39:02.800
because movement requires reference frames. But that's not a controversial statement,

39:02.800 --> 39:07.360
it's just a fact. You're going to have to have no where things are in the world.

39:08.000 --> 39:13.680
And then the final, there's a set of things, but one of the other big ones, which we haven't

39:13.680 --> 39:18.080
talked about yet, and which is where the title of the book comes from, A Thousand Brains, is that

39:18.640 --> 39:23.520
the way to think about our near cortex, it has 150,000 of these columns,

39:23.520 --> 39:29.120
we have essentially 150,000 separate modeling systems going on in our brain. And they work

39:29.120 --> 39:36.240
together by voting. And so that concept of a distributed intelligence system is important.

39:36.960 --> 39:41.520
We're not just one thing, we, it feels like we're one thing, but we're really 150,000 of these

39:41.520 --> 39:45.680
things. And we're only conscious of being one thing, but that's not really what's happening

39:45.680 --> 39:50.800
under the covers. So those are some of the key ideas. I've just picked a very, very high idea.

39:50.800 --> 39:54.880
It has to have an embodiment, it has to be able to move its sensors, it has to be able to

39:54.880 --> 40:00.320
organize information and reference frames, and it has to be distributed. And that's how we can do

40:00.960 --> 40:03.680
multiple sensors and sensory integration and things like that.

40:04.960 --> 40:13.840
I guess I question the criteria of embodiment and movement, right? I mean, I understand that

40:14.560 --> 40:20.480
practically speaking, that's how a useful intelligence can get trained up in our world

40:20.480 --> 40:27.520
to do things physically in our world. But it seems like you could have a perfectly intelligent

40:27.520 --> 40:37.440
system, i.e. a mind that is turned loose on simulated worlds and are capable of solving

40:37.440 --> 40:45.040
problems that don't require effectors of any kind. I mean, chess is obviously a very low-level

40:45.040 --> 40:50.240
analogy, but just imagine a thousand things like chess that represent real

40:51.680 --> 40:57.520
theory building or cognition in a box. Yeah, I think you're right. And so

40:58.160 --> 41:02.720
when I use the word movement or embodiment, and I'm careful to define it in the book because

41:03.520 --> 41:10.480
it doesn't have to be physical. An example I gave, you can imagine an intelligent agent that

41:10.480 --> 41:16.240
lives in the Internet, and movement is following links. It's not a physical thing,

41:16.800 --> 41:22.880
but there's still this conceptual mathematical idea of what it means to move. And so you're

41:22.880 --> 41:29.520
changing the location of some representation, and that could be virtual. It doesn't have to

41:29.520 --> 41:35.520
have a physical embodiment, but in the end, you can't learn about the world just by looking at

41:35.520 --> 41:42.720
a set of pictures. That's not going to happen. You can learn to classify pictures, but so some

41:42.720 --> 41:49.040
AI systems will have to be physically embodied like a robot, if I guess you want. Many will not

41:49.040 --> 41:54.880
be, many will be virtual, but they all have this internal process which I could point to the thing

41:54.880 --> 41:58.560
that says, here's where the reference frame is, here's where your current location is, here's

41:58.560 --> 42:03.920
how it's moving to a new location based on some movement vector. Like a verb, a word, you can

42:03.920 --> 42:08.800
think of that as like an action. And so you can have an action that's not physical, but it's

42:08.800 --> 42:12.480
still an action, and it moves to a new location in this internal representation.

42:12.480 --> 42:17.680
Right, right. Okay, well, let's talk about risk, because this is the place where I think you and

42:17.680 --> 42:25.760
I have very different intuitions. You are, as far as I can tell from your book, you seem very

42:25.760 --> 42:35.200
sanguine about AI risk. And really, you seem to think that the only real risk, the serious risk

42:35.200 --> 42:42.080
of things going very badly for us is that bad people will do bad things with much more powerful

42:42.080 --> 42:47.760
tools. So the heuristic here would be, you know, don't give your super intelligent AI to the next

42:47.760 --> 42:54.400
Hitler, because that would be bad. But other than that, the generic problem of self-replication,

42:54.400 --> 42:59.920
which you talk about briefly, and you point out, we face that on other fronts, like with,

42:59.920 --> 43:03.920
you know, with the pandemic, where we've been dealing with, I mean, so natural viruses and

43:03.920 --> 43:09.840
bacteria or computer viruses, I mean, there's anything that can self-replicate can be dangerous.

43:10.480 --> 43:17.840
But that aside, you seem quite confident that AI will not get away from us. There won't be an

43:17.840 --> 43:23.920
intelligence explosion. And we don't have to worry too much about the so-called alignment problem.

43:24.960 --> 43:29.600
And at one point, you even question whether it makes sense to expect that we'll produce

43:30.560 --> 43:35.360
something that can be appropriately called superhuman intelligence. So Brett, perhaps you

43:35.360 --> 43:42.960
can explain the basis for your optimism here. So I think what most people, and perhaps yourself,

43:43.600 --> 43:51.200
have fears about is they use humans as an example of how things can go wrong.

43:51.200 --> 43:56.080
And so we think about the alignment problem, or we think about, you know, motivations of an AI

43:56.080 --> 44:01.680
system. Well, okay, does the AI system have motivations or not? Does it have a desire to do

44:01.680 --> 44:09.440
anything? Now, as a human, an animal, we all have desires, right? But if you take apart what parts

44:09.440 --> 44:15.040
of the human brain are doing, different parts, there's some parts that are just building this

44:15.040 --> 44:20.560
model of the world. And this is the core of our intelligence. This is what it means to be intelligent.

44:20.560 --> 44:27.040
That part itself is benign. It has no motivations on its own. It doesn't desire to do anything.

44:27.040 --> 44:33.920
I use an example of a map. You know, a map is a model of the world. And my map can be

44:34.640 --> 44:40.960
very powerful tool for some to do good or to do bad. But on its own, the map doesn't do anything.

44:41.520 --> 44:46.240
So if you think about the neocortex on its own, it sits on top of the rest of your brain.

44:46.880 --> 44:51.760
And the rest of your brain is really what makes us motivated. It gets us, you know, we have our

44:52.880 --> 44:58.160
good sides and our bad sides, you know, our desire to maintain our life and have sex and

44:58.160 --> 45:01.840
aggression and all these stuff. The neocortex is just sitting there. It's like a map. It says,

45:01.840 --> 45:06.800
you know, I understand the world and you can use me as how you want. So when we build intelligent

45:06.800 --> 45:13.120
machines, we have the option and, and I think almost imperative not to build the old parts

45:13.120 --> 45:18.720
of the brain, too. You know, why do that? We just have this thing, which is inherently smart,

45:18.720 --> 45:23.680
but on its own doesn't really want to do anything. And so there's some of the some of the risks that

45:23.680 --> 45:31.120
come about from the people's fears about the alignment problem, specifically, is that the

45:32.000 --> 45:36.800
intelligent agent will decide on its own or decide for some reason to do things that are

45:36.800 --> 45:41.120
in its best interest, not in our best interest, or maybe it'll listen to us, but then not listen

45:41.120 --> 45:47.120
to us or something like this. I just don't see how that can physically happen. And for people,

45:47.120 --> 45:51.360
most people don't understand the separation. They just assume that this intelligence is wrapped up

45:51.360 --> 45:55.920
in these, all these, all the things that make us human. The intelligence explosion problem is a

45:55.920 --> 46:01.040
separate issue. I'm not sure which one of those you're more worried about. Yeah, well, let's,

46:01.040 --> 46:07.040
let's deal with the alignment issue first. And I do think that's more critical. But

46:08.000 --> 46:13.440
let me see if I can capture what troubles me about this picture you've painted here. It seems

46:13.440 --> 46:23.280
that you're, to my mind, you're being strangely anthropomorphic on one side, but not anthropomorphic

46:23.280 --> 46:29.920
enough on the other. I mean, so like, you know, you think that to understand intelligence and

46:30.000 --> 46:37.120
actually truly implement it in machines, we really have to be focused on ourselves first. And we

46:37.120 --> 46:43.840
have to understand how the human brain works and then emulate those principles pretty directly in

46:43.840 --> 46:50.320
machines. That strikes me as possibly true, but possibly not true. And if, if I had to bet, I

46:50.320 --> 46:58.720
think I would probably bet against it. Although even here, you seem to be not taking full account of

46:59.280 --> 47:04.240
what the human brain is doing. I mean, like, we, you know, we can't partition reason and emotion

47:05.040 --> 47:10.480
as clearly as we thought we could hundreds of years ago. And in fact, you know, certain emotions,

47:10.480 --> 47:15.840
you know, certain drives are built into our being able to reason effectively.

47:15.840 --> 47:20.720
I think that's, you know, I'll take an exception to that. I know, I know this is an opinion that

47:21.360 --> 47:24.160
you had Lisa Barrett on your program recently.

47:24.800 --> 47:27.920
Antonio Demasio is the person who's banged on about this the most.

47:27.920 --> 47:33.120
Yeah, I know. And I just disagree. I just, it's, you know, you can separate these two.

47:33.680 --> 47:40.320
And I can say this because I understand actually what's going on in the New York Cortex.

47:40.320 --> 47:44.720
And I can see what I have a very good sense of what these actual neurons are actually doing

47:44.720 --> 47:51.120
when it's modeling the world and so on. And you do not, it does not require this emotional component.

47:51.200 --> 47:56.640
A human does. Now, you say, you know, on one hand, I don't argue we should replicate the brain.

47:56.640 --> 48:01.040
I say we should replicate the structures of the New York Cortex, which is not replicating the brain.

48:01.760 --> 48:06.800
It's just one part of the brain. And so I'm specifically saying, you know, I don't really

48:06.800 --> 48:11.920
care too much about how this spinal cord works or how, you know, the brainstem does this or that.

48:11.920 --> 48:15.920
It's interesting. Maybe I know a little bit about it, but that's not important. The cortex sits on

48:15.920 --> 48:20.160
top of another structure and the cortex does its own thing and they interact. Of course,

48:20.240 --> 48:24.560
they interact. And our emotions affect what we learn and what we don't learn.

48:24.560 --> 48:29.600
But it doesn't have to be that way in a system, another system that we build.

48:29.600 --> 48:30.800
That's the way humans are structured.

48:30.800 --> 48:34.640
Yeah. Okay. So I would agree with that except the boundary between

48:35.520 --> 48:43.920
what is an emotion or a drive or a motivation or a goal and what is a value neutral mapping of

48:43.920 --> 48:51.680
reality. You know, I think that boundary is perhaps harder to specify than you think it is.

48:51.680 --> 48:57.200
And that certain of these things are connected, right? Which is to, I mean, here's an example.

48:57.200 --> 49:03.120
This is probably not a perfect analogy, but this gets at some of the surprising features of cognition

49:03.120 --> 49:12.160
that may await us. So we think intuitively that understanding a proposition is cognitively quite

49:12.240 --> 49:18.000
distinct from believing it, right? So I can give you a statement that you can believe or

49:18.000 --> 49:22.560
disbelieve or be uncertain about. I can say, you know, there's two plus two equals four,

49:22.560 --> 49:28.160
two plus two equals five, and that can give you some gigantic number and say this number is prime.

49:28.160 --> 49:33.120
And presumably in the first condition, you'll say, yes, I believe that. In the second, you'll say,

49:33.120 --> 49:38.240
no, that's false. And in the third, you won't know whether or not it's prime or not.

49:38.720 --> 49:43.600
So those are distinct states that we can intuitively differentiate. But there's also evidence

49:44.240 --> 49:49.440
to suggest that merely comprehending a statement, if I give you a statement and you

49:49.440 --> 49:57.520
parse it successfully, the parsing itself contains an actual default acceptance of it as true.

49:58.240 --> 50:05.120
And rejecting it as false is a separate operation added to that. I mean, there's not a ton of

50:05.120 --> 50:10.160
evidence for this, but there's certainly some behavioral evidence. So if I put you in a paradigm

50:10.160 --> 50:15.040
where we gave you statements that were true and false, and all you had to do was judge them true

50:15.040 --> 50:21.680
and false, and they were all matched for complexity. So, you know, two plus two equals four is no more

50:21.680 --> 50:27.360
or less complex than two plus two equals five. But it'll take you longer, systematically longer,

50:27.360 --> 50:32.000
to judge very simple statements to be false than to judge them to be true,

50:32.000 --> 50:37.280
suggesting that you're doing a further operation. Now, we can remain agnostic as to whether or

50:37.280 --> 50:43.120
not that's actually true. But if true, it's counterintuitive that merely understanding

50:43.120 --> 50:49.840
something entails some credence, epistemic credence given to it by default, and that

50:49.840 --> 50:56.320
to reject it as false represents a subsequent act. But that's the kind of thing that already

50:56.400 --> 51:03.360
were on territory that is not coldly rational. Some of the all too apish appetites have kind

51:03.360 --> 51:10.800
of crept into cognition here in ways that we didn't really budget for. And so the question is,

51:10.800 --> 51:15.200
just how much of that is avoidable in building a new type of mind?

51:16.160 --> 51:21.600
Well, you know, I'm not familiar with that specific research. And so I haven't heard of that. But

51:22.320 --> 51:29.520
to me, none of these things are surprising in any way. Just if you start thinking about the brain

51:29.520 --> 51:33.440
is basically trying to build models, it's constantly trying to build models. In fact,

51:34.640 --> 51:39.280
as you walk around your life, day to day, moment to moment, and you see things,

51:39.280 --> 51:42.480
you're building the model, the model is being constructed, even like where are things in the

51:42.480 --> 51:46.160
refrigerator right now, your brain will update, you open the refrigerator, oh, the milk's on the

51:46.160 --> 51:50.480
left today, whatever. And so if someone gives you a proposition like two plus two equals five,

51:51.040 --> 51:54.560
you know, I don't know what the evidence that you believe it and then falsify it.

51:54.560 --> 51:58.720
But I certainly imagine you can imagine it trying to see if it's right. It'd be like me saying to

51:59.360 --> 52:02.720
you, hey, you know, Sam, the milk was on the right in your refrigerator. And you'd have to

52:02.720 --> 52:06.800
think about it for a second. You say, well, let me think. No, last time I saw it was on the left.

52:06.800 --> 52:11.360
You know, no, that's wrong. But you would walk through the process of trying to imagine it

52:11.360 --> 52:17.360
and trying to see, does that fit my model? And yes or no. And I don't, it's not surprising to me

52:17.360 --> 52:22.800
that you would have to process it the way as if it was true. It's just matters saying,

52:22.800 --> 52:27.520
can you imagine this? Go imagine it. Do you think it's right? It's not like I believe that now I

52:27.520 --> 52:32.480
falsified it. It's more likely. Well, actually, I'll just give you one other datum here because

52:32.480 --> 52:38.560
it's just intellectually interesting and socially all too consequential. This effect goes by

52:38.560 --> 52:43.760
several names, I think. But one is the illusory truth effect, which is even in the act of

52:43.760 --> 52:49.680
disconfirming something to be false, you know, some specious rumor or conspiracy theory,

52:50.320 --> 52:55.680
merely having to invoke it, I mean, to have people entertain the concept again, even in the context

52:55.680 --> 53:04.000
of debunking it, ramifies a belief in it in many, many people. It's just, it becomes harder to

53:04.000 --> 53:06.640
discredit things because you have to talk about them in the first place.

53:06.640 --> 53:12.320
Yeah. I mean, so look, we're talking about language here, right? And in language,

53:12.400 --> 53:17.360
so much of what we humans know is via language. And we have no idea if it's true when someone says

53:17.360 --> 53:24.240
something to you, right? How do you know? And so you'd have to, so I mean, I gave an example like,

53:24.240 --> 53:28.880
I've never been to the city of Havana. Well, I believe it's there. I believe it's true. I don't

53:28.880 --> 53:33.120
know. I've never been there. I've never actually touched or smelled it or saw it. So maybe it's

53:33.120 --> 53:38.720
false. So I just, I mean, this is one of the issues we have. I have a whole chapter on false

53:38.720 --> 53:45.040
beliefs because so much of our knowledge of the world is built up on language. And the default

53:45.040 --> 53:50.480
assumption under language that if someone says something, it's true. It's like, it's a pattern

53:50.480 --> 53:54.800
in the world. You're going to accept it. If I touch a coffee cup, I accept that that's what it feels.

53:54.800 --> 53:59.280
Right. And if I look at something, I accept that's what it looks like. Well, if someone says

53:59.280 --> 54:04.160
something, my initial acceptance is, okay, that's what it is. So, you know, and then I'm going to

54:04.160 --> 54:08.240
say, in fact, well, if someone says something that's false, of course, well, that's a problem

54:08.240 --> 54:14.080
because just by the fact that I've experienced it, it's now part of my world model. And that's

54:14.080 --> 54:19.120
what you're referring to. I can see this is really a problem of language we face. And this is the

54:19.120 --> 54:23.840
root cause of almost all of our false beliefs, is that someone just says something enough times.

54:24.640 --> 54:30.240
And that's good enough. And you have to seek out contrary evidence for it.

54:30.720 --> 54:34.000
Yeah, sometimes it's good enough. Even when you're the one saying it, you just overhear

54:35.280 --> 54:42.080
the voice of your own mind saying it. And no, I know. That's been proven that everyone is

54:42.080 --> 54:46.800
susceptible to that kind of distortion of our beliefs, especially our memories,

54:46.800 --> 54:48.800
just remembering something over and over again changes it.

54:48.800 --> 54:55.200
Yeah. Okay, so let's get back to AI risk here because here's where I think you and I

54:55.200 --> 54:59.520
have very different intuitions. I mean, the intuition that many of us have,

55:00.320 --> 55:05.120
you know, that the people who have informed my views here, people like Stuart Russell,

55:05.120 --> 55:11.280
who you probably know at Berkeley, and Nick Bostrom, and Eleazar Yudkowski, and just lots of

55:11.280 --> 55:18.400
people in this spot worrying about the same thing to one another degree. The intuition is that

55:18.960 --> 55:27.920
you don't get a second chance to create a truly autonomous superintelligence. It seems that in

55:27.920 --> 55:33.360
principle, this is the kind of thing you have to get right on the first try. And having to get

55:33.360 --> 55:39.280
anything right on the first try just seems extraordinarily dangerous because we rarely,

55:39.280 --> 55:45.280
if ever, do that when doing something complicated. And another way of putting this is that it seems

55:45.280 --> 55:53.120
like in the space of all possible superintelligent minds, there are more ways to build one that

55:53.120 --> 56:00.720
isn't perfectly aligned with our long-term well-being than there are ways to build one

56:00.720 --> 56:07.360
that is perfectly aligned with our long-term well-being. And from my point of view, what

56:08.400 --> 56:13.280
your optimism and the optimism of many other people who take your side of this debate

56:14.000 --> 56:22.960
is based on is not really taking the prospect of intelligence seriously enough and the autonomy

56:22.960 --> 56:31.440
that is intrinsic to it. I mean, if we actually built a true general intelligence, what that means

56:31.440 --> 56:37.920
is that we would suddenly find ourselves in relationship to something that we actually

56:38.480 --> 56:45.760
can't perfectly understand. It's like it will be analogous to a strange person walking into the room,

56:45.760 --> 56:52.400
you know, you're in relationship. And if this person can think a thousand times or a million

56:52.400 --> 56:59.920
times faster than you can and has goals that are less than perfectly aligned with your own,

57:01.040 --> 57:06.560
that's going to be a problem eventually. We can't find ourselves in a state of perpetual

57:06.560 --> 57:12.240
negotiation with systems that are more competent and powerful and intelligent.

57:12.240 --> 57:17.280
I think there's two mistakes in your argument. The first one is you say

57:17.280 --> 57:22.800
my intuition and your intuition. I think most of the people who have this fear have an intuition

57:22.800 --> 57:33.280
about what happened. If you'd like to continue listening to this conversation,

57:33.280 --> 57:38.160
you'll need to subscribe at SamHarris.org. Once you do, you'll get access to all full-length

57:38.160 --> 57:43.280
episodes of the Making Sense podcast, along with other subscriber-only content, including bonus

57:43.280 --> 57:48.640
episodes and AMAs and the conversations I've been having on the Waking Up app. The Making Sense

57:48.640 --> 57:56.000
podcast is ad-free and relies entirely on listener support, and you can subscribe now at SamHarris.org.

