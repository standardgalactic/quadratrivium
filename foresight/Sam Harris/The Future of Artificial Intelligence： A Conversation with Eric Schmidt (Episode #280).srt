1
00:00:00,000 --> 00:00:12,200
Welcome to the Making Sense podcast. This is Sam Harris. Just a note to say that if

2
00:00:12,200 --> 00:00:16,400
you're hearing this, you are not currently on our subscriber feed and will only be hearing

3
00:00:16,400 --> 00:00:20,880
the first part of this conversation. In order to access full episodes of the Making Sense

4
00:00:20,880 --> 00:00:26,440
podcast, you'll need to subscribe at SamHarris.org. There you'll find our private RSS feed to

5
00:00:26,440 --> 00:00:31,080
add to your favorite podcatcher, along with other subscriber-only content. We don't run

6
00:00:31,080 --> 00:00:35,000
ads on the podcast, and therefore it's made possible entirely through the support of our

7
00:00:35,000 --> 00:00:38,840
subscribers. So if you enjoy what we're doing here, please consider becoming one.

8
00:00:46,600 --> 00:00:53,640
Okay, jumping right into it today. Today I'm speaking with Eric Schmidt. Eric is a technologist,

9
00:00:53,640 --> 00:01:02,200
entrepreneur, and philanthropist. He joined Google in 2001 and served as its CEO and chairman from

10
00:01:02,200 --> 00:01:11,080
2001 to 2011, and as executive chairman and technical advisor thereafter. In 2017, he co-founded

11
00:01:11,080 --> 00:01:16,280
Schmidt Futures, a philanthropic initiative that bets early on exceptional people who are helping

12
00:01:16,280 --> 00:01:21,960
to make the world better. He is the host of Reimagine with Eric Schmidt, his own podcast,

13
00:01:22,920 --> 00:01:28,520
and most recently, he's the co-author of a new book, The Age of AI and Our Human Future.

14
00:01:29,320 --> 00:01:35,720
And that is the topic of today's conversation. We cover how AI is affecting the foundations of

15
00:01:35,720 --> 00:01:41,400
our knowledge and how it raises questions of existential risk. So we talk about the good and

16
00:01:41,400 --> 00:01:49,400
the bad of AI, both narrow AI and ultimately, AGI, Artificial General Intelligence.

17
00:01:50,360 --> 00:01:56,440
We discuss breakthroughs in pharmaceuticals and other good things, but we also talk about

18
00:01:57,320 --> 00:02:04,040
cyber war and autonomous weapons and how our thinking about containing the risk here

19
00:02:04,040 --> 00:02:10,440
by analogy to the proliferation of nuclear weapons probably needs to be revised.

20
00:02:11,320 --> 00:02:14,840
Anyway, an important conversation, which I hope you find useful.

21
00:02:15,800 --> 00:02:17,560
And I bring you Eric Schmidt.

22
00:02:24,200 --> 00:02:27,000
I am here with Eric Schmidt. Eric, thanks for joining me.

23
00:02:27,560 --> 00:02:28,440
Glad to be with you.

24
00:02:30,440 --> 00:02:35,080
I think we have a hard hour here, so amazingly, that's a short podcast for me,

25
00:02:35,080 --> 00:02:39,800
so there's going to be a spirit of urgency hanging over the place, and we will be

26
00:02:40,440 --> 00:02:46,680
efficient in covering the fascinating book that you have written with Henry Kissinger and Daniel

27
00:02:46,680 --> 00:02:51,080
Huttenlocher. That's right. And Dr. Kissinger, of course, is the former Secretary of State,

28
00:02:51,080 --> 00:02:57,080
and Dan Huttenlocher is now the Dean of Artificial Intelligence and Computer Science

29
00:02:57,080 --> 00:03:01,000
at the Schwartzman Center at MIT. He's a proper computer scientist.

30
00:03:01,000 --> 00:03:06,360
Yeah, and that book is The Age of AI and Our Human Future, where you cover,

31
00:03:07,080 --> 00:03:13,640
most of what I have said about AI thus far and every case where I have worried about

32
00:03:13,640 --> 00:03:20,280
our possible AI future has been focused on the topic of AGI, Artificial General Intelligence,

33
00:03:20,280 --> 00:03:25,080
which you discuss briefly in the book, but it's not your main focus. So I thought maybe we could

34
00:03:25,080 --> 00:03:30,920
save that for the end, because I would love to get your take on AGI, but there are far more near

35
00:03:30,920 --> 00:03:38,200
term concerns here and considerations that we could cover, and you are quite well placed

36
00:03:38,200 --> 00:03:44,360
to cover them, because if I'm not mistaken, you ran Google for what was it, 10 years?

37
00:03:44,360 --> 00:03:50,280
That's correct. What was your background before that? How did you come to be the CEO of Google?

38
00:03:50,280 --> 00:03:56,440
Well, I'm a computer scientist. I have a PhD in the area, and I worked for 45 years in tech,

39
00:03:56,440 --> 00:04:00,680
in one way or the other, a whole bunch of companies. Larry and Sergey brought me in

40
00:04:00,680 --> 00:04:06,840
as the early CEO of the company, and we built it together. After a decade, I became chairman, Larry

41
00:04:06,840 --> 00:04:11,960
became CEO, and then he replaced himself with Sundar, who's now doing a phenomenal job at Google.

42
00:04:12,600 --> 00:04:17,320
So I'd say collectively, this group of which I'm a member built one of the great companies,

43
00:04:17,320 --> 00:04:23,960
and I'm really proud of that. Yeah, and obviously, Google is quite involved in developing AI. I just

44
00:04:23,960 --> 00:04:31,000
saw just the other day that there's a new, I think it's a 540 billion parameter language model

45
00:04:31,000 --> 00:04:38,760
that is beating the average human at something like 150 cognitive tests now, and it seems like

46
00:04:39,560 --> 00:04:42,760
the light is at the end of the tunnel there. I mean, it's just going to be a larger model

47
00:04:42,760 --> 00:04:48,120
that's going to beat every human at those same tasks. But before we get into some of the details

48
00:04:48,120 --> 00:04:54,920
here, I just wanted to organize our general approach to this. There are three questions that

49
00:04:54,920 --> 00:05:01,480
Kant asked, and his critique of pure reason, I think it was, which seem unusually relevant

50
00:05:02,120 --> 00:05:09,080
to the development of AI. The first is, what can we know? The second is, what should we do?

51
00:05:10,280 --> 00:05:17,720
And the third is, what is it reasonable to hope for? And I think those are really do capture

52
00:05:18,600 --> 00:05:25,960
almost every aspect of concern here, because as you point out in the book, AI really promises to,

53
00:05:25,960 --> 00:05:31,640
and it has already begun to shift the foundations of human knowledge. So the question of what we

54
00:05:31,640 --> 00:05:37,000
can know and how we can know it is enormously salient now, and maybe we can talk about some

55
00:05:37,000 --> 00:05:41,720
of those examples. But obviously, this question of what should we do and what can we reasonably

56
00:05:41,720 --> 00:05:49,320
hope for captures the risks we're running in developing these systems. And we're running

57
00:05:49,320 --> 00:05:55,720
these risks well short of producing anything like artificial general intelligence. And it's

58
00:05:55,720 --> 00:06:02,520
interesting that we're on a path now where we're really not free to decline to produce this technology.

59
00:06:02,520 --> 00:06:08,680
I mean, to my eye, there's really no break to pull. I mean, we're in a kind of AI arms race now.

60
00:06:09,400 --> 00:06:16,200
And the question is how to put that race for more intelligence on a footing that is not running

61
00:06:16,200 --> 00:06:22,360
cataclysmic risk for us. So before we jump into the details, I guess I'd like to get

62
00:06:22,360 --> 00:06:29,160
your general thoughts on how you view the stakes here and where you view the field to be at the

63
00:06:29,160 --> 00:06:35,560
moment. Well, of course, we wrote the book, Age of AI, precisely to help answer the questions

64
00:06:35,560 --> 00:06:42,200
you're describing, which are perfectly cast. And what's happened in the book, which is written

65
00:06:42,200 --> 00:06:48,600
roughly a year ago and then published, we described a number of examples to illustrate the point. One

66
00:06:48,600 --> 00:06:55,640
is the development of new moves in the game of Go, which is 2,500 years, which were discovered by

67
00:06:55,640 --> 00:07:01,240
computer, which humans had never discovered. It's hard to imagine that humans wouldn't have

68
00:07:01,240 --> 00:07:05,720
discovered these strategies, but they didn't. And that calls the question of are there

69
00:07:06,280 --> 00:07:13,400
things which AI can learn that humans cannot master? That's a question. The second example that we use

70
00:07:13,400 --> 00:07:19,000
is the development of a new drug called Hallison, which is a broad spectrum antibiotic, which could

71
00:07:19,000 --> 00:07:25,640
not be done by humans, but a set of neuroscientists, biologists, and computer scientists put together

72
00:07:25,720 --> 00:07:30,040
a set of programs that ultimately searched through 100 million different compounds

73
00:07:30,760 --> 00:07:36,200
and came up with candidates that were then subsequently tested, advancing drugs at an

74
00:07:36,200 --> 00:07:41,720
enormous rate. That's another category of success in AI. And then the third is what you've already

75
00:07:41,720 --> 00:07:46,920
mentioned, which is large language models. And we profile in the book, GPT-3, which is the

76
00:07:46,920 --> 00:07:53,000
predecessor of the one you described. And it's eerie. On the back cover of our book, we say,

77
00:07:53,000 --> 00:07:59,960
to the GPT-3 computer, are you capable of human reasoning? And it answers, oh, I am not.

78
00:08:01,000 --> 00:08:07,080
You may wonder why I give you that answer. And the answer is that you are a human reasoning

79
00:08:07,080 --> 00:08:13,560
machine, whereas I am a language model that's not been taught how to do that. Now, is that

80
00:08:13,560 --> 00:08:21,240
awareness or is that clever mimicry? We don't know. But each of these three examples show the

81
00:08:21,240 --> 00:08:27,960
potential to answer consquestions. What can we know? What will happen? What can we do about it?

82
00:08:27,960 --> 00:08:32,840
Since then, this past few weeks, we've seen the announcement that you mentioned of this

83
00:08:32,840 --> 00:08:39,160
enormous large language model, which can beat humans on many things. And we've also seen something

84
00:08:39,160 --> 00:08:44,600
called DAAL-E, which is a text to art program. You describe roughly what you want, and it can

85
00:08:44,600 --> 00:08:50,280
generate art for you. Now, these are the beginnings of the impact of artificial intelligence on us

86
00:08:50,280 --> 00:08:56,760
as humans. So Dr. Kissinger, Dan, and myself, when we looked at those, we thought, what happens to

87
00:08:56,760 --> 00:09:02,920
society when you have these kinds of intelligence? Now, they're not human intelligence. They're

88
00:09:02,920 --> 00:09:08,600
different kinds of intelligence in everyday life. And we talk about all the positives of which

89
00:09:08,600 --> 00:09:15,000
they're incredible positives, better materials, better drugs, more efficient systems, better

90
00:09:15,000 --> 00:09:20,520
understanding, better monitoring of the earth, additional solutions for climate change. There's

91
00:09:20,520 --> 00:09:25,800
a long, long list, which I can go through. Very, very exciting. And indeed, in my personal

92
00:09:25,800 --> 00:09:32,200
philanthropy, we are working really hard to fund AI-enabled science discoveries. We recently announced

93
00:09:32,760 --> 00:09:39,320
a grant, a structure with a guy named James Manika, who's a friend of mine, to $125 million

94
00:09:40,280 --> 00:09:47,320
to actually go and fund research on the really hard problems in AI, the ones that you're mentioning

95
00:09:47,320 --> 00:09:51,800
and others, and also the economic impacts and so forth. Because I think people don't really know.

96
00:09:51,800 --> 00:09:57,240
The real question is, what happens when these systems become more commonplace?

97
00:09:58,120 --> 00:10:03,800
Dr. Kissinger says, if you look at history, when a system that is not understandable

98
00:10:04,440 --> 00:10:10,120
is imposed on people, they do one of two things. They either invent it as a religion,

99
00:10:11,240 --> 00:10:17,880
or they fight it with guns. So my concern, and I'll say it very directly, is we're playing with

100
00:10:18,760 --> 00:10:25,800
the information space of humans. We're experimenting at scale without a set of principles

101
00:10:25,800 --> 00:10:30,840
as to what we want to do. Do we care more about freedom? Do we care more about efficiency? Do we

102
00:10:30,840 --> 00:10:37,400
care more about education and so forth? And Dr. Kissinger would say, the problem is that these

103
00:10:37,400 --> 00:10:42,680
decisions are being made by technical people who are ignorant of the philosophical questions that

104
00:10:42,680 --> 00:10:50,840
you so ably asked. And I agree with him, speaking as an example of that. So we recommend, and indeed

105
00:10:50,840 --> 00:10:57,480
I'm trying to now fund, that people begin in a multidisciplinary way to discuss the implications

106
00:10:57,480 --> 00:11:03,640
of this. What happens to national security? What happens to military intelligence? What happens to

107
00:11:03,640 --> 00:11:09,320
social media? What happens to your children when your child's best friend is a computer?

108
00:11:10,440 --> 00:11:17,000
And for the audience who might be still thinking about the killer robot, we're not building killer

109
00:11:17,000 --> 00:11:23,320
robots, and I hope we never do. This is really about information systems that are human-like,

110
00:11:23,400 --> 00:11:27,880
that are learning, they're dynamic, and they're emergent, and they're imprecise,

111
00:11:28,600 --> 00:11:37,080
being used and imposed on humans around the world. That process is unstoppable. It's simply too many

112
00:11:37,080 --> 00:11:42,040
people working on it, too many ways in which people are going to manipulate it, including

113
00:11:42,040 --> 00:11:48,200
for hostile reasons, too many businesses being built, and too much success for some of the early

114
00:11:48,200 --> 00:11:54,920
work. Yeah, I guess if I can just emphasize that point, the unstoppable is pretty interesting,

115
00:11:54,920 --> 00:12:02,520
because it's just anchored to this basic fact that intelligence is, almost by definition,

116
00:12:02,520 --> 00:12:08,920
the most valuable thing on earth. And if we can get more of it, we're going to. And we clearly can,

117
00:12:09,560 --> 00:12:15,320
and all of these narrow intelligences we've built thus far, all that are effective, that come to

118
00:12:15,400 --> 00:12:22,760
market that we pour resources into, are superhuman more or less right out of the gate. It's not a

119
00:12:22,760 --> 00:12:29,560
question of human level intelligence, it's a bit of a mirage, because the moment we get something

120
00:12:29,560 --> 00:12:34,920
that's general, it's going to be superhuman. And so we can leave the generality aside. All of these

121
00:12:34,920 --> 00:12:43,320
piecemeal intelligences are superhuman. And the example you give of the new antibiotic,

122
00:12:43,320 --> 00:12:50,520
Hallison, it's fascinating because it's not just a matter of doing human work faster. If I understand

123
00:12:50,520 --> 00:12:58,600
what happened in that case, this is an AI detecting patterns and relationships in molecules already

124
00:12:58,600 --> 00:13:07,880
known to be safe and efficacious as antibiotics, and detecting new properties that human beings

125
00:13:07,960 --> 00:13:15,400
very likely would never have conceived of and may in fact be opaque to the people who built the AI

126
00:13:15,400 --> 00:13:19,960
and may remain opaque. I mean, one of the issues you just raised is the issue of transparency.

127
00:13:20,520 --> 00:13:27,640
Many of these systems are built in such a way as to be black boxes. And we don't know how the AI is

128
00:13:27,640 --> 00:13:34,920
doing what it's doing in any specific way. It's just training against data and against its own

129
00:13:34,920 --> 00:13:41,720
performance so as to produce a better and better result, which qualifies as intelligent and even

130
00:13:41,720 --> 00:13:47,720
superhumanly so. And yet it may remain a black box. Maybe we can just close the loop on that

131
00:13:47,720 --> 00:13:57,720
specific problem here. Are you concerned that transparency is a necessity when decision making

132
00:13:57,720 --> 00:14:03,560
is important? I mean, just imagine the case where we have something like an AI oracle

133
00:14:04,280 --> 00:14:09,960
that we are convinced makes better decisions than any person or even any group of people,

134
00:14:10,600 --> 00:14:14,360
but we don't actually know the details of how it's making those decisions.

135
00:14:16,520 --> 00:14:22,280
I mean, you can just multiply examples as you like, but just questions of who should get out

136
00:14:22,280 --> 00:14:29,480
of prison, the likelihood of recidivism in the case of any person or who's likely to be more

137
00:14:29,480 --> 00:14:36,040
violent at the level of conviction. What should the prison sentence be? It's very easy to see that

138
00:14:36,040 --> 00:14:44,520
if we're shunting that to a black box, people are going to get fairly alarmed that in any

139
00:14:44,520 --> 00:14:50,520
differences in outcome that are not transparent. Perhaps you have other examples of concern,

140
00:14:50,520 --> 00:14:56,680
but do you think transparency is something that... I mean, one question is technically feasible to

141
00:14:56,680 --> 00:15:03,720
render black boxes transparent when it matters, and two, is transparency as important as we

142
00:15:03,720 --> 00:15:09,320
intuitively may think it is? Well, I wonder how important transparency is for the simple fact

143
00:15:09,320 --> 00:15:15,320
that we have teenagers among our midst and the teenagers cannot explain themselves at all,

144
00:15:16,040 --> 00:15:20,040
and yet we tolerate their behavior with some restrictions because they're not full adults.

145
00:15:20,440 --> 00:15:27,800
So, but we wouldn't let a teenager fly an airplane or operate on a patient. So, I think a pretty simple

146
00:15:27,800 --> 00:15:33,080
model is that at the moment, these systems cannot explain how they became to their decision. There

147
00:15:33,080 --> 00:15:38,280
are many people working on the explainability problem. Until then, I think it's going to be really

148
00:15:38,280 --> 00:15:44,040
important that these systems not be used in what I'm going to call life safety situations, and this

149
00:15:44,040 --> 00:15:49,240
creates all sorts of problems. For example, in automated war, automated conflict, cyber war,

150
00:15:49,320 --> 00:15:54,120
those sorts of things where the speed of decision making is faster than what humans can,

151
00:15:54,120 --> 00:16:00,200
what happens if it makes a mistake? And so, again, we're at the beginning of this process,

152
00:16:00,200 --> 00:16:05,480
and most people, including myself, believe that the explainability problem and the bias problems

153
00:16:06,040 --> 00:16:10,520
will get resolved because there's just too much money, too many people working on it.

154
00:16:10,520 --> 00:16:14,200
Maybe at some cost, but we'll get there. That's historically how these things work. You start

155
00:16:14,200 --> 00:16:18,280
off with stuff that works well enough, but it shows a hint of the future, and then it gets

156
00:16:18,280 --> 00:16:25,400
industrialized. I'm actually much more focused on what's it like to be human when you have these

157
00:16:25,400 --> 00:16:31,960
specialized systems floating around. My favorite example here is Facebook, where they change their

158
00:16:31,960 --> 00:16:40,200
feed to amp it using AI, and the AI that they built was around engagement, and we know from a

159
00:16:40,200 --> 00:16:44,680
great deal of social science that outrage creates more engagement, and so therefore,

160
00:16:44,680 --> 00:16:51,400
there's more outrage on your feed. Now, that was a clearly deliberate decision on part of Facebook,

161
00:16:51,400 --> 00:16:54,760
presumably thought it was a good product idea, but it also maximized their revenue.

162
00:16:55,400 --> 00:17:00,120
That's a pretty big social experiment, given the number of users that they have,

163
00:17:00,120 --> 00:17:05,640
which is not done with an understanding, in my view, of the impact of political polarization.

164
00:17:06,440 --> 00:17:10,280
Now, you sit there and you go, okay, well, he doesn't work at Facebook, he doesn't really

165
00:17:10,280 --> 00:17:17,720
understand, but many, many people have commented on this problem. This is an image of what happens

166
00:17:17,720 --> 00:17:24,280
in a world where all of the information around you can be boosted or manipulated by AI to sell

167
00:17:24,280 --> 00:17:29,560
to you, to anchor you, to change your opinion, and so forth. We're going to face some interesting

168
00:17:29,560 --> 00:17:36,040
questions in the information space that television and movies and things you see online, and so forth.

169
00:17:36,520 --> 00:17:41,960
Do there need to be restrictions on how AI uses the information it has about you

170
00:17:42,600 --> 00:17:47,880
to pitch to you, to market to you, to entertain you? These are questions we don't have answers,

171
00:17:48,600 --> 00:17:54,600
but it makes perfect sense that in the industrialization of these tools, the tools that I'm describing,

172
00:17:54,600 --> 00:17:59,480
which were invented in places like Google and Facebook, will become available to everyone

173
00:17:59,480 --> 00:18:07,000
and every government. Another example is a simple one, which is the kid is a two-year-old

174
00:18:07,000 --> 00:18:12,520
and gets a toy, and the toy gets upgraded every year and the kid gets smarter, the toy is now,

175
00:18:12,520 --> 00:18:18,760
the kid is now 12, and there's 10 years from now, there's a great toy, and this toy is smart enough

176
00:18:19,240 --> 00:18:26,760
in non-human terms to be able to watch television and decide if the kid likes the show. The toy

177
00:18:26,760 --> 00:18:31,640
is watching the television and the kid, the toy says to the kid, I don't like this show,

178
00:18:32,520 --> 00:18:38,120
knowing that the kid's not going to like it, and the kid goes, I agree with you. Now, is that okay?

179
00:18:38,920 --> 00:18:46,120
Probably. Well, what happens if that same system that's also learning learns something

180
00:18:46,120 --> 00:18:54,600
that's not true, and it goes, you know, kid, I have a secret, and the kid goes, tell me, tell me,

181
00:18:55,400 --> 00:18:59,320
and the secret is something which is prejudicial or false or bad or something like that.

182
00:19:00,040 --> 00:19:06,280
We don't know how to describe, especially for young people, the impact of these systems on their

183
00:19:06,280 --> 00:19:13,160
cognitive development. Now, we have a long history in America of having school boards and textbooks

184
00:19:13,160 --> 00:19:19,000
which are approved at the state level. Are the states going to monitor this? And you sit there

185
00:19:19,080 --> 00:19:24,600
and you say, well, no parent would allow that, but let's say that the normal behavior of this toy

186
00:19:24,600 --> 00:19:29,480
is smart enough, understands the kid well enough to know the kid's not good at multiplication.

187
00:19:30,280 --> 00:19:36,360
So the kid's bored, and the toy says, I think we should play a game. The kid goes great. And,

188
00:19:36,360 --> 00:19:41,960
of course, it's a game which strengthens his or her multiplication capability. So on the one hand,

189
00:19:41,960 --> 00:19:48,280
you want these systems to make people smarter, make them develop, make them more serious adults,

190
00:19:48,280 --> 00:19:53,000
make the adults more productive. Another example would be my physics friends. They just want a

191
00:19:53,000 --> 00:19:58,280
system to read all the physics books every night and make suggestions to them. Well, the physicists

192
00:19:58,280 --> 00:20:03,400
are adults who can deal with this. But what about kids? So you're going to end up in a situation,

193
00:20:03,400 --> 00:20:09,400
at least with kids and with elderly who are isolated, where these tools are going to have

194
00:20:09,400 --> 00:20:16,040
an out of proportion impact on society as they perceive it. We've never run that experiment,

195
00:20:16,040 --> 00:20:23,480
dynamic, emergent, and not precise. I'm not worried about airplanes being flown by AI because

196
00:20:23,480 --> 00:20:28,440
they're not going to be reliable enough to do it for a while. Now, we should also say for the

197
00:20:28,440 --> 00:20:34,520
listeners here that we're talking about a term which is generally known as narrow AI.

198
00:20:35,400 --> 00:20:41,240
It's very specific, and we're using specific examples, drug discovery, education,

199
00:20:41,320 --> 00:20:47,640
entertainment. But the eventual state of AI is called general intelligence, where you get

200
00:20:47,640 --> 00:20:54,760
human kind of reasoning. In the book, what we describe that as the point where the computer

201
00:20:54,760 --> 00:21:00,840
can set its own objective. And today, the good news is, the computer can't choose its objective.

202
00:21:02,200 --> 00:21:06,920
At some point, that will not be true. Yeah. Yeah. Well, hopefully we'll get to AGI

203
00:21:07,720 --> 00:21:15,080
at the end of this hour. But I think we should talk about the good and the bad in that order.

204
00:21:15,080 --> 00:21:20,440
And maybe just spend a few minutes on the good because the good is all too obvious. Again,

205
00:21:20,440 --> 00:21:27,000
and intelligence is the most valuable thing on earth. It's the thing that gives us every other

206
00:21:27,000 --> 00:21:33,320
thing we want. And it's the thing that safeguards everything we have. And if there are problems,

207
00:21:34,040 --> 00:21:38,360
we can't solve, well, then we can't solve them. But if there are problems that can be solved,

208
00:21:38,360 --> 00:21:45,480
the way we will solve them is through greater uses of our intelligence. And if, you know,

209
00:21:45,480 --> 00:21:51,080
insofar as we can leverage artificial intelligence to solve those problems, we will do that more

210
00:21:51,080 --> 00:21:56,520
or less regardless of the attendant risks. And that's the problem because the attendant risks are

211
00:21:57,080 --> 00:22:03,640
increasingly obvious and it seems not at all trivial. And what we've already proven, we're

212
00:22:03,640 --> 00:22:11,640
capable of implementing massive technological change without really thinking about the consequences

213
00:22:11,640 --> 00:22:17,640
at all. You cite the massive psychological experiment we've performed on all of humanity

214
00:22:17,640 --> 00:22:23,880
with no one really consenting that is social media. And it's, you know, the effects are

215
00:22:24,680 --> 00:22:29,560
ambiguous at best. I mean, there's some obviously bad effects. And it's not even

216
00:22:29,560 --> 00:22:35,960
straightforward to say that democracy or even civilization can survive contact with social

217
00:22:35,960 --> 00:22:41,880
media. I mean, that remains to be seen given how divisive some of its effects are. I consider

218
00:22:41,880 --> 00:22:49,160
social media to be far less alarming than the prospect of having an ongoing nuclear doctrine

219
00:22:49,800 --> 00:22:58,360
anchored to a proliferating regime of cyber espionage, cyber terrorism, cyber war,

220
00:22:59,000 --> 00:23:05,080
all of which will be improved massively by layering AI onto all of that. So

221
00:23:06,040 --> 00:23:10,040
before we jump into the bad, which is, you know, really capturing my attention,

222
00:23:10,920 --> 00:23:15,880
is there anything specifically you want to say about the good here? I mean, if this goes well,

223
00:23:16,760 --> 00:23:19,080
what are you hoping for? What are you expecting?

224
00:23:19,960 --> 00:23:25,240
Well, there are so many positive examples that we honestly just don't have time to make a list.

225
00:23:25,240 --> 00:23:31,960
We give you a few. In physics and math, the physicists and mathematicians have worked out

226
00:23:31,960 --> 00:23:37,720
the formulas for how the world works, at least at the scientific level. But many of their calculations

227
00:23:37,720 --> 00:23:44,200
are not computable by modern computers. They're just too complicated. An example is how do clouds

228
00:23:44,280 --> 00:23:49,400
actually work is a function of something called the Navier-Stokes equations, which for a normal

229
00:23:49,400 --> 00:23:55,720
sized cloud would take 100 million years for a computer to figure out. But using an AI system,

230
00:23:55,720 --> 00:24:01,000
and there's a group at Caltech doing this, they can come up with a simulation of the things that

231
00:24:01,000 --> 00:24:09,720
they care about. In other words, the AI provides enough accuracy in order to solve the more general

232
00:24:09,720 --> 00:24:16,360
climate modeling problem. If you look at quantum chemistry, which is sort of how does, how do chemical

233
00:24:16,360 --> 00:24:23,160
bonds work together? Not computable by modern methods. However, AI can provide enough of a

234
00:24:23,160 --> 00:24:28,760
simulation that we can figure out how these molecules bind, which is the Hallisyn example.

235
00:24:29,880 --> 00:24:37,160
In drug discovery, we know enough about biology that we can basically predict that if you do

236
00:24:37,800 --> 00:24:44,120
these compounds with, you know, this antibody, we can make it stronger, we can make it weaker,

237
00:24:44,120 --> 00:24:49,960
and so forth, in the computer, and then you go reproduce it in the lab. There's example after

238
00:24:49,960 --> 00:24:59,080
example, where AI is being used from existing data to simulate a non-computable function in science.

239
00:24:59,080 --> 00:25:03,800
And you say, what's he talking about? I'm talking about the fact that the scientists have been stuck

240
00:25:04,520 --> 00:25:09,560
for decades because they know what they want to do, but they couldn't get through this barrier.

241
00:25:10,280 --> 00:25:17,080
That unleashes new materials, new drugs, new forms of steel, new forms of concrete, and so forth and

242
00:25:17,080 --> 00:25:22,520
so on. It also helps us with climate change, for example, because climate change is really about

243
00:25:22,520 --> 00:25:27,240
energy and CO2 emission and so forth. These new surfaces, discoveries, and so forth will make a

244
00:25:27,240 --> 00:25:33,400
material difference. And I'm talking about really significant numbers. So that's an example. Another

245
00:25:33,400 --> 00:25:37,320
example is what's happening with these large language models that you mentioned earlier,

246
00:25:38,120 --> 00:25:41,880
that people are figuring out a way to put a conversational system in front of it so that

247
00:25:41,880 --> 00:25:46,760
you can talk to it. And the conversational system has enough state that it can remember what it's

248
00:25:46,760 --> 00:25:51,560
talking about. It's not like a question-answer-question-answer, and it doesn't remember. It actually

249
00:25:51,560 --> 00:25:55,720
remembers the context of, oh, we're talking about the Oscars, and we're talking about what happened

250
00:25:55,720 --> 00:26:01,160
at the Oscars, and what do I think? And then it sort of goes and it gives you a thoughtful answer

251
00:26:01,240 --> 00:26:06,760
as to what happened and what is possible. In my case, I was playing with one of them

252
00:26:07,880 --> 00:26:14,920
a few months ago, and this one, I asked the question, what is the device that's in 2001

253
00:26:14,920 --> 00:26:20,360
a space odyssey that I'm using today? There's something from 1969 that I'm using today that was

254
00:26:20,360 --> 00:26:26,600
foreshadowed in the movie, and it comes right back and says the iPad. Now, that's a question

255
00:26:26,600 --> 00:26:34,360
that Google won't answer if you ask it the way I did. So I believe that the biggest positive impact

256
00:26:35,160 --> 00:26:40,360
will be that you'll have a system that you can verbally or by writing ask it questions,

257
00:26:40,920 --> 00:26:47,000
and it will make you incredibly smarter. It'll give you the nuance and the understanding in the

258
00:26:47,000 --> 00:26:51,560
context, and you can ask it another question, and you can refine your question. Now, if you think

259
00:26:51,560 --> 00:26:56,760
about it in the work you do, or that I do, or that a scientist does, or a politician, or an artist,

260
00:26:56,760 --> 00:27:05,640
this is enormously transformative. So example after example, these systems are going to build

261
00:27:05,640 --> 00:27:11,160
scientific breakthroughs, scalable breakthroughs. Another example was that a group at DeepMind

262
00:27:11,960 --> 00:27:17,560
figured out the folding structure of proteins, and proteins are the way in which biology works,

263
00:27:17,560 --> 00:27:21,320
and the way they fold determines their effectiveness, what they actually do.

264
00:27:22,040 --> 00:27:27,000
And it was thought to be not really computable, and using these techniques in a very complicated way

265
00:27:27,000 --> 00:27:32,600
with a whole bunch of protein scientists, they managed to do it, and their result was replicated

266
00:27:32,600 --> 00:27:37,000
in a different mechanism with different AI from something called the Baker Lab in University of

267
00:27:37,000 --> 00:27:43,480
Washington. The two together have given us a map of how proteins work, which in my view is worthy

268
00:27:43,480 --> 00:27:48,440
of a Nobel Prize. That's how big a discovery that is. All of a sudden, we are unlocking the way

269
00:27:48,440 --> 00:27:54,040
biology works, and it affects us directly. But those are some positive examples. I think the

270
00:27:54,040 --> 00:28:01,160
negative examples... Well, let's wait because I'm chock full of negative examples. But I'm interested

271
00:28:01,160 --> 00:28:10,600
in how even the positive can disclose a surprisingly negative possibility, or at least it becomes

272
00:28:10,600 --> 00:28:17,960
negative if we haven't planned for it ethically, politically, economically. So you imagine this

273
00:28:17,960 --> 00:28:25,160
success, you imagine that more and more... So what you've just pictured was a future of machine and

274
00:28:25,160 --> 00:28:33,400
human cooperation and facilitation, where people just get smarter by being able to have access

275
00:28:33,400 --> 00:28:40,440
to these tools, or they get effectively smarter. But you can imagine just in the limit, more and

276
00:28:40,440 --> 00:28:46,200
more getting seated to AI, because AI is just better at doing these things. It's better at

277
00:28:46,200 --> 00:28:50,920
proving theorems. It's better at designing software. It's better. It's better. It's better. And

278
00:28:50,920 --> 00:28:56,920
all of a sudden, the need for human developers at all, or human mathematicians at all, or you

279
00:28:56,920 --> 00:29:06,440
just make the list as long as you want, it seems like some of the highest status jobs cognitively

280
00:29:07,240 --> 00:29:12,760
might be among the first to fall, which is to say, I certainly expect at this point

281
00:29:13,560 --> 00:29:23,000
to have an AI radiologist, certainly, before I have an AI plumber. And there's a lot more

282
00:29:23,640 --> 00:29:28,760
above and beyond the radiology side of that comparison that I think is going to fall before

283
00:29:29,720 --> 00:29:37,880
the basic manual tasks fall to robots. And this is a picture of real success, right? Because

284
00:29:38,520 --> 00:29:41,880
in the end, all we're going to care about is performance. We're not going to care about

285
00:29:42,600 --> 00:29:49,960
keeping a monkey in the loop just for reasons of sentimentality. If you're telling me that my car

286
00:29:49,960 --> 00:29:56,600
can drive a thousand times better than I can, which is to say that it's going to reduce my risk of

287
00:29:56,600 --> 00:29:59,800
getting in a fatal accident, you know, killing myself or killing someone else

288
00:30:00,440 --> 00:30:05,960
by a factor of a thousand, if I just flip on autopilot, well, then not only am I going to flip

289
00:30:05,960 --> 00:30:12,360
it on, I'm going to consider anyone who declines to do that to be negligent to the point of

290
00:30:12,360 --> 00:30:16,760
criminality. And that's never going to change. Everything is going to be in the position

291
00:30:17,480 --> 00:30:24,600
of a current chess master who knows that the best player on earth is never going to be a person

292
00:30:24,600 --> 00:30:32,040
ever again, right? Because of alpha zero. So I disagree a little bit, and I'll tell you why.

293
00:30:32,680 --> 00:30:38,200
I think you're correct in about 30 years, but I don't think that argument is true in the short term.

294
00:30:39,080 --> 00:30:43,240
Yeah, no, I was not just to be clear. I'm not suggesting any time frame there. I'm just saying

295
00:30:43,240 --> 00:30:49,720
ultimately, if we continue to make progress, something like this seems bound to happen.

296
00:30:49,720 --> 00:30:57,400
Yes, but what I want to say is, I defy you to argue with me that making people smarter

297
00:30:57,960 --> 00:31:06,120
is a bad thing. Okay, right. So let's start with the premise of the human assistant that is the

298
00:31:06,120 --> 00:31:13,720
thing that you're using will make humans smarter. It'll make it deeper, better analysis, better

299
00:31:13,720 --> 00:31:21,880
choices. But at least the current technology cannot replace the essentially the free will of

300
00:31:21,880 --> 00:31:26,920
humans. They sort of wake up in the morning, you have a new idea, you decide something, you say,

301
00:31:26,920 --> 00:31:32,120
that's a bad idea, so forth and so on. We don't know how to do that yet. And I have some speculation

302
00:31:32,120 --> 00:31:37,960
on how that will happen. But in the next decade, we're going to not be solving that problem,

303
00:31:37,960 --> 00:31:42,680
we'll be solving a different problem, which is how do we get the existing people doing existing

304
00:31:42,680 --> 00:31:50,200
jobs to do them more efficiently that is smarter, better, faster? When we looked at the funding

305
00:31:50,200 --> 00:31:55,880
for this AI program that I've since announced, the funding 125 million, a fair chunk of it is

306
00:31:55,880 --> 00:32:01,000
going to really hard computer science problems. Some of them include, we don't really understand

307
00:32:01,000 --> 00:32:06,280
how to explain what they're doing. As I mentioned, they're also brittle. When they fail, they can

308
00:32:06,280 --> 00:32:11,560
fail catastrophically, like why did it fail? And no one can explain. They're hardening,

309
00:32:11,560 --> 00:32:15,080
there are resistance to attack problems, there are a number of problems of this kind.

310
00:32:15,960 --> 00:32:18,760
These are hard computer science problems, which I think we will get through.

311
00:32:19,480 --> 00:32:24,120
They use a lot of power, the algorithms are expensive, that sort of thing. But we have also

312
00:32:24,120 --> 00:32:30,680
focusing around the impact on jobs and employment and economics. We're also focusing on national

313
00:32:30,680 --> 00:32:35,880
security. And we're focusing on the question that you're asking, which is, what's our identity?

314
00:32:35,880 --> 00:32:42,520
What does it mean to be human? Before general intelligence comes, we have to deal with the

315
00:32:42,520 --> 00:32:48,680
fact that these systems are not capable of choosing their own outcome, but they can be

316
00:32:48,680 --> 00:32:55,320
applied to you as a citizen by somebody else against your own satisfaction. So the negatives

317
00:32:55,320 --> 00:33:03,960
before AGI are all of the form, misinformation, misleading information, creating dangerous

318
00:33:03,960 --> 00:33:10,600
tools, and for example, dangerous viruses. For the same reason that we built a fantastic new

319
00:33:10,600 --> 00:33:17,560
antibiotic drug, it looks like you could also imagine a similar evil team producing an incredible

320
00:33:17,560 --> 00:33:22,520
number of bad viruses, things that would hurt people. And you could imagine in that scenario,

321
00:33:22,520 --> 00:33:27,240
they might be clever enough to be able to hurt a particular race or a particular sex or something

322
00:33:27,240 --> 00:33:32,600
like that, which would be totally evil and obviously a very bad thing. We don't have a way

323
00:33:32,600 --> 00:33:38,120
of discussing that today. So when I look at the positives and negatives right now, I think the

324
00:33:38,120 --> 00:33:44,680
positives, as with many technologies, really overwhelm the negatives, but the negatives need

325
00:33:44,680 --> 00:33:51,240
to be looked at. And we need to have the conversation right now about, let's use social media, which

326
00:33:51,240 --> 00:33:57,400
is an easy whipping boy here. I would like, so I'm clear what my political position is,

327
00:33:57,400 --> 00:34:04,760
I'm a very strong proponent of freedom of speech for humans. I am not in favor of freedom of speech

328
00:34:04,760 --> 00:34:11,000
for computers, robots, bots, so forth and so on. I want an option with social media, which says,

329
00:34:11,000 --> 00:34:17,480
I only want to see things that a human has actually communicated from themselves. I want to know that

330
00:34:17,480 --> 00:34:22,840
it wasn't snuck in by some Russian agent. I want proof of providence. And I want to know that it's

331
00:34:22,840 --> 00:34:29,080
a human. And if it's a real human who's, in fact, an idiot or crazy or whatever, I want to be able

332
00:34:29,080 --> 00:34:33,960
to hear their voice and I want to be able to decide I don't agree with it. What's happening instead

333
00:34:33,960 --> 00:34:41,720
is these systems are being boosted. They're being pitched. They're being sold by AI. And I think

334
00:34:41,720 --> 00:34:47,480
that's got to be limited in some way. I'm in favor of free speech, but I don't want only some people

335
00:34:47,480 --> 00:34:52,920
to have microphones. And if you talk to politicians and you look at the political structure in the

336
00:34:52,920 --> 00:35:00,600
country, this isn't of completely unintended effect of getting everyone wired. Now, is it a human?

337
00:35:00,600 --> 00:35:07,320
Or is it a computer? Is it a Russian? A Russian Compromat Plan? Or is it an American? Those things

338
00:35:07,320 --> 00:35:11,320
need to get resolved. You cannot run a democracy without some level of trust.

339
00:35:12,120 --> 00:35:19,000
Yeah. Well, let's take that piece here and obviously it extends beyond the problem of AI's

340
00:35:19,000 --> 00:35:25,800
involvement in it. But the misinformation problem is enormous. What are your thoughts about it?

341
00:35:25,800 --> 00:35:32,520
Because I'm just imagining we've been spared thus far the worst possible case of this, which is

342
00:35:32,520 --> 00:35:40,520
just imagine under conditions of where we had something like perfect deep fakes,

343
00:35:41,000 --> 00:35:46,680
that were truly difficult to tell apart from real video, what would the controversy around

344
00:35:46,680 --> 00:35:52,840
the 2020 election have looked like? Or the war in Ukraine and our dealings with Putin at this

345
00:35:52,840 --> 00:36:01,640
moment? Just imagine a perfect deep fake of Putin declaring a nuclear first strike on the U.S.

346
00:36:01,640 --> 00:36:08,360
or whatever. Just imagine essentially a writer's room from hell where you have smart creative

347
00:36:08,360 --> 00:36:16,760
people spending their waking hours figuring out how to produce media that is shattering to every

348
00:36:16,760 --> 00:36:25,720
open society and conducive to provoking international conflict. That is clearly coming in some form.

349
00:36:26,520 --> 00:36:32,920
I guess my first question is, are you hopeful that the moment that arrives, we will have the same

350
00:36:33,000 --> 00:36:38,920
level of technology that can spot deep fakes? Or is there going to be a lag there of

351
00:36:39,720 --> 00:36:43,000
months, years that are going to be difficult to navigate?

352
00:36:43,560 --> 00:36:48,680
We don't know. There are people working really hard on generating deep fakes and there are people

353
00:36:48,680 --> 00:36:54,760
working really hard on detecting deep fakes. And one of the general problems with misinformation

354
00:36:54,760 --> 00:37:00,920
is we don't have enough training data. The term here is in order to get an AI system to

355
00:37:00,920 --> 00:37:05,720
know something. You have to give it enough examples of good, bad, good, bad. And eventually you

356
00:37:05,720 --> 00:37:10,840
can say, oh, here's something new and I know if it's good or bad. And one of the core problems

357
00:37:10,840 --> 00:37:15,640
in misinformation is we don't have enough agreement on what is misinformation or what have you.

358
00:37:16,680 --> 00:37:22,520
The thought experiment I would offer is President Putin in Russia has already shut down the internet

359
00:37:22,520 --> 00:37:28,440
and free speech and controls the media and so forth. So let's imagine that he was furthering

360
00:37:28,440 --> 00:37:34,200
it. If you'd like to continue listening to this conversation, you'll need to subscribe

361
00:37:34,200 --> 00:37:39,080
at SamHarris.org. Once you do, you'll get access to all full-length episodes of the Making Sense

362
00:37:39,080 --> 00:37:45,000
podcast, along with other subscriber-only content, including bonus episodes and AMAs

363
00:37:45,000 --> 00:37:50,120
and the conversations I've been having on the Waking Up app. The Making Sense podcast is ad-free

364
00:37:50,120 --> 00:37:55,800
and relies entirely on listener support. And you can subscribe now at SamHarris.org.

