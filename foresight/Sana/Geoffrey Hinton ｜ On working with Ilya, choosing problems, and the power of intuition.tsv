start	end	text
0	8000	Have you reflected a lot on how to select talent or has that mostly been intuitive to you?
8000	12400	Ilya just shows up and you're like, this is a clever guy, let's work together.
12400	14600	Or have you thought a lot about that?
14600	17600	Should we roll this?
17600	18600	Yeah, let's roll this.
18600	20000	We're good, yeah, yeah.
20000	21600	You're juggling into a repetition.
21600	22600	Okay.
25000	26000	Sun is working.
30000	34000	So I remember when I first got to Carnegie Mellon from England.
34000	39000	In England, at a research unit, it would get to be six o'clock and you'd all go for a drink in the pub.
39000	44000	At Carnegie Mellon, I remember after I'd been there a few weeks, it was Saturday night.
44000	47000	I didn't have any friends yet and I didn't know what to do.
47000	52000	So I decided I'd go into the lab and do some programming because I had a list machine and you couldn't program it from home.
52000	57000	So I went into the lab at about nine o'clock on a Saturday night and it was swarming.
57000	59000	All the students were there.
59000	62000	And they were all there because what they were working on was the future.
62000	67000	They all believed that what they did next was going to change the course of computer science.
67000	69000	And it was just so different from England.
69000	72000	And so that was very refreshing.
72000	78000	Take me back to the very beginning, Geoff, at Cambridge, trying to understand the brain.
78000	80000	What was that like?
80000	82000	It was very disappointing.
82000	87000	So I did physiology and in the summer term, they were going to teach us how the brain worked.
87000	94000	And all they taught us was how neurons conduct action potentials, which is very interesting, but it doesn't tell you how the brain works.
94000	96000	So that was extremely disappointing.
96000	98000	I switched to philosophy then.
98000	100000	I thought maybe they'd tell us how the mind worked.
100000	102000	That was very disappointing.
102000	105000	I eventually ended up going to Edinburgh to do AI.
105000	106000	And that was more interesting.
106000	110000	At least you could simulate things so you could test out theories.
110000	113000	And did you remember what intrigued you about AI?
113000	115000	Was it a paper?
115000	119000	Was it any particular person that exposed you to those ideas?
119000	124000	I guess it was a book I read by Donald Hebb that influenced me a lot.
124000	129000	He was very interested in how you learn the connection strengths in neural nets.
129000	139000	I also read a book by John von Neumann early on, who was very interested in how the brain computes and how it's different from normal computers.
139000	145000	And did you get that conviction that these ideas would work out at that point?
145000	149000	Or what was your intuition back at the Edinburgh days?
149000	154000	It seemed to me there has to be a way that the brain learns.
154000	161000	And it's clearly not by having all sorts of things programmed into it and then using logical rules of inference.
161000	165000	That just seemed to me crazy from the outset.
165000	173000	So we had to figure out how the brain learned to modify connections in a neural net so that it could do complicated things.
173000	176000	And von Neumann believed that, Turing believed that.
176000	181000	So von Neumann and Turing were both pretty good at logic, but they didn't believe in this logical approach.
181000	191000	And what was your split between studying the ideas from neuroscience and just doing what seemed to be good algorithms for AI?
191000	194000	How much inspiration did you take early on?
194000	196000	So I never did that much study of neuroscience.
196000	202000	I was always inspired by what I'd learned about how the brain works, that there's a bunch of neurons.
202000	204000	They perform relatively simple operations.
204000	212000	They're non-linear, but they collect inputs, they weight them, and then they give an output that depends on that weighted input.
212000	216000	And the question is, how do you change those weights to make the whole thing do something good?
216000	218000	It seems like a fairly simple question.
218000	222000	What collaborations do you remember from that time?
222000	226000	The main collaboration I had at Carnegie Mellon was with someone who wasn't at Carnegie Mellon.
226000	231000	I was interacting a lot with Terry Sinovsky, who was in Baltimore at Johns Hopkins.
231000	235000	And about once a month, either he would drive to Pittsburgh or I would drive to Baltimore.
235000	239000	It's 250 miles away, and we would spend a weekend together working on Baltimore machines.
239000	241000	That was a wonderful collaboration.
241000	243000	We were both convinced it was how the brain worked.
243000	245000	That was the most exciting research I've ever done.
245000	251000	And a lot of technical results came out that were very interesting, but I think it's not how the brain works.
251000	258000	I also had a very good collaboration with Peter Brown, who was a very good statistician,
258000	260000	and he worked on speech recognition at IBM.
260000	268000	And then he came as a more mature student at Carnegie Mellon just to get a PhD, but he already knew a lot.
268000	272000	He taught me a lot about speech, and he in fact taught me about hidden Markov models.
272000	275000	I think I learned more from him than he learned from me.
275000	277000	That's the kind of student you want.
277000	283000	And when he taught me about hidden Markov models, I was doing backprop with hidden layers.
283000	285000	Only they weren't called hidden layers then.
285000	292000	And I decided that name they use in hidden Markov models is a great name for variables that you don't know what they're up to.
292000	297000	And so that's where the name hidden in neural nets came from.
297000	302000	Me and Peter decided that was a great name for the hidden layers of neural nets.
302000	305000	But I learned a lot from Peter about speech.
305000	310000	Take us back to when Ilya showed up at your office.
310000	315000	I was in my office, probably on a Sunday, and I was programming, I think.
315000	318000	And there was a knock on the door, not just any knock, but it went kinda...
318000	321000	That's sort of an urgent knock.
321000	324000	So I went and answered the door, and this was this young student there.
324000	328000	And he said he was cooking fries over the summer, but he'd rather be working in my lab.
328000	332000	And so I said, well, why don't you make an appointment and we'll talk?
332000	334000	And so Ilya said, how about now?
334000	337000	And that sort of was Ilya's character.
337000	343000	So we talked for a bit, and I gave him a paper to read, which was the Nature Paper and Back Propagation.
343000	349000	And we made another meeting for a week later, and he came back and he said, I didn't understand it.
349000	353000	And I was very disappointed. I thought, he seemed like a bright guy, but it's only the chain rule.
353000	355000	It's not that hard to understand.
355000	358000	And he said, oh, no, no, I understood that.
358000	363000	I just don't understand why you don't give the gradient to a sensible function optimizer,
363000	366000	which took us quite a few years to think about.
366000	368000	And it kept on like that with Ilya.
368000	372000	He had very good, his raw intuitions about things were always very good.
372000	377000	What do you think had enabled those intuitions for Ilya?
377000	380000	I don't know. I think he always thought for himself.
380000	383000	He was always interested in AI from a young age.
383000	387000	He's obviously good at math, but it's very hard to know.
387000	391000	And what was that collaboration between the two of you like?
391000	394000	What part would you play and what part would Ilya play?
394000	396000	It was a lot of fun.
396000	404000	I remember one occasion when we were trying to do a complicated thing with producing maps of data,
404000	406000	where I had a kind of mixture model.
406000	409000	So you could take the same bunch of similarities and make two maps,
409000	415000	so that in one map, bank could be close to greed and in another map, bank could be close to river.
416000	419000	Because in one map, you can't have it close to both, right?
419000	421000	Because river and greed are one way apart.
421000	423000	So we'd have a mixture of maps.
423000	425000	And we were doing it in MATLAB,
425000	429000	and this involved a lot of reorganization of the code to do the right matrix multiplies,
429000	431000	and only got fed up with that.
431000	433000	So he came one day and said,
433000	436000	I'm going to write an interface for MATLAB,
436000	438000	so I program in this different language,
438000	441000	and then I have something that just converts it into MATLAB.
441000	445000	And I said, no Ilya, that'll take you a month to do.
445000	446000	We've got to get on with this project.
446000	448000	Don't get diverted by that.
448000	450000	And Ilya said, it's okay, I did it this morning.
453000	455000	That's quite incredible.
455000	462000	And throughout those years, the biggest shift wasn't necessarily just the algorithms,
462000	464000	but also the skill.
464000	469000	How did you sort of view that skill over the years?
469000	471000	Ilya got that intuition very early.
471000	477000	So Ilya was always preaching that you just make it bigger and it'll work better.
477000	479000	And I always thought that was a bit of a cop-out,
479000	481000	that you're going to have to have new ideas too.
481000	483000	It turns out Ilya was basically right.
483000	484000	New ideas help.
484000	486000	Things like transformers helped a lot.
486000	490000	But it was really the scale of the data and the scale of the computation.
490000	495000	And back then, we had no idea computers would get like a billion times faster.
495000	497000	We thought maybe they'd get 100 times faster.
497000	500000	We were trying to do things by coming up with clever ideas
500000	504000	that would have just solved themselves if we had had bigger scale of the data and computation.
504000	509000	In about 2011, Ilya and another graduate student called James Martins and I
509000	513000	had a paper using character level prediction.
513000	518000	So we took Wikipedia and we tried to predict the next HTML character.
518000	520000	And that worked remarkably well.
520000	523000	And we were always amazed at how well it worked.
523000	527000	And that was using a fancy optimizer on GPUs.
527000	531000	And we could never quite believe that it understood anything,
531000	533000	but it looked as though it understood.
533000	536000	And that just seemed incredible.
536000	542000	Can you take us through how are these models trained to predict the next word
542000	547000	and why is it the wrong way of thinking about them?
547000	550000	Okay, I don't actually believe it is the wrong way.
550000	555000	So, in fact, I think I made the first neural net language model
555000	557000	that used embeddings and backpropagation.
557000	560000	So it's very simple data, just triples.
560000	564000	And it was turning each symbol into an embedding,
564000	569000	then having the embeddings interact to predict the embedding of the next symbol
569000	571000	and then from that predict the next symbol.
571000	575000	And then it was backpropagating through that whole process to learn these triples.
575000	577000	And I showed it could generalize.
578000	581000	About 10 years later, Yoshio Benji used a very similar network
581000	583000	and showed it worked with real text.
583000	587000	And about 10 years after that, Linguist started believing in embeddings.
587000	588000	It was a slow process.
588000	592000	The reason I think it's not just predicting the next symbol
592000	595000	is if you ask, well, what does it take to predict the next symbol?
595000	603000	Particularly if you ask me a question and then the first word of the answer is the next symbol,
603000	606000	you have to understand the question.
606000	612000	So I think by predicting the next symbol, it's very unlike old-fashioned autocomplete.
612000	615000	For old-fashioned autocomplete, you'd store sort of triples of words.
615000	620000	And then if you saw a pair of words, you see how often different words came third
620000	622000	and that way you could predict the next symbol.
622000	625000	And that's what most people think autocomplete is like.
625000	627000	It's no longer at all like that.
627000	630000	To predict the next symbol, you have to understand what's been said.
630000	634000	So I think you're forcing it to understand by making it predict the next symbol.
634000	637000	And I think it's understanding in much the same way we are.
637000	641000	So a lot of people will tell you these things aren't like us.
641000	643000	They're just predicting the next symbol.
643000	645000	They're not reasoning like us.
645000	649000	But actually, in order to predict the next symbol, it's going to have to do some reasoning.
649000	654000	And we've seen now that if you make big ones, without putting in any special stuff to do reasoning,
654000	656000	they can already do some reasoning.
656000	659000	And I think as you make them bigger, they're going to be able to do more and more reasoning.
659000	663000	Do you think I'm doing anything else than predicting the next symbol right now?
663000	665000	I think that's how you're learning.
665000	668000	I think you're predicting the next video frame.
668000	671000	You're predicting the next sound.
671000	675000	But I think that's a pretty plausible theory of how the brain's learning.
675000	681000	What enables these models to learn such a wide variety of fields?
681000	685000	What these big language models are doing is they're looking for common structure.
685000	690000	And by finding common structure, they can encode things using the common structure and that's more efficient.
690000	692000	So let me give you an example.
692000	697000	If you ask GPT-4, why is a compost heap like an atom bomb?
697000	699000	Most people can't answer that.
699000	703000	Most people haven't thought they think atom bombs and compost heap are very different things.
703000	709000	But GPT-4 will tell you, well, the energy scales are very different and the time scales are very different.
709000	714000	But the thing that's the same is that when the compost heap gets hotter, it generates heat faster.
714000	719000	And when the atom bomb produces more neutrons, it produces more neutrons faster.
720000	723000	And so it gets the idea of a chain reaction.
723000	730000	And I believe it's understood that both forms of chain reaction is using that understanding to compress all that information into its weights.
730000	738000	And if it's doing that, then it's going to be doing that for hundreds of things where we haven't seen the analogies yet, but it has.
738000	743000	And that's where you get creativity from, from seeing these analogies between apparently very different things.
743000	747000	And so I think GPT-4 is going to end up, when it gets bigger, being very creative.
747000	755000	I think this idea that it's just regurgitating what it's learned, just pastishing together text it's learned already, that's completely wrong.
755000	758000	It's going to be even more creative than people, I think.
758000	767000	You'd argue that it won't just repeat the human knowledge we've developed so far, but could also progress beyond that.
767000	770000	I think that's something we haven't quite seen yet.
770000	773000	We've started seeing some examples of it.
773000	778000	But to a large extent, we're sort of still at the current level of science.
778000	780000	What do you think will enable it to go beyond that?
780000	783000	Well, we've seen that in more limited contexts.
783000	794000	Like if you take AlphaGo, in that famous competition with Lysidol, there was Move 37, where AlphaGo made a move that all the experts said must have been a mistake.
794000	797000	But actually later they realized it was a brilliant move.
797000	800000	So that was creative within that limited domain.
801000	804000	I think we'll see a lot more of that as these things get bigger.
804000	815000	The difference with AlphaGo as well was that it was using reinforcement learning, that that subsequently sort of enabled it to go beyond the current state.
815000	822000	So it started with imitation learning, watching how humans play the game, and then it would, through self-play, develop way beyond that.
822000	825000	Do you think that's the missing component of the current evidence?
825000	828000	I think that may well be a missing component, yes.
828000	836000	That the self-play in AlphaGo and AlphaZero are a large part of why it could make these creative moves.
836000	839000	But I don't think it's entirely necessary.
839000	846000	So there's a little experiment I did a long time ago where you're training on your own net to recognize 100 digits.
846000	848000	I love that example, the MNIST example.
848000	852000	And you give it training data where half the answers are wrong.
854000	856000	And the question is, how well will it learn?
858000	863000	And you make half the answers wrong once and keep them like that.
863000	869000	So it can't average away the wrongness by just seeing the same example, but with the right answer sometimes and the wrong answer sometimes.
869000	874000	When it sees that example, half of the examples, when it sees the example, the answer is always wrong.
874000	878000	And so the training data has 50% error.
878000	884000	But if you train up bank propagation, it gets down to 5% error, or less.
884000	891000	In other words, from badly labeled data, it can get much better results.
891000	893000	It can see that the training data is wrong.
893000	896000	And that's how smart students can be smarter than their advisor.
896000	902000	And their advisor tells them all this stuff, and for half of what their advisor tells them, they think, no, rubbish.
902000	906000	And they listen to the other half, and then they end up smarter than the advisor.
906000	911000	So these big neural nets can actually do, they can do much better than their training data.
911000	913000	And most people don't realize that.
913000	918000	So how do you expect these models to add reasoning into them?
918000	925000	So I mean, one approach is you add sort of the heuristics on top of them, which a lot of the research is doing now,
925000	930000	where you have sort of chain of thought, you just feedback its reasoning into itself.
930000	935000	And another way would be in the model itself, as you scale it up.
935000	937000	What's your intuition around that?
937000	942000	So my intuition is that as we scale up these models, they get better at reasoning.
942000	949000	And if you ask how people work, roughly speaking, we have these intuitions, and we can do reasoning.
949000	953000	And we use the reasoning to correct our intuitions.
953000	956000	Of course, we use the intuitions during the reasoning to do the reasoning.
956000	961000	But if the conclusion of the reasoning conflicts with our intuitions, we realize the intuitions need to be changed.
961000	971000	That's much like in AlphaGo or AlphaZero, where you have an evaluation function that just looks at the board and says, how good is that for me?
971000	979000	But then you do the Monte Carlo rollout, and now you get a more accurate idea, and you can revise your evaluation function.
979000	983000	So you can train it by getting it to agree with the results of reasoning.
983000	986000	And I think these large language models have to start doing that.
986000	993000	They have to start training their raw intuitions about what should come next by doing reasoning and realizing that's not right.
993000	999000	And so that way they can get more training data than just mimicking what people did.
999000	1003000	And that's exactly why AlphaGo could do this creative move 37.
1003000	1009000	It had much more training data because it was using reasoning to check out what the right next move should have been.
1009000	1012000	And what do you think about multimodality?
1012000	1017000	So we spoke about these analogies, and often the analogies are way beyond what we could see.
1017000	1025000	It's discovering analogies that are far beyond humans and at maybe abstraction levels that we'll never be able to understand.
1025000	1033000	Now, when we introduce images to that and video and sound, how do you think that will change the models?
1033000	1038000	And how do you think it will change the analogies that it will be able to make?
1038000	1045000	I think it'll change it a lot. I think it'll make it much better at understanding spatial things, for example.
1045000	1054000	From language alone, it's quite hard to understand some spatial things, although remarkably GPT-4 can do that even before it was multimodal.
1054000	1061000	But when you make it multimodal, if you have it both doing vision and reaching out and grabbing things,
1061000	1065000	it'll understand objects much better if it can pick them up and turn them over and so on.
1065000	1073000	So although you can learn an awful lot from language, it's easier to learn if you are multimodal.
1073000	1075000	And in fact, you then need less language.
1075000	1080000	And there's an awful lot of YouTube video for predicting the next frame or something like that.
1080000	1084000	So I think these multimodal models are clearly going to take over.
1084000	1088000	You can get more data that way. They need less language.
1088000	1093000	So there's really a philosophical point that you could learn a very good model from language alone,
1093000	1096000	but it's much easier to learn it from a multimodal system.
1096000	1100000	And how do you think it will impact the model's reasoning?
1100000	1103000	I think it'll make it much better at reasoning about space, for example.
1103000	1105000	Reasoning about what happens if you pick objects up.
1105000	1109000	If you actually try picking objects up, you're going to get all sorts of training data that's going to help.
1109000	1115000	Do you think the human brain evolved to work well with language?
1115000	1119000	Or do you think language evolved to work well with the human brain?
1119000	1122000	I think the question of whether language evolved to work with the brain
1122000	1125000	or the brain evolved to work with language, I think that's a very good question.
1125000	1128000	I think both happened.
1128000	1133000	I used to think we would do a lot of cognition without needing language at all.
1133000	1136000	Now I've changed my mind a bit.
1136000	1142000	So let me give you three different views of language and how it relates to cognition.
1142000	1149000	There's the old-fashioned symbolic view, which is cognition consists of having strings of symbols
1149000	1153000	in some kind of cleaned up logical language where there's no ambiguity
1153000	1155000	and applying rules of inference.
1155000	1157000	And that's what cognition is.
1157000	1162000	It's just these symbolic manipulations on things that are like strings of language symbols.
1162000	1164000	So that's one extreme view.
1164000	1169000	The opposite extreme view is, no, no, once you get inside the head, it's all vectors.
1169000	1173000	So symbols come in, you convert those symbols into big vectors
1173000	1176000	and all the stuff inside is done with big vectors,
1176000	1179000	and then if you want to produce output, you produce symbols again.
1179000	1183000	So there was a point in machine translation in about 2014
1183000	1188000	when people were using recurrent neural nets and words would keep coming in
1188000	1193000	and they'd have a hidden state and they'd keep accumulating information in this hidden state.
1193000	1197000	And then they got to the end of a sentence that have a big hidden vector
1197000	1199000	that captured the meaning of that sentence
1199000	1202000	that could then be used for producing the sentences in another language.
1202000	1204000	That was called a thought vector.
1204000	1206000	And that's the sort of second view of language.
1206000	1210000	You convert the language into a big vector that's nothing like language
1210000	1212000	and that's what cognition is all about.
1212000	1215000	But then there's a third view, which is what I believe now,
1215000	1221000	which is that you take these symbols
1221000	1225000	and you convert the symbols into embeddings and you use multiple layers of that
1225000	1227000	so you get these very rich embeddings.
1227000	1229000	But the embeddings are still tied to the symbols
1229000	1233000	in the sense that you've got a big vector for this symbol and a big vector for that symbol
1233000	1238000	and these vectors interact to produce the vector for the symbol for the next word.
1238000	1240000	And that's what understanding is.
1240000	1244000	Understanding is knowing how to convert the symbols into these vectors
1244000	1248000	and knowing how the elements of the vectors should interact to predict the vector for the next symbol.
1248000	1252000	That's what understanding is, both in these big language models and in our brains.
1252000	1256000	And that's an example which is sort of in between.
1256000	1261000	You're staying with the symbols, but you're interpreting them as these big vectors
1261000	1263000	and that's where all the work is.
1263000	1266000	And all the knowledge is in what vectors you use
1266000	1270000	and how the elements of those vectors interact, not in symbolic rules.
1270000	1274000	But it's not saying that you get away from the symbols altogether.
1274000	1277000	It's saying you turn the symbols into big vectors
1277000	1280000	but you stay with that surface structure of the symbols.
1280000	1282000	And that's how these models are working.
1282000	1285000	And that's, and I seem to be, a more plausible model of human thought too.
1285000	1291000	You were one of the first folks to get the idea of using GPUs.
1291000	1294000	And I know Jensen loves you for that.
1294000	1299000	Back in 2009 you mentioned that, you told Jensen that this could be a quite good idea
1299000	1302000	for training neural nets.
1302000	1308000	Take us back to that early intuition of using GPUs for training neural nets.
1308000	1314000	So actually I think in about 2006 I had a former graduate student called Rick Zeliski.
1314000	1316000	He's a very good computer vision guy.
1316000	1319000	And I talked to him at a meeting.
1319000	1323000	He said, you know, you ought to think about using graphics processing cards
1323000	1325000	because they're very good at matrix multiplies.
1325000	1328000	And what you're doing is basically all matrix multiplies.
1328000	1330000	So I thought about that for a bit.
1330000	1336000	And then we learned about these Tesla systems that had four GPUs in.
1336000	1344000	And initially we just got gaming GPUs and discovered they made things go 30 times faster.
1344000	1347000	And then we bought one of these Tesla systems with four GPUs.
1347000	1351000	And we did speech on that and it worked very well.
1351000	1354000	And then in 2009 I gave a talk at NIPS.
1354000	1357000	And I told a thousand machine learning researchers
1357000	1359000	that you should all go and buy Nvidia GPUs.
1359000	1360000	They're the future.
1360000	1362000	You need them for doing machine learning.
1362000	1366000	And I actually then sent mail to Nvidia saying,
1366000	1368000	I told a thousand machine learning researchers to buy your boards.
1368000	1369000	Could you give me a free one?
1369000	1370000	And they said no.
1370000	1371000	Actually they didn't say no.
1371000	1373000	They just didn't reply.
1373000	1378000	But when I told Jensen this story later on, he gave me a free one.
1378000	1380000	That's very, very good.
1380000	1387000	I think what's interesting as well is sort of how GPUs has evolved alongside the field.
1387000	1392000	So where do you think we should go next in the computer?
1392000	1398000	So my last couple of years at Google I was thinking about ways of trying to make analog computation.
1398000	1403000	So instead of using like a megawatt we could use like 30 watts like the brain.
1403000	1407000	And we could run these big language models in analog hardware.
1407000	1410000	And I never made it work.
1410000	1415000	But I started really appreciating digital computation.
1415000	1420000	So if you're going to use that low power analog computation,
1420000	1423000	every piece of hardware is going to be a bit different.
1423000	1428000	And the idea is the learning is going to make use of the specific properties of that hardware.
1428000	1429000	And that's what happens with people.
1429000	1431000	All our brains are different.
1431000	1436000	So we can't then take the weights in your brain and put them in my brain.
1436000	1437000	The hardware is different.
1437000	1440000	The precise properties of the individual neurons are different.
1440000	1443000	The learning has learned to make use of all that.
1443000	1448000	And so we're mortal in the sense that the weights in my brain are no good for any other brain.
1448000	1450000	When I die those weights are useless.
1450000	1457000	We can get information from one to another rather inefficiently by I produce sentences
1457000	1460000	and you figure out how to change your weight so you would have said the same thing.
1460000	1462000	That's called distillation.
1462000	1465000	But that's a very inefficient way of communicating knowledge.
1465000	1470000	And with digital systems they're immortal because once you've got some weights
1470000	1474000	you can throw away the computer, just store the weights on a tape somewhere
1474000	1477000	and now build another computer, put those same weights in.
1477000	1481000	And if it's digital it can compute exactly the same thing as the other system did.
1481000	1484000	So digital systems can share weights.
1484000	1487000	And that's incredibly much more efficient.
1487000	1493000	If you've got a whole bunch of digital systems and they each go into a tiny bit of learning
1493000	1496000	and they start with the same weights, they do a tiny bit of learning
1496000	1500000	and then they share their weights again, they all know what all the others learned.
1500000	1502000	We can't do that.
1502000	1505000	And so they're far superior to us in being able to share knowledge.
1505000	1511000	A lot of the ideas that have been deployed in the field are very old school ideas.
1511000	1516000	It's the ideas that have been around in neuroscience for forever.
1516000	1520000	What do you think is sort of left to apply to the systems that we develop?
1520000	1527000	So one big thing that we still have to catch up with neuroscience on
1527000	1530000	is the time scales for changes.
1530000	1536000	So in nearly all the neural nets there's a fast time scale for changing activities.
1536000	1540000	So input comes in, the activities, the embedding vectors all change.
1540000	1543000	And then there's a slow time scale which is changing the weights.
1543000	1545000	And that's long-term learning.
1545000	1547000	And you just have those two time scales.
1547000	1551000	In the brain there's many time scales at which weights change.
1551000	1555000	So for example, if I say an unexpected word like cucumber,
1555000	1560000	and now five minutes later you put headphones on, there's a lot of noise
1560000	1565000	and there's very faint words, you'll be much better at recognising the word cucumber
1565000	1567000	because I said it five minutes ago.
1567000	1570000	So where is that knowledge in the brain?
1570000	1573000	And that knowledge is obviously in temporary changes to synapses.
1573000	1576000	It's not neurons that go in cucumber, cucumber, cucumber,
1576000	1578000	you don't have enough neurons for that.
1578000	1581000	It's in temporary changes to the weights.
1581000	1584000	And you can do a lot of things with temporary weight changes,
1584000	1586000	what I call fast weights.
1586000	1588000	We don't do that in these neural models.
1588000	1593000	And the reason we don't do it is because if you have temporary changes to the weights
1593000	1595000	that depend on the input data,
1595000	1600000	then you can't process a whole bunch of different cases at the same time.
1600000	1603000	At present we take a whole bunch of different strings,
1603000	1607000	we stack them together and we process them all in parallel
1607000	1611000	because then we can do matrix, matrix, multiplies, which is much more efficient.
1611000	1615000	And just that efficiency is stopping us using fast weights.
1615000	1619000	But the brain clearly uses fast weights for temporary memory.
1619000	1622000	And there's all sorts of things you can do that way that we don't do at present.
1622000	1624000	I think that's one of the biggest things we have to do.
1624000	1627000	I was very hopeful that things like Graphcore,
1627000	1631000	if they went sequential and did just online learning,
1631000	1633000	then they could use fast weights.
1633000	1636000	But that hasn't worked out yet.
1636000	1640000	I think it'll work out eventually when people are using conductances for weights.
1640000	1644000	How has knowing how these models work
1644000	1649000	and knowing how the brain works impacted the way you think?
1649000	1655000	I think there's been one big impact, which is at a fairly abstract level,
1655000	1660000	which is that for many years people were very scornful
1660000	1663000	about the idea of having a big random neural net
1663000	1665000	and just giving it a lot of training data
1665000	1667000	and it would learn to do complicated things.
1667000	1671000	If you talk to statisticians or linguists or most people in AI,
1671000	1673000	they say, that's just a pipe dream.
1673000	1676000	There's no way you're going to learn to really complicated things
1676000	1678000	without some kind of innate knowledge,
1678000	1680000	without a lot of architectural restrictions.
1680000	1682000	It turns out that's completely wrong.
1682000	1684000	You can take a big random neural network
1684000	1687000	and you can learn a whole bunch of stuff just from data.
1688000	1691000	So the idea that stochastic gradient descent
1691000	1695000	to repeatedly adjust the weights using a gradient,
1695000	1699000	that will learn things and will learn big complicated things,
1699000	1702000	that's been validated by these big models.
1702000	1705000	And that's a very important thing to know about the brain.
1705000	1708000	It doesn't have to have all this innate structure.
1708000	1710000	Now, obviously it's got a lot of innate structure,
1710000	1715000	but it certainly doesn't need an innate structure for things that are easily learned.
1716000	1718000	And so the idea coming from Chomsky,
1718000	1721000	that you won't learn anything complicated like language
1721000	1725000	unless it's all kind of wired in already and just matures,
1725000	1728000	that idea is now clearly nonsense.
1728000	1732000	I'm sure Chomsky would appreciate you calling his ideas nonsense.
1732000	1737000	Well, I think a lot of Chomsky's political ideas are very sensible.
1737000	1741000	I'm always struck by how come someone with such sensible ideas about the Middle East
1741000	1743000	could be so wrong about linguistics.
1744000	1747000	What do you think would make these models
1747000	1751000	simulate consciousness of humans more effectively?
1751000	1756000	But imagine you had the AI assistant that you've spoken to in your entire life,
1756000	1760000	and instead of that being like Chatipiti today,
1760000	1764000	that sort of deletes the memory of the conversation and you start fresh all of the time,
1764000	1767000	it had self-reflection.
1767000	1772000	At some point you pass away and you tell that to the assistant.
1773000	1776000	I mean, not me, somebody else tells that to the assistant.
1776000	1781000	Yeah, it would be difficult for you to tell that to the assistant.
1781000	1785000	Do you think that assistant would feel at that point?
1785000	1787000	Yes, I think they can have feelings too.
1787000	1791000	So I think just as we have this inner theater model for perception,
1791000	1793000	we have an inner theater model for feelings,
1793000	1797000	there are things that I can experience but other people can't.
1799000	1801000	I think that model is equally wrong.
1802000	1807000	Suppose I say, I feel like punching Gary on the nose, which I often do.
1807000	1811000	Let's try and abstract that away from the idea of an inner theater.
1811000	1814000	What I'm really saying to you is,
1814000	1818000	if it weren't for the inhibition coming from my frontal lobes,
1818000	1820000	I would perform an action.
1820000	1825000	So when we talk about feelings, we're really talking about actions we would perform
1825000	1829000	if it weren't for constraints.
1829000	1831000	And that's really what feelings are.
1831000	1835000	The actions we would do if it weren't for constraints.
1835000	1838000	So I think you can give the same kind of explanation for feelings
1838000	1840000	and there's no reason why these things can't have feelings.
1840000	1846000	In fact, in 1973, I saw a robot have an emotion.
1846000	1850000	So in Edinburgh, they had a robot with two grippers like this
1850000	1858000	that could assemble a toy car if you put the pieces separately on a piece of green felt.
1858000	1860000	But if you put them in a pile,
1860000	1863000	its vision wasn't good enough to figure out what was going on.
1863000	1865000	So it put its grip on it and it went whack!
1865000	1868000	And it knocked them so they were scattered and then it coupled them together.
1868000	1871000	If you saw that in a person, you'd say it was crossed with the situation
1871000	1874000	because it didn't understand it so it destroyed it.
1874000	1877000	That's profound.
1877000	1884000	We spoke previously, you described humans and the LLMs as analogy machines.
1884000	1891000	What do you think has been the most powerful analogies that you've found throughout your life?
1891000	1902000	Throughout my life, I guess probably a sort of weak analogy that has influenced me a lot
1902000	1910000	is the analogy between religious belief and between belief and symbol processing.
1911000	1915000	So when I was very young, I came from an atheist family
1915000	1918000	and went to school and was confronted with religious belief.
1918000	1920000	And it just seemed nonsense to me.
1920000	1922000	It still seems nonsense to me.
1922000	1926000	And when I saw symbol processing as an explanation of how people worked,
1926000	1929000	I thought it was just the same.
1929000	1931000	Nonsense.
1931000	1934000	I don't think it's quite so much nonsense now
1934000	1937000	because I think actually we do do symbol processing.
1937000	1941000	It's just we do it by giving these big embedding vectors to the symbols.
1941000	1943000	But we are actually symbol processing.
1943000	1947000	But not at all in the way people thought where you match symbols
1947000	1951000	and the only thing a symbol has is it's identical to another symbol or it's not identical.
1951000	1953000	That's the only property a symbol has.
1953000	1955000	We don't do that at all.
1955000	1957000	We use the context to give embedding vectors to symbols
1957000	1962000	and then use the interactions between the components of these embedding vectors to do thinking.
1963000	1967000	But there's a very good researcher at Google called Fernando Pereira
1967000	1971000	who said, yes, we do have symbolic reasoning
1971000	1973000	and the only symbolic we have is natural language.
1973000	1976000	Natural language is a symbolic language and we reason with it.
1976000	1978000	I believe that now.
1978000	1983000	You've done some of the most meaningful research in the history of computer science.
1983000	1988000	Can you walk us through like how do you select the right problems to work on?
1988000	1990000	Well, first let me correct you.
1990000	1994000	Me and my students have done a lot of the most meaningful things
1994000	1997000	and it's mainly been a very good collaboration with students
1997000	2000000	and my ability to select very good students.
2000000	2004000	And that came from the fact there were very few people doing neural nets
2004000	2007000	in the 70s and 80s and 90s and 2000s.
2007000	2011000	And so the few people doing neural nets got to pick the very best students.
2011000	2013000	So that was a piece of luck.
2013000	2017000	But my way of selecting problems is basically, well,
2017000	2019000	when scientists talk about how they work,
2019000	2021000	they have theories about how they work
2021000	2023000	which probably don't have much to do with the truth.
2023000	2029000	But my theory is that I look for something where everybody's agreed about something
2029000	2031000	and it feels wrong.
2031000	2034000	Just there's a slight intuition of something wrong about it.
2034000	2038000	And then I work on that and see if I can elaborate why it is I think it's wrong.
2038000	2042000	And maybe I can make a little demo with a small computer program
2042000	2046000	that shows that it doesn't work the way you might expect.
2046000	2048000	So let me take one example.
2048000	2052000	Most people think that if you add noise to a neural net, it's going to work worse.
2052000	2057000	If, for example, each time you put a training example through,
2057000	2063000	you make half of the neurons be silent, it'll work worse.
2063000	2068000	Actually, we know it'll generalize better if you do that.
2068000	2073000	And you can demonstrate that in a simple example.
2073000	2075000	That's what's nice about computer simulation.
2075000	2079000	You can show this idea you had that adding noise is going to make it worse
2079000	2082000	and dropping out half the neurons will make it work worse,
2082000	2084000	which you will in the short term.
2084000	2087000	But if you train it like that, in the end it'll work better.
2087000	2089000	You can demonstrate that with a small computer program
2089000	2091000	and then you can think hard about why that is
2091000	2096000	and how it stops big, elaborate co-adaptations.
2096000	2099000	But I think that that's my method of working.
2099000	2102000	Find something that sounds suspicious and work on it
2102000	2106000	and see if you can give a simple demonstration of why it's wrong.
2106000	2108000	What sounds suspicious to you now?
2108000	2111000	Well, that we don't use fast weight sounds suspicious.
2111000	2113000	That we only have these two timescales.
2113000	2116000	That's just wrong. That's not at all like the brain.
2116000	2120000	And in the long run, I think we're going to have to have many more timescales.
2120000	2121000	So that's an example now.
2121000	2126000	And if you had your group of students today and they came to you
2126000	2129000	and they said the hamming question that we talked about previously,
2129000	2132000	what's the most important problem in your field?
2132000	2136000	What would you suggest that they take on and work on next?
2136000	2138000	We spoke about reasoning, timescales.
2138000	2142000	What would be sort of the highest priority problem that you'd give them?
2142000	2147000	For me right now, it's the same question I've had for the last like 30 years or so,
2147000	2151000	which is, does the brain do back propagation?
2151000	2153000	I believe the brain is getting gradients.
2153000	2157000	If you don't get gradients, your learning is just much worse than if you do get gradients.
2157000	2159000	But how is the brain getting gradients?
2159000	2164000	And is it somehow implementing some approximate version of back propagation?
2164000	2166000	Or is it some completely different technique?
2166000	2168000	That's a big open question.
2168000	2172000	And if I kept on doing research, that's what I would be doing research on.
2172000	2178000	And when you look back at your career now, you've been right about so many things,
2178000	2184000	but what were you wrong about that you wish you sort of spent less time pursuing a certain direction?
2184000	2185000	Okay, those are two separate questions.
2185000	2187000	One is what were you wrong about?
2187000	2190000	And two, do you wish you'd spent less time on it?
2190000	2195000	I think I was wrong about Boltzmann machines, and I'm glad I spent a long time on it.
2195000	2199000	There are much more beautiful theory of how you get gradients than back propagation.
2199000	2202000	Back propagation is just ordinary and sensible, and it's just a chain rule.
2202000	2206000	Boltzmann machines is clever, and it's a very interesting way to get gradients.
2206000	2211000	And I would love for that to be how the brain works, but I think it isn't.
2211000	2217000	Did you spend much time imagining what would happen post these systems developing as well?
2217000	2221000	Did you ever have an idea that, okay, if we could make these systems work really well,
2221000	2226000	we could democratise education, we could make knowledge way more accessible,
2226000	2230000	we could solve some tough problems in medicine,
2230000	2234000	or was it more to you about understanding the brain?
2234000	2240000	Yes, I sort of feel scientists ought to be doing things that are going to help society,
2240000	2243000	but actually, that's not how you do your best research.
2243000	2246000	You do your best research when it's driven by curiosity.
2246000	2250000	You just have to understand something.
2250000	2255000	Much more recently, I've realised these things could do a lot of harm as well as a lot of good,
2255000	2259000	and I've become much more concerned about the effects they're going to have on society.
2259000	2261000	But that's not what was motivating me.
2261000	2265000	I just wanted to understand how on earth can the brain learn to do things?
2265000	2266000	That's what I want to know.
2266000	2267000	And I sort of failed.
2267000	2272000	As a side effect of that failure, we got some nice engineering.
2272000	2275000	Yeah, it was a good failure for the world.
2275000	2279000	If you take the lens of the things that could go really right,
2279000	2283000	what do you think are the most promising applications?
2283000	2288000	I think healthcare is clearly a big one.
2288000	2294000	With healthcare, there's almost no end to how much healthcare society can absorb.
2294000	2299000	If you take someone old, they could use five doctors full time.
2299000	2305000	So when AI gets better than people are doing things,
2305000	2310000	you'd like it to get better in areas where you could do with a lot more of that stuff,
2310000	2312000	and we could do with a lot more doctors.
2312000	2315000	If everybody had three doctors of their own, that would be great,
2315000	2318000	and we're going to get to that point.
2318000	2321000	So that's one reason why healthcare is good.
2321000	2326000	There's also just in new engineering, developing new materials, for example,
2326000	2330000	for better solar panels or for superconductivity,
2330000	2335000	or for just understanding how the body works.
2335000	2337000	There's going to be huge impacts there.
2337000	2339000	Those are all going to be good things.
2339000	2343000	What I worry about is bad actors using them for bad things.
2343000	2350000	We've facilitated people like Putin or Xi or Trump using AI for killer robots
2350000	2353000	for manipulating public opinion or for mass surveillance.
2353000	2355000	And those are all very worrying things.
2355000	2362000	Are you ever concerned that slowing down the field could also slow down the positives?
2362000	2363000	Oh, absolutely.
2363000	2368000	And I think there's not much chance that the field will slow down,
2368000	2370000	partly because it's international,
2370000	2373000	and if one country slows down, the other countries aren't going to slow down.
2373000	2377000	So there's a race clearly between China and the US,
2377000	2379000	and neither is going to slow down.
2379000	2383000	So yeah, I mean, there was this partition saying we should slow down for six months.
2383000	2386000	I didn't sign it just because I thought it was never going to happen.
2386000	2389000	I maybe should have signed it because even though it was never going to happen,
2389000	2390000	it made a political point.
2390000	2394000	It's often good to ask for things you know you can't get just to make a point.
2394000	2396000	But I don't think we're going to slow down.
2396000	2403000	And how do you think that it will impact the AI research process having this assistance?
2403000	2405000	I think it'll make it a lot more efficient.
2405000	2410000	AI research will get a lot more efficient when you've got these assistance to help you program,
2410000	2415000	but also help you think through things and probably help you a lot with equations too.
2415000	2419000	Have you reflected much on the process of selecting talent?
2419000	2421000	Has that been mostly intuitive to you?
2421000	2426000	Like when Ilya shows up at the door, you feel this is smart guy, let's work together.
2426000	2430000	So for selecting talent, sometimes you just know.
2430000	2434000	So after talking to Ilya for not very long, he seemed very smart.
2434000	2437000	And then talking to him a bit more, he clearly was very smart
2437000	2441000	and had very good intuitions as well as being good at math.
2441000	2442000	So that was a no-brainer.
2442000	2447000	There's another case where I was at a NIPS conference.
2447000	2453000	We had a poster and someone came up and he started asking questions about the poster.
2453000	2457000	And every question he asked was a sort of deep insight into what we'd done wrong.
2457000	2460000	And after five minutes, I offered him a postdoc position.
2460000	2465000	That guy was David McKay, who was just brilliant and it's very sad he died,
2465000	2469000	but he was very obvious you'd want him.
2469000	2471000	Other times it's not so obvious.
2471000	2475000	And one thing I did learn was that people are different.
2475000	2479000	There's not just one type of good student.
2479000	2485000	So there's some students who aren't that creative but are technically extremely strong
2485000	2487000	and will make anything work.
2487000	2491000	There's other students who aren't technically strong but are very creative.
2491000	2494000	Of course you want the ones who are both, but you don't always get that.
2494000	2499000	But I think actually in the lab you need a variety of different kinds of graduate students.
2499000	2504000	But I still go with my gut intuition that sometimes you talk to somebody
2504000	2509000	and they just get it and those are the ones you want.
2509000	2514000	What do you think is the reason for some folks having better intuition?
2514000	2521000	Do they just have better training data than others or how can you develop your intuition?
2521000	2525000	I think it's partly they don't stand for nonsense.
2525000	2529000	So here's a way to get bad intuitions, believe everything you're told.
2529000	2531000	That's fatal.
2531000	2534000	You have to be able to, I think here's what some people do.
2534000	2537000	They have a whole framework for understanding reality.
2537000	2543000	And when someone tells them something, they try and sort of figure out how that fits into their framework.
2543000	2546000	And if it doesn't, they just reject it.
2546000	2549000	And that's a very good strategy.
2549000	2556000	People who try and incorporate whatever they're told end up with a framework that's sort of very fuzzy
2556000	2560000	and sort of can believe everything and that's useless.
2560000	2567000	So I think actually having a strong view of the world and trying to manipulate incoming facts to fit in with your view.
2567000	2575000	Obviously it can lead you into deep religious belief in fatal flaws and so on, like my belief in Boltzmann machines.
2575000	2577000	But I think that's the way to go.
2577000	2580000	If you've got good intuitions, you should trust them.
2580000	2586000	If you've got bad intuitions, it doesn't matter what you do, so you might as well trust them.
2586000	2589000	Very good point.
2589000	2596000	When you look at the types of research that's being done today,
2596000	2603000	do you think we're putting all of our eggs in one basket and we should diversify our ideas a bit more in the field?
2603000	2605000	Or do you think this is the most promising direction?
2605000	2608000	So let's go all in on it.
2608000	2615000	I think having big models and training them on multimodal data, even if it's only to predict the next word,
2615000	2618000	is such a promising approach that we should go pretty much all in on it.
2618000	2621000	Obviously there's lots and lots of people doing it now.
2621000	2625000	And there's lots of people doing apparently crazy things and that's good.
2625000	2630000	But I think it's fine for most of the people to be following this path because it's working very well.
2630000	2636000	Do you think that the learning algorithms matter that much, or is it just a scale?
2636000	2642000	Are there basically millions of ways that we could get to human level in intelligence,
2642000	2646000	or are there sort of a select few that we need to discover?
2646000	2651000	Yes, so this issue of whether particular learning algorithms are very important,
2651000	2655000	or whether there's a great variety of learning algorithms that will do the job,
2655000	2657000	I don't know the answer.
2657000	2662000	It seems to me though that by propagation there's a sense in which it's the correct thing to do.
2662000	2666000	Getting the gradient so that you change a parameter to make it work better,
2666000	2671000	that seems like the right thing to do, and it's been amazingly successful.
2671000	2676000	There may well be other learning algorithms that are alternative ways of getting that same gradient,
2676000	2681000	all that are getting the gradient to something else and that also work.
2681000	2688000	I think that's all open and a very interesting issue now about whether there's other things you can try and maximize
2688000	2693000	that will give you good systems, and maybe the brain's doing that because it's easier.
2693000	2700000	But backprop is in a sense the right thing to do, and we know that doing it works really well.
2700000	2705000	And one last question, when you look back at your decades of research,
2705000	2708000	what are you most proud of? Is it the students? Is it the research?
2708000	2712000	What makes you most proud of when you look back at your life's work?
2712000	2715000	The learning algorithm for Boltzmann machines.
2715000	2719000	So the learning algorithm for Boltzmann machines is beautifully elegant.
2719000	2727000	It's maybe hopeless in practice, but it's the thing I enjoyed most developing that with Terry,
2727000	2732000	and it's what I'm proudest of, even if it's wrong.
2737000	2741000	What questions do you spend most of your time thinking about now?
2741000	2745000	What should I watch on Netflix?
