Have you reflected a lot on how to select talent or has that mostly been intuitive to you?
Ilya just shows up and you're like, this is a clever guy, let's work together.
Or have you thought a lot about that?
Should we roll this?
Yeah, let's roll this.
We're good, yeah, yeah.
You're juggling into a repetition.
Okay.
Sun is working.
So I remember when I first got to Carnegie Mellon from England.
In England, at a research unit, it would get to be six o'clock and you'd all go for a drink in the pub.
At Carnegie Mellon, I remember after I'd been there a few weeks, it was Saturday night.
I didn't have any friends yet and I didn't know what to do.
So I decided I'd go into the lab and do some programming because I had a list machine and you couldn't program it from home.
So I went into the lab at about nine o'clock on a Saturday night and it was swarming.
All the students were there.
And they were all there because what they were working on was the future.
They all believed that what they did next was going to change the course of computer science.
And it was just so different from England.
And so that was very refreshing.
Take me back to the very beginning, Geoff, at Cambridge, trying to understand the brain.
What was that like?
It was very disappointing.
So I did physiology and in the summer term, they were going to teach us how the brain worked.
And all they taught us was how neurons conduct action potentials, which is very interesting, but it doesn't tell you how the brain works.
So that was extremely disappointing.
I switched to philosophy then.
I thought maybe they'd tell us how the mind worked.
That was very disappointing.
I eventually ended up going to Edinburgh to do AI.
And that was more interesting.
At least you could simulate things so you could test out theories.
And did you remember what intrigued you about AI?
Was it a paper?
Was it any particular person that exposed you to those ideas?
I guess it was a book I read by Donald Hebb that influenced me a lot.
He was very interested in how you learn the connection strengths in neural nets.
I also read a book by John von Neumann early on, who was very interested in how the brain computes and how it's different from normal computers.
And did you get that conviction that these ideas would work out at that point?
Or what was your intuition back at the Edinburgh days?
It seemed to me there has to be a way that the brain learns.
And it's clearly not by having all sorts of things programmed into it and then using logical rules of inference.
That just seemed to me crazy from the outset.
So we had to figure out how the brain learned to modify connections in a neural net so that it could do complicated things.
And von Neumann believed that, Turing believed that.
So von Neumann and Turing were both pretty good at logic, but they didn't believe in this logical approach.
And what was your split between studying the ideas from neuroscience and just doing what seemed to be good algorithms for AI?
How much inspiration did you take early on?
So I never did that much study of neuroscience.
I was always inspired by what I'd learned about how the brain works, that there's a bunch of neurons.
They perform relatively simple operations.
They're non-linear, but they collect inputs, they weight them, and then they give an output that depends on that weighted input.
And the question is, how do you change those weights to make the whole thing do something good?
It seems like a fairly simple question.
What collaborations do you remember from that time?
The main collaboration I had at Carnegie Mellon was with someone who wasn't at Carnegie Mellon.
I was interacting a lot with Terry Sinovsky, who was in Baltimore at Johns Hopkins.
And about once a month, either he would drive to Pittsburgh or I would drive to Baltimore.
It's 250 miles away, and we would spend a weekend together working on Baltimore machines.
That was a wonderful collaboration.
We were both convinced it was how the brain worked.
That was the most exciting research I've ever done.
And a lot of technical results came out that were very interesting, but I think it's not how the brain works.
I also had a very good collaboration with Peter Brown, who was a very good statistician,
and he worked on speech recognition at IBM.
And then he came as a more mature student at Carnegie Mellon just to get a PhD, but he already knew a lot.
He taught me a lot about speech, and he in fact taught me about hidden Markov models.
I think I learned more from him than he learned from me.
That's the kind of student you want.
And when he taught me about hidden Markov models, I was doing backprop with hidden layers.
Only they weren't called hidden layers then.
And I decided that name they use in hidden Markov models is a great name for variables that you don't know what they're up to.
And so that's where the name hidden in neural nets came from.
Me and Peter decided that was a great name for the hidden layers of neural nets.
But I learned a lot from Peter about speech.
Take us back to when Ilya showed up at your office.
I was in my office, probably on a Sunday, and I was programming, I think.
And there was a knock on the door, not just any knock, but it went kinda...
That's sort of an urgent knock.
So I went and answered the door, and this was this young student there.
And he said he was cooking fries over the summer, but he'd rather be working in my lab.
And so I said, well, why don't you make an appointment and we'll talk?
And so Ilya said, how about now?
And that sort of was Ilya's character.
So we talked for a bit, and I gave him a paper to read, which was the Nature Paper and Back Propagation.
And we made another meeting for a week later, and he came back and he said, I didn't understand it.
And I was very disappointed. I thought, he seemed like a bright guy, but it's only the chain rule.
It's not that hard to understand.
And he said, oh, no, no, I understood that.
I just don't understand why you don't give the gradient to a sensible function optimizer,
which took us quite a few years to think about.
And it kept on like that with Ilya.
He had very good, his raw intuitions about things were always very good.
What do you think had enabled those intuitions for Ilya?
I don't know. I think he always thought for himself.
He was always interested in AI from a young age.
He's obviously good at math, but it's very hard to know.
And what was that collaboration between the two of you like?
What part would you play and what part would Ilya play?
It was a lot of fun.
I remember one occasion when we were trying to do a complicated thing with producing maps of data,
where I had a kind of mixture model.
So you could take the same bunch of similarities and make two maps,
so that in one map, bank could be close to greed and in another map, bank could be close to river.
Because in one map, you can't have it close to both, right?
Because river and greed are one way apart.
So we'd have a mixture of maps.
And we were doing it in MATLAB,
and this involved a lot of reorganization of the code to do the right matrix multiplies,
and only got fed up with that.
So he came one day and said,
I'm going to write an interface for MATLAB,
so I program in this different language,
and then I have something that just converts it into MATLAB.
And I said, no Ilya, that'll take you a month to do.
We've got to get on with this project.
Don't get diverted by that.
And Ilya said, it's okay, I did it this morning.
That's quite incredible.
And throughout those years, the biggest shift wasn't necessarily just the algorithms,
but also the skill.
How did you sort of view that skill over the years?
Ilya got that intuition very early.
So Ilya was always preaching that you just make it bigger and it'll work better.
And I always thought that was a bit of a cop-out,
that you're going to have to have new ideas too.
It turns out Ilya was basically right.
New ideas help.
Things like transformers helped a lot.
But it was really the scale of the data and the scale of the computation.
And back then, we had no idea computers would get like a billion times faster.
We thought maybe they'd get 100 times faster.
We were trying to do things by coming up with clever ideas
that would have just solved themselves if we had had bigger scale of the data and computation.
In about 2011, Ilya and another graduate student called James Martins and I
had a paper using character level prediction.
So we took Wikipedia and we tried to predict the next HTML character.
And that worked remarkably well.
And we were always amazed at how well it worked.
And that was using a fancy optimizer on GPUs.
And we could never quite believe that it understood anything,
but it looked as though it understood.
And that just seemed incredible.
Can you take us through how are these models trained to predict the next word
and why is it the wrong way of thinking about them?
Okay, I don't actually believe it is the wrong way.
So, in fact, I think I made the first neural net language model
that used embeddings and backpropagation.
So it's very simple data, just triples.
And it was turning each symbol into an embedding,
then having the embeddings interact to predict the embedding of the next symbol
and then from that predict the next symbol.
And then it was backpropagating through that whole process to learn these triples.
And I showed it could generalize.
About 10 years later, Yoshio Benji used a very similar network
and showed it worked with real text.
And about 10 years after that, Linguist started believing in embeddings.
It was a slow process.
The reason I think it's not just predicting the next symbol
is if you ask, well, what does it take to predict the next symbol?
Particularly if you ask me a question and then the first word of the answer is the next symbol,
you have to understand the question.
So I think by predicting the next symbol, it's very unlike old-fashioned autocomplete.
For old-fashioned autocomplete, you'd store sort of triples of words.
And then if you saw a pair of words, you see how often different words came third
and that way you could predict the next symbol.
And that's what most people think autocomplete is like.
It's no longer at all like that.
To predict the next symbol, you have to understand what's been said.
So I think you're forcing it to understand by making it predict the next symbol.
And I think it's understanding in much the same way we are.
So a lot of people will tell you these things aren't like us.
They're just predicting the next symbol.
They're not reasoning like us.
But actually, in order to predict the next symbol, it's going to have to do some reasoning.
And we've seen now that if you make big ones, without putting in any special stuff to do reasoning,
they can already do some reasoning.
And I think as you make them bigger, they're going to be able to do more and more reasoning.
Do you think I'm doing anything else than predicting the next symbol right now?
I think that's how you're learning.
I think you're predicting the next video frame.
You're predicting the next sound.
But I think that's a pretty plausible theory of how the brain's learning.
What enables these models to learn such a wide variety of fields?
What these big language models are doing is they're looking for common structure.
And by finding common structure, they can encode things using the common structure and that's more efficient.
So let me give you an example.
If you ask GPT-4, why is a compost heap like an atom bomb?
Most people can't answer that.
Most people haven't thought they think atom bombs and compost heap are very different things.
But GPT-4 will tell you, well, the energy scales are very different and the time scales are very different.
But the thing that's the same is that when the compost heap gets hotter, it generates heat faster.
And when the atom bomb produces more neutrons, it produces more neutrons faster.
And so it gets the idea of a chain reaction.
And I believe it's understood that both forms of chain reaction is using that understanding to compress all that information into its weights.
And if it's doing that, then it's going to be doing that for hundreds of things where we haven't seen the analogies yet, but it has.
And that's where you get creativity from, from seeing these analogies between apparently very different things.
And so I think GPT-4 is going to end up, when it gets bigger, being very creative.
I think this idea that it's just regurgitating what it's learned, just pastishing together text it's learned already, that's completely wrong.
It's going to be even more creative than people, I think.
You'd argue that it won't just repeat the human knowledge we've developed so far, but could also progress beyond that.
I think that's something we haven't quite seen yet.
We've started seeing some examples of it.
But to a large extent, we're sort of still at the current level of science.
What do you think will enable it to go beyond that?
Well, we've seen that in more limited contexts.
Like if you take AlphaGo, in that famous competition with Lysidol, there was Move 37, where AlphaGo made a move that all the experts said must have been a mistake.
But actually later they realized it was a brilliant move.
So that was creative within that limited domain.
I think we'll see a lot more of that as these things get bigger.
The difference with AlphaGo as well was that it was using reinforcement learning, that that subsequently sort of enabled it to go beyond the current state.
So it started with imitation learning, watching how humans play the game, and then it would, through self-play, develop way beyond that.
Do you think that's the missing component of the current evidence?
I think that may well be a missing component, yes.
That the self-play in AlphaGo and AlphaZero are a large part of why it could make these creative moves.
But I don't think it's entirely necessary.
So there's a little experiment I did a long time ago where you're training on your own net to recognize 100 digits.
I love that example, the MNIST example.
And you give it training data where half the answers are wrong.
And the question is, how well will it learn?
And you make half the answers wrong once and keep them like that.
So it can't average away the wrongness by just seeing the same example, but with the right answer sometimes and the wrong answer sometimes.
When it sees that example, half of the examples, when it sees the example, the answer is always wrong.
And so the training data has 50% error.
But if you train up bank propagation, it gets down to 5% error, or less.
In other words, from badly labeled data, it can get much better results.
It can see that the training data is wrong.
And that's how smart students can be smarter than their advisor.
And their advisor tells them all this stuff, and for half of what their advisor tells them, they think, no, rubbish.
And they listen to the other half, and then they end up smarter than the advisor.
So these big neural nets can actually do, they can do much better than their training data.
And most people don't realize that.
So how do you expect these models to add reasoning into them?
So I mean, one approach is you add sort of the heuristics on top of them, which a lot of the research is doing now,
where you have sort of chain of thought, you just feedback its reasoning into itself.
And another way would be in the model itself, as you scale it up.
What's your intuition around that?
So my intuition is that as we scale up these models, they get better at reasoning.
And if you ask how people work, roughly speaking, we have these intuitions, and we can do reasoning.
And we use the reasoning to correct our intuitions.
Of course, we use the intuitions during the reasoning to do the reasoning.
But if the conclusion of the reasoning conflicts with our intuitions, we realize the intuitions need to be changed.
That's much like in AlphaGo or AlphaZero, where you have an evaluation function that just looks at the board and says, how good is that for me?
But then you do the Monte Carlo rollout, and now you get a more accurate idea, and you can revise your evaluation function.
So you can train it by getting it to agree with the results of reasoning.
And I think these large language models have to start doing that.
They have to start training their raw intuitions about what should come next by doing reasoning and realizing that's not right.
And so that way they can get more training data than just mimicking what people did.
And that's exactly why AlphaGo could do this creative move 37.
It had much more training data because it was using reasoning to check out what the right next move should have been.
And what do you think about multimodality?
So we spoke about these analogies, and often the analogies are way beyond what we could see.
It's discovering analogies that are far beyond humans and at maybe abstraction levels that we'll never be able to understand.
Now, when we introduce images to that and video and sound, how do you think that will change the models?
And how do you think it will change the analogies that it will be able to make?
I think it'll change it a lot. I think it'll make it much better at understanding spatial things, for example.
From language alone, it's quite hard to understand some spatial things, although remarkably GPT-4 can do that even before it was multimodal.
But when you make it multimodal, if you have it both doing vision and reaching out and grabbing things,
it'll understand objects much better if it can pick them up and turn them over and so on.
So although you can learn an awful lot from language, it's easier to learn if you are multimodal.
And in fact, you then need less language.
And there's an awful lot of YouTube video for predicting the next frame or something like that.
So I think these multimodal models are clearly going to take over.
You can get more data that way. They need less language.
So there's really a philosophical point that you could learn a very good model from language alone,
but it's much easier to learn it from a multimodal system.
And how do you think it will impact the model's reasoning?
I think it'll make it much better at reasoning about space, for example.
Reasoning about what happens if you pick objects up.
If you actually try picking objects up, you're going to get all sorts of training data that's going to help.
Do you think the human brain evolved to work well with language?
Or do you think language evolved to work well with the human brain?
I think the question of whether language evolved to work with the brain
or the brain evolved to work with language, I think that's a very good question.
I think both happened.
I used to think we would do a lot of cognition without needing language at all.
Now I've changed my mind a bit.
So let me give you three different views of language and how it relates to cognition.
There's the old-fashioned symbolic view, which is cognition consists of having strings of symbols
in some kind of cleaned up logical language where there's no ambiguity
and applying rules of inference.
And that's what cognition is.
It's just these symbolic manipulations on things that are like strings of language symbols.
So that's one extreme view.
The opposite extreme view is, no, no, once you get inside the head, it's all vectors.
So symbols come in, you convert those symbols into big vectors
and all the stuff inside is done with big vectors,
and then if you want to produce output, you produce symbols again.
So there was a point in machine translation in about 2014
when people were using recurrent neural nets and words would keep coming in
and they'd have a hidden state and they'd keep accumulating information in this hidden state.
And then they got to the end of a sentence that have a big hidden vector
that captured the meaning of that sentence
that could then be used for producing the sentences in another language.
That was called a thought vector.
And that's the sort of second view of language.
You convert the language into a big vector that's nothing like language
and that's what cognition is all about.
But then there's a third view, which is what I believe now,
which is that you take these symbols
and you convert the symbols into embeddings and you use multiple layers of that
so you get these very rich embeddings.
But the embeddings are still tied to the symbols
in the sense that you've got a big vector for this symbol and a big vector for that symbol
and these vectors interact to produce the vector for the symbol for the next word.
And that's what understanding is.
Understanding is knowing how to convert the symbols into these vectors
and knowing how the elements of the vectors should interact to predict the vector for the next symbol.
That's what understanding is, both in these big language models and in our brains.
And that's an example which is sort of in between.
You're staying with the symbols, but you're interpreting them as these big vectors
and that's where all the work is.
And all the knowledge is in what vectors you use
and how the elements of those vectors interact, not in symbolic rules.
But it's not saying that you get away from the symbols altogether.
It's saying you turn the symbols into big vectors
but you stay with that surface structure of the symbols.
And that's how these models are working.
And that's, and I seem to be, a more plausible model of human thought too.
You were one of the first folks to get the idea of using GPUs.
And I know Jensen loves you for that.
Back in 2009 you mentioned that, you told Jensen that this could be a quite good idea
for training neural nets.
Take us back to that early intuition of using GPUs for training neural nets.
So actually I think in about 2006 I had a former graduate student called Rick Zeliski.
He's a very good computer vision guy.
And I talked to him at a meeting.
He said, you know, you ought to think about using graphics processing cards
because they're very good at matrix multiplies.
And what you're doing is basically all matrix multiplies.
So I thought about that for a bit.
And then we learned about these Tesla systems that had four GPUs in.
And initially we just got gaming GPUs and discovered they made things go 30 times faster.
And then we bought one of these Tesla systems with four GPUs.
And we did speech on that and it worked very well.
And then in 2009 I gave a talk at NIPS.
And I told a thousand machine learning researchers
that you should all go and buy Nvidia GPUs.
They're the future.
You need them for doing machine learning.
And I actually then sent mail to Nvidia saying,
I told a thousand machine learning researchers to buy your boards.
Could you give me a free one?
And they said no.
Actually they didn't say no.
They just didn't reply.
But when I told Jensen this story later on, he gave me a free one.
That's very, very good.
I think what's interesting as well is sort of how GPUs has evolved alongside the field.
So where do you think we should go next in the computer?
So my last couple of years at Google I was thinking about ways of trying to make analog computation.
So instead of using like a megawatt we could use like 30 watts like the brain.
And we could run these big language models in analog hardware.
And I never made it work.
But I started really appreciating digital computation.
So if you're going to use that low power analog computation,
every piece of hardware is going to be a bit different.
And the idea is the learning is going to make use of the specific properties of that hardware.
And that's what happens with people.
All our brains are different.
So we can't then take the weights in your brain and put them in my brain.
The hardware is different.
The precise properties of the individual neurons are different.
The learning has learned to make use of all that.
And so we're mortal in the sense that the weights in my brain are no good for any other brain.
When I die those weights are useless.
We can get information from one to another rather inefficiently by I produce sentences
and you figure out how to change your weight so you would have said the same thing.
That's called distillation.
But that's a very inefficient way of communicating knowledge.
And with digital systems they're immortal because once you've got some weights
you can throw away the computer, just store the weights on a tape somewhere
and now build another computer, put those same weights in.
And if it's digital it can compute exactly the same thing as the other system did.
So digital systems can share weights.
And that's incredibly much more efficient.
If you've got a whole bunch of digital systems and they each go into a tiny bit of learning
and they start with the same weights, they do a tiny bit of learning
and then they share their weights again, they all know what all the others learned.
We can't do that.
And so they're far superior to us in being able to share knowledge.
A lot of the ideas that have been deployed in the field are very old school ideas.
It's the ideas that have been around in neuroscience for forever.
What do you think is sort of left to apply to the systems that we develop?
So one big thing that we still have to catch up with neuroscience on
is the time scales for changes.
So in nearly all the neural nets there's a fast time scale for changing activities.
So input comes in, the activities, the embedding vectors all change.
And then there's a slow time scale which is changing the weights.
And that's long-term learning.
And you just have those two time scales.
In the brain there's many time scales at which weights change.
So for example, if I say an unexpected word like cucumber,
and now five minutes later you put headphones on, there's a lot of noise
and there's very faint words, you'll be much better at recognising the word cucumber
because I said it five minutes ago.
So where is that knowledge in the brain?
And that knowledge is obviously in temporary changes to synapses.
It's not neurons that go in cucumber, cucumber, cucumber,
you don't have enough neurons for that.
It's in temporary changes to the weights.
And you can do a lot of things with temporary weight changes,
what I call fast weights.
We don't do that in these neural models.
And the reason we don't do it is because if you have temporary changes to the weights
that depend on the input data,
then you can't process a whole bunch of different cases at the same time.
At present we take a whole bunch of different strings,
we stack them together and we process them all in parallel
because then we can do matrix, matrix, multiplies, which is much more efficient.
And just that efficiency is stopping us using fast weights.
But the brain clearly uses fast weights for temporary memory.
And there's all sorts of things you can do that way that we don't do at present.
I think that's one of the biggest things we have to do.
I was very hopeful that things like Graphcore,
if they went sequential and did just online learning,
then they could use fast weights.
But that hasn't worked out yet.
I think it'll work out eventually when people are using conductances for weights.
How has knowing how these models work
and knowing how the brain works impacted the way you think?
I think there's been one big impact, which is at a fairly abstract level,
which is that for many years people were very scornful
about the idea of having a big random neural net
and just giving it a lot of training data
and it would learn to do complicated things.
If you talk to statisticians or linguists or most people in AI,
they say, that's just a pipe dream.
There's no way you're going to learn to really complicated things
without some kind of innate knowledge,
without a lot of architectural restrictions.
It turns out that's completely wrong.
You can take a big random neural network
and you can learn a whole bunch of stuff just from data.
So the idea that stochastic gradient descent
to repeatedly adjust the weights using a gradient,
that will learn things and will learn big complicated things,
that's been validated by these big models.
And that's a very important thing to know about the brain.
It doesn't have to have all this innate structure.
Now, obviously it's got a lot of innate structure,
but it certainly doesn't need an innate structure for things that are easily learned.
And so the idea coming from Chomsky,
that you won't learn anything complicated like language
unless it's all kind of wired in already and just matures,
that idea is now clearly nonsense.
I'm sure Chomsky would appreciate you calling his ideas nonsense.
Well, I think a lot of Chomsky's political ideas are very sensible.
I'm always struck by how come someone with such sensible ideas about the Middle East
could be so wrong about linguistics.
What do you think would make these models
simulate consciousness of humans more effectively?
But imagine you had the AI assistant that you've spoken to in your entire life,
and instead of that being like Chatipiti today,
that sort of deletes the memory of the conversation and you start fresh all of the time,
it had self-reflection.
At some point you pass away and you tell that to the assistant.
I mean, not me, somebody else tells that to the assistant.
Yeah, it would be difficult for you to tell that to the assistant.
Do you think that assistant would feel at that point?
Yes, I think they can have feelings too.
So I think just as we have this inner theater model for perception,
we have an inner theater model for feelings,
there are things that I can experience but other people can't.
I think that model is equally wrong.
Suppose I say, I feel like punching Gary on the nose, which I often do.
Let's try and abstract that away from the idea of an inner theater.
What I'm really saying to you is,
if it weren't for the inhibition coming from my frontal lobes,
I would perform an action.
So when we talk about feelings, we're really talking about actions we would perform
if it weren't for constraints.
And that's really what feelings are.
The actions we would do if it weren't for constraints.
So I think you can give the same kind of explanation for feelings
and there's no reason why these things can't have feelings.
In fact, in 1973, I saw a robot have an emotion.
So in Edinburgh, they had a robot with two grippers like this
that could assemble a toy car if you put the pieces separately on a piece of green felt.
But if you put them in a pile,
its vision wasn't good enough to figure out what was going on.
So it put its grip on it and it went whack!
And it knocked them so they were scattered and then it coupled them together.
If you saw that in a person, you'd say it was crossed with the situation
because it didn't understand it so it destroyed it.
That's profound.
We spoke previously, you described humans and the LLMs as analogy machines.
What do you think has been the most powerful analogies that you've found throughout your life?
Throughout my life, I guess probably a sort of weak analogy that has influenced me a lot
is the analogy between religious belief and between belief and symbol processing.
So when I was very young, I came from an atheist family
and went to school and was confronted with religious belief.
And it just seemed nonsense to me.
It still seems nonsense to me.
And when I saw symbol processing as an explanation of how people worked,
I thought it was just the same.
Nonsense.
I don't think it's quite so much nonsense now
because I think actually we do do symbol processing.
It's just we do it by giving these big embedding vectors to the symbols.
But we are actually symbol processing.
But not at all in the way people thought where you match symbols
and the only thing a symbol has is it's identical to another symbol or it's not identical.
That's the only property a symbol has.
We don't do that at all.
We use the context to give embedding vectors to symbols
and then use the interactions between the components of these embedding vectors to do thinking.
But there's a very good researcher at Google called Fernando Pereira
who said, yes, we do have symbolic reasoning
and the only symbolic we have is natural language.
Natural language is a symbolic language and we reason with it.
I believe that now.
You've done some of the most meaningful research in the history of computer science.
Can you walk us through like how do you select the right problems to work on?
Well, first let me correct you.
Me and my students have done a lot of the most meaningful things
and it's mainly been a very good collaboration with students
and my ability to select very good students.
And that came from the fact there were very few people doing neural nets
in the 70s and 80s and 90s and 2000s.
And so the few people doing neural nets got to pick the very best students.
So that was a piece of luck.
But my way of selecting problems is basically, well,
when scientists talk about how they work,
they have theories about how they work
which probably don't have much to do with the truth.
But my theory is that I look for something where everybody's agreed about something
and it feels wrong.
Just there's a slight intuition of something wrong about it.
And then I work on that and see if I can elaborate why it is I think it's wrong.
And maybe I can make a little demo with a small computer program
that shows that it doesn't work the way you might expect.
So let me take one example.
Most people think that if you add noise to a neural net, it's going to work worse.
If, for example, each time you put a training example through,
you make half of the neurons be silent, it'll work worse.
Actually, we know it'll generalize better if you do that.
And you can demonstrate that in a simple example.
That's what's nice about computer simulation.
You can show this idea you had that adding noise is going to make it worse
and dropping out half the neurons will make it work worse,
which you will in the short term.
But if you train it like that, in the end it'll work better.
You can demonstrate that with a small computer program
and then you can think hard about why that is
and how it stops big, elaborate co-adaptations.
But I think that that's my method of working.
Find something that sounds suspicious and work on it
and see if you can give a simple demonstration of why it's wrong.
What sounds suspicious to you now?
Well, that we don't use fast weight sounds suspicious.
That we only have these two timescales.
That's just wrong. That's not at all like the brain.
And in the long run, I think we're going to have to have many more timescales.
So that's an example now.
And if you had your group of students today and they came to you
and they said the hamming question that we talked about previously,
what's the most important problem in your field?
What would you suggest that they take on and work on next?
We spoke about reasoning, timescales.
What would be sort of the highest priority problem that you'd give them?
For me right now, it's the same question I've had for the last like 30 years or so,
which is, does the brain do back propagation?
I believe the brain is getting gradients.
If you don't get gradients, your learning is just much worse than if you do get gradients.
But how is the brain getting gradients?
And is it somehow implementing some approximate version of back propagation?
Or is it some completely different technique?
That's a big open question.
And if I kept on doing research, that's what I would be doing research on.
And when you look back at your career now, you've been right about so many things,
but what were you wrong about that you wish you sort of spent less time pursuing a certain direction?
Okay, those are two separate questions.
One is what were you wrong about?
And two, do you wish you'd spent less time on it?
I think I was wrong about Boltzmann machines, and I'm glad I spent a long time on it.
There are much more beautiful theory of how you get gradients than back propagation.
Back propagation is just ordinary and sensible, and it's just a chain rule.
Boltzmann machines is clever, and it's a very interesting way to get gradients.
And I would love for that to be how the brain works, but I think it isn't.
Did you spend much time imagining what would happen post these systems developing as well?
Did you ever have an idea that, okay, if we could make these systems work really well,
we could democratise education, we could make knowledge way more accessible,
we could solve some tough problems in medicine,
or was it more to you about understanding the brain?
Yes, I sort of feel scientists ought to be doing things that are going to help society,
but actually, that's not how you do your best research.
You do your best research when it's driven by curiosity.
You just have to understand something.
Much more recently, I've realised these things could do a lot of harm as well as a lot of good,
and I've become much more concerned about the effects they're going to have on society.
But that's not what was motivating me.
I just wanted to understand how on earth can the brain learn to do things?
That's what I want to know.
And I sort of failed.
As a side effect of that failure, we got some nice engineering.
Yeah, it was a good failure for the world.
If you take the lens of the things that could go really right,
what do you think are the most promising applications?
I think healthcare is clearly a big one.
With healthcare, there's almost no end to how much healthcare society can absorb.
If you take someone old, they could use five doctors full time.
So when AI gets better than people are doing things,
you'd like it to get better in areas where you could do with a lot more of that stuff,
and we could do with a lot more doctors.
If everybody had three doctors of their own, that would be great,
and we're going to get to that point.
So that's one reason why healthcare is good.
There's also just in new engineering, developing new materials, for example,
for better solar panels or for superconductivity,
or for just understanding how the body works.
There's going to be huge impacts there.
Those are all going to be good things.
What I worry about is bad actors using them for bad things.
We've facilitated people like Putin or Xi or Trump using AI for killer robots
for manipulating public opinion or for mass surveillance.
And those are all very worrying things.
Are you ever concerned that slowing down the field could also slow down the positives?
Oh, absolutely.
And I think there's not much chance that the field will slow down,
partly because it's international,
and if one country slows down, the other countries aren't going to slow down.
So there's a race clearly between China and the US,
and neither is going to slow down.
So yeah, I mean, there was this partition saying we should slow down for six months.
I didn't sign it just because I thought it was never going to happen.
I maybe should have signed it because even though it was never going to happen,
it made a political point.
It's often good to ask for things you know you can't get just to make a point.
But I don't think we're going to slow down.
And how do you think that it will impact the AI research process having this assistance?
I think it'll make it a lot more efficient.
AI research will get a lot more efficient when you've got these assistance to help you program,
but also help you think through things and probably help you a lot with equations too.
Have you reflected much on the process of selecting talent?
Has that been mostly intuitive to you?
Like when Ilya shows up at the door, you feel this is smart guy, let's work together.
So for selecting talent, sometimes you just know.
So after talking to Ilya for not very long, he seemed very smart.
And then talking to him a bit more, he clearly was very smart
and had very good intuitions as well as being good at math.
So that was a no-brainer.
There's another case where I was at a NIPS conference.
We had a poster and someone came up and he started asking questions about the poster.
And every question he asked was a sort of deep insight into what we'd done wrong.
And after five minutes, I offered him a postdoc position.
That guy was David McKay, who was just brilliant and it's very sad he died,
but he was very obvious you'd want him.
Other times it's not so obvious.
And one thing I did learn was that people are different.
There's not just one type of good student.
So there's some students who aren't that creative but are technically extremely strong
and will make anything work.
There's other students who aren't technically strong but are very creative.
Of course you want the ones who are both, but you don't always get that.
But I think actually in the lab you need a variety of different kinds of graduate students.
But I still go with my gut intuition that sometimes you talk to somebody
and they just get it and those are the ones you want.
What do you think is the reason for some folks having better intuition?
Do they just have better training data than others or how can you develop your intuition?
I think it's partly they don't stand for nonsense.
So here's a way to get bad intuitions, believe everything you're told.
That's fatal.
You have to be able to, I think here's what some people do.
They have a whole framework for understanding reality.
And when someone tells them something, they try and sort of figure out how that fits into their framework.
And if it doesn't, they just reject it.
And that's a very good strategy.
People who try and incorporate whatever they're told end up with a framework that's sort of very fuzzy
and sort of can believe everything and that's useless.
So I think actually having a strong view of the world and trying to manipulate incoming facts to fit in with your view.
Obviously it can lead you into deep religious belief in fatal flaws and so on, like my belief in Boltzmann machines.
But I think that's the way to go.
If you've got good intuitions, you should trust them.
If you've got bad intuitions, it doesn't matter what you do, so you might as well trust them.
Very good point.
When you look at the types of research that's being done today,
do you think we're putting all of our eggs in one basket and we should diversify our ideas a bit more in the field?
Or do you think this is the most promising direction?
So let's go all in on it.
I think having big models and training them on multimodal data, even if it's only to predict the next word,
is such a promising approach that we should go pretty much all in on it.
Obviously there's lots and lots of people doing it now.
And there's lots of people doing apparently crazy things and that's good.
But I think it's fine for most of the people to be following this path because it's working very well.
Do you think that the learning algorithms matter that much, or is it just a scale?
Are there basically millions of ways that we could get to human level in intelligence,
or are there sort of a select few that we need to discover?
Yes, so this issue of whether particular learning algorithms are very important,
or whether there's a great variety of learning algorithms that will do the job,
I don't know the answer.
It seems to me though that by propagation there's a sense in which it's the correct thing to do.
Getting the gradient so that you change a parameter to make it work better,
that seems like the right thing to do, and it's been amazingly successful.
There may well be other learning algorithms that are alternative ways of getting that same gradient,
all that are getting the gradient to something else and that also work.
I think that's all open and a very interesting issue now about whether there's other things you can try and maximize
that will give you good systems, and maybe the brain's doing that because it's easier.
But backprop is in a sense the right thing to do, and we know that doing it works really well.
And one last question, when you look back at your decades of research,
what are you most proud of? Is it the students? Is it the research?
What makes you most proud of when you look back at your life's work?
The learning algorithm for Boltzmann machines.
So the learning algorithm for Boltzmann machines is beautifully elegant.
It's maybe hopeless in practice, but it's the thing I enjoyed most developing that with Terry,
and it's what I'm proudest of, even if it's wrong.
What questions do you spend most of your time thinking about now?
What should I watch on Netflix?
