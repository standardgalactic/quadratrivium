WEBVTT

00:00.000 --> 00:08.000
Have you reflected a lot on how to select talent or has that mostly been intuitive to you?

00:08.000 --> 00:12.400
Ilya just shows up and you're like, this is a clever guy, let's work together.

00:12.400 --> 00:14.600
Or have you thought a lot about that?

00:14.600 --> 00:17.600
Should we roll this?

00:17.600 --> 00:18.600
Yeah, let's roll this.

00:18.600 --> 00:20.000
We're good, yeah, yeah.

00:20.000 --> 00:21.600
You're juggling into a repetition.

00:21.600 --> 00:22.600
Okay.

00:25.000 --> 00:26.000
Sun is working.

00:30.000 --> 00:34.000
So I remember when I first got to Carnegie Mellon from England.

00:34.000 --> 00:39.000
In England, at a research unit, it would get to be six o'clock and you'd all go for a drink in the pub.

00:39.000 --> 00:44.000
At Carnegie Mellon, I remember after I'd been there a few weeks, it was Saturday night.

00:44.000 --> 00:47.000
I didn't have any friends yet and I didn't know what to do.

00:47.000 --> 00:52.000
So I decided I'd go into the lab and do some programming because I had a list machine and you couldn't program it from home.

00:52.000 --> 00:57.000
So I went into the lab at about nine o'clock on a Saturday night and it was swarming.

00:57.000 --> 00:59.000
All the students were there.

00:59.000 --> 01:02.000
And they were all there because what they were working on was the future.

01:02.000 --> 01:07.000
They all believed that what they did next was going to change the course of computer science.

01:07.000 --> 01:09.000
And it was just so different from England.

01:09.000 --> 01:12.000
And so that was very refreshing.

01:12.000 --> 01:18.000
Take me back to the very beginning, Geoff, at Cambridge, trying to understand the brain.

01:18.000 --> 01:20.000
What was that like?

01:20.000 --> 01:22.000
It was very disappointing.

01:22.000 --> 01:27.000
So I did physiology and in the summer term, they were going to teach us how the brain worked.

01:27.000 --> 01:34.000
And all they taught us was how neurons conduct action potentials, which is very interesting, but it doesn't tell you how the brain works.

01:34.000 --> 01:36.000
So that was extremely disappointing.

01:36.000 --> 01:38.000
I switched to philosophy then.

01:38.000 --> 01:40.000
I thought maybe they'd tell us how the mind worked.

01:40.000 --> 01:42.000
That was very disappointing.

01:42.000 --> 01:45.000
I eventually ended up going to Edinburgh to do AI.

01:45.000 --> 01:46.000
And that was more interesting.

01:46.000 --> 01:50.000
At least you could simulate things so you could test out theories.

01:50.000 --> 01:53.000
And did you remember what intrigued you about AI?

01:53.000 --> 01:55.000
Was it a paper?

01:55.000 --> 01:59.000
Was it any particular person that exposed you to those ideas?

01:59.000 --> 02:04.000
I guess it was a book I read by Donald Hebb that influenced me a lot.

02:04.000 --> 02:09.000
He was very interested in how you learn the connection strengths in neural nets.

02:09.000 --> 02:19.000
I also read a book by John von Neumann early on, who was very interested in how the brain computes and how it's different from normal computers.

02:19.000 --> 02:25.000
And did you get that conviction that these ideas would work out at that point?

02:25.000 --> 02:29.000
Or what was your intuition back at the Edinburgh days?

02:29.000 --> 02:34.000
It seemed to me there has to be a way that the brain learns.

02:34.000 --> 02:41.000
And it's clearly not by having all sorts of things programmed into it and then using logical rules of inference.

02:41.000 --> 02:45.000
That just seemed to me crazy from the outset.

02:45.000 --> 02:53.000
So we had to figure out how the brain learned to modify connections in a neural net so that it could do complicated things.

02:53.000 --> 02:56.000
And von Neumann believed that, Turing believed that.

02:56.000 --> 03:01.000
So von Neumann and Turing were both pretty good at logic, but they didn't believe in this logical approach.

03:01.000 --> 03:11.000
And what was your split between studying the ideas from neuroscience and just doing what seemed to be good algorithms for AI?

03:11.000 --> 03:14.000
How much inspiration did you take early on?

03:14.000 --> 03:16.000
So I never did that much study of neuroscience.

03:16.000 --> 03:22.000
I was always inspired by what I'd learned about how the brain works, that there's a bunch of neurons.

03:22.000 --> 03:24.000
They perform relatively simple operations.

03:24.000 --> 03:32.000
They're non-linear, but they collect inputs, they weight them, and then they give an output that depends on that weighted input.

03:32.000 --> 03:36.000
And the question is, how do you change those weights to make the whole thing do something good?

03:36.000 --> 03:38.000
It seems like a fairly simple question.

03:38.000 --> 03:42.000
What collaborations do you remember from that time?

03:42.000 --> 03:46.000
The main collaboration I had at Carnegie Mellon was with someone who wasn't at Carnegie Mellon.

03:46.000 --> 03:51.000
I was interacting a lot with Terry Sinovsky, who was in Baltimore at Johns Hopkins.

03:51.000 --> 03:55.000
And about once a month, either he would drive to Pittsburgh or I would drive to Baltimore.

03:55.000 --> 03:59.000
It's 250 miles away, and we would spend a weekend together working on Baltimore machines.

03:59.000 --> 04:01.000
That was a wonderful collaboration.

04:01.000 --> 04:03.000
We were both convinced it was how the brain worked.

04:03.000 --> 04:05.000
That was the most exciting research I've ever done.

04:05.000 --> 04:11.000
And a lot of technical results came out that were very interesting, but I think it's not how the brain works.

04:11.000 --> 04:18.000
I also had a very good collaboration with Peter Brown, who was a very good statistician,

04:18.000 --> 04:20.000
and he worked on speech recognition at IBM.

04:20.000 --> 04:28.000
And then he came as a more mature student at Carnegie Mellon just to get a PhD, but he already knew a lot.

04:28.000 --> 04:32.000
He taught me a lot about speech, and he in fact taught me about hidden Markov models.

04:32.000 --> 04:35.000
I think I learned more from him than he learned from me.

04:35.000 --> 04:37.000
That's the kind of student you want.

04:37.000 --> 04:43.000
And when he taught me about hidden Markov models, I was doing backprop with hidden layers.

04:43.000 --> 04:45.000
Only they weren't called hidden layers then.

04:45.000 --> 04:52.000
And I decided that name they use in hidden Markov models is a great name for variables that you don't know what they're up to.

04:52.000 --> 04:57.000
And so that's where the name hidden in neural nets came from.

04:57.000 --> 05:02.000
Me and Peter decided that was a great name for the hidden layers of neural nets.

05:02.000 --> 05:05.000
But I learned a lot from Peter about speech.

05:05.000 --> 05:10.000
Take us back to when Ilya showed up at your office.

05:10.000 --> 05:15.000
I was in my office, probably on a Sunday, and I was programming, I think.

05:15.000 --> 05:18.000
And there was a knock on the door, not just any knock, but it went kinda...

05:18.000 --> 05:21.000
That's sort of an urgent knock.

05:21.000 --> 05:24.000
So I went and answered the door, and this was this young student there.

05:24.000 --> 05:28.000
And he said he was cooking fries over the summer, but he'd rather be working in my lab.

05:28.000 --> 05:32.000
And so I said, well, why don't you make an appointment and we'll talk?

05:32.000 --> 05:34.000
And so Ilya said, how about now?

05:34.000 --> 05:37.000
And that sort of was Ilya's character.

05:37.000 --> 05:43.000
So we talked for a bit, and I gave him a paper to read, which was the Nature Paper and Back Propagation.

05:43.000 --> 05:49.000
And we made another meeting for a week later, and he came back and he said, I didn't understand it.

05:49.000 --> 05:53.000
And I was very disappointed. I thought, he seemed like a bright guy, but it's only the chain rule.

05:53.000 --> 05:55.000
It's not that hard to understand.

05:55.000 --> 05:58.000
And he said, oh, no, no, I understood that.

05:58.000 --> 06:03.000
I just don't understand why you don't give the gradient to a sensible function optimizer,

06:03.000 --> 06:06.000
which took us quite a few years to think about.

06:06.000 --> 06:08.000
And it kept on like that with Ilya.

06:08.000 --> 06:12.000
He had very good, his raw intuitions about things were always very good.

06:12.000 --> 06:17.000
What do you think had enabled those intuitions for Ilya?

06:17.000 --> 06:20.000
I don't know. I think he always thought for himself.

06:20.000 --> 06:23.000
He was always interested in AI from a young age.

06:23.000 --> 06:27.000
He's obviously good at math, but it's very hard to know.

06:27.000 --> 06:31.000
And what was that collaboration between the two of you like?

06:31.000 --> 06:34.000
What part would you play and what part would Ilya play?

06:34.000 --> 06:36.000
It was a lot of fun.

06:36.000 --> 06:44.000
I remember one occasion when we were trying to do a complicated thing with producing maps of data,

06:44.000 --> 06:46.000
where I had a kind of mixture model.

06:46.000 --> 06:49.000
So you could take the same bunch of similarities and make two maps,

06:49.000 --> 06:55.000
so that in one map, bank could be close to greed and in another map, bank could be close to river.

06:56.000 --> 06:59.000
Because in one map, you can't have it close to both, right?

06:59.000 --> 07:01.000
Because river and greed are one way apart.

07:01.000 --> 07:03.000
So we'd have a mixture of maps.

07:03.000 --> 07:05.000
And we were doing it in MATLAB,

07:05.000 --> 07:09.000
and this involved a lot of reorganization of the code to do the right matrix multiplies,

07:09.000 --> 07:11.000
and only got fed up with that.

07:11.000 --> 07:13.000
So he came one day and said,

07:13.000 --> 07:16.000
I'm going to write an interface for MATLAB,

07:16.000 --> 07:18.000
so I program in this different language,

07:18.000 --> 07:21.000
and then I have something that just converts it into MATLAB.

07:21.000 --> 07:25.000
And I said, no Ilya, that'll take you a month to do.

07:25.000 --> 07:26.000
We've got to get on with this project.

07:26.000 --> 07:28.000
Don't get diverted by that.

07:28.000 --> 07:30.000
And Ilya said, it's okay, I did it this morning.

07:33.000 --> 07:35.000
That's quite incredible.

07:35.000 --> 07:42.000
And throughout those years, the biggest shift wasn't necessarily just the algorithms,

07:42.000 --> 07:44.000
but also the skill.

07:44.000 --> 07:49.000
How did you sort of view that skill over the years?

07:49.000 --> 07:51.000
Ilya got that intuition very early.

07:51.000 --> 07:57.000
So Ilya was always preaching that you just make it bigger and it'll work better.

07:57.000 --> 07:59.000
And I always thought that was a bit of a cop-out,

07:59.000 --> 08:01.000
that you're going to have to have new ideas too.

08:01.000 --> 08:03.000
It turns out Ilya was basically right.

08:03.000 --> 08:04.000
New ideas help.

08:04.000 --> 08:06.000
Things like transformers helped a lot.

08:06.000 --> 08:10.000
But it was really the scale of the data and the scale of the computation.

08:10.000 --> 08:15.000
And back then, we had no idea computers would get like a billion times faster.

08:15.000 --> 08:17.000
We thought maybe they'd get 100 times faster.

08:17.000 --> 08:20.000
We were trying to do things by coming up with clever ideas

08:20.000 --> 08:24.000
that would have just solved themselves if we had had bigger scale of the data and computation.

08:24.000 --> 08:29.000
In about 2011, Ilya and another graduate student called James Martins and I

08:29.000 --> 08:33.000
had a paper using character level prediction.

08:33.000 --> 08:38.000
So we took Wikipedia and we tried to predict the next HTML character.

08:38.000 --> 08:40.000
And that worked remarkably well.

08:40.000 --> 08:43.000
And we were always amazed at how well it worked.

08:43.000 --> 08:47.000
And that was using a fancy optimizer on GPUs.

08:47.000 --> 08:51.000
And we could never quite believe that it understood anything,

08:51.000 --> 08:53.000
but it looked as though it understood.

08:53.000 --> 08:56.000
And that just seemed incredible.

08:56.000 --> 09:02.000
Can you take us through how are these models trained to predict the next word

09:02.000 --> 09:07.000
and why is it the wrong way of thinking about them?

09:07.000 --> 09:10.000
Okay, I don't actually believe it is the wrong way.

09:10.000 --> 09:15.000
So, in fact, I think I made the first neural net language model

09:15.000 --> 09:17.000
that used embeddings and backpropagation.

09:17.000 --> 09:20.000
So it's very simple data, just triples.

09:20.000 --> 09:24.000
And it was turning each symbol into an embedding,

09:24.000 --> 09:29.000
then having the embeddings interact to predict the embedding of the next symbol

09:29.000 --> 09:31.000
and then from that predict the next symbol.

09:31.000 --> 09:35.000
And then it was backpropagating through that whole process to learn these triples.

09:35.000 --> 09:37.000
And I showed it could generalize.

09:38.000 --> 09:41.000
About 10 years later, Yoshio Benji used a very similar network

09:41.000 --> 09:43.000
and showed it worked with real text.

09:43.000 --> 09:47.000
And about 10 years after that, Linguist started believing in embeddings.

09:47.000 --> 09:48.000
It was a slow process.

09:48.000 --> 09:52.000
The reason I think it's not just predicting the next symbol

09:52.000 --> 09:55.000
is if you ask, well, what does it take to predict the next symbol?

09:55.000 --> 10:03.000
Particularly if you ask me a question and then the first word of the answer is the next symbol,

10:03.000 --> 10:06.000
you have to understand the question.

10:06.000 --> 10:12.000
So I think by predicting the next symbol, it's very unlike old-fashioned autocomplete.

10:12.000 --> 10:15.000
For old-fashioned autocomplete, you'd store sort of triples of words.

10:15.000 --> 10:20.000
And then if you saw a pair of words, you see how often different words came third

10:20.000 --> 10:22.000
and that way you could predict the next symbol.

10:22.000 --> 10:25.000
And that's what most people think autocomplete is like.

10:25.000 --> 10:27.000
It's no longer at all like that.

10:27.000 --> 10:30.000
To predict the next symbol, you have to understand what's been said.

10:30.000 --> 10:34.000
So I think you're forcing it to understand by making it predict the next symbol.

10:34.000 --> 10:37.000
And I think it's understanding in much the same way we are.

10:37.000 --> 10:41.000
So a lot of people will tell you these things aren't like us.

10:41.000 --> 10:43.000
They're just predicting the next symbol.

10:43.000 --> 10:45.000
They're not reasoning like us.

10:45.000 --> 10:49.000
But actually, in order to predict the next symbol, it's going to have to do some reasoning.

10:49.000 --> 10:54.000
And we've seen now that if you make big ones, without putting in any special stuff to do reasoning,

10:54.000 --> 10:56.000
they can already do some reasoning.

10:56.000 --> 10:59.000
And I think as you make them bigger, they're going to be able to do more and more reasoning.

10:59.000 --> 11:03.000
Do you think I'm doing anything else than predicting the next symbol right now?

11:03.000 --> 11:05.000
I think that's how you're learning.

11:05.000 --> 11:08.000
I think you're predicting the next video frame.

11:08.000 --> 11:11.000
You're predicting the next sound.

11:11.000 --> 11:15.000
But I think that's a pretty plausible theory of how the brain's learning.

11:15.000 --> 11:21.000
What enables these models to learn such a wide variety of fields?

11:21.000 --> 11:25.000
What these big language models are doing is they're looking for common structure.

11:25.000 --> 11:30.000
And by finding common structure, they can encode things using the common structure and that's more efficient.

11:30.000 --> 11:32.000
So let me give you an example.

11:32.000 --> 11:37.000
If you ask GPT-4, why is a compost heap like an atom bomb?

11:37.000 --> 11:39.000
Most people can't answer that.

11:39.000 --> 11:43.000
Most people haven't thought they think atom bombs and compost heap are very different things.

11:43.000 --> 11:49.000
But GPT-4 will tell you, well, the energy scales are very different and the time scales are very different.

11:49.000 --> 11:54.000
But the thing that's the same is that when the compost heap gets hotter, it generates heat faster.

11:54.000 --> 11:59.000
And when the atom bomb produces more neutrons, it produces more neutrons faster.

12:00.000 --> 12:03.000
And so it gets the idea of a chain reaction.

12:03.000 --> 12:10.000
And I believe it's understood that both forms of chain reaction is using that understanding to compress all that information into its weights.

12:10.000 --> 12:18.000
And if it's doing that, then it's going to be doing that for hundreds of things where we haven't seen the analogies yet, but it has.

12:18.000 --> 12:23.000
And that's where you get creativity from, from seeing these analogies between apparently very different things.

12:23.000 --> 12:27.000
And so I think GPT-4 is going to end up, when it gets bigger, being very creative.

12:27.000 --> 12:35.000
I think this idea that it's just regurgitating what it's learned, just pastishing together text it's learned already, that's completely wrong.

12:35.000 --> 12:38.000
It's going to be even more creative than people, I think.

12:38.000 --> 12:47.000
You'd argue that it won't just repeat the human knowledge we've developed so far, but could also progress beyond that.

12:47.000 --> 12:50.000
I think that's something we haven't quite seen yet.

12:50.000 --> 12:53.000
We've started seeing some examples of it.

12:53.000 --> 12:58.000
But to a large extent, we're sort of still at the current level of science.

12:58.000 --> 13:00.000
What do you think will enable it to go beyond that?

13:00.000 --> 13:03.000
Well, we've seen that in more limited contexts.

13:03.000 --> 13:14.000
Like if you take AlphaGo, in that famous competition with Lysidol, there was Move 37, where AlphaGo made a move that all the experts said must have been a mistake.

13:14.000 --> 13:17.000
But actually later they realized it was a brilliant move.

13:17.000 --> 13:20.000
So that was creative within that limited domain.

13:21.000 --> 13:24.000
I think we'll see a lot more of that as these things get bigger.

13:24.000 --> 13:35.000
The difference with AlphaGo as well was that it was using reinforcement learning, that that subsequently sort of enabled it to go beyond the current state.

13:35.000 --> 13:42.000
So it started with imitation learning, watching how humans play the game, and then it would, through self-play, develop way beyond that.

13:42.000 --> 13:45.000
Do you think that's the missing component of the current evidence?

13:45.000 --> 13:48.000
I think that may well be a missing component, yes.

13:48.000 --> 13:56.000
That the self-play in AlphaGo and AlphaZero are a large part of why it could make these creative moves.

13:56.000 --> 13:59.000
But I don't think it's entirely necessary.

13:59.000 --> 14:06.000
So there's a little experiment I did a long time ago where you're training on your own net to recognize 100 digits.

14:06.000 --> 14:08.000
I love that example, the MNIST example.

14:08.000 --> 14:12.000
And you give it training data where half the answers are wrong.

14:14.000 --> 14:16.000
And the question is, how well will it learn?

14:18.000 --> 14:23.000
And you make half the answers wrong once and keep them like that.

14:23.000 --> 14:29.000
So it can't average away the wrongness by just seeing the same example, but with the right answer sometimes and the wrong answer sometimes.

14:29.000 --> 14:34.000
When it sees that example, half of the examples, when it sees the example, the answer is always wrong.

14:34.000 --> 14:38.000
And so the training data has 50% error.

14:38.000 --> 14:44.000
But if you train up bank propagation, it gets down to 5% error, or less.

14:44.000 --> 14:51.000
In other words, from badly labeled data, it can get much better results.

14:51.000 --> 14:53.000
It can see that the training data is wrong.

14:53.000 --> 14:56.000
And that's how smart students can be smarter than their advisor.

14:56.000 --> 15:02.000
And their advisor tells them all this stuff, and for half of what their advisor tells them, they think, no, rubbish.

15:02.000 --> 15:06.000
And they listen to the other half, and then they end up smarter than the advisor.

15:06.000 --> 15:11.000
So these big neural nets can actually do, they can do much better than their training data.

15:11.000 --> 15:13.000
And most people don't realize that.

15:13.000 --> 15:18.000
So how do you expect these models to add reasoning into them?

15:18.000 --> 15:25.000
So I mean, one approach is you add sort of the heuristics on top of them, which a lot of the research is doing now,

15:25.000 --> 15:30.000
where you have sort of chain of thought, you just feedback its reasoning into itself.

15:30.000 --> 15:35.000
And another way would be in the model itself, as you scale it up.

15:35.000 --> 15:37.000
What's your intuition around that?

15:37.000 --> 15:42.000
So my intuition is that as we scale up these models, they get better at reasoning.

15:42.000 --> 15:49.000
And if you ask how people work, roughly speaking, we have these intuitions, and we can do reasoning.

15:49.000 --> 15:53.000
And we use the reasoning to correct our intuitions.

15:53.000 --> 15:56.000
Of course, we use the intuitions during the reasoning to do the reasoning.

15:56.000 --> 16:01.000
But if the conclusion of the reasoning conflicts with our intuitions, we realize the intuitions need to be changed.

16:01.000 --> 16:11.000
That's much like in AlphaGo or AlphaZero, where you have an evaluation function that just looks at the board and says, how good is that for me?

16:11.000 --> 16:19.000
But then you do the Monte Carlo rollout, and now you get a more accurate idea, and you can revise your evaluation function.

16:19.000 --> 16:23.000
So you can train it by getting it to agree with the results of reasoning.

16:23.000 --> 16:26.000
And I think these large language models have to start doing that.

16:26.000 --> 16:33.000
They have to start training their raw intuitions about what should come next by doing reasoning and realizing that's not right.

16:33.000 --> 16:39.000
And so that way they can get more training data than just mimicking what people did.

16:39.000 --> 16:43.000
And that's exactly why AlphaGo could do this creative move 37.

16:43.000 --> 16:49.000
It had much more training data because it was using reasoning to check out what the right next move should have been.

16:49.000 --> 16:52.000
And what do you think about multimodality?

16:52.000 --> 16:57.000
So we spoke about these analogies, and often the analogies are way beyond what we could see.

16:57.000 --> 17:05.000
It's discovering analogies that are far beyond humans and at maybe abstraction levels that we'll never be able to understand.

17:05.000 --> 17:13.000
Now, when we introduce images to that and video and sound, how do you think that will change the models?

17:13.000 --> 17:18.000
And how do you think it will change the analogies that it will be able to make?

17:18.000 --> 17:25.000
I think it'll change it a lot. I think it'll make it much better at understanding spatial things, for example.

17:25.000 --> 17:34.000
From language alone, it's quite hard to understand some spatial things, although remarkably GPT-4 can do that even before it was multimodal.

17:34.000 --> 17:41.000
But when you make it multimodal, if you have it both doing vision and reaching out and grabbing things,

17:41.000 --> 17:45.000
it'll understand objects much better if it can pick them up and turn them over and so on.

17:45.000 --> 17:53.000
So although you can learn an awful lot from language, it's easier to learn if you are multimodal.

17:53.000 --> 17:55.000
And in fact, you then need less language.

17:55.000 --> 18:00.000
And there's an awful lot of YouTube video for predicting the next frame or something like that.

18:00.000 --> 18:04.000
So I think these multimodal models are clearly going to take over.

18:04.000 --> 18:08.000
You can get more data that way. They need less language.

18:08.000 --> 18:13.000
So there's really a philosophical point that you could learn a very good model from language alone,

18:13.000 --> 18:16.000
but it's much easier to learn it from a multimodal system.

18:16.000 --> 18:20.000
And how do you think it will impact the model's reasoning?

18:20.000 --> 18:23.000
I think it'll make it much better at reasoning about space, for example.

18:23.000 --> 18:25.000
Reasoning about what happens if you pick objects up.

18:25.000 --> 18:29.000
If you actually try picking objects up, you're going to get all sorts of training data that's going to help.

18:29.000 --> 18:35.000
Do you think the human brain evolved to work well with language?

18:35.000 --> 18:39.000
Or do you think language evolved to work well with the human brain?

18:39.000 --> 18:42.000
I think the question of whether language evolved to work with the brain

18:42.000 --> 18:45.000
or the brain evolved to work with language, I think that's a very good question.

18:45.000 --> 18:48.000
I think both happened.

18:48.000 --> 18:53.000
I used to think we would do a lot of cognition without needing language at all.

18:53.000 --> 18:56.000
Now I've changed my mind a bit.

18:56.000 --> 19:02.000
So let me give you three different views of language and how it relates to cognition.

19:02.000 --> 19:09.000
There's the old-fashioned symbolic view, which is cognition consists of having strings of symbols

19:09.000 --> 19:13.000
in some kind of cleaned up logical language where there's no ambiguity

19:13.000 --> 19:15.000
and applying rules of inference.

19:15.000 --> 19:17.000
And that's what cognition is.

19:17.000 --> 19:22.000
It's just these symbolic manipulations on things that are like strings of language symbols.

19:22.000 --> 19:24.000
So that's one extreme view.

19:24.000 --> 19:29.000
The opposite extreme view is, no, no, once you get inside the head, it's all vectors.

19:29.000 --> 19:33.000
So symbols come in, you convert those symbols into big vectors

19:33.000 --> 19:36.000
and all the stuff inside is done with big vectors,

19:36.000 --> 19:39.000
and then if you want to produce output, you produce symbols again.

19:39.000 --> 19:43.000
So there was a point in machine translation in about 2014

19:43.000 --> 19:48.000
when people were using recurrent neural nets and words would keep coming in

19:48.000 --> 19:53.000
and they'd have a hidden state and they'd keep accumulating information in this hidden state.

19:53.000 --> 19:57.000
And then they got to the end of a sentence that have a big hidden vector

19:57.000 --> 19:59.000
that captured the meaning of that sentence

19:59.000 --> 20:02.000
that could then be used for producing the sentences in another language.

20:02.000 --> 20:04.000
That was called a thought vector.

20:04.000 --> 20:06.000
And that's the sort of second view of language.

20:06.000 --> 20:10.000
You convert the language into a big vector that's nothing like language

20:10.000 --> 20:12.000
and that's what cognition is all about.

20:12.000 --> 20:15.000
But then there's a third view, which is what I believe now,

20:15.000 --> 20:21.000
which is that you take these symbols

20:21.000 --> 20:25.000
and you convert the symbols into embeddings and you use multiple layers of that

20:25.000 --> 20:27.000
so you get these very rich embeddings.

20:27.000 --> 20:29.000
But the embeddings are still tied to the symbols

20:29.000 --> 20:33.000
in the sense that you've got a big vector for this symbol and a big vector for that symbol

20:33.000 --> 20:38.000
and these vectors interact to produce the vector for the symbol for the next word.

20:38.000 --> 20:40.000
And that's what understanding is.

20:40.000 --> 20:44.000
Understanding is knowing how to convert the symbols into these vectors

20:44.000 --> 20:48.000
and knowing how the elements of the vectors should interact to predict the vector for the next symbol.

20:48.000 --> 20:52.000
That's what understanding is, both in these big language models and in our brains.

20:52.000 --> 20:56.000
And that's an example which is sort of in between.

20:56.000 --> 21:01.000
You're staying with the symbols, but you're interpreting them as these big vectors

21:01.000 --> 21:03.000
and that's where all the work is.

21:03.000 --> 21:06.000
And all the knowledge is in what vectors you use

21:06.000 --> 21:10.000
and how the elements of those vectors interact, not in symbolic rules.

21:10.000 --> 21:14.000
But it's not saying that you get away from the symbols altogether.

21:14.000 --> 21:17.000
It's saying you turn the symbols into big vectors

21:17.000 --> 21:20.000
but you stay with that surface structure of the symbols.

21:20.000 --> 21:22.000
And that's how these models are working.

21:22.000 --> 21:25.000
And that's, and I seem to be, a more plausible model of human thought too.

21:25.000 --> 21:31.000
You were one of the first folks to get the idea of using GPUs.

21:31.000 --> 21:34.000
And I know Jensen loves you for that.

21:34.000 --> 21:39.000
Back in 2009 you mentioned that, you told Jensen that this could be a quite good idea

21:39.000 --> 21:42.000
for training neural nets.

21:42.000 --> 21:48.000
Take us back to that early intuition of using GPUs for training neural nets.

21:48.000 --> 21:54.000
So actually I think in about 2006 I had a former graduate student called Rick Zeliski.

21:54.000 --> 21:56.000
He's a very good computer vision guy.

21:56.000 --> 21:59.000
And I talked to him at a meeting.

21:59.000 --> 22:03.000
He said, you know, you ought to think about using graphics processing cards

22:03.000 --> 22:05.000
because they're very good at matrix multiplies.

22:05.000 --> 22:08.000
And what you're doing is basically all matrix multiplies.

22:08.000 --> 22:10.000
So I thought about that for a bit.

22:10.000 --> 22:16.000
And then we learned about these Tesla systems that had four GPUs in.

22:16.000 --> 22:24.000
And initially we just got gaming GPUs and discovered they made things go 30 times faster.

22:24.000 --> 22:27.000
And then we bought one of these Tesla systems with four GPUs.

22:27.000 --> 22:31.000
And we did speech on that and it worked very well.

22:31.000 --> 22:34.000
And then in 2009 I gave a talk at NIPS.

22:34.000 --> 22:37.000
And I told a thousand machine learning researchers

22:37.000 --> 22:39.000
that you should all go and buy Nvidia GPUs.

22:39.000 --> 22:40.000
They're the future.

22:40.000 --> 22:42.000
You need them for doing machine learning.

22:42.000 --> 22:46.000
And I actually then sent mail to Nvidia saying,

22:46.000 --> 22:48.000
I told a thousand machine learning researchers to buy your boards.

22:48.000 --> 22:49.000
Could you give me a free one?

22:49.000 --> 22:50.000
And they said no.

22:50.000 --> 22:51.000
Actually they didn't say no.

22:51.000 --> 22:53.000
They just didn't reply.

22:53.000 --> 22:58.000
But when I told Jensen this story later on, he gave me a free one.

22:58.000 --> 23:00.000
That's very, very good.

23:00.000 --> 23:07.000
I think what's interesting as well is sort of how GPUs has evolved alongside the field.

23:07.000 --> 23:12.000
So where do you think we should go next in the computer?

23:12.000 --> 23:18.000
So my last couple of years at Google I was thinking about ways of trying to make analog computation.

23:18.000 --> 23:23.000
So instead of using like a megawatt we could use like 30 watts like the brain.

23:23.000 --> 23:27.000
And we could run these big language models in analog hardware.

23:27.000 --> 23:30.000
And I never made it work.

23:30.000 --> 23:35.000
But I started really appreciating digital computation.

23:35.000 --> 23:40.000
So if you're going to use that low power analog computation,

23:40.000 --> 23:43.000
every piece of hardware is going to be a bit different.

23:43.000 --> 23:48.000
And the idea is the learning is going to make use of the specific properties of that hardware.

23:48.000 --> 23:49.000
And that's what happens with people.

23:49.000 --> 23:51.000
All our brains are different.

23:51.000 --> 23:56.000
So we can't then take the weights in your brain and put them in my brain.

23:56.000 --> 23:57.000
The hardware is different.

23:57.000 --> 24:00.000
The precise properties of the individual neurons are different.

24:00.000 --> 24:03.000
The learning has learned to make use of all that.

24:03.000 --> 24:08.000
And so we're mortal in the sense that the weights in my brain are no good for any other brain.

24:08.000 --> 24:10.000
When I die those weights are useless.

24:10.000 --> 24:17.000
We can get information from one to another rather inefficiently by I produce sentences

24:17.000 --> 24:20.000
and you figure out how to change your weight so you would have said the same thing.

24:20.000 --> 24:22.000
That's called distillation.

24:22.000 --> 24:25.000
But that's a very inefficient way of communicating knowledge.

24:25.000 --> 24:30.000
And with digital systems they're immortal because once you've got some weights

24:30.000 --> 24:34.000
you can throw away the computer, just store the weights on a tape somewhere

24:34.000 --> 24:37.000
and now build another computer, put those same weights in.

24:37.000 --> 24:41.000
And if it's digital it can compute exactly the same thing as the other system did.

24:41.000 --> 24:44.000
So digital systems can share weights.

24:44.000 --> 24:47.000
And that's incredibly much more efficient.

24:47.000 --> 24:53.000
If you've got a whole bunch of digital systems and they each go into a tiny bit of learning

24:53.000 --> 24:56.000
and they start with the same weights, they do a tiny bit of learning

24:56.000 --> 25:00.000
and then they share their weights again, they all know what all the others learned.

25:00.000 --> 25:02.000
We can't do that.

25:02.000 --> 25:05.000
And so they're far superior to us in being able to share knowledge.

25:05.000 --> 25:11.000
A lot of the ideas that have been deployed in the field are very old school ideas.

25:11.000 --> 25:16.000
It's the ideas that have been around in neuroscience for forever.

25:16.000 --> 25:20.000
What do you think is sort of left to apply to the systems that we develop?

25:20.000 --> 25:27.000
So one big thing that we still have to catch up with neuroscience on

25:27.000 --> 25:30.000
is the time scales for changes.

25:30.000 --> 25:36.000
So in nearly all the neural nets there's a fast time scale for changing activities.

25:36.000 --> 25:40.000
So input comes in, the activities, the embedding vectors all change.

25:40.000 --> 25:43.000
And then there's a slow time scale which is changing the weights.

25:43.000 --> 25:45.000
And that's long-term learning.

25:45.000 --> 25:47.000
And you just have those two time scales.

25:47.000 --> 25:51.000
In the brain there's many time scales at which weights change.

25:51.000 --> 25:55.000
So for example, if I say an unexpected word like cucumber,

25:55.000 --> 26:00.000
and now five minutes later you put headphones on, there's a lot of noise

26:00.000 --> 26:05.000
and there's very faint words, you'll be much better at recognising the word cucumber

26:05.000 --> 26:07.000
because I said it five minutes ago.

26:07.000 --> 26:10.000
So where is that knowledge in the brain?

26:10.000 --> 26:13.000
And that knowledge is obviously in temporary changes to synapses.

26:13.000 --> 26:16.000
It's not neurons that go in cucumber, cucumber, cucumber,

26:16.000 --> 26:18.000
you don't have enough neurons for that.

26:18.000 --> 26:21.000
It's in temporary changes to the weights.

26:21.000 --> 26:24.000
And you can do a lot of things with temporary weight changes,

26:24.000 --> 26:26.000
what I call fast weights.

26:26.000 --> 26:28.000
We don't do that in these neural models.

26:28.000 --> 26:33.000
And the reason we don't do it is because if you have temporary changes to the weights

26:33.000 --> 26:35.000
that depend on the input data,

26:35.000 --> 26:40.000
then you can't process a whole bunch of different cases at the same time.

26:40.000 --> 26:43.000
At present we take a whole bunch of different strings,

26:43.000 --> 26:47.000
we stack them together and we process them all in parallel

26:47.000 --> 26:51.000
because then we can do matrix, matrix, multiplies, which is much more efficient.

26:51.000 --> 26:55.000
And just that efficiency is stopping us using fast weights.

26:55.000 --> 26:59.000
But the brain clearly uses fast weights for temporary memory.

26:59.000 --> 27:02.000
And there's all sorts of things you can do that way that we don't do at present.

27:02.000 --> 27:04.000
I think that's one of the biggest things we have to do.

27:04.000 --> 27:07.000
I was very hopeful that things like Graphcore,

27:07.000 --> 27:11.000
if they went sequential and did just online learning,

27:11.000 --> 27:13.000
then they could use fast weights.

27:13.000 --> 27:16.000
But that hasn't worked out yet.

27:16.000 --> 27:20.000
I think it'll work out eventually when people are using conductances for weights.

27:20.000 --> 27:24.000
How has knowing how these models work

27:24.000 --> 27:29.000
and knowing how the brain works impacted the way you think?

27:29.000 --> 27:35.000
I think there's been one big impact, which is at a fairly abstract level,

27:35.000 --> 27:40.000
which is that for many years people were very scornful

27:40.000 --> 27:43.000
about the idea of having a big random neural net

27:43.000 --> 27:45.000
and just giving it a lot of training data

27:45.000 --> 27:47.000
and it would learn to do complicated things.

27:47.000 --> 27:51.000
If you talk to statisticians or linguists or most people in AI,

27:51.000 --> 27:53.000
they say, that's just a pipe dream.

27:53.000 --> 27:56.000
There's no way you're going to learn to really complicated things

27:56.000 --> 27:58.000
without some kind of innate knowledge,

27:58.000 --> 28:00.000
without a lot of architectural restrictions.

28:00.000 --> 28:02.000
It turns out that's completely wrong.

28:02.000 --> 28:04.000
You can take a big random neural network

28:04.000 --> 28:07.000
and you can learn a whole bunch of stuff just from data.

28:08.000 --> 28:11.000
So the idea that stochastic gradient descent

28:11.000 --> 28:15.000
to repeatedly adjust the weights using a gradient,

28:15.000 --> 28:19.000
that will learn things and will learn big complicated things,

28:19.000 --> 28:22.000
that's been validated by these big models.

28:22.000 --> 28:25.000
And that's a very important thing to know about the brain.

28:25.000 --> 28:28.000
It doesn't have to have all this innate structure.

28:28.000 --> 28:30.000
Now, obviously it's got a lot of innate structure,

28:30.000 --> 28:35.000
but it certainly doesn't need an innate structure for things that are easily learned.

28:36.000 --> 28:38.000
And so the idea coming from Chomsky,

28:38.000 --> 28:41.000
that you won't learn anything complicated like language

28:41.000 --> 28:45.000
unless it's all kind of wired in already and just matures,

28:45.000 --> 28:48.000
that idea is now clearly nonsense.

28:48.000 --> 28:52.000
I'm sure Chomsky would appreciate you calling his ideas nonsense.

28:52.000 --> 28:57.000
Well, I think a lot of Chomsky's political ideas are very sensible.

28:57.000 --> 29:01.000
I'm always struck by how come someone with such sensible ideas about the Middle East

29:01.000 --> 29:03.000
could be so wrong about linguistics.

29:04.000 --> 29:07.000
What do you think would make these models

29:07.000 --> 29:11.000
simulate consciousness of humans more effectively?

29:11.000 --> 29:16.000
But imagine you had the AI assistant that you've spoken to in your entire life,

29:16.000 --> 29:20.000
and instead of that being like Chatipiti today,

29:20.000 --> 29:24.000
that sort of deletes the memory of the conversation and you start fresh all of the time,

29:24.000 --> 29:27.000
it had self-reflection.

29:27.000 --> 29:32.000
At some point you pass away and you tell that to the assistant.

29:33.000 --> 29:36.000
I mean, not me, somebody else tells that to the assistant.

29:36.000 --> 29:41.000
Yeah, it would be difficult for you to tell that to the assistant.

29:41.000 --> 29:45.000
Do you think that assistant would feel at that point?

29:45.000 --> 29:47.000
Yes, I think they can have feelings too.

29:47.000 --> 29:51.000
So I think just as we have this inner theater model for perception,

29:51.000 --> 29:53.000
we have an inner theater model for feelings,

29:53.000 --> 29:57.000
there are things that I can experience but other people can't.

29:59.000 --> 30:01.000
I think that model is equally wrong.

30:02.000 --> 30:07.000
Suppose I say, I feel like punching Gary on the nose, which I often do.

30:07.000 --> 30:11.000
Let's try and abstract that away from the idea of an inner theater.

30:11.000 --> 30:14.000
What I'm really saying to you is,

30:14.000 --> 30:18.000
if it weren't for the inhibition coming from my frontal lobes,

30:18.000 --> 30:20.000
I would perform an action.

30:20.000 --> 30:25.000
So when we talk about feelings, we're really talking about actions we would perform

30:25.000 --> 30:29.000
if it weren't for constraints.

30:29.000 --> 30:31.000
And that's really what feelings are.

30:31.000 --> 30:35.000
The actions we would do if it weren't for constraints.

30:35.000 --> 30:38.000
So I think you can give the same kind of explanation for feelings

30:38.000 --> 30:40.000
and there's no reason why these things can't have feelings.

30:40.000 --> 30:46.000
In fact, in 1973, I saw a robot have an emotion.

30:46.000 --> 30:50.000
So in Edinburgh, they had a robot with two grippers like this

30:50.000 --> 30:58.000
that could assemble a toy car if you put the pieces separately on a piece of green felt.

30:58.000 --> 31:00.000
But if you put them in a pile,

31:00.000 --> 31:03.000
its vision wasn't good enough to figure out what was going on.

31:03.000 --> 31:05.000
So it put its grip on it and it went whack!

31:05.000 --> 31:08.000
And it knocked them so they were scattered and then it coupled them together.

31:08.000 --> 31:11.000
If you saw that in a person, you'd say it was crossed with the situation

31:11.000 --> 31:14.000
because it didn't understand it so it destroyed it.

31:14.000 --> 31:17.000
That's profound.

31:17.000 --> 31:24.000
We spoke previously, you described humans and the LLMs as analogy machines.

31:24.000 --> 31:31.000
What do you think has been the most powerful analogies that you've found throughout your life?

31:31.000 --> 31:42.000
Throughout my life, I guess probably a sort of weak analogy that has influenced me a lot

31:42.000 --> 31:50.000
is the analogy between religious belief and between belief and symbol processing.

31:51.000 --> 31:55.000
So when I was very young, I came from an atheist family

31:55.000 --> 31:58.000
and went to school and was confronted with religious belief.

31:58.000 --> 32:00.000
And it just seemed nonsense to me.

32:00.000 --> 32:02.000
It still seems nonsense to me.

32:02.000 --> 32:06.000
And when I saw symbol processing as an explanation of how people worked,

32:06.000 --> 32:09.000
I thought it was just the same.

32:09.000 --> 32:11.000
Nonsense.

32:11.000 --> 32:14.000
I don't think it's quite so much nonsense now

32:14.000 --> 32:17.000
because I think actually we do do symbol processing.

32:17.000 --> 32:21.000
It's just we do it by giving these big embedding vectors to the symbols.

32:21.000 --> 32:23.000
But we are actually symbol processing.

32:23.000 --> 32:27.000
But not at all in the way people thought where you match symbols

32:27.000 --> 32:31.000
and the only thing a symbol has is it's identical to another symbol or it's not identical.

32:31.000 --> 32:33.000
That's the only property a symbol has.

32:33.000 --> 32:35.000
We don't do that at all.

32:35.000 --> 32:37.000
We use the context to give embedding vectors to symbols

32:37.000 --> 32:42.000
and then use the interactions between the components of these embedding vectors to do thinking.

32:43.000 --> 32:47.000
But there's a very good researcher at Google called Fernando Pereira

32:47.000 --> 32:51.000
who said, yes, we do have symbolic reasoning

32:51.000 --> 32:53.000
and the only symbolic we have is natural language.

32:53.000 --> 32:56.000
Natural language is a symbolic language and we reason with it.

32:56.000 --> 32:58.000
I believe that now.

32:58.000 --> 33:03.000
You've done some of the most meaningful research in the history of computer science.

33:03.000 --> 33:08.000
Can you walk us through like how do you select the right problems to work on?

33:08.000 --> 33:10.000
Well, first let me correct you.

33:10.000 --> 33:14.000
Me and my students have done a lot of the most meaningful things

33:14.000 --> 33:17.000
and it's mainly been a very good collaboration with students

33:17.000 --> 33:20.000
and my ability to select very good students.

33:20.000 --> 33:24.000
And that came from the fact there were very few people doing neural nets

33:24.000 --> 33:27.000
in the 70s and 80s and 90s and 2000s.

33:27.000 --> 33:31.000
And so the few people doing neural nets got to pick the very best students.

33:31.000 --> 33:33.000
So that was a piece of luck.

33:33.000 --> 33:37.000
But my way of selecting problems is basically, well,

33:37.000 --> 33:39.000
when scientists talk about how they work,

33:39.000 --> 33:41.000
they have theories about how they work

33:41.000 --> 33:43.000
which probably don't have much to do with the truth.

33:43.000 --> 33:49.000
But my theory is that I look for something where everybody's agreed about something

33:49.000 --> 33:51.000
and it feels wrong.

33:51.000 --> 33:54.000
Just there's a slight intuition of something wrong about it.

33:54.000 --> 33:58.000
And then I work on that and see if I can elaborate why it is I think it's wrong.

33:58.000 --> 34:02.000
And maybe I can make a little demo with a small computer program

34:02.000 --> 34:06.000
that shows that it doesn't work the way you might expect.

34:06.000 --> 34:08.000
So let me take one example.

34:08.000 --> 34:12.000
Most people think that if you add noise to a neural net, it's going to work worse.

34:12.000 --> 34:17.000
If, for example, each time you put a training example through,

34:17.000 --> 34:23.000
you make half of the neurons be silent, it'll work worse.

34:23.000 --> 34:28.000
Actually, we know it'll generalize better if you do that.

34:28.000 --> 34:33.000
And you can demonstrate that in a simple example.

34:33.000 --> 34:35.000
That's what's nice about computer simulation.

34:35.000 --> 34:39.000
You can show this idea you had that adding noise is going to make it worse

34:39.000 --> 34:42.000
and dropping out half the neurons will make it work worse,

34:42.000 --> 34:44.000
which you will in the short term.

34:44.000 --> 34:47.000
But if you train it like that, in the end it'll work better.

34:47.000 --> 34:49.000
You can demonstrate that with a small computer program

34:49.000 --> 34:51.000
and then you can think hard about why that is

34:51.000 --> 34:56.000
and how it stops big, elaborate co-adaptations.

34:56.000 --> 34:59.000
But I think that that's my method of working.

34:59.000 --> 35:02.000
Find something that sounds suspicious and work on it

35:02.000 --> 35:06.000
and see if you can give a simple demonstration of why it's wrong.

35:06.000 --> 35:08.000
What sounds suspicious to you now?

35:08.000 --> 35:11.000
Well, that we don't use fast weight sounds suspicious.

35:11.000 --> 35:13.000
That we only have these two timescales.

35:13.000 --> 35:16.000
That's just wrong. That's not at all like the brain.

35:16.000 --> 35:20.000
And in the long run, I think we're going to have to have many more timescales.

35:20.000 --> 35:21.000
So that's an example now.

35:21.000 --> 35:26.000
And if you had your group of students today and they came to you

35:26.000 --> 35:29.000
and they said the hamming question that we talked about previously,

35:29.000 --> 35:32.000
what's the most important problem in your field?

35:32.000 --> 35:36.000
What would you suggest that they take on and work on next?

35:36.000 --> 35:38.000
We spoke about reasoning, timescales.

35:38.000 --> 35:42.000
What would be sort of the highest priority problem that you'd give them?

35:42.000 --> 35:47.000
For me right now, it's the same question I've had for the last like 30 years or so,

35:47.000 --> 35:51.000
which is, does the brain do back propagation?

35:51.000 --> 35:53.000
I believe the brain is getting gradients.

35:53.000 --> 35:57.000
If you don't get gradients, your learning is just much worse than if you do get gradients.

35:57.000 --> 35:59.000
But how is the brain getting gradients?

35:59.000 --> 36:04.000
And is it somehow implementing some approximate version of back propagation?

36:04.000 --> 36:06.000
Or is it some completely different technique?

36:06.000 --> 36:08.000
That's a big open question.

36:08.000 --> 36:12.000
And if I kept on doing research, that's what I would be doing research on.

36:12.000 --> 36:18.000
And when you look back at your career now, you've been right about so many things,

36:18.000 --> 36:24.000
but what were you wrong about that you wish you sort of spent less time pursuing a certain direction?

36:24.000 --> 36:25.000
Okay, those are two separate questions.

36:25.000 --> 36:27.000
One is what were you wrong about?

36:27.000 --> 36:30.000
And two, do you wish you'd spent less time on it?

36:30.000 --> 36:35.000
I think I was wrong about Boltzmann machines, and I'm glad I spent a long time on it.

36:35.000 --> 36:39.000
There are much more beautiful theory of how you get gradients than back propagation.

36:39.000 --> 36:42.000
Back propagation is just ordinary and sensible, and it's just a chain rule.

36:42.000 --> 36:46.000
Boltzmann machines is clever, and it's a very interesting way to get gradients.

36:46.000 --> 36:51.000
And I would love for that to be how the brain works, but I think it isn't.

36:51.000 --> 36:57.000
Did you spend much time imagining what would happen post these systems developing as well?

36:57.000 --> 37:01.000
Did you ever have an idea that, okay, if we could make these systems work really well,

37:01.000 --> 37:06.000
we could democratise education, we could make knowledge way more accessible,

37:06.000 --> 37:10.000
we could solve some tough problems in medicine,

37:10.000 --> 37:14.000
or was it more to you about understanding the brain?

37:14.000 --> 37:20.000
Yes, I sort of feel scientists ought to be doing things that are going to help society,

37:20.000 --> 37:23.000
but actually, that's not how you do your best research.

37:23.000 --> 37:26.000
You do your best research when it's driven by curiosity.

37:26.000 --> 37:30.000
You just have to understand something.

37:30.000 --> 37:35.000
Much more recently, I've realised these things could do a lot of harm as well as a lot of good,

37:35.000 --> 37:39.000
and I've become much more concerned about the effects they're going to have on society.

37:39.000 --> 37:41.000
But that's not what was motivating me.

37:41.000 --> 37:45.000
I just wanted to understand how on earth can the brain learn to do things?

37:45.000 --> 37:46.000
That's what I want to know.

37:46.000 --> 37:47.000
And I sort of failed.

37:47.000 --> 37:52.000
As a side effect of that failure, we got some nice engineering.

37:52.000 --> 37:55.000
Yeah, it was a good failure for the world.

37:55.000 --> 37:59.000
If you take the lens of the things that could go really right,

37:59.000 --> 38:03.000
what do you think are the most promising applications?

38:03.000 --> 38:08.000
I think healthcare is clearly a big one.

38:08.000 --> 38:14.000
With healthcare, there's almost no end to how much healthcare society can absorb.

38:14.000 --> 38:19.000
If you take someone old, they could use five doctors full time.

38:19.000 --> 38:25.000
So when AI gets better than people are doing things,

38:25.000 --> 38:30.000
you'd like it to get better in areas where you could do with a lot more of that stuff,

38:30.000 --> 38:32.000
and we could do with a lot more doctors.

38:32.000 --> 38:35.000
If everybody had three doctors of their own, that would be great,

38:35.000 --> 38:38.000
and we're going to get to that point.

38:38.000 --> 38:41.000
So that's one reason why healthcare is good.

38:41.000 --> 38:46.000
There's also just in new engineering, developing new materials, for example,

38:46.000 --> 38:50.000
for better solar panels or for superconductivity,

38:50.000 --> 38:55.000
or for just understanding how the body works.

38:55.000 --> 38:57.000
There's going to be huge impacts there.

38:57.000 --> 38:59.000
Those are all going to be good things.

38:59.000 --> 39:03.000
What I worry about is bad actors using them for bad things.

39:03.000 --> 39:10.000
We've facilitated people like Putin or Xi or Trump using AI for killer robots

39:10.000 --> 39:13.000
for manipulating public opinion or for mass surveillance.

39:13.000 --> 39:15.000
And those are all very worrying things.

39:15.000 --> 39:22.000
Are you ever concerned that slowing down the field could also slow down the positives?

39:22.000 --> 39:23.000
Oh, absolutely.

39:23.000 --> 39:28.000
And I think there's not much chance that the field will slow down,

39:28.000 --> 39:30.000
partly because it's international,

39:30.000 --> 39:33.000
and if one country slows down, the other countries aren't going to slow down.

39:33.000 --> 39:37.000
So there's a race clearly between China and the US,

39:37.000 --> 39:39.000
and neither is going to slow down.

39:39.000 --> 39:43.000
So yeah, I mean, there was this partition saying we should slow down for six months.

39:43.000 --> 39:46.000
I didn't sign it just because I thought it was never going to happen.

39:46.000 --> 39:49.000
I maybe should have signed it because even though it was never going to happen,

39:49.000 --> 39:50.000
it made a political point.

39:50.000 --> 39:54.000
It's often good to ask for things you know you can't get just to make a point.

39:54.000 --> 39:56.000
But I don't think we're going to slow down.

39:56.000 --> 40:03.000
And how do you think that it will impact the AI research process having this assistance?

40:03.000 --> 40:05.000
I think it'll make it a lot more efficient.

40:05.000 --> 40:10.000
AI research will get a lot more efficient when you've got these assistance to help you program,

40:10.000 --> 40:15.000
but also help you think through things and probably help you a lot with equations too.

40:15.000 --> 40:19.000
Have you reflected much on the process of selecting talent?

40:19.000 --> 40:21.000
Has that been mostly intuitive to you?

40:21.000 --> 40:26.000
Like when Ilya shows up at the door, you feel this is smart guy, let's work together.

40:26.000 --> 40:30.000
So for selecting talent, sometimes you just know.

40:30.000 --> 40:34.000
So after talking to Ilya for not very long, he seemed very smart.

40:34.000 --> 40:37.000
And then talking to him a bit more, he clearly was very smart

40:37.000 --> 40:41.000
and had very good intuitions as well as being good at math.

40:41.000 --> 40:42.000
So that was a no-brainer.

40:42.000 --> 40:47.000
There's another case where I was at a NIPS conference.

40:47.000 --> 40:53.000
We had a poster and someone came up and he started asking questions about the poster.

40:53.000 --> 40:57.000
And every question he asked was a sort of deep insight into what we'd done wrong.

40:57.000 --> 41:00.000
And after five minutes, I offered him a postdoc position.

41:00.000 --> 41:05.000
That guy was David McKay, who was just brilliant and it's very sad he died,

41:05.000 --> 41:09.000
but he was very obvious you'd want him.

41:09.000 --> 41:11.000
Other times it's not so obvious.

41:11.000 --> 41:15.000
And one thing I did learn was that people are different.

41:15.000 --> 41:19.000
There's not just one type of good student.

41:19.000 --> 41:25.000
So there's some students who aren't that creative but are technically extremely strong

41:25.000 --> 41:27.000
and will make anything work.

41:27.000 --> 41:31.000
There's other students who aren't technically strong but are very creative.

41:31.000 --> 41:34.000
Of course you want the ones who are both, but you don't always get that.

41:34.000 --> 41:39.000
But I think actually in the lab you need a variety of different kinds of graduate students.

41:39.000 --> 41:44.000
But I still go with my gut intuition that sometimes you talk to somebody

41:44.000 --> 41:49.000
and they just get it and those are the ones you want.

41:49.000 --> 41:54.000
What do you think is the reason for some folks having better intuition?

41:54.000 --> 42:01.000
Do they just have better training data than others or how can you develop your intuition?

42:01.000 --> 42:05.000
I think it's partly they don't stand for nonsense.

42:05.000 --> 42:09.000
So here's a way to get bad intuitions, believe everything you're told.

42:09.000 --> 42:11.000
That's fatal.

42:11.000 --> 42:14.000
You have to be able to, I think here's what some people do.

42:14.000 --> 42:17.000
They have a whole framework for understanding reality.

42:17.000 --> 42:23.000
And when someone tells them something, they try and sort of figure out how that fits into their framework.

42:23.000 --> 42:26.000
And if it doesn't, they just reject it.

42:26.000 --> 42:29.000
And that's a very good strategy.

42:29.000 --> 42:36.000
People who try and incorporate whatever they're told end up with a framework that's sort of very fuzzy

42:36.000 --> 42:40.000
and sort of can believe everything and that's useless.

42:40.000 --> 42:47.000
So I think actually having a strong view of the world and trying to manipulate incoming facts to fit in with your view.

42:47.000 --> 42:55.000
Obviously it can lead you into deep religious belief in fatal flaws and so on, like my belief in Boltzmann machines.

42:55.000 --> 42:57.000
But I think that's the way to go.

42:57.000 --> 43:00.000
If you've got good intuitions, you should trust them.

43:00.000 --> 43:06.000
If you've got bad intuitions, it doesn't matter what you do, so you might as well trust them.

43:06.000 --> 43:09.000
Very good point.

43:09.000 --> 43:16.000
When you look at the types of research that's being done today,

43:16.000 --> 43:23.000
do you think we're putting all of our eggs in one basket and we should diversify our ideas a bit more in the field?

43:23.000 --> 43:25.000
Or do you think this is the most promising direction?

43:25.000 --> 43:28.000
So let's go all in on it.

43:28.000 --> 43:35.000
I think having big models and training them on multimodal data, even if it's only to predict the next word,

43:35.000 --> 43:38.000
is such a promising approach that we should go pretty much all in on it.

43:38.000 --> 43:41.000
Obviously there's lots and lots of people doing it now.

43:41.000 --> 43:45.000
And there's lots of people doing apparently crazy things and that's good.

43:45.000 --> 43:50.000
But I think it's fine for most of the people to be following this path because it's working very well.

43:50.000 --> 43:56.000
Do you think that the learning algorithms matter that much, or is it just a scale?

43:56.000 --> 44:02.000
Are there basically millions of ways that we could get to human level in intelligence,

44:02.000 --> 44:06.000
or are there sort of a select few that we need to discover?

44:06.000 --> 44:11.000
Yes, so this issue of whether particular learning algorithms are very important,

44:11.000 --> 44:15.000
or whether there's a great variety of learning algorithms that will do the job,

44:15.000 --> 44:17.000
I don't know the answer.

44:17.000 --> 44:22.000
It seems to me though that by propagation there's a sense in which it's the correct thing to do.

44:22.000 --> 44:26.000
Getting the gradient so that you change a parameter to make it work better,

44:26.000 --> 44:31.000
that seems like the right thing to do, and it's been amazingly successful.

44:31.000 --> 44:36.000
There may well be other learning algorithms that are alternative ways of getting that same gradient,

44:36.000 --> 44:41.000
all that are getting the gradient to something else and that also work.

44:41.000 --> 44:48.000
I think that's all open and a very interesting issue now about whether there's other things you can try and maximize

44:48.000 --> 44:53.000
that will give you good systems, and maybe the brain's doing that because it's easier.

44:53.000 --> 45:00.000
But backprop is in a sense the right thing to do, and we know that doing it works really well.

45:00.000 --> 45:05.000
And one last question, when you look back at your decades of research,

45:05.000 --> 45:08.000
what are you most proud of? Is it the students? Is it the research?

45:08.000 --> 45:12.000
What makes you most proud of when you look back at your life's work?

45:12.000 --> 45:15.000
The learning algorithm for Boltzmann machines.

45:15.000 --> 45:19.000
So the learning algorithm for Boltzmann machines is beautifully elegant.

45:19.000 --> 45:27.000
It's maybe hopeless in practice, but it's the thing I enjoyed most developing that with Terry,

45:27.000 --> 45:32.000
and it's what I'm proudest of, even if it's wrong.

45:37.000 --> 45:41.000
What questions do you spend most of your time thinking about now?

45:41.000 --> 45:45.000
What should I watch on Netflix?

