1
00:00:00,000 --> 00:00:08,000
Have you reflected a lot on how to select talent or has that mostly been intuitive to you?

2
00:00:08,000 --> 00:00:12,400
Ilya just shows up and you're like, this is a clever guy, let's work together.

3
00:00:12,400 --> 00:00:14,600
Or have you thought a lot about that?

4
00:00:14,600 --> 00:00:17,600
Should we roll this?

5
00:00:17,600 --> 00:00:18,600
Yeah, let's roll this.

6
00:00:18,600 --> 00:00:20,000
We're good, yeah, yeah.

7
00:00:20,000 --> 00:00:21,600
You're juggling into a repetition.

8
00:00:21,600 --> 00:00:22,600
Okay.

9
00:00:25,000 --> 00:00:26,000
Sun is working.

10
00:00:30,000 --> 00:00:34,000
So I remember when I first got to Carnegie Mellon from England.

11
00:00:34,000 --> 00:00:39,000
In England, at a research unit, it would get to be six o'clock and you'd all go for a drink in the pub.

12
00:00:39,000 --> 00:00:44,000
At Carnegie Mellon, I remember after I'd been there a few weeks, it was Saturday night.

13
00:00:44,000 --> 00:00:47,000
I didn't have any friends yet and I didn't know what to do.

14
00:00:47,000 --> 00:00:52,000
So I decided I'd go into the lab and do some programming because I had a list machine and you couldn't program it from home.

15
00:00:52,000 --> 00:00:57,000
So I went into the lab at about nine o'clock on a Saturday night and it was swarming.

16
00:00:57,000 --> 00:00:59,000
All the students were there.

17
00:00:59,000 --> 00:01:02,000
And they were all there because what they were working on was the future.

18
00:01:02,000 --> 00:01:07,000
They all believed that what they did next was going to change the course of computer science.

19
00:01:07,000 --> 00:01:09,000
And it was just so different from England.

20
00:01:09,000 --> 00:01:12,000
And so that was very refreshing.

21
00:01:12,000 --> 00:01:18,000
Take me back to the very beginning, Geoff, at Cambridge, trying to understand the brain.

22
00:01:18,000 --> 00:01:20,000
What was that like?

23
00:01:20,000 --> 00:01:22,000
It was very disappointing.

24
00:01:22,000 --> 00:01:27,000
So I did physiology and in the summer term, they were going to teach us how the brain worked.

25
00:01:27,000 --> 00:01:34,000
And all they taught us was how neurons conduct action potentials, which is very interesting, but it doesn't tell you how the brain works.

26
00:01:34,000 --> 00:01:36,000
So that was extremely disappointing.

27
00:01:36,000 --> 00:01:38,000
I switched to philosophy then.

28
00:01:38,000 --> 00:01:40,000
I thought maybe they'd tell us how the mind worked.

29
00:01:40,000 --> 00:01:42,000
That was very disappointing.

30
00:01:42,000 --> 00:01:45,000
I eventually ended up going to Edinburgh to do AI.

31
00:01:45,000 --> 00:01:46,000
And that was more interesting.

32
00:01:46,000 --> 00:01:50,000
At least you could simulate things so you could test out theories.

33
00:01:50,000 --> 00:01:53,000
And did you remember what intrigued you about AI?

34
00:01:53,000 --> 00:01:55,000
Was it a paper?

35
00:01:55,000 --> 00:01:59,000
Was it any particular person that exposed you to those ideas?

36
00:01:59,000 --> 00:02:04,000
I guess it was a book I read by Donald Hebb that influenced me a lot.

37
00:02:04,000 --> 00:02:09,000
He was very interested in how you learn the connection strengths in neural nets.

38
00:02:09,000 --> 00:02:19,000
I also read a book by John von Neumann early on, who was very interested in how the brain computes and how it's different from normal computers.

39
00:02:19,000 --> 00:02:25,000
And did you get that conviction that these ideas would work out at that point?

40
00:02:25,000 --> 00:02:29,000
Or what was your intuition back at the Edinburgh days?

41
00:02:29,000 --> 00:02:34,000
It seemed to me there has to be a way that the brain learns.

42
00:02:34,000 --> 00:02:41,000
And it's clearly not by having all sorts of things programmed into it and then using logical rules of inference.

43
00:02:41,000 --> 00:02:45,000
That just seemed to me crazy from the outset.

44
00:02:45,000 --> 00:02:53,000
So we had to figure out how the brain learned to modify connections in a neural net so that it could do complicated things.

45
00:02:53,000 --> 00:02:56,000
And von Neumann believed that, Turing believed that.

46
00:02:56,000 --> 00:03:01,000
So von Neumann and Turing were both pretty good at logic, but they didn't believe in this logical approach.

47
00:03:01,000 --> 00:03:11,000
And what was your split between studying the ideas from neuroscience and just doing what seemed to be good algorithms for AI?

48
00:03:11,000 --> 00:03:14,000
How much inspiration did you take early on?

49
00:03:14,000 --> 00:03:16,000
So I never did that much study of neuroscience.

50
00:03:16,000 --> 00:03:22,000
I was always inspired by what I'd learned about how the brain works, that there's a bunch of neurons.

51
00:03:22,000 --> 00:03:24,000
They perform relatively simple operations.

52
00:03:24,000 --> 00:03:32,000
They're non-linear, but they collect inputs, they weight them, and then they give an output that depends on that weighted input.

53
00:03:32,000 --> 00:03:36,000
And the question is, how do you change those weights to make the whole thing do something good?

54
00:03:36,000 --> 00:03:38,000
It seems like a fairly simple question.

55
00:03:38,000 --> 00:03:42,000
What collaborations do you remember from that time?

56
00:03:42,000 --> 00:03:46,000
The main collaboration I had at Carnegie Mellon was with someone who wasn't at Carnegie Mellon.

57
00:03:46,000 --> 00:03:51,000
I was interacting a lot with Terry Sinovsky, who was in Baltimore at Johns Hopkins.

58
00:03:51,000 --> 00:03:55,000
And about once a month, either he would drive to Pittsburgh or I would drive to Baltimore.

59
00:03:55,000 --> 00:03:59,000
It's 250 miles away, and we would spend a weekend together working on Baltimore machines.

60
00:03:59,000 --> 00:04:01,000
That was a wonderful collaboration.

61
00:04:01,000 --> 00:04:03,000
We were both convinced it was how the brain worked.

62
00:04:03,000 --> 00:04:05,000
That was the most exciting research I've ever done.

63
00:04:05,000 --> 00:04:11,000
And a lot of technical results came out that were very interesting, but I think it's not how the brain works.

64
00:04:11,000 --> 00:04:18,000
I also had a very good collaboration with Peter Brown, who was a very good statistician,

65
00:04:18,000 --> 00:04:20,000
and he worked on speech recognition at IBM.

66
00:04:20,000 --> 00:04:28,000
And then he came as a more mature student at Carnegie Mellon just to get a PhD, but he already knew a lot.

67
00:04:28,000 --> 00:04:32,000
He taught me a lot about speech, and he in fact taught me about hidden Markov models.

68
00:04:32,000 --> 00:04:35,000
I think I learned more from him than he learned from me.

69
00:04:35,000 --> 00:04:37,000
That's the kind of student you want.

70
00:04:37,000 --> 00:04:43,000
And when he taught me about hidden Markov models, I was doing backprop with hidden layers.

71
00:04:43,000 --> 00:04:45,000
Only they weren't called hidden layers then.

72
00:04:45,000 --> 00:04:52,000
And I decided that name they use in hidden Markov models is a great name for variables that you don't know what they're up to.

73
00:04:52,000 --> 00:04:57,000
And so that's where the name hidden in neural nets came from.

74
00:04:57,000 --> 00:05:02,000
Me and Peter decided that was a great name for the hidden layers of neural nets.

75
00:05:02,000 --> 00:05:05,000
But I learned a lot from Peter about speech.

76
00:05:05,000 --> 00:05:10,000
Take us back to when Ilya showed up at your office.

77
00:05:10,000 --> 00:05:15,000
I was in my office, probably on a Sunday, and I was programming, I think.

78
00:05:15,000 --> 00:05:18,000
And there was a knock on the door, not just any knock, but it went kinda...

79
00:05:18,000 --> 00:05:21,000
That's sort of an urgent knock.

80
00:05:21,000 --> 00:05:24,000
So I went and answered the door, and this was this young student there.

81
00:05:24,000 --> 00:05:28,000
And he said he was cooking fries over the summer, but he'd rather be working in my lab.

82
00:05:28,000 --> 00:05:32,000
And so I said, well, why don't you make an appointment and we'll talk?

83
00:05:32,000 --> 00:05:34,000
And so Ilya said, how about now?

84
00:05:34,000 --> 00:05:37,000
And that sort of was Ilya's character.

85
00:05:37,000 --> 00:05:43,000
So we talked for a bit, and I gave him a paper to read, which was the Nature Paper and Back Propagation.

86
00:05:43,000 --> 00:05:49,000
And we made another meeting for a week later, and he came back and he said, I didn't understand it.

87
00:05:49,000 --> 00:05:53,000
And I was very disappointed. I thought, he seemed like a bright guy, but it's only the chain rule.

88
00:05:53,000 --> 00:05:55,000
It's not that hard to understand.

89
00:05:55,000 --> 00:05:58,000
And he said, oh, no, no, I understood that.

90
00:05:58,000 --> 00:06:03,000
I just don't understand why you don't give the gradient to a sensible function optimizer,

91
00:06:03,000 --> 00:06:06,000
which took us quite a few years to think about.

92
00:06:06,000 --> 00:06:08,000
And it kept on like that with Ilya.

93
00:06:08,000 --> 00:06:12,000
He had very good, his raw intuitions about things were always very good.

94
00:06:12,000 --> 00:06:17,000
What do you think had enabled those intuitions for Ilya?

95
00:06:17,000 --> 00:06:20,000
I don't know. I think he always thought for himself.

96
00:06:20,000 --> 00:06:23,000
He was always interested in AI from a young age.

97
00:06:23,000 --> 00:06:27,000
He's obviously good at math, but it's very hard to know.

98
00:06:27,000 --> 00:06:31,000
And what was that collaboration between the two of you like?

99
00:06:31,000 --> 00:06:34,000
What part would you play and what part would Ilya play?

100
00:06:34,000 --> 00:06:36,000
It was a lot of fun.

101
00:06:36,000 --> 00:06:44,000
I remember one occasion when we were trying to do a complicated thing with producing maps of data,

102
00:06:44,000 --> 00:06:46,000
where I had a kind of mixture model.

103
00:06:46,000 --> 00:06:49,000
So you could take the same bunch of similarities and make two maps,

104
00:06:49,000 --> 00:06:55,000
so that in one map, bank could be close to greed and in another map, bank could be close to river.

105
00:06:56,000 --> 00:06:59,000
Because in one map, you can't have it close to both, right?

106
00:06:59,000 --> 00:07:01,000
Because river and greed are one way apart.

107
00:07:01,000 --> 00:07:03,000
So we'd have a mixture of maps.

108
00:07:03,000 --> 00:07:05,000
And we were doing it in MATLAB,

109
00:07:05,000 --> 00:07:09,000
and this involved a lot of reorganization of the code to do the right matrix multiplies,

110
00:07:09,000 --> 00:07:11,000
and only got fed up with that.

111
00:07:11,000 --> 00:07:13,000
So he came one day and said,

112
00:07:13,000 --> 00:07:16,000
I'm going to write an interface for MATLAB,

113
00:07:16,000 --> 00:07:18,000
so I program in this different language,

114
00:07:18,000 --> 00:07:21,000
and then I have something that just converts it into MATLAB.

115
00:07:21,000 --> 00:07:25,000
And I said, no Ilya, that'll take you a month to do.

116
00:07:25,000 --> 00:07:26,000
We've got to get on with this project.

117
00:07:26,000 --> 00:07:28,000
Don't get diverted by that.

118
00:07:28,000 --> 00:07:30,000
And Ilya said, it's okay, I did it this morning.

119
00:07:33,000 --> 00:07:35,000
That's quite incredible.

120
00:07:35,000 --> 00:07:42,000
And throughout those years, the biggest shift wasn't necessarily just the algorithms,

121
00:07:42,000 --> 00:07:44,000
but also the skill.

122
00:07:44,000 --> 00:07:49,000
How did you sort of view that skill over the years?

123
00:07:49,000 --> 00:07:51,000
Ilya got that intuition very early.

124
00:07:51,000 --> 00:07:57,000
So Ilya was always preaching that you just make it bigger and it'll work better.

125
00:07:57,000 --> 00:07:59,000
And I always thought that was a bit of a cop-out,

126
00:07:59,000 --> 00:08:01,000
that you're going to have to have new ideas too.

127
00:08:01,000 --> 00:08:03,000
It turns out Ilya was basically right.

128
00:08:03,000 --> 00:08:04,000
New ideas help.

129
00:08:04,000 --> 00:08:06,000
Things like transformers helped a lot.

130
00:08:06,000 --> 00:08:10,000
But it was really the scale of the data and the scale of the computation.

131
00:08:10,000 --> 00:08:15,000
And back then, we had no idea computers would get like a billion times faster.

132
00:08:15,000 --> 00:08:17,000
We thought maybe they'd get 100 times faster.

133
00:08:17,000 --> 00:08:20,000
We were trying to do things by coming up with clever ideas

134
00:08:20,000 --> 00:08:24,000
that would have just solved themselves if we had had bigger scale of the data and computation.

135
00:08:24,000 --> 00:08:29,000
In about 2011, Ilya and another graduate student called James Martins and I

136
00:08:29,000 --> 00:08:33,000
had a paper using character level prediction.

137
00:08:33,000 --> 00:08:38,000
So we took Wikipedia and we tried to predict the next HTML character.

138
00:08:38,000 --> 00:08:40,000
And that worked remarkably well.

139
00:08:40,000 --> 00:08:43,000
And we were always amazed at how well it worked.

140
00:08:43,000 --> 00:08:47,000
And that was using a fancy optimizer on GPUs.

141
00:08:47,000 --> 00:08:51,000
And we could never quite believe that it understood anything,

142
00:08:51,000 --> 00:08:53,000
but it looked as though it understood.

143
00:08:53,000 --> 00:08:56,000
And that just seemed incredible.

144
00:08:56,000 --> 00:09:02,000
Can you take us through how are these models trained to predict the next word

145
00:09:02,000 --> 00:09:07,000
and why is it the wrong way of thinking about them?

146
00:09:07,000 --> 00:09:10,000
Okay, I don't actually believe it is the wrong way.

147
00:09:10,000 --> 00:09:15,000
So, in fact, I think I made the first neural net language model

148
00:09:15,000 --> 00:09:17,000
that used embeddings and backpropagation.

149
00:09:17,000 --> 00:09:20,000
So it's very simple data, just triples.

150
00:09:20,000 --> 00:09:24,000
And it was turning each symbol into an embedding,

151
00:09:24,000 --> 00:09:29,000
then having the embeddings interact to predict the embedding of the next symbol

152
00:09:29,000 --> 00:09:31,000
and then from that predict the next symbol.

153
00:09:31,000 --> 00:09:35,000
And then it was backpropagating through that whole process to learn these triples.

154
00:09:35,000 --> 00:09:37,000
And I showed it could generalize.

155
00:09:38,000 --> 00:09:41,000
About 10 years later, Yoshio Benji used a very similar network

156
00:09:41,000 --> 00:09:43,000
and showed it worked with real text.

157
00:09:43,000 --> 00:09:47,000
And about 10 years after that, Linguist started believing in embeddings.

158
00:09:47,000 --> 00:09:48,000
It was a slow process.

159
00:09:48,000 --> 00:09:52,000
The reason I think it's not just predicting the next symbol

160
00:09:52,000 --> 00:09:55,000
is if you ask, well, what does it take to predict the next symbol?

161
00:09:55,000 --> 00:10:03,000
Particularly if you ask me a question and then the first word of the answer is the next symbol,

162
00:10:03,000 --> 00:10:06,000
you have to understand the question.

163
00:10:06,000 --> 00:10:12,000
So I think by predicting the next symbol, it's very unlike old-fashioned autocomplete.

164
00:10:12,000 --> 00:10:15,000
For old-fashioned autocomplete, you'd store sort of triples of words.

165
00:10:15,000 --> 00:10:20,000
And then if you saw a pair of words, you see how often different words came third

166
00:10:20,000 --> 00:10:22,000
and that way you could predict the next symbol.

167
00:10:22,000 --> 00:10:25,000
And that's what most people think autocomplete is like.

168
00:10:25,000 --> 00:10:27,000
It's no longer at all like that.

169
00:10:27,000 --> 00:10:30,000
To predict the next symbol, you have to understand what's been said.

170
00:10:30,000 --> 00:10:34,000
So I think you're forcing it to understand by making it predict the next symbol.

171
00:10:34,000 --> 00:10:37,000
And I think it's understanding in much the same way we are.

172
00:10:37,000 --> 00:10:41,000
So a lot of people will tell you these things aren't like us.

173
00:10:41,000 --> 00:10:43,000
They're just predicting the next symbol.

174
00:10:43,000 --> 00:10:45,000
They're not reasoning like us.

175
00:10:45,000 --> 00:10:49,000
But actually, in order to predict the next symbol, it's going to have to do some reasoning.

176
00:10:49,000 --> 00:10:54,000
And we've seen now that if you make big ones, without putting in any special stuff to do reasoning,

177
00:10:54,000 --> 00:10:56,000
they can already do some reasoning.

178
00:10:56,000 --> 00:10:59,000
And I think as you make them bigger, they're going to be able to do more and more reasoning.

179
00:10:59,000 --> 00:11:03,000
Do you think I'm doing anything else than predicting the next symbol right now?

180
00:11:03,000 --> 00:11:05,000
I think that's how you're learning.

181
00:11:05,000 --> 00:11:08,000
I think you're predicting the next video frame.

182
00:11:08,000 --> 00:11:11,000
You're predicting the next sound.

183
00:11:11,000 --> 00:11:15,000
But I think that's a pretty plausible theory of how the brain's learning.

184
00:11:15,000 --> 00:11:21,000
What enables these models to learn such a wide variety of fields?

185
00:11:21,000 --> 00:11:25,000
What these big language models are doing is they're looking for common structure.

186
00:11:25,000 --> 00:11:30,000
And by finding common structure, they can encode things using the common structure and that's more efficient.

187
00:11:30,000 --> 00:11:32,000
So let me give you an example.

188
00:11:32,000 --> 00:11:37,000
If you ask GPT-4, why is a compost heap like an atom bomb?

189
00:11:37,000 --> 00:11:39,000
Most people can't answer that.

190
00:11:39,000 --> 00:11:43,000
Most people haven't thought they think atom bombs and compost heap are very different things.

191
00:11:43,000 --> 00:11:49,000
But GPT-4 will tell you, well, the energy scales are very different and the time scales are very different.

192
00:11:49,000 --> 00:11:54,000
But the thing that's the same is that when the compost heap gets hotter, it generates heat faster.

193
00:11:54,000 --> 00:11:59,000
And when the atom bomb produces more neutrons, it produces more neutrons faster.

194
00:12:00,000 --> 00:12:03,000
And so it gets the idea of a chain reaction.

195
00:12:03,000 --> 00:12:10,000
And I believe it's understood that both forms of chain reaction is using that understanding to compress all that information into its weights.

196
00:12:10,000 --> 00:12:18,000
And if it's doing that, then it's going to be doing that for hundreds of things where we haven't seen the analogies yet, but it has.

197
00:12:18,000 --> 00:12:23,000
And that's where you get creativity from, from seeing these analogies between apparently very different things.

198
00:12:23,000 --> 00:12:27,000
And so I think GPT-4 is going to end up, when it gets bigger, being very creative.

199
00:12:27,000 --> 00:12:35,000
I think this idea that it's just regurgitating what it's learned, just pastishing together text it's learned already, that's completely wrong.

200
00:12:35,000 --> 00:12:38,000
It's going to be even more creative than people, I think.

201
00:12:38,000 --> 00:12:47,000
You'd argue that it won't just repeat the human knowledge we've developed so far, but could also progress beyond that.

202
00:12:47,000 --> 00:12:50,000
I think that's something we haven't quite seen yet.

203
00:12:50,000 --> 00:12:53,000
We've started seeing some examples of it.

204
00:12:53,000 --> 00:12:58,000
But to a large extent, we're sort of still at the current level of science.

205
00:12:58,000 --> 00:13:00,000
What do you think will enable it to go beyond that?

206
00:13:00,000 --> 00:13:03,000
Well, we've seen that in more limited contexts.

207
00:13:03,000 --> 00:13:14,000
Like if you take AlphaGo, in that famous competition with Lysidol, there was Move 37, where AlphaGo made a move that all the experts said must have been a mistake.

208
00:13:14,000 --> 00:13:17,000
But actually later they realized it was a brilliant move.

209
00:13:17,000 --> 00:13:20,000
So that was creative within that limited domain.

210
00:13:21,000 --> 00:13:24,000
I think we'll see a lot more of that as these things get bigger.

211
00:13:24,000 --> 00:13:35,000
The difference with AlphaGo as well was that it was using reinforcement learning, that that subsequently sort of enabled it to go beyond the current state.

212
00:13:35,000 --> 00:13:42,000
So it started with imitation learning, watching how humans play the game, and then it would, through self-play, develop way beyond that.

213
00:13:42,000 --> 00:13:45,000
Do you think that's the missing component of the current evidence?

214
00:13:45,000 --> 00:13:48,000
I think that may well be a missing component, yes.

215
00:13:48,000 --> 00:13:56,000
That the self-play in AlphaGo and AlphaZero are a large part of why it could make these creative moves.

216
00:13:56,000 --> 00:13:59,000
But I don't think it's entirely necessary.

217
00:13:59,000 --> 00:14:06,000
So there's a little experiment I did a long time ago where you're training on your own net to recognize 100 digits.

218
00:14:06,000 --> 00:14:08,000
I love that example, the MNIST example.

219
00:14:08,000 --> 00:14:12,000
And you give it training data where half the answers are wrong.

220
00:14:14,000 --> 00:14:16,000
And the question is, how well will it learn?

221
00:14:18,000 --> 00:14:23,000
And you make half the answers wrong once and keep them like that.

222
00:14:23,000 --> 00:14:29,000
So it can't average away the wrongness by just seeing the same example, but with the right answer sometimes and the wrong answer sometimes.

223
00:14:29,000 --> 00:14:34,000
When it sees that example, half of the examples, when it sees the example, the answer is always wrong.

224
00:14:34,000 --> 00:14:38,000
And so the training data has 50% error.

225
00:14:38,000 --> 00:14:44,000
But if you train up bank propagation, it gets down to 5% error, or less.

226
00:14:44,000 --> 00:14:51,000
In other words, from badly labeled data, it can get much better results.

227
00:14:51,000 --> 00:14:53,000
It can see that the training data is wrong.

228
00:14:53,000 --> 00:14:56,000
And that's how smart students can be smarter than their advisor.

229
00:14:56,000 --> 00:15:02,000
And their advisor tells them all this stuff, and for half of what their advisor tells them, they think, no, rubbish.

230
00:15:02,000 --> 00:15:06,000
And they listen to the other half, and then they end up smarter than the advisor.

231
00:15:06,000 --> 00:15:11,000
So these big neural nets can actually do, they can do much better than their training data.

232
00:15:11,000 --> 00:15:13,000
And most people don't realize that.

233
00:15:13,000 --> 00:15:18,000
So how do you expect these models to add reasoning into them?

234
00:15:18,000 --> 00:15:25,000
So I mean, one approach is you add sort of the heuristics on top of them, which a lot of the research is doing now,

235
00:15:25,000 --> 00:15:30,000
where you have sort of chain of thought, you just feedback its reasoning into itself.

236
00:15:30,000 --> 00:15:35,000
And another way would be in the model itself, as you scale it up.

237
00:15:35,000 --> 00:15:37,000
What's your intuition around that?

238
00:15:37,000 --> 00:15:42,000
So my intuition is that as we scale up these models, they get better at reasoning.

239
00:15:42,000 --> 00:15:49,000
And if you ask how people work, roughly speaking, we have these intuitions, and we can do reasoning.

240
00:15:49,000 --> 00:15:53,000
And we use the reasoning to correct our intuitions.

241
00:15:53,000 --> 00:15:56,000
Of course, we use the intuitions during the reasoning to do the reasoning.

242
00:15:56,000 --> 00:16:01,000
But if the conclusion of the reasoning conflicts with our intuitions, we realize the intuitions need to be changed.

243
00:16:01,000 --> 00:16:11,000
That's much like in AlphaGo or AlphaZero, where you have an evaluation function that just looks at the board and says, how good is that for me?

244
00:16:11,000 --> 00:16:19,000
But then you do the Monte Carlo rollout, and now you get a more accurate idea, and you can revise your evaluation function.

245
00:16:19,000 --> 00:16:23,000
So you can train it by getting it to agree with the results of reasoning.

246
00:16:23,000 --> 00:16:26,000
And I think these large language models have to start doing that.

247
00:16:26,000 --> 00:16:33,000
They have to start training their raw intuitions about what should come next by doing reasoning and realizing that's not right.

248
00:16:33,000 --> 00:16:39,000
And so that way they can get more training data than just mimicking what people did.

249
00:16:39,000 --> 00:16:43,000
And that's exactly why AlphaGo could do this creative move 37.

250
00:16:43,000 --> 00:16:49,000
It had much more training data because it was using reasoning to check out what the right next move should have been.

251
00:16:49,000 --> 00:16:52,000
And what do you think about multimodality?

252
00:16:52,000 --> 00:16:57,000
So we spoke about these analogies, and often the analogies are way beyond what we could see.

253
00:16:57,000 --> 00:17:05,000
It's discovering analogies that are far beyond humans and at maybe abstraction levels that we'll never be able to understand.

254
00:17:05,000 --> 00:17:13,000
Now, when we introduce images to that and video and sound, how do you think that will change the models?

255
00:17:13,000 --> 00:17:18,000
And how do you think it will change the analogies that it will be able to make?

256
00:17:18,000 --> 00:17:25,000
I think it'll change it a lot. I think it'll make it much better at understanding spatial things, for example.

257
00:17:25,000 --> 00:17:34,000
From language alone, it's quite hard to understand some spatial things, although remarkably GPT-4 can do that even before it was multimodal.

258
00:17:34,000 --> 00:17:41,000
But when you make it multimodal, if you have it both doing vision and reaching out and grabbing things,

259
00:17:41,000 --> 00:17:45,000
it'll understand objects much better if it can pick them up and turn them over and so on.

260
00:17:45,000 --> 00:17:53,000
So although you can learn an awful lot from language, it's easier to learn if you are multimodal.

261
00:17:53,000 --> 00:17:55,000
And in fact, you then need less language.

262
00:17:55,000 --> 00:18:00,000
And there's an awful lot of YouTube video for predicting the next frame or something like that.

263
00:18:00,000 --> 00:18:04,000
So I think these multimodal models are clearly going to take over.

264
00:18:04,000 --> 00:18:08,000
You can get more data that way. They need less language.

265
00:18:08,000 --> 00:18:13,000
So there's really a philosophical point that you could learn a very good model from language alone,

266
00:18:13,000 --> 00:18:16,000
but it's much easier to learn it from a multimodal system.

267
00:18:16,000 --> 00:18:20,000
And how do you think it will impact the model's reasoning?

268
00:18:20,000 --> 00:18:23,000
I think it'll make it much better at reasoning about space, for example.

269
00:18:23,000 --> 00:18:25,000
Reasoning about what happens if you pick objects up.

270
00:18:25,000 --> 00:18:29,000
If you actually try picking objects up, you're going to get all sorts of training data that's going to help.

271
00:18:29,000 --> 00:18:35,000
Do you think the human brain evolved to work well with language?

272
00:18:35,000 --> 00:18:39,000
Or do you think language evolved to work well with the human brain?

273
00:18:39,000 --> 00:18:42,000
I think the question of whether language evolved to work with the brain

274
00:18:42,000 --> 00:18:45,000
or the brain evolved to work with language, I think that's a very good question.

275
00:18:45,000 --> 00:18:48,000
I think both happened.

276
00:18:48,000 --> 00:18:53,000
I used to think we would do a lot of cognition without needing language at all.

277
00:18:53,000 --> 00:18:56,000
Now I've changed my mind a bit.

278
00:18:56,000 --> 00:19:02,000
So let me give you three different views of language and how it relates to cognition.

279
00:19:02,000 --> 00:19:09,000
There's the old-fashioned symbolic view, which is cognition consists of having strings of symbols

280
00:19:09,000 --> 00:19:13,000
in some kind of cleaned up logical language where there's no ambiguity

281
00:19:13,000 --> 00:19:15,000
and applying rules of inference.

282
00:19:15,000 --> 00:19:17,000
And that's what cognition is.

283
00:19:17,000 --> 00:19:22,000
It's just these symbolic manipulations on things that are like strings of language symbols.

284
00:19:22,000 --> 00:19:24,000
So that's one extreme view.

285
00:19:24,000 --> 00:19:29,000
The opposite extreme view is, no, no, once you get inside the head, it's all vectors.

286
00:19:29,000 --> 00:19:33,000
So symbols come in, you convert those symbols into big vectors

287
00:19:33,000 --> 00:19:36,000
and all the stuff inside is done with big vectors,

288
00:19:36,000 --> 00:19:39,000
and then if you want to produce output, you produce symbols again.

289
00:19:39,000 --> 00:19:43,000
So there was a point in machine translation in about 2014

290
00:19:43,000 --> 00:19:48,000
when people were using recurrent neural nets and words would keep coming in

291
00:19:48,000 --> 00:19:53,000
and they'd have a hidden state and they'd keep accumulating information in this hidden state.

292
00:19:53,000 --> 00:19:57,000
And then they got to the end of a sentence that have a big hidden vector

293
00:19:57,000 --> 00:19:59,000
that captured the meaning of that sentence

294
00:19:59,000 --> 00:20:02,000
that could then be used for producing the sentences in another language.

295
00:20:02,000 --> 00:20:04,000
That was called a thought vector.

296
00:20:04,000 --> 00:20:06,000
And that's the sort of second view of language.

297
00:20:06,000 --> 00:20:10,000
You convert the language into a big vector that's nothing like language

298
00:20:10,000 --> 00:20:12,000
and that's what cognition is all about.

299
00:20:12,000 --> 00:20:15,000
But then there's a third view, which is what I believe now,

300
00:20:15,000 --> 00:20:21,000
which is that you take these symbols

301
00:20:21,000 --> 00:20:25,000
and you convert the symbols into embeddings and you use multiple layers of that

302
00:20:25,000 --> 00:20:27,000
so you get these very rich embeddings.

303
00:20:27,000 --> 00:20:29,000
But the embeddings are still tied to the symbols

304
00:20:29,000 --> 00:20:33,000
in the sense that you've got a big vector for this symbol and a big vector for that symbol

305
00:20:33,000 --> 00:20:38,000
and these vectors interact to produce the vector for the symbol for the next word.

306
00:20:38,000 --> 00:20:40,000
And that's what understanding is.

307
00:20:40,000 --> 00:20:44,000
Understanding is knowing how to convert the symbols into these vectors

308
00:20:44,000 --> 00:20:48,000
and knowing how the elements of the vectors should interact to predict the vector for the next symbol.

309
00:20:48,000 --> 00:20:52,000
That's what understanding is, both in these big language models and in our brains.

310
00:20:52,000 --> 00:20:56,000
And that's an example which is sort of in between.

311
00:20:56,000 --> 00:21:01,000
You're staying with the symbols, but you're interpreting them as these big vectors

312
00:21:01,000 --> 00:21:03,000
and that's where all the work is.

313
00:21:03,000 --> 00:21:06,000
And all the knowledge is in what vectors you use

314
00:21:06,000 --> 00:21:10,000
and how the elements of those vectors interact, not in symbolic rules.

315
00:21:10,000 --> 00:21:14,000
But it's not saying that you get away from the symbols altogether.

316
00:21:14,000 --> 00:21:17,000
It's saying you turn the symbols into big vectors

317
00:21:17,000 --> 00:21:20,000
but you stay with that surface structure of the symbols.

318
00:21:20,000 --> 00:21:22,000
And that's how these models are working.

319
00:21:22,000 --> 00:21:25,000
And that's, and I seem to be, a more plausible model of human thought too.

320
00:21:25,000 --> 00:21:31,000
You were one of the first folks to get the idea of using GPUs.

321
00:21:31,000 --> 00:21:34,000
And I know Jensen loves you for that.

322
00:21:34,000 --> 00:21:39,000
Back in 2009 you mentioned that, you told Jensen that this could be a quite good idea

323
00:21:39,000 --> 00:21:42,000
for training neural nets.

324
00:21:42,000 --> 00:21:48,000
Take us back to that early intuition of using GPUs for training neural nets.

325
00:21:48,000 --> 00:21:54,000
So actually I think in about 2006 I had a former graduate student called Rick Zeliski.

326
00:21:54,000 --> 00:21:56,000
He's a very good computer vision guy.

327
00:21:56,000 --> 00:21:59,000
And I talked to him at a meeting.

328
00:21:59,000 --> 00:22:03,000
He said, you know, you ought to think about using graphics processing cards

329
00:22:03,000 --> 00:22:05,000
because they're very good at matrix multiplies.

330
00:22:05,000 --> 00:22:08,000
And what you're doing is basically all matrix multiplies.

331
00:22:08,000 --> 00:22:10,000
So I thought about that for a bit.

332
00:22:10,000 --> 00:22:16,000
And then we learned about these Tesla systems that had four GPUs in.

333
00:22:16,000 --> 00:22:24,000
And initially we just got gaming GPUs and discovered they made things go 30 times faster.

334
00:22:24,000 --> 00:22:27,000
And then we bought one of these Tesla systems with four GPUs.

335
00:22:27,000 --> 00:22:31,000
And we did speech on that and it worked very well.

336
00:22:31,000 --> 00:22:34,000
And then in 2009 I gave a talk at NIPS.

337
00:22:34,000 --> 00:22:37,000
And I told a thousand machine learning researchers

338
00:22:37,000 --> 00:22:39,000
that you should all go and buy Nvidia GPUs.

339
00:22:39,000 --> 00:22:40,000
They're the future.

340
00:22:40,000 --> 00:22:42,000
You need them for doing machine learning.

341
00:22:42,000 --> 00:22:46,000
And I actually then sent mail to Nvidia saying,

342
00:22:46,000 --> 00:22:48,000
I told a thousand machine learning researchers to buy your boards.

343
00:22:48,000 --> 00:22:49,000
Could you give me a free one?

344
00:22:49,000 --> 00:22:50,000
And they said no.

345
00:22:50,000 --> 00:22:51,000
Actually they didn't say no.

346
00:22:51,000 --> 00:22:53,000
They just didn't reply.

347
00:22:53,000 --> 00:22:58,000
But when I told Jensen this story later on, he gave me a free one.

348
00:22:58,000 --> 00:23:00,000
That's very, very good.

349
00:23:00,000 --> 00:23:07,000
I think what's interesting as well is sort of how GPUs has evolved alongside the field.

350
00:23:07,000 --> 00:23:12,000
So where do you think we should go next in the computer?

351
00:23:12,000 --> 00:23:18,000
So my last couple of years at Google I was thinking about ways of trying to make analog computation.

352
00:23:18,000 --> 00:23:23,000
So instead of using like a megawatt we could use like 30 watts like the brain.

353
00:23:23,000 --> 00:23:27,000
And we could run these big language models in analog hardware.

354
00:23:27,000 --> 00:23:30,000
And I never made it work.

355
00:23:30,000 --> 00:23:35,000
But I started really appreciating digital computation.

356
00:23:35,000 --> 00:23:40,000
So if you're going to use that low power analog computation,

357
00:23:40,000 --> 00:23:43,000
every piece of hardware is going to be a bit different.

358
00:23:43,000 --> 00:23:48,000
And the idea is the learning is going to make use of the specific properties of that hardware.

359
00:23:48,000 --> 00:23:49,000
And that's what happens with people.

360
00:23:49,000 --> 00:23:51,000
All our brains are different.

361
00:23:51,000 --> 00:23:56,000
So we can't then take the weights in your brain and put them in my brain.

362
00:23:56,000 --> 00:23:57,000
The hardware is different.

363
00:23:57,000 --> 00:24:00,000
The precise properties of the individual neurons are different.

364
00:24:00,000 --> 00:24:03,000
The learning has learned to make use of all that.

365
00:24:03,000 --> 00:24:08,000
And so we're mortal in the sense that the weights in my brain are no good for any other brain.

366
00:24:08,000 --> 00:24:10,000
When I die those weights are useless.

367
00:24:10,000 --> 00:24:17,000
We can get information from one to another rather inefficiently by I produce sentences

368
00:24:17,000 --> 00:24:20,000
and you figure out how to change your weight so you would have said the same thing.

369
00:24:20,000 --> 00:24:22,000
That's called distillation.

370
00:24:22,000 --> 00:24:25,000
But that's a very inefficient way of communicating knowledge.

371
00:24:25,000 --> 00:24:30,000
And with digital systems they're immortal because once you've got some weights

372
00:24:30,000 --> 00:24:34,000
you can throw away the computer, just store the weights on a tape somewhere

373
00:24:34,000 --> 00:24:37,000
and now build another computer, put those same weights in.

374
00:24:37,000 --> 00:24:41,000
And if it's digital it can compute exactly the same thing as the other system did.

375
00:24:41,000 --> 00:24:44,000
So digital systems can share weights.

376
00:24:44,000 --> 00:24:47,000
And that's incredibly much more efficient.

377
00:24:47,000 --> 00:24:53,000
If you've got a whole bunch of digital systems and they each go into a tiny bit of learning

378
00:24:53,000 --> 00:24:56,000
and they start with the same weights, they do a tiny bit of learning

379
00:24:56,000 --> 00:25:00,000
and then they share their weights again, they all know what all the others learned.

380
00:25:00,000 --> 00:25:02,000
We can't do that.

381
00:25:02,000 --> 00:25:05,000
And so they're far superior to us in being able to share knowledge.

382
00:25:05,000 --> 00:25:11,000
A lot of the ideas that have been deployed in the field are very old school ideas.

383
00:25:11,000 --> 00:25:16,000
It's the ideas that have been around in neuroscience for forever.

384
00:25:16,000 --> 00:25:20,000
What do you think is sort of left to apply to the systems that we develop?

385
00:25:20,000 --> 00:25:27,000
So one big thing that we still have to catch up with neuroscience on

386
00:25:27,000 --> 00:25:30,000
is the time scales for changes.

387
00:25:30,000 --> 00:25:36,000
So in nearly all the neural nets there's a fast time scale for changing activities.

388
00:25:36,000 --> 00:25:40,000
So input comes in, the activities, the embedding vectors all change.

389
00:25:40,000 --> 00:25:43,000
And then there's a slow time scale which is changing the weights.

390
00:25:43,000 --> 00:25:45,000
And that's long-term learning.

391
00:25:45,000 --> 00:25:47,000
And you just have those two time scales.

392
00:25:47,000 --> 00:25:51,000
In the brain there's many time scales at which weights change.

393
00:25:51,000 --> 00:25:55,000
So for example, if I say an unexpected word like cucumber,

394
00:25:55,000 --> 00:26:00,000
and now five minutes later you put headphones on, there's a lot of noise

395
00:26:00,000 --> 00:26:05,000
and there's very faint words, you'll be much better at recognising the word cucumber

396
00:26:05,000 --> 00:26:07,000
because I said it five minutes ago.

397
00:26:07,000 --> 00:26:10,000
So where is that knowledge in the brain?

398
00:26:10,000 --> 00:26:13,000
And that knowledge is obviously in temporary changes to synapses.

399
00:26:13,000 --> 00:26:16,000
It's not neurons that go in cucumber, cucumber, cucumber,

400
00:26:16,000 --> 00:26:18,000
you don't have enough neurons for that.

401
00:26:18,000 --> 00:26:21,000
It's in temporary changes to the weights.

402
00:26:21,000 --> 00:26:24,000
And you can do a lot of things with temporary weight changes,

403
00:26:24,000 --> 00:26:26,000
what I call fast weights.

404
00:26:26,000 --> 00:26:28,000
We don't do that in these neural models.

405
00:26:28,000 --> 00:26:33,000
And the reason we don't do it is because if you have temporary changes to the weights

406
00:26:33,000 --> 00:26:35,000
that depend on the input data,

407
00:26:35,000 --> 00:26:40,000
then you can't process a whole bunch of different cases at the same time.

408
00:26:40,000 --> 00:26:43,000
At present we take a whole bunch of different strings,

409
00:26:43,000 --> 00:26:47,000
we stack them together and we process them all in parallel

410
00:26:47,000 --> 00:26:51,000
because then we can do matrix, matrix, multiplies, which is much more efficient.

411
00:26:51,000 --> 00:26:55,000
And just that efficiency is stopping us using fast weights.

412
00:26:55,000 --> 00:26:59,000
But the brain clearly uses fast weights for temporary memory.

413
00:26:59,000 --> 00:27:02,000
And there's all sorts of things you can do that way that we don't do at present.

414
00:27:02,000 --> 00:27:04,000
I think that's one of the biggest things we have to do.

415
00:27:04,000 --> 00:27:07,000
I was very hopeful that things like Graphcore,

416
00:27:07,000 --> 00:27:11,000
if they went sequential and did just online learning,

417
00:27:11,000 --> 00:27:13,000
then they could use fast weights.

418
00:27:13,000 --> 00:27:16,000
But that hasn't worked out yet.

419
00:27:16,000 --> 00:27:20,000
I think it'll work out eventually when people are using conductances for weights.

420
00:27:20,000 --> 00:27:24,000
How has knowing how these models work

421
00:27:24,000 --> 00:27:29,000
and knowing how the brain works impacted the way you think?

422
00:27:29,000 --> 00:27:35,000
I think there's been one big impact, which is at a fairly abstract level,

423
00:27:35,000 --> 00:27:40,000
which is that for many years people were very scornful

424
00:27:40,000 --> 00:27:43,000
about the idea of having a big random neural net

425
00:27:43,000 --> 00:27:45,000
and just giving it a lot of training data

426
00:27:45,000 --> 00:27:47,000
and it would learn to do complicated things.

427
00:27:47,000 --> 00:27:51,000
If you talk to statisticians or linguists or most people in AI,

428
00:27:51,000 --> 00:27:53,000
they say, that's just a pipe dream.

429
00:27:53,000 --> 00:27:56,000
There's no way you're going to learn to really complicated things

430
00:27:56,000 --> 00:27:58,000
without some kind of innate knowledge,

431
00:27:58,000 --> 00:28:00,000
without a lot of architectural restrictions.

432
00:28:00,000 --> 00:28:02,000
It turns out that's completely wrong.

433
00:28:02,000 --> 00:28:04,000
You can take a big random neural network

434
00:28:04,000 --> 00:28:07,000
and you can learn a whole bunch of stuff just from data.

435
00:28:08,000 --> 00:28:11,000
So the idea that stochastic gradient descent

436
00:28:11,000 --> 00:28:15,000
to repeatedly adjust the weights using a gradient,

437
00:28:15,000 --> 00:28:19,000
that will learn things and will learn big complicated things,

438
00:28:19,000 --> 00:28:22,000
that's been validated by these big models.

439
00:28:22,000 --> 00:28:25,000
And that's a very important thing to know about the brain.

440
00:28:25,000 --> 00:28:28,000
It doesn't have to have all this innate structure.

441
00:28:28,000 --> 00:28:30,000
Now, obviously it's got a lot of innate structure,

442
00:28:30,000 --> 00:28:35,000
but it certainly doesn't need an innate structure for things that are easily learned.

443
00:28:36,000 --> 00:28:38,000
And so the idea coming from Chomsky,

444
00:28:38,000 --> 00:28:41,000
that you won't learn anything complicated like language

445
00:28:41,000 --> 00:28:45,000
unless it's all kind of wired in already and just matures,

446
00:28:45,000 --> 00:28:48,000
that idea is now clearly nonsense.

447
00:28:48,000 --> 00:28:52,000
I'm sure Chomsky would appreciate you calling his ideas nonsense.

448
00:28:52,000 --> 00:28:57,000
Well, I think a lot of Chomsky's political ideas are very sensible.

449
00:28:57,000 --> 00:29:01,000
I'm always struck by how come someone with such sensible ideas about the Middle East

450
00:29:01,000 --> 00:29:03,000
could be so wrong about linguistics.

451
00:29:04,000 --> 00:29:07,000
What do you think would make these models

452
00:29:07,000 --> 00:29:11,000
simulate consciousness of humans more effectively?

453
00:29:11,000 --> 00:29:16,000
But imagine you had the AI assistant that you've spoken to in your entire life,

454
00:29:16,000 --> 00:29:20,000
and instead of that being like Chatipiti today,

455
00:29:20,000 --> 00:29:24,000
that sort of deletes the memory of the conversation and you start fresh all of the time,

456
00:29:24,000 --> 00:29:27,000
it had self-reflection.

457
00:29:27,000 --> 00:29:32,000
At some point you pass away and you tell that to the assistant.

458
00:29:33,000 --> 00:29:36,000
I mean, not me, somebody else tells that to the assistant.

459
00:29:36,000 --> 00:29:41,000
Yeah, it would be difficult for you to tell that to the assistant.

460
00:29:41,000 --> 00:29:45,000
Do you think that assistant would feel at that point?

461
00:29:45,000 --> 00:29:47,000
Yes, I think they can have feelings too.

462
00:29:47,000 --> 00:29:51,000
So I think just as we have this inner theater model for perception,

463
00:29:51,000 --> 00:29:53,000
we have an inner theater model for feelings,

464
00:29:53,000 --> 00:29:57,000
there are things that I can experience but other people can't.

465
00:29:59,000 --> 00:30:01,000
I think that model is equally wrong.

466
00:30:02,000 --> 00:30:07,000
Suppose I say, I feel like punching Gary on the nose, which I often do.

467
00:30:07,000 --> 00:30:11,000
Let's try and abstract that away from the idea of an inner theater.

468
00:30:11,000 --> 00:30:14,000
What I'm really saying to you is,

469
00:30:14,000 --> 00:30:18,000
if it weren't for the inhibition coming from my frontal lobes,

470
00:30:18,000 --> 00:30:20,000
I would perform an action.

471
00:30:20,000 --> 00:30:25,000
So when we talk about feelings, we're really talking about actions we would perform

472
00:30:25,000 --> 00:30:29,000
if it weren't for constraints.

473
00:30:29,000 --> 00:30:31,000
And that's really what feelings are.

474
00:30:31,000 --> 00:30:35,000
The actions we would do if it weren't for constraints.

475
00:30:35,000 --> 00:30:38,000
So I think you can give the same kind of explanation for feelings

476
00:30:38,000 --> 00:30:40,000
and there's no reason why these things can't have feelings.

477
00:30:40,000 --> 00:30:46,000
In fact, in 1973, I saw a robot have an emotion.

478
00:30:46,000 --> 00:30:50,000
So in Edinburgh, they had a robot with two grippers like this

479
00:30:50,000 --> 00:30:58,000
that could assemble a toy car if you put the pieces separately on a piece of green felt.

480
00:30:58,000 --> 00:31:00,000
But if you put them in a pile,

481
00:31:00,000 --> 00:31:03,000
its vision wasn't good enough to figure out what was going on.

482
00:31:03,000 --> 00:31:05,000
So it put its grip on it and it went whack!

483
00:31:05,000 --> 00:31:08,000
And it knocked them so they were scattered and then it coupled them together.

484
00:31:08,000 --> 00:31:11,000
If you saw that in a person, you'd say it was crossed with the situation

485
00:31:11,000 --> 00:31:14,000
because it didn't understand it so it destroyed it.

486
00:31:14,000 --> 00:31:17,000
That's profound.

487
00:31:17,000 --> 00:31:24,000
We spoke previously, you described humans and the LLMs as analogy machines.

488
00:31:24,000 --> 00:31:31,000
What do you think has been the most powerful analogies that you've found throughout your life?

489
00:31:31,000 --> 00:31:42,000
Throughout my life, I guess probably a sort of weak analogy that has influenced me a lot

490
00:31:42,000 --> 00:31:50,000
is the analogy between religious belief and between belief and symbol processing.

491
00:31:51,000 --> 00:31:55,000
So when I was very young, I came from an atheist family

492
00:31:55,000 --> 00:31:58,000
and went to school and was confronted with religious belief.

493
00:31:58,000 --> 00:32:00,000
And it just seemed nonsense to me.

494
00:32:00,000 --> 00:32:02,000
It still seems nonsense to me.

495
00:32:02,000 --> 00:32:06,000
And when I saw symbol processing as an explanation of how people worked,

496
00:32:06,000 --> 00:32:09,000
I thought it was just the same.

497
00:32:09,000 --> 00:32:11,000
Nonsense.

498
00:32:11,000 --> 00:32:14,000
I don't think it's quite so much nonsense now

499
00:32:14,000 --> 00:32:17,000
because I think actually we do do symbol processing.

500
00:32:17,000 --> 00:32:21,000
It's just we do it by giving these big embedding vectors to the symbols.

501
00:32:21,000 --> 00:32:23,000
But we are actually symbol processing.

502
00:32:23,000 --> 00:32:27,000
But not at all in the way people thought where you match symbols

503
00:32:27,000 --> 00:32:31,000
and the only thing a symbol has is it's identical to another symbol or it's not identical.

504
00:32:31,000 --> 00:32:33,000
That's the only property a symbol has.

505
00:32:33,000 --> 00:32:35,000
We don't do that at all.

506
00:32:35,000 --> 00:32:37,000
We use the context to give embedding vectors to symbols

507
00:32:37,000 --> 00:32:42,000
and then use the interactions between the components of these embedding vectors to do thinking.

508
00:32:43,000 --> 00:32:47,000
But there's a very good researcher at Google called Fernando Pereira

509
00:32:47,000 --> 00:32:51,000
who said, yes, we do have symbolic reasoning

510
00:32:51,000 --> 00:32:53,000
and the only symbolic we have is natural language.

511
00:32:53,000 --> 00:32:56,000
Natural language is a symbolic language and we reason with it.

512
00:32:56,000 --> 00:32:58,000
I believe that now.

513
00:32:58,000 --> 00:33:03,000
You've done some of the most meaningful research in the history of computer science.

514
00:33:03,000 --> 00:33:08,000
Can you walk us through like how do you select the right problems to work on?

515
00:33:08,000 --> 00:33:10,000
Well, first let me correct you.

516
00:33:10,000 --> 00:33:14,000
Me and my students have done a lot of the most meaningful things

517
00:33:14,000 --> 00:33:17,000
and it's mainly been a very good collaboration with students

518
00:33:17,000 --> 00:33:20,000
and my ability to select very good students.

519
00:33:20,000 --> 00:33:24,000
And that came from the fact there were very few people doing neural nets

520
00:33:24,000 --> 00:33:27,000
in the 70s and 80s and 90s and 2000s.

521
00:33:27,000 --> 00:33:31,000
And so the few people doing neural nets got to pick the very best students.

522
00:33:31,000 --> 00:33:33,000
So that was a piece of luck.

523
00:33:33,000 --> 00:33:37,000
But my way of selecting problems is basically, well,

524
00:33:37,000 --> 00:33:39,000
when scientists talk about how they work,

525
00:33:39,000 --> 00:33:41,000
they have theories about how they work

526
00:33:41,000 --> 00:33:43,000
which probably don't have much to do with the truth.

527
00:33:43,000 --> 00:33:49,000
But my theory is that I look for something where everybody's agreed about something

528
00:33:49,000 --> 00:33:51,000
and it feels wrong.

529
00:33:51,000 --> 00:33:54,000
Just there's a slight intuition of something wrong about it.

530
00:33:54,000 --> 00:33:58,000
And then I work on that and see if I can elaborate why it is I think it's wrong.

531
00:33:58,000 --> 00:34:02,000
And maybe I can make a little demo with a small computer program

532
00:34:02,000 --> 00:34:06,000
that shows that it doesn't work the way you might expect.

533
00:34:06,000 --> 00:34:08,000
So let me take one example.

534
00:34:08,000 --> 00:34:12,000
Most people think that if you add noise to a neural net, it's going to work worse.

535
00:34:12,000 --> 00:34:17,000
If, for example, each time you put a training example through,

536
00:34:17,000 --> 00:34:23,000
you make half of the neurons be silent, it'll work worse.

537
00:34:23,000 --> 00:34:28,000
Actually, we know it'll generalize better if you do that.

538
00:34:28,000 --> 00:34:33,000
And you can demonstrate that in a simple example.

539
00:34:33,000 --> 00:34:35,000
That's what's nice about computer simulation.

540
00:34:35,000 --> 00:34:39,000
You can show this idea you had that adding noise is going to make it worse

541
00:34:39,000 --> 00:34:42,000
and dropping out half the neurons will make it work worse,

542
00:34:42,000 --> 00:34:44,000
which you will in the short term.

543
00:34:44,000 --> 00:34:47,000
But if you train it like that, in the end it'll work better.

544
00:34:47,000 --> 00:34:49,000
You can demonstrate that with a small computer program

545
00:34:49,000 --> 00:34:51,000
and then you can think hard about why that is

546
00:34:51,000 --> 00:34:56,000
and how it stops big, elaborate co-adaptations.

547
00:34:56,000 --> 00:34:59,000
But I think that that's my method of working.

548
00:34:59,000 --> 00:35:02,000
Find something that sounds suspicious and work on it

549
00:35:02,000 --> 00:35:06,000
and see if you can give a simple demonstration of why it's wrong.

550
00:35:06,000 --> 00:35:08,000
What sounds suspicious to you now?

551
00:35:08,000 --> 00:35:11,000
Well, that we don't use fast weight sounds suspicious.

552
00:35:11,000 --> 00:35:13,000
That we only have these two timescales.

553
00:35:13,000 --> 00:35:16,000
That's just wrong. That's not at all like the brain.

554
00:35:16,000 --> 00:35:20,000
And in the long run, I think we're going to have to have many more timescales.

555
00:35:20,000 --> 00:35:21,000
So that's an example now.

556
00:35:21,000 --> 00:35:26,000
And if you had your group of students today and they came to you

557
00:35:26,000 --> 00:35:29,000
and they said the hamming question that we talked about previously,

558
00:35:29,000 --> 00:35:32,000
what's the most important problem in your field?

559
00:35:32,000 --> 00:35:36,000
What would you suggest that they take on and work on next?

560
00:35:36,000 --> 00:35:38,000
We spoke about reasoning, timescales.

561
00:35:38,000 --> 00:35:42,000
What would be sort of the highest priority problem that you'd give them?

562
00:35:42,000 --> 00:35:47,000
For me right now, it's the same question I've had for the last like 30 years or so,

563
00:35:47,000 --> 00:35:51,000
which is, does the brain do back propagation?

564
00:35:51,000 --> 00:35:53,000
I believe the brain is getting gradients.

565
00:35:53,000 --> 00:35:57,000
If you don't get gradients, your learning is just much worse than if you do get gradients.

566
00:35:57,000 --> 00:35:59,000
But how is the brain getting gradients?

567
00:35:59,000 --> 00:36:04,000
And is it somehow implementing some approximate version of back propagation?

568
00:36:04,000 --> 00:36:06,000
Or is it some completely different technique?

569
00:36:06,000 --> 00:36:08,000
That's a big open question.

570
00:36:08,000 --> 00:36:12,000
And if I kept on doing research, that's what I would be doing research on.

571
00:36:12,000 --> 00:36:18,000
And when you look back at your career now, you've been right about so many things,

572
00:36:18,000 --> 00:36:24,000
but what were you wrong about that you wish you sort of spent less time pursuing a certain direction?

573
00:36:24,000 --> 00:36:25,000
Okay, those are two separate questions.

574
00:36:25,000 --> 00:36:27,000
One is what were you wrong about?

575
00:36:27,000 --> 00:36:30,000
And two, do you wish you'd spent less time on it?

576
00:36:30,000 --> 00:36:35,000
I think I was wrong about Boltzmann machines, and I'm glad I spent a long time on it.

577
00:36:35,000 --> 00:36:39,000
There are much more beautiful theory of how you get gradients than back propagation.

578
00:36:39,000 --> 00:36:42,000
Back propagation is just ordinary and sensible, and it's just a chain rule.

579
00:36:42,000 --> 00:36:46,000
Boltzmann machines is clever, and it's a very interesting way to get gradients.

580
00:36:46,000 --> 00:36:51,000
And I would love for that to be how the brain works, but I think it isn't.

581
00:36:51,000 --> 00:36:57,000
Did you spend much time imagining what would happen post these systems developing as well?

582
00:36:57,000 --> 00:37:01,000
Did you ever have an idea that, okay, if we could make these systems work really well,

583
00:37:01,000 --> 00:37:06,000
we could democratise education, we could make knowledge way more accessible,

584
00:37:06,000 --> 00:37:10,000
we could solve some tough problems in medicine,

585
00:37:10,000 --> 00:37:14,000
or was it more to you about understanding the brain?

586
00:37:14,000 --> 00:37:20,000
Yes, I sort of feel scientists ought to be doing things that are going to help society,

587
00:37:20,000 --> 00:37:23,000
but actually, that's not how you do your best research.

588
00:37:23,000 --> 00:37:26,000
You do your best research when it's driven by curiosity.

589
00:37:26,000 --> 00:37:30,000
You just have to understand something.

590
00:37:30,000 --> 00:37:35,000
Much more recently, I've realised these things could do a lot of harm as well as a lot of good,

591
00:37:35,000 --> 00:37:39,000
and I've become much more concerned about the effects they're going to have on society.

592
00:37:39,000 --> 00:37:41,000
But that's not what was motivating me.

593
00:37:41,000 --> 00:37:45,000
I just wanted to understand how on earth can the brain learn to do things?

594
00:37:45,000 --> 00:37:46,000
That's what I want to know.

595
00:37:46,000 --> 00:37:47,000
And I sort of failed.

596
00:37:47,000 --> 00:37:52,000
As a side effect of that failure, we got some nice engineering.

597
00:37:52,000 --> 00:37:55,000
Yeah, it was a good failure for the world.

598
00:37:55,000 --> 00:37:59,000
If you take the lens of the things that could go really right,

599
00:37:59,000 --> 00:38:03,000
what do you think are the most promising applications?

600
00:38:03,000 --> 00:38:08,000
I think healthcare is clearly a big one.

601
00:38:08,000 --> 00:38:14,000
With healthcare, there's almost no end to how much healthcare society can absorb.

602
00:38:14,000 --> 00:38:19,000
If you take someone old, they could use five doctors full time.

603
00:38:19,000 --> 00:38:25,000
So when AI gets better than people are doing things,

604
00:38:25,000 --> 00:38:30,000
you'd like it to get better in areas where you could do with a lot more of that stuff,

605
00:38:30,000 --> 00:38:32,000
and we could do with a lot more doctors.

606
00:38:32,000 --> 00:38:35,000
If everybody had three doctors of their own, that would be great,

607
00:38:35,000 --> 00:38:38,000
and we're going to get to that point.

608
00:38:38,000 --> 00:38:41,000
So that's one reason why healthcare is good.

609
00:38:41,000 --> 00:38:46,000
There's also just in new engineering, developing new materials, for example,

610
00:38:46,000 --> 00:38:50,000
for better solar panels or for superconductivity,

611
00:38:50,000 --> 00:38:55,000
or for just understanding how the body works.

612
00:38:55,000 --> 00:38:57,000
There's going to be huge impacts there.

613
00:38:57,000 --> 00:38:59,000
Those are all going to be good things.

614
00:38:59,000 --> 00:39:03,000
What I worry about is bad actors using them for bad things.

615
00:39:03,000 --> 00:39:10,000
We've facilitated people like Putin or Xi or Trump using AI for killer robots

616
00:39:10,000 --> 00:39:13,000
for manipulating public opinion or for mass surveillance.

617
00:39:13,000 --> 00:39:15,000
And those are all very worrying things.

618
00:39:15,000 --> 00:39:22,000
Are you ever concerned that slowing down the field could also slow down the positives?

619
00:39:22,000 --> 00:39:23,000
Oh, absolutely.

620
00:39:23,000 --> 00:39:28,000
And I think there's not much chance that the field will slow down,

621
00:39:28,000 --> 00:39:30,000
partly because it's international,

622
00:39:30,000 --> 00:39:33,000
and if one country slows down, the other countries aren't going to slow down.

623
00:39:33,000 --> 00:39:37,000
So there's a race clearly between China and the US,

624
00:39:37,000 --> 00:39:39,000
and neither is going to slow down.

625
00:39:39,000 --> 00:39:43,000
So yeah, I mean, there was this partition saying we should slow down for six months.

626
00:39:43,000 --> 00:39:46,000
I didn't sign it just because I thought it was never going to happen.

627
00:39:46,000 --> 00:39:49,000
I maybe should have signed it because even though it was never going to happen,

628
00:39:49,000 --> 00:39:50,000
it made a political point.

629
00:39:50,000 --> 00:39:54,000
It's often good to ask for things you know you can't get just to make a point.

630
00:39:54,000 --> 00:39:56,000
But I don't think we're going to slow down.

631
00:39:56,000 --> 00:40:03,000
And how do you think that it will impact the AI research process having this assistance?

632
00:40:03,000 --> 00:40:05,000
I think it'll make it a lot more efficient.

633
00:40:05,000 --> 00:40:10,000
AI research will get a lot more efficient when you've got these assistance to help you program,

634
00:40:10,000 --> 00:40:15,000
but also help you think through things and probably help you a lot with equations too.

635
00:40:15,000 --> 00:40:19,000
Have you reflected much on the process of selecting talent?

636
00:40:19,000 --> 00:40:21,000
Has that been mostly intuitive to you?

637
00:40:21,000 --> 00:40:26,000
Like when Ilya shows up at the door, you feel this is smart guy, let's work together.

638
00:40:26,000 --> 00:40:30,000
So for selecting talent, sometimes you just know.

639
00:40:30,000 --> 00:40:34,000
So after talking to Ilya for not very long, he seemed very smart.

640
00:40:34,000 --> 00:40:37,000
And then talking to him a bit more, he clearly was very smart

641
00:40:37,000 --> 00:40:41,000
and had very good intuitions as well as being good at math.

642
00:40:41,000 --> 00:40:42,000
So that was a no-brainer.

643
00:40:42,000 --> 00:40:47,000
There's another case where I was at a NIPS conference.

644
00:40:47,000 --> 00:40:53,000
We had a poster and someone came up and he started asking questions about the poster.

645
00:40:53,000 --> 00:40:57,000
And every question he asked was a sort of deep insight into what we'd done wrong.

646
00:40:57,000 --> 00:41:00,000
And after five minutes, I offered him a postdoc position.

647
00:41:00,000 --> 00:41:05,000
That guy was David McKay, who was just brilliant and it's very sad he died,

648
00:41:05,000 --> 00:41:09,000
but he was very obvious you'd want him.

649
00:41:09,000 --> 00:41:11,000
Other times it's not so obvious.

650
00:41:11,000 --> 00:41:15,000
And one thing I did learn was that people are different.

651
00:41:15,000 --> 00:41:19,000
There's not just one type of good student.

652
00:41:19,000 --> 00:41:25,000
So there's some students who aren't that creative but are technically extremely strong

653
00:41:25,000 --> 00:41:27,000
and will make anything work.

654
00:41:27,000 --> 00:41:31,000
There's other students who aren't technically strong but are very creative.

655
00:41:31,000 --> 00:41:34,000
Of course you want the ones who are both, but you don't always get that.

656
00:41:34,000 --> 00:41:39,000
But I think actually in the lab you need a variety of different kinds of graduate students.

657
00:41:39,000 --> 00:41:44,000
But I still go with my gut intuition that sometimes you talk to somebody

658
00:41:44,000 --> 00:41:49,000
and they just get it and those are the ones you want.

659
00:41:49,000 --> 00:41:54,000
What do you think is the reason for some folks having better intuition?

660
00:41:54,000 --> 00:42:01,000
Do they just have better training data than others or how can you develop your intuition?

661
00:42:01,000 --> 00:42:05,000
I think it's partly they don't stand for nonsense.

662
00:42:05,000 --> 00:42:09,000
So here's a way to get bad intuitions, believe everything you're told.

663
00:42:09,000 --> 00:42:11,000
That's fatal.

664
00:42:11,000 --> 00:42:14,000
You have to be able to, I think here's what some people do.

665
00:42:14,000 --> 00:42:17,000
They have a whole framework for understanding reality.

666
00:42:17,000 --> 00:42:23,000
And when someone tells them something, they try and sort of figure out how that fits into their framework.

667
00:42:23,000 --> 00:42:26,000
And if it doesn't, they just reject it.

668
00:42:26,000 --> 00:42:29,000
And that's a very good strategy.

669
00:42:29,000 --> 00:42:36,000
People who try and incorporate whatever they're told end up with a framework that's sort of very fuzzy

670
00:42:36,000 --> 00:42:40,000
and sort of can believe everything and that's useless.

671
00:42:40,000 --> 00:42:47,000
So I think actually having a strong view of the world and trying to manipulate incoming facts to fit in with your view.

672
00:42:47,000 --> 00:42:55,000
Obviously it can lead you into deep religious belief in fatal flaws and so on, like my belief in Boltzmann machines.

673
00:42:55,000 --> 00:42:57,000
But I think that's the way to go.

674
00:42:57,000 --> 00:43:00,000
If you've got good intuitions, you should trust them.

675
00:43:00,000 --> 00:43:06,000
If you've got bad intuitions, it doesn't matter what you do, so you might as well trust them.

676
00:43:06,000 --> 00:43:09,000
Very good point.

677
00:43:09,000 --> 00:43:16,000
When you look at the types of research that's being done today,

678
00:43:16,000 --> 00:43:23,000
do you think we're putting all of our eggs in one basket and we should diversify our ideas a bit more in the field?

679
00:43:23,000 --> 00:43:25,000
Or do you think this is the most promising direction?

680
00:43:25,000 --> 00:43:28,000
So let's go all in on it.

681
00:43:28,000 --> 00:43:35,000
I think having big models and training them on multimodal data, even if it's only to predict the next word,

682
00:43:35,000 --> 00:43:38,000
is such a promising approach that we should go pretty much all in on it.

683
00:43:38,000 --> 00:43:41,000
Obviously there's lots and lots of people doing it now.

684
00:43:41,000 --> 00:43:45,000
And there's lots of people doing apparently crazy things and that's good.

685
00:43:45,000 --> 00:43:50,000
But I think it's fine for most of the people to be following this path because it's working very well.

686
00:43:50,000 --> 00:43:56,000
Do you think that the learning algorithms matter that much, or is it just a scale?

687
00:43:56,000 --> 00:44:02,000
Are there basically millions of ways that we could get to human level in intelligence,

688
00:44:02,000 --> 00:44:06,000
or are there sort of a select few that we need to discover?

689
00:44:06,000 --> 00:44:11,000
Yes, so this issue of whether particular learning algorithms are very important,

690
00:44:11,000 --> 00:44:15,000
or whether there's a great variety of learning algorithms that will do the job,

691
00:44:15,000 --> 00:44:17,000
I don't know the answer.

692
00:44:17,000 --> 00:44:22,000
It seems to me though that by propagation there's a sense in which it's the correct thing to do.

693
00:44:22,000 --> 00:44:26,000
Getting the gradient so that you change a parameter to make it work better,

694
00:44:26,000 --> 00:44:31,000
that seems like the right thing to do, and it's been amazingly successful.

695
00:44:31,000 --> 00:44:36,000
There may well be other learning algorithms that are alternative ways of getting that same gradient,

696
00:44:36,000 --> 00:44:41,000
all that are getting the gradient to something else and that also work.

697
00:44:41,000 --> 00:44:48,000
I think that's all open and a very interesting issue now about whether there's other things you can try and maximize

698
00:44:48,000 --> 00:44:53,000
that will give you good systems, and maybe the brain's doing that because it's easier.

699
00:44:53,000 --> 00:45:00,000
But backprop is in a sense the right thing to do, and we know that doing it works really well.

700
00:45:00,000 --> 00:45:05,000
And one last question, when you look back at your decades of research,

701
00:45:05,000 --> 00:45:08,000
what are you most proud of? Is it the students? Is it the research?

702
00:45:08,000 --> 00:45:12,000
What makes you most proud of when you look back at your life's work?

703
00:45:12,000 --> 00:45:15,000
The learning algorithm for Boltzmann machines.

704
00:45:15,000 --> 00:45:19,000
So the learning algorithm for Boltzmann machines is beautifully elegant.

705
00:45:19,000 --> 00:45:27,000
It's maybe hopeless in practice, but it's the thing I enjoyed most developing that with Terry,

706
00:45:27,000 --> 00:45:32,000
and it's what I'm proudest of, even if it's wrong.

707
00:45:37,000 --> 00:45:41,000
What questions do you spend most of your time thinking about now?

708
00:45:41,000 --> 00:45:45,000
What should I watch on Netflix?

