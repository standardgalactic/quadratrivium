start	end	text
0	8000	Hello and welcome all of you to Math 2050 Applied Linear Algebra.
8000	14000	I think we'll start off by talking about just what is linear algebra.
14000	18000	You actually are familiar with it somewhat.
18000	22000	You've had courses in algebra, I'm sure.
22000	28000	In general, what we think of in particular for this course is that
28000	32000	linear algebra is a study of systems of linear equations.
32000	36000	So that begs the question, what's a linear equation?
36000	40000	Well, a linear equation is one of this form.
40000	46000	a1x1 plus a2x2 plus dot dot dot anxn equals b.
46000	50000	And here the x's are the variables.
50000	54000	The a's are called coefficients.
54000	58000	And b is the right hand side value.
58000	64000	Here all the a's and b are typically real numbers.
64000	70000	And then you ask, well, what's a real number?
70000	77000	Well, a real number is basically you can think of that as any number that you can find on a number line.
77000	85000	So that includes integers, rational numbers, irrational numbers,
85000	91000	negative numbers, positive numbers, zero, just about any number.
91000	93000	Or any number you can find on a number line.
93000	94000	It's a real number.
94000	99000	This is as opposed to complex or imaginary numbers.
99000	101000	Those would not be real numbers.
101000	105000	So for our purposes, we'll not be dealing with complex numbers.
105000	111000	We'll stick to just real numbers.
111000	122000	In our equations, the linear part means that we don't allow nonlinear expressions of the variables.
122000	129000	So terms like x1 squared or x1 times x2, those type terms are nonlinear.
129000	136000	And so we don't allow those type expressions.
136000	141000	So here's a couple of sample linear equations.
141000	144000	Those are just typical.
144000	146000	The first one has two variables here.
146000	149000	Here's one with three variables.
149000	153000	The first one, you'll recognize, has two variables.
153000	159000	So that just defines a line in the plane, in the xy plane here.
159000	160000	Got three variables.
160000	165000	So that actually defines a plane in three dimensions.
165000	171000	So we'll talk more about that as we go along.
171000	174000	So we're back to what is linear algebra.
174000	177000	So we've talked about what's a linear equation.
177000	182000	And linear algebra is a study of systems of linear equations.
182000	184000	So what's a system?
184000	187000	That just means you have multiple equations.
187000	194000	So for example, let's go to the drawing board.
194000	199000	Here's a system of two equations and two unknowns.
199000	203000	And all of you have had college algebra.
203000	206000	So you're familiar with the system of this form.
206000	215000	And you probably learned a couple of different ways to solve this system when you were in algebra.
215000	220000	And so we're just going to talk about that first off here.
220000	225000	If I asked you how would you solve this system, then you might say,
225000	231000	well, I'd use substitution because that seems like a way to go.
231000	235000	If you look here, you've got the x2 term.
235000	237000	You can solve this equation for x2.
237000	239000	So let's do that.
239000	247000	So from this equation, I can get x2 equals 28 minus 4x1.
247000	252000	And then I substitute this expression for x2 back into the first equation.
252000	262000	So that gives me 2x1 plus 3 times this stuff here, 28 minus 4x1.
262000	265000	And that equals 24.
265000	278000	So then I simply 2x1 plus 3 times 28 is 84 minus 12x1 equals 24.
278000	287000	So that gives me, it looks like negative 10x1 equals 24 minus 84 would be negative 60.
287000	291000	So it looks like x1 is equal to 6.
291000	296000	And then if x1 is 6, we can go back and plug in here to figure out what x2 is.
296000	303000	So x2 is going to be 28 minus 4 times 6.
303000	307000	So x2 is equal to 4.
307000	311000	So, oops, back up here.
311000	322000	So x, there we go, x2 equals 4, x1 equals 6 is our solution.
322000	325000	So that's one way to solve this system.
325000	332000	That's using the method of substitution.
332000	344000	Another method that's sometimes used is to go back and look at the system and try to eliminate one of the variables
344000	351000	by multiplying one of the equations, or maybe even both the equations,
351000	357000	by constant and then adding or subtracting one from the other.
357000	361000	So I'm going to try that method here.
361000	368000	So I'm going to multiply this first equation by negative 2.
368000	372000	And I'll just write what I get down here.
372000	377000	So negative 2 times 2x1 gives me negative 4x1.
377000	385000	And then negative 6x2, then negative 2 times 24 would be negative 48.
385000	391000	And then if I add these two together, these two that I have here,
391000	394000	then notice that the x1's cancel out.
394000	395000	So they're gone.
395000	397000	I've eliminated x1 in here.
397000	405000	If I add, I get negative 5x2 equals negative 20.
405000	410000	So that says x2 equals 4.
410000	417000	And then to get x1, I can substitute that back into either of these equations.
417000	423000	And we already know that x1 is 6, so I won't go through that process.
423000	424000	So there we go.
424000	425000	We have our solution.
425000	431000	And that's using the method of elimination.
431000	435000	You systematically eliminate variables.
435000	442000	So those are the two methods that you probably learned back in algebra class
442000	446000	for solving the system.
446000	451000	Let's take a look at what we're doing graphically.
451000	453000	Let's take a graphical look at it.
453000	460000	So I go back to my equations and I draw them on a graph.
460000	465000	Let's see if I can come over here.
465000	471000	And I'll try to do this.
471000	472000	All right.
472000	478000	So if I graph my equations where this is going to be my x1 axis,
478000	482000	this will be my x2 axis.
482000	483000	Let's see.
483000	487000	If I look at the first equation, if x1 is 0,
487000	492000	well, actually I'm going to start with this one because if x1 is 0 here,
492000	493000	then x2 is 28.
493000	496000	So that gets us way up here.
496000	498000	So there's 28.
498000	504000	So I've got one point right there at 0, 28.
504000	509000	Then if x2 is 0, we get x1 equals 7.
509000	514000	So I'm going to scoot this over just a little bit.
514000	517000	Just give me a little more room here.
517000	520000	Let me get this out.
520000	521000	OK.
521000	528000	So it looks like I want to try to get my scale as best I can.
528000	532000	So it looks like this would be about 7.
532000	537000	And so my other point is here at 7, 0.
537000	543000	So I've got one line that comes like this.
543000	548000	And let me see if I can do a better job drawing that.
548000	557000	Here not only I'm not going to get a little better angle on that with my computer here.
557000	558000	OK.
558000	561000	Well, that's not going to get either.
561000	565000	Let's try one more time.
565000	569000	Looks like I am not going to be able to make this work.
569000	570000	Let's see.
570000	572000	One more time.
572000	573000	OK.
573000	574000	I'm going to just call that good.
574000	577000	Let's call this 7 right here.
577000	578000	All right.
578000	581000	So then let's do, let's look at the other one.
581000	586000	The other equation is I've got this one.
586000	590000	If x1 is 0, it looks like x2 is 8.
590000	594000	So that's going to be along about right there.
594000	600000	And if x2 is 0, it looks like x1 is 12.
600000	604000	So that's going to be somewhere along about right here.
604000	608000	Let me try once again, see if I can make a line.
608000	611000	That one's not terrible there.
611000	612000	All right.
612000	617000	So this is about 7 right there.
617000	619000	Let's take that away.
619000	620000	All right.
620000	624000	So if we look, my scale is better, be better to see.
624000	629000	But along about there, that's the point where those lines intersect.
629000	632000	And so that's the point 6, 4.
632000	634000	So my scale is better.
634000	637000	It would look better to get the general idea.
637000	642000	So what we're looking at here is we've got this system.
642000	648000	And we want to find x1 and x2 that satisfy both those equations.
648000	651000	So since each of those equations represents a line,
651000	656000	then when we're looking for a point that satisfies both,
656000	658000	then that means it's a point that's on both lines.
658000	663000	So that's the point where the lines intersect.
663000	664000	OK.
664000	672000	So in this example, we ended up with a unique solution.
672000	673000	All right.
673000	674000	A unique solution.
674000	682000	That means exactly one solution.
682000	683000	OK.
683000	684000	Now let's think.
684000	687000	Are there any other possibilities?
687000	692000	No matter what your system is, you're always going to have exactly one solution.
692000	696000	I think you know that that is not the case.
696000	699000	So what are the possibilities?
699000	701000	You don't have exactly one solution.
701000	704000	How about do you have a solution every time?
704000	708000	And you probably know that the answer is no.
708000	714000	And that happens when you have a situation like this.
714000	715000	There's one line.
715000	716000	There's another.
716000	718000	They're parallel.
718000	720000	And so they never intersect.
720000	724000	And so this case, you have no solution.
724000	727000	No solution in that case.
727000	728000	OK.
728000	731000	So we've got zero solutions.
731000	732000	You have one solution.
732000	733000	How about two?
733000	737000	Can you come up with a system where you have two solutions?
737000	739000	Let's think on that.
739000	740000	Two solutions.
740000	747000	Now in my class one day, I had somebody say, well, what if you drew another line here?
747000	750000	So you had a line like this, right?
750000	753000	Then you got the intersect there, the intersect there.
753000	756000	So there's two solutions, right?
756000	763000	And the answer is, well, no, because here you've got three lines.
763000	767000	And so if you were trying to solve a system with these three lines,
767000	773000	then that means that you need to find a point where all three intersect at the same time.
773000	782000	So in this case, there's still no solution because there's no point at which all three lines intersect.
782000	785000	I've also had students say, well, hey, what if you had a parabola?
785000	787000	So you had a case like this.
787000	788000	There's a parabola.
788000	790000	You got a line going through it.
790000	793000	There's the intersect right there, intersect right there.
793000	796000	So you got two solutions.
796000	803000	Now this is legitimate because you got two functions in the intersect at these two places.
803000	809000	The problem is that this parabola is not a linear function, right?
809000	816000	The parabola is quadratic, right?
816000	819000	Not linear.
819000	824000	So that case doesn't work either.
824000	839000	So it turns out that if you have a system of linear equations and you don't have no solution,
839000	845000	you don't have exactly one solution, then you must have an infinite number of solutions, right?
845000	856000	So infinite number of solutions.
856000	863000	And in two dimensions, that's a little bit tough to come up with a good example.
863000	865000	The best you can do is two dimensions.
865000	873000	It is like one like this where you have something like 2x1 plus 3x2 equals 10.
873000	878000	And then maybe 4x1 plus 6x2 equals 20.
878000	883000	Now on the surface, it looks like you got two equations.
883000	889000	But when you go to graph this, what do you find?
889000	895000	Well, you find that this is actually the same equation because I wrote it so that the second one was just a multiple of the first.
895000	900000	So when you draw it, if you were drawing the graph, whatever it might look like,
900000	905000	you graph one of the lines and then you graph the other one and it sits right on top of the first one.
905000	909000	So that, in that case, any point on the line would be a solution.
909000	915000	And so there would be an infinite number of solutions.
915000	924000	Okay, so just to recap, let's go back to our slide.
924000	927000	Let's go to the next one.
927000	930000	So these are your possibilities.
930000	938000	A system of equations has either no solution, exactly one solution, in which case we say it's a unique solution,
938000	942000	or it has an infinite number of solutions.
942000	946000	And a little more terminology for you.
946000	949000	We say a system is consistent.
949000	956000	Consistence, the word we'll use a lot, so a system is consistent if it has at least one solution.
956000	960000	So that means either unique or an infinite number, but it has at least one solution.
960000	962000	It's consistent.
962000	964000	Otherwise, we say it's inconsistent.
964000	971000	So inconsistent means that it has no solution at all.
971000	982000	Okay, so I think we'll stop there for this one and we'll pick up with this on the next video.
982000	984000	Okay, we're back now.
984000	988000	We're talking about solving systems of equations.
988000	997000	And we need a systematic method because probably as you can recall from your days in college algebra,
997000	1002000	once you get up to even a three by three system, then things start to get a little messy.
1002000	1008000	And so we're going to talk now about how to, a systematic method.
1008000	1013000	Okay, a method that will work no matter what size your system, no matter how many rows, how many columns,
1013000	1016000	or how many variables, how many equations.
1016000	1020000	It's an algorithm that will work no matter what.
1020000	1026000	Before we do that, I want to go back and look at that system that we were talking about earlier,
1026000	1030000	which I have on my notepad here.
1030000	1035000	So remember, I just want to refresh your memory about what we did there.
1035000	1046000	The second way we used to solve this system, if you recall, was we decided to multiply this equation by negative two.
1046000	1051000	And then we added that to the second equation.
1051000	1054000	So let's just go through that quickly again.
1054000	1062000	So we ended with negative 4x1 minus 6x2 equals negative 48.
1062000	1065000	And then we added that.
1065000	1068000	So the x1's canceled out.
1068000	1077000	Then we ended up with negative 5x2 here equals negative 20.
1077000	1080000	So x2 is equal to 4.
1080000	1086000	And then we didn't actually do it because we'd done it before, but I'll go through.
1086000	1089000	At this point to find x1, you have to plug back in.
1089000	1091000	You can plug into either one.
1091000	1093000	Let's just go back to the first one.
1093000	1098000	First equation will have 2x1 plus 3 times x2.
1098000	1100000	And we know that x2 is 4.
1100000	1103000	So we plug that in equals 24.
1103000	1108000	So we get 2x1 equals, looks like 12.
1108000	1110000	So x1 equals 6.
1110000	1118000	So there's our solution that we found in the previous video.
1119000	1126000	To do this in a systematic way, we set up a matrix.
1126000	1133000	And a matrix is just a rectangular array of numbers.
1133000	1145000	So in this case, we would be interested in this matrix where I take the coefficients.
1145000	1147000	So I just write it like that.
1147000	1151000	So the first row here represents the first equation.
1151000	1154000	The second row represents the second equation.
1154000	1163000	This matrix is called an augmented matrix.
1163000	1174000	And it's called augmented because it has this right hand side column included.
1174000	1181000	A lot of times we're only interested in this matrix, which we call the coefficient matrix.
1181000	1190000	Since it includes only the coefficients of the variables.
1190000	1192000	So that's coefficient matrix.
1192000	1200000	If we augment or add on the right hand side, then we get the augmented matrix, which is this one.
1200000	1205000	And this is actually the one that we'll work on most of the time when we're solving.
1205000	1210000	If we want to solve a particular system, then we would work with the augmented matrix.
1210000	1213000	Now, what do we do with the augmented matrix?
1213000	1225000	Well, it turns out that there are three row operations, three operations in our toolbox that we can perform on this matrix in order to solve the system.
1225000	1232000	So to do that, let's go back to our slides here.
1232000	1237000	And here we'll talk about our elementary row operations.
1237000	1241000	So number one is to interchange or swap two rows.
1241000	1254000	So that since a row in a matrix represents an equation, that just means we're reordering the equations, which as you know has no effect on the solution.
1254000	1261000	We could also multiply all the entries in a particular row by a nonzero constant.
1261000	1270000	No, I can't multiply by zero because that would be essentially throwing out that equation and that wouldn't work, but you can multiply by any nonzero constant.
1270000	1275000	And the other is sounds kind of complicated, but it's really not.
1275000	1281000	And really this third one is the one that you'll do 99% of the time.
1281000	1284000	And it is the replacement operation.
1284000	1295000	It's where you replace one row by the sum of itself and that should be a multiple of another row.
1295000	1301000	So you replace one row by the sum of itself and a multiple of another row.
1301000	1304000	It's an error right there.
1304000	1307000	So let's talk about how you do that.
1307000	1310000	Let's go back to our example.
1310000	1314000	And that operation is actually what we did here.
1314000	1326000	If you go back and look, you can think of it as we were replacing the second equation by the sum of itself and a multiple of the first row.
1326000	1336000	So we multiply the first row, took a multiple of the first row, added it to the second, and so we ended up with this new equation right here.
1337000	1342000	So we're going to do the same thing except in matrix form.
1342000	1347000	And I like to use a shorthand when I'm doing these operations.
1347000	1352000	And so for the replacement operation, I write it like this.
1352000	1359000	I'm going to multiply negative two times row one and add that to row two.
1359000	1362000	So negative two times row one, added to row two.
1362000	1364000	Now this doesn't change row one.
1364000	1370000	So I'm just going to write that just as I normally would.
1370000	1372000	No change there.
1372000	1374000	But row two is going to change.
1374000	1380000	So I'm going to multiply each entry in row one by negative two and add it to the corresponding entry in row two.
1380000	1384000	So negative two times two is negative four plus four.
1384000	1386000	That gives me zero.
1386000	1390000	So I've eliminated x one from this row.
1390000	1396000	Then negative two times three is negative six plus one is negative five.
1396000	1401000	Okay, so just a reminder there, how did I get that?
1401000	1407000	I multiplied negative two times three and added it to one.
1407000	1411000	So negative two times this entry plus one.
1411000	1413000	Then the same thing in the third column.
1413000	1421000	So negative two times negative 24 is negative 48 plus 28 is negative 20.
1421000	1426000	So here I'll get negative 20 and just make that clear.
1426000	1432000	That's negative two times 24 plus 28.
1432000	1438000	Okay, and so you can see if you wrote out the equation that corresponded to this row,
1438000	1445000	it looks exactly like this equation here that we ended up with.
1445000	1452000	And so at this point, now I have in my second row only one variable and so I can solve.
1452000	1459000	So I would say, okay, from row two I get negative five x two equals negative 20.
1459000	1463000	So that means x two equals four.
1463000	1465000	Okay, just like I did before.
1465000	1471000	And then we do something a little more space here.
1471000	1483000	Then we do a process called back substitution.
1483000	1489000	Okay, and that just means you back up to the next row and you substitute.
1489000	1493000	So if we back up to the next row, then that would be backing up to the first row.
1493000	1497000	And there I've got two x one plus three x two.
1497000	1499000	But now I know what x two is.
1499000	1500000	It's four.
1500000	1502000	So that's the substitution part.
1502000	1505000	Okay, so I plug that in equals 24.
1505000	1509000	And we know that we end up with x one equals six at this point.
1509000	1521000	Okay, so here we solve the system using elementary row operations and then using back substitution.
1521000	1527000	Let's see, just one more note on why this is okay to do this.
1527000	1532000	Let's go back to our slides.
1532000	1535000	A little more terminology for you.
1535000	1538000	We say two matrices are row equivalent.
1538000	1545000	If there is a sequence of elementary row operations that transforms one matrix into the other.
1545000	1553000	So the way I usually say that is they're row equivalent if you can get from one to the other by doing elementary row operations.
1553000	1556000	Okay, doesn't mean they're equal.
1556000	1558000	Row equivalent does not mean the matrices are equal.
1558000	1563000	It just means that you can get from one to the other by doing elementary row operations.
1563000	1566000	Okay, and then here's the clincher.
1566000	1568000	That's why this works.
1568000	1572000	So remember our goal is to solve a system.
1572000	1585000	And so we need to know that when we do these row operations, we're not changing the solution or the solution set.
1585000	1589000	Okay, and so this statement gives us that assurance.
1589000	1596000	It says if the augmented matrices of two systems are row equivalent, then the two systems have the same solution set.
1596000	1599000	So that means we can start off with one matrix.
1599000	1615000	We can do row operations and get to another one that we can solve easily and know that the solution that we get based on the last matrix will be the same for the first one.
1615000	1622000	So that's why we can do these row operations in order to solve a system of equations.
1622000	1629000	This leads us to two questions.
1629000	1635000	The first one is, is the system consistent?
1635000	1639000	Does it have at least one solution?
1639000	1650000	And if that's so, if it's consistent, then is that solution unique or are there an infinite number of solutions?
1650000	1653000	So we'll talk about how to determine.
1653000	1655000	Number one is the system consistent.
1655000	1663000	Number two, if it is, then how do you know if there's a unique solution or an infinite number of solutions?
1663000	1672000	Okay, we're going to stop here and I'll do one more video that has some problems worked out for you.
1672000	1676000	So that'll be next.
1676000	1684000	Okay, now I want to do a few sample problems from section 1.1 of the textbook.
1684000	1687000	So first let's look at number 14.
1687000	1692000	So in the problem, they're giving you a system of equations and asking you to solve it.
1692000	1698000	So I've already got it here in the augmented matrix form.
1698000	1706000	And so we just need to do elementary row operations in order to try to solve the system.
1706000	1718000	So the first thing we want to do is make sure that up here in the 1-1 position, that's the first row, first column, we've got a non-zero value if possible.
1718000	1721000	Well, it's already non-zero, so we don't need to do anything.
1721000	1728000	If this had been a zero, we would want to swap rows in order to get a non-zero value here.
1728000	1737000	Okay, we already have one, so then we can next turn our attention to creating zeros in the column below this leading entry.
1737000	1744000	A leading entry is the first non-zero value in a row, so we want to create zeros below it.
1744000	1747000	So we need to create a zero in this position here.
1747000	1755000	So we need to figure out what do we multiply by this one and add to negative one to get zero.
1755000	1760000	Clearly, multiply by one or don't multiply by anything.
1760000	1765000	We really just need to add the first row to the second and that will give us a zero here.
1765000	1776000	So I'm going to add row one to row two.
1776000	1781000	Now that does not change row one, so I'm just going to repeat it here.
1781000	1786000	And it doesn't change row three.
1786000	1793000	All right, so now row two, we've got a zero there.
1793000	1799000	Then we'll have negative three plus one gives us negative two.
1799000	1807000	Zero plus five is five and five plus two gives us seven here.
1807000	1813000	All right, so now we've got our leading entry zeros below it.
1813000	1818000	We're done with that column, so we move on to the next column and down a row.
1818000	1823000	So we want a non-zero element here, which we already have.
1823000	1828000	And normally we would just want a zero out below it.
1828000	1838000	But given that we have a one here, that's kind of a nicer number to work with.
1838000	1845000	So I'm going to actually swap rows two and three at this point.
1845000	1853000	Swap row two and row three.
1853000	1862000	So row one doesn't change.
1862000	1866000	Nero two is the old row three.
1866000	1871000	And the nero three is the old row two.
1871000	1873000	All right, so I've got that.
1873000	1879000	Now the reason I did that was because this just makes the arithmetic easier
1879000	1881000	and I don't have to deal with fractions.
1881000	1883000	Otherwise, if I left the negative two there, I'd have to figure out,
1883000	1887000	you know, what do I multiply by this to add to one to get zero?
1887000	1891000	And I would have to carry that out through the rest of that row.
1891000	1894000	So this is just easier arithmetic wise.
1894000	1899000	So now I've got a one here and I need to zero out underneath it.
1899000	1904000	So I need to multiply by two and add the negative two to get a zero here.
1904000	1915000	So my next operation is two times row two added to row three.
1915000	1921000	So row one again doesn't change.
1921000	1924000	Row two doesn't change.
1924000	1929000	And row three, let's get my zero there and I'll get two.
1929000	1934000	So we're here two times one plus five is seven.
1934000	1939000	Then we've got two times zero plus seven is seven.
1939000	1943000	Okay, now at this point I'm done with the second column
1943000	1949000	and I move over and I'm done with a matrix because now in the third row
1949000	1952000	I've eliminated all the variables except for one.
1952000	1962000	So from the third row, I get seven x three equals seven.
1962000	1965000	So I can solve for x three, it's going to be one.
1965000	1972000	And then I do the back substitution.
1972000	1978000	Alright, so that means back up, so back up to the second row
1978000	1982000	which looks like if I write down the equation corresponding to the second row
1982000	1986000	the x two plus x three equals zero.
1986000	1991000	But I know what x three is, so I plug that in.
1991000	1994000	And I've got x two equals negative one.
1994000	1999000	Okay, then I back up again to the first row.
1999000	2007000	The first row, that equation looks like x one minus three x two equals five.
2007000	2017000	I know what x two is, plug that in.
2017000	2020000	So a little more room here.
2020000	2025000	And so I get x one is equal to, let's see, that's a plus three.
2025000	2028000	So we're going over minus three, so it would be two.
2028000	2036000	So x one equals two, x two equals negative one, x three equals one.
2036000	2039000	There's a solution to my system.
2039000	2049000	So this system here had a unique solution, just one.
2049000	2054000	Alright, let's go on and look at another homework problem.
2054000	2059000	This time I want to go to number twelve.
2059000	2064000	Okay, so still in section one point one, problem number twelve.
2064000	2069000	Alright, so before we do that, let's look at one thing.
2069000	2071000	Before we do that, let's look at one other thing.
2071000	2076000	Suppose I have a system that looks like this.
2076000	2082000	Two x one plus three x two equals ten.
2082000	2089000	And how about four x one plus six x two equals twelve.
2089000	2095000	Now if you examine that a little bit, you're going to see that to get this second equation,
2095000	2098000	I just multiply the first one by two.
2098000	2101000	So two times two is four, two times three is six.
2101000	2104000	But then I didn't multiply this by two.
2104000	2109000	Actually I did, and then I didn't want to get that same value, so I changed it.
2109000	2116000	So just based on that analysis, it seems that this system should have no solution.
2116000	2124000	And if you were solving it using elimination, say, maybe you'd multiply,
2124000	2127000	maybe you didn't notice that it's not going to have a solution.
2127000	2131000	So you're just merely going along your way solving it.
2131000	2138000	So if we multiply three by negative two, we're going to get negative four x one minus six x two
2138000	2142000	equals negative two times ten, so negative twenty.
2142000	2146000	Then we'd add that, and here we get zero.
2146000	2151000	Here we get zero, so on the left we've got zero.
2151000	2153000	And on the right, what do we have?
2153000	2155000	Negative eight.
2155000	2159000	Well, zero can't equal negative eight, right?
2159000	2164000	So this is what tells us that this system has no solution.
2164000	2174000	We get to this point where we have zero on the left and something not zero on the right.
2174000	2178000	You've got a non-zero value over here on the right.
2178000	2193000	So if we had thrown that into a matrix, it would look like this.
2193000	2197000	Alright, so we do one row operation, same one we did up there.
2197000	2201000	Negative two, row one plus row two.
2201000	2205000	Alright, so row one doesn't change.
2205000	2212000	And row two, get negative two times two is negative four plus four gives us zero.
2212000	2217000	Negative two times three is negative six plus six gives me zero.
2217000	2224000	Negative two times ten is negative twenty plus twelve is negative eight.
2224000	2228000	Alright, so if you look at this, we're the same thing, right?
2228000	2233000	We've got all zeros.
2233000	2238000	And over here on the right, we've got something that's not zero.
2238000	2254000	Okay, this is the marker that you look for to indicate no solution.
2254000	2259000	Okay, all zeros on the left, something not zero on the right.
2259000	2261000	Okay, so keep that in mind.
2261000	2264000	We're going to be using that all semester.
2264000	2273000	Okay, so now let's go to number twelve.
2273000	2275000	Okay, still in section one point one.
2275000	2287000	So number twelve, one negative three, four negative four.
2287000	2292000	Negative seven, seven.
2292000	2302000	Negative eight, negative four, six, negative one, and seven.
2302000	2305000	Okay, and this is an augmented matrix.
2305000	2308000	So we start, we do our row operations.
2308000	2313000	So we look, we've got a non-zero value here, so we want to zero out underneath.
2313000	2320000	So the first thing I'm going to do is negative three times row one plus row two,
2320000	2324000	so that I can generate a zero there.
2324000	2330000	So I've got one, negative three, four, negative four.
2330000	2336000	Alright, first row and the third row don't change.
2336000	2339000	And this one, I'm going to get my zero there.
2339000	2346000	Alright, negative three times negative three is nine, minus seven gives me two there.
2346000	2353000	Negative three times four is negative twelve, plus seven is negative five.
2353000	2359000	Negative three times negative four is twelve, minus eight is four.
2359000	2361000	Okay, so I've got my zero there.
2361000	2364000	I need to keep working down, get a zero in this position.
2364000	2374000	So the next operation I'm going to do will be four times row one plus row three.
2374000	2380000	So again, row one doesn't change.
2380000	2384000	Row two doesn't change.
2384000	2391000	And row three, I get my zero there, so I'm going to put four, minus four gives me zero.
2391000	2398000	Four times negative three is negative twelve, plus six is negative six.
2398000	2403000	Four times four is sixteen, minus one is fifteen.
2403000	2411000	Four times negative four is negative sixteen, plus seven is negative nine.
2411000	2414000	Alright, so that first column is looking good.
2414000	2417000	Then I move over to the second column, second row.
2417000	2426000	I've got a non-zero value here, so I want a zero out underneath, so I need to work on that sixth there.
2426000	2434000	Okay, so I'm going to multiply row two by three, and add that to row three,
2434000	2440000	because three times two is six, minus six gives me the zero that I need right here.
2440000	2450000	So one, negative three, four, negative four, row two doesn't change.
2450000	2455000	And row three, I get my zero, it's already zero, so three times zero plus zero zero,
2455000	2459000	three times two is six, minus six, there's my new zero.
2459000	2465000	Three times negative five is negative fifteen, plus fifteen is zero.
2465000	2474000	Okay, and remember we're looking at this point, you can see we've got all zeros here.
2474000	2479000	Okay, and then we're ready to get the last position, and what do we have here?
2479000	2486000	Three times four is twelve, minus nine gives us three.
2486000	2492000	So what do we have here? We've got all zeros on the left,
2492000	2497000	and on the right we've got a value that's not zero.
2497000	2503000	So as we saw before, that means this system has no solution.
2503000	2513000	Okay, I want to do one more problem from this section.
2513000	2522000	And this one is, let's see, actually I'm going to get a couple more.
2522000	2529000	About, let's look at number, let's see, number twenty-two.
2529000	2535000	Okay, twenty-two, twenty-two looks like this.
2535000	2544000	Okay, we put it in the matrix form.
2544000	2550000	Okay, now the directions ask you to find the values of h, this parameter,
2550000	2553000	for which this system is consistent.
2553000	2562000	Okay, so to do that you want to just get an echelon form.
2562000	2564000	Actually we don't know what echelon form is yet.
2564000	2566000	Let's just do what we've been doing.
2566000	2573000	So we've got our non-zero value here, we zero out underneath it, so multiply by three.
2573000	2581000	So I'm going to do three times row one plus row two, so that gives me a zero there.
2581000	2584000	This doesn't change.
2584000	2588000	Okay, so three times two is six, minus six gives me zero here.
2588000	2595000	Three times negative three is negative nine plus nine gives me zero here.
2595000	2602000	And three times h plus five is just three times h plus five.
2602000	2607000	Alright, now at this point it looks just like the one previous, right?
2607000	2610000	Look back at the one we were just looking at.
2610000	2613000	We had all zeros and then something not zero.
2613000	2619000	Well here we have all zeros, guys, so that's significant.
2619000	2630000	But over here, we don't know what this is because we're asked to find h so that this system is consistent.
2630000	2649000	So we want this to, we want three h plus five must equal zero for the system to be consistent.
2649000	2656000	Okay, so that means that three h must equal negative five.
2656000	2659000	So h has to equal negative five-thirds.
2659000	2662000	Okay, so that's what we're looking for for this one.
2662000	2666000	Alright, because this has to equal zero for the system to be consistent.
2666000	2673000	Alright, because if this is not zero, we're like this case up here where there's no solution.
2673000	2676000	Alright, let's look at one more.
2676000	2683000	And I'm kind of going out of order here just because I like to kind of stick with the same thing.
2683000	2691000	So I want to back up to number 20 and it's a similar kind of question as 22.
2691000	2695000	But this time the h is not on the right side.
2695000	2698000	It's part of the coefficient matrix.
2698000	2700000	So it's a little bit different.
2700000	2704000	So again, we do our row operation.
2704000	2706000	So we got a non-zero here.
2706000	2708000	We want to zero out this position.
2708000	2713000	So two times row one plus row two.
2713000	2718000	So I've got one h negative three.
2718000	2720000	And I've got zero here.
2720000	2724000	Two times h plus four.
2724000	2730000	And two times negative three is negative six plus six is zero.
2730000	2735000	Alright, so I've got this situation.
2735000	2741000	The problem is to find h such that the system is consistent.
2741000	2749000	Well, system is consistent as long as we don't have a case like we had up here.
2749000	2759000	System is consistent as long as we don't have a row that looks like zero, zero, and then something not zero.
2759000	2761000	Well, notice that that can never happen.
2761000	2767000	We can never have zero, zero, something not zero because the right hand side is already zero.
2767000	2788000	Okay, so that means this system is always consistent.
2788000	2808000	Because we can never have a row of the form zero, zero, and then on the augmented side something not zero.
2808000	2813000	It can't happen in this case because the right hand side is a zero.
2813000	2820000	So this system is always consistent regardless of what h is.
2820000	2824000	Alright, so I think that's about it for section 1.1.
2824000	2834000	As always, keep in mind that you can call me up, email me anytime to let me know if you have any questions.
2834000	2839000	Alright, good luck with those homework problems.
2839000	2844000	Okay, today we're going to be talking about section 1.2.
2844000	2858000	And really the main thing I want to cover today is to talk about a specific form that we want to get a matrix into when we're trying to solve a system.
2858000	2867000	So we've kind of talked about that a little bit up to now, but at this point we're going to get very specific about that.
2867000	2872000	So the form that we're trying to get at is called echelon form.
2872000	2878000	And echelon form is defined by three properties.
2878000	2887000	So the first one of these is that if there are any rows of all zeros, they're at the bottom of the matrix.
2887000	2890000	So you swap rows to get those at the bottom.
2890000	2895000	The leading entry, remember that's the first non-zero entry.
2895000	2903000	So the leading entry in each row is in a column to the right of the leading entry in the row above it.
2903000	2907000	And I'll show you that in just a sec.
2907000	2913000	Now the third one is that all entries in a column below a leading entry are zeros.
2913000	2915000	So we've done that up to now.
2915000	2918000	You know, we get our leading entry and then we zero out below it.
2918000	2921000	So that's that idea.
2921000	2923000	Here's an example.
2923000	2930000	The black squares here represent non-zero values and they're the leading entries in each of these rows.
2930000	2935000	And so you can see here we have a leading entry and zeros underneath.
2935000	2939000	Here we have another leading entry, zeros underneath.
2939000	2943000	And notice that this leading entry is to the right of the one before it.
2943000	2948000	And here's another leading entry and it's to the right of the one that came before it.
2948000	2956000	Okay, so that second property is basically what gets you the stair step kind of structure in your matrix.
2956000	2961000	All right, see, here's another one.
2961000	2964000	This one's also an echelon form.
2964000	2967000	Leading entry here, zeros below it.
2967000	2972000	Another leading entry, zeros below it and it's to the right of the previous one.
2972000	2976000	And we have a row zeros and it's at the bottom.
2976000	2979000	So we satisfy all the criteria.
2979000	2987000	Now, sometimes you want to go a little bit farther and get your matrix in reduced echelon form.
2987000	2994000	So if it's in reduced echelon form, that means it's an echelon form plus it satisfies two more properties.
2994000	3000000	Okay, and these properties are that each leading entry is a one.
3000000	3004000	So you scale the rows to make each leading entry equal to one.
3004000	3009000	And that one is the only non-zero entry in its column.
3009000	3014000	So you zero out not only below it, but also above it.
3014000	3020000	So here's an example of a matrix that's in reduced echelon form.
3020000	3027000	Each leading entry is a one and it's the only non-zero entry in its column.
3027000	3035000	So we zeroed out below here, zeroed out above and below and zeroed out above in this case.
3035000	3039000	Alright, here's another one.
3039000	3046000	This one's in reduced echelon form because each leading entry we've only got two and they're each one.
3046000	3049000	And we've zeroed out above and below each one.
3049000	3051000	Zero zeros at the bottom.
3051000	3055000	Okay, so that's in reduced echelon form.
3055000	3062000	Now for purposes of just solving a system, typically you just want to get it in echelon form.
3062000	3070000	Although there will be one particular case that I'll talk about later in another section coming up
3070000	3074000	where you want to get it in reduced echelon form.
3074000	3076000	It just makes life easier then.
3076000	3080000	But for the most part at this point, echelon form is fine.
3080000	3086000	Now I'll show you a couple of things about these forms.
3086000	3092000	One thing to note is that the reduced echelon form of a matrix is unique.
3092000	3098000	So that means that no matter what sequence or row operations you use to get a matrix in reduced echelon form,
3098000	3101000	you'll end up at the same place.
3101000	3104000	Not so for the echelon form.
3104000	3106000	Okay, it's not unique.
3106000	3113000	It means that all of you could be working on the same matrix and end up with a different echelon form of the same matrix.
3113000	3120000	And that's because you can scale a row, okay, multiply a row by a constant and get a different echelon form.
3120000	3123000	Still be an echelon form, but it would be a different matrix.
3123000	3126000	Okay, so the reduced form is unique.
3126000	3129000	The echelon form is not.
3129000	3136000	Okay, so you either want to get your matrix in echelon or reduced echelon form when you're solving a system.
3136000	3142000	And the latter, get it in reduced echelon form, takes more work.
3142000	3145000	But when you do that, the solution is obvious.
3145000	3148000	And so let me show you what I mean by that.
3148000	3153000	Here we have an augmented matrix that's already in echelon form.
3153000	3156000	Notice leading entry, zero's underneath.
3156000	3158000	Another leading entry, zero's underneath.
3158000	3161000	And no rows of zero's.
3161000	3164000	Each leading entry is to the right of the one before it.
3164000	3167000	So it's an echelon form.
3167000	3174000	And then we do use back substitution to solve for the variable.
3174000	3176000	So start with the third row.
3176000	3181000	And equation for the third row, we get seven times x3 equals seven.
3181000	3183000	So we solve for x3.
3183000	3188000	Then we back substitute to solve for x2 in the second row.
3188000	3194000	So second row says negative two times x2 plus five x3 equals seven.
3194000	3196000	And we know that x3 is one.
3196000	3197000	We just solved for that.
3197000	3202000	So we plug that in here and we end up with x2 equals negative one.
3202000	3206000	And then we keep going back up again to the first row.
3206000	3212000	Equation for the first row looks like x1 minus three x2 equals five.
3212000	3215000	And we know what x2 is because we just solved for that.
3215000	3219000	So we plug it in and then we end up with x1 equals two.
3219000	3224000	So that's the method that we were using in section 1.1
3224000	3227000	and probably what you want to continue using for the most part.
3227000	3234000	However, if you choose to get your matrix in reduced echelon form.
3234000	3238000	So this is the same matrix, but in reduced echelon form.
3238000	3243000	Then notice that you end up with the solution values of your variables
3243000	3246000	just sitting in the right hand column.
3246000	3251000	Because if you look at each row, the first row says one times x1 equals two.
3251000	3255000	Second row, one times x2 equals negative one and so forth.
3255000	3261000	So if you go back up, you can see we got x1 is two, x2 negative one, one.
3261000	3263000	So two negative one, one.
3263000	3265000	And there's two negative one, one.
3265000	3269000	So that's one advantage of getting it in reduced echelon form.
3269000	3274000	Because then the solution is obvious and you don't have to actually do any back substitution.
3274000	3283000	So it's kind of your mileage may vary, whichever you like will be fine.
3283000	3290000	And at this point we'll stop here and we'll pick up here in the next video.
3290000	3294000	Okay, so picking up from the previous video.
3294000	3300000	Let's talk a little bit about a couple more pieces of terminology
3300000	3304000	that we'll be using extensively throughout the course.
3304000	3312000	Okay, so remember the leading entry is the first non-zero entry in a row
3312000	3315000	in a matrix in echelon form.
3315000	3322000	So it turns out that the leading entries in the reduced echelon form
3322000	3329000	and in any echelon form are in the same positions, I mean the same location in the matrix.
3329000	3332000	So here's the one we're just looking at.
3332000	3339000	There's a leading entry in the 1, 1 first row, first column, second row, second column, third row, third column.
3339000	3344000	And in the reduced echelon form, same positions in the matrix.
3344000	3349000	So it's convenient to talk about those positions as opposed to the actual value.
3349000	3355000	The leading entry is the actual value, but we really are more interested in the position.
3355000	3357000	And so we call that a pivot position.
3357000	3362000	So a pivot position is a position that contains a leading entry.
3362000	3370000	So the 1, 1, 2, 2, 3, 3 positions would be pivot positions in this matrix.
3370000	3375000	Okay, so as I said, the leading entry is the actual value, the pivot position is the location.
3375000	3379000	So the leading entry here is 1, it's in the first row, first column.
3379000	3381000	So that's the pivot position.
3381000	3388000	Here, this row, the leading entry is negative 2, it's in the second row, second column position.
3388000	3394000	Okay, and then one other term is pivot column.
3394000	3398000	Pivot column is a column that contains a pivot position.
3398000	3404000	So I'll be using the terms pivot position and pivot column until probably the last day of the semester.
3404000	3410000	So you will hear that quite a bit to make sure you understand what those are.
3410000	3412000	Okay, two more terms.
3412000	3418000	A basic variable is one that corresponds to a pivot column.
3418000	3423000	And a free variable is one that corresponds to a non-pivot column.
3423000	3434000	Okay, so for each column, except for the augmented column, there's a corresponding variable.
3434000	3441000	So if you look and there's a pivot column, that means that variable is basic.
3441000	3446000	If the column is not a pivot column, that means that variable is called free.
3447000	3454000	Okay, so let's look at how all this relates to solving the system of equation.
3454000	3457000	So I've got a tail of three matrices.
3457000	3459000	Here's number one.
3459000	3463000	Now notice this matrix.
3463000	3466000	It's already an echelon form, right?
3466000	3471000	Here's leading entry, zero's underneath, leading entry, zero's underneath, leading entry.
3471000	3474000	You don't need to worry about zero's underneath yet.
3474000	3478000	So we start the process of solving the system.
3478000	3480000	So we start with row three.
3480000	3483000	And here's the equation that corresponds to row three.
3483000	3487000	And if I simplify the left side, it's just zero.
3487000	3492000	So when it was zero equals seven, well, we know that can't happen.
3492000	3499000	So that is our clue that this system has no solution.
3499000	3505000	So if the echelon form of an augmented matrix has a row of this form,
3505000	3514000	where on the left you've got all zeros, then in the augmented column you have something not zero.
3514000	3517000	So all zeros on the left, not zero on the right.
3517000	3518000	That's what we have here.
3518000	3522000	All zeros on the left, not zero on the right.
3522000	3525000	Then the corresponding system has no solution.
3525000	3527000	Okay, so that's the marker.
3527000	3533000	That's what you need to look for to see if your system is consistent or not.
3533000	3536000	All right, matrix number two.
3536000	3541000	Change that first one just a little bit and I put a non-zero value here.
3541000	3550000	And so we've got leading entry, zero's underneath, leading entry, zero's underneath, another leading entry.
3550000	3552000	So this one's in echelon form.
3552000	3558000	So we solve, and this is the same one we solved earlier, so I'm not going to go back through that.
3558000	3561000	And this was our solution.
3561000	3564000	And so this one, what?
3564000	3566000	Has a unique solution.
3566000	3568000	The system is consistent.
3568000	3574000	It's consistent because it doesn't have a row where there's all zeros and then something not zero.
3574000	3577000	And it has no free variables.
3577000	3579000	That's the key here for unique solution.
3579000	3586000	Notice that there's a pivot position in every column of the coefficient part of the matrix.
3586000	3590000	So pivot position in column one, so x1 is basic.
3590000	3593000	Pivot position in column two, so x2 is basic.
3593000	3597000	Pivot position in column three, so x3 is basic.
3597000	3599000	So system is consistent.
3599000	3603000	All the variables are basic.
3603000	3608000	It has no free variables, and so we get a unique solution.
3609000	3612000	Alright, one more matrix.
3612000	3616000	Notice here that we've got a row of all zeros.
3616000	3621000	This one is in echelon form, but we've got a row of all zeros.
3621000	3623000	Let's look for just a second.
3623000	3626000	x1 here has a pivot position.
3626000	3629000	The column two has a pivot position.
3629000	3632000	Column three does not have a pivot position.
3632000	3635000	So that means that we have a free variable.
3635000	3638000	x1 and x2 are basic, but x3 is a free variable
3638000	3643000	because there's no pivot position in column three.
3643000	3648000	So if we try to solve it, here's what we get from row two.
3648000	3651000	Negative two x2 plus five x3 equals seven.
3651000	3656000	We solve for x2, and notice that it's written in terms of x3.
3656000	3659000	And then we back up to row one,
3659000	3663000	and that x1 minus three x2 equals five.
3663000	3668000	We substitute in for x2, and here's what we end up with for x1.
3668000	3672000	Notice it's also written in terms of x3.
3672000	3675000	Notice that that's all we've got, these two equations,
3675000	3678000	and so there's nothing constraining x3,
3678000	3681000	and that's because x3 is a free variable.
3681000	3685000	And so if we write our solution, here's what it looks like.
3685000	3690000	And in your book, that's what they call the general form of the solution.
3691000	3696000	So this is just for x1, x2, what we just computed, x3 is free.
3696000	3700000	This is the general form of the solution.
3700000	3705000	So notice that we can plug in any value for x3 that we want,
3705000	3709000	and any value that x3 produces a different solution.
3709000	3714000	So that means we have an infinite number of solutions to this system.
3714000	3719000	The easiest thing, if you want a specific solution,
3719000	3722000	the easiest thing is to just plug in zero for x3.
3722000	3726000	If you do that, then you get x1's negative eleven-halves,
3726000	3730000	x2 negative seven-halves, x3 is zero.
3730000	3737000	Or you could plug in x3 equals one and get another solution.
3737000	3740000	I always tell my students, plug in your favorite number.
3740000	3743000	So if your favorite number is pi, plug in pi for x3.
3743000	3748000	And if you do that, then here's what you get for x1 and x2.
3748000	3755000	So no matter what value of x3 you plug in, that generates a different solution,
3755000	3758000	and so you have an infinite number of solutions.
3758000	3764000	And so the bottom line is that if the system is consistent,
3764000	3767000	and that's a key, you've got to make sure it's consistent to start with,
3767000	3770000	and has at least one free variable,
3770000	3775000	then you will have an infinite number of solutions.
3775000	3778000	So let's recap.
3778000	3781000	When you're solving a system of equations,
3781000	3787000	number one, put the augmented matrix in echelon form.
3787000	3792000	Number two, if you have a row of this form,
3792000	3796000	and this form is, again, all zeros on the left,
3796000	3800000	something not zero in the augmented column,
3800000	3805000	then that means the corresponding system has no solution.
3805000	3808000	At that point, stop because you've got no solution.
3808000	3812000	If that's not the case, then that means the system is consistent.
3812000	3816000	So at that point, you look to see if there are free variables or not.
3816000	3823000	If there are no free variables, that means the system has a unique solution.
3823000	3827000	If it's consistent and there's at least one free variable,
3827000	3830000	then you have an infinite number of solutions.
3830000	3836000	So that's really your algorithm there for how to determine,
3836000	3840000	how to solve a system and determine which case it is.
3840000	3841000	Is it no solution?
3841000	3845000	Is it a unique solution or an infinite number of solutions?
3845000	3852000	Okay, and that is it for this video.
3852000	3858000	All right, we're going to start here with a graphical look at vectors.
3858000	3864000	So we've kind of seen, we've played around with matrices and augmented matrices
3864000	3866000	and solving systems.
3866000	3872000	But let's take a look at vectors from a graphical point of view.
3872000	3878000	Before we do that, though, we need to discuss some terminologies.
3878000	3880000	We'll start with Rn.
3880000	3885000	So you've seen this symbol, the fancy looking R.
3885000	3888000	That stands for the real numbers.
3888000	3895000	And when you see a superscript, like here with an N, excuse me,
3895000	3900000	that indicates that you're talking about vectors.
3900000	3905000	And so this, as I say here, you just read this as Rn.
3906000	3912000	Okay, so Rn is the set of all vectors having N components or N elements,
3912000	3914000	each of which is a real number.
3914000	3920000	So from a set point of view, you can look at it like this, set notation.
3920000	3927000	All vectors look like this N components, where each one of them is a real number.
3927000	3931000	A lot of times we'll just talk about R2.
3932000	3935000	Excuse me.
3935000	3939000	In R2, the two indicates vectors with two components.
3939000	3940000	So there you go.
3940000	3943000	And there's R3, similar sort of thing.
3943000	3947000	We've got three components where each one is a real number.
3947000	3951000	So for example, here are three vectors in R2.
3951000	3955000	You know they're in R2 because they're vectors with two components.
3955000	3958000	Each component's a real number.
3958000	3964000	And here they're vectors in R3 because they have three components.
3967000	3972000	There are two operations that we need to know how to perform on vectors.
3972000	3975000	The first of these is addition of vectors.
3975000	3981000	So if we have two vectors, X and Y, and Rn, then we can compute their sum.
3981000	3986000	As indicated here, okay, here's X, here's Y, both have N components.
3986000	3992000	To compute their sum, you just add components that are in the same position.
3992000	3998000	So first position X1 plus Y1 gives you the first position in the sum.
3998000	4003000	Then X2 plus Y2 and so forth to Xn plus Yn.
4003000	4010000	So for example, if here's X and here's Y, to get their sum, 0 plus 2 gives you the 2 here.
4010000	4016000	1 plus 4, 5, and negative 3 plus negative 2 gives you negative 5.
4016000	4023000	The other operation that we need to know how to perform on vectors is scalar multiplication.
4026000	4027000	Excuse me again.
4030000	4038000	Scalar multiplication of vectors is based on the idea that you have a scalar,
4038000	4044000	which is just a real number, a single number, that you want to multiply by a vector.
4044000	4047000	And so the way you do that is indicated here.
4047000	4050000	You just multiply that number by each element in the vector.
4050000	4054000	So we get CX1, CX2, down to CXn.
4056000	4059000	So for example, here's your X.
4059000	4064000	You want to multiply 4 times X, then you just multiply each component in the vector by 4.
4064000	4076000	Okay, now with that, we can start to talk about a graphical representation of vectors and we'll restrict our attention at first, at least to R2.
4076000	4080000	Pardon me.
4080000	4088000	A vector in R2, also called the plane, and you're probably familiar with that terminology,
4088000	4097000	is indicated by a ray that begins at the origin and terminates at the point defined by the vector.
4097000	4103000	So here's a couple of vectors and you can see how they look on a graph.
4103000	4114000	Okay, so U is 2, 1, so we have a ray that begins at 0, 0, and terminates at 2, 1, similarly for V.
4115000	4120000	Analytically, we know we can compute U plus V in this fashion.
4120000	4125000	Just sum the like terms, 2 plus 1, 1 plus 4, give you 3, 5.
4125000	4130000	Graphically, we use the parallelogram method.
4130000	4138000	You might be familiar with this from your physics class or engineering or maybe even like Calc 3 if you had that.
4139000	4148000	So the way that works is you go to the end of one of the vectors and you draw a line that's parallel to the other one.
4148000	4154000	Okay, so this line here would be parallel to V, and we do the same thing for the other vectors.
4154000	4161000	So go to the end of V, draw a line that's parallel to U, and this line is parallel to U.
4161000	4168000	And where those two lines intersect, that is the sum of the two vectors.
4168000	4174000	So this vector here would be U plus V, and you can see that it's at 3, 5.
4178000	4182000	Scalar multiple of a vector.
4182000	4185000	I did a couple of examples based on those.
4185000	4191000	We were just looking at it, 3 times 2, 1 is just 6, 3, and here's a half of V.
4191000	4202000	If you look at that graphically, 3 times U is a vector that's in the same direction as U, and it's just 3 times as long.
4202000	4211000	So basically you got 3 U's stacked there, and so you can see that it corresponds to there's 6, 3.
4211000	4217000	And one half of V is what you think, one half of V.
4217000	4224000	So you can see that it's a vector in the same direction as V, but half the length.
4224000	4227000	We can add those two together.
4227000	4231000	So this is how we do it, 3 times U plus a half V.
4231000	4233000	We already know how to do these operations.
4234000	4245000	And graphically, take our one half V, align parallel to 3 U, do similarly over here.
4245000	4253000	And the resultant vector here is what we think it should be, about 13 halves, 5.
4254000	4258000	So notice that you can scale U to whatever length you want.
4258000	4264000	You can scale V to whatever length you want, and then add those two vectors together.
4264000	4269000	And if you do that, then think about where the vectors would end up.
4269000	4276000	Any vectors that you can scale U however you want, scale V however you want, and then add them together.
4276000	4280000	And let's think about that.
4280000	4285000	So this would be any multiple of U, any multiple of V.
4285000	4292000	And for the moment, let's restrict our attention to positive multiples or non-negative multiples.
4292000	4296000	So let's assume that C and D are both non-negative.
4296000	4307000	If we do that, then the vectors that we can produce or generate are here in this cone defined by U and V.
4307000	4314000	Because again, we can scale U to whatever length we want, we can scale V to whatever length we want,
4314000	4322000	and then when we complete the parallelogram, then we can end up with any vector in this cone area here.
4323000	4335000	If you allow yourself to take negative multiples of U and V, then think about where those vectors would be in the plane.
4335000	4342000	The easiest way I think to think about this is to, instead of thinking about taking negative multiples of U and V,
4342000	4348000	about thinking of it as taking positive multiples of negative U and negative V.
4348000	4355000	And if you do that, then it's a similar thing as what you just saw.
4355000	4366000	So here is negative V, here's negative U, and so when you take positive multiples of U and V,
4366000	4373000	or positive multiples of negative U and negative V, and then add those two vectors together,
4373000	4379000	you end up with vectors in this region here.
4379000	4391000	Okay, so other possibilities for C and D, let's think about that.
4391000	4402000	If we allow positive multiples of U and negative multiples of V, then we're in this yellow region here.
4402000	4409000	And if we allow positive multiples of V but negative multiples of U, then we're in this region here.
4409000	4423000	And so what you see is that we can generate any vector in the plane that we'd like just by taking a multiple of U and a multiple of V and adding them together.
4423000	4430000	Okay, so we're going to see in a little bit that this means that U and V span the plane.
4430000	4437000	So we can take multiples of each one of those and add them together and generate any vector in the plane.
4437000	4441000	That's what it means to span R2.
4441000	4447000	All right, so I'm going to stop here for now and we'll pick up this in our next.
4447000	4451000	Okay, we're back where we left off with the first video.
4451000	4459000	And we were looking at this picture where we determined that based on our vectors U and V here,
4459000	4468000	we could take multiples of each one of those and add them together to produce any vector in the plane.
4468000	4481000	So any vector in R2 or the plane can be written in the form C times U plus D times V for some values of C and D.
4481000	4484000	Let's look at that analytically now.
4484000	4490000	Okay, so according to our graphical argument, this system is consistent for every B in R2.
4490000	4497000	You can take some multiple of U, some multiple of V, add them together and get any vector in R2.
4497000	4506000	So if we break that down, we can multiply C times U. This is U. This is V.
4506000	4509000	There's our generic right-hand side vector.
4509000	4513000	We multiply it through by the scalars. We end up with this.
4513000	4518000	And then adding these two together gives us this matrix here.
4518000	4521000	Now, what does it mean for two vectors to be equal?
4521000	4525000	Well, it means that component-wise they're equal.
4525000	4529000	So the first component here is equal to the first component here.
4529000	4532000	And likewise the second component.
4532000	4534000	So what we have is a system of equations.
4534000	4539000	2C plus D equals B1. C plus 4D equals B2.
4539000	4542000	So we can put that in an augmented matrix.
4542000	4545000	And that's what I've got here.
4545000	4553000	And notice that, again, according to our graphical argument, our picture that we looked at,
4553000	4560000	this system corresponding to this augmented matrix is consistent no matter what B1 and B2 are.
4560000	4564000	And also notice that these are equivalent.
4564000	4573000	So when we have a multiple of a vector plus another multiple of a vector, then equals some right-hand side vector.
4573000	4577000	Notice how this system relates to this augmented matrix.
4577000	4580000	So the first vector here just goes in the first column.
4580000	4582000	The second vector goes in the second column.
4582000	4584000	And the right-hand side is just in the right-hand side column.
4584000	4589000	So these are equivalent.
4589000	4593000	Okay, let's look at this system a little more closely.
4593000	4600000	I swapped rows to make the arithmetic a little easier to get the 1 and the pivot position here.
4600000	4603000	We can do a row operation to zero out underneath.
4603000	4605000	And we end up with this matrix.
4605000	4610000	Now remember, this has a solution no matter what the right-hand side is.
4610000	4613000	Okay, now how do we know that that's the case?
4613000	4618000	We know it graphically, but how can we look at this augmented matrix and tell that?
4618000	4622000	Well, notice that there's no way to have a row of this form.
4622000	4625000	Zero, zero, something not zero form.
4625000	4629000	And that's because in each row, we have a pivot position.
4629000	4633000	There's a pivot position here, so that's never going to be zero.
4633000	4636000	There's a pivot position here, so that's never going to be zero.
4636000	4642000	So there's no way to have a row of this form here.
4642000	4650000	Okay, so the bottom line is that your system will be consistent no matter what the right-hand side is
4650000	4655000	if you have a pivot position in every row of the coefficient matrix.
4655000	4661000	Okay, now I'm going to talk about the coefficient matrix because I'm talking about just this part here.
4661000	4663000	Not looking at the right-hand side.
4663000	4671000	You don't want your right-hand side to have a pivot position because then that would indicate the system's inconsistent.
4672000	4686000	Alright, let's kind of take what we've looked at so far and put it in the terminology of the text.
4686000	4696000	Okay, so if we start off with an arbitrary set of vectors v1 through vp and scalars c1 through cp,
4696000	4705000	then if we apply the scalars to the vectors and add them up, and that's what we were doing earlier,
4705000	4709000	we only had two vectors, but here we've got an arbitrary number of vectors,
4709000	4717000	this is called a linear combination of the v's with weights c1 through cp.
4717000	4727000	Okay, so when we've been multiplying by scalar and adding vectors together, we were taking linear combinations.
4727000	4730000	Okay, so a linear combination just looks like this.
4730000	4739000	You've got a scalar multiplied times a vector, and you have that for however many vectors you have, and then you add them all up.
4739000	4743000	That's what a linear combination is.
4743000	4746000	Okay, so we were doing c times u plus v times v.
4746000	4754000	That was a linear combination of u and v where c and d were the weights we were using.
4754000	4757000	Okay, now here's a linear combination.
4757000	4762000	Here the x's are being the weights and the a's are vectors.
4762000	4768000	Okay, so this is x1a1 plus x2a2 and so forth.
4768000	4782000	Oh, that should be xn an, and that has the same solution set as the system given by this augmented matrix.
4782000	4789000	Now if you remember before, we started off with an equation like this, we just had two vectors,
4789000	4795000	but then when we got it into matrix form, remember I showed you, you take your first vector here,
4795000	4798000	it goes in the first column in the augmented matrix.
4798000	4801000	The second vector goes in the second column and so forth.
4801000	4808000	The last vector goes in the last column and then you've got your right hand side.
4808000	4816000	Here we see that we've got u ended up c times u plus d times v,
4816000	4822000	and we ended up with that augmented matrix that we saw earlier.
4822000	4833000	Okay, one more term, and I used this in the previous video, but we'll define it more formally here.
4833000	4845000	The set of all linear combinations of the set of vectors v1 through vp in Rn is denoted by the span of v1 through vp.
4845000	4852000	Okay, so set of all linear combinations of vectors is the span of that set of vectors.
4852000	4862000	And the span of that set of vectors is just called the subset that is spanned or generated by those vectors.
4862000	4875000	So if you recall back when we were looking at our example, I talked about how you could span my two vectors,
4875000	4878000	u and v, that we had, they spanned R2.
4878000	4881000	We're going to look at that in just a sec.
4881000	4883000	Here's another way of looking at it.
4883000	4887000	The span of v1 through vp is all vectors that can be written in this form.
4887000	4888000	What is this form?
4888000	4890000	It's just linear combinations.
4890000	4896000	Okay, so the span is just a set of all linear combinations of those vectors.
4896000	4899000	And here's the previous example.
4899000	4907000	We've got c times, this is u, plus d times v equals v1, v2.
4907000	4916000	Okay, so that means that these vectors span R2.
4916000	4918000	Let's back up just a little bit.
4918000	4922000	What if we just looked at the span of a single vector?
4922000	4924000	What would we get there?
4924000	4929000	Well, back to the definition, all linear combinations of that vector,
4929000	4935000	and linear combinations of just one vector means just all multiples of that vector.
4935000	4943000	And so graphically what that is is just the line that goes through the origin and that vector.
4943000	4945000	It's just all multiples of u.
4945000	4950000	So anything on this line.
4950000	4953000	Here's another thing to think about.
4953000	4960000	The span of the vector 2, 1 is the same as the span of this set of vectors.
4960000	4963000	Now why would that be?
4963000	4966000	Well, here's definition.
4966000	4970000	A span of 2, 1 is just all multiples of 2, 1.
4970000	4976000	The span of the 2 of them is all linear combinations of the 2.
4976000	4981000	Now why would these 2 sets be the same?
4981000	4986000	Well, the key is to notice that 4, 2 is a multiple of 2, 1.
4986000	4991000	So it really doesn't add anything because you can take a multiple of 2, 1 to get 4, 2.
4991000	4999000	So by adding 4, 2 to the set as I did here, it doesn't get you any more vectors.
5000000	5008000	And so you have to move off that line defined by the vector 2, 1 to generate more vectors.
5008000	5014000	And that's in our example before we had 2 vectors that were not collinear.
5014000	5021000	And so in that case, you could generate or span all of our 2.
5021000	5026000	Okay, let's look a little bit at a three-dimensional example.
5026000	5036000	So if we look at the span of this vector, that would just be all multiples of that vector in graphically.
5036000	5045000	We're saying that x has to be 0, z has to be 0, but y could be anything because you can multiply anything by 1.
5045000	5049000	So essentially you're on the y-axis.
5049000	5057000	So x has to be 0, z has to be 0, so anything along the y-axis.
5057000	5064000	If we add another vector to it, so I throw in this one, now we take all linear combinations of these two
5064000	5070000	and notice that in this case, you get all vectors that look like this.
5070000	5075000	So it means x has to be 0, but y and z can be anything we want them to be.
5075000	5086000	And so that gives us a plane where x is 0, but y and z are anything that we want them to be.
5086000	5096000	So these two vectors in our 3 generate a plane.
5096000	5104000	We're going to start off today talking about multiplying a matrix with a vector.
5104000	5112000	So some terminology there, let's suppose that we start off with an M by N matrix A
5112000	5119000	and we're going to let A1, A2 through AN be the columns of A.
5119000	5125000	So each of these is a column vector and not a scalar.
5125000	5137000	Each A sub i is a vector in our M. Since there are M rows in A, each column would have M entries.
5137000	5142000	And let's let x be a vector in our N.
5142000	5148000	Then the product of A and x is computed as shown here.
5148000	5155000	So here's A. Remember each of these is a vector, a column of A. Here's x.
5155000	5164000	And so to compute A times x, basically we just match up the components of x with the columns of A.
5164000	5171000	So we end up with x1 times A1 plus x2 times A2 plus out to xN times AN.
5171000	5180000	You'll recognize this as a linear combination of the columns of A.
5180000	5190000	So here's the deal. Ax is a linear combination of the columns of A using the corresponding entries in x as the weight.
5190000	5195000	So here's an example. Here's a matrix A and a vector x.
5195000	5206000	Then to compute A times x, we've got the first entry in x times the first column of A plus the second entry in x times the second column of A.
5206000	5211000	And then plus the third entry in x times the third column of A.
5211000	5220000	And we do the scalar multiplication, add them together, and here's our result.
5220000	5228000	Alright, so notice what we have. Here's a matrix times a vector, and we get this vector.
5228000	5240000	Or another way of looking at it. Here's a linear combination of the columns of the matrix, yielding the same vector, clearly.
5240000	5244000	And then here's another way to look at it.
5244000	5260000	If we looked at this augmented matrix, this is A here, the coefficient matrix A, with this vector augmented onto the right hand side.
5260000	5269000	We essentially have the augmented matrix here that corresponds to this system.
5269000	5276000	So if we put it in reduced echelon form, which I have here, notice that we get what we think we should get, right?
5276000	5282000	The solution to the system is just the vector x that we started out with up here.
5282000	5288000	So these are three different ways to look at the same system.
5289000	5293000	So let's formalize that idea.
5293000	5307000	So if we have a matrix A that's m by n with columns A1, A2, to An, and if B is a vector in RM,
5307000	5316000	then these equations, Ax equals B, and this linear combination of the columns of A, set equal to B.
5316000	5324000	In the system corresponding to this augmented matrix, where you've got all the columns of A with B tacked on at the augmented column,
5324000	5327000	all have the same solution set.
5327000	5330000	So these all essentially mean the same thing.
5330000	5336000	They're just different ways of looking at the same problem.
5336000	5345000	So notice that the equation Ax equals B as a solution, f and only if B is a linear combination of the columns of A.
5345000	5351000	Since these are all equivalent, this is a linear combination of the columns of A that we're setting equal to B.
5351000	5357000	So if that has a solution, that means Ax equals B has a solution.
5357000	5368000	And another way of stating that is that Ax equals B has a solution if and only if B is in the span of the columns of A.
5368000	5378000	So if B is a linear combination of the columns of A, that means B is in the span of the columns of A.
5378000	5394000	We're going to move over to a maple session at this point because it's easier to show you some of this with some of the graphics that maple lets me use.
5394000	5404000	And so I'll also give you a little lesson in using maple as well.
5404000	5415000	So I'm loading here the student linear algebra package and the plots package.
5415000	5422000	And then I'm defining two vectors here, A1 and A2.
5422000	5432000	And then I'm creating with this command a matrix A whose columns are the vectors A1 and A2.
5432000	5437000	So you see the output that you get from that.
5437000	5444000	I'm defining these vectors because I want to look at the plane that they lie on.
5444000	5457000	And as I explained here, this plane is defined by this equation x minus y equals zero.
5457000	5467000	So notice that both of my vectors here have x minus y equals zero and z can be essentially whatever it wants to be.
5467000	5475000	And so what that amounts to is a plane that if you think about the line y equals x in the plane.
5475000	5481000	So it's kind of defined by that line and then just straight up and straight down from that line.
5481000	5487000	So let's kind of look at a picture here.
5487000	5492000	I'm defining plots so that we can see what these look like.
5493000	5500000	Okay, so let's see.
5500000	5506000	It looks like I've changed this.
5506000	5510000	Let me change this.
5510000	5512000	This is supposed to be 222.
5512000	5514000	It really doesn't matter.
5514000	5517000	But just to be consistent, let me make that change.
5517000	5524000	Okay, then so here's what those two vectors look like.
5524000	5530000	And let me kind of rotate this to make it look like what I want.
5530000	5540000	Basically, if you look, you can kind of see that they lie on the same plane.
5540000	5549000	And that plane is, if you look at it, this is looking down from the z perspective.
5549000	5556000	And so that's, if you look down at the bottom of that box, that's the line y equals x.
5556000	5561000	So they both lie on that line.
5561000	5568000	It's easier if you look at the plane also and then you can see the vectors as well.
5568000	5572000	So let's do that.
5572000	5577000	Here you can see the plane.
5577000	5583000	And you've got the two vectors that are lying on that plane.
5583000	5589000	Okay, so you can see here's the x-axis this way, y going this way, and z's up and down.
5589000	5591000	And you can see there's the plane.
5591000	5597000	And looking from it from up above, you can see that we're cutting across
5597000	5600000	on the line y equals x.
5600000	5606000	And both those vectors lie in that plane.
5606000	5614000	So if we look at any other vector in that plane,
5614000	5620000	any other vector where the x and y components are the same, it should lie on that plane.
5620000	5625000	So I've defined a new vector, a 113.
5625000	5632000	And then I've plotted it on this same set of axes.
5632000	5639000	And so if you look, let's get it like this.
5639000	5644000	There you can see the new 1113.
5644000	5647000	I made it black so you can see it here.
5647000	5655000	And then looking at it from above again, you can see that those are all on the same set of axes, or same plane.
5655000	5658000	So all those lie on the same plane.
5658000	5667000	So that means that b here is in the span of the original two vectors that we started out with.
5667000	5671000	Or it's a linear combination of the two vectors we started out with.
5671000	5677000	So if we solve the system ax equals b, it should be consistent.
5677000	5679000	So that's what I'm doing right here.
5679000	5684000	I'm forming the matrix, an augmented matrix here.
5684000	5691000	So I've got my matrix a, and then I'm augmenting on the new vector b here.
5691000	5700000	And all in the same command, I'm reducing that matrix to echelon form, or row echelon form.
5701000	5704000	And here's what we get.
5704000	5713000	Clearly, this system has a solution because we have no rows where it's all zeros and then something not zero.
5713000	5716000	You can see we've got a row of all zeros, but that's okay.
5716000	5721000	And in fact, we have a unique solution because there's no free variables.
5721000	5730000	And the solution says that x1 is 3 halves, x2 is negative one.
5730000	5739000	So if we take that linear combination, 3 halves times the first column of a, plus negative one times the second column of a,
5739000	5744000	we should get b because that's the system we just solved.
5744000	5751000	If you look, indeed, this is that linear combination here, and this is b.
5751000	5753000	All right.
5753000	5762000	Now looking ahead, if we choose a vector whose first components are not equal, then it should not be on that plane.
5762000	5767000	And if we solve the system with it on the right-hand side, we should get no solution.
5767000	5769000	So let's give that a try.
5769000	5776000	We're going to create another vector, c, which has the x and y components are not equal.
5776000	5777000	All right.
5777000	5779000	And I plotted it.
5779000	5783000	So here's that vector.
5783000	5786000	I'm going to give you a view on that.
5786000	5790000	There you can see there.
5790000	5791000	There.
5791000	5792000	That's a good view right there.
5792000	5798000	You can see this new vector is the cyan-colored one.
5798000	5804000	And clearly it's not on the plane that the other three vectors lie on.
5804000	5805000	All right.
5805000	5808000	It's coming off the plane.
5808000	5814000	So that means that it is not in the span of the other three.
5814000	5822000	So it's not a linear combination of the other three, but really we're only interested in the other two.
5822000	5826000	It's not a linear combination of the columns of a.
5826000	5839000	So if we solve the system with c on the right-hand side, then here's what we end up with.
5839000	5840000	Okay.
5840000	5841000	So I didn't get it in reduced echelon form.
5841000	5843000	I wanted to leave it like this.
5843000	5845000	So this is just echelon form.
5845000	5850000	But notice the third row, you got zero, zero, and then negative two.
5850000	5853000	So zero, zero, something not zero.
5853000	5856000	So that tells us there's no solution.
5856000	5857000	Okay.
5857000	5868000	And then again, because that vector is not in the span, it's not in the space that's generated by the columns of a.
5868000	5869000	One more look.
5869000	5873000	I just created just a generic vector I called d.
5873000	5882000	And then let's look at the echelon form of the augmented matrix with a and d.
5882000	5883000	All right.
5883000	5886000	So it looks like this.
5886000	5887000	This is nice thing about maple.
5887000	5900000	It does this symbolic computation so you can do the computation with variables or parameters instead of all just constants.
5900000	5906000	And so we can see, notice the bottom row, we got zero, zero, and then something else over here.
5906000	5908000	And notice what that says.
5908000	5916000	It says that if this system has a solution, then d two minus d one has to equal zero.
5916000	5917000	Right.
5917000	5925000	Because if this is non zero over here, then we've got zero, zero, something not zero in the augmented column, which means no solution.
5925000	5933000	So for this system to have a solution, d two minus d one has to equal zero, which of course means that d two and d one are equal.
5933000	5943000	And since those back up here, those are the first two components, that's just saying that the X and Y components have to be equal for this system to have a solution.
5943000	5955000	Or graphically, the first two components have to be equal for the vector to lie on the plane that is spanned by a one and a two.
5955000	5962000	Okay, I'm going to stop there and we'll pick up the next slide in the next video.
5963000	5966000	Okay, I want to start back here with theorem four.
5966000	5969000	This is a very important theorem in your book.
5969000	5973000	So you want to play, pay close attention to this one.
5973000	5976000	So it says let A be an M by N matrix.
5976000	5980000	Then the following statements are logically equivalent.
5980000	5987000	That means for particular matrix A, either they are all true or they're all false.
5987000	5991000	And so that's a powerful theorem that we have here.
5991000	5997000	Okay, first statement is for each B and RM, the equation AX equals B is consistent.
5997000	6006000	So that's saying no matter what the right hand side is, for this particular matrix A, AX equals B is always consistent.
6006000	6014000	Another way to say that is that each B and RM is a linear combination of the columns of A.
6014000	6019000	Okay, because we saw just earlier that A times X, when you multiply A times X,
6019000	6026000	you're actually taking a linear combination of the columns of A.
6026000	6030000	This also means that the columns of A span RM, right?
6030000	6040000	Because you can produce any vector in RM as a linear combination of the columns of A.
6040000	6047000	And then down to the nitty-gritty, how you actually determine whether this is true or not,
6047000	6050000	is by putting A in echelon form.
6050000	6055000	And these are all true if A has a pivot position in every row.
6055000	6060000	So we saw that in the last, in the Maple demo.
6060000	6067000	Okay, if there's a pivot position in every row of A, that means of the coefficient matrix,
6067000	6072000	then there's no way the system could be inconsistent because you could never have a row of all zeros
6072000	6082000	and then in the augmented column, something non-zero.
6082000	6087000	Okay, we're going to look at one other way to compute A times X.
6087000	6095000	But first, we're looking at the inner product of two vectors because we're going to use that.
6095000	6100000	So if we're given two vectors in our n, say X and Y, then the inner product,
6100000	6107000	which is also called the dot product of X and Y, is computed as follows.
6107000	6115000	It's just X1 times Y1 plus X2 times Y2 plus so forth plus Xn times Yn.
6115000	6119000	Okay, so just multiplying light components and adding them up.
6119000	6121000	So here's an example.
6121000	6128000	We've got X is 1, 2, 3, Y is 4, 5, 6, then the inner product of X and Y, or X dot Y,
6128000	6134000	is 1 times 4 plus 2 times 5 plus 3 times 6.
6134000	6140000	Okay, so now let's talk about two ways we can compute A times X.
6140000	6143000	We already looked at one of these.
6143000	6147000	That's by taking a linear combination of the columns of A.
6147000	6154000	So here's an example to compute the product of this matrix A times X.
6154000	6162000	Then we take the first element of X and apply that to the first column of A.
6162000	6167000	Then second element negative 4 times the second column of A and so forth.
6167000	6174000	So we generate this linear combination and end up with this vector.
6174000	6179000	On the second way, we use inner products.
6179000	6194000	And the i-th element of the inner product of, the i-th element of AX is the inner product of the i-th row of A with X.
6194000	6203000	So looking at that, you look at, to get the first element here, the negative 3,
6203000	6208000	then it's the first row of A inner product with X.
6208000	6215000	So that's going to be 1 times 6 plus 2 times negative 4 plus 1 times negative 1,
6215000	6220000	which is what we have here in this first element here.
6220000	6230000	Then to get the second component of A times X, it's the second row of A inner product with X.
6230000	6239000	So negative 3 times 6 plus negative 1 times 4 plus 2 times negative 1 as we see here.
6239000	6248000	Then to get the third element, it's the third row of A inner product with the vector X.
6248000	6254000	So those are two different ways you can compute the product of a matrix and a vector.
6254000	6259000	For the most part, we will use this method, the linear combination of the columns of A,
6259000	6263000	just because that fits in with the way I want you to be thinking.
6263000	6267000	However, this method down here is probably quicker.
6267000	6273000	So if you just need to compute the product of a matrix and a vector,
6273000	6280000	you might want to just use this method using the inner product method just because it's faster.
6280000	6285000	So you should practice both just to make sure you know how to do each of these methods.
6285000	6291000	And I think that's it for this lesson.
6291000	6296000	Alright, so today we're going to talk about homogeneous linear systems.
6296000	6304000	And a homogeneous linear system is simply one in which the right hand side is the zero vector.
6304000	6308000	Zero vector is just a vector of all zeros.
6308000	6312000	So here's another way to look at it.
6312000	6319000	We look at A times X as a linear combination of the columns of A.
6319000	6325000	Is it here? X1A1 plus X2A2 plus dot dot dot XNAN.
6325000	6328000	And we want that to equal the zero vector.
6328000	6331000	Now, how can that happen?
6331000	6339000	Well, if you look at it a little bit, it's pretty clear that if you set each one of these X values equal to zero,
6339000	6344000	then you did a linear combination and you'll end up with the zero vector.
6344000	6349000	So therefore, a homogeneous system is always consistent.
6349000	6354000	X equals zero and all X values equal to zero is always a solution.
6354000	6362000	And since it's always a solution and it's obvious that it's a solution, we call it the trivial solution.
6362000	6366000	So we know that the system is consistent.
6366000	6372000	And if you think back to the two questions that we asked when we're solving the system,
6372000	6374000	the first one is, is it consistent?
6374000	6376000	So now we know this one is.
6376000	6383000	We wanted to question two, which is, okay, is the solution unique or are there an infinite number of solutions?
6383000	6389000	So in our case, if the solution is unique, that means you only have the trivial solution.
6389000	6395000	So what we want to know is, do there exist any non-trivial solutions?
6395000	6402000	Is the solution set an infinite set?
6402000	6407000	Well, the answer is just like it would be for any other system.
6407000	6412000	You'll have non-trivial solutions or you'll have an infinite number of solutions,
6412000	6421000	if and only if your system of equations has at least one free variable.
6421000	6425000	So we're going to look at a system here and solve it.
6425000	6432000	And we're going to write our solution in parametric vector form.
6432000	6436000	So that's something that's new in this section.
6436000	6439000	All right, so here's the system.
6439000	6441000	We want to solve x equals zero.
6441000	6443000	So we put it in an augmented matrix.
6443000	6448000	So we tack on the column of zeros and we do some row operations.
6448000	6454000	Notice that I have put my augmented matrix in reduced echelon form,
6454000	6457000	not just echelon form, but reduced echelon form.
6457000	6464000	And that is because that makes it easier to not only write the solution in general form,
6464000	6468000	but it makes it easier to write it in parametric vector form.
6468000	6472000	So anytime your system has an infinite number of solutions,
6472000	6479000	then it tends to be easier to write out the set of solutions
6479000	6484000	if you put your matrix in reduced echelon form first.
6484000	6490000	So looking at this matrix here, you can see that x1 and x2,
6490000	6496000	they're pivot columns in the x1 and x2 columns, pivot positions in the x1 and x2 columns.
6496000	6499000	So those are basic variables.
6499000	6505000	In the x3 column, we have no pivot position, so therefore x3 is a free variable.
6505000	6509000	Okay, so that means we've got an infinite number of solutions,
6509000	6513000	or this homogeneous system has non-trivial solutions.
6513000	6515000	So what do they look like?
6515000	6520000	Well, we can look at the second row and we get that x2,
6520000	6523000	it must be equal to 3 times x3,
6523000	6528000	and x1 is going to be equal to negative 4 times x3.
6528000	6532000	So we can write the general form of the solution as it's given here.
6532000	6539000	x3 is free, x1 and x2 are written in terms of the free variable.
6539000	6542000	Now to get the parametric vector form of the solution,
6542000	6546000	we start off with just a generic solution vector,
6546000	6551000	and basically just copy down the general form of the solution.
6551000	6557000	So this, what you see here, comes straight from looking at the general form of the solution.
6557000	6560000	And then to write it in parametric vector form,
6560000	6563000	we simply factor out the parameter.
6563000	6568000	In this case, x3 is the parameter because we can set x3 to be whatever we want
6568000	6572000	in order to generate as many solutions as we want.
6572000	6575000	So we have it now in parametric vector form.
6575000	6579000	Here's our parameter and here's our vector.
6579000	6584000	You should note that if you have more than one free variable,
6584000	6591000	then you're going to have more than one parameter and vector
6591000	6594000	when you write your solution in parametric vector form.
6594000	6597000	So you'd have, you know, if we had x4 that was a free variable,
6597000	6602000	we'd have x3 times this vector plus x4 times some other vector and so forth.
6602000	6608000	You'd have a vector for each free variable.
6609000	6617000	Okay, let's think about the relationship between the systems x equals 0 and ax equals b.
6617000	6623000	So what we've done to this point is solve ax equals 0 for a particular matrix a.
6623000	6632000	And now I want to keep that same a and solve the system ax equals b for this vector b given here.
6632000	6636000	And we'll write our solution in parametric vector form.
6636000	6638000	So we approach it the same way.
6638000	6640000	Here's our augmented matrix.
6640000	6642000	We do row operations.
6642000	6646000	Oops, sorry.
6646000	6649000	Backing up.
6649000	6651000	There we go.
6651000	6656000	We're doing row operations to get the matrix in reduced echelon form.
6656000	6662000	And if you want to go back and compare with what we did with ax equals 0,
6662000	6665000	these are the exact same row operations.
6665000	6670000	And that's because they were based on what was in the coefficient part of the matrix.
6670000	6672000	And so that hasn't changed.
6672000	6674000	So we do the same operation.
6674000	6678000	So the only thing that's changed is what's on the right-hand side.
6678000	6688000	Let me, oops, I'm backing up again.
6688000	6691000	Trying to get used to my mouse track pad here.
6691000	6692000	Sorry about that.
6692000	6701000	Let me point out that when you're solving ax equals 0 and you have all zeros on the right-hand side,
6701000	6703000	then those zeros never change.
6703000	6709000	Because when you do elementary row operations, any of those three elementary row operations,
6709000	6711000	they will never change.
6711000	6715000	If you start off with all zeros, you'll end up with all zeros.
6715000	6718000	Okay, so back to this example though.
6718000	6724000	When we write out our solution, those x3 is still free variable.
6724000	6734000	And now x2, when we write x2, then it's going to be 3, the 3 here, plus 3x3.
6734000	6738000	And x1 is going to be negative 5 minus 4x3.
6738000	6740000	So here's our solution.
6740000	6745000	And if we just write it in general form, here's what it looks like.
6745000	6751000	And parametric vector form, we start out just like we did previously,
6751000	6758000	write out what the general form looks like in a vector format.
6758000	6766000	And then we're going to separate out the part that involves a parameter and the constant part.
6766000	6771000	And then take it one step further and factor out the parameter.
6771000	6780000	And so notice that what we have here is exactly the solution that we had to ax equals 0.
6780000	6786000	And then with ax equals b, we have that plus this constant vector.
6786000	6790000	Now notice that the constant vector doesn't have a parameter associated with it.
6790000	6792000	It's just constant and it doesn't change.
6792000	6800000	But here we can multiply, we can set our parameter x3 to be anything and scale this vector.
6801000	6808000	Okay, so in general, the solution looks like this.
6808000	6810000	Here was solution to ax equals 0.
6810000	6813000	And notice that it's in this form.
6813000	6817000	It's in the form some parameter t times a vector v.
6817000	6822000	So in this case, x3 is plane t and this is the vector v.
6822000	6827000	So it's just a vector that we can scale any way we want.
6827000	6832000	Whereas for ax equals b, you still got the tv part, right?
6832000	6834000	You got x3 times this vector.
6834000	6836000	But you've got this constant vector.
6836000	6838000	So that's what I'm calling p.
6838000	6845000	And so for ax equals 0, you just have a vector that you can scale.
6845000	6851000	For ax equals b, you've still got the vector you can scale plus this other vector.
6851000	6856000	So let's look at graphically what is going on there.
6856000	6862000	Now for this, I'm going to just look at a case in R2.
6862000	6870000	So what you're going to see here is not the solutions to the system we were just looking at because that was in R3.
6870000	6873000	But it's a little bit easier when you draw pictures in R2.
6873000	6875000	So that's what I've got here.
6875000	6882000	So for ax equals 0, the solution set is all vectors of the form t times v.
6882000	6884000	So here's a vector v.
6884000	6888000	And we know that t times v is just any vector on this line.
6888000	6895000	So the line that you see here is the set of all solutions to ax equals 0.
6895000	6901000	Now let's look when we consider the vector p.
6901000	6912000	So for a solution set for ax equals b is of the form tv plus p.
6912000	6919000	And so remember all the vectors along here are the t times v.
6919000	6929000	And then so for any of these vectors, any point on this line, you can add p and you'll have a vector of the form tv plus p.
6929000	6932000	So here's one.
6932000	6934000	I just did v plus p.
6934000	6941000	So one times v plus p and doing the parallelogram method, you end up with this vector here.
6941000	6945000	We can do it for other vectors.
6945000	6949000	So here's p plus 2 times v.
6949000	6953000	Here's p plus 0 times v.
6953000	6956000	Here's p plus negative 1 times v.
6956000	6962000	So notice though that they all fall along this line.
6962000	6965000	So right here would be p plus 1 half v.
6965000	6967000	Here p plus 3 fourths v.
6967000	6970000	Here p plus 3 halves v and so forth.
6970000	6983000	So for every vector, every solution to ax equals 0 that you have on this line, there's a corresponding solution to ax equals b over here on this line.
6983000	6993000	And so we end up with this approach where along the blue line, that's all the solutions to ax equals 0.
6993000	6998000	Along the magenta line here, all solutions to ax equals b.
6998000	7008000	So notice that for ax equals 0, it's aligned through the origin because that's the trivial solution to ax equals 0.
7008000	7021000	And for ax equals b, you're moving away from the origin, so off this line, but parallel to the solution set for ax equals 0.
7021000	7032000	And if you look in three dimensions, let's suppose we have a problem where the solution set is a plane in three dimensions.
7032000	7038000	So for ax equals 0, it's a plane like this red one that's going to go through the origin.
7038000	7044000	And then for ax equals b, you get a parallel plane that's moved off the origin.
7044000	7050000	So for ax equals 0, you get this red plane that goes through the origin.
7050000	7056000	For ax equals b, you get a parallel plane that's off the origin.
7062000	7067000	All right, today we're going to talk about the concept of linear independence.
7067000	7081000	So if we have a set of vectors, let's say p vectors in Rn, the set is said to be linearly independent if we take a linear combination, set it equal to the 0 vector.
7081000	7085000	And this equation has only the trivial solution.
7085000	7098000	So the only way we can take a linear combination of the vectors and get the 0 vector is if all the coefficients are equal to 0.
7098000	7111000	Since this equation here is equivalent to the system defined by this augmented matrix, then we also say that the set is linearly independent.
7111000	7124000	If the system corresponding to the augmented matrix where we put all the vectors in as the columns, augment on the 0 vector, if that has only the trivial solution.
7124000	7128000	So these are equivalent.
7128000	7137000	Now, if this has only the trivial solution, then that means it has a unique solution, right, has only the trivial solution.
7137000	7142000	So all the c's equal to 0 is the only solution, so it's unique.
7142000	7145000	So that means it has no free variables.
7145000	7161000	So kind of the typical method of determining if a system or if a set of vectors is linearly independent is to stick them in the columns of a matrix as done here.
7161000	7172000	Take on the 0 vector for the augmented column and see if that matrix has any free variables in that system.
7172000	7179000	If there are no free variables, then you know the vectors are linearly independent.
7179000	7188000	If you do find a free variable, then you know that there are an infinite number of solutions to that system, and therefore the vectors are not linearly independent.
7188000	7192000	In that case, we say they're linearly dependent.
7192000	7206000	So here's a couple of vectors, and if we want to find out if they're linearly independent, then as I showed you before, take a linear combination of them and set it equal to 0 as I've done here.
7206000	7209000	We put that into an augmented matrix.
7209000	7217000	It looks like this with just the two vectors stuck in the columns and the 0 vector tacked on as the augmented column.
7217000	7224000	And we can do one row operation and zero out in this position.
7224000	7231000	And notice that we have no free variables, and therefore that means that we have only the trivial solution.
7231000	7234000	You can see the solution is unique.
7234000	7240000	And so these vectors are linearly independent.
7240000	7243000	Here's another set of vectors.
7244000	7247000	Just change just a little bit.
7247000	7250000	Let's see if they're linearly independent.
7250000	7262000	So again, we take a linear combination, set it equal to 0, stick that in as an augmented matrix, do one row operation, and we end up with this matrix here.
7262000	7270000	And notice that we've got a free variable, x2 here, the second column, has no pivot position, so x2 is a free variable.
7270000	7276000	So therefore these vectors are linearly dependent.
7276000	7283000	Now if you look at the vectors that we had, the first set, if we graph them, looks like this.
7283000	7285000	We had 2, 1, and 1, 4.
7285000	7286000	So they look like that.
7286000	7288000	They're linearly independent.
7288000	7294000	Whereas that second set, I believe we had 1, 4, and 2, 8.
7295000	7297000	1, 4, and 2, 8.
7297000	7299000	And so they're multiples of each other.
7299000	7303000	So they're indicated in this picture.
7303000	7306000	And they're linearly dependent.
7306000	7322000	Interestingly, you can see that these two vectors, if you think about the span of these two vectors, then you can take a linear combination of these two
7322000	7325000	and produce any vector in R2.
7325000	7336000	However, for these two, any linear combination of these two vectors only gets you vectors on the line that's defined by these vectors and going back in the negative direction here.
7336000	7342000	But you get no vectors off that line.
7342000	7351000	We've been looking at the two vector case, which at times can be misleading, but let's examine what we can say about it.
7351000	7362000	When you have just two vectors, then they are linearly dependent if at least one is a multiple of the other.
7362000	7370000	So if they're multiples of each other or one's a multiple of the other, then they are linearly dependent.
7370000	7373000	They're linearly independent if neither is a multiple of the other.
7373000	7378000	So that works when you only have two vectors.
7378000	7386000	Okay, so let's kind of expand our scope here and look at a set of three vectors.
7386000	7395000	So I took the set we were looking at initially, one, four, and two, one, and threw in another vector in there.
7395000	7400000	Now, what do you think? Are these vectors linearly independent?
7400000	7405000	Well, if we take a linear combination of them, set it equal to zero vector.
7405000	7408000	This is just going back to the first definition we talked about.
7408000	7415000	Then throw that in an augmented matrix, end up with this, and what happens?
7415000	7419000	No need to do any row operations. Why is that?
7419000	7429000	Let's think. How do we know if the system is, or if the system, we want to know does the system have only the trivial solution or not?
7429000	7434000	Or another way is, does it have any free variables?
7434000	7438000	Well, we can look at this one and say, yep, there's at least one free variable.
7438000	7444000	There has to be, because we have three variables and only two equations.
7444000	7449000	So three columns, only two rows. So we have to have at least one free variable.
7449000	7454000	So we can have at most two pivot positions.
7454000	7460000	So these vectors are linearly dependent.
7460000	7466000	Okay, so here's a rule. If the set contains more vectors, then there are entries in each vector.
7466000	7468000	Then the set's linearly dependent.
7468000	7473000	That's the case we just looked at. We had three vectors in R2.
7473000	7478000	So more entries, and more vectors, then there are entries in each vector.
7478000	7487000	Okay, there it is. Got three vectors and only two entries in each vector.
7487000	7496000	If we look at that augmented matrix and do some row operations on it,
7496000	7498000	we end up at this point.
7498000	7502000	Now remember, back here, just looking at this, you knew they were linearly dependent
7502000	7506000	because you had more vectors than there were entries in each vector.
7506000	7511000	But if we do these row operations and get this matrix in reduced echelon form,
7511000	7513000	then we end up here.
7513000	7519000	So if we wrote the solution of that system, it looks like this.
7519000	7524000	From the second row, you get x2 is going to equal 2x3.
7524000	7530000	x1 is equal to negative x3. x3 is a free variable.
7530000	7536000	So you can plug in anything you want for x3 to generate specific solutions.
7536000	7540000	For instance, if we said x3 equals 0, that's going to give us the trivial solution.
7540000	7547000	Because x3 is 0, and based on what x1 and x2 are,
7547000	7551000	defined in terms of x3, they're going to be 0 also.
7551000	7559000	If we said x3 equal to 1, then we get x1 is negative 1, x2 is 2, and x3 is 1.
7559000	7563000	Now, where does that apply to?
7563000	7568000	Well, notice we were trying to find a solution to this equation.
7568000	7574000	We were trying to see can we find nonzero values for x1, x2, and x3,
7574000	7579000	at least one of them nonzero, so that we can produce the zero vector.
7579000	7589000	So this general form of the solution tells us how to form the values for x1, x2, and x3.
7589000	7596000	So we can take our values that we got for x3 equals 1, plug them in.
7596000	7600000	So notice if we do that, we end up with negative 1 times 1,
7600000	7609000	and negative 1 plus 4 here is negative 3, I mean positive 3, minus 3 gives us 0.
7609000	7617000	And in the second position, we get negative 4 plus 2 is negative 2 plus 2 is 0.
7617000	7624000	So we can find specific values for these coefficients so that we can produce the zero vector.
7624000	7632000	Now notice from this, we can take that equation and solve it for negative 3, 2.
7632000	7644000	So we can just take this over to the other side and notice that negative 3, 2 can be written as a linear combination of 1, 4, and 2, 1.
7644000	7646000	That's what we've just done here.
7646000	7652000	We've written negative 3, 2 as a linear combination of the other two vectors.
7652000	7661000	Another way of saying that is that negative 3, 2 is in the span of these other two vectors.
7661000	7667000	It's a linear combination of them, so that means it's in the span of those two vectors.
7667000	7671000	And if you think back to our picture, that makes sense.
7671000	7676000	Here were the first two vectors, 2, 1, and 1, 4.
7676000	7680000	And remember before, I talked about how these span are two.
7680000	7690000	We can take a linear combination, we can scale each of them, and then add those scaled vectors together to produce any vector in R2.
7690000	7693000	And how do we know that?
7693000	7701000	Well, just look at those two vectors, do a row operation, and look, we have a pivot position in every row.
7701000	7710000	So by theorem 4, going back to section 1.4, theorem 4, that says that the vectors span R2.
7710000	7727000	One of the things in theorem 4 is if you have a pivot position in every row, that means the columns of the original matrix span the space in which they live, which in this case were in R2.
7727000	7732000	Okay, so here's another rule that relates to linear dependence.
7732000	7741000	A set of two or more vectors is linearly dependent, if and only if at least one of the vectors is a linear combination of the others.
7741000	7749000	So back here, we saw that negative 3, 2 is a linear combination of the other two vectors.
7749000	7753000	And in fact, we could have solved this system for any of these three vectors.
7753000	7756000	We could solve for any of these three vectors in terms of the other two.
7756000	7763000	So any of these vectors is a linear combination of the other two.
7763000	7767000	Okay, there you see it, again.
7767000	7777000	Alright, moving on, if there's another rule, keep in mind, if a set contains the zero vector, then that set is linearly dependent.
7777000	7781000	So if you have a zero vector in the set, then the set has to be linearly dependent.
7781000	7783000	Here's an example of that.
7783000	7785000	You get the zero vector.
7785000	7788000	Notice that it's always going to be a free variable.
7788000	7797000	Okay, the variable corresponding to the zero vector is always going to be free, and hence the set would be linearly dependent.
7797000	7802000	So summary, set of vectors is linearly independent.
7802000	7808000	If you take a linear combination, set it equal to zero, and you have only the trivial solution.
7808000	7812000	So all the x is equal to zero is the only solution.
7812000	7827000	Equivalently, you throw all those vectors into a matrix, tack on the zero vector for the augmented column, solve that system, and you get only the trivial solution.
7827000	7835000	Okay, on the other hand, it's linearly dependent if the system has at least one free variable.
7835000	7839000	Because in that case, you would have an infinite number of solutions.
7839000	7851000	And another way of characterizing linear dependence is a set's linearly dependent if at least one of the vectors is a linear combination of the others.
7851000	7856000	Okay, and that's it for section 1.7.
7856000	7867000	Okay, we're going to start today talking about linear transformations.
7867000	7880000	First, we're going to look back at something you're familiar with from algebra or calculus.
7880000	7889000	And we'll just look at a basic function which typically maps some real number to another real number.
7889000	7893000	So we map the set of real numbers to the set of real numbers.
7893000	7898000	So for example, here's a graph of a function f of x equals x squared.
7898000	7909000	And so you input a real number, okay, x is a real number, you square it, and you get another real number.
7909000	7913000	So it's mapping the real numbers to the real numbers.
7913000	7916000	Here's a little picture view of what's happening.
7916000	7921000	Here you're putting in a real number into your function.
7921000	7926000	And for our purposes here, we know what the function does, just squares the number.
7926000	7930000	But you really don't have to know exactly what the function does.
7930000	7936000	You know that what comes out though is another real number.
7936000	7950000	So this notation down here at the bottom is red f is a function that maps the real numbers to the real numbers.
7950000	7955000	Now let's move to talking about matrix transformations.
7955000	7959000	And it's the same basic idea as a function.
7959000	7962000	It's just defined in terms of a matrix.
7962000	7966000	So here t of x, t for transformation.
7966000	7968000	So we apply a transformation to x.
7968000	7976000	We multiply a times x where a is some m by n matrix that defines this transformation.
7976000	7979000	So here's a picture view of what's going on.
7979000	7985000	You have a vector from Rn that is the input to your transformation.
7985000	7988000	And the output is a vector in Rm.
7988000	7993000	The key here is that a is an m by n matrix.
7993000	7998000	And the transformation occurs by multiplying a times x.
7998000	8003000	So that means that x has to have as many entries as there are columns of a.
8003000	8007000	Because a times x is just a linear combination of the column.
8007000	8011000	So we need an x component to match up with each column of a.
8011000	8015000	And then each column of a has m entries.
8015000	8019000	So when we do that linear combination, we're going to end up with a vector with m entries.
8019000	8025000	And hence, we end up with a vector in Rm.
8025000	8034000	So we say that t maps Rn to Rm.
8034000	8036000	So for example, here's a matrix A.
8036000	8041000	And let's suppose that we have transformation t defined on this matrix.
8041000	8045000	So t of x equals A times x.
8045000	8050000	So here's a vector x equals 2, 1.
8050000	8053000	Then t of x is A times x.
8053000	8058000	So we do that multiplication, take a linear combination, and we end up with 1, 2.
8058000	8063000	Okay, so we started with a vector in R2 and ended up with a vector in R2.
8063000	8068000	Which makes sense, since A is 2 by 2.
8068000	8071000	Here's another example.
8071000	8074000	We're applying the transformation to the vector 4 negative 2.
8074000	8077000	So we multiply A times that vector.
8077000	8085000	And so we take a linear combination of the columns of A and end up with negative 2, 4.
8085000	8094000	If we look at it in a general case, so we just apply t to a generic vector x1, x2 from R2.
8094000	8096000	We did the same way as we did the previous two.
8096000	8099000	And notice we end up with x2, x1.
8099000	8103000	So this transformation has just reversed the order of the elements.
8103000	8111000	If you look back up at these other two, 2, 1, went to 1, 2, 4, negative 2, went to negative 2, 4.
8111000	8113000	So that's all this transformation is doing.
8113000	8121000	Swapping the order of the elements in the input vector.
8121000	8127000	If you look graphically at what's going on, here's one of the vectors, 2, 1.
8127000	8133000	And here was its transformation, which is 1, 2.
8133000	8139000	And here's the other one, 4 negative 2, which was transformed into negative 2, 4.
8139000	8149000	And so you can see from this picture that this transformation is reflecting a vector across the line y equals x.
8149000	8152000	Okay.
8152000	8157000	This is kind of a standard sort of thing that you see in computer graphics.
8157000	8164000	And so matrix transformations are really the fundamental element in computer graphics.
8164000	8168000	So if you were to go on and take that course in the computer science department,
8168000	8173000	you would be seeing a lot of matrix transformations.
8173000	8174000	Okay.
8174000	8190000	So to be a little more precise, we say a transformation t from rn to rm is a rule that assigns to each vector x and rn a vector t of x and rm.
8190000	8192000	There are more terminology.
8192000	8199000	Here, rn is called the domain of t, and rm is called the co-domain of t.
8199000	8205000	So rn is where the inputs to the transformation come from.
8205000	8210000	rm is where the outputs come from.
8210000	8211000	Okay.
8211000	8217000	For a given x in rn, t of x is called the image of x.
8217000	8223000	So the image of x is just the vector that x maps to.
8223000	8233000	If we look at all images, all possible images that we get under the transformation t, this is called the range of t.
8233000	8234000	Okay.
8234000	8248000	So the range of t is all images that we get when we map all possible vectors x through this transformation.
8248000	8252000	Now, you might be thinking that, well, isn't that the co-domain?
8252000	8255000	And the answer is sometimes it is, and sometimes it isn't.
8255000	8256000	Okay.
8256000	8261000	In general, the range is a subset of the co-domain.
8261000	8264000	In some cases, they are equal.
8264000	8266000	In others, they're not.
8266000	8269000	There are elements of the co-domain that are not in the range.
8269000	8278000	So not in general, not everything in the co-domain is mapped to necessarily.
8278000	8279000	Okay.
8279000	8284000	So here's back to our previous example.
8284000	8289000	We've got this transformation maps r2 to r2.
8289000	8291000	So the domain is r2.
8291000	8300000	And really the domain is the first set that you see here when you write down the transformation in this fashion.
8300000	8302000	The co-domain is also r2.
8302000	8309000	So we're mapping vectors from r2 to vectors in r2 also.
8309000	8311000	Now, the range, let's think about that.
8311000	8318000	Now, the range, think of the range as just everything that gets mapped to under this transformation.
8318000	8327000	And if you look at the picture that we had before, if you wanted to produce any vector,
8327000	8333000	then you can figure out what vector you need that would map to it because you just need to reverse the order of the elements.
8333000	8338000	So that means that the range of this transformation is all of r2.
8338000	8347000	So the co-domain and the range are equal in this case because any vector in r2 is mapped to.
8347000	8348000	All right.
8348000	8350000	Let's look at a different example.
8350000	8351000	Okay.
8351000	8355000	Here's one defined by different matrix A, still a 2 by 2 matrix.
8355000	8360000	So this transformation also maps r2 to r2.
8360000	8361000	Okay.
8361000	8362000	So the domain is r2.
8362000	8364000	The co-domain is r2.
8364000	8369000	Let's think about what is the range of this transformation?
8369000	8373000	Well, let's play around with it for just a little bit.
8373000	8378000	Let's take our old vector 2, 1, and see what it maps to.
8378000	8385000	If we apply the transformation to 2, 1, multiply A times 2, 1, and we end up with a vector 4, 8.
8385000	8386000	Okay.
8386000	8387000	How about another one?
8387000	8389000	Negative 3, 2.
8389000	8391000	A times that vector.
8391000	8393000	Take the linear combination.
8393000	8395000	You end up with a 1, 2.
8395000	8398000	So let's look at the general case.
8398000	8399000	What happens?
8399000	8403000	We apply it to just a generic vector x1, x2.
8403000	8405000	Do that matrix multiplication.
8405000	8419000	And we end up with this vector, which I can rewrite in this form to illustrate that the second component of the vector is just twice the first one.
8419000	8422000	And you see that in these other two that we did, right here.
8422000	8424000	The second component is 2 times the first.
8424000	8428000	Here, again, second component is 2 times the first.
8428000	8441000	So if we look at that graphically, every vector that we can map to is of this form, the second component is 2 times the first.
8441000	8446000	So that's actually all vectors on the line y equals 2x.
8446000	8449000	The second component is 2 times the first.
8449000	8453000	Here's 2, 1, and it maps to 4, 8.
8453000	8455000	So it's on this line.
8455000	8464000	The vector negative 3, 2, I believe it was that we looked at, it mapped to 1, 2.
8464000	8466000	So here it falls on the line.
8466000	8473000	So no matter what vector you choose, it's going to be projected onto this line.
8473000	8478000	So the range of this transformation is simply this line.
8478000	8483000	The line y equals 2x, or all vectors of the form x1, 2x1.
8483000	8487000	So all vectors where the second component is just 2 times the first.
8487000	8492000	So here, the range is not the same as the co-domain.
8492000	8498000	The co-domain is r2, but the range is just this line y equals 2x.
8498000	8503000	So it's just part of the co-domain.
8503000	8513000	Okay, suppose at this point, we know given a transformation in the matrix that defines it,
8513000	8518000	we can compute t of x, and just multiply 8 times x.
8518000	8523000	A little more difficult question is this one.
8523000	8530000	How do we determine if a given vector b is in the range of a particular transformation?
8530000	8536000	So we're asking, is there a vector x that maps to b?
8536000	8541000	Or is there a vector x such that t of x is equal to b?
8541000	8550000	And since t of x equals 8 times x, we can say, does there exist a vector x such that 8 times x equals b?
8550000	8555000	And now we're back into the realm of looking at systems of equations.
8555000	8558000	And we're very familiar with that.
8558000	8562000	So simply a system of equations to solve.
8562000	8567000	So let's go back to that previous example.
8567000	8573000	And suppose you're asked, is this vector 612 in the range of t?
8573000	8579000	And we know that since the range of t is all vectors on the line y equals 2x,
8579000	8584000	then it should be because the second component 12 is 2 times the first.
8584000	8587000	So let's solve the system of equations.
8587000	8593000	Here's the augmented matrix corresponding to ax equals b, where b is 612.
8593000	8596000	And we do one row operation, and here we go.
8596000	8600000	It's an echelon form, and clearly the system is consistent.
8600000	8608000	So therefore, the vector is in the range of t.
8608000	8611000	About another one, about 610.
8611000	8615000	Now in this case, notice the second component is not 2 times the first.
8615000	8620000	So you would expect this vector not to be in the range of t,
8620000	8625000	which would mean that the system should be inconsistent.
8625000	8630000	And as you see, do one row operation, and you end up with 00 and then negative 2.
8630000	8634000	So that's clearly the system is inconsistent.
8634000	8637000	So this vector is not in the range of t.
8637000	8642000	So to determine if a vector is in the range of a linear transformation,
8642000	8647000	you need to solve a system of equations.
8647000	8651000	Okay, changing gears just a little bit.
8651000	8660000	Previously, we learned that matrix multiplication has certain properties,
8660000	8663000	and one of them is this one.
8663000	8669000	If you multiply matrix times the sum of two vectors, then you can distribute.
8669000	8674000	So a times u plus v is equal to au plus av.
8674000	8681000	And similarly, if you multiply a times a scalar times a vector,
8681000	8684000	then you can move the scalar out.
8684000	8689000	So a times cu is the same as c times au.
8689000	8695000	Now I bring that up because that applies here.
8695000	8703000	Okay, so a transformation is said to be linear if the following conditions hold.
8703000	8708000	That t of u plus v is t of u plus t of v.
8708000	8712000	And t of c times u is c times t of u.
8712000	8717000	Okay, so these two conditions look very similar to what we have up there.
8717000	8724000	And in fact, since we define our transformation as a times x,
8724000	8728000	then these are really equivalent.
8728000	8737000	Okay, so linear transformations preserve the operations of vector addition and scalar multiplication.
8737000	8740000	And we'll stop here at this point.
8740000	8744000	And then the next video will be for section 1.9,
8744000	8753000	which will include more information on linear transformations.
8753000	8759000	All right, we're going to start with more on linear transformations.
8759000	8762000	Let's suppose we have a transformation t.
8762000	8771000	And suppose that all you know about t is that the vector 1, 0 gets mapped to 2, 0, 1.
8771000	8776000	And you know that 0, 1 gets mapped to this vector.
8776000	8783000	So just from looking at this, you can tell that t maps r2 to r3.
8783000	8791000	So we're taking vectors in r2 and we're mapping them to r3.
8791000	8802000	But how can we find a general rule for how to determine t of just a generic vector x?
8802000	8809000	So t of x1, x2 for any vector x1, x2.
8809000	8813000	Well, to get this just based on the information that we have,
8813000	8817000	we need to use the properties of linear transformation.
8817000	8826000	So remember, we talked about these last time that t of u plus v is t of u plus t of v.
8826000	8834000	And t of scalar times the vector is that scalar times t of the vector.
8834000	8838000	So let's examine what we have.
8838000	8846000	Notice that we can write a generic vector x1, x2 as x1 times 1, 0 plus x2 times 0, 1.
8846000	8854000	Now remember on backwards, we know what we get when we apply t to 1, 0.
8854000	8857000	And we know what we get when we apply t to 0, 1.
8857000	8859000	So keep that in mind.
8859000	8865000	So we're going to write x1, x2 as x1 times 1, 0, x2 times 0, 1.
8865000	8869000	So then I'm going to apply t to both sides of this equation.
8869000	8877000	So I'll have t of x1, x2 is equal to t of this stuff here.
8877000	8888000	Now since t's a linear transformation, t of, this is like t of a plus, or u plus v,
8888000	8893000	if you want to think of it like that, so we can break it up into t of u plus t of v.
8893000	8897000	And then this is like a constant times a vector.
8897000	8905000	So t of a constant times a vector, we can rewrite as that constant times t of that vector.
8905000	8907000	Okay, so we end up with this.
8907000	8910000	And we know what t of 1, 0 is.
8910000	8913000	It's just this vector, and we know what t of 0, 1 is.
8913000	8915000	It's this vector.
8915000	8917000	So here's what we end up with.
8917000	8924000	And so notice this is just a linear combination of these two vectors, which we can rewrite in this form
8924000	8930000	as a matrix with those columns times the components x1, x2.
8930000	8937000	So notice that we've written t of x, okay, generic vector x, as a times x.
8937000	8944000	So now we have the matrix with which to apply the transformation.
8944000	8959000	And as you can see, that matrix is simply the vectors that you get when you apply t to the vector 1, 0, and 0, 1.
8959000	8968000	Now those columns that we just looked at, 1, 0, and 0, 1, those are special vectors.
8968000	8973000	And to talk about that, let's first define the identity matrix.
8973000	8976000	So the identity matrix is an n by n matrix.
8976000	8981000	So let's square and we write it as i sub n.
8981000	8984000	And it has 1's on the main diagonal.
8984000	8990000	The main diagonal is the 1, 1 position, 2, 2, 3, 3, 4, 4.
8990000	8994000	So it has 1's down the diagonal and 0's everywhere else.
8994000	8995000	So here are a few examples.
8995000	8998000	i2's, 2 by 2, got 1's on the diagonal.
8998000	9003000	i3's a 3 by 3, 1's on the diagonal, 0's everywhere else.
9003000	9005000	And here's i4.
9005000	9009000	So i sub n is just an n by n identity matrix.
9009000	9011000	Now notice i sub 2.
9011000	9016000	You see those columns that we were just dealing with, those vectors 1, 0, and 0, 1.
9016000	9020000	So they are the columns of the identity matrix.
9020000	9024000	And they are so special that we give them their own names.
9024000	9029000	We call them e1, e2, so forth, the e sub n, depending on what n is.
9029000	9035000	So this column here would be e sub 1, this one e sub 2, and a 2 by 2 matrix.
9035000	9041000	Here for 3 by 3, this would be e sub 1, this e sub 2, this e sub 3.
9041000	9048000	So what they actually are depends on the size of the matrix that we're talking about.
9048000	9052000	There it is with 2 by 2.
9052000	9059000	Now, as we saw in the previous example, if you know t of e1, t of e2, and so forth,
9059000	9065000	if you know what you get when you apply the transformation to these columns of the identity matrix,
9065000	9069000	then you can compute t of x for any vector x.
9069000	9073000	That's what we showed in that earlier example.
9073000	9077000	So here it is written in a little more formal manner.
9077000	9086000	If we have t that maps rn to rm, then there is a unique matrix A such that t of x equals ax.
9086000	9094000	In fact, A is the m by n matrix whose jth column is the vector t of ej,
9094000	9097000	where ej is the jth column of the identity matrix.
9097000	9103000	And so if you think back to the example we just did, this is exactly what we came out with.
9103000	9110000	What our matrix A was t of e1 and t of e2, because we had a 2 by 2 example.
9110000	9117000	This matrix A is called the standard matrix for the linear transformation t.
9117000	9121000	So it's a special matrix.
9121000	9129000	Now, we're interested in determining conditions under which a transformation has an inverse.
9129000	9134000	So that means if you have a transformation that maps rn to rm,
9134000	9144000	then given some b in rm, can we find an x in rn that maps to that b?
9144000	9148000	Or can we find x such that t of x equals b?
9148000	9156000	Let's go back to our functions just defining the real numbers first of all.
9156000	9163000	So before we looked at the function f of x equals x squared.
9163000	9166000	And this is what it looks like.
9166000	9169000	Now, a couple things to notice.
9169000	9175000	One is that you never get a negative number.
9175000	9181000	When you plug in any x, no matter what x you plug in, since we're starting with real numbers,
9181000	9187000	no matter what real number x you plug in, you always get a non-negative value.
9187000	9189000	Just keep that in mind.
9189000	9195000	And also note that we can get the same y value.
9195000	9202000	So if I drew a horizontal line, I would hit a couple of places on this graph.
9202000	9211000	So for a given y value, we've got a couple different x values that can produce that y value.
9211000	9215000	So keep that in mind.
9215000	9222000	f of x equals x squared that we just looked at is not invertible for two reasons.
9222000	9226000	One, there exists y in r.
9226000	9232000	So there's some real numbers for which there's no x such that f of x equals y.
9232000	9236000	There's no x such that x squared equals y.
9236000	9238000	So consider y equals negative one.
9238000	9244000	We cannot come up with a real number x such that x squared equals negative one.
9244000	9249000	That means that there are values in the co-domain that are not in the range.
9249000	9252000	The co-domain here is the real numbers.
9252000	9261000	But the range for this function is all non-negative real numbers.
9261000	9269000	So we can never get a negative number when we apply this function.
9269000	9274000	So we say that this function is not on to the real numbers.
9274000	9278000	There are real numbers that don't get mapped to.
9278000	9281000	I'm going to define that term in just a minute.
9281000	9290000	But I want to give you an example, something that's pretty easy for you to grasp.
9290000	9293000	So I said f is not invertible for two reasons.
9293000	9295000	Here's the second one.
9295000	9298000	That there exists y in r.
9298000	9305000	So there exists real numbers y for which a squared is y and b squared is y.
9305000	9307000	But a and b are not equal.
9307000	9312000	And that's that horizontal line thing I was talking about earlier.
9312000	9318000	So for example, if y is four, we can get that with two because two squared is four.
9318000	9323000	We can also get there with negative two because negative two squared is four.
9323000	9330000	So if we were trying to get an inverse, we started with four and we wouldn't know how did we get there.
9330000	9335000	Well, we don't know because it could have been from two or it could have been from negative two.
9335000	9342000	So we say that this function is not one to one because of this reason.
9342000	9346000	Okay, so here's a little more formal way to look at it.
9346000	9355000	We say a transformation from rn to rm is onto rm, onto the co-domain.
9355000	9368000	If each b in rm is the image of, or another way to think of it is, is mapped to by at least one x in rn.
9368000	9375000	So every b in the co-domain is mapped to by at least one x in the domain.
9375000	9380000	So every b gets mapped to.
9380000	9388000	Okay, so this means that no matter what vector b you choose from rm, there's some x that maps to it.
9388000	9395000	So for every b in rm, there's some x such that t of x equals b.
9395000	9404000	And since t of x equals a times x, this means that for every b, there's some x such that a times x equals b.
9404000	9414000	If there's an x such that t of x is b, then it has to be the same x that makes a times x equals b.
9414000	9421000	Now, when is it true that no matter what right hand side you have, you'll always have a solution?
9421000	9427000	Well, that's true when you have a pivot position in every row of a.
9427000	9438000	So if there's a pivot position in every row of a, then the transformation will be on two.
9438000	9440000	Okay, let's talk about one-to-one.
9440000	9453000	Transformation from rn to rm is said to be one-to-one if each b in rm is the image of, or is mapped to by at most one x in rn.
9454000	9458000	So remember back to the function f of x equals x squared.
9458000	9464000	This wasn't true because you could get to four from two and from negative two.
9464000	9473000	So there was an element in the range that was mapped to by more than one element in the domain.
9473000	9475000	So it wasn't one-to-one.
9475000	9485000	Here one-to-one means each b in rm is mapped to by at most one x in rn.
9485000	9495000	So that means that for each b in rm, there's at most one vector x such that t of x equals b.
9495000	9505000	So that means that since t of x equals ax, if we look at the system ax equals b, it can't have an infinite number of solutions.
9505000	9509000	Because then there would be more than one x that maps to that b.
9509000	9516000	And if our system can't have an infinite number of solutions, that means it can't have free variables.
9516000	9521000	So x equals b can have no free variables.
9521000	9524000	And it has no free variables.
9524000	9530000	That means there must be a pivot position in each column of a.
9530000	9540000	So the transformation is one-to-one if there's a pivot position in each column of a.
9540000	9545000	So one-to-one, there's a pivot position in each column.
9545000	9549000	Onto, there must be a pivot position in each row.
9549000	9562000	So that's how you can keep straight one-to-one and onto.
9562000	9568000	This is section 2.1 on matrix operations.
9568000	9572000	Just some basics out of the way first.
9572000	9581000	We refer to an m by n matrix A as a matrix with m rows and n columns.
9581000	9597000	And the individual elements in the matrix we denote by lowercase a with a subscript ij where i is the row index and j is the column index.
9597000	9604000	The diagonal entries in a matrix A are those where the row and column indices are the same.
9604000	9607000	So like a11, a22, and so forth.
9607000	9614000	These elements we call the main diagonal of A.
9614000	9624000	A diagonal matrix is a square matrix where all the entries off the main diagonal are zero.
9624000	9631000	So the only place you can have non-zero values is on the diagonal.
9631000	9634000	So here are a couple of diagonal matrices.
9634000	9641000	Notice that the key is off the diagonal, you have all zeros.
9641000	9646000	So the main diagonal is just a 11, 22, and so forth.
9646000	9649000	33 in this case.
9649000	9653000	So off the diagonal, off the main diagonal you have all zeros.
9653000	9662000	It's okay to have a zero on the diagonal, we really don't care what's on the diagonal, we just want zeros off the diagonal.
9662000	9666000	Here's another diagonal matrix.
9666000	9670000	This one has a name, we call it the identity matrix.
9670000	9677000	So it's a diagonal matrix because everything off the diagonal is zeros.
9677000	9688000	And the zero matrix is technically a diagonal matrix because everything off the main diagonal is zero.
9688000	9695000	We say that two matrices are equal if they have the same size, so the same number of rows, same number of columns,
9695000	9698000	and their corresponding entries are equal.
9698000	9703000	So the one one entry in one is equal to the one one entry in another.
9703000	9710000	In general, the i jth entry in one is equal to the i jth entry in the other.
9710000	9712000	We can compute the sum of two matrices.
9712000	9715000	We add two matrices together.
9715000	9719000	This is defined if both matrices are the same size.
9719000	9723000	So they have the same number of rows and same number of columns.
9723000	9738000	And in this case, the i jth entry in the sum is just the sum of the i jth entries in each of the original matrices.
9738000	9747000	So here's an example one that this sum cannot be computed because these matrices are not the same size.
9747000	9750000	Here's one where we can't compute the sum.
9750000	9757000	And notice the seven here is just the three plus four to get the three.
9757000	9760000	It's this two plus one and so forth.
9760000	9771000	So to get the i jth entry over here, you sum the corresponding i jth entries in the two original matrices.
9771000	9774000	We can compute the scalar multiple of a matrix.
9774000	9778000	In the same way, we compute the scalar multiple of a vector.
9778000	9782000	We just multiply each entry by that scalar.
9782000	9784000	So here's an example.
9784000	9795000	Multiply three times each entry in this matrix and we produce this one.
9795000	9802000	Your book lists and properties of addition and scalar multiplication of matrices.
9802000	9806000	The first three are regarding addition.
9806000	9815000	So you'll find that since addition of matrices is defined essentially the same as addition of real numbers,
9815000	9818000	then a lot of those same properties carry over.
9818000	9820000	So a plus b is b plus a.
9820000	9828000	The second one allows you to associate the parentheses as you'd like.
9828000	9830000	So that's the associative property.
9830000	9837000	And if you add any matrix to the zero matrix, you get the same matrix back.
9837000	9842000	Then the last three are regarding scalar multiplication.
9842000	9850000	And essentially they say that you can kind of stick the scalar wherever you'd like to, wherever it's convenient.
9850000	9856000	All is equivalent.
9856000	9861000	Okay, let's move on to matrix multiplication.
9861000	9869000	Because matrix multiplication is not defined simply as matrix addition is.
9869000	9871000	It's more complicated.
9871000	9875000	It's not just multiplying the corresponding entries together.
9875000	9877000	So here we go.
9877000	9882000	If a is an m by n matrix and the sizes are important here.
9882000	9888000	So suppose a is m by n and b is n by p.
9888000	9890000	Okay, so that's the key.
9890000	9900000	The number of columns in the first has to equal the number of rows in the second for the product to be defined.
9900000	9910000	Okay, so if you have that situation and we say that the columns of b are b1 through bp,
9910000	9917000	then we define the product ab as the m by p matrix.
9917000	9923000	And notice that the m is number rows in a, p's number columns in b.
9923000	9927000	So we kind of always look to see if these inner dimensions match up.
9927000	9931000	So the n here matches up with the n here.
9931000	9933000	And that means the product is defined.
9933000	9940000	And then the product itself, the dimensions of it will be the first, the number of rows,
9940000	9944000	which is m by the number of columns of b.
9944000	9947000	So it will be m by p.
9947000	9950000	Okay, so how do we compute a times b?
9950000	9953000	Well, notice what this is.
9953000	9959000	It's the m by p matrix whose columns are a times b1 out to a times bp.
9959000	9963000	That is, we want to multiply a times b.
9963000	9968000	Then we look at it as a times each of the columns of b.
9968000	9976000	Okay, so the first entry, the first column of the product is a times the first column of b.
9976000	9983000	The second column of the product is a times the second column of b, and so forth.
9983000	9993000	So notice that each column of the product, each column of a, b, is of the form a times b1 or a times b2.
9993000	10004000	And we know that when we compute a times some column, then we are taking a linear combination of the columns of a
10004000	10009000	using the entries in the vector as the weights.
10009000	10018000	So to get the first column of a times b, we're taking a linear combination of the columns of a using the first column of b as the weights.
10018000	10030000	To get the second column, another linear combination of the columns of a using the entries in the second column of b as the weights, and so forth.
10030000	10037000	Okay, so if we want to compute the product a, b, where here are matrices a and b,
10037000	10040000	then we're going to do it as I just described.
10040000	10046000	First, note that a is a 3 by 2 matrix and b is 2 by 2.
10046000	10050000	So we check to see if the 2 here matches up with the 2 here, which it does.
10050000	10060000	And then we look at the outer dimensions, the 3 and the 2, and that will be the dimensions of the product.
10060000	10063000	So a product will be 3 by 2 in this case.
10064000	10069000	To get the first column of the product, it's a times the first column of b.
10069000	10075000	So here's a, first column of b, 4, negative 1, and this is just a linear combination.
10075000	10080000	So 4 times the first column of a minus 1 times the second column.
10080000	10083000	And this is what we get.
10083000	10086000	Do the same thing to get the second column of the product.
10086000	10089000	It's a times the second column of b.
10089000	10092000	Second column of b is 1, 8.
10092000	10098000	So we take that linear combination, 1 times this first column of a, plus 8 times the second column.
10098000	10100000	Do that linear combination.
10100000	10105000	And then we have our first and second columns of the product.
10105000	10108000	And so we're done.
10108000	10113000	There is a times b.
10113000	10116000	There's another way to compute a, b.
10116000	10122000	Probably most of you learned this when you took algebra, college algebra probably.
10122000	10128000	Notice that it's defined in terms of inner products.
10128000	10139000	So the i, jth entry in the product, a, b, is the inner product of row i of a with column j of b.
10139000	10141000	So let's see how that works.
10141000	10144000	So this is back to our same a and b.
10144000	10157000	And to get the 1, 1 entry in the product, then it's going to be row 1 of a times column 1 of b.
10157000	10160000	Actually, these should not be a's.
10160000	10162000	They should be the product.
10162000	10164000	I'm talking about the product here.
10164000	10168000	So row 1 of a times column 1 of b.
10168000	10170000	Do that inner product.
10170000	10177000	So it's going to be 3, 2 inner product with 4, negative 1 to give you 10 in this case.
10177000	10178000	And so forth.
10178000	10186000	So to get to the 3, 2 entry of the product, it's row 3 of a times column 2 of b.
10186000	10190000	So 5 times 1 plus 4 times 8.
10190000	10192000	It gives you 37.
10192000	10193000	So there you go.
10193000	10201000	You have the same matrix as we ended up with before.
10201000	10202000	Okay.
10202000	10211000	Now you should note that since matrix multiplication is not defined simply because it's not a simple operation like matrix addition is,
10211000	10222000	then properties of real numbers that apply to multiplication do not follow over to matrix multiplication.
10222000	10224000	They did with matrix addition.
10224000	10227000	They don't with matrix multiplication.
10227000	10230000	So some warnings.
10230000	10235000	If you're dealing with real numbers, a times b equals b times a.
10235000	10238000	That does not hold true for matrices.
10238000	10242000	A times b is not necessarily equal to b times a.
10242000	10248000	Of course it sometimes could be, but in general, a times b is not equal to b times a.
10248000	10253000	One thing, one reason is that sometimes they're not even both defined.
10253000	10255000	Not even both defined.
10255000	10262000	Maybe you can multiply a times b, but you might not be able to multiply b times a and vice versa.
10263000	10269000	However, even if they are both defined, you can't guarantee they'll be equal.
10269000	10271000	So here's an example.
10271000	10276000	Here's a matrix a times b, and we get this product.
10276000	10281000	If we reverse the order, multiply b times a, we get a totally different matrix.
10281000	10290000	So just because you can multiply both, compute both products, that doesn't mean that they'll be the same result.
10290000	10294000	So don't make that mistake.
10294000	10296000	Second warning.
10296000	10303000	For real numbers, if you know that a times b equals a times c, and you know that a is not equal to zero,
10303000	10307000	you can divide both sides by a and end up with b equals c.
10307000	10310000	You can't do that with matrices.
10310000	10315000	If a, b is ac, that doesn't mean that b equals c.
10315000	10317000	Okay, here's an example of that.
10317000	10319000	Here's a matrix a.
10319000	10322000	A times b is this matrix.
10322000	10324000	Here's a times c.
10324000	10330000	Get the same matrix, but b and c are clearly different.
10330000	10332000	And another one.
10332000	10334000	The last warning.
10334000	10341000	For real numbers, if a times b is equal to zero, you can assume that either a or b is equal to zero,
10341000	10343000	or perhaps both are equal to zero.
10343000	10345000	Not true for matrices.
10345000	10349000	You have a product of two matrices that equals a zero matrix.
10349000	10353000	That doesn't mean that a or b is equal to the zero matrix.
10353000	10355000	And here's an example of that.
10355000	10359000	Here's matrix a times b, and we end up with a zero matrix.
10359000	10365000	But neither of these is equal to the zero matrix, far from it, in fact.
10365000	10370000	Okay, some properties of matrix multiplication.
10370000	10372000	You have the associative property.
10372000	10380000	So you can move the parentheses around, but notice that the order that the matrices appear is unchanged.
10380000	10386000	Okay, so we can't move the, we can't change the order that the matrices appear.
10386000	10391000	Can change the order in which we do the multiplication.
10391000	10396000	We can distribute either from the left or from the right here in numbers two or three.
10396000	10397000	Oops.
10397000	10398000	Back, back, back.
10398000	10399000	Oops.
10399000	10401000	Sorry about that.
10401000	10406000	And if we have a scalar, we can move it wherever we'd like.
10406000	10414000	And apparently I thought that was quite an important thing since I've got an exclamation mark there.
10414000	10419000	And any matrix multiplied by the identity is just itself.
10419000	10423000	Now notice that I'm assuming a is m by n here.
10423000	10427000	So if I multiply on the left, I have to multiply by the m by m identity.
10427000	10434000	Whereas if I multiply on the right, I have to multiply by the n by n identity.
10434000	10435000	Okay.
10435000	10450000	One other thing, the transpose of a matrix is computed by taking the ijth entry of the original matrix,
10450000	10454000	and that becomes the j ith entry of the transpose.
10454000	10463000	So for example, if this is your matrix a, then I take the one one entry here becomes one one entry here.
10463000	10467000	One two entry becomes the two one entry and so forth.
10467000	10473000	I mean, perhaps an easier way to think of it is that the columns of a become the rows of a transpose.
10473000	10480000	So column one is three one five row one of the transpose three one five and so forth.
10480000	10485000	Or alternatively, the rows of a become the columns of the transpose.
10485000	10491000	So here's a row first row here three two first column here three two and so forth.
10491000	10494000	Some properties of the transpose.
10494000	10499000	If I take the transpose of the transpose of a matrix, I'm back where I started.
10499000	10509000	You can look here if I take the transpose of this matrix, then I end up with a row one here becomes column one of a and so forth.
10509000	10514000	So the transpose of the transpose is the original matrix.
10514000	10526000	Transpose of a sum is the sum of the transposes number two can move a scalar around number three number four kind of interesting.
10526000	10536000	If you want to take the transpose of a product that is the product of the transposes but in reverse order reverse order.
10536000	10539000	So here's an example of that.
10539000	10543000	If I want to compute the transpose of a product AB.
10543000	10545000	So here's a here's B.
10545000	10549000	I want to compute their products then take the transpose.
10549000	10558000	So I compute the product here it is transpose that and end up with this matrix.
10558000	10560000	Or I could do it this way.
10560000	10566000	Transpose each of them initially but reverse the order that I do the multiplication.
10566000	10569000	So here's B transpose right here's B up here.
10569000	10571000	So take the transpose that's here.
10571000	10586000	Here's a transpose and I compute that product and I get the same thing as I did previously.
10586000	10590000	Okay let's talk about the inverse of a matrix.
10590000	10596000	First let's go back and think about the inverse of just a scalar or real number a.
10596000	10607000	The multiplicative inverse as opposed to the additive inverse of a number a is just one over a assuming a is not equal to zero.
10607000	10612000	Because if we multiply a times one over a we get one or one over a times equals one.
10612000	10617000	And one is the identity element for multiplication.
10617000	10620000	We can use the same idea for matrices.
10620000	10625000	The inverse of a matrix a exists only under certain conditions.
10625000	10633000	So not every matrix has an inverse just like not every number has an inverse zero doesn't have an inverse.
10633000	10640000	Okay we're going to write the inverse of a as a with a superscript negative one.
10640000	10649000	And we have a times a inverse we just read this as a inverse equals the identity matrix.
10649000	10653000	Okay this is not the number one this is the identity matrix.
10653000	10657000	And also a inverse times a is the identity matrix.
10657000	10663000	Okay one of the requirements for a inverse to exist is that a is a square matrix.
10663000	10673000	Okay and you have to have that so that you can multiply on the left and on the right by the same matrix.
10673000	10678000	Okay let's start off by talking about finding the inverse of a two by two.
10678000	10684000	Okay in this case there happens to be a specific formula for the inverse.
10684000	10689000	So if we have a two by two matrix a that looks like this a, b, c, d.
10689000	10696000	Then a inverse exists if a times d minus b times c is not equal to zero.
10696000	10706000	Okay so that's a times d so multiply the elements on the main diagonal and subtract off the product of the elements on the off diagonal.
10706000	10714000	So a times d minus b times c I call it the crisscross applesauce a times d minus b times c.
10714000	10720000	Alright so if that's not zero a inverse exists and here's a formula for it.
10720000	10726000	Notice that we divide by a d minus b c that's why it has to be non-zero.
10726000	10737000	And then the matrix is formed from a by swapping the numbers in the a and d positions.
10737000	10741000	Okay so we swap a and d and negate d and c.
10741000	10747000	Okay so that's how I remember it and you swap a and d negate b and c.
10747000	10750000	So for example here's a matrix a.
10750000	10761000	If we look at the a d minus b c that's four times negative three so negative twelve minus negative nine times two that's negative eighteen.
10761000	10767000	So negative twelve minus negative eighteen would be negative twelve plus eighteen which is six.
10767000	10775000	So we're dividing one over six this thing here is six and then we swap the four and the negative three.
10775000	10784000	Okay they swap positions and negate the negative nine and two and then multiply that out and we get this matrix.
10784000	10792000	Okay so that's the inverse of a and if you check multiply a times a inverse you get the identity matrix.
10792000	10799000	If we multiply a inverse times a we also get the identity matrix.
10799000	10805000	Okay I also get another example here's another two by two matrix.
10805000	10815000	But oops in this case if we multiply the a d minus b c we got one times four minus two times two which gives us zero.
10815000	10821000	So that tells us that this matrix does not have an inverse.
10821000	10828000	And if you look at that matrix a little bit you see that the second column is just two times the first.
10828000	10833000	Okay those columns are multiples of each other so they're linearly dependent.
10833000	10840000	If we look at a in reduced echelon form it has a row of all zeros.
10840000	10850000	We have a free variable to the point we say that a is not row equivalent to the identity matrix.
10850000	10859000	And it turns out that if a any matrix a if it's not row equivalent to the identity matrix then it will not be invertible.
10859000	10868000	And if it is row equivalent to the identity matrix it will be invertible.
10868000	10873000	We don't have a formula for the inverse of a three by three.
10873000	10881000	So we need to develop a method for computing the inverse or for determining that the inverse does not exist.
10881000	10890000	So what we'd like to do is find a matrix let's just call it b such that a times b is equal to the identity matrix.
10890000	10896000	Okay so if we assume that a is n by n then that means that b also has to be n by n.
10896000	10905000	And we'll denote the columns of b by these column vectors b1, b2 through bn.
10905000	10917000	So recalling from the previous section if we multiply a times b we can write that in this form a times the matrix here's b.
10918000	10927000	And that equals just a times b1 in the first column, a times b2 in the second column and so forth.
10927000	10943000	So remember this that a times b you get the first column as a linear combination of the columns of a using the elements in b1 as the multipliers and so forth for the other columns.
10943000	10949000	So remember we want that to equal the identity matrix so here's the identity matrix.
10949000	10958000	So we want a times b1 to be the first column of the identity matrix, a times b2 to be the second column and so forth.
10958000	10966000	So in order to determine the matrix b here we need to solve n systems of equations.
10967000	10975000	So if we solve those n systems of equations we'll have an inverse if it exists.
10975000	10982000	So let's look at this matrix and see if we can follow that method.
10982000	10990000	It's a 2 by 2 so we could just use the formula we have but let's try applying this method that we just discussed.
10990000	10996000	So a times b, a times b1, a times b2 and we want that to equal the identity matrix.
10996000	11004000	So we have these two systems to solve, a times b1 is 1, 0, a times b2 is 0, 1.
11004000	11014000	So here we go to solve this system, set up an augmented matrix, do some row operations,
11014000	11023000	and notice that a, to this point you see that a is row equivalent to the identity matrix so that means that a inverse does exist.
11023000	11031000	And here's the first column, b1 is negative one half three half so that's the first column of the inverse.
11031000	11040000	And to get the second column we solve this system, a times b2 equals 0, 1, the second column of the identity matrix.
11040000	11055000	And again I set up my augmented matrix, tack on 0, 1, do some row operations, and notice that the row operations that I'm doing here are exactly the same ones that I did up here.
11055000	11059000	So first multiply row 1 by 1 fourth, same thing down here.
11059000	11064000	Then 9 times row 1 plus row 2, same as down here and so forth.
11064000	11067000	And that carries through the whole way.
11067000	11079000	And if you give that some thought, you'd say oh yeah that makes sense because the row operations that I'm doing are based on the entries in a.
11079000	11082000	I'm trying to get a in reduced echelon form.
11082000	11092000	So since a doesn't change, same a here and up here, then it makes sense that the row operations I do don't change.
11092000	11096000	The only thing that's changed is what's in the last column.
11096000	11101000	And so we end up with this for b2.
11101000	11108000	So I've got my two columns of b which is what the inverse of a is.
11108000	11116000	And if you look at this you think well that seems like to be a lot of repetitive work and it is.
11116000	11124000	And so we have a more efficient method and that is to solve both systems at one time.
11124000	11137000	So instead of just tacking on one column in an augmented matrix, we tack on both columns or the whole identity matrix depending on what size your original matrix is.
11137000	11146000	And here we go through that same sequence of row operations and we end up with the two columns of b that we found earlier.
11146000	11151000	And this of course is the inverse of a.
11151000	11164000	Now in general what this looks like is you start off with this matrix a on one side then augment on the identity matrix.
11164000	11171000	And you do row operations until you get a in reduced echelon form.
11171000	11175000	And what you're aiming for is to make it look like the identity matrix.
11175000	11180000	And they said that may or may not be possible depending on a.
11180000	11189000	If it is then you'll end up with the identity matrix here and the inverse of a on the right.
11190000	11193000	So that's what you're aiming for.
11193000	11200000	And just while we're here looking at this, notice that we could go backwards because we know that all these row operations are reversible.
11200000	11203000	We could go backwards and look what happens.
11203000	11212000	We start off with a inverse with the identity tacked on and we get back here and we can make a inverse look like the identity matrix.
11213000	11226000	And so with a over here so that tells you that if you wanted to take the inverse of a inverse then you just get the original matrix a back.
11226000	11231000	So the inverse of a inverse is just a.
11231000	11234000	Okay so what happens if a is not invertible?
11234000	11239000	In this case it's not row equivalent to the identity.
11239000	11243000	So here's the example we had before where the second column is a multiple of the first.
11243000	11246000	Tack on the identity matrix.
11246000	11253000	Do one row operation here and you get to this point and you say a is not row equivalent to the identity matrix.
11253000	11259000	And hence I can't get it in this form where I've got the identity and then a inverse.
11259000	11265000	That tells you that the matrix a does not have an inverse.
11265000	11274000	Okay so if you can't get a to look like the identity matrix by doing row operations then a is not invertible.
11274000	11278000	Or a inverse does not exist.
11278000	11286000	Alright now this method works no matter what size your matrix is so I scaled it up for a three by three.
11286000	11293000	Okay start off with this matrix a and tack on the three by three identity matrix.
11293000	11298000	And go through some row operations.
11298000	11301000	A lot of arithmetic here.
11301000	11308000	And if you look at the last matrix I have here the first three columns look like the identity matrix.
11308000	11313000	And so sitting over here would be the inverse of a.
11313000	11322000	Alright so this last three columns would be the inverse of my matrix a.
11322000	11329000	If you know a inverse then solving a x equals b is trivial.
11329000	11335000	Alright because our whole focus in this course is solving a x equals b solving systems of equations.
11335000	11347000	And so if you know a inverse then it turns out that it's super simple to solve a system of equations involving a.
11347000	11350000	And basically here's the rationale.
11350000	11353000	Start off with a x equals b.
11353000	11359000	And you know a inverse exists and that means you can multiply both sides of your system by a inverse.
11359000	11361000	So I do that.
11361000	11368000	And the reason to do that is because if you look over here we can re-associate the parentheses.
11368000	11372000	And so we can end up with a inverse times a together.
11372000	11376000	And the advantage of that is that that equals the identity matrix.
11376000	11380000	So we end up with just the identity matrix times x.
11380000	11384000	And anything times the identity matrix is just that anything.
11384000	11386000	So i times x is x.
11386000	11388000	So x is simply a inverse b.
11388000	11392000	So if you know a inverse then you don't need to do row operations.
11392000	11394000	You don't need to do any of that stuff.
11394000	11399000	Just multiply a inverse times b and you have the solution to your system.
11399000	11403000	Clearly that's going to save you a lot of work if you have a inverse.
11403000	11408000	So if I have this matrix a and this vector b and I want to solve a x equals b.
11408000	11413000	If I don't know a inverse then I got to go through you know set up my augmented matrix.
11413000	11415000	Do all these row operations.
11415000	11421000	And I end up here with my solution for negative 7, negative 16.
11421000	11425000	But if I know a inverse.
11425000	11427000	So I know a inverse.
11427000	11430000	So here's a here's b and I know a inverse.
11430000	11432000	Then to solve a x equals b.
11432000	11435000	As I said just multiply a inverse times b.
11435000	11438000	And here's your solution.
11438000	11444000	So back went through all this work to get 4, negative 7, negative 16.
11444000	11450000	If you know a inverse all you do is simple multiply matrix times a vector.
11450000	11452000	And you have the solution.
11452000	11458000	Now of course the downside is that most of the time you don't have a inverse.
11458000	11465000	And to get a inverse well you saw what work is required in that.
11465000	11470000	More work than simply going back and through to do this.
11470000	11477000	Because to find a inverse for a 3 by 3 we had to solve let's go back right.
11477000	11481000	We had to solve three systems of equations.
11481000	11487000	Just to solve this system you need to solve one system of equations.
11487000	11494000	But if you know a inverse then solving system is trivial.
11494000	11497000	Okay let's see.
11497000	11500000	Your book lists some properties of matrix inverses.
11500000	11502000	This one we've already talked about.
11502000	11506000	Take the inverse of a inverse you get a back.
11506000	11509000	If you have the inverse of a product.
11509000	11511000	So a b quantity inverse.
11511000	11516000	That is the product of the inverses.
11516000	11518000	But in reverse order.
11518000	11520000	So b inverse times a inverse.
11520000	11523000	Instead of a inverse times b inverse.
11523000	11527000	And if you take the inverse of a transpose of a matrix.
11527000	11540000	That's equivalent to transposing the inverse.
11540000	11545000	Okay the main part of section 2.3 is the invertible matrix theorem.
11545000	11552000	And this theorem actually ties together everything that we've done so far in the course actually.
11552000	11559000	Everything in chapter 1 and up through chapter 2 where we are.
11559000	11565000	So this theorem says if you have a given n by n matrix a.
11565000	11567000	The following are equivalent.
11567000	11572000	That means that they're either all true or all false.
11572000	11574000	Okay the first two look like this.
11574000	11577000	One a is an invertible matrix.
11577000	11581000	And number two a is row equivalent to the identity matrix.
11581000	11587000	So that should make sense because the way we found out if a is invertible.
11587000	11593000	Is to do those row operations and try to get it in the form of the identity matrix.
11593000	11596000	And if we could do that then that means a is invertible.
11596000	11601000	If we couldn't do that then a is not invertible.
11601000	11605000	Okay another statement a has n pivot positions.
11605000	11610000	So this is actually the key to the whole theorem in my opinion.
11610000	11618000	Everything here you can relate to pivot positions and that's how I would recommend that you approach this.
11618000	11624000	So looking ahead you need to know what this theorem says.
11624000	11628000	You need to be able to write down parts of this theorem.
11628000	11636000	I don't suggest that you memorize it because I think that's somewhat difficult.
11636000	11643000	But I don't so don't memorize it unless that works for you.
11643000	11649000	But the way I would suggest that you remember it is to relate everything back to pivot positions.
11649000	11653000	Because that's what we've done up to now and that should still work for you.
11653000	11655000	So let's look at these next three.
11655000	11661000	And number four says the equation ax equals zero has only the trivial solution.
11662000	11671000	Okay so if it has only the trivial solution then that means that there are no free variables.
11671000	11674000	That means every column of a is a pivot column.
11674000	11679000	And so that means since there are n columns there's n pivot columns.
11679000	11681000	So n pivot positions.
11681000	11684000	Okay so that relates back to pivot positions.
11684000	11688000	Number five the columns are linearly independent.
11688000	11689000	How do we know that?
11689000	11696000	Well again no free variables so pivot position in every column are in pivot positions.
11696000	11700000	And number six the transformation x to ax is one to one.
11700000	11702000	And how do we know that?
11702000	11709000	Well that means we want to have a unique solution to every system ax equals b.
11709000	11711000	Or at most one solution.
11711000	11716000	They know multiple solutions for any right hand side.
11716000	11719000	So that means that we can't have any free variables.
11719000	11722000	So again pivot position in every column.
11722000	11728000	So four, five, and six all relate to the fact that you need a pivot position in every column.
11728000	11734000	These next four relate to having a pivot position in every row.
11734000	11743000	Okay which again since it's an n by n matrix that would mean you have n pivot positions.
11743000	11749000	So number seven says ax equals b has at least one solution for each b and rn.
11749000	11754000	So if that's straight that means that system is consistent no matter what the right hand side is.
11754000	11758000	That only happens when you have a pivot position in every row.
11758000	11762000	N rows means n pivot positions.
11762000	11764000	Columns of a span are n.
11764000	11769000	Okay that again means that there's a pivot position in every row.
11769000	11774000	But no matter what you put on the right side the system will be consistent.
11774000	11778000	So again pivot position every row so n pivot positions.
11778000	11783000	And number nine x to ax is on to rn.
11783000	11790000	So that means again there's a solution no matter what the right hand side of the system ax equals b is.
11790000	11795000	So there has to be a pivot position in every row.
11795000	11798000	And then last a transpose is invertible.
11798000	11805000	It's kind of a tag along there but if a is invertible a transpose is invertible.
11805000	11814000	Okay so I've taken a few of the even numbered problems from this section just to kind of talk through those
11814000	11819000	to give you an idea about how to go through some of the logic here.
11819000	11820000	So here's one.
11820000	11828000	Is it possible for a five by five matrix to be invertible when its columns do not span r5?
11828000	11830000	Okay well let's see.
11830000	11838000	If the columns don't span r5 then that means that there's not a pivot position in every row.
11838000	11842000	And that means that the matrix is not row equivalent to the identity matrix.
11842000	11848000	Or you could say that there's less than five pivot positions.
11848000	11858000	Either one of those gets you to the matrix cannot be invertible.
11858000	11860000	Alright here's one.
11860000	11869000	Got a six by six matrix and the equation cx equals v is consistent for every v in r6.
11869000	11876000	Is it possible that for some v the equation cx equals v has more than one solution?
11876000	11887000	Okay well if cx equals v is consistent for every v in r6 then that means that there must be a pivot position in every row.
11887000	11897000	And since this is a square matrix if there's a pivot position in every row then there's also one in every column.
11897000	11902000	And if there's a pivot position in every column that means there's no free variables.
11902000	11911000	And so hence any system will, there will be no system that has an infinite number of solutions.
11911000	11914000	Only a unique solution.
11914000	11925000	Alright let's suppose h is n by n and the equation hx equals c is inconsistent for some c.
11925000	11932000	Does the system hx equals zero have non-trivial solutions?
11932000	11942000	Well if hx equals c is inconsistent for some right hand side then there can't be a pivot position in every row.
11942000	11948000	And that means that there can't be a pivot position in every column since h is a square matrix.
11948000	11952000	And if there's not a pivot position in every column then that means you have free variables.
11952000	11960000	And hence there are non-trivial solutions to hx equals zero.
11960000	11962000	Okay another one.
11962000	11968000	If l is n by n and lx equals zero has only the trivial solution.
11968000	11971000	Do the columns of l span rn?
11971000	11977000	Well if lx equals zero has only the trivial solution then that means there's no free variables.
11977000	11987000	So we have a pivot position in every column of l which means we have a pivot position in every row of l since l is square.
11987000	11995000	And if there's a pivot position in every row then that means that the columns span rn.
11995000	12000000	Alright let's talk about invertible linear transformations.
12000000	12008000	Let's suppose we have a linear transformation t from rn to rn that's defined by the matrix a.
12008000	12012000	So a here would be n by n.
12012000	12017000	Alright for any x we can compute ax right?
12017000	12023000	For any x we can compute t of x or a times x.
12023000	12026000	Let's think about going backwards though.
12026000	12039000	Suppose you have some b and you would like to know is there a unique vector x such that ax equals b?
12039000	12052000	Okay so think of b as being in the co-domain and we want to go backwards to see what x mapped to that b.
12052000	12057000	Well this works if t is both one to one and onto.
12057000	12070000	Okay so for us to be able to go backwards and find out what x mapped to b we can do that if we know that t is one to one and onto.
12070000	12072000	Okay so why is that?
12072000	12081000	Well if t is onto then that means that every b and rn is mapped to by at least one x.
12081000	12085000	Every b is mapped to by at least one x.
12085000	12091000	So that means that if we go backward there's an x to go back to.
12091000	12098000	If it's one to one then that means that every b is mapped to by at most one x.
12098000	12101000	Every b is mapped to by at most one x.
12101000	12107000	So that means that if we can go backwards there's only one vector to go backwards to.
12107000	12111000	We don't have multiple vectors mapping to the same b.
12111000	12120000	So if you put those two things together t is one to one and onto then every b is mapped to by exactly one x.
12120000	12124000	Which means there's a unique solution.
12124000	12126000	So how do we find that x?
12126000	12132000	Given a particular vector b how do we find out what x mapped to it?
12132000	12146000	Well we need to solve ax equals b and the way to do that if we know that a is invertible is to simply multiply a inverse times b.
12146000	12153000	So the matrix that defines the inverse transformation t inverse is simply a inverse.
12153000	12170000	So a takes you forward from x you multiply a times x to get b in the co-domain to go backwards to reverse that.
12170000	12173000	You multiply by a inverse.
12173000	12180000	So t is defined by a, t inverse is defined by a inverse.
12181000	12184000	Another way to look at it is this.
12184000	12194000	Under what conditions does there exist a unique vector x such that ax equals b for every b in our n?
12194000	12205000	When can we know that the system ax equals b is always consistent and always has a unique solution?
12205000	12210000	Well it's consistent if there's a pivot position in every row of a.
12210000	12217000	Ax equals b is consistent for every b if there's a pivot position in every row of a.
12217000	12222000	And if the solution is unique then there has to be no free variables.
12222000	12226000	So that means there has to be a pivot position in every column of a.
12226000	12231000	So backing up it's consistent.
12231000	12235000	If it's consistent for every b we have to have a pivot position in every row.
12235000	12243000	If there's a unique solution then that means we have no free variables so we have to have a pivot position in every column.
12243000	12252000	So t of x equals ax is invertible when a has a pivot position in every row and column.
12252000	12259000	i.e. when a has n pivot positions i.e. when a is invertible.
12266000	12269000	Okay we're going to talk about determinants.
12269000	12279000	The determinant of a matrix is just a scalar value that is associated with any square matrix.
12279000	12284000	So we only talk about the determinant of a square matrix.
12284000	12298000	The notation is d e t of a or sometimes you see it with a looks like absolute value of a with the vertical bars around a.
12298000	12305000	We've actually already seen the determinant in the 2 by 2 case.
12305000	12313000	If we have a is just this generic matrix a b c d then the determinant of a is a times d minus b times c.
12313000	12321000	And we've seen that because we saw that in the little formula for the inverse of a 2 by 2.
12321000	12327000	So remember we multiplied by 1 over a d minus b c.
12327000	12331000	So we're multiplying by 1 over the determinant of the matrix.
12331000	12338000	And then we kind of rearrange the terms and negate couple to get the inverse of a matrix.
12338000	12350000	So notice that at least in the case of a 2 by 2 matrix we can see that the inverse exists when the determinant of the matrix is not equal to 0.
12350000	12357000	Here in this case if a d minus b c is equal to 0 then a inverse does not exist.
12357000	12361000	And it turns out that this is true for any square matrix.
12361000	12369000	If the determinant is not equal to 0 that means the matrix is invertible and it's invertible then the determinant is not equal to 0.
12369000	12379000	So this is actually another installment in the invertible matrix theorem which we had from section 2.3.
12379000	12388000	So determinant of a not equal to 0 is logically equivalent to a is invertible.
12388000	12396000	Alright if we move up beyond a 2 by 2 there's no nice formula for competing the determinant like there was for a 2 by 2.
12396000	12400000	And we use a method called cofactor expansion.
12400000	12407000	And this method works for any size matrix 3 by 3 on up.
12407000	12412000	In this method we have to choose a row or column to expand about.
12412000	12416000	That's what we call it. We're going to choose a row or column to expand about.
12416000	12425000	You'll see as we go on that it's advantageous to choose a row or column that has the most zeros in it because that eliminates some of the terms.
12425000	12432000	But for our first cut here I'm just going to expand about the first row.
12432000	12439000	And I've kind of color coded this to make it easier for you to see where the terms come from.
12439000	12445000	Okay so if we expand about the first row we start with the first entry which is the 5.
12445000	12457000	And then we multiply 5 by the determinant of the matrix that you're left with if you eliminate the row or column that contains the 5.
12457000	12467000	So if you eliminate the first row and eliminate the first column then you see that you have this little 2 by 2 matrix that's sitting right here.
12467000	12470000	And so we're going to take the determinant of that.
12470000	12475000	And then we move over to the next entry in the first row which is the 2.
12475000	12484000	And we multiply that by the determinant of the matrix that you get if you eliminate the first row and the second column.
12484000	12487000	Okay the row and the column containing the 2.
12487000	12493000	So we're left with the 0, 2 here and the negative 5, 7 here.
12493000	12496000	Okay so that's how we get this matrix here.
12496000	12499000	Then we continue moving across the first row.
12499000	12508000	Then we've got a 4 and we multiply 4 by the matrix that you get if you eliminate the first row and the third column.
12508000	12510000	Okay eliminate the row and the column containing the 4.
12510000	12515000	So you're left with this little 2 by 2 here, 0, 3, 2, negative 4.
12515000	12517000	And that's what we have here.
12517000	12520000	Now notice I haven't combined these terms at all.
12520000	12522000	I've just written them out here.
12522000	12527000	And that's because there's a method for combining the terms.
12527000	12530000	Okay we have to put them together somehow.
12530000	12541000	And basically you apply a coefficient to each term which is negative 1 to the i plus j.
12541000	12547000	Where i and j are the row and column indices corresponding to that term.
12547000	12553000	So if we look back up to the 5, 5 came from row 1 column 1.
12553000	12558000	So the coefficient that we put in front of that term is minus 1 to the 1 plus 1.
12558000	12561000	Because 5 came from row 1 column 1.
12561000	12569000	Alright then for the next term, this one, the 2, the blue term, came from this entry here.
12569000	12573000	The 2 is in the first row second column.
12573000	12577000	So that means the coefficient here is minus 1 to the 1 plus 2.
12577000	12581000	Row 1 column 2, that's where the 1 plus 2 comes from.
12581000	12588000	And then for the 4, the green term, it's in the first row third column.
12588000	12596000	So we have minus 1 to the 1 plus 3 as the coefficient in front of that term.
12596000	12604000	And so if we multiply and add, then we get minus 1 to the 1 plus 1.
12604000	12606000	So that's minus 1 squared.
12606000	12608000	So that's just plus 1 times 5.
12608000	12610000	So we bring down 5.
12610000	12613000	And the determinant here, remember it's just the crisscross.
12613000	12617000	3 times 7 minus negative 4 times negative 5.
12617000	12619000	So that's what we have here.
12619000	12622000	Alright, then moving on here to the blue.
12622000	12624000	We got minus 1 to the 1 plus 2.
12624000	12626000	That's minus 1 cubed.
12626000	12629000	So that's negative 1 times 2.
12629000	12632000	Gives us the negative 2 here.
12632000	12638000	Times the determinant which is 0 times 7 minus 2 times negative 5.
12638000	12643000	Alright, then moving over to the next term, the green one comes from the 4.
12643000	12645000	And that's first row third column.
12645000	12649000	So that's the minus 1 to the 1 plus 3.
12649000	12654000	Which is minus 1 to the fourth, which is positive 1 times 4.
12654000	12659000	And so you get a plus 4 in front of that term.
12659000	12665000	Times the determinant which is 0 times negative 4 minus 2 times 3.
12665000	12668000	Alright, then combining a little more.
12668000	12672000	We got 21 minus 20 here.
12672000	12674000	Times 5.
12674000	12677000	Then we've got 0 plus 10.
12677000	12679000	So just a 10 there.
12679000	12681000	And then here's 0 minus 6.
12681000	12683000	So negative 6 there.
12683000	12690000	And we combine and we end up with negative 39 for the determinant of this matrix.
12690000	12693000	Okay, now just a little aside.
12693000	12701000	It's really not necessary to explicitly compute this minus 1 to the i plus j term every time.
12701000	12704000	If you just look, here's a 4 by 4.
12704000	12709000	Where I've put as the entries in the matrix just the row index plus the column index.
12709000	12711000	So 1, 1 position, we got 1 plus 1.
12711000	12714000	1, 2 position, 1 plus 2, and so forth.
12714000	12718000	So if we look at what that is, we end up with these numbers.
12718000	12722000	And remember we want minus 1 raised to each of these powers.
12722000	12729000	And so you can see since they all differ by one, going from one term to an adjacent term,
12729000	12731000	either in the same row or the same column,
12731000	12735000	you either go from an even number to an odd or an odd to an even.
12735000	12741000	So in any case, alternating terms are always going to have opposite signs.
12741000	12746000	And you always start off in the upper left with a plus, plus 1,
12746000	12750000	because that's minus 1 that the 1 plus 1 or minus 1 squared.
12750000	12754000	So you always know that the 1, 1 position is a plus term,
12754000	12758000	and then everything alternates after that.
12758000	12760000	So it's always plus, minus, plus, minus.
12760000	12769000	So if you know the sign that goes with the first term in the row or the column that you're expanding about,
12769000	12773000	then you only need to alternate terms after that.
12773000	12776000	So you always have this checkerboard patterns.
12776000	12784000	And you just need to alternate terms based on the sign of the position where you start.
12784000	12789000	So let's look at this matrix again.
12789000	12794000	It's the same one, but I'm going to expand about the second column this time,
12794000	12796000	just for something different.
12796000	12801000	Now, first note that the second column or the first entry there is the 2,
12801000	12803000	and it's in the 1, 2 position.
12803000	12809000	So minus 1 to the 1 plus 2, if you want to compete it like that, minus 1 cubed, that's a negative 1.
12809000	12812000	So this is a negative position here.
12812000	12815000	Or you can say, I always know that the 1, 1 position is a plus.
12815000	12819000	So this is plus, the next one over has to be a minus.
12819000	12823000	That's typically how I do it.
12823000	12830000	So we end up with minus 2 times, again, the determinant of the matrix that you get
12830000	12833000	if you eliminate the row and the column containing the 2.
12833000	12836000	So we end up with a 0, 2, negative 5, 7.
12836000	12838000	So that's where we end up with that matrix.
12838000	12841000	Alright, then we move to the 3.
12841000	12845000	Now since the 2 is a negative, the 3 is going to be a plus.
12845000	12850000	And we eliminate the row and the column containing the 3.
12850000	12854000	So we end up with 5, 4, 2, 7.
12854000	12857000	And then we move on to the negative 4.
12857000	12862000	So again, if you forget, we'll go back to the 1, 1, that's a plus.
12862000	12864000	Moving over, that's a minus.
12864000	12866000	Moving down, that's a plus.
12866000	12868000	Moving down again, that's a minus.
12868000	12870000	So we're going to subtract off.
12870000	12872000	That's where the minus comes from.
12872000	12877000	Minus negative 4 times the matrix that you get if you eliminate the last row in the middle column.
12877000	12880000	5, 0, 4, negative 5.
12880000	12883000	Alright, we compute those determinants.
12883000	12887000	Here we're going to have 0 times 7 minus 2 times negative 5.
12887000	12890000	Here 5 times 7 minus 2 times 4.
12890000	12894000	And here 5 times negative 5 minus 0 times 4.
12894000	12898000	And we can simplify.
12898000	12902000	And we end up with negative 39 again, which we should.
12902000	12906000	Because no matter which row or column you choose to expand about,
12906000	12911000	you should end up with the same answer.
12911000	12917000	Alright, let's move on to a 4 by 4.
12917000	12923000	Now we're going to take advantage of the fact that we've got a row with 3 zeros in it.
12923000	12928000	So I'm going to expand about the second row.
12928000	12940000	And if you notice the first entry in the second column.
12940000	12941000	Okay, there we go.
12941000	12944000	Expand about the second row to take advantage of the zeros.
12944000	12951000	So if we look, the 0 here is, or go back to the 1, that's a plus position.
12951000	12953000	So we move down, that's a minus.
12953000	12959000	So the first term is going to be minus 0 times the determinant of the matrix that you get
12959000	12963000	if you eliminate the second row in the first column.
12963000	12969000	So you can see how the first row is going to be negative 2, 5, 2.
12969000	12970000	That's what we get there.
12970000	12976000	Then we've got these two rows right here as the second two rows.
12976000	12979000	Alright, then we move over to the next entry.
12979000	12981000	Okay, plus here, minus.
12981000	12987000	This is a plus entry, so it's plus 0 times the determinant of the matrix that you get
12987000	12991000	when you eliminate the second row, second column.
12991000	12998000	Alright, then minus the next term, so minus 3 times the matrix that you get
12998000	13002000	if you eliminate second row, third column.
13002000	13008000	And then plus 0 times the determinant of the matrix that you get
13008000	13012000	when you eliminate the last column and the second row.
13012000	13015000	So that's where all these terms come from.
13015000	13019000	And you can see the advantage of choosing a row with a bunch of zeros
13019000	13026000	because the red term, the green term, and the orange term all just disappear
13026000	13030000	because they are all multiplied by 0.
13030000	13037000	The blue one though, we have to compute now a 3 by 3 determinant.
13037000	13042000	So we have negative 3 times the determinant of this matrix.
13042000	13046000	So we use the same method, choose a row or a column to expand about.
13046000	13048000	I chose the third row.
13048000	13054000	Okay, so I've got plus, minus, plus, 5's in a plus position.
13054000	13059000	So it's 5 times this 2 by 2 determinant here.
13059000	13066000	Then move over minus 0 times the 1, 2, 2, 5.
13066000	13073000	And then plus 4 times the determinant of this little matrix up here.
13073000	13078000	And then plus 0 because this term is multiplied by 0.
13078000	13083000	So you can see that everything simplifies except for this blue part.
13083000	13088000	And we go through and evaluate these determinants
13088000	13092000	and combine terms and simplify.
13092000	13096000	We end up with negative 6 for the determinant of this matrix.
13096000	13101000	Now just stop for a minute here and think about how much more work would have been required
13101000	13104000	if we didn't have any zeros in this matrix.
13104000	13107000	We would have had to done what we did with the blue part.
13107000	13111000	We would have had to do that for the red and the green and the orange.
13111000	13117000	So this turns into significant work fairly quickly.
13117000	13124000	Okay, it's a recursive method because, for example, to compute a 5 by 5 determinant,
13124000	13127000	you have to compute 5, 4 by 4 determinants.
13127000	13129000	Now this is assuming there's no zeros in the matrix.
13129000	13135000	So the worst case for a 5 by 5, you have to compute 5, 4 by 4 determinants.
13135000	13141000	And each of those 4 by 4 determinants requires competing 4, 3 by 3 determinants.
13141000	13145000	And each 3 by 3 means you have to compute 3, 2 by 2 determinants.
13145000	13153000	As you can see, this gets very work intensive very quickly.
13153000	13157000	The amount of work increases exponentially.
13157000	13161000	So clearly this method does not scale well at all.
13161000	13168000	As your matrix gets bigger, the amount of work required increases exponentially.
13168000	13176000	So it's not a good tool and a good method to use to compute the determinant for an arbitrary size matrix.
13176000	13178000	3 by 3 is okay.
13178000	13187000	4 by 4 really turns into too much work unless you've got some significant number of zeros there.
13187000	13195000	Okay, now let's go off on a little different tangent here for a bit.
13195000	13200000	Let's first define what a triangular matrix is.
13200000	13208000	A triangular matrix is a square matrix in which all the entries either above or below the main diagonal are zero.
13208000	13211000	Okay, so here are some examples.
13211000	13217000	I've highlighted the zeros in red just to make it clear.
13217000	13222000	The first one here has all zeros below the main diagonal.
13222000	13225000	We say that this matrix is upper triangular.
13225000	13229000	So all the interesting stuff is in the upper part of the matrix.
13229000	13237000	This next one, the middle one, is lower triangular because all the interesting stuff is in the lower part of the matrix.
13237000	13239000	Everything above the diagonal is zeros.
13239000	13241000	And then here's another one.
13241000	13245000	This one is upper triangular also.
13245000	13257000	And I just want to make it clear that what defines a triangular matrix is that either above or below, or even both, the diagonal you have to have all zeros.
13257000	13263000	So this one is upper triangular because below the diagonal we have all zeros.
13263000	13267000	And it's okay to have some zeros above the diagonal or even on the diagonal.
13267000	13271000	But that has really nothing to do with whether this is a triangular matrix or not.
13271000	13278000	This is a triangular matrix because everything below the diagonal is zeros.
13278000	13289000	Alright, so we have a theorem that says if A is a triangular matrix, then the determinant of A is the product of the entries on the main diagonal of A.
13289000	13296000	So this is making it easy to compute the determinant if your matrix is triangular.
13296000	13298000	It's pretty easy to see how that works.
13298000	13311000	If we take a triangular matrix, then if we were going to compute the determinant directly, then I would expand about the first column.
13311000	13316000	So my determinant would be one, that's plus.
13316000	13322000	So one times the determinant of the matrix that you get if you eliminate the first row and first column.
13322000	13325000	So you got this three by three that we take here.
13325000	13329000	And notice that all the rest of the terms would be zero.
13329000	13336000	So it's one times this determinant plus this zero here times, or actually minus, this zero here times another determinant.
13336000	13342000	Plus this zero times another determinant minus this zero times another determinant.
13342000	13346000	So the only term that's non-zero is the one associated with the one.
13346000	13349000	So that's the only one I've written here.
13349000	13355000	And then if we take the determinant of this three by three, notice we do the same thing, expand about the first column.
13355000	13359000	So we've got one times five, because five is in the one, one position.
13359000	13362000	So that's a plus.
13362000	13368000	So one times five times this two by two sitting right here.
13368000	13374000	And notice again, all the other terms in that column are zero, so we don't need to worry about them.
13374000	13378000	And so we end up with one times five times this determinant.
13378000	13381000	And so it's going to be eight times one minus zero times nine.
13381000	13383000	So it's just eight times one.
13383000	13391000	So our determinants one times five times eight times one, which you see are the entries on the main diagonal.
13391000	13395000	So if your matrix is triangular, life is good.
13395000	13396000	Life is easy.
13396000	13401000	Just multiply the entries on the diagonal.
13401000	13408000	Just thinking ahead just a little bit, notice that a triangular matrix is an echelon form.
13408000	13412000	Or if it's upper triangular, it's an echelon form.
13412000	13417000	If it was lower triangular, we could swap rows.
13417000	13419000	And we'd have to swap some columns too.
13419000	13425000	But let's just think about upper triangular at this point.
13425000	13428000	Now, so something to think about.
13428000	13435000	To compute the determinant of a large matrix, let's just say large is four by four or bigger.
13435000	13444000	Could we first put an echelon form and then just take the determinant by multiplying the diagonal entries?
13444000	13446000	Would that work?
13446000	13454000	Well, to answer that, we need to explore how elementary row operations affect the determinant of a matrix.
13454000	13462000	So if we can nail that down, then it is indeed possible that we could just put our matrix in echelon form
13462000	13466000	and then easily compute the determinant that way.
13466000	13475000	So that's what we'll ponder and we'll discuss that in the next section.
13475000	13480000	Okay, last time we talked about how to compute the determinant of a matrix.
13480000	13491000	And we discovered that as the matrix got larger, the amount of work required to compute the determinant increased dramatically.
13491000	13500000	Even a four by four without any zeros in it, as a considerable amount of works, take the determinant up.
13500000	13511000	So we also saw that if a matrix is triangular, then computing the determinant is easy.
13511000	13515000	You just multiply the diagonal elements.
13515000	13536000	And so we thought, hmm, I wonder if we could just do some row operations and either just generate some zeros or get the matrix totally in echelon form,
13536000	13547000	which would be upper triangular form, and then compute the determinant just so we can eliminate some of the work required.
13547000	13552000	So we're going to first talk today about how row operations affect the determinant.
13552000	13565000	Because if you do row operations, you need to know what effect that has on the determinant if you're going to try to go down that road in order to compute the determinant.
13565000	13571000	Okay, so first thing we'll look at is the effect of swapping rows.
13571000	13576000	So the example I have here, I just swapped the rows.
13576000	13581000	So the first matrix I call A, swap the rows, call that matrix B.
13581000	13587000	And obviously we know the determinant of A is just AD minus BC.
13587000	13597000	The determinant of B is BC minus AD, which is the negative of AD minus BC.
13597000	13603000	So we have the determinant of A equals minus the determinant of B.
13603000	13610000	And this holds true no matter what size your matrix is.
13610000	13621000	So we have a theorem that says if you have a matrix A and you exchange two rows or swap two rows of A to produce B,
13621000	13626000	then the determinant of B is just the negative of the determinant of A.
13626000	13634000	So if you swapped rows again, you negate the determinant again.
13634000	13637000	Suppose we have this example in a three by three.
13637000	13641000	Let's just suppose the determinant of this matrix is T.
13641000	13649000	Then we swap the first two rows and the determinant of this matrix, we've negated the determinant of the original one.
13649000	13653000	So the determinant of this one is negative T.
13653000	13660000	Then if we swap two rows again, so to get to this matrix here,
13660000	13668000	we have interchanged the second and third rows of the previous matrix.
13668000	13671000	And so I've negated the determinant again.
13671000	13677000	So the determinant will be negative of the determinant here, which is negative T.
13677000	13679000	And notice we're back to where we started.
13679000	13684000	So as you interchange rows, that just negates the determinant.
13685000	13691000	If you do two of them, then you're back to where you started.
13691000	13694000	Alright, another row operation.
13694000	13700000	Here we're multiplying a constant by one row and adding to another.
13700000	13704000	So again, the determinant of A is just AD minus BC.
13704000	13716000	The determinant of B is A times KB plus D minus B times KA plus C.
13716000	13718000	And we can do a little algebra.
13718000	13722000	Notice that the KAB terms, there's two of them and they cancel each other out.
13722000	13725000	So you're just left with AD minus BC.
13725000	13731000	So with this row operation, you don't change the determinant at all, which is kind of nice.
13731000	13735000	Because we know that when we're doing row operations on a matrix,
13735000	13738000	this is the one we're doing 99% of the time.
13738000	13744000	And so it has absolutely no effect on the determinant.
13744000	13746000	Okay, so another theorem.
13746000	13753000	If a multiple of one row of A is added to another row to produce a matrix B,
13753000	13758000	then the determinant of B is equal to the determinant of A.
13758000	13761000	Okay, the third row operation is multiplying a row by a constant.
13761000	13765000	So here I've multiplied row two by K.
13765000	13773000	So again, determinant of A, AD minus BC, determinant of B is A times KD minus B times KC.
13773000	13778000	In fact, we're out of K and we end up with K times AD minus BC.
13778000	13785000	So we see that the determinant of B is K times the determinant of A.
13785000	13792000	Or when you're going backwards, you know, if you were using the determinant of B
13792000	13796000	to try to go back to get the determinant of A,
13796000	13801000	the determinant of A would be one over K times the determinant of B.
13801000	13803000	So another theorem.
13803000	13806000	If a row of A is multiplied by K to produce a matrix B,
13806000	13810000	then the determinant of B is equal to K times the determinant of A.
13810000	13816000	That holds true for any square matrix A.
13816000	13820000	Kind of tagging along with this, I said,
13820000	13826000	what happens if we multiply the whole matrix by a constant?
13826000	13830000	So start off with our ABCD, multiply one row.
13830000	13832000	This is the matrix we had before.
13832000	13837000	Multiply it by row two by K to get this matrix.
13837000	13843000	Multiply the row one by K to get this one, which is just equal to K times the whole matrix.
13843000	13850000	So we know the determinant of A is AD minus BC.
13850000	13854000	The determinant of KA, take the determinant of this matrix.
13854000	13858000	We end up with K squared times AD minus BC,
13858000	13863000	which makes sense because we know what the determinant of this one is.
13863000	13867000	It was the one we looked at previously, and it was just K times the determinant of A.
13867000	13873000	So we just did one more row operation, multiplying another row by K.
13873000	13880000	And so we should incur that K term one more time, and hence we get the K squared.
13880000	13884000	So the determinant of K times the matrix.
13884000	13888000	Now this is just in general not a row operation here,
13888000	13892000	but if you just multiply K times the whole matrix,
13892000	13895000	you get K squared times the determinant of A.
13895000	13898000	That's for this case with a two by two.
13898000	13905000	Notice that if A had been three by three, we would have had to multiply each of three rows by K,
13905000	13910000	and we would incur that K term three times, not two, is in this case.
13910000	13915000	So in general, if A is N by N, and K is a scalar,
13915000	13921000	then the determinant of K times A is K to the N times the determinant of A.
13921000	13924000	So if A is three by three, you're going to get K cubed.
13924000	13929000	That's four by four. You're going to get a K to the fourth term.
13929000	13936000	Okay, so here's an example of how you can use these ideas that we've talked about to compute the determinant.
13936000	13941000	Now if we were just doing this one straight away using cofactor expansion,
13941000	13946000	we would clearly expand about the fourth column here.
13946000	13951000	But notice that it's simple to do one row operation here
13951000	13957000	and generate another zero in this column, which will eliminate a significant amount of work.
13957000	13961000	You know, as it is, we would need to do two three by three determinants.
13961000	13966000	But if we do one row operation to generate a zero here where the six is,
13966000	13974000	then we cut our work in half, because then we only need to evaluate one three by three determinant.
13974000	13981000	So let's do two times row four, or negative two times row four plus row three.
13981000	13988000	And notice that that kind of row operation we call does not change the determinant.
13988000	13995000	Okay, so if we do that, we can do this row operation.
13995000	14000000	We get this matrix, notice a zero here where the six was.
14000000	14005000	And then we just expand about the last column.
14005000	14011000	So we need this three, we need to figure out what sign goes with three.
14011000	14018000	So start here, one one position is a plus, minus, plus, minus, plus, minus, plus.
14018000	14025000	So it's plus three times the determinant of this three by three matrix sitting right here.
14025000	14027000	Okay, so that's what I have here.
14027000	14040000	And then I've expanded about the third row, which now I'm seeing that this is not looking right,
14040000	14043000	because this should be negative three here.
14043000	14047000	Oh, no, no, no, negative three here, my mistake.
14047000	14050000	This three here comes from the ones outside.
14050000	14055000	And then when I expand about the third row, the negative three is right here.
14055000	14062000	All right, so in the negative three is in a plus position, because we start with a one one, that's plus, minus, plus.
14062000	14067000	So we have plus negative three, okay, times this two by two.
14067000	14070000	And then we got a zero, so we can skip that.
14070000	14073000	So it would be minus zero, then plus negative two.
14073000	14075000	So here's the other term.
14075000	14081000	And if we continue to work it out, we end up with a hundred and fourteen.
14081000	14084000	Okay, so that was pretty straightforward.
14084000	14091000	I want to show you one more just that has a little more work involved.
14091000	14098000	And for that, I'm going to go to maple just because that kind of makes it a little bit easier.
14098000	14103000	So I've got it set up.
14103000	14107000	Here's the matrix that we're going to start with.
14108000	14119000	And notice this, I guess I could do a row operation to generate a zero in the third column here.
14119000	14121000	That would probably make life easier.
14121000	14125000	That would be the way to go if you were actually doing this by hand.
14125000	14133000	But I would have wanted to illustrate the concept of how you could use these row operations.
14133000	14138000	So I have actually done the row operations to get it in triangular form.
14138000	14147000	And then I want to show you how to go back and recreate the determinant of the original matrix based on the row operations that we've done.
14147000	14158000	Okay, so the first thing I did was I just started out to generate zeros in the first column.
14158000	14166000	So I used the negative three here to generate a zero here and here.
14166000	14174000	So the first operation I did was to this command here add row.
14174000	14179000	What it does is take row A, so we're matrix A.
14179000	14182000	So we're operating on the original matrix A.
14182000	14191000	And we're going to change row three by multiplying row one by negative one.
14191000	14197000	So this operation is negative one times row one plus row three.
14197000	14200000	And we generated zero in row three.
14200000	14206000	Then I do row one plus row four.
14206000	14208000	So that's the next command.
14208000	14212000	Now I'm operating on A1, this matrix here.
14212000	14217000	And I'm going to change row four by multiplying row one by one.
14217000	14224000	So the negative three plus three gives me zero here.
14224000	14229000	Then I'm going to swap rows.
14229000	14241000	I'm going to swap the first and the second row because then that gives me a one in the one one position.
14241000	14243000	So I'm just swapping rows.
14243000	14246000	Now notice at this point we've negated the determinant.
14246000	14253000	So whatever if I took the determinant of A3, it would be the negative of the determinant of the original matrix.
14253000	14257000	So keep in mind that we've done one row swap.
14257000	14262000	Now I'm going to generate a zero here where the negative three is.
14262000	14268000	And so to do that I'm operating on A3, matrix A3.
14268000	14274000	I'm going to change row two by multiplying row one by three.
14274000	14284000	So three times row one minus three gives me zero there in that first position.
14284000	14288000	Okay now I'm ready to move over to the next column.
14288000	14295000	And so the first thing I'm going to do here since there's no nice numbers here to work with,
14295000	14299000	I'm going to just multiply row two by one seventh.
14299000	14303000	So the command to do that is this one, multiply row.
14303000	14307000	So I'm going to multiply row working on A4.
14307000	14311000	I'm going to multiply row two by one seventh.
14311000	14316000	So that gives me a one in the two-two position.
14316000	14320000	And now I'm ready to zero out underneath that.
14320000	14327000	So my next operation is to work on A5.
14327000	14335000	And I'm going to multiply, I'm going to change row three by multiplying row two by negative six.
14335000	14341000	Alright so negative six times one plus this six gives me the zero here.
14341000	14346000	And then I need to do a similar thing to generate a zero in this position.
14346000	14355000	So I'm working on A6, multiply, or I'm going to change row four by multiplying row two by six.
14355000	14364000	So two times row two plus row four, I mean six times row two plus row four gives me a zero here.
14364000	14371000	So up to this point, the only thing that I've changed in terms of the determinant of the original matrix
14371000	14373000	is doing the one row swap.
14373000	14375000	So we've negated the determinant.
14375000	14377000	Oh we did the multiply row.
14377000	14383000	So we've changed the determinant by a factor of seven also.
14383000	14386000	Alright so keep those two things in mind.
14386000	14390000	And here the next thing I'm going to do is generate a one in this position.
14390000	14398000	So I'm going to do another multiply row, multiply row three by negative seven over twenty seven.
14398000	14403000	And that gives me the one in this position.
14403000	14413000	So now we've done a row swap and we've done two operations where we multiplied a row by a scalar.
14413000	14416000	So keep that in mind.
14416000	14420000	One more row operation to get a zero in this position.
14420000	14429000	So I'm going to multiply row three by negative thirteen sevenths and add that to row four.
14429000	14431000	So I get a zero there.
14431000	14436000	Now my matrix is in triangular form, upper triangular form.
14436000	14446000	So to take the determinant of this matrix it's just one times one times one times two.
14446000	14454000	And then to go back to get the matrix, the determinant of the original matrix,
14454000	14457000	I've got to back out these row operations.
14457000	14460000	So that's what I've done here.
14460000	14467000	So I've taken the determinant of A nine, which is, as I said, one times one times one times two.
14467000	14470000	So we could just do that.
14470000	14480000	Times seven and I'm multiplying times seven because of this row operation
14480000	14486000	because I've multiplied by one seventh to sort of back that out to get the original determinant.
14486000	14492000	So I've multiplied by seven and then I multiply by negative seven over twenty seven.
14492000	14500000	So I'm multiplying by the reciprocal of that and then multiplying by negative one because we did the row swap.
14500000	14502000	So put all that together.
14502000	14508000	I get the determinant of the original matrix is fifty four and then just to check,
14508000	14515000	I'll just use maple to determine that original determinant and we get fifty four again.
14515000	14527000	So this is a way you can back out the determinant of the original matrix by keeping track of the row operations that you did
14527000	14532000	and how they affect the determinant.
14532000	14537000	Okay, so back to this.
14537000	14544000	We're ready to look at a few theorems about determinants.
14544000	14549000	The first one here is actually another installment in the invertible matrix theorem.
14549000	14555000	Okay, so it says a square matrix A is invertible if and only if the determinant of A is not equal to zero.
14555000	14566000	So we've seen this, we saw this last time, but add that on to the invertible matrix theorem.
14566000	14569000	So this is an equivalent statement.
14569000	14578000	Determinant A equals not equal to zero is equivalent to A is invertible.
14578000	14585000	The determinant of A transpose is equal to the determinant of A.
14585000	14595000	If you think about that for a little bit, it actually is pretty easy to see why that is true.
14595000	14602000	Because if you think about taking the determinant of A, you choose some row or column to expand about.
14602000	14606000	For example, maybe expand about the first row.
14606000	14614000	Then if you're taking the determinant of A transpose, then you can do the exact same operations by expanding about the first column.
14614000	14621000	Since the first column of A transpose is equal to the first row of A, then you would be computing the determinant of A transpose
14621000	14625000	exactly how you would compute the determinant of the matrix A.
14625000	14630000	So you end up with getting the same value.
14630000	14632000	Okay, another theorem.
14632000	14642000	If A and B are square matrices, then the determinant of the product A times B is the determinant of A times the determinant of B.
14642000	14647000	The determinant of the product is the product of the determinants.
14647000	14651000	Notice, though, that that doesn't apply to sums.
14651000	14658000	So the determinant of A plus B is in general not equal to the determinant of A plus the determinant of B.
14658000	14662000	So don't make that mistake.
14662000	14671000	Okay, and that's all we have for this section.
14671000	14674000	Alright, let's talk about Kramer's Rule.
14674000	14681000	Kramer's Rule is actually one of the beautiful things in mathematics, I believe.
14681000	14686000	It's just a neat little idea.
14686000	14693000	And it's a way to solve a system of equations using only determinants.
14693000	14696000	So it's just very interesting.
14696000	14698000	So here's the way it works.
14698000	14700000	Kramer's Rule.
14700000	14704000	We have A is an n by n matrix.
14704000	14720000	Then for any B in Rn, the unique solution x of Ax equals B is given by this formula here, which looks rather odd.
14720000	14726000	Notice that we've got a way to compute x of i for each i.
14726000	14731000	And it's just a ratio of determinants.
14731000	14734000	In the denominator, we've got just the determinant of A.
14734000	14739000	In the numerator, it's the determinant of A sub i of B.
14739000	14747000	So A sub i of B refers to the matrix formed by replacing the i-th column of A with B.
14747000	14750000	Notice, we replace the i-th column of A with B.
14750000	14758000	So to get x1, we replace the first column of A with B and compute that determinant and divide by the determinant of A.
14758000	14761000	That's the value for x1 and so forth.
14761000	14763000	So a neat idea.
14763000	14767000	So let's just use it to solve this system.
14767000	14770000	So we need the determinant of A.
14770000	14774000	That's just 8 minus 5, just 3.
14774000	14778000	The determinant of A sub 1 of B.
14778000	14782000	Notice that I've just taken B, 6, 7,
14782000	14784000	plopped it into the first column of A.
14784000	14787000	Take the determinant, we get 5.
14787000	14790000	Same thing, A sub 2 of B.
14790000	14794000	We take B and plop it in the second column of A.
14794000	14797000	And take that determinant, we get negative 2.
14797000	14803000	And so x1 is just the 5 divided by 3.
14803000	14807000	And x2 is the negative 2 divided by 3.
14807000	14811000	And those are the values of x1 and x2.
14811000	14816000	Just to check, we take 5-thirds times the first column of A,
14816000	14819000	minus 2-thirds times the second column of A,
14819000	14823000	and we indeed get 6, 7.
14823000	14826000	Yippee!
14826000	14830000	It's nice when things work like they're supposed to.
14830000	14833000	I use the same method for 3 by 3 system.
14833000	14838000	This time, you've got to compute a determinant for each of 3 variables,
14838000	14841000	plus the determinant of A.
14841000	14845000	So here's a matrix A on the right-hand side, B.
14845000	14849000	Compute the determinant of A first,
14849000	14854000	and then we compute the determinant of A sub 1 of B.
14854000	14857000	So again, take B, take the vector B,
14857000	14859000	stick it in the first column of A,
14859000	14863000	leave the rest of A alone, and compute that determinant.
14863000	14867000	Same thing, A sub 2 of B, we take B,
14867000	14870000	stick it in the second column of A,
14870000	14873000	and leave the rest of A alone, compute that determinant.
14873000	14878000	A sub 3 of B, we take the B,
14878000	14881000	replace the third column of A,
14881000	14884000	compute that determinant, and then put it all together.
14884000	14888000	X1 is going to be negative 16 over 4.
14888000	14891000	X2 is going to be 52 over 4,
14891000	14894000	and X3 is going to be negative 4 over 4.
14894000	14897000	And so there's your solution to this system.
14897000	14901000	So just a really neat idea
14901000	14904000	in a neat way to solve systems of equations.
14904000	14908000	Clearly, we've run into the same problem for this 3 by 3.
14908000	14912000	It really wasn't that bad, but if it was a 4 by 4 system,
14912000	14917000	then you're computing 4 by 4 determinants.
14917000	14920000	Because you have to compute one for each variable,
14920000	14922000	one for each column.
14922000	14926000	So you have A1, A2, A3, and A4 of B.
14926000	14929000	Plus you need to compute the original determinant.
14929000	14931000	So you have 5, 4 by 4 determinants,
14931000	14933000	and that would be a lot of work.
14933000	14936000	So this is not practical in a general sense,
14936000	14943000	but for theoretical and for small matrices like this,
14943000	14946000	it's kind of a neat idea.
14946000	14950000	We can also apply it to a question like this.
14950000	14953000	Determine the values of the parameter s
14953000	14957000	for which the system given by this augmented matrix
14957000	14959000	has a unique solution.
14959000	14962000	And then describe the solution.
14962000	14964000	So we know it has a unique solution
14964000	14968000	when the determinant is not 0.
14968000	14970000	Because the determinant is not 0,
14970000	14975000	A is invertible, and there's a unique solution.
14975000	14980000	So if we take the determinant of the coefficient part of that matrix,
14980000	14985000	we end up with 15 times s squared plus 3.
14985000	14990000	And note that 15 times s squared plus 3
14990000	14993000	is not equal to 0 for all values of s.
14993000	14999000	You can never end up with that equal to 0
14999000	15002000	as long as s is a real number.
15002000	15007000	Because s squared plus 3 is always going to be positive.
15007000	15011000	And so this system is going to have a unique solution
15011000	15014000	no matter what the value of s is.
15014000	15018000	So we compute to get the,
15018000	15021000	to be able to describe the solution,
15021000	15024000	we can use Kramer's rule.
15024000	15028000	And so I've just computed A1 of B,
15028000	15032000	that takes B, the 3, 2, sticks it in the first column of A.
15032000	15034000	We compute that determinant.
15034000	15037000	Then same thing, A sub 2 of B,
15037000	15040000	we stick right hand side on the second column,
15040000	15042000	compute that determinant.
15042000	15046000	And then kind of massage things a little bit.
15046000	15052000	And we have expressions for the value of x1 and x2
15052000	15056000	for any value of s.
15056000	15060000	Okay, so it's just the 5 times 3s plus 2
15060000	15063000	divided by the determinant of A here.
15063000	15066000	And the 3 times 2s minus 9
15066000	15071000	divided by the determinant of A for x2.
15071000	15075000	We can also use Kramer's rule to compute A inverse.
15075000	15078000	Because as we saw in chapter 2,
15078000	15080000	to compute A inverse,
15080000	15085000	we simply need to solve some systems of equations.
15085000	15089000	So if we consider this matrix A,
15089000	15093000	we want to find B such that A times B equals the identity matrix.
15093000	15097000	So B here would be the inverse of A.
15097000	15106000	So let's let B1 and B2 denote the columns of B
15106000	15111000	and let E1 and E2 denote the columns of the identity matrix.
15111000	15114000	And we've used that notation before.
15114000	15118000	So we need to solve A times B1 equals E1
15118000	15123000	and A times B2 equals E2 here in this 2 by 2 case.
15123000	15126000	Alright, so if we take the determinant of A,
15126000	15128000	go back up and look at A here.
15128000	15131000	2 times 8, 16 minus 3 times 5.
15131000	15133000	So 16 minus 15.
15133000	15136000	The determinant of A is just 1.
15136000	15138000	Okay, now to get B1,
15138000	15140000	we're solving this system here.
15140000	15142000	A times B1 equals E1.
15142000	15148000	So to get the first entry in B1,
15148000	15152000	that's the determinant of A1 of E1.
15152000	15157000	Alright, we're solving, look at this augmented matrix.
15157000	15161000	So to get the first entry in the solution,
15161000	15166000	we replace the first column of A by the right-hand side,
15166000	15168000	which is E1.
15168000	15170000	Compute that determinant,
15170000	15172000	get 8, 8 over 1 is just 8.
15172000	15180000	So the 1, 1 entry in the inverse is 8.
15180000	15188000	Alright, now to get the second entry in the solution to this system here,
15188000	15193000	we substitute E1 into the second column of A.
15193000	15196000	Alright, so here we go with that.
15196000	15199000	And compute that determinant,
15199000	15202000	negative 5 divided by 1, we get negative 5.
15202000	15206000	So we've got the first column of the inverse, right?
15206000	15208000	Because we were solving this system,
15208000	15210000	A times B1 equals E1.
15210000	15212000	And so that's the first,
15212000	15214000	so we get the first column of the inverse,
15214000	15218000	B1 would be 8, negative 5.
15218000	15222000	To get B2, we solve a similar system.
15222000	15226000	Just now the right-hand side is 0, 1 instead of 1, 0.
15226000	15231000	And so to get the first entry in the solution to this system,
15231000	15235000	we substitute 0, 1 in the first column of A.
15235000	15237000	Alright, so you see that here.
15237000	15239000	Compute the determinant, get negative 3,
15239000	15241000	divided by 1, negative 3.
15241000	15244000	And then the second entry means substitute
15244000	15247000	the right-hand side in the second column of A.
15247000	15249000	Compute that determinant, get 2,
15249000	15251000	divided by the determinant of A.
15251000	15253000	And we have 2.
15253000	15256000	So we have A inverse, which is B,
15256000	15262000	given by this matrix that we just computed.
15262000	15266000	So in general, the ijth entry in the inverse
15266000	15272000	is given by the determinant of A sub i of Ej,
15272000	15274000	divided by the determinant of A.
15274000	15277000	So if you go back, for instance here,
15277000	15281000	to get the 1, 2 entry in the inverse,
15281000	15287000	we computed the determinant of A sub 1 of E2,
15287000	15289000	divided by the determinant of A.
15289000	15292000	To get the 2, 2 entry, it was A2 of E2,
15292000	15294000	divided by the determinant of A.
15294000	15297000	So in general, the ijth entry in the inverse
15297000	15301000	is the determinant of A sub i of Ej,
15301000	15306000	divided by the determinant of A.
15306000	15311000	We actually create a matrix,
15311000	15313000	which we call the adjugate,
15313000	15315000	or the adjoint of a matrix,
15315000	15317000	and stick in these values,
15317000	15322000	these determinants of A sub i E sub j.
15322000	15324000	We call that the,
15324000	15327000	the book uses the term adjugate primarily.
15327000	15329000	I typically use adjoint,
15329000	15333000	so those are synonymous terms.
15333000	15337000	But it's composed of all these determinants.
15337000	15340000	And so basically it's just these things,
15340000	15343000	but with the determinant of A term factored out.
15343000	15346000	So in general, A inverse is equal to
15346000	15352000	1 over the determinant of A times the adjoint of A.
15352000	15358000	So let's use that to find the inverse of this matrix here.
15358000	15361000	Okay, so first we'll find the adjoint,
15361000	15363000	or the adjugate of A.
15363000	15367000	And remember to get the,
15367000	15371000	to get the ijth entry in the adjoint,
15371000	15374000	it's the determinant of A sub i E sub j.
15374000	15376000	So for the 1, 1 entry,
15376000	15379000	it's the determinant of A1 E1.
15379000	15380000	So the 2, 1 entry,
15380000	15383000	determinant of A2 E1.
15383000	15384000	3, 1 entry,
15384000	15387000	determinant of A3 E1.
15387000	15389000	Alright, so I've computed all those.
15389000	15391000	A sub 1 of E1,
15391000	15394000	or E1 is the first column of the identity matrix.
15394000	15398000	The A1 says substitute that into the first column of A.
15398000	15402000	So there you have E1, 1, 0, 0,
15402000	15404000	substitute in the first column of A.
15404000	15407000	And take that determinant.
15407000	15411000	A sub 2 of E1 says put E1 into the second column of A.
15411000	15413000	So there we have it,
15413000	15415000	determinant there is 0.
15415000	15419000	A sub 3 of E1 says put E1 in the third column of A.
15419000	15420000	So there we go.
15420000	15422000	Take that determinant.
15422000	15424000	And do the same thing
15424000	15427000	for the other 6 entries in the adjoint.
15427000	15428000	1, 2 position,
15428000	15431000	determinant of A1 of E2.
15431000	15434000	So take E2, which is 0, 1, 0,
15434000	15437000	and substitute that in the first column of A.
15437000	15438000	So there.
15438000	15439000	So you can see,
15439000	15441000	here we have 0, 1, 0 in the first column,
15441000	15443000	here it's in the second column,
15443000	15446000	here it's in the third column.
15446000	15449000	And we compute each one of those determinants.
15449000	15452000	Then do it one more time for the third column.
15452000	15455000	Let's get the 1, 3, 2, 3, and 3, 3 elements.
15455000	15458000	So E3 is 0, 0, 1.
15458000	15460000	So we stick it in the first column of A,
15460000	15462000	then the second column of A,
15462000	15463000	third column of A,
15463000	15465000	and compute all those determinants.
15465000	15468000	Then we put it all together.
15468000	15470000	This is what we get.
15470000	15473000	And we need the determinant of A,
15473000	15475000	which we can compute.
15475000	15476000	That's 5.
15476000	15479000	So the inverse of A
15479000	15481000	is just 1 over the determinant of A.
15481000	15484000	So 1 5th times the adjoint of A,
15484000	15486000	which is this matrix.
15486000	15487000	And there's your inverse,
15487000	15490000	computed totally by doing elementary,
15490000	15492000	not elementary, row operations,
15492000	15494000	totally using determinants.
15501000	15504000	Okay, today we're going to start section 4.1
15504000	15506000	on vector spaces.
15506000	15509000	So we'll start with the definition
15509000	15512000	of a vector space.
15512000	15515000	You'll probably want to have your book out with you
15515000	15517000	because we're going to refer back
15517000	15519000	to this definition quite a bit.
15519000	15525000	So in your textbooks on page 217,
15525000	15528000	there's a lot to this definition,
15528000	15530000	so we'll spend a little time just talking about it
15530000	15534000	before we move on from there.
15534000	15537000	A vector space is said to be
15537000	15542000	a non-empty set V of objects
15542000	15544000	called vectors.
15544000	15545000	You know, let's stop there
15545000	15547000	because that's a little bit odd.
15547000	15552000	It's a set of objects called vectors.
15552000	15554000	I want to make the point here
15554000	15559000	that when the author uses the term vector
15559000	15560000	in this context,
15560000	15563000	he's not necessarily talking about a vector
15563000	15565000	of the type we're familiar with,
15565000	15568000	like in R2 or R3 or so forth.
15568000	15571000	He's using it in a more generic sense,
15571000	15574000	and I actually like to use the term object
15574000	15578000	just to keep from confusing you
15578000	15581000	between exactly what he's referring to
15581000	15583000	by using the word vector.
15583000	15585000	And as we go along today,
15585000	15588000	you'll kind of see what I'm talking about here.
15588000	15590000	Okay, so starting over.
15590000	15595000	A vector space is a non-empty set V of objects
15595000	15599000	on which are defined two operations
15599000	15603000	called addition and multiplication by scalars,
15603000	15605000	which are real numbers.
15605000	15608000	Now again, this seems sort of odd,
15608000	15610000	I would think to you.
15610000	15613000	Because he's saying there's two operations,
15613000	15615000	and instead of saying two operations,
15615000	15618000	addition and multiplication by scalars,
15618000	15620000	he's saying there's two operations
15620000	15624000	called addition and multiplication by scalars,
15624000	15627000	which is kind of an odd way to phrase it.
15627000	15630000	And the reason why he says it like that
15630000	15634000	is because since these objects
15634000	15636000	are not necessarily vectors
15636000	15639000	in the traditional sense that we're used to,
15639000	15643000	then addition and multiplication by scalars
15643000	15647000	are not necessarily defined as we're accustomed to.
15647000	15652000	They can in fact be defined in any way that you would like,
15652000	15656000	as long as these properties here are satisfied.
15656000	15661000	So we're in an abstract sense here.
15661000	15664000	We're talking about objects and operations.
15665000	15671000	And so don't assume that we're talking about
15671000	15674000	vectors in the traditional sense,
15674000	15677000	nor addition and scalar multiplication
15677000	15680000	in the traditional sense.
15680000	15684000	Okay, so we have this collection of objects
15684000	15686000	in these two operations,
15686000	15690000	and they are subject to the ten axioms given here.
15690000	15693000	And these axioms must hold for all vectors,
15693000	15696000	u, v, and w, in the set v,
15696000	15699000	and for all scalars, c, and d.
15699000	15701000	Okay, so I've kind of highlighted some
15701000	15706000	because some are a little bit more interesting than others.
15706000	15709000	Number one says the sum of u and v,
15709000	15713000	denoted by u plus v, is in the set v.
15713000	15716000	So it just says if you take any two vectors from the set
15716000	15719000	and you add them together,
15719000	15723000	then you get another vector that is still in the set.
15723000	15725000	And as we'll see in a minute,
15725000	15728000	this property is called closure,
15728000	15731000	or we say that the set is closed under addition
15731000	15735000	if this property holds.
15735000	15740000	Okay, two and three are some standard properties.
15740000	15744000	Number two, we have a community property.
15744000	15747000	Three, associative.
15747000	15751000	Then number four says that in the set v,
15751000	15753000	there is a zero vector,
15753000	15756000	such that when you add it to any vector in the set,
15756000	15758000	you just get that vector back.
15758000	15762000	Okay, so u plus zero is equal to u.
15762000	15770000	Okay, so notice number one is referring to
15770000	15772000	an element that has to be in the set.
15772000	15774000	Okay, if u and v are in the set,
15774000	15776000	their sum has to be in the set.
15776000	15779000	Number four, also saying that there has to be
15779000	15781000	this zero vector in the set.
15781000	15783000	Okay, so we'll see it.
15783000	15787000	Number six kind of follows along with that, too.
15787000	15792000	Number five, the first says for each vector in the set,
15792000	15796000	you have another vector such that when you add it to the first,
15796000	15798000	you end up with a zero vector.
15798000	15800000	So it's essentially saying that every element
15800000	15803000	has an additive inverse in the set.
15803000	15808000	The number six says the scalar multiple of any vector
15808000	15812000	by constant c is denoted by c times u,
15812000	15814000	and that is in the set.
15814000	15817000	Okay, so this says that when you take the scalar multiple
15817000	15820000	of any vector in the set or any object in the set,
15820000	15823000	the result is also in the set.
15823000	15826000	So you can see that number one and four and number six
15826000	15828000	all are specifically referring to whether or not
15828000	15831000	particular elements are in the set.
15831000	15835000	And that will be important as we go along.
15835000	15838000	Number seven through ten are, again,
15838000	15840000	standard sorts of properties.
15840000	15845000	We have distributive properties,
15845000	15850000	and number ten, we have a multiplicative inverse property.
15850000	15855000	One times any element in the set is equal to that element.
15855000	15858000	And you might be looking especially at number ten
15858000	15863000	and going, well, when would that ever not be the case?
15863000	15868000	And the answer is going back to what I said originally
15868000	15881000	is that the set V can contain items other than just vectors
15881000	15883000	like from R2 or R3.
15883000	15886000	And these operations, addition and scalar multiplication,
15886000	15890000	can be defined in non-standard ways.
15890000	15894000	And so based on that, then sometimes it could be the case
15894000	15898000	that number ten would not be true.
15898000	15902000	Okay, so let's start with R2.
15902000	15905000	That's a simple set to think about.
15905000	15910000	And let's talk about whether R2 is a vector space.
15910000	15915000	The answer is yes, because all the properties except one and six
15915000	15919000	were in fact explicitly given in section 1.3.
15919000	15922000	So if you want to go back to page 32 and check that out,
15922000	15927000	all the properties except one and six are explicitly listed there.
15927000	15929000	So let's think about one and six.
15929000	15934000	Number one, we want to know if you take two vectors from R2
15934000	15937000	and add them together, do you get another vector in R2?
15937000	15942000	Well, here's two vectors in R2, A, B, and C, D.
15942000	15945000	When we add them together, we get A plus C, B plus D.
15945000	15947000	That's another vector in R2.
15947000	15951000	So the answer is yes there.
15951000	15955000	Number six, if you multiply a vector in R2 by a scalar,
15955000	15957000	do you get another vector in R2?
15957000	15961000	Is it closed under scalar multiplication?
15961000	15966000	And here we see multiply a vector times the scalar.
15966000	15971000	We get another vector in R2, both of whose components are real numbers.
15971000	15976000	And therefore R2 is a vector space.
15976000	15981000	And in fact, the set of real numbers,
15981000	15986000	a set of all vectors with two components, which is R2,
15986000	15995000	R3, and Rn in general are all vector spaces.
15996000	16001000	Going back to these property one and property six,
16001000	16003000	just to take a second look at those,
16003000	16008000	property one said that if you take any two vectors from the set
16008000	16012000	and compute their sum, then it is also in the set.
16012000	16018000	And if that is true, we say that the set is closed under addition.
16018000	16022000	Property six says if you take a scalar multiple of a vector in the set,
16022000	16024000	then the result is also in the set.
16024000	16030000	Then if this is true, we say that the set is closed under scalar multiplication.
16030000	16035000	Okay, so keep those terms in mind.
16035000	16038000	Let's move on and consider another set.
16038000	16040000	Here's another set.
16040000	16042000	This is a subset of R2.
16042000	16046000	So I'm saying we have a set S here,
16046000	16052000	which consists of vectors of the form X0, where X is a real number.
16052000	16058000	Okay, so basically everything in R2 where the second component is zero.
16058000	16060000	If you think about this graphically,
16060000	16065000	it's just saying that the Y component is zero.
16065000	16072000	So therefore, graphically speaking, S would just be the X axis.
16072000	16079000	So I want to know is S with the operations of addition
16079000	16082000	and scalar multiplication as traditionally defined.
16082000	16087000	Okay, what you're used to is S of vector space.
16087000	16094000	Well, again, axioms 2 and 3 and 7 through 10 follow automatically.
16094000	16096000	Since S is a subset of R2,
16096000	16103000	because 2, 3 and 7 through 10 are true of everything in R2.
16103000	16105000	And I'm assuming here that you get your book open
16105000	16110000	so you know which of these I'm referring to.
16110000	16115000	The other axioms depend on certain elements being in the set.
16115000	16118000	So we have to look at this closer.
16118000	16121000	So number one is S closed under addition,
16121000	16124000	i.e. if you take two elements in the set,
16124000	16129000	do you get another element in the set?
16129000	16131000	You take two elements in the set and add them together.
16131000	16134000	Do you get another element in the set?
16134000	16138000	Well, here I've taken a couple of generic elements from the set,
16138000	16140000	U and V.
16140000	16143000	And if we add them together, as I've done here,
16143000	16146000	notice that you get this vector of this form,
16146000	16149000	U1 plus V1 in the first component,
16149000	16151000	zero in the second component.
16151000	16160000	And this vector is in S instead of V.
16160000	16164000	Since U1 plus V1 is a real number,
16164000	16166000	let's pause here,
16166000	16173000	why do we know that U1 plus V1 is a real number?
16173000	16178000	The reason we know that is because U and V are both in S,
16178000	16182000	and therefore U1 has to be a real number,
16182000	16184000	V1 has to be a real number,
16184000	16186000	so we add two real numbers together,
16186000	16188000	we get another real number.
16188000	16192000	Okay, so we know the first component is a real number,
16192000	16194000	second component is zero,
16194000	16196000	and that's what it takes to be in the set S.
16196000	16201000	So S is closed under addition.
16201000	16205000	By number four, does S contain a zero element?
16205000	16208000	Does S contain a vector of zero such that
16208000	16211000	when you add zero to any vector in the set,
16211000	16214000	you just get that vector back?
16214000	16219000	Well, the obvious choice would be zero, zero.
16219000	16222000	So the question is, is that vector in S?
16222000	16224000	And the answer is yes,
16224000	16226000	because remember to be in S,
16226000	16229000	the first component has to be a real number,
16229000	16230000	which zero is,
16230000	16232000	and the second component has to be zero,
16232000	16237000	which we have here in the zero vector.
16237000	16239000	Okay, number five,
16239000	16243000	does each element have an additive inverse?
16243000	16247000	Well, if we start off with a generic element of the set,
16247000	16251000	say U equals U10,
16251000	16256000	we can add to that this vector negative U10,
16256000	16258000	and it is in the set.
16258000	16260000	How do we know that?
16260000	16262000	Well, if U1 is a real number,
16262000	16264000	then negative U1 has to be a real number,
16264000	16266000	and we have the second component is zero.
16266000	16272000	When we add those together, we get the zero vector.
16273000	16276000	It is S closed under scalar multiplication.
16276000	16278000	So if we multiply,
16278000	16280000	take a generic element from the set,
16280000	16281000	multiply it by a scalar,
16281000	16284000	do we get another element in the set?
16284000	16287000	Well, here's a generic element of the set,
16287000	16289000	U equals U10,
16289000	16291000	C is a scalar,
16291000	16293000	that just means it's just any real number,
16293000	16296000	and if we multiply C times U,
16296000	16299000	we end up with C U1 in the first component.
16299000	16301000	Now that has to be a real number,
16301000	16304000	which it is because C is real,
16304000	16305000	U1 is real.
16305000	16307000	We've got the product of two real numbers,
16307000	16309000	and that's another real number,
16309000	16311000	and the second component is zero.
16311000	16314000	That's what it takes again to be in S.
16314000	16319000	So S is indeed closed under scalar multiplication,
16319000	16321000	and at this point,
16321000	16327000	we've established that all 10 of the axioms are satisfied.
16327000	16330000	So S, our set S, is a vector space.
16333000	16337000	Okay, now it turns out that when you're dealing with subsets
16337000	16339000	of known vector spaces,
16339000	16341000	just like we were here,
16341000	16343000	S was a subset of R2,
16343000	16346000	then we really need to only examine
16346000	16348000	three of these properties
16348000	16350000	to see if the set's a vector space.
16350000	16352000	And actually,
16352000	16355000	we typically call it a subspace
16355000	16358000	of the larger vector space,
16358000	16361000	even though it is actually in itself a vector space.
16364000	16367000	Okay, so there's the three properties
16367000	16369000	are given here.
16369000	16372000	We say a subspace of a vector space V
16372000	16376000	is a subset H of V that has three properties.
16376000	16380000	The zero vector of V is an H.
16380000	16383000	H is closed under vector addition,
16383000	16387000	and H is closed under scalar multiplication.
16387000	16389000	Okay, so if we want to check to see
16389000	16392000	if a set is a subspace of a vector space,
16392000	16395000	we need to only check these three properties.
16397000	16399000	All right, so let's do that here.
16399000	16401000	There's another set, T.
16401000	16403000	And T is a subset of R2,
16403000	16405000	and it consists of all vectors
16405000	16408000	where the first entry is any real number,
16408000	16411000	and the second entry is a two.
16412000	16417000	So I want to know is T a subspace of R2?
16417000	16419000	So looking at the first one,
16419000	16422000	does T contain the zero vector of R2?
16422000	16425000	That was the first requirement to be a subspace.
16425000	16427000	So I'm asking, is zero zero
16427000	16430000	is the vector zero zero in T?
16430000	16432000	And if you look up here at T,
16432000	16434000	remember what does it take to be in T?
16434000	16436000	First components, any real number.
16436000	16438000	Second components, two.
16438000	16441000	If you look at zero zero, does that fit that bill?
16441000	16443000	No, it doesn't.
16443000	16444000	Why not?
16444000	16447000	Because the second component here is zero.
16447000	16450000	To be in T, the second component has to be a two.
16452000	16455000	So zero zero is not in T.
16455000	16457000	And at this point, we could stop and say,
16457000	16460000	no, T is not a subspace of R2
16460000	16464000	because it doesn't contain the zero vector of R2.
16464000	16467000	But for practice, let's just keep going.
16467000	16471000	And consider the other two conditions.
16471000	16473000	So here's T again.
16473000	16475000	Is it closed under addition?
16475000	16478000	So if I take two elements of T and add them together,
16478000	16481000	do I get another element of T?
16481000	16485000	Well, here's a couple of generic elements of T.
16485000	16488000	If I add them together, what happens?
16490000	16493000	Oops, look at that second element here.
16493000	16496000	Second element in the sum of these two is a four.
16496000	16499000	So this is not in T.
16499000	16503000	Because to be in T, your second component has to be a two.
16503000	16505000	Here it's a four.
16505000	16510000	So T is not closed under addition.
16510000	16512000	How about scalar multiplication?
16512000	16516000	Is it closed under scalar multiplication?
16516000	16519000	Well, let's take a generic element of T.
16519000	16520000	Here's one.
16520000	16522000	Got a real number on top.
16522000	16524000	Two in the second component.
16524000	16526000	We multiply by scalar.
16526000	16528000	And what do we get?
16528000	16530000	We get this vector.
16530000	16533000	Actually, we shouldn't get that vector.
16533000	16535000	That V1 should not be there.
16535000	16537000	Apologize for that.
16537000	16539000	Should be just C1 in the top.
16539000	16542000	Two C down in the bottom.
16542000	16544000	Make a note of that.
16546000	16549000	So again, this plus V1 shouldn't be here.
16549000	16551000	But let's look.
16551000	16553000	What's important really is the second component,
16553000	16555000	the two C here.
16555000	16560000	Because what we want to know is, is that zero?
16560000	16565000	And the answer is, well, it's zero only if C is zero.
16565000	16567000	And C is not restricted to be zero.
16567000	16572000	And therefore, we can easily come up with a counter example
16572000	16576000	to show that we can take a scalar multiple of an element of T
16576000	16580000	and end up with something outside of T.
16580000	16583000	So T is not closed under scalar multiplication.
16586000	16587000	All right.
16587000	16589000	Make one more note.
16589000	16591000	All right.
16591000	16593000	So in this case, this set T,
16593000	16596000	failed all three of the subspace tests.
16596000	16600000	Again, if you were simply trying to determine
16600000	16603000	if T is a subspace of R2,
16603000	16606000	you only need to find one that it fails.
16606000	16607000	And then you could stop there.
16607000	16612000	I just showed y'all three just for practice.
16612000	16613000	Okay.
16613000	16619000	So as I said before, these vector spaces such as R2
16619000	16625000	and R3 and Rn, those we are familiar with.
16625000	16629000	But there's a lot of other ones in which the elements
16629000	16631000	don't look like traditional vectors.
16631000	16635000	So let's kind of examine that a little bit.
16635000	16640000	Consider this set I'm calling M sub 2 by 2,
16640000	16644000	which is the set of all 2 by 2 matrices, okay,
16644000	16648000	where all the components are real numbers.
16648000	16651000	Turns out that M sub 2 by 2 is also a vector space,
16651000	16653000	even though its elements are matrices
16653000	16657000	instead of vectors in the traditional sense.
16657000	16660000	So let's go back through those properties
16660000	16664000	with M sub 2 by 2 in mind.
16664000	16667000	Okay, so let's start off and suppose
16667000	16672000	that A, B, and C are our 2 by 2 matrices
16672000	16675000	and that P and Q are scalars.
16675000	16680000	So property number one, if we add A and B together,
16680000	16682000	do we get another 2 by 2 matrix?
16682000	16686000	This is saying is M sub 2 by 2 closed under addition.
16686000	16687000	And clearly it is.
16687000	16690000	If you add two 2 by 2 matrices together,
16690000	16695000	you get another 2 by 2 matrix.
16695000	16699000	Alright, here are the second and third properties.
16699000	16704000	Those fall straight out from properties of matrices.
16704000	16707000	Number four, is there a zero matrix?
16707000	16710000	Alright, is there some 2 by 2 matrix
16710000	16713000	such that you can add to any other 2 by 2 matrix
16713000	16715000	and get that same matrix back?
16715000	16717000	Well, clearly there is.
16717000	16723000	So matrix with zeros in all the positions.
16723000	16726000	Number five, if you add negative A to A,
16726000	16729000	you get the zero matrix.
16729000	16737000	Number six, is M sub 2 by 2 closed under scalar multiplication.
16737000	16739000	So if you take any 2 by 2 matrix
16739000	16741000	and multiply it by scalar,
16741000	16743000	do you get another 2 by 2 matrix?
16743000	16745000	And of course you do.
16745000	16749000	So it's closed under scalar multiplication.
16749000	16754000	And then these final four properties fall straight out
16754000	16757000	from what we know about matrices.
16757000	16762000	So M sub 2 by 2 is indeed a vector space.
16762000	16764000	Another one about polynomials.
16764000	16770000	Polynomials are some standard examples of vector spaces.
16770000	16773000	Let's first start with piece of 2.
16773000	16776000	Piece of 2 is the set of all polynomials
16776000	16779000	of degree 2 or less.
16779000	16783000	So we wanted to write it in formal terms.
16783000	16785000	It would look like this.
16785000	16789000	A naught, set of all A naught plus A1 times t
16789000	16791000	plus A2 times t squared,
16791000	16794000	where A naught, A1, A2 are real numbers.
16794000	16797000	So here's some sample elements of piece of 2.
16797000	16799000	3 plus 2t.
16799000	16802000	Here 3 is A sub 0 is 3.
16802000	16803000	A1 is 2.
16803000	16806000	A2 in this case would be 0.
16806000	16808000	4t squared, that's in there.
16808000	16812000	8t squared minus 13t plus 45, that's in there.
16812000	16817000	Any polynomial of degree 2 or less.
16817000	16822000	And piece of 2 is indeed a vector space.
16822000	16827000	Let's look at these properties in terms of piece of 2.
16827000	16831000	So if we add two polynomials of degree 2 together,
16831000	16834000	do you get another polynomial of degree 2?
16834000	16836000	Okay, well, yes you do.
16836000	16840000	Here's how that works.
16840000	16844000	We've got two, let's call it p of t, which looks like this.
16844000	16846000	Q of t, which looks like this.
16846000	16848000	And if we add them together,
16848000	16851000	then we get this polynomial here,
16851000	16856000	which is again of degree 2 or less.
16857000	16861000	Okay, they satisfy the properties 2 and 3.
16861000	16865000	Let you explore that some more.
16865000	16868000	Number 4, do you have a zero element?
16868000	16870000	Is there a zero polynomial?
16870000	16871000	Well, yes there is.
16871000	16872000	It's just zero.
16872000	16874000	We add zero to any polynomial,
16874000	16877000	we get that same polynomial back.
16877000	16882000	If we negate all the coefficients in a polynomial
16882000	16884000	and add it to the original one,
16884000	16888000	we get the zero polynomial.
16888000	16891000	Number 6, is this set of polynomials
16891000	16895000	closed under scalar multiplication?
16895000	16899000	So if you multiply any polynomial of degree 2 or less by a scalar,
16899000	16903000	do you get another polynomial of degree 2 or less?
16903000	16905000	And the answer is yes.
16905000	16908000	Okay, here, multiply by a scalar
16908000	16913000	and we get another polynomial of degree 2 or less.
16913000	16919000	And again, 7 through 10 kind of fall out pretty straightforwardly.
16919000	16922000	Here's another set.
16922000	16924000	This is a subset, Q sub 2,
16924000	16928000	here's a subset of P sub 2.
16928000	16932000	Here, I've got all polynomials,
16932000	16934000	not of degree 2 or less,
16934000	16938000	but all polynomials of degree exactly 2.
16938000	16941000	So the way I've defined it is in this form,
16941000	16943000	looks like P sub 2, with this addition,
16943000	16946000	A sub 2 has to be non-zero.
16946000	16950000	So that makes it where you're going to have
16950000	16952000	a second degree polynomial.
16952000	16955000	You're going to have A sub 2, 9 equals 0,
16955000	16960000	so you're going to have the t squared term showing up.
16960000	16966000	So I want to know, is Q sub 2 a subspace of P sub 2?
16966000	16969000	So to show that it's a subspace,
16969000	16973000	we need to show that it satisfies those three properties, right?
16973000	16976000	It includes the zero element, closed under addition,
16976000	16978000	closed under scalar multiplication.
16978000	16981000	So is that the case?
16981000	16983000	Well, let's start with number one.
16983000	16988000	This Q sub 2 contains the zero element of P sub 2.
16988000	16992000	Well, zero element of P sub 2 is the zero polynomial, right?
16992000	16994000	Just zero.
16994000	16997000	So is that in Q sub 2?
16997000	16999000	And the answer is no, because,
16999000	17002000	go back up and look at the definition of Q sub 2,
17002000	17006000	A sub 2 can't be zero, right?
17006000	17011000	But down here, the zero polynomial, A sub 2 is zero.
17011000	17017000	So Q sub 2 does not contain the zero element of P sub 2.
17017000	17019000	So we could stop right here and say,
17019000	17025000	no, Q sub 2 is not a subspace of P sub 2.
17025000	17029000	But again, just for more experience,
17029000	17035000	we're going to go on and look at the other two.
17035000	17039000	So the second one is Q sub 2 closed under addition.
17039000	17044000	So if we add any two elements of Q sub 2 together,
17044000	17046000	do we get another element of Q sub 2?
17046000	17052000	Or if you add any two polynomials of degree exactly two,
17052000	17057000	or less, not or less, a degree exactly two,
17057000	17062000	do you get another polynomial of degree exactly two?
17062000	17064000	There it is there.
17064000	17066000	So here's an example.
17066000	17071000	But P of t be 6t squared minus 3t.
17071000	17078000	That is in Q sub 2 because we've got a 6,
17078000	17081000	okay, the A sub 2 term is not equal to zero.
17081000	17088000	And here's Q of t where the coefficient of t squared is not zero.
17088000	17091000	So we've got two polynomials here of degree exactly two.
17091000	17098000	When we add them together, what happens?
17098000	17100000	The t squared terms go away.
17100000	17103000	We're left with negative 3t plus 5.
17103000	17106000	And that is not a degree exactly two.
17106000	17112000	So Q sub 2 is not closed under addition.
17112000	17115000	About scalar multiplication.
17115000	17121000	If we multiply a polynomial of degree exactly two by a scalar,
17121000	17125000	do we always get another polynomial of degree exactly two?
17125000	17127000	So think about that.
17127000	17133000	So you start off with any sort of polynomial that's degree exactly two.
17133000	17135000	And you multiply it by any scalar.
17135000	17139000	Do you always get another polynomial of degree exactly two?
17139000	17142000	And if you don't think about that very long,
17142000	17144000	you might say, well, yeah, you multiply it by anything
17144000	17147000	and you're going to get another polynomial of degree exactly two.
17147000	17150000	And that's almost always true.
17150000	17157000	But there's one coefficient, one scalar you can use to multiply by zero.
17157000	17166000	Then you get the zero polynomial and it is not a degree exactly two.
17166000	17172000	So this means that Q2 is not closed under scalar multiplication.
17172000	17181000	So Q sub 2 here failed all three of the requirements to be a subspace of P sub 2.
17181000	17185000	Yeah, it only has to fail one and not to not be a subspace.
17185000	17190000	But this one, in fact, failed all three properties.
17190000	17193000	Okay, let's look at one more.
17193000	17196000	Let's look at z set of all integers.
17196000	17202000	Okay, and I want to know is z a subset of the real numbers, a subspace of the real numbers.
17202000	17203000	Excuse me.
17203000	17206000	Clearly it's a subset of the real numbers.
17206000	17208000	All right, so we want to know number one.
17208000	17212000	Does z contain the zero element of the real numbers?
17212000	17219000	Well, the zero element of the real numbers is just zero and zero is an integer.
17219000	17224000	So the answer here is yes.
17224000	17227000	Is z closed under addition?
17227000	17232000	If you take two integers and add them together, do you get another integer?
17232000	17237000	And the answer is yes, because the sum of any two integers is always an integer.
17237000	17240000	So we pass number one, we pass number two.
17240000	17242000	How about number three?
17242000	17247000	Is it closed under scalar multiplication?
17247000	17254000	So if we multiply an integer by a scalar, do we always get another integer?
17254000	17259000	Let's think about that.
17259000	17262000	Trick here is that the scalar could be a real number.
17262000	17264000	You don't have to multiply by an integer.
17264000	17266000	Scalar just means a real number.
17266000	17269000	That real number might not be an integer.
17269000	17281000	If we multiply our integer by a non-integer, then we open up the chance that we could end up with a non-integer.
17281000	17283000	So the answer is no.
17283000	17285000	It's not closed under scalar multiplication.
17285000	17286000	And here's an example.
17286000	17293000	If I multiply 0.5, there's my scalar, times three, my integer, I get 1.5.
17293000	17296000	1.5 is not an integer.
17296000	17301000	So here's a scalar, times my integer, I don't get another integer.
17301000	17306000	And so therefore Z is not closed under scalar multiplication.
17306000	17318000	And therefore it's not a subspace of the real numbers.
17318000	17325000	Okay, we're continuing in section 4.1 with more about vector spaces.
17325000	17347000	And there's one way that we've talked about for finding if a set is a subspace of another vector space is to show that it satisfies the three properties that it contains the zero vector of the parent vector space.
17347000	17354000	It's closed under addition and it's closed under scalar multiplication.
17355000	17363000	So to show that a set is a subspace, then you can show that these three things hold.
17363000	17368000	Or to show that it's not a subspace, show that at least one of these does not hold.
17368000	17376000	But we have another method for showing that a set is a subspace.
17376000	17387000	And this doesn't work in all cases, but in some cases you can use this theorem and it makes life much easier because you don't have to go through and show those three properties hold.
17387000	17404000	So the theorem says if v1 through vp are vectors in some vector space v, then the span of v1 through vp is a subspace of v.
17404000	17426000	So what this says is that if we can write our set as the span of a finite set of vectors, right in this form, span of v1 through vp for some vectors, then automatically we can conclude that the set is a subspace.
17427000	17429000	So let's look at this example.
17429000	17433000	We have a set S which consists of all vectors of this form.
17433000	17437000	It's a subset of R3.
17437000	17441000	First component is two times some real number t.
17441000	17450000	Second component is zero and the third component is the negative of that real number t that we had up in the first component.
17450000	17456000	Okay, so how we want to know if S is a subspace of R3.
17456000	17468000	If we use the original method, that is to show that the three properties hold, then we would start with saying does S contain the zero vector.
17468000	17477000	And in this case, if we set t equal to zero, then we get zero, zero, zero.
17477000	17483000	And so the zero vector is contained in the set S.
17483000	17489000	We go to the next property as S closed under addition.
17489000	17503000	We need two generic vectors from S, so let's call those u and v, and u looks like two u, zero, negative u, and we'll say v is two v, zero, negative v.
17503000	17511000	And we need to add those together and see if the resulting vector is in the form that has to be an S.
17511000	17519000	So we add u and v and rearrange terms just a little bit and we end up with this vector here.
17519000	17528000	We've got two times u plus v on the first component, zero on the second component, negative u plus v in the third component.
17528000	17537000	And this is in the form that it needs to be to be in the set S because u plus v here is a real number.
17537000	17543000	We know that because these came from the vectors u and v.
17543000	17550000	So we have two times u plus v in the first component, negative u plus v in the last component, zero in the middle.
17550000	17555000	So it is in S. So S is closed under addition.
17555000	17561000	So we move on to the third property is S closed under scalar multiplication.
17561000	17571000	Well, again, we need a generic vector from the set S in a scalar, which we'll call C, and we compute C times u.
17571000	17583000	And again, rearranging terms a little bit, we can write it as two times cu in the first component, negative c times u in the last component, zero in the middle.
17583000	17590000	And since both c and u are real numbers, then c times u is a real number, so this vector is in S.
17590000	17596000	So we have all three properties satisfied, and therefore S is a subspace of R3.
17596000	17599000	Now there's a fair amount of work that went into that.
17599000	17609000	And so let's look at how we could use or if we could use this theorem to make the work a little easier.
17609000	17617000	All right, if we take a generic vector from the set S and we write it in parametric vector form.
17617000	17620000	So that means factor out the parameters.
17620000	17629000	In this case, there's only one t, and so we can write any vector in S as t times zero, two, zero, negative one.
17629000	17634000	So any vector in S is a multiple of this vector.
17634000	17640000	Therefore, S is equal to the span of this vector.
17640000	17649000	And since we've written S as the span of a finite set of vectors, that means by the theorem, S is a subspace of R3.
17649000	17659000	So you can see here that if your problem is one in which you can use this theorem, then it makes the work much easier.
17659000	17661000	Here's another example.
17661000	17666000	That set we'll call W. It's a subset of R4.
17666000	17672000	And we want to know is W a subspace of R4.
17672000	17680000	So again, we could go through the three properties, zero vector closed under addition, closed under scalar multiplication,
17680000	17685000	but it's much easier to apply the theorem.
17685000	17690000	So here we write this generic vector in parametric vector form.
17690000	17693000	There are three parameters here, a, b, and c.
17693000	17695000	We factor those out.
17695000	17706000	And so at this point, we have written this generic vector as a linear combination of these three vectors.
17706000	17713000	That is exactly what it means when we say that W is the span of those vectors.
17713000	17717000	Because any vector in W can be written as a linear combination of these three.
17717000	17724000	So again, we've got W is the span of a finite set of vectors.
17724000	17730000	And so by the theorem, it must be a subspace of R4.
17730000	17732000	All right, here's another one.
17732000	17735000	Set T given here.
17735000	17739000	And we want to know is it a subspace of R3.
17739000	17742000	So we proceed as we did before.
17742000	17743000	Apply.
17743000	17752000	I try to write this generic vector as a linear combination.
17753000	17755000	I apologize for the phone there.
17755000	17762000	Linear combination of vectors in R3.
17762000	17767000	And so we can write it, but notice what happens.
17767000	17777000	If we factor out the a and the b, we're left down here with a vector, but no parameter associated with it.
17777000	17784000	And so we can't write this vector as a linear combination of vectors.
17784000	17791000	Because a linear combination means you've got a parameter or a multiplier in front of each vector.
17791000	17794000	And here the multiplier is set at one.
17794000	17796000	We can't alter this.
17796000	17799000	So this is not a linear combination.
17799000	17805000	And so we cannot apply the theorem here.
17805000	17806000	All right.
17806000	17813000	And so in this case, our only alternative is to go back to the first method.
17813000	17821000	And so I'm going to go through that here just because it's always good to have practice in doing that.
17821000	17826000	So we look and we say is the zero vector in the set.
17826000	17834000	And a lot of times when you have a constant term like this, that's a red flag.
17834000	17840000	And you have to think about that very carefully because a lot of times in those cases it will not be a subspace.
17840000	17847000	Because in a lot of cases it moves you away from the origin, which means that your set does not contain the origin.
17847000	17853000	So that's what we're asking here is the zero vector in T.
17853000	17863000	Well, for the zero vector to be in there, that means that each of the elements, we need to be able to make each of the elements equal to zero.
17863000	17871000	So if you look at the first or the second component for this to be zero, that means A has to equal 6B.
17871000	17876000	And then the third component, it would mean A has to equal negative 2 times B.
17876000	17884000	Okay, so if we put that together, then that means that 6B is equal to negative 2B and down here at this point.
17884000	17889000	And solving that, that means that B has to be zero.
17889000	17894000	Alright, and if B has to be zero, A is 6B, so that means A is zero.
17894000	17900000	And in that case, if you look back at the first component, that says you get a one there.
17900000	17905000	And so therefore the origin is not contained in this set.
17905000	17912000	Alright, because to get zeros in the second and third positions, that means we're going to end up with a one in the first position.
17912000	17915000	So T does not contain the zero vector.
17915000	17921000	Now at this point, you could stop and say T is not a space of R3.
17921000	17929000	But just for practice, I'm going to continue on and say, is T closed under addition?
17929000	17934000	And the answer is no, it's not.
17934000	17940000	Now to show that, we need to add two vectors together.
17940000	17950000	We need to take two vectors from T, add them together, and see if we get another vector that is in the set.
17950000	17957000	So I picked 100, it's in the set because it's what we just talked about.
17957000	17963000	If the second two components are zero, then the first one has to be one.
17963000	17968000	And so 100 is in the set.
17968000	17971000	So I'm just going to add it to itself.
17971000	17975000	And the result is 200.
17975000	17978000	And so let's think, is that in the set?
17978000	17987000	And the answer is no, because as we said before, if the second two components are both zero, then the first one has to be one.
17987000	17995000	So this vector is not in T, and therefore, T is not closed under addition.
17995000	18000000	So just for practice again, let's check, is it closed under scalar multiplication?
18000000	18010000	And here we have no, the answer is no, because we've got, again, I just chose 100 because we know that vector is in T.
18010000	18014000	And multiply, actually we can multiply by anything other than one.
18014000	18022000	I multiply by two, we get 200, and that is not in T.
18022000	18029000	All right, so this particular set, T fails all three of those properties.
18029000	18033000	Now here's one that looks similar to T.
18033000	18038000	It's got the plus one, so it's got that constant term in it.
18038000	18049000	But notice here that the A in the first component here is not constrained by anything in the other two components.
18049000	18052000	And so this one's a little bit different.
18052000	18059000	Still, we cannot apply the theorem because of that one in the first component.
18059000	18073000	We cannot write S as a linear combination of vectors because we can't account for that one by doing that.
18073000	18085000	So my point really with this example is to show that just because you can't apply the theorem does not mean that the set is not a subspace.
18085000	18092000	And so in fact, this one is a subspace of R3, so we're going to go through and just show that.
18092000	18096000	So we have to go back to the original method for that.
18096000	18101000	And so we ask, is the zero vector in, that should be in the S.
18101000	18102000	Sorry about that.
18102000	18105000	This is zero vector in the set S.
18105000	18113000	And the answer is yes, because B and C can certainly be zero, and we can set A equal to negative one.
18113000	18118000	And in that case, we end up with a zero vector.
18118000	18120000	Is it closed under addition?
18120000	18128000	And the answer is, well, we take two arbitrary elements of the set, say U and V.
18128000	18132000	So here's what U looks like, here's what V looks like.
18132000	18137000	And when we add those together, we get this vector.
18137000	18141000	So just U2 plus V2 in the second component, U3 plus V3 in the third.
18141000	18145000	In the first, we get U1 plus V1 plus 2.
18145000	18149000	Now you ask, is this vector in the set?
18149000	18156000	And the answer is, well, it's not clear from looking at this whether it is or not.
18156000	18167000	However, if we write it in this form, then it's clear, because now, obviously the second two components are just real numbers.
18167000	18178000	And the first one, by factoring out a plus one, and then gathering what's left here in parentheses.
18178000	18183000	Now this thing here, U1 plus V1 plus 1, is a real number.
18183000	18188000	And so now, my first component looks like some real number plus one.
18188000	18192000	And so that's a real number.
18192000	18199000	U1 plus V1 plus 1 is real, U2 plus V2 is real, and U3 plus V3 are real.
18199000	18207000	Therefore, U plus V has to be in T, or in S. Sorry about that.
18207000	18210000	So it's closed under addition.
18210000	18214000	We take a similar approach to show that it's closed under scalar multiplication.
18214000	18219000	Again, take a generic vector and a scalar.
18219000	18222000	Multiply the two.
18222000	18232000	And again, here I had to factor out that plus one, so that I could make it look like the form that it has to be to be in the set.
18232000	18237000	And that leaves me, in this case, with this quantity here.
18237000	18243000	But that's a real number, right? Because C is real, U1 is real, obviously 1 is real.
18243000	18245000	So this quantity is a real number.
18245000	18250000	Plus one, and then the second two components are clearly real numbers.
18250000	18252000	And that's what it takes to be in the set.
18252000	18257000	So we can conclude that the set is closed under scalar multiplication.
18257000	18262000	And so it's a subspace of R3.
18262000	18271000	Another example, this one, first glance, looks like maybe you could use the theorem here.
18271000	18275000	But it turns out that you can't with that A times B in the first component.
18275000	18279000	There's no way to break that up in a linear combination.
18279000	18284000	And so this one, the theorem, does not apply.
18284000	18289000	And so we go back, again, to the original method.
18289000	18294000	And look and see, does this contain the zero vector?
18294000	18301000	And clearly it does, because you can set A and B both equal to zero, and that gives you the zero vector.
18301000	18304000	Is it closed under addition?
18304000	18310000	Well, here, let's go back and look at that set.
18310000	18317000	Maybe it's not clear, just looking at that, whether it would be closed under addition or not.
18317000	18323000	You have to do some thinking, a little work to arrive at a conclusion on that.
18323000	18328000	So really there are two approaches you could take.
18328000	18335000	One of them is to play around with the numbers and try to find a counter example to show that it's not closed.
18335000	18345000	So a counter example would be a specific example, specific numbers that you plug in and show that the set is not closed.
18345000	18350000	The other approach is to try to make a formal argument to show that it is closed.
18350000	18361000	And whichever one of these you pick really depends on the problem, and whether you have some intuition one way or the other.
18361000	18366000	And it also actually depends a little bit on your personality.
18366000	18372000	Would you rather play around with the numbers and try to come up with a counter example,
18372000	18379000	or would you rather take a more straightforward approach, which is to try to show that it's closed?
18379000	18386000	Because a lot of times in that case you're trying to show that it's closed, you will either succeed,
18386000	18392000	or you will get to a point where you see why it's not closed.
18392000	18400000	And so taking that route is a little more of a deliberate approach.
18400000	18405000	Playing with the numbers to try to find a counter example is a little more of a random approach.
18405000	18412000	But either is valid, and it kind of depends on, again, on your intuition,
18412000	18418000	whether you have a gut feeling one way or the other, and which you'd rather do.
18418000	18425000	What you think is the better way to go.
18425000	18428000	With this one, I'm going to trace the latter approach.
18428000	18435000	Because maybe I don't know, it's just not clear to me whether it's closed or not closed.
18435000	18438000	Maybe I don't have a gut feeling on that.
18438000	18442000	So I'm just going to take the safe deliberate approach and try to show that it's closed
18442000	18445000	and see where that takes me.
18445000	18448000	So I picked two arbitrary elements from the set.
18448000	18452000	These vectors I've written here, u and v.
18452000	18457000	And when I add them together, I get this vector.
18457000	18464000	And so what I want to know is if I multiply the second and third terms together,
18464000	18467000	does that equal the first term?
18467000	18475000	So I'm asking, does first term pq plus xy equal p plus x times q plus y?
18475000	18477000	And as I look at that, I say, well, no.
18477000	18480000	Sometimes it might, but in general, no.
18480000	18482000	That does not hold.
18482000	18488000	So that tells me this is probably not closed under addition.
18488000	18498000	And so I think I'm going to switch horses now, switch gears and try to find a counter example.
18498000	18502000	And so at this point, you kind of scratch this out.
18502000	18504000	Consider that to be your scratch work.
18504000	18506000	And you start over.
18506000	18508000	So start over here.
18508000	18511000	And this time, I'm going to try to find a counter example.
18511000	18518000	And so my advice is make life simple.
18518000	18522000	You can do a lot with just ones and zeros.
18522000	18526000	So here I chose u to be 1, 1, 1.
18526000	18527000	Second two components are 1.
18527000	18528000	Multiply them together.
18528000	18529000	You get 1.
18529000	18532000	So that means the first component is 1.
18532000	18536000	And v, I have 1, 2.
18536000	18539000	And the second and third components multiply those together and get 2.
18539000	18542000	So the first component in v has to be 2.
18542000	18543000	All right.
18543000	18546000	So both these vectors are in s.
18546000	18548000	And I add them together.
18548000	18551000	And so I get this vector 3, 2, 3.
18551000	18554000	Just adding component-wise, u and v.
18554000	18556000	And then I check, is this vector in s?
18556000	18557000	And the answer is no.
18557000	18563000	Because when I multiply the second two components together, 2 times 3, I get 6.
18563000	18566000	But my first component is not equal to 6.
18567000	18570000	It's equal to 3.
18570000	18575000	And so this vector, u plus v, is not in s.
18575000	18579000	So s is not closed under addition.
18579000	18582000	Now at this point, we know that s is not a subspace of r3.
18582000	18586000	But again, for practice, let's keep going and look and see,
18586000	18589000	well, is it closed under scalar multiplication?
18589000	18593000	And again, you got the choice, which approach you want to take.
18593000	18599000	Find a counter example or work on a formal argument to show that it is closed.
18599000	18604000	In this case, I'm going to think, hmm, you know, it wasn't closed under addition.
18604000	18610000	So I'm going to just bet that it's probably not closed under scalar multiplication.
18610000	18614000	So I'm going to fudge around and see if I can find a counter example.
18614000	18618000	So I need to come up with a vector that's of the general form.
18618000	18622000	And a scalar c, such that when I multiply c times the vector,
18622000	18626000	I get one that's not in the set.
18626000	18630000	This is a trial and error process.
18630000	18634000	And again, I would say start simple.
18634000	18636000	1's and 0's are good.
18636000	18639000	Now from before, we know that 1, 1, 1 is in the set,
18639000	18643000	because second two components, 1 times 1, is equal to 1.
18643000	18649000	And so, you know, I might multiply by 0, but then I'm going to get 0, 0, 0,
18649000	18651000	and that is in the set.
18651000	18654000	Multiplying by 1 doesn't do me any good, because that doesn't change the vector.
18654000	18656000	I want something that's outside the set.
18656000	18658000	So how about 2?
18658000	18662000	If I multiply 2 times u, I get 222.
18662000	18664000	And then I ask, is that in the set?
18664000	18668000	And so you multiply the second and third components together.
18668000	18673000	2 times 2 gives you 4, but the first component is not 4.
18673000	18676000	So therefore, this vector is not in the set,
18676000	18683000	and we've shown that s is not closed under scalar multiplication.
18683000	18688000	Okay, and so now you have some examples,
18688000	18690000	examples that you can apply this theorem on,
18690000	18693000	and some examples that you can't.
18693000	18697000	My advice is, you know, to, if you can't apply that theorem,
18697000	18702000	you want to, because it makes your life easier, much less work.
18702000	18705000	If you can't, then you must go back to the original definition
18705000	18710000	and show that the three properties either show that they all hold
18710000	18714000	or show that one of them doesn't hold.
18714000	18720000	And when you're doing that to show that one of the properties holds,
18720000	18723000	you must make a generic argument.
18723000	18726000	You know, to show that a set's closed under addition
18726000	18728000	or closed under scalar multiplication,
18728000	18730000	you must make a generic argument.
18730000	18735000	It's not sufficient to show that you can find two vectors
18735000	18738000	that you can add together and get one that's in the set.
18738000	18741000	You have to show that that holds for any two vectors that you pick,
18741000	18745000	and that means you must make a general argument
18745000	18753000	to show that a set's not closed under either addition or scalar multiplication.
18753000	18755000	You need to find a counter example,
18755000	18759000	and a counter example means come up with a specific example
18759000	18766000	with real numbers, and by real numbers, I mean actual numbers,
18766000	18769000	just like this one that's on your screen now.
18769000	18775000	You know, a specific vector, 1, 1, 1, a specific scalar, 2.
18775000	18778000	Multiply those together and see what you get
18778000	18780000	and show that that's not in the set.
18780000	18782000	So that's a counter example.
18782000	18786000	So to show that it is closed, make a general argument
18786000	18795000	to show that it's not closed, find a specific example.
18795000	18800000	In section 4.2, we're going to talk about a couple of concepts
18800000	18804000	that aren't familiar with the terminology,
18804000	18808000	but we are familiar with the concepts behind these terms.
18808000	18812000	First of these is the null space of a matrix.
18812000	18816000	We say the null space of an M by N matrix A,
18816000	18819000	written as null A,
18819000	18824000	is a set of all solutions to the homogeneous equation Ax equals 0.
18824000	18826000	So null A is simply a set of all x,
18826000	18832000	such that x is in Rn and Ax equals 0.
18832000	18837000	X has to be in Rn because A is an M by N matrix,
18837000	18840000	and we want to be able to multiply A times x,
18840000	18844000	and there must be an entry in x for every column of A,
18844000	18848000	and so x must be from Rn.
18848000	18855000	So null A is simply the set of all solutions to Ax equals 0.
18855000	18860000	The fact that we're calling it the null space probably gives you a clue
18860000	18862000	that it's a subspace,
18862000	18863000	and indeed it is.
18863000	18866000	The theorem here says the null space of an M by N matrix
18866000	18871000	is a subspace of Rn.
18871000	18875000	Let's look at this matrix A given here.
18875000	18879000	To generate an explicit description of null A,
18879000	18881000	we have to solve Ax equals 0.
18881000	18884000	An explicit description means you can look at it
18884000	18888000	and generate an entry in null A.
18888000	18892000	So to do that, we have to solve Ax equals 0.
18892000	18895000	So we throw that into an augmented matrix
18895000	18898000	and do a couple of row operations,
18898000	18900000	giving us the matrix given here.
18900000	18907000	So we see the general solution is x1 equals 6x2 plus 2x4,
18907000	18910000	and x3 equals negative x4,
18910000	18913000	and x2 and x4 are free variables.
18913000	18916000	If we put it in parametric vector form,
18916000	18920000	we have x1 is 6x2 plus 2x4,
18920000	18925000	x2 is just x2, and so forth.
18925000	18928000	So notice that any vector in the null space of A
18928000	18932000	is a linear combination of these two vectors given here.
18932000	18937000	So if we look at those,
18937000	18940000	we can say any vector in the null space of A
18940000	18946000	is in the span of those two vectors.
18946000	18951000	And if you remember the theorem from section 4.1,
18951000	18954000	it said that if you can write your set
18954000	18956000	as the span of a finite set of vectors,
18956000	18960000	then it's automatically a subspace of that apparent vector space.
18960000	18964000	So here we've written, for this particular matrix A,
18964000	18967000	we've written a set of solutions to Ax equals 0
18967000	18970000	as the span of these two vectors.
18970000	18975000	So therefore, the null space of A is a subspace of R4.
18976000	18982000	The second concept that we're going to talk about here
18982000	18985000	in this section is the column space of a matrix.
18985000	18988000	And once again, it's new terminology,
18988000	18992000	but not new fundamental material here.
18992000	18995000	The column space of an M by N matrix A,
18995000	18998000	which we write as call A,
18998000	19002000	is the set of all linear combinations of the columns of A.
19002000	19007000	All linear combinations of the columns of A.
19007000	19011000	We know that to be the span of the columns of A.
19011000	19014000	So if A is equal to A1 through An,
19014000	19020000	the column space of A is just the span of A1 through An.
19020000	19026000	And since the definition here is that column space of A
19026000	19028000	is the span of a set of vectors,
19028000	19034000	so it too is a subspace of some vector space.
19034000	19037000	In this case, if A is M by N,
19037000	19041000	then the column space is going to be in Rm,
19041000	19046000	because when you take a linear combination of vectors with M components,
19046000	19049000	you're going to get another vector with M components.
19049000	19055000	So the column space of A is in Rm.
19055000	19057000	Another way to look at the column space of A
19057000	19059000	is to write it as a set of all B,
19059000	19063000	such that B is equal to A times X for some X in Rn.
19063000	19065000	Because when you multiply A times X,
19065000	19073000	you're simply taking a linear combination of the columns of A.
19073000	19076000	Here's a set S,
19076000	19079000	defined in terms of this generic vector,
19079000	19082000	and you're asked to find a matrix A
19082000	19086000	such that S is equal to the column space of A.
19086000	19090000	So we simply take that generic vector,
19090000	19093000	write it in parametric vector form,
19093000	19095000	so we can write anything in S
19095000	19100000	as a linear combination of these three vectors given here.
19100000	19103000	So if we put those vectors into a matrix,
19103000	19106000	then we can say that anything in S
19106000	19110000	is a linear combination of the columns of A.
19110000	19118000	And so S is equal to the column space of A.
19118000	19120000	As I said before, the null space of A
19120000	19123000	and the column space of A are simply new terms
19123000	19127000	for describing entities with which we are already familiar.
19127000	19131000	Null A is just a set of all solutions to AX equals zero.
19131000	19137000	That dates back to section 1.31.4 or somewhere back there.
19137000	19142000	A column space of A is just the set of all linear combinations of the columns
19142000	19145000	or the span of the columns.
19145000	19149000	So again, we're going back to fundamental information
19149000	19153000	that we learned in chapter 1.
19153000	19156000	Okay, so if we have a matrix A,
19156000	19159000	how do we determine if a particular vector X
19159000	19162000	is in the null space of that matrix?
19162000	19165000	Well, go back to the definition.
19165000	19169000	Null space of A is a set of all vectors satisfying AX equals zero.
19169000	19175000	So we just need to multiply A times X and see if we get zero.
19175000	19181000	Again, to multiply A times X,
19181000	19185000	we're taking a linear combination of the columns of A.
19185000	19187000	So there needs to be a component of X
19187000	19190000	that corresponds to each column of A.
19190000	19193000	There are n columns in A,
19193000	19195000	and n elements in the null space of A.
19195000	19202000	So null A is in Rn.
19202000	19204000	Okay, so here's a vector,
19204000	19208000	and we're asked is this in the null space of the given matrix?
19208000	19213000	So we simply multiply the matrix times this vector.
19213000	19217000	So we're taking a linear combination of the columns here
19217000	19220000	and go through the arithmetic,
19220000	19222000	and we end up with a zero vector.
19222000	19225000	So the answer is yes, it is in the null space of A,
19225000	19229000	because A times X is equal to zero here.
19229000	19234000	Alright, what if we have a matrix A and another vector,
19234000	19239000	and we want to know if that vector is in the column space of the matrix?
19239000	19241000	So again, look at the definition.
19241000	19247000	B is in the column space of A if AX equals B is consistent.
19247000	19250000	So we need to solve a system to determine
19250000	19253000	if B is in the column space of A.
19253000	19260000	And again, the column space of A is going to be a subset of Rm,
19260000	19266000	because we're taking linear combinations of vectors with m components,
19266000	19271000	and so we get another vector with m components.
19271000	19274000	Alright, so here's a vector,
19274000	19276000	and we want to know if it's in the column space of A
19276000	19278000	for the given matrix A.
19278000	19282000	So we need to solve the system,
19282000	19285000	set up the augmented matrix, solve the system.
19285000	19289000	So I've done that, left out the row operations,
19289000	19291000	but we end up with this matrix here.
19291000	19293000	And so the question is,
19293000	19296000	does this correspond to a consistent system?
19296000	19297000	And the answer is yes,
19297000	19302000	because we have no rows, zero, zero, zero, something not zero.
19302000	19306000	The fact that we have row of all zeros is really irrelevant.
19306000	19309000	The fact that we have free variable is irrelevant.
19309000	19314000	The only thing that's relevant is that we don't have a row that's zero, zero, zero,
19314000	19318000	not zero, because that would tell us it's inconsistent.
19318000	19320000	So since the system is consistent,
19320000	19326000	that means that this vector is indeed in the column space of A.
19326000	19329000	Alright, let's look at this example.
19329000	19332000	Here's the set W, and we're asked,
19332000	19337000	is it a subspace of R4?
19337000	19339000	So we can write it,
19339000	19342000	write any generic element of W
19342000	19344000	as the linear combination of these two vectors.
19344000	19347000	So we write it in parametric vector form.
19347000	19351000	And so we can say that W is equal to column space of this matrix.
19351000	19354000	Just taking these two vectors, sticking them in the columns of a matrix.
19354000	19360000	The column space of this matrix is all linear combinations of these columns,
19360000	19362000	which is exactly W.
19362000	19368000	So that means that W is a subspace.
19368000	19371000	Alright, how about this example?
19371000	19381000	This one, it's not so easy or so clear to see,
19381000	19388000	because we have this constant term one here,
19388000	19396000	and so we can't write this set as a linear combination of vectors.
19396000	19399000	So in this case, we have to go back to the definition of subspace.
19399000	19405000	That's those three properties that every subspace must satisfy.
19405000	19410000	So first one was does it contain the zero vector?
19410000	19414000	And if we look, for it to contain the zero vector,
19414000	19418000	clearly D has to be zero,
19418000	19424000	which would mean that the second component would be two times zero plus one,
19424000	19426000	which is one.
19426000	19429000	And so if you get zero here, you're going to have one up here.
19429000	19435000	And therefore, the zero vector is not in this set.
19435000	19441000	So it is not a subspace of R4.
19441000	19445000	About this set, is this set a subspace of R4?
19445000	19451000	So we've got all vectors A, B, C, D that satisfy these two equations.
19451000	19455000	There's actually two ways we could go about doing this,
19455000	19459000	using what we've learned in this section.
19459000	19462000	One is easier than the other.
19462000	19467000	Let's take the hard way first.
19467000	19473000	And we're going to try to write S as the column space of some matrix A.
19473000	19477000	So we need to figure out what A would be.
19477000	19480000	We're going to simplify things a little bit.
19480000	19485000	Instead of C here, we're just going to substitute A plus 3B.
19485000	19489000	And then instead of D, we're going to substitute A plus B plus C.
19489000	19495000	But we're going to plug in what C is since we've gotten rid of C.
19495000	19502000	So D is B plus C plus A, and C is A plus 3B.
19502000	19504000	So plug that in for C.
19504000	19508000	And we end up with D equals 2A plus 4B.
19508000	19513000	So we can write any vector in S in this form.
19513000	19519000	So we replace C with A plus 3B and D with 2A plus 4B.
19519000	19525000	And then it's just a matter of writing that in parametric vector form
19525000	19527000	and throwing those columns into a matrix.
19527000	19533000	And so S is equal to the column space of A, where A is given here.
19533000	19538000	And thus, S would be a subspace of R4,
19538000	19542000	since the column space of any matrix is a subspace.
19542000	19546000	Now the other way to look at it, so I rewrote the problem here.
19546000	19552000	The other way to look at it is to note that we could write S in this form.
19552000	19557000	So basically I've taken these two equations that we have here.
19557000	19561000	And I've taken all the variables over to the left side.
19561000	19564000	And so we've got zeros on the right.
19564000	19569000	And so when you see zeros on the right, you should think,
19569000	19572000	that's a homogeneous system.
19572000	19578000	And so we can write S as the null space of A,
19578000	19583000	where A is equal to the coefficient matrix from this system of equations.
19583000	19590000	So you see 1, 3, negative 1 for ABC and then 0 for D and from the first equation.
19590000	19596000	And then the second equation we get 1 times A plus 1 times B plus 1 times C minus 1 times D.
19596000	19599000	So that's where the second row comes from.
19599000	19601000	So remember what the null space of a matrix is.
19601000	19605000	It's just the set of all solutions to AX equals 0.
19605000	19610000	And so I've just taken this system of equations, written it,
19610000	19615000	or taken these equations, written them as a homogeneous system.
19615000	19621000	And then I can use the fact that the null space of a matrix is always a subspace.
19621000	19622000	And we're done.
19622000	19625000	So that's why I said one was harder than the other.
19625000	19630000	This is clearly the easiest way.
19630000	19635000	All right.
19635000	19642000	Let's revisit the concept of linear transformations for just a bit.
19642000	19650000	And this is, again, back to chapter 1, where we talked about linear transformations.
19650000	19658000	One new term that we didn't learn back in chapter 1 was the idea of the kernel of a linear transformation.
19658000	19666000	And the kernel of a linear transformation is simply the set of all vectors that map to the 0 vector.
19666000	19679000	So the kernel of a linear transformation is exactly the null space of the matrix that defines the transformation.
19679000	19685000	So kernel and null space are analogous concepts.
19685000	19693000	The range of a transformation is the set of all vectors that get mapped to.
19693000	19697000	So the range is in the co-domain.
19697000	19699000	Sometimes it's all of the co-domain.
19699000	19700000	Sometimes not.
19700000	19709000	But the range is a set of all vectors that get mapped to by some vector from the domain.
19709000	19722000	And so the range of t is actually the column space of a, where a is the matrix that defines the transformation.
19722000	19726000	So the kernel of the transformation is the null space of a.
19726000	19732000	The range of the transformation is the column space of a.
19732000	19744000	Okay, last I want to talk about just looking at the contrast between the null space of a matrix and the column space.
19744000	19749000	These really on the surface are very different sets.
19749000	19753000	We will kind of pull them together a little bit later on in this chapter.
19753000	19764000	But for right now, they're really very different sets and they don't share analogous sorts of ideas.
19764000	19768000	But that's kind of the extent of it at this point.
19768000	19771000	So let's assume a is an m by n matrix.
19771000	19777000	Then the null space is in our n column space is in our m.
19777000	19780000	The null space is implicitly defined.
19780000	19785000	That means you're given a condition that vectors in the null space must satisfy.
19785000	19791000	But you can't look at a matrix a and know which vectors are in it.
19791000	19795000	You have to solve that system a equals zero.
19795000	19799000	On the other hand, the column space of a is explicitly defined.
19799000	19803000	Because it's just the set of all linear combinations of the columns.
19803000	19811000	So you can look at the matrix a and you know that the columns that you're looking at are actually in the column space.
19811000	19816000	And you know how to create more entries in the column space.
19816000	19819000	To find vectors in the null space requires work.
19819000	19823000	You have to solve the system a x equals zero.
19823000	19828000	Note, however, that the zero vector is always in the null space of a.
19828000	19836000	Because a times the zero vector gives you the zero vector.
19836000	19841000	To find vectors in the column space of a, you just compute linear combinations of the columns.
19841000	19848000	So it's a direct process to do that.
19848000	19852000	There's no obvious relationship between null a and the entries in a.
19852000	19862000	On the other hand, the relationship between a and the column space of a is obvious since each column is in the column space.
19862000	19868000	The typical vector v in the null space satisfies a times v equals zero.
19868000	19876000	Typical vector in the column space has the property that a x equals v is consistent.
19876000	19884000	For null space, you're multiplying a times v to see if you get zero.
19884000	19889000	To see if a vector is in the column space, you're solving a system a x equals v.
19889000	19892000	So v is on the right hand side in that case.
19892000	19897000	Given a specific vector v, it's easy to determine if v is in the null space.
19897000	19900000	You just see if a times v is equal to zero.
19900000	19905000	To determine if v is in the column space, you have to solve a system of equations.
19906000	19918000	The null space equals only the zero vector if and only if a x equals zero has only the trivial solution.
19918000	19920000	So how do you get what's in the null space?
19920000	19922000	We have to solve a x equals zero.
19922000	19929000	And if you get only the trivial solution, then that means there's only one solution, which is a zero vector.
19930000	19938000	The column space of a is equal to r m if and only if a x equals b is consistent for every b in r m.
19938000	19946000	That means that everything, no matter what you put on the right hand side, the system will be consistent.
19946000	19950000	So every b is a linear combination of the columns of a.
19950000	19958000	And then relating it to linear transformations, the null space of a is equal to the zero vector only.
19958000	19963000	If and only if the transformation x to a x is one to one.
19963000	19973000	So if you back up to the previous one, we had that null a is equal to only the zero vector if a x equals zero has only the trivial solution.
19973000	19982000	Remember that's going to occur when you have no free variables and or if there's a pivot position in every column.
19982000	19987000	And so we know that indicates that the transformation is one to one.
19987000	19995000	And then the column space is equal to r m if and only if a x equals b is consistent for every b.
19995000	19999000	So that means every b in the co domain gets mapped to.
19999000	20003000	So the transformation must be onto r m.
20003000	20015000	Okay, we're going to start today talking about a basis for a vector space.
20015000	20032000	And the idea of a basis is that it's a minimal in terms of number of vectors and representative in that it represents the vector space.
20032000	20037000	Set, so it's a minimal representative set.
20037000	20043000	So let's explore what that actually means.
20043000	20053000	Before we really get into that, let's back up a little bit and remember what a couple of concepts are that are key here.
20053000	20057000	One is linear independence and the other is spanning.
20057000	20060000	So let's start with linear independence.
20060000	20062000	So here's a definition.
20062000	20068000	A set of vectors v1 through vp is said to be linearly independent.
20068000	20077000	If the only solution to the equation, we take a linear combination of the vectors and set it equal to zero.
20077000	20080000	This linear combination set equal to zero.
20080000	20087000	The only solution to that is when you set all the coefficients equal to zero.
20087000	20091000	Note that this is always a solution.
20091000	20095000	We can always set the coefficients equal to zero and generate the zero vector.
20095000	20099000	What we want to know is if this is the only solution.
20099000	20107000	So thinking in terms of a system of equations, we know the system's consistent.
20107000	20113000	We want to know if the solution is unique or if they're an infinite number of solutions.
20113000	20116000	So that's when we get into talking about free variables.
20116000	20120000	Does the system have a free variable or not?
20120000	20126000	So we set up that augmented matrix for that system.
20126000	20130000	We put an initial inform and we want to see if there are free variables.
20130000	20138000	Because if there are free variables, that means an infinite number of solutions and the vectors would not be linearly independent.
20138000	20150000	And if we don't have free variables, that means the solution is unique so the vectors are linearly independent.
20150000	20159000	So to check for linear independence, we really don't have to tack on that zero column on the augmented side.
20159000	20166000	We can just look at the coefficient matrix, put it in an initial inform and see if there's a pivot position in every column.
20166000	20169000	Because we want to know are there free variables or not.
20169000	20175000	So is there a pivot position in every column?
20175000	20179000	Okay, there are a few cases that are obvious.
20179000	20187000	One is where you have more vectors than you have entries in each vector like this matrix given here.
20187000	20193000	These three vectors could not be linearly independent because you have three vectors in R2.
20193000	20200000	There's no way you can have a pivot position in each column.
20200000	20206000	If you just have two vectors, they're linearly independent if neither is a multiple of the other.
20206000	20211000	And in general, a set of vectors is linearly independent if none.
20211000	20220000	Okay, I can't find any of the vectors in the set that can be written as a linear combination of the others.
20220000	20224000	Alright, let's move on to spanning sets.
20224000	20230000	Take that same set of vectors, v1 through vp, and assume they're in Rn.
20230000	20238000	And they are said to span Rn if the equation, where you take a linear combination of those vectors and set it equal to b,
20238000	20242000	is consistent for every b in Rn.
20242000	20249000	That means you can take a linear combination of those vectors and generate any vector in Rn.
20249000	20257000	Now obviously we can't solve this system for every b in Rn.
20257000	20262000	So how do we know if it's going to be consistent for every b or not?
20262000	20268000	Then linear independence, right hand side was zero, we can solve that system.
20268000	20271000	But here we want it to be consistent for every b.
20271000	20277000	So we can't go and plug in every possible vector b on the right and solve the system
20277000	20280000	to see if it's going to be consistent for everyone.
20280000	20282000	So what do we do?
20282000	20297000	Well, we know that it will be consistent if we never end up with a row where we've got all zeros and then something not zero on the right hand side.
20297000	20301000	When we put the matrix in echelon form.
20301000	20308000	So if we have this, where all zeros and then something not zero, then it's inconsistent.
20308000	20318000	So we want to know if we never get that in the way that that happens is we have a pivot position in every row.
20318000	20323000	If there's a pivot position in every row of the coefficient part of the matrix,
20323000	20329000	then you'll never end up with all zeros in a row of the coefficient part of the matrix.
20329000	20341000	So to check to see if a set of vectors is spans, whatever vector space they're in,
20341000	20347000	we need to have a pivot position in every row.
20347000	20353000	So the columns of A are linearly independent if there's a pivot position in every column of A
20353000	20359000	and they span RM if there's a pivot position in every row of A.
20359000	20365000	It's assuming A is an M by N matrix, so the columns are in RM.
20365000	20367000	Alright, so let's look at some examples.
20367000	20370000	Here's a set with just one vector.
20370000	20373000	Is it linearly independent?
20373000	20380000	And the answer is yes, because you have a single non-zero vector.
20380000	20383000	It's always going to be linearly independent.
20383000	20391000	The only time just a single vector is linearly dependent is if that vector is the zero vector.
20391000	20394000	Does this set span R2?
20394000	20401000	And the answer is no, because if you look at all linear combinations of that vector,
20401000	20406000	you're just getting multiples of that vector, which is a line in R2.
20406000	20411000	So you only get a line, not all of R2.
20411000	20416000	Alright, let's look at this set T.
20416000	20425000	Now I've got three vectors and first let's ask is T linearly independent?
20425000	20433000	And the answer is no, because you've got more vectors than there are entries in each vector.
20433000	20440000	So if you put those vectors in a matrix, there's no way you can have a pivot position in every column.
20440000	20444000	Does this set span R2?
20444000	20452000	Yes, because if you just even look at the first two vectors, they are not multiples of each other,
20452000	20458000	and so not collinear, so they will span the plane.
20458000	20465000	Now if you want to go back and just look at it in matrix form, take those vectors,
20465000	20472000	put them in a matrix, put it in echelon form, and look and see, do you have a pivot position in every column?
20472000	20475000	No, so they're not linearly independent.
20475000	20477000	Do you have a pivot position in every row?
20477000	20485000	Yes, so they do span R2.
20485000	20489000	Okay, now let's look at one more set.
20489000	20494000	Now this one is like the other one, except I just took out that last vector.
20494000	20497000	So again, is it linearly independent?
20497000	20506000	Yes, we've got two vectors and neither is a multiple of the other, so they're linearly independent.
20506000	20508000	So it's span R2.
20508000	20511000	Notice that should be U instead of T there.
20511000	20513000	Does it span R2?
20513000	20519000	And again, it's yes for the same reason as that used before.
20519000	20521000	Look at it in matrix form.
20521000	20526000	You've got a pivot position in every row, so the vectors span R2.
20526000	20531000	You have a pivot position in each column, so they're linearly independent.
20531000	20541000	So we see that any set that spans R2 has to have at least two vectors because you have to have one in each row, a pivot position in each row.
20541000	20547000	Any set that's linearly independent must have two or fewer vectors, right?
20547000	20551000	Because once you get over two, you can't have a pivot position in each column.
20551000	20560000	So any set that's both linearly independent and spans R2 has to have exactly two vectors, right?
20560000	20565000	Exactly two, because to span, it needs at least two.
20565000	20568000	To be linearly independent, it needs no more than two.
20568000	20574000	So if you want both, then you have to have exactly two vectors.
20574000	20583000	Such a set that's both linearly independent and spans R2 is said to be a basis for R2.
20583000	20585000	Here's a formal definition.
20585000	20599000	Set of vectors b1 through bp is a basis for some subspace h if the set's linearly independent and the span of the set is the subspace.
20599000	20603000	Okay, so we need two pieces to be a basis.
20603000	20608000	One linear independence, the other must span the subspace.
20608000	20611000	So look at a few examples.
20611000	20618000	Here's a set, and I want to know is this a basis for R3?
20618000	20627000	So pick one of the criteria, either linearly independent or spans, and see if those are satisfied.
20627000	20631000	So let's start off with is it linearly independent?
20631000	20639000	Well, if you set up the augmented matrix, set it equal to zero,
20639000	20646000	then you can clearly see you don't have a pivot position in the second column here,
20646000	20650000	and so can't be linearly independent.
20650000	20653000	So it's not a basis.
20653000	20655000	Alright, about another set.
20655000	20663000	There's one not so clear here whether these vectors are linearly independent.
20663000	20673000	So we put them in a matrix, do some row operations, and we end up here with a coefficient matrix.
20673000	20680000	Notice it has a pivot position in every column, so the only solution is the trivial solution.
20680000	20684000	Therefore, the vectors are linearly independent.
20684000	20690000	We can also look at the matrix here and see that there's a pivot position in every row.
20690000	20696000	So they span R3, and therefore they must be a basis for R3.
20696000	20701000	So the key is looking at the coefficient matrix in echelon form.
20701000	20705000	You can see a pivot position in every column, so they're linearly independent,
20705000	20711000	pivot position in every row, so they span R3.
20711000	20715000	Now here's another set, is this a basis for R3?
20715000	20720000	And you're probably thinking, well, there's only two vectors,
20720000	20727000	and in R3 you need three vectors to span, and that's right, because if you look at that matrix,
20727000	20733000	you can't have a pivot position in every row, so therefore these vectors can't span R3,
20733000	20738000	and therefore can't be a basis for R3.
20738000	20740000	Now how about this one?
20740000	20745000	Look at this one, maybe you're thinking, oh, we've got four vectors in R3,
20745000	20748000	they can't be linearly independent, right?
20748000	20757000	Because if you put them in a matrix, there's no way to have a pivot position in every column.
20757000	20761000	We've got four vectors, only three rows, can't have a pivot position in every column,
20761000	20765000	so not linearly independent.
20765000	20769000	Now here's a little different question.
20769000	20777000	Here I've got a set of four vectors, and I'm not asking though, is this a basis for R3?
20777000	20783000	Clearly it's not, because four vectors can't be linearly independent.
20783000	20790000	What I'm asking though is for you to find a basis for the span of this set.
20790000	20794000	Okay, so we don't really know what the span of this set is,
20794000	20801000	it's either all of R3 or some piece of R3, we don't know.
20801000	20812000	So we want to know whatever that space is, what is the basis for it?
20812000	20817000	Well, one of the criteria for a basis is that it spans the set.
20817000	20826000	So clearly, if we just take all four vectors, they're going to span W, because that's how W is defined.
20826000	20828000	So the question is, are they linearly independent?
20828000	20836000	And we already know that they're not, because there's four vectors in R3.
20836000	20841000	So remember, since they're not linearly independent,
20841000	20846000	then that means that at least one of them is a linear combination of the others.
20846000	20854000	And so one strategy would be to throw out the ones that are dependent on the others,
20854000	20863000	and then reduce that down to where we have a linearly independent set.
20863000	20870000	Now in this case, I'll point out to you that the second vector here is twice the first,
20870000	20875000	you can look at that, two times one is two, two times negative four, negative eight, and so forth.
20875000	20880000	And the last one, vector four here, is the sum of the first and the third.
20880000	20887000	So we've got one plus zero gives you one, negative four plus three is negative one, so forth.
20887000	20894000	So that tells us that the second vector is dependent on the first.
20894000	20901000	It's a linear combination of the first, and the last one is a linear combination of the first and the third.
20901000	20904000	So it seems like we should be able to throw those out.
20904000	20907000	We don't need them.
20907000	20911000	And if we do, then we're left with these two vectors,
20911000	20916000	and we can look at it and see that it's linearly independent,
20916000	20920000	because we've got two vectors that are not multiples of each other.
20920000	20927000	It's still a little iffy on whether this set spans the original set W.
20927000	20935000	So because we threw out some, how do we know that these two still span the subspace?
20935000	20941000	Well, let's introduce a little shortcut notation here.
20941000	20947000	Let's say we're going to call the four original vectors V1 through V4.
20947000	20958000	And so any vector in the span of those vectors, or any vector in W, can be written in this form.
20958000	20960000	Linear combination of those four vectors.
20960000	20968000	Now, we know, though, that V2, the second vector, was two times the first one.
20968000	20974000	So we have this relationship here, and we know that V4 was the sum of the first and the third.
20974000	20976000	So we have this relationship.
20976000	20980000	So we can plug that in back up in this equation.
20980000	20986000	So substituting for V2, we can plug in 2V1.
20986000	20990000	And for V4, we can plug in V1 plus V3.
20990000	20997000	And then if we do a little rearranging, we can write V as just a linear combination of V1 and V3.
20997000	21000000	We've got different coefficients, but that's okay.
21000000	21003000	Just needs to be a linear combination of V1 and V3.
21003000	21015000	So here, we've shown that any vector that was a linear combination of V1, V2, V3, V4 is also a linear combination of just V1 and V3.
21015000	21024000	Therefore, just V1 and V3, just those two vectors, will span W.
21024000	21026000	And we've already said that they're linearly independent.
21026000	21029000	So therefore, they are a basis for W.
21030000	21037000	So we start off with our big set throughout the ones that were dependent on the others.
21037000	21042000	And what we were left with still spans, and it's linearly independent.
21042000	21044000	So it's a basis.
21046000	21051000	Now, in that problem, I told you which vectors were dependent on the others.
21051000	21055000	So what can you do when that's not so obvious?
21055000	21062000	How can you figure out which ones are dependent when you can't just look at it and tell?
21062000	21066000	Well, there's an amazing thing.
21066000	21077000	And that is that when you do elementary row operations, the dependence relations among the columns are not changed.
21077000	21082000	So these are the four vectors we had before.
21082000	21085000	Remember, the second one is two times the first.
21085000	21088000	The last one is the sum of the first and the third.
21088000	21092000	Alright, so I put it in echelon form, and I get this matrix.
21092000	21097000	And look, the second column here is two times the first, right?
21097000	21102000	Two times one is two, two times zero zero, two times zero zero.
21102000	21107000	And the last column is still the sum of the first and the third.
21107000	21111000	One plus zero is one, zero plus one is one, zero plus zero zero.
21111000	21118000	So you can take your original matrix or original vectors, throw them in a matrix,
21118000	21123000	put the matrix in echelon form or reduced echelon form, it doesn't matter.
21123000	21134000	And in that matrix, the vectors will still exhibit the same relationships among each other.
21134000	21142000	So what you see here is that the ones that we want to keep are the ones where we have pivot positions.
21142000	21150000	Okay, because if one, like the second one here is dependent on the first one, there's no pivot position in column two.
21150000	21156000	Column four is dependent on the first and the third, there's no pivot position in column four.
21156000	21168000	So the pivot columns indicate which columns are, indicate the columns that are linearly, that should be independent.
21168000	21173000	Pivot columns indicate the columns that are linearly independent.
21173000	21177000	Okay, so those are the ones we want to keep.
21177000	21185000	We have a theorem that says that the pivot columns form a basis for the column space of A.
21185000	21194000	All right, now along with this comes a warning, which is to be careful to use the pivot columns of A.
21194000	21200000	That is the original matrix, not some echelon or reduced echelon form of A.
21200000	21208000	And that's because elementary row operations can change the column space of a matrix.
21208000	21212000	Okay, so we go back and look at that one example one more time.
21212000	21223000	The column space of A, that is all linear combinations of the columns of A, is not the same as a set of all linear combinations of the columns of B.
21223000	21225000	Those are not the same.
21225000	21232000	And so the column spaces are not the same.
21232000	21238000	For example, the first column of A is clearly in the column space of A.
21238000	21242000	One times first column, zero times the others, and that's what you get.
21242000	21246000	But that column is not in the column space of B.
21246000	21247000	Why?
21247000	21257000	Well, when we set up the system, we've got this right hand side, notice system's inconsistent, because we've got all zeros and then something not zero.
21257000	21260000	Okay, it's not in the column space.
21260000	21262000	So the column spaces are different.
21262000	21270000	Therefore, you can't take the pivot columns from B and say that that's a basis for the column space of A.
21270000	21283000	You need to look in your echelon form or your reduced echelon form, see which ones are the pivot columns, and then go back and pull those from the original matrix.
21283000	21285000	Alright, here's another example.
21285000	21290000	Here's a matrix and you're asked to find a basis for the column space of this matrix.
21290000	21300000	So we put it in echelon form, get this matrix, and notice we've got pivot column in a pivot position in the first column, third column, and the fifth column.
21300000	21310000	So therefore, we go back and take the first, third, and fifth columns of A for a basis for the column space of A.
21310000	21312000	Okay, and here's a little different question.
21312000	21317000	Same matrix, you're asked to find a basis for the null space of A.
21317000	21322000	Alright, remember the null space, set of all solutions to AX equals zero.
21322000	21325000	So let's solve AX equals zero.
21325000	21328000	Get the same matrix here.
21328000	21332000	We look at the general form of the solution.
21332000	21338000	From the first row, we get X1 is going to be negative 2X2 minus 4X4.
21338000	21343000	X3 is going to be 7 fifths, X4.
21343000	21347000	And from the third row, we get X5 is just equal to zero.
21347000	21350000	X2 and X4 are free variables.
21350000	21356000	So if we put that in parametric vector form, we get what's given here.
21356000	21363000	And so any vector in the null space of A can be written as a linear combination of these two vectors.
21363000	21368000	Alright, so that means that they span the null space of A.
21368000	21371000	So we're halfway to being a basis.
21371000	21382000	Still need to show that they are linearly independent or reduce the set so that we get a linearly independent set.
21382000	21387000	So the question is, we know that they span, are they linearly independent?
21387000	21393000	And if you just look at them, it's clear that they are because you've got two vectors that are not multiples of each other.
21393000	21396000	So they must be linearly independent.
21396000	21402000	So these two vectors will be a basis for the null space of A.
21402000	21411000	Now, an important point is that in general, when you solve AX equals zero and write your solution in parametric vector form,
21411000	21416000	the vectors that you get will always be linearly independent.
21416000	21420000	So in fact, when you write that solution in parametric vector form,
21420000	21427000	you've got a set of vectors that spans and they will be linearly independent.
21427000	21430000	Now, why are they linearly independent?
21430000	21437000	Because the only way to produce the zero vector is to set each of the free variables equal to zero.
21437000	21440000	So if you go back and look at the one we were just looking at,
21440000	21446000	if you didn't know, if you can just look at that and tell that they're not multiples of each other,
21446000	21449000	so they're linearly independent, if you tried to solve the system,
21449000	21460000	then notice that in the second component, you've got X2 plus zero would equal zero.
21460000	21462000	So X2 has to be zero.
21462000	21466000	And X4 times one equals zero means X4 has to be zero.
21466000	21477000	Because of the way these vectors are produced, you always end up with the element that corresponds to the free variable.
21477000	21481000	So in this case, the second component, since this is multiplied by X2,
21481000	21490000	if you looked at that whole row, then the only solution would be set X2 equal to zero.
21490000	21493000	And similarly for X4, X4 has to be zero.
21494000	21498000	So when you write your solution in parametric vector form,
21498000	21506000	automatically you have a basis because they span and they're linearly independent.
21506000	21510000	Alright, one more example.
21510000	21521000	I've got three vectors here and we're given that V1 minus 3V2 plus 5V3 is equal to the zero vector.
21521000	21525000	And you're asked to find a basis for the span of these vectors.
21525000	21532000	So again, clearly you know that if you took all three, you have a set that spans.
21532000	21536000	But we know from what we're given that they're not linearly independent.
21536000	21542000	So we need to figure out which vectors to throw out until we get to a linearly independent set.
21542000	21548000	And since we can write any of the three as a linear combination of the other two,
21548000	21555000	simply solve this equation for any of the three vectors, then we could throw any of them out.
21555000	21565000	But it's kind of a convention to throw out the first one that's linearly dependent, which would be V3.
21565000	21568000	And so we're left with V1 and V2.
21568000	21573000	And if we just look at those, we can see that they're not multiples of each other.
21573000	21577000	And so therefore the set V1 and V2 is linearly independent.
21577000	21586000	It still spans and therefore it's a basis for W.
21586000	21595000	Okay, today we're going to talk about using a basis as a coordinate system for a vector space.
21595000	21599000	We start off with the unique representation theorem.
21599000	21607000	And it says, suppose you have a basis B, which consists of the vectors B1 through Bn.
21607000	21613000	And you choose some x from the vector space V.
21613000	21623000	And the theorem says that no matter which x you pick from the vector space, there are a unique set of scalars,
21623000	21632000	C1 through Cn that you can use to take a linear combination of the vectors in your basis to produce x.
21632000	21635000	Okay, now, so it's really saying two things.
21635000	21637000	It's saying that that system's consistent.
21637000	21643000	Take a linear combination of the B's and you can produce any vector in the vector space.
21643000	21648000	And it's also saying that the solution to that system is unique.
21648000	21655000	And both these things follow from the fact that B is a basis for V.
21655000	21662000	The fact that it's a basis tells you that these vectors span V.
21662000	21667000	So therefore that system is going to have a solution no matter what x is.
21667000	21671000	And it also tells you that these vectors are linearly independent.
21671000	21675000	And since they're linearly independent, the solution to that system will be unique.
21675000	21679000	Since there's no free variables.
21679000	21691000	Okay, now the weights that we use, these C values, are called the coordinates of x relative to the basis B.
21691000	21699000	Okay, and we write them using your book's notation as x with the brackets around it.
21699000	21704000	This notation here with the subscript B.
21704000	21713000	And we call that the coordinate vector of x relative to the basis B.
21713000	21719000	Alright, so for example, we have a basis for R2 here.
21719000	21722000	The vectors 1, 2, 3, 4.
21722000	21727000	Just a quick aside, how do we know that that's a basis for R2?
21727000	21732000	Well, the simplest way is to say, well, we know the dimension of R2 is 2.
21732000	21736000	And here we have a set with two vectors that are linearly independent.
21736000	21741000	And we know they're linearly independent because there's two vectors and neither's a multiple of the other.
21741000	21753000	Alright, so my question is, based on this particular basis, can you find the coordinates of the vector 0 or 1, 0 relative to this basis?
21753000	21758000	So I'm just setting up the system that we talked about previously.
21758000	21763000	I need to take a linear combination of the vectors and the basis to produce x.
21763000	21770000	And so we can set up that system in an augmented matrix and solve the system.
21770000	21777000	And this tells us that the numbers we're looking for are negative 2 and 1.
21777000	21782000	Just a quick check. We multiply negative 2 times the first vector plus 1 times the second vector.
21782000	21786000	That gives us 1, 0, which is what we were looking for.
21786000	21797000	So that tells us that the coordinate vector of x relative to B, which we write in this notation, is negative 2, 1.
21797000	21799000	Alright, suppose we go the other way.
21799000	21805000	Suppose you're given the coordinate vector and you want to find the corresponding vector.
21805000	21810000	So in this case, all you need to do is compute that linear combination.
21810000	21811000	You have the coordinates.
21811000	21816000	So in this case, it's 5 times the first vector plus 10 times the second.
21816000	21822000	And we compute that to be 3550.
21822000	21832000	Okay, this is a little bit of an aside, but kind of leads us into what we're going to talk about next.
21832000	21845000	If you have any vector, say just AB, then the vector itself is the coordinate vector relative to the standard basis.
21845000	21849000	So let's refresh our memory about what the standard basis is.
21849000	21856000	That's just for R2, just the two columns of the identity matrix.
21856000	21864000	Now your book refers to that as the labels that set with a script E.
21864000	21873000	Alright, so notice that if you've got a vector AB, then that's just A times 1, 0 plus B times 0, 1.
21873000	21876000	So your coordinates are A and B.
21876000	21881000	So therefore the coordinate vector is just equal to the vector itself, which is just AB.
21881000	21892000	So relative to the standard basis, the entries in a vector are actually the entries in the coordinate vector.
21892000	21896000	Okay, so just a little review.
21896000	21904000	If we have a basis and a coordinate vector, how do we find the corresponding vector?
21904000	21908000	Well, we just compute that linear combination.
21908000	21917000	Alright, so we know that this linear combination can be written in as a matrix times a vector.
21917000	21926000	And so if we define this matrix, we'll call it piece of B to be the matrix consisting of the vectors and the basis.
21926000	21933000	Then this linear combination can be written as piece of B times the coordinate vector.
21939000	21949000	And we say that piece of B is the change of coordinates matrix from B to the standard basis of our N.
21949000	21958000	Alright, so we're taking piece of B, multiplying it by the coordinate vector relative to B,
21958000	21966000	and we end up with the coordinate vector relative to the standard basis, which as we saw before is just the vector itself.
21967000	21973000	Alright, so let's talk about how we go back the other way.
21973000	21977000	For given a basis and a vector, how do we find its coordinate vector?
21977000	21987000	Well, we need to solve a system, right?
21987000	21991000	Take the linear combination, set it equal to the vector, and solve the system.
21991000	21994000	We need to figure out what those multipliers need to be.
21994000	22003000	So looking at it in matrix terms, we have this relationship, but we don't know what the coordinate vector is.
22003000	22006000	So therefore we need to solve the system for the coordinate vector.
22006000	22018000	And one way to do that is to multiply both sides by the inverse of the matrix, and that'll give us the coordinate vector.
22018000	22025000	Now, the danger there is, well, how do we know that matrix is invertible?
22025000	22034000	Well, we actually do know that it's invertible because we know that its columns are a basis for our N.
22034000	22045000	And so that's actually straight out of the invertible matrix theorem, but you can kind of get there in a couple of steps
22045000	22051000	if you think, well, columns are a basis for our N, so that means that the columns must be linearly independent,
22051000	22056000	which means there's a pivot position in every column, therefore N pivot positions.
22056000	22061000	So the matrix is invertible.
22061000	22066000	Okay, so we're going to put those two ideas together in this question.
22066000	22070000	So here I've got two bases for R2.
22070000	22075000	Well, bases, that's the plural of bases.
22075000	22080000	If you've seen that kind of confusing, but bases is just the plural of bases.
22080000	22085000	So we have two bases for R2 here, we call them B and C.
22085000	22092000	And I have a coordinate vector for a vector x relative to basis B.
22092000	22096000	Alright, so coordinate vector of x relative to B, which is this vector here.
22096000	22102000	And I want to know how do we find the coordinate vector of x relative to C?
22102000	22109000	Well, you can think of it probably easiest as a two-step process.
22109000	22117000	So first, use the coordinate vector relative to B to find actually what x is.
22117000	22122000	And once you know what x is, then you can find the coordinate vector of x relative to C.
22122000	22124000	Alright, so we'll do those two steps.
22124000	22133000	So the first one here, to find x, we just multiply our piece of B matrix,
22133000	22138000	the matrix consisting of the vectors in B times our coordinate vector,
22138000	22142000	and we find that x is 244.
22142000	22148000	And once we know that, then we just need to take a linear combination of the vectors in C
22148000	22153000	and set that equal to 244 and solve that system.
22153000	22162000	Okay, or look at it like this, where we solve the system by taking the inverse of that matrix times x.
22162000	22166000	So here's a piece of C.
22166000	22169000	These are just the columns that are in the basis C.
22169000	22173000	We invert that matrix, multiply by 244.
22173000	22181000	So what we have here, I'm taking one over the determinant times this matrix times 244,
22181000	22184000	and I end up with a vector of 4, 2.
22184000	22188000	So this is coordinate vector of x relative to C.
22188000	22195000	Now just to check, we can take that, these coordinates, times our vectors in C,
22195000	22200000	and see if we get x, which we do, 244.
22200000	22204000	So let's go back and take a look at what we did.
22204000	22210000	First step was to compute x, which we did by piece of B times x of B,
22210000	22218000	and then compute the coordinate vector relative to C by piece of C inverse times x.
22218000	22223000	Now we can't put that all together, just plug in for x,
22223000	22231000	and we have the coordinate vector of x relative to C is just PC inverse times piece of B times coordinate vector relative to B.
22231000	22238000	And so what we have is a change of coordinates matrix from B to C.
22238000	22244000	So this matrix takes us from coordinates in B to coordinates in C.
22244000	22255000	So PC inverse times piece of B is how we can change B coordinates to C coordinates.
22255000	22261000	Alright, we're going to switch gears just a little bit and talk about polynomials.
22261000	22264000	Let's examine this problem.
22264000	22269000	I want to show that these polynomials are a basis for piece of 3.
22269000	22278000	Okay, this is actually a homework problem from section 4.5, I think.
22278000	22282000	Okay, so to show that these are a basis for piece of 3,
22282000	22287000	we need to show that they span piece of 3 and that they're linearly independent.
22287000	22291000	So let's start off and show that they're linearly independent.
22291000	22298000	So to show any sets linearly independent, we have to take a linear combination and set it equal to the zero vector.
22298000	22302000	Or in this case, the zero element of piece of 3.
22302000	22304000	So here's a linear combination.
22304000	22309000	Alright, C sub 1 times the first element, C sub 2 times the second one, so forth.
22309000	22314000	Equals, here's the zero element of piece of 3.
22314000	22320000	Zero plus zero times t plus zero times t squared plus zero times t cubed.
22320000	22329000	Alright, and we want to be able to show that the only solution to this system is that all the C's have to be zero.
22329000	22337000	So the way to approach that is to collect like terms, collect all the constant terms,
22337000	22346000	collect all the coefficients of t and so forth, and then equate those coefficients to what we have on the right-hand side.
22346000	22352000	So we do that, we end up with this, so this comes from, we've got C1 here,
22352000	22358000	and then from this term we get a 2 times C3, so that's our constant term.
22358000	22369000	Coefficient of t here, we've got a 2 times C2 from here, and over here a negative 12 times C4, so that's where that comes from.
22369000	22377000	For t squared I've got 4 times C sub 3, and for t cubed I've got an 8 times C sub 4.
22377000	22382000	Alright, so those are equal to this zero vector again.
22382000	22391000	Now, if you write out that system of equations, right, we're going to get C sub 1 minus 2C3 equals zero.
22391000	22399000	2C2 minus 12C4 has to be zero, and 4C3 has to be zero, and 8C4 has to be zero.
22399000	22407000	So if you write that out, it looks like this, and when you look at it like that, it's obvious that all the C's have to be zero.
22407000	22410000	Alright, from the last equation you get C4 has to be zero.
22410000	22420000	From this one we get C sub 3 has to be zero, and since both C3 and C4 are zero, then that means that C1 and C2 have to be zero.
22420000	22427000	Alright, so we've shown here that the polynomials are linearly independent.
22427000	22431000	Let's just look at it in matrix form.
22431000	22438000	If we just look at the coefficients, it's already in echelon form, and we have a pivot position in every row.
22438000	22447000	So from that we know that these polynomials must span piece of three, so no matter what the right-hand side is, there'll be a solution.
22447000	22453000	Alright, so we have that they're linearly independent, and they span piece of three, so they have to be a basis for piece of three.
22453000	22466000	Now there's actually a somewhat easier way to look at this, and that is to look at the polynomials and what we ended up with in the columns of this matrix.
22466000	22481000	Now, actually the way it works is a polynomial of this form, here's a degree three polynomial, actually has a one-to-one correspondence with the vector in R4 that looks like this.
22481000	22488000	First comes the constant term, then the coefficient of t, coefficient of t squared, and coefficient of t cubed.
22488000	22497000	So for example, our polynomial, one of them was just one, so that corresponds to the vector 1, 0, 0, 0 in R4.
22497000	22503000	And if you look, that's what the first column of this matrix, our coefficient matrix was.
22503000	22508000	And then just pick another one, how about the last one, negative 12t plus 8t cubed.
22508000	22518000	Alright, then that's going to have, that's 0 for the constant term, negative 12 for the coefficient of t, 0 coefficient of t squared, and 8 for the coefficient of t cubed.
22518000	22523000	And that's what the last column in our coefficient matrix looks like.
22523000	22531000	So the bottom line is that every polynomial in piece of n can be represented as a vector in Rn plus 1.
22531000	22544000	So there's a one-to-one correspondence between polynomials in piece of n and vectors in Rn plus 1.
22544000	22549000	Alright, let's talk about the dimension of a vector space.
22549000	22554000	To do this, we need to go back and review what a basis is.
22554000	22566000	We call that a basis is a set of vectors that is linearly independent and spans the subspace that it's a basis for.
22566000	22570000	So let's think about R3.
22570000	22577000	If you have more than three vectors in R3, then they must be linearly dependent.
22577000	22583000	Put them in a matrix, put it in an echelon form, then you would have free variables.
22583000	22587000	If you have less than three vectors, then they don't span R3.
22587000	22594000	Alright, if you had only two, you wouldn't be able to have a pivot position in every column if you stuck them in a matrix.
22594000	22599000	So, or wouldn't have a pivot position in every row, excuse me.
22599000	22605000	So they would not be, they would not span R3.
22605000	22609000	So put those two together.
22609000	22615000	You need at least three to span.
22615000	22618000	If you have more than three, they're linearly dependent.
22618000	22625000	So that means that any basis for R3 has to have exactly three vectors.
22625000	22636000	Alright, our theorem says that if a vector space has a basis of n vectors, then every basis for that space must have exactly n vectors.
22636000	22648000	This number of vectors in a basis for a vector space is called the dimension of that vector space.
22648000	22657000	And we have a special case where the vector space has only the zero element.
22657000	22659000	Okay, we call that the zero vector space.
22659000	22669000	And since there's no basis for that set, then we define its dimension to be zero.
22669000	22677000	Okay, so to find the dimension of a vector space or subspace, then one approach is simply to find a basis for that space.
22677000	22683000	And count the number of vectors in that basis.
22683000	22689000	The standard basis for Rn consists of the vectors e1 through en.
22689000	22699000	So you might recall that e sub i is the vector in Rn that has a one in the i-th position and zeroes out everywhere else.
22699000	22706000	Or you can think of e sub i as just the i-th column of the identity matrix.
22706000	22716000	So the standard basis for R3 consists of these three vectors, which you'll recognize as the columns of the three by three identity matrix.
22716000	22721000	In general, the dimension of Rn is n.
22721000	22724000	Alright, let's talk about polynomials.
22724000	22730000	P sub n, if you recall, is the set of polynomials of degree n or less.
22730000	22739000	So for example, P sub 2 consists of all polynomials that are quadratic or less.
22739000	22748000	Okay, all polynomials that look like a naught plus a1 times t plus a2 times t squared, where the a values are just real numbers.
22748000	22757000	The standard basis for P sub 2 is this set, 1, t, and t squared.
22757000	22767000	So any vector in P sub 2, any polynomial in P sub 2 can be written as a linear combination of these three objects.
22767000	22770000	So the dimension of P sub 2 is 3.
22770000	22776000	And in general, the dimension of P sub n is n plus 1.
22776000	22784000	Okay, what if we have a set like this defined in terms of these parameters?
22784000	22789000	How do we find the dimension of such a set?
22789000	22793000	Well, first let's find a basis.
22793000	22799000	And to do that, we can write this generic vector in parametric vector form.
22799000	22805000	And so at this point, we know that these three vectors span our set S.
22806000	22811000	So we want to know, are they linearly independent?
22811000	22814000	And to check, see if they're linearly independent, we throw them in a matrix.
22814000	22816000	These are row operations.
22816000	22821000	Get that in echelon or reduced echelon form.
22821000	22824000	I got it in reduced echelon form.
22824000	22829000	And we can see here, there's not a pivot position in every column.
22829000	22833000	Therefore, they're not linearly independent.
22833000	22837000	We can see that there's a pivot position in the first two columns.
22837000	22841000	So that means that the first two vectors are linearly independent.
22841000	22850000	And notice that the negative one-third here means that the third column is minus one-third of the first column.
22850000	22853000	And if you look back, that is true.
22853000	22856000	Minus one-third of three is negative one.
22856000	22861000	Minus one-third of six is negative two and so forth.
22861000	22863000	All right.
22863000	22867000	So that means that we can throw out that third column.
22867000	22871000	And a basis for S would just consist of the first two vectors.
22871000	22877000	And once we have a basis, then we need to only count the vectors in the basis to get the dimension.
22877000	22879000	So there's two vectors in the basis.
22879000	22881000	That means the dimension of S is two.
22881000	22893000	And we don't say that S is R2, because it's not, because these vectors are in R4.
22893000	22899000	But what we do say is that S is a two-dimensional subspace of R4.
22899000	22903000	All right.
22903000	22905000	The basis theorem, this is an important theorem.
22905000	22914000	It says that if you have a vector space with dimension P, where P is greater than or equal to one, then the following conditions hold.
22914000	22922000	Number one, any linearly independent set of P elements in V is a basis for V.
22922000	22923000	Automatically.
22923000	22925000	Don't have to check to see if it spans.
22925000	22932000	Similarly, any set of P elements that spans V is automatically a basis.
22932000	22938000	Don't have to check to see if it's linearly independent.
22938000	22946000	Since we know the dimension of R3 is three, if you have a set of three vectors in R3 that are linearly independent, then you know they're a basis.
22946000	22950000	You don't have to check to see if they span, because they will.
22950000	22956000	Similarly, if you have a set of three vectors that span R3, then you know they're a basis for R3.
22956000	22961000	You don't have to check to see if they're linearly independent, because they will be.
22961000	22967000	So we basically end up with three pieces of the puzzle.
22967000	22969000	Let's go back to that.
22969000	22979000	Three pieces of the puzzle, knowing the dimension, having a set of that many vectors that spans, or having a set of that many vectors that's linearly independent.
22979000	22990000	If you have any two of those three pieces, then you can conclude that you have a basis for the vector space you're dealing with.
22990000	22993000	Okay.
22993000	22998000	For giving a matrix A, how do we find the dimension of the column space of A?
22998000	23005000	Well, we go back to our method, find a basis, and count the number of vectors in the basis.
23005000	23011000	Now let's think back, how do we find a basis for the column space of a matrix?
23011000	23016000	We will put A in echelon form so that we can find the pivot columns.
23016000	23024000	We pull those columns from A, not from the echelon form, but from the original matrix A, and that's our basis.
23024000	23029000	So here's one I think we looked at this one last time.
23029000	23033000	We take the matrix A, put it in echelon form.
23033000	23043000	We can see that the first, third, and fifth columns are pivot columns, so we pull the first, third, and fifth columns from A, and that's a basis for the column space of A.
23043000	23053000	And once you have a basis, then getting the dimensions trivial, three vectors in the basis, so the dimension of the column space of A is three.
23053000	23063000	In general, the dimension of the column space of a matrix is simply equal to the number of pivot columns in the matrix.
23063000	23064000	Alright, how about the null space?
23064000	23070000	We have a matrix A, how do we find the dimension of the null space of A?
23070000	23074000	Well, once again, find a basis and then count the number of vectors.
23074000	23081000	So let's think back, how do we find a basis for the null space of a matrix?
23081000	23087000	Well, we have to solve AX equals zero and write the solution in parametric vector form.
23087000	23095000	And the vectors that we get there both span the null space of A and they're linearly independent.
23095000	23106000	We made that argument last time that when you write those in parametric vector form, they will be linearly independent and therefore there'll be a basis for the null space of A.
23106000	23119000	So if we start with this matrix A, we solve AX equals zero and write the solution in parametric vector form.
23119000	23138000	So these two vectors are linearly independent, they span the null space, so there a basis for the null space.
23138000	23144000	Therefore the dimension of the null space for this particular matrix is two.
23144000	23152000	In general, the dimension of the null space is equal to the number of free variables in AX equals zero.
23152000	23158000	Because you have a vector here corresponding to each free variable.
23158000	23161000	And where do free variables come from?
23161000	23164000	They come from columns that don't have a pivot position.
23164000	23170000	So the dimension of the null space is equal to the number of non-pivot columns in A.
23170000	23175000	And recall the dimension of the column space is equal to the number of pivot columns.
23175000	23179000	Null space dimension is equal to the number of non-pivot columns.
23179000	23187000	So if we add those two quantities together, the dimension of the null space plus the dimension of the column space,
23187000	23193000	we get the number of pivot columns plus the number of non-pivot columns, which is equal to the number of columns.
23193000	23198000	And we'll hit on that some more next time.
23198000	23204380	Let's move on to talk about the rank of a matrix today.
23204380	23206820	Now first, a little bit of review.
23206820	23210480	Remember, the column space of a matrix
23210480	23214340	is the set of all linear combinations of the columns,
23214340	23220700	which is also the span of the set of columns of the matrix.
23220700	23225180	So we're going to talk about another subspace.
23225180	23227780	This time, the row space of a matrix.
23227780	23229740	So we know what the column space is,
23229740	23233660	so what do you think the row space of a matrix is?
23236860	23242180	Well, if you just take the analogous route,
23242180	23246060	then we get the row space of a matrix
23246060	23249020	is a set of all linear combinations of the rows
23249020	23252780	or the span of the set of rows of the matrix.
23252780	23258660	So it is truly just the analogous term to the column space.
23258660	23261820	We have the column space, all linear combinations of the columns,
23261820	23266220	row space, all linear combinations of the rows.
23266220	23270020	OK, so think back to find a basis for the column space
23270020	23271380	of a matrix.
23271380	23273260	We put the matrix initial on form
23273260	23277020	so that we can determine which columns are the pivot columns.
23277020	23278780	Then we go back to the original matrix
23278780	23280660	and pull out those columns, and that's
23280660	23283340	the basis for the column space of the original matrix.
23286020	23288460	So the question is, how do we find a basis for the row
23288460	23290020	space of a matrix?
23290020	23293180	And here, the similarities end somewhat,
23293180	23296620	although there still are some similarities.
23296620	23300180	But this theorem tells us how we can find a basis for the row
23300180	23301580	space of a matrix.
23301580	23305260	OK, so it says, if two matrices A and B are row equivalent,
23305260	23307840	so you can get from one to the other doing elementary row
23307840	23312280	operations, then their row spaces are the same.
23312280	23316480	Now you'll recall that that is not true for the column spaces.
23316480	23318680	When you do elementary row operations,
23318680	23323520	you may be changing the column space of the matrix.
23323520	23326400	But this theorem tells us that for row space,
23326400	23330200	that stays the same when you do row operations.
23330200	23335600	OK, if B is an echelon form, the non-zero rows of B
23335600	23340040	form a basis for the row space of A as well as for that of B.
23340040	23344200	So this tells us to find a basis for the row space of A.
23344200	23349040	We put it in echelon form and pull the non-zero rows
23349040	23351480	from that echelon form matrix.
23351480	23355840	And we have a basis for the row space of the original matrix
23355840	23361280	plus the echelon form matrix and any intermediate matrices
23361280	23362280	that we encountered.
23362280	23372200	OK, so again, elementary row operations do not change of this.
23372200	23373360	Back up here.
23373360	23375320	This is telling me that the linear dependence
23375320	23377280	relationships among the columns, OK?
23377280	23379480	So that means like if the second column of A
23379480	23382600	is 10 times the first column, then when
23382600	23385840	you do elementary row operations, that doesn't change.
23385840	23387680	So elementary row operations do not
23387680	23390040	change the linear dependence relationships
23390080	23392880	among the columns of a matrix.
23392880	23396360	However, they can change the linear dependence relationships
23396360	23399560	among the rows of a matrix.
23399560	23402480	OK, so let's look at a little simple example.
23402480	23406600	We start off with this matrix A. We do one row operation
23406600	23410640	and end up with this one, which I'll call B. Now, a basis
23410640	23416240	for the column space of A, we could just look at A
23416240	23418320	and say, well, I can see that the second column is
23418320	23419480	a multiple of the first.
23419480	23422280	So I throw it out and just keep the first column.
23422280	23426360	Or you could look at B and see where the pivot positions in B.
23426360	23428480	Well, there's only one in the first column.
23428480	23432520	So that means we want to pull the first column from A
23432520	23438120	to be a basis for the column space of A. If we want a basis
23438120	23441160	for the column space of B, that's straightforward
23441160	23443080	because B is already an echelon form.
23443080	23446160	So we know what the pivot columns are.
23446160	23448160	There's only one first column.
23448160	23454200	So that's a basis for the column space of B. So if we look
23454200	23459240	graphically at the column space of each of these matrices,
23459240	23463320	the red line here, you can see this vector right here.
23463320	23464680	This is 1, 1.
23464680	23469280	And so this red line is any multiple of the vector 1, 1.
23469280	23474720	So the red line is the column space of A. This vector here
23474720	23476600	is 1, 0.
23476600	23479720	And since that's a basis for the column space of B,
23479720	23484200	the blue line, or the x-axis, is the column space of B.
23484200	23486760	All multiples of 1, 0.
23486760	23491080	So clearly here, these column spaces are not the same.
23493880	23495480	These matrices are row equivalent,
23495480	23497600	but their column spaces are not the same.
23498600	23504600	All right, let's look again, same set of matrices.
23504600	23507960	Now, the row spaces are the same because we want a basis
23507960	23513200	for the row space of A. Then, right, we look at each row of A.
23513200	23515400	Well, they're the same row, right?
23515400	23518520	Each row is the same, and so we only need one of them.
23518520	23521040	Throw out the second one, and we keep the first row,
23521040	23525000	and that's a basis for the row space of A.
23525080	23528720	If we were using theorem, we would get A in echelon form,
23528720	23532120	which we have here, and take the non-zero rows of that matrix.
23532120	23536080	Well, the non-zero rows is just that row 1, 2.
23536080	23539120	So either way you look at it, you end up with this basis
23539120	23545120	for the row space of A. And similarly for the row space of B,
23545120	23548280	it's going to be all multiples of the row 1, 2,
23548280	23552080	because 0, 0 didn't add anything to the picture.
23552080	23556280	So this set, which consists of just that one row,
23556280	23562000	is a basis for both the row space of A and the row space of B.
23562000	23567480	Now, let's look at a little more interesting example.
23567480	23571480	There's a big matrix A, and after some row operations,
23571480	23576080	we end up with this version of A. This is an echelon form,
23576080	23579880	and we'll call that matrix B. So to get a basis
23579880	23583000	for the column space of A, we look at B,
23583000	23586120	we can see that there's a pivot position in the first column,
23586120	23589600	the second column, and the fourth column.
23589600	23594040	So we choose those columns out of A, first column,
23594040	23596240	second column, and the fourth column,
23596240	23599480	and that's a basis for the column space of A.
23599480	23602240	So the dimension of the column space of A
23602240	23608320	is the number of vectors in this basis, which is 3.
23608320	23612280	To get a basis for the column space of B, we look at B.
23612280	23618000	It's already an echelon form, so we choose the pivot columns,
23618000	23621480	and so we get the first, second, and fourth columns.
23621480	23624480	That's the basis for the column space of B.
23624480	23627080	And the dimension of the column space of B
23627080	23628760	counts the vectors in a basis.
23628760	23630120	That's 3.
23630120	23635680	So the dimension of the column space of B is 3.
23635680	23638720	All right, what about a basis for the row space of A,
23638720	23640040	in which we know from the theorem
23640040	23643000	will also be a basis for the row space of B?
23643000	23645520	Well, according to the theorem, put A in echelon form
23645520	23647840	and choose the non-zero rows.
23647840	23651640	So we do that, we get the first, second, and third rows of B,
23651640	23654840	which are given here.
23654840	23657760	So the dimension of the row space of A
23657760	23659880	is equal to the dimension of the row space of B,
23659880	23663000	and notice there's three vectors here, so that is 3.
23666680	23669680	All right, so we got the dimension of the column space of A
23669680	23673680	is 3, dimension of the row space of A is 3,
23673680	23676680	and the question is, is this a coincidence
23676680	23679680	that they're both the same value?
23679680	23683680	And if you think about that just a little bit,
23683680	23686680	you can say, no, I don't think so,
23686680	23689680	because the dimension of the column space of A
23689680	23693680	is the number of pivot columns or pivot positions in A,
23693680	23696680	and that's when we look at B to figure that out,
23696680	23699680	but there's three pivot positions,
23699680	23702680	and the dimension of the row space of A
23702680	23704680	is also equal to the number of pivot positions,
23704680	23708680	because there's a pivot position in each non-zero row
23708680	23712680	in B here, or in an echelon form of A.
23712680	23716680	So each of these values is based off
23716680	23718680	of the number of pivot positions, right,
23718680	23721680	because a pivot position defines a pivot column,
23721680	23725680	and a pivot position defines a non-zero row.
23725680	23729680	So for any matrix, the dimension of the column space
23729680	23731680	is equal to the dimension of the row space,
23731680	23737680	which is equal to the number of pivot positions in that matrix.
23737680	23743680	Okay, and this quantity is what we call the rank of a matrix.
23743680	23747680	The rank of a matrix is the dimension of the column space,
23747680	23750680	which is also equal to the dimension of the row space
23750680	23752680	of that matrix.
23752680	23757680	Okay, as we just called that, the rank of the matrix.
23757680	23759680	All right, back to this one.
23759680	23761680	Let's talk about the null space.
23761680	23764680	What is the dimension of the null space of A?
23764680	23770680	Well, we end up with a...
23770680	23774680	We want to find a basis for the null space, right?
23774680	23777680	We solve AX equals zero
23777680	23780680	and write our solution in parametric vector form,
23780680	23784680	and those vectors will be a basis for the null space of A.
23784680	23788680	Now, how many vectors do you end up with in that case?
23788680	23791680	Well, you end up with one for each free variable.
23791680	23795680	Okay, the key is one for each free variable.
23795680	23798680	And if you think about that a little more,
23798680	23802680	you think, hmm, well, where do I get a free variable?
23802680	23805680	Well, a free variable is one whose column
23805680	23807680	does not contain a pivot position.
23807680	23809680	So the dimension of the null space
23809680	23813680	is a number of free variables in AX equals zero,
23813680	23819680	also equal to the number of non-pivot columns in the matrix.
23819680	23822680	Okay, for this particular matrix,
23822680	23824680	dimension of the null space is three,
23824680	23826680	because here's a non-pivot column,
23826680	23829680	the third one, and the fourth and...
23829680	23830680	Oh, fifth and sixth.
23830680	23833680	So the third column, fifth column, sixth column,
23833680	23835680	is called non-pivot columns,
23835680	23838680	so we have three non-pivot columns,
23838680	23843680	and therefore the dimension of the null space is three.
23843680	23847680	All right, so once again, this number three pops up.
23847680	23849680	We have dimension of the column space is three,
23849680	23851680	dimension of the null space is three.
23851680	23855680	So I ask again, is this a coincidence?
23855680	23861680	Hmm, and the answer is...
23861680	23863680	It is a coincidence.
23863680	23867680	It's a coincidence because dimension of the column space
23867680	23869680	is the number of pivot columns,
23869680	23873680	dimension of the null space is the number of non-pivot columns.
23873680	23876680	So when you add them together, you get what?
23876680	23878680	The number of columns in the matrix.
23878680	23881680	So it just so happened that there were six columns
23881680	23884680	in this matrix, and so if the dimension of the column space
23884680	23887680	is three, then the dimension of the null space
23887680	23890680	is going to be six minus three, which is also three.
23890680	23892680	That's totally coincidence.
23892680	23896680	Had there been seven columns, the dimension of the null space
23896680	23898680	would have been four.
23898680	23901680	All right, so we have the dimension of the column space
23901680	23903680	plus the dimension of the null space is equal
23903680	23905680	to the number of columns,
23905680	23907680	and since the dimension of the column space
23907680	23909680	is equal to the dimension of the row space,
23909680	23911680	we have the dimension of the row space
23911680	23913680	plus the dimension of the null space
23913680	23916680	is equal to the number of columns,
23916680	23924380	one more time since the dimension of the column space and the dimension of the row space are equal to the rank of a.
23924380	23932460	We have the rank plus the dimension of the null space is equal to the number of columns.
23932460	23939260	Okay, so in this section we have another installment of the invertible matrix theorem.
23939260	23950160	And if you recall, we had this version of it or this installment of it back in section 2.3.
23950160	23955260	And so this should be seared into your memory at this point.
23955260	23958360	If not, you go back and review it.
23958360	23967260	But one of the pertinent things is recall that the whole idea here is that all these statements are equivalent,
23967260	23969960	which means that they're either all true or all false.
23969960	23973560	Number one here says A is an invertible matrix.
23973560	23980160	Now, one of the most important pieces of this is number three, that A has n pivot positions.
23980160	23986360	Because if you recall, when I was telling you about how to remember all this,
23986360	23991260	I told you the easiest way is to relate everything to pivot positions.
23991260	23999160	So if we can establish for any part of this that A has n pivot positions, then we're done.
23999160	24006660	All right, so what we have new here, we have six new things, four of them are given here.
24006660	24008660	So let's take a look at these.
24008660	24013760	Number one says the columns of A form a basis for our end.
24013760	24023960	Okay, if that's true, their basis for our end, then that means that they must be linearly independent and they must span our end.
24023960	24029760	Either one of those says that there's a pivot position in every row and every column.
24029760	24032260	Therefore, their end pivot positions.
24032260	24035060	So A is invertible.
24035060	24039160	The column space of A equals our end.
24039160	24047760	That means that the columns of A span our end, which again, they can only do that if there's a pivot position in every row,
24047760	24050760	meaning there's n pivot positions.
24050760	24053960	The dimension of the column space is n.
24053960	24057660	That actually falls from number two here.
24057660	24063460	If column space of A equals our end, we know that there are n vectors and a basis for our end.
24063460	24067060	So therefore, the dimension of the column space is n.
24067060	24068460	And the rank of A equals n.
24068460	24074760	That falls from number three since the dimension of the column space is equal to the rank of a matrix.
24074760	24078560	Okay, then we have two more that deal with the null space.
24078560	24082760	Null space of A is just the zero vector.
24082760	24091460	Okay, so if that's true, then that means that if we look at the system AX equals zero, it has only the trivial solution.
24091460	24101460	That happens when there are no free variables, which means there's a pivot position in every column, which means they're in pivot positions.
24101460	24111060	And from number five follows number six because if you have only the zero vector, then that's a special case.
24111060	24117560	And we define the dimension of that vector space to be zero.
24117560	24122760	All right, so all these are equivalent to the statement that A is an invertible matrix.
24122760	24129760	So just like with those, the first ten, I believe it was, let's see, yep, first ten.
24129760	24140960	These extra six, you need to commit them to memory relating them to each other or to pivot positions.
24140960	24148360	All right, I want to go through some of the some of the problems at the end of this section because there's some excellent problems here.
24148360	24154260	In fact, this section I think is the most important section in the course.
24154260	24160960	So make sure that you really work on these problems and understand what you're doing here.
24160960	24164760	So I'm going to do several of them at the end of the section.
24164760	24172860	Okay, so the first one says suppose a five by six matrix, let's call it A, has four pivot columns.
24172860	24177160	Okay, so it's a five by six matrix with four pivot columns.
24177160	24184260	Okay, if it has four pivot columns, then that means it has two non-pivot columns.
24184260	24187060	So what's the dimension of the null space of A?
24187060	24188660	Must be two, right?
24188660	24191860	Because you got two non-pivot columns.
24191860	24194960	Is the column space of A equal to R4?
24194960	24203660	Well, the dimension of the column space is four because we have four pivot columns.
24203660	24217260	Okay, so the dimension of the column space is four, but the columns are in R5 and therefore they are the span of the columns is not equal to R4.
24217260	24221560	It's just a subset of R5.
24221560	24222960	All right, next one.
24222960	24228160	If the null space of a seven by six matrix is five dimensional.
24228160	24242660	Okay, so that means you have five non-pivot columns, which means out of six columns, one is a pivot column, then therefore the dimension of the column space has to be one.
24242660	24250260	All right, suppose the null space of a five by six matrix is four dimensional.
24250260	24260360	Then that means you have four non-pivot columns, so that leaves two pivot columns and we want to know the dimension of the row space.
24260360	24266760	We have two pivot columns, so that means we have two non-zero rows when we put the matrix in echelon form.
24266760	24270860	So the dimension of the row space is two.
24270860	24273460	All right, and what if A is four by three?
24273460	24278760	What is the largest possible dimension of the row space?
24278760	24292160	All right, well, if A is four by three, then we could have at most three pivot positions, so at most three non-zero rows when we put it in echelon form.
24292160	24300760	So the maximum dimension of the row space is three.
24300760	24307560	All right, if A is three by four, what's the largest possible dimension of the row space?
24307560	24314360	Three by four, then again, the maximum number of pivot positions we can have is three.
24314360	24322860	So we have at most three non-zero rows, so the maximum dimension of the row space is three.
24322860	24324960	How about if A is six by four?
24324960	24328760	What is the smallest possible dimension of the null space?
24328760	24338360	That's asking what's the smallest number of free variables you can have, or what's the smallest number of non-pivot columns you could have?
24338360	24352160	Well, since we have more rows than columns, every column could be a pivot column, in which case there's no free variables, and so the minimum dimension of the null space of A would be zero.
24352160	24357460	All right, a little more complicated one here.
24357460	24367860	Suppose a non-homogeneous system of six equations and eight unknowns has a solution with two free variables.
24367860	24376160	Is it possible to change some constants on the equation's right sides to make the new system inconsistent?
24376160	24377560	All right, we got it.
24377560	24384460	A non-homogeneous system, six equations and eight unknowns, and it's consistent with two free variables.
24384460	24394060	And what that tells you with the two free variables is that you have, since there's eight unknowns, we got eight columns.
24394060	24400060	Two are free, so that means there are six that are pivot columns.
24400060	24401360	Okay, so that would look like this.
24401360	24409060	We've got six rows here, eight columns, two free variables, which leaves six pivot columns.
24409160	24417660	All right, and it's saying if we change the right-hand side, will the system still be consistent?
24417660	24424360	And the answer is yes.
24424360	24428160	Plug my computer in.
24428160	24432360	Is it possible to change some constants to make the new system inconsistent?
24432560	24443360	The answer is no to that, because no matter what's over here, since you have a pivot position in each row, the system's always going to be consistent.
24443360	24446660	No, I was thinking the question was, is the system always consistent?
24446660	24450060	And the answer to that is yes.
24450060	24453760	And that is because you have a pivot position in every row.
24453760	24460360	So you're never going to end up with a row of all zeros and then something not zero over here.
24460360	24461660	All right, another one.
24461660	24474160	Is it possible that all solutions of a homogeneous system of two equations and four unknowns are multiples of one fixed nonzero solution?
24474160	24479660	Okay, homogeneous system, two equations and four unknowns.
24479660	24486760	So if you think about that, we've got two rows and four columns, so we have at least two free variables.
24486760	24493860	So if we wrote our solution in parametric vector form, so we had two free variables, then it would look like this.
24493860	24503160	You know, what this says is that each solution is a linear combination of two fixed nonzero solutions, right?
24503160	24509060	Each one of these vectors is what they're calling a fixed nonzero solution.
24509060	24512360	So we're going to have at least two of these vectors.
24512360	24523160	So that means that it can't be the case that all the solutions are just multiples of one nonzero solution.
24523160	24532860	If that was the case, we would only have one vector here and we can't have that because we have at least two free variables.
24532860	24535760	All right, another one.
24535760	24546660	Is it possible for a non-homogeneous system of three equations and two unknowns to have a unique solution for some right-hand side of constants?
24546660	24550660	So three equations, two variables.
24550660	24555160	All right, so that might look like this, three equations, two variables.
24555160	24562660	So you could have a pivot position in each column and you could have a row of all zeros.
24562660	24571960	In that case, yes, the system would be consistent and there would be a unique solution.
24571960	24573560	All right, how about this one?
24573560	24579660	Is it possible for such a system to have a unique solution for every right-hand side?
24579660	24587860	And, okay, look at that, you could have this situation still have pivot position in each column,
24587860	24594660	but have zero, zero, something not zero in which case there's no solution, right?
24594660	24602460	So in this case, it's not possible for such a system to have any sort of solution for every right-hand side, right?
24602460	24607060	In some cases, it's simply going to be inconsistent.
24607060	24612560	All right, that's it for this one.
24612560	24617260	Okay, our topic for today is Markov chains.
24617260	24625260	And to get us going on that, look at a small example.
24625260	24636160	So let's suppose in a given urban area, each year, 5% of the population that's in the city moves to the suburbs
24636160	24640860	and 3% of the suburban population moves to the city.
24640860	24646160	And we're going to assume that this is a closed system here.
24646160	24652560	So any movement just goes from the city to the suburbs or suburbs to the city.
24652560	24658260	So we're not considering moving elsewhere or people moving in from elsewhere.
24658260	24663460	So just city to suburban, suburban to city.
24663460	24667860	Okay, so let's assume that the current city population is 600,000
24667860	24671560	and the current suburban population is 400,000.
24671560	24677460	Then what will the population of each be a year from now?
24677460	24687160	Well, based on what we know next year, the city population is going to be 95% of what it is now, right?
24687160	24692060	Because we each year 5% leave, so we're keeping 95%.
24692060	24701760	And then 3% of the suburban population moves to the city, so we add on 0.03 times the current suburban population.
24701760	24709660	So that's 0.95 times 600,000 plus 0.03 times 400,000, which gives us 582,000.
24709660	24713460	So it makes sense that the city population is going down.
24713460	24718860	We've got more moving to the suburbs than we have coming in.
24719060	24726260	To find the suburban population in a year, we could just say, well, since the total is a million,
24726260	24732760	then we can just say a million minus 582,000 and get it that way, which is correct.
24732760	24736560	But we could also look at it the way we did with the city and say, well,
24736560	24746360	we're getting 5% of the city population moving to the suburbs and 97% of the current suburban population is staying there.
24746360	24762060	So we get 0.05 times 600,000 plus 0.97 times 400,000, which gives us 418,000, and 418 plus 582 gives a million total.
24762060	24775160	Another way to look at it is let's take a closer look at those equations and notice that we've got 600,000 times the 0.95, 0.05 here.
24775160	24779360	And we've got the 400,000 times the 0.03 and the 0.97.
24779360	24788160	And that looks like we're taking a linear combination of a couple of vectors, the 0.95, the 0.05 and the 0.03, 0.97.
24788160	24793160	So we're going to rewrite it in this form as a matrix times a vector.
24793160	24799660	And so the linear combination is just 600,000 times the first column plus 400,000 times the second column.
24799660	24804560	And that gives us our population figures for one year from now.
24804560	24813560	You can also look at it as percentages, which is typical instead of actual raw numbers.
24813560	24827060	So if we look at the population vectors 0.6, 0.4, we do the same sort of computation and end up with this population vector for a year from now.
24827060	24835760	And if you notice that each one of these vectors, the columns of the matrix plus our two vectors here, each one of them sums to one.
24835760	24844560	So they're special vectors because they represent probabilities or percentages.
24844560	24847560	And we'll talk about that a little more in just a minute.
24847560	24856960	But before we get to that, notice that what we have here, this is of the form, some matrix, which I'm calling m, times a vector I'll call x naught,
24856960	24863060	which is like the original state of the system, original population.
24863060	24869460	We multiply those together to get x1, which is the state in year one.
24869460	24879060	We can continue that process to figure out what the population in year two would be, year three and so forth, year k plus one.
24879060	24885060	For any year, it's just m times the population from the previous year.
24885160	24895960	So for instance, if I wanted to figure out 10 years from now, what's the population, then I could compute x2, which is, well, to compute x10, I need x9,
24895960	24898360	because x10 is m times x9.
24898360	24902760	To get x9, I need x8, to get x8, I need x7 and so forth.
24902760	24908360	So I'd have to compute all those intermediate population vectors.
24908360	24914760	There is another way to do it, which doesn't require that computation.
24914760	24918960	And that is to look at it as I have here at the bottom.
24918960	24925760	If we look at x2, for example, that's m times x1, but we know that x1 is m times x0.
24925760	24929960	So x2 turns out to be m squared times x0.
24929960	24935260	Similarly, x3 is m times x2, but we know x2 is m squared x0.
24935260	24938660	So x3 is just m cubed times x0.
24938660	24947260	And in general, we have xk, your k population would be m to the k times the original population vector.
24950060	24957660	Okay, so a vector with non-negative entries that add up to 1 is called a probability vector.
24957660	24964860	So back to these, all these are probability vectors because they all add up to 1.
24964860	24967060	Each column adds up to 1.
24968860	24976060	Okay, a stochastic matrix is a square matrix whose columns are probability vectors.
24976060	24983960	So the matrix that we had here, that is a stochastic matrix because each of its columns is a probability vector.
24983960	24990060	A vector where their entries are non-negative and they sum to 1.
24990060	24995960	A Markov chain is a sequence of probability vectors, x0, x1, x2, and so forth,
24995960	25003360	together with a stochastic matrix m, such that x sub k plus 1 is just m times x sub k,
25003360	25007360	which is the pattern that we had in our example.
25009860	25017560	The i-th entry in xk is the probability that the system is in state i at time k.
25017660	25025960	Okay, and we call xk a state vector because it represents the state of the system at time k.
25028060	25033360	Okay, now an interesting question related to our population example is,
25033360	25037260	will there ever be a point at which there's no net population change?
25037260	25043160	That is, the number of people moving to the suburbs is equal to the number of people moving to the city.
25044160	25050460	Okay, another way to look at it is, will there ever be a point at which xk plus 1 is equal to xk?
25050460	25056960	Alright, so you compute xk plus 1, and it's the same as xk.
25058960	25064360	Or will there ever be a point at which you have your current state as some vector x?
25064360	25070560	You apply the transition matrix m to it, and you end up back with the same vector x.
25071560	25074960	Okay, and so this is the system we want to look at.
25074960	25079960	It's similar to systems that we know how to solve, but it's a little bit different,
25079960	25083360	because on the right-hand side here, we've got a variable.
25083360	25088760	It's not a constant, so it's not like an ax equals b system.
25089760	25099560	So we need to manipulate it a bit before we can use the things that we know about solving systems.
25101560	25104560	Okay, so we want to know, is it consistent?
25104560	25112560	And the way we determine that is, we bring the x over to the left-hand side.
25112560	25115560	We get mx minus x is a zero vector.
25115560	25119560	And then the key is to factor out that x.
25119560	25124560	And if we factor out the x, then we're left with m minus the identity matrix.
25124560	25126560	So i here is the identity matrix.
25126560	25132560	So what we end up with is some matrix, which is m minus the identity matrix, which we can compute directly.
25132560	25134560	We know both of those.
25134560	25137560	Times x, which we don't know, equals a zero vector.
25137560	25142560	So when we write it in this form here, it's a simple homogeneous system.
25142560	25150560	Now clearly, it's consistent because x can be the zero vector, and that obviously makes that consistent.
25150560	25153560	However, that's not a very interesting case.
25153560	25160560	What we really want to know is, does this system have an infinite number of solutions?
25160560	25162560	Are there any free variables?
25164560	25170560	Alright, well if we look at this matrix for our example,
25170560	25174560	so we subtract off the identity, and we're left with this one,
25174560	25178560	and it's clear when you look at that that the columns are not linearly independent,
25178560	25181560	and therefore you have a free variable.
25181560	25183560	So we solve this system.
25183560	25185560	It's pretty straightforward.
25185560	25190560	One row operation, and we zeroed out the bottom row.
25190560	25198560	And so first row tells us that 5 multiplied by 100 makes life simple.
25198560	25201560	So we get 5x1 equals 3x2.
25201560	25207560	So that means x1 is 3 fifths times x2, where x2 is a free variable.
25207560	25223560	And to get the actual vector, the probability vector that corresponds to this relationship between x1 and x2,
25223560	25227560	we want x1 plus x2 to equal a million.
25227560	25232560	Or we could say 1, I'm doing it just for raw population numbers.
25232560	25237560	And x1 is 3 fifths x2, so we get this equation.
25237560	25244560	Putting these two together, 8 fifths x2 is a million, which says x2 is 625,000,
25244560	25249560	and therefore x1 is 375,000.
25249560	25259560	Alright, so we've reached the point here that the number moving out of the city
25259560	25267560	is 0.05 times 375,000, because every year 5% leave the city.
25267560	25274560	So the population of the city at this point is 375,000, so 5% of that's 18,750.
25274560	25281560	Likewise, the total moving out of the suburbs, which is 3% of the suburb population,
25281560	25287560	which at this point is 625,000, is also equal to 18,750.
25287560	25293560	So we've reached the point here where the number moving out of the city is equal to the number moving in.
25293560	25304560	So there's no net population change once the system gets to this point.
25304560	25308560	Alright, a little more terminology for you.
25308560	25315560	If P is a stochastic matrix, then a steady state vector, also called equilibrium vector,
25315560	25321560	for P is a probability vector Q such that P times Q is equal to Q.
25321560	25326560	Before we said M times X equals X, talking about the same thing here.
25326560	25337560	Alright, we have this theorem that says if P is a stochastic matrix, then it has a unique steady state vector.
25337560	25345560	Even though the system has an infinite number of solutions, if we require the elements sum to 1,
25345560	25349560	then that makes it a unique vector.
25349560	25359560	Furthermore, if X naught is any initial state and we have X sub K plus 1 equals P times X sub K,
25359560	25365560	then the Markov chain converges to Q as K goes to infinity.
25365560	25372560	So what's interesting about this is that the initial state is unimportant.
25372560	25387560	It says if X naught is any initial state, then the Markov chain will converge to this steady state vector as if you go to a big enough K.
25387560	25390560	Okay, so let's look at another example.
25390560	25397560	Here's a matrix, so let's get a 3 by 3 and find a steady state vector for this matrix.
25397560	25402560	So we need to compute P minus the identity, so there's that.
25402560	25406560	And then we need to solve P minus I times X equals 0.
25406560	25411560	So do a few row operations, end up with this matrix.
25411560	25419560	And so the solution is X1 is X3, X2 is a half of X3, where X3 is free.
25419560	25425560	You can see X1 equals X3, X2 is a half of X3, and X3 is free.
25425560	25435560	And again, the elements need to sum to 1, so we get, since X1 is X3, X2 is a half of X3,
25435560	25438560	that's where this second equation comes from.
25438560	25446560	And it turns out that X3 has to be 2 fifths, so that means X1 is 2 fifths and X2 is 1 fifth.
25446560	25450560	So the steady state vector is this one here.
25450560	25457560	And if you check out what P times X is, multiply P times the steady state vector,
25457560	25465560	turns out you get that steady state vector back, so it checks out.
25465560	25471560	Alright, I want to show you one more application problem, and this one I've taken from another book,
25471560	25475560	by Andrillian Hecker, it's kind of interesting when I thought.
25475560	25477560	And it's based on banks.
25477560	25483560	I suppose you've got three banks in a certain town that compete for business.
25483560	25489560	And Bank A right now has 40% of the customers, B has 10%, and C has 50%.
25489560	25500560	So we write that vector as, given here, 0.4, corresponds to Bank A, 0.1 for Bank B, and C with a 0.5.
25501560	25508560	Okay, so the banks are obviously trying to woo customers from the other banks.
25508560	25516560	And what we have here is information about how successful they are at that.
25516560	25522560	So we see that records show that each year Bank A keeps half of its investors with the remainder,
25522560	25524560	switching equally to B and C.
25524560	25527560	So if you look at the first column of this matrix,
25527560	25533560	that is, you can think of this column as being related to Bank A.
25533560	25541560	In each row, the first row corresponds to Bank A, the second row to B, and the third row to C.
25541560	25547560	So I've got an A column, a B column, a C column, and an A row, a B row, and a C row.
25547560	25555560	So the 0.5 here represents the probability that a person at Bank A is going to stay at Bank A.
25555560	25563560	Then the 0.25, here, represents the probability that a person in Bank A is going to switch to Bank B.
25563560	25571560	And the 0.25 at the bottom represents the probability that a person currently at Bank A is going to switch to Bank C.
25571560	25577640	All right and says bank B keeps two-thirds of its investors with the remainder switching
25577640	25579320	equally to the other two.
25579320	25585720	So the point 667, that's the probability that someone who starts at bank B or who's currently
25585720	25590880	at bank B is going to stay there for the next year.
25590880	25596400	And then the point 167 here, that's probability someone who currently is at bank B switches
25596400	25600480	to bank A. Okay, and same thing down here.
25600480	25606720	Probably at bank, a probability that a bank B customer will switch to bank C. Then we
25606720	25611440	say bank C keeps half of its investors and the remainder switching equally the other
25611440	25612440	two.
25612440	25618240	So the point five here is probability that customer at bank C stays at bank C. And these
25618240	25623120	point two-fives here, probability that somebody at bank C switches to A here and switches
25623120	25624520	to B here.
25625240	25630520	So this is what we call our transition matrix.
25630520	25637320	And as I said, the ijth entry represents the fraction of current investors going from bank
25637320	25640560	J to bank I.
25640560	25652040	So we think of, again, think of this as labeled columns ABC, rows ABC, the entry, a particular
25652040	25659640	entry represents person switching from whatever column they're in to whatever row that value
25659640	25660640	is.
25660640	25668480	Like to find the distribution of investors after one year, we take our transition matrix
25668480	25674160	multiplied by the current state vector, which is this one.
25674160	25678240	And that gives us this vector here.
25678240	25686480	After two years, multiply the transition matrix times the distribution after one year.
25686480	25692120	So just take that one, slide it in here, do that multiplication, and here's what we have
25692120	25693320	after two years.
25693320	25699320	And so you can keep doing that for as many years as you're interested in.
25699320	25704880	If you would like to find the steady state vector, then you compute M minus the identity,
25705320	25709840	which gives you this matrix, and then you solve the system M minus i times x equals
25709840	25717840	zero, which do some row operations that takes you to this point.
25717840	25726400	And so we have x1 equals x3, x2 is equal to 1.5 times x3, x3 is free.
25726400	25730400	You should always end up with a free variable here, because otherwise you have the unique
25730400	25736360	solution, which is the zero vector, and that's an error, because it should always be a free
25736360	25739360	variable.
25739360	25749200	I have the element sum to 1 tells us that x3 is 2 7ths, and so our steady state vector
25749200	25758360	looks like this, 2 7ths, 3 7ths, 2 7ths, and I didn't actually compute those probabilities.
25759320	25767320	But this is the method that you would use.
25767320	25773240	OK, today our topic is eigenvalues and eigenvectors.
25773240	25778840	The equation that we're interested in when trying to find eigenvalues and eigenvectors
25778840	25784320	is this one, ax equals lambda times x.
25784320	25787680	Here a is an n by n matrix.
25787680	25793520	X is a vector in our n, and lambda is a scalar.
25793520	25800680	So we're essentially asking, can you find x and lambda, we don't know either of those,
25800680	25808200	but can you find x and lambda such that when you multiply a times x, you get the same result
25808200	25814320	as simply scaling x by lambda.
25814320	25821240	So if x is not equal to 0 and satisfies ax equals lambda x, then it's said to be an
25821240	25826880	eigenvector of a with associated eigenvalue lambda.
25826880	25833200	So we require that eigenvectors not be 0, because clearly the zero vector satisfies this
25833200	25838000	equation, so we're looking for nonzero solutions.
25838000	25844120	So x would be an eigenvector, lambda, and eigenvalue.
25844280	25850680	We've seen a similar system to this when we were finding the steady state vector for
25850680	25855480	a transition matrix back when we were looking at Markov chains.
25855480	25862640	And that, if you remember, looked like this, we had mx equals x, so we wanted to know if
25862640	25867240	there was a vector x such that you apply the transition matrix to it and you get the same
25867240	25871040	vector back.
25871320	25881000	You just look at the x here as having a coefficient of 1, so we have mx equals 1 times x, then
25881000	25889520	it's of the same form as our eigenvector and eigenvalue equation, ax equals lambda x.
25889520	25894160	So we have seen systems like this before, and we're going to solve them in a similar
25894160	25898040	way as we did when we found the steady state vector.
25898040	25902400	So if you remember what we did then, we took the x over to the other side and factored
25902400	25908840	it out, and we ended up with m minus i times x equals 0, and that was the equation to solve
25908840	25911840	to find the steady state vector.
25911840	25916920	We're going to do a similar thing with our eigenvector system, bringing the lambda x
25916920	25923760	to the other side, factor out x, and we end up with a minus lambda times the identity
25923760	25927560	matrix times x equals 0.
25927560	25930520	So this is a system that we need to solve.
25930520	25935560	Now it's complicated here more so than with the Markov chains because we don't know what
25935560	25936560	lambda is.
25936560	25940080	We don't know x and we don't know lambda.
25940080	25944680	All right, now let's think about this a bit.
25944680	25951800	We know that x to be an eigenvector can't be the zero vector, so we have a homogeneous
25951800	25957560	system here, and we want to find non-trivial solutions to it.
25957560	25961960	All right, so some things that we know.
25961960	25970600	This system is going to have non-trivial solutions if it has free variables, and it has free
25970600	25979560	variables if the matrix a minus lambda i does not have a pivot position in every column.
25979560	25985320	And that happens when a minus lambda i is not invertible, right, because it doesn't
25985320	25992360	have a pivot position in every column, has fewer than n pivot positions, it's not invertible.
25992360	26000840	And that happens when the determinant of that matrix, a minus lambda i, is equal to zero.
26001160	26007360	This last item here is the key to how we are going to find eigenvalues.
26007360	26016920	Okay, so we're going to compute the determinant of a minus lambda i, and that actually turns
26016920	26024400	out to give us a polynomial, and we're going to set it equal to zero and solve for lambda.
26024400	26030080	Once we have the eigenvalues, then we can proceed to finding the eigenvectors by plugging
26030080	26036760	in what we know for lambda for the eigenvalues, and then we just have a simple homogeneous
26036760	26038600	system to solve to find x.
26038600	26043360	All right, so let's look at an example.
26043360	26047480	Here's a matrix a, it's a two by two.
26047480	26055120	We first find the eigenvalues of a by solving the determinant, a minus lambda i equals zero.
26055120	26057840	So here's a minus lambda i.
26057840	26062800	Here's a minus lambda times the identity matrix.
26062800	26068640	So we end up basically, what you end up with is just subtracting lambda off the diagonal
26068640	26074400	elements, and then we want to take the determinant of that matrix.
26074400	26083840	So we take the determinant, either criss cross, and we get this stuff, and we expand, put
26083840	26090440	it all together, and then factor it, and when we factor it, we get lambda minus eight times
26090440	26095880	lambda plus two, and we set that equal to zero, because that's what we want the determinant
26095880	26101080	of this matrix to equal zero, and clearly the solutions are lambda equals eight and lambda
26101080	26102760	equals negative two.
26102760	26106960	So those are our eigenvalues.
26107960	26115080	All right, here we ended up, we go back, we ended up with a lambda squared minus six
26115080	26119800	lambda minus sixteen, that's a quadratic function.
26119800	26122760	So there's two by two matrix, we ended up with a quadratic.
26122760	26127800	In general, if you have an n by n matrix, then the determinant of a minus lambda i will
26127800	26130960	be a polynomial of degree n.
26130960	26136040	So for a three by three, you get a cubic function, four by four, you get a degree four, and so
26136040	26137040	forth.
26137040	26146880	Okay, so at this point we have two eigenvalues for a, and we want to find the eigenvectors
26146880	26153320	associated with each of these eigenvalues.
26153320	26156800	So let's start off with lambda equals eight.
26156800	26164360	So we want to solve a minus lambda i, lambda in this case is eight, so a minus eight i times
26164360	26167080	x equals a zero vector.
26167080	26173320	So first we need to compute a minus eight i, and there it is, and then we set up that
26173320	26179080	homogeneous system, and remember that the whole point of finding the eigenvalues was
26179080	26185240	so that we would have a free variable, and so if you end up working on problems and you're
26185240	26188720	trying to find an eigenvector and you don't end up with a free variable, then you've made
26188720	26190960	a mistake somewhere.
26191160	26195600	Either you don't have a correct eigenvalue, or you made a mistake in solving this system.
26195600	26201200	So you should always end up with at least one free variable so that the system has
26201200	26203280	non-trivial solutions.
26203280	26209360	So in this case, we end up with x1 equals three x2, x2 is free.
26209360	26217360	So if we write it in parametric vector form, it looks like this, and so that tells us that
26217360	26225080	any multiple, any vector of this form, multiple of three one, is an eigenvector of a corresponding
26225080	26229240	to lambda equals eight.
26229240	26231880	You can check that to make sure it works.
26231880	26237720	A x should equal lambda x, so if we multiply eight times x, here's a, here's x, we end
26237720	26244600	up with a vector 24 eight, and we can factor out lambda, which is eight, and that we get
26244600	26250000	eight times three, eight times one, and three one is our vector x, so that's equal to lambda
26250000	26252920	x, so it works out.
26252920	26256760	This is an eigenvector corresponding to lambda equals eight.
26256760	26263560	All right, we got another eigenvalue to look at, lambda equals negative two, so we need
26263560	26271040	to solve a minus lambda i x equals zero again for lambda equals negative two.
26271040	26278480	So we got a minus minus two times i, it's just eight plus two times the identity, which
26278480	26286600	is this matrix, and we set up the homogeneous system, and again, do one row operation, the
26286600	26291840	second row goes away, we got a free variable, and here's our solution.
26291840	26299600	So if we write it in parametric vector form, we get this, and so that tells us that any
26299600	26306280	multiple of the vector negative one third one is an eigenvector of a corresponding to
26306280	26313480	the eigenvalue negative two, so you can check again like we did last time, multiply eight
26313480	26320000	times x, you get two negative six, and that is equal to negative two times negative one
26320000	26326280	three, which was the eigenvector that I picked.
26326280	26331280	You might be saying, hey, that didn't look like this one that we got here, and it doesn't
26331280	26336920	because I just multiplied, I scaled it, because remember, any multiple of this vector is an
26336920	26346120	eigenvector, so I just chose x two to be three, and scaled it, and so three times negative
26346120	26350960	one third gave me the negative one, and then three times one gave me three, and I did that
26350960	26353400	just to get rid of the fraction there.
26353400	26359600	But any multiple, any non-zero multiple, I should say, any non-zero multiple of this
26359600	26369400	vector would be an eigenvector corresponding to lambda equals negative two, alright.
26369400	26376000	So given an eigenvalue lambda of a matrix A, we solve this homogeneous system, A minus
26376000	26383960	lambda i times x equals zero to find the associated eigenvector, or eigenvectors, alright.
26383960	26390640	Now a little bit of terminology here, think back to the previous chapter, we talked about
26390640	26397800	the null space of a matrix, and remember the null space of a matrix is just the set of
26397800	26405720	solutions to the homogeneous system involving that matrix, and so if you look at this system,
26405720	26410960	you're solving to find the eigenvectors, you can see it's a homogeneous system, so
26410960	26420080	the set of solutions to this will be elements of the null space, or actually will comprise
26420080	26430080	the null space of a minus lambda i, and we give that space, since it's associated with
26430080	26436440	finding eigenvectors, we give it a special name and call it the eigenspace of A. So
26436440	26442600	the eigenspace of A corresponding to the eigenvalue lambda is just the null space of
26442600	26450280	a minus lambda i, right, set of all solutions to this system, a minus lambda i times x equals
26450280	26453640	zero.
26453640	26460200	So the eigenspace of A corresponding to lambda consists of the zero vector, because that's
26460200	26469960	in every null space, and all eigenvectors of a minus lambda i.
26469960	26476360	Therefore a basis for the eigenspace of A corresponding to lambda is the same as a basis
26476360	26480200	for the null space of a minus lambda i.
26481200	26485320	Okay, let's look at another example.
26485320	26492120	Let's find all the eigenvalues and eigenvectors of this matrix A. So we start off to find
26492120	26501960	the eigenvalues, take the determinant of A minus lambda i, and here it gets a little
26501960	26507360	complicated because it's got a three by three, so we have to expand about one of the rows
26507360	26508440	or columns.
26508440	26514640	I expanded about the first column, and so we get negative four minus lambda times the
26514640	26520120	determinant of this two by two matrix here, which that's what you see right here.
26520120	26525200	Then it's, since this is a plus position, next be minus, so that's where the minus six
26525200	26531120	comes from, and then it's the matrix that you get when you delete that second row in
26531120	26532640	the first column.
26532640	26538840	So we get this, and then it's a minus term, so the next one's plus, so I get plus six
26538840	26546880	times this little two by two sitting up here in the upper right corner.
26546880	26552200	And then it's just algebra, you know, take these two by two determinants, do some algebra,
26552200	26562520	oops, there we go, and we end up with this cubic function, which makes sense because
26562520	26571040	three by three matrix, and we end up being able to rewrite it like this, negative lambda
26571040	26578080	times lambda minus two squared equals zero, and so clearly lambda equals zero is a solution
26578080	26581360	and lambda equals two is a solution.
26581360	26588440	Now in this case, since it's lambda minus two squared, the eigenvalue two actually occurs
26588440	26591760	twice because it's a solution twice here.
26591760	26598600	So we say that lambda equals two has multiplicity two, that means it occurs two times.
26598600	26603520	Lambda equals zero, it only occurs once, so it has multiplicity one.
26603520	26610960	All right, then we need to do the same process as before to find the eigenvectors.
26610960	26616480	So for lambda equals zero, we need to solve a minus zero times the identity times x equals
26616480	26618480	zero.
26618480	26627760	So in this case, a minus lambda i is just a, so we set up the homogeneous system and
26627760	26635040	get that matrix into reduced echelon form, and here's the solution, so we write that
26635040	26645480	in parametric vector form, and so any multiple of negative one one one would be an eigenvector
26645480	26649440	associated with lambda equals zero.
26649440	26657720	Okay, then we move on to the next eigenvalue, which is two, so we solve a minus two i times
26657720	26666040	x equals zero, and so compute a minus two i, then set up the homogeneous system and
26666040	26672080	get it in reduced echelon form, and notice here that we've got two free variables, and
26672200	26678880	so we're going to end up with two linearly independent eigenvectors here, and sometimes
26678880	26686400	that happens because remember lambda equals two was an eigenvalue that occurred twice,
26686400	26692720	had multiplicity two, and so sometimes in that case, you end up with two linearly independent
26692720	26695880	eigenvectors, sometimes only one.
26695880	26700880	In this case, we're going to have two because we have two free variables.
26700880	26711400	So the solution looks like this, every eigenvector of a associated with lambda equals two is
26711400	26720320	a linear combination of these two vectors, and clearly they are linearly independent.
26720320	26727880	So to review this problem we've just looked at, here's our matrix a, eigenvalues were
26727880	26734880	zero and two, for lambda equals zero we ended up with one eigenvector, lambda equals two,
26734880	26741320	we've got two linearly independent eigenvectors, now clearly any multiple of this one is an
26741320	26746720	eigenvector associated with lambda equals zero, and any linear combination of these
26746720	26752720	two is an eigenvector associated with lambda equals two, but we're really concerned about
26752720	26758880	how many linearly independent eigenvectors we have, and so from lambda equals zero we
26758880	26764680	got one, from lambda equals two we got two, right?
26764680	26771040	So we can say that the vector negative one, one, one is a basis for the eigenspace of
26771040	26779600	a associated with lambda equals zero, similarly these other two vectors would form a basis
26779720	26785840	for the eigenspace of a associated with lambda equals two.
26785840	26794120	Alright, this brings us to a theorem which says if v1 through vr are eigenvectors that
26794120	26800400	correspond to distinct eigenvalues, lambda one through lambda r, of an n by n matrix
26800400	26806440	a, then the set v1 through vr is linearly independent.
26806440	26812800	I'm going to put in a little simpler terms, this just says eigenvectors that come from
26812800	26820240	different eigenvalues are linearly independent, eigenvectors that come from different eigenvalues
26820240	26823240	are linearly independent.
26823240	26829920	So from our previous example, this first one came from a different eigenvalue from the
26829920	26836480	second two, so we know that this set is linearly independent, right?
26836480	26842520	Because the latter two came from the same eigenvalue, but they were linearly independent,
26842520	26848120	we knew that from before, and then when you throw in this other one that came from the
26848120	26852800	other eigenvalue, we know that this set is linearly independent because of the theorem,
26852800	26857480	because they came from different, the first one and the second two came from different
26857840	26859840	eigenvalues.
26859840	26866840	Alright, we're going to hit that a little bit more in the next section, but we'll leave
26866840	26869200	it at that for right now.
26869200	26874680	And I just want to talk about a little easier problem, okay?
26874680	26879080	What we've done so far is just taken, basically done the hard problem.
26879080	26882600	Here's a matrix, find all the eigenvalues and eigenvectors.
26882600	26889600	But sometimes, you just want to know if a given vector is an eigenvector of a matrix,
26890520	26894520	and that is a much easier problem, okay?
26894520	26899960	So suppose we just want to know if this vector x, given here, is an eigenvector of the matrix
26899960	26902460	a.
26902460	26908460	For this, go back to the original equation and think, does ax equal lambda x?
26908860	26912740	Well we can multiply a times x, that's trivial, right?
26912740	26915700	And then how would we know if it's an eigenvector?
26915700	26922700	Well the result of a times x should be some scalar times x.
26922700	26928860	So we multiply a times x, and in this case we get this vector here, and notice that I
26928860	26935140	can factor out a negative two, and I end up with negative two times this vector, which
26935140	26936820	is the original x.
26936820	26943820	So in this case we have ax equals negative two times x, so that means that x is indeed
26943820	26950660	an eigenvector of a, and the associated eigenvalue is negative two, okay?
26950660	26954980	So we didn't have to solve any systems of equations, or take determinants, or anything
26954980	26955980	like that.
26955980	26962980	It was just a simple plug it into the basic equation and see if it satisfies it.
26963700	26970700	Alright, another similar question is, given a scalar value, check to see if it's an eigenvalue
26974980	26977180	of a matrix.
26977180	26983100	And again, you don't have to go through the process that we were doing to find the eigenvalues.
26983100	26988980	Just to check to see if a number is an eigenvalue, you need only look at the determinant of a
26988980	26993500	minus lambda i, where lambda is the value that you're given.
26993500	26998860	So in this case, we look at the determinant of a minus three times the identity.
26998860	27005860	So we set up that matrix, take its determinant, and in this case we end up with zero, and
27005860	27012860	so the determinant of a minus three i is equal to zero, that tells us that three is indeed
27012860	27014940	an eigenvalue of a.
27014940	27018500	If we didn't get zero, then it wouldn't be an eigenvalue.
27018500	27023420	Okay, a little more terminology.
27023420	27028540	This expression, the determinant of a minus lambda i, okay, that's what you end up with
27028540	27035540	when you are trying to figure out the eigenvalues of a matrix, and we said that you end up with
27036700	27039020	a polynomial there.
27039020	27043060	It's called the characteristic polynomial of a.
27043060	27048420	And when you set it equal to zero, then we call it the characteristic equation of a.
27048420	27052980	So the characteristic polynomial is just what you get when you take the determinant, and
27052980	27057980	then you set that equal to zero, and we call that the characteristic equation.
27057980	27064980	Okay, we have now the final installment of the invertible matrix theorem.
27065980	27072980	So, for a's and n by n matrix, a is invertible if and only if zero is not an eigenvalue of
27076780	27083780	a, okay, zero is not an eigenvalue of a, and actually it's pretty easy to see that if
27086060	27089260	you think about it a little bit.
27089260	27095700	The eigenvalue of a satisfies the characteristic equation, which is the determinant of a minus
27095700	27097620	lambda equals zero.
27097620	27102860	So if zero was an eigenvalue of a, then we'd have the determinant of a minus zero times
27102860	27109140	i would be zero, but a minus zero times i is just a, so we would have the determinant
27109140	27115140	of a is equal to zero, and that means that a is not invertible, okay, so therefore zero
27115220	27122220	can't be an eigenvalue of a if a is invertible.
27123220	27130220	Alright, let's talk today about matrix diagonalization.
27135100	27141700	First we're going to introduce the concept of a similar matrix, so if a and b are n by
27141740	27148740	n matrices, then we say a is similar to b, and also b is similar to a. If there's an invertible
27150780	27157780	matrix p such that a is equal to pbp inverse, and we have this theorem, which says if you
27161580	27167500	have two matrices which are similar, then they have the same characteristic polynomial
27167500	27174420	and hence the same eigenvalues with the same multiplicities. So remember, the characteristic
27174420	27180420	polynomial is just the determinant of a minus lambda i, that's the polynomial that you get
27180420	27187420	when you're trying to find the eigenvalues of a matrix, so this theorem tells us if
27187420	27192220	you have two matrices that are similar, then they have the same characteristic polynomial
27192220	27199220	and therefore they're going to have the same eigenvalues. Okay, now why is this of interest?
27199500	27206500	Well, think back to our section on Markov chains. We had this sort of equation that
27211340	27218340	to get to x1, we multiplied our transition matrix, which I'm calling a here, times the
27218500	27225500	previous state vector, so over x1, a times x0, a2 is a times x1 and so forth, x to the
27226300	27233300	x sub k is a times x sub k minus 1. Now, we can, if we look at it like this, then to
27237700	27244060	find say our state vector at time 10, then you need to find the state vector at time
27244060	27248820	9, and to find the one at time 9, you need the one at time 8 and so forth, and so to
27248820	27255820	compute x sub 10, you need x1, x2, x3, up through x sub 9. Another way to look at it
27258100	27265100	is like this, that we know for instance for x2, that's a times x1, but we know x1 is
27266220	27273220	a times x0, so we make that substitution and so we end up with x2 as a squared times x0.
27273660	27280660	And similarly, x3 is a cubed times x0. So in general, xk is a to the k times x0. Now,
27283260	27290260	unless a to the k is relatively easy to compute, this really doesn't help us. However, if
27292340	27299340	a to the k is easy to compute, then we can compute x sub k much more quicker than going
27300340	27307340	through and computing all these subsequent state vectors. Alright, so, let's suppose
27310020	27317020	that a is similar to a diagonal matrix. That is, a can be written as pdp inverse, where
27318700	27325700	p is invertible and d is a diagonal matrix. Now, what does that get for you? Well, just
27326260	27332460	look at what happens when you compute a squared. A squared is a times a, and when we substitute
27332460	27339460	pdp inverse in for a, we have this. Now, the trick here is to group the p inverse times
27340180	27344620	p together. So, I actually got p inverse p right here in the middle. If we group that
27344620	27351620	together, that's the identity matrix. And so then we get pd times the identity times
27352460	27357620	dp inverse. And of course, anything times the identity is just that anything. And we've
27357620	27364620	got d times d, which gives us d squared. So, a squared is pd squared p inverse. How
27365900	27372900	about a cubed? A cubed is a squared times a. And so, we take the a squared, we compute
27373660	27380660	it up here, plug it in, times a, and do our trick again. Reassociate to get the p inverse
27381060	27388060	p together. And we end up with pd cubed p inverse. So, it looks like in general, a to
27388580	27395580	the k is p times d to the k p inverse. Now, what good is that? Well, let's look at a diagonal
27398580	27405580	matrix. Here's a simple 2 by 2 diagonal matrix. If we compute d squared, then notice that
27406260	27412260	we end up with another diagonal matrix. And the entries on the diagonal are just the original
27412260	27418140	diagonal entries raised to the second power. So, in this case, we were computing d squared.
27418140	27425140	So, these are the original diagonal entries squared. D cubed. We get another diagonal
27425620	27432620	matrix. And the entries are the original diagonal entries cubed. So, just looking at this example,
27433620	27440620	it appears that it's easy to compute powers of a diagonal matrix. We just raise each diagonal
27440620	27447620	entry to whatever power we're trying to compute. So, if a is similar to a diagonal matrix,
27448020	27455020	computing powers of a is also easy. So, we say that a square matrix a is said to be diagonalizable
27456020	27463020	if a is similar to a diagonal matrix. That is, if a is equal to pdp inverse for some invertible
27467540	27474540	matrix p and some diagonal matrix d. Now, here is a very important theorem. This is
27475540	27482540	an n by n matrix a is diagonalizable if and only if a has n linearly independent eigenvectors.
27488940	27495940	So, a is diagonalizable if it has n linearly independent eigenvectors. And furthermore,
27496940	27503940	a equals pdp inverse with d a diagonal matrix if and only if the columns of p are n linearly
27508260	27514140	independent eigenvectors of a. So, this theorem not only tells us the condition, a condition
27514140	27521140	under which a is diagonalizable, it also tells us how to compute p and d, okay? p is a matrix
27521140	27528140	that consists of n linearly independent eigenvectors of a. And continuing on, it says the diagonal
27529260	27536260	entries of d are eigenvalues of a that correspond respectively to the eigenvectors in p. So,
27539140	27546140	p consists of the eigenvectors, d consists of the eigenvalues on the diagonal. So, p
27552140	27558140	let's look at an example. Here is a matrix a. You can recognize this matrix. We looked
27558140	27565140	at it in the video on the intro to eigenvalues and eigenvectors. Okay, so is it diagonalizable?
27569140	27576140	Well, let's find the eigenvalues. So, we take the determinant of a minus lambda i and we
27577140	27584140	end up with lambda minus eight times lambda plus two is our characteristic polynomial.
27585140	27590140	We set that equal to zero, and so our eigenvalues are lambda equals eight and lambda equals negative
27590140	27597140	two. Now, at this point, we know that a is diagonalizable because a has two distinct eigenvalues.
27598140	27605140	So, it's a two by two matrix. It has two distinct eigenvalues. That means since we know that
27611140	27617140	distinct eigenvalues give us linearly independent eigenvectors, eigenvectors that come from distinct
27619140	27626140	eigenvalues are guaranteed to be linearly independent. And therefore, a is diagonalizable. Now, to find
27628140	27635140	p, we have to compute the eigenvectors. We already know what d is. d is a matrix, diagonal matrix,
27636140	27642140	with the eigenvalues on the diagonal. So, take lambda equals eight, and we need to solve
27643140	27650140	a minus eight times i times x equals a zero vector. And we, so we solve that system and
27651140	27658140	we end up with this vector in parametric vector form. So, any multiple of three one is an eigenvector
27661140	27668140	of a corresponding to lambda equals eight. So, at this point, we have one column of p. To get the other column,
27669140	27676140	we look at the eigenvalue negative two and solve a minus negative two i times x equals
27676140	27683140	zero. And here's that. We end up with negative one third one as our eigenvector. So, any
27691140	27698140	multiple of that, any nonzero multiple of that would be an eigenvector. And so, I multiply
27699140	27705140	by three. Go back to that. I multiplied it by three. And so, I end up with negative one
27706140	27713140	three for our second eigenvector. So, the three one came from lambda equals eight. Negative
27714140	27720140	one three came from lambda equals negative two. And so, d consists of the eigenvalues. And the order
27721140	27728140	that you put the eigenvectors into p is not important. However, once you establish an order, you
27729140	27735140	have to hold that true for both p and d. So, the eigenvector here three one came from lambda equals eight.
27738140	27744140	So, we need the eigenvalue eight in the first column of d. And then the eigenvector in the second column
27745140	27752140	of p should correspond to the eigenvalue in the second column of d. All right. And then,
27753140	27760140	if we want to just check our work, we multiply pdp inverse. And we end up with a. Go through
27764140	27770140	that computation and you end up with a. So, pdp inverse is equal to a. Let's look at another
27771140	27778140	example. Here's another matrix three by three in this case. And we want to know if it is diagonalizable.
27780140	27787140	So, as before, we find its eigenvalues by looking at the determinant of a minus lambda i. We end
27790140	27797140	up with this polynomial, negative lambda times lambda minus two squared equals zero. So, we
27800140	27807140	find that lambda equals two and the solutions will be lambda equals zero and lambda equals
27808140	27814140	two. Now, lambda equals zero occurs once. Lambda equals two occurs twice since it's a
27815140	27822140	squared term. So, lambda equals two has multiplicity two. It appears twice as an eigenvalue. Now, at this
27823140	27830140	point, we know that a has only two distinct eigenvalues. So, therefore, we don't know if it has three linearly
27834140	27840140	independent eigenvectors or not. So, we don't know if a is diagonalizable or not. Just because it doesn't
27841140	27847140	have three distinct eigenvalues doesn't mean that it's not diagonalizable. That theory only goes one
27847140	27856140	way. If it has indistinct eigenvalues, then it will be diagonalizable. If it doesn't have indistinct
27857140	27863140	eigenvalues, you don't know whether it's diagonalizable or not. You have to actually see if you can compute in
27864140	27871140	linearly independent eigenvectors. So, in this case, that's what we've got to try. And actually, for this particular
27872140	27880140	problem, the real question is whether lambda equals two, which occurred with multiplicity two, will have one or two
27881140	27890140	linearly independent eigenvectors. So, let's look at lambda equals two. We solve a minus two i times x equals zero.
27891140	27899140	And we end up with, notice in the matrix here, you can see that we've got two free variables. So, therefore, we're
27900140	27912140	going to get two linearly independent eigenvectors from this eigenvalue. And so, at this point, we've got two linearly independent
27913140	27920140	eigenvectors from lambda equals two. We know we'll get one more from the other eigenvalue. So, at this point, we know that
27921140	27931140	a is diagonalizable. And actually, we're two-thirds of the way towards producing the p such that a equals p d p inverse.
27932140	27941140	So, we look at the other eigenvalue of zero and solve a minus zero i times x equals zero. Then, notice we've got one free
27941140	27951140	variable. So, we end up with one linearly independent eigenvector. And so, we know that a is diagonalizable.
27952140	27960140	Here's our eigenvectors that we found. So, we know that a is equal to p d p inverse where p is this matrix.
27961140	27969140	Now, notice that the negative one one one came from lambda equals zero. So, that means in the first column of b, I've got zero
27970140	27980140	here in the diagonal entry. These last two eigenvectors came from lambda equals two. And so, in the second column and the third column of
27981140	27992140	b, I've got two on the diagonal. Now, before, in the two by two example, we actually checked our answer by computing p d p inverse.
27992140	28001140	But for three by three, it's not so simple to compute a inverse. And if you've got a calculator, that's not that bad.
28002140	28013140	But if you don't, then there's an easier way to check your answer. And that is to note that if a is equal to p d p inverse,
28013140	28021140	if we multiply both sides of that equation by p on the right, then we end up with p inverse times p, which goes away.
28022140	28031140	And then we end up on the left-hand side with a times p. So, a p would equal p d. And so, we can compute both those matrices.
28032140	28040140	Since it only involves multiplication of matrices, you don't have to find any inverses. So, you can check to see if a p is equal to p d.
28040140	28051140	So, if we compute both of those, you can see that in this case, they are indeed the same. And so, we can conclude that our p and d are correct.
28052140	28060140	Alright, let's look at one more example. Here's another three by three. We want to know if it is diagonalizable.
28061140	28074140	So, we find the eigenvalues. I'm expanding about the first column here when I take the determinant and end up with lambda minus four times lambda plus two squared.
28075140	28080140	So, again, I don't have three distinct eigenvalues. I only have two.
28080140	28088140	Lambda equals four occurs with multiplicity one. Lambda equals negative two occurs with multiplicity two.
28089140	28102140	So, we want to know does lambda equal negative two have or we're going to get two linearly independent eigenvectors out of it.
28103140	28112140	And when we solve a minus minus two i times x equals zero, notice that we end up with this matrix in echelon form.
28113140	28126140	And so, we only have one free variable. Alright, x three is free. And so, we don't get two linearly independent eigenvectors out of this eigenvalue.
28126140	28137140	And so, we only get one. We're only going to get one linearly independent eigenvector from the other eigenvalue, lambda equals four.
28138140	28142140	So, that means we're only going to end up with two linearly independent eigenvectors.
28143140	28147140	And that means that a is not diagonalizable in this case.
28148140	28154140	Okay, so we're into chapter six now and section six point one.
28155140	28159140	And let's start off with inner products.
28160140	28166140	We've discussed this before, I believe, but we're talking about how to multiply a matrix by a vector.
28167140	28169140	But let's hit that again here.
28169140	28176140	And so, the inner product is defined on two vectors that have to be the same length.
28177140	28180140	So, let's suppose that u and v are vectors in Rn.
28181140	28194140	The inner product, also called the dot product of u and v, is given by u one times v one plus u two times v two plus dot dot u n times v n.
28195140	28201140	So, we simply match up the elements, corresponding elements, multiply, and then add them all up.
28202140	28211140	The notation is indicated here where there's a dot u dot v, and that's where the term dot product came from, just because you normally see it written like that.
28213140	28219140	So, for an example, suppose we have these two vectors, u and v, to compute their inner product.
28219140	28225140	We just match up elements again, so one times four plus two times five plus three times six.
28226140	28227140	And so, we end up with 32.
28227140	28231140	So, the inner product of two vectors yields a scalar value.
28234140	28238140	Okay, here's some properties of inner products.
28239140	28249140	The fourth one, I think, is the most interesting because it discusses the inner product of a vector with itself.
28250140	28264140	And if we examine that, you see that u dot u is just u one squared plus u two squared plus dot dot dot plus u n squared.
28264140	28275140	So, since we're adding up a bunch of squared terms, then it has to be non-negative.
28276140	28288140	And notice that the only way that it can be zero, as it's always going to be greater than or equal to zero, but the only way it can equal zero is if each of these terms is zero.
28288140	28296140	And since they're all positive or non-negative, they all have to be zero for the sum to be zero.
28297140	28307140	Okay, so u dot u is always greater than or equal to zero, and it equals zero, if and only if u is the zero vector.
28307140	28313140	Okay, on to the length of a vector.
28313140	28318140	The length of a vector also called the norm of a vector.
28318140	28327140	If you go and take more math courses, you might encounter the idea of a norm later on, and there are different types of norms.
28327140	28329140	This is just one of them, actually.
28329140	28335140	It's called the two norm because we're squaring the entries and then taking the square root.
28336140	28346140	Okay, but you see the notation, the double vertical bars indicates length or norm, and it's just the square root of a vector with itself.
28346140	28353140	So we know what v dot v is, some of these squares, and so then we just take the square root of that.
28354140	28359140	In two dimensions, it's easy to see how this works.
28359140	28363140	So we have this vector, the blue one here, let's say it's v1, v2.
28364140	28372140	Then we know that this distance here along the x-axis is v1, because that's the first component.
28372140	28380140	And we know that the distance vertically is v2, since that's the second component, and we have a right triangle here.
28380140	28391140	And so by the Pythagorean theorem, we know that the length of the hypotenuse here, which is the length of v, is square root of v1 squared plus v2 squared.
28392140	28402140	So this is clear in two dimensions, and actually the scales, as you see, scales to end dimensions still use the same formula.
28406140	28411140	At times, we're interested in finding the distance between two vectors.
28412140	28421140	And so we note that in this form, this u and v, and that's equal to the length of the vector u minus v.
28421140	28423140	So let's see how that works.
28423140	28430140	So here we have a vector u, and here's a vector v, and what we'd like to determine is the distance between u and v.
28430140	28435140	So that's the length of this line segment here, the black line segment.
28436140	28440140	So let's examine what u minus v looks like.
28440140	28444140	Well, that's u plus minus v.
28444140	28449140	Okay, so here's minus v down here, u still here.
28449140	28458140	And so if we do the parallelogram method to compute u plus negative v, we end up with this vector.
28458140	28468140	So here's u minus v right here, and you can see from the picture that it's the same length as this distance between u and v.
28468140	28476140	And so that's why we compute the distance between u and v as the length of u minus v.
28477140	28486140	All right, we say two vectors are orthogonal to each other if their inner product is zero.
28486140	28498140	Orthogonal is, looks like two vectors are orthogonal, means it looks like they're perpendicular.
28498140	28503140	As you see here, here's a couple of vectors, two one and three negative six.
28503140	28506140	If you take the inner product, you end up with zero.
28506140	28511140	And if you look at those plotted, then you see a right angle there.
28511140	28515140	So they are orthogonal.
28518140	28530140	We say a set is an orthogonal set if you can pull out any two vectors from the set and their inner product is zero.
28530140	28543140	Okay, so if I have this set v1, v2, v3, and I want to see if it's an orthogonal set, then I need to compute the inner product of each pair of vectors.
28543140	28553140	So start off v1 with v2, and so I get one times negative three plus two times zero plus one times three, and that's zero.
28553140	28563140	Then I'll do inner product of v1 with v3, and then v2 with v3, and you see that all of those are zero.
28563140	28573140	So that means that the set itself is orthogonal because we took all possible pairs and did inner products and each one of those was zero.
