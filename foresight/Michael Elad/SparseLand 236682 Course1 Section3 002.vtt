WEBVTT

00:00.000 --> 00:10.800
We start diving into the greedy methods by presenting an algorithm called the orthogonal

00:10.800 --> 00:13.000
matching pursuit.

00:13.000 --> 00:14.000
The idea is the following.

00:14.000 --> 00:20.960
We are looking at the system ax equals b and we are searching for a spar solution to it.

00:20.960 --> 00:25.880
We will find the support of x by operating sequentially and gradually.

00:25.880 --> 00:30.800
We start by searching the best possible solution with the support of only one atom.

00:30.800 --> 00:36.160
This is done by sweeping through all the m possibilities and finding the column in a

00:36.160 --> 00:39.840
that leads to the best match between ax and b.

00:39.840 --> 00:44.680
Once found, this column will stay with us and we will search for the second atom to

00:44.680 --> 00:45.680
join it.

00:45.680 --> 00:50.920
Again, sweeping through all the a minus one possibilities, we choose the second atom such

00:50.920 --> 00:55.360
that the match between ax and b is the best possible.

00:55.360 --> 00:59.320
This way, the support grows by one non-zero at a time.

00:59.320 --> 01:05.200
The algorithm stops when ax and b are close enough to each other.

01:05.200 --> 01:10.720
Put in terms of the tree we have seen before, we start with an empty support and then check

01:10.720 --> 01:15.880
m options for the first atom thus getting to the best chosen solution with cardinality

01:15.880 --> 01:16.880
one.

01:16.880 --> 01:22.000
Other than checking all the possibilities of cardinality two, we check only the options

01:22.000 --> 01:25.640
that rely on the already found solution.

01:25.640 --> 01:32.040
This process proceeds this way again and again until we get to a good enough approximation.

01:32.040 --> 01:37.640
This way, instead of a combinatorial number of tests, we apply order of m tests to complete

01:37.640 --> 01:39.360
the algorithm.

01:39.360 --> 01:47.840
The highlighted path describes how the support has grown by one non-zero in each step.

01:47.840 --> 01:52.480
While the above description may seem clear enough, it is actually a bit vague and various

01:52.480 --> 01:56.920
greedy algorithms could be proposed while being based on this rationale.

01:56.920 --> 02:02.600
We shall introduce several such methods and start by focusing on the OMP, orthogonal matching

02:02.600 --> 02:04.960
pursuit.

02:04.960 --> 02:10.160
Like all other greedy methods, the OMP generates series of solutions with gradually growing

02:10.160 --> 02:11.480
support.

02:11.480 --> 02:16.220
We denote these solutions as x0, x1, etc.

02:16.220 --> 02:20.680
These proposed solutions do not obey the equation ax equals b.

02:20.680 --> 02:28.580
And we shall denote the error vector b minus a times xk as rk, standing for the residual

02:28.580 --> 02:31.820
vector at the kth step.

02:31.820 --> 02:38.060
The main point in the OMP is to use the residual in each step in order to choose the next atom.

02:38.060 --> 02:43.540
This will be done such that the chosen atom leads to a maximal reduction of the residual

02:43.540 --> 02:45.060
energy.

02:45.060 --> 02:49.980
Once we start with x0 being 0, the residual starts as the vector b.

02:49.980 --> 02:55.940
We update x by adding one non-zero, becoming x1, the energy of r1 gets smaller.

02:55.940 --> 03:00.700
We proceed this way, adding one non-zero at a time to the solution and reducing the energy

03:00.700 --> 03:05.600
of the residual until it becomes 0 or sufficiently small.

03:05.600 --> 03:09.540
And now to a detailed description of the OMP.

03:09.540 --> 03:17.580
In initialization, we set k to be 0, x0 to be 0, the support s0 to be empty.

03:17.580 --> 03:19.860
The residual is the vector b.

03:19.860 --> 03:23.700
We increase k by 1 and perform the following steps.

03:23.700 --> 03:29.980
Given the residual rk minus 1, we search for the best column to choose from a, such that

03:29.980 --> 03:35.700
when multiplied by a scalar, it gives the smallest L2 difference from the residual.

03:35.700 --> 03:40.860
Suppose that we did these M tests and got the error values Ei.

03:40.860 --> 03:45.700
The best atom to choose is the one leading to the smallest error.

03:45.700 --> 03:48.740
Let's assume that it is the atom I0.

03:48.740 --> 03:54.980
Then this index joined the supports and now sk is updated to include it.

03:54.980 --> 04:00.700
We proceed by updating the actual coefficients of x in the chosen support location that would

04:00.700 --> 04:05.260
give the smallest L2 error between axk and b.

04:05.260 --> 04:11.260
This is a simple L2 process and its result is an updated xk.

04:11.260 --> 04:19.300
Our last step is to update the residual rk to be b minus a times xk.

04:19.300 --> 04:24.660
If the obtained residual is small enough, we may choose to stop the algorithm, otherwise

04:24.660 --> 04:28.940
we increase k by 1 and proceed.

04:28.940 --> 04:34.300
Now let's zoom in on several of the steps described in order to better understand them.

04:34.300 --> 04:38.500
We shall assume that the columns of a are L2 normalized, a fact that we'll prove useful

04:38.500 --> 04:40.180
later on.

04:40.180 --> 04:45.460
Looking closely at the computation of Ei, we are optimizing with respect to the scalar

04:45.460 --> 04:49.420
z that multiplies the ith column taken from a.

04:49.420 --> 04:55.020
The optimal value of z is given by a simple derivative of this L2 expression.

04:55.020 --> 05:01.300
zopt equals the inner product between ai and the residual rk minus 1.

05:01.300 --> 05:07.100
Notice how the denominator vanished since the atoms are normalized.

05:07.100 --> 05:12.100
Plugging the expression of zopt back into the L2 error and applying few simplifying algebraic

05:12.100 --> 05:18.380
steps, we get that ei equals the square norm of the residual minus the square of the inner

05:18.380 --> 05:21.300
product between ai and the residual.

05:21.300 --> 05:26.380
Therefore, instead of minimizing the error ei, we might as well maximize the absolute

05:26.380 --> 05:30.540
value of this inner product.

05:30.540 --> 05:36.340
That means that the choice of the next atom in the OMP can be done in this way.

05:36.340 --> 05:42.060
Take a transpose and multiply by the current residual rk minus 1 and take the absolute

05:42.060 --> 05:43.460
value.

05:43.460 --> 05:49.380
The resulting vector is of length m and its maximal absolute entry points to the atom

05:49.380 --> 05:52.420
to be chosen.

05:52.420 --> 05:57.780
Focusing on the step of updating xk, this is a least squares computation over the portion

05:57.780 --> 06:00.340
of the whole vector x.

06:00.340 --> 06:05.700
Given the support as k, we are to extract only the green columns and solve for the green

06:05.700 --> 06:08.060
entries in the vector x.

06:08.060 --> 06:13.820
Thus, this amounts to a simple least squares with k unknowns and the solution is given

06:13.820 --> 06:18.020
by the pseudo inverse of as times b.

06:18.020 --> 06:21.740
Why is this algorithm called orthogonal matching pursuit?

06:21.740 --> 06:26.660
The term matching refers to the correlations we apply between the residual and the atoms

06:26.660 --> 06:29.780
in A in order to find the next atom.

06:29.780 --> 06:35.020
By orthogonal, well, in the least squares we have just discussed, the optimal solution

06:35.020 --> 06:39.700
can be found by the derivative of the L2 error with respect to x.

06:39.700 --> 06:45.660
This leads to this expression, as transpose, multiplying this yellow term.

06:45.660 --> 06:49.540
The yellow term is nothing but the new updated residual.

06:49.540 --> 06:54.980
This means that after updating the solution xk, the inner product of the new residual

06:54.980 --> 07:00.580
is orthogonal to the atoms in the support, being the bros of AS transpose.

07:00.580 --> 07:06.140
This orthogonality is an asset, because it implies that once an atom has been chosen,

07:06.140 --> 07:13.740
it will never be chosen again, since its inner product with the residual is zero.

07:13.740 --> 07:17.980
Still on the matter of the least squares that updates xk, there is an effective numerical

07:17.980 --> 07:20.460
shortcut worth mentioning.

07:20.460 --> 07:26.100
The regular solution involves an inversion of a gram matrix of size k by k computed for

07:26.100 --> 07:27.820
the matrix ASk.

07:27.820 --> 07:34.700
However, one step before we inverted a similar matrix of size k minus 1 by k minus 1 that

07:34.700 --> 07:39.060
referred to ASk minus 1 with one atom less.

07:39.060 --> 07:43.020
Could we leverage the previous result in building the new one?

07:43.020 --> 07:45.260
The answer is positive.

07:45.260 --> 07:50.940
A new column has been added to create ASk, and along with it, a new scalar unknown has

07:50.940 --> 07:53.220
been added to the vector x.

07:53.220 --> 07:57.700
There is a recursive method for solving sequence of growing least squares problems of this

07:57.700 --> 08:04.580
form exactly, which implies that we do not need to invert any matrix in the OMP.

08:04.580 --> 08:09.060
We will not discuss this numerical shortcut further here.

08:09.060 --> 08:13.820
To summarize, the OMP involves two main steps of computations.

08:13.820 --> 08:17.940
The first is the sweep stage that searches for the next atom to add.

08:17.940 --> 08:23.540
This requires the multiplication of A transpose by the residual vector, thus requiring mn

08:23.540 --> 08:25.180
operations.

08:25.180 --> 08:29.420
The second is the least square step that updates xk.

08:29.420 --> 08:35.940
The main effort here is in computing AS transpose times AS, but there are various shortcuts

08:35.940 --> 08:38.380
that can be applied here.

08:38.380 --> 08:44.060
From line, the overall number of operations that OMP requires is on the order of k times

08:44.060 --> 08:48.260
m times n, where k is the cardinality of the final solution.

