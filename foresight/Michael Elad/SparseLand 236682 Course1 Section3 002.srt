1
00:00:00,000 --> 00:00:10,800
We start diving into the greedy methods by presenting an algorithm called the orthogonal

2
00:00:10,800 --> 00:00:13,000
matching pursuit.

3
00:00:13,000 --> 00:00:14,000
The idea is the following.

4
00:00:14,000 --> 00:00:20,960
We are looking at the system ax equals b and we are searching for a spar solution to it.

5
00:00:20,960 --> 00:00:25,880
We will find the support of x by operating sequentially and gradually.

6
00:00:25,880 --> 00:00:30,800
We start by searching the best possible solution with the support of only one atom.

7
00:00:30,800 --> 00:00:36,160
This is done by sweeping through all the m possibilities and finding the column in a

8
00:00:36,160 --> 00:00:39,840
that leads to the best match between ax and b.

9
00:00:39,840 --> 00:00:44,680
Once found, this column will stay with us and we will search for the second atom to

10
00:00:44,680 --> 00:00:45,680
join it.

11
00:00:45,680 --> 00:00:50,920
Again, sweeping through all the a minus one possibilities, we choose the second atom such

12
00:00:50,920 --> 00:00:55,360
that the match between ax and b is the best possible.

13
00:00:55,360 --> 00:00:59,320
This way, the support grows by one non-zero at a time.

14
00:00:59,320 --> 00:01:05,200
The algorithm stops when ax and b are close enough to each other.

15
00:01:05,200 --> 00:01:10,720
Put in terms of the tree we have seen before, we start with an empty support and then check

16
00:01:10,720 --> 00:01:15,880
m options for the first atom thus getting to the best chosen solution with cardinality

17
00:01:15,880 --> 00:01:16,880
one.

18
00:01:16,880 --> 00:01:22,000
Other than checking all the possibilities of cardinality two, we check only the options

19
00:01:22,000 --> 00:01:25,640
that rely on the already found solution.

20
00:01:25,640 --> 00:01:32,040
This process proceeds this way again and again until we get to a good enough approximation.

21
00:01:32,040 --> 00:01:37,640
This way, instead of a combinatorial number of tests, we apply order of m tests to complete

22
00:01:37,640 --> 00:01:39,360
the algorithm.

23
00:01:39,360 --> 00:01:47,840
The highlighted path describes how the support has grown by one non-zero in each step.

24
00:01:47,840 --> 00:01:52,480
While the above description may seem clear enough, it is actually a bit vague and various

25
00:01:52,480 --> 00:01:56,920
greedy algorithms could be proposed while being based on this rationale.

26
00:01:56,920 --> 00:02:02,600
We shall introduce several such methods and start by focusing on the OMP, orthogonal matching

27
00:02:02,600 --> 00:02:04,960
pursuit.

28
00:02:04,960 --> 00:02:10,160
Like all other greedy methods, the OMP generates series of solutions with gradually growing

29
00:02:10,160 --> 00:02:11,480
support.

30
00:02:11,480 --> 00:02:16,220
We denote these solutions as x0, x1, etc.

31
00:02:16,220 --> 00:02:20,680
These proposed solutions do not obey the equation ax equals b.

32
00:02:20,680 --> 00:02:28,580
And we shall denote the error vector b minus a times xk as rk, standing for the residual

33
00:02:28,580 --> 00:02:31,820
vector at the kth step.

34
00:02:31,820 --> 00:02:38,060
The main point in the OMP is to use the residual in each step in order to choose the next atom.

35
00:02:38,060 --> 00:02:43,540
This will be done such that the chosen atom leads to a maximal reduction of the residual

36
00:02:43,540 --> 00:02:45,060
energy.

37
00:02:45,060 --> 00:02:49,980
Once we start with x0 being 0, the residual starts as the vector b.

38
00:02:49,980 --> 00:02:55,940
We update x by adding one non-zero, becoming x1, the energy of r1 gets smaller.

39
00:02:55,940 --> 00:03:00,700
We proceed this way, adding one non-zero at a time to the solution and reducing the energy

40
00:03:00,700 --> 00:03:05,600
of the residual until it becomes 0 or sufficiently small.

41
00:03:05,600 --> 00:03:09,540
And now to a detailed description of the OMP.

42
00:03:09,540 --> 00:03:17,580
In initialization, we set k to be 0, x0 to be 0, the support s0 to be empty.

43
00:03:17,580 --> 00:03:19,860
The residual is the vector b.

44
00:03:19,860 --> 00:03:23,700
We increase k by 1 and perform the following steps.

45
00:03:23,700 --> 00:03:29,980
Given the residual rk minus 1, we search for the best column to choose from a, such that

46
00:03:29,980 --> 00:03:35,700
when multiplied by a scalar, it gives the smallest L2 difference from the residual.

47
00:03:35,700 --> 00:03:40,860
Suppose that we did these M tests and got the error values Ei.

48
00:03:40,860 --> 00:03:45,700
The best atom to choose is the one leading to the smallest error.

49
00:03:45,700 --> 00:03:48,740
Let's assume that it is the atom I0.

50
00:03:48,740 --> 00:03:54,980
Then this index joined the supports and now sk is updated to include it.

51
00:03:54,980 --> 00:04:00,700
We proceed by updating the actual coefficients of x in the chosen support location that would

52
00:04:00,700 --> 00:04:05,260
give the smallest L2 error between axk and b.

53
00:04:05,260 --> 00:04:11,260
This is a simple L2 process and its result is an updated xk.

54
00:04:11,260 --> 00:04:19,300
Our last step is to update the residual rk to be b minus a times xk.

55
00:04:19,300 --> 00:04:24,660
If the obtained residual is small enough, we may choose to stop the algorithm, otherwise

56
00:04:24,660 --> 00:04:28,940
we increase k by 1 and proceed.

57
00:04:28,940 --> 00:04:34,300
Now let's zoom in on several of the steps described in order to better understand them.

58
00:04:34,300 --> 00:04:38,500
We shall assume that the columns of a are L2 normalized, a fact that we'll prove useful

59
00:04:38,500 --> 00:04:40,180
later on.

60
00:04:40,180 --> 00:04:45,460
Looking closely at the computation of Ei, we are optimizing with respect to the scalar

61
00:04:45,460 --> 00:04:49,420
z that multiplies the ith column taken from a.

62
00:04:49,420 --> 00:04:55,020
The optimal value of z is given by a simple derivative of this L2 expression.

63
00:04:55,020 --> 00:05:01,300
zopt equals the inner product between ai and the residual rk minus 1.

64
00:05:01,300 --> 00:05:07,100
Notice how the denominator vanished since the atoms are normalized.

65
00:05:07,100 --> 00:05:12,100
Plugging the expression of zopt back into the L2 error and applying few simplifying algebraic

66
00:05:12,100 --> 00:05:18,380
steps, we get that ei equals the square norm of the residual minus the square of the inner

67
00:05:18,380 --> 00:05:21,300
product between ai and the residual.

68
00:05:21,300 --> 00:05:26,380
Therefore, instead of minimizing the error ei, we might as well maximize the absolute

69
00:05:26,380 --> 00:05:30,540
value of this inner product.

70
00:05:30,540 --> 00:05:36,340
That means that the choice of the next atom in the OMP can be done in this way.

71
00:05:36,340 --> 00:05:42,060
Take a transpose and multiply by the current residual rk minus 1 and take the absolute

72
00:05:42,060 --> 00:05:43,460
value.

73
00:05:43,460 --> 00:05:49,380
The resulting vector is of length m and its maximal absolute entry points to the atom

74
00:05:49,380 --> 00:05:52,420
to be chosen.

75
00:05:52,420 --> 00:05:57,780
Focusing on the step of updating xk, this is a least squares computation over the portion

76
00:05:57,780 --> 00:06:00,340
of the whole vector x.

77
00:06:00,340 --> 00:06:05,700
Given the support as k, we are to extract only the green columns and solve for the green

78
00:06:05,700 --> 00:06:08,060
entries in the vector x.

79
00:06:08,060 --> 00:06:13,820
Thus, this amounts to a simple least squares with k unknowns and the solution is given

80
00:06:13,820 --> 00:06:18,020
by the pseudo inverse of as times b.

81
00:06:18,020 --> 00:06:21,740
Why is this algorithm called orthogonal matching pursuit?

82
00:06:21,740 --> 00:06:26,660
The term matching refers to the correlations we apply between the residual and the atoms

83
00:06:26,660 --> 00:06:29,780
in A in order to find the next atom.

84
00:06:29,780 --> 00:06:35,020
By orthogonal, well, in the least squares we have just discussed, the optimal solution

85
00:06:35,020 --> 00:06:39,700
can be found by the derivative of the L2 error with respect to x.

86
00:06:39,700 --> 00:06:45,660
This leads to this expression, as transpose, multiplying this yellow term.

87
00:06:45,660 --> 00:06:49,540
The yellow term is nothing but the new updated residual.

88
00:06:49,540 --> 00:06:54,980
This means that after updating the solution xk, the inner product of the new residual

89
00:06:54,980 --> 00:07:00,580
is orthogonal to the atoms in the support, being the bros of AS transpose.

90
00:07:00,580 --> 00:07:06,140
This orthogonality is an asset, because it implies that once an atom has been chosen,

91
00:07:06,140 --> 00:07:13,740
it will never be chosen again, since its inner product with the residual is zero.

92
00:07:13,740 --> 00:07:17,980
Still on the matter of the least squares that updates xk, there is an effective numerical

93
00:07:17,980 --> 00:07:20,460
shortcut worth mentioning.

94
00:07:20,460 --> 00:07:26,100
The regular solution involves an inversion of a gram matrix of size k by k computed for

95
00:07:26,100 --> 00:07:27,820
the matrix ASk.

96
00:07:27,820 --> 00:07:34,700
However, one step before we inverted a similar matrix of size k minus 1 by k minus 1 that

97
00:07:34,700 --> 00:07:39,060
referred to ASk minus 1 with one atom less.

98
00:07:39,060 --> 00:07:43,020
Could we leverage the previous result in building the new one?

99
00:07:43,020 --> 00:07:45,260
The answer is positive.

100
00:07:45,260 --> 00:07:50,940
A new column has been added to create ASk, and along with it, a new scalar unknown has

101
00:07:50,940 --> 00:07:53,220
been added to the vector x.

102
00:07:53,220 --> 00:07:57,700
There is a recursive method for solving sequence of growing least squares problems of this

103
00:07:57,700 --> 00:08:04,580
form exactly, which implies that we do not need to invert any matrix in the OMP.

104
00:08:04,580 --> 00:08:09,060
We will not discuss this numerical shortcut further here.

105
00:08:09,060 --> 00:08:13,820
To summarize, the OMP involves two main steps of computations.

106
00:08:13,820 --> 00:08:17,940
The first is the sweep stage that searches for the next atom to add.

107
00:08:17,940 --> 00:08:23,540
This requires the multiplication of A transpose by the residual vector, thus requiring mn

108
00:08:23,540 --> 00:08:25,180
operations.

109
00:08:25,180 --> 00:08:29,420
The second is the least square step that updates xk.

110
00:08:29,420 --> 00:08:35,940
The main effort here is in computing AS transpose times AS, but there are various shortcuts

111
00:08:35,940 --> 00:08:38,380
that can be applied here.

112
00:08:38,380 --> 00:08:44,060
From line, the overall number of operations that OMP requires is on the order of k times

113
00:08:44,060 --> 00:08:48,260
m times n, where k is the cardinality of the final solution.

