start	end	text
0	2000	Imagine you're my professor.
2000	4000	Maybe you actually were my professor,
4000	8000	in which case you may already be sweating before I say anymore.
8000	10000	The subject matter is neural networks.
10000	15000	You draw an illustration on the board with a node's inputs and its outputs via transfer function.
15000	19000	You inform us of this mathematical fact that the transfer function cannot be linear,
19000	24000	or the whole model would reduce to a linear function.
24000	26000	I immediately raise my hand.
26000	29000	The speed with which I raise it and the not very subtle forward pose
29000	33000	suggests that I want to pluck an abstract idea from the whiteboard and pervert it.
33000	36000	You know this look, and you are reluctant to call on me.
36000	38000	But no other students are raising their hands.
38000	40000	You have no choice.
40000	41000	Tom.
41000	43000	It's more like a statement than a question.
43000	46000	It includes the tone of spoken punctuation that, if it could,
46000	49000	ends the entire conversation before it begins.
49000	52000	I go on and on about some technicality.
52000	56000	That due to approximate math on the computer, this mathematical fact won't be true.
56000	61000	You say, okay, technically that's right, but for all practical purposes it doesn't matter.
61000	65000	And I say, well, what about impractical purposes?
65000	69000	And you, in a moment of weakness, vigorously strangle me.
69000	71000	And that's how I died.
71000	74000	Murdered in cold blood.
74000	79000	That was about 20 years ago, but the world will not let us stop thinking about neural networks.
79000	82000	We're really just pressing all the gas pedals at once on this one,
82000	85000	heading towards a utopia or a dystopia.
85000	88000	Some kind of topia, for sure. We're getting there real fast.
88000	90000	So this question has been on my mind for some time.
90000	92000	And just to be clear, the professor is right.
92000	96000	I might be technically correct here, but it doesn't matter for practical purposes.
96000	100000	But I like to work at the intersection of theory and impractice.
100000	103000	And so by doing a lot of work, we can make it matter.
103000	106000	And then I'll be even more right, both theoretically right,
106000	109000	and it will only matter for most practical purposes.
109000	112000	So in this video, in its lengthy accompanying technical report,
112000	116000	I have an exhaustive exploration of what you can get away with.
116000	120000	And I'll see how we can absolutely use linear transfer functions in neural networks
120000	123000	and all sorts of other things where they shouldn't be enough.
123000	126000	I'm Tom Seven, and this is Impractical Engineering.
126000	130000	Okay, let's repeat the professor's lesson so we can understand the nature of the dispute.
130000	133000	If you feel like you already know everything about neural networks,
133000	136000	this section is safely skippable, but so is the whole video.
136000	141000	So fundamentally, a neural network takes in inputs, which are a bunch of numbers,
141000	145000	and transforms those numbers, and then outputs some other numbers.
145000	148000	In this drawing, I have three inputs and one output.
148000	153000	So every one of these circles is going to be filled in with some number as we run the network.
153000	157000	So call the inputs x, y, z, and let's just look at how r is computed.
157000	158000	That's that middle one.
158000	161000	We start with a weighted sum of x, y, and z.
161000	165000	So we take all the inputs, we multiply each one by some weight, and add those together.
165000	168000	And these weights are determined when we train the network.
168000	169000	At this point, they're just constants.
169000	171000	When we're running the network, they're just constants.
171000	175000	We also learn a bias parameter, which becomes a constant, and that just gets added in as well.
175000	178000	The important part for today is this transfer function, tf.
178000	183000	This gets applied to the weighted sum, and it transforms it, in this case, with a sigmoid.
183000	188000	And the intuition here is somehow that this node r, this neuron r,
188000	190000	fires with some probability.
190000	192000	That depends on its connection with these other neurons.
192000	197000	But because it's a probability, it ranges from zero to one
197000	199000	instead of, like, negative infinity to infinity.
199000	203000	And so the more the input ones fire, the more likely this one is to fire.
203000	205000	That was the classic idea, anyway.
205000	210000	These days, pretty much everyone uses the rectified linear transfer function.
210000	214000	It's super simple to implement, and for various reasons, it actually works better,
214000	216000	especially for the internal layers.
216000	218000	And actually, all sorts of functions will work here.
218000	221000	It needs to be differentiable because of the way we train these things.
221000	226000	But the only other apparently necessary quality is that the function be nonlinear.
226000	228000	At least so says the professor.
228000	230000	Now, the reason for this is mathematically nice.
230000	232000	Let's look at the formula for r again.
232000	238000	And let's say the transfer function is linear, so it's like mx plus b2.
238000	242000	Then you can multiply m by all these terms, and you get another linear function.
242000	247000	So r is a linear function of the inputs, and then so is q, and then so is s.
247000	250000	And then o is a linear function of q, r, and s.
250000	255000	And what this would mean is that the output would just equal some linear function of the inputs.
255000	258000	And all of this complexity of the neural network would just simplify away.
258000	260000	We wouldn't need any of the hidden layers.
260000	262000	We would just have a function of the input layer.
262000	267000	There are lots of functions such as XOR that can't be approximated by linear functions like this.
267000	273000	We definitely want our neural networks to be able to model things that are complicated, like XOR or human thought.
273000	275000	So the story goes.
275000	278000	So that would be true if we were using real math.
278000	283000	On a computer, we're going to use IEEE floating point, which isn't associative or distributive.
283000	287000	So if we simplify the whole network, we won't actually get the same result.
287000	294000	So my goal today will be to create a transfer function that, despite being mathematically linear, will not be computationally linear.
294000	298000	And thus, I'll be able to use it to train models that have interesting behavior.
298000	302000	Now, my smart math friend Jason, who probably makes the professor sweat even more,
302000	308000	reminds me that this is actually an affine function because I add something at the end.
308000	309000	That's fine. I'm going to call it linear.
309000	313000	He refers to this as high school linear in a pejorative way, and that's fine.
313000	317000	I'm comfortable with that. In a lot of ways, I'm mentally still in high school.
317000	323000	So this means two operations, addition and multiplication by constants or scaling.
323000	325000	Them's the rules.
325000	330000	Or equivalently, would it simplify mathematically to a polynomial of at most degree one?
330000	334000	So 3x plus 2x all times five, that would simplify.
334000	338000	But 2x times x would yield 2x squared, and that's a degree two polynomial.
338000	341000	So that's disallowed. Okay?
341000	348000	Floating point comes in a number of different spice levels, corresponding to how many bits you're using to represent it.
348000	353000	So you may be familiar with double and float. Those are 64 and 32 bits.
353000	356000	Half precision is 16 bits, and it gets even lower.
356000	361000	It's usually used because then you need half as much memory to store your numbers.
361000	367000	That'll be good for us. We'll be happy to save the memory, but the real reason to use half precision is that it is less precise.
367000	371000	And imprecision is going to be a desirable quality in this work.
371000	376000	Being only 16 bit, there are 65,000 different values that we could represent.
376000	378000	So it's clearly not all of the numbers.
378000	385000	This is an exponential format, so the main thing to remember about floating point precision is that there's more numbers near zero than elsewhere.
385000	393000	So when you get to the largest finite numbers, like 65,504, only multiples of 32 are even representable.
393000	397000	Between 2048 and 4096, only even numbers are there.
397000	399000	Below that, only integers.
399000	402000	And actually, most of the action happens near zero, where you get a lot of fractions.
402000	406000	Now this stuff about comparing against epsilon is okay, but it's kind of naive.
406000	409000	Like for one thing, what is epsilon supposed to be?
409000	412000	If you're working with really small numbers, you can use a really small epsilon.
412000	419000	But if you're working with larger numbers, you might need to use an epsilon of up to 32, or maybe half that, for half precision.
419000	423000	Actually, a while back, I wrote a paper called What if Anything is Epsilon?
423000	429000	Where I looked at what programmers picked in practice for their value of epsilon by going through a whole bunch of code on GitHub.
429000	434000	I enjoyed laughing at their bugs, like minus 1e10, which is negative 10 billion.
434000	438000	They meant 1e-10, 1 over 10 billion.
438000	440000	And I like to compare these by language.
440000	445000	For example, I found that JavaScript programmers were the most tolerant of error, which makes sense.
445000	446000	They're pretty sloppy.
446000	449000	One programmer picked 6 million on purpose, that wasn't a typo.
449000	453000	Well, a lot of stuff will be equal if you use a really large epsilon.
453000	456000	But anyway, the error you get is not like random.
456000	458000	These have a much more useful definition.
458000	465000	If I take two numbers and add them using IEEE floating point, the answer is defined to be the real math answer, x plus y,
465000	468000	but rounded to the nearest representable floating point number.
468000	470000	So if the result is high, I might need to round a lot.
470000	474000	If the result is small, I might need to round a small amount.
474000	476000	I also might not need to round at all.
476000	481000	Like 2 plus 2 is literally equal to 4 exactly in floating point, like you'd want.
481000	483000	And we're going to use that kind of thing later.
483000	487000	So the most important thing to remember about floating point for this project is,
487000	491000	rounding error depends on how far you are out on the number line.
491000	495000	Large error for large numbers, small error for small numbers.
495000	497000	But it also depends on the specific value.
497000	499000	Not everything needs to be rounded.
499000	504000	Surrounding is a complicated function, and we're going to abuse that complexity in order to get behavior that we like.
504000	509000	Let's look at how we can start abusing the imprecision of floating point numbers.
509000	513000	We just have addition and multiplication by constants, so we're going to try both of those.
513000	520000	Here's what happens if I add 128 to the input and then subtract 128 back out.
520000	524000	Mathematically, of course, this is just the identity function. I get back the input.
524000	528000	But because there aren't that many values representable near 128,
528000	532000	we can only get back eight different values between 0 and 1.
532000	536000	Note that the zigzag is finer in the negative region than in the positive region.
536000	540000	If we look at the actual numbers that are representable near 128,
540000	545000	we see that we have eighths above 128, but sixteenths below 128.
545000	548000	So we get more precision in the negative region.
548000	553000	Okay, that's plus. It's definitely not a line, but it's basically just a line.
553000	556000	Also, this thing has a terrible derivative. It just has all these flat segments,
556000	560000	so the derivative there is 0 and it's undefined at the discontinuities.
560000	563000	So it's going to actually be pretty hard to use as a transfer function,
563000	565000	but we're going to try it out anyway.
565000	567000	Now multiplication looks a lot more subtle.
567000	570000	Here I'm multiplying by 100 and then by 1 over 100,
570000	575000	which also should give me back just the identity f of x equals x.
575000	578000	So if we zoom in on this one, we'll start to see some detail.
578000	581000	Actually, maybe that's just my laser printer.
581000	583000	Well, that's the problem with using imprecise tools.
583000	585000	Maybe we should do this on the computer.
585000	587000	Okay, here we are on the computer.
587000	590000	And if I zoom in on this line, and I got to zoom in a lot,
590000	593000	near zero, it's pretty much perfect.
593000	595000	But as we get near one, it gets a lot more jacked.
595000	599000	And this is real imprecision, and it depends where you are on the number line.
599000	601000	You get different rounding error.
601000	604000	This is actually pretty hard to reason about, to be honest.
604000	607000	So just suffice to say, when you multiply,
607000	609000	you get a little bit of error all throughout the number line,
609000	611000	but it depends on where you are.
611000	613000	So that's multiplication.
613000	615000	Now we can try to put these together in various ways.
615000	618000	So I play with this a lot,
618000	622000	and I produced a whole bunch of just totally bonkers functions.
622000	625000	But actually the best shape that I was able to make
625000	627000	came from just using multiplication.
627000	631000	And what I do is I multiply by the first number that's smaller than 1.
631000	633000	So it's just slightly less than 1.
633000	636000	That's 1 minus 1 over 2048.
636000	638000	I keep multiplying by that over and over again.
638000	641000	And as I do, I accumulate more and more error in different parts of the graph.
641000	643000	Of course, I'm also making the number smaller.
643000	647000	So at the end, I want to normalize back so that f of 1 is 1.
647000	649000	Here's what it looks like if I do that iteratively.
649000	651000	Accumulating error.
651000	653000	I found that 500 steps was a good stopping point.
653000	656000	So this is a function that I call grad1.
656000	658000	Grad for the name of this project, which I can't pronounce.
658000	661000	Gradient half decent.
661000	665000	I triply half gradient descent, you get it.
665000	668000	And it has this nice zigzag shape.
668000	670000	Importantly, it's kind of smooth.
670000	673000	If we zoom in on it, we'll see that it's, you know, putting aside the pixels,
673000	675000	that it's piecewise linear.
675000	677000	So that's nice.
677000	680000	Now you might wonder, why does it get this zigzag shape?
680000	683000	And truth be told, round-off error is just kind of hard to reason about.
683000	685000	Let me show you two illustrations that are at least nice to look at.
685000	689000	In this rainbow, I've given each of the numbers between 0 and 1 a color on the x-axis.
689000	693000	And then on the y-axis, I'm successively multiplying by that constant.
693000	697000	And you could see that they get exponentially smaller as expected, but not smoothly.
697000	699000	And these changes in direction come from different exponents,
699000	702000	and we see some of that reflected in the zigzags.
702000	704000	On this image, the x-axis is 0 to 1 again.
704000	707000	The y-axis is successive multiplication by the constant.
707000	710000	But the green pixels is when my round-off is too high compared to the correct result,
710000	712000	and magenta when it's too low.
712000	714000	This line at the top is at 500 iterations.
714000	718000	And you can see how it slices both green and magenta regions.
718000	720000	Too high and too low.
720000	724000	One more thing, in order to train models with this thing, we need to know it's derivative.
724000	727000	And for reasons of implementation tricks that I'm not going to get into,
727000	731000	I actually need the derivative in terms of the y-coordinate instead of the x-coordinate.
731000	734000	Now, I'm not good enough at math to figure this out analytically.
734000	738000	In any way, it would probably just be a table of the values for these different segments.
738000	742000	But since it's 16-bit and there's only 65,000 values that are possible,
742000	746000	I can just use a computer program to compute the derivative for every point.
746000	748000	So here that has plotted along the y-axis.
748000	750000	I think it looks pretty cool like an oscilloscope.
750000	753000	You'll notice that the derivative isn't a perfect square wave,
753000	759000	and it wouldn't be because there are in fact little imperfections in this curve from round-off error.
759000	762000	I'm actually applying a low-pass filter here, it would be even noisier.
762000	765000	But anyway, now we've got the function and we've got its derivative,
765000	768000	so we can do some machine learning.
768000	771000	But first, a bonus digression.
771000	773000	Here's a bonus digression.
773000	778000	Having freed myself from the need to quote-unquote do math in order to differentiate functions,
778000	781000	because I'm just going to generate a table programmatically,
781000	784000	I can now consider all sorts of exotic transfer functions.
784000	788000	I can even betray the central thesis of this work and consider functions that are not linear.
788000	793000	One thing I think is really funny is when you use data sort of as the wrong type,
793000	797000	you may be familiar with the fast inverse square root technique.
797000	802000	I love that one, and I think it's worth considering if a transfer function even needs to use floating point operations
802000	804000	in order to be implemented.
804000	807000	I tried to find the fastest, simplest thing you could do that might work.
807000	810000	My favorite was to treat the float as just 16 bits,
810000	814000	shift them down by 2, and then treat that back as a float.
814000	817000	For integers, shifting by 2 is just division by 4.
817000	820000	But for a floating point number, since there are different fields within the word,
820000	822000	this moves bits between fields.
822000	826000	So for example, the sine bit gets moved into the exponent,
826000	831000	which means you have a much larger exponent for negative numbers than for positive ones.
831000	834000	The result will always be positive because we fill with zeros.
834000	837000	Dividing the exponent by 4 has a logarithmic effect on the result,
837000	840000	and then some of the exponent bits also go into the mantissa.
840000	843000	So you get a kind of crazy function that looks like this.
843000	847000	The negative values are much larger, as we said, and it logarithmically approaches zero.
847000	852000	The positive region is actually a very small upward slope, which you can't see on this graph.
852000	857000	But since the exponent will start with two zeros, these tend to be pretty small.
857000	861000	This is the full range that's explored by the positive values,
861000	866000	and you probably don't care, but here is its computed derivative in terms of the y-coordinate.
866000	870000	So in the experiments which are coming up next, I'm going to also compare this transfer function.
870000	874000	This wouldn't prove the professor wrong because it uses a forbidden operation,
874000	877000	but it is about as fast as you could do anything on a computer.
877000	881000	So if it does turn out to work, it might be a half-decent choice.
881000	886000	To compare the transfer functions, I tried them out on different machine learning problems.
886000	891000	Fortunately, I do have my own bespoke GPU-based system for training neural networks,
891000	896000	which has appeared on this channel before in videos such as 30 weird chess algorithms.
896000	900000	It's not that good, but it is the kind of thing you want if you're going to do a silly experiment like,
900000	902000	what if deep learning, but worse?
902000	906000	So I made a bunch of modifications for this project, for example, to do the forward step with half-precision,
906000	909000	and to support these tabled transfer functions.
909000	914000	Then I trained a network using the same structure and initialization, changing only the transfer function.
914000	918000	The first problem is the MNIST digit recognition dataset.
918000	922000	The original CAPTCHA, you get 50,000 labeled examples of these tiny digits,
922000	925000	and you have to learn to predict the digits 0 through 9,
925000	928000	and then there are 10,000 held-out examples to judge your accuracy on.
928000	931000	I chose this classic problem partly for trollish reasons,
931000	933000	because even at the time of publication decades ago,
933000	937000	various techniques had already achieved extremely high accuracy.
937000	939000	The networks I trained looked like this.
939000	943000	They take in the input pixels, then there's a number of internal layers,
943000	947000	and then a dense output layer with one output for each of the 10 digits.
947000	951000	You can see the paper, the code for details, if you really want.
951000	956000	But one important thing I want to point out for these experiments is that the output layer uses a strict linear transfer function,
956000	958000	the identity, for each of the models.
958000	962000	It's not a good choice for these categorical problems, but it allows the network to output any value,
962000	966000	even if the transfer function, for example, only outputs positive numbers.
966000	969000	And since it's linear, it complies with our goal of proving the professor wrong.
969000	973000	Throughout the rest of the network, all the internal layers use the transfer function that we're studying.
973000	977000	So I trained one of these models with the same initial conditions and the same data,
977000	979000	but using a different transfer function.
979000	982000	I do that for 200,000 rounds, which takes about a day each.
982000	986000	We can then compare the final accuracy and other dimensions, such as their aesthetics.
986000	988000	The functions are as follows.
988000	992000	We have two classic sigmoids, the hyperbolic tangent and the logistic function.
992000	995000	After that, the rectified linear unit.
995000	999000	Here I'm using a leaky version where the region below zero actually has a small slope.
999000	1001000	That seems to work better for me.
1001000	1003000	This function is very popular today.
1003000	1006000	None of those functions are linear, as expected by the professor.
1006000	1009000	Then we have a couple that abuse floating point round off error.
1009000	1010000	First a really simple one.
1010000	1012000	I add 64 and subtract 64.
1012000	1014000	We saw how that discretizes the line.
1014000	1021000	Then the grad one function, which multiplies by a number near one 500 times in order to smoothly magnify the round off error.
1021000	1025000	Then we have our bonus content, downshift two, which manipulates bits directly.
1025000	1031000	Finally, we'll evaluate the identity function, which is what the professor thinks a linear model must be equivalent to.
1031000	1034000	On the MNIST problem, all of the transfer functions do well.
1034000	1037000	As expected, the classics are nearing 100% accuracy.
1037000	1042000	Even a simple linear model using the identity function gets like 82%.
1042000	1047000	Plus 64, which gets a little bit of non-linearity with round off error outperforms it slightly.
1047000	1051000	But the nice smooth grad one function is almost in the same class as the classic functions.
1051000	1053000	It's working quite well.
1053000	1058000	So it seems like our hypothesis is panning out, and I can sense the professor beginning to sweat.
1058000	1061000	The next problem is the SIFAR-10 dataset.
1061000	1066000	This is a lot like MNIST, but instead of recognizing digits, you have to recognize spirit animals.
1066000	1068000	There are 10 spirit animals.
1068000	1074000	Airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.
1074000	1075000	This problem is much harder.
1075000	1079000	With these tiny thumbnails, I sometimes can't figure out what it is.
1079000	1080000	But same idea.
1080000	1082000	Days later, we get results.
1082000	1086000	The ranking here is the same for this problem, and we can draw basically the same conclusion.
1086000	1090000	An accuracy of 53 doesn't sound that good, but keep in mind there are 10 different classes.
1090000	1093000	So if you just guess randomly, that's an accuracy of 10%.
1093000	1095000	So we are substantially learning here.
1095000	1099000	But another way to understand our accuracy is to compare it to what's come before us.
1099000	1104000	These are standardized problems, and so a lot of researchers have posted their results.
1104000	1108000	So I can check the leaderboard and scrolling all the way down to the bottom.
1108000	1114000	I can see that my results are, in fact, the worst result of all time.
1114000	1115000	That's not too bad.
1115000	1118000	Last place is the last winner.
1118000	1123000	Now putting aside the aesthetic and ideological considerations, there is something to this.
1123000	1128000	Recently, I feel like deep learning is getting a little too good, a little too fast.
1128000	1131000	So maybe we could just slow it down a bit.
1131000	1134000	The third problem is, of course, chess.
1134000	1140000	I take millions of positions from a public database, and I ask Stockfish a strong chess engine, which side is winning?
1140000	1142000	There's two classes of results here.
1142000	1147000	One side could have an edge, and it's standard to give this advantage in terms of pawns.
1147000	1155000	Or it could be of the form mate in negative 24, which means that black, because it's negative, has a mate in 24 moves, no matter what white does.
1155000	1159000	Mate is always favorable to a mirror edge, no matter how big your advantage is.
1159000	1166000	One thing that's funny to me about this pawn score is that the advantage can be sort of arbitrarily high if Stockfish can't find a mate.
1166000	1171000	So it can give you more than 64 pawn advantage, which is funny because how are you even going to fit those on the board?
1171000	1172000	Dual wheeled?
1172000	1174000	Actually, here's an exercise for the reader.
1174000	1180000	Find a position with the largest possible advantage according to Stockfish, but where it can't find mate.
1180000	1182000	Here's the best that I could do.
1182000	1186000	Huge advantage for white, but no mate even after depth 89.
1186000	1194000	I mapped these scores into the interval from negative 1 to 1, where negative 1 is the best possible result for black, me and 1, and plus 1 is the same for white.
1194000	1198000	And this gives me a machine learning problem, which is to learn how Stockfish rates each board.
1198000	1205000	For the models that score chess positions, we could compare their predictions directly to Stockfish to understand how accurate they are.
1205000	1210000	And I did that, but I think it's more fun to use these models to create chess players and then have them play against each other.
1210000	1217000	These players just look at each legal move and score the resulting board and then take the move that would be most favorable to their side.
1217000	1219000	So there's no game tree search here.
1219000	1221000	Here are the results of a tournament.
1221000	1226000	The rows are each player as white and the columns as black, and they're ordered by their final ALO rating.
1226000	1230000	There's some complexity here, but the fixed versions are the ones to look at.
1230000	1239000	As usual, the rectified linear unit is performing the best, but our quote-unquote linear transfer function, grad 1, is actually in second place and not far behind.
1239000	1247000	It's close to being as good as Chessmaster for the Nintendo Entertainment System, and outperforms Stockfish deluded with half random moves.
1247000	1254000	This is actually pretty impressive given that it's doing no game tree search, it's just using its intuitions about what boards are good.
1254000	1258000	Of course, raw performance on these problems is not the only thing.
1258000	1262000	We ought to think about the speed of the function, as well as its aesthetics.
1262000	1265000	Some of them have nice shapes, and others look dumb.
1265000	1270000	Since training takes days, where all you have to do is stare at graphs of activations,
1270000	1276000	whether those look cool, or boring, or vaporwave, also bears some consideration.
1276000	1280000	The key finding here is that the professor was wrong.
1280000	1286000	You absolutely can use a linear transfer function, as long as you don't need it to be both good and fast.
1286000	1287000	Defeated.
1287000	1290000	Having gotten my revenge, we could stop there.
1290000	1295000	But when have huge breakthroughs in science and technology ever happened by stopping there?
1295000	1298000	So, it's on to the next level.
1298000	1301000	So far, all the functions we've considered have been monotonic.
1301000	1306000	That's because both plus and multiplication, even when you round, have this property.
1306000	1308000	But we're certainly not limited to this.
1308000	1314000	For example, if x appears multiple times under addition or subtraction, we can get much more interesting functions.
1314000	1318000	Another way to look at this is interference patterns between linear functions are linear.
1318000	1325000	For example, x minus 4,096 minus x plus 4,096 is linear.
1325000	1327000	It's mathematically equal to zero.
1327000	1331000	But in half precision floating point, it produces this square wave function.
1331000	1334000	Now, this function isn't as well behaved as it looks.
1334000	1340000	One of those intervals is width one exactly, but the other is very slightly smaller than one.
1340000	1343000	And again, this has to do with perversities of roundoff error.
1343000	1350000	Or if we take that grad one function that we've studied and subtract x from that, we get this nice triangle wave.
1350000	1354000	By stringing functions together, we can make all sorts of interesting patterns.
1354000	1359000	In fact, if we have any shape in mind, we can try approximating it with one of these functions,
1359000	1362000	subtract it from the desired shape to get a new shape,
1362000	1365000	and as long as we're getting smaller, we can just keep doing this,
1365000	1369000	successively approximating that shape like a Taylor series.
1370000	1374000	So if we could make any shape, let's make a fractal. Those are good shapes.
1374000	1377000	The Mandelbrot set is the radiohead of fractals.
1377000	1382000	Here we're going to use complex numbers, and we use a coordinate system where the x-coordinate is the real part,
1382000	1385000	and the y-coordinate is the imaginary part.
1385000	1388000	For any given point C, we repeatedly square and add,
1388000	1391000	and this point moves around in a crazy way.
1391000	1394000	And based on how quickly it converges or diverges, we give it a color.
1394000	1400000	Boom. 2D Mandelbrot.
1400000	1404000	Now adding C is linear. Squaring, however, is not,
1404000	1408000	but we just said we can approximate any shape using interference patterns.
1408000	1414000	So here's a rough approximation of f of x equals x squared, using only linear operations.
1414000	1417000	So this has some funny business near the origin,
1417000	1421000	but you might think we could use this to plot a kind of perverted Mandelbrot.
1421000	1424000	Unfortunately, if we try, we get this piece of garbage.
1424000	1426000	This stupid blotch sucks.
1426000	1430000	To understand why, we need to look at the definition of squaring for complex numbers.
1430000	1434000	When we multiply this out, the A and the B get mixed together.
1434000	1438000	The real part has some A in it, and because I squared is negative 1,
1438000	1442000	some B in it, and the imaginary part also has some A and some B in it.
1442000	1445000	So we get this cross-pollination, and that means x and y-coordinates are mixed,
1445000	1447000	and you get a kind of weird rotation.
1447000	1451000	But let's look at the purely linear operations on complex numbers.
1451000	1456000	For both plus and scaling, the real parts stay real,
1456000	1460000	and the imaginary parts stay imaginary, with no cross-pollination.
1460000	1463000	So no matter how we use these operations, even with floating-point roundoff,
1463000	1466000	we're not going to get any mixing between the coordinates.
1466000	1469000	And that's why the fractal has these rows and columns of sameness.
1469000	1473000	It's really just two independent functions, one on the x-axis and one on the y-axis.
1473000	1478000	So professors take note, the complex numbers do provide some refuge.
1478000	1481000	It's time for another bonus digression.
1481000	1484000	You might think you could just make a 3D Mandelbrot.
1484000	1486000	Just do the same thing we did before,
1486000	1488000	but with numbers that have three components,
1488000	1492000	a real part and imaginary part and, like, a very imaginary part.
1492000	1496000	If you try it, this old professor of Frobenius will come along
1496000	1499000	and educate you with this cool math fact.
1499000	1504000	No matter what you do, any three-dimensional algebra is equivalent to the real or complex numbers,
1504000	1508000	so it's like you didn't do anything at all, or not associative,
1508000	1510000	meaning the order of operations will matter.
1510000	1513000	But you know what else isn't associative?
1513000	1515000	The floating-point numbers, my dude.
1515000	1519000	So it seems we don't need associativity to make fractals anyway.
1519000	1525000	Enter the baffling numbers, which is an ill-advised generalization of the complex numbers, to three dimensions.
1525000	1528000	Yes, it won't work, but we can just do it.
1528000	1530000	Frobenius can't stop me.
1530000	1534000	And I can use this to make a 3D fractal called the bafflebrot.
1534000	1538000	Here it's sliced in half, showing a perfect ripe Mandelbrot inside.
1538000	1543000	The resulting 2-gigabyte file crashes every piece of software I throw at it.
1543000	1544000	I admire its spirit.
1544000	1547000	Boom, 3D fractal.
1547000	1549000	Bezeked.
1549000	1552000	We don't actually need squaring to create fractals, though.
1552000	1554000	We just need something kind of chaotic.
1554000	1558000	I just take this function, which consists of 36,000 linear operations,
1558000	1563000	and I iterate it, adding C each time, and plot the resulting magnitude.
1563000	1564000	I think it looks pretty nice.
1564000	1567000	I think this is a fractal, in the sense that it is chaotic.
1567000	1571000	It has a color gradient, and could be on the cover of an electronic music album.
1571000	1576000	It is not a fractal, in the sense that if you zoom in on it, you get infinite detail of self-similar shapes.
1576000	1579000	In fact, as we zoom in on it only a modest amount,
1579000	1583000	we see rectangular pixels as we reach the limits of half-precision floating-point.
1583000	1586000	And because this fractal is built by abusing those very limits,
1586000	1589000	it's not even possible to get more detail by increasing the accuracy.
1589000	1594000	Alright, drawing fractals is fun and everything, but it's not really a game you can win.
1594000	1597000	There's no goal other than to make a cool picture.
1597000	1600000	So next, I turn to something with a clearer challenge to overcome.
1600000	1602000	Linear cryptography.
1602000	1605000	Cryptography is fractals minus drugs.
1605000	1607000	You take some data and mess it up,
1607000	1610000	but in a way where you can get it back again if you want.
1610000	1615000	Possibly the most fundamental building block of cryptography is the pseudo-random number generator.
1615000	1618000	This is a function that takes in a state, like a 64-bit integer,
1618000	1622000	and returns a new state that, quote-unquote, looks random.
1622000	1626000	With one of those, you can generate a hash function by mixing it with some input data,
1626000	1629000	or a symmetric block cipher using a Feistel network.
1629000	1631000	So naturally, I want one of these.
1631000	1636000	Now, another thing that professors will tell you is that cryptographic algorithms cannot be linear.
1636000	1641000	Here, linear includes within some modular ring like integers mod 256, the bytes,
1641000	1643000	or mod 2, the bits.
1643000	1648000	So in contrast, even though we said before that XOR can't be modeled by linear function on reels,
1648000	1651000	XOR is considered linear in this context.
1651000	1654000	The reason for that is linear cryptanalysis.
1654000	1656000	If your function is even a little bit linear,
1656000	1661000	then with a large collection of input-output pairs, like messages and their encrypted versions,
1661000	1665000	you can deduce information about secrets like an encryption key.
1665000	1670000	So the standard advice to construct these things is to alternate linear operations like XOR
1670000	1673000	with nonlinear operations like substitution.
1673000	1677000	Substitution is make a table of all the bytes, but permute them randomly,
1677000	1679000	and then just do table lookup.
1679000	1682000	In fact, Bruce Schneier writes in the big red book,
1682000	1685000	substitutions are generally the only nonlinear step in an algorithm.
1685000	1688000	They are what give the block cipher its security.
1688000	1692000	So of course, what we're going to do is prove this adage wrong by developing a good pseudo-random function
1692000	1696000	that only uses linear operations on half-precision floating-point numbers.
1696000	1698000	Now, what does it mean to be good?
1698000	1702000	This is less subjective than fractals, but it is still a little tricky.
1702000	1706000	We don't actually even know if pseudo-random number generators exist.
1706000	1710000	The best results assume that other problems are hard, but we don't have proofs of that either.
1710000	1714000	There's lots of stuff that looks random, but actually isn't, like it hides a backdoor.
1714000	1717000	Never forget that RSA security.
1717000	1724000	Yes, that RSA took a $10 million bribe from the NSA to hide a backdoor in one of their pseudo-random number generators.
1724000	1730000	Practically speaking, though, we can subject the function to a stringent battery of statistical tests,
1730000	1733000	and if it passes all of those, that's a really good start.
1733000	1738000	The function will work on half-precision floating-point numbers in the interval from negative one to one,
1738000	1741000	just like the transfer functions we've been considering so far.
1741000	1743000	Now, this is not a good choice.
1743000	1746000	It's unnecessarily hard, but all of this is unnecessarily hard.
1746000	1749000	Now, I have to work with 64 bits.
1749000	1753000	I could represent each bit as a half, but that makes it too easy.
1753000	1757000	So I'm going to represent it as 8 bytes, each byte represented by a half.
1757000	1763000	To represent a byte as a half, I'll divide the interval from negative one to one into 256 segments,
1763000	1768000	and I'll allow any floating-point value within that interval to represent the corresponding byte.
1769000	1776000	So anything from 124 over 128 to 125 over 128 will represent the number 252.
1776000	1779000	And again, allowing any number here is unnecessarily hard.
1779000	1782000	In the next section, we'll see a much better way to do this that's much faster.
1782000	1788000	But by struggling with this one, we'll at least demonstrate complete mastery over the sort of continuous domain.
1788000	1791000	So this function will take in 8 halves and return 8 halves.
1791000	1794000	And the crux of this function will be this substitution.
1794000	1798000	That's a table lookup where each of the 256 bytes is swapped for another byte,
1798000	1803000	or plotted as a function, each of these discrete intervals is mapped to a different interval.
1803000	1807000	The approach we used in the previous section of fitting functions doesn't work here.
1807000	1809000	We need something more exact.
1809000	1813000	So I study a family of well-behaved functions called choppy functions.
1813000	1816000	To be choppy, the function has to have a few properties.
1816000	1819000	For any value in an interval that represents some integer,
1819000	1822000	the function has to produce the exact same result,
1822000	1825000	and its output has to be the lowest value within some interval.
1825000	1828000	Of course, these functions can only use addition and scaling,
1828000	1832000	and since they're maximally permissive about what they accept and very strict about what they generate,
1832000	1834000	they'll be quite easy to reason about and compose.
1834000	1838000	In fact, we'll be able to think about them as functions from integers to integers.
1838000	1840000	So I went on a hunt for choppy functions.
1840000	1843000	I wish I could tell you that I cracked the code of how to make these from scratch,
1843000	1845000	but I found them by computer search.
1845000	1848000	Here's an example that I can't believe I'm going to write out by hand.
1848000	1850000	This function is mathematically linear.
1850000	1852000	It's actually equal to a constant.
1852000	1854000	The x's cancel out.
1854000	1859000	What this function does is return 1 if the input represents the number 249 or 0 otherwise.
1859000	1862000	So this is a pretty useful choppy function.
1862000	1865000	Since each of these represents a function from a byte to a byte,
1865000	1869000	I can think of it as just a table of the bytes that it produces for each input.
1869000	1873000	It's a little more complicated than this because the outputs are actually from negative 1 to 1,
1873000	1875000	but this is the basic idea.
1875000	1878000	So what I did is I generated a whole bunch of these kinds of functions.
1878000	1881000	And every time I get a new one, or a faster version of an old one,
1881000	1884000	I put it in a database keyed by these integers.
1884000	1888000	I can also take any two of them and get their difference by subtracting them.
1888000	1890000	That'll also be a choppy function.
1890000	1893000	So here's say they only differ in these two components.
1893000	1896000	And so I get 0's everywhere except for those two columns,
1896000	1899000	and this might give me a new choppy function I didn't have before.
1899000	1903000	Observe that if I ever find one that's 0 everywhere except for a single 1,
1903000	1908000	then I can use that to modify the column in any other vector to any value that I want.
1908000	1911000	So these are special, these are basis vectors.
1911000	1913000	So once I've done that, this column is kind of done,
1913000	1916000	and I never need to find new variations of that column.
1916000	1918000	So if I take a large collection of these choppy functions,
1918000	1923000	I can do a process kind of like Gauss Jordan elimination to deduce a set of basis vectors.
1923000	1926000	And if I find a basis vector for every column for every position,
1926000	1931000	then I can just add those up to make any choppy function I want, for example, our substitution.
1931000	1934000	So that's pretty nice, I just need to find these basis vectors.
1934000	1937000	You might think that once you had a single basis vector,
1937000	1940000	you could shift that column around, like to another position,
1940000	1943000	by just calling your function on a shifted version of x.
1943000	1945000	And in real mathematics, that would work.
1945000	1949000	But since these functions are abusing floating point roundoff error,
1949000	1953000	which depends on the specific value of x, this approach will not work.
1953000	1956000	You can do stuff to the output of the function like scale it or add to it,
1956000	1959000	and you can combine functions by taking their interference pattern.
1959000	1962000	But you can't straightforwardly manipulate the input side.
1962000	1966000	This problem is worst near the origin, where the precision is highest.
1966000	1969000	This meant that it was particularly hard to find a choppy function
1969000	1972000	that distinguished negative and non-negative numbers exactly.
1972000	1976000	In essence, the middle two columns of my vectors would always have the same value,
1976000	1978000	and so they wouldn't be independent.
1978000	1980000	I need to find some way to distinguish those two.
1980000	1985000	Going back to our earliest example, if we just add 128 and then subtract 128,
1985000	1988000	we do get different behavior for negative and positive numbers,
1988000	1992000	but if we look at the rounding near zero, a lot of negative values round up,
1992000	1994000	like small positive values round down.
1994000	1995000	And this makes sense.
1995000	1999000	If you add a small negative number to 128, you get 128.
1999000	2002000	So I hunted for the zero threshold function,
2002000	2004000	and there was a lot of manual fiddling with that.
2004000	2007000	But I did eventually find one, and it looks like this.
2007000	2009000	It's pretty involved.
2009000	2013000	One of the key things is to do a whole bunch of multiplications at the beginning,
2013000	2015000	since these will preserve the sign.
2015000	2019000	Spread values away from zero without causing any corruptive rounding
2019000	2022000	until you can do the same old loss of precision techniques
2022000	2025000	to make all the finite values the same on either side.
2025000	2029000	With that zero threshold function solved, I can now create a basis
2029000	2032000	and therefore create any function from a byte to a byte.
2032000	2035000	So back to our pseudo-random number generator.
2035000	2040000	The structure I'm going to use is a classic substitution permutation network.
2040000	2043000	It takes eight bytes in A through H,
2043000	2046000	and I apply the substitution function to each of the eight bytes.
2046000	2048000	Then I rearrange the bits,
2048000	2052000	apply a few more linear operations like modular plus and minus,
2052000	2054000	and then I have a new state as the output.
2054000	2057000	And by iterating this, you create a pseudo-random stream.
2057000	2059000	The substitution function we already talked about.
2059000	2064000	For permuting the bits, each of the output bytes depends on all of the input bytes.
2064000	2066000	So it's not a function of one variable,
2066000	2069000	but I can construct it from functions of one variable.
2069000	2072000	If I look at this first byte in the output, let's call it y,
2072000	2075000	and I can look at the first byte in the input of the permutation that's x.
2075000	2078000	Note that there's just a single bit that it reads.
2078000	2081000	Remember, I can create any function that I want of a single variable,
2081000	2086000	so I construct a function that returns 128 if that bit is set in the input of the y0,
2086000	2089000	and I do a similar thing for all the other bytes,
2089000	2091000	and then I can just add up those results,
2091000	2094000	and they all set different bits, so adding is like logical or.
2094000	2097000	That technique of adding independent things is really useful,
2097000	2098000	and we're going to use it more later.
2098000	2100000	The last piece is modular addition.
2100000	2103000	I have addition, of course, but on bytes it needs to wrap around
2103000	2106000	if the result is greater than 256, or in this case, greater than 1.
2106000	2111000	So if I add two of these values together, I get a result that might be as high as 2,
2111000	2114000	so it looks like this, but I want it to look like this.
2114000	2116000	Once it gets past 1, it should go back to negative 1.
2116000	2120000	Fortunately, I do have a way to test whether the value is greater than a threshold like 1.
2120000	2124000	So modular plus takes in two arguments and adds them together,
2124000	2127000	and that result might be either too low or too high.
2127000	2129000	We'll talk about the case that it's too high.
2129000	2134000	We test whether it's higher than 1 using the 0 threshold function,
2134000	2139000	which returns either 1 or 0, multiply that by 2, and then subtract it away.
2139000	2143000	So that allows us to add this corrective factor and put it back into the right range.
2143000	2146000	Now, I mentioned before that you can't necessarily shift around functions
2146000	2149000	because of loss of precision, but this will actually work for the 0 threshold function.
2149000	2151000	We're going to come back to that in a second,
2151000	2156000	but first I want to evaluate this random number generator to see how good it is.
2156000	2161000	In order to test this thing, I used a pre-existing suite of statistical tests called Big Crush.
2161000	2164000	This is like hundreds of tests that if you do things with the random numbers
2164000	2169000	that should have a correct mathematical result, you in fact get that mathematical result,
2169000	2171000	and not something that's a little biased.
2171000	2173000	It's really hard to pass these tests.
2173000	2175000	You can try it out on some handmade functions if you want.
2175000	2177000	It's pretty good at finding bias.
2177000	2181000	This test needs like 1.6 billion bits of input to do its thing,
2181000	2185000	so I actually ran it on an equivalent C implementation of this function,
2185000	2188000	but I also test that they produce exactly the same result.
2188000	2192000	Even with the C implementation, this takes days to run,
2192000	2195000	but it did, and it passes every single test,
2195000	2200000	so it's reasonable to believe that this function could be the basis of a decent encryption algorithm.
2200000	2202000	Defeated.
2202000	2208000	Now, one downside is that if you run this using the native half-precision implementation,
2208000	2212000	it produces 25.8 bytes a second of randomness, which is very slow.
2212000	2217000	Now, you can produce tables ahead of time so that each of those operations is just a 16-bit table lookup,
2217000	2222000	and then it'll produce 18.5 kilobytes per second, and that's still slow.
2222000	2227000	But if you were trapped on a desert island and all you had were linear floating-point operations,
2227000	2229000	I guess you could do worse than this.
2229000	2234000	Of course, if you're trapped on a desert island, I don't recommend encrypting your messages.
2234000	2237000	This is just not a good way to get rescued.
2237000	2240000	So I said I'd come back to this bit here.
2240000	2244000	We used the zero-threshold function to test if a sum was greater than one
2244000	2248000	so that we could implement modular arithmetic by subtracting off a corrective factor.
2248000	2252000	Once upon a time, I told you you couldn't just shift around the inputs to functions,
2252000	2256000	and this is true in general, but the zero-threshold function, because it operates at zero,
2256000	2262000	which is the most precise region for floating-point, actually does admit this behavior within a certain range.
2262000	2265000	If I have some value in mind, like 0.125,
2265000	2270000	and I want a function that tests whether the input is just greater than or equal to 0.125,
2270000	2272000	that looks like this,
2272000	2277000	and I can do that by just subtracting 0.125 from the input and passing it to the zero-threshold function.
2277000	2281000	And if the input is exactly 0.125, we get back exactly zero.
2281000	2285000	This works for most numbers, but there are some limits.
2285000	2288000	So on the y-axis here, we have different choices of threshold.
2288000	2293000	On the x-axis, we have all of the possible inputs, and this is all of the finite floating-point values.
2293000	2295000	The green region is where we get the right answer.
2295000	2297000	The red region is where it's wrong.
2297000	2302000	The only reason it's ever wrong is that we end up getting an infinite value during that computation.
2302000	2304000	Otherwise, this would all work out.
2304000	2306000	The green region is pretty big.
2306000	2309000	It always works out when the input and the threshold is exactly the same, because then you get zero,
2309000	2311000	and then you're not going to have any infinities.
2311000	2315000	But as they get farther from one another, the value you're testing is larger,
2315000	2318000	and therefore you're more likely to encounter infinities.
2318000	2321000	The highlighted region is everything from negative one to one,
2321000	2323000	which accounts for almost half of the finite numbers.
2323000	2326000	And you can see we've covered pretty much this entire interval.
2326000	2331000	There is this one corner, like a couple numbers that don't work.
2331000	2333000	But it's, I mean, we can do...
2333000	2334000	Okay.
2334000	2335000	Alright, fine.
2335000	2337000	I'll fix it.
2338000	2340000	Alright, now I can sleep soundly.
2340000	2342000	Here's a new version of the zero-threshold function,
2342000	2345000	which works on the entire negative one to one interval.
2345000	2347000	And more than that, in fact,
2347000	2349000	I found this with computer search again,
2349000	2351000	trying to maximize the size of the interval on which it works.
2351000	2353000	And basically it's the same as before,
2353000	2355000	but more careful about intermediate computations
2355000	2357000	so that it doesn't touch infinity by accident.
2357000	2360000	So now that I know that this works for every value in there,
2360000	2364000	I can actually use it to generate literally any function that I want on that interval.
2364000	2367000	The first step is to take this general-purpose greater-than function
2367000	2371000	and turn it into a general-purpose exact equals function.
2371000	2374000	I check whether the input is greater than or equal to the value,
2374000	2377000	but then subtract off a corrective factor.
2377000	2380000	If the input is greater than or equal to the next floating-point number,
2380000	2382000	that's this next after thing.
2382000	2384000	This returns one if the input is exactly v.
2384000	2386000	And then I just make an enormous expression.
2386000	2388000	There's only a finite number of floating-point inputs.
2388000	2391000	So for each one, I test whether it's exactly equal to that,
2391000	2393000	giving zero or one.
2393000	2396000	And I multiply that by the constant value that I want to have at that point,
2396000	2397000	the y-coordinate.
2397000	2400000	Then I sum those all up and it makes any shape that I like.
2400000	2401000	So that's great.
2401000	2403000	Linear functions can do anything.
2403000	2407000	And one thing I don't like about this is how big this expression is.
2407000	2409000	In some sense, that's funny,
2409000	2412000	but it's starting to look like this thing is turn-complete,
2412000	2414000	and I'd like to build a computer to demonstrate,
2414000	2416000	since that's what you do.
2416000	2420000	But I don't know, everything is slow turn-complete these days.
2420000	2423000	So I want to figure out how we could make it a bit more practical,
2423000	2427000	because I like to work at the intersection of theory and impractice and practice.
2427000	2430000	So I consulted my extensive computer science library
2430000	2433000	for performance-enhancing substances.
2433000	2437000	I found a relevant-looking article in the 2018 SIGBOVIC
2437000	2440000	called The Fluent 8 Software Integer Library,
2440000	2442000	by Jim McCann, he sounds smart,
2442000	2444000	and Tom Murphy V.
2444000	2448000	Wait, I already wrote this paper?
2448000	2450000	God damn it.
2450000	2452000	Yeah, this looks familiar.
2452000	2454000	Uh, man.
2454000	2459000	Well, the Fluent 8 library implements unsigned 8-bit integers,
2459000	2462000	using 32-bit floating point.
2462000	2464000	That sounds pretty familiar,
2464000	2467000	but it does make some different design decisions than what we're doing today.
2467000	2471000	One superficial difference is that it uses 32-bit full-precision floating point.
2471000	2473000	That's easy to change.
2473000	2475000	It also uses some nonlinear operations,
2475000	2477000	so we're going to need to fix that.
2477000	2479000	But it's core idea, and the reason it can be much faster,
2479000	2483000	is that each integer is represented by the corresponding floating point integer.
2483000	2486000	And the operations will only work if the input is exactly an integer,
2486000	2488000	and they produce integers as output.
2488000	2491000	So we don't need to worry about numbers that are really close to zero,
2491000	2493000	or negative numbers like we did,
2493000	2496000	when we were working on the entire interval from negative 1 to 1.
2496000	2498000	This allows us to pull some more tricks,
2498000	2500000	and then do things more quickly.
2500000	2502000	So we're going to combine the power of what we've done so far,
2502000	2505000	and Fluent 8, and get Fluent 8.
2505000	2507000	Fluent 8.
2507000	2508000	Fluent 8.
2508000	2510000	Fluent 8.
2510000	2512000	Ah, close enough.
2512000	2516000	This time this stands for half floating linear U and 8,
2516000	2519000	and then we're going to use that to implement a classic computer.
2519000	2523000	So each byte will be represented by a half precision floating point number.
2523000	2527000	And since bytes are integers, we'll represent it by the corresponding floating point number,
2527000	2529000	which is exactly that integer.
2529000	2532000	All 256 of them have exact representations.
2532000	2535000	Let's first look at a helper function that's familiar.
2535000	2537000	This is another threshold function.
2537000	2540000	It requires an integer, but that integer can be as high as 511.
2540000	2542000	9 bits.
2542000	2546000	If the number is greater than or equal to 256, it returns 1.0,
2546000	2548000	otherwise 0.0.
2548000	2550000	So this is like a threshold 256 function,
2550000	2552000	or a downshift by 8 bits.
2552000	2556000	It uses the same kind of loss of precision tricks we've been using all along,
2556000	2558000	but we can get it done with 4 operations this time,
2558000	2561000	because it only needs to work on 512 different inputs.
2561000	2565000	It's similarly easy to downshift by 1 or 2 or 3 or 4 bits,
2565000	2567000	and we have functions for that as well.
2567000	2570000	And now we can implement modular addition the same way we did before.
2570000	2572000	We just compute the sum natively.
2572000	2575000	Now that could be over 255.
2575000	2579000	But we have a way to test whether it is and compute 1.0 or 0.0.
2579000	2583000	So we multiply that by the constant 256, which gives us either 256 or 0,
2583000	2586000	and we subtract that off so that the result is back in range.
2586000	2587000	Cool.
2587000	2590000	We only did 7 floating point operations here, which is not bad.
2590000	2592000	I'm certainly not going to show you all of the code,
2592000	2595000	but I wanted to give a taste of some of the interesting problems
2595000	2597000	that we need to solve in order to do this efficiently.
2597000	2600000	While addition is already kind of linear except for overflow,
2600000	2602000	bitwise operations like and are not even close.
2602000	2606000	But we can do it pretty cleanly with some of the operations we've already constructed.
2606000	2609000	I'll run this loop exactly 8 times once for each bit,
2609000	2612000	and this will be unrolled by the compiler, so we're not even doing these comparisons.
2612000	2614000	It's as though we wrote this 8 times.
2614000	2619000	Since it's unrolled, we can compute something like a constant 2 to the i at compile time as well.
2619000	2622000	We work bit by bit starting with the lowest order 1.
2622000	2626000	The first thing we do is shift each input down by 1 bit using a function we've already seen.
2626000	2628000	Then we shift it back up.
2628000	2630000	As long as the input is less than 128, which it will be,
2630000	2634000	you can shift up by 1 by just multiplying by 2 or adding it to itself.
2634000	2638000	Now we know the last bit is 0, so if I subtract this from the original argument,
2638000	2641000	I get the lowest order bit of the input, either 1 or 0.
2641000	2643000	So I've extracted the lowest order bit of both args,
2643000	2646000	but I still don't have and even on 1 bit.
2646000	2649000	Multiplying the two bits together would give me the right answer,
2649000	2652000	and multiplication is one of the linear operations.
2652000	2654000	But remember that we only allow multiplication by a constant.
2654000	2659000	For example, if you were to compute x and x, both a bit and b bit would depend on x,
2659000	2664000	and so here you'd have x times x, or x squared, which is not mathematically linear.
2664000	2666000	So we're not going to use multiplication, but we do have a nice trick,
2666000	2669000	which is to add the bits together and then shift down by 1.
2669000	2674000	If we look at the truth table, we see that this only produces a 1 when both of the inputs were 1.
2674000	2680000	I take the resulting bit and multiply it by that round's scale, which is a power of 2, a constant,
2680000	2682000	and then I just add all of those up.
2682000	2686000	Since the components will be 0 everywhere except for that one bit, plus is equivalent to or.
2686000	2689000	Ah, this reminds me of a slip-up in one of my previous videos
2689000	2694000	where I was computing the or function using and and xor and plus.
2694000	2700000	It totally works, but millions of people wrote in to tell me that I could do it with another xor instead of plus,
2700000	2702000	which would have been a little faster.
2702000	2705000	But here plus is the right option. We don't have xor, it's not linear.
2705000	2709000	I was just, like, foreshadowing this, getting you ready.
2709000	2711000	Defeated.
2711000	2714000	Anywho, that's all we need for bitwise and.
2714000	2718000	It's a little involved, but it's a far cry from the 9,000 operations we did before
2718000	2720000	just to test if a value is greater than 0.
2720000	2726000	A spreckin of which we can now quickly test whether a value is exactly 0.
2726000	2730000	We do this by negating the bits, subtracting from 255.
2730000	2734000	Then we add one, and that'll only overflow if the original value was 0.
2734000	2738000	With that, testing whether two values are equal is just a matter of subtracting them
2738000	2740000	and then seeing whether the result is 0.
2740000	2742000	And that's how it goes. That's how it always goes.
2742000	2745000	You build up some constructs, and you use those to make some more.
2745000	2748000	You gain more and more power until you have all of the things you want.
2748000	2752000	There are some good puzzles in here, and you may enjoy trying to work some of these out yourself.
2752000	2754000	And you may improve upon them, and please tell me if you do.
2754000	2757000	For example, on screen I'm showing you a straightforward way to do if,
2757000	2761000	but if you check my code, I do a thing that's way more mysterious and fancy
2761000	2764000	in order to squeeze the last bits of performance out of it.
2764000	2767000	And I am going to care about performance for this application.
2767000	2772000	The last time I made a computer out of floating point numbers, which did happen before,
2772000	2777000	this computer was focused on beauty with no concessions to practicality.
2777000	2781000	Frankly, the computer was sort of boring to use because it had no I.O.
2781000	2783000	and it didn't do anything you could observe.
2783000	2785000	So this time I want to do the opposite.
2785000	2789000	I'm willing to make some concessions on beauty as long as the result is entertaining.
2789000	2793000	Now the most entertaining computer is the Nintendo Entertainment System.
2793000	2795000	And so this is a natural choice.
2795000	2801000	After all, I like to work at the intersection of theory and impractice and practice and entertainment.
2801000	2805000	The Nintendo Entertainment System consists of a basically reasonable computer
2805000	2808000	and a bunch of other weird stuff for entertainment purposes only.
2808000	2814000	The core of the computer is an 8-bit microprocessor that's more or less the Motorola 6502.
2814000	2818000	And that other stuff includes video and audio hardware and the controllers and the game cartridge,
2818000	2822000	which itself might include hardware and stuff like that.
2822000	2827000	My goal is to replace that 8-bit microprocessor with something that only runs linear floating point operations.
2827000	2829000	So I'm not going to implement any of the weird stuff.
2829000	2832000	And that's good because I'm going to do this in a software emulator,
2832000	2834000	which is my own hacked up copy of FCE Ultra,
2834000	2836000	and this emulator is so complicated.
2836000	2839000	But the code that emulates the processor is basically tractable.
2839000	2843000	The processor state consists of a small number of 8-bit registers,
2843000	2846000	each of which will represent with a fluent 8.
2846000	2848000	There's also a 16-bit program counter.
2848000	2850000	We'll only need a few 16-bit operations,
2850000	2854000	and it's quite easy to build 16-bit integers using two 8-bit integers.
2854000	2856000	So I won't say any more about that.
2856000	2858000	And at a high level, the processor is just a loop.
2858000	2861000	It reads one byte from memory at the program counter,
2861000	2865000	which tells it which of the 256 instructions it's going to run next.
2865000	2869000	It runs that instruction, which updates the state like the registers in the program counter,
2869000	2871000	and then starts the loop again.
2871000	2873000	Of course, there are copious details here.
2873000	2876000	First, let's look at a simple instruction so you can kind of see how it goes.
2876000	2878000	A really simple instruction is tax.
2878000	2882000	And speaking of tax, I'll have you know that video editing is so tedious
2882000	2887000	that while making this video, I actually procrastinated it by doing my taxes.
2887000	2892000	Anyway, TAX on the 6502 transfers the value from the register A to the register X.
2892000	2894000	There are still several steps, though.
2894000	2898000	After we copy it over, we need to update the negative and zero bits of the processor flags
2898000	2900000	and increment the program counter.
2900000	2904000	But this code is actually quite nice because we've already done all the work of implementing 8-bit integers.
2904000	2908000	It makes use of bitwise AND and is zero and shifting and so on.
2908000	2911000	If all the instructions were like that, this thing would be really simple.
2911000	2913000	So let's look at a harder instruction.
2913000	2916000	This is a branching instruction, branch on conditions set.
2916000	2921000	It modifies the program counter to basically do a jump if one of the processor flags is set.
2921000	2923000	Otherwise, it just advances to the next instruction.
2923000	2928000	First problem we'll see is that there's a branch in the implementation of the processor, which is not linear.
2928000	2930000	That's this if else.
2930000	2932000	But we do have a fluent 8 version of if.
2932000	2937000	So we can change this to update the program counter, but to a value that depends on the condition.
2937000	2938000	That'll look like this.
2938000	2941000	But the other problem is this memory access.
2941000	2947000	Now the Nintendo has a main memory of 2 kilobytes, and we could create 2,000 fluent 8s and implement this array subscript.
2947000	2949000	That's not really the problem.
2949000	2951000	The problem is that accessing memory has side effects.
2951000	2956000	So in the previous version of this code where an if wasn't executed, we wouldn't have accessed the memory.
2956000	2959000	This is sort of obvious for memory writes because writing changes memory.
2959000	2964000	Less obvious is that writes and reads often have side effects because of memory mapped IO.
2964000	2971000	For example, writing 2 bytes to 2006 will load them as an address into the PPU, that's one of those weird things.
2971000	2976000	And then writes that happen to 2007 will pass through to video memory at that address.
2976000	2984000	Or writing to 4014 will start a DMA that transfers 256 bytes to video memory and stalls the processor for 512 cycles.
2984000	2986000	So these are not small things.
2986000	2993000	And worse, there isn't even a small set of them because lots of cartridges have hardware inside them that does arbitrary stuff on reads and writes.
2993000	2996000	And weirdos are making new weird cartridges all the time.
2996000	2998000	And so here we have the main concession.
2998000	3002000	The emulator API for this chip offers a conditional read and write.
3002000	3005000	These take the address, but also a condition.
3005000	3007000	If the condition is true, you do what you'd expect.
3007000	3010000	But if it's false, nothing happens and an arbitrary value is returned.
3010000	3014000	Of course, this isn't linear, but it's not really that unrealistic if we were making a processor.
3014000	3019000	We would just wire this through to the memory controller, which would then ignore the read or write if the bit isn't set.
3019000	3024000	The real 6502, for example, has a pin that indicates whether it's doing a read or a write.
3024000	3026000	But I accept your criticism.
3026000	3028000	Feel free to defeat me by doing without this.
3028000	3033000	Another challenge is that the 6502 has a load of undocumented and really weird instructions.
3033000	3038000	And this wouldn't be so bad except that the emulator source code I'm working from is extremely hard to understand.
3038000	3046000	It's filled with all sorts of macro hacks that assume specific variable names, references to mysterious global variables like temp and foo,
3046000	3052000	pirate jokes, feuds between developers commenting out each other's wrong code, and so on.
3052000	3057000	And unfortunately, I don't have any Nintendo games that actually execute many of these instructions.
3057000	3063000	So in the course of development, I made my own cartridge that executes a whole bunch of undocumented instructions when it starts up.
3063000	3068000	It displays the results of those on the screen so that I can test whether my implementation matches the reference.
3068000	3075000	This cartridge might be the world's most boring Nintendo game, even more boring than Wall Street Kid.
3075000	3077000	Here's what that game looks like.
3077000	3079000	You can't win it or even play it.
3079000	3082000	It exists only to destroy your mind.
3082000	3085000	The last puzzle to solve is instruction dispatch.
3085000	3089000	When we read the instruction byte, we look at it and decide which instruction to execute.
3089000	3093000	The natural way to implement this is with a switch statement and a case for each instruction.
3093000	3095000	But that of course is not linear.
3095000	3097000	We can't do any branching of control flow.
3097000	3101000	We have to execute the same series of additions and multiplications each time.
3101000	3105000	So what I'll do is execute every single instruction every time.
3105000	3107000	Now I only want the right instruction to do anything.
3107000	3113000	So the first thing I do is make 256 copies of the CPU state, basically the registers.
3113000	3115000	That's a finite number of variables.
3115000	3117000	I also have an active flag for each one of those.
3117000	3121000	And exactly one of those active flags will be set to one for the correct instruction.
3121000	3125000	Then I run all of the instructions on their own copies of the CPU state.
3125000	3129000	If I do any conditional reads or write, I include the active flag in the condition.
3129000	3131000	So only the correct effects will happen.
3131000	3137000	So then I have the resulting 256 states and I need to copy the one from the correct instruction back into the main state.
3137000	3141000	The way to do this is to zero them all out except for the active one.
3141000	3143000	And we can do that with if and then just sum them all up.
3143000	3146000	They'll all be zero except for the correct one, so we'll get the right answer.
3146000	3150000	Now it's kind of annoying to run every instruction on every tick of the CPU.
3150000	3153000	And this technique is the main reason that it's not going to be that fast.
3153000	3154000	But it is completely linear.
3154000	3157000	Another upside is that each instruction is completely independent.
3157000	3159000	So they can actually be run in parallel.
3159000	3161000	So let's start up the benchmark.
3161000	3163000	Super Mario Brothers.
3163000	3167000	Here it's splitting the instructions across eight cores running in parallel.
3167000	3171000	If not for that instruction dispatch, this thing would run at playable frame rates.
3171000	3173000	But, and yes, it is already running.
3173000	3176000	The cost of not cheating is that it runs pretty slow.
3176000	3179000	The hardware Nintendo runs at 60 frames per second.
3179000	3184000	And the emulator free to run non-linear instructions gets 3,500 frames per second.
3184000	3195000	But the linear version, and I did do a lot of optimization, gets 0.11 frames per second or 8.6 seconds per frame.
3195000	3197000	Which ain't fast.
3197000	3200000	Maybe you could help me out by putting this video in 2x speed.
3200000	3208000	I will say, though, in comparison that I have played AAA titles that at launch, inexplicably, on high-end hardware had comparable frame rates.
3208000	3213000	And they were no doubt executing a great many non-linear instructions.
3213000	3219000	Speed aside, we now have a general-purpose computer, which renders everything we've done up until this point moot.
3219000	3223000	If we want a non-linear transfer function, we can just implement the hyperbolic tangent.
3223000	3228000	If we want fractals, we can just write code that draws the Mandelbrot set on the Nintendo.
3228000	3231000	We can just write a good encryption algorithm like AES.
3231000	3233000	We can have a chess engine with search.
3233000	3235000	In fact, we already have one.
3235000	3238000	Chessmaster for the NES was included in our tournament already.
3238000	3242000	And by running it on our linear emulator, we have a linear model.
3242000	3244000	So it seems I defeated even myself.
3244000	3247000	And I can finally be done with this damn thing.
3250000	3253000	Alright, so what have we learned today?
3253000	3255000	It's the same thing we learned every time.
3255000	3260000	Complexity is everywhere, even with something as simple as plus and multiplication by constants,
3260000	3263000	which mathematically can only create lines.
3263000	3269000	Given a tiny foothold by way of rounding error, we can bend them to our will and make them do anything.
3269000	3272000	And I think this is the same story of computer science.
3272000	3276000	Complexity from simplicity, and maybe even of the universe.
3276000	3279000	So don't underestimate simple things put together.
3279000	3282000	Anyway, if you made it this far, thank you for watching.
3282000	3283000	Thank you for your attention.
3283000	3287000	And if you didn't make it this far, I don't even know what we're talking about.
3287000	3289000	I'm sure I'll be back soon with more stupid stuff.
3289000	3294000	In any case, I've been Tom7, and this was Impractical Engineering.
3294000	3295000	See you soon.
