WEBVTT

00:00.000 --> 00:02.000
Imagine you're my professor.

00:02.000 --> 00:04.000
Maybe you actually were my professor,

00:04.000 --> 00:08.000
in which case you may already be sweating before I say anymore.

00:08.000 --> 00:10.000
The subject matter is neural networks.

00:10.000 --> 00:15.000
You draw an illustration on the board with a node's inputs and its outputs via transfer function.

00:15.000 --> 00:19.000
You inform us of this mathematical fact that the transfer function cannot be linear,

00:19.000 --> 00:24.000
or the whole model would reduce to a linear function.

00:24.000 --> 00:26.000
I immediately raise my hand.

00:26.000 --> 00:29.000
The speed with which I raise it and the not very subtle forward pose

00:29.000 --> 00:33.000
suggests that I want to pluck an abstract idea from the whiteboard and pervert it.

00:33.000 --> 00:36.000
You know this look, and you are reluctant to call on me.

00:36.000 --> 00:38.000
But no other students are raising their hands.

00:38.000 --> 00:40.000
You have no choice.

00:40.000 --> 00:41.000
Tom.

00:41.000 --> 00:43.000
It's more like a statement than a question.

00:43.000 --> 00:46.000
It includes the tone of spoken punctuation that, if it could,

00:46.000 --> 00:49.000
ends the entire conversation before it begins.

00:49.000 --> 00:52.000
I go on and on about some technicality.

00:52.000 --> 00:56.000
That due to approximate math on the computer, this mathematical fact won't be true.

00:56.000 --> 01:01.000
You say, okay, technically that's right, but for all practical purposes it doesn't matter.

01:01.000 --> 01:05.000
And I say, well, what about impractical purposes?

01:05.000 --> 01:09.000
And you, in a moment of weakness, vigorously strangle me.

01:09.000 --> 01:11.000
And that's how I died.

01:11.000 --> 01:14.000
Murdered in cold blood.

01:14.000 --> 01:19.000
That was about 20 years ago, but the world will not let us stop thinking about neural networks.

01:19.000 --> 01:22.000
We're really just pressing all the gas pedals at once on this one,

01:22.000 --> 01:25.000
heading towards a utopia or a dystopia.

01:25.000 --> 01:28.000
Some kind of topia, for sure. We're getting there real fast.

01:28.000 --> 01:30.000
So this question has been on my mind for some time.

01:30.000 --> 01:32.000
And just to be clear, the professor is right.

01:32.000 --> 01:36.000
I might be technically correct here, but it doesn't matter for practical purposes.

01:36.000 --> 01:40.000
But I like to work at the intersection of theory and impractice.

01:40.000 --> 01:43.000
And so by doing a lot of work, we can make it matter.

01:43.000 --> 01:46.000
And then I'll be even more right, both theoretically right,

01:46.000 --> 01:49.000
and it will only matter for most practical purposes.

01:49.000 --> 01:52.000
So in this video, in its lengthy accompanying technical report,

01:52.000 --> 01:56.000
I have an exhaustive exploration of what you can get away with.

01:56.000 --> 02:00.000
And I'll see how we can absolutely use linear transfer functions in neural networks

02:00.000 --> 02:03.000
and all sorts of other things where they shouldn't be enough.

02:03.000 --> 02:06.000
I'm Tom Seven, and this is Impractical Engineering.

02:06.000 --> 02:10.000
Okay, let's repeat the professor's lesson so we can understand the nature of the dispute.

02:10.000 --> 02:13.000
If you feel like you already know everything about neural networks,

02:13.000 --> 02:16.000
this section is safely skippable, but so is the whole video.

02:16.000 --> 02:21.000
So fundamentally, a neural network takes in inputs, which are a bunch of numbers,

02:21.000 --> 02:25.000
and transforms those numbers, and then outputs some other numbers.

02:25.000 --> 02:28.000
In this drawing, I have three inputs and one output.

02:28.000 --> 02:33.000
So every one of these circles is going to be filled in with some number as we run the network.

02:33.000 --> 02:37.000
So call the inputs x, y, z, and let's just look at how r is computed.

02:37.000 --> 02:38.000
That's that middle one.

02:38.000 --> 02:41.000
We start with a weighted sum of x, y, and z.

02:41.000 --> 02:45.000
So we take all the inputs, we multiply each one by some weight, and add those together.

02:45.000 --> 02:48.000
And these weights are determined when we train the network.

02:48.000 --> 02:49.000
At this point, they're just constants.

02:49.000 --> 02:51.000
When we're running the network, they're just constants.

02:51.000 --> 02:55.000
We also learn a bias parameter, which becomes a constant, and that just gets added in as well.

02:55.000 --> 02:58.000
The important part for today is this transfer function, tf.

02:58.000 --> 03:03.000
This gets applied to the weighted sum, and it transforms it, in this case, with a sigmoid.

03:03.000 --> 03:08.000
And the intuition here is somehow that this node r, this neuron r,

03:08.000 --> 03:10.000
fires with some probability.

03:10.000 --> 03:12.000
That depends on its connection with these other neurons.

03:12.000 --> 03:17.000
But because it's a probability, it ranges from zero to one

03:17.000 --> 03:19.000
instead of, like, negative infinity to infinity.

03:19.000 --> 03:23.000
And so the more the input ones fire, the more likely this one is to fire.

03:23.000 --> 03:25.000
That was the classic idea, anyway.

03:25.000 --> 03:30.000
These days, pretty much everyone uses the rectified linear transfer function.

03:30.000 --> 03:34.000
It's super simple to implement, and for various reasons, it actually works better,

03:34.000 --> 03:36.000
especially for the internal layers.

03:36.000 --> 03:38.000
And actually, all sorts of functions will work here.

03:38.000 --> 03:41.000
It needs to be differentiable because of the way we train these things.

03:41.000 --> 03:46.000
But the only other apparently necessary quality is that the function be nonlinear.

03:46.000 --> 03:48.000
At least so says the professor.

03:48.000 --> 03:50.000
Now, the reason for this is mathematically nice.

03:50.000 --> 03:52.000
Let's look at the formula for r again.

03:52.000 --> 03:58.000
And let's say the transfer function is linear, so it's like mx plus b2.

03:58.000 --> 04:02.000
Then you can multiply m by all these terms, and you get another linear function.

04:02.000 --> 04:07.000
So r is a linear function of the inputs, and then so is q, and then so is s.

04:07.000 --> 04:10.000
And then o is a linear function of q, r, and s.

04:10.000 --> 04:15.000
And what this would mean is that the output would just equal some linear function of the inputs.

04:15.000 --> 04:18.000
And all of this complexity of the neural network would just simplify away.

04:18.000 --> 04:20.000
We wouldn't need any of the hidden layers.

04:20.000 --> 04:22.000
We would just have a function of the input layer.

04:22.000 --> 04:27.000
There are lots of functions such as XOR that can't be approximated by linear functions like this.

04:27.000 --> 04:33.000
We definitely want our neural networks to be able to model things that are complicated, like XOR or human thought.

04:33.000 --> 04:35.000
So the story goes.

04:35.000 --> 04:38.000
So that would be true if we were using real math.

04:38.000 --> 04:43.000
On a computer, we're going to use IEEE floating point, which isn't associative or distributive.

04:43.000 --> 04:47.000
So if we simplify the whole network, we won't actually get the same result.

04:47.000 --> 04:54.000
So my goal today will be to create a transfer function that, despite being mathematically linear, will not be computationally linear.

04:54.000 --> 04:58.000
And thus, I'll be able to use it to train models that have interesting behavior.

04:58.000 --> 05:02.000
Now, my smart math friend Jason, who probably makes the professor sweat even more,

05:02.000 --> 05:08.000
reminds me that this is actually an affine function because I add something at the end.

05:08.000 --> 05:09.000
That's fine. I'm going to call it linear.

05:09.000 --> 05:13.000
He refers to this as high school linear in a pejorative way, and that's fine.

05:13.000 --> 05:17.000
I'm comfortable with that. In a lot of ways, I'm mentally still in high school.

05:17.000 --> 05:23.000
So this means two operations, addition and multiplication by constants or scaling.

05:23.000 --> 05:25.000
Them's the rules.

05:25.000 --> 05:30.000
Or equivalently, would it simplify mathematically to a polynomial of at most degree one?

05:30.000 --> 05:34.000
So 3x plus 2x all times five, that would simplify.

05:34.000 --> 05:38.000
But 2x times x would yield 2x squared, and that's a degree two polynomial.

05:38.000 --> 05:41.000
So that's disallowed. Okay?

05:41.000 --> 05:48.000
Floating point comes in a number of different spice levels, corresponding to how many bits you're using to represent it.

05:48.000 --> 05:53.000
So you may be familiar with double and float. Those are 64 and 32 bits.

05:53.000 --> 05:56.000
Half precision is 16 bits, and it gets even lower.

05:56.000 --> 06:01.000
It's usually used because then you need half as much memory to store your numbers.

06:01.000 --> 06:07.000
That'll be good for us. We'll be happy to save the memory, but the real reason to use half precision is that it is less precise.

06:07.000 --> 06:11.000
And imprecision is going to be a desirable quality in this work.

06:11.000 --> 06:16.000
Being only 16 bit, there are 65,000 different values that we could represent.

06:16.000 --> 06:18.000
So it's clearly not all of the numbers.

06:18.000 --> 06:25.000
This is an exponential format, so the main thing to remember about floating point precision is that there's more numbers near zero than elsewhere.

06:25.000 --> 06:33.000
So when you get to the largest finite numbers, like 65,504, only multiples of 32 are even representable.

06:33.000 --> 06:37.000
Between 2048 and 4096, only even numbers are there.

06:37.000 --> 06:39.000
Below that, only integers.

06:39.000 --> 06:42.000
And actually, most of the action happens near zero, where you get a lot of fractions.

06:42.000 --> 06:46.000
Now this stuff about comparing against epsilon is okay, but it's kind of naive.

06:46.000 --> 06:49.000
Like for one thing, what is epsilon supposed to be?

06:49.000 --> 06:52.000
If you're working with really small numbers, you can use a really small epsilon.

06:52.000 --> 06:59.000
But if you're working with larger numbers, you might need to use an epsilon of up to 32, or maybe half that, for half precision.

06:59.000 --> 07:03.000
Actually, a while back, I wrote a paper called What if Anything is Epsilon?

07:03.000 --> 07:09.000
Where I looked at what programmers picked in practice for their value of epsilon by going through a whole bunch of code on GitHub.

07:09.000 --> 07:14.000
I enjoyed laughing at their bugs, like minus 1e10, which is negative 10 billion.

07:14.000 --> 07:18.000
They meant 1e-10, 1 over 10 billion.

07:18.000 --> 07:20.000
And I like to compare these by language.

07:20.000 --> 07:25.000
For example, I found that JavaScript programmers were the most tolerant of error, which makes sense.

07:25.000 --> 07:26.000
They're pretty sloppy.

07:26.000 --> 07:29.000
One programmer picked 6 million on purpose, that wasn't a typo.

07:29.000 --> 07:33.000
Well, a lot of stuff will be equal if you use a really large epsilon.

07:33.000 --> 07:36.000
But anyway, the error you get is not like random.

07:36.000 --> 07:38.000
These have a much more useful definition.

07:38.000 --> 07:45.000
If I take two numbers and add them using IEEE floating point, the answer is defined to be the real math answer, x plus y,

07:45.000 --> 07:48.000
but rounded to the nearest representable floating point number.

07:48.000 --> 07:50.000
So if the result is high, I might need to round a lot.

07:50.000 --> 07:54.000
If the result is small, I might need to round a small amount.

07:54.000 --> 07:56.000
I also might not need to round at all.

07:56.000 --> 08:01.000
Like 2 plus 2 is literally equal to 4 exactly in floating point, like you'd want.

08:01.000 --> 08:03.000
And we're going to use that kind of thing later.

08:03.000 --> 08:07.000
So the most important thing to remember about floating point for this project is,

08:07.000 --> 08:11.000
rounding error depends on how far you are out on the number line.

08:11.000 --> 08:15.000
Large error for large numbers, small error for small numbers.

08:15.000 --> 08:17.000
But it also depends on the specific value.

08:17.000 --> 08:19.000
Not everything needs to be rounded.

08:19.000 --> 08:24.000
Surrounding is a complicated function, and we're going to abuse that complexity in order to get behavior that we like.

08:24.000 --> 08:29.000
Let's look at how we can start abusing the imprecision of floating point numbers.

08:29.000 --> 08:33.000
We just have addition and multiplication by constants, so we're going to try both of those.

08:33.000 --> 08:40.000
Here's what happens if I add 128 to the input and then subtract 128 back out.

08:40.000 --> 08:44.000
Mathematically, of course, this is just the identity function. I get back the input.

08:44.000 --> 08:48.000
But because there aren't that many values representable near 128,

08:48.000 --> 08:52.000
we can only get back eight different values between 0 and 1.

08:52.000 --> 08:56.000
Note that the zigzag is finer in the negative region than in the positive region.

08:56.000 --> 09:00.000
If we look at the actual numbers that are representable near 128,

09:00.000 --> 09:05.000
we see that we have eighths above 128, but sixteenths below 128.

09:05.000 --> 09:08.000
So we get more precision in the negative region.

09:08.000 --> 09:13.000
Okay, that's plus. It's definitely not a line, but it's basically just a line.

09:13.000 --> 09:16.000
Also, this thing has a terrible derivative. It just has all these flat segments,

09:16.000 --> 09:20.000
so the derivative there is 0 and it's undefined at the discontinuities.

09:20.000 --> 09:23.000
So it's going to actually be pretty hard to use as a transfer function,

09:23.000 --> 09:25.000
but we're going to try it out anyway.

09:25.000 --> 09:27.000
Now multiplication looks a lot more subtle.

09:27.000 --> 09:30.000
Here I'm multiplying by 100 and then by 1 over 100,

09:30.000 --> 09:35.000
which also should give me back just the identity f of x equals x.

09:35.000 --> 09:38.000
So if we zoom in on this one, we'll start to see some detail.

09:38.000 --> 09:41.000
Actually, maybe that's just my laser printer.

09:41.000 --> 09:43.000
Well, that's the problem with using imprecise tools.

09:43.000 --> 09:45.000
Maybe we should do this on the computer.

09:45.000 --> 09:47.000
Okay, here we are on the computer.

09:47.000 --> 09:50.000
And if I zoom in on this line, and I got to zoom in a lot,

09:50.000 --> 09:53.000
near zero, it's pretty much perfect.

09:53.000 --> 09:55.000
But as we get near one, it gets a lot more jacked.

09:55.000 --> 09:59.000
And this is real imprecision, and it depends where you are on the number line.

09:59.000 --> 10:01.000
You get different rounding error.

10:01.000 --> 10:04.000
This is actually pretty hard to reason about, to be honest.

10:04.000 --> 10:07.000
So just suffice to say, when you multiply,

10:07.000 --> 10:09.000
you get a little bit of error all throughout the number line,

10:09.000 --> 10:11.000
but it depends on where you are.

10:11.000 --> 10:13.000
So that's multiplication.

10:13.000 --> 10:15.000
Now we can try to put these together in various ways.

10:15.000 --> 10:18.000
So I play with this a lot,

10:18.000 --> 10:22.000
and I produced a whole bunch of just totally bonkers functions.

10:22.000 --> 10:25.000
But actually the best shape that I was able to make

10:25.000 --> 10:27.000
came from just using multiplication.

10:27.000 --> 10:31.000
And what I do is I multiply by the first number that's smaller than 1.

10:31.000 --> 10:33.000
So it's just slightly less than 1.

10:33.000 --> 10:36.000
That's 1 minus 1 over 2048.

10:36.000 --> 10:38.000
I keep multiplying by that over and over again.

10:38.000 --> 10:41.000
And as I do, I accumulate more and more error in different parts of the graph.

10:41.000 --> 10:43.000
Of course, I'm also making the number smaller.

10:43.000 --> 10:47.000
So at the end, I want to normalize back so that f of 1 is 1.

10:47.000 --> 10:49.000
Here's what it looks like if I do that iteratively.

10:49.000 --> 10:51.000
Accumulating error.

10:51.000 --> 10:53.000
I found that 500 steps was a good stopping point.

10:53.000 --> 10:56.000
So this is a function that I call grad1.

10:56.000 --> 10:58.000
Grad for the name of this project, which I can't pronounce.

10:58.000 --> 11:01.000
Gradient half decent.

11:01.000 --> 11:05.000
I triply half gradient descent, you get it.

11:05.000 --> 11:08.000
And it has this nice zigzag shape.

11:08.000 --> 11:10.000
Importantly, it's kind of smooth.

11:10.000 --> 11:13.000
If we zoom in on it, we'll see that it's, you know, putting aside the pixels,

11:13.000 --> 11:15.000
that it's piecewise linear.

11:15.000 --> 11:17.000
So that's nice.

11:17.000 --> 11:20.000
Now you might wonder, why does it get this zigzag shape?

11:20.000 --> 11:23.000
And truth be told, round-off error is just kind of hard to reason about.

11:23.000 --> 11:25.000
Let me show you two illustrations that are at least nice to look at.

11:25.000 --> 11:29.000
In this rainbow, I've given each of the numbers between 0 and 1 a color on the x-axis.

11:29.000 --> 11:33.000
And then on the y-axis, I'm successively multiplying by that constant.

11:33.000 --> 11:37.000
And you could see that they get exponentially smaller as expected, but not smoothly.

11:37.000 --> 11:39.000
And these changes in direction come from different exponents,

11:39.000 --> 11:42.000
and we see some of that reflected in the zigzags.

11:42.000 --> 11:44.000
On this image, the x-axis is 0 to 1 again.

11:44.000 --> 11:47.000
The y-axis is successive multiplication by the constant.

11:47.000 --> 11:50.000
But the green pixels is when my round-off is too high compared to the correct result,

11:50.000 --> 11:52.000
and magenta when it's too low.

11:52.000 --> 11:54.000
This line at the top is at 500 iterations.

11:54.000 --> 11:58.000
And you can see how it slices both green and magenta regions.

11:58.000 --> 12:00.000
Too high and too low.

12:00.000 --> 12:04.000
One more thing, in order to train models with this thing, we need to know it's derivative.

12:04.000 --> 12:07.000
And for reasons of implementation tricks that I'm not going to get into,

12:07.000 --> 12:11.000
I actually need the derivative in terms of the y-coordinate instead of the x-coordinate.

12:11.000 --> 12:14.000
Now, I'm not good enough at math to figure this out analytically.

12:14.000 --> 12:18.000
In any way, it would probably just be a table of the values for these different segments.

12:18.000 --> 12:22.000
But since it's 16-bit and there's only 65,000 values that are possible,

12:22.000 --> 12:26.000
I can just use a computer program to compute the derivative for every point.

12:26.000 --> 12:28.000
So here that has plotted along the y-axis.

12:28.000 --> 12:30.000
I think it looks pretty cool like an oscilloscope.

12:30.000 --> 12:33.000
You'll notice that the derivative isn't a perfect square wave,

12:33.000 --> 12:39.000
and it wouldn't be because there are in fact little imperfections in this curve from round-off error.

12:39.000 --> 12:42.000
I'm actually applying a low-pass filter here, it would be even noisier.

12:42.000 --> 12:45.000
But anyway, now we've got the function and we've got its derivative,

12:45.000 --> 12:48.000
so we can do some machine learning.

12:48.000 --> 12:51.000
But first, a bonus digression.

12:51.000 --> 12:53.000
Here's a bonus digression.

12:53.000 --> 12:58.000
Having freed myself from the need to quote-unquote do math in order to differentiate functions,

12:58.000 --> 13:01.000
because I'm just going to generate a table programmatically,

13:01.000 --> 13:04.000
I can now consider all sorts of exotic transfer functions.

13:04.000 --> 13:08.000
I can even betray the central thesis of this work and consider functions that are not linear.

13:08.000 --> 13:13.000
One thing I think is really funny is when you use data sort of as the wrong type,

13:13.000 --> 13:17.000
you may be familiar with the fast inverse square root technique.

13:17.000 --> 13:22.000
I love that one, and I think it's worth considering if a transfer function even needs to use floating point operations

13:22.000 --> 13:24.000
in order to be implemented.

13:24.000 --> 13:27.000
I tried to find the fastest, simplest thing you could do that might work.

13:27.000 --> 13:30.000
My favorite was to treat the float as just 16 bits,

13:30.000 --> 13:34.000
shift them down by 2, and then treat that back as a float.

13:34.000 --> 13:37.000
For integers, shifting by 2 is just division by 4.

13:37.000 --> 13:40.000
But for a floating point number, since there are different fields within the word,

13:40.000 --> 13:42.000
this moves bits between fields.

13:42.000 --> 13:46.000
So for example, the sine bit gets moved into the exponent,

13:46.000 --> 13:51.000
which means you have a much larger exponent for negative numbers than for positive ones.

13:51.000 --> 13:54.000
The result will always be positive because we fill with zeros.

13:54.000 --> 13:57.000
Dividing the exponent by 4 has a logarithmic effect on the result,

13:57.000 --> 14:00.000
and then some of the exponent bits also go into the mantissa.

14:00.000 --> 14:03.000
So you get a kind of crazy function that looks like this.

14:03.000 --> 14:07.000
The negative values are much larger, as we said, and it logarithmically approaches zero.

14:07.000 --> 14:12.000
The positive region is actually a very small upward slope, which you can't see on this graph.

14:12.000 --> 14:17.000
But since the exponent will start with two zeros, these tend to be pretty small.

14:17.000 --> 14:21.000
This is the full range that's explored by the positive values,

14:21.000 --> 14:26.000
and you probably don't care, but here is its computed derivative in terms of the y-coordinate.

14:26.000 --> 14:30.000
So in the experiments which are coming up next, I'm going to also compare this transfer function.

14:30.000 --> 14:34.000
This wouldn't prove the professor wrong because it uses a forbidden operation,

14:34.000 --> 14:37.000
but it is about as fast as you could do anything on a computer.

14:37.000 --> 14:41.000
So if it does turn out to work, it might be a half-decent choice.

14:41.000 --> 14:46.000
To compare the transfer functions, I tried them out on different machine learning problems.

14:46.000 --> 14:51.000
Fortunately, I do have my own bespoke GPU-based system for training neural networks,

14:51.000 --> 14:56.000
which has appeared on this channel before in videos such as 30 weird chess algorithms.

14:56.000 --> 15:00.000
It's not that good, but it is the kind of thing you want if you're going to do a silly experiment like,

15:00.000 --> 15:02.000
what if deep learning, but worse?

15:02.000 --> 15:06.000
So I made a bunch of modifications for this project, for example, to do the forward step with half-precision,

15:06.000 --> 15:09.000
and to support these tabled transfer functions.

15:09.000 --> 15:14.000
Then I trained a network using the same structure and initialization, changing only the transfer function.

15:14.000 --> 15:18.000
The first problem is the MNIST digit recognition dataset.

15:18.000 --> 15:22.000
The original CAPTCHA, you get 50,000 labeled examples of these tiny digits,

15:22.000 --> 15:25.000
and you have to learn to predict the digits 0 through 9,

15:25.000 --> 15:28.000
and then there are 10,000 held-out examples to judge your accuracy on.

15:28.000 --> 15:31.000
I chose this classic problem partly for trollish reasons,

15:31.000 --> 15:33.000
because even at the time of publication decades ago,

15:33.000 --> 15:37.000
various techniques had already achieved extremely high accuracy.

15:37.000 --> 15:39.000
The networks I trained looked like this.

15:39.000 --> 15:43.000
They take in the input pixels, then there's a number of internal layers,

15:43.000 --> 15:47.000
and then a dense output layer with one output for each of the 10 digits.

15:47.000 --> 15:51.000
You can see the paper, the code for details, if you really want.

15:51.000 --> 15:56.000
But one important thing I want to point out for these experiments is that the output layer uses a strict linear transfer function,

15:56.000 --> 15:58.000
the identity, for each of the models.

15:58.000 --> 16:02.000
It's not a good choice for these categorical problems, but it allows the network to output any value,

16:02.000 --> 16:06.000
even if the transfer function, for example, only outputs positive numbers.

16:06.000 --> 16:09.000
And since it's linear, it complies with our goal of proving the professor wrong.

16:09.000 --> 16:13.000
Throughout the rest of the network, all the internal layers use the transfer function that we're studying.

16:13.000 --> 16:17.000
So I trained one of these models with the same initial conditions and the same data,

16:17.000 --> 16:19.000
but using a different transfer function.

16:19.000 --> 16:22.000
I do that for 200,000 rounds, which takes about a day each.

16:22.000 --> 16:26.000
We can then compare the final accuracy and other dimensions, such as their aesthetics.

16:26.000 --> 16:28.000
The functions are as follows.

16:28.000 --> 16:32.000
We have two classic sigmoids, the hyperbolic tangent and the logistic function.

16:32.000 --> 16:35.000
After that, the rectified linear unit.

16:35.000 --> 16:39.000
Here I'm using a leaky version where the region below zero actually has a small slope.

16:39.000 --> 16:41.000
That seems to work better for me.

16:41.000 --> 16:43.000
This function is very popular today.

16:43.000 --> 16:46.000
None of those functions are linear, as expected by the professor.

16:46.000 --> 16:49.000
Then we have a couple that abuse floating point round off error.

16:49.000 --> 16:50.000
First a really simple one.

16:50.000 --> 16:52.000
I add 64 and subtract 64.

16:52.000 --> 16:54.000
We saw how that discretizes the line.

16:54.000 --> 17:01.000
Then the grad one function, which multiplies by a number near one 500 times in order to smoothly magnify the round off error.

17:01.000 --> 17:05.000
Then we have our bonus content, downshift two, which manipulates bits directly.

17:05.000 --> 17:11.000
Finally, we'll evaluate the identity function, which is what the professor thinks a linear model must be equivalent to.

17:11.000 --> 17:14.000
On the MNIST problem, all of the transfer functions do well.

17:14.000 --> 17:17.000
As expected, the classics are nearing 100% accuracy.

17:17.000 --> 17:22.000
Even a simple linear model using the identity function gets like 82%.

17:22.000 --> 17:27.000
Plus 64, which gets a little bit of non-linearity with round off error outperforms it slightly.

17:27.000 --> 17:31.000
But the nice smooth grad one function is almost in the same class as the classic functions.

17:31.000 --> 17:33.000
It's working quite well.

17:33.000 --> 17:38.000
So it seems like our hypothesis is panning out, and I can sense the professor beginning to sweat.

17:38.000 --> 17:41.000
The next problem is the SIFAR-10 dataset.

17:41.000 --> 17:46.000
This is a lot like MNIST, but instead of recognizing digits, you have to recognize spirit animals.

17:46.000 --> 17:48.000
There are 10 spirit animals.

17:48.000 --> 17:54.000
Airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.

17:54.000 --> 17:55.000
This problem is much harder.

17:55.000 --> 17:59.000
With these tiny thumbnails, I sometimes can't figure out what it is.

17:59.000 --> 18:00.000
But same idea.

18:00.000 --> 18:02.000
Days later, we get results.

18:02.000 --> 18:06.000
The ranking here is the same for this problem, and we can draw basically the same conclusion.

18:06.000 --> 18:10.000
An accuracy of 53 doesn't sound that good, but keep in mind there are 10 different classes.

18:10.000 --> 18:13.000
So if you just guess randomly, that's an accuracy of 10%.

18:13.000 --> 18:15.000
So we are substantially learning here.

18:15.000 --> 18:19.000
But another way to understand our accuracy is to compare it to what's come before us.

18:19.000 --> 18:24.000
These are standardized problems, and so a lot of researchers have posted their results.

18:24.000 --> 18:28.000
So I can check the leaderboard and scrolling all the way down to the bottom.

18:28.000 --> 18:34.000
I can see that my results are, in fact, the worst result of all time.

18:34.000 --> 18:35.000
That's not too bad.

18:35.000 --> 18:38.000
Last place is the last winner.

18:38.000 --> 18:43.000
Now putting aside the aesthetic and ideological considerations, there is something to this.

18:43.000 --> 18:48.000
Recently, I feel like deep learning is getting a little too good, a little too fast.

18:48.000 --> 18:51.000
So maybe we could just slow it down a bit.

18:51.000 --> 18:54.000
The third problem is, of course, chess.

18:54.000 --> 19:00.000
I take millions of positions from a public database, and I ask Stockfish a strong chess engine, which side is winning?

19:00.000 --> 19:02.000
There's two classes of results here.

19:02.000 --> 19:07.000
One side could have an edge, and it's standard to give this advantage in terms of pawns.

19:07.000 --> 19:15.000
Or it could be of the form mate in negative 24, which means that black, because it's negative, has a mate in 24 moves, no matter what white does.

19:15.000 --> 19:19.000
Mate is always favorable to a mirror edge, no matter how big your advantage is.

19:19.000 --> 19:26.000
One thing that's funny to me about this pawn score is that the advantage can be sort of arbitrarily high if Stockfish can't find a mate.

19:26.000 --> 19:31.000
So it can give you more than 64 pawn advantage, which is funny because how are you even going to fit those on the board?

19:31.000 --> 19:32.000
Dual wheeled?

19:32.000 --> 19:34.000
Actually, here's an exercise for the reader.

19:34.000 --> 19:40.000
Find a position with the largest possible advantage according to Stockfish, but where it can't find mate.

19:40.000 --> 19:42.000
Here's the best that I could do.

19:42.000 --> 19:46.000
Huge advantage for white, but no mate even after depth 89.

19:46.000 --> 19:54.000
I mapped these scores into the interval from negative 1 to 1, where negative 1 is the best possible result for black, me and 1, and plus 1 is the same for white.

19:54.000 --> 19:58.000
And this gives me a machine learning problem, which is to learn how Stockfish rates each board.

19:58.000 --> 20:05.000
For the models that score chess positions, we could compare their predictions directly to Stockfish to understand how accurate they are.

20:05.000 --> 20:10.000
And I did that, but I think it's more fun to use these models to create chess players and then have them play against each other.

20:10.000 --> 20:17.000
These players just look at each legal move and score the resulting board and then take the move that would be most favorable to their side.

20:17.000 --> 20:19.000
So there's no game tree search here.

20:19.000 --> 20:21.000
Here are the results of a tournament.

20:21.000 --> 20:26.000
The rows are each player as white and the columns as black, and they're ordered by their final ALO rating.

20:26.000 --> 20:30.000
There's some complexity here, but the fixed versions are the ones to look at.

20:30.000 --> 20:39.000
As usual, the rectified linear unit is performing the best, but our quote-unquote linear transfer function, grad 1, is actually in second place and not far behind.

20:39.000 --> 20:47.000
It's close to being as good as Chessmaster for the Nintendo Entertainment System, and outperforms Stockfish deluded with half random moves.

20:47.000 --> 20:54.000
This is actually pretty impressive given that it's doing no game tree search, it's just using its intuitions about what boards are good.

20:54.000 --> 20:58.000
Of course, raw performance on these problems is not the only thing.

20:58.000 --> 21:02.000
We ought to think about the speed of the function, as well as its aesthetics.

21:02.000 --> 21:05.000
Some of them have nice shapes, and others look dumb.

21:05.000 --> 21:10.000
Since training takes days, where all you have to do is stare at graphs of activations,

21:10.000 --> 21:16.000
whether those look cool, or boring, or vaporwave, also bears some consideration.

21:16.000 --> 21:20.000
The key finding here is that the professor was wrong.

21:20.000 --> 21:26.000
You absolutely can use a linear transfer function, as long as you don't need it to be both good and fast.

21:26.000 --> 21:27.000
Defeated.

21:27.000 --> 21:30.000
Having gotten my revenge, we could stop there.

21:30.000 --> 21:35.000
But when have huge breakthroughs in science and technology ever happened by stopping there?

21:35.000 --> 21:38.000
So, it's on to the next level.

21:38.000 --> 21:41.000
So far, all the functions we've considered have been monotonic.

21:41.000 --> 21:46.000
That's because both plus and multiplication, even when you round, have this property.

21:46.000 --> 21:48.000
But we're certainly not limited to this.

21:48.000 --> 21:54.000
For example, if x appears multiple times under addition or subtraction, we can get much more interesting functions.

21:54.000 --> 21:58.000
Another way to look at this is interference patterns between linear functions are linear.

21:58.000 --> 22:05.000
For example, x minus 4,096 minus x plus 4,096 is linear.

22:05.000 --> 22:07.000
It's mathematically equal to zero.

22:07.000 --> 22:11.000
But in half precision floating point, it produces this square wave function.

22:11.000 --> 22:14.000
Now, this function isn't as well behaved as it looks.

22:14.000 --> 22:20.000
One of those intervals is width one exactly, but the other is very slightly smaller than one.

22:20.000 --> 22:23.000
And again, this has to do with perversities of roundoff error.

22:23.000 --> 22:30.000
Or if we take that grad one function that we've studied and subtract x from that, we get this nice triangle wave.

22:30.000 --> 22:34.000
By stringing functions together, we can make all sorts of interesting patterns.

22:34.000 --> 22:39.000
In fact, if we have any shape in mind, we can try approximating it with one of these functions,

22:39.000 --> 22:42.000
subtract it from the desired shape to get a new shape,

22:42.000 --> 22:45.000
and as long as we're getting smaller, we can just keep doing this,

22:45.000 --> 22:49.000
successively approximating that shape like a Taylor series.

22:50.000 --> 22:54.000
So if we could make any shape, let's make a fractal. Those are good shapes.

22:54.000 --> 22:57.000
The Mandelbrot set is the radiohead of fractals.

22:57.000 --> 23:02.000
Here we're going to use complex numbers, and we use a coordinate system where the x-coordinate is the real part,

23:02.000 --> 23:05.000
and the y-coordinate is the imaginary part.

23:05.000 --> 23:08.000
For any given point C, we repeatedly square and add,

23:08.000 --> 23:11.000
and this point moves around in a crazy way.

23:11.000 --> 23:14.000
And based on how quickly it converges or diverges, we give it a color.

23:14.000 --> 23:20.000
Boom. 2D Mandelbrot.

23:20.000 --> 23:24.000
Now adding C is linear. Squaring, however, is not,

23:24.000 --> 23:28.000
but we just said we can approximate any shape using interference patterns.

23:28.000 --> 23:34.000
So here's a rough approximation of f of x equals x squared, using only linear operations.

23:34.000 --> 23:37.000
So this has some funny business near the origin,

23:37.000 --> 23:41.000
but you might think we could use this to plot a kind of perverted Mandelbrot.

23:41.000 --> 23:44.000
Unfortunately, if we try, we get this piece of garbage.

23:44.000 --> 23:46.000
This stupid blotch sucks.

23:46.000 --> 23:50.000
To understand why, we need to look at the definition of squaring for complex numbers.

23:50.000 --> 23:54.000
When we multiply this out, the A and the B get mixed together.

23:54.000 --> 23:58.000
The real part has some A in it, and because I squared is negative 1,

23:58.000 --> 24:02.000
some B in it, and the imaginary part also has some A and some B in it.

24:02.000 --> 24:05.000
So we get this cross-pollination, and that means x and y-coordinates are mixed,

24:05.000 --> 24:07.000
and you get a kind of weird rotation.

24:07.000 --> 24:11.000
But let's look at the purely linear operations on complex numbers.

24:11.000 --> 24:16.000
For both plus and scaling, the real parts stay real,

24:16.000 --> 24:20.000
and the imaginary parts stay imaginary, with no cross-pollination.

24:20.000 --> 24:23.000
So no matter how we use these operations, even with floating-point roundoff,

24:23.000 --> 24:26.000
we're not going to get any mixing between the coordinates.

24:26.000 --> 24:29.000
And that's why the fractal has these rows and columns of sameness.

24:29.000 --> 24:33.000
It's really just two independent functions, one on the x-axis and one on the y-axis.

24:33.000 --> 24:38.000
So professors take note, the complex numbers do provide some refuge.

24:38.000 --> 24:41.000
It's time for another bonus digression.

24:41.000 --> 24:44.000
You might think you could just make a 3D Mandelbrot.

24:44.000 --> 24:46.000
Just do the same thing we did before,

24:46.000 --> 24:48.000
but with numbers that have three components,

24:48.000 --> 24:52.000
a real part and imaginary part and, like, a very imaginary part.

24:52.000 --> 24:56.000
If you try it, this old professor of Frobenius will come along

24:56.000 --> 24:59.000
and educate you with this cool math fact.

24:59.000 --> 25:04.000
No matter what you do, any three-dimensional algebra is equivalent to the real or complex numbers,

25:04.000 --> 25:08.000
so it's like you didn't do anything at all, or not associative,

25:08.000 --> 25:10.000
meaning the order of operations will matter.

25:10.000 --> 25:13.000
But you know what else isn't associative?

25:13.000 --> 25:15.000
The floating-point numbers, my dude.

25:15.000 --> 25:19.000
So it seems we don't need associativity to make fractals anyway.

25:19.000 --> 25:25.000
Enter the baffling numbers, which is an ill-advised generalization of the complex numbers, to three dimensions.

25:25.000 --> 25:28.000
Yes, it won't work, but we can just do it.

25:28.000 --> 25:30.000
Frobenius can't stop me.

25:30.000 --> 25:34.000
And I can use this to make a 3D fractal called the bafflebrot.

25:34.000 --> 25:38.000
Here it's sliced in half, showing a perfect ripe Mandelbrot inside.

25:38.000 --> 25:43.000
The resulting 2-gigabyte file crashes every piece of software I throw at it.

25:43.000 --> 25:44.000
I admire its spirit.

25:44.000 --> 25:47.000
Boom, 3D fractal.

25:47.000 --> 25:49.000
Bezeked.

25:49.000 --> 25:52.000
We don't actually need squaring to create fractals, though.

25:52.000 --> 25:54.000
We just need something kind of chaotic.

25:54.000 --> 25:58.000
I just take this function, which consists of 36,000 linear operations,

25:58.000 --> 26:03.000
and I iterate it, adding C each time, and plot the resulting magnitude.

26:03.000 --> 26:04.000
I think it looks pretty nice.

26:04.000 --> 26:07.000
I think this is a fractal, in the sense that it is chaotic.

26:07.000 --> 26:11.000
It has a color gradient, and could be on the cover of an electronic music album.

26:11.000 --> 26:16.000
It is not a fractal, in the sense that if you zoom in on it, you get infinite detail of self-similar shapes.

26:16.000 --> 26:19.000
In fact, as we zoom in on it only a modest amount,

26:19.000 --> 26:23.000
we see rectangular pixels as we reach the limits of half-precision floating-point.

26:23.000 --> 26:26.000
And because this fractal is built by abusing those very limits,

26:26.000 --> 26:29.000
it's not even possible to get more detail by increasing the accuracy.

26:29.000 --> 26:34.000
Alright, drawing fractals is fun and everything, but it's not really a game you can win.

26:34.000 --> 26:37.000
There's no goal other than to make a cool picture.

26:37.000 --> 26:40.000
So next, I turn to something with a clearer challenge to overcome.

26:40.000 --> 26:42.000
Linear cryptography.

26:42.000 --> 26:45.000
Cryptography is fractals minus drugs.

26:45.000 --> 26:47.000
You take some data and mess it up,

26:47.000 --> 26:50.000
but in a way where you can get it back again if you want.

26:50.000 --> 26:55.000
Possibly the most fundamental building block of cryptography is the pseudo-random number generator.

26:55.000 --> 26:58.000
This is a function that takes in a state, like a 64-bit integer,

26:58.000 --> 27:02.000
and returns a new state that, quote-unquote, looks random.

27:02.000 --> 27:06.000
With one of those, you can generate a hash function by mixing it with some input data,

27:06.000 --> 27:09.000
or a symmetric block cipher using a Feistel network.

27:09.000 --> 27:11.000
So naturally, I want one of these.

27:11.000 --> 27:16.000
Now, another thing that professors will tell you is that cryptographic algorithms cannot be linear.

27:16.000 --> 27:21.000
Here, linear includes within some modular ring like integers mod 256, the bytes,

27:21.000 --> 27:23.000
or mod 2, the bits.

27:23.000 --> 27:28.000
So in contrast, even though we said before that XOR can't be modeled by linear function on reels,

27:28.000 --> 27:31.000
XOR is considered linear in this context.

27:31.000 --> 27:34.000
The reason for that is linear cryptanalysis.

27:34.000 --> 27:36.000
If your function is even a little bit linear,

27:36.000 --> 27:41.000
then with a large collection of input-output pairs, like messages and their encrypted versions,

27:41.000 --> 27:45.000
you can deduce information about secrets like an encryption key.

27:45.000 --> 27:50.000
So the standard advice to construct these things is to alternate linear operations like XOR

27:50.000 --> 27:53.000
with nonlinear operations like substitution.

27:53.000 --> 27:57.000
Substitution is make a table of all the bytes, but permute them randomly,

27:57.000 --> 27:59.000
and then just do table lookup.

27:59.000 --> 28:02.000
In fact, Bruce Schneier writes in the big red book,

28:02.000 --> 28:05.000
substitutions are generally the only nonlinear step in an algorithm.

28:05.000 --> 28:08.000
They are what give the block cipher its security.

28:08.000 --> 28:12.000
So of course, what we're going to do is prove this adage wrong by developing a good pseudo-random function

28:12.000 --> 28:16.000
that only uses linear operations on half-precision floating-point numbers.

28:16.000 --> 28:18.000
Now, what does it mean to be good?

28:18.000 --> 28:22.000
This is less subjective than fractals, but it is still a little tricky.

28:22.000 --> 28:26.000
We don't actually even know if pseudo-random number generators exist.

28:26.000 --> 28:30.000
The best results assume that other problems are hard, but we don't have proofs of that either.

28:30.000 --> 28:34.000
There's lots of stuff that looks random, but actually isn't, like it hides a backdoor.

28:34.000 --> 28:37.000
Never forget that RSA security.

28:37.000 --> 28:44.000
Yes, that RSA took a $10 million bribe from the NSA to hide a backdoor in one of their pseudo-random number generators.

28:44.000 --> 28:50.000
Practically speaking, though, we can subject the function to a stringent battery of statistical tests,

28:50.000 --> 28:53.000
and if it passes all of those, that's a really good start.

28:53.000 --> 28:58.000
The function will work on half-precision floating-point numbers in the interval from negative one to one,

28:58.000 --> 29:01.000
just like the transfer functions we've been considering so far.

29:01.000 --> 29:03.000
Now, this is not a good choice.

29:03.000 --> 29:06.000
It's unnecessarily hard, but all of this is unnecessarily hard.

29:06.000 --> 29:09.000
Now, I have to work with 64 bits.

29:09.000 --> 29:13.000
I could represent each bit as a half, but that makes it too easy.

29:13.000 --> 29:17.000
So I'm going to represent it as 8 bytes, each byte represented by a half.

29:17.000 --> 29:23.000
To represent a byte as a half, I'll divide the interval from negative one to one into 256 segments,

29:23.000 --> 29:28.000
and I'll allow any floating-point value within that interval to represent the corresponding byte.

29:29.000 --> 29:36.000
So anything from 124 over 128 to 125 over 128 will represent the number 252.

29:36.000 --> 29:39.000
And again, allowing any number here is unnecessarily hard.

29:39.000 --> 29:42.000
In the next section, we'll see a much better way to do this that's much faster.

29:42.000 --> 29:48.000
But by struggling with this one, we'll at least demonstrate complete mastery over the sort of continuous domain.

29:48.000 --> 29:51.000
So this function will take in 8 halves and return 8 halves.

29:51.000 --> 29:54.000
And the crux of this function will be this substitution.

29:54.000 --> 29:58.000
That's a table lookup where each of the 256 bytes is swapped for another byte,

29:58.000 --> 30:03.000
or plotted as a function, each of these discrete intervals is mapped to a different interval.

30:03.000 --> 30:07.000
The approach we used in the previous section of fitting functions doesn't work here.

30:07.000 --> 30:09.000
We need something more exact.

30:09.000 --> 30:13.000
So I study a family of well-behaved functions called choppy functions.

30:13.000 --> 30:16.000
To be choppy, the function has to have a few properties.

30:16.000 --> 30:19.000
For any value in an interval that represents some integer,

30:19.000 --> 30:22.000
the function has to produce the exact same result,

30:22.000 --> 30:25.000
and its output has to be the lowest value within some interval.

30:25.000 --> 30:28.000
Of course, these functions can only use addition and scaling,

30:28.000 --> 30:32.000
and since they're maximally permissive about what they accept and very strict about what they generate,

30:32.000 --> 30:34.000
they'll be quite easy to reason about and compose.

30:34.000 --> 30:38.000
In fact, we'll be able to think about them as functions from integers to integers.

30:38.000 --> 30:40.000
So I went on a hunt for choppy functions.

30:40.000 --> 30:43.000
I wish I could tell you that I cracked the code of how to make these from scratch,

30:43.000 --> 30:45.000
but I found them by computer search.

30:45.000 --> 30:48.000
Here's an example that I can't believe I'm going to write out by hand.

30:48.000 --> 30:50.000
This function is mathematically linear.

30:50.000 --> 30:52.000
It's actually equal to a constant.

30:52.000 --> 30:54.000
The x's cancel out.

30:54.000 --> 30:59.000
What this function does is return 1 if the input represents the number 249 or 0 otherwise.

30:59.000 --> 31:02.000
So this is a pretty useful choppy function.

31:02.000 --> 31:05.000
Since each of these represents a function from a byte to a byte,

31:05.000 --> 31:09.000
I can think of it as just a table of the bytes that it produces for each input.

31:09.000 --> 31:13.000
It's a little more complicated than this because the outputs are actually from negative 1 to 1,

31:13.000 --> 31:15.000
but this is the basic idea.

31:15.000 --> 31:18.000
So what I did is I generated a whole bunch of these kinds of functions.

31:18.000 --> 31:21.000
And every time I get a new one, or a faster version of an old one,

31:21.000 --> 31:24.000
I put it in a database keyed by these integers.

31:24.000 --> 31:28.000
I can also take any two of them and get their difference by subtracting them.

31:28.000 --> 31:30.000
That'll also be a choppy function.

31:30.000 --> 31:33.000
So here's say they only differ in these two components.

31:33.000 --> 31:36.000
And so I get 0's everywhere except for those two columns,

31:36.000 --> 31:39.000
and this might give me a new choppy function I didn't have before.

31:39.000 --> 31:43.000
Observe that if I ever find one that's 0 everywhere except for a single 1,

31:43.000 --> 31:48.000
then I can use that to modify the column in any other vector to any value that I want.

31:48.000 --> 31:51.000
So these are special, these are basis vectors.

31:51.000 --> 31:53.000
So once I've done that, this column is kind of done,

31:53.000 --> 31:56.000
and I never need to find new variations of that column.

31:56.000 --> 31:58.000
So if I take a large collection of these choppy functions,

31:58.000 --> 32:03.000
I can do a process kind of like Gauss Jordan elimination to deduce a set of basis vectors.

32:03.000 --> 32:06.000
And if I find a basis vector for every column for every position,

32:06.000 --> 32:11.000
then I can just add those up to make any choppy function I want, for example, our substitution.

32:11.000 --> 32:14.000
So that's pretty nice, I just need to find these basis vectors.

32:14.000 --> 32:17.000
You might think that once you had a single basis vector,

32:17.000 --> 32:20.000
you could shift that column around, like to another position,

32:20.000 --> 32:23.000
by just calling your function on a shifted version of x.

32:23.000 --> 32:25.000
And in real mathematics, that would work.

32:25.000 --> 32:29.000
But since these functions are abusing floating point roundoff error,

32:29.000 --> 32:33.000
which depends on the specific value of x, this approach will not work.

32:33.000 --> 32:36.000
You can do stuff to the output of the function like scale it or add to it,

32:36.000 --> 32:39.000
and you can combine functions by taking their interference pattern.

32:39.000 --> 32:42.000
But you can't straightforwardly manipulate the input side.

32:42.000 --> 32:46.000
This problem is worst near the origin, where the precision is highest.

32:46.000 --> 32:49.000
This meant that it was particularly hard to find a choppy function

32:49.000 --> 32:52.000
that distinguished negative and non-negative numbers exactly.

32:52.000 --> 32:56.000
In essence, the middle two columns of my vectors would always have the same value,

32:56.000 --> 32:58.000
and so they wouldn't be independent.

32:58.000 --> 33:00.000
I need to find some way to distinguish those two.

33:00.000 --> 33:05.000
Going back to our earliest example, if we just add 128 and then subtract 128,

33:05.000 --> 33:08.000
we do get different behavior for negative and positive numbers,

33:08.000 --> 33:12.000
but if we look at the rounding near zero, a lot of negative values round up,

33:12.000 --> 33:14.000
like small positive values round down.

33:14.000 --> 33:15.000
And this makes sense.

33:15.000 --> 33:19.000
If you add a small negative number to 128, you get 128.

33:19.000 --> 33:22.000
So I hunted for the zero threshold function,

33:22.000 --> 33:24.000
and there was a lot of manual fiddling with that.

33:24.000 --> 33:27.000
But I did eventually find one, and it looks like this.

33:27.000 --> 33:29.000
It's pretty involved.

33:29.000 --> 33:33.000
One of the key things is to do a whole bunch of multiplications at the beginning,

33:33.000 --> 33:35.000
since these will preserve the sign.

33:35.000 --> 33:39.000
Spread values away from zero without causing any corruptive rounding

33:39.000 --> 33:42.000
until you can do the same old loss of precision techniques

33:42.000 --> 33:45.000
to make all the finite values the same on either side.

33:45.000 --> 33:49.000
With that zero threshold function solved, I can now create a basis

33:49.000 --> 33:52.000
and therefore create any function from a byte to a byte.

33:52.000 --> 33:55.000
So back to our pseudo-random number generator.

33:55.000 --> 34:00.000
The structure I'm going to use is a classic substitution permutation network.

34:00.000 --> 34:03.000
It takes eight bytes in A through H,

34:03.000 --> 34:06.000
and I apply the substitution function to each of the eight bytes.

34:06.000 --> 34:08.000
Then I rearrange the bits,

34:08.000 --> 34:12.000
apply a few more linear operations like modular plus and minus,

34:12.000 --> 34:14.000
and then I have a new state as the output.

34:14.000 --> 34:17.000
And by iterating this, you create a pseudo-random stream.

34:17.000 --> 34:19.000
The substitution function we already talked about.

34:19.000 --> 34:24.000
For permuting the bits, each of the output bytes depends on all of the input bytes.

34:24.000 --> 34:26.000
So it's not a function of one variable,

34:26.000 --> 34:29.000
but I can construct it from functions of one variable.

34:29.000 --> 34:32.000
If I look at this first byte in the output, let's call it y,

34:32.000 --> 34:35.000
and I can look at the first byte in the input of the permutation that's x.

34:35.000 --> 34:38.000
Note that there's just a single bit that it reads.

34:38.000 --> 34:41.000
Remember, I can create any function that I want of a single variable,

34:41.000 --> 34:46.000
so I construct a function that returns 128 if that bit is set in the input of the y0,

34:46.000 --> 34:49.000
and I do a similar thing for all the other bytes,

34:49.000 --> 34:51.000
and then I can just add up those results,

34:51.000 --> 34:54.000
and they all set different bits, so adding is like logical or.

34:54.000 --> 34:57.000
That technique of adding independent things is really useful,

34:57.000 --> 34:58.000
and we're going to use it more later.

34:58.000 --> 35:00.000
The last piece is modular addition.

35:00.000 --> 35:03.000
I have addition, of course, but on bytes it needs to wrap around

35:03.000 --> 35:06.000
if the result is greater than 256, or in this case, greater than 1.

35:06.000 --> 35:11.000
So if I add two of these values together, I get a result that might be as high as 2,

35:11.000 --> 35:14.000
so it looks like this, but I want it to look like this.

35:14.000 --> 35:16.000
Once it gets past 1, it should go back to negative 1.

35:16.000 --> 35:20.000
Fortunately, I do have a way to test whether the value is greater than a threshold like 1.

35:20.000 --> 35:24.000
So modular plus takes in two arguments and adds them together,

35:24.000 --> 35:27.000
and that result might be either too low or too high.

35:27.000 --> 35:29.000
We'll talk about the case that it's too high.

35:29.000 --> 35:34.000
We test whether it's higher than 1 using the 0 threshold function,

35:34.000 --> 35:39.000
which returns either 1 or 0, multiply that by 2, and then subtract it away.

35:39.000 --> 35:43.000
So that allows us to add this corrective factor and put it back into the right range.

35:43.000 --> 35:46.000
Now, I mentioned before that you can't necessarily shift around functions

35:46.000 --> 35:49.000
because of loss of precision, but this will actually work for the 0 threshold function.

35:49.000 --> 35:51.000
We're going to come back to that in a second,

35:51.000 --> 35:56.000
but first I want to evaluate this random number generator to see how good it is.

35:56.000 --> 36:01.000
In order to test this thing, I used a pre-existing suite of statistical tests called Big Crush.

36:01.000 --> 36:04.000
This is like hundreds of tests that if you do things with the random numbers

36:04.000 --> 36:09.000
that should have a correct mathematical result, you in fact get that mathematical result,

36:09.000 --> 36:11.000
and not something that's a little biased.

36:11.000 --> 36:13.000
It's really hard to pass these tests.

36:13.000 --> 36:15.000
You can try it out on some handmade functions if you want.

36:15.000 --> 36:17.000
It's pretty good at finding bias.

36:17.000 --> 36:21.000
This test needs like 1.6 billion bits of input to do its thing,

36:21.000 --> 36:25.000
so I actually ran it on an equivalent C implementation of this function,

36:25.000 --> 36:28.000
but I also test that they produce exactly the same result.

36:28.000 --> 36:32.000
Even with the C implementation, this takes days to run,

36:32.000 --> 36:35.000
but it did, and it passes every single test,

36:35.000 --> 36:40.000
so it's reasonable to believe that this function could be the basis of a decent encryption algorithm.

36:40.000 --> 36:42.000
Defeated.

36:42.000 --> 36:48.000
Now, one downside is that if you run this using the native half-precision implementation,

36:48.000 --> 36:52.000
it produces 25.8 bytes a second of randomness, which is very slow.

36:52.000 --> 36:57.000
Now, you can produce tables ahead of time so that each of those operations is just a 16-bit table lookup,

36:57.000 --> 37:02.000
and then it'll produce 18.5 kilobytes per second, and that's still slow.

37:02.000 --> 37:07.000
But if you were trapped on a desert island and all you had were linear floating-point operations,

37:07.000 --> 37:09.000
I guess you could do worse than this.

37:09.000 --> 37:14.000
Of course, if you're trapped on a desert island, I don't recommend encrypting your messages.

37:14.000 --> 37:17.000
This is just not a good way to get rescued.

37:17.000 --> 37:20.000
So I said I'd come back to this bit here.

37:20.000 --> 37:24.000
We used the zero-threshold function to test if a sum was greater than one

37:24.000 --> 37:28.000
so that we could implement modular arithmetic by subtracting off a corrective factor.

37:28.000 --> 37:32.000
Once upon a time, I told you you couldn't just shift around the inputs to functions,

37:32.000 --> 37:36.000
and this is true in general, but the zero-threshold function, because it operates at zero,

37:36.000 --> 37:42.000
which is the most precise region for floating-point, actually does admit this behavior within a certain range.

37:42.000 --> 37:45.000
If I have some value in mind, like 0.125,

37:45.000 --> 37:50.000
and I want a function that tests whether the input is just greater than or equal to 0.125,

37:50.000 --> 37:52.000
that looks like this,

37:52.000 --> 37:57.000
and I can do that by just subtracting 0.125 from the input and passing it to the zero-threshold function.

37:57.000 --> 38:01.000
And if the input is exactly 0.125, we get back exactly zero.

38:01.000 --> 38:05.000
This works for most numbers, but there are some limits.

38:05.000 --> 38:08.000
So on the y-axis here, we have different choices of threshold.

38:08.000 --> 38:13.000
On the x-axis, we have all of the possible inputs, and this is all of the finite floating-point values.

38:13.000 --> 38:15.000
The green region is where we get the right answer.

38:15.000 --> 38:17.000
The red region is where it's wrong.

38:17.000 --> 38:22.000
The only reason it's ever wrong is that we end up getting an infinite value during that computation.

38:22.000 --> 38:24.000
Otherwise, this would all work out.

38:24.000 --> 38:26.000
The green region is pretty big.

38:26.000 --> 38:29.000
It always works out when the input and the threshold is exactly the same, because then you get zero,

38:29.000 --> 38:31.000
and then you're not going to have any infinities.

38:31.000 --> 38:35.000
But as they get farther from one another, the value you're testing is larger,

38:35.000 --> 38:38.000
and therefore you're more likely to encounter infinities.

38:38.000 --> 38:41.000
The highlighted region is everything from negative one to one,

38:41.000 --> 38:43.000
which accounts for almost half of the finite numbers.

38:43.000 --> 38:46.000
And you can see we've covered pretty much this entire interval.

38:46.000 --> 38:51.000
There is this one corner, like a couple numbers that don't work.

38:51.000 --> 38:53.000
But it's, I mean, we can do...

38:53.000 --> 38:54.000
Okay.

38:54.000 --> 38:55.000
Alright, fine.

38:55.000 --> 38:57.000
I'll fix it.

38:58.000 --> 39:00.000
Alright, now I can sleep soundly.

39:00.000 --> 39:02.000
Here's a new version of the zero-threshold function,

39:02.000 --> 39:05.000
which works on the entire negative one to one interval.

39:05.000 --> 39:07.000
And more than that, in fact,

39:07.000 --> 39:09.000
I found this with computer search again,

39:09.000 --> 39:11.000
trying to maximize the size of the interval on which it works.

39:11.000 --> 39:13.000
And basically it's the same as before,

39:13.000 --> 39:15.000
but more careful about intermediate computations

39:15.000 --> 39:17.000
so that it doesn't touch infinity by accident.

39:17.000 --> 39:20.000
So now that I know that this works for every value in there,

39:20.000 --> 39:24.000
I can actually use it to generate literally any function that I want on that interval.

39:24.000 --> 39:27.000
The first step is to take this general-purpose greater-than function

39:27.000 --> 39:31.000
and turn it into a general-purpose exact equals function.

39:31.000 --> 39:34.000
I check whether the input is greater than or equal to the value,

39:34.000 --> 39:37.000
but then subtract off a corrective factor.

39:37.000 --> 39:40.000
If the input is greater than or equal to the next floating-point number,

39:40.000 --> 39:42.000
that's this next after thing.

39:42.000 --> 39:44.000
This returns one if the input is exactly v.

39:44.000 --> 39:46.000
And then I just make an enormous expression.

39:46.000 --> 39:48.000
There's only a finite number of floating-point inputs.

39:48.000 --> 39:51.000
So for each one, I test whether it's exactly equal to that,

39:51.000 --> 39:53.000
giving zero or one.

39:53.000 --> 39:56.000
And I multiply that by the constant value that I want to have at that point,

39:56.000 --> 39:57.000
the y-coordinate.

39:57.000 --> 40:00.000
Then I sum those all up and it makes any shape that I like.

40:00.000 --> 40:01.000
So that's great.

40:01.000 --> 40:03.000
Linear functions can do anything.

40:03.000 --> 40:07.000
And one thing I don't like about this is how big this expression is.

40:07.000 --> 40:09.000
In some sense, that's funny,

40:09.000 --> 40:12.000
but it's starting to look like this thing is turn-complete,

40:12.000 --> 40:14.000
and I'd like to build a computer to demonstrate,

40:14.000 --> 40:16.000
since that's what you do.

40:16.000 --> 40:20.000
But I don't know, everything is slow turn-complete these days.

40:20.000 --> 40:23.000
So I want to figure out how we could make it a bit more practical,

40:23.000 --> 40:27.000
because I like to work at the intersection of theory and impractice and practice.

40:27.000 --> 40:30.000
So I consulted my extensive computer science library

40:30.000 --> 40:33.000
for performance-enhancing substances.

40:33.000 --> 40:37.000
I found a relevant-looking article in the 2018 SIGBOVIC

40:37.000 --> 40:40.000
called The Fluent 8 Software Integer Library,

40:40.000 --> 40:42.000
by Jim McCann, he sounds smart,

40:42.000 --> 40:44.000
and Tom Murphy V.

40:44.000 --> 40:48.000
Wait, I already wrote this paper?

40:48.000 --> 40:50.000
God damn it.

40:50.000 --> 40:52.000
Yeah, this looks familiar.

40:52.000 --> 40:54.000
Uh, man.

40:54.000 --> 40:59.000
Well, the Fluent 8 library implements unsigned 8-bit integers,

40:59.000 --> 41:02.000
using 32-bit floating point.

41:02.000 --> 41:04.000
That sounds pretty familiar,

41:04.000 --> 41:07.000
but it does make some different design decisions than what we're doing today.

41:07.000 --> 41:11.000
One superficial difference is that it uses 32-bit full-precision floating point.

41:11.000 --> 41:13.000
That's easy to change.

41:13.000 --> 41:15.000
It also uses some nonlinear operations,

41:15.000 --> 41:17.000
so we're going to need to fix that.

41:17.000 --> 41:19.000
But it's core idea, and the reason it can be much faster,

41:19.000 --> 41:23.000
is that each integer is represented by the corresponding floating point integer.

41:23.000 --> 41:26.000
And the operations will only work if the input is exactly an integer,

41:26.000 --> 41:28.000
and they produce integers as output.

41:28.000 --> 41:31.000
So we don't need to worry about numbers that are really close to zero,

41:31.000 --> 41:33.000
or negative numbers like we did,

41:33.000 --> 41:36.000
when we were working on the entire interval from negative 1 to 1.

41:36.000 --> 41:38.000
This allows us to pull some more tricks,

41:38.000 --> 41:40.000
and then do things more quickly.

41:40.000 --> 41:42.000
So we're going to combine the power of what we've done so far,

41:42.000 --> 41:45.000
and Fluent 8, and get Fluent 8.

41:45.000 --> 41:47.000
Fluent 8.

41:47.000 --> 41:48.000
Fluent 8.

41:48.000 --> 41:50.000
Fluent 8.

41:50.000 --> 41:52.000
Ah, close enough.

41:52.000 --> 41:56.000
This time this stands for half floating linear U and 8,

41:56.000 --> 41:59.000
and then we're going to use that to implement a classic computer.

41:59.000 --> 42:03.000
So each byte will be represented by a half precision floating point number.

42:03.000 --> 42:07.000
And since bytes are integers, we'll represent it by the corresponding floating point number,

42:07.000 --> 42:09.000
which is exactly that integer.

42:09.000 --> 42:12.000
All 256 of them have exact representations.

42:12.000 --> 42:15.000
Let's first look at a helper function that's familiar.

42:15.000 --> 42:17.000
This is another threshold function.

42:17.000 --> 42:20.000
It requires an integer, but that integer can be as high as 511.

42:20.000 --> 42:22.000
9 bits.

42:22.000 --> 42:26.000
If the number is greater than or equal to 256, it returns 1.0,

42:26.000 --> 42:28.000
otherwise 0.0.

42:28.000 --> 42:30.000
So this is like a threshold 256 function,

42:30.000 --> 42:32.000
or a downshift by 8 bits.

42:32.000 --> 42:36.000
It uses the same kind of loss of precision tricks we've been using all along,

42:36.000 --> 42:38.000
but we can get it done with 4 operations this time,

42:38.000 --> 42:41.000
because it only needs to work on 512 different inputs.

42:41.000 --> 42:45.000
It's similarly easy to downshift by 1 or 2 or 3 or 4 bits,

42:45.000 --> 42:47.000
and we have functions for that as well.

42:47.000 --> 42:50.000
And now we can implement modular addition the same way we did before.

42:50.000 --> 42:52.000
We just compute the sum natively.

42:52.000 --> 42:55.000
Now that could be over 255.

42:55.000 --> 42:59.000
But we have a way to test whether it is and compute 1.0 or 0.0.

42:59.000 --> 43:03.000
So we multiply that by the constant 256, which gives us either 256 or 0,

43:03.000 --> 43:06.000
and we subtract that off so that the result is back in range.

43:06.000 --> 43:07.000
Cool.

43:07.000 --> 43:10.000
We only did 7 floating point operations here, which is not bad.

43:10.000 --> 43:12.000
I'm certainly not going to show you all of the code,

43:12.000 --> 43:15.000
but I wanted to give a taste of some of the interesting problems

43:15.000 --> 43:17.000
that we need to solve in order to do this efficiently.

43:17.000 --> 43:20.000
While addition is already kind of linear except for overflow,

43:20.000 --> 43:22.000
bitwise operations like and are not even close.

43:22.000 --> 43:26.000
But we can do it pretty cleanly with some of the operations we've already constructed.

43:26.000 --> 43:29.000
I'll run this loop exactly 8 times once for each bit,

43:29.000 --> 43:32.000
and this will be unrolled by the compiler, so we're not even doing these comparisons.

43:32.000 --> 43:34.000
It's as though we wrote this 8 times.

43:34.000 --> 43:39.000
Since it's unrolled, we can compute something like a constant 2 to the i at compile time as well.

43:39.000 --> 43:42.000
We work bit by bit starting with the lowest order 1.

43:42.000 --> 43:46.000
The first thing we do is shift each input down by 1 bit using a function we've already seen.

43:46.000 --> 43:48.000
Then we shift it back up.

43:48.000 --> 43:50.000
As long as the input is less than 128, which it will be,

43:50.000 --> 43:54.000
you can shift up by 1 by just multiplying by 2 or adding it to itself.

43:54.000 --> 43:58.000
Now we know the last bit is 0, so if I subtract this from the original argument,

43:58.000 --> 44:01.000
I get the lowest order bit of the input, either 1 or 0.

44:01.000 --> 44:03.000
So I've extracted the lowest order bit of both args,

44:03.000 --> 44:06.000
but I still don't have and even on 1 bit.

44:06.000 --> 44:09.000
Multiplying the two bits together would give me the right answer,

44:09.000 --> 44:12.000
and multiplication is one of the linear operations.

44:12.000 --> 44:14.000
But remember that we only allow multiplication by a constant.

44:14.000 --> 44:19.000
For example, if you were to compute x and x, both a bit and b bit would depend on x,

44:19.000 --> 44:24.000
and so here you'd have x times x, or x squared, which is not mathematically linear.

44:24.000 --> 44:26.000
So we're not going to use multiplication, but we do have a nice trick,

44:26.000 --> 44:29.000
which is to add the bits together and then shift down by 1.

44:29.000 --> 44:34.000
If we look at the truth table, we see that this only produces a 1 when both of the inputs were 1.

44:34.000 --> 44:40.000
I take the resulting bit and multiply it by that round's scale, which is a power of 2, a constant,

44:40.000 --> 44:42.000
and then I just add all of those up.

44:42.000 --> 44:46.000
Since the components will be 0 everywhere except for that one bit, plus is equivalent to or.

44:46.000 --> 44:49.000
Ah, this reminds me of a slip-up in one of my previous videos

44:49.000 --> 44:54.000
where I was computing the or function using and and xor and plus.

44:54.000 --> 45:00.000
It totally works, but millions of people wrote in to tell me that I could do it with another xor instead of plus,

45:00.000 --> 45:02.000
which would have been a little faster.

45:02.000 --> 45:05.000
But here plus is the right option. We don't have xor, it's not linear.

45:05.000 --> 45:09.000
I was just, like, foreshadowing this, getting you ready.

45:09.000 --> 45:11.000
Defeated.

45:11.000 --> 45:14.000
Anywho, that's all we need for bitwise and.

45:14.000 --> 45:18.000
It's a little involved, but it's a far cry from the 9,000 operations we did before

45:18.000 --> 45:20.000
just to test if a value is greater than 0.

45:20.000 --> 45:26.000
A spreckin of which we can now quickly test whether a value is exactly 0.

45:26.000 --> 45:30.000
We do this by negating the bits, subtracting from 255.

45:30.000 --> 45:34.000
Then we add one, and that'll only overflow if the original value was 0.

45:34.000 --> 45:38.000
With that, testing whether two values are equal is just a matter of subtracting them

45:38.000 --> 45:40.000
and then seeing whether the result is 0.

45:40.000 --> 45:42.000
And that's how it goes. That's how it always goes.

45:42.000 --> 45:45.000
You build up some constructs, and you use those to make some more.

45:45.000 --> 45:48.000
You gain more and more power until you have all of the things you want.

45:48.000 --> 45:52.000
There are some good puzzles in here, and you may enjoy trying to work some of these out yourself.

45:52.000 --> 45:54.000
And you may improve upon them, and please tell me if you do.

45:54.000 --> 45:57.000
For example, on screen I'm showing you a straightforward way to do if,

45:57.000 --> 46:01.000
but if you check my code, I do a thing that's way more mysterious and fancy

46:01.000 --> 46:04.000
in order to squeeze the last bits of performance out of it.

46:04.000 --> 46:07.000
And I am going to care about performance for this application.

46:07.000 --> 46:12.000
The last time I made a computer out of floating point numbers, which did happen before,

46:12.000 --> 46:17.000
this computer was focused on beauty with no concessions to practicality.

46:17.000 --> 46:21.000
Frankly, the computer was sort of boring to use because it had no I.O.

46:21.000 --> 46:23.000
and it didn't do anything you could observe.

46:23.000 --> 46:25.000
So this time I want to do the opposite.

46:25.000 --> 46:29.000
I'm willing to make some concessions on beauty as long as the result is entertaining.

46:29.000 --> 46:33.000
Now the most entertaining computer is the Nintendo Entertainment System.

46:33.000 --> 46:35.000
And so this is a natural choice.

46:35.000 --> 46:41.000
After all, I like to work at the intersection of theory and impractice and practice and entertainment.

46:41.000 --> 46:45.000
The Nintendo Entertainment System consists of a basically reasonable computer

46:45.000 --> 46:48.000
and a bunch of other weird stuff for entertainment purposes only.

46:48.000 --> 46:54.000
The core of the computer is an 8-bit microprocessor that's more or less the Motorola 6502.

46:54.000 --> 46:58.000
And that other stuff includes video and audio hardware and the controllers and the game cartridge,

46:58.000 --> 47:02.000
which itself might include hardware and stuff like that.

47:02.000 --> 47:07.000
My goal is to replace that 8-bit microprocessor with something that only runs linear floating point operations.

47:07.000 --> 47:09.000
So I'm not going to implement any of the weird stuff.

47:09.000 --> 47:12.000
And that's good because I'm going to do this in a software emulator,

47:12.000 --> 47:14.000
which is my own hacked up copy of FCE Ultra,

47:14.000 --> 47:16.000
and this emulator is so complicated.

47:16.000 --> 47:19.000
But the code that emulates the processor is basically tractable.

47:19.000 --> 47:23.000
The processor state consists of a small number of 8-bit registers,

47:23.000 --> 47:26.000
each of which will represent with a fluent 8.

47:26.000 --> 47:28.000
There's also a 16-bit program counter.

47:28.000 --> 47:30.000
We'll only need a few 16-bit operations,

47:30.000 --> 47:34.000
and it's quite easy to build 16-bit integers using two 8-bit integers.

47:34.000 --> 47:36.000
So I won't say any more about that.

47:36.000 --> 47:38.000
And at a high level, the processor is just a loop.

47:38.000 --> 47:41.000
It reads one byte from memory at the program counter,

47:41.000 --> 47:45.000
which tells it which of the 256 instructions it's going to run next.

47:45.000 --> 47:49.000
It runs that instruction, which updates the state like the registers in the program counter,

47:49.000 --> 47:51.000
and then starts the loop again.

47:51.000 --> 47:53.000
Of course, there are copious details here.

47:53.000 --> 47:56.000
First, let's look at a simple instruction so you can kind of see how it goes.

47:56.000 --> 47:58.000
A really simple instruction is tax.

47:58.000 --> 48:02.000
And speaking of tax, I'll have you know that video editing is so tedious

48:02.000 --> 48:07.000
that while making this video, I actually procrastinated it by doing my taxes.

48:07.000 --> 48:12.000
Anyway, TAX on the 6502 transfers the value from the register A to the register X.

48:12.000 --> 48:14.000
There are still several steps, though.

48:14.000 --> 48:18.000
After we copy it over, we need to update the negative and zero bits of the processor flags

48:18.000 --> 48:20.000
and increment the program counter.

48:20.000 --> 48:24.000
But this code is actually quite nice because we've already done all the work of implementing 8-bit integers.

48:24.000 --> 48:28.000
It makes use of bitwise AND and is zero and shifting and so on.

48:28.000 --> 48:31.000
If all the instructions were like that, this thing would be really simple.

48:31.000 --> 48:33.000
So let's look at a harder instruction.

48:33.000 --> 48:36.000
This is a branching instruction, branch on conditions set.

48:36.000 --> 48:41.000
It modifies the program counter to basically do a jump if one of the processor flags is set.

48:41.000 --> 48:43.000
Otherwise, it just advances to the next instruction.

48:43.000 --> 48:48.000
First problem we'll see is that there's a branch in the implementation of the processor, which is not linear.

48:48.000 --> 48:50.000
That's this if else.

48:50.000 --> 48:52.000
But we do have a fluent 8 version of if.

48:52.000 --> 48:57.000
So we can change this to update the program counter, but to a value that depends on the condition.

48:57.000 --> 48:58.000
That'll look like this.

48:58.000 --> 49:01.000
But the other problem is this memory access.

49:01.000 --> 49:07.000
Now the Nintendo has a main memory of 2 kilobytes, and we could create 2,000 fluent 8s and implement this array subscript.

49:07.000 --> 49:09.000
That's not really the problem.

49:09.000 --> 49:11.000
The problem is that accessing memory has side effects.

49:11.000 --> 49:16.000
So in the previous version of this code where an if wasn't executed, we wouldn't have accessed the memory.

49:16.000 --> 49:19.000
This is sort of obvious for memory writes because writing changes memory.

49:19.000 --> 49:24.000
Less obvious is that writes and reads often have side effects because of memory mapped IO.

49:24.000 --> 49:31.000
For example, writing 2 bytes to 2006 will load them as an address into the PPU, that's one of those weird things.

49:31.000 --> 49:36.000
And then writes that happen to 2007 will pass through to video memory at that address.

49:36.000 --> 49:44.000
Or writing to 4014 will start a DMA that transfers 256 bytes to video memory and stalls the processor for 512 cycles.

49:44.000 --> 49:46.000
So these are not small things.

49:46.000 --> 49:53.000
And worse, there isn't even a small set of them because lots of cartridges have hardware inside them that does arbitrary stuff on reads and writes.

49:53.000 --> 49:56.000
And weirdos are making new weird cartridges all the time.

49:56.000 --> 49:58.000
And so here we have the main concession.

49:58.000 --> 50:02.000
The emulator API for this chip offers a conditional read and write.

50:02.000 --> 50:05.000
These take the address, but also a condition.

50:05.000 --> 50:07.000
If the condition is true, you do what you'd expect.

50:07.000 --> 50:10.000
But if it's false, nothing happens and an arbitrary value is returned.

50:10.000 --> 50:14.000
Of course, this isn't linear, but it's not really that unrealistic if we were making a processor.

50:14.000 --> 50:19.000
We would just wire this through to the memory controller, which would then ignore the read or write if the bit isn't set.

50:19.000 --> 50:24.000
The real 6502, for example, has a pin that indicates whether it's doing a read or a write.

50:24.000 --> 50:26.000
But I accept your criticism.

50:26.000 --> 50:28.000
Feel free to defeat me by doing without this.

50:28.000 --> 50:33.000
Another challenge is that the 6502 has a load of undocumented and really weird instructions.

50:33.000 --> 50:38.000
And this wouldn't be so bad except that the emulator source code I'm working from is extremely hard to understand.

50:38.000 --> 50:46.000
It's filled with all sorts of macro hacks that assume specific variable names, references to mysterious global variables like temp and foo,

50:46.000 --> 50:52.000
pirate jokes, feuds between developers commenting out each other's wrong code, and so on.

50:52.000 --> 50:57.000
And unfortunately, I don't have any Nintendo games that actually execute many of these instructions.

50:57.000 --> 51:03.000
So in the course of development, I made my own cartridge that executes a whole bunch of undocumented instructions when it starts up.

51:03.000 --> 51:08.000
It displays the results of those on the screen so that I can test whether my implementation matches the reference.

51:08.000 --> 51:15.000
This cartridge might be the world's most boring Nintendo game, even more boring than Wall Street Kid.

51:15.000 --> 51:17.000
Here's what that game looks like.

51:17.000 --> 51:19.000
You can't win it or even play it.

51:19.000 --> 51:22.000
It exists only to destroy your mind.

51:22.000 --> 51:25.000
The last puzzle to solve is instruction dispatch.

51:25.000 --> 51:29.000
When we read the instruction byte, we look at it and decide which instruction to execute.

51:29.000 --> 51:33.000
The natural way to implement this is with a switch statement and a case for each instruction.

51:33.000 --> 51:35.000
But that of course is not linear.

51:35.000 --> 51:37.000
We can't do any branching of control flow.

51:37.000 --> 51:41.000
We have to execute the same series of additions and multiplications each time.

51:41.000 --> 51:45.000
So what I'll do is execute every single instruction every time.

51:45.000 --> 51:47.000
Now I only want the right instruction to do anything.

51:47.000 --> 51:53.000
So the first thing I do is make 256 copies of the CPU state, basically the registers.

51:53.000 --> 51:55.000
That's a finite number of variables.

51:55.000 --> 51:57.000
I also have an active flag for each one of those.

51:57.000 --> 52:01.000
And exactly one of those active flags will be set to one for the correct instruction.

52:01.000 --> 52:05.000
Then I run all of the instructions on their own copies of the CPU state.

52:05.000 --> 52:09.000
If I do any conditional reads or write, I include the active flag in the condition.

52:09.000 --> 52:11.000
So only the correct effects will happen.

52:11.000 --> 52:17.000
So then I have the resulting 256 states and I need to copy the one from the correct instruction back into the main state.

52:17.000 --> 52:21.000
The way to do this is to zero them all out except for the active one.

52:21.000 --> 52:23.000
And we can do that with if and then just sum them all up.

52:23.000 --> 52:26.000
They'll all be zero except for the correct one, so we'll get the right answer.

52:26.000 --> 52:30.000
Now it's kind of annoying to run every instruction on every tick of the CPU.

52:30.000 --> 52:33.000
And this technique is the main reason that it's not going to be that fast.

52:33.000 --> 52:34.000
But it is completely linear.

52:34.000 --> 52:37.000
Another upside is that each instruction is completely independent.

52:37.000 --> 52:39.000
So they can actually be run in parallel.

52:39.000 --> 52:41.000
So let's start up the benchmark.

52:41.000 --> 52:43.000
Super Mario Brothers.

52:43.000 --> 52:47.000
Here it's splitting the instructions across eight cores running in parallel.

52:47.000 --> 52:51.000
If not for that instruction dispatch, this thing would run at playable frame rates.

52:51.000 --> 52:53.000
But, and yes, it is already running.

52:53.000 --> 52:56.000
The cost of not cheating is that it runs pretty slow.

52:56.000 --> 52:59.000
The hardware Nintendo runs at 60 frames per second.

52:59.000 --> 53:04.000
And the emulator free to run non-linear instructions gets 3,500 frames per second.

53:04.000 --> 53:15.000
But the linear version, and I did do a lot of optimization, gets 0.11 frames per second or 8.6 seconds per frame.

53:15.000 --> 53:17.000
Which ain't fast.

53:17.000 --> 53:20.000
Maybe you could help me out by putting this video in 2x speed.

53:20.000 --> 53:28.000
I will say, though, in comparison that I have played AAA titles that at launch, inexplicably, on high-end hardware had comparable frame rates.

53:28.000 --> 53:33.000
And they were no doubt executing a great many non-linear instructions.

53:33.000 --> 53:39.000
Speed aside, we now have a general-purpose computer, which renders everything we've done up until this point moot.

53:39.000 --> 53:43.000
If we want a non-linear transfer function, we can just implement the hyperbolic tangent.

53:43.000 --> 53:48.000
If we want fractals, we can just write code that draws the Mandelbrot set on the Nintendo.

53:48.000 --> 53:51.000
We can just write a good encryption algorithm like AES.

53:51.000 --> 53:53.000
We can have a chess engine with search.

53:53.000 --> 53:55.000
In fact, we already have one.

53:55.000 --> 53:58.000
Chessmaster for the NES was included in our tournament already.

53:58.000 --> 54:02.000
And by running it on our linear emulator, we have a linear model.

54:02.000 --> 54:04.000
So it seems I defeated even myself.

54:04.000 --> 54:07.000
And I can finally be done with this damn thing.

54:10.000 --> 54:13.000
Alright, so what have we learned today?

54:13.000 --> 54:15.000
It's the same thing we learned every time.

54:15.000 --> 54:20.000
Complexity is everywhere, even with something as simple as plus and multiplication by constants,

54:20.000 --> 54:23.000
which mathematically can only create lines.

54:23.000 --> 54:29.000
Given a tiny foothold by way of rounding error, we can bend them to our will and make them do anything.

54:29.000 --> 54:32.000
And I think this is the same story of computer science.

54:32.000 --> 54:36.000
Complexity from simplicity, and maybe even of the universe.

54:36.000 --> 54:39.000
So don't underestimate simple things put together.

54:39.000 --> 54:42.000
Anyway, if you made it this far, thank you for watching.

54:42.000 --> 54:43.000
Thank you for your attention.

54:43.000 --> 54:47.000
And if you didn't make it this far, I don't even know what we're talking about.

54:47.000 --> 54:49.000
I'm sure I'll be back soon with more stupid stuff.

54:49.000 --> 54:54.000
In any case, I've been Tom7, and this was Impractical Engineering.

54:54.000 --> 54:55.000
See you soon.

