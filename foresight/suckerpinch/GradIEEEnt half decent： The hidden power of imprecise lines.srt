1
00:00:00,000 --> 00:00:02,000
Imagine you're my professor.

2
00:00:02,000 --> 00:00:04,000
Maybe you actually were my professor,

3
00:00:04,000 --> 00:00:08,000
in which case you may already be sweating before I say anymore.

4
00:00:08,000 --> 00:00:10,000
The subject matter is neural networks.

5
00:00:10,000 --> 00:00:15,000
You draw an illustration on the board with a node's inputs and its outputs via transfer function.

6
00:00:15,000 --> 00:00:19,000
You inform us of this mathematical fact that the transfer function cannot be linear,

7
00:00:19,000 --> 00:00:24,000
or the whole model would reduce to a linear function.

8
00:00:24,000 --> 00:00:26,000
I immediately raise my hand.

9
00:00:26,000 --> 00:00:29,000
The speed with which I raise it and the not very subtle forward pose

10
00:00:29,000 --> 00:00:33,000
suggests that I want to pluck an abstract idea from the whiteboard and pervert it.

11
00:00:33,000 --> 00:00:36,000
You know this look, and you are reluctant to call on me.

12
00:00:36,000 --> 00:00:38,000
But no other students are raising their hands.

13
00:00:38,000 --> 00:00:40,000
You have no choice.

14
00:00:40,000 --> 00:00:41,000
Tom.

15
00:00:41,000 --> 00:00:43,000
It's more like a statement than a question.

16
00:00:43,000 --> 00:00:46,000
It includes the tone of spoken punctuation that, if it could,

17
00:00:46,000 --> 00:00:49,000
ends the entire conversation before it begins.

18
00:00:49,000 --> 00:00:52,000
I go on and on about some technicality.

19
00:00:52,000 --> 00:00:56,000
That due to approximate math on the computer, this mathematical fact won't be true.

20
00:00:56,000 --> 00:01:01,000
You say, okay, technically that's right, but for all practical purposes it doesn't matter.

21
00:01:01,000 --> 00:01:05,000
And I say, well, what about impractical purposes?

22
00:01:05,000 --> 00:01:09,000
And you, in a moment of weakness, vigorously strangle me.

23
00:01:09,000 --> 00:01:11,000
And that's how I died.

24
00:01:11,000 --> 00:01:14,000
Murdered in cold blood.

25
00:01:14,000 --> 00:01:19,000
That was about 20 years ago, but the world will not let us stop thinking about neural networks.

26
00:01:19,000 --> 00:01:22,000
We're really just pressing all the gas pedals at once on this one,

27
00:01:22,000 --> 00:01:25,000
heading towards a utopia or a dystopia.

28
00:01:25,000 --> 00:01:28,000
Some kind of topia, for sure. We're getting there real fast.

29
00:01:28,000 --> 00:01:30,000
So this question has been on my mind for some time.

30
00:01:30,000 --> 00:01:32,000
And just to be clear, the professor is right.

31
00:01:32,000 --> 00:01:36,000
I might be technically correct here, but it doesn't matter for practical purposes.

32
00:01:36,000 --> 00:01:40,000
But I like to work at the intersection of theory and impractice.

33
00:01:40,000 --> 00:01:43,000
And so by doing a lot of work, we can make it matter.

34
00:01:43,000 --> 00:01:46,000
And then I'll be even more right, both theoretically right,

35
00:01:46,000 --> 00:01:49,000
and it will only matter for most practical purposes.

36
00:01:49,000 --> 00:01:52,000
So in this video, in its lengthy accompanying technical report,

37
00:01:52,000 --> 00:01:56,000
I have an exhaustive exploration of what you can get away with.

38
00:01:56,000 --> 00:02:00,000
And I'll see how we can absolutely use linear transfer functions in neural networks

39
00:02:00,000 --> 00:02:03,000
and all sorts of other things where they shouldn't be enough.

40
00:02:03,000 --> 00:02:06,000
I'm Tom Seven, and this is Impractical Engineering.

41
00:02:06,000 --> 00:02:10,000
Okay, let's repeat the professor's lesson so we can understand the nature of the dispute.

42
00:02:10,000 --> 00:02:13,000
If you feel like you already know everything about neural networks,

43
00:02:13,000 --> 00:02:16,000
this section is safely skippable, but so is the whole video.

44
00:02:16,000 --> 00:02:21,000
So fundamentally, a neural network takes in inputs, which are a bunch of numbers,

45
00:02:21,000 --> 00:02:25,000
and transforms those numbers, and then outputs some other numbers.

46
00:02:25,000 --> 00:02:28,000
In this drawing, I have three inputs and one output.

47
00:02:28,000 --> 00:02:33,000
So every one of these circles is going to be filled in with some number as we run the network.

48
00:02:33,000 --> 00:02:37,000
So call the inputs x, y, z, and let's just look at how r is computed.

49
00:02:37,000 --> 00:02:38,000
That's that middle one.

50
00:02:38,000 --> 00:02:41,000
We start with a weighted sum of x, y, and z.

51
00:02:41,000 --> 00:02:45,000
So we take all the inputs, we multiply each one by some weight, and add those together.

52
00:02:45,000 --> 00:02:48,000
And these weights are determined when we train the network.

53
00:02:48,000 --> 00:02:49,000
At this point, they're just constants.

54
00:02:49,000 --> 00:02:51,000
When we're running the network, they're just constants.

55
00:02:51,000 --> 00:02:55,000
We also learn a bias parameter, which becomes a constant, and that just gets added in as well.

56
00:02:55,000 --> 00:02:58,000
The important part for today is this transfer function, tf.

57
00:02:58,000 --> 00:03:03,000
This gets applied to the weighted sum, and it transforms it, in this case, with a sigmoid.

58
00:03:03,000 --> 00:03:08,000
And the intuition here is somehow that this node r, this neuron r,

59
00:03:08,000 --> 00:03:10,000
fires with some probability.

60
00:03:10,000 --> 00:03:12,000
That depends on its connection with these other neurons.

61
00:03:12,000 --> 00:03:17,000
But because it's a probability, it ranges from zero to one

62
00:03:17,000 --> 00:03:19,000
instead of, like, negative infinity to infinity.

63
00:03:19,000 --> 00:03:23,000
And so the more the input ones fire, the more likely this one is to fire.

64
00:03:23,000 --> 00:03:25,000
That was the classic idea, anyway.

65
00:03:25,000 --> 00:03:30,000
These days, pretty much everyone uses the rectified linear transfer function.

66
00:03:30,000 --> 00:03:34,000
It's super simple to implement, and for various reasons, it actually works better,

67
00:03:34,000 --> 00:03:36,000
especially for the internal layers.

68
00:03:36,000 --> 00:03:38,000
And actually, all sorts of functions will work here.

69
00:03:38,000 --> 00:03:41,000
It needs to be differentiable because of the way we train these things.

70
00:03:41,000 --> 00:03:46,000
But the only other apparently necessary quality is that the function be nonlinear.

71
00:03:46,000 --> 00:03:48,000
At least so says the professor.

72
00:03:48,000 --> 00:03:50,000
Now, the reason for this is mathematically nice.

73
00:03:50,000 --> 00:03:52,000
Let's look at the formula for r again.

74
00:03:52,000 --> 00:03:58,000
And let's say the transfer function is linear, so it's like mx plus b2.

75
00:03:58,000 --> 00:04:02,000
Then you can multiply m by all these terms, and you get another linear function.

76
00:04:02,000 --> 00:04:07,000
So r is a linear function of the inputs, and then so is q, and then so is s.

77
00:04:07,000 --> 00:04:10,000
And then o is a linear function of q, r, and s.

78
00:04:10,000 --> 00:04:15,000
And what this would mean is that the output would just equal some linear function of the inputs.

79
00:04:15,000 --> 00:04:18,000
And all of this complexity of the neural network would just simplify away.

80
00:04:18,000 --> 00:04:20,000
We wouldn't need any of the hidden layers.

81
00:04:20,000 --> 00:04:22,000
We would just have a function of the input layer.

82
00:04:22,000 --> 00:04:27,000
There are lots of functions such as XOR that can't be approximated by linear functions like this.

83
00:04:27,000 --> 00:04:33,000
We definitely want our neural networks to be able to model things that are complicated, like XOR or human thought.

84
00:04:33,000 --> 00:04:35,000
So the story goes.

85
00:04:35,000 --> 00:04:38,000
So that would be true if we were using real math.

86
00:04:38,000 --> 00:04:43,000
On a computer, we're going to use IEEE floating point, which isn't associative or distributive.

87
00:04:43,000 --> 00:04:47,000
So if we simplify the whole network, we won't actually get the same result.

88
00:04:47,000 --> 00:04:54,000
So my goal today will be to create a transfer function that, despite being mathematically linear, will not be computationally linear.

89
00:04:54,000 --> 00:04:58,000
And thus, I'll be able to use it to train models that have interesting behavior.

90
00:04:58,000 --> 00:05:02,000
Now, my smart math friend Jason, who probably makes the professor sweat even more,

91
00:05:02,000 --> 00:05:08,000
reminds me that this is actually an affine function because I add something at the end.

92
00:05:08,000 --> 00:05:09,000
That's fine. I'm going to call it linear.

93
00:05:09,000 --> 00:05:13,000
He refers to this as high school linear in a pejorative way, and that's fine.

94
00:05:13,000 --> 00:05:17,000
I'm comfortable with that. In a lot of ways, I'm mentally still in high school.

95
00:05:17,000 --> 00:05:23,000
So this means two operations, addition and multiplication by constants or scaling.

96
00:05:23,000 --> 00:05:25,000
Them's the rules.

97
00:05:25,000 --> 00:05:30,000
Or equivalently, would it simplify mathematically to a polynomial of at most degree one?

98
00:05:30,000 --> 00:05:34,000
So 3x plus 2x all times five, that would simplify.

99
00:05:34,000 --> 00:05:38,000
But 2x times x would yield 2x squared, and that's a degree two polynomial.

100
00:05:38,000 --> 00:05:41,000
So that's disallowed. Okay?

101
00:05:41,000 --> 00:05:48,000
Floating point comes in a number of different spice levels, corresponding to how many bits you're using to represent it.

102
00:05:48,000 --> 00:05:53,000
So you may be familiar with double and float. Those are 64 and 32 bits.

103
00:05:53,000 --> 00:05:56,000
Half precision is 16 bits, and it gets even lower.

104
00:05:56,000 --> 00:06:01,000
It's usually used because then you need half as much memory to store your numbers.

105
00:06:01,000 --> 00:06:07,000
That'll be good for us. We'll be happy to save the memory, but the real reason to use half precision is that it is less precise.

106
00:06:07,000 --> 00:06:11,000
And imprecision is going to be a desirable quality in this work.

107
00:06:11,000 --> 00:06:16,000
Being only 16 bit, there are 65,000 different values that we could represent.

108
00:06:16,000 --> 00:06:18,000
So it's clearly not all of the numbers.

109
00:06:18,000 --> 00:06:25,000
This is an exponential format, so the main thing to remember about floating point precision is that there's more numbers near zero than elsewhere.

110
00:06:25,000 --> 00:06:33,000
So when you get to the largest finite numbers, like 65,504, only multiples of 32 are even representable.

111
00:06:33,000 --> 00:06:37,000
Between 2048 and 4096, only even numbers are there.

112
00:06:37,000 --> 00:06:39,000
Below that, only integers.

113
00:06:39,000 --> 00:06:42,000
And actually, most of the action happens near zero, where you get a lot of fractions.

114
00:06:42,000 --> 00:06:46,000
Now this stuff about comparing against epsilon is okay, but it's kind of naive.

115
00:06:46,000 --> 00:06:49,000
Like for one thing, what is epsilon supposed to be?

116
00:06:49,000 --> 00:06:52,000
If you're working with really small numbers, you can use a really small epsilon.

117
00:06:52,000 --> 00:06:59,000
But if you're working with larger numbers, you might need to use an epsilon of up to 32, or maybe half that, for half precision.

118
00:06:59,000 --> 00:07:03,000
Actually, a while back, I wrote a paper called What if Anything is Epsilon?

119
00:07:03,000 --> 00:07:09,000
Where I looked at what programmers picked in practice for their value of epsilon by going through a whole bunch of code on GitHub.

120
00:07:09,000 --> 00:07:14,000
I enjoyed laughing at their bugs, like minus 1e10, which is negative 10 billion.

121
00:07:14,000 --> 00:07:18,000
They meant 1e-10, 1 over 10 billion.

122
00:07:18,000 --> 00:07:20,000
And I like to compare these by language.

123
00:07:20,000 --> 00:07:25,000
For example, I found that JavaScript programmers were the most tolerant of error, which makes sense.

124
00:07:25,000 --> 00:07:26,000
They're pretty sloppy.

125
00:07:26,000 --> 00:07:29,000
One programmer picked 6 million on purpose, that wasn't a typo.

126
00:07:29,000 --> 00:07:33,000
Well, a lot of stuff will be equal if you use a really large epsilon.

127
00:07:33,000 --> 00:07:36,000
But anyway, the error you get is not like random.

128
00:07:36,000 --> 00:07:38,000
These have a much more useful definition.

129
00:07:38,000 --> 00:07:45,000
If I take two numbers and add them using IEEE floating point, the answer is defined to be the real math answer, x plus y,

130
00:07:45,000 --> 00:07:48,000
but rounded to the nearest representable floating point number.

131
00:07:48,000 --> 00:07:50,000
So if the result is high, I might need to round a lot.

132
00:07:50,000 --> 00:07:54,000
If the result is small, I might need to round a small amount.

133
00:07:54,000 --> 00:07:56,000
I also might not need to round at all.

134
00:07:56,000 --> 00:08:01,000
Like 2 plus 2 is literally equal to 4 exactly in floating point, like you'd want.

135
00:08:01,000 --> 00:08:03,000
And we're going to use that kind of thing later.

136
00:08:03,000 --> 00:08:07,000
So the most important thing to remember about floating point for this project is,

137
00:08:07,000 --> 00:08:11,000
rounding error depends on how far you are out on the number line.

138
00:08:11,000 --> 00:08:15,000
Large error for large numbers, small error for small numbers.

139
00:08:15,000 --> 00:08:17,000
But it also depends on the specific value.

140
00:08:17,000 --> 00:08:19,000
Not everything needs to be rounded.

141
00:08:19,000 --> 00:08:24,000
Surrounding is a complicated function, and we're going to abuse that complexity in order to get behavior that we like.

142
00:08:24,000 --> 00:08:29,000
Let's look at how we can start abusing the imprecision of floating point numbers.

143
00:08:29,000 --> 00:08:33,000
We just have addition and multiplication by constants, so we're going to try both of those.

144
00:08:33,000 --> 00:08:40,000
Here's what happens if I add 128 to the input and then subtract 128 back out.

145
00:08:40,000 --> 00:08:44,000
Mathematically, of course, this is just the identity function. I get back the input.

146
00:08:44,000 --> 00:08:48,000
But because there aren't that many values representable near 128,

147
00:08:48,000 --> 00:08:52,000
we can only get back eight different values between 0 and 1.

148
00:08:52,000 --> 00:08:56,000
Note that the zigzag is finer in the negative region than in the positive region.

149
00:08:56,000 --> 00:09:00,000
If we look at the actual numbers that are representable near 128,

150
00:09:00,000 --> 00:09:05,000
we see that we have eighths above 128, but sixteenths below 128.

151
00:09:05,000 --> 00:09:08,000
So we get more precision in the negative region.

152
00:09:08,000 --> 00:09:13,000
Okay, that's plus. It's definitely not a line, but it's basically just a line.

153
00:09:13,000 --> 00:09:16,000
Also, this thing has a terrible derivative. It just has all these flat segments,

154
00:09:16,000 --> 00:09:20,000
so the derivative there is 0 and it's undefined at the discontinuities.

155
00:09:20,000 --> 00:09:23,000
So it's going to actually be pretty hard to use as a transfer function,

156
00:09:23,000 --> 00:09:25,000
but we're going to try it out anyway.

157
00:09:25,000 --> 00:09:27,000
Now multiplication looks a lot more subtle.

158
00:09:27,000 --> 00:09:30,000
Here I'm multiplying by 100 and then by 1 over 100,

159
00:09:30,000 --> 00:09:35,000
which also should give me back just the identity f of x equals x.

160
00:09:35,000 --> 00:09:38,000
So if we zoom in on this one, we'll start to see some detail.

161
00:09:38,000 --> 00:09:41,000
Actually, maybe that's just my laser printer.

162
00:09:41,000 --> 00:09:43,000
Well, that's the problem with using imprecise tools.

163
00:09:43,000 --> 00:09:45,000
Maybe we should do this on the computer.

164
00:09:45,000 --> 00:09:47,000
Okay, here we are on the computer.

165
00:09:47,000 --> 00:09:50,000
And if I zoom in on this line, and I got to zoom in a lot,

166
00:09:50,000 --> 00:09:53,000
near zero, it's pretty much perfect.

167
00:09:53,000 --> 00:09:55,000
But as we get near one, it gets a lot more jacked.

168
00:09:55,000 --> 00:09:59,000
And this is real imprecision, and it depends where you are on the number line.

169
00:09:59,000 --> 00:10:01,000
You get different rounding error.

170
00:10:01,000 --> 00:10:04,000
This is actually pretty hard to reason about, to be honest.

171
00:10:04,000 --> 00:10:07,000
So just suffice to say, when you multiply,

172
00:10:07,000 --> 00:10:09,000
you get a little bit of error all throughout the number line,

173
00:10:09,000 --> 00:10:11,000
but it depends on where you are.

174
00:10:11,000 --> 00:10:13,000
So that's multiplication.

175
00:10:13,000 --> 00:10:15,000
Now we can try to put these together in various ways.

176
00:10:15,000 --> 00:10:18,000
So I play with this a lot,

177
00:10:18,000 --> 00:10:22,000
and I produced a whole bunch of just totally bonkers functions.

178
00:10:22,000 --> 00:10:25,000
But actually the best shape that I was able to make

179
00:10:25,000 --> 00:10:27,000
came from just using multiplication.

180
00:10:27,000 --> 00:10:31,000
And what I do is I multiply by the first number that's smaller than 1.

181
00:10:31,000 --> 00:10:33,000
So it's just slightly less than 1.

182
00:10:33,000 --> 00:10:36,000
That's 1 minus 1 over 2048.

183
00:10:36,000 --> 00:10:38,000
I keep multiplying by that over and over again.

184
00:10:38,000 --> 00:10:41,000
And as I do, I accumulate more and more error in different parts of the graph.

185
00:10:41,000 --> 00:10:43,000
Of course, I'm also making the number smaller.

186
00:10:43,000 --> 00:10:47,000
So at the end, I want to normalize back so that f of 1 is 1.

187
00:10:47,000 --> 00:10:49,000
Here's what it looks like if I do that iteratively.

188
00:10:49,000 --> 00:10:51,000
Accumulating error.

189
00:10:51,000 --> 00:10:53,000
I found that 500 steps was a good stopping point.

190
00:10:53,000 --> 00:10:56,000
So this is a function that I call grad1.

191
00:10:56,000 --> 00:10:58,000
Grad for the name of this project, which I can't pronounce.

192
00:10:58,000 --> 00:11:01,000
Gradient half decent.

193
00:11:01,000 --> 00:11:05,000
I triply half gradient descent, you get it.

194
00:11:05,000 --> 00:11:08,000
And it has this nice zigzag shape.

195
00:11:08,000 --> 00:11:10,000
Importantly, it's kind of smooth.

196
00:11:10,000 --> 00:11:13,000
If we zoom in on it, we'll see that it's, you know, putting aside the pixels,

197
00:11:13,000 --> 00:11:15,000
that it's piecewise linear.

198
00:11:15,000 --> 00:11:17,000
So that's nice.

199
00:11:17,000 --> 00:11:20,000
Now you might wonder, why does it get this zigzag shape?

200
00:11:20,000 --> 00:11:23,000
And truth be told, round-off error is just kind of hard to reason about.

201
00:11:23,000 --> 00:11:25,000
Let me show you two illustrations that are at least nice to look at.

202
00:11:25,000 --> 00:11:29,000
In this rainbow, I've given each of the numbers between 0 and 1 a color on the x-axis.

203
00:11:29,000 --> 00:11:33,000
And then on the y-axis, I'm successively multiplying by that constant.

204
00:11:33,000 --> 00:11:37,000
And you could see that they get exponentially smaller as expected, but not smoothly.

205
00:11:37,000 --> 00:11:39,000
And these changes in direction come from different exponents,

206
00:11:39,000 --> 00:11:42,000
and we see some of that reflected in the zigzags.

207
00:11:42,000 --> 00:11:44,000
On this image, the x-axis is 0 to 1 again.

208
00:11:44,000 --> 00:11:47,000
The y-axis is successive multiplication by the constant.

209
00:11:47,000 --> 00:11:50,000
But the green pixels is when my round-off is too high compared to the correct result,

210
00:11:50,000 --> 00:11:52,000
and magenta when it's too low.

211
00:11:52,000 --> 00:11:54,000
This line at the top is at 500 iterations.

212
00:11:54,000 --> 00:11:58,000
And you can see how it slices both green and magenta regions.

213
00:11:58,000 --> 00:12:00,000
Too high and too low.

214
00:12:00,000 --> 00:12:04,000
One more thing, in order to train models with this thing, we need to know it's derivative.

215
00:12:04,000 --> 00:12:07,000
And for reasons of implementation tricks that I'm not going to get into,

216
00:12:07,000 --> 00:12:11,000
I actually need the derivative in terms of the y-coordinate instead of the x-coordinate.

217
00:12:11,000 --> 00:12:14,000
Now, I'm not good enough at math to figure this out analytically.

218
00:12:14,000 --> 00:12:18,000
In any way, it would probably just be a table of the values for these different segments.

219
00:12:18,000 --> 00:12:22,000
But since it's 16-bit and there's only 65,000 values that are possible,

220
00:12:22,000 --> 00:12:26,000
I can just use a computer program to compute the derivative for every point.

221
00:12:26,000 --> 00:12:28,000
So here that has plotted along the y-axis.

222
00:12:28,000 --> 00:12:30,000
I think it looks pretty cool like an oscilloscope.

223
00:12:30,000 --> 00:12:33,000
You'll notice that the derivative isn't a perfect square wave,

224
00:12:33,000 --> 00:12:39,000
and it wouldn't be because there are in fact little imperfections in this curve from round-off error.

225
00:12:39,000 --> 00:12:42,000
I'm actually applying a low-pass filter here, it would be even noisier.

226
00:12:42,000 --> 00:12:45,000
But anyway, now we've got the function and we've got its derivative,

227
00:12:45,000 --> 00:12:48,000
so we can do some machine learning.

228
00:12:48,000 --> 00:12:51,000
But first, a bonus digression.

229
00:12:51,000 --> 00:12:53,000
Here's a bonus digression.

230
00:12:53,000 --> 00:12:58,000
Having freed myself from the need to quote-unquote do math in order to differentiate functions,

231
00:12:58,000 --> 00:13:01,000
because I'm just going to generate a table programmatically,

232
00:13:01,000 --> 00:13:04,000
I can now consider all sorts of exotic transfer functions.

233
00:13:04,000 --> 00:13:08,000
I can even betray the central thesis of this work and consider functions that are not linear.

234
00:13:08,000 --> 00:13:13,000
One thing I think is really funny is when you use data sort of as the wrong type,

235
00:13:13,000 --> 00:13:17,000
you may be familiar with the fast inverse square root technique.

236
00:13:17,000 --> 00:13:22,000
I love that one, and I think it's worth considering if a transfer function even needs to use floating point operations

237
00:13:22,000 --> 00:13:24,000
in order to be implemented.

238
00:13:24,000 --> 00:13:27,000
I tried to find the fastest, simplest thing you could do that might work.

239
00:13:27,000 --> 00:13:30,000
My favorite was to treat the float as just 16 bits,

240
00:13:30,000 --> 00:13:34,000
shift them down by 2, and then treat that back as a float.

241
00:13:34,000 --> 00:13:37,000
For integers, shifting by 2 is just division by 4.

242
00:13:37,000 --> 00:13:40,000
But for a floating point number, since there are different fields within the word,

243
00:13:40,000 --> 00:13:42,000
this moves bits between fields.

244
00:13:42,000 --> 00:13:46,000
So for example, the sine bit gets moved into the exponent,

245
00:13:46,000 --> 00:13:51,000
which means you have a much larger exponent for negative numbers than for positive ones.

246
00:13:51,000 --> 00:13:54,000
The result will always be positive because we fill with zeros.

247
00:13:54,000 --> 00:13:57,000
Dividing the exponent by 4 has a logarithmic effect on the result,

248
00:13:57,000 --> 00:14:00,000
and then some of the exponent bits also go into the mantissa.

249
00:14:00,000 --> 00:14:03,000
So you get a kind of crazy function that looks like this.

250
00:14:03,000 --> 00:14:07,000
The negative values are much larger, as we said, and it logarithmically approaches zero.

251
00:14:07,000 --> 00:14:12,000
The positive region is actually a very small upward slope, which you can't see on this graph.

252
00:14:12,000 --> 00:14:17,000
But since the exponent will start with two zeros, these tend to be pretty small.

253
00:14:17,000 --> 00:14:21,000
This is the full range that's explored by the positive values,

254
00:14:21,000 --> 00:14:26,000
and you probably don't care, but here is its computed derivative in terms of the y-coordinate.

255
00:14:26,000 --> 00:14:30,000
So in the experiments which are coming up next, I'm going to also compare this transfer function.

256
00:14:30,000 --> 00:14:34,000
This wouldn't prove the professor wrong because it uses a forbidden operation,

257
00:14:34,000 --> 00:14:37,000
but it is about as fast as you could do anything on a computer.

258
00:14:37,000 --> 00:14:41,000
So if it does turn out to work, it might be a half-decent choice.

259
00:14:41,000 --> 00:14:46,000
To compare the transfer functions, I tried them out on different machine learning problems.

260
00:14:46,000 --> 00:14:51,000
Fortunately, I do have my own bespoke GPU-based system for training neural networks,

261
00:14:51,000 --> 00:14:56,000
which has appeared on this channel before in videos such as 30 weird chess algorithms.

262
00:14:56,000 --> 00:15:00,000
It's not that good, but it is the kind of thing you want if you're going to do a silly experiment like,

263
00:15:00,000 --> 00:15:02,000
what if deep learning, but worse?

264
00:15:02,000 --> 00:15:06,000
So I made a bunch of modifications for this project, for example, to do the forward step with half-precision,

265
00:15:06,000 --> 00:15:09,000
and to support these tabled transfer functions.

266
00:15:09,000 --> 00:15:14,000
Then I trained a network using the same structure and initialization, changing only the transfer function.

267
00:15:14,000 --> 00:15:18,000
The first problem is the MNIST digit recognition dataset.

268
00:15:18,000 --> 00:15:22,000
The original CAPTCHA, you get 50,000 labeled examples of these tiny digits,

269
00:15:22,000 --> 00:15:25,000
and you have to learn to predict the digits 0 through 9,

270
00:15:25,000 --> 00:15:28,000
and then there are 10,000 held-out examples to judge your accuracy on.

271
00:15:28,000 --> 00:15:31,000
I chose this classic problem partly for trollish reasons,

272
00:15:31,000 --> 00:15:33,000
because even at the time of publication decades ago,

273
00:15:33,000 --> 00:15:37,000
various techniques had already achieved extremely high accuracy.

274
00:15:37,000 --> 00:15:39,000
The networks I trained looked like this.

275
00:15:39,000 --> 00:15:43,000
They take in the input pixels, then there's a number of internal layers,

276
00:15:43,000 --> 00:15:47,000
and then a dense output layer with one output for each of the 10 digits.

277
00:15:47,000 --> 00:15:51,000
You can see the paper, the code for details, if you really want.

278
00:15:51,000 --> 00:15:56,000
But one important thing I want to point out for these experiments is that the output layer uses a strict linear transfer function,

279
00:15:56,000 --> 00:15:58,000
the identity, for each of the models.

280
00:15:58,000 --> 00:16:02,000
It's not a good choice for these categorical problems, but it allows the network to output any value,

281
00:16:02,000 --> 00:16:06,000
even if the transfer function, for example, only outputs positive numbers.

282
00:16:06,000 --> 00:16:09,000
And since it's linear, it complies with our goal of proving the professor wrong.

283
00:16:09,000 --> 00:16:13,000
Throughout the rest of the network, all the internal layers use the transfer function that we're studying.

284
00:16:13,000 --> 00:16:17,000
So I trained one of these models with the same initial conditions and the same data,

285
00:16:17,000 --> 00:16:19,000
but using a different transfer function.

286
00:16:19,000 --> 00:16:22,000
I do that for 200,000 rounds, which takes about a day each.

287
00:16:22,000 --> 00:16:26,000
We can then compare the final accuracy and other dimensions, such as their aesthetics.

288
00:16:26,000 --> 00:16:28,000
The functions are as follows.

289
00:16:28,000 --> 00:16:32,000
We have two classic sigmoids, the hyperbolic tangent and the logistic function.

290
00:16:32,000 --> 00:16:35,000
After that, the rectified linear unit.

291
00:16:35,000 --> 00:16:39,000
Here I'm using a leaky version where the region below zero actually has a small slope.

292
00:16:39,000 --> 00:16:41,000
That seems to work better for me.

293
00:16:41,000 --> 00:16:43,000
This function is very popular today.

294
00:16:43,000 --> 00:16:46,000
None of those functions are linear, as expected by the professor.

295
00:16:46,000 --> 00:16:49,000
Then we have a couple that abuse floating point round off error.

296
00:16:49,000 --> 00:16:50,000
First a really simple one.

297
00:16:50,000 --> 00:16:52,000
I add 64 and subtract 64.

298
00:16:52,000 --> 00:16:54,000
We saw how that discretizes the line.

299
00:16:54,000 --> 00:17:01,000
Then the grad one function, which multiplies by a number near one 500 times in order to smoothly magnify the round off error.

300
00:17:01,000 --> 00:17:05,000
Then we have our bonus content, downshift two, which manipulates bits directly.

301
00:17:05,000 --> 00:17:11,000
Finally, we'll evaluate the identity function, which is what the professor thinks a linear model must be equivalent to.

302
00:17:11,000 --> 00:17:14,000
On the MNIST problem, all of the transfer functions do well.

303
00:17:14,000 --> 00:17:17,000
As expected, the classics are nearing 100% accuracy.

304
00:17:17,000 --> 00:17:22,000
Even a simple linear model using the identity function gets like 82%.

305
00:17:22,000 --> 00:17:27,000
Plus 64, which gets a little bit of non-linearity with round off error outperforms it slightly.

306
00:17:27,000 --> 00:17:31,000
But the nice smooth grad one function is almost in the same class as the classic functions.

307
00:17:31,000 --> 00:17:33,000
It's working quite well.

308
00:17:33,000 --> 00:17:38,000
So it seems like our hypothesis is panning out, and I can sense the professor beginning to sweat.

309
00:17:38,000 --> 00:17:41,000
The next problem is the SIFAR-10 dataset.

310
00:17:41,000 --> 00:17:46,000
This is a lot like MNIST, but instead of recognizing digits, you have to recognize spirit animals.

311
00:17:46,000 --> 00:17:48,000
There are 10 spirit animals.

312
00:17:48,000 --> 00:17:54,000
Airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.

313
00:17:54,000 --> 00:17:55,000
This problem is much harder.

314
00:17:55,000 --> 00:17:59,000
With these tiny thumbnails, I sometimes can't figure out what it is.

315
00:17:59,000 --> 00:18:00,000
But same idea.

316
00:18:00,000 --> 00:18:02,000
Days later, we get results.

317
00:18:02,000 --> 00:18:06,000
The ranking here is the same for this problem, and we can draw basically the same conclusion.

318
00:18:06,000 --> 00:18:10,000
An accuracy of 53 doesn't sound that good, but keep in mind there are 10 different classes.

319
00:18:10,000 --> 00:18:13,000
So if you just guess randomly, that's an accuracy of 10%.

320
00:18:13,000 --> 00:18:15,000
So we are substantially learning here.

321
00:18:15,000 --> 00:18:19,000
But another way to understand our accuracy is to compare it to what's come before us.

322
00:18:19,000 --> 00:18:24,000
These are standardized problems, and so a lot of researchers have posted their results.

323
00:18:24,000 --> 00:18:28,000
So I can check the leaderboard and scrolling all the way down to the bottom.

324
00:18:28,000 --> 00:18:34,000
I can see that my results are, in fact, the worst result of all time.

325
00:18:34,000 --> 00:18:35,000
That's not too bad.

326
00:18:35,000 --> 00:18:38,000
Last place is the last winner.

327
00:18:38,000 --> 00:18:43,000
Now putting aside the aesthetic and ideological considerations, there is something to this.

328
00:18:43,000 --> 00:18:48,000
Recently, I feel like deep learning is getting a little too good, a little too fast.

329
00:18:48,000 --> 00:18:51,000
So maybe we could just slow it down a bit.

330
00:18:51,000 --> 00:18:54,000
The third problem is, of course, chess.

331
00:18:54,000 --> 00:19:00,000
I take millions of positions from a public database, and I ask Stockfish a strong chess engine, which side is winning?

332
00:19:00,000 --> 00:19:02,000
There's two classes of results here.

333
00:19:02,000 --> 00:19:07,000
One side could have an edge, and it's standard to give this advantage in terms of pawns.

334
00:19:07,000 --> 00:19:15,000
Or it could be of the form mate in negative 24, which means that black, because it's negative, has a mate in 24 moves, no matter what white does.

335
00:19:15,000 --> 00:19:19,000
Mate is always favorable to a mirror edge, no matter how big your advantage is.

336
00:19:19,000 --> 00:19:26,000
One thing that's funny to me about this pawn score is that the advantage can be sort of arbitrarily high if Stockfish can't find a mate.

337
00:19:26,000 --> 00:19:31,000
So it can give you more than 64 pawn advantage, which is funny because how are you even going to fit those on the board?

338
00:19:31,000 --> 00:19:32,000
Dual wheeled?

339
00:19:32,000 --> 00:19:34,000
Actually, here's an exercise for the reader.

340
00:19:34,000 --> 00:19:40,000
Find a position with the largest possible advantage according to Stockfish, but where it can't find mate.

341
00:19:40,000 --> 00:19:42,000
Here's the best that I could do.

342
00:19:42,000 --> 00:19:46,000
Huge advantage for white, but no mate even after depth 89.

343
00:19:46,000 --> 00:19:54,000
I mapped these scores into the interval from negative 1 to 1, where negative 1 is the best possible result for black, me and 1, and plus 1 is the same for white.

344
00:19:54,000 --> 00:19:58,000
And this gives me a machine learning problem, which is to learn how Stockfish rates each board.

345
00:19:58,000 --> 00:20:05,000
For the models that score chess positions, we could compare their predictions directly to Stockfish to understand how accurate they are.

346
00:20:05,000 --> 00:20:10,000
And I did that, but I think it's more fun to use these models to create chess players and then have them play against each other.

347
00:20:10,000 --> 00:20:17,000
These players just look at each legal move and score the resulting board and then take the move that would be most favorable to their side.

348
00:20:17,000 --> 00:20:19,000
So there's no game tree search here.

349
00:20:19,000 --> 00:20:21,000
Here are the results of a tournament.

350
00:20:21,000 --> 00:20:26,000
The rows are each player as white and the columns as black, and they're ordered by their final ALO rating.

351
00:20:26,000 --> 00:20:30,000
There's some complexity here, but the fixed versions are the ones to look at.

352
00:20:30,000 --> 00:20:39,000
As usual, the rectified linear unit is performing the best, but our quote-unquote linear transfer function, grad 1, is actually in second place and not far behind.

353
00:20:39,000 --> 00:20:47,000
It's close to being as good as Chessmaster for the Nintendo Entertainment System, and outperforms Stockfish deluded with half random moves.

354
00:20:47,000 --> 00:20:54,000
This is actually pretty impressive given that it's doing no game tree search, it's just using its intuitions about what boards are good.

355
00:20:54,000 --> 00:20:58,000
Of course, raw performance on these problems is not the only thing.

356
00:20:58,000 --> 00:21:02,000
We ought to think about the speed of the function, as well as its aesthetics.

357
00:21:02,000 --> 00:21:05,000
Some of them have nice shapes, and others look dumb.

358
00:21:05,000 --> 00:21:10,000
Since training takes days, where all you have to do is stare at graphs of activations,

359
00:21:10,000 --> 00:21:16,000
whether those look cool, or boring, or vaporwave, also bears some consideration.

360
00:21:16,000 --> 00:21:20,000
The key finding here is that the professor was wrong.

361
00:21:20,000 --> 00:21:26,000
You absolutely can use a linear transfer function, as long as you don't need it to be both good and fast.

362
00:21:26,000 --> 00:21:27,000
Defeated.

363
00:21:27,000 --> 00:21:30,000
Having gotten my revenge, we could stop there.

364
00:21:30,000 --> 00:21:35,000
But when have huge breakthroughs in science and technology ever happened by stopping there?

365
00:21:35,000 --> 00:21:38,000
So, it's on to the next level.

366
00:21:38,000 --> 00:21:41,000
So far, all the functions we've considered have been monotonic.

367
00:21:41,000 --> 00:21:46,000
That's because both plus and multiplication, even when you round, have this property.

368
00:21:46,000 --> 00:21:48,000
But we're certainly not limited to this.

369
00:21:48,000 --> 00:21:54,000
For example, if x appears multiple times under addition or subtraction, we can get much more interesting functions.

370
00:21:54,000 --> 00:21:58,000
Another way to look at this is interference patterns between linear functions are linear.

371
00:21:58,000 --> 00:22:05,000
For example, x minus 4,096 minus x plus 4,096 is linear.

372
00:22:05,000 --> 00:22:07,000
It's mathematically equal to zero.

373
00:22:07,000 --> 00:22:11,000
But in half precision floating point, it produces this square wave function.

374
00:22:11,000 --> 00:22:14,000
Now, this function isn't as well behaved as it looks.

375
00:22:14,000 --> 00:22:20,000
One of those intervals is width one exactly, but the other is very slightly smaller than one.

376
00:22:20,000 --> 00:22:23,000
And again, this has to do with perversities of roundoff error.

377
00:22:23,000 --> 00:22:30,000
Or if we take that grad one function that we've studied and subtract x from that, we get this nice triangle wave.

378
00:22:30,000 --> 00:22:34,000
By stringing functions together, we can make all sorts of interesting patterns.

379
00:22:34,000 --> 00:22:39,000
In fact, if we have any shape in mind, we can try approximating it with one of these functions,

380
00:22:39,000 --> 00:22:42,000
subtract it from the desired shape to get a new shape,

381
00:22:42,000 --> 00:22:45,000
and as long as we're getting smaller, we can just keep doing this,

382
00:22:45,000 --> 00:22:49,000
successively approximating that shape like a Taylor series.

383
00:22:50,000 --> 00:22:54,000
So if we could make any shape, let's make a fractal. Those are good shapes.

384
00:22:54,000 --> 00:22:57,000
The Mandelbrot set is the radiohead of fractals.

385
00:22:57,000 --> 00:23:02,000
Here we're going to use complex numbers, and we use a coordinate system where the x-coordinate is the real part,

386
00:23:02,000 --> 00:23:05,000
and the y-coordinate is the imaginary part.

387
00:23:05,000 --> 00:23:08,000
For any given point C, we repeatedly square and add,

388
00:23:08,000 --> 00:23:11,000
and this point moves around in a crazy way.

389
00:23:11,000 --> 00:23:14,000
And based on how quickly it converges or diverges, we give it a color.

390
00:23:14,000 --> 00:23:20,000
Boom. 2D Mandelbrot.

391
00:23:20,000 --> 00:23:24,000
Now adding C is linear. Squaring, however, is not,

392
00:23:24,000 --> 00:23:28,000
but we just said we can approximate any shape using interference patterns.

393
00:23:28,000 --> 00:23:34,000
So here's a rough approximation of f of x equals x squared, using only linear operations.

394
00:23:34,000 --> 00:23:37,000
So this has some funny business near the origin,

395
00:23:37,000 --> 00:23:41,000
but you might think we could use this to plot a kind of perverted Mandelbrot.

396
00:23:41,000 --> 00:23:44,000
Unfortunately, if we try, we get this piece of garbage.

397
00:23:44,000 --> 00:23:46,000
This stupid blotch sucks.

398
00:23:46,000 --> 00:23:50,000
To understand why, we need to look at the definition of squaring for complex numbers.

399
00:23:50,000 --> 00:23:54,000
When we multiply this out, the A and the B get mixed together.

400
00:23:54,000 --> 00:23:58,000
The real part has some A in it, and because I squared is negative 1,

401
00:23:58,000 --> 00:24:02,000
some B in it, and the imaginary part also has some A and some B in it.

402
00:24:02,000 --> 00:24:05,000
So we get this cross-pollination, and that means x and y-coordinates are mixed,

403
00:24:05,000 --> 00:24:07,000
and you get a kind of weird rotation.

404
00:24:07,000 --> 00:24:11,000
But let's look at the purely linear operations on complex numbers.

405
00:24:11,000 --> 00:24:16,000
For both plus and scaling, the real parts stay real,

406
00:24:16,000 --> 00:24:20,000
and the imaginary parts stay imaginary, with no cross-pollination.

407
00:24:20,000 --> 00:24:23,000
So no matter how we use these operations, even with floating-point roundoff,

408
00:24:23,000 --> 00:24:26,000
we're not going to get any mixing between the coordinates.

409
00:24:26,000 --> 00:24:29,000
And that's why the fractal has these rows and columns of sameness.

410
00:24:29,000 --> 00:24:33,000
It's really just two independent functions, one on the x-axis and one on the y-axis.

411
00:24:33,000 --> 00:24:38,000
So professors take note, the complex numbers do provide some refuge.

412
00:24:38,000 --> 00:24:41,000
It's time for another bonus digression.

413
00:24:41,000 --> 00:24:44,000
You might think you could just make a 3D Mandelbrot.

414
00:24:44,000 --> 00:24:46,000
Just do the same thing we did before,

415
00:24:46,000 --> 00:24:48,000
but with numbers that have three components,

416
00:24:48,000 --> 00:24:52,000
a real part and imaginary part and, like, a very imaginary part.

417
00:24:52,000 --> 00:24:56,000
If you try it, this old professor of Frobenius will come along

418
00:24:56,000 --> 00:24:59,000
and educate you with this cool math fact.

419
00:24:59,000 --> 00:25:04,000
No matter what you do, any three-dimensional algebra is equivalent to the real or complex numbers,

420
00:25:04,000 --> 00:25:08,000
so it's like you didn't do anything at all, or not associative,

421
00:25:08,000 --> 00:25:10,000
meaning the order of operations will matter.

422
00:25:10,000 --> 00:25:13,000
But you know what else isn't associative?

423
00:25:13,000 --> 00:25:15,000
The floating-point numbers, my dude.

424
00:25:15,000 --> 00:25:19,000
So it seems we don't need associativity to make fractals anyway.

425
00:25:19,000 --> 00:25:25,000
Enter the baffling numbers, which is an ill-advised generalization of the complex numbers, to three dimensions.

426
00:25:25,000 --> 00:25:28,000
Yes, it won't work, but we can just do it.

427
00:25:28,000 --> 00:25:30,000
Frobenius can't stop me.

428
00:25:30,000 --> 00:25:34,000
And I can use this to make a 3D fractal called the bafflebrot.

429
00:25:34,000 --> 00:25:38,000
Here it's sliced in half, showing a perfect ripe Mandelbrot inside.

430
00:25:38,000 --> 00:25:43,000
The resulting 2-gigabyte file crashes every piece of software I throw at it.

431
00:25:43,000 --> 00:25:44,000
I admire its spirit.

432
00:25:44,000 --> 00:25:47,000
Boom, 3D fractal.

433
00:25:47,000 --> 00:25:49,000
Bezeked.

434
00:25:49,000 --> 00:25:52,000
We don't actually need squaring to create fractals, though.

435
00:25:52,000 --> 00:25:54,000
We just need something kind of chaotic.

436
00:25:54,000 --> 00:25:58,000
I just take this function, which consists of 36,000 linear operations,

437
00:25:58,000 --> 00:26:03,000
and I iterate it, adding C each time, and plot the resulting magnitude.

438
00:26:03,000 --> 00:26:04,000
I think it looks pretty nice.

439
00:26:04,000 --> 00:26:07,000
I think this is a fractal, in the sense that it is chaotic.

440
00:26:07,000 --> 00:26:11,000
It has a color gradient, and could be on the cover of an electronic music album.

441
00:26:11,000 --> 00:26:16,000
It is not a fractal, in the sense that if you zoom in on it, you get infinite detail of self-similar shapes.

442
00:26:16,000 --> 00:26:19,000
In fact, as we zoom in on it only a modest amount,

443
00:26:19,000 --> 00:26:23,000
we see rectangular pixels as we reach the limits of half-precision floating-point.

444
00:26:23,000 --> 00:26:26,000
And because this fractal is built by abusing those very limits,

445
00:26:26,000 --> 00:26:29,000
it's not even possible to get more detail by increasing the accuracy.

446
00:26:29,000 --> 00:26:34,000
Alright, drawing fractals is fun and everything, but it's not really a game you can win.

447
00:26:34,000 --> 00:26:37,000
There's no goal other than to make a cool picture.

448
00:26:37,000 --> 00:26:40,000
So next, I turn to something with a clearer challenge to overcome.

449
00:26:40,000 --> 00:26:42,000
Linear cryptography.

450
00:26:42,000 --> 00:26:45,000
Cryptography is fractals minus drugs.

451
00:26:45,000 --> 00:26:47,000
You take some data and mess it up,

452
00:26:47,000 --> 00:26:50,000
but in a way where you can get it back again if you want.

453
00:26:50,000 --> 00:26:55,000
Possibly the most fundamental building block of cryptography is the pseudo-random number generator.

454
00:26:55,000 --> 00:26:58,000
This is a function that takes in a state, like a 64-bit integer,

455
00:26:58,000 --> 00:27:02,000
and returns a new state that, quote-unquote, looks random.

456
00:27:02,000 --> 00:27:06,000
With one of those, you can generate a hash function by mixing it with some input data,

457
00:27:06,000 --> 00:27:09,000
or a symmetric block cipher using a Feistel network.

458
00:27:09,000 --> 00:27:11,000
So naturally, I want one of these.

459
00:27:11,000 --> 00:27:16,000
Now, another thing that professors will tell you is that cryptographic algorithms cannot be linear.

460
00:27:16,000 --> 00:27:21,000
Here, linear includes within some modular ring like integers mod 256, the bytes,

461
00:27:21,000 --> 00:27:23,000
or mod 2, the bits.

462
00:27:23,000 --> 00:27:28,000
So in contrast, even though we said before that XOR can't be modeled by linear function on reels,

463
00:27:28,000 --> 00:27:31,000
XOR is considered linear in this context.

464
00:27:31,000 --> 00:27:34,000
The reason for that is linear cryptanalysis.

465
00:27:34,000 --> 00:27:36,000
If your function is even a little bit linear,

466
00:27:36,000 --> 00:27:41,000
then with a large collection of input-output pairs, like messages and their encrypted versions,

467
00:27:41,000 --> 00:27:45,000
you can deduce information about secrets like an encryption key.

468
00:27:45,000 --> 00:27:50,000
So the standard advice to construct these things is to alternate linear operations like XOR

469
00:27:50,000 --> 00:27:53,000
with nonlinear operations like substitution.

470
00:27:53,000 --> 00:27:57,000
Substitution is make a table of all the bytes, but permute them randomly,

471
00:27:57,000 --> 00:27:59,000
and then just do table lookup.

472
00:27:59,000 --> 00:28:02,000
In fact, Bruce Schneier writes in the big red book,

473
00:28:02,000 --> 00:28:05,000
substitutions are generally the only nonlinear step in an algorithm.

474
00:28:05,000 --> 00:28:08,000
They are what give the block cipher its security.

475
00:28:08,000 --> 00:28:12,000
So of course, what we're going to do is prove this adage wrong by developing a good pseudo-random function

476
00:28:12,000 --> 00:28:16,000
that only uses linear operations on half-precision floating-point numbers.

477
00:28:16,000 --> 00:28:18,000
Now, what does it mean to be good?

478
00:28:18,000 --> 00:28:22,000
This is less subjective than fractals, but it is still a little tricky.

479
00:28:22,000 --> 00:28:26,000
We don't actually even know if pseudo-random number generators exist.

480
00:28:26,000 --> 00:28:30,000
The best results assume that other problems are hard, but we don't have proofs of that either.

481
00:28:30,000 --> 00:28:34,000
There's lots of stuff that looks random, but actually isn't, like it hides a backdoor.

482
00:28:34,000 --> 00:28:37,000
Never forget that RSA security.

483
00:28:37,000 --> 00:28:44,000
Yes, that RSA took a $10 million bribe from the NSA to hide a backdoor in one of their pseudo-random number generators.

484
00:28:44,000 --> 00:28:50,000
Practically speaking, though, we can subject the function to a stringent battery of statistical tests,

485
00:28:50,000 --> 00:28:53,000
and if it passes all of those, that's a really good start.

486
00:28:53,000 --> 00:28:58,000
The function will work on half-precision floating-point numbers in the interval from negative one to one,

487
00:28:58,000 --> 00:29:01,000
just like the transfer functions we've been considering so far.

488
00:29:01,000 --> 00:29:03,000
Now, this is not a good choice.

489
00:29:03,000 --> 00:29:06,000
It's unnecessarily hard, but all of this is unnecessarily hard.

490
00:29:06,000 --> 00:29:09,000
Now, I have to work with 64 bits.

491
00:29:09,000 --> 00:29:13,000
I could represent each bit as a half, but that makes it too easy.

492
00:29:13,000 --> 00:29:17,000
So I'm going to represent it as 8 bytes, each byte represented by a half.

493
00:29:17,000 --> 00:29:23,000
To represent a byte as a half, I'll divide the interval from negative one to one into 256 segments,

494
00:29:23,000 --> 00:29:28,000
and I'll allow any floating-point value within that interval to represent the corresponding byte.

495
00:29:29,000 --> 00:29:36,000
So anything from 124 over 128 to 125 over 128 will represent the number 252.

496
00:29:36,000 --> 00:29:39,000
And again, allowing any number here is unnecessarily hard.

497
00:29:39,000 --> 00:29:42,000
In the next section, we'll see a much better way to do this that's much faster.

498
00:29:42,000 --> 00:29:48,000
But by struggling with this one, we'll at least demonstrate complete mastery over the sort of continuous domain.

499
00:29:48,000 --> 00:29:51,000
So this function will take in 8 halves and return 8 halves.

500
00:29:51,000 --> 00:29:54,000
And the crux of this function will be this substitution.

501
00:29:54,000 --> 00:29:58,000
That's a table lookup where each of the 256 bytes is swapped for another byte,

502
00:29:58,000 --> 00:30:03,000
or plotted as a function, each of these discrete intervals is mapped to a different interval.

503
00:30:03,000 --> 00:30:07,000
The approach we used in the previous section of fitting functions doesn't work here.

504
00:30:07,000 --> 00:30:09,000
We need something more exact.

505
00:30:09,000 --> 00:30:13,000
So I study a family of well-behaved functions called choppy functions.

506
00:30:13,000 --> 00:30:16,000
To be choppy, the function has to have a few properties.

507
00:30:16,000 --> 00:30:19,000
For any value in an interval that represents some integer,

508
00:30:19,000 --> 00:30:22,000
the function has to produce the exact same result,

509
00:30:22,000 --> 00:30:25,000
and its output has to be the lowest value within some interval.

510
00:30:25,000 --> 00:30:28,000
Of course, these functions can only use addition and scaling,

511
00:30:28,000 --> 00:30:32,000
and since they're maximally permissive about what they accept and very strict about what they generate,

512
00:30:32,000 --> 00:30:34,000
they'll be quite easy to reason about and compose.

513
00:30:34,000 --> 00:30:38,000
In fact, we'll be able to think about them as functions from integers to integers.

514
00:30:38,000 --> 00:30:40,000
So I went on a hunt for choppy functions.

515
00:30:40,000 --> 00:30:43,000
I wish I could tell you that I cracked the code of how to make these from scratch,

516
00:30:43,000 --> 00:30:45,000
but I found them by computer search.

517
00:30:45,000 --> 00:30:48,000
Here's an example that I can't believe I'm going to write out by hand.

518
00:30:48,000 --> 00:30:50,000
This function is mathematically linear.

519
00:30:50,000 --> 00:30:52,000
It's actually equal to a constant.

520
00:30:52,000 --> 00:30:54,000
The x's cancel out.

521
00:30:54,000 --> 00:30:59,000
What this function does is return 1 if the input represents the number 249 or 0 otherwise.

522
00:30:59,000 --> 00:31:02,000
So this is a pretty useful choppy function.

523
00:31:02,000 --> 00:31:05,000
Since each of these represents a function from a byte to a byte,

524
00:31:05,000 --> 00:31:09,000
I can think of it as just a table of the bytes that it produces for each input.

525
00:31:09,000 --> 00:31:13,000
It's a little more complicated than this because the outputs are actually from negative 1 to 1,

526
00:31:13,000 --> 00:31:15,000
but this is the basic idea.

527
00:31:15,000 --> 00:31:18,000
So what I did is I generated a whole bunch of these kinds of functions.

528
00:31:18,000 --> 00:31:21,000
And every time I get a new one, or a faster version of an old one,

529
00:31:21,000 --> 00:31:24,000
I put it in a database keyed by these integers.

530
00:31:24,000 --> 00:31:28,000
I can also take any two of them and get their difference by subtracting them.

531
00:31:28,000 --> 00:31:30,000
That'll also be a choppy function.

532
00:31:30,000 --> 00:31:33,000
So here's say they only differ in these two components.

533
00:31:33,000 --> 00:31:36,000
And so I get 0's everywhere except for those two columns,

534
00:31:36,000 --> 00:31:39,000
and this might give me a new choppy function I didn't have before.

535
00:31:39,000 --> 00:31:43,000
Observe that if I ever find one that's 0 everywhere except for a single 1,

536
00:31:43,000 --> 00:31:48,000
then I can use that to modify the column in any other vector to any value that I want.

537
00:31:48,000 --> 00:31:51,000
So these are special, these are basis vectors.

538
00:31:51,000 --> 00:31:53,000
So once I've done that, this column is kind of done,

539
00:31:53,000 --> 00:31:56,000
and I never need to find new variations of that column.

540
00:31:56,000 --> 00:31:58,000
So if I take a large collection of these choppy functions,

541
00:31:58,000 --> 00:32:03,000
I can do a process kind of like Gauss Jordan elimination to deduce a set of basis vectors.

542
00:32:03,000 --> 00:32:06,000
And if I find a basis vector for every column for every position,

543
00:32:06,000 --> 00:32:11,000
then I can just add those up to make any choppy function I want, for example, our substitution.

544
00:32:11,000 --> 00:32:14,000
So that's pretty nice, I just need to find these basis vectors.

545
00:32:14,000 --> 00:32:17,000
You might think that once you had a single basis vector,

546
00:32:17,000 --> 00:32:20,000
you could shift that column around, like to another position,

547
00:32:20,000 --> 00:32:23,000
by just calling your function on a shifted version of x.

548
00:32:23,000 --> 00:32:25,000
And in real mathematics, that would work.

549
00:32:25,000 --> 00:32:29,000
But since these functions are abusing floating point roundoff error,

550
00:32:29,000 --> 00:32:33,000
which depends on the specific value of x, this approach will not work.

551
00:32:33,000 --> 00:32:36,000
You can do stuff to the output of the function like scale it or add to it,

552
00:32:36,000 --> 00:32:39,000
and you can combine functions by taking their interference pattern.

553
00:32:39,000 --> 00:32:42,000
But you can't straightforwardly manipulate the input side.

554
00:32:42,000 --> 00:32:46,000
This problem is worst near the origin, where the precision is highest.

555
00:32:46,000 --> 00:32:49,000
This meant that it was particularly hard to find a choppy function

556
00:32:49,000 --> 00:32:52,000
that distinguished negative and non-negative numbers exactly.

557
00:32:52,000 --> 00:32:56,000
In essence, the middle two columns of my vectors would always have the same value,

558
00:32:56,000 --> 00:32:58,000
and so they wouldn't be independent.

559
00:32:58,000 --> 00:33:00,000
I need to find some way to distinguish those two.

560
00:33:00,000 --> 00:33:05,000
Going back to our earliest example, if we just add 128 and then subtract 128,

561
00:33:05,000 --> 00:33:08,000
we do get different behavior for negative and positive numbers,

562
00:33:08,000 --> 00:33:12,000
but if we look at the rounding near zero, a lot of negative values round up,

563
00:33:12,000 --> 00:33:14,000
like small positive values round down.

564
00:33:14,000 --> 00:33:15,000
And this makes sense.

565
00:33:15,000 --> 00:33:19,000
If you add a small negative number to 128, you get 128.

566
00:33:19,000 --> 00:33:22,000
So I hunted for the zero threshold function,

567
00:33:22,000 --> 00:33:24,000
and there was a lot of manual fiddling with that.

568
00:33:24,000 --> 00:33:27,000
But I did eventually find one, and it looks like this.

569
00:33:27,000 --> 00:33:29,000
It's pretty involved.

570
00:33:29,000 --> 00:33:33,000
One of the key things is to do a whole bunch of multiplications at the beginning,

571
00:33:33,000 --> 00:33:35,000
since these will preserve the sign.

572
00:33:35,000 --> 00:33:39,000
Spread values away from zero without causing any corruptive rounding

573
00:33:39,000 --> 00:33:42,000
until you can do the same old loss of precision techniques

574
00:33:42,000 --> 00:33:45,000
to make all the finite values the same on either side.

575
00:33:45,000 --> 00:33:49,000
With that zero threshold function solved, I can now create a basis

576
00:33:49,000 --> 00:33:52,000
and therefore create any function from a byte to a byte.

577
00:33:52,000 --> 00:33:55,000
So back to our pseudo-random number generator.

578
00:33:55,000 --> 00:34:00,000
The structure I'm going to use is a classic substitution permutation network.

579
00:34:00,000 --> 00:34:03,000
It takes eight bytes in A through H,

580
00:34:03,000 --> 00:34:06,000
and I apply the substitution function to each of the eight bytes.

581
00:34:06,000 --> 00:34:08,000
Then I rearrange the bits,

582
00:34:08,000 --> 00:34:12,000
apply a few more linear operations like modular plus and minus,

583
00:34:12,000 --> 00:34:14,000
and then I have a new state as the output.

584
00:34:14,000 --> 00:34:17,000
And by iterating this, you create a pseudo-random stream.

585
00:34:17,000 --> 00:34:19,000
The substitution function we already talked about.

586
00:34:19,000 --> 00:34:24,000
For permuting the bits, each of the output bytes depends on all of the input bytes.

587
00:34:24,000 --> 00:34:26,000
So it's not a function of one variable,

588
00:34:26,000 --> 00:34:29,000
but I can construct it from functions of one variable.

589
00:34:29,000 --> 00:34:32,000
If I look at this first byte in the output, let's call it y,

590
00:34:32,000 --> 00:34:35,000
and I can look at the first byte in the input of the permutation that's x.

591
00:34:35,000 --> 00:34:38,000
Note that there's just a single bit that it reads.

592
00:34:38,000 --> 00:34:41,000
Remember, I can create any function that I want of a single variable,

593
00:34:41,000 --> 00:34:46,000
so I construct a function that returns 128 if that bit is set in the input of the y0,

594
00:34:46,000 --> 00:34:49,000
and I do a similar thing for all the other bytes,

595
00:34:49,000 --> 00:34:51,000
and then I can just add up those results,

596
00:34:51,000 --> 00:34:54,000
and they all set different bits, so adding is like logical or.

597
00:34:54,000 --> 00:34:57,000
That technique of adding independent things is really useful,

598
00:34:57,000 --> 00:34:58,000
and we're going to use it more later.

599
00:34:58,000 --> 00:35:00,000
The last piece is modular addition.

600
00:35:00,000 --> 00:35:03,000
I have addition, of course, but on bytes it needs to wrap around

601
00:35:03,000 --> 00:35:06,000
if the result is greater than 256, or in this case, greater than 1.

602
00:35:06,000 --> 00:35:11,000
So if I add two of these values together, I get a result that might be as high as 2,

603
00:35:11,000 --> 00:35:14,000
so it looks like this, but I want it to look like this.

604
00:35:14,000 --> 00:35:16,000
Once it gets past 1, it should go back to negative 1.

605
00:35:16,000 --> 00:35:20,000
Fortunately, I do have a way to test whether the value is greater than a threshold like 1.

606
00:35:20,000 --> 00:35:24,000
So modular plus takes in two arguments and adds them together,

607
00:35:24,000 --> 00:35:27,000
and that result might be either too low or too high.

608
00:35:27,000 --> 00:35:29,000
We'll talk about the case that it's too high.

609
00:35:29,000 --> 00:35:34,000
We test whether it's higher than 1 using the 0 threshold function,

610
00:35:34,000 --> 00:35:39,000
which returns either 1 or 0, multiply that by 2, and then subtract it away.

611
00:35:39,000 --> 00:35:43,000
So that allows us to add this corrective factor and put it back into the right range.

612
00:35:43,000 --> 00:35:46,000
Now, I mentioned before that you can't necessarily shift around functions

613
00:35:46,000 --> 00:35:49,000
because of loss of precision, but this will actually work for the 0 threshold function.

614
00:35:49,000 --> 00:35:51,000
We're going to come back to that in a second,

615
00:35:51,000 --> 00:35:56,000
but first I want to evaluate this random number generator to see how good it is.

616
00:35:56,000 --> 00:36:01,000
In order to test this thing, I used a pre-existing suite of statistical tests called Big Crush.

617
00:36:01,000 --> 00:36:04,000
This is like hundreds of tests that if you do things with the random numbers

618
00:36:04,000 --> 00:36:09,000
that should have a correct mathematical result, you in fact get that mathematical result,

619
00:36:09,000 --> 00:36:11,000
and not something that's a little biased.

620
00:36:11,000 --> 00:36:13,000
It's really hard to pass these tests.

621
00:36:13,000 --> 00:36:15,000
You can try it out on some handmade functions if you want.

622
00:36:15,000 --> 00:36:17,000
It's pretty good at finding bias.

623
00:36:17,000 --> 00:36:21,000
This test needs like 1.6 billion bits of input to do its thing,

624
00:36:21,000 --> 00:36:25,000
so I actually ran it on an equivalent C implementation of this function,

625
00:36:25,000 --> 00:36:28,000
but I also test that they produce exactly the same result.

626
00:36:28,000 --> 00:36:32,000
Even with the C implementation, this takes days to run,

627
00:36:32,000 --> 00:36:35,000
but it did, and it passes every single test,

628
00:36:35,000 --> 00:36:40,000
so it's reasonable to believe that this function could be the basis of a decent encryption algorithm.

629
00:36:40,000 --> 00:36:42,000
Defeated.

630
00:36:42,000 --> 00:36:48,000
Now, one downside is that if you run this using the native half-precision implementation,

631
00:36:48,000 --> 00:36:52,000
it produces 25.8 bytes a second of randomness, which is very slow.

632
00:36:52,000 --> 00:36:57,000
Now, you can produce tables ahead of time so that each of those operations is just a 16-bit table lookup,

633
00:36:57,000 --> 00:37:02,000
and then it'll produce 18.5 kilobytes per second, and that's still slow.

634
00:37:02,000 --> 00:37:07,000
But if you were trapped on a desert island and all you had were linear floating-point operations,

635
00:37:07,000 --> 00:37:09,000
I guess you could do worse than this.

636
00:37:09,000 --> 00:37:14,000
Of course, if you're trapped on a desert island, I don't recommend encrypting your messages.

637
00:37:14,000 --> 00:37:17,000
This is just not a good way to get rescued.

638
00:37:17,000 --> 00:37:20,000
So I said I'd come back to this bit here.

639
00:37:20,000 --> 00:37:24,000
We used the zero-threshold function to test if a sum was greater than one

640
00:37:24,000 --> 00:37:28,000
so that we could implement modular arithmetic by subtracting off a corrective factor.

641
00:37:28,000 --> 00:37:32,000
Once upon a time, I told you you couldn't just shift around the inputs to functions,

642
00:37:32,000 --> 00:37:36,000
and this is true in general, but the zero-threshold function, because it operates at zero,

643
00:37:36,000 --> 00:37:42,000
which is the most precise region for floating-point, actually does admit this behavior within a certain range.

644
00:37:42,000 --> 00:37:45,000
If I have some value in mind, like 0.125,

645
00:37:45,000 --> 00:37:50,000
and I want a function that tests whether the input is just greater than or equal to 0.125,

646
00:37:50,000 --> 00:37:52,000
that looks like this,

647
00:37:52,000 --> 00:37:57,000
and I can do that by just subtracting 0.125 from the input and passing it to the zero-threshold function.

648
00:37:57,000 --> 00:38:01,000
And if the input is exactly 0.125, we get back exactly zero.

649
00:38:01,000 --> 00:38:05,000
This works for most numbers, but there are some limits.

650
00:38:05,000 --> 00:38:08,000
So on the y-axis here, we have different choices of threshold.

651
00:38:08,000 --> 00:38:13,000
On the x-axis, we have all of the possible inputs, and this is all of the finite floating-point values.

652
00:38:13,000 --> 00:38:15,000
The green region is where we get the right answer.

653
00:38:15,000 --> 00:38:17,000
The red region is where it's wrong.

654
00:38:17,000 --> 00:38:22,000
The only reason it's ever wrong is that we end up getting an infinite value during that computation.

655
00:38:22,000 --> 00:38:24,000
Otherwise, this would all work out.

656
00:38:24,000 --> 00:38:26,000
The green region is pretty big.

657
00:38:26,000 --> 00:38:29,000
It always works out when the input and the threshold is exactly the same, because then you get zero,

658
00:38:29,000 --> 00:38:31,000
and then you're not going to have any infinities.

659
00:38:31,000 --> 00:38:35,000
But as they get farther from one another, the value you're testing is larger,

660
00:38:35,000 --> 00:38:38,000
and therefore you're more likely to encounter infinities.

661
00:38:38,000 --> 00:38:41,000
The highlighted region is everything from negative one to one,

662
00:38:41,000 --> 00:38:43,000
which accounts for almost half of the finite numbers.

663
00:38:43,000 --> 00:38:46,000
And you can see we've covered pretty much this entire interval.

664
00:38:46,000 --> 00:38:51,000
There is this one corner, like a couple numbers that don't work.

665
00:38:51,000 --> 00:38:53,000
But it's, I mean, we can do...

666
00:38:53,000 --> 00:38:54,000
Okay.

667
00:38:54,000 --> 00:38:55,000
Alright, fine.

668
00:38:55,000 --> 00:38:57,000
I'll fix it.

669
00:38:58,000 --> 00:39:00,000
Alright, now I can sleep soundly.

670
00:39:00,000 --> 00:39:02,000
Here's a new version of the zero-threshold function,

671
00:39:02,000 --> 00:39:05,000
which works on the entire negative one to one interval.

672
00:39:05,000 --> 00:39:07,000
And more than that, in fact,

673
00:39:07,000 --> 00:39:09,000
I found this with computer search again,

674
00:39:09,000 --> 00:39:11,000
trying to maximize the size of the interval on which it works.

675
00:39:11,000 --> 00:39:13,000
And basically it's the same as before,

676
00:39:13,000 --> 00:39:15,000
but more careful about intermediate computations

677
00:39:15,000 --> 00:39:17,000
so that it doesn't touch infinity by accident.

678
00:39:17,000 --> 00:39:20,000
So now that I know that this works for every value in there,

679
00:39:20,000 --> 00:39:24,000
I can actually use it to generate literally any function that I want on that interval.

680
00:39:24,000 --> 00:39:27,000
The first step is to take this general-purpose greater-than function

681
00:39:27,000 --> 00:39:31,000
and turn it into a general-purpose exact equals function.

682
00:39:31,000 --> 00:39:34,000
I check whether the input is greater than or equal to the value,

683
00:39:34,000 --> 00:39:37,000
but then subtract off a corrective factor.

684
00:39:37,000 --> 00:39:40,000
If the input is greater than or equal to the next floating-point number,

685
00:39:40,000 --> 00:39:42,000
that's this next after thing.

686
00:39:42,000 --> 00:39:44,000
This returns one if the input is exactly v.

687
00:39:44,000 --> 00:39:46,000
And then I just make an enormous expression.

688
00:39:46,000 --> 00:39:48,000
There's only a finite number of floating-point inputs.

689
00:39:48,000 --> 00:39:51,000
So for each one, I test whether it's exactly equal to that,

690
00:39:51,000 --> 00:39:53,000
giving zero or one.

691
00:39:53,000 --> 00:39:56,000
And I multiply that by the constant value that I want to have at that point,

692
00:39:56,000 --> 00:39:57,000
the y-coordinate.

693
00:39:57,000 --> 00:40:00,000
Then I sum those all up and it makes any shape that I like.

694
00:40:00,000 --> 00:40:01,000
So that's great.

695
00:40:01,000 --> 00:40:03,000
Linear functions can do anything.

696
00:40:03,000 --> 00:40:07,000
And one thing I don't like about this is how big this expression is.

697
00:40:07,000 --> 00:40:09,000
In some sense, that's funny,

698
00:40:09,000 --> 00:40:12,000
but it's starting to look like this thing is turn-complete,

699
00:40:12,000 --> 00:40:14,000
and I'd like to build a computer to demonstrate,

700
00:40:14,000 --> 00:40:16,000
since that's what you do.

701
00:40:16,000 --> 00:40:20,000
But I don't know, everything is slow turn-complete these days.

702
00:40:20,000 --> 00:40:23,000
So I want to figure out how we could make it a bit more practical,

703
00:40:23,000 --> 00:40:27,000
because I like to work at the intersection of theory and impractice and practice.

704
00:40:27,000 --> 00:40:30,000
So I consulted my extensive computer science library

705
00:40:30,000 --> 00:40:33,000
for performance-enhancing substances.

706
00:40:33,000 --> 00:40:37,000
I found a relevant-looking article in the 2018 SIGBOVIC

707
00:40:37,000 --> 00:40:40,000
called The Fluent 8 Software Integer Library,

708
00:40:40,000 --> 00:40:42,000
by Jim McCann, he sounds smart,

709
00:40:42,000 --> 00:40:44,000
and Tom Murphy V.

710
00:40:44,000 --> 00:40:48,000
Wait, I already wrote this paper?

711
00:40:48,000 --> 00:40:50,000
God damn it.

712
00:40:50,000 --> 00:40:52,000
Yeah, this looks familiar.

713
00:40:52,000 --> 00:40:54,000
Uh, man.

714
00:40:54,000 --> 00:40:59,000
Well, the Fluent 8 library implements unsigned 8-bit integers,

715
00:40:59,000 --> 00:41:02,000
using 32-bit floating point.

716
00:41:02,000 --> 00:41:04,000
That sounds pretty familiar,

717
00:41:04,000 --> 00:41:07,000
but it does make some different design decisions than what we're doing today.

718
00:41:07,000 --> 00:41:11,000
One superficial difference is that it uses 32-bit full-precision floating point.

719
00:41:11,000 --> 00:41:13,000
That's easy to change.

720
00:41:13,000 --> 00:41:15,000
It also uses some nonlinear operations,

721
00:41:15,000 --> 00:41:17,000
so we're going to need to fix that.

722
00:41:17,000 --> 00:41:19,000
But it's core idea, and the reason it can be much faster,

723
00:41:19,000 --> 00:41:23,000
is that each integer is represented by the corresponding floating point integer.

724
00:41:23,000 --> 00:41:26,000
And the operations will only work if the input is exactly an integer,

725
00:41:26,000 --> 00:41:28,000
and they produce integers as output.

726
00:41:28,000 --> 00:41:31,000
So we don't need to worry about numbers that are really close to zero,

727
00:41:31,000 --> 00:41:33,000
or negative numbers like we did,

728
00:41:33,000 --> 00:41:36,000
when we were working on the entire interval from negative 1 to 1.

729
00:41:36,000 --> 00:41:38,000
This allows us to pull some more tricks,

730
00:41:38,000 --> 00:41:40,000
and then do things more quickly.

731
00:41:40,000 --> 00:41:42,000
So we're going to combine the power of what we've done so far,

732
00:41:42,000 --> 00:41:45,000
and Fluent 8, and get Fluent 8.

733
00:41:45,000 --> 00:41:47,000
Fluent 8.

734
00:41:47,000 --> 00:41:48,000
Fluent 8.

735
00:41:48,000 --> 00:41:50,000
Fluent 8.

736
00:41:50,000 --> 00:41:52,000
Ah, close enough.

737
00:41:52,000 --> 00:41:56,000
This time this stands for half floating linear U and 8,

738
00:41:56,000 --> 00:41:59,000
and then we're going to use that to implement a classic computer.

739
00:41:59,000 --> 00:42:03,000
So each byte will be represented by a half precision floating point number.

740
00:42:03,000 --> 00:42:07,000
And since bytes are integers, we'll represent it by the corresponding floating point number,

741
00:42:07,000 --> 00:42:09,000
which is exactly that integer.

742
00:42:09,000 --> 00:42:12,000
All 256 of them have exact representations.

743
00:42:12,000 --> 00:42:15,000
Let's first look at a helper function that's familiar.

744
00:42:15,000 --> 00:42:17,000
This is another threshold function.

745
00:42:17,000 --> 00:42:20,000
It requires an integer, but that integer can be as high as 511.

746
00:42:20,000 --> 00:42:22,000
9 bits.

747
00:42:22,000 --> 00:42:26,000
If the number is greater than or equal to 256, it returns 1.0,

748
00:42:26,000 --> 00:42:28,000
otherwise 0.0.

749
00:42:28,000 --> 00:42:30,000
So this is like a threshold 256 function,

750
00:42:30,000 --> 00:42:32,000
or a downshift by 8 bits.

751
00:42:32,000 --> 00:42:36,000
It uses the same kind of loss of precision tricks we've been using all along,

752
00:42:36,000 --> 00:42:38,000
but we can get it done with 4 operations this time,

753
00:42:38,000 --> 00:42:41,000
because it only needs to work on 512 different inputs.

754
00:42:41,000 --> 00:42:45,000
It's similarly easy to downshift by 1 or 2 or 3 or 4 bits,

755
00:42:45,000 --> 00:42:47,000
and we have functions for that as well.

756
00:42:47,000 --> 00:42:50,000
And now we can implement modular addition the same way we did before.

757
00:42:50,000 --> 00:42:52,000
We just compute the sum natively.

758
00:42:52,000 --> 00:42:55,000
Now that could be over 255.

759
00:42:55,000 --> 00:42:59,000
But we have a way to test whether it is and compute 1.0 or 0.0.

760
00:42:59,000 --> 00:43:03,000
So we multiply that by the constant 256, which gives us either 256 or 0,

761
00:43:03,000 --> 00:43:06,000
and we subtract that off so that the result is back in range.

762
00:43:06,000 --> 00:43:07,000
Cool.

763
00:43:07,000 --> 00:43:10,000
We only did 7 floating point operations here, which is not bad.

764
00:43:10,000 --> 00:43:12,000
I'm certainly not going to show you all of the code,

765
00:43:12,000 --> 00:43:15,000
but I wanted to give a taste of some of the interesting problems

766
00:43:15,000 --> 00:43:17,000
that we need to solve in order to do this efficiently.

767
00:43:17,000 --> 00:43:20,000
While addition is already kind of linear except for overflow,

768
00:43:20,000 --> 00:43:22,000
bitwise operations like and are not even close.

769
00:43:22,000 --> 00:43:26,000
But we can do it pretty cleanly with some of the operations we've already constructed.

770
00:43:26,000 --> 00:43:29,000
I'll run this loop exactly 8 times once for each bit,

771
00:43:29,000 --> 00:43:32,000
and this will be unrolled by the compiler, so we're not even doing these comparisons.

772
00:43:32,000 --> 00:43:34,000
It's as though we wrote this 8 times.

773
00:43:34,000 --> 00:43:39,000
Since it's unrolled, we can compute something like a constant 2 to the i at compile time as well.

774
00:43:39,000 --> 00:43:42,000
We work bit by bit starting with the lowest order 1.

775
00:43:42,000 --> 00:43:46,000
The first thing we do is shift each input down by 1 bit using a function we've already seen.

776
00:43:46,000 --> 00:43:48,000
Then we shift it back up.

777
00:43:48,000 --> 00:43:50,000
As long as the input is less than 128, which it will be,

778
00:43:50,000 --> 00:43:54,000
you can shift up by 1 by just multiplying by 2 or adding it to itself.

779
00:43:54,000 --> 00:43:58,000
Now we know the last bit is 0, so if I subtract this from the original argument,

780
00:43:58,000 --> 00:44:01,000
I get the lowest order bit of the input, either 1 or 0.

781
00:44:01,000 --> 00:44:03,000
So I've extracted the lowest order bit of both args,

782
00:44:03,000 --> 00:44:06,000
but I still don't have and even on 1 bit.

783
00:44:06,000 --> 00:44:09,000
Multiplying the two bits together would give me the right answer,

784
00:44:09,000 --> 00:44:12,000
and multiplication is one of the linear operations.

785
00:44:12,000 --> 00:44:14,000
But remember that we only allow multiplication by a constant.

786
00:44:14,000 --> 00:44:19,000
For example, if you were to compute x and x, both a bit and b bit would depend on x,

787
00:44:19,000 --> 00:44:24,000
and so here you'd have x times x, or x squared, which is not mathematically linear.

788
00:44:24,000 --> 00:44:26,000
So we're not going to use multiplication, but we do have a nice trick,

789
00:44:26,000 --> 00:44:29,000
which is to add the bits together and then shift down by 1.

790
00:44:29,000 --> 00:44:34,000
If we look at the truth table, we see that this only produces a 1 when both of the inputs were 1.

791
00:44:34,000 --> 00:44:40,000
I take the resulting bit and multiply it by that round's scale, which is a power of 2, a constant,

792
00:44:40,000 --> 00:44:42,000
and then I just add all of those up.

793
00:44:42,000 --> 00:44:46,000
Since the components will be 0 everywhere except for that one bit, plus is equivalent to or.

794
00:44:46,000 --> 00:44:49,000
Ah, this reminds me of a slip-up in one of my previous videos

795
00:44:49,000 --> 00:44:54,000
where I was computing the or function using and and xor and plus.

796
00:44:54,000 --> 00:45:00,000
It totally works, but millions of people wrote in to tell me that I could do it with another xor instead of plus,

797
00:45:00,000 --> 00:45:02,000
which would have been a little faster.

798
00:45:02,000 --> 00:45:05,000
But here plus is the right option. We don't have xor, it's not linear.

799
00:45:05,000 --> 00:45:09,000
I was just, like, foreshadowing this, getting you ready.

800
00:45:09,000 --> 00:45:11,000
Defeated.

801
00:45:11,000 --> 00:45:14,000
Anywho, that's all we need for bitwise and.

802
00:45:14,000 --> 00:45:18,000
It's a little involved, but it's a far cry from the 9,000 operations we did before

803
00:45:18,000 --> 00:45:20,000
just to test if a value is greater than 0.

804
00:45:20,000 --> 00:45:26,000
A spreckin of which we can now quickly test whether a value is exactly 0.

805
00:45:26,000 --> 00:45:30,000
We do this by negating the bits, subtracting from 255.

806
00:45:30,000 --> 00:45:34,000
Then we add one, and that'll only overflow if the original value was 0.

807
00:45:34,000 --> 00:45:38,000
With that, testing whether two values are equal is just a matter of subtracting them

808
00:45:38,000 --> 00:45:40,000
and then seeing whether the result is 0.

809
00:45:40,000 --> 00:45:42,000
And that's how it goes. That's how it always goes.

810
00:45:42,000 --> 00:45:45,000
You build up some constructs, and you use those to make some more.

811
00:45:45,000 --> 00:45:48,000
You gain more and more power until you have all of the things you want.

812
00:45:48,000 --> 00:45:52,000
There are some good puzzles in here, and you may enjoy trying to work some of these out yourself.

813
00:45:52,000 --> 00:45:54,000
And you may improve upon them, and please tell me if you do.

814
00:45:54,000 --> 00:45:57,000
For example, on screen I'm showing you a straightforward way to do if,

815
00:45:57,000 --> 00:46:01,000
but if you check my code, I do a thing that's way more mysterious and fancy

816
00:46:01,000 --> 00:46:04,000
in order to squeeze the last bits of performance out of it.

817
00:46:04,000 --> 00:46:07,000
And I am going to care about performance for this application.

818
00:46:07,000 --> 00:46:12,000
The last time I made a computer out of floating point numbers, which did happen before,

819
00:46:12,000 --> 00:46:17,000
this computer was focused on beauty with no concessions to practicality.

820
00:46:17,000 --> 00:46:21,000
Frankly, the computer was sort of boring to use because it had no I.O.

821
00:46:21,000 --> 00:46:23,000
and it didn't do anything you could observe.

822
00:46:23,000 --> 00:46:25,000
So this time I want to do the opposite.

823
00:46:25,000 --> 00:46:29,000
I'm willing to make some concessions on beauty as long as the result is entertaining.

824
00:46:29,000 --> 00:46:33,000
Now the most entertaining computer is the Nintendo Entertainment System.

825
00:46:33,000 --> 00:46:35,000
And so this is a natural choice.

826
00:46:35,000 --> 00:46:41,000
After all, I like to work at the intersection of theory and impractice and practice and entertainment.

827
00:46:41,000 --> 00:46:45,000
The Nintendo Entertainment System consists of a basically reasonable computer

828
00:46:45,000 --> 00:46:48,000
and a bunch of other weird stuff for entertainment purposes only.

829
00:46:48,000 --> 00:46:54,000
The core of the computer is an 8-bit microprocessor that's more or less the Motorola 6502.

830
00:46:54,000 --> 00:46:58,000
And that other stuff includes video and audio hardware and the controllers and the game cartridge,

831
00:46:58,000 --> 00:47:02,000
which itself might include hardware and stuff like that.

832
00:47:02,000 --> 00:47:07,000
My goal is to replace that 8-bit microprocessor with something that only runs linear floating point operations.

833
00:47:07,000 --> 00:47:09,000
So I'm not going to implement any of the weird stuff.

834
00:47:09,000 --> 00:47:12,000
And that's good because I'm going to do this in a software emulator,

835
00:47:12,000 --> 00:47:14,000
which is my own hacked up copy of FCE Ultra,

836
00:47:14,000 --> 00:47:16,000
and this emulator is so complicated.

837
00:47:16,000 --> 00:47:19,000
But the code that emulates the processor is basically tractable.

838
00:47:19,000 --> 00:47:23,000
The processor state consists of a small number of 8-bit registers,

839
00:47:23,000 --> 00:47:26,000
each of which will represent with a fluent 8.

840
00:47:26,000 --> 00:47:28,000
There's also a 16-bit program counter.

841
00:47:28,000 --> 00:47:30,000
We'll only need a few 16-bit operations,

842
00:47:30,000 --> 00:47:34,000
and it's quite easy to build 16-bit integers using two 8-bit integers.

843
00:47:34,000 --> 00:47:36,000
So I won't say any more about that.

844
00:47:36,000 --> 00:47:38,000
And at a high level, the processor is just a loop.

845
00:47:38,000 --> 00:47:41,000
It reads one byte from memory at the program counter,

846
00:47:41,000 --> 00:47:45,000
which tells it which of the 256 instructions it's going to run next.

847
00:47:45,000 --> 00:47:49,000
It runs that instruction, which updates the state like the registers in the program counter,

848
00:47:49,000 --> 00:47:51,000
and then starts the loop again.

849
00:47:51,000 --> 00:47:53,000
Of course, there are copious details here.

850
00:47:53,000 --> 00:47:56,000
First, let's look at a simple instruction so you can kind of see how it goes.

851
00:47:56,000 --> 00:47:58,000
A really simple instruction is tax.

852
00:47:58,000 --> 00:48:02,000
And speaking of tax, I'll have you know that video editing is so tedious

853
00:48:02,000 --> 00:48:07,000
that while making this video, I actually procrastinated it by doing my taxes.

854
00:48:07,000 --> 00:48:12,000
Anyway, TAX on the 6502 transfers the value from the register A to the register X.

855
00:48:12,000 --> 00:48:14,000
There are still several steps, though.

856
00:48:14,000 --> 00:48:18,000
After we copy it over, we need to update the negative and zero bits of the processor flags

857
00:48:18,000 --> 00:48:20,000
and increment the program counter.

858
00:48:20,000 --> 00:48:24,000
But this code is actually quite nice because we've already done all the work of implementing 8-bit integers.

859
00:48:24,000 --> 00:48:28,000
It makes use of bitwise AND and is zero and shifting and so on.

860
00:48:28,000 --> 00:48:31,000
If all the instructions were like that, this thing would be really simple.

861
00:48:31,000 --> 00:48:33,000
So let's look at a harder instruction.

862
00:48:33,000 --> 00:48:36,000
This is a branching instruction, branch on conditions set.

863
00:48:36,000 --> 00:48:41,000
It modifies the program counter to basically do a jump if one of the processor flags is set.

864
00:48:41,000 --> 00:48:43,000
Otherwise, it just advances to the next instruction.

865
00:48:43,000 --> 00:48:48,000
First problem we'll see is that there's a branch in the implementation of the processor, which is not linear.

866
00:48:48,000 --> 00:48:50,000
That's this if else.

867
00:48:50,000 --> 00:48:52,000
But we do have a fluent 8 version of if.

868
00:48:52,000 --> 00:48:57,000
So we can change this to update the program counter, but to a value that depends on the condition.

869
00:48:57,000 --> 00:48:58,000
That'll look like this.

870
00:48:58,000 --> 00:49:01,000
But the other problem is this memory access.

871
00:49:01,000 --> 00:49:07,000
Now the Nintendo has a main memory of 2 kilobytes, and we could create 2,000 fluent 8s and implement this array subscript.

872
00:49:07,000 --> 00:49:09,000
That's not really the problem.

873
00:49:09,000 --> 00:49:11,000
The problem is that accessing memory has side effects.

874
00:49:11,000 --> 00:49:16,000
So in the previous version of this code where an if wasn't executed, we wouldn't have accessed the memory.

875
00:49:16,000 --> 00:49:19,000
This is sort of obvious for memory writes because writing changes memory.

876
00:49:19,000 --> 00:49:24,000
Less obvious is that writes and reads often have side effects because of memory mapped IO.

877
00:49:24,000 --> 00:49:31,000
For example, writing 2 bytes to 2006 will load them as an address into the PPU, that's one of those weird things.

878
00:49:31,000 --> 00:49:36,000
And then writes that happen to 2007 will pass through to video memory at that address.

879
00:49:36,000 --> 00:49:44,000
Or writing to 4014 will start a DMA that transfers 256 bytes to video memory and stalls the processor for 512 cycles.

880
00:49:44,000 --> 00:49:46,000
So these are not small things.

881
00:49:46,000 --> 00:49:53,000
And worse, there isn't even a small set of them because lots of cartridges have hardware inside them that does arbitrary stuff on reads and writes.

882
00:49:53,000 --> 00:49:56,000
And weirdos are making new weird cartridges all the time.

883
00:49:56,000 --> 00:49:58,000
And so here we have the main concession.

884
00:49:58,000 --> 00:50:02,000
The emulator API for this chip offers a conditional read and write.

885
00:50:02,000 --> 00:50:05,000
These take the address, but also a condition.

886
00:50:05,000 --> 00:50:07,000
If the condition is true, you do what you'd expect.

887
00:50:07,000 --> 00:50:10,000
But if it's false, nothing happens and an arbitrary value is returned.

888
00:50:10,000 --> 00:50:14,000
Of course, this isn't linear, but it's not really that unrealistic if we were making a processor.

889
00:50:14,000 --> 00:50:19,000
We would just wire this through to the memory controller, which would then ignore the read or write if the bit isn't set.

890
00:50:19,000 --> 00:50:24,000
The real 6502, for example, has a pin that indicates whether it's doing a read or a write.

891
00:50:24,000 --> 00:50:26,000
But I accept your criticism.

892
00:50:26,000 --> 00:50:28,000
Feel free to defeat me by doing without this.

893
00:50:28,000 --> 00:50:33,000
Another challenge is that the 6502 has a load of undocumented and really weird instructions.

894
00:50:33,000 --> 00:50:38,000
And this wouldn't be so bad except that the emulator source code I'm working from is extremely hard to understand.

895
00:50:38,000 --> 00:50:46,000
It's filled with all sorts of macro hacks that assume specific variable names, references to mysterious global variables like temp and foo,

896
00:50:46,000 --> 00:50:52,000
pirate jokes, feuds between developers commenting out each other's wrong code, and so on.

897
00:50:52,000 --> 00:50:57,000
And unfortunately, I don't have any Nintendo games that actually execute many of these instructions.

898
00:50:57,000 --> 00:51:03,000
So in the course of development, I made my own cartridge that executes a whole bunch of undocumented instructions when it starts up.

899
00:51:03,000 --> 00:51:08,000
It displays the results of those on the screen so that I can test whether my implementation matches the reference.

900
00:51:08,000 --> 00:51:15,000
This cartridge might be the world's most boring Nintendo game, even more boring than Wall Street Kid.

901
00:51:15,000 --> 00:51:17,000
Here's what that game looks like.

902
00:51:17,000 --> 00:51:19,000
You can't win it or even play it.

903
00:51:19,000 --> 00:51:22,000
It exists only to destroy your mind.

904
00:51:22,000 --> 00:51:25,000
The last puzzle to solve is instruction dispatch.

905
00:51:25,000 --> 00:51:29,000
When we read the instruction byte, we look at it and decide which instruction to execute.

906
00:51:29,000 --> 00:51:33,000
The natural way to implement this is with a switch statement and a case for each instruction.

907
00:51:33,000 --> 00:51:35,000
But that of course is not linear.

908
00:51:35,000 --> 00:51:37,000
We can't do any branching of control flow.

909
00:51:37,000 --> 00:51:41,000
We have to execute the same series of additions and multiplications each time.

910
00:51:41,000 --> 00:51:45,000
So what I'll do is execute every single instruction every time.

911
00:51:45,000 --> 00:51:47,000
Now I only want the right instruction to do anything.

912
00:51:47,000 --> 00:51:53,000
So the first thing I do is make 256 copies of the CPU state, basically the registers.

913
00:51:53,000 --> 00:51:55,000
That's a finite number of variables.

914
00:51:55,000 --> 00:51:57,000
I also have an active flag for each one of those.

915
00:51:57,000 --> 00:52:01,000
And exactly one of those active flags will be set to one for the correct instruction.

916
00:52:01,000 --> 00:52:05,000
Then I run all of the instructions on their own copies of the CPU state.

917
00:52:05,000 --> 00:52:09,000
If I do any conditional reads or write, I include the active flag in the condition.

918
00:52:09,000 --> 00:52:11,000
So only the correct effects will happen.

919
00:52:11,000 --> 00:52:17,000
So then I have the resulting 256 states and I need to copy the one from the correct instruction back into the main state.

920
00:52:17,000 --> 00:52:21,000
The way to do this is to zero them all out except for the active one.

921
00:52:21,000 --> 00:52:23,000
And we can do that with if and then just sum them all up.

922
00:52:23,000 --> 00:52:26,000
They'll all be zero except for the correct one, so we'll get the right answer.

923
00:52:26,000 --> 00:52:30,000
Now it's kind of annoying to run every instruction on every tick of the CPU.

924
00:52:30,000 --> 00:52:33,000
And this technique is the main reason that it's not going to be that fast.

925
00:52:33,000 --> 00:52:34,000
But it is completely linear.

926
00:52:34,000 --> 00:52:37,000
Another upside is that each instruction is completely independent.

927
00:52:37,000 --> 00:52:39,000
So they can actually be run in parallel.

928
00:52:39,000 --> 00:52:41,000
So let's start up the benchmark.

929
00:52:41,000 --> 00:52:43,000
Super Mario Brothers.

930
00:52:43,000 --> 00:52:47,000
Here it's splitting the instructions across eight cores running in parallel.

931
00:52:47,000 --> 00:52:51,000
If not for that instruction dispatch, this thing would run at playable frame rates.

932
00:52:51,000 --> 00:52:53,000
But, and yes, it is already running.

933
00:52:53,000 --> 00:52:56,000
The cost of not cheating is that it runs pretty slow.

934
00:52:56,000 --> 00:52:59,000
The hardware Nintendo runs at 60 frames per second.

935
00:52:59,000 --> 00:53:04,000
And the emulator free to run non-linear instructions gets 3,500 frames per second.

936
00:53:04,000 --> 00:53:15,000
But the linear version, and I did do a lot of optimization, gets 0.11 frames per second or 8.6 seconds per frame.

937
00:53:15,000 --> 00:53:17,000
Which ain't fast.

938
00:53:17,000 --> 00:53:20,000
Maybe you could help me out by putting this video in 2x speed.

939
00:53:20,000 --> 00:53:28,000
I will say, though, in comparison that I have played AAA titles that at launch, inexplicably, on high-end hardware had comparable frame rates.

940
00:53:28,000 --> 00:53:33,000
And they were no doubt executing a great many non-linear instructions.

941
00:53:33,000 --> 00:53:39,000
Speed aside, we now have a general-purpose computer, which renders everything we've done up until this point moot.

942
00:53:39,000 --> 00:53:43,000
If we want a non-linear transfer function, we can just implement the hyperbolic tangent.

943
00:53:43,000 --> 00:53:48,000
If we want fractals, we can just write code that draws the Mandelbrot set on the Nintendo.

944
00:53:48,000 --> 00:53:51,000
We can just write a good encryption algorithm like AES.

945
00:53:51,000 --> 00:53:53,000
We can have a chess engine with search.

946
00:53:53,000 --> 00:53:55,000
In fact, we already have one.

947
00:53:55,000 --> 00:53:58,000
Chessmaster for the NES was included in our tournament already.

948
00:53:58,000 --> 00:54:02,000
And by running it on our linear emulator, we have a linear model.

949
00:54:02,000 --> 00:54:04,000
So it seems I defeated even myself.

950
00:54:04,000 --> 00:54:07,000
And I can finally be done with this damn thing.

951
00:54:10,000 --> 00:54:13,000
Alright, so what have we learned today?

952
00:54:13,000 --> 00:54:15,000
It's the same thing we learned every time.

953
00:54:15,000 --> 00:54:20,000
Complexity is everywhere, even with something as simple as plus and multiplication by constants,

954
00:54:20,000 --> 00:54:23,000
which mathematically can only create lines.

955
00:54:23,000 --> 00:54:29,000
Given a tiny foothold by way of rounding error, we can bend them to our will and make them do anything.

956
00:54:29,000 --> 00:54:32,000
And I think this is the same story of computer science.

957
00:54:32,000 --> 00:54:36,000
Complexity from simplicity, and maybe even of the universe.

958
00:54:36,000 --> 00:54:39,000
So don't underestimate simple things put together.

959
00:54:39,000 --> 00:54:42,000
Anyway, if you made it this far, thank you for watching.

960
00:54:42,000 --> 00:54:43,000
Thank you for your attention.

961
00:54:43,000 --> 00:54:47,000
And if you didn't make it this far, I don't even know what we're talking about.

962
00:54:47,000 --> 00:54:49,000
I'm sure I'll be back soon with more stupid stuff.

963
00:54:49,000 --> 00:54:54,000
In any case, I've been Tom7, and this was Impractical Engineering.

964
00:54:54,000 --> 00:54:55,000
See you soon.

