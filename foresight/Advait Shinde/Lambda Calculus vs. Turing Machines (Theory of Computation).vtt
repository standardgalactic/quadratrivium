WEBVTT

00:01.000 --> 00:02.760
All right, let's jump in.

00:02.760 --> 00:07.320
Cool, introduction to the theory of computation.

00:07.320 --> 00:10.080
I have been more excited about this talk

00:10.080 --> 00:13.520
than any other talk that I've given in my entire life.

00:13.520 --> 00:14.880
So forget about the company vision.

00:14.880 --> 00:17.120
We're talking about the theory of computing.

00:19.000 --> 00:21.980
I learned, I was introduced to some of this stuff

00:21.980 --> 00:26.680
when I was an undergrad at UCLA about 10 years ago

00:26.680 --> 00:29.480
and I didn't understand it.

00:29.480 --> 00:32.020
And it was kind of presented as like,

00:33.040 --> 00:36.520
here's the sort of tablets from the mountain

00:36.520 --> 00:40.560
and learn and memorize them and this is why computing is.

00:40.560 --> 00:43.440
And it took me 10 years and I finally understand it.

00:43.440 --> 00:45.640
And it's so mind blowing that I wanted to talk

00:45.640 --> 00:46.480
to you guys about it.

00:46.480 --> 00:48.400
So this is what it's about.

00:48.400 --> 00:50.800
The subtext of this is it's a tale of two towers

00:50.800 --> 00:52.840
and this will make sense as we get into it.

00:52.840 --> 00:56.980
But the preface is an intro to axiomatic thinking.

00:56.980 --> 00:59.200
This is a kind of strange way of thinking

00:59.240 --> 01:02.280
if you guys haven't been introduced to formal math.

01:03.840 --> 01:05.480
It's not, it's strange.

01:05.480 --> 01:06.460
Let's just jump in.

01:06.460 --> 01:11.460
So two plus three times seven, this is I think 23.

01:14.240 --> 01:16.680
Why do we know the answer to this?

01:16.680 --> 01:19.760
And I asked this question and I encourage you guys

01:19.760 --> 01:23.040
to take the perspective as if you were like an alien

01:23.040 --> 01:27.000
who has never really seen symbols like this.

01:27.000 --> 01:28.560
Or perhaps you didn't even understand

01:28.600 --> 01:30.920
the concept of multiplication or addition.

01:30.920 --> 01:32.840
How do you know what to do over here?

01:32.840 --> 01:34.600
So let's just step through it.

01:34.600 --> 01:37.400
The first thing that you would do is three times seven is 21

01:37.400 --> 01:39.320
and two plus 21 is 23.

01:39.320 --> 01:40.880
So what do we have over here?

01:40.880 --> 01:44.280
We have these symbols called numbers.

01:44.280 --> 01:46.800
We have these things called operators.

01:46.800 --> 01:48.800
And then we have this interesting thing over here

01:48.800 --> 01:53.800
where we can substitute an operator for its equivalent form.

01:54.320 --> 01:57.400
So three times seven is the same thing as 21.

01:57.400 --> 02:00.080
Here we've kind of made that substitution.

02:00.080 --> 02:02.520
And then these operators also have precedents.

02:02.520 --> 02:05.320
Like we knew to do three times seven first

02:05.320 --> 02:07.600
because of all of the middle school math homework

02:07.600 --> 02:08.760
that we did.

02:08.760 --> 02:11.960
And then we have this final reduced form.

02:11.960 --> 02:16.440
So in theory, each of these levels are equal to each other

02:16.440 --> 02:18.960
because we've just kind of made substitutions along the way.

02:18.960 --> 02:21.880
And what I'm trying to get at is we have these collections

02:21.880 --> 02:23.840
of rules that we kind of take for granted

02:23.840 --> 02:25.920
that we never really thought too much about.

02:25.960 --> 02:30.960
That if you really examine like across representing

02:32.000 --> 02:36.080
an operation or this little like two and one

02:36.080 --> 02:38.720
like right next to each other represent 21

02:38.720 --> 02:41.760
these are really kind of non-trivial concepts.

02:41.760 --> 02:43.800
So let's dig into some of these.

02:43.800 --> 02:47.760
The first idea is that three times seven

02:47.760 --> 02:50.440
is actually seven plus seven plus seven.

02:50.440 --> 02:53.400
And so this is to say that the rule of multiplication

02:53.400 --> 02:55.560
is actually defined in terms of addition.

02:58.120 --> 02:59.240
So this is interesting.

03:01.280 --> 03:04.000
If some rules are defined in terms of each other

03:04.000 --> 03:07.040
we can say that the rule is kind of redundant.

03:07.040 --> 03:10.480
So in theory, like we don't really need multiplication

03:10.480 --> 03:12.680
in math because every time we multiply

03:12.680 --> 03:14.520
we can just add instead.

03:14.520 --> 03:16.720
And therefore multiplication hasn't really given us

03:16.720 --> 03:20.080
any more expressive power than addition already provides

03:20.080 --> 03:21.200
for us, right?

03:21.200 --> 03:24.520
So if some rules are redundant

03:24.520 --> 03:26.960
then maybe we can ask the question of like

03:26.960 --> 03:29.360
what rules are non-redundant?

03:30.480 --> 03:32.360
Or really what is the minimum subset of rules

03:32.360 --> 03:34.280
necessary to describe all of math?

03:35.280 --> 03:38.440
And so we can call this minimum subset of rules axioms

03:38.440 --> 03:41.360
and this comes from the word Greek axioma

03:41.360 --> 03:43.200
that which is self-evident.

03:43.200 --> 03:46.800
And then we can call all of the other derived rules theorems

03:48.280 --> 03:51.000
which is like a proposition to be proved.

03:51.000 --> 03:53.480
And so the question is what are the axioms for math?

03:53.480 --> 03:56.080
The minimum non-redundant set of rules

03:56.080 --> 03:57.880
to define all of math.

03:59.680 --> 04:02.920
This is a question that nobody really thought about

04:04.400 --> 04:06.400
or nobody had a really compelling answer to

04:06.400 --> 04:07.560
until this guy came around.

04:07.560 --> 04:09.640
His name is Giuseppe Piano.

04:09.640 --> 04:11.760
He was an Italian mathematician.

04:11.760 --> 04:15.760
And in 1889, so a little over 100 years ago

04:17.080 --> 04:18.320
he put forth these axioms.

04:18.320 --> 04:19.360
There's nine of them.

04:20.200 --> 04:21.880
Only three of them are interesting.

04:21.880 --> 04:23.960
So bear with me for a moment and keep in mind

04:23.960 --> 04:26.480
like we're starting with a blank slate.

04:26.480 --> 04:28.120
So there's no numbers yet

04:28.120 --> 04:30.080
and we have to define what numbers are.

04:30.080 --> 04:35.080
So first we define the first number, which is zero.

04:35.080 --> 04:37.640
And so Piano says zero is a natural number.

04:39.000 --> 04:41.440
The next thing he does is define what equality is

04:41.440 --> 04:42.640
because we don't have that either, right?

04:42.640 --> 04:45.520
So for a thing x, x is equal to x.

04:45.520 --> 04:47.320
This is what equality is.

04:47.320 --> 04:48.800
This one is not very interesting either.

04:48.800 --> 04:51.080
If x equals y, y also equals x.

04:51.080 --> 04:54.080
If x equals y and y equals z, then x equals z.

04:54.080 --> 04:55.520
This is the transit property.

04:57.440 --> 05:00.120
If b is a natural number and a equals b,

05:00.120 --> 05:02.200
then a is also a natural number.

05:02.200 --> 05:04.640
So this is sort of saying we have this collection

05:04.640 --> 05:05.880
of things called natural numbers.

05:05.880 --> 05:08.760
Right now we only have one of them, zero.

05:08.760 --> 05:12.280
And if b is a natural number and a equals b,

05:12.280 --> 05:14.240
then a is also a natural number.

05:14.240 --> 05:16.040
So you have this like glue-like property

05:16.040 --> 05:17.200
of natural numbers.

05:19.480 --> 05:20.960
This is an interesting one.

05:20.960 --> 05:24.240
We define this function s such that s of n

05:24.240 --> 05:25.120
is a natural number.

05:25.120 --> 05:27.240
s is like a successor function.

05:27.240 --> 05:28.840
It's what it literally stands for.

05:28.840 --> 05:32.240
And so now we have a way of going from zero

05:32.240 --> 05:35.520
to producing the successor of zero with this function.

05:35.520 --> 05:38.760
So s of zero is the successor of zero

05:38.760 --> 05:41.120
and s of zero is also a natural number.

05:41.120 --> 05:42.280
Is everyone following?

05:43.320 --> 05:45.360
And I think the best way to go about this

05:45.360 --> 05:46.960
is to just immediately stop me

05:46.960 --> 05:48.080
if you guys have any questions

05:48.200 --> 05:50.640
because this is gonna get more and more complex.

05:50.640 --> 05:52.200
Sounds good?

05:52.200 --> 05:53.040
Great.

05:54.640 --> 05:56.480
m and n are equal to each other

05:56.480 --> 05:59.400
if and only if their successors are equal to each other.

06:01.240 --> 06:05.280
There's no n such that the successor of n equals zero.

06:05.280 --> 06:07.800
So here we're not going into negative numbers.

06:07.800 --> 06:09.800
We're just defining the natural numbers.

06:10.920 --> 06:12.920
So there's no successor for zero.

06:12.920 --> 06:14.960
Or zero is not the successor of anything.

06:14.960 --> 06:16.840
That's what this is saying.

06:16.840 --> 06:18.880
And then finally, the very last one,

06:18.880 --> 06:22.760
if k is a set such that zero is in k

06:24.120 --> 06:28.440
and if n is in k, it means that s and n is in k,

06:30.160 --> 06:32.320
then k contains every natural number.

06:33.620 --> 06:36.840
This is like the base case, zero is in k,

06:36.840 --> 06:38.840
then the inductive case, if n is in k,

06:38.840 --> 06:40.760
it means that s of n is in k

06:40.760 --> 06:43.640
and then therefore k contains all the natural numbers.

06:43.640 --> 06:46.440
So here they're saying there's no like loops.

06:46.920 --> 06:48.720
It's just like this directed graph

06:48.720 --> 06:51.120
that goes all the way out to infinity.

06:51.120 --> 06:51.960
Sound good?

06:53.080 --> 06:53.920
This is it.

06:55.000 --> 06:57.160
This is all you need to define all of math.

06:58.800 --> 07:03.800
So you'll notice we never define numbers besides zero.

07:03.840 --> 07:05.840
We just define the concept of zero, right?

07:05.840 --> 07:08.080
And yet we use these symbols like one and two

07:08.080 --> 07:10.400
and three and four and so on.

07:10.400 --> 07:12.000
And here I'm proposing the concept

07:12.000 --> 07:16.340
that one is actually just syntactic sugar for s of zero.

07:16.760 --> 07:19.560
Which is to say that these two forms are equal to each other

07:19.560 --> 07:20.760
and if we were more precise,

07:20.760 --> 07:23.120
we would actually prefer the form on the right,

07:23.120 --> 07:24.960
but it would be kind of annoying.

07:24.960 --> 07:26.880
And so we have this concept of one

07:26.880 --> 07:30.840
and then two is the successor of one and so on.

07:31.840 --> 07:34.400
Seven is actually s of s of s of zero.

07:34.400 --> 07:36.240
I think you guys get the idea, right?

07:37.680 --> 07:40.000
We haven't added any new information here, right?

07:40.000 --> 07:43.000
And in theory, when we do our math,

07:44.000 --> 07:48.760
we should prefer to reduce, if we were a piano,

07:48.760 --> 07:51.640
we would prefer to reduce the form all the way down

07:51.640 --> 07:53.560
into like this thing over here,

07:53.560 --> 07:54.840
but that's just too confusing.

07:54.840 --> 07:57.280
So we're okay with the syntactic sugar representation

07:57.280 --> 07:58.760
on the left.

07:58.760 --> 07:59.600
Sound good?

08:01.560 --> 08:05.280
Okay, so syntactic sugar is sort of convenience rules

08:05.280 --> 08:08.760
or symbols that we don't need to further reduce down

08:08.760 --> 08:11.080
into the primitive forms.

08:11.080 --> 08:14.480
So let's now define addition.

08:15.480 --> 08:17.160
So addition can be thought of as an operation

08:17.160 --> 08:20.520
that maps two natural numbers to another natural number,

08:20.520 --> 08:21.360
right?

08:21.360 --> 08:23.800
And the syntax is a plus b, so you guys know.

08:24.760 --> 08:27.020
And we just need two rules.

08:27.020 --> 08:28.800
The first is sort of the base case.

08:28.800 --> 08:31.400
a plus zero equals a, sort of obvious.

08:32.720 --> 08:33.880
This one you might have to pause

08:33.880 --> 08:35.680
and think about it a little bit.

08:35.680 --> 08:38.600
a plus the successor of b

08:38.600 --> 08:40.520
is equal to the successor of a plus b.

08:41.680 --> 08:44.880
So what we're doing over here is b

08:44.880 --> 08:47.960
without this little s in the wrapping, right?

08:47.960 --> 08:49.400
We're just kind of taking this s

08:49.400 --> 08:52.480
and wrapping it around the whole thing, right?

08:52.480 --> 08:54.880
And so the term on the right, b,

08:54.880 --> 08:57.900
is actually one less than the term of successor of b.

08:57.900 --> 08:59.840
So we're kind of going down.

08:59.840 --> 09:01.480
This will make sense in just a moment.

09:01.480 --> 09:02.560
I think let's go through an example.

09:02.560 --> 09:05.360
So three plus two, the rules up there are on the right.

09:06.520 --> 09:09.720
The first thing that we do is expand out our syntactic sugar.

09:09.720 --> 09:14.200
So we have s of s of s of zero plus s of s of zero, right?

09:14.200 --> 09:16.320
And now we need to apply one of our rules.

09:16.320 --> 09:18.840
Obviously, we can't apply this rule

09:18.840 --> 09:19.760
because it doesn't match.

09:19.760 --> 09:21.640
So we have to apply this rule, right?

09:21.640 --> 09:25.080
So a is in purple on the left or pink.

09:25.080 --> 09:29.640
And then we have s of b, b is this purple thing over here,

09:29.640 --> 09:32.780
equals s of a plus b, right?

09:32.780 --> 09:34.560
So s of a plus b.

09:35.500 --> 09:37.200
Do you guys see that substitution?

09:40.080 --> 09:42.320
And then we do that again.

09:42.320 --> 09:43.960
And then now we're in a form

09:43.960 --> 09:45.560
where we can apply rule number one,

09:45.560 --> 09:50.060
a plus zero equals just a by itself, right?

09:51.000 --> 09:53.400
And that's five.

09:53.400 --> 09:54.440
And now we can add.

09:57.040 --> 10:00.020
All right, so basically this is compelling

10:00.020 --> 10:03.400
because we didn't have the concept of addition

10:03.400 --> 10:05.480
in terms of the axioms.

10:05.480 --> 10:07.640
And we define the concept of addition

10:07.640 --> 10:10.480
as recursive incrementing, essentially.

10:10.480 --> 10:13.880
And now we have this property of addition,

10:13.880 --> 10:15.960
which we can use to define some other things.

10:15.960 --> 10:18.320
For example, we can define multiplication.

10:18.320 --> 10:20.440
a times zero equals zero,

10:20.440 --> 10:24.460
a times s of b equals a plus a times b.

10:24.460 --> 10:27.720
And it can work out that obviously intuitively

10:27.720 --> 10:29.520
that addition and multiplication

10:29.520 --> 10:32.800
are kinds of intrinsically related to each other.

10:32.800 --> 10:34.560
So we have the piano axioms,

10:34.560 --> 10:35.680
so then we built addition

10:35.680 --> 10:38.600
and then we built multiplication on top, right?

10:38.600 --> 10:41.080
And it's this type of thinking

10:41.080 --> 10:44.400
that I wanna really imprint in your minds.

10:44.400 --> 10:48.080
And here I'm inventing this new concept called axiom towers.

10:48.080 --> 10:50.120
And you can think of axiom towers

10:50.120 --> 10:52.600
as having a foundation, which is the axioms themselves.

10:52.600 --> 10:54.340
So in this case, the piano axioms.

10:54.340 --> 10:56.560
And then on top of that, we built addition.

10:56.560 --> 10:58.840
And then on top of that, we built multiplication.

10:58.840 --> 11:01.920
And then maybe we can build more stuff on top.

11:01.920 --> 11:04.120
And it turns out that the piano axioms

11:04.120 --> 11:07.280
are sufficient to basically describe like most of math.

11:08.800 --> 11:11.480
So from multiplication, you can kind of imagine

11:11.480 --> 11:14.640
that you can build division and from division

11:14.640 --> 11:15.920
or not from division necessarily,

11:15.920 --> 11:19.160
but you can imagine also you can represent the integers

11:19.160 --> 11:22.960
which are negative numbers from just the real numbers.

11:22.960 --> 11:24.160
We won't get into the proofs,

11:24.160 --> 11:26.240
but I'm sure you guys can kind of envision

11:26.240 --> 11:28.200
how this might be the case.

11:28.200 --> 11:30.280
And then once you have negative numbers,

11:30.280 --> 11:32.960
you can imagine defining rational numbers,

11:32.960 --> 11:36.920
which are just kind of, in the context of division,

11:36.920 --> 11:39.880
a rational number is just a numerator and a denominator.

11:41.760 --> 11:43.040
And then from rational numbers,

11:43.040 --> 11:44.920
you can maybe get to exponentials,

11:44.920 --> 11:48.520
which is just sort of repeated multiplication.

11:48.520 --> 11:49.680
And then from exponentials,

11:49.680 --> 11:51.440
you can get to irrational numbers,

11:51.440 --> 11:55.440
like the exponent of a fraction gets you irrational numbers.

11:55.440 --> 11:58.880
And then maybe you can build imaginary numbers

11:58.880 --> 12:00.160
and so on and so forth.

12:01.160 --> 12:03.680
And all of it is kind of stacked on top

12:03.680 --> 12:06.240
of just these core axioms at the very bottom.

12:07.360 --> 12:08.200
Sound good?

12:09.520 --> 12:10.360
Cool.

12:11.720 --> 12:13.720
This is gonna get interesting, I promise.

12:13.720 --> 12:15.360
All right, so the first idea is

12:15.360 --> 12:17.520
that the axioms are not divine, right?

12:17.520 --> 12:18.720
There's nothing special about them.

12:18.720 --> 12:21.960
In fact, when Piano first wrote his papers,

12:21.960 --> 12:25.000
he started off with one as like the root.

12:25.000 --> 12:26.360
He didn't start off with zero,

12:26.360 --> 12:28.400
but then mathematicians later said like,

12:28.440 --> 12:30.920
no, no, let's start with zero, it's better.

12:30.920 --> 12:32.640
So you can imagine starting off with one and say,

12:32.640 --> 12:36.000
you can imagine using like a predecessor function

12:36.000 --> 12:37.840
instead of like a successor function.

12:39.720 --> 12:42.280
But here I'm making a claim that some axiom towers

12:42.280 --> 12:44.680
are better than other axiom towers.

12:44.680 --> 12:46.720
Let's say more useful than other axiom towers.

12:46.720 --> 12:50.160
Like for example, Roman numerals are just horribly inconvenient

12:50.160 --> 12:53.680
at doing anything useful, like multiplication.

12:53.680 --> 12:56.760
And yet everything that you can do in regular numbers,

12:56.760 --> 12:58.800
you can also do in terms of Roman numerals.

12:58.800 --> 13:00.640
And so Roman numerals aren't like,

13:00.640 --> 13:02.040
they're just a different set of axioms

13:02.040 --> 13:04.320
that are somehow slightly less useful.

13:06.800 --> 13:10.220
All right, axiom towers don't have to correspond to reality.

13:11.480 --> 13:14.960
So Euclid was a Greek philosopher

13:14.960 --> 13:17.920
and he's sort of like the father of geometry.

13:17.920 --> 13:22.480
And way before Piano, he put forth the axioms for geometry

13:22.480 --> 13:26.580
and we call his flavor of geometry Euclidean geometry.

13:26.580 --> 13:29.900
And one of the axioms that he kind of put forth

13:29.900 --> 13:32.460
was that if you have two parallel lines,

13:32.460 --> 13:34.900
let's say like this Y right here,

13:34.900 --> 13:38.660
as well as like the Y axis, these are parallel to each other.

13:38.660 --> 13:40.220
So if two lines are parallel,

13:40.220 --> 13:42.220
then they stay parallel forever.

13:42.220 --> 13:43.620
They never intersect.

13:43.620 --> 13:45.100
That was one of his axioms.

13:46.140 --> 13:48.660
But it turns out that you can have these things

13:48.660 --> 13:50.660
called non-Euclidean geometries,

13:50.660 --> 13:53.060
which essentially forego that axiom.

13:53.060 --> 13:55.020
And the example is sort of like a globe

13:55.020 --> 13:59.860
where you have these vertical longitude lines or meridians.

13:59.860 --> 14:02.380
And the meridians are all parallel to each other.

14:03.540 --> 14:07.020
But as you see at the poles, they all kind of intersect.

14:07.020 --> 14:11.420
So a non-Euclidean geometry is one that foregoes

14:11.420 --> 14:14.260
this notion of like parallel lines don't intersect.

14:14.260 --> 14:15.620
And it turns out that there's all sorts

14:15.620 --> 14:18.460
of really interesting non-Euclidean spaces

14:18.460 --> 14:21.700
that you can imagine that don't at all correspond to reality.

14:21.700 --> 14:26.060
And so there's this whole sets of branches of mathematics

14:26.060 --> 14:29.540
that kind of conceptualize all sorts of different axioms

14:29.540 --> 14:31.420
that are unique and interesting

14:31.420 --> 14:34.260
and form this sort of logically coherent axiom tower

14:34.260 --> 14:36.060
on the basis of those axioms.

14:36.060 --> 14:38.980
And in many ways, those towers don't correspond

14:38.980 --> 14:40.420
at all to reality.

14:40.420 --> 14:42.860
And it's just sort of mathematicians having fun.

14:44.060 --> 14:45.820
Interesting idea.

14:45.820 --> 14:48.020
Okay, symbols.

14:48.020 --> 14:50.220
So we talked a bunch about symbols.

14:50.220 --> 14:52.260
It might be interesting to think of the symbols

14:52.260 --> 14:54.980
as sort of separate from the rules.

14:54.980 --> 14:58.820
But it turns out that if you really examine the situation,

14:58.820 --> 15:02.180
the symbols don't really make sense without the rules.

15:02.180 --> 15:05.780
And the rules can't really be expressed without symbols.

15:05.780 --> 15:07.380
And so symbols here are making the claim

15:07.380 --> 15:10.100
that they're kind of intrinsically related to each other,

15:10.100 --> 15:11.780
really two sides of the same coin.

15:11.780 --> 15:15.820
So this symbol, if you've done any sort of computing,

15:15.820 --> 15:20.620
0x20 is the hexadecimal number 32, right?

15:22.820 --> 15:24.300
But it's also like, sorry,

15:24.300 --> 15:29.300
it's also the ASCII symbol for space, the space character.

15:29.460 --> 15:31.780
So whether you're interpreting the symbol

15:31.780 --> 15:33.700
in the context of hexadecimal math

15:33.700 --> 15:37.740
or this axiom tower of ASCII or Unicode,

15:37.740 --> 15:40.980
like the symbol has meaning only in the context

15:40.980 --> 15:43.260
of a particular like frame of reference,

15:43.260 --> 15:44.740
which is the axiom tower

15:44.740 --> 15:46.740
that you're interpreting the symbol in.

15:46.740 --> 15:47.700
And they're one and the same.

15:47.700 --> 15:50.780
You can't separate out these ideas.

15:50.780 --> 15:52.780
Another interesting example is DNA.

15:52.780 --> 15:55.260
So there's this like funny concept

15:55.260 --> 15:57.980
that DNA consists of these base pairs

15:57.980 --> 16:00.020
and that all of the human genome

16:00.020 --> 16:03.220
is sort of some ridiculously small amount of data.

16:03.220 --> 16:05.980
And it's just like claim that,

16:05.980 --> 16:09.060
therefore like life is not really that complex

16:09.060 --> 16:11.820
because there's really not much information in DNA.

16:11.820 --> 16:13.860
But if you really examine this question,

16:13.860 --> 16:17.660
DNA by itself is completely meaningless and useless

16:17.660 --> 16:20.260
without the corresponding like cellular machinery

16:20.260 --> 16:22.220
that's able to actually unpack it and read it

16:22.220 --> 16:24.560
and build actual life from it.

16:24.560 --> 16:27.260
So DNA and the thing that reads the DNA,

16:27.260 --> 16:30.180
they're intrinsically linked to each other.

16:30.180 --> 16:32.700
The symbols and the rules are one and the same.

16:36.220 --> 16:37.860
Here's an interesting philosophical claim.

16:37.860 --> 16:41.580
I think that math is actually discovered and not invented.

16:41.580 --> 16:43.060
And the analogy that I have

16:43.060 --> 16:46.220
is sort of a visualization of this axiom tower

16:46.220 --> 16:49.820
and the top levels of this axiom tower are kind of obscured.

16:49.820 --> 16:52.340
It's not exactly clear what they should be.

16:53.300 --> 16:54.860
And what you're doing as a mathematician

16:54.860 --> 16:57.460
is kind of like discovering consequences

16:57.460 --> 16:59.660
of having initial axioms.

17:01.660 --> 17:02.500
Sound good?

17:02.500 --> 17:03.340
Questions?

17:06.620 --> 17:07.980
All right.

17:07.980 --> 17:09.740
So recap, axioms are self-evident.

17:09.740 --> 17:11.540
They're taken as given.

17:12.540 --> 17:15.180
Theorems are derived from redundant rules.

17:15.180 --> 17:16.940
Axioms and theorems stack up together

17:16.940 --> 17:19.660
to build these things called axiom towers.

17:19.660 --> 17:23.060
And some symbols are actually just syntactic sugar.

17:23.060 --> 17:26.620
Symbols and rules are intrinsically related.

17:26.620 --> 17:29.200
And math is a discovery of the consequences

17:29.200 --> 17:30.700
of foundational axioms.

17:32.620 --> 17:34.020
And the axioms are arbitrary,

17:34.020 --> 17:36.620
but some axiom towers are more useful than others.

17:37.740 --> 17:39.380
All right.

17:39.380 --> 17:40.220
You guys are comfortable.

17:40.220 --> 17:42.100
We're gonna get to the exciting part.

17:43.260 --> 17:44.100
Computation.

17:47.300 --> 17:49.540
This is a graph of certain things

17:49.540 --> 17:51.060
that we kind of take for granted today,

17:51.060 --> 17:54.420
like running water and electric power over time.

17:54.420 --> 17:58.020
And here I wanna point out that in the 1930s,

17:59.540 --> 18:02.500
more people had electric power than running water.

18:02.500 --> 18:06.300
And that number was around 65, 70%.

18:06.300 --> 18:09.140
So you can imagine being in the 1930s, right?

18:11.020 --> 18:15.340
And at this point, algorithms had already existed, right?

18:15.340 --> 18:17.780
So way back in 2000 BC,

18:17.780 --> 18:20.420
Egyptians figured out how to multiply two numbers together.

18:20.420 --> 18:22.900
Babylonians figured out how to factorize things

18:22.900 --> 18:24.700
and find square roots.

18:24.700 --> 18:26.580
Euclid's algorithm, which is really cool,

18:26.580 --> 18:28.700
the same Euclid as geometry.

18:28.700 --> 18:31.920
He figured out how to get the greatest common factor

18:31.920 --> 18:33.500
between two numbers.

18:33.500 --> 18:35.820
And this algorithm is actually really beautiful

18:35.820 --> 18:36.740
if you've never seen it.

18:36.740 --> 18:39.500
It's called, I've actually never pronounced it,

18:39.500 --> 18:43.180
but I've read it, it's sieve or aerotostinese.

18:43.180 --> 18:46.280
It's a way to generate prime numbers.

18:47.760 --> 18:51.180
And Al-Quarizmi figured out how to solve linear equations

18:51.180 --> 18:52.360
and quadratic equations.

18:52.360 --> 18:53.860
And it turns out that the word algorithm

18:53.860 --> 18:55.340
actually comes from his name.

18:56.900 --> 18:59.700
So we have like hundreds or thousands of years

18:59.700 --> 19:02.980
of understanding of these things called algorithms,

19:02.980 --> 19:05.660
which really were kind of informal at the time.

19:05.660 --> 19:08.420
And you can kind of consider them as like sequences

19:08.460 --> 19:11.980
of instructions to follow to do something, right?

19:11.980 --> 19:15.580
But we didn't have like a precise axiomatic definition

19:15.580 --> 19:18.820
of computing in the way that Piano defined

19:18.820 --> 19:22.180
the axiomatic definition of mass, right?

19:22.180 --> 19:23.340
In the 30s.

19:23.340 --> 19:28.340
And so these guys pretty much at exactly the same time

19:30.760 --> 19:33.220
did that independently.

19:34.520 --> 19:37.740
Alan Turing created these things called Turing Machines.

19:38.420 --> 19:40.460
Alonzo Church created these things,

19:40.460 --> 19:42.260
this thing called Lambda Calculus.

19:43.220 --> 19:45.020
And Kurt Godel created these things

19:45.020 --> 19:47.460
called general recursive functions.

19:47.460 --> 19:49.020
So we're gonna ignore the last one

19:49.020 --> 19:51.180
and actually drill into these two,

19:51.180 --> 19:53.620
Turing Machines and Lambda Calculus.

19:53.620 --> 19:56.100
And the really, really cool thing is that

19:56.100 --> 20:00.740
these axiomatic systems are both reasonable

20:00.740 --> 20:02.620
and good definitions for computing,

20:02.620 --> 20:05.100
but they look very, very different from each other.

20:05.100 --> 20:07.540
So we're gonna talk about what they are.

20:08.420 --> 20:09.620
So Turing Machines.

20:11.180 --> 20:12.900
You can envision a Turing machine.

20:12.900 --> 20:15.180
So Alan Turing was thinking about,

20:15.180 --> 20:18.340
like you can kind of empathize with what he was doing.

20:18.340 --> 20:21.460
He was looking at all these algorithms that we have

20:21.460 --> 20:25.180
and he was trying to reduce like all of the algorithms down

20:25.180 --> 20:29.180
into their most principle like reduced forms, right?

20:29.180 --> 20:32.340
And then essentially use that as the base of an axiom tower

20:32.340 --> 20:35.180
and build higher level constructs on top of that, right?

20:35.180 --> 20:36.340
This was his goal.

20:36.380 --> 20:39.860
And so he envisioned this concept called a Turing machine.

20:39.860 --> 20:43.020
And a Turing machine starts off with this thing,

20:43.020 --> 20:45.380
which is like an infinitely long tape.

20:46.820 --> 20:48.660
And the tape is actually broken up

20:48.660 --> 20:50.580
into these things called cells.

20:50.580 --> 20:53.660
And in the cells, you can actually,

20:54.620 --> 20:57.180
like each cell can either be marked or unmarked.

20:57.180 --> 20:58.820
So here we have empty cells

20:58.820 --> 21:01.300
and the X's correspond to marked cells, right?

21:02.140 --> 21:03.140
And this is infinitely long.

21:03.140 --> 21:04.820
It goes on in both directions.

21:04.820 --> 21:07.540
And you have this thing called the head

21:07.540 --> 21:10.760
and the head can do some stuff.

21:10.760 --> 21:12.100
You can move it to the right.

21:12.100 --> 21:15.060
In every case, it's always pointing to a particular cell.

21:15.060 --> 21:16.500
You can move it to the left.

21:18.020 --> 21:21.620
You can mark the box that the head is pointing at.

21:21.620 --> 21:23.500
You can unmark the box.

21:23.500 --> 21:26.300
And if the box is marked, you go to end.

21:26.300 --> 21:28.180
We'll talk about go to end in just a moment.

21:28.180 --> 21:30.780
And if the box is unmarked, also go to end.

21:30.780 --> 21:32.740
These are all different instructions

21:32.740 --> 21:35.540
that you can provide to this like turning machine.

21:36.980 --> 21:39.340
And the execution of the turning machine,

21:39.340 --> 21:43.580
essentially, you start off with a blank tape

21:43.580 --> 21:45.140
and a list of instructions

21:45.140 --> 21:46.980
that you want this turning machine to execute.

21:46.980 --> 21:48.500
And the list of instructions are ordered

21:48.500 --> 21:49.780
from zero through N.

21:50.800 --> 21:53.940
And you execute the first instruction.

21:53.940 --> 21:55.220
And if it's an ordinary instruction,

21:55.220 --> 21:57.540
you execute the next instruction.

21:57.540 --> 22:00.180
And if it's one of these like jump instructions

22:00.180 --> 22:03.260
at the bottom, then if it tells you to jump,

22:03.260 --> 22:04.820
you go to the Nth instruction.

22:05.940 --> 22:06.780
Sound good?

22:08.460 --> 22:12.500
And basically Alan Turing showed that this model

22:12.500 --> 22:14.660
is sufficient for all of computation.

22:15.660 --> 22:17.940
Anything that can be computed

22:17.940 --> 22:20.300
can be computed with just these primitives.

22:22.100 --> 22:23.100
That's all it takes.

22:25.500 --> 22:28.940
Okay, so there is an actual virtual implementation

22:28.940 --> 22:31.500
of a Turing machine called brain flap.

22:31.500 --> 22:32.940
It's a technical term.

22:32.940 --> 22:36.220
And basically there's a few instructions.

22:36.220 --> 22:38.860
And it essentially you can imagine it

22:38.860 --> 22:40.980
as like a virtual Turing machine.

22:40.980 --> 22:43.060
And it has these instructions.

22:43.060 --> 22:44.540
You can move the head to the right.

22:44.540 --> 22:46.340
You can move the head to the left.

22:46.340 --> 22:48.220
This is slightly different than a Turing machine

22:48.220 --> 22:52.180
because the cells don't contain just like binary values

22:52.180 --> 22:53.500
of marked and unmarked.

22:53.500 --> 22:55.020
Instead, they contain numbers.

22:55.020 --> 22:57.100
And you can increment numbers and decrement numbers.

22:57.100 --> 22:58.700
And just integers, right?

22:59.740 --> 23:04.740
And then these characters, so the open bracket,

23:04.740 --> 23:07.020
like if the value at the head is zero,

23:07.020 --> 23:10.940
jump forward to the matching like closed bracket.

23:10.940 --> 23:13.420
And then the closed bracket is if the value at the head

23:13.420 --> 23:17.380
is non-zero, jump back to the matching open bracket.

23:17.380 --> 23:22.380
So here we've kind of defined our jumping behavior.

23:23.060 --> 23:26.500
And then this language also provides like functionality

23:26.500 --> 23:28.300
for input and output, which is something

23:28.300 --> 23:31.060
that Turing didn't necessarily require.

23:31.060 --> 23:33.620
But this makes the language a little bit more useful

23:33.620 --> 23:37.140
because you can have it do stuff like print out output.

23:38.980 --> 23:41.140
And every other character is ignored.

23:41.140 --> 23:43.900
So anything that's not this magenta color

23:43.900 --> 23:46.340
is basically ignored in this language.

23:46.340 --> 23:51.340
So this is the implementation of adding two numbers together.

23:52.340 --> 23:57.340
And if you squint, you can kind of see the recursive definition

23:57.820 --> 24:01.140
that Piano kind of described earlier, right?

24:01.140 --> 24:04.820
We'll actually go to a much clearer example.

24:04.820 --> 24:07.580
So in the beginning, you can imagine the head

24:07.580 --> 24:10.740
is pointing at cell C0 and we increment it twice.

24:10.740 --> 24:12.980
So now C0 has a value two.

24:12.980 --> 24:15.020
And then you move to the right to cell C1

24:15.020 --> 24:16.540
and you increment it five times.

24:16.540 --> 24:18.980
So C1 has a value five, right?

24:19.660 --> 24:23.340
And then you start your loop.

24:23.340 --> 24:25.940
The first thing that you do is you go back to C0

24:25.940 --> 24:27.980
and you add one to it.

24:27.980 --> 24:30.100
And then you go right to C1

24:30.100 --> 24:33.500
and then you subtract one from it and you keep looping.

24:33.500 --> 24:35.940
And your loop will basically end.

24:37.540 --> 24:38.900
I can't think properly right now,

24:38.900 --> 24:43.300
but once C1 essentially reaches zero,

24:43.300 --> 24:45.540
then the loop will end and your program will terminate.

24:45.540 --> 24:47.820
And now you have this ability to add two numbers,

24:47.820 --> 24:49.620
C0 and C1, right?

24:53.460 --> 24:56.460
This is an algorithm for computing

24:56.460 --> 24:59.740
the Mandelbrot fractal set.

24:59.740 --> 25:01.580
If you guys haven't heard of this,

25:01.580 --> 25:06.580
it's just a really cool like fractal,

25:06.780 --> 25:07.700
I won't get into fractals,

25:07.700 --> 25:10.980
but basically this is a program, it prints out this, right?

25:11.940 --> 25:13.860
So just using our turning machine,

25:13.860 --> 25:17.420
we were able to now output like this fractal.

25:23.060 --> 25:23.900
Cool.

25:23.900 --> 25:26.060
So basically turning created an axiom tower for computing

25:26.060 --> 25:28.540
and an algorithm is computable

25:28.540 --> 25:31.260
if and only if it can be encoded as a turning machine.

25:32.260 --> 25:34.140
And Turing showed this before the existence

25:34.140 --> 25:35.700
of electrical computers.

25:35.700 --> 25:38.300
And he also showed this when he was 24 years old.

25:40.980 --> 25:42.620
If you guys have heard of the Turing Award,

25:42.620 --> 25:47.340
it's basically like the Nobel Prize for computer science

25:47.340 --> 25:49.020
and it's named after Alan Turing.

25:50.180 --> 25:54.260
So some observations, you need an infinite tape

25:54.260 --> 25:55.820
and you need a program,

25:55.820 --> 25:58.100
like which is a sequence of instructions to follow

25:59.140 --> 26:01.940
and you're constantly modifying the tape.

26:01.940 --> 26:03.460
So you can think of the tape

26:03.460 --> 26:05.780
as sort of like the state of your program

26:05.780 --> 26:08.020
and every instruction that you execute

26:08.020 --> 26:11.260
that modifies the tape is in theory

26:11.260 --> 26:15.860
kind of modifying the way that the program

26:15.860 --> 26:17.860
kind of unfolds itself, right?

26:18.860 --> 26:21.380
And the behavior of the program

26:21.380 --> 26:23.260
is changed with every single tape modification.

26:23.260 --> 26:25.060
And so therefore reasoning about the behavior

26:25.060 --> 26:27.820
of the program requires understanding the state of the tape

26:27.820 --> 26:30.060
at every moment of modification.

26:30.060 --> 26:32.740
And so you can imagine sort of debugging

26:32.740 --> 26:35.580
or turning machine as perhaps similar

26:35.580 --> 26:38.060
to debugging like an application

26:38.060 --> 26:40.980
where you kind of think about how the application's

26:40.980 --> 26:42.700
memory state changed over time

26:42.700 --> 26:47.580
and all of a sudden your ideal understanding

26:47.580 --> 26:49.540
of how it's supposed to change like differs

26:49.540 --> 26:51.940
from the way it actually changed and there's your bug.

26:54.460 --> 26:56.340
From here, Turing defined this concept

26:56.340 --> 26:57.540
called Turing completeness

26:57.540 --> 27:01.580
because you said that you can have other forms of computing.

27:01.580 --> 27:05.980
For example, you can imagine like different instruction sets

27:05.980 --> 27:07.940
for this Turing machine, right?

27:08.940 --> 27:12.660
And he basically said that an axiom tower

27:12.660 --> 27:14.860
that's sort of different than a Turing machine

27:14.860 --> 27:17.660
is called Turing complete if and only if

27:17.660 --> 27:21.060
it can be used to emulate a Turing machine.

27:21.060 --> 27:23.500
And if it can emulate a Turing machine,

27:23.500 --> 27:25.940
then it can compute anything that's computable.

27:26.940 --> 27:27.860
That sounds good.

27:27.860 --> 27:30.460
This concept of Turing completeness has now popped up.

27:32.060 --> 27:33.940
So it turns out that there's some interesting things

27:33.940 --> 27:35.060
that are Turing complete.

27:36.060 --> 27:39.580
If you guys have heard of Conway's Game of Life,

27:39.580 --> 27:44.580
it's this basically life simulator, emulator, I guess.

27:45.700 --> 27:46.820
And it's very simple.

27:46.820 --> 27:48.740
You have this grid of squares

27:48.740 --> 27:53.740
and each square corresponds to a living thing

27:53.980 --> 27:55.860
and it's either alive or dead.

27:55.860 --> 27:57.900
And at every step in time,

27:57.900 --> 28:02.340
there's some certain rules for allowing

28:02.340 --> 28:06.460
like whether in the next time step,

28:06.460 --> 28:08.180
the cell is alive or dead.

28:08.180 --> 28:10.580
And so it essentially, oops,

28:12.900 --> 28:15.380
let's see if we can get this to play.

28:15.380 --> 28:16.300
I won't get into the rules

28:16.300 --> 28:18.540
because they're not really relevant,

28:18.540 --> 28:21.740
but every kind of step in this animation

28:21.740 --> 28:23.780
is like the universe kind of unfolding

28:23.780 --> 28:26.060
according to the rules of Conway's Game of Life.

28:26.060 --> 28:28.540
And it turns out that the basic rules

28:28.540 --> 28:31.900
are sufficient to represent a Turing machine.

28:31.900 --> 28:34.140
And so Conway's Game of Life is Turing complete.

28:34.140 --> 28:37.140
And so any algorithm that's computable

28:37.140 --> 28:39.300
can be represented in Conway's Game of Life.

28:42.140 --> 28:44.620
Magic the Gathering is also Turing complete.

28:44.620 --> 28:46.780
So some researchers got together

28:46.780 --> 28:51.660
and they looked at some specific cards

28:51.660 --> 28:53.740
that allow you to place these like counters

28:53.740 --> 28:55.500
and they use the counters as a way

28:55.500 --> 28:58.940
to represent an actual Turing tape.

28:58.940 --> 29:02.700
And so just following the rules of Magic the Gathering,

29:02.700 --> 29:04.180
they're sufficiently complex enough

29:04.180 --> 29:07.340
that you can compute all of the prime numbers in the game.

29:10.100 --> 29:12.140
Microsoft PowerPoint is Turing complete.

29:13.180 --> 29:15.260
Obviously it has like macros and stuff,

29:15.260 --> 29:19.000
but here using only auto shape, hyperlink and transition.

29:19.000 --> 29:20.820
And this paper is hilarious.

29:20.820 --> 29:22.300
Given PowerPoint's versatility

29:22.300 --> 29:24.340
and cross-platform compatibility,

29:24.340 --> 29:26.540
some have asked whether any other applications

29:26.540 --> 29:27.980
are necessary at all,

29:27.980 --> 29:29.980
or if all computational tasks

29:29.980 --> 29:32.220
can be accomplished through PowerPoint.

29:32.220 --> 29:35.020
This research aims to definitively answer these questions

29:35.020 --> 29:36.860
in the affirmative through the creation

29:36.860 --> 29:38.300
of a PowerPoint Turing machine.

29:41.420 --> 29:43.940
Okay, we've talked about Turing machines.

29:43.940 --> 29:45.820
Now let's talk about Lambda calculus.

29:45.820 --> 29:48.020
So to me, when I first learned about Turing machines,

29:48.020 --> 29:51.100
I thought it was really kind of unintuitive

29:51.100 --> 29:53.860
that such a simple thing can be used

29:53.860 --> 29:55.660
to represent so much complexity.

29:55.660 --> 29:57.060
But then after really thinking about it,

29:57.060 --> 30:00.260
I realized that wait, the piano axioms are also very simple

30:00.260 --> 30:01.860
and we can get all of math from it.

30:01.860 --> 30:03.740
So it must follow that you can have

30:03.740 --> 30:06.100
simple computing axioms and that's the case.

30:06.100 --> 30:09.540
And I think for computer scientists and software engineers,

30:09.540 --> 30:12.020
this is sort of what we're in the business of doing.

30:12.020 --> 30:15.380
Like we take like simple building blocks

30:15.380 --> 30:17.980
and we compose them together to build complexity.

30:17.980 --> 30:19.700
And we have ways of reasoning about

30:19.700 --> 30:22.940
how these things combine together to build complexity.

30:22.940 --> 30:26.340
And we, it's sort of our job to make sure

30:26.340 --> 30:29.060
that the complexity that we build

30:29.060 --> 30:32.140
is actually founded and not buggy, let's say.

30:33.460 --> 30:36.660
So it turns out that there's another flavor

30:36.660 --> 30:38.280
or another axiom tower for computing

30:38.280 --> 30:39.860
that was invented basically

30:39.860 --> 30:42.940
or discovered exactly around the same time.

30:42.940 --> 30:46.300
And it was discovered by Alonzo Church

30:46.300 --> 30:49.780
and it's a thing called Lambda calculus.

30:49.780 --> 30:54.660
And the way, basically in Alonzo Church's original paper,

30:54.660 --> 30:59.540
he has a particular syntax for how he denotes Lambda calculus.

30:59.540 --> 31:02.980
And JavaScript also has its own syntax

31:02.980 --> 31:04.760
for declaring anonymous functions.

31:04.760 --> 31:06.900
And because most of us are more familiar with JavaScript,

31:06.900 --> 31:09.380
I'm gonna write both Alonzo Church's notation

31:09.380 --> 31:11.020
as well as the JavaScript notation

31:11.020 --> 31:12.680
to represent the same ideas.

31:12.680 --> 31:16.180
So the first idea that he introduced was,

31:16.180 --> 31:17.760
you can have variables.

31:17.760 --> 31:19.460
So here X is a variable.

31:19.460 --> 31:21.100
It's like a placeholder for a value.

31:22.100 --> 31:26.220
Second idea is you can have functions.

31:26.220 --> 31:29.100
And a function, this is on the left,

31:29.100 --> 31:32.180
Alonzo Church's definition or notation.

31:32.180 --> 31:34.940
And on the right, you have the ES6 equivalent syntax.

31:34.940 --> 31:37.740
This is just a function that takes in one parameter Y

31:37.740 --> 31:40.700
and has somebody M and M itself,

31:40.700 --> 31:42.540
itself another Lambda expression.

31:43.460 --> 31:45.100
So you have function definition.

31:46.060 --> 31:48.540
And then finally you have function application.

31:48.540 --> 31:50.140
So F of M, right?

31:50.140 --> 31:52.380
So calling function F with a particular M.

31:52.380 --> 31:57.380
So if in this case we call Y with some value,

31:58.220 --> 32:00.460
like everything inside the body of M

32:00.460 --> 32:03.460
gets replaced with whatever value we call it.

32:03.460 --> 32:07.060
You guys should be really familiar with this concept.

32:07.060 --> 32:10.780
And it turns out that this is all you need.

32:10.780 --> 32:13.460
And with just these three concepts,

32:13.460 --> 32:15.980
you can get something that's turned complete.

32:16.980 --> 32:20.140
And so this is really, really unintuitive.

32:20.140 --> 32:23.500
For me, it was way more unintuitive than the turning machine

32:23.500 --> 32:25.120
which felt like this mechanical thing.

32:25.120 --> 32:27.420
And therefore because you can operate it mechanically,

32:27.420 --> 32:29.180
perhaps it can do some computation.

32:29.180 --> 32:32.740
Here there's no notion of mechanics.

32:32.740 --> 32:35.300
I mean, maybe you have function application.

32:35.300 --> 32:36.460
And so we'll get into like,

32:36.460 --> 32:39.400
how can this possibly do stuff?

32:41.980 --> 32:44.140
So the first thing is you have in Lambda calculus

32:44.140 --> 32:45.940
this concept called identity function.

32:45.940 --> 32:48.160
This is the Lambda definition on the left

32:48.160 --> 32:50.760
and the JavaScript definition on the right.

32:50.760 --> 32:53.220
Obviously a very simple construct.

32:53.220 --> 32:57.120
In JavaScript we can have optional braces

32:57.120 --> 32:58.480
for the input parameter, right?

32:58.480 --> 33:00.180
So these two forms are equivalent.

33:00.180 --> 33:01.620
So I'm gonna drop the braces.

33:03.640 --> 33:05.320
And the names are just placeholders, right?

33:05.320 --> 33:09.260
So X and X and Z and Q that are,

33:09.260 --> 33:12.260
all of these constructs mean the same thing, right?

33:12.260 --> 33:14.760
So there's nothing special about X.

33:16.420 --> 33:17.740
So in Lambda calculus,

33:17.740 --> 33:19.540
you can call the identity function on itself.

33:19.540 --> 33:21.740
And basically what this is doing is,

33:21.740 --> 33:23.860
this is the function, right?

33:23.860 --> 33:26.840
And this is the thing that you're calling it with, right?

33:26.840 --> 33:29.020
This is a JavaScript equivalent, right?

33:29.020 --> 33:32.060
So what you do is for,

33:32.060 --> 33:33.980
this is the input variable X

33:33.980 --> 33:36.580
and this is the body of the function M.

33:36.580 --> 33:38.480
And inside the body,

33:38.480 --> 33:39.540
whenever you call this function,

33:39.540 --> 33:42.220
you replace every occurrence of X

33:42.220 --> 33:44.140
with whatever you call it with.

33:44.140 --> 33:46.820
So here every occurrence of X is replaced

33:46.820 --> 33:49.260
with this Lambda function with the purple Xs

33:49.260 --> 33:50.460
and you get this output.

33:51.620 --> 33:52.700
Not very interesting.

33:55.860 --> 33:57.720
Next you have this concept called curing.

33:57.720 --> 34:00.060
So in modern programming languages,

34:00.060 --> 34:03.700
most of them have this notion of having functions

34:03.700 --> 34:07.000
that accept multiple input parameters.

34:07.000 --> 34:10.140
But it turns out that you don't actually need this.

34:10.140 --> 34:14.700
And the way Alonzo church got around this idea

34:14.700 --> 34:16.380
is that instead of a function taking in

34:16.380 --> 34:18.100
two input parameters like this,

34:18.100 --> 34:22.100
we just have a function which returns another function

34:22.100 --> 34:24.620
which takes in an input parameter.

34:24.620 --> 34:25.460
That make sense?

34:26.940 --> 34:30.300
So this construct and this construct are equivalent.

34:31.360 --> 34:33.300
And Alonzo church said,

34:33.300 --> 34:35.860
instead of kind of being verbose like this,

34:36.460 --> 34:40.500
I'm gonna denote Lambda XY.M as equivalent to this.

34:42.260 --> 34:44.220
So it's not equivalent to this thing on the right

34:44.220 --> 34:45.220
because here in JavaScript,

34:45.220 --> 34:47.660
we have a function that takes in two input parameters.

34:47.660 --> 34:50.380
It's instead equivalent to this thing on the top right.

34:50.380 --> 34:52.020
Sorry for that's a little confusing.

34:52.020 --> 34:53.620
This concept is called curing.

34:58.140 --> 35:00.700
Next we're gonna define some true and false symbols.

35:00.700 --> 35:04.540
So you'll notice we didn't have any definition of numbers.

35:05.540 --> 35:08.300
And we didn't really have any definition of types

35:08.300 --> 35:09.500
or booleans or anything like that.

35:09.500 --> 35:12.100
We just had variables, function definition

35:12.100 --> 35:13.500
and function application.

35:13.500 --> 35:16.780
And so now we're adding more semantics to our language

35:17.940 --> 35:21.300
by defining these symbols called true and false.

35:21.300 --> 35:25.100
So very similar to how the number seven as a symbol

35:25.100 --> 35:27.540
is defined in terms of the successor function.

35:27.540 --> 35:32.140
Here the symbol T is defined as this function over here.

35:32.300 --> 35:33.420
And what this function is,

35:33.420 --> 35:35.900
is it's function that takes in two parameters

35:35.900 --> 35:38.940
and returns the first parameter.

35:40.300 --> 35:42.180
And the false symbol is the function

35:42.180 --> 35:43.660
that also takes in two parameters,

35:43.660 --> 35:46.540
but it returns the second symbol or second parameter.

35:47.780 --> 35:48.620
Is it following?

35:50.820 --> 35:51.900
Cool.

35:51.900 --> 35:54.340
So this is similar to our definition of seven.

35:57.060 --> 36:00.620
And from here, now we can sort of build an end function

36:00.620 --> 36:01.780
because we have boolean values.

36:01.780 --> 36:04.180
Let's see how we can build and.

36:04.180 --> 36:06.940
So this is actually the definition of and

36:06.940 --> 36:08.980
and we can kind of try it together.

36:08.980 --> 36:11.860
So and apply to true and false.

36:11.860 --> 36:15.220
Like logically we know that this should be false.

36:15.220 --> 36:16.500
So if we step through it,

36:16.500 --> 36:18.980
the first thing that we do is we replace and

36:18.980 --> 36:21.300
with this body over here.

36:21.300 --> 36:24.460
So lambda x, y, x, y, f, T and f, right?

36:24.460 --> 36:27.460
So here we have some lambda function

36:27.460 --> 36:30.380
and here we're denoting that we want to apply

36:30.380 --> 36:32.580
T to this function.

36:32.580 --> 36:35.500
So what we do is the first parameter is x.

36:35.500 --> 36:37.380
And so it's in this body.

36:37.380 --> 36:40.380
Every single time this x appears,

36:40.380 --> 36:41.940
we want to replace it with a T.

36:44.100 --> 36:46.460
And so what we're left with is lambda y, T, y, f.

36:46.460 --> 36:48.300
So this x has now become a T

36:48.300 --> 36:50.540
and we have one more input parameter

36:50.540 --> 36:51.860
that we need to resolve.

36:52.740 --> 36:55.860
And then same sort of deal, every occurrence of y.

36:56.100 --> 36:57.820
Now we're calling this function with f.

36:57.820 --> 37:01.580
Every occurrence of y, we want to replace with an f.

37:02.980 --> 37:06.100
And so we get T, f, f, okay?

37:06.100 --> 37:08.700
And so as you guys remember,

37:08.700 --> 37:12.060
true is actually defined as a function

37:12.060 --> 37:14.060
that takes in two input parameters

37:14.060 --> 37:15.420
and returns the first one.

37:15.420 --> 37:17.620
So in this case, it takes in two input parameters

37:17.620 --> 37:20.460
and then just returns the first one, which is f.

37:20.460 --> 37:23.700
So now we have some way of doing the and function.

37:24.700 --> 37:28.300
All right, let's try another example and T and T.

37:28.300 --> 37:31.580
Similarly, we expand and out to this thing

37:31.580 --> 37:32.980
and then we apply T to this thing,

37:32.980 --> 37:36.540
replace all the x's with T's and we get T, y, f.

37:36.540 --> 37:38.260
And then replace all the y's with T's

37:38.260 --> 37:40.580
and then we get T, T, f.

37:40.580 --> 37:43.260
And very similarly, T resolves

37:43.260 --> 37:46.420
to picking the first parameter and then we get T.

37:47.380 --> 37:51.980
So with just variables, functions and function application,

37:51.980 --> 37:54.940
all of a sudden now we have like Boolean logic.

37:54.940 --> 37:56.740
You can imagine how we can implement

37:57.820 --> 38:00.060
or an XOR and so on, right?

38:01.060 --> 38:03.260
So this is super unintuitive to me.

38:03.260 --> 38:08.260
Like the concept of defining true and false

38:08.300 --> 38:11.460
as these functions, like a true is actually a function

38:11.460 --> 38:12.740
which takes in two parameters

38:12.740 --> 38:14.060
and false is also a function

38:14.060 --> 38:16.060
which takes in two other parameters.

38:16.060 --> 38:20.180
And from there, building other functions like and,

38:20.180 --> 38:25.180
we can now do logical, like all of Boolean logic, right?

38:28.100 --> 38:28.940
Cool.

38:30.460 --> 38:33.740
This is the hardest slide, so you'll have to deal with me.

38:35.340 --> 38:36.660
I'm gonna talk about the y-combinator.

38:36.660 --> 38:40.300
So it turns out that many of you guys know about yc

38:40.300 --> 38:44.620
up in the Bay and it was essentially founded

38:44.620 --> 38:48.580
by a computer scientist who got the name

38:48.580 --> 38:49.420
from this principle.

38:49.420 --> 38:52.540
It's actually a Lambda calculus construct

38:52.540 --> 38:53.980
and it looks like this.

38:53.980 --> 38:55.220
And we're gonna go really slow

38:55.220 --> 38:57.660
and we'll go through it together, right?

38:57.660 --> 39:00.820
The first thing to notice is that y is just a function

39:00.820 --> 39:03.780
that takes in some input parameter y

39:03.780 --> 39:06.060
and it returns something, right?

39:06.060 --> 39:07.500
So nothing too crazy.

39:08.740 --> 39:11.420
What we can do is apply y,

39:11.420 --> 39:14.020
so let's say we have this function r, right?

39:14.020 --> 39:17.640
We wanna apply y to r, right?

39:17.640 --> 39:19.640
So in order to apply y to r,

39:19.640 --> 39:23.760
what we need to do is every occurrence of this yellow y

39:23.760 --> 39:26.520
inside this body, we need to replace

39:26.520 --> 39:30.120
with our input parameter r, okay?

39:30.120 --> 39:33.640
So all the yellow y's have now just become blue r's.

39:35.080 --> 39:36.760
Sound good?

39:36.760 --> 39:39.280
Okay, now if you look at this body,

39:39.280 --> 39:41.000
we can actually reduce it further.

39:41.000 --> 39:43.240
This first piece over here is a function

39:44.200 --> 39:48.920
and the second piece over here is a value

39:48.920 --> 39:50.760
that we can bind to this function

39:50.760 --> 39:54.200
or we can call this function with this value on the right.

39:54.200 --> 39:55.120
Okay?

39:55.120 --> 39:58.080
So what we're gonna do is this is the value on the right

39:58.080 --> 40:01.520
and if you look at this body, r, open print, x, x,

40:01.520 --> 40:05.520
close print, every occurrence of this magenta x,

40:05.520 --> 40:09.640
we're gonna replace with this body over here, okay?

40:10.520 --> 40:14.640
So r, x, x, has now been replaced with r,

40:14.640 --> 40:18.320
this body, this body, okay?

40:18.320 --> 40:20.640
We haven't done anything like too crazy

40:21.640 --> 40:25.200
and now if you'll notice like this line over here

40:25.200 --> 40:29.200
and the thing inside the parentheses of this r,

40:29.200 --> 40:31.040
they're actually the same thing.

40:32.040 --> 40:33.360
You guys see it?

40:33.360 --> 40:35.800
Here you have magenta values

40:35.800 --> 40:37.640
and here you have purple values.

40:38.560 --> 40:42.240
And the only difference is that this row

40:42.240 --> 40:45.600
has like an enclosed r, do you guys see that?

40:46.960 --> 40:50.600
Okay, so what we can do is take this yr

40:50.600 --> 40:53.120
because these yr and this thing on the right

40:53.120 --> 40:55.400
are equal to each other, so we can say

40:56.600 --> 41:01.600
yr is r of yr and it's not readily clear

41:03.880 --> 41:05.760
like why this is actually interesting

41:06.760 --> 41:11.520
or useful, but if you kind of sit down

41:11.520 --> 41:13.920
and think about it, what we've really done

41:13.920 --> 41:17.920
is define yr in terms of itself.

41:18.840 --> 41:23.080
So we've created like a recursive definition right here.

41:23.080 --> 41:26.400
And so what's actually happened is that this y-combinator

41:26.400 --> 41:29.400
is this thing over here allows you to take

41:29.400 --> 41:34.400
like a non-recursive concept and create recursion from nothing.

41:34.400 --> 41:37.080
So we just have variables, function definitions

41:37.080 --> 41:38.760
and function application

41:38.760 --> 41:41.480
and from those things we're able to create recursion.

41:43.040 --> 41:46.160
So this is like a crazy construct to me.

41:46.160 --> 41:47.800
Like we've created booleans

41:47.800 --> 41:49.320
and therefore we've created conditionals

41:49.320 --> 41:53.920
and from the same sort of raw axioms

41:53.920 --> 41:55.280
we've created recursion.

41:56.360 --> 41:58.880
Now, I encourage you guys to spend some time

41:58.880 --> 42:00.800
if you're interested like really examining this

42:00.800 --> 42:03.080
and coming to an understanding

42:03.080 --> 42:05.080
of why it's actually interesting and profound.

42:05.080 --> 42:07.480
But for now just take it on faith

42:07.480 --> 42:10.120
that we're able to create recursion from nothing

42:10.120 --> 42:11.880
and that's what the compelling aspect

42:11.880 --> 42:13.280
of the y-combinator is.

42:15.680 --> 42:19.280
Okay, the church Turing thesis.

42:19.280 --> 42:23.040
So we have these two independent models of computation

42:23.040 --> 42:24.720
the Turing machine and Lambda calculus

42:24.720 --> 42:26.720
invented at exactly the same time.

42:26.720 --> 42:30.160
And eventually these guys got together

42:30.160 --> 42:32.560
and they realized that their models of computation

42:32.560 --> 42:34.040
were actually equivalent.

42:34.040 --> 42:37.040
So originally when church was defining Lambda calculus

42:37.040 --> 42:40.160
he didn't define it in terms of turning machines

42:40.160 --> 42:42.240
and Turing when he was defining turning machines

42:42.240 --> 42:44.440
he didn't define it in terms of Lambda calculus

42:44.440 --> 42:46.760
they were sort of separate axiom towers.

42:46.760 --> 42:48.720
And so these guys got together and they said,

42:48.720 --> 42:53.440
wait, we have two different models of computation

42:53.440 --> 42:56.640
that we've proven independently to be sufficient

42:56.640 --> 42:59.600
to be able to compute anything that's computable.

42:59.680 --> 43:03.360
Is it true that our models are equivalent to each other?

43:03.360 --> 43:04.320
Was the question.

43:04.320 --> 43:05.640
And so they published this paper

43:05.640 --> 43:07.440
called the church Turing thesis.

43:07.440 --> 43:09.960
And it turns out that all Turing machines

43:09.960 --> 43:12.240
can be rewritten as Lambda expressions

43:12.240 --> 43:14.280
and all Lambda expressions can be rewritten

43:14.280 --> 43:15.840
as Turing machines.

43:15.840 --> 43:17.240
And we didn't really talk about

43:17.240 --> 43:18.760
Godel's recursive functions

43:18.760 --> 43:22.000
but it turns out that those are also equivalent.

43:22.000 --> 43:24.280
And so the conclusion here is that Lambda calculus

43:24.280 --> 43:25.160
is turning complete.

43:27.200 --> 43:29.080
Without any notions of explicit recursion,

43:29.080 --> 43:30.560
conditional state, et cetera.

43:32.200 --> 43:33.840
So all we need is variables, functions

43:33.840 --> 43:35.040
and function application.

43:35.040 --> 43:37.760
So let's go into the peculiarities of Lambda calculus

43:37.760 --> 43:40.480
because as software engineers we're sort of,

43:42.480 --> 43:45.040
we can think of the Turing machine as this thing

43:45.040 --> 43:46.520
that's very similar to a computer.

43:46.520 --> 43:48.320
And I'm gonna get to that in a moment

43:48.320 --> 43:52.440
but it's not really clear what this Lambda calculus thing is

43:52.440 --> 43:54.200
and how to do computation with it.

43:55.100 --> 43:58.600
So the first idea is that there's no notion of global state.

43:58.600 --> 43:59.520
There's no tape.

44:01.400 --> 44:04.400
All you have is the input arguments to your functions.

44:04.400 --> 44:07.560
That's the only semblance of state that you have.

44:07.560 --> 44:10.000
The second idea is all functions are pure.

44:10.000 --> 44:14.400
So purity is sort of this mathematical concept

44:14.400 --> 44:16.360
which is to say that it's a math function

44:16.360 --> 44:18.480
in that for any given input,

44:18.480 --> 44:21.740
it always, a function always returns the same output.

44:21.740 --> 44:24.040
So if you have a function for example,

44:24.040 --> 44:27.280
f of x equals x squared for an input three,

44:27.280 --> 44:28.840
call it this function with three,

44:28.840 --> 44:31.720
it's always gonna return nine no matter what.

44:31.720 --> 44:34.640
So all functions in Lambda calculus are pure.

44:36.480 --> 44:37.960
All values are immutable.

44:37.960 --> 44:42.220
So you can't modify an input parameter.

44:45.340 --> 44:47.200
But what you can do is generate a new value

44:47.200 --> 44:48.280
from an existing one.

44:49.920 --> 44:51.880
And there's also no loops.

44:51.880 --> 44:53.640
So you can't really iterate on things

44:53.640 --> 44:55.360
but the way we actually handle iteration

44:55.400 --> 44:59.760
in Lambda calculus like structures is through recursion.

45:01.720 --> 45:04.600
And then functions are your unit of composition.

45:04.600 --> 45:07.300
And the way you compose functions

45:07.300 --> 45:10.740
is sort of passing them as parameters to each other.

45:10.740 --> 45:14.640
And because of the nature of Lambda calculus,

45:14.640 --> 45:17.640
you don't have to reason about this global state.

45:17.640 --> 45:20.360
So when you're combining two simple functions together,

45:20.360 --> 45:25.280
all you need to know is what the consuming function does

45:25.320 --> 45:26.440
with the input.

45:26.440 --> 45:28.360
You don't have to reason about side effects

45:28.360 --> 45:29.880
or any other properties.

45:29.880 --> 45:33.480
So my claim over here is that because there's no global state,

45:33.480 --> 45:35.520
when you compose two things together,

45:35.520 --> 45:37.240
you can be sure that that composition

45:37.240 --> 45:38.480
is like really, really solid

45:38.480 --> 45:40.200
and it's not gonna result in bugs.

45:42.880 --> 45:44.680
Okay, the two towers.

45:45.920 --> 45:48.280
So we have turning machines on one hand

45:48.280 --> 45:50.120
and Lambda calculus on the other hand.

45:51.080 --> 45:52.360
And I've not so subtly drawn

45:52.360 --> 45:54.320
this Lambda calculus tower is perfect.

45:56.280 --> 45:59.120
But first we need to make a brief aside.

46:00.440 --> 46:02.960
In the 1940s, so less than a decade

46:02.960 --> 46:05.600
after turning machines came out,

46:06.680 --> 46:09.520
people started to ask the question of,

46:09.520 --> 46:12.100
okay, wait, this is great as a mathematical construct,

46:12.100 --> 46:16.400
but ultimately like I need to compute real stuff for my job.

46:16.400 --> 46:19.180
And so can we actually build a physical machine

46:19.180 --> 46:21.040
that does computation?

46:21.040 --> 46:24.080
And one of the core people involved in this work

46:24.080 --> 46:25.800
was this guy named John Von Neumann.

46:25.800 --> 46:27.800
He was a computer scientist.

46:27.800 --> 46:29.440
And he proposed this model

46:29.440 --> 46:31.720
for how we should build computing machines.

46:31.720 --> 46:35.160
And what he started with was this concept of memory,

46:35.160 --> 46:38.680
the RAM, and memory is basically just like a Turing tape

46:38.680 --> 46:41.080
in that it's put up into these cells

46:41.080 --> 46:42.820
and the cells can contain values.

46:43.960 --> 46:47.000
And then he proposed this thing called a CPU,

46:47.000 --> 46:49.360
which is composed of two components essentially,

46:49.360 --> 46:51.840
a control unit and a logic unit.

46:52.120 --> 46:54.180
And the CPU interacts with the memory

46:54.180 --> 46:57.040
by reading stuff from it and writing stuff to it.

46:58.080 --> 47:02.000
And Von Neumann proposed like a small set of instructions.

47:02.000 --> 47:05.400
You can load a value X from the memory cell

47:05.400 --> 47:07.320
at the location P.

47:07.320 --> 47:12.160
You can store a value X into the memory cell location P.

47:13.200 --> 47:15.400
You can add, subtract, and multiply.

47:15.400 --> 47:17.720
And so here's sort of like a minor deviation

47:17.720 --> 47:19.080
from Turing's model.

47:19.080 --> 47:21.600
Turing had no notion of numbers or addition

47:21.600 --> 47:25.000
or so on and Turing as a mathematician just basically said,

47:25.000 --> 47:27.240
those are levels of the axiom tower

47:27.240 --> 47:29.320
that you can obviously derive for yourself.

47:29.320 --> 47:32.160
Like I don't need to embed those in my axioms.

47:32.160 --> 47:34.120
But Von Neumann wanted to build something

47:34.120 --> 47:35.600
that actually computed stuff.

47:35.600 --> 47:37.520
So did the addition and so on.

47:37.520 --> 47:41.680
And so rather than having to do addition manually

47:41.680 --> 47:43.880
in the form of like incrementing

47:43.880 --> 47:46.120
or marking and unmarking cells,

47:46.120 --> 47:49.560
Von Neumann said, why don't we just build like circuitry

47:49.600 --> 47:51.840
that does the addition of two numbers

47:53.000 --> 47:54.400
and embed that into the CPU.

47:54.400 --> 47:56.800
So if I wanna take a value from cell A

47:56.800 --> 47:58.760
and a value from cell B and add them together

47:58.760 --> 48:00.880
and store them into cell C,

48:00.880 --> 48:03.920
instead of manually doing that computation

48:03.920 --> 48:05.840
like incrementing and decrementing,

48:05.840 --> 48:07.880
let's create circuitry that does the addition

48:07.880 --> 48:09.960
so that it's faster, okay?

48:11.960 --> 48:13.120
And that's what the logic unit

48:13.120 --> 48:15.240
is essentially responsible for.

48:15.240 --> 48:18.520
Then you also have these instructions called branches

48:18.560 --> 48:19.480
or jumps.

48:19.480 --> 48:24.480
So if the memory cell at location P contains zero,

48:24.720 --> 48:25.760
go to N.

48:25.760 --> 48:27.320
And if it doesn't contain zero,

48:27.320 --> 48:29.200
go to N, it's another instruction.

48:29.200 --> 48:30.520
And what I'm trying to get at

48:30.520 --> 48:33.360
is that this looks very much like a Turing machine.

48:33.360 --> 48:36.840
And Von Neumann proposed the actual physical circuitry

48:36.840 --> 48:39.600
that could implement something like this.

48:39.600 --> 48:41.080
And the first computers,

48:42.200 --> 48:43.720
the very first general computer

48:43.720 --> 48:45.560
was this thing called ENIAC.

48:45.560 --> 48:47.440
And I think it popped up in the 40s,

48:47.440 --> 48:49.800
like 47 or something like that.

48:49.800 --> 48:53.520
And basically it was like a room almost this size

48:53.520 --> 48:57.320
and there was no notion of like a program

48:57.320 --> 48:58.360
that you give to it.

48:58.360 --> 49:00.520
All it had was like circuitry

49:00.520 --> 49:03.080
and you had these like engineers that would go up

49:03.080 --> 49:07.240
and unplug and replug stuff to program the ENIAC

49:07.240 --> 49:08.680
and then it would operate

49:08.680 --> 49:10.720
and turn through the computation, right?

49:10.720 --> 49:12.400
But ultimately it looked exactly like this.

49:12.400 --> 49:16.240
It had some mechanism to store values in memory

49:16.240 --> 49:18.840
and then it had some mechanism to read

49:18.840 --> 49:19.880
those values from memory,

49:19.880 --> 49:22.200
combine them together in useful mathematical ways

49:22.200 --> 49:23.600
and store the results back.

49:27.360 --> 49:30.960
Cool, so the Turing machine tower.

49:30.960 --> 49:32.040
Start off with Turing machines

49:32.040 --> 49:33.840
and then we have this Von Neumann model.

49:33.840 --> 49:35.280
And the compelling aspect of this

49:35.280 --> 49:37.680
is sort of like a deviation from Turing machines

49:37.680 --> 49:40.840
in that it can be actually physically implemented.

49:40.840 --> 49:41.960
And one limitation here

49:41.960 --> 49:43.600
is that you don't have an infinite tape,

49:43.600 --> 49:46.640
you just have a finite amount of memory, right?

49:46.640 --> 49:48.760
But if you embrace that constraint,

49:48.760 --> 49:51.440
now all of a sudden you can actually compute things

49:51.440 --> 49:55.600
instead of just leaving it up to a mathematician, okay?

49:58.240 --> 50:03.160
In 1949, people got tired of manually plugging

50:03.160 --> 50:04.760
and replugging in wires

50:04.760 --> 50:08.920
and they wanted like a human level way

50:08.920 --> 50:11.640
to reason about what the instructions were.

50:11.640 --> 50:14.520
So they gave each of these instructions names,

50:14.520 --> 50:17.600
like small names like add, store, mold, divide,

50:17.600 --> 50:18.680
things like that.

50:18.680 --> 50:23.080
And programs were written like by hand first

50:23.080 --> 50:26.240
in this sort of ways that humans could reason about.

50:26.240 --> 50:28.480
And then later they were assembled down

50:28.480 --> 50:32.600
to the actual programming of the computer,

50:32.600 --> 50:34.480
like programming the instructions into the computer.

50:34.480 --> 50:37.880
And so what we've done is created a higher level construct

50:37.920 --> 50:40.000
called assembly that humans are able to reason about

50:40.000 --> 50:44.440
more easily, which maps down to the Von Neumann model

50:44.440 --> 50:47.040
in terms of actually programming the computer.

50:47.040 --> 50:48.480
Does that make sense?

50:48.480 --> 50:51.000
Ultimately, it's sort of like syntactic sugar

50:51.000 --> 50:55.360
or addition in that assembly doesn't add any more constructs.

50:55.360 --> 50:57.960
Like there, all of the rules of assembly are defined

50:57.960 --> 51:00.480
in terms of the Von Neumann axioms.

51:03.360 --> 51:05.400
And then we have Fortran.

51:05.400 --> 51:07.760
So Fortran is even higher level

51:07.760 --> 51:11.480
and here it adds constructs like if statements and loops.

51:11.480 --> 51:14.360
And you can imagine in 1957,

51:14.360 --> 51:17.960
there really wasn't anyone who had conceived

51:17.960 --> 51:21.400
of like a general notion of loops

51:21.400 --> 51:22.960
or even like conditionals, right?

51:22.960 --> 51:25.920
All we had were these like branch instructions

51:25.920 --> 51:27.760
and maybe it was sort of implicitly defined

51:27.760 --> 51:30.040
that you could make looping constructs from it.

51:30.040 --> 51:31.440
But then people were like, wait,

51:31.440 --> 51:34.280
why don't we just embrace this high level notion

51:34.280 --> 51:39.080
of a looping construct and embedded in our language?

51:39.080 --> 51:40.280
But just like assembly,

51:40.280 --> 51:43.480
looping doesn't actually give you any more expressivity.

51:43.480 --> 51:45.120
Every single loop can be defined

51:45.120 --> 51:47.720
in terms of the lower level constructs.

51:49.160 --> 51:50.320
Then we have C.

51:51.440 --> 51:53.320
C introduces these things called functions

51:53.320 --> 51:56.880
and then we have the ability to create more complex

51:56.880 --> 51:59.080
structures of data called structs.

51:59.080 --> 52:02.000
And then we have the ability to dynamically allocate

52:02.000 --> 52:06.360
in free memory as opposed to just using,

52:06.360 --> 52:09.640
you can imagine kind of manually dealing with

52:09.640 --> 52:11.560
all of the memory on your physical hardware

52:11.560 --> 52:14.920
as opposed to some other memory manager, right?

52:17.040 --> 52:20.240
And then finally, we have C++ in 1985,

52:20.240 --> 52:23.280
introduces this concept called classes and objects.

52:23.280 --> 52:26.320
I'm not sure if like these concepts on the right

52:26.320 --> 52:29.040
were introduced solely by the languages.

52:29.040 --> 52:30.800
I probably not, they probably came

52:30.800 --> 52:33.560
in some other flavor,

52:33.560 --> 52:35.680
but I think these languages over here

52:35.680 --> 52:39.200
are the most significant in terms of widespread use.

52:39.200 --> 52:40.560
So that's really what I'm trying to get at.

52:40.560 --> 52:43.040
It's not as much attribution as it much

52:43.040 --> 52:45.520
as it is sort of relatively speaking

52:45.520 --> 52:48.400
when these ideas popped up into existence.

52:48.400 --> 52:51.800
But just like pianos axiom towers,

52:51.800 --> 52:53.460
where you have kind of irrational numbers

52:53.460 --> 52:56.320
like up at the top, classes and objects

52:56.320 --> 52:59.280
are really just defined in relation

52:59.280 --> 53:01.960
to von Neumann instructions.

53:01.960 --> 53:03.800
Everything boils down to those things.

53:06.400 --> 53:08.800
So we can think of the von Neumann machine instructions

53:08.800 --> 53:12.240
almost like the axioms for computing,

53:12.240 --> 53:13.680
for modern computing really.

53:14.800 --> 53:18.040
And the Turing, so this claim is my own.

53:18.040 --> 53:21.800
Like after studying like the history of this,

53:21.800 --> 53:25.580
I asked myself the question, like, why is this tower,

53:25.620 --> 53:28.740
like these languages specifically so much more popular

53:28.740 --> 53:31.060
compared to the corresponding languages

53:31.060 --> 53:33.840
and ideas in the Lambda calculus tower.

53:33.840 --> 53:38.220
And my belief is that the Turing machine axiom tower

53:38.220 --> 53:40.620
is actually easily implementable in hardware

53:40.620 --> 53:43.020
because it's sort of like a physical device.

53:43.020 --> 53:44.580
And because you can implement it in hardware,

53:44.580 --> 53:47.260
you can actually compute stuff with it

53:47.260 --> 53:49.980
as opposed to it being relegated to pure math, right?

53:51.300 --> 53:53.660
The final idea is that a compiler

53:53.660 --> 53:57.020
is just something that takes like a higher level construct

53:57.020 --> 54:01.500
and reduces it down to its axiomatic von Neumann definition.

54:01.500 --> 54:03.180
That's all what a compiler is.

54:05.700 --> 54:06.540
Sound good?

54:09.940 --> 54:12.140
Okay, the Lambda calculus tower.

54:12.140 --> 54:13.980
So this one looks very different

54:13.980 --> 54:16.900
because the first thing that we have

54:16.900 --> 54:20.420
is just variables, functions and function application.

54:20.420 --> 54:22.140
And we've already kind of seen some constructs

54:22.140 --> 54:24.100
that you can build on top of that.

54:24.100 --> 54:25.860
But one of the most interesting ones

54:25.860 --> 54:28.100
is this idea called Lisp,

54:28.100 --> 54:31.660
which came about in the 1950s.

54:31.660 --> 54:34.580
And it came about also by a mathematician,

54:34.580 --> 54:36.460
his name was John McCarthy.

54:36.460 --> 54:39.980
And what McCarthy did was,

54:39.980 --> 54:42.780
if you look at piano's axioms,

54:42.780 --> 54:45.740
the definitions of those axioms were sort of defined

54:45.740 --> 54:49.300
in terms of English and mathematical notation, right?

54:49.300 --> 54:53.500
McCarthy said, what if we could take Lambda calculus

54:53.500 --> 54:55.500
or structures like that

54:55.500 --> 55:00.500
and define those axioms in the language itself?

55:01.980 --> 55:04.820
And he created this language called Lisp.

55:04.820 --> 55:09.820
And basically the implementation of Lisp is in Lisp itself.

55:10.500 --> 55:12.700
And because he was a mathematician,

55:12.700 --> 55:15.940
he had no need to actually implement it on a real computer.

55:15.940 --> 55:18.660
And so this was sort of the first example

55:18.820 --> 55:21.020
of what we call like a meta-circular construct.

55:21.020 --> 55:23.620
So the construct is defined in terms of itself

55:23.620 --> 55:25.380
and it's fully self-containing.

55:25.380 --> 55:26.860
And I think a rite of passage

55:26.860 --> 55:29.100
for like every single computer scientist

55:29.100 --> 55:32.180
is to build your own Lisp interpreter.

55:32.180 --> 55:36.940
And so McCarthy kind of proposed this idea in 1958

55:36.940 --> 55:38.940
and then his students went along

55:38.940 --> 55:41.140
and actually implemented Lisp

55:41.140 --> 55:46.140
as on top of one Neumann machine to actually compute stuff.

55:46.820 --> 55:49.940
The next idea is System F.

55:49.940 --> 55:52.780
So this popped up in 1972

55:52.780 --> 55:55.740
and you can think of System F as like Lambda calculus

55:55.740 --> 55:58.020
except it has support for types.

55:58.020 --> 56:01.940
So the Lambda calculus that I kind of showed you before

56:01.940 --> 56:03.020
didn't really have any types.

56:03.020 --> 56:04.620
So it's kind of like JavaScript.

56:04.620 --> 56:08.220
System F is kind of the typescript equivalent

56:08.220 --> 56:09.500
of Lambda calculus.

56:10.820 --> 56:12.900
But a lot more sophisticated for reasons

56:12.900 --> 56:14.140
that I don't want to get into.

56:14.140 --> 56:17.340
But really every single System F construct

56:17.340 --> 56:18.780
can be boiled down

56:18.780 --> 56:21.420
into its corresponding Lambda calculus construct.

56:21.420 --> 56:25.060
So very similar to how Fortran didn't add any expressivity.

56:25.060 --> 56:27.740
System F didn't really add any expressivity either.

56:29.020 --> 56:33.180
Then on top of this, we have these languages called ML.

56:33.180 --> 56:36.220
I think ML stands for meta language and OCaml

56:36.220 --> 56:40.420
which is the sort of most widely used flavor of ML.

56:41.300 --> 56:43.980
And it introduced like higher level constructs

56:43.980 --> 56:45.180
like pattern matching.

56:45.180 --> 56:47.580
You guys haven't spent much time in functional programming.

56:47.580 --> 56:50.740
Like it's, this whole tower is super weird

56:50.740 --> 56:54.660
because these constructs don't actually carry over cleanly

56:54.660 --> 56:58.380
to the imperative tower.

56:58.380 --> 57:01.060
Sorry, the Von Neumann tower, the Turing tower.

57:02.060 --> 57:06.020
On top of this, we have this language called Haskell

57:06.020 --> 57:09.180
which earliest roots of it popped up in 1985.

57:10.460 --> 57:13.220
Basically the same year that C++ came about

57:13.220 --> 57:15.300
was when Haskell came about

57:15.300 --> 57:18.140
or the predecessor to Haskell came about.

57:18.140 --> 57:19.820
And the cool thing about Haskell

57:19.820 --> 57:22.860
is that it is a general purpose programming language

57:22.860 --> 57:25.540
that can do IO and things like that.

57:25.540 --> 57:29.140
But its constructs are still pure.

57:29.140 --> 57:32.580
So it still has pure math functions like everywhere.

57:32.580 --> 57:34.820
And today Haskell is sort of like the king

57:34.820 --> 57:37.620
of statically typed functional programming languages.

57:39.500 --> 57:40.820
But now we get into some stuff

57:40.820 --> 57:44.700
which might be more relevant to your guys' experience.

57:44.700 --> 57:47.260
In 2012, Elm kind of popped up

57:47.260 --> 57:50.140
and Elm is very much a functional programming language

57:50.140 --> 57:52.580
even though it compiles down into JavaScript.

57:52.580 --> 57:57.140
And the Elm people essentially pioneered

57:57.140 --> 57:59.420
the flux-redex pattern.

57:59.420 --> 58:02.780
So this idea that actions result in

58:03.980 --> 58:05.740
essentially the production of a new state

58:05.740 --> 58:08.220
and that new state can be used to render a new view

58:08.220 --> 58:10.420
and there's a sort of like one-way data flow.

58:10.460 --> 58:13.300
This idea popped up in 2012.

58:13.300 --> 58:15.660
But if you're kind of thinking about the world

58:15.660 --> 58:17.660
in terms of the Lambda calculus tower,

58:17.660 --> 58:20.260
this idea is actually not that novel.

58:20.260 --> 58:23.420
It's sort of a very obvious outcome

58:23.420 --> 58:25.820
of dealing with the constraints of the Lambda tower.

58:27.580 --> 58:30.900
Then from here in 2013, we have React.

58:30.900 --> 58:33.740
React kind of makes a claim that the view

58:33.740 --> 58:37.540
needs to be a pure function of the state or your props.

58:38.580 --> 58:39.780
But really it's kind of just the same.

58:39.780 --> 58:43.300
So given a state, we can always render the same view

58:43.300 --> 58:45.820
like deterministically as a pure function.

58:45.820 --> 58:48.940
And at first, like if you're coming from jQuery,

58:48.940 --> 58:50.860
adopting the React pattern was probably

58:50.860 --> 58:53.300
like extremely frustrating.

58:53.300 --> 58:56.980
And for those of you, if you kind of recall back

58:56.980 --> 58:58.900
to your first experiences with React,

58:59.900 --> 59:02.380
you kind of felt like there was these artificial constraints

59:02.380 --> 59:03.460
being imposed upon you.

59:03.460 --> 59:05.620
Like I just want to hide the modal.

59:05.620 --> 59:07.180
Why can't I do that?

59:07.380 --> 59:08.220
Right?

59:09.620 --> 59:11.100
But then eventually as you start to build

59:11.100 --> 59:14.260
larger and larger apps, you realize

59:14.260 --> 59:17.020
that like this sort of one-way data flow constraint

59:17.020 --> 59:19.700
makes it way more easy to reason about

59:19.700 --> 59:22.500
what your view is going to look like given a state.

59:22.500 --> 59:25.020
And the point I'm trying to make over here

59:25.020 --> 59:29.260
is that one, these ideas are not new.

59:30.140 --> 59:33.180
Like Lambda calculus kind of forces us

59:33.180 --> 59:36.260
to embody this perspective that the output of a function

59:36.300 --> 59:40.260
is a pure outcome of its inputs, right?

59:40.260 --> 59:43.780
And it's just now in like 2012, 2013

59:43.780 --> 59:46.500
that we're starting to re-embrace these ideas.

59:46.500 --> 59:49.260
And I guess most of us believe

59:49.260 --> 59:52.340
that I can't even imagine building a UI

59:52.340 --> 59:54.460
in a non-reactive way.

59:54.460 --> 59:56.060
It's sort of like taken as given.

59:56.060 --> 59:59.700
And so I think that if more engineers spend time

59:59.700 --> 01:00:03.180
kind of thinking about the history of this thing,

01:00:03.220 --> 01:00:05.100
it becomes a lot more like,

01:00:06.220 --> 01:00:09.860
you can understand more like why React looks the way it does

01:00:09.860 --> 01:00:11.980
or why Elm looks the way it does.

01:00:11.980 --> 01:00:16.180
And rather than trying to apply your like Turing machine

01:00:16.180 --> 01:00:19.180
imperative programming mindset to functional programming,

01:00:19.180 --> 01:00:21.420
you can kind of build your way up

01:00:21.420 --> 01:00:23.220
starting with Lambda calculus going up.

01:00:23.220 --> 01:00:26.700
And I think that path actually makes it much more easy

01:00:26.700 --> 01:00:28.540
to reason about functional programming.

01:00:28.660 --> 01:00:33.660
As a fun side effect or a fun anecdote,

01:00:37.940 --> 01:00:40.180
the original compiler for React

01:00:40.180 --> 01:00:43.860
when it was still like an experimental project at Facebook

01:00:43.860 --> 01:00:45.140
was written in OCaml.

01:00:48.060 --> 01:00:50.740
All right, so the final like concession that I'll make

01:00:50.740 --> 01:00:53.740
is that Lambda calculus is really hard

01:00:53.740 --> 01:00:55.900
to implement in hardware.

01:00:55.900 --> 01:00:59.700
And whereas the Turing machine von Neumann model

01:00:59.700 --> 01:01:01.980
is obviously very easy to implement in hardware.

01:01:05.420 --> 01:01:07.860
Okay, final slide.

01:01:07.860 --> 01:01:09.780
React is to jQuery as Lambda calculus

01:01:09.780 --> 01:01:10.980
is to the Turing machine.

01:01:12.140 --> 01:01:15.300
So in jQuery, you have this concept called

01:01:15.300 --> 01:01:17.300
like the DOM is your state.

01:01:17.300 --> 01:01:21.060
So all of the HTML elements that are there is your state.

01:01:21.060 --> 01:01:24.380
You've probably written jQuery code that looks like this.

01:01:24.380 --> 01:01:27.300
jQuery.model.show and shows the modal.

01:01:27.300 --> 01:01:29.100
And basically what I'm trying to get at here

01:01:29.100 --> 01:01:32.220
is that whether the modal is being shown

01:01:33.100 --> 01:01:37.300
is encoded in the DOM itself.

01:01:39.260 --> 01:01:41.580
Anything can make modifications to the DOM

01:01:41.580 --> 01:01:44.380
and the DOM as a result ends up

01:01:44.380 --> 01:01:46.660
in these weird unexpected states

01:01:46.660 --> 01:01:48.980
because you didn't precisely reason

01:01:48.980 --> 01:01:50.820
about state modifications,

01:01:50.820 --> 01:01:54.080
kind of just wrote code like this over and over again

01:01:54.080 --> 01:01:56.520
until it essentially resulted

01:01:56.520 --> 01:01:58.880
in a Turing machine like construct

01:01:58.880 --> 01:02:03.440
where it's difficult to reason about the tape.

01:02:03.440 --> 01:02:07.520
And so in React, the state is explicitly defined, right?

01:02:07.520 --> 01:02:11.100
It's an input to your render function kind of implicitly

01:02:11.100 --> 01:02:13.400
and your view is a pure function of the state.

01:02:14.320 --> 01:02:17.940
And you don't modify the state, you produce a new state.

01:02:19.240 --> 01:02:21.960
And so React's constraints actually make it easier

01:02:22.000 --> 01:02:24.880
to reason about the state and the DOM.

01:02:24.880 --> 01:02:28.240
And by analogy, functional programming's constraints

01:02:28.240 --> 01:02:30.600
make it easier to reason about programs.

01:02:32.640 --> 01:02:34.320
And so if you're intrigued

01:02:36.440 --> 01:02:38.640
and wanna learn more about the Lambda Tower,

01:02:38.640 --> 01:02:41.740
I highly recommend taking this approach.

01:02:41.740 --> 01:02:43.240
If you guys haven't done Advent of Code,

01:02:43.240 --> 01:02:45.980
it's essentially this wonderful set of problems

01:02:45.980 --> 01:02:47.960
that show up every December.

01:02:47.960 --> 01:02:49.720
Solve those problems in Elm.

01:02:50.400 --> 01:02:52.960
Elm is a really good intro to functional programming

01:02:52.960 --> 01:02:57.960
because the compiler messages were meant for humans

01:02:59.720 --> 01:03:01.880
and the whole ecosystem is built

01:03:01.880 --> 01:03:03.920
so that it's easy to pick up and learn.

01:03:04.940 --> 01:03:07.880
And if you're familiar with the React-Redux pattern,

01:03:07.880 --> 01:03:11.840
that kind of came from Elm and it becomes like,

01:03:11.840 --> 01:03:14.720
you can build some cool stuff right out of the get-go.

01:03:16.960 --> 01:03:18.960
All right, that's all I got.

01:03:18.980 --> 01:03:20.320
Thanks.

01:03:20.320 --> 01:03:21.160
Thank you.

01:03:21.160 --> 01:03:22.000
Thank you.

01:03:22.000 --> 01:03:22.840
Thanks.

01:03:26.840 --> 01:03:27.680
Yes.

01:03:41.880 --> 01:03:45.440
It's tough for me to say it because I wasn't there.

01:03:45.440 --> 01:03:48.960
But I think it's sort of like a chicken and egg situation,

01:03:48.960 --> 01:03:52.000
because we didn't have machines that could compute stuff.

01:03:52.000 --> 01:03:54.880
We didn't rely on those machines to compute the stuff.

01:03:54.880 --> 01:03:55.840
But then all of a sudden,

01:03:55.840 --> 01:03:57.800
the machine to compute stuff popped up

01:03:57.800 --> 01:04:00.120
and I got to imagine the first sets of calculations

01:04:00.120 --> 01:04:03.440
were just silly, solving linear equations.

01:04:03.440 --> 01:04:05.400
But then eventually people started to realize

01:04:05.400 --> 01:04:07.520
we could do compelling things with this.

01:04:07.520 --> 01:04:11.120
I'm sure the military was one of the first users of it.

01:04:11.120 --> 01:04:13.440
We can do ballistic missile trajectory calculations

01:04:13.440 --> 01:04:14.480
very easily.

01:04:14.480 --> 01:04:17.080
And then, obviously, computing is now universal.

01:04:36.920 --> 01:04:38.440
Questions, questions.

01:04:38.440 --> 01:04:39.280
Yeah.

01:04:45.480 --> 01:04:48.480
Have you heard of ReasonML?

01:04:48.480 --> 01:04:52.480
Okay, so ReasonML is a rewrite of the OCaml syntax

01:04:52.480 --> 01:04:57.480
to make it more comfortable for JavaScript developers.

01:04:57.480 --> 01:05:00.480
Because the OCaml syntax is kind of stodgy

01:05:00.480 --> 01:05:02.480
if you first look at it.

01:05:02.480 --> 01:05:04.480
Whereas ReasonML, if you're coming from JavaScript,

01:05:04.480 --> 01:05:05.480
it looks very similar.

01:05:05.480 --> 01:05:10.480
But ReasonML is not a new language.

01:05:10.480 --> 01:05:12.480
All it does is transpile down to OCaml.

01:05:13.480 --> 01:05:16.480
And so if you want to get started with OCaml,

01:05:16.480 --> 01:05:18.480
I wouldn't necessarily recommend it.

01:05:18.480 --> 01:05:20.480
I would recommend starting with Elm first.

01:05:20.480 --> 01:05:24.480
But then from there, if you want to build programs

01:05:24.480 --> 01:05:27.480
that can interoperate with JavaScript really easily,

01:05:27.480 --> 01:05:31.480
I think ReasonML is the best way to go.

01:05:31.480 --> 01:05:32.480
Yeah.

01:05:32.480 --> 01:05:33.480
Yeah.

01:05:49.480 --> 01:05:52.480
Yeah, there was a lot of researchers in the 1980s

01:05:52.480 --> 01:05:54.480
that actually tried this.

01:05:54.480 --> 01:05:56.480
I think they built some prototypes.

01:05:56.480 --> 01:06:00.480
The problem is that you have these sort of positive feedback

01:06:00.480 --> 01:06:01.480
loops in ecosystems.

01:06:01.480 --> 01:06:04.480
And so the Turing model and the Von Neumann model

01:06:04.480 --> 01:06:11.480
essentially caught on so well that even though in theory

01:06:11.480 --> 01:06:14.480
the Lambda Tower might allow for more expressivity,

01:06:14.480 --> 01:06:18.480
practically speaking, the best computers are in the Turing model.

01:06:18.480 --> 01:06:21.480
And therefore, more attention kind of gravitates towards there.

01:06:21.480 --> 01:06:23.480
People build more stuff for it.

01:06:23.480 --> 01:06:26.480
And now, I don't know, 90% plus of all languages

01:06:26.480 --> 01:06:28.480
are kind of all Turing-based.

01:06:28.480 --> 01:06:32.480
So in the 80s, they did build functional programming computers.

01:06:32.480 --> 01:06:37.480
But because most of investment into these technologies

01:06:37.480 --> 01:06:41.480
comes as a function of industry, like businesses,

01:06:41.480 --> 01:06:43.480
like needing to solve business problems,

01:06:43.480 --> 01:06:48.480
then the positive feedback loop of the Turing Tower

01:06:48.480 --> 01:06:52.480
kind of diminished the Lambda Tower.

01:06:52.480 --> 01:07:11.480
Yeah, so I would probably boil it down to just the three

01:07:11.480 --> 01:07:12.480
constructs, right?

01:07:12.480 --> 01:07:14.480
You have variables.

01:07:14.480 --> 01:07:17.480
You have functions and function applications.

01:07:17.480 --> 01:07:21.480
So is there a way to represent a variable in some sort

01:07:21.480 --> 01:07:22.480
of circuitry?

01:07:22.480 --> 01:07:25.480
Is there a way to represent a function in circuitry

01:07:25.480 --> 01:07:27.480
as well as function application?

01:07:27.480 --> 01:07:29.480
I think the answer to all those is you probably

01:07:29.480 --> 01:07:30.480
conceive of some way.

01:07:30.480 --> 01:07:35.480
I don't know the details of how the actual Lambda computers

01:07:35.480 --> 01:07:38.480
were built, but it might be an interesting thing to look into.

01:07:38.480 --> 01:07:40.480
But they obviously fizzled out.

01:07:40.480 --> 01:07:43.480
The nature of the Von Neumann model in Turing machines

01:07:43.480 --> 01:07:47.480
is that it maps so cleanly to binary circuitry

01:07:47.480 --> 01:07:51.480
and originally vacuum tubes, but now transistors

01:07:51.480 --> 01:07:54.480
map so perfectly down to that model,

01:07:54.480 --> 01:07:56.480
whereas the concept of a function doesn't

01:07:56.480 --> 01:08:12.480
map to logic gates cleanly.

01:08:12.480 --> 01:08:14.480
All right.

01:08:14.480 --> 01:08:16.480
Well, I guess with that we'll wrap up.

01:08:16.480 --> 01:08:17.480
Thank you everyone.

