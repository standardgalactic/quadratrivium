{"text": " Okay, great, welcome everyone and thank you for joining us. This is the last distinguished lecture series for the Institute of Expansion AI for the academic year. We'll resume again in September with a full program for the year. As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series, which is designed to feature a lot of our Northeastern University experts and faculty and so forth. In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic who's at the Curie College. My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential AI and also Professor of the Practice in the Curie College for Computer Sciences and it is my pleasure today to introduce Yan Lokhan. Yan is a very well-known name in the field. I've known him for many years, I think at one point in my life I interviewed at Bell Labs or AT&T Labs and that's when he was there. He is VP and Chief AI Scientist at Meta, also known as Facebook, and Silver Professor at NYU affiliated with the Kauan Institute of Mathematical Sciences and the Center for Data Science, which he actually founded. He was the founding director of FAIR, I learned this morning that FAIR used to stand for Facebook AI Research, now it's changed to MetaFair for Fundamental AI Research and of course he founded the NYU Center for Data Science, received an engineering diploma from SEA in Paris and a PhD from the Saoubon University. After a postdoc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs in 1996 as Head of Image Processing Research. He joined NYU as Professor in 2003 and Meta or Facebook in 2013. He is the recipient of the 2018 ACM Touring Award, along with Jeffrey Hinton and Yashua Benjio, and for those of you who don't know the Touring Award, it's essentially the equivalent of the Nobel Prize for Computer Science, the toughest award to get from the ACM. The award was for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. He is a member of the National Academy of Sciences and the National Academy of Engineering, amongst many others. His interests include AI, machine learning, computer perception, robotics and computational neuroscience, and I'm sure you're all eager to hear from Jan on what's been happening with generative AI and what all the buzz is about, hopefully we'll get into the technical details and immediately following his talk, we will do a fireside chat where I will try to ask him some tough questions. And then we will also get questions from the audience. By the way, we did get online questions from the audience. We got 150 questions, so there's no way we're going to walk you through all of those. So we'll see how much time allows us to answer. Thank you and please join me in welcoming Jan to Northeastern University. Thank you, Samap. A real pleasure to be here and thanks for coming here so numerous or for listening in online. So I'm going to talk a bit about the state of the art in AI but also about the next step because I'm always interested in the next step and how we can make machines more intelligent. And we need to figure out how to get machines that cannot just learn but also can reason and plan and current AI really does not allow current systems to do this. So I'll try to kind of sketch a potential pathway towards such systems. I can't say that we built it completely but we built some components and I go through this. So AI is in the news, everybody is playing with it at the moment. It's pretty amazing how it works. There's a lot of success. It's been very widely deployed very much in many applications that are behind the curtain but in some of them much more visible. So LLMs have the advantage of being visible but for the last 10 years or so there's massive use of AI and the latest development of AI for such thing as ranking for search engine and social networks or for content moderation, things like that. But overall machine learning requires a lot of data and the machines that we have are somewhat brittle, specialized. They don't have human-level intelligence despite what we may be led to believe. So in short, machine learning sucks at least compared to humans and animals. We've been using supervised learning which really was the workhorse of machine learning and AI systems until very recently. Reinforcement learning is insanely inefficient but it works really well for games but not many other things. So one thing that has taken over the AI world in the last few years is something called self-supervised learning which I will talk about at length. But current AI systems are specialized and brittle. They make stupid mistakes. They don't really reason and plan with a few exceptions for a game playing for example. Compared to humans and animals, they can learn new tasks extremely quickly, understand how the world works, can reason and plan have some level of common sense. Machines still don't have common sense. So how do we get machines to reason and plan like animals and humans learn as fast as animals and humans? And we'll need machines that can understand how the world works, can predict the consequences of their actions, can perform change of reasoning with unlimited number of steps, can plan complex tasks by decomposing them into simpler tasks. So let's start with this idea of self-supervised learning. It's really taken over the world. Every sort of top machine learning system today uses some form of self-supervised learning as a first step to pre-train the system. And it's used everywhere. What does it consist of? It's really the idea that instead of having, of training a system with an input and an output, which is the case in supervised learning, or with an input and a reward, which is the case for reinforcement learning, you train the system to basically model its input. You don't train it for any particular task other than capture the dependency between different parts of its input. So one thing you might do is, for example, take a piece of video, a piece of text, show a piece of the video to the system and ask it to predict the missing piece, like the continuation of that video. And after a while, you reveal the rest of the video and you adjust the system so that it does a better job at predicting. So prediction really is kind of the essence of intelligence. And to some extent, by training a system to predict, it doesn't have to be predicting the future. It could be predicting the past or the left from the right. You're training the system to represent data, essentially. And that's been nothing short of astonishingly successful in the domain of natural language understanding. So every type performing NLP system today is pre-trained the following way, or with some form of the following way, which is a special case of an old idea called denoising autoencoder. And the idea is that you take a piece of text, sequence of words from a corpus. Typically it would be a few hundred or a few thousand words long. Those words immediately get turned into vectors, but let me not talk about this for just now. So the first thing you do is you corrupt this text. You remove some of the words and replace them by blank markers, or you substitute them for another word. And then you train some gigantic neural net to predict the words that are missing. In the process of doing so, the system has to basically develop some sort of understanding of the text, because if you want to be able to predict what word comes here, you have to understand the role of the word in the sentence, the type of word that comes here, and the whole meaning of the sentence. So the system basically learns to represent text. And the amazing thing is that just by doing this, you can train a system to represent the meaning of text in pretty much any language, as long as you have data. With a single system, you can have a system that represents the meaning of a piece of text in any language. So pretty cool. You can use this to build translation systems, systems that detect hate speech on social networks or figure out what something talks about. The way you do this is that you chop off the last few layers of that gigantic neural net, and you use the representation, the internal representation, learned by the system as input to a subsequent downstream task that you train supervised, like, say, translation. And it's really astonishing how well this works. So from this to a generative AI system, there's a small step, particularly for text generation. Text generation is a completely different thing, which I'm not going to talk about, but although some systems use the same technique. So what is a generative text generation system, a large language model? It's a system of the type I just described, except that when you train it, you don't remove random words in the text that you show at the input, you only remove the last one. So you train the system to predict the last word in a sequence of words. So show a sequence of words, and then show the last word, and train some gigantic neural net, perhaps with billions or hundreds of billions of parameters, to predict the next word. And you have to train this on trillions of text snippets, typically one to two trillion for the biggest models. Once you have that system, you can use it to generate text using what's called autoregressive prediction, which is a very classical thing to do in single processing. So you take a piece of text called a prompt, you enter it into the system, you have it predict the next word, and then you shift that word into the input. So now it becomes part of the input to the system, and now you can predict the next word, shift it in, predict the third word, shift it in. That's autoregressive prediction. And that's how all the bigger alarms that everybody has played with work. That's how they've been trained. That's how they generate text. So those alarms are kind of amazing in terms of the performance that they produce. So again, they're trained on something like one to two trillion tokens. A token is like a word or a subword unit. And there's a whole bunch of those models, most of which you probably haven't heard of, but there's a few that have become household names. So we've heard of chatGBT and GPT-4 from OpenAI, which are kind of usable, barred from Google, and derivative of chatGBT and GPT-4 from Microsoft, married with Bing. But there's a long history of those things that goes back several years, some from Fair, Lunderbot, and Galactica. Galactica was trained on the scientific literature and is designed to help scientists write papers. And a more recent one, called LAMA, which is the code is open source. The model, you can get it on request if you are using it for research purpose. And it's the same level of performance as things like chatGBT, but it's not fine-tuned. You have to fine-tune it for application. And in fact, people have done this, so Alpaca is a model which basically is a fine-tuned version of LAMA that was built by people at Stanford for answering questions and things like that, instruction. So they're pretty amazing, they surprised a lot of people in how well they work, but they make a lot of factual errors, logical errors, inconsistencies, limited reasoning abilities, things like that. And they are easy to, they're pretty gullible. So you tell them, what is 2 plus 2, and the system will say 4, and you say, no, actually 2 plus 2 equals 5. Oh yeah, you're right, I made a mistake. So they kind of, they predict answers that would sound like someone could produce these answers, but the details might be wrong. So you can't really use them for factual answers, but you can use them certainly for writing aids. And particularly, it works really well for text or for standard sort of templatized text that you need to write, like I don't know, there's a bunch of professors here that have to spend quite a bit of time writing recommendation letters for students, very useful for that. And very useful for code generation. So the software industry is probably going to be revolutionized by such tools. So this is an example of code generated from a prompt by the Lama 65 billion model, the open source one. So, you know, ask it, you know, find real roots of AX squared plus BX plus C, and the thing just writes a function in Python or whatever, whatever you want, or Reg X or whatever, who remembers the syntax of Reg X? Like. You can have it, you know, hallucinate text that might sound plausible or completely implausible like this. Did you know that Yanukun dropped a rap album last year? We listened to it and here is what we thought. And the thing writes a review of my alleged rap album. I'm not much of a rap person, I'm more of a jazz person, so when my colleagues showed this to me, I told them, like, can you do the same for a jazz album that would be kind of more appropriate? I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't work very well because there's not enough training data on the web of reviews of jazz albums. I found that incredibly sad, I cried. So you need a lot of data to train those things, right? In fact, the amount of data, like something like 1.5 trillion tokens that Lama is trained on, it would take about 22,000 years for a human reading eight hours a day at every speed to read the whole material. So obviously those things can accumulate a lot of knowledge, at least approximately. So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not good for producing factual and consistent answers, at least not yet. So a lot of LLMs are being augmented or extended so that they can use tools like calculators or database engines or whatever to search for information and then refer to the source. They're not good at all for reasoning, planning, or even for arithmetic. So but we are easily fooled by their language fluency into thinking that they are intelligent. They're not that intelligent. And they really have no understanding of the physical world because they're trained with text. And there's another flaw, which is a huge problem. It's the fact that if you imagine that there is the set of all possible answers represented by this sphere, disk, which is really a tree, right? Every token you add put, you have a certain number of options for what the token should be, what the word is. So it's a tree of all possible answers. Within this tree, there is a small subtree that corresponds to correct answers for the question being asked. And imagine that there is a probability E for any token that is produced by the system to be outside, to take you outside that tree of correct answers. Once you go outside that tree, you can't come back because it's a tree. So let's imagine that the probability per token is E. So the probability that a sequence of N tokens would be correct is 1 minus E to the power N, making the assumption that the errors are independent, which of course they're not, but that's kind of a crude assumption. And so the problem with this is that it's an exponentially divergent process, this autoregressive prediction, errors accumulate. And if you produce too many tokens, the thing will sort of diverge away from the set of correct answers, exponentially. And that's not fixable with the current architecture. You can fine tune those systems a lot to reduce E, but you're not going to make it go away. So I have a bold prediction, which is that the shelf life of autoregressive LLM is very short. My prediction is that five years from now, nobody in their right mind would use them. So enjoy it while it lasts. They'll be replaced by things that are better. And I'll hint about directions to kind of perhaps fix up those problems. So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema magazine, which is a philosophy magazine, about the fact that a system that is purely trained from text, from language, cannot possibly attain human level intelligence because much of what humans know is actually derived from experience of the physical world. This is true for a lot of human knowledge, but it's true certainly for almost the totality of animal knowledge. It's all about the world is no linguistic related, no language related. So linguistic abilities and fluency are not related to the ability to think. Those are two different things. There are some criticisms of autoregressive LLMs from people coming from the cognitive science realm who say like, this is not at all the way the human mind works. There is essential missing pieces. Other criticism for people who come from sort of more classical AI, pre-deep learning, they say like, you know, AI systems are supposed to be able to plan and reason, and those LLMs can do it. Or at least not, you know, they can do it maybe in very sort of primitive forms. Perhaps they can plan things in situations that correspond to a template that they've been trained on, but they're not so innovative. So we should ask, how is it that humans and animals can run so quickly? And I've been using this diagram for quite a while now, several, many years from Emmanuel Dupu, who's a cognitive scientist in Paris. And we tried to sort of make a chart of at what age babies learn basic concepts about the world, so things like distinguishing between animate objects and inanimate objects, learning the notion of object permanence, the fact that when an object is hidden behind another one, it still exists. Notion of rigidity, solidity, things like natural categories, babies don't need to know the name of an object to actually know that there are different categories of objects around four months or so. And then it takes about nine months for babies to really understand that sort of intuitive physics that objects that are not supported will fall, that, you know, objects have a momentum, weight, friction, you know, knowing that if I push on this object, you know, light objects like this, they're going to move, but if I push on an object that's heavier, it's not going to move unless I push harder. So things like that. So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where you have a little car on the platform, you push the car off the platform, it appears to float in the air, a five-month-old baby will pay attention, a 10-month-old baby will go like this because she understood that by then that objects are not supported or supposed to fall and this object appears to be floating in the air. So we can determine that her mental model of the world is being violated, okay? That's how this chart was built. So we accumulate as babies an enormous amount of background knowledge about how the world works, mostly by observation, a little bit by interaction, when we start being able to kind of grab things, but in the first few months it's mostly just observation. So we don't know how to reproduce this with this type of learning with machines. Once we accumulate all this background knowledge, you know, in a number of years, learning a new task like driving is very fast. So any teenager can learn to drive in about 20 hours of practice, mostly without causing any accident. So the teenager doesn't have to run off a cliff to figure out that the car, that nothing good is going to happen if you run off a cliff. The mental model of the world is already there, okay? We still won't have level five salivating cars. So obviously we're missing something pretty big. Any 10-year-old can clear up the dinner table and fill up the dishwasher. We're nowhere near having robots that can do this and it's not because of mechanical design, it's because we don't know how to build the minds behind it. So we're missing something big, right? The past towards human-level AI is not just making LLMs bigger, that's just not going to get us there. It's been a common recurring error by AI scientists and engineers over the last six decades to imagine that the one thing that they just discovered was the solution to human-level AI, only to discover a few years later that no, there was actually a big obstacle, another obstacle they had to clear. It's a recurring history story in AI. So common sense will probably emerge from the ability of machines to learn how the world works by observation, the way babies and animals do it. So I see three challenges for AI research over the next decade also, learning representations of the world and predictive models of the world, I'll say why in a minute, and self-supervised learning is going to be the key component of that, learning to reason. So psychologists talk about system one and system two. System one is the type of control that our brains use to kind of react to something without really having to think about it, like subconscious action. So if you're an experienced driver, you don't have to think about driving, you can just drive and you can talk to someone at the same time and barely pay attention. So that's system one. But then when you are learning to drive, you pay attention to absolutely everything. You use your entire focus, consciousness, attention to drive and that's system two. And then the last thing is learning to plan complex action sequences, decomposing them into simpler ones. So I wrote a sort of vision paper about a year ago, which I posted on open review for comments, so you're welcome to comment on it. I give a bunch of technical talks about it, one of the earliest one was at Berkeley, but you are having a more recent version of it right now, so you don't need to look at that one. And it's based on what's called a cognitive architecture. So basically how can we sort of design a system with different modules so that those modules may implement all the properties that I was telling you about so systems can perceive, reason, predict, in particular predict the consequences of their actions and then plan a sequence of actions to optimize, to satisfy a particular objective. So the main components of the system is the key component, I would say, is the world model and the world model is what allows the system to predict ahead, imagine what's gonna happen. This is to some extent what current AI systems don't really have. Perception system basically gets an estimate of the state of the world and initializes the world model with it. The cost here is a really important module, so basically the entire purpose of the agent is to minimize this cost. So the cost is something that uses a measurement of the state of the agent, particularly the prediction from the world model, and predicts whether an act is going to be good or bad. And the entire purpose of the agent here is to figure out a sequence of actions, so this is taking place in the actor, figure out a sequence of actions such that when I predict what's gonna happen as a consequence of those actions using my world model, my objective, my cost function will be minimized. So if my cost function is, so the cost function is basically our measures of discomfort of the agent. Biological brains have things like that in the visual language, so this is the thing that tells you when you're hungry, for example, or you're hurting. So nature tells you you're hungry, nature doesn't tell you how to feed, you have to figure that out by yourself, perhaps using your world model and your planning abilities. So this is the same thing here, imagine this is a robot and the robot battery are kind of starting to get drained, so there's a cost function here that says be careful, you're running out of power. And so the system, according to this world model, would say, well, I can recharge my battery by plugging myself into a socket. So it figures out the sequence of actions to plug itself into a socket and that will eventually minimize the cost function that just appeared. So in fact, there's two ways to operate that system one is the kind of system one where the system makes an estimate of the state of the world, run this to a perception system called an encoder here, produces an estimate of the state of the world called S0 and that runs into a neural net called a policy network that just produces an action and the action goes into the world. LLNs are like this, they are system one, you give them a pump, that's X, they produce an action, that's the token they predict, that goes back into the world and the world is very simplistic here, it's just you shift in the input. So no reasoning necessary, here is system two. So you use the same system here and this is a sort of time-enrolled version of the system. So we have the world model, the world model is this green module and the different instances of that green module are the state of the system at different time steps, so think of it as like a recurrent net that you unfolded, so it's really the same module at different time steps. What the world model is supposed to be able to predict is given the representation of the state of the world at time t and given an action that I'm imagining taking, what is going to be the predicted state of the world at time t plus one. So I can imagine a sequence of actions that I might take, imagine the effect on the world using my world model and then I can plug the state of the world over this trajectory through my cost and measure whether my cost is going to be minimized by this action sequence, my objectives. So what I should do is run some sort of optimization procedure that will try to search for a sequence of actions that minimizes the cost given the prediction given to me by the produced by the world model. This type of planning is very classical in optimal control. It's called model predictive control. In classical optimal control, the model is not learned usually, it's handcrafted. Here we are thinking about a situation where the world model is learned by, for example, watching the world go by, by video, but also by observing actions being taken in the world and seeing the effect. So to get a good accurate model here, I'm going to have to observe the state of the world, observe, like, take an action and observe the effect or observe someone else take an action and observe the effect. Let me skip this for now. Ultimately what we want is a hierarchical version of this because if you want the system to be able to plan complex actions, we can't plan it at the lowest level. So for example, if I want to plan to go from here to New York City, I would have to basically plan every millisecond exactly what muscle actions I should take, okay? And it's impossible, right? You can't plan an entire trip from here to New York City millisecond by millisecond, partly because you don't have a perfect model of the environment, like you don't know if, when you're going to walk up the room here, whether someone is going to be on the way, in the way and you're going to have to go around. So you can't completely plan in advance, right? So what we do is we plan hierarchically, say like, okay, I want to go to New York City, so the cost function at the top here measures my distance to New York City. And the first thing I have to do is go to the airport and catch a train or go to the train station and catch a train or go to the airport catch a plane. So the top predictors are predictors at a high level that says, oh, okay, if I catch a taxi, it might take me to the airport. If I catch, or to the train station, then if I catch a train, it'll take me to New York City. Okay, so you have those two hidden actions, those Z variables here. And they define a cost function for the next level down. So if the first action is I'm taking a taxi to the train station, the lower level is how do I catch a taxi here? I go down in the street and hail the taxi. No, this is Boston. I need to call it Uber or something. Okay, so I go on the street and I call it Uber. How do I go in the street? There's going to be lower levels. I have to get out of this building. How do we get out of this building? I have to walk through the door. How do I work through the door? I have to put one leg in front of the other over obstacles. And all the way down to millisecond. Also control for a short period, which is replanned as we go. Okay, no AI systems today can do any of this. This is completely virgin territory. Okay, there's a lot of people who've worked on hierarchical planning, but in situations where the representations at every level are hardwired, they're known in advance. They're predetermined. It's sort of like the equivalent of a vision system where the features at every level are hardwired or designed by hand. There's no system today. They can learn hierarchical representations for action plans. So that's a big challenge. The cost function, so here's what's important here. A lot of people today are talking about the fact that AI systems are difficult to control and that's terrible, maybe toxic, various things. The system I describe cannot produce outputs that do not minimize the objectives. And so if you have terms in the objective that guarantee certain conditions, that system will have no choice but obeying those conditions. Okay, so having a system that is designed like this, that whose output is produced by minimizing a set of objectives, according to a model, will basically help guarantee the safety of that system. Because you can hardwire intrinsic objectives on the left here that basically guarantee the safety. And the system cannot escape the satisfaction of those constraints. So let me take a very simple example. Let's say someone figures out how to build a domestic robot they can cook. This robot will have to be able to kind of handle a kitchen knife. And you might put a cost function that says, don't flail your arm if you have a kitchen knife in your arm and there is people around. Okay, because it's dangerous. So you can imagine putting a lot of kind of safety conditions in those systems to make them steerable. So I don't think the problem of making AI systems safe is such a huge problem that some people who are very vocal are seeing it is that AI is going to kill us all. It's not going to kill us all. We would have to screw up really badly for that to happen. Okay, now here's the thing. How do we build the world model? And that's basically the biggest challenge that we have at the moment. How do we build a system that can predict what's going to happen in the world? For example, by training itself to predict videos. Now the problem with predicting videos is that the world is not entirely predictable. It may not be deterministic, but even if it were deterministic, it wouldn't be completely predictable. So in fact, here is an example here. If you take a video, this is a top-down video of a highway that looks like cars driving around just following the blue car. And you train a neural net to predict what's going to happen in the video after the first few frames. It produces blurry, it makes blurry prediction because it can't predict if the car that's behind you is going to accelerate or break or change lane or whatever. So it makes an average of all the possible future and that's a blurry image. Same with, this is an old paper where we attempted to do video prediction using neural nets and the predictions are blurry because there's too many things that can plausibly happen and the system can only predict one thing. So it predicts the average. So that's no good. The solution to this is what I call a joint evading predictive architecture. And this is really the most important slide of the talk. So the normal way to make predictions is through a generative model. What's a generative model? It's a model where you have a bunch of variables you observe, let's say the initial segment of a video. You run it through an encoder and through a predictor and the predictor predicts y, which is, let's say, the continuation of that video. And you have some cost function that measures the discrepancy divergence between the predicted y and the actual y you observe. This is when you train your world model. It could be that the predictor has an action variable that comes in, but in this example there isn't. So examples of this are things like variational auto encoders, mass auto encoders, or denoising auto encoders, which is a more general concept. And so basically all NLP systems, including LMS, are of this type, the generative models. But here is the thing. You don't want to be predicting every detail about the world. Here you have to predict every single detail about the world. So it's easy if it's text, because text is discrete. So predicting the next word, I cannot predict the next word from a text. But I can predict within 10 possible words some probability distribution of the, over all the words in the dictionary of which word comes next, right? They can represent distributions over discrete variables. I cannot do this over the set of all possible video frames. I cannot usefully represent a distribution over the set of all possible video frames. So I can't use the same trick for video that is used for language. The reason why we have LMS that works so well is because text is easy. Language is simple. We only popped up in the last few hundred thousand years anyway, so it can be that complicated. And it's also processed in the brain by two tiny areas called the Vernike area for understanding and the Borke area for production. What about the rest of the brain? The prefrontal cortex, that's where we think, okay? That's not part of LMS, the LMS are perhaps good models of Vernike and Borke, but that's it. Okay, so what I'm proposing here is to replace this generative architecture by a joint embedding architecture and the essential characteristic of it is that the variable that you want to capture the dependency of with respect to X goes itself through an encoder and the encoder eliminates the relevant information that is not useful for anything. Okay, so for example, if I had a video of this, if I was shooting a video of the room here and then panning the camera and asking a system to predict what's the rest of the room, it would probably predict that the rest of the room looks like the initial part that there'd be a lot of people in different seats, but it couldn't predict your age, gender, hairstyle, clothing, or the texture, precise texture of the floor or things like that, right? So there's details that cannot possibly be predicted and one way to avoid predicting them is to basically eliminate that information from the variable to be predicted through an encoder. So that's a joint embedding architecture or predictive architecture because it has a predictor. Now there's an issue with this thing, which is that if you train a system with, let's say, a piece of video and the following piece of video and you just train it to minimize the prediction error, you train the whole thing, it collapses. It collapses, basically the encoders ignore the inputs, they produce constant vectors for SX and SY and the predictor just needs to map SX to SY and it's a constant, so it's super easy, okay, bad. So the question now is how do we prevent this from happening? How do we prevent it collapse? It doesn't happen with generative models because they can't collapse. So there are three flavors of those joint embedding architectures, a simple one where you're basically trying to make the two representation of SX and SY identical. So for example, X and Y are two different views of the same scene and you want SX to represent the content of the scene, so it doesn't matter where you look it from. You just want to make the representations equal. When the encoders are identical, this is called a Syme's network. This is another idea that goes back to the early 90s. You have deterministic joint embedding architectures and then you have joint predictive architectures that may be non-deterministic where the predictor function has a latent variable that could be drawn from a distribution or taken in a set that would allow that system to make multiple predictions if necessary. Now, we have to ask ourselves the question of how do we train those things? And I'm going to use a symbolism here where that I've used the rectangles and squares of cost functions, energy terms, the circles of variables, observed or not, and those symbols here are deterministic functions. Imagine a neural net, okay, trainable. We may have to hardwire some cost functions in the system to have it, to drive it to focus on aspects of the input that are important. So that's the purpose of that C cost function at the top. Okay, but to explain how to train those things, I'm going to have to explain a little bit what energy based models is about because the classical kind of probabilistic modeling in machine learning kind of goes at the window when we use the joint embedding architectures. So what's an energy based model? Energy based model is a learning system that captures the dependency between two sets of variable x and y through an energy function that is supposed to take low values, low energies around data, training samples. So imagine those black dots are training samples. You want that energy function f of x, y to take low values around the training samples and then higher values outside. And that system will capture the dependencies between x and y. If I give you a value of x and I ask you what can be the possible values for y, you're going to tell me, well, it's either this or that or maybe that other thing at the top. Okay, so it's not a mapping from x to y, it's an implicit function. And by figuring out what value of y minimizes the energy function, you can do inference. You can infer y, possibly, but you don't necessarily have to do that. So that's energy based model. It's kind of a weaker form of modeling than probabilistic modeling. And so now the learning problem becomes how do you train this energy function, which is going to be some big neural net, so that the energy takes low value around the training samples and high values outside. If you're not careful, you're going to get a collapse so that the same type of collapse I was telling you about before, if you just pull down the energy of the training samples, minimize the prediction error in this joint invading architecture, you're going to get zero energy for everything. It's not a good way to capture the dependencies. You have two classes of methods, contrastive methods. So contrastive methods consist in generating those green points, which are outside the region of data, and then push the energy up while you push down on the energy of the data points. Okay, so that's going to create a groove in the energy surface, and the system will have captured the dependency between x and y. But there's an alternative here, which is regularized methods, where the point of those methods is to minimize the volume of space that can take low energy, so that when you push down on the energy of data points, the rest of the space takes higher energy because there is only a small amount of, a small region of low energy to go around. So those are the two classes of methods. Every method you ever heard of in machine learning can be viewed as one of those two. Most probabilistic methods actually belong to the contrastive category. Anything that uses Monte Carlo sampling, for example, is contrastive. And then things like sparse coding and k-means and things like that are more on the regularized method side of things. Okay, so I'm asking you to do four things. Abandoned generative models in favor of the joint embedding architectures, right? So generative models are the most popular thing at the moment. Forget about it, at least if you're interested in getting to the next step in AI. Abandoned probabilistic models, because if you have those joint embedding architectures, you cannot actually use it to derive a pure y given x. The only thing you can use is sort of energy-based view. Abandoned contrastive methods in favor of those regularized methods, which I'll talk about a bit more. And then something I've said for many years now, abandoned reinforcement modeling because it's too inefficient. So those are some of the pillars of machine learning. And I realize this is not a very popular opinion here, but okay. So what about those regularized methods? I'm just going to give you one example. There's a whole bunch of them. There's like a dozen of them, but I'm just going to give you one called Vicreg. And the basic idea of it is to prevent those representations from collapsing. We're going to use a criterion that attempts to maximize the information content that comes out of those representations. Okay, so we're going to measure the information content in some way, and then maximize the information content or minimize the negative information content. We're going to do this for both SX and SY. We're also going to minimize the prediction error. And if we have a latent variable, we're going to have to minimize the information content of that latent variable. I can't explain why, because it would take too long. But you have to do that also to prevent another type of collapse. I'm going to focus on how you do that. So the sad news is we don't have good ways to measure information content, or we don't have any good ways to estimate lower bounds on information content, so that if we push up on this lower bound, the information content will go up. We only have upper bounds for information content. So we're going to do a very stupid thing, which is push up on the upper bound of information content, and hope the actual information content will follow. And it works. So, there's a simple way to prevent the encoder from completely collapsing. Which is to insist that every variable in SX, SX is a vector. And you insist that every variable, as measured over a batch, has a standard deviation that is at least one. Okay, so this is the cost that you see at the top here. Measure the standard deviation of each component of SX, and put it in a hinge loss so that the standard deviation is at least one. So that prevents the system from completely collapsing. But it can still cheat by making all the components of SX equal or correlated. So the second term says I want to minimize the off diagonal terms of the covariance matrix of those vectors measured over a batch, right? So I want pairs of variables to be uncorrelated. So basically, the collection of those two criterion says, if I measure the covariance matrix of those vectors, SX and SY, coming out over a batch, I want the covariance matrix to be as close to the identity as possible. There's a number of different methods that have been proposed to, that are kind of similar to this, Barlow-Twins. So this one is called Vic-Rag from my group at Meta in collaboration with Jean-Ponce. And then variations of it, but like similar methods from Berkeley in the E-Mise group at Berkeley called NCR squared. Yeah, maybe one minute. Yeah, so this works really well and I'm going to not bore you with tables of results that show you how well it works. Only to mention something else, which is another method to do this kind of self supervised running which is closer to this JEPA architecture called IJEPA. So this is for learning features for images without having to do the documentation. But basically it's for masking and this works amazingly well, it's very fast. It's a new method, paper is on archive. I don't have time to explain how it works, but basically you run an image through two encoders, one is the full image and the other one is sort of a masked image, partially masked image. You run them through the same encoder or very similar encoder and you try to predict or to predict the full feature representation of the full image from the representation obtained from the partial image. And just doing this produces really good features for images. You get really good performance on object recognition in images and stuff like that. Again, tables that show you that's true. But I'm coming to the end, so the reason for training those JEPA is to build world models. So architectures are this type. So this is a JEPA, but it's also a world model. That, given an observation about the state of the world, is going to be able to enter an action or imagined action in latent variables. It's going to predict what's going to happen next in the world. And once the time passes by, we're going to observe what happens and then perhaps adjust our system to train. But we want to use a hierarchical version of this where we can have a higher level, higher abstraction, higher level of abstraction representation that will allow us to make predictions further in the future. Okay, I can't tell you the details of how I'm going to get to the train station, but I know I'm going to have to be at the train station around 4 PM. Okay, so that's the high level. And we have early experiments with sort of various complicated neural net architectures which I'm not going to detail to train from video, try to predict basically what's going to happen in the video using warping and stuff like that and it works really well. But in the end, what we'll have is a hierarchical system from which we can do a hierarchical planning and then we'll have been trained to predict what's going to happen in the world as a consequence of actions or latent variables that we can observe, that we can infer. And those systems will be able to plan and reason and will be controllable because the behavior is entirely controlled by the cost functions we ask you to minimize. And so much more controllable than current LNMs and that's pretty much the end. So cell supervised learning is really the ticket. Handling and certainty can be done with this energy-based model method and using the joint embedding architecture that allows us to avoid predicting all the details that are irrelevant about the world. Learning world models from observation and interaction and then reasoning and planning is done by basically gradient-based minimization with respect to actions. And that's it, thank you very much. Thank you, John, for the great talk. Now we'll have the second part, which is the Fireside Chat between John and Osama, so please. Thank you very much, John. It was actually truly inspirational because it is definitely different than your typical machine learning talk, so I enjoyed that. Well, to you to throw away all the basic pillars of machine learning, so yes. So I've collected a bunch of questions, some coming from the audience, some coming from our institute and our faculty. And we'll try to go through them in 20 minutes or whatever we can cover. Normally, I would commit to answering every question on social media, but because we got 150 questions, I'm afraid to commit my time or yours to this at this point, but we'll try our best. So I'll start with my first question. It's been a long-standing wisdom in statistical inference and probabilistic reasoning that when the number of parameters of a model gets large enough, you kind of lose your ability to generalize and you start just memorizing data, and we all know that that's no good. That's just too detailed, the bias variance trade off. But somehow, deep learning seems to have broken through this barrier. When we went from regular neural nets to the deep nets, and is there an intuition or understanding today as to why this is working in LLMs with hundreds of billions and now trillions of parameters. Right, well, the fact that it is working, that you can train a ridiculously over-sized neural net, and it will still work reasonably and generalize is dumb-founding. So much that it contradicts every single thing that has been written in every statistical textbook. That you should never have more parameters than you have training samples, right? If you're fitting a polynomial or something like this. But we knew experimentally, even in the late 80s and early 90s, that you could make those neural nets pretty big. And even if you didn't have a huge amount of training data, it would still work pretty well. There was just no theoretical explanation. So the theorists told us, you're wrong, you're stupid. This cannot possibly work, so I'm not gonna believe your results. And that's in part what made it very difficult to get neural nets accepted in the late 90s to the 2000s. But it turns out there is a phenomenon that has since been named double descent, which is that if you increase the number of parameters in a model for a constant size training set, your training error, of course, is gonna go down, right, to zero, probably. But your test error is first gonna go down, go through a minimum, and then go up when you start having parameters, a number of parameters that is commensurate with the number of samples that you have. Okay, so that's when the model starts to be over parameterized, and it goes up. But here is the thing, if you keep going up, if you keep making the model more complex, the tester will go down again. It will go through a maximum and then go down again. That's called the double descent phenomenon, nowadays. And it will do this if you regularize the parameters somehow. You don't necessarily need to regularize explicitly because neural nets have some sort of implicit regularization in them. But you see this phenomenon, even works if you fit a polynomial, right? Fit a 10 degree polynomial with 11 data points. And your fit will be horrible, right? Because the polynomial has to go to every single point and it's gonna go wild in between. But if you increase the degree of the polynomial to something like 20 or 30, and you regularize the coefficient, your error goes down again, your test error goes down again, the fitted polynomial goes through every point. But it's less irregular than with just degree 10. So this existed all along, it's just that people didn't realize it was a thing, or at least people who were not practitioners of neural nets who had realized this was a thing. So do we have any explanation why this is a thing? So there's a lot of conjectures, there is some theoretical work. Some people claim it's about the dynamics of gradient descent. There is some sort of implicit self regularization in neural nets that occurs. Whereby the system kind of recruits just a number of virtual parameters that it needs somehow. Some say it's regularization due to stochastic gradient. So stochastic gradient descent, which is noisy. And so perhaps that forces the system to find robust minima in the objective, in the loss, that generalize better. It's not clear, there's a bunch of different things. Yeah, definitely one of the mysteries that keep us interested. This question comes from Raman Chandrasekharan or Chandra, who's one of our senior research scientists in Seattle. How long before LLM, and maybe, I don't know, models in general, can genuinely start saying, I don't know the answer to this question. As opposed to attempting to guess the right autocomplete anyway, because that's what it's programmed to do. Yeah, so I don't think current LLMs can really do this at the moment. I think it's probably possible with architectures, the type that I show. Because if there are no good minima to the objective that the system is attempting to minimize to produce it, it's output, it's gonna say, well, I found this thing, it seems to be minimizing this objective, but not very well. So probably it's not the right answer you were looking for. Or by the shape of the minimum, of this energy minimum, perhaps, you could say, like if it's really a sharp minimum, then that's the one answer that corresponds to the question. If it's kind of a shadow minimum, maybe there are multiple answers that are possible. So you might be able to attribute, to map energy levels to, of different answers to a confidence level. To, this is a question from me, I guess. Two aspects of critical importance to, let's say, GPT or large language models that are not talked about a lot by the companies who do them are data curation. Getting that clean data, that balanced data, that representative data, which by the way, counter to popular belief, open AI spent a lot of its money on curating just that right corpus so that they can do the training reliably. And the second part, which is something we're big believers in at the Institute for experiential AI, experiential AI stands for AI with the human in the loop. Having that human intervention through relevance feedback, that we know now open AI is doing and has been doing. And some of the queries are actually taken over by humans at some point when they make enough errors to come back. But the good thing is they learn from them and we think that's a great practice. Why do you think the companies don't want to talk about the importance of the data and the importance of the human in the loop? I don't know if they don't want to talk about it. I mean, it's clearly very expensive to create data to produce a good LLM. But in my opinion, it's doomed to failure in the long run for two reasons. The first one is the curation requires going through this enormous amount of data that you want to train the system on. And any data you eliminate, it's less training data for your model. But the second thing is even with human feedback, human feedback that rate different answers or fine tune the system for certain question and answers, sort of manually curated. If you want those systems ultimately to be the repository of all human knowledge, the dimension of that space of all human knowledge is enormous. And you're not going to do it by paying a few thousand people in Kenya or India rating answers. You're going to have to do it with millions of volunteers that find the system for all possible questions that might possibly be asked. And those volunteers will have to be vetted in the way Wikipedia is being done, right? So think of LLMs in the long run as a version of Wikipedia plus your favorite newspapers plus the scientific literature plus everything, but you can talk to it. You don't have to read articles, you can just talk to it. And so if it's supposed to become the repository of all human knowledge, the thing it's been trained to do will have to be curated by quite sourcing the way Wikipedia is to cover all the possible things that may be covered. This is a very strong argument for having open source based models for LLMs. So in my opinion, the future is inevitably going to be that you're going to have a small number of open source based LLMs that are not trained for any particular application, they're trained on enormous amounts of data that requires a lot of money. So you're not going to have 25 of them, you're going to have two or three. And then actual applications are going to be built on top of it by finding those systems for particular vertical applications. That's the future. Sadly, in the industry, there are people who are lobbying governments to actually make the open sourcing of large scale LLM illegal. What they're worried about is potential misuse of LLMs by bad actors, potential users. So some people in the US, for example, are worried, oh, if we open source our LLMs, you know China and North Korea and Iran will put their hands on it and that's going to be bad. And then some people are worried that the real powerful LLMs are going to be super intelligent and destroy humanity, which I think is preposterous, even though some of my friends that I respect actually believe this. So I think it would be really, really bad if those lobbying attempts succeed. I'm very much in favor of a future with open based models. And there's going to be bad actors, but there's going to be countermeasures against them. It's going to be, you know, or powerful good AI cop against their nefarious AI, essentially. So let's shift to this trend. And this is, I've merged a question from Jimmy Shanahan from our AI solutions hub with questions from Tomo Lasovic and Ken Church at EAI. The trend nowadays seems to be heading towards bigger is better, more compute, more parameters. There's been some studies even suggesting that by open AI themselves that they're moving at a pace faster than Moore's law, even though now they seem to be normalizing towards it, although Moore's law itself is slowing down. So the real question here is how long can this go on? And will we ask them, what do you think? I know that we may not have the final answer here, but it seems crazy. Like all you have to do is wait a few weeks and you hear about the next big model. Well, so actually in the last few months, you've seen a decrease in the size. So Lama, for example, the 13 billion version of Lama in terms of raw performance on standard benchmarks is actually better than GPT-3, which has 175 billion parameters. And so it's not clear that bigger is better. With the architecture I propose, I think you can get away with smaller systems that perform at least as well. The reason being that when you train in a current autoregressive LLM, you have to train it to not just accumulate knowledge, not just predict the next word, but also solve a lot of problems. So basically, know how to produce the right answer when you specify the question in the prompt. And so everything is wrapped into the weights of that single model. Whereas in the model I propose here, the architecture I propose, the word model is just a word model. The task is specified by the objective function, which may include the prompt. So it may include the representation of the prompt. And so you're separating different things. You're separating the inference procedure that produces the output from the word model, the sort of the mental model of the world that the system uses, from the task itself, which is specified by the objective. And you can probably get away with smaller networks for the same performance. But yes, I mean, there were a few years ago models by Google that had like a trillion parameters. There were basically multiple models that were stuck together with some sort of gating. Yeah, between them, they've kind of backpedaled on this a little bit. If you want the system to be practical, like to be used by everyone, you can't make them like a trillion parameters. Right now, it'd be just too expensive. So you have to minimize that size. Now you can run things like Lama 7 billion on a Mac. You know, you can run on a laptop. You can't train it on a laptop, but you can run it. Yes. So clearly, you believe you're advocating for a different view of what the machine learning and AI community should be doing as opposed to what they are doing today. That's the story of my career. Yes. And this question is coming from Ken Church. A former colleague from AT&T. From AT&T. He is at the Institute for AI in Silicon Valley. Do you believe? Well, I guess the question is, how long do you think it will take to pivot the field from where it is to where you would like it to be? Well, last time I tried, it took 15 years. If not more, actually, depending on how you count, it might have been 20. So I don't know. I think I see a phenomenon in kind of this is a sociology of science question. When there is something that seems to work, everybody gets excited about it. And it's a fashion trend type phenomenon where every paper written is about this trend. I saw this in computer vision back in the early to mid 2000. Everybody was working on boosting. That was the thing, you had to work on boosting for computer vision. And then someone in 2006 and 2005 came up with a different way of doing vision using dense features like sift and stuff like that using unsupervised running for a middle layer and then an SVM on top. All of a sudden, everybody was doing this. And then starting in 2013, everybody started using convolutional nets. That came from results. So now we are in a phase where everybody is focused on LLMs. And if you don't work on LLMs, nobody wants to talk to you. But it will change. So you think it's 15 years? No, I think it's more like five. Like I made that prediction that autoregressive LLMs will probably... Five years, that's true, yeah, they're doomed. Yeah, I mean, I might be wrong, obviously. We will hold you to that. I'll come back and revisit in five years. Maybe it's a wishful thinking, self-fulfilling prophecy perhaps. A question for something different here from Sam Scarpino, director of AI and Life Sciences at the Institute for Experiential AI. What are the biggest gaps on the education side for graduates of higher education in AI and in particular the new directions AI is taking? What do you think is missing? So I think what's missing... So it depends which major you're following. Most computer science curricula in the US are very weak in mathematics. The requirements for mathematics in a typical CS degree, the minimum requirement is very, very small, right? It's one course in discrete math and perhaps in algebra if you're lucky. Maybe a probability if you are courageous. But what about optimization? That would be something that would be very useful. And then there is courses in physics because the mathematics of inference and variational autoencoder and stuff like that, graphical models, etc. The mathematics of this is from statistical physics. And so if you have a choice between taking your course in, I don't know, mobile app programming or quantum mechanics, take quantum mechanics. I'm not kidding. This is a question that came from the audience and a few of the people at the Institute. Your thoughts on the current, you know, these recent congressional hearings where certainly seems like much of the testimony by some Altman was understandably self-serving. You know, they need to be allowed to compete and have their way of working protected. At the same time, he's encouraging the rest of the community to be open source. What would you have said to Congress? Have you been on those hearings? I was not invited. I was not invited to the White House either before that. So what I would have recommended is that if you want a vibrant ecosystem on top of current AI technology, you need to have sort of open source based models on top of which an industry can be built. And that industry will build vertical applications for particular domains on top of a base model. You don't want to have 25 companies selling 25 different base models and keep them closed source. If you want an industry to be built on top of it, the infrastructure has to be open. Because that's the only way to really sort of know what you're doing, essentially. And to have some control about your future, right? You can't just go like this and pray that. Unix versus Windows. Right, so if you go back to the history of the Internet, there was a similar story where back in 1992 when the built Internet Al Gore started to figure out like what, you know, how do we build the information superhighway? They went to see, you know, the big communication companies like AT&T and AT&T told them, oh, you know, leave it to us. We'll build the stuff. It's going to be, you know, ATM and ISD enter the home and blah, blah, blah. It'll be wonderful and you'll have to pay, you know, $5 per hour. And Al Gore said no. He said we're going to make the, what was an ARPANET that became the Internet, basically available to the public and delocalized and, you know, self, basically open in terms of standard and no company is going to control it. And that was a really, really good idea. We can thank Al Gore for this. The world can thank Al Gore, not just the U.S. He did invent the Internet. And then a similar story happened several years later when people started to realize that you could use, you know, graphic browsers like Mosaic and Netscape and stuff like that, right, when the World Wide Web became popular. So there was a war between Sun Microsystems and Microsoft. Sun Microsystems said, oh, we're going to sell you servers running Solaris, the version of Unix, with, you know, our web server infrastructure and Java. And you're going to be able to build, like, anything you want. Microsoft said no, it's going to be Windows NT with the IIT web server and the ASP website, you know, server-side protocol framework, whatever. They both lost. Sun Microsystem went bankrupt, was sold for parts to Oracle, and Microsoft essentially exited the market. One was Linux and Apache, open source. And the reason is because it's such an essential basic infrastructure that it has to be open. It progresses faster if it's open, and it's more reliable, it's more secure. I mean, there's all the advantages. And, you know, it's easier for startups to build on top of it. So in the future, we're going to see AI systems as basic infrastructure. All of our interactions ten years from now with the digital world will be through an intelligent virtual agent that will be with us all the time. It's like every one of us will have a staff of intelligent people working for us. Okay? We shouldn't be threatened by the fact that those things will be smarter than us. Like everybody that, you know, is working with me at fair is smarter than me. So I don't feel threatened by that. You're not a very good manager if you're threatened by people who are smarter than you. So your purpose actually should be to hire people, only people who are smarter than you. But anyway, so we're going to have those intelligent systems that are going to be under control that are going to help us, you know, daily lives. And we need those systems to be open because if it's kind of a closed system controlled by some company in California, it's going to be able to control our entire knowledge and data diet. And that's just too dangerous. And it's not necessary. It's necessary for a search engine or a social network because it has to be centralized for various reasons. But for an agent like this, it could run on your local device. It could run on your laptop. You don't have to talk necessarily with big servers in California. You don't want to give all your, you know, deepest secrets to that. So it's going to have to be an open fact form for that reason. If nothing else, governments around the world are going to insist that this is the case. So that's why I would tell Congress, make it so that, like, don't ban open source LLMs. They're not going to destroy humanity. Yeah, they're going to be bad actors, but you know, you can have countermeasures and make it open. It's the only way to make it safe. I'll ask, we'll make this a quick question with a quick answer. And then I know we have some questions live, so we'll switch to those. In a way, you kind of answered this question when you said LLMs are doomed. But if LLMs were to become perfect, at least in language, would that ever give us insight into how language and natural language understanding works? The language model today is distributed over these billions of parameters. And do you think we'll ever have an understandable LLM, like, for example, we use PCA to understand what regression is doing? Or is that hopeless? At some point, I think it's going to be right to be hopeless. I mean, we'll probably learn a lot about, you know, how the systems represent data and, like, how they manipulate it and stuff like that. So this is not opaque, right? We can completely kind of, there's complete visibility on how the systems operate. Now, the question is understanding really how the decisions are being made. So I'm actually not particularly interested in those questions, like, you know, as long as they work properly. The same way, I'm not particularly interested in figuring out exactly how the brain works. I'm more interested in figuring out how the brain builds itself so that it works, right? So I'm more interested in learning than in studying the result of learning, if you want. So it's the same for those systems. I'm more interested in how you get them to learn what you want and how to solve the problem in the end is kind of considerably less interesting, in my opinion. But at some point, they're going to be, you know, super intelligent, repulsory of all human knowledge. You know, it's going to be too big for us to kind of comprehend at a deep level. Fair. And by the way, I failed to acknowledge that question came from Walid Saba, who's one of our senior research scientists at the EAI up in Portland, Maine. The next question I'll use, and then I'll switch over to audience questions, comes from Gene Tunic, the director of AI Plus Health at the Institute for Experiential AI. You believe that deep learning can eventually lead to human-like understanding, and you have said that self-supervised learning from unlabeled data can be a powerful tool, although it seems like in human learning, as I was watching your examples, for example, a lot of that data is, in a way, supervised or tied to some kind of reinforcement feedback around what to expect, is it good, is it bad, etc. So how, where do you draw that line between, you know, can we really truly go towards unsupervised, or there's a huge dependence on supervised and on those labels to get it right? Because the world is, in a way, is telling us indirectly through supervision. So self-supervised learning, I mean, the reason it's called self-supervised is that deep down it's actually supervised learning. It's just supervised learning where the supervision signal is the input itself, right? So in a way, that's kind of, you know, a kind of simple answer to that question. It's still supervised learning in the end, but with particular architectures to handle uncertainty and dimensionality and things like that. Regarding reinforcement learning, there is a point at which you need some form of reinforcement learning, and you need it in two situations, or at least techniques that have been developed in the context of reinforcement learning. The first situation is if the objective function that is optimized by your system does not reflect the ultimate objective function, you actually want to optimize. So for example, you're learning to ride a bike, your objective function is the, you know, time to the next fall or something, or the inverse time to next fall, you want to minimize that, right? But you don't know how to compute this from the internal state of your system. And so you need to train an objective function to approximate this real cost, which in the context of reinforcement learning is called a critic. So that's when you need one of those things. The other situation where you need it is when your world model is not accurate because it's not been trained in all corners of the state space, and you happen to be in a part of the state space that it wasn't trained on. Your world model is going to be bad, and your predictions are going to be bad, your planning is going to be bad. So to prevent this, you need to train your world model using things that are called curiosity or exploration. And that's another concept that comes from reinforcement learning. So don't completely abandon reinforcement learning, but minimize its use. As we switch over to the live questions, let me, I can't help but ask you this question. It comes from several anonymous people as well as Ken Church, your former colleague. Did you actually say the revolution will not be supervised? I did, yeah. Okay. But actually, I stole it from Adi Asha Efros from Berkeley. He had a magnificent slide that was a picture of a wall painting in Chile someplace, which was one of those kind of revolutionary thing. And I took that picture and overlaid on it. The revolution will not be supervised. Yes. Okay. So I stole that from him. I deserve no credit. Shall we switch over to a question from the audience? Yeah. So first question from Glenn Jenkinson is, what two questions about AI do you wish you would ask more often? Two questions. I don't know. I get asked a lot of questions. I can't imagine a question have not been asked. That's relevant. I mean, I think the important questions are the ones that I'm asking myself. And I wish other people would sort of frame the problems in the same way. So big question. How is it that any teenager can learn to drive a car in 20 hours? And we still don't have level five autonomous driving? That was the first question. So second question is, what are we missing? That's the answer I want. Joe. Next question from Juan Leylanda. Do you think quantum computing will have a significant role in the future of AI? No. Or at least not any time soon. But the time this happens, I probably won't be alive anymore. So I'm not taking a big risk. No, I don't think so. I mean, there's precious few situations today where quantum computing could be useful. There's no situation where it actually is useful because the quantum computers are not big enough at the moment. So it's a huge bet. I think scientifically it's fascinating. I'm really fascinated by quantum computing at the conceptual level. I have one or two papers with Seth Lloyd on connections between neural nets and quantum computing. I think it's a very interesting topic, but I don't think it has any practical value in the short term. Joe. One last question from Anton Dabura. To what extent do you see ML models being used for problems that we already have pretty good algorithms to solve, such as sorting shortest path, linear integer programming, and so on? How would you characterize the boundary, if any? So there's a lot of problems that we can currently solve that are NP-complete or NP-hard, and so we can solve them within limits. What we need very often are approximate algorithms, so methods that give us approximate solutions to complex problems that, in theory, are NP-hard, NP-complete, whatever, but if you reduce yourself to accepting approximate solutions, might become solvable. So I think there is a lot to be said for ML methods that do something that has become to be known as amortized inference. So amortized inference is this idea that you might have a problem that is formulated as an optimization problem. Every computing problem can be formulated as an optimization problem. And what you might be able to do is solve that problem in certain cases, give a solution, and now what you do with this is that you train in your net of some kind to predict, to approximate the solution to that optimization problem from the specification of the problem, from the inputs. So that system will not be able to completely solve the problem in those situations, but for the type of problem that you train it on, it's going to be able to give you an approximate solution really quickly. Amortized inference. There is a tutorial on this that was written and given at a recent conference by one of my colleagues at fair called Brendan Amos, AMOS, very interesting concept. I will close my questions with one last question, then we'll take a real live one and call it the end. I have to use this. It comes from one of our faculty who wanted to remain anonymous, I don't know why, but given the big excitement around LLMs and not without a reason, what are some of the research directions that are possible to tackle for non-Google slash Facebook type sized institutions that are under studies? Space for foundational research, big open questions in need of creative solutions. Thus, if you were a young investigator today, like a starting assistant professor, what would you do in this environment? I mean, that's a problem I have to face when I have PhD students at NYU that don't have access to 16,000 GPUs, unlike people at fair. So I think a lot of most good ideas still come from academia, so you're not going to be Google or Meta or Microsoft on beating the record on translation or something like that. You don't want to do this in universities. But coming up with new ideas, for example, the problem I mentioned of how do you do hierarchical planning? How do you train a system to figure out how to represent the world and action spaces so that you can do hierarchical planning? It's completely unsolved. You can do this with toy problems. If you have any idea of how you might approach that problem on toy problems, you don't have to have tons of GPUs for that. You will have an idea that might have a huge impact. So if you have a good architecture that you can show, can learn some simple world model from video, it's the same. You don't have to train on all of YouTube. You can train on artificial environments and stuff like that and demonstrate that it works. It doesn't have to be large scale. So this is the kind of stuff you want to do. And then there is a new domain which is building on top of open source base models. So unfortunately, right now, the best base models, LLMs are the LAMA class of models from $7 billion to $65 billion. They're not usable for commercial use. They are distributed with a license for non-commercial use, only for research, which you can of course use in the university. And there's a lot of work to be done to figure out how to make those things safe, factual, etc. And you can work from those base models. You don't have to retrain them from scratch. So you don't need to have roomful, rooms full of GPUs. We'll try for one last question, maybe two. Go ahead, please, with your question. Hi. So my question really dwells from the side of, or we'd love to hear your thoughts, on impact and control of these large language models or any of these models, the fancy models that you showed with billions of trillions of parameters. So the impact side is, do you really give or how much thought do you give to the impact that would have on the community or on the people in general, based on what that model does? And control is, once that model is out there, how do I make sure that it doesn't do a certain things it's not supposed to do with regular, the way people used to use internet before those models. It used to be very controlled environment where you could have, in a way, regulate those environments. But now with models, it's getting increasingly difficult and a slow process to have or do not have certain things in those models. Okay. So there is a long view, a very positive one, which is imagine that all of us have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff of people working for us, but like super people working for us. This is going to create a new renaissance for humanity. It's going to increase humanity's intelligence, however you want to measure it. That has to be intrinsically good. It's been the case in the past that anytime a new medium was invented or a new way of communication was invented, like the printing press. Humanity kind of went to the next step. The printing press let the dissemination of philosophy, science, secularism, democracy, all that stuff. The US would not exist without the French philosophers of the 18th century. So neither would the French revolution. So I think same for the internet, that gave people instant access to an enormous wealth of knowledge. Also disinformation, but okay, I mean, we have to have countermeasures for every technology can be used for good and bad. We need to have countermeasures for the worst aspects. But ultimately, I think we need widest possible access to those AI systems by everyone. Now, how do we make sure those systems don't lie to us? How do we make sure that the information they give us is not under the control of someone that has nefarious purpose, you know, things like that, which is I think a good reason for them to be open as I stated earlier. But I think it's a bright future for humanity, you know, contrary to some people who tell young people don't expect to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the next question, but we are five minutes over our time limit. And I know we have to grab a bite and deliver you to the train station on time, according to the hierarchical plan. So with that, please join me in thanking Jan for an amazing session today. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.96, "text": " Okay, great, welcome everyone and thank you for joining us.", "tokens": [50364, 1033, 11, 869, 11, 2928, 1518, 293, 1309, 291, 337, 5549, 505, 13, 51162], "temperature": 0.0, "avg_logprob": -0.28217690451103344, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.09251191467046738}, {"id": 1, "seek": 0, "start": 15.96, "end": 22.080000000000002, "text": " This is the last distinguished lecture series for the Institute of Expansion AI for the", "tokens": [51162, 639, 307, 264, 1036, 21702, 7991, 2638, 337, 264, 9446, 295, 21391, 599, 313, 7318, 337, 264, 51468], "temperature": 0.0, "avg_logprob": -0.28217690451103344, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.09251191467046738}, {"id": 2, "seek": 0, "start": 22.080000000000002, "end": 23.080000000000002, "text": " academic year.", "tokens": [51468, 7778, 1064, 13, 51518], "temperature": 0.0, "avg_logprob": -0.28217690451103344, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.09251191467046738}, {"id": 3, "seek": 0, "start": 23.080000000000002, "end": 27.36, "text": " We'll resume again in September with a full program for the year.", "tokens": [51518, 492, 603, 15358, 797, 294, 7216, 365, 257, 1577, 1461, 337, 264, 1064, 13, 51732], "temperature": 0.0, "avg_logprob": -0.28217690451103344, "compression_ratio": 1.3734939759036144, "no_speech_prob": 0.09251191467046738}, {"id": 4, "seek": 2736, "start": 27.36, "end": 35.28, "text": " As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series,", "tokens": [50364, 1018, 291, 611, 458, 11, 294, 8952, 633, 732, 3259, 321, 1190, 264, 48603, 2451, 294, 21391, 599, 313, 7318, 2638, 11, 50760], "temperature": 0.0, "avg_logprob": -0.1851656495071039, "compression_ratio": 1.4460093896713615, "no_speech_prob": 0.0039045598823577166}, {"id": 5, "seek": 2736, "start": 35.28, "end": 41.519999999999996, "text": " which is designed to feature a lot of our Northeastern University experts and faculty", "tokens": [50760, 597, 307, 4761, 281, 4111, 257, 688, 295, 527, 4067, 68, 32579, 3535, 8572, 293, 6389, 51072], "temperature": 0.0, "avg_logprob": -0.1851656495071039, "compression_ratio": 1.4460093896713615, "no_speech_prob": 0.0039045598823577166}, {"id": 6, "seek": 2736, "start": 41.519999999999996, "end": 43.56, "text": " and so forth.", "tokens": [51072, 293, 370, 5220, 13, 51174], "temperature": 0.0, "avg_logprob": -0.1851656495071039, "compression_ratio": 1.4460093896713615, "no_speech_prob": 0.0039045598823577166}, {"id": 7, "seek": 2736, "start": 43.56, "end": 50.92, "text": " In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic", "tokens": [51174, 682, 732, 3259, 11, 2138, 3917, 505, 337, 257, 751, 538, 6943, 28226, 2012, 347, 322, 257, 1687, 1880, 4829, 51542], "temperature": 0.0, "avg_logprob": -0.1851656495071039, "compression_ratio": 1.4460093896713615, "no_speech_prob": 0.0039045598823577166}, {"id": 8, "seek": 2736, "start": 50.92, "end": 52.6, "text": " who's at the Curie College.", "tokens": [51542, 567, 311, 412, 264, 7907, 414, 6745, 13, 51626], "temperature": 0.0, "avg_logprob": -0.1851656495071039, "compression_ratio": 1.4460093896713615, "no_speech_prob": 0.0039045598823577166}, {"id": 9, "seek": 5260, "start": 52.6, "end": 56.96, "text": " My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential", "tokens": [50364, 1222, 1315, 307, 8875, 2404, 48889, 345, 11, 286, 478, 264, 20658, 7680, 295, 264, 9446, 337, 12522, 1196, 831, 50582], "temperature": 0.0, "avg_logprob": -0.23063324309967376, "compression_ratio": 1.476595744680851, "no_speech_prob": 0.007524552289396524}, {"id": 10, "seek": 5260, "start": 56.96, "end": 62.88, "text": " AI and also Professor of the Practice in the Curie College for Computer Sciences and it", "tokens": [50582, 7318, 293, 611, 8419, 295, 264, 27904, 294, 264, 7907, 414, 6745, 337, 22289, 21108, 293, 309, 50878], "temperature": 0.0, "avg_logprob": -0.23063324309967376, "compression_ratio": 1.476595744680851, "no_speech_prob": 0.007524552289396524}, {"id": 11, "seek": 5260, "start": 62.88, "end": 68.88, "text": " is my pleasure today to introduce Yan Lokhan.", "tokens": [50878, 307, 452, 6834, 965, 281, 5366, 13633, 46278, 3451, 13, 51178], "temperature": 0.0, "avg_logprob": -0.23063324309967376, "compression_ratio": 1.476595744680851, "no_speech_prob": 0.007524552289396524}, {"id": 12, "seek": 5260, "start": 68.88, "end": 73.48, "text": " Yan is a very well-known name in the field.", "tokens": [51178, 13633, 307, 257, 588, 731, 12, 6861, 1315, 294, 264, 2519, 13, 51408], "temperature": 0.0, "avg_logprob": -0.23063324309967376, "compression_ratio": 1.476595744680851, "no_speech_prob": 0.007524552289396524}, {"id": 13, "seek": 5260, "start": 73.48, "end": 80.0, "text": " I've known him for many years, I think at one point in my life I interviewed at Bell", "tokens": [51408, 286, 600, 2570, 796, 337, 867, 924, 11, 286, 519, 412, 472, 935, 294, 452, 993, 286, 19770, 412, 11485, 51734], "temperature": 0.0, "avg_logprob": -0.23063324309967376, "compression_ratio": 1.476595744680851, "no_speech_prob": 0.007524552289396524}, {"id": 14, "seek": 8000, "start": 80.0, "end": 84.88, "text": " Labs or AT&T Labs and that's when he was there.", "tokens": [50364, 40047, 420, 8872, 5, 51, 40047, 293, 300, 311, 562, 415, 390, 456, 13, 50608], "temperature": 0.0, "avg_logprob": -0.2515703426894321, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.01588256284594536}, {"id": 15, "seek": 8000, "start": 84.88, "end": 92.76, "text": " He is VP and Chief AI Scientist at Meta, also known as Facebook, and Silver Professor at", "tokens": [50608, 634, 307, 35812, 293, 10068, 7318, 18944, 468, 412, 6377, 64, 11, 611, 2570, 382, 4384, 11, 293, 15861, 8419, 412, 51002], "temperature": 0.0, "avg_logprob": -0.2515703426894321, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.01588256284594536}, {"id": 16, "seek": 8000, "start": 92.76, "end": 98.03999999999999, "text": " NYU affiliated with the Kauan Institute of Mathematical Sciences and the Center for Data", "tokens": [51002, 42682, 42174, 365, 264, 591, 1459, 282, 9446, 295, 15776, 8615, 804, 21108, 293, 264, 5169, 337, 11888, 51266], "temperature": 0.0, "avg_logprob": -0.2515703426894321, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.01588256284594536}, {"id": 17, "seek": 8000, "start": 98.03999999999999, "end": 101.88, "text": " Science, which he actually founded.", "tokens": [51266, 8976, 11, 597, 415, 767, 13234, 13, 51458], "temperature": 0.0, "avg_logprob": -0.2515703426894321, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.01588256284594536}, {"id": 18, "seek": 8000, "start": 101.88, "end": 108.64, "text": " He was the founding director of FAIR, I learned this morning that FAIR used to stand for Facebook", "tokens": [51458, 634, 390, 264, 22223, 5391, 295, 19894, 7740, 11, 286, 3264, 341, 2446, 300, 19894, 7740, 1143, 281, 1463, 337, 4384, 51796], "temperature": 0.0, "avg_logprob": -0.2515703426894321, "compression_ratio": 1.521186440677966, "no_speech_prob": 0.01588256284594536}, {"id": 19, "seek": 10864, "start": 108.64, "end": 117.52, "text": " AI Research, now it's changed to MetaFair for Fundamental AI Research and of course", "tokens": [50364, 7318, 10303, 11, 586, 309, 311, 3105, 281, 6377, 64, 37, 1246, 337, 13493, 44538, 7318, 10303, 293, 295, 1164, 50808], "temperature": 0.0, "avg_logprob": -0.27237553066677517, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.002934807213023305}, {"id": 20, "seek": 10864, "start": 117.52, "end": 124.08, "text": " he founded the NYU Center for Data Science, received an engineering diploma from SEA in", "tokens": [50808, 415, 13234, 264, 42682, 5169, 337, 11888, 8976, 11, 4613, 364, 7043, 35770, 490, 10269, 32, 294, 51136], "temperature": 0.0, "avg_logprob": -0.27237553066677517, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.002934807213023305}, {"id": 21, "seek": 10864, "start": 124.08, "end": 129.48, "text": " Paris and a PhD from the Saoubon University.", "tokens": [51136, 8380, 293, 257, 14476, 490, 264, 6299, 263, 4351, 3535, 13, 51406], "temperature": 0.0, "avg_logprob": -0.27237553066677517, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.002934807213023305}, {"id": 22, "seek": 10864, "start": 129.48, "end": 138.56, "text": " After a postdoc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs", "tokens": [51406, 2381, 257, 2183, 39966, 294, 14140, 11, 415, 6869, 8872, 5, 51, 11485, 40047, 11, 597, 658, 40949, 281, 8872, 5, 51, 40047, 51860], "temperature": 0.0, "avg_logprob": -0.27237553066677517, "compression_ratio": 1.419811320754717, "no_speech_prob": 0.002934807213023305}, {"id": 23, "seek": 13856, "start": 138.56, "end": 143.52, "text": " in 1996 as Head of Image Processing Research.", "tokens": [50364, 294, 22690, 382, 11398, 295, 29903, 31093, 278, 10303, 13, 50612], "temperature": 0.0, "avg_logprob": -0.20145914554595948, "compression_ratio": 1.3952380952380952, "no_speech_prob": 0.007951246574521065}, {"id": 24, "seek": 13856, "start": 143.52, "end": 151.32, "text": " He joined NYU as Professor in 2003 and Meta or Facebook in 2013.", "tokens": [50612, 634, 6869, 42682, 382, 8419, 294, 16416, 293, 6377, 64, 420, 4384, 294, 9012, 13, 51002], "temperature": 0.0, "avg_logprob": -0.20145914554595948, "compression_ratio": 1.3952380952380952, "no_speech_prob": 0.007951246574521065}, {"id": 25, "seek": 13856, "start": 151.32, "end": 158.0, "text": " He is the recipient of the 2018 ACM Touring Award, along with Jeffrey Hinton and Yashua", "tokens": [51002, 634, 307, 264, 26216, 295, 264, 6096, 8157, 44, 13077, 278, 13894, 11, 2051, 365, 28721, 389, 12442, 293, 398, 1299, 4398, 51336], "temperature": 0.0, "avg_logprob": -0.20145914554595948, "compression_ratio": 1.3952380952380952, "no_speech_prob": 0.007951246574521065}, {"id": 26, "seek": 13856, "start": 158.0, "end": 162.48000000000002, "text": " Benjio, and for those of you who don't know the Touring Award, it's essentially the equivalent", "tokens": [51336, 3964, 73, 1004, 11, 293, 337, 729, 295, 291, 567, 500, 380, 458, 264, 13077, 278, 13894, 11, 309, 311, 4476, 264, 10344, 51560], "temperature": 0.0, "avg_logprob": -0.20145914554595948, "compression_ratio": 1.3952380952380952, "no_speech_prob": 0.007951246574521065}, {"id": 27, "seek": 16248, "start": 162.48, "end": 169.51999999999998, "text": " of the Nobel Prize for Computer Science, the toughest award to get from the ACM.", "tokens": [50364, 295, 264, 24611, 22604, 337, 22289, 8976, 11, 264, 35037, 7130, 281, 483, 490, 264, 8157, 44, 13, 50716], "temperature": 0.0, "avg_logprob": -0.12008157041337755, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.01718735322356224}, {"id": 28, "seek": 16248, "start": 169.51999999999998, "end": 174.67999999999998, "text": " The award was for conceptual and engineering breakthroughs that have made deep neural networks", "tokens": [50716, 440, 7130, 390, 337, 24106, 293, 7043, 22397, 82, 300, 362, 1027, 2452, 18161, 9590, 50974], "temperature": 0.0, "avg_logprob": -0.12008157041337755, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.01718735322356224}, {"id": 29, "seek": 16248, "start": 174.67999999999998, "end": 177.23999999999998, "text": " a critical component of computing.", "tokens": [50974, 257, 4924, 6542, 295, 15866, 13, 51102], "temperature": 0.0, "avg_logprob": -0.12008157041337755, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.01718735322356224}, {"id": 30, "seek": 16248, "start": 177.23999999999998, "end": 184.2, "text": " He is a member of the National Academy of Sciences and the National Academy of Engineering,", "tokens": [51102, 634, 307, 257, 4006, 295, 264, 4862, 11735, 295, 21108, 293, 264, 4862, 11735, 295, 16215, 11, 51450], "temperature": 0.0, "avg_logprob": -0.12008157041337755, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.01718735322356224}, {"id": 31, "seek": 16248, "start": 184.2, "end": 187.51999999999998, "text": " amongst many others.", "tokens": [51450, 12918, 867, 2357, 13, 51616], "temperature": 0.0, "avg_logprob": -0.12008157041337755, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.01718735322356224}, {"id": 32, "seek": 18752, "start": 187.52, "end": 193.8, "text": " His interests include AI, machine learning, computer perception, robotics and computational", "tokens": [50364, 2812, 8847, 4090, 7318, 11, 3479, 2539, 11, 3820, 12860, 11, 34145, 293, 28270, 50678], "temperature": 0.0, "avg_logprob": -0.157037274221356, "compression_ratio": 1.552, "no_speech_prob": 0.31814220547676086}, {"id": 33, "seek": 18752, "start": 193.8, "end": 200.76000000000002, "text": " neuroscience, and I'm sure you're all eager to hear from Jan on what's been happening", "tokens": [50678, 42762, 11, 293, 286, 478, 988, 291, 434, 439, 18259, 281, 1568, 490, 4956, 322, 437, 311, 668, 2737, 51026], "temperature": 0.0, "avg_logprob": -0.157037274221356, "compression_ratio": 1.552, "no_speech_prob": 0.31814220547676086}, {"id": 34, "seek": 18752, "start": 200.76000000000002, "end": 207.36, "text": " with generative AI and what all the buzz is about, hopefully we'll get into the technical", "tokens": [51026, 365, 1337, 1166, 7318, 293, 437, 439, 264, 13036, 307, 466, 11, 4696, 321, 603, 483, 666, 264, 6191, 51356], "temperature": 0.0, "avg_logprob": -0.157037274221356, "compression_ratio": 1.552, "no_speech_prob": 0.31814220547676086}, {"id": 35, "seek": 18752, "start": 207.36, "end": 212.76000000000002, "text": " details and immediately following his talk, we will do a fireside chat where I will try", "tokens": [51356, 4365, 293, 4258, 3480, 702, 751, 11, 321, 486, 360, 257, 15044, 482, 5081, 689, 286, 486, 853, 51626], "temperature": 0.0, "avg_logprob": -0.157037274221356, "compression_ratio": 1.552, "no_speech_prob": 0.31814220547676086}, {"id": 36, "seek": 18752, "start": 212.76000000000002, "end": 216.0, "text": " to ask him some tough questions.", "tokens": [51626, 281, 1029, 796, 512, 4930, 1651, 13, 51788], "temperature": 0.0, "avg_logprob": -0.157037274221356, "compression_ratio": 1.552, "no_speech_prob": 0.31814220547676086}, {"id": 37, "seek": 21600, "start": 216.0, "end": 218.96, "text": " And then we will also get questions from the audience.", "tokens": [50364, 400, 550, 321, 486, 611, 483, 1651, 490, 264, 4034, 13, 50512], "temperature": 0.0, "avg_logprob": -0.16860489618210567, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.06318899989128113}, {"id": 38, "seek": 21600, "start": 218.96, "end": 222.8, "text": " By the way, we did get online questions from the audience.", "tokens": [50512, 3146, 264, 636, 11, 321, 630, 483, 2950, 1651, 490, 264, 4034, 13, 50704], "temperature": 0.0, "avg_logprob": -0.16860489618210567, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.06318899989128113}, {"id": 39, "seek": 21600, "start": 222.8, "end": 227.68, "text": " We got 150 questions, so there's no way we're going to walk you through all of those.", "tokens": [50704, 492, 658, 8451, 1651, 11, 370, 456, 311, 572, 636, 321, 434, 516, 281, 1792, 291, 807, 439, 295, 729, 13, 50948], "temperature": 0.0, "avg_logprob": -0.16860489618210567, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.06318899989128113}, {"id": 40, "seek": 21600, "start": 227.68, "end": 230.52, "text": " So we'll see how much time allows us to answer.", "tokens": [50948, 407, 321, 603, 536, 577, 709, 565, 4045, 505, 281, 1867, 13, 51090], "temperature": 0.0, "avg_logprob": -0.16860489618210567, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.06318899989128113}, {"id": 41, "seek": 21600, "start": 230.52, "end": 242.52, "text": " Thank you and please join me in welcoming Jan to Northeastern University.", "tokens": [51090, 1044, 291, 293, 1767, 3917, 385, 294, 17378, 4956, 281, 4067, 68, 32579, 3535, 13, 51690], "temperature": 0.0, "avg_logprob": -0.16860489618210567, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.06318899989128113}, {"id": 42, "seek": 24252, "start": 242.52, "end": 243.52, "text": " Thank you, Samap.", "tokens": [50364, 1044, 291, 11, 4832, 569, 13, 50414], "temperature": 0.0, "avg_logprob": -0.16406051234195107, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.14322367310523987}, {"id": 43, "seek": 24252, "start": 243.52, "end": 251.92000000000002, "text": " A real pleasure to be here and thanks for coming here so numerous or for listening in online.", "tokens": [50414, 316, 957, 6834, 281, 312, 510, 293, 3231, 337, 1348, 510, 370, 12546, 420, 337, 4764, 294, 2950, 13, 50834], "temperature": 0.0, "avg_logprob": -0.16406051234195107, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.14322367310523987}, {"id": 44, "seek": 24252, "start": 251.92000000000002, "end": 256.40000000000003, "text": " So I'm going to talk a bit about the state of the art in AI but also about the next step", "tokens": [50834, 407, 286, 478, 516, 281, 751, 257, 857, 466, 264, 1785, 295, 264, 1523, 294, 7318, 457, 611, 466, 264, 958, 1823, 51058], "temperature": 0.0, "avg_logprob": -0.16406051234195107, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.14322367310523987}, {"id": 45, "seek": 24252, "start": 256.40000000000003, "end": 264.52, "text": " because I'm always interested in the next step and how we can make machines more intelligent.", "tokens": [51058, 570, 286, 478, 1009, 3102, 294, 264, 958, 1823, 293, 577, 321, 393, 652, 8379, 544, 13232, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16406051234195107, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.14322367310523987}, {"id": 46, "seek": 24252, "start": 264.52, "end": 271.52, "text": " And we need to figure out how to get machines that cannot just learn but also can reason", "tokens": [51464, 400, 321, 643, 281, 2573, 484, 577, 281, 483, 8379, 300, 2644, 445, 1466, 457, 611, 393, 1778, 51814], "temperature": 0.0, "avg_logprob": -0.16406051234195107, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.14322367310523987}, {"id": 47, "seek": 27152, "start": 271.52, "end": 277.4, "text": " and plan and current AI really does not allow current systems to do this.", "tokens": [50364, 293, 1393, 293, 2190, 7318, 534, 775, 406, 2089, 2190, 3652, 281, 360, 341, 13, 50658], "temperature": 0.0, "avg_logprob": -0.17713143198113693, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.03535424917936325}, {"id": 48, "seek": 27152, "start": 277.4, "end": 283.4, "text": " So I'll try to kind of sketch a potential pathway towards such systems.", "tokens": [50658, 407, 286, 603, 853, 281, 733, 295, 12325, 257, 3995, 18590, 3030, 1270, 3652, 13, 50958], "temperature": 0.0, "avg_logprob": -0.17713143198113693, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.03535424917936325}, {"id": 49, "seek": 27152, "start": 283.4, "end": 290.08, "text": " I can't say that we built it completely but we built some components and I go through this.", "tokens": [50958, 286, 393, 380, 584, 300, 321, 3094, 309, 2584, 457, 321, 3094, 512, 6677, 293, 286, 352, 807, 341, 13, 51292], "temperature": 0.0, "avg_logprob": -0.17713143198113693, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.03535424917936325}, {"id": 50, "seek": 27152, "start": 290.08, "end": 296.08, "text": " So AI is in the news, everybody is playing with it at the moment.", "tokens": [51292, 407, 7318, 307, 294, 264, 2583, 11, 2201, 307, 2433, 365, 309, 412, 264, 1623, 13, 51592], "temperature": 0.0, "avg_logprob": -0.17713143198113693, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.03535424917936325}, {"id": 51, "seek": 27152, "start": 296.08, "end": 298.44, "text": " It's pretty amazing how it works.", "tokens": [51592, 467, 311, 1238, 2243, 577, 309, 1985, 13, 51710], "temperature": 0.0, "avg_logprob": -0.17713143198113693, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.03535424917936325}, {"id": 52, "seek": 27152, "start": 298.44, "end": 299.76, "text": " There's a lot of success.", "tokens": [51710, 821, 311, 257, 688, 295, 2245, 13, 51776], "temperature": 0.0, "avg_logprob": -0.17713143198113693, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.03535424917936325}, {"id": 53, "seek": 29976, "start": 299.76, "end": 303.71999999999997, "text": " It's been very widely deployed very much in many applications that are behind the curtain", "tokens": [50364, 467, 311, 668, 588, 13371, 17826, 588, 709, 294, 867, 5821, 300, 366, 2261, 264, 26789, 50562], "temperature": 0.0, "avg_logprob": -0.1876086969485228, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.005900254938751459}, {"id": 54, "seek": 29976, "start": 303.71999999999997, "end": 306.2, "text": " but in some of them much more visible.", "tokens": [50562, 457, 294, 512, 295, 552, 709, 544, 8974, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1876086969485228, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.005900254938751459}, {"id": 55, "seek": 29976, "start": 306.2, "end": 312.76, "text": " So LLMs have the advantage of being visible but for the last 10 years or so there's massive", "tokens": [50686, 407, 441, 43, 26386, 362, 264, 5002, 295, 885, 8974, 457, 337, 264, 1036, 1266, 924, 420, 370, 456, 311, 5994, 51014], "temperature": 0.0, "avg_logprob": -0.1876086969485228, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.005900254938751459}, {"id": 56, "seek": 29976, "start": 312.76, "end": 320.08, "text": " use of AI and the latest development of AI for such thing as ranking for search engine", "tokens": [51014, 764, 295, 7318, 293, 264, 6792, 3250, 295, 7318, 337, 1270, 551, 382, 17833, 337, 3164, 2848, 51380], "temperature": 0.0, "avg_logprob": -0.1876086969485228, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.005900254938751459}, {"id": 57, "seek": 29976, "start": 320.08, "end": 325.88, "text": " and social networks or for content moderation, things like that.", "tokens": [51380, 293, 2093, 9590, 420, 337, 2701, 49471, 11, 721, 411, 300, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1876086969485228, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.005900254938751459}, {"id": 58, "seek": 32588, "start": 325.88, "end": 330.64, "text": " But overall machine learning requires a lot of data and the machines that we have are", "tokens": [50364, 583, 4787, 3479, 2539, 7029, 257, 688, 295, 1412, 293, 264, 8379, 300, 321, 362, 366, 50602], "temperature": 0.0, "avg_logprob": -0.17756891250610352, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0037598791532218456}, {"id": 59, "seek": 32588, "start": 330.64, "end": 332.64, "text": " somewhat brittle, specialized.", "tokens": [50602, 8344, 49325, 11, 19813, 13, 50702], "temperature": 0.0, "avg_logprob": -0.17756891250610352, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0037598791532218456}, {"id": 60, "seek": 32588, "start": 332.64, "end": 339.56, "text": " They don't have human-level intelligence despite what we may be led to believe.", "tokens": [50702, 814, 500, 380, 362, 1952, 12, 12418, 7599, 7228, 437, 321, 815, 312, 4684, 281, 1697, 13, 51048], "temperature": 0.0, "avg_logprob": -0.17756891250610352, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0037598791532218456}, {"id": 61, "seek": 32588, "start": 339.56, "end": 347.28, "text": " So in short, machine learning sucks at least compared to humans and animals.", "tokens": [51048, 407, 294, 2099, 11, 3479, 2539, 15846, 412, 1935, 5347, 281, 6255, 293, 4882, 13, 51434], "temperature": 0.0, "avg_logprob": -0.17756891250610352, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0037598791532218456}, {"id": 62, "seek": 32588, "start": 347.28, "end": 352.52, "text": " We've been using supervised learning which really was the workhorse of machine learning", "tokens": [51434, 492, 600, 668, 1228, 46533, 2539, 597, 534, 390, 264, 589, 45079, 295, 3479, 2539, 51696], "temperature": 0.0, "avg_logprob": -0.17756891250610352, "compression_ratio": 1.6334841628959276, "no_speech_prob": 0.0037598791532218456}, {"id": 63, "seek": 35252, "start": 352.52, "end": 356.52, "text": " and AI systems until very recently.", "tokens": [50364, 293, 7318, 3652, 1826, 588, 3938, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 64, "seek": 35252, "start": 356.52, "end": 360.64, "text": " Reinforcement learning is insanely inefficient but it works really well for games but not", "tokens": [50564, 42116, 9382, 2539, 307, 40965, 43495, 457, 309, 1985, 534, 731, 337, 2813, 457, 406, 50770], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 65, "seek": 35252, "start": 360.64, "end": 362.71999999999997, "text": " many other things.", "tokens": [50770, 867, 661, 721, 13, 50874], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 66, "seek": 35252, "start": 362.71999999999997, "end": 367.56, "text": " So one thing that has taken over the AI world in the last few years is something called self-supervised", "tokens": [50874, 407, 472, 551, 300, 575, 2726, 670, 264, 7318, 1002, 294, 264, 1036, 1326, 924, 307, 746, 1219, 2698, 12, 48172, 24420, 51116], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 67, "seek": 35252, "start": 367.56, "end": 371.88, "text": " learning which I will talk about at length.", "tokens": [51116, 2539, 597, 286, 486, 751, 466, 412, 4641, 13, 51332], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 68, "seek": 35252, "start": 371.88, "end": 374.15999999999997, "text": " But current AI systems are specialized and brittle.", "tokens": [51332, 583, 2190, 7318, 3652, 366, 19813, 293, 49325, 13, 51446], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 69, "seek": 35252, "start": 374.15999999999997, "end": 375.15999999999997, "text": " They make stupid mistakes.", "tokens": [51446, 814, 652, 6631, 8038, 13, 51496], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 70, "seek": 35252, "start": 375.15999999999997, "end": 381.44, "text": " They don't really reason and plan with a few exceptions for a game playing for example.", "tokens": [51496, 814, 500, 380, 534, 1778, 293, 1393, 365, 257, 1326, 22847, 337, 257, 1216, 2433, 337, 1365, 13, 51810], "temperature": 0.0, "avg_logprob": -0.15968736309871495, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.005131307058036327}, {"id": 71, "seek": 38144, "start": 381.44, "end": 385.4, "text": " Compared to humans and animals, they can learn new tasks extremely quickly, understand", "tokens": [50364, 30539, 281, 6255, 293, 4882, 11, 436, 393, 1466, 777, 9608, 4664, 2661, 11, 1223, 50562], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 72, "seek": 38144, "start": 385.4, "end": 390.76, "text": " how the world works, can reason and plan have some level of common sense.", "tokens": [50562, 577, 264, 1002, 1985, 11, 393, 1778, 293, 1393, 362, 512, 1496, 295, 2689, 2020, 13, 50830], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 73, "seek": 38144, "start": 390.76, "end": 392.96, "text": " Machines still don't have common sense.", "tokens": [50830, 12089, 1652, 920, 500, 380, 362, 2689, 2020, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 74, "seek": 38144, "start": 392.96, "end": 398.48, "text": " So how do we get machines to reason and plan like animals and humans learn as fast as animals", "tokens": [50940, 407, 577, 360, 321, 483, 8379, 281, 1778, 293, 1393, 411, 4882, 293, 6255, 1466, 382, 2370, 382, 4882, 51216], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 75, "seek": 38144, "start": 398.48, "end": 400.32, "text": " and humans?", "tokens": [51216, 293, 6255, 30, 51308], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 76, "seek": 38144, "start": 400.32, "end": 404.32, "text": " And we'll need machines that can understand how the world works, can predict the consequences", "tokens": [51308, 400, 321, 603, 643, 8379, 300, 393, 1223, 577, 264, 1002, 1985, 11, 393, 6069, 264, 10098, 51508], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 77, "seek": 38144, "start": 404.32, "end": 411.04, "text": " of their actions, can perform change of reasoning with unlimited number of steps, can plan complex", "tokens": [51508, 295, 641, 5909, 11, 393, 2042, 1319, 295, 21577, 365, 21950, 1230, 295, 4439, 11, 393, 1393, 3997, 51844], "temperature": 0.0, "avg_logprob": -0.1438767762310737, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.02680841088294983}, {"id": 78, "seek": 41104, "start": 411.64000000000004, "end": 415.08000000000004, "text": " tasks by decomposing them into simpler tasks.", "tokens": [50394, 9608, 538, 22867, 6110, 552, 666, 18587, 9608, 13, 50566], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 79, "seek": 41104, "start": 415.08000000000004, "end": 418.36, "text": " So let's start with this idea of self-supervised learning.", "tokens": [50566, 407, 718, 311, 722, 365, 341, 1558, 295, 2698, 12, 48172, 24420, 2539, 13, 50730], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 80, "seek": 41104, "start": 418.36, "end": 420.8, "text": " It's really taken over the world.", "tokens": [50730, 467, 311, 534, 2726, 670, 264, 1002, 13, 50852], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 81, "seek": 41104, "start": 420.8, "end": 424.84000000000003, "text": " Every sort of top machine learning system today uses some form of self-supervised learning", "tokens": [50852, 2048, 1333, 295, 1192, 3479, 2539, 1185, 965, 4960, 512, 1254, 295, 2698, 12, 48172, 24420, 2539, 51054], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 82, "seek": 41104, "start": 424.84000000000003, "end": 429.08000000000004, "text": " as a first step to pre-train the system.", "tokens": [51054, 382, 257, 700, 1823, 281, 659, 12, 83, 7146, 264, 1185, 13, 51266], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 83, "seek": 41104, "start": 429.08000000000004, "end": 430.58000000000004, "text": " And it's used everywhere.", "tokens": [51266, 400, 309, 311, 1143, 5315, 13, 51341], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 84, "seek": 41104, "start": 430.58000000000004, "end": 431.58000000000004, "text": " What does it consist of?", "tokens": [51341, 708, 775, 309, 4603, 295, 30, 51391], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 85, "seek": 41104, "start": 431.58000000000004, "end": 436.84000000000003, "text": " It's really the idea that instead of having, of training a system with an input and an", "tokens": [51391, 467, 311, 534, 264, 1558, 300, 2602, 295, 1419, 11, 295, 3097, 257, 1185, 365, 364, 4846, 293, 364, 51654], "temperature": 0.0, "avg_logprob": -0.18001776878986883, "compression_ratio": 1.6929460580912863, "no_speech_prob": 0.004460597410798073}, {"id": 86, "seek": 43684, "start": 436.84, "end": 441.59999999999997, "text": " output, which is the case in supervised learning, or with an input and a reward, which is the", "tokens": [50364, 5598, 11, 597, 307, 264, 1389, 294, 46533, 2539, 11, 420, 365, 364, 4846, 293, 257, 7782, 11, 597, 307, 264, 50602], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 87, "seek": 43684, "start": 441.59999999999997, "end": 446.2, "text": " case for reinforcement learning, you train the system to basically model its input.", "tokens": [50602, 1389, 337, 29280, 2539, 11, 291, 3847, 264, 1185, 281, 1936, 2316, 1080, 4846, 13, 50832], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 88, "seek": 43684, "start": 446.2, "end": 450.32, "text": " You don't train it for any particular task other than capture the dependency between", "tokens": [50832, 509, 500, 380, 3847, 309, 337, 604, 1729, 5633, 661, 813, 7983, 264, 33621, 1296, 51038], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 89, "seek": 43684, "start": 450.32, "end": 451.56, "text": " different parts of its input.", "tokens": [51038, 819, 3166, 295, 1080, 4846, 13, 51100], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 90, "seek": 43684, "start": 451.56, "end": 458.08, "text": " So one thing you might do is, for example, take a piece of video, a piece of text, show", "tokens": [51100, 407, 472, 551, 291, 1062, 360, 307, 11, 337, 1365, 11, 747, 257, 2522, 295, 960, 11, 257, 2522, 295, 2487, 11, 855, 51426], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 91, "seek": 43684, "start": 458.08, "end": 463.28, "text": " a piece of the video to the system and ask it to predict the missing piece, like the", "tokens": [51426, 257, 2522, 295, 264, 960, 281, 264, 1185, 293, 1029, 309, 281, 6069, 264, 5361, 2522, 11, 411, 264, 51686], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 92, "seek": 43684, "start": 463.28, "end": 464.52, "text": " continuation of that video.", "tokens": [51686, 29357, 295, 300, 960, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12694334983825684, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.0016932125436142087}, {"id": 93, "seek": 46452, "start": 464.52, "end": 468.91999999999996, "text": " And after a while, you reveal the rest of the video and you adjust the system so that", "tokens": [50364, 400, 934, 257, 1339, 11, 291, 10658, 264, 1472, 295, 264, 960, 293, 291, 4369, 264, 1185, 370, 300, 50584], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 94, "seek": 46452, "start": 468.91999999999996, "end": 471.96, "text": " it does a better job at predicting.", "tokens": [50584, 309, 775, 257, 1101, 1691, 412, 32884, 13, 50736], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 95, "seek": 46452, "start": 471.96, "end": 475.2, "text": " So prediction really is kind of the essence of intelligence.", "tokens": [50736, 407, 17630, 534, 307, 733, 295, 264, 12801, 295, 7599, 13, 50898], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 96, "seek": 46452, "start": 475.2, "end": 479.2, "text": " And to some extent, by training a system to predict, it doesn't have to be predicting", "tokens": [50898, 400, 281, 512, 8396, 11, 538, 3097, 257, 1185, 281, 6069, 11, 309, 1177, 380, 362, 281, 312, 32884, 51098], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 97, "seek": 46452, "start": 479.2, "end": 480.2, "text": " the future.", "tokens": [51098, 264, 2027, 13, 51148], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 98, "seek": 46452, "start": 480.2, "end": 483.15999999999997, "text": " It could be predicting the past or the left from the right.", "tokens": [51148, 467, 727, 312, 32884, 264, 1791, 420, 264, 1411, 490, 264, 558, 13, 51296], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 99, "seek": 46452, "start": 483.15999999999997, "end": 486.56, "text": " You're training the system to represent data, essentially.", "tokens": [51296, 509, 434, 3097, 264, 1185, 281, 2906, 1412, 11, 4476, 13, 51466], "temperature": 0.0, "avg_logprob": -0.16695428848266602, "compression_ratio": 1.75, "no_speech_prob": 0.00023048619914334267}, {"id": 100, "seek": 48656, "start": 486.56, "end": 495.28000000000003, "text": " And that's been nothing short of astonishingly successful in the domain of natural language", "tokens": [50364, 400, 300, 311, 668, 1825, 2099, 295, 35264, 356, 4406, 294, 264, 9274, 295, 3303, 2856, 50800], "temperature": 0.0, "avg_logprob": -0.16067769493855222, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.0011874922784045339}, {"id": 101, "seek": 48656, "start": 495.28000000000003, "end": 496.52, "text": " understanding.", "tokens": [50800, 3701, 13, 50862], "temperature": 0.0, "avg_logprob": -0.16067769493855222, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.0011874922784045339}, {"id": 102, "seek": 48656, "start": 496.52, "end": 506.08, "text": " So every type performing NLP system today is pre-trained the following way, or with some", "tokens": [50862, 407, 633, 2010, 10205, 426, 45196, 1185, 965, 307, 659, 12, 17227, 2001, 264, 3480, 636, 11, 420, 365, 512, 51340], "temperature": 0.0, "avg_logprob": -0.16067769493855222, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.0011874922784045339}, {"id": 103, "seek": 48656, "start": 506.08, "end": 511.6, "text": " form of the following way, which is a special case of an old idea called denoising autoencoder.", "tokens": [51340, 1254, 295, 264, 3480, 636, 11, 597, 307, 257, 2121, 1389, 295, 364, 1331, 1558, 1219, 1441, 78, 3436, 8399, 22660, 19866, 13, 51616], "temperature": 0.0, "avg_logprob": -0.16067769493855222, "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.0011874922784045339}, {"id": 104, "seek": 51160, "start": 511.6, "end": 520.12, "text": " And the idea is that you take a piece of text, sequence of words from a corpus.", "tokens": [50364, 400, 264, 1558, 307, 300, 291, 747, 257, 2522, 295, 2487, 11, 8310, 295, 2283, 490, 257, 1181, 31624, 13, 50790], "temperature": 0.0, "avg_logprob": -0.13715337753295898, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.02060459926724434}, {"id": 105, "seek": 51160, "start": 520.12, "end": 523.88, "text": " Typically it would be a few hundred or a few thousand words long.", "tokens": [50790, 23129, 309, 576, 312, 257, 1326, 3262, 420, 257, 1326, 4714, 2283, 938, 13, 50978], "temperature": 0.0, "avg_logprob": -0.13715337753295898, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.02060459926724434}, {"id": 106, "seek": 51160, "start": 523.88, "end": 529.52, "text": " Those words immediately get turned into vectors, but let me not talk about this for just now.", "tokens": [50978, 3950, 2283, 4258, 483, 3574, 666, 18875, 11, 457, 718, 385, 406, 751, 466, 341, 337, 445, 586, 13, 51260], "temperature": 0.0, "avg_logprob": -0.13715337753295898, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.02060459926724434}, {"id": 107, "seek": 51160, "start": 529.52, "end": 532.5600000000001, "text": " So the first thing you do is you corrupt this text.", "tokens": [51260, 407, 264, 700, 551, 291, 360, 307, 291, 17366, 341, 2487, 13, 51412], "temperature": 0.0, "avg_logprob": -0.13715337753295898, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.02060459926724434}, {"id": 108, "seek": 51160, "start": 532.5600000000001, "end": 537.5600000000001, "text": " You remove some of the words and replace them by blank markers, or you substitute them for", "tokens": [51412, 509, 4159, 512, 295, 264, 2283, 293, 7406, 552, 538, 8247, 19175, 11, 420, 291, 15802, 552, 337, 51662], "temperature": 0.0, "avg_logprob": -0.13715337753295898, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.02060459926724434}, {"id": 109, "seek": 51160, "start": 537.5600000000001, "end": 538.5600000000001, "text": " another word.", "tokens": [51662, 1071, 1349, 13, 51712], "temperature": 0.0, "avg_logprob": -0.13715337753295898, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.02060459926724434}, {"id": 110, "seek": 53856, "start": 539.1199999999999, "end": 543.28, "text": " And then you train some gigantic neural net to predict the words that are missing.", "tokens": [50392, 400, 550, 291, 3847, 512, 26800, 18161, 2533, 281, 6069, 264, 2283, 300, 366, 5361, 13, 50600], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 111, "seek": 53856, "start": 543.28, "end": 546.88, "text": " In the process of doing so, the system has to basically develop some sort of understanding", "tokens": [50600, 682, 264, 1399, 295, 884, 370, 11, 264, 1185, 575, 281, 1936, 1499, 512, 1333, 295, 3701, 50780], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 112, "seek": 53856, "start": 546.88, "end": 551.52, "text": " of the text, because if you want to be able to predict what word comes here, you have", "tokens": [50780, 295, 264, 2487, 11, 570, 498, 291, 528, 281, 312, 1075, 281, 6069, 437, 1349, 1487, 510, 11, 291, 362, 51012], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 113, "seek": 53856, "start": 551.52, "end": 557.7199999999999, "text": " to understand the role of the word in the sentence, the type of word that comes here,", "tokens": [51012, 281, 1223, 264, 3090, 295, 264, 1349, 294, 264, 8174, 11, 264, 2010, 295, 1349, 300, 1487, 510, 11, 51322], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 114, "seek": 53856, "start": 557.7199999999999, "end": 558.8, "text": " and the whole meaning of the sentence.", "tokens": [51322, 293, 264, 1379, 3620, 295, 264, 8174, 13, 51376], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 115, "seek": 53856, "start": 558.8, "end": 561.68, "text": " So the system basically learns to represent text.", "tokens": [51376, 407, 264, 1185, 1936, 27152, 281, 2906, 2487, 13, 51520], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 116, "seek": 53856, "start": 561.68, "end": 566.1199999999999, "text": " And the amazing thing is that just by doing this, you can train a system to represent", "tokens": [51520, 400, 264, 2243, 551, 307, 300, 445, 538, 884, 341, 11, 291, 393, 3847, 257, 1185, 281, 2906, 51742], "temperature": 0.0, "avg_logprob": -0.12425737458515942, "compression_ratio": 1.9622641509433962, "no_speech_prob": 0.0015482648741453886}, {"id": 117, "seek": 56612, "start": 566.12, "end": 571.2, "text": " the meaning of text in pretty much any language, as long as you have data.", "tokens": [50364, 264, 3620, 295, 2487, 294, 1238, 709, 604, 2856, 11, 382, 938, 382, 291, 362, 1412, 13, 50618], "temperature": 0.0, "avg_logprob": -0.13798095519284168, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.0009845501044765115}, {"id": 118, "seek": 56612, "start": 571.2, "end": 576.52, "text": " With a single system, you can have a system that represents the meaning of a piece of", "tokens": [50618, 2022, 257, 2167, 1185, 11, 291, 393, 362, 257, 1185, 300, 8855, 264, 3620, 295, 257, 2522, 295, 50884], "temperature": 0.0, "avg_logprob": -0.13798095519284168, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.0009845501044765115}, {"id": 119, "seek": 56612, "start": 576.52, "end": 579.76, "text": " text in any language.", "tokens": [50884, 2487, 294, 604, 2856, 13, 51046], "temperature": 0.0, "avg_logprob": -0.13798095519284168, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.0009845501044765115}, {"id": 120, "seek": 56612, "start": 579.76, "end": 580.84, "text": " So pretty cool.", "tokens": [51046, 407, 1238, 1627, 13, 51100], "temperature": 0.0, "avg_logprob": -0.13798095519284168, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.0009845501044765115}, {"id": 121, "seek": 56612, "start": 580.84, "end": 587.16, "text": " You can use this to build translation systems, systems that detect hate speech on social", "tokens": [51100, 509, 393, 764, 341, 281, 1322, 12853, 3652, 11, 3652, 300, 5531, 4700, 6218, 322, 2093, 51416], "temperature": 0.0, "avg_logprob": -0.13798095519284168, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.0009845501044765115}, {"id": 122, "seek": 56612, "start": 587.16, "end": 590.48, "text": " networks or figure out what something talks about.", "tokens": [51416, 9590, 420, 2573, 484, 437, 746, 6686, 466, 13, 51582], "temperature": 0.0, "avg_logprob": -0.13798095519284168, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.0009845501044765115}, {"id": 123, "seek": 59048, "start": 590.48, "end": 596.28, "text": " The way you do this is that you chop off the last few layers of that gigantic neural", "tokens": [50364, 440, 636, 291, 360, 341, 307, 300, 291, 7931, 766, 264, 1036, 1326, 7914, 295, 300, 26800, 18161, 50654], "temperature": 0.0, "avg_logprob": -0.15205640290912828, "compression_ratio": 1.6491935483870968, "no_speech_prob": 9.460013097850606e-05}, {"id": 124, "seek": 59048, "start": 596.28, "end": 601.72, "text": " net, and you use the representation, the internal representation, learned by the system as input", "tokens": [50654, 2533, 11, 293, 291, 764, 264, 10290, 11, 264, 6920, 10290, 11, 3264, 538, 264, 1185, 382, 4846, 50926], "temperature": 0.0, "avg_logprob": -0.15205640290912828, "compression_ratio": 1.6491935483870968, "no_speech_prob": 9.460013097850606e-05}, {"id": 125, "seek": 59048, "start": 601.72, "end": 608.8000000000001, "text": " to a subsequent downstream task that you train supervised, like, say, translation.", "tokens": [50926, 281, 257, 19962, 30621, 5633, 300, 291, 3847, 46533, 11, 411, 11, 584, 11, 12853, 13, 51280], "temperature": 0.0, "avg_logprob": -0.15205640290912828, "compression_ratio": 1.6491935483870968, "no_speech_prob": 9.460013097850606e-05}, {"id": 126, "seek": 59048, "start": 608.8000000000001, "end": 612.64, "text": " And it's really astonishing how well this works.", "tokens": [51280, 400, 309, 311, 534, 35264, 577, 731, 341, 1985, 13, 51472], "temperature": 0.0, "avg_logprob": -0.15205640290912828, "compression_ratio": 1.6491935483870968, "no_speech_prob": 9.460013097850606e-05}, {"id": 127, "seek": 59048, "start": 612.64, "end": 619.88, "text": " So from this to a generative AI system, there's a small step, particularly for text generation.", "tokens": [51472, 407, 490, 341, 281, 257, 1337, 1166, 7318, 1185, 11, 456, 311, 257, 1359, 1823, 11, 4098, 337, 2487, 5125, 13, 51834], "temperature": 0.0, "avg_logprob": -0.15205640290912828, "compression_ratio": 1.6491935483870968, "no_speech_prob": 9.460013097850606e-05}, {"id": 128, "seek": 61988, "start": 619.88, "end": 622.6, "text": " Text generation is a completely different thing, which I'm not going to talk about,", "tokens": [50364, 18643, 5125, 307, 257, 2584, 819, 551, 11, 597, 286, 478, 406, 516, 281, 751, 466, 11, 50500], "temperature": 0.0, "avg_logprob": -0.172065369626309, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0007649631588719785}, {"id": 129, "seek": 61988, "start": 622.6, "end": 626.76, "text": " but although some systems use the same technique.", "tokens": [50500, 457, 4878, 512, 3652, 764, 264, 912, 6532, 13, 50708], "temperature": 0.0, "avg_logprob": -0.172065369626309, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0007649631588719785}, {"id": 130, "seek": 61988, "start": 626.76, "end": 634.4399999999999, "text": " So what is a generative text generation system, a large language model?", "tokens": [50708, 407, 437, 307, 257, 1337, 1166, 2487, 5125, 1185, 11, 257, 2416, 2856, 2316, 30, 51092], "temperature": 0.0, "avg_logprob": -0.172065369626309, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0007649631588719785}, {"id": 131, "seek": 61988, "start": 634.4399999999999, "end": 642.12, "text": " It's a system of the type I just described, except that when you train it, you don't remove", "tokens": [51092, 467, 311, 257, 1185, 295, 264, 2010, 286, 445, 7619, 11, 3993, 300, 562, 291, 3847, 309, 11, 291, 500, 380, 4159, 51476], "temperature": 0.0, "avg_logprob": -0.172065369626309, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0007649631588719785}, {"id": 132, "seek": 61988, "start": 642.12, "end": 647.28, "text": " random words in the text that you show at the input, you only remove the last one.", "tokens": [51476, 4974, 2283, 294, 264, 2487, 300, 291, 855, 412, 264, 4846, 11, 291, 787, 4159, 264, 1036, 472, 13, 51734], "temperature": 0.0, "avg_logprob": -0.172065369626309, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0007649631588719785}, {"id": 133, "seek": 64728, "start": 647.28, "end": 653.12, "text": " So you train the system to predict the last word in a sequence of words.", "tokens": [50364, 407, 291, 3847, 264, 1185, 281, 6069, 264, 1036, 1349, 294, 257, 8310, 295, 2283, 13, 50656], "temperature": 0.0, "avg_logprob": -0.13342088133424193, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.00045111210783943534}, {"id": 134, "seek": 64728, "start": 653.12, "end": 658.6, "text": " So show a sequence of words, and then show the last word, and train some gigantic neural", "tokens": [50656, 407, 855, 257, 8310, 295, 2283, 11, 293, 550, 855, 264, 1036, 1349, 11, 293, 3847, 512, 26800, 18161, 50930], "temperature": 0.0, "avg_logprob": -0.13342088133424193, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.00045111210783943534}, {"id": 135, "seek": 64728, "start": 658.6, "end": 662.3199999999999, "text": " net, perhaps with billions or hundreds of billions of parameters, to predict the next", "tokens": [50930, 2533, 11, 4317, 365, 17375, 420, 6779, 295, 17375, 295, 9834, 11, 281, 6069, 264, 958, 51116], "temperature": 0.0, "avg_logprob": -0.13342088133424193, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.00045111210783943534}, {"id": 136, "seek": 64728, "start": 662.3199999999999, "end": 663.3199999999999, "text": " word.", "tokens": [51116, 1349, 13, 51166], "temperature": 0.0, "avg_logprob": -0.13342088133424193, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.00045111210783943534}, {"id": 137, "seek": 64728, "start": 663.3199999999999, "end": 672.0799999999999, "text": " And you have to train this on trillions of text snippets, typically one to two trillion", "tokens": [51166, 400, 291, 362, 281, 3847, 341, 322, 504, 46279, 295, 2487, 35623, 1385, 11, 5850, 472, 281, 732, 18723, 51604], "temperature": 0.0, "avg_logprob": -0.13342088133424193, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.00045111210783943534}, {"id": 138, "seek": 64728, "start": 672.0799999999999, "end": 674.56, "text": " for the biggest models.", "tokens": [51604, 337, 264, 3880, 5245, 13, 51728], "temperature": 0.0, "avg_logprob": -0.13342088133424193, "compression_ratio": 1.8159203980099503, "no_speech_prob": 0.00045111210783943534}, {"id": 139, "seek": 67456, "start": 674.56, "end": 677.9599999999999, "text": " Once you have that system, you can use it to generate text using what's called autoregressive", "tokens": [50364, 3443, 291, 362, 300, 1185, 11, 291, 393, 764, 309, 281, 8460, 2487, 1228, 437, 311, 1219, 1476, 418, 3091, 488, 50534], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 140, "seek": 67456, "start": 677.9599999999999, "end": 681.3199999999999, "text": " prediction, which is a very classical thing to do in single processing.", "tokens": [50534, 17630, 11, 597, 307, 257, 588, 13735, 551, 281, 360, 294, 2167, 9007, 13, 50702], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 141, "seek": 67456, "start": 681.3199999999999, "end": 685.7199999999999, "text": " So you take a piece of text called a prompt, you enter it into the system, you have it", "tokens": [50702, 407, 291, 747, 257, 2522, 295, 2487, 1219, 257, 12391, 11, 291, 3242, 309, 666, 264, 1185, 11, 291, 362, 309, 50922], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 142, "seek": 67456, "start": 685.7199999999999, "end": 691.92, "text": " predict the next word, and then you shift that word into the input.", "tokens": [50922, 6069, 264, 958, 1349, 11, 293, 550, 291, 5513, 300, 1349, 666, 264, 4846, 13, 51232], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 143, "seek": 67456, "start": 691.92, "end": 697.16, "text": " So now it becomes part of the input to the system, and now you can predict the next word,", "tokens": [51232, 407, 586, 309, 3643, 644, 295, 264, 4846, 281, 264, 1185, 11, 293, 586, 291, 393, 6069, 264, 958, 1349, 11, 51494], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 144, "seek": 67456, "start": 697.16, "end": 699.8, "text": " shift it in, predict the third word, shift it in.", "tokens": [51494, 5513, 309, 294, 11, 6069, 264, 2636, 1349, 11, 5513, 309, 294, 13, 51626], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 145, "seek": 67456, "start": 699.8, "end": 702.1999999999999, "text": " That's autoregressive prediction.", "tokens": [51626, 663, 311, 1476, 418, 3091, 488, 17630, 13, 51746], "temperature": 0.0, "avg_logprob": -0.11487165347550267, "compression_ratio": 2.058333333333333, "no_speech_prob": 0.008570745587348938}, {"id": 146, "seek": 70220, "start": 702.2, "end": 706.9200000000001, "text": " And that's how all the bigger alarms that everybody has played with work.", "tokens": [50364, 400, 300, 311, 577, 439, 264, 3801, 45039, 300, 2201, 575, 3737, 365, 589, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 147, "seek": 70220, "start": 706.9200000000001, "end": 707.9200000000001, "text": " That's how they've been trained.", "tokens": [50600, 663, 311, 577, 436, 600, 668, 8895, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 148, "seek": 70220, "start": 707.9200000000001, "end": 711.1600000000001, "text": " That's how they generate text.", "tokens": [50650, 663, 311, 577, 436, 8460, 2487, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 149, "seek": 70220, "start": 711.1600000000001, "end": 720.4000000000001, "text": " So those alarms are kind of amazing in terms of the performance that they produce.", "tokens": [50812, 407, 729, 45039, 366, 733, 295, 2243, 294, 2115, 295, 264, 3389, 300, 436, 5258, 13, 51274], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 150, "seek": 70220, "start": 720.4000000000001, "end": 723.76, "text": " So again, they're trained on something like one to two trillion tokens.", "tokens": [51274, 407, 797, 11, 436, 434, 8895, 322, 746, 411, 472, 281, 732, 18723, 22667, 13, 51442], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 151, "seek": 70220, "start": 723.76, "end": 727.72, "text": " A token is like a word or a subword unit.", "tokens": [51442, 316, 14862, 307, 411, 257, 1349, 420, 257, 1422, 7462, 4985, 13, 51640], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 152, "seek": 70220, "start": 727.72, "end": 730.4000000000001, "text": " And there's a whole bunch of those models, most of which you probably haven't heard of,", "tokens": [51640, 400, 456, 311, 257, 1379, 3840, 295, 729, 5245, 11, 881, 295, 597, 291, 1391, 2378, 380, 2198, 295, 11, 51774], "temperature": 0.0, "avg_logprob": -0.2004404505458447, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.00026110748876817524}, {"id": 153, "seek": 73040, "start": 730.4, "end": 735.8, "text": " but there's a few that have become household names.", "tokens": [50364, 457, 456, 311, 257, 1326, 300, 362, 1813, 9888, 5288, 13, 50634], "temperature": 0.0, "avg_logprob": -0.2335542634476063, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.05357866361737251}, {"id": 154, "seek": 73040, "start": 735.8, "end": 746.0, "text": " So we've heard of chatGBT and GPT-4 from OpenAI, which are kind of usable, barred from Google,", "tokens": [50634, 407, 321, 600, 2198, 295, 5081, 8769, 51, 293, 26039, 51, 12, 19, 490, 7238, 48698, 11, 597, 366, 733, 295, 29975, 11, 2159, 986, 490, 3329, 11, 51144], "temperature": 0.0, "avg_logprob": -0.2335542634476063, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.05357866361737251}, {"id": 155, "seek": 73040, "start": 746.0, "end": 753.1999999999999, "text": " and derivative of chatGBT and GPT-4 from Microsoft, married with Bing.", "tokens": [51144, 293, 13760, 295, 5081, 8769, 51, 293, 26039, 51, 12, 19, 490, 8116, 11, 5259, 365, 30755, 13, 51504], "temperature": 0.0, "avg_logprob": -0.2335542634476063, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.05357866361737251}, {"id": 156, "seek": 73040, "start": 753.1999999999999, "end": 758.16, "text": " But there's a long history of those things that goes back several years, some from Fair,", "tokens": [51504, 583, 456, 311, 257, 938, 2503, 295, 729, 721, 300, 1709, 646, 2940, 924, 11, 512, 490, 12157, 11, 51752], "temperature": 0.0, "avg_logprob": -0.2335542634476063, "compression_ratio": 1.4854368932038835, "no_speech_prob": 0.05357866361737251}, {"id": 157, "seek": 75816, "start": 758.16, "end": 759.16, "text": " Lunderbot, and Galactica.", "tokens": [50364, 441, 6617, 18870, 11, 293, 7336, 578, 2262, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 158, "seek": 75816, "start": 759.16, "end": 767.4, "text": " Galactica was trained on the scientific literature and is designed to help scientists write papers.", "tokens": [50414, 7336, 578, 2262, 390, 8895, 322, 264, 8134, 10394, 293, 307, 4761, 281, 854, 7708, 2464, 10577, 13, 50826], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 159, "seek": 75816, "start": 767.4, "end": 771.9599999999999, "text": " And a more recent one, called LAMA, which is the code is open source.", "tokens": [50826, 400, 257, 544, 5162, 472, 11, 1219, 441, 38136, 11, 597, 307, 264, 3089, 307, 1269, 4009, 13, 51054], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 160, "seek": 75816, "start": 771.9599999999999, "end": 776.76, "text": " The model, you can get it on request if you are using it for research purpose.", "tokens": [51054, 440, 2316, 11, 291, 393, 483, 309, 322, 5308, 498, 291, 366, 1228, 309, 337, 2132, 4334, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 161, "seek": 75816, "start": 776.76, "end": 780.36, "text": " And it's the same level of performance as things like chatGBT, but it's not fine-tuned.", "tokens": [51294, 400, 309, 311, 264, 912, 1496, 295, 3389, 382, 721, 411, 5081, 8769, 51, 11, 457, 309, 311, 406, 2489, 12, 83, 43703, 13, 51474], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 162, "seek": 75816, "start": 780.36, "end": 782.36, "text": " You have to fine-tune it for application.", "tokens": [51474, 509, 362, 281, 2489, 12, 83, 2613, 309, 337, 3861, 13, 51574], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 163, "seek": 75816, "start": 782.36, "end": 787.0799999999999, "text": " And in fact, people have done this, so Alpaca is a model which basically is a fine-tuned", "tokens": [51574, 400, 294, 1186, 11, 561, 362, 1096, 341, 11, 370, 967, 79, 6628, 307, 257, 2316, 597, 1936, 307, 257, 2489, 12, 83, 43703, 51810], "temperature": 0.0, "avg_logprob": -0.2245106351548347, "compression_ratio": 1.6883561643835616, "no_speech_prob": 0.02913537435233593}, {"id": 164, "seek": 78708, "start": 787.08, "end": 793.4000000000001, "text": " version of LAMA that was built by people at Stanford for answering questions and things", "tokens": [50364, 3037, 295, 441, 38136, 300, 390, 3094, 538, 561, 412, 20374, 337, 13430, 1651, 293, 721, 50680], "temperature": 0.0, "avg_logprob": -0.17732932832505968, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.03668247535824776}, {"id": 165, "seek": 78708, "start": 793.4000000000001, "end": 795.2, "text": " like that, instruction.", "tokens": [50680, 411, 300, 11, 10951, 13, 50770], "temperature": 0.0, "avg_logprob": -0.17732932832505968, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.03668247535824776}, {"id": 166, "seek": 78708, "start": 795.2, "end": 801.12, "text": " So they're pretty amazing, they surprised a lot of people in how well they work, but", "tokens": [50770, 407, 436, 434, 1238, 2243, 11, 436, 6100, 257, 688, 295, 561, 294, 577, 731, 436, 589, 11, 457, 51066], "temperature": 0.0, "avg_logprob": -0.17732932832505968, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.03668247535824776}, {"id": 167, "seek": 78708, "start": 801.12, "end": 807.2, "text": " they make a lot of factual errors, logical errors, inconsistencies, limited reasoning", "tokens": [51066, 436, 652, 257, 688, 295, 48029, 13603, 11, 14978, 13603, 11, 22039, 4821, 4629, 11, 5567, 21577, 51370], "temperature": 0.0, "avg_logprob": -0.17732932832505968, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.03668247535824776}, {"id": 168, "seek": 78708, "start": 807.2, "end": 809.72, "text": " abilities, things like that.", "tokens": [51370, 11582, 11, 721, 411, 300, 13, 51496], "temperature": 0.0, "avg_logprob": -0.17732932832505968, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.03668247535824776}, {"id": 169, "seek": 78708, "start": 809.72, "end": 813.5600000000001, "text": " And they are easy to, they're pretty gullible.", "tokens": [51496, 400, 436, 366, 1858, 281, 11, 436, 434, 1238, 695, 285, 964, 13, 51688], "temperature": 0.0, "avg_logprob": -0.17732932832505968, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.03668247535824776}, {"id": 170, "seek": 81356, "start": 813.56, "end": 818.56, "text": " So you tell them, what is 2 plus 2, and the system will say 4, and you say, no, actually", "tokens": [50364, 407, 291, 980, 552, 11, 437, 307, 568, 1804, 568, 11, 293, 264, 1185, 486, 584, 1017, 11, 293, 291, 584, 11, 572, 11, 767, 50614], "temperature": 0.0, "avg_logprob": -0.2087409210205078, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.001597295398823917}, {"id": 171, "seek": 81356, "start": 818.56, "end": 819.56, "text": " 2 plus 2 equals 5.", "tokens": [50614, 568, 1804, 568, 6915, 1025, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2087409210205078, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.001597295398823917}, {"id": 172, "seek": 81356, "start": 819.56, "end": 821.64, "text": " Oh yeah, you're right, I made a mistake.", "tokens": [50664, 876, 1338, 11, 291, 434, 558, 11, 286, 1027, 257, 6146, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2087409210205078, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.001597295398823917}, {"id": 173, "seek": 81356, "start": 821.64, "end": 831.64, "text": " So they kind of, they predict answers that would sound like someone could produce these", "tokens": [50768, 407, 436, 733, 295, 11, 436, 6069, 6338, 300, 576, 1626, 411, 1580, 727, 5258, 613, 51268], "temperature": 0.0, "avg_logprob": -0.2087409210205078, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.001597295398823917}, {"id": 174, "seek": 81356, "start": 831.64, "end": 834.0, "text": " answers, but the details might be wrong.", "tokens": [51268, 6338, 11, 457, 264, 4365, 1062, 312, 2085, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2087409210205078, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.001597295398823917}, {"id": 175, "seek": 81356, "start": 834.0, "end": 841.4799999999999, "text": " So you can't really use them for factual answers, but you can use them certainly for", "tokens": [51386, 407, 291, 393, 380, 534, 764, 552, 337, 48029, 6338, 11, 457, 291, 393, 764, 552, 3297, 337, 51760], "temperature": 0.0, "avg_logprob": -0.2087409210205078, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.001597295398823917}, {"id": 176, "seek": 84148, "start": 841.48, "end": 843.12, "text": " writing aids.", "tokens": [50364, 3579, 28447, 13, 50446], "temperature": 0.0, "avg_logprob": -0.19417521319811856, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007811990100890398}, {"id": 177, "seek": 84148, "start": 843.12, "end": 850.76, "text": " And particularly, it works really well for text or for standard sort of templatized text", "tokens": [50446, 400, 4098, 11, 309, 1985, 534, 731, 337, 2487, 420, 337, 3832, 1333, 295, 9100, 267, 1602, 2487, 50828], "temperature": 0.0, "avg_logprob": -0.19417521319811856, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007811990100890398}, {"id": 178, "seek": 84148, "start": 850.76, "end": 856.04, "text": " that you need to write, like I don't know, there's a bunch of professors here that have", "tokens": [50828, 300, 291, 643, 281, 2464, 11, 411, 286, 500, 380, 458, 11, 456, 311, 257, 3840, 295, 15924, 510, 300, 362, 51092], "temperature": 0.0, "avg_logprob": -0.19417521319811856, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007811990100890398}, {"id": 179, "seek": 84148, "start": 856.04, "end": 866.2, "text": " to spend quite a bit of time writing recommendation letters for students, very useful for that.", "tokens": [51092, 281, 3496, 1596, 257, 857, 295, 565, 3579, 11879, 7825, 337, 1731, 11, 588, 4420, 337, 300, 13, 51600], "temperature": 0.0, "avg_logprob": -0.19417521319811856, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007811990100890398}, {"id": 180, "seek": 84148, "start": 866.2, "end": 867.72, "text": " And very useful for code generation.", "tokens": [51600, 400, 588, 4420, 337, 3089, 5125, 13, 51676], "temperature": 0.0, "avg_logprob": -0.19417521319811856, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007811990100890398}, {"id": 181, "seek": 86772, "start": 867.72, "end": 872.5600000000001, "text": " So the software industry is probably going to be revolutionized by such tools.", "tokens": [50364, 407, 264, 4722, 3518, 307, 1391, 516, 281, 312, 8894, 1602, 538, 1270, 3873, 13, 50606], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 182, "seek": 86772, "start": 872.5600000000001, "end": 878.08, "text": " So this is an example of code generated from a prompt by the Lama 65 billion model, the", "tokens": [50606, 407, 341, 307, 364, 1365, 295, 3089, 10833, 490, 257, 12391, 538, 264, 441, 2404, 11624, 5218, 2316, 11, 264, 50882], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 183, "seek": 86772, "start": 878.08, "end": 879.08, "text": " open source one.", "tokens": [50882, 1269, 4009, 472, 13, 50932], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 184, "seek": 86772, "start": 879.08, "end": 884.1600000000001, "text": " So, you know, ask it, you know, find real roots of AX squared plus BX plus C, and the", "tokens": [50932, 407, 11, 291, 458, 11, 1029, 309, 11, 291, 458, 11, 915, 957, 10669, 295, 316, 55, 8889, 1804, 363, 55, 1804, 383, 11, 293, 264, 51186], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 185, "seek": 86772, "start": 884.1600000000001, "end": 889.52, "text": " thing just writes a function in Python or whatever, whatever you want, or Reg X or whatever,", "tokens": [51186, 551, 445, 13657, 257, 2445, 294, 15329, 420, 2035, 11, 2035, 291, 528, 11, 420, 4791, 1783, 420, 2035, 11, 51454], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 186, "seek": 86772, "start": 889.52, "end": 893.28, "text": " who remembers the syntax of Reg X?", "tokens": [51454, 567, 26228, 264, 28431, 295, 4791, 1783, 30, 51642], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 187, "seek": 86772, "start": 893.28, "end": 895.28, "text": " Like.", "tokens": [51642, 1743, 13, 51742], "temperature": 0.0, "avg_logprob": -0.260221326673353, "compression_ratio": 1.55, "no_speech_prob": 0.0019407414365559816}, {"id": 188, "seek": 89528, "start": 895.28, "end": 900.48, "text": " You can have it, you know, hallucinate text that might sound plausible or completely implausible", "tokens": [50364, 509, 393, 362, 309, 11, 291, 458, 11, 35212, 13923, 2487, 300, 1062, 1626, 39925, 420, 2584, 8484, 8463, 964, 50624], "temperature": 0.0, "avg_logprob": -0.24195851882298788, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.004326220136135817}, {"id": 189, "seek": 89528, "start": 900.48, "end": 901.48, "text": " like this.", "tokens": [50624, 411, 341, 13, 50674], "temperature": 0.0, "avg_logprob": -0.24195851882298788, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.004326220136135817}, {"id": 190, "seek": 89528, "start": 901.48, "end": 904.56, "text": " Did you know that Yanukun dropped a rap album last year?", "tokens": [50674, 2589, 291, 458, 300, 13633, 2034, 409, 8119, 257, 5099, 6030, 1036, 1064, 30, 50828], "temperature": 0.0, "avg_logprob": -0.24195851882298788, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.004326220136135817}, {"id": 191, "seek": 89528, "start": 904.56, "end": 907.1999999999999, "text": " We listened to it and here is what we thought.", "tokens": [50828, 492, 13207, 281, 309, 293, 510, 307, 437, 321, 1194, 13, 50960], "temperature": 0.0, "avg_logprob": -0.24195851882298788, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.004326220136135817}, {"id": 192, "seek": 89528, "start": 907.1999999999999, "end": 917.92, "text": " And the thing writes a review of my alleged rap album.", "tokens": [50960, 400, 264, 551, 13657, 257, 3131, 295, 452, 26317, 5099, 6030, 13, 51496], "temperature": 0.0, "avg_logprob": -0.24195851882298788, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.004326220136135817}, {"id": 193, "seek": 89528, "start": 917.92, "end": 922.24, "text": " I'm not much of a rap person, I'm more of a jazz person, so when my colleagues showed", "tokens": [51496, 286, 478, 406, 709, 295, 257, 5099, 954, 11, 286, 478, 544, 295, 257, 15066, 954, 11, 370, 562, 452, 7734, 4712, 51712], "temperature": 0.0, "avg_logprob": -0.24195851882298788, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.004326220136135817}, {"id": 194, "seek": 92224, "start": 922.72, "end": 926.6800000000001, "text": " this to me, I told them, like, can you do the same for a jazz album that would be kind", "tokens": [50388, 341, 281, 385, 11, 286, 1907, 552, 11, 411, 11, 393, 291, 360, 264, 912, 337, 257, 15066, 6030, 300, 576, 312, 733, 50586], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 195, "seek": 92224, "start": 926.6800000000001, "end": 927.6800000000001, "text": " of more appropriate?", "tokens": [50586, 295, 544, 6854, 30, 50636], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 196, "seek": 92224, "start": 927.6800000000001, "end": 933.16, "text": " I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't", "tokens": [50636, 286, 914, 11, 286, 478, 257, 6237, 30248, 11, 457, 11, 293, 286, 848, 11, 1338, 11, 321, 3031, 1217, 11, 457, 309, 994, 380, 50910], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 197, "seek": 92224, "start": 933.16, "end": 938.12, "text": " work very well because there's not enough training data on the web of reviews of jazz", "tokens": [50910, 589, 588, 731, 570, 456, 311, 406, 1547, 3097, 1412, 322, 264, 3670, 295, 10229, 295, 15066, 51158], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 198, "seek": 92224, "start": 938.12, "end": 939.12, "text": " albums.", "tokens": [51158, 24795, 13, 51208], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 199, "seek": 92224, "start": 939.12, "end": 944.32, "text": " I found that incredibly sad, I cried.", "tokens": [51208, 286, 1352, 300, 6252, 4227, 11, 286, 16266, 13, 51468], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 200, "seek": 92224, "start": 944.32, "end": 946.72, "text": " So you need a lot of data to train those things, right?", "tokens": [51468, 407, 291, 643, 257, 688, 295, 1412, 281, 3847, 729, 721, 11, 558, 30, 51588], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 201, "seek": 92224, "start": 946.72, "end": 950.96, "text": " In fact, the amount of data, like something like 1.5 trillion tokens that Lama is trained", "tokens": [51588, 682, 1186, 11, 264, 2372, 295, 1412, 11, 411, 746, 411, 502, 13, 20, 18723, 22667, 300, 441, 2404, 307, 8895, 51800], "temperature": 0.0, "avg_logprob": -0.21091947340427486, "compression_ratio": 1.6232876712328768, "no_speech_prob": 0.005551595706492662}, {"id": 202, "seek": 95096, "start": 950.96, "end": 956.0, "text": " on, it would take about 22,000 years for a human reading eight hours a day at every", "tokens": [50364, 322, 11, 309, 576, 747, 466, 5853, 11, 1360, 924, 337, 257, 1952, 3760, 3180, 2496, 257, 786, 412, 633, 50616], "temperature": 0.0, "avg_logprob": -0.12926388653841886, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0016730206552892923}, {"id": 203, "seek": 95096, "start": 956.0, "end": 958.48, "text": " speed to read the whole material.", "tokens": [50616, 3073, 281, 1401, 264, 1379, 2527, 13, 50740], "temperature": 0.0, "avg_logprob": -0.12926388653841886, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0016730206552892923}, {"id": 204, "seek": 95096, "start": 958.48, "end": 964.1600000000001, "text": " So obviously those things can accumulate a lot of knowledge, at least approximately.", "tokens": [50740, 407, 2745, 729, 721, 393, 33384, 257, 688, 295, 3601, 11, 412, 1935, 10447, 13, 51024], "temperature": 0.0, "avg_logprob": -0.12926388653841886, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0016730206552892923}, {"id": 205, "seek": 95096, "start": 964.1600000000001, "end": 969.24, "text": " So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not", "tokens": [51024, 407, 1338, 11, 3579, 34949, 11, 3089, 5125, 11, 700, 11206, 295, 257, 688, 295, 1507, 11, 436, 434, 406, 51278], "temperature": 0.0, "avg_logprob": -0.12926388653841886, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0016730206552892923}, {"id": 206, "seek": 95096, "start": 969.24, "end": 972.6800000000001, "text": " good for producing factual and consistent answers, at least not yet.", "tokens": [51278, 665, 337, 10501, 48029, 293, 8398, 6338, 11, 412, 1935, 406, 1939, 13, 51450], "temperature": 0.0, "avg_logprob": -0.12926388653841886, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0016730206552892923}, {"id": 207, "seek": 95096, "start": 972.6800000000001, "end": 978.6800000000001, "text": " So a lot of LLMs are being augmented or extended so that they can use tools like calculators", "tokens": [51450, 407, 257, 688, 295, 441, 43, 26386, 366, 885, 36155, 420, 10913, 370, 300, 436, 393, 764, 3873, 411, 4322, 3391, 51750], "temperature": 0.0, "avg_logprob": -0.12926388653841886, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0016730206552892923}, {"id": 208, "seek": 97868, "start": 978.68, "end": 986.4, "text": " or database engines or whatever to search for information and then refer to the source.", "tokens": [50364, 420, 8149, 12982, 420, 2035, 281, 3164, 337, 1589, 293, 550, 2864, 281, 264, 4009, 13, 50750], "temperature": 0.0, "avg_logprob": -0.15396899738531003, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.00037357237306423485}, {"id": 209, "seek": 97868, "start": 986.4, "end": 991.9599999999999, "text": " They're not good at all for reasoning, planning, or even for arithmetic.", "tokens": [50750, 814, 434, 406, 665, 412, 439, 337, 21577, 11, 5038, 11, 420, 754, 337, 42973, 13, 51028], "temperature": 0.0, "avg_logprob": -0.15396899738531003, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.00037357237306423485}, {"id": 210, "seek": 97868, "start": 991.9599999999999, "end": 999.4399999999999, "text": " So but we are easily fooled by their language fluency into thinking that they are intelligent.", "tokens": [51028, 407, 457, 321, 366, 3612, 33372, 538, 641, 2856, 5029, 3020, 666, 1953, 300, 436, 366, 13232, 13, 51402], "temperature": 0.0, "avg_logprob": -0.15396899738531003, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.00037357237306423485}, {"id": 211, "seek": 97868, "start": 999.4399999999999, "end": 1002.5999999999999, "text": " They're not that intelligent.", "tokens": [51402, 814, 434, 406, 300, 13232, 13, 51560], "temperature": 0.0, "avg_logprob": -0.15396899738531003, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.00037357237306423485}, {"id": 212, "seek": 97868, "start": 1002.5999999999999, "end": 1006.4399999999999, "text": " And they really have no understanding of the physical world because they're trained with", "tokens": [51560, 400, 436, 534, 362, 572, 3701, 295, 264, 4001, 1002, 570, 436, 434, 8895, 365, 51752], "temperature": 0.0, "avg_logprob": -0.15396899738531003, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.00037357237306423485}, {"id": 213, "seek": 97868, "start": 1006.4399999999999, "end": 1007.4399999999999, "text": " text.", "tokens": [51752, 2487, 13, 51802], "temperature": 0.0, "avg_logprob": -0.15396899738531003, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.00037357237306423485}, {"id": 214, "seek": 100744, "start": 1007.72, "end": 1011.2, "text": " And there's another flaw, which is a huge problem.", "tokens": [50378, 400, 456, 311, 1071, 13717, 11, 597, 307, 257, 2603, 1154, 13, 50552], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 215, "seek": 100744, "start": 1011.2, "end": 1016.8800000000001, "text": " It's the fact that if you imagine that there is the set of all possible answers represented", "tokens": [50552, 467, 311, 264, 1186, 300, 498, 291, 3811, 300, 456, 307, 264, 992, 295, 439, 1944, 6338, 10379, 50836], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 216, "seek": 100744, "start": 1016.8800000000001, "end": 1022.08, "text": " by this sphere, disk, which is really a tree, right?", "tokens": [50836, 538, 341, 16687, 11, 12355, 11, 597, 307, 534, 257, 4230, 11, 558, 30, 51096], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 217, "seek": 100744, "start": 1022.08, "end": 1025.0800000000002, "text": " Every token you add put, you have a certain number of options for what the token should", "tokens": [51096, 2048, 14862, 291, 909, 829, 11, 291, 362, 257, 1629, 1230, 295, 3956, 337, 437, 264, 14862, 820, 51246], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 218, "seek": 100744, "start": 1025.0800000000002, "end": 1026.68, "text": " be, what the word is.", "tokens": [51246, 312, 11, 437, 264, 1349, 307, 13, 51326], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 219, "seek": 100744, "start": 1026.68, "end": 1028.48, "text": " So it's a tree of all possible answers.", "tokens": [51326, 407, 309, 311, 257, 4230, 295, 439, 1944, 6338, 13, 51416], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 220, "seek": 100744, "start": 1028.48, "end": 1033.6000000000001, "text": " Within this tree, there is a small subtree that corresponds to correct answers for the", "tokens": [51416, 15996, 341, 4230, 11, 456, 307, 257, 1359, 7257, 701, 300, 23249, 281, 3006, 6338, 337, 264, 51672], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 221, "seek": 100744, "start": 1033.6000000000001, "end": 1036.1200000000001, "text": " question being asked.", "tokens": [51672, 1168, 885, 2351, 13, 51798], "temperature": 0.0, "avg_logprob": -0.16003220768298132, "compression_ratio": 1.739463601532567, "no_speech_prob": 0.000429676118073985}, {"id": 222, "seek": 103612, "start": 1036.12, "end": 1042.6799999999998, "text": " And imagine that there is a probability E for any token that is produced by the system", "tokens": [50364, 400, 3811, 300, 456, 307, 257, 8482, 462, 337, 604, 14862, 300, 307, 7126, 538, 264, 1185, 50692], "temperature": 0.0, "avg_logprob": -0.1300241217321279, "compression_ratio": 1.7882882882882882, "no_speech_prob": 3.647567791631445e-05}, {"id": 223, "seek": 103612, "start": 1042.6799999999998, "end": 1046.28, "text": " to be outside, to take you outside that tree of correct answers.", "tokens": [50692, 281, 312, 2380, 11, 281, 747, 291, 2380, 300, 4230, 295, 3006, 6338, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1300241217321279, "compression_ratio": 1.7882882882882882, "no_speech_prob": 3.647567791631445e-05}, {"id": 224, "seek": 103612, "start": 1046.28, "end": 1049.1999999999998, "text": " Once you go outside that tree, you can't come back because it's a tree.", "tokens": [50872, 3443, 291, 352, 2380, 300, 4230, 11, 291, 393, 380, 808, 646, 570, 309, 311, 257, 4230, 13, 51018], "temperature": 0.0, "avg_logprob": -0.1300241217321279, "compression_ratio": 1.7882882882882882, "no_speech_prob": 3.647567791631445e-05}, {"id": 225, "seek": 103612, "start": 1049.1999999999998, "end": 1057.4799999999998, "text": " So let's imagine that the probability per token is E. So the probability that a sequence", "tokens": [51018, 407, 718, 311, 3811, 300, 264, 8482, 680, 14862, 307, 462, 13, 407, 264, 8482, 300, 257, 8310, 51432], "temperature": 0.0, "avg_logprob": -0.1300241217321279, "compression_ratio": 1.7882882882882882, "no_speech_prob": 3.647567791631445e-05}, {"id": 226, "seek": 103612, "start": 1057.4799999999998, "end": 1061.3999999999999, "text": " of N tokens would be correct is 1 minus E to the power N, making the assumption that", "tokens": [51432, 295, 426, 22667, 576, 312, 3006, 307, 502, 3175, 462, 281, 264, 1347, 426, 11, 1455, 264, 15302, 300, 51628], "temperature": 0.0, "avg_logprob": -0.1300241217321279, "compression_ratio": 1.7882882882882882, "no_speech_prob": 3.647567791631445e-05}, {"id": 227, "seek": 106140, "start": 1061.4, "end": 1066.92, "text": " the errors are independent, which of course they're not, but that's kind of a crude assumption.", "tokens": [50364, 264, 13603, 366, 6695, 11, 597, 295, 1164, 436, 434, 406, 11, 457, 300, 311, 733, 295, 257, 30796, 15302, 13, 50640], "temperature": 0.0, "avg_logprob": -0.17578716867977812, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.0003919494920410216}, {"id": 228, "seek": 106140, "start": 1066.92, "end": 1071.3200000000002, "text": " And so the problem with this is that it's an exponentially divergent process, this", "tokens": [50640, 400, 370, 264, 1154, 365, 341, 307, 300, 309, 311, 364, 37330, 18558, 6930, 1399, 11, 341, 50860], "temperature": 0.0, "avg_logprob": -0.17578716867977812, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.0003919494920410216}, {"id": 229, "seek": 106140, "start": 1071.3200000000002, "end": 1075.48, "text": " autoregressive prediction, errors accumulate.", "tokens": [50860, 1476, 418, 3091, 488, 17630, 11, 13603, 33384, 13, 51068], "temperature": 0.0, "avg_logprob": -0.17578716867977812, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.0003919494920410216}, {"id": 230, "seek": 106140, "start": 1075.48, "end": 1081.0, "text": " And if you produce too many tokens, the thing will sort of diverge away from the set of", "tokens": [51068, 400, 498, 291, 5258, 886, 867, 22667, 11, 264, 551, 486, 1333, 295, 18558, 432, 1314, 490, 264, 992, 295, 51344], "temperature": 0.0, "avg_logprob": -0.17578716867977812, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.0003919494920410216}, {"id": 231, "seek": 106140, "start": 1081.0, "end": 1082.5600000000002, "text": " correct answers, exponentially.", "tokens": [51344, 3006, 6338, 11, 37330, 13, 51422], "temperature": 0.0, "avg_logprob": -0.17578716867977812, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.0003919494920410216}, {"id": 232, "seek": 106140, "start": 1082.5600000000002, "end": 1086.8000000000002, "text": " And that's not fixable with the current architecture.", "tokens": [51422, 400, 300, 311, 406, 3191, 712, 365, 264, 2190, 9482, 13, 51634], "temperature": 0.0, "avg_logprob": -0.17578716867977812, "compression_ratio": 1.6652719665271967, "no_speech_prob": 0.0003919494920410216}, {"id": 233, "seek": 108680, "start": 1086.8, "end": 1092.76, "text": " You can fine tune those systems a lot to reduce E, but you're not going to make it", "tokens": [50364, 509, 393, 2489, 10864, 729, 3652, 257, 688, 281, 5407, 462, 11, 457, 291, 434, 406, 516, 281, 652, 309, 50662], "temperature": 0.0, "avg_logprob": -0.16746242567040454, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00038407850661315024}, {"id": 234, "seek": 108680, "start": 1092.76, "end": 1094.8799999999999, "text": " go away.", "tokens": [50662, 352, 1314, 13, 50768], "temperature": 0.0, "avg_logprob": -0.16746242567040454, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00038407850661315024}, {"id": 235, "seek": 108680, "start": 1094.8799999999999, "end": 1101.6399999999999, "text": " So I have a bold prediction, which is that the shelf life of autoregressive LLM is very", "tokens": [50768, 407, 286, 362, 257, 11928, 17630, 11, 597, 307, 300, 264, 15222, 993, 295, 1476, 418, 3091, 488, 441, 43, 44, 307, 588, 51106], "temperature": 0.0, "avg_logprob": -0.16746242567040454, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00038407850661315024}, {"id": 236, "seek": 108680, "start": 1101.6399999999999, "end": 1103.0, "text": " short.", "tokens": [51106, 2099, 13, 51174], "temperature": 0.0, "avg_logprob": -0.16746242567040454, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00038407850661315024}, {"id": 237, "seek": 108680, "start": 1103.0, "end": 1108.36, "text": " My prediction is that five years from now, nobody in their right mind would use them.", "tokens": [51174, 1222, 17630, 307, 300, 1732, 924, 490, 586, 11, 5079, 294, 641, 558, 1575, 576, 764, 552, 13, 51442], "temperature": 0.0, "avg_logprob": -0.16746242567040454, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00038407850661315024}, {"id": 238, "seek": 108680, "start": 1108.36, "end": 1112.76, "text": " So enjoy it while it lasts.", "tokens": [51442, 407, 2103, 309, 1339, 309, 20669, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16746242567040454, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.00038407850661315024}, {"id": 239, "seek": 111276, "start": 1112.76, "end": 1115.72, "text": " They'll be replaced by things that are better.", "tokens": [50364, 814, 603, 312, 10772, 538, 721, 300, 366, 1101, 13, 50512], "temperature": 0.0, "avg_logprob": -0.2219815725808615, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.07198041677474976}, {"id": 240, "seek": 111276, "start": 1115.72, "end": 1121.68, "text": " And I'll hint about directions to kind of perhaps fix up those problems.", "tokens": [50512, 400, 286, 603, 12075, 466, 11095, 281, 733, 295, 4317, 3191, 493, 729, 2740, 13, 50810], "temperature": 0.0, "avg_logprob": -0.2219815725808615, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.07198041677474976}, {"id": 241, "seek": 111276, "start": 1121.68, "end": 1129.4, "text": " So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema", "tokens": [50810, 407, 341, 307, 257, 3035, 300, 15822, 8030, 278, 11, 567, 311, 257, 29805, 11, 293, 286, 6572, 294, 264, 883, 5619, 51196], "temperature": 0.0, "avg_logprob": -0.2219815725808615, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.07198041677474976}, {"id": 242, "seek": 111276, "start": 1129.4, "end": 1134.8, "text": " magazine, which is a philosophy magazine, about the fact that a system that is purely", "tokens": [51196, 11332, 11, 597, 307, 257, 10675, 11332, 11, 466, 264, 1186, 300, 257, 1185, 300, 307, 17491, 51466], "temperature": 0.0, "avg_logprob": -0.2219815725808615, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.07198041677474976}, {"id": 243, "seek": 111276, "start": 1134.8, "end": 1141.6, "text": " trained from text, from language, cannot possibly attain human level intelligence because much", "tokens": [51466, 8895, 490, 2487, 11, 490, 2856, 11, 2644, 6264, 23766, 1952, 1496, 7599, 570, 709, 51806], "temperature": 0.0, "avg_logprob": -0.2219815725808615, "compression_ratio": 1.5622489959839359, "no_speech_prob": 0.07198041677474976}, {"id": 244, "seek": 114160, "start": 1141.6, "end": 1147.9199999999998, "text": " of what humans know is actually derived from experience of the physical world.", "tokens": [50364, 295, 437, 6255, 458, 307, 767, 18949, 490, 1752, 295, 264, 4001, 1002, 13, 50680], "temperature": 0.0, "avg_logprob": -0.13471689007499002, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.005595704074949026}, {"id": 245, "seek": 114160, "start": 1147.9199999999998, "end": 1153.52, "text": " This is true for a lot of human knowledge, but it's true certainly for almost the totality", "tokens": [50680, 639, 307, 2074, 337, 257, 688, 295, 1952, 3601, 11, 457, 309, 311, 2074, 3297, 337, 1920, 264, 1993, 1860, 50960], "temperature": 0.0, "avg_logprob": -0.13471689007499002, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.005595704074949026}, {"id": 246, "seek": 114160, "start": 1153.52, "end": 1155.76, "text": " of animal knowledge.", "tokens": [50960, 295, 5496, 3601, 13, 51072], "temperature": 0.0, "avg_logprob": -0.13471689007499002, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.005595704074949026}, {"id": 247, "seek": 114160, "start": 1155.76, "end": 1160.24, "text": " It's all about the world is no linguistic related, no language related.", "tokens": [51072, 467, 311, 439, 466, 264, 1002, 307, 572, 43002, 4077, 11, 572, 2856, 4077, 13, 51296], "temperature": 0.0, "avg_logprob": -0.13471689007499002, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.005595704074949026}, {"id": 248, "seek": 114160, "start": 1160.24, "end": 1165.9599999999998, "text": " So linguistic abilities and fluency are not related to the ability to think.", "tokens": [51296, 407, 43002, 11582, 293, 5029, 3020, 366, 406, 4077, 281, 264, 3485, 281, 519, 13, 51582], "temperature": 0.0, "avg_logprob": -0.13471689007499002, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.005595704074949026}, {"id": 249, "seek": 114160, "start": 1165.9599999999998, "end": 1170.6799999999998, "text": " Those are two different things.", "tokens": [51582, 3950, 366, 732, 819, 721, 13, 51818], "temperature": 0.0, "avg_logprob": -0.13471689007499002, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.005595704074949026}, {"id": 250, "seek": 117068, "start": 1170.68, "end": 1177.2, "text": " There are some criticisms of autoregressive LLMs from people coming from the cognitive", "tokens": [50364, 821, 366, 512, 48519, 295, 1476, 418, 3091, 488, 441, 43, 26386, 490, 561, 1348, 490, 264, 15605, 50690], "temperature": 0.0, "avg_logprob": -0.23332885667389514, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.002647646237164736}, {"id": 251, "seek": 117068, "start": 1177.2, "end": 1183.2, "text": " science realm who say like, this is not at all the way the human mind works.", "tokens": [50690, 3497, 15355, 567, 584, 411, 11, 341, 307, 406, 412, 439, 264, 636, 264, 1952, 1575, 1985, 13, 50990], "temperature": 0.0, "avg_logprob": -0.23332885667389514, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.002647646237164736}, {"id": 252, "seek": 117068, "start": 1183.2, "end": 1186.16, "text": " There is essential missing pieces.", "tokens": [50990, 821, 307, 7115, 5361, 3755, 13, 51138], "temperature": 0.0, "avg_logprob": -0.23332885667389514, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.002647646237164736}, {"id": 253, "seek": 117068, "start": 1186.16, "end": 1190.28, "text": " Other criticism for people who come from sort of more classical AI, pre-deep learning,", "tokens": [51138, 5358, 15835, 337, 561, 567, 808, 490, 1333, 295, 544, 13735, 7318, 11, 659, 12, 38422, 2539, 11, 51344], "temperature": 0.0, "avg_logprob": -0.23332885667389514, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.002647646237164736}, {"id": 254, "seek": 117068, "start": 1190.28, "end": 1196.76, "text": " they say like, you know, AI systems are supposed to be able to plan and reason, and those LLMs", "tokens": [51344, 436, 584, 411, 11, 291, 458, 11, 7318, 3652, 366, 3442, 281, 312, 1075, 281, 1393, 293, 1778, 11, 293, 729, 441, 43, 26386, 51668], "temperature": 0.0, "avg_logprob": -0.23332885667389514, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.002647646237164736}, {"id": 255, "seek": 117068, "start": 1196.76, "end": 1197.76, "text": " can do it.", "tokens": [51668, 393, 360, 309, 13, 51718], "temperature": 0.0, "avg_logprob": -0.23332885667389514, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.002647646237164736}, {"id": 256, "seek": 119776, "start": 1197.76, "end": 1202.32, "text": " Or at least not, you know, they can do it maybe in very sort of primitive forms.", "tokens": [50364, 1610, 412, 1935, 406, 11, 291, 458, 11, 436, 393, 360, 309, 1310, 294, 588, 1333, 295, 28540, 6422, 13, 50592], "temperature": 0.0, "avg_logprob": -0.15251599981429728, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0007313389796763659}, {"id": 257, "seek": 119776, "start": 1202.32, "end": 1208.04, "text": " Perhaps they can plan things in situations that correspond to a template that they've", "tokens": [50592, 10517, 436, 393, 1393, 721, 294, 6851, 300, 6805, 281, 257, 12379, 300, 436, 600, 50878], "temperature": 0.0, "avg_logprob": -0.15251599981429728, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0007313389796763659}, {"id": 258, "seek": 119776, "start": 1208.04, "end": 1212.2, "text": " been trained on, but they're not so innovative.", "tokens": [50878, 668, 8895, 322, 11, 457, 436, 434, 406, 370, 12999, 13, 51086], "temperature": 0.0, "avg_logprob": -0.15251599981429728, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0007313389796763659}, {"id": 259, "seek": 119776, "start": 1212.2, "end": 1220.24, "text": " So we should ask, how is it that humans and animals can run so quickly?", "tokens": [51086, 407, 321, 820, 1029, 11, 577, 307, 309, 300, 6255, 293, 4882, 393, 1190, 370, 2661, 30, 51488], "temperature": 0.0, "avg_logprob": -0.15251599981429728, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0007313389796763659}, {"id": 260, "seek": 119776, "start": 1220.24, "end": 1226.84, "text": " And I've been using this diagram for quite a while now, several, many years from Emmanuel", "tokens": [51488, 400, 286, 600, 668, 1228, 341, 10686, 337, 1596, 257, 1339, 586, 11, 2940, 11, 867, 924, 490, 44421, 51818], "temperature": 0.0, "avg_logprob": -0.15251599981429728, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0007313389796763659}, {"id": 261, "seek": 122684, "start": 1226.84, "end": 1230.6799999999998, "text": " Dupu, who's a cognitive scientist in Paris.", "tokens": [50364, 413, 1010, 84, 11, 567, 311, 257, 15605, 12662, 294, 8380, 13, 50556], "temperature": 0.0, "avg_logprob": -0.19197502876948386, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.00676950765773654}, {"id": 262, "seek": 122684, "start": 1230.6799999999998, "end": 1236.4399999999998, "text": " And we tried to sort of make a chart of at what age babies learn basic concepts about", "tokens": [50556, 400, 321, 3031, 281, 1333, 295, 652, 257, 6927, 295, 412, 437, 3205, 10917, 1466, 3875, 10392, 466, 50844], "temperature": 0.0, "avg_logprob": -0.19197502876948386, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.00676950765773654}, {"id": 263, "seek": 122684, "start": 1236.4399999999998, "end": 1243.3999999999999, "text": " the world, so things like distinguishing between animate objects and inanimate objects, learning", "tokens": [50844, 264, 1002, 11, 370, 721, 411, 11365, 3807, 1296, 36439, 6565, 293, 33113, 2905, 6565, 11, 2539, 51192], "temperature": 0.0, "avg_logprob": -0.19197502876948386, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.00676950765773654}, {"id": 264, "seek": 122684, "start": 1243.3999999999999, "end": 1246.56, "text": " the notion of object permanence, the fact that when an object is hidden behind another", "tokens": [51192, 264, 10710, 295, 2657, 8105, 655, 11, 264, 1186, 300, 562, 364, 2657, 307, 7633, 2261, 1071, 51350], "temperature": 0.0, "avg_logprob": -0.19197502876948386, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.00676950765773654}, {"id": 265, "seek": 122684, "start": 1246.56, "end": 1249.76, "text": " one, it still exists.", "tokens": [51350, 472, 11, 309, 920, 8198, 13, 51510], "temperature": 0.0, "avg_logprob": -0.19197502876948386, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.00676950765773654}, {"id": 266, "seek": 122684, "start": 1249.76, "end": 1254.4399999999998, "text": " Notion of rigidity, solidity, things like natural categories, babies don't need to", "tokens": [51510, 1726, 313, 295, 8329, 17711, 11, 5100, 507, 11, 721, 411, 3303, 10479, 11, 10917, 500, 380, 643, 281, 51744], "temperature": 0.0, "avg_logprob": -0.19197502876948386, "compression_ratio": 1.6991869918699187, "no_speech_prob": 0.00676950765773654}, {"id": 267, "seek": 125444, "start": 1254.44, "end": 1258.68, "text": " know the name of an object to actually know that there are different categories of objects", "tokens": [50364, 458, 264, 1315, 295, 364, 2657, 281, 767, 458, 300, 456, 366, 819, 10479, 295, 6565, 50576], "temperature": 0.0, "avg_logprob": -0.15921734497610446, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.0012795546790584922}, {"id": 268, "seek": 125444, "start": 1258.68, "end": 1260.24, "text": " around four months or so.", "tokens": [50576, 926, 1451, 2493, 420, 370, 13, 50654], "temperature": 0.0, "avg_logprob": -0.15921734497610446, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.0012795546790584922}, {"id": 269, "seek": 125444, "start": 1260.24, "end": 1264.8400000000001, "text": " And then it takes about nine months for babies to really understand that sort of intuitive", "tokens": [50654, 400, 550, 309, 2516, 466, 4949, 2493, 337, 10917, 281, 534, 1223, 300, 1333, 295, 21769, 50884], "temperature": 0.0, "avg_logprob": -0.15921734497610446, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.0012795546790584922}, {"id": 270, "seek": 125444, "start": 1264.8400000000001, "end": 1271.3200000000002, "text": " physics that objects that are not supported will fall, that, you know, objects have a", "tokens": [50884, 10649, 300, 6565, 300, 366, 406, 8104, 486, 2100, 11, 300, 11, 291, 458, 11, 6565, 362, 257, 51208], "temperature": 0.0, "avg_logprob": -0.15921734497610446, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.0012795546790584922}, {"id": 271, "seek": 125444, "start": 1271.3200000000002, "end": 1279.56, "text": " momentum, weight, friction, you know, knowing that if I push on this object, you know, light", "tokens": [51208, 11244, 11, 3364, 11, 17710, 11, 291, 458, 11, 5276, 300, 498, 286, 2944, 322, 341, 2657, 11, 291, 458, 11, 1442, 51620], "temperature": 0.0, "avg_logprob": -0.15921734497610446, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.0012795546790584922}, {"id": 272, "seek": 125444, "start": 1279.56, "end": 1284.1200000000001, "text": " objects like this, they're going to move, but if I push on an object that's heavier,", "tokens": [51620, 6565, 411, 341, 11, 436, 434, 516, 281, 1286, 11, 457, 498, 286, 2944, 322, 364, 2657, 300, 311, 18279, 11, 51848], "temperature": 0.0, "avg_logprob": -0.15921734497610446, "compression_ratio": 1.8255813953488371, "no_speech_prob": 0.0012795546790584922}, {"id": 273, "seek": 128412, "start": 1284.12, "end": 1286.32, "text": " it's not going to move unless I push harder.", "tokens": [50364, 309, 311, 406, 516, 281, 1286, 5969, 286, 2944, 6081, 13, 50474], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 274, "seek": 128412, "start": 1286.32, "end": 1287.32, "text": " So things like that.", "tokens": [50474, 407, 721, 411, 300, 13, 50524], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 275, "seek": 128412, "start": 1287.32, "end": 1291.8799999999999, "text": " So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where", "tokens": [50524, 407, 498, 291, 855, 257, 2309, 12, 23534, 12, 2641, 3186, 420, 257, 1732, 12, 23534, 12, 2641, 3186, 11, 264, 9005, 510, 322, 264, 1411, 689, 50752], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 276, "seek": 128412, "start": 1291.8799999999999, "end": 1296.1599999999999, "text": " you have a little car on the platform, you push the car off the platform, it appears", "tokens": [50752, 291, 362, 257, 707, 1032, 322, 264, 3663, 11, 291, 2944, 264, 1032, 766, 264, 3663, 11, 309, 7038, 50966], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 277, "seek": 128412, "start": 1296.1599999999999, "end": 1302.0, "text": " to float in the air, a five-month-old baby will pay attention, a 10-month-old baby will", "tokens": [50966, 281, 15706, 294, 264, 1988, 11, 257, 1732, 12, 23534, 12, 2641, 3186, 486, 1689, 3202, 11, 257, 1266, 12, 23534, 12, 2641, 3186, 486, 51258], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 278, "seek": 128412, "start": 1302.0, "end": 1308.4399999999998, "text": " go like this because she understood that by then that objects are not supported or supposed", "tokens": [51258, 352, 411, 341, 570, 750, 7320, 300, 538, 550, 300, 6565, 366, 406, 8104, 420, 3442, 51580], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 279, "seek": 128412, "start": 1308.4399999999998, "end": 1312.36, "text": " to fall and this object appears to be floating in the air.", "tokens": [51580, 281, 2100, 293, 341, 2657, 7038, 281, 312, 12607, 294, 264, 1988, 13, 51776], "temperature": 0.0, "avg_logprob": -0.16984919345740115, "compression_ratio": 1.90234375, "no_speech_prob": 0.010807128623127937}, {"id": 280, "seek": 131236, "start": 1312.36, "end": 1319.1999999999998, "text": " So we can determine that her mental model of the world is being violated, okay?", "tokens": [50364, 407, 321, 393, 6997, 300, 720, 4973, 2316, 295, 264, 1002, 307, 885, 33239, 11, 1392, 30, 50706], "temperature": 0.0, "avg_logprob": -0.21255385738679733, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.00010550281149335206}, {"id": 281, "seek": 131236, "start": 1319.1999999999998, "end": 1323.08, "text": " That's how this chart was built.", "tokens": [50706, 663, 311, 577, 341, 6927, 390, 3094, 13, 50900], "temperature": 0.0, "avg_logprob": -0.21255385738679733, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.00010550281149335206}, {"id": 282, "seek": 131236, "start": 1323.08, "end": 1327.6799999999998, "text": " So we accumulate as babies an enormous amount of background knowledge about how the world", "tokens": [50900, 407, 321, 33384, 382, 10917, 364, 11322, 2372, 295, 3678, 3601, 466, 577, 264, 1002, 51130], "temperature": 0.0, "avg_logprob": -0.21255385738679733, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.00010550281149335206}, {"id": 283, "seek": 131236, "start": 1327.6799999999998, "end": 1331.6799999999998, "text": " works, mostly by observation, a little bit by interaction, when we start being able to", "tokens": [51130, 1985, 11, 5240, 538, 14816, 11, 257, 707, 857, 538, 9285, 11, 562, 321, 722, 885, 1075, 281, 51330], "temperature": 0.0, "avg_logprob": -0.21255385738679733, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.00010550281149335206}, {"id": 284, "seek": 131236, "start": 1331.6799999999998, "end": 1337.6, "text": " kind of grab things, but in the first few months it's mostly just observation.", "tokens": [51330, 733, 295, 4444, 721, 11, 457, 294, 264, 700, 1326, 2493, 309, 311, 5240, 445, 14816, 13, 51626], "temperature": 0.0, "avg_logprob": -0.21255385738679733, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.00010550281149335206}, {"id": 285, "seek": 133760, "start": 1337.6, "end": 1343.6, "text": " So we don't know how to reproduce this with this type of learning with machines.", "tokens": [50364, 407, 321, 500, 380, 458, 577, 281, 29501, 341, 365, 341, 2010, 295, 2539, 365, 8379, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11874809070509307, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0006560751353390515}, {"id": 286, "seek": 133760, "start": 1343.6, "end": 1349.8799999999999, "text": " Once we accumulate all this background knowledge, you know, in a number of years, learning a", "tokens": [50664, 3443, 321, 33384, 439, 341, 3678, 3601, 11, 291, 458, 11, 294, 257, 1230, 295, 924, 11, 2539, 257, 50978], "temperature": 0.0, "avg_logprob": -0.11874809070509307, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0006560751353390515}, {"id": 287, "seek": 133760, "start": 1349.8799999999999, "end": 1353.1599999999999, "text": " new task like driving is very fast.", "tokens": [50978, 777, 5633, 411, 4840, 307, 588, 2370, 13, 51142], "temperature": 0.0, "avg_logprob": -0.11874809070509307, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0006560751353390515}, {"id": 288, "seek": 133760, "start": 1353.1599999999999, "end": 1358.6, "text": " So any teenager can learn to drive in about 20 hours of practice, mostly without causing", "tokens": [51142, 407, 604, 21440, 393, 1466, 281, 3332, 294, 466, 945, 2496, 295, 3124, 11, 5240, 1553, 9853, 51414], "temperature": 0.0, "avg_logprob": -0.11874809070509307, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0006560751353390515}, {"id": 289, "seek": 133760, "start": 1358.6, "end": 1361.32, "text": " any accident.", "tokens": [51414, 604, 6398, 13, 51550], "temperature": 0.0, "avg_logprob": -0.11874809070509307, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0006560751353390515}, {"id": 290, "seek": 133760, "start": 1361.32, "end": 1364.9199999999998, "text": " So the teenager doesn't have to run off a cliff to figure out that the car, that nothing", "tokens": [51550, 407, 264, 21440, 1177, 380, 362, 281, 1190, 766, 257, 22316, 281, 2573, 484, 300, 264, 1032, 11, 300, 1825, 51730], "temperature": 0.0, "avg_logprob": -0.11874809070509307, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0006560751353390515}, {"id": 291, "seek": 136492, "start": 1364.92, "end": 1368.0800000000002, "text": " good is going to happen if you run off a cliff.", "tokens": [50364, 665, 307, 516, 281, 1051, 498, 291, 1190, 766, 257, 22316, 13, 50522], "temperature": 0.0, "avg_logprob": -0.1868574264201712, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0026719567831605673}, {"id": 292, "seek": 136492, "start": 1368.0800000000002, "end": 1373.0, "text": " The mental model of the world is already there, okay?", "tokens": [50522, 440, 4973, 2316, 295, 264, 1002, 307, 1217, 456, 11, 1392, 30, 50768], "temperature": 0.0, "avg_logprob": -0.1868574264201712, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0026719567831605673}, {"id": 293, "seek": 136492, "start": 1373.0, "end": 1376.48, "text": " We still won't have level five salivating cars.", "tokens": [50768, 492, 920, 1582, 380, 362, 1496, 1732, 1845, 592, 990, 5163, 13, 50942], "temperature": 0.0, "avg_logprob": -0.1868574264201712, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0026719567831605673}, {"id": 294, "seek": 136492, "start": 1376.48, "end": 1381.0, "text": " So obviously we're missing something pretty big.", "tokens": [50942, 407, 2745, 321, 434, 5361, 746, 1238, 955, 13, 51168], "temperature": 0.0, "avg_logprob": -0.1868574264201712, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0026719567831605673}, {"id": 295, "seek": 136492, "start": 1381.0, "end": 1385.52, "text": " Any 10-year-old can clear up the dinner table and fill up the dishwasher.", "tokens": [51168, 2639, 1266, 12, 5294, 12, 2641, 393, 1850, 493, 264, 6148, 3199, 293, 2836, 493, 264, 38009, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1868574264201712, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0026719567831605673}, {"id": 296, "seek": 136492, "start": 1385.52, "end": 1388.48, "text": " We're nowhere near having robots that can do this and it's not because of mechanical", "tokens": [51394, 492, 434, 11159, 2651, 1419, 14733, 300, 393, 360, 341, 293, 309, 311, 406, 570, 295, 12070, 51542], "temperature": 0.0, "avg_logprob": -0.1868574264201712, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.0026719567831605673}, {"id": 297, "seek": 138848, "start": 1388.48, "end": 1396.1200000000001, "text": " design, it's because we don't know how to build the minds behind it.", "tokens": [50364, 1715, 11, 309, 311, 570, 321, 500, 380, 458, 577, 281, 1322, 264, 9634, 2261, 309, 13, 50746], "temperature": 0.0, "avg_logprob": -0.17557108111497832, "compression_ratio": 1.4734299516908214, "no_speech_prob": 0.003591432934626937}, {"id": 298, "seek": 138848, "start": 1396.1200000000001, "end": 1398.6, "text": " So we're missing something big, right?", "tokens": [50746, 407, 321, 434, 5361, 746, 955, 11, 558, 30, 50870], "temperature": 0.0, "avg_logprob": -0.17557108111497832, "compression_ratio": 1.4734299516908214, "no_speech_prob": 0.003591432934626937}, {"id": 299, "seek": 138848, "start": 1398.6, "end": 1404.48, "text": " The past towards human-level AI is not just making LLMs bigger, that's just not going", "tokens": [50870, 440, 1791, 3030, 1952, 12, 12418, 7318, 307, 406, 445, 1455, 441, 43, 26386, 3801, 11, 300, 311, 445, 406, 516, 51164], "temperature": 0.0, "avg_logprob": -0.17557108111497832, "compression_ratio": 1.4734299516908214, "no_speech_prob": 0.003591432934626937}, {"id": 300, "seek": 138848, "start": 1404.48, "end": 1406.64, "text": " to get us there.", "tokens": [51164, 281, 483, 505, 456, 13, 51272], "temperature": 0.0, "avg_logprob": -0.17557108111497832, "compression_ratio": 1.4734299516908214, "no_speech_prob": 0.003591432934626937}, {"id": 301, "seek": 138848, "start": 1406.64, "end": 1413.72, "text": " It's been a common recurring error by AI scientists and engineers over the last six decades to", "tokens": [51272, 467, 311, 668, 257, 2689, 32279, 6713, 538, 7318, 7708, 293, 11955, 670, 264, 1036, 2309, 7878, 281, 51626], "temperature": 0.0, "avg_logprob": -0.17557108111497832, "compression_ratio": 1.4734299516908214, "no_speech_prob": 0.003591432934626937}, {"id": 302, "seek": 141372, "start": 1413.72, "end": 1419.96, "text": " imagine that the one thing that they just discovered was the solution to human-level", "tokens": [50364, 3811, 300, 264, 472, 551, 300, 436, 445, 6941, 390, 264, 3827, 281, 1952, 12, 12418, 50676], "temperature": 0.0, "avg_logprob": -0.17360875939810147, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.001726025016978383}, {"id": 303, "seek": 141372, "start": 1419.96, "end": 1424.76, "text": " AI, only to discover a few years later that no, there was actually a big obstacle, another", "tokens": [50676, 7318, 11, 787, 281, 4411, 257, 1326, 924, 1780, 300, 572, 11, 456, 390, 767, 257, 955, 23112, 11, 1071, 50916], "temperature": 0.0, "avg_logprob": -0.17360875939810147, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.001726025016978383}, {"id": 304, "seek": 141372, "start": 1424.76, "end": 1426.56, "text": " obstacle they had to clear.", "tokens": [50916, 23112, 436, 632, 281, 1850, 13, 51006], "temperature": 0.0, "avg_logprob": -0.17360875939810147, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.001726025016978383}, {"id": 305, "seek": 141372, "start": 1426.56, "end": 1431.24, "text": " It's a recurring history story in AI.", "tokens": [51006, 467, 311, 257, 32279, 2503, 1657, 294, 7318, 13, 51240], "temperature": 0.0, "avg_logprob": -0.17360875939810147, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.001726025016978383}, {"id": 306, "seek": 141372, "start": 1431.24, "end": 1436.0, "text": " So common sense will probably emerge from the ability of machines to learn how the world", "tokens": [51240, 407, 2689, 2020, 486, 1391, 21511, 490, 264, 3485, 295, 8379, 281, 1466, 577, 264, 1002, 51478], "temperature": 0.0, "avg_logprob": -0.17360875939810147, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.001726025016978383}, {"id": 307, "seek": 141372, "start": 1436.0, "end": 1441.1200000000001, "text": " works by observation, the way babies and animals do it.", "tokens": [51478, 1985, 538, 14816, 11, 264, 636, 10917, 293, 4882, 360, 309, 13, 51734], "temperature": 0.0, "avg_logprob": -0.17360875939810147, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.001726025016978383}, {"id": 308, "seek": 144112, "start": 1441.12, "end": 1445.8, "text": " So I see three challenges for AI research over the next decade also, learning representations", "tokens": [50364, 407, 286, 536, 1045, 4759, 337, 7318, 2132, 670, 264, 958, 10378, 611, 11, 2539, 33358, 50598], "temperature": 0.0, "avg_logprob": -0.14590517003485498, "compression_ratio": 1.6639344262295082, "no_speech_prob": 0.004563231021165848}, {"id": 309, "seek": 144112, "start": 1445.8, "end": 1450.1599999999999, "text": " of the world and predictive models of the world, I'll say why in a minute, and self-supervised", "tokens": [50598, 295, 264, 1002, 293, 35521, 5245, 295, 264, 1002, 11, 286, 603, 584, 983, 294, 257, 3456, 11, 293, 2698, 12, 48172, 24420, 50816], "temperature": 0.0, "avg_logprob": -0.14590517003485498, "compression_ratio": 1.6639344262295082, "no_speech_prob": 0.004563231021165848}, {"id": 310, "seek": 144112, "start": 1450.1599999999999, "end": 1456.12, "text": " learning is going to be the key component of that, learning to reason.", "tokens": [50816, 2539, 307, 516, 281, 312, 264, 2141, 6542, 295, 300, 11, 2539, 281, 1778, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14590517003485498, "compression_ratio": 1.6639344262295082, "no_speech_prob": 0.004563231021165848}, {"id": 311, "seek": 144112, "start": 1456.12, "end": 1459.84, "text": " So psychologists talk about system one and system two.", "tokens": [51114, 407, 41562, 751, 466, 1185, 472, 293, 1185, 732, 13, 51300], "temperature": 0.0, "avg_logprob": -0.14590517003485498, "compression_ratio": 1.6639344262295082, "no_speech_prob": 0.004563231021165848}, {"id": 312, "seek": 144112, "start": 1459.84, "end": 1468.3999999999999, "text": " System one is the type of control that our brains use to kind of react to something without", "tokens": [51300, 8910, 472, 307, 264, 2010, 295, 1969, 300, 527, 15442, 764, 281, 733, 295, 4515, 281, 746, 1553, 51728], "temperature": 0.0, "avg_logprob": -0.14590517003485498, "compression_ratio": 1.6639344262295082, "no_speech_prob": 0.004563231021165848}, {"id": 313, "seek": 146840, "start": 1468.4, "end": 1471.76, "text": " really having to think about it, like subconscious action.", "tokens": [50364, 534, 1419, 281, 519, 466, 309, 11, 411, 27389, 3069, 13, 50532], "temperature": 0.0, "avg_logprob": -0.16080694198608397, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.058173857629299164}, {"id": 314, "seek": 146840, "start": 1471.76, "end": 1476.1200000000001, "text": " So if you're an experienced driver, you don't have to think about driving, you can just drive", "tokens": [50532, 407, 498, 291, 434, 364, 6751, 6787, 11, 291, 500, 380, 362, 281, 519, 466, 4840, 11, 291, 393, 445, 3332, 50750], "temperature": 0.0, "avg_logprob": -0.16080694198608397, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.058173857629299164}, {"id": 315, "seek": 146840, "start": 1476.1200000000001, "end": 1481.24, "text": " and you can talk to someone at the same time and barely pay attention.", "tokens": [50750, 293, 291, 393, 751, 281, 1580, 412, 264, 912, 565, 293, 10268, 1689, 3202, 13, 51006], "temperature": 0.0, "avg_logprob": -0.16080694198608397, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.058173857629299164}, {"id": 316, "seek": 146840, "start": 1481.24, "end": 1482.24, "text": " So that's system one.", "tokens": [51006, 407, 300, 311, 1185, 472, 13, 51056], "temperature": 0.0, "avg_logprob": -0.16080694198608397, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.058173857629299164}, {"id": 317, "seek": 146840, "start": 1482.24, "end": 1486.4, "text": " But then when you are learning to drive, you pay attention to absolutely everything.", "tokens": [51056, 583, 550, 562, 291, 366, 2539, 281, 3332, 11, 291, 1689, 3202, 281, 3122, 1203, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16080694198608397, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.058173857629299164}, {"id": 318, "seek": 146840, "start": 1486.4, "end": 1496.92, "text": " You use your entire focus, consciousness, attention to drive and that's system two.", "tokens": [51264, 509, 764, 428, 2302, 1879, 11, 10081, 11, 3202, 281, 3332, 293, 300, 311, 1185, 732, 13, 51790], "temperature": 0.0, "avg_logprob": -0.16080694198608397, "compression_ratio": 1.8237885462555066, "no_speech_prob": 0.058173857629299164}, {"id": 319, "seek": 149692, "start": 1496.92, "end": 1501.4, "text": " And then the last thing is learning to plan complex action sequences, decomposing them", "tokens": [50364, 400, 550, 264, 1036, 551, 307, 2539, 281, 1393, 3997, 3069, 22978, 11, 22867, 6110, 552, 50588], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 320, "seek": 149692, "start": 1501.4, "end": 1502.4, "text": " into simpler ones.", "tokens": [50588, 666, 18587, 2306, 13, 50638], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 321, "seek": 149692, "start": 1502.4, "end": 1513.4, "text": " So I wrote a sort of vision paper about a year ago, which I posted on open review for", "tokens": [50638, 407, 286, 4114, 257, 1333, 295, 5201, 3035, 466, 257, 1064, 2057, 11, 597, 286, 9437, 322, 1269, 3131, 337, 51188], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 322, "seek": 149692, "start": 1513.4, "end": 1516.1200000000001, "text": " comments, so you're welcome to comment on it.", "tokens": [51188, 3053, 11, 370, 291, 434, 2928, 281, 2871, 322, 309, 13, 51324], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 323, "seek": 149692, "start": 1516.1200000000001, "end": 1520.88, "text": " I give a bunch of technical talks about it, one of the earliest one was at Berkeley, but", "tokens": [51324, 286, 976, 257, 3840, 295, 6191, 6686, 466, 309, 11, 472, 295, 264, 20573, 472, 390, 412, 23684, 11, 457, 51562], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 324, "seek": 149692, "start": 1520.88, "end": 1524.68, "text": " you are having a more recent version of it right now, so you don't need to look at that", "tokens": [51562, 291, 366, 1419, 257, 544, 5162, 3037, 295, 309, 558, 586, 11, 370, 291, 500, 380, 643, 281, 574, 412, 300, 51752], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 325, "seek": 149692, "start": 1524.68, "end": 1526.8000000000002, "text": " one.", "tokens": [51752, 472, 13, 51858], "temperature": 0.0, "avg_logprob": -0.19422161795876242, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.07818510383367538}, {"id": 326, "seek": 152680, "start": 1526.8, "end": 1532.3999999999999, "text": " And it's based on what's called a cognitive architecture.", "tokens": [50364, 400, 309, 311, 2361, 322, 437, 311, 1219, 257, 15605, 9482, 13, 50644], "temperature": 0.0, "avg_logprob": -0.19085915509392234, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00043022798490710557}, {"id": 327, "seek": 152680, "start": 1532.3999999999999, "end": 1538.24, "text": " So basically how can we sort of design a system with different modules so that those modules", "tokens": [50644, 407, 1936, 577, 393, 321, 1333, 295, 1715, 257, 1185, 365, 819, 16679, 370, 300, 729, 16679, 50936], "temperature": 0.0, "avg_logprob": -0.19085915509392234, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00043022798490710557}, {"id": 328, "seek": 152680, "start": 1538.24, "end": 1546.6399999999999, "text": " may implement all the properties that I was telling you about so systems can perceive,", "tokens": [50936, 815, 4445, 439, 264, 7221, 300, 286, 390, 3585, 291, 466, 370, 3652, 393, 20281, 11, 51356], "temperature": 0.0, "avg_logprob": -0.19085915509392234, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00043022798490710557}, {"id": 329, "seek": 152680, "start": 1546.6399999999999, "end": 1551.6, "text": " reason, predict, in particular predict the consequences of their actions and then plan", "tokens": [51356, 1778, 11, 6069, 11, 294, 1729, 6069, 264, 10098, 295, 641, 5909, 293, 550, 1393, 51604], "temperature": 0.0, "avg_logprob": -0.19085915509392234, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00043022798490710557}, {"id": 330, "seek": 152680, "start": 1551.6, "end": 1556.56, "text": " a sequence of actions to optimize, to satisfy a particular objective.", "tokens": [51604, 257, 8310, 295, 5909, 281, 19719, 11, 281, 19319, 257, 1729, 10024, 13, 51852], "temperature": 0.0, "avg_logprob": -0.19085915509392234, "compression_ratio": 1.6485355648535565, "no_speech_prob": 0.00043022798490710557}, {"id": 331, "seek": 155656, "start": 1556.56, "end": 1564.72, "text": " So the main components of the system is the key component, I would say, is the world model", "tokens": [50364, 407, 264, 2135, 6677, 295, 264, 1185, 307, 264, 2141, 6542, 11, 286, 576, 584, 11, 307, 264, 1002, 2316, 50772], "temperature": 0.0, "avg_logprob": -0.1491690120477786, "compression_ratio": 1.7609756097560976, "no_speech_prob": 0.0009686693083494902}, {"id": 332, "seek": 155656, "start": 1564.72, "end": 1573.76, "text": " and the world model is what allows the system to predict ahead, imagine what's gonna happen.", "tokens": [50772, 293, 264, 1002, 2316, 307, 437, 4045, 264, 1185, 281, 6069, 2286, 11, 3811, 437, 311, 799, 1051, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1491690120477786, "compression_ratio": 1.7609756097560976, "no_speech_prob": 0.0009686693083494902}, {"id": 333, "seek": 155656, "start": 1573.76, "end": 1578.8, "text": " This is to some extent what current AI systems don't really have.", "tokens": [51224, 639, 307, 281, 512, 8396, 437, 2190, 7318, 3652, 500, 380, 534, 362, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1491690120477786, "compression_ratio": 1.7609756097560976, "no_speech_prob": 0.0009686693083494902}, {"id": 334, "seek": 155656, "start": 1578.8, "end": 1583.28, "text": " Perception system basically gets an estimate of the state of the world and initializes", "tokens": [51476, 3026, 7311, 1185, 1936, 2170, 364, 12539, 295, 264, 1785, 295, 264, 1002, 293, 5883, 5660, 51700], "temperature": 0.0, "avg_logprob": -0.1491690120477786, "compression_ratio": 1.7609756097560976, "no_speech_prob": 0.0009686693083494902}, {"id": 335, "seek": 155656, "start": 1583.28, "end": 1585.6799999999998, "text": " the world model with it.", "tokens": [51700, 264, 1002, 2316, 365, 309, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1491690120477786, "compression_ratio": 1.7609756097560976, "no_speech_prob": 0.0009686693083494902}, {"id": 336, "seek": 158568, "start": 1585.68, "end": 1591.16, "text": " The cost here is a really important module, so basically the entire purpose of the agent", "tokens": [50364, 440, 2063, 510, 307, 257, 534, 1021, 10088, 11, 370, 1936, 264, 2302, 4334, 295, 264, 9461, 50638], "temperature": 0.0, "avg_logprob": -0.15157993980076, "compression_ratio": 1.807511737089202, "no_speech_prob": 4.1331972170155495e-05}, {"id": 337, "seek": 158568, "start": 1591.16, "end": 1595.0800000000002, "text": " is to minimize this cost.", "tokens": [50638, 307, 281, 17522, 341, 2063, 13, 50834], "temperature": 0.0, "avg_logprob": -0.15157993980076, "compression_ratio": 1.807511737089202, "no_speech_prob": 4.1331972170155495e-05}, {"id": 338, "seek": 158568, "start": 1595.0800000000002, "end": 1602.8400000000001, "text": " So the cost is something that uses a measurement of the state of the agent, particularly the", "tokens": [50834, 407, 264, 2063, 307, 746, 300, 4960, 257, 13160, 295, 264, 1785, 295, 264, 9461, 11, 4098, 264, 51222], "temperature": 0.0, "avg_logprob": -0.15157993980076, "compression_ratio": 1.807511737089202, "no_speech_prob": 4.1331972170155495e-05}, {"id": 339, "seek": 158568, "start": 1602.8400000000001, "end": 1608.24, "text": " prediction from the world model, and predicts whether an act is going to be good or bad.", "tokens": [51222, 17630, 490, 264, 1002, 2316, 11, 293, 6069, 82, 1968, 364, 605, 307, 516, 281, 312, 665, 420, 1578, 13, 51492], "temperature": 0.0, "avg_logprob": -0.15157993980076, "compression_ratio": 1.807511737089202, "no_speech_prob": 4.1331972170155495e-05}, {"id": 340, "seek": 158568, "start": 1608.24, "end": 1611.88, "text": " And the entire purpose of the agent here is to figure out a sequence of actions, so this", "tokens": [51492, 400, 264, 2302, 4334, 295, 264, 9461, 510, 307, 281, 2573, 484, 257, 8310, 295, 5909, 11, 370, 341, 51674], "temperature": 0.0, "avg_logprob": -0.15157993980076, "compression_ratio": 1.807511737089202, "no_speech_prob": 4.1331972170155495e-05}, {"id": 341, "seek": 161188, "start": 1611.88, "end": 1616.92, "text": " is taking place in the actor, figure out a sequence of actions such that when I predict", "tokens": [50364, 307, 1940, 1081, 294, 264, 8747, 11, 2573, 484, 257, 8310, 295, 5909, 1270, 300, 562, 286, 6069, 50616], "temperature": 0.0, "avg_logprob": -0.23374557495117188, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.003938411828130484}, {"id": 342, "seek": 161188, "start": 1616.92, "end": 1621.5200000000002, "text": " what's gonna happen as a consequence of those actions using my world model, my objective,", "tokens": [50616, 437, 311, 799, 1051, 382, 257, 18326, 295, 729, 5909, 1228, 452, 1002, 2316, 11, 452, 10024, 11, 50846], "temperature": 0.0, "avg_logprob": -0.23374557495117188, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.003938411828130484}, {"id": 343, "seek": 161188, "start": 1621.5200000000002, "end": 1624.3200000000002, "text": " my cost function will be minimized.", "tokens": [50846, 452, 2063, 2445, 486, 312, 4464, 1602, 13, 50986], "temperature": 0.0, "avg_logprob": -0.23374557495117188, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.003938411828130484}, {"id": 344, "seek": 161188, "start": 1624.3200000000002, "end": 1631.68, "text": " So if my cost function is, so the cost function is basically our measures of discomfort of", "tokens": [50986, 407, 498, 452, 2063, 2445, 307, 11, 370, 264, 2063, 2445, 307, 1936, 527, 8000, 295, 28552, 295, 51354], "temperature": 0.0, "avg_logprob": -0.23374557495117188, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.003938411828130484}, {"id": 345, "seek": 161188, "start": 1631.68, "end": 1635.0800000000002, "text": " the agent.", "tokens": [51354, 264, 9461, 13, 51524], "temperature": 0.0, "avg_logprob": -0.23374557495117188, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.003938411828130484}, {"id": 346, "seek": 161188, "start": 1635.0800000000002, "end": 1637.96, "text": " Biological brains have things like that in the visual language, so this is the thing", "tokens": [51524, 13007, 4383, 15442, 362, 721, 411, 300, 294, 264, 5056, 2856, 11, 370, 341, 307, 264, 551, 51668], "temperature": 0.0, "avg_logprob": -0.23374557495117188, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.003938411828130484}, {"id": 347, "seek": 163796, "start": 1637.96, "end": 1643.32, "text": " that tells you when you're hungry, for example, or you're hurting.", "tokens": [50364, 300, 5112, 291, 562, 291, 434, 8067, 11, 337, 1365, 11, 420, 291, 434, 17744, 13, 50632], "temperature": 0.0, "avg_logprob": -0.13917894363403321, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.01280977576971054}, {"id": 348, "seek": 163796, "start": 1643.32, "end": 1649.8, "text": " So nature tells you you're hungry, nature doesn't tell you how to feed, you have to", "tokens": [50632, 407, 3687, 5112, 291, 291, 434, 8067, 11, 3687, 1177, 380, 980, 291, 577, 281, 3154, 11, 291, 362, 281, 50956], "temperature": 0.0, "avg_logprob": -0.13917894363403321, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.01280977576971054}, {"id": 349, "seek": 163796, "start": 1649.8, "end": 1654.52, "text": " figure that out by yourself, perhaps using your world model and your planning abilities.", "tokens": [50956, 2573, 300, 484, 538, 1803, 11, 4317, 1228, 428, 1002, 2316, 293, 428, 5038, 11582, 13, 51192], "temperature": 0.0, "avg_logprob": -0.13917894363403321, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.01280977576971054}, {"id": 350, "seek": 163796, "start": 1654.52, "end": 1658.04, "text": " So this is the same thing here, imagine this is a robot and the robot battery are kind", "tokens": [51192, 407, 341, 307, 264, 912, 551, 510, 11, 3811, 341, 307, 257, 7881, 293, 264, 7881, 5809, 366, 733, 51368], "temperature": 0.0, "avg_logprob": -0.13917894363403321, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.01280977576971054}, {"id": 351, "seek": 163796, "start": 1658.04, "end": 1663.4, "text": " of starting to get drained, so there's a cost function here that says be careful, you're", "tokens": [51368, 295, 2891, 281, 483, 37018, 11, 370, 456, 311, 257, 2063, 2445, 510, 300, 1619, 312, 5026, 11, 291, 434, 51636], "temperature": 0.0, "avg_logprob": -0.13917894363403321, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.01280977576971054}, {"id": 352, "seek": 163796, "start": 1663.4, "end": 1665.48, "text": " running out of power.", "tokens": [51636, 2614, 484, 295, 1347, 13, 51740], "temperature": 0.0, "avg_logprob": -0.13917894363403321, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.01280977576971054}, {"id": 353, "seek": 166548, "start": 1665.48, "end": 1669.96, "text": " And so the system, according to this world model, would say, well, I can recharge my", "tokens": [50364, 400, 370, 264, 1185, 11, 4650, 281, 341, 1002, 2316, 11, 576, 584, 11, 731, 11, 286, 393, 31366, 452, 50588], "temperature": 0.0, "avg_logprob": -0.19299986196118732, "compression_ratio": 1.6227272727272728, "no_speech_prob": 5.143688031239435e-05}, {"id": 354, "seek": 166548, "start": 1669.96, "end": 1673.48, "text": " battery by plugging myself into a socket.", "tokens": [50588, 5809, 538, 42975, 2059, 666, 257, 19741, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19299986196118732, "compression_ratio": 1.6227272727272728, "no_speech_prob": 5.143688031239435e-05}, {"id": 355, "seek": 166548, "start": 1673.48, "end": 1678.56, "text": " So it figures out the sequence of actions to plug itself into a socket and that will", "tokens": [50764, 407, 309, 9624, 484, 264, 8310, 295, 5909, 281, 5452, 2564, 666, 257, 19741, 293, 300, 486, 51018], "temperature": 0.0, "avg_logprob": -0.19299986196118732, "compression_ratio": 1.6227272727272728, "no_speech_prob": 5.143688031239435e-05}, {"id": 356, "seek": 166548, "start": 1678.56, "end": 1684.0, "text": " eventually minimize the cost function that just appeared.", "tokens": [51018, 4728, 17522, 264, 2063, 2445, 300, 445, 8516, 13, 51290], "temperature": 0.0, "avg_logprob": -0.19299986196118732, "compression_ratio": 1.6227272727272728, "no_speech_prob": 5.143688031239435e-05}, {"id": 357, "seek": 166548, "start": 1684.0, "end": 1691.24, "text": " So in fact, there's two ways to operate that system one is the kind of system one where", "tokens": [51290, 407, 294, 1186, 11, 456, 311, 732, 2098, 281, 9651, 300, 1185, 472, 307, 264, 733, 295, 1185, 472, 689, 51652], "temperature": 0.0, "avg_logprob": -0.19299986196118732, "compression_ratio": 1.6227272727272728, "no_speech_prob": 5.143688031239435e-05}, {"id": 358, "seek": 169124, "start": 1691.24, "end": 1695.6, "text": " the system makes an estimate of the state of the world, run this to a perception system", "tokens": [50364, 264, 1185, 1669, 364, 12539, 295, 264, 1785, 295, 264, 1002, 11, 1190, 341, 281, 257, 12860, 1185, 50582], "temperature": 0.0, "avg_logprob": -0.21630655708959548, "compression_ratio": 2.0954545454545452, "no_speech_prob": 0.0017260094173252583}, {"id": 359, "seek": 169124, "start": 1695.6, "end": 1700.1200000000001, "text": " called an encoder here, produces an estimate of the state of the world called S0 and that", "tokens": [50582, 1219, 364, 2058, 19866, 510, 11, 14725, 364, 12539, 295, 264, 1785, 295, 264, 1002, 1219, 318, 15, 293, 300, 50808], "temperature": 0.0, "avg_logprob": -0.21630655708959548, "compression_ratio": 2.0954545454545452, "no_speech_prob": 0.0017260094173252583}, {"id": 360, "seek": 169124, "start": 1700.1200000000001, "end": 1705.0, "text": " runs into a neural net called a policy network that just produces an action and the action", "tokens": [50808, 6676, 666, 257, 18161, 2533, 1219, 257, 3897, 3209, 300, 445, 14725, 364, 3069, 293, 264, 3069, 51052], "temperature": 0.0, "avg_logprob": -0.21630655708959548, "compression_ratio": 2.0954545454545452, "no_speech_prob": 0.0017260094173252583}, {"id": 361, "seek": 169124, "start": 1705.0, "end": 1707.0, "text": " goes into the world.", "tokens": [51052, 1709, 666, 264, 1002, 13, 51152], "temperature": 0.0, "avg_logprob": -0.21630655708959548, "compression_ratio": 2.0954545454545452, "no_speech_prob": 0.0017260094173252583}, {"id": 362, "seek": 169124, "start": 1707.0, "end": 1712.44, "text": " LLNs are like this, they are system one, you give them a pump, that's X, they produce", "tokens": [51152, 441, 43, 45, 82, 366, 411, 341, 11, 436, 366, 1185, 472, 11, 291, 976, 552, 257, 5889, 11, 300, 311, 1783, 11, 436, 5258, 51424], "temperature": 0.0, "avg_logprob": -0.21630655708959548, "compression_ratio": 2.0954545454545452, "no_speech_prob": 0.0017260094173252583}, {"id": 363, "seek": 169124, "start": 1712.44, "end": 1717.4, "text": " an action, that's the token they predict, that goes back into the world and the world", "tokens": [51424, 364, 3069, 11, 300, 311, 264, 14862, 436, 6069, 11, 300, 1709, 646, 666, 264, 1002, 293, 264, 1002, 51672], "temperature": 0.0, "avg_logprob": -0.21630655708959548, "compression_ratio": 2.0954545454545452, "no_speech_prob": 0.0017260094173252583}, {"id": 364, "seek": 171740, "start": 1717.4, "end": 1722.88, "text": " is very simplistic here, it's just you shift in the input.", "tokens": [50364, 307, 588, 44199, 510, 11, 309, 311, 445, 291, 5513, 294, 264, 4846, 13, 50638], "temperature": 0.0, "avg_logprob": -0.1624200293358336, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.002714211121201515}, {"id": 365, "seek": 171740, "start": 1722.88, "end": 1726.5600000000002, "text": " So no reasoning necessary, here is system two.", "tokens": [50638, 407, 572, 21577, 4818, 11, 510, 307, 1185, 732, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1624200293358336, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.002714211121201515}, {"id": 366, "seek": 171740, "start": 1726.5600000000002, "end": 1734.8400000000001, "text": " So you use the same system here and this is a sort of time-enrolled version of the system.", "tokens": [50822, 407, 291, 764, 264, 912, 1185, 510, 293, 341, 307, 257, 1333, 295, 565, 12, 268, 28850, 3037, 295, 264, 1185, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1624200293358336, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.002714211121201515}, {"id": 367, "seek": 171740, "start": 1734.8400000000001, "end": 1740.0800000000002, "text": " So we have the world model, the world model is this green module and the different instances", "tokens": [51236, 407, 321, 362, 264, 1002, 2316, 11, 264, 1002, 2316, 307, 341, 3092, 10088, 293, 264, 819, 14519, 51498], "temperature": 0.0, "avg_logprob": -0.1624200293358336, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.002714211121201515}, {"id": 368, "seek": 171740, "start": 1740.0800000000002, "end": 1744.6000000000001, "text": " of that green module are the state of the system at different time steps, so think of", "tokens": [51498, 295, 300, 3092, 10088, 366, 264, 1785, 295, 264, 1185, 412, 819, 565, 4439, 11, 370, 519, 295, 51724], "temperature": 0.0, "avg_logprob": -0.1624200293358336, "compression_ratio": 1.829268292682927, "no_speech_prob": 0.002714211121201515}, {"id": 369, "seek": 174460, "start": 1744.6, "end": 1748.82, "text": " it as like a recurrent net that you unfolded, so it's really the same module at different", "tokens": [50364, 309, 382, 411, 257, 18680, 1753, 2533, 300, 291, 17980, 292, 11, 370, 309, 311, 534, 264, 912, 10088, 412, 819, 50575], "temperature": 0.0, "avg_logprob": -0.14198621114095053, "compression_ratio": 1.8143459915611815, "no_speech_prob": 0.00029583260766230524}, {"id": 370, "seek": 174460, "start": 1748.82, "end": 1749.82, "text": " time steps.", "tokens": [50575, 565, 4439, 13, 50625], "temperature": 0.0, "avg_logprob": -0.14198621114095053, "compression_ratio": 1.8143459915611815, "no_speech_prob": 0.00029583260766230524}, {"id": 371, "seek": 174460, "start": 1749.82, "end": 1754.56, "text": " What the world model is supposed to be able to predict is given the representation of", "tokens": [50625, 708, 264, 1002, 2316, 307, 3442, 281, 312, 1075, 281, 6069, 307, 2212, 264, 10290, 295, 50862], "temperature": 0.0, "avg_logprob": -0.14198621114095053, "compression_ratio": 1.8143459915611815, "no_speech_prob": 0.00029583260766230524}, {"id": 372, "seek": 174460, "start": 1754.56, "end": 1759.6, "text": " the state of the world at time t and given an action that I'm imagining taking, what", "tokens": [50862, 264, 1785, 295, 264, 1002, 412, 565, 256, 293, 2212, 364, 3069, 300, 286, 478, 27798, 1940, 11, 437, 51114], "temperature": 0.0, "avg_logprob": -0.14198621114095053, "compression_ratio": 1.8143459915611815, "no_speech_prob": 0.00029583260766230524}, {"id": 373, "seek": 174460, "start": 1759.6, "end": 1764.12, "text": " is going to be the predicted state of the world at time t plus one.", "tokens": [51114, 307, 516, 281, 312, 264, 19147, 1785, 295, 264, 1002, 412, 565, 256, 1804, 472, 13, 51340], "temperature": 0.0, "avg_logprob": -0.14198621114095053, "compression_ratio": 1.8143459915611815, "no_speech_prob": 0.00029583260766230524}, {"id": 374, "seek": 174460, "start": 1764.12, "end": 1769.32, "text": " So I can imagine a sequence of actions that I might take, imagine the effect on the world", "tokens": [51340, 407, 286, 393, 3811, 257, 8310, 295, 5909, 300, 286, 1062, 747, 11, 3811, 264, 1802, 322, 264, 1002, 51600], "temperature": 0.0, "avg_logprob": -0.14198621114095053, "compression_ratio": 1.8143459915611815, "no_speech_prob": 0.00029583260766230524}, {"id": 375, "seek": 176932, "start": 1769.32, "end": 1774.72, "text": " using my world model and then I can plug the state of the world over this trajectory", "tokens": [50364, 1228, 452, 1002, 2316, 293, 550, 286, 393, 5452, 264, 1785, 295, 264, 1002, 670, 341, 21512, 50634], "temperature": 0.0, "avg_logprob": -0.11842065161847054, "compression_ratio": 1.78125, "no_speech_prob": 0.001366693526506424}, {"id": 376, "seek": 176932, "start": 1774.72, "end": 1782.4399999999998, "text": " through my cost and measure whether my cost is going to be minimized by this action sequence,", "tokens": [50634, 807, 452, 2063, 293, 3481, 1968, 452, 2063, 307, 516, 281, 312, 4464, 1602, 538, 341, 3069, 8310, 11, 51020], "temperature": 0.0, "avg_logprob": -0.11842065161847054, "compression_ratio": 1.78125, "no_speech_prob": 0.001366693526506424}, {"id": 377, "seek": 176932, "start": 1782.4399999999998, "end": 1783.4399999999998, "text": " my objectives.", "tokens": [51020, 452, 15961, 13, 51070], "temperature": 0.0, "avg_logprob": -0.11842065161847054, "compression_ratio": 1.78125, "no_speech_prob": 0.001366693526506424}, {"id": 378, "seek": 176932, "start": 1783.4399999999998, "end": 1790.1599999999999, "text": " So what I should do is run some sort of optimization procedure that will try to search for a sequence", "tokens": [51070, 407, 437, 286, 820, 360, 307, 1190, 512, 1333, 295, 19618, 10747, 300, 486, 853, 281, 3164, 337, 257, 8310, 51406], "temperature": 0.0, "avg_logprob": -0.11842065161847054, "compression_ratio": 1.78125, "no_speech_prob": 0.001366693526506424}, {"id": 379, "seek": 176932, "start": 1790.1599999999999, "end": 1794.9199999999998, "text": " of actions that minimizes the cost given the prediction given to me by the produced by", "tokens": [51406, 295, 5909, 300, 4464, 5660, 264, 2063, 2212, 264, 17630, 2212, 281, 385, 538, 264, 7126, 538, 51644], "temperature": 0.0, "avg_logprob": -0.11842065161847054, "compression_ratio": 1.78125, "no_speech_prob": 0.001366693526506424}, {"id": 380, "seek": 176932, "start": 1794.9199999999998, "end": 1796.32, "text": " the world model.", "tokens": [51644, 264, 1002, 2316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11842065161847054, "compression_ratio": 1.78125, "no_speech_prob": 0.001366693526506424}, {"id": 381, "seek": 179632, "start": 1796.32, "end": 1800.28, "text": " This type of planning is very classical in optimal control.", "tokens": [50364, 639, 2010, 295, 5038, 307, 588, 13735, 294, 16252, 1969, 13, 50562], "temperature": 0.0, "avg_logprob": -0.19673142225846, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.00028237697551958263}, {"id": 382, "seek": 179632, "start": 1800.28, "end": 1802.96, "text": " It's called model predictive control.", "tokens": [50562, 467, 311, 1219, 2316, 35521, 1969, 13, 50696], "temperature": 0.0, "avg_logprob": -0.19673142225846, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.00028237697551958263}, {"id": 383, "seek": 179632, "start": 1802.96, "end": 1810.6399999999999, "text": " In classical optimal control, the model is not learned usually, it's handcrafted.", "tokens": [50696, 682, 13735, 16252, 1969, 11, 264, 2316, 307, 406, 3264, 2673, 11, 309, 311, 1011, 5611, 292, 13, 51080], "temperature": 0.0, "avg_logprob": -0.19673142225846, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.00028237697551958263}, {"id": 384, "seek": 179632, "start": 1810.6399999999999, "end": 1815.3999999999999, "text": " Here we are thinking about a situation where the world model is learned by, for example,", "tokens": [51080, 1692, 321, 366, 1953, 466, 257, 2590, 689, 264, 1002, 2316, 307, 3264, 538, 11, 337, 1365, 11, 51318], "temperature": 0.0, "avg_logprob": -0.19673142225846, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.00028237697551958263}, {"id": 385, "seek": 179632, "start": 1815.3999999999999, "end": 1822.24, "text": " watching the world go by, by video, but also by observing actions being taken in the world", "tokens": [51318, 1976, 264, 1002, 352, 538, 11, 538, 960, 11, 457, 611, 538, 22107, 5909, 885, 2726, 294, 264, 1002, 51660], "temperature": 0.0, "avg_logprob": -0.19673142225846, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.00028237697551958263}, {"id": 386, "seek": 179632, "start": 1822.24, "end": 1823.3999999999999, "text": " and seeing the effect.", "tokens": [51660, 293, 2577, 264, 1802, 13, 51718], "temperature": 0.0, "avg_logprob": -0.19673142225846, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.00028237697551958263}, {"id": 387, "seek": 182340, "start": 1823.4, "end": 1828.8000000000002, "text": " So to get a good accurate model here, I'm going to have to observe the state of the", "tokens": [50364, 407, 281, 483, 257, 665, 8559, 2316, 510, 11, 286, 478, 516, 281, 362, 281, 11441, 264, 1785, 295, 264, 50634], "temperature": 0.0, "avg_logprob": -0.17662426032642325, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.00011223591718589887}, {"id": 388, "seek": 182340, "start": 1828.8000000000002, "end": 1833.68, "text": " world, observe, like, take an action and observe the effect or observe someone else", "tokens": [50634, 1002, 11, 11441, 11, 411, 11, 747, 364, 3069, 293, 11441, 264, 1802, 420, 11441, 1580, 1646, 50878], "temperature": 0.0, "avg_logprob": -0.17662426032642325, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.00011223591718589887}, {"id": 389, "seek": 182340, "start": 1833.68, "end": 1836.0400000000002, "text": " take an action and observe the effect.", "tokens": [50878, 747, 364, 3069, 293, 11441, 264, 1802, 13, 50996], "temperature": 0.0, "avg_logprob": -0.17662426032642325, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.00011223591718589887}, {"id": 390, "seek": 182340, "start": 1836.0400000000002, "end": 1840.88, "text": " Let me skip this for now.", "tokens": [50996, 961, 385, 10023, 341, 337, 586, 13, 51238], "temperature": 0.0, "avg_logprob": -0.17662426032642325, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.00011223591718589887}, {"id": 391, "seek": 182340, "start": 1840.88, "end": 1845.48, "text": " Ultimately what we want is a hierarchical version of this because if you want the system", "tokens": [51238, 23921, 437, 321, 528, 307, 257, 35250, 804, 3037, 295, 341, 570, 498, 291, 528, 264, 1185, 51468], "temperature": 0.0, "avg_logprob": -0.17662426032642325, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.00011223591718589887}, {"id": 392, "seek": 182340, "start": 1845.48, "end": 1850.72, "text": " to be able to plan complex actions, we can't plan it at the lowest level.", "tokens": [51468, 281, 312, 1075, 281, 1393, 3997, 5909, 11, 321, 393, 380, 1393, 309, 412, 264, 12437, 1496, 13, 51730], "temperature": 0.0, "avg_logprob": -0.17662426032642325, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.00011223591718589887}, {"id": 393, "seek": 185072, "start": 1850.72, "end": 1859.92, "text": " So for example, if I want to plan to go from here to New York City, I would have to basically", "tokens": [50364, 407, 337, 1365, 11, 498, 286, 528, 281, 1393, 281, 352, 490, 510, 281, 1873, 3609, 4392, 11, 286, 576, 362, 281, 1936, 50824], "temperature": 0.0, "avg_logprob": -0.16389914060893812, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0028429548256099224}, {"id": 394, "seek": 185072, "start": 1859.92, "end": 1865.52, "text": " plan every millisecond exactly what muscle actions I should take, okay?", "tokens": [50824, 1393, 633, 27940, 18882, 2293, 437, 8679, 5909, 286, 820, 747, 11, 1392, 30, 51104], "temperature": 0.0, "avg_logprob": -0.16389914060893812, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0028429548256099224}, {"id": 395, "seek": 185072, "start": 1865.52, "end": 1867.68, "text": " And it's impossible, right?", "tokens": [51104, 400, 309, 311, 6243, 11, 558, 30, 51212], "temperature": 0.0, "avg_logprob": -0.16389914060893812, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0028429548256099224}, {"id": 396, "seek": 185072, "start": 1867.68, "end": 1873.28, "text": " You can't plan an entire trip from here to New York City millisecond by millisecond,", "tokens": [51212, 509, 393, 380, 1393, 364, 2302, 4931, 490, 510, 281, 1873, 3609, 4392, 27940, 18882, 538, 27940, 18882, 11, 51492], "temperature": 0.0, "avg_logprob": -0.16389914060893812, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0028429548256099224}, {"id": 397, "seek": 185072, "start": 1873.28, "end": 1879.2, "text": " partly because you don't have a perfect model of the environment, like you don't know if,", "tokens": [51492, 17031, 570, 291, 500, 380, 362, 257, 2176, 2316, 295, 264, 2823, 11, 411, 291, 500, 380, 458, 498, 11, 51788], "temperature": 0.0, "avg_logprob": -0.16389914060893812, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0028429548256099224}, {"id": 398, "seek": 187920, "start": 1879.2, "end": 1885.48, "text": " when you're going to walk up the room here, whether someone is going to be on the way,", "tokens": [50364, 562, 291, 434, 516, 281, 1792, 493, 264, 1808, 510, 11, 1968, 1580, 307, 516, 281, 312, 322, 264, 636, 11, 50678], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 399, "seek": 187920, "start": 1885.48, "end": 1886.8400000000001, "text": " in the way and you're going to have to go around.", "tokens": [50678, 294, 264, 636, 293, 291, 434, 516, 281, 362, 281, 352, 926, 13, 50746], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 400, "seek": 187920, "start": 1886.8400000000001, "end": 1889.2, "text": " So you can't completely plan in advance, right?", "tokens": [50746, 407, 291, 393, 380, 2584, 1393, 294, 7295, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 401, "seek": 187920, "start": 1889.2, "end": 1893.44, "text": " So what we do is we plan hierarchically, say like, okay, I want to go to New York City,", "tokens": [50864, 407, 437, 321, 360, 307, 321, 1393, 35250, 984, 11, 584, 411, 11, 1392, 11, 286, 528, 281, 352, 281, 1873, 3609, 4392, 11, 51076], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 402, "seek": 187920, "start": 1893.44, "end": 1897.32, "text": " so the cost function at the top here measures my distance to New York City.", "tokens": [51076, 370, 264, 2063, 2445, 412, 264, 1192, 510, 8000, 452, 4560, 281, 1873, 3609, 4392, 13, 51270], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 403, "seek": 187920, "start": 1897.32, "end": 1904.04, "text": " And the first thing I have to do is go to the airport and catch a train or go to the train", "tokens": [51270, 400, 264, 700, 551, 286, 362, 281, 360, 307, 352, 281, 264, 10155, 293, 3745, 257, 3847, 420, 352, 281, 264, 3847, 51606], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 404, "seek": 187920, "start": 1904.04, "end": 1908.0800000000002, "text": " station and catch a train or go to the airport catch a plane.", "tokens": [51606, 5214, 293, 3745, 257, 3847, 420, 352, 281, 264, 10155, 3745, 257, 5720, 13, 51808], "temperature": 0.0, "avg_logprob": -0.17773216136180572, "compression_ratio": 1.8764044943820224, "no_speech_prob": 0.014055142179131508}, {"id": 405, "seek": 190808, "start": 1909.08, "end": 1913.04, "text": " So the top predictors are predictors at a high level that says, oh, okay,", "tokens": [50414, 407, 264, 1192, 6069, 830, 366, 6069, 830, 412, 257, 1090, 1496, 300, 1619, 11, 1954, 11, 1392, 11, 50612], "temperature": 0.0, "avg_logprob": -0.18716648646763392, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.822139115072787e-05}, {"id": 406, "seek": 190808, "start": 1913.04, "end": 1917.8, "text": " if I catch a taxi, it might take me to the airport.", "tokens": [50612, 498, 286, 3745, 257, 18984, 11, 309, 1062, 747, 385, 281, 264, 10155, 13, 50850], "temperature": 0.0, "avg_logprob": -0.18716648646763392, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.822139115072787e-05}, {"id": 407, "seek": 190808, "start": 1917.8, "end": 1923.12, "text": " If I catch, or to the train station, then if I catch a train, it'll take me to New York City.", "tokens": [50850, 759, 286, 3745, 11, 420, 281, 264, 3847, 5214, 11, 550, 498, 286, 3745, 257, 3847, 11, 309, 603, 747, 385, 281, 1873, 3609, 4392, 13, 51116], "temperature": 0.0, "avg_logprob": -0.18716648646763392, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.822139115072787e-05}, {"id": 408, "seek": 190808, "start": 1923.12, "end": 1929.1999999999998, "text": " Okay, so you have those two hidden actions, those Z variables here.", "tokens": [51116, 1033, 11, 370, 291, 362, 729, 732, 7633, 5909, 11, 729, 1176, 9102, 510, 13, 51420], "temperature": 0.0, "avg_logprob": -0.18716648646763392, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.822139115072787e-05}, {"id": 409, "seek": 190808, "start": 1929.1999999999998, "end": 1932.96, "text": " And they define a cost function for the next level down.", "tokens": [51420, 400, 436, 6964, 257, 2063, 2445, 337, 264, 958, 1496, 760, 13, 51608], "temperature": 0.0, "avg_logprob": -0.18716648646763392, "compression_ratio": 1.6226415094339623, "no_speech_prob": 3.822139115072787e-05}, {"id": 410, "seek": 193296, "start": 1933.16, "end": 1939.6000000000001, "text": " So if the first action is I'm taking a taxi to the train station,", "tokens": [50374, 407, 498, 264, 700, 3069, 307, 286, 478, 1940, 257, 18984, 281, 264, 3847, 5214, 11, 50696], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 411, "seek": 193296, "start": 1939.6000000000001, "end": 1943.52, "text": " the lower level is how do I catch a taxi here?", "tokens": [50696, 264, 3126, 1496, 307, 577, 360, 286, 3745, 257, 18984, 510, 30, 50892], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 412, "seek": 193296, "start": 1943.52, "end": 1945.24, "text": " I go down in the street and hail the taxi.", "tokens": [50892, 286, 352, 760, 294, 264, 4838, 293, 38157, 264, 18984, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 413, "seek": 193296, "start": 1945.24, "end": 1946.08, "text": " No, this is Boston.", "tokens": [50978, 883, 11, 341, 307, 12333, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 414, "seek": 193296, "start": 1946.08, "end": 1948.8400000000001, "text": " I need to call it Uber or something.", "tokens": [51020, 286, 643, 281, 818, 309, 21839, 420, 746, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 415, "seek": 193296, "start": 1948.8400000000001, "end": 1955.52, "text": " Okay, so I go on the street and I call it Uber.", "tokens": [51158, 1033, 11, 370, 286, 352, 322, 264, 4838, 293, 286, 818, 309, 21839, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 416, "seek": 193296, "start": 1955.52, "end": 1956.76, "text": " How do I go in the street?", "tokens": [51492, 1012, 360, 286, 352, 294, 264, 4838, 30, 51554], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 417, "seek": 193296, "start": 1956.76, "end": 1958.1200000000001, "text": " There's going to be lower levels.", "tokens": [51554, 821, 311, 516, 281, 312, 3126, 4358, 13, 51622], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 418, "seek": 193296, "start": 1958.1200000000001, "end": 1959.6000000000001, "text": " I have to get out of this building.", "tokens": [51622, 286, 362, 281, 483, 484, 295, 341, 2390, 13, 51696], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 419, "seek": 193296, "start": 1959.6000000000001, "end": 1960.52, "text": " How do we get out of this building?", "tokens": [51696, 1012, 360, 321, 483, 484, 295, 341, 2390, 30, 51742], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 420, "seek": 193296, "start": 1960.52, "end": 1962.0, "text": " I have to walk through the door.", "tokens": [51742, 286, 362, 281, 1792, 807, 264, 2853, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1807506048857276, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0014101119013503194}, {"id": 421, "seek": 196200, "start": 1962.08, "end": 1963.4, "text": " How do I work through the door?", "tokens": [50368, 1012, 360, 286, 589, 807, 264, 2853, 30, 50434], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 422, "seek": 196200, "start": 1963.4, "end": 1967.2, "text": " I have to put one leg in front of the other over obstacles.", "tokens": [50434, 286, 362, 281, 829, 472, 1676, 294, 1868, 295, 264, 661, 670, 17735, 13, 50624], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 423, "seek": 196200, "start": 1967.2, "end": 1969.48, "text": " And all the way down to millisecond.", "tokens": [50624, 400, 439, 264, 636, 760, 281, 27940, 18882, 13, 50738], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 424, "seek": 196200, "start": 1969.48, "end": 1973.0, "text": " Also control for a short period, which is replanned as we go.", "tokens": [50738, 2743, 1969, 337, 257, 2099, 2896, 11, 597, 307, 3248, 5943, 382, 321, 352, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 425, "seek": 196200, "start": 1974.16, "end": 1977.44, "text": " Okay, no AI systems today can do any of this.", "tokens": [50972, 1033, 11, 572, 7318, 3652, 965, 393, 360, 604, 295, 341, 13, 51136], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 426, "seek": 196200, "start": 1977.44, "end": 1981.64, "text": " This is completely virgin territory.", "tokens": [51136, 639, 307, 2584, 26404, 11360, 13, 51346], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 427, "seek": 196200, "start": 1981.64, "end": 1985.68, "text": " Okay, there's a lot of people who've worked on hierarchical planning,", "tokens": [51346, 1033, 11, 456, 311, 257, 688, 295, 561, 567, 600, 2732, 322, 35250, 804, 5038, 11, 51548], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 428, "seek": 196200, "start": 1985.68, "end": 1990.16, "text": " but in situations where the representations at every level are hardwired,", "tokens": [51548, 457, 294, 6851, 689, 264, 33358, 412, 633, 1496, 366, 1152, 86, 1824, 11, 51772], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 429, "seek": 196200, "start": 1990.16, "end": 1991.56, "text": " they're known in advance.", "tokens": [51772, 436, 434, 2570, 294, 7295, 13, 51842], "temperature": 0.0, "avg_logprob": -0.2112259864807129, "compression_ratio": 1.5765124555160142, "no_speech_prob": 4.757109854836017e-05}, {"id": 430, "seek": 199156, "start": 1991.56, "end": 1993.08, "text": " They're predetermined.", "tokens": [50364, 814, 434, 3852, 35344, 2001, 13, 50440], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 431, "seek": 199156, "start": 1993.08, "end": 1995.84, "text": " It's sort of like the equivalent of a vision system where the features", "tokens": [50440, 467, 311, 1333, 295, 411, 264, 10344, 295, 257, 5201, 1185, 689, 264, 4122, 50578], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 432, "seek": 199156, "start": 1995.84, "end": 1998.08, "text": " at every level are hardwired or designed by hand.", "tokens": [50578, 412, 633, 1496, 366, 1152, 86, 1824, 420, 4761, 538, 1011, 13, 50690], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 433, "seek": 199156, "start": 1999.08, "end": 1999.9199999999998, "text": " There's no system today.", "tokens": [50740, 821, 311, 572, 1185, 965, 13, 50782], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 434, "seek": 199156, "start": 1999.9199999999998, "end": 2002.48, "text": " They can learn hierarchical representations for action plans.", "tokens": [50782, 814, 393, 1466, 35250, 804, 33358, 337, 3069, 5482, 13, 50910], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 435, "seek": 199156, "start": 2003.76, "end": 2005.6799999999998, "text": " So that's a big challenge.", "tokens": [50974, 407, 300, 311, 257, 955, 3430, 13, 51070], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 436, "seek": 199156, "start": 2005.6799999999998, "end": 2009.6, "text": " The cost function, so here's what's important here.", "tokens": [51070, 440, 2063, 2445, 11, 370, 510, 311, 437, 311, 1021, 510, 13, 51266], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 437, "seek": 199156, "start": 2009.6, "end": 2013.96, "text": " A lot of people today are talking about the fact that AI systems are difficult", "tokens": [51266, 316, 688, 295, 561, 965, 366, 1417, 466, 264, 1186, 300, 7318, 3652, 366, 2252, 51484], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 438, "seek": 199156, "start": 2013.96, "end": 2019.2, "text": " to control and that's terrible, maybe toxic, various things.", "tokens": [51484, 281, 1969, 293, 300, 311, 6237, 11, 1310, 12786, 11, 3683, 721, 13, 51746], "temperature": 0.0, "avg_logprob": -0.18286446281101393, "compression_ratio": 1.6327272727272728, "no_speech_prob": 7.483138324460015e-05}, {"id": 439, "seek": 201920, "start": 2020.2, "end": 2026.56, "text": " The system I describe cannot produce outputs that do not minimize the objectives.", "tokens": [50414, 440, 1185, 286, 6786, 2644, 5258, 23930, 300, 360, 406, 17522, 264, 15961, 13, 50732], "temperature": 0.0, "avg_logprob": -0.18187781303159653, "compression_ratio": 1.7974137931034482, "no_speech_prob": 7.483768422389403e-05}, {"id": 440, "seek": 201920, "start": 2026.56, "end": 2033.32, "text": " And so if you have terms in the objective that guarantee certain conditions,", "tokens": [50732, 400, 370, 498, 291, 362, 2115, 294, 264, 10024, 300, 10815, 1629, 4487, 11, 51070], "temperature": 0.0, "avg_logprob": -0.18187781303159653, "compression_ratio": 1.7974137931034482, "no_speech_prob": 7.483768422389403e-05}, {"id": 441, "seek": 201920, "start": 2033.32, "end": 2036.3600000000001, "text": " that system will have no choice but obeying those conditions.", "tokens": [51070, 300, 1185, 486, 362, 572, 3922, 457, 36346, 1840, 729, 4487, 13, 51222], "temperature": 0.0, "avg_logprob": -0.18187781303159653, "compression_ratio": 1.7974137931034482, "no_speech_prob": 7.483768422389403e-05}, {"id": 442, "seek": 201920, "start": 2036.3600000000001, "end": 2038.96, "text": " Okay, so having a system that is designed like this,", "tokens": [51222, 1033, 11, 370, 1419, 257, 1185, 300, 307, 4761, 411, 341, 11, 51352], "temperature": 0.0, "avg_logprob": -0.18187781303159653, "compression_ratio": 1.7974137931034482, "no_speech_prob": 7.483768422389403e-05}, {"id": 443, "seek": 201920, "start": 2038.96, "end": 2043.1200000000001, "text": " that whose output is produced by minimizing a set of objectives,", "tokens": [51352, 300, 6104, 5598, 307, 7126, 538, 46608, 257, 992, 295, 15961, 11, 51560], "temperature": 0.0, "avg_logprob": -0.18187781303159653, "compression_ratio": 1.7974137931034482, "no_speech_prob": 7.483768422389403e-05}, {"id": 444, "seek": 201920, "start": 2043.1200000000001, "end": 2048.0, "text": " according to a model, will basically help guarantee the safety of that system.", "tokens": [51560, 4650, 281, 257, 2316, 11, 486, 1936, 854, 10815, 264, 4514, 295, 300, 1185, 13, 51804], "temperature": 0.0, "avg_logprob": -0.18187781303159653, "compression_ratio": 1.7974137931034482, "no_speech_prob": 7.483768422389403e-05}, {"id": 445, "seek": 204800, "start": 2048.0, "end": 2052.68, "text": " Because you can hardwire intrinsic objectives on the left here", "tokens": [50364, 1436, 291, 393, 1152, 42689, 35698, 15961, 322, 264, 1411, 510, 50598], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 446, "seek": 204800, "start": 2052.68, "end": 2054.6, "text": " that basically guarantee the safety.", "tokens": [50598, 300, 1936, 10815, 264, 4514, 13, 50694], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 447, "seek": 204800, "start": 2054.6, "end": 2058.84, "text": " And the system cannot escape the satisfaction of those constraints.", "tokens": [50694, 400, 264, 1185, 2644, 7615, 264, 18715, 295, 729, 18491, 13, 50906], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 448, "seek": 204800, "start": 2058.84, "end": 2060.88, "text": " So let me take a very simple example.", "tokens": [50906, 407, 718, 385, 747, 257, 588, 2199, 1365, 13, 51008], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 449, "seek": 204800, "start": 2060.88, "end": 2064.12, "text": " Let's say someone figures out how to build a domestic robot they can cook.", "tokens": [51008, 961, 311, 584, 1580, 9624, 484, 577, 281, 1322, 257, 10939, 7881, 436, 393, 2543, 13, 51170], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 450, "seek": 204800, "start": 2065.72, "end": 2070.32, "text": " This robot will have to be able to kind of handle a kitchen knife.", "tokens": [51250, 639, 7881, 486, 362, 281, 312, 1075, 281, 733, 295, 4813, 257, 6525, 7976, 13, 51480], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 451, "seek": 204800, "start": 2070.32, "end": 2073.68, "text": " And you might put a cost function that says, don't flail your arm", "tokens": [51480, 400, 291, 1062, 829, 257, 2063, 2445, 300, 1619, 11, 500, 380, 932, 864, 428, 3726, 51648], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 452, "seek": 204800, "start": 2073.68, "end": 2076.52, "text": " if you have a kitchen knife in your arm and there is people around.", "tokens": [51648, 498, 291, 362, 257, 6525, 7976, 294, 428, 3726, 293, 456, 307, 561, 926, 13, 51790], "temperature": 0.0, "avg_logprob": -0.15433309832189837, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00017120073607657105}, {"id": 453, "seek": 207652, "start": 2077.52, "end": 2078.96, "text": " Okay, because it's dangerous.", "tokens": [50414, 1033, 11, 570, 309, 311, 5795, 13, 50486], "temperature": 0.0, "avg_logprob": -0.20477589693936435, "compression_ratio": 1.6108374384236452, "no_speech_prob": 7.252334035001695e-05}, {"id": 454, "seek": 207652, "start": 2078.96, "end": 2082.64, "text": " So you can imagine putting a lot of kind of safety conditions in those systems", "tokens": [50486, 407, 291, 393, 3811, 3372, 257, 688, 295, 733, 295, 4514, 4487, 294, 729, 3652, 50670], "temperature": 0.0, "avg_logprob": -0.20477589693936435, "compression_ratio": 1.6108374384236452, "no_speech_prob": 7.252334035001695e-05}, {"id": 455, "seek": 207652, "start": 2082.64, "end": 2084.08, "text": " to make them steerable.", "tokens": [50670, 281, 652, 552, 30814, 712, 13, 50742], "temperature": 0.0, "avg_logprob": -0.20477589693936435, "compression_ratio": 1.6108374384236452, "no_speech_prob": 7.252334035001695e-05}, {"id": 456, "seek": 207652, "start": 2084.08, "end": 2091.12, "text": " So I don't think the problem of making AI systems safe is such a huge problem", "tokens": [50742, 407, 286, 500, 380, 519, 264, 1154, 295, 1455, 7318, 3652, 3273, 307, 1270, 257, 2603, 1154, 51094], "temperature": 0.0, "avg_logprob": -0.20477589693936435, "compression_ratio": 1.6108374384236452, "no_speech_prob": 7.252334035001695e-05}, {"id": 457, "seek": 207652, "start": 2091.12, "end": 2095.84, "text": " that some people who are very vocal are seeing it is that AI is going to kill us all.", "tokens": [51094, 300, 512, 561, 567, 366, 588, 11657, 366, 2577, 309, 307, 300, 7318, 307, 516, 281, 1961, 505, 439, 13, 51330], "temperature": 0.0, "avg_logprob": -0.20477589693936435, "compression_ratio": 1.6108374384236452, "no_speech_prob": 7.252334035001695e-05}, {"id": 458, "seek": 207652, "start": 2098.28, "end": 2099.88, "text": " It's not going to kill us all.", "tokens": [51452, 467, 311, 406, 516, 281, 1961, 505, 439, 13, 51532], "temperature": 0.0, "avg_logprob": -0.20477589693936435, "compression_ratio": 1.6108374384236452, "no_speech_prob": 7.252334035001695e-05}, {"id": 459, "seek": 209988, "start": 2099.88, "end": 2106.12, "text": " We would have to screw up really badly for that to happen.", "tokens": [50364, 492, 576, 362, 281, 5630, 493, 534, 13425, 337, 300, 281, 1051, 13, 50676], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 460, "seek": 209988, "start": 2106.12, "end": 2109.6400000000003, "text": " Okay, now here's the thing.", "tokens": [50676, 1033, 11, 586, 510, 311, 264, 551, 13, 50852], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 461, "seek": 209988, "start": 2109.6400000000003, "end": 2111.6, "text": " How do we build the world model?", "tokens": [50852, 1012, 360, 321, 1322, 264, 1002, 2316, 30, 50950], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 462, "seek": 209988, "start": 2113.6, "end": 2117.7200000000003, "text": " And that's basically the biggest challenge that we have at the moment.", "tokens": [51050, 400, 300, 311, 1936, 264, 3880, 3430, 300, 321, 362, 412, 264, 1623, 13, 51256], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 463, "seek": 209988, "start": 2117.7200000000003, "end": 2121.36, "text": " How do we build a system that can predict what's going to happen in the world?", "tokens": [51256, 1012, 360, 321, 1322, 257, 1185, 300, 393, 6069, 437, 311, 516, 281, 1051, 294, 264, 1002, 30, 51438], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 464, "seek": 209988, "start": 2121.36, "end": 2124.08, "text": " For example, by training itself to predict videos.", "tokens": [51438, 1171, 1365, 11, 538, 3097, 2564, 281, 6069, 2145, 13, 51574], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 465, "seek": 209988, "start": 2124.08, "end": 2128.1600000000003, "text": " Now the problem with predicting videos is that the world is not entirely predictable.", "tokens": [51574, 823, 264, 1154, 365, 32884, 2145, 307, 300, 264, 1002, 307, 406, 7696, 27737, 13, 51778], "temperature": 0.0, "avg_logprob": -0.20425456585270343, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.0002912972413469106}, {"id": 466, "seek": 212988, "start": 2130.7200000000003, "end": 2132.96, "text": " It may not be deterministic, but even if it were deterministic,", "tokens": [50406, 467, 815, 406, 312, 15957, 3142, 11, 457, 754, 498, 309, 645, 15957, 3142, 11, 50518], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 467, "seek": 212988, "start": 2132.96, "end": 2134.8, "text": " it wouldn't be completely predictable.", "tokens": [50518, 309, 2759, 380, 312, 2584, 27737, 13, 50610], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 468, "seek": 212988, "start": 2134.8, "end": 2136.56, "text": " So in fact, here is an example here.", "tokens": [50610, 407, 294, 1186, 11, 510, 307, 364, 1365, 510, 13, 50698], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 469, "seek": 212988, "start": 2138.32, "end": 2143.96, "text": " If you take a video, this is a top-down video of a highway that looks like cars", "tokens": [50786, 759, 291, 747, 257, 960, 11, 341, 307, 257, 1192, 12, 5093, 960, 295, 257, 17205, 300, 1542, 411, 5163, 51068], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 470, "seek": 212988, "start": 2143.96, "end": 2146.44, "text": " driving around just following the blue car.", "tokens": [51068, 4840, 926, 445, 3480, 264, 3344, 1032, 13, 51192], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 471, "seek": 212988, "start": 2146.44, "end": 2149.1600000000003, "text": " And you train a neural net to predict what's going to happen in the video", "tokens": [51192, 400, 291, 3847, 257, 18161, 2533, 281, 6069, 437, 311, 516, 281, 1051, 294, 264, 960, 51328], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 472, "seek": 212988, "start": 2149.1600000000003, "end": 2150.6, "text": " after the first few frames.", "tokens": [51328, 934, 264, 700, 1326, 12083, 13, 51400], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 473, "seek": 212988, "start": 2150.6, "end": 2156.4, "text": " It produces blurry, it makes blurry prediction because it can't predict if", "tokens": [51400, 467, 14725, 37644, 11, 309, 1669, 37644, 17630, 570, 309, 393, 380, 6069, 498, 51690], "temperature": 0.0, "avg_logprob": -0.18402347230074698, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00024146649229805917}, {"id": 474, "seek": 215640, "start": 2156.4, "end": 2162.0, "text": " the car that's behind you is going to accelerate or break or change lane or whatever.", "tokens": [50364, 264, 1032, 300, 311, 2261, 291, 307, 516, 281, 21341, 420, 1821, 420, 1319, 12705, 420, 2035, 13, 50644], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 475, "seek": 215640, "start": 2162.0, "end": 2165.6, "text": " So it makes an average of all the possible future and that's a blurry image.", "tokens": [50644, 407, 309, 1669, 364, 4274, 295, 439, 264, 1944, 2027, 293, 300, 311, 257, 37644, 3256, 13, 50824], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 476, "seek": 215640, "start": 2165.6, "end": 2169.2000000000003, "text": " Same with, this is an old paper where we attempted to do video prediction using", "tokens": [50824, 10635, 365, 11, 341, 307, 364, 1331, 3035, 689, 321, 18997, 281, 360, 960, 17630, 1228, 51004], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 477, "seek": 215640, "start": 2169.2000000000003, "end": 2174.12, "text": " neural nets and the predictions are blurry because there's too many things", "tokens": [51004, 18161, 36170, 293, 264, 21264, 366, 37644, 570, 456, 311, 886, 867, 721, 51250], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 478, "seek": 215640, "start": 2174.12, "end": 2176.6, "text": " that can plausibly happen and the system can only predict one thing.", "tokens": [51250, 300, 393, 34946, 3545, 1051, 293, 264, 1185, 393, 787, 6069, 472, 551, 13, 51374], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 479, "seek": 215640, "start": 2176.6, "end": 2177.6, "text": " So it predicts the average.", "tokens": [51374, 407, 309, 6069, 82, 264, 4274, 13, 51424], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 480, "seek": 215640, "start": 2179.84, "end": 2180.84, "text": " So that's no good.", "tokens": [51536, 407, 300, 311, 572, 665, 13, 51586], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 481, "seek": 215640, "start": 2180.84, "end": 2184.6, "text": " The solution to this is what I call a joint evading predictive architecture.", "tokens": [51586, 440, 3827, 281, 341, 307, 437, 286, 818, 257, 7225, 1073, 8166, 35521, 9482, 13, 51774], "temperature": 0.0, "avg_logprob": -0.15718395479263797, "compression_ratio": 1.7832167832167831, "no_speech_prob": 0.000351062451954931}, {"id": 482, "seek": 218460, "start": 2184.6, "end": 2188.2799999999997, "text": " And this is really the most important slide of the talk.", "tokens": [50364, 400, 341, 307, 534, 264, 881, 1021, 4137, 295, 264, 751, 13, 50548], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 483, "seek": 218460, "start": 2189.7999999999997, "end": 2195.52, "text": " So the normal way to make predictions is through a generative model.", "tokens": [50624, 407, 264, 2710, 636, 281, 652, 21264, 307, 807, 257, 1337, 1166, 2316, 13, 50910], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 484, "seek": 218460, "start": 2195.52, "end": 2196.4, "text": " What's a generative model?", "tokens": [50910, 708, 311, 257, 1337, 1166, 2316, 30, 50954], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 485, "seek": 218460, "start": 2196.4, "end": 2198.6, "text": " It's a model where you have a bunch of variables you observe,", "tokens": [50954, 467, 311, 257, 2316, 689, 291, 362, 257, 3840, 295, 9102, 291, 11441, 11, 51064], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 486, "seek": 218460, "start": 2198.6, "end": 2201.8399999999997, "text": " let's say the initial segment of a video.", "tokens": [51064, 718, 311, 584, 264, 5883, 9469, 295, 257, 960, 13, 51226], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 487, "seek": 218460, "start": 2201.8399999999997, "end": 2204.92, "text": " You run it through an encoder and through a predictor and the predictor predicts", "tokens": [51226, 509, 1190, 309, 807, 364, 2058, 19866, 293, 807, 257, 6069, 284, 293, 264, 6069, 284, 6069, 82, 51380], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 488, "seek": 218460, "start": 2204.92, "end": 2208.44, "text": " y, which is, let's say, the continuation of that video.", "tokens": [51380, 288, 11, 597, 307, 11, 718, 311, 584, 11, 264, 29357, 295, 300, 960, 13, 51556], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 489, "seek": 218460, "start": 2209.56, "end": 2212.6, "text": " And you have some cost function that measures the discrepancy divergence", "tokens": [51612, 400, 291, 362, 512, 2063, 2445, 300, 8000, 264, 2983, 265, 6040, 1344, 47387, 51764], "temperature": 0.0, "avg_logprob": -0.16148451508068648, "compression_ratio": 1.7923076923076924, "no_speech_prob": 6.807838508393615e-05}, {"id": 490, "seek": 221260, "start": 2212.6, "end": 2215.7999999999997, "text": " between the predicted y and the actual y you observe.", "tokens": [50364, 1296, 264, 19147, 288, 293, 264, 3539, 288, 291, 11441, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 491, "seek": 221260, "start": 2215.7999999999997, "end": 2217.6, "text": " This is when you train your world model.", "tokens": [50524, 639, 307, 562, 291, 3847, 428, 1002, 2316, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 492, "seek": 221260, "start": 2219.2799999999997, "end": 2222.3199999999997, "text": " It could be that the predictor has an action variable that comes in,", "tokens": [50698, 467, 727, 312, 300, 264, 6069, 284, 575, 364, 3069, 7006, 300, 1487, 294, 11, 50850], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 493, "seek": 221260, "start": 2222.3199999999997, "end": 2223.88, "text": " but in this example there isn't.", "tokens": [50850, 457, 294, 341, 1365, 456, 1943, 380, 13, 50928], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 494, "seek": 221260, "start": 2225.56, "end": 2229.92, "text": " So examples of this are things like variational auto encoders,", "tokens": [51012, 407, 5110, 295, 341, 366, 721, 411, 3034, 1478, 8399, 2058, 378, 433, 11, 51230], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 495, "seek": 221260, "start": 2229.92, "end": 2234.4, "text": " mass auto encoders, or denoising auto encoders, which is a more general concept.", "tokens": [51230, 2758, 8399, 2058, 378, 433, 11, 420, 1441, 78, 3436, 8399, 2058, 378, 433, 11, 597, 307, 257, 544, 2674, 3410, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 496, "seek": 221260, "start": 2234.4, "end": 2238.6, "text": " And so basically all NLP systems, including LMS, are of this type,", "tokens": [51454, 400, 370, 1936, 439, 426, 45196, 3652, 11, 3009, 441, 10288, 11, 366, 295, 341, 2010, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 497, "seek": 221260, "start": 2238.6, "end": 2239.36, "text": " the generative models.", "tokens": [51664, 264, 1337, 1166, 5245, 13, 51702], "temperature": 0.0, "avg_logprob": -0.1852324189258223, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0001511440786998719}, {"id": 498, "seek": 223936, "start": 2240.36, "end": 2242.36, "text": " But here is the thing.", "tokens": [50414, 583, 510, 307, 264, 551, 13, 50514], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 499, "seek": 223936, "start": 2242.36, "end": 2245.44, "text": " You don't want to be predicting every detail about the world.", "tokens": [50514, 509, 500, 380, 528, 281, 312, 32884, 633, 2607, 466, 264, 1002, 13, 50668], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 500, "seek": 223936, "start": 2245.44, "end": 2248.36, "text": " Here you have to predict every single detail about the world.", "tokens": [50668, 1692, 291, 362, 281, 6069, 633, 2167, 2607, 466, 264, 1002, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 501, "seek": 223936, "start": 2248.36, "end": 2250.88, "text": " So it's easy if it's text, because text is discrete.", "tokens": [50814, 407, 309, 311, 1858, 498, 309, 311, 2487, 11, 570, 2487, 307, 27706, 13, 50940], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 502, "seek": 223936, "start": 2250.88, "end": 2255.04, "text": " So predicting the next word, I cannot predict the next word from a text.", "tokens": [50940, 407, 32884, 264, 958, 1349, 11, 286, 2644, 6069, 264, 958, 1349, 490, 257, 2487, 13, 51148], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 503, "seek": 223936, "start": 2255.04, "end": 2261.32, "text": " But I can predict within 10 possible words some probability distribution of", "tokens": [51148, 583, 286, 393, 6069, 1951, 1266, 1944, 2283, 512, 8482, 7316, 295, 51462], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 504, "seek": 223936, "start": 2261.32, "end": 2265.1200000000003, "text": " the, over all the words in the dictionary of which word comes next, right?", "tokens": [51462, 264, 11, 670, 439, 264, 2283, 294, 264, 25890, 295, 597, 1349, 1487, 958, 11, 558, 30, 51652], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 505, "seek": 223936, "start": 2265.1200000000003, "end": 2269.32, "text": " They can represent distributions over discrete variables.", "tokens": [51652, 814, 393, 2906, 37870, 670, 27706, 9102, 13, 51862], "temperature": 0.0, "avg_logprob": -0.21503683616375102, "compression_ratio": 1.8150943396226416, "no_speech_prob": 5.143115413375199e-05}, {"id": 506, "seek": 226932, "start": 2269.32, "end": 2272.92, "text": " I cannot do this over the set of all possible video frames.", "tokens": [50364, 286, 2644, 360, 341, 670, 264, 992, 295, 439, 1944, 960, 12083, 13, 50544], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 507, "seek": 226932, "start": 2274.0800000000004, "end": 2277.56, "text": " I cannot usefully represent a distribution over the set of all possible video frames.", "tokens": [50602, 286, 2644, 764, 2277, 2906, 257, 7316, 670, 264, 992, 295, 439, 1944, 960, 12083, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 508, "seek": 226932, "start": 2279.56, "end": 2285.2000000000003, "text": " So I can't use the same trick for video that is used for language.", "tokens": [50876, 407, 286, 393, 380, 764, 264, 912, 4282, 337, 960, 300, 307, 1143, 337, 2856, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 509, "seek": 226932, "start": 2285.2000000000003, "end": 2289.8, "text": " The reason why we have LMS that works so well is because text is easy.", "tokens": [51158, 440, 1778, 983, 321, 362, 441, 10288, 300, 1985, 370, 731, 307, 570, 2487, 307, 1858, 13, 51388], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 510, "seek": 226932, "start": 2289.8, "end": 2290.52, "text": " Language is simple.", "tokens": [51388, 24445, 307, 2199, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 511, "seek": 226932, "start": 2291.84, "end": 2294.28, "text": " We only popped up in the last few hundred thousand years anyway, so", "tokens": [51490, 492, 787, 21545, 493, 294, 264, 1036, 1326, 3262, 4714, 924, 4033, 11, 370, 51612], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 512, "seek": 226932, "start": 2294.28, "end": 2295.2400000000002, "text": " it can be that complicated.", "tokens": [51612, 309, 393, 312, 300, 6179, 13, 51660], "temperature": 0.0, "avg_logprob": -0.2283318089503868, "compression_ratio": 1.7272727272727273, "no_speech_prob": 5.3865587688051164e-05}, {"id": 513, "seek": 229524, "start": 2296.24, "end": 2302.04, "text": " And it's also processed in the brain by two tiny areas called the Vernike area", "tokens": [50414, 400, 309, 311, 611, 18846, 294, 264, 3567, 538, 732, 5870, 3179, 1219, 264, 33220, 1123, 1859, 50704], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 514, "seek": 229524, "start": 2302.04, "end": 2304.6, "text": " for understanding and the Borke area for production.", "tokens": [50704, 337, 3701, 293, 264, 13739, 330, 1859, 337, 4265, 13, 50832], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 515, "seek": 229524, "start": 2305.64, "end": 2306.7999999999997, "text": " What about the rest of the brain?", "tokens": [50884, 708, 466, 264, 1472, 295, 264, 3567, 30, 50942], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 516, "seek": 229524, "start": 2306.7999999999997, "end": 2308.9199999999996, "text": " The prefrontal cortex, that's where we think, okay?", "tokens": [50942, 440, 659, 11496, 304, 33312, 11, 300, 311, 689, 321, 519, 11, 1392, 30, 51048], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 517, "seek": 229524, "start": 2310.2, "end": 2314.3599999999997, "text": " That's not part of LMS, the LMS are perhaps good models of Vernike and Borke,", "tokens": [51112, 663, 311, 406, 644, 295, 441, 10288, 11, 264, 441, 10288, 366, 4317, 665, 5245, 295, 33220, 1123, 293, 13739, 330, 11, 51320], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 518, "seek": 229524, "start": 2314.3599999999997, "end": 2314.8799999999997, "text": " but that's it.", "tokens": [51320, 457, 300, 311, 309, 13, 51346], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 519, "seek": 229524, "start": 2317.8799999999997, "end": 2321.8799999999997, "text": " Okay, so what I'm proposing here is to replace this generative architecture by", "tokens": [51496, 1033, 11, 370, 437, 286, 478, 29939, 510, 307, 281, 7406, 341, 1337, 1166, 9482, 538, 51696], "temperature": 0.0, "avg_logprob": -0.35630441595006873, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00022330555657390505}, {"id": 520, "seek": 232188, "start": 2321.88, "end": 2325.4, "text": " a joint embedding architecture and the essential characteristic of it is that", "tokens": [50364, 257, 7225, 12240, 3584, 9482, 293, 264, 7115, 16282, 295, 309, 307, 300, 50540], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 521, "seek": 232188, "start": 2325.4, "end": 2329.6400000000003, "text": " the variable that you want to capture the dependency of with respect to X goes", "tokens": [50540, 264, 7006, 300, 291, 528, 281, 7983, 264, 33621, 295, 365, 3104, 281, 1783, 1709, 50752], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 522, "seek": 232188, "start": 2329.6400000000003, "end": 2333.44, "text": " itself through an encoder and the encoder eliminates the relevant information", "tokens": [50752, 2564, 807, 364, 2058, 19866, 293, 264, 2058, 19866, 49893, 264, 7340, 1589, 50942], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 523, "seek": 232188, "start": 2333.44, "end": 2335.6, "text": " that is not useful for anything.", "tokens": [50942, 300, 307, 406, 4420, 337, 1340, 13, 51050], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 524, "seek": 232188, "start": 2335.6, "end": 2341.8, "text": " Okay, so for example, if I had a video of this, if I was shooting a video of", "tokens": [51050, 1033, 11, 370, 337, 1365, 11, 498, 286, 632, 257, 960, 295, 341, 11, 498, 286, 390, 5942, 257, 960, 295, 51360], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 525, "seek": 232188, "start": 2341.8, "end": 2345.6800000000003, "text": " the room here and then panning the camera and asking a system to predict", "tokens": [51360, 264, 1808, 510, 293, 550, 2462, 773, 264, 2799, 293, 3365, 257, 1185, 281, 6069, 51554], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 526, "seek": 232188, "start": 2345.6800000000003, "end": 2350.96, "text": " what's the rest of the room, it would probably predict that the rest of", "tokens": [51554, 437, 311, 264, 1472, 295, 264, 1808, 11, 309, 576, 1391, 6069, 300, 264, 1472, 295, 51818], "temperature": 0.0, "avg_logprob": -0.19322877094663424, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00029127750894986093}, {"id": 527, "seek": 235096, "start": 2350.96, "end": 2357.2, "text": " the room looks like the initial part that there'd be a lot of people in", "tokens": [50364, 264, 1808, 1542, 411, 264, 5883, 644, 300, 456, 1116, 312, 257, 688, 295, 561, 294, 50676], "temperature": 0.0, "avg_logprob": -0.21959614753723145, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00021643353102263063}, {"id": 528, "seek": 235096, "start": 2357.2, "end": 2365.92, "text": " different seats, but it couldn't predict your age, gender, hairstyle, clothing,", "tokens": [50676, 819, 11069, 11, 457, 309, 2809, 380, 6069, 428, 3205, 11, 7898, 11, 32770, 11, 11502, 11, 51112], "temperature": 0.0, "avg_logprob": -0.21959614753723145, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00021643353102263063}, {"id": 529, "seek": 235096, "start": 2367.2, "end": 2371.8, "text": " or the texture, precise texture of the floor or things like that, right?", "tokens": [51176, 420, 264, 8091, 11, 13600, 8091, 295, 264, 4123, 420, 721, 411, 300, 11, 558, 30, 51406], "temperature": 0.0, "avg_logprob": -0.21959614753723145, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00021643353102263063}, {"id": 530, "seek": 235096, "start": 2371.8, "end": 2376.44, "text": " So there's details that cannot possibly be predicted and one way to avoid", "tokens": [51406, 407, 456, 311, 4365, 300, 2644, 6264, 312, 19147, 293, 472, 636, 281, 5042, 51638], "temperature": 0.0, "avg_logprob": -0.21959614753723145, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00021643353102263063}, {"id": 531, "seek": 235096, "start": 2376.44, "end": 2380.2, "text": " predicting them is to basically eliminate that information from the variable to be", "tokens": [51638, 32884, 552, 307, 281, 1936, 13819, 300, 1589, 490, 264, 7006, 281, 312, 51826], "temperature": 0.0, "avg_logprob": -0.21959614753723145, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.00021643353102263063}, {"id": 532, "seek": 238020, "start": 2380.2, "end": 2382.7599999999998, "text": " predicted through an encoder.", "tokens": [50364, 19147, 807, 364, 2058, 19866, 13, 50492], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 533, "seek": 238020, "start": 2382.7599999999998, "end": 2385.68, "text": " So that's a joint embedding architecture or predictive architecture because it has", "tokens": [50492, 407, 300, 311, 257, 7225, 12240, 3584, 9482, 420, 35521, 9482, 570, 309, 575, 50638], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 534, "seek": 238020, "start": 2385.68, "end": 2386.68, "text": " a predictor.", "tokens": [50638, 257, 6069, 284, 13, 50688], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 535, "seek": 238020, "start": 2386.68, "end": 2391.04, "text": " Now there's an issue with this thing, which is that if you train a system with,", "tokens": [50688, 823, 456, 311, 364, 2734, 365, 341, 551, 11, 597, 307, 300, 498, 291, 3847, 257, 1185, 365, 11, 50906], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 536, "seek": 238020, "start": 2391.04, "end": 2394.6, "text": " let's say, a piece of video and the following piece of video and you just train", "tokens": [50906, 718, 311, 584, 11, 257, 2522, 295, 960, 293, 264, 3480, 2522, 295, 960, 293, 291, 445, 3847, 51084], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 537, "seek": 238020, "start": 2394.6, "end": 2399.2, "text": " it to minimize the prediction error, you train the whole thing, it collapses.", "tokens": [51084, 309, 281, 17522, 264, 17630, 6713, 11, 291, 3847, 264, 1379, 551, 11, 309, 48765, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 538, "seek": 238020, "start": 2399.2, "end": 2404.68, "text": " It collapses, basically the encoders ignore the inputs, they produce constant", "tokens": [51314, 467, 48765, 11, 1936, 264, 2058, 378, 433, 11200, 264, 15743, 11, 436, 5258, 5754, 51588], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 539, "seek": 238020, "start": 2404.68, "end": 2409.0, "text": " vectors for SX and SY and the predictor just needs to map SX to SY and it's a", "tokens": [51588, 18875, 337, 318, 55, 293, 32624, 293, 264, 6069, 284, 445, 2203, 281, 4471, 318, 55, 281, 32624, 293, 309, 311, 257, 51804], "temperature": 0.0, "avg_logprob": -0.19592701305042615, "compression_ratio": 1.8210526315789475, "no_speech_prob": 0.00026112986961379647}, {"id": 540, "seek": 240900, "start": 2409.0, "end": 2413.28, "text": " constant, so it's super easy, okay, bad.", "tokens": [50364, 5754, 11, 370, 309, 311, 1687, 1858, 11, 1392, 11, 1578, 13, 50578], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 541, "seek": 240900, "start": 2413.28, "end": 2417.12, "text": " So the question now is how do we prevent this from happening?", "tokens": [50578, 407, 264, 1168, 586, 307, 577, 360, 321, 4871, 341, 490, 2737, 30, 50770], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 542, "seek": 240900, "start": 2417.12, "end": 2418.64, "text": " How do we prevent it collapse?", "tokens": [50770, 1012, 360, 321, 4871, 309, 15584, 30, 50846], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 543, "seek": 240900, "start": 2418.64, "end": 2423.0, "text": " It doesn't happen with generative models because they can't collapse.", "tokens": [50846, 467, 1177, 380, 1051, 365, 1337, 1166, 5245, 570, 436, 393, 380, 15584, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 544, "seek": 240900, "start": 2423.0, "end": 2427.28, "text": " So there are three flavors of those joint embedding architectures, a simple one", "tokens": [51064, 407, 456, 366, 1045, 16303, 295, 729, 7225, 12240, 3584, 6331, 1303, 11, 257, 2199, 472, 51278], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 545, "seek": 240900, "start": 2427.28, "end": 2431.44, "text": " where you're basically trying to make the two representation of SX and SY identical.", "tokens": [51278, 689, 291, 434, 1936, 1382, 281, 652, 264, 732, 10290, 295, 318, 55, 293, 32624, 14800, 13, 51486], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 546, "seek": 240900, "start": 2431.44, "end": 2436.64, "text": " So for example, X and Y are two different views of the same scene and you want SX", "tokens": [51486, 407, 337, 1365, 11, 1783, 293, 398, 366, 732, 819, 6809, 295, 264, 912, 4145, 293, 291, 528, 318, 55, 51746], "temperature": 0.0, "avg_logprob": -0.21563578066618547, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00062564731342718}, {"id": 547, "seek": 243664, "start": 2436.64, "end": 2440.6, "text": " to represent the content of the scene, so it doesn't matter where you look it from.", "tokens": [50364, 281, 2906, 264, 2701, 295, 264, 4145, 11, 370, 309, 1177, 380, 1871, 689, 291, 574, 309, 490, 13, 50562], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 548, "seek": 243664, "start": 2440.6, "end": 2443.16, "text": " You just want to make the representations equal.", "tokens": [50562, 509, 445, 528, 281, 652, 264, 33358, 2681, 13, 50690], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 549, "seek": 243664, "start": 2443.16, "end": 2445.64, "text": " When the encoders are identical, this is called a Syme's network.", "tokens": [50690, 1133, 264, 2058, 378, 433, 366, 14800, 11, 341, 307, 1219, 257, 3902, 1398, 311, 3209, 13, 50814], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 550, "seek": 243664, "start": 2445.64, "end": 2450.48, "text": " This is another idea that goes back to the early 90s.", "tokens": [50814, 639, 307, 1071, 1558, 300, 1709, 646, 281, 264, 2440, 4289, 82, 13, 51056], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 551, "seek": 243664, "start": 2450.48, "end": 2453.64, "text": " You have deterministic joint embedding architectures and then you have joint", "tokens": [51056, 509, 362, 15957, 3142, 7225, 12240, 3584, 6331, 1303, 293, 550, 291, 362, 7225, 51214], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 552, "seek": 243664, "start": 2453.64, "end": 2458.08, "text": " predictive architectures that may be non-deterministic where the predictor", "tokens": [51214, 35521, 6331, 1303, 300, 815, 312, 2107, 12, 49136, 259, 3142, 689, 264, 6069, 284, 51436], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 553, "seek": 243664, "start": 2458.08, "end": 2462.7999999999997, "text": " function has a latent variable that could be drawn from a distribution or taken", "tokens": [51436, 2445, 575, 257, 48994, 7006, 300, 727, 312, 10117, 490, 257, 7316, 420, 2726, 51672], "temperature": 0.0, "avg_logprob": -0.22547045324602696, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0003401835565455258}, {"id": 554, "seek": 246280, "start": 2462.8, "end": 2466.7200000000003, "text": " in a set that would allow that system to make multiple predictions if necessary.", "tokens": [50364, 294, 257, 992, 300, 576, 2089, 300, 1185, 281, 652, 3866, 21264, 498, 4818, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2676967274058949, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.601688412250951e-05}, {"id": 555, "seek": 246280, "start": 2470.7200000000003, "end": 2475.96, "text": " Now, we have to ask ourselves the question of how do we train those things?", "tokens": [50760, 823, 11, 321, 362, 281, 1029, 4175, 264, 1168, 295, 577, 360, 321, 3847, 729, 721, 30, 51022], "temperature": 0.0, "avg_logprob": -0.2676967274058949, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.601688412250951e-05}, {"id": 556, "seek": 246280, "start": 2475.96, "end": 2481.5600000000004, "text": " And I'm going to use a symbolism here where that I've used the rectangles", "tokens": [51022, 400, 286, 478, 516, 281, 764, 257, 47061, 510, 689, 300, 286, 600, 1143, 264, 24077, 904, 51302], "temperature": 0.0, "avg_logprob": -0.2676967274058949, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.601688412250951e-05}, {"id": 557, "seek": 246280, "start": 2481.5600000000004, "end": 2487.48, "text": " and squares of cost functions, energy terms, the circles of variables,", "tokens": [51302, 293, 19368, 295, 2063, 6828, 11, 2281, 2115, 11, 264, 13040, 295, 9102, 11, 51598], "temperature": 0.0, "avg_logprob": -0.2676967274058949, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.601688412250951e-05}, {"id": 558, "seek": 246280, "start": 2487.48, "end": 2491.32, "text": " observed or not, and those symbols here are deterministic functions.", "tokens": [51598, 13095, 420, 406, 11, 293, 729, 16944, 510, 366, 15957, 3142, 6828, 13, 51790], "temperature": 0.0, "avg_logprob": -0.2676967274058949, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.601688412250951e-05}, {"id": 559, "seek": 249132, "start": 2491.32, "end": 2495.48, "text": " Imagine a neural net, okay, trainable.", "tokens": [50364, 11739, 257, 18161, 2533, 11, 1392, 11, 3847, 712, 13, 50572], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 560, "seek": 249132, "start": 2495.48, "end": 2499.7200000000003, "text": " We may have to hardwire some cost functions in the system to have it,", "tokens": [50572, 492, 815, 362, 281, 1152, 42689, 512, 2063, 6828, 294, 264, 1185, 281, 362, 309, 11, 50784], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 561, "seek": 249132, "start": 2499.7200000000003, "end": 2503.6000000000004, "text": " to drive it to focus on aspects of the input that are important.", "tokens": [50784, 281, 3332, 309, 281, 1879, 322, 7270, 295, 264, 4846, 300, 366, 1021, 13, 50978], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 562, "seek": 249132, "start": 2503.6000000000004, "end": 2508.2000000000003, "text": " So that's the purpose of that C cost function at the top.", "tokens": [50978, 407, 300, 311, 264, 4334, 295, 300, 383, 2063, 2445, 412, 264, 1192, 13, 51208], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 563, "seek": 249132, "start": 2508.2000000000003, "end": 2513.2000000000003, "text": " Okay, but to explain how to train those things,", "tokens": [51208, 1033, 11, 457, 281, 2903, 577, 281, 3847, 729, 721, 11, 51458], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 564, "seek": 249132, "start": 2513.2000000000003, "end": 2517.1200000000003, "text": " I'm going to have to explain a little bit what energy based models is about because", "tokens": [51458, 286, 478, 516, 281, 362, 281, 2903, 257, 707, 857, 437, 2281, 2361, 5245, 307, 466, 570, 51654], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 565, "seek": 249132, "start": 2517.1200000000003, "end": 2520.7200000000003, "text": " the classical kind of probabilistic modeling in machine learning kind of goes", "tokens": [51654, 264, 13735, 733, 295, 31959, 3142, 15983, 294, 3479, 2539, 733, 295, 1709, 51834], "temperature": 0.0, "avg_logprob": -0.21443509625958967, "compression_ratio": 1.6768060836501901, "no_speech_prob": 6.398255936801434e-05}, {"id": 566, "seek": 252072, "start": 2520.72, "end": 2524.52, "text": " at the window when we use the joint embedding architectures.", "tokens": [50364, 412, 264, 4910, 562, 321, 764, 264, 7225, 12240, 3584, 6331, 1303, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 567, "seek": 252072, "start": 2524.52, "end": 2525.64, "text": " So what's an energy based model?", "tokens": [50554, 407, 437, 311, 364, 2281, 2361, 2316, 30, 50610], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 568, "seek": 252072, "start": 2525.64, "end": 2530.2799999999997, "text": " Energy based model is a learning system that captures the dependency between", "tokens": [50610, 14939, 2361, 2316, 307, 257, 2539, 1185, 300, 27986, 264, 33621, 1296, 50842], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 569, "seek": 252072, "start": 2530.2799999999997, "end": 2534.3999999999996, "text": " two sets of variable x and y through an energy function that is supposed to take", "tokens": [50842, 732, 6352, 295, 7006, 2031, 293, 288, 807, 364, 2281, 2445, 300, 307, 3442, 281, 747, 51048], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 570, "seek": 252072, "start": 2534.3999999999996, "end": 2539.56, "text": " low values, low energies around data, training samples.", "tokens": [51048, 2295, 4190, 11, 2295, 25737, 926, 1412, 11, 3097, 10938, 13, 51306], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 571, "seek": 252072, "start": 2539.56, "end": 2543.48, "text": " So imagine those black dots are training samples.", "tokens": [51306, 407, 3811, 729, 2211, 15026, 366, 3097, 10938, 13, 51502], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 572, "seek": 252072, "start": 2543.48, "end": 2547.68, "text": " You want that energy function f of x, y to take low values around the training", "tokens": [51502, 509, 528, 300, 2281, 2445, 283, 295, 2031, 11, 288, 281, 747, 2295, 4190, 926, 264, 3097, 51712], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 573, "seek": 252072, "start": 2547.68, "end": 2550.3999999999996, "text": " samples and then higher values outside.", "tokens": [51712, 10938, 293, 550, 2946, 4190, 2380, 13, 51848], "temperature": 0.0, "avg_logprob": -0.2392448734592747, "compression_ratio": 1.8521400778210118, "no_speech_prob": 9.016304829856381e-05}, {"id": 574, "seek": 255040, "start": 2550.6, "end": 2553.28, "text": " And that system will capture the dependencies between x and y.", "tokens": [50374, 400, 300, 1185, 486, 7983, 264, 36606, 1296, 2031, 293, 288, 13, 50508], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 575, "seek": 255040, "start": 2553.28, "end": 2556.6800000000003, "text": " If I give you a value of x and I ask you what can be the possible values for y,", "tokens": [50508, 759, 286, 976, 291, 257, 2158, 295, 2031, 293, 286, 1029, 291, 437, 393, 312, 264, 1944, 4190, 337, 288, 11, 50678], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 576, "seek": 255040, "start": 2556.6800000000003, "end": 2560.0, "text": " you're going to tell me, well, it's either this or that or maybe that other thing at", "tokens": [50678, 291, 434, 516, 281, 980, 385, 11, 731, 11, 309, 311, 2139, 341, 420, 300, 420, 1310, 300, 661, 551, 412, 50844], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 577, "seek": 255040, "start": 2560.0, "end": 2561.4, "text": " the top.", "tokens": [50844, 264, 1192, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 578, "seek": 255040, "start": 2561.4, "end": 2565.64, "text": " Okay, so it's not a mapping from x to y, it's an implicit function.", "tokens": [50914, 1033, 11, 370, 309, 311, 406, 257, 18350, 490, 2031, 281, 288, 11, 309, 311, 364, 26947, 2445, 13, 51126], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 579, "seek": 255040, "start": 2565.64, "end": 2568.96, "text": " And by figuring out what value of y minimizes the energy function,", "tokens": [51126, 400, 538, 15213, 484, 437, 2158, 295, 288, 4464, 5660, 264, 2281, 2445, 11, 51292], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 580, "seek": 255040, "start": 2568.96, "end": 2570.2000000000003, "text": " you can do inference.", "tokens": [51292, 291, 393, 360, 38253, 13, 51354], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 581, "seek": 255040, "start": 2570.2000000000003, "end": 2574.08, "text": " You can infer y, possibly, but you don't necessarily have to do that.", "tokens": [51354, 509, 393, 13596, 288, 11, 6264, 11, 457, 291, 500, 380, 4725, 362, 281, 360, 300, 13, 51548], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 582, "seek": 255040, "start": 2575.6, "end": 2576.6800000000003, "text": " So that's energy based model.", "tokens": [51624, 407, 300, 311, 2281, 2361, 2316, 13, 51678], "temperature": 0.0, "avg_logprob": -0.20588799346265174, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.668811586976517e-06}, {"id": 583, "seek": 257668, "start": 2576.68, "end": 2581.04, "text": " It's kind of a weaker form of modeling than probabilistic modeling.", "tokens": [50364, 467, 311, 733, 295, 257, 24286, 1254, 295, 15983, 813, 31959, 3142, 15983, 13, 50582], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 584, "seek": 257668, "start": 2581.04, "end": 2584.96, "text": " And so now the learning problem becomes how do you train this energy function,", "tokens": [50582, 400, 370, 586, 264, 2539, 1154, 3643, 577, 360, 291, 3847, 341, 2281, 2445, 11, 50778], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 585, "seek": 257668, "start": 2584.96, "end": 2590.48, "text": " which is going to be some big neural net, so that the energy takes low value around", "tokens": [50778, 597, 307, 516, 281, 312, 512, 955, 18161, 2533, 11, 370, 300, 264, 2281, 2516, 2295, 2158, 926, 51054], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 586, "seek": 257668, "start": 2590.48, "end": 2592.7599999999998, "text": " the training samples and high values outside.", "tokens": [51054, 264, 3097, 10938, 293, 1090, 4190, 2380, 13, 51168], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 587, "seek": 257668, "start": 2592.7599999999998, "end": 2595.52, "text": " If you're not careful, you're going to get a collapse so that the same type of", "tokens": [51168, 759, 291, 434, 406, 5026, 11, 291, 434, 516, 281, 483, 257, 15584, 370, 300, 264, 912, 2010, 295, 51306], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 588, "seek": 257668, "start": 2595.52, "end": 2598.6, "text": " collapse I was telling you about before, if you just pull down the energy of the", "tokens": [51306, 15584, 286, 390, 3585, 291, 466, 949, 11, 498, 291, 445, 2235, 760, 264, 2281, 295, 264, 51460], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 589, "seek": 257668, "start": 2598.6, "end": 2602.96, "text": " training samples, minimize the prediction error in this joint invading architecture,", "tokens": [51460, 3097, 10938, 11, 17522, 264, 17630, 6713, 294, 341, 7225, 1048, 8166, 9482, 11, 51678], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 590, "seek": 257668, "start": 2602.96, "end": 2604.52, "text": " you're going to get zero energy for everything.", "tokens": [51678, 291, 434, 516, 281, 483, 4018, 2281, 337, 1203, 13, 51756], "temperature": 0.0, "avg_logprob": -0.15625626700265066, "compression_ratio": 1.8534201954397393, "no_speech_prob": 6.813203799538314e-05}, {"id": 591, "seek": 260452, "start": 2604.52, "end": 2607.2, "text": " It's not a good way to capture the dependencies.", "tokens": [50364, 467, 311, 406, 257, 665, 636, 281, 7983, 264, 36606, 13, 50498], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 592, "seek": 260452, "start": 2607.2, "end": 2609.28, "text": " You have two classes of methods, contrastive methods.", "tokens": [50498, 509, 362, 732, 5359, 295, 7150, 11, 8712, 488, 7150, 13, 50602], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 593, "seek": 260452, "start": 2609.28, "end": 2613.04, "text": " So contrastive methods consist in generating those green points,", "tokens": [50602, 407, 8712, 488, 7150, 4603, 294, 17746, 729, 3092, 2793, 11, 50790], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 594, "seek": 260452, "start": 2613.04, "end": 2618.0, "text": " which are outside the region of data, and then push the energy up while you push", "tokens": [50790, 597, 366, 2380, 264, 4458, 295, 1412, 11, 293, 550, 2944, 264, 2281, 493, 1339, 291, 2944, 51038], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 595, "seek": 260452, "start": 2618.0, "end": 2620.12, "text": " down on the energy of the data points.", "tokens": [51038, 760, 322, 264, 2281, 295, 264, 1412, 2793, 13, 51144], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 596, "seek": 260452, "start": 2620.12, "end": 2625.04, "text": " Okay, so that's going to create a groove in the energy surface, and", "tokens": [51144, 1033, 11, 370, 300, 311, 516, 281, 1884, 257, 26910, 294, 264, 2281, 3753, 11, 293, 51390], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 597, "seek": 260452, "start": 2625.04, "end": 2627.32, "text": " the system will have captured the dependency between x and y.", "tokens": [51390, 264, 1185, 486, 362, 11828, 264, 33621, 1296, 2031, 293, 288, 13, 51504], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 598, "seek": 260452, "start": 2628.52, "end": 2631.52, "text": " But there's an alternative here, which is regularized methods,", "tokens": [51564, 583, 456, 311, 364, 8535, 510, 11, 597, 307, 3890, 1602, 7150, 11, 51714], "temperature": 0.0, "avg_logprob": -0.21044409477104575, "compression_ratio": 1.8113207547169812, "no_speech_prob": 2.5069481125683524e-05}, {"id": 599, "seek": 263152, "start": 2631.52, "end": 2636.0, "text": " where the point of those methods is to minimize the volume of space that can take", "tokens": [50364, 689, 264, 935, 295, 729, 7150, 307, 281, 17522, 264, 5523, 295, 1901, 300, 393, 747, 50588], "temperature": 0.0, "avg_logprob": -0.17985939025878905, "compression_ratio": 1.775330396475771, "no_speech_prob": 0.0001606092555448413}, {"id": 600, "seek": 263152, "start": 2636.0, "end": 2640.28, "text": " low energy, so that when you push down on the energy of data points,", "tokens": [50588, 2295, 2281, 11, 370, 300, 562, 291, 2944, 760, 322, 264, 2281, 295, 1412, 2793, 11, 50802], "temperature": 0.0, "avg_logprob": -0.17985939025878905, "compression_ratio": 1.775330396475771, "no_speech_prob": 0.0001606092555448413}, {"id": 601, "seek": 263152, "start": 2642.56, "end": 2647.04, "text": " the rest of the space takes higher energy because there is only a small amount of,", "tokens": [50916, 264, 1472, 295, 264, 1901, 2516, 2946, 2281, 570, 456, 307, 787, 257, 1359, 2372, 295, 11, 51140], "temperature": 0.0, "avg_logprob": -0.17985939025878905, "compression_ratio": 1.775330396475771, "no_speech_prob": 0.0001606092555448413}, {"id": 602, "seek": 263152, "start": 2647.04, "end": 2649.52, "text": " a small region of low energy to go around.", "tokens": [51140, 257, 1359, 4458, 295, 2295, 2281, 281, 352, 926, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17985939025878905, "compression_ratio": 1.775330396475771, "no_speech_prob": 0.0001606092555448413}, {"id": 603, "seek": 263152, "start": 2649.52, "end": 2651.48, "text": " So those are the two classes of methods.", "tokens": [51264, 407, 729, 366, 264, 732, 5359, 295, 7150, 13, 51362], "temperature": 0.0, "avg_logprob": -0.17985939025878905, "compression_ratio": 1.775330396475771, "no_speech_prob": 0.0001606092555448413}, {"id": 604, "seek": 263152, "start": 2651.48, "end": 2655.96, "text": " Every method you ever heard of in machine learning can be viewed as one of those two.", "tokens": [51362, 2048, 3170, 291, 1562, 2198, 295, 294, 3479, 2539, 393, 312, 19174, 382, 472, 295, 729, 732, 13, 51586], "temperature": 0.0, "avg_logprob": -0.17985939025878905, "compression_ratio": 1.775330396475771, "no_speech_prob": 0.0001606092555448413}, {"id": 605, "seek": 265596, "start": 2655.96, "end": 2661.12, "text": " Most probabilistic methods actually belong to the contrastive category.", "tokens": [50364, 4534, 31959, 3142, 7150, 767, 5784, 281, 264, 8712, 488, 7719, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2234854354071863, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0002165033365599811}, {"id": 606, "seek": 265596, "start": 2662.48, "end": 2667.4, "text": " Anything that uses Monte Carlo sampling, for example, is contrastive.", "tokens": [50690, 11998, 300, 4960, 38105, 45112, 21179, 11, 337, 1365, 11, 307, 8712, 488, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2234854354071863, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0002165033365599811}, {"id": 607, "seek": 265596, "start": 2667.4, "end": 2670.68, "text": " And then things like sparse coding and k-means and", "tokens": [50936, 400, 550, 721, 411, 637, 11668, 17720, 293, 350, 12, 1398, 599, 293, 51100], "temperature": 0.0, "avg_logprob": -0.2234854354071863, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0002165033365599811}, {"id": 608, "seek": 265596, "start": 2670.68, "end": 2673.76, "text": " things like that are more on the regularized method side of things.", "tokens": [51100, 721, 411, 300, 366, 544, 322, 264, 3890, 1602, 3170, 1252, 295, 721, 13, 51254], "temperature": 0.0, "avg_logprob": -0.2234854354071863, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0002165033365599811}, {"id": 609, "seek": 265596, "start": 2675.88, "end": 2679.32, "text": " Okay, so I'm asking you to do four things.", "tokens": [51360, 1033, 11, 370, 286, 478, 3365, 291, 281, 360, 1451, 721, 13, 51532], "temperature": 0.0, "avg_logprob": -0.2234854354071863, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0002165033365599811}, {"id": 610, "seek": 265596, "start": 2679.32, "end": 2684.2, "text": " Abandoned generative models in favor of the joint embedding architectures, right?", "tokens": [51532, 2847, 8518, 292, 1337, 1166, 5245, 294, 2294, 295, 264, 7225, 12240, 3584, 6331, 1303, 11, 558, 30, 51776], "temperature": 0.0, "avg_logprob": -0.2234854354071863, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.0002165033365599811}, {"id": 611, "seek": 268420, "start": 2684.2, "end": 2687.2799999999997, "text": " So generative models are the most popular thing at the moment.", "tokens": [50364, 407, 1337, 1166, 5245, 366, 264, 881, 3743, 551, 412, 264, 1623, 13, 50518], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 612, "seek": 268420, "start": 2687.2799999999997, "end": 2693.7599999999998, "text": " Forget about it, at least if you're interested in getting to the next step in AI.", "tokens": [50518, 18675, 466, 309, 11, 412, 1935, 498, 291, 434, 3102, 294, 1242, 281, 264, 958, 1823, 294, 7318, 13, 50842], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 613, "seek": 268420, "start": 2693.7599999999998, "end": 2696.6, "text": " Abandoned probabilistic models, because if you have those joint embedding", "tokens": [50842, 2847, 8518, 292, 31959, 3142, 5245, 11, 570, 498, 291, 362, 729, 7225, 12240, 3584, 50984], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 614, "seek": 268420, "start": 2696.6, "end": 2700.3599999999997, "text": " architectures, you cannot actually use it to derive a pure y given x.", "tokens": [50984, 6331, 1303, 11, 291, 2644, 767, 764, 309, 281, 28446, 257, 6075, 288, 2212, 2031, 13, 51172], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 615, "seek": 268420, "start": 2702.16, "end": 2704.2, "text": " The only thing you can use is sort of energy-based view.", "tokens": [51262, 440, 787, 551, 291, 393, 764, 307, 1333, 295, 2281, 12, 6032, 1910, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 616, "seek": 268420, "start": 2705.9199999999996, "end": 2708.6, "text": " Abandoned contrastive methods in favor of those regularized methods,", "tokens": [51450, 2847, 8518, 292, 8712, 488, 7150, 294, 2294, 295, 729, 3890, 1602, 7150, 11, 51584], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 617, "seek": 268420, "start": 2708.6, "end": 2711.16, "text": " which I'll talk about a bit more.", "tokens": [51584, 597, 286, 603, 751, 466, 257, 857, 544, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 618, "seek": 268420, "start": 2711.16, "end": 2713.48, "text": " And then something I've said for many years now, abandoned reinforcement", "tokens": [51712, 400, 550, 746, 286, 600, 848, 337, 867, 924, 586, 11, 13732, 29280, 51828], "temperature": 0.0, "avg_logprob": -0.2015238900221031, "compression_ratio": 1.686084142394822, "no_speech_prob": 4.0689723391551524e-05}, {"id": 619, "seek": 271348, "start": 2713.48, "end": 2714.6, "text": " modeling because it's too inefficient.", "tokens": [50364, 15983, 570, 309, 311, 886, 43495, 13, 50420], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 620, "seek": 271348, "start": 2717.36, "end": 2719.88, "text": " So those are some of the pillars of machine learning.", "tokens": [50558, 407, 729, 366, 512, 295, 264, 26729, 295, 3479, 2539, 13, 50684], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 621, "seek": 271348, "start": 2721.0, "end": 2725.8, "text": " And I realize this is not a very popular opinion here, but okay.", "tokens": [50740, 400, 286, 4325, 341, 307, 406, 257, 588, 3743, 4800, 510, 11, 457, 1392, 13, 50980], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 622, "seek": 271348, "start": 2725.8, "end": 2727.64, "text": " So what about those regularized methods?", "tokens": [50980, 407, 437, 466, 729, 3890, 1602, 7150, 30, 51072], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 623, "seek": 271348, "start": 2727.64, "end": 2728.72, "text": " I'm just going to give you one example.", "tokens": [51072, 286, 478, 445, 516, 281, 976, 291, 472, 1365, 13, 51126], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 624, "seek": 271348, "start": 2728.72, "end": 2729.48, "text": " There's a whole bunch of them.", "tokens": [51126, 821, 311, 257, 1379, 3840, 295, 552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 625, "seek": 271348, "start": 2729.48, "end": 2735.36, "text": " There's like a dozen of them, but I'm just going to give you one called Vicreg.", "tokens": [51164, 821, 311, 411, 257, 16654, 295, 552, 11, 457, 286, 478, 445, 516, 281, 976, 291, 472, 1219, 33316, 3375, 13, 51458], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 626, "seek": 271348, "start": 2735.36, "end": 2741.28, "text": " And the basic idea of it is to prevent those representations from collapsing.", "tokens": [51458, 400, 264, 3875, 1558, 295, 309, 307, 281, 4871, 729, 33358, 490, 45339, 13, 51754], "temperature": 0.0, "avg_logprob": -0.28110597814832416, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.00037992841680534184}, {"id": 627, "seek": 274128, "start": 2741.28, "end": 2744.0400000000004, "text": " We're going to use a criterion that attempts to maximize the information", "tokens": [50364, 492, 434, 516, 281, 764, 257, 46691, 300, 15257, 281, 19874, 264, 1589, 50502], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 628, "seek": 274128, "start": 2744.0400000000004, "end": 2745.88, "text": " content that comes out of those representations.", "tokens": [50502, 2701, 300, 1487, 484, 295, 729, 33358, 13, 50594], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 629, "seek": 274128, "start": 2747.32, "end": 2750.2400000000002, "text": " Okay, so we're going to measure the information content in some way, and", "tokens": [50666, 1033, 11, 370, 321, 434, 516, 281, 3481, 264, 1589, 2701, 294, 512, 636, 11, 293, 50812], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 630, "seek": 274128, "start": 2750.2400000000002, "end": 2753.28, "text": " then maximize the information content or minimize the negative information content.", "tokens": [50812, 550, 19874, 264, 1589, 2701, 420, 17522, 264, 3671, 1589, 2701, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 631, "seek": 274128, "start": 2754.48, "end": 2757.1600000000003, "text": " We're going to do this for both SX and SY.", "tokens": [51024, 492, 434, 516, 281, 360, 341, 337, 1293, 318, 55, 293, 32624, 13, 51158], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 632, "seek": 274128, "start": 2757.1600000000003, "end": 2759.48, "text": " We're also going to minimize the prediction error.", "tokens": [51158, 492, 434, 611, 516, 281, 17522, 264, 17630, 6713, 13, 51274], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 633, "seek": 274128, "start": 2759.48, "end": 2761.2000000000003, "text": " And if we have a latent variable, we're going to have to minimize", "tokens": [51274, 400, 498, 321, 362, 257, 48994, 7006, 11, 321, 434, 516, 281, 362, 281, 17522, 51360], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 634, "seek": 274128, "start": 2761.2000000000003, "end": 2763.2400000000002, "text": " the information content of that latent variable.", "tokens": [51360, 264, 1589, 2701, 295, 300, 48994, 7006, 13, 51462], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 635, "seek": 274128, "start": 2763.2400000000002, "end": 2765.1600000000003, "text": " I can't explain why, because it would take too long.", "tokens": [51462, 286, 393, 380, 2903, 983, 11, 570, 309, 576, 747, 886, 938, 13, 51558], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 636, "seek": 274128, "start": 2765.1600000000003, "end": 2769.6400000000003, "text": " But you have to do that also to prevent another type of collapse.", "tokens": [51558, 583, 291, 362, 281, 360, 300, 611, 281, 4871, 1071, 2010, 295, 15584, 13, 51782], "temperature": 0.0, "avg_logprob": -0.17789645989735922, "compression_ratio": 2.1041666666666665, "no_speech_prob": 0.00017940785619430244}, {"id": 637, "seek": 276964, "start": 2769.64, "end": 2772.04, "text": " I'm going to focus on how you do that.", "tokens": [50364, 286, 478, 516, 281, 1879, 322, 577, 291, 360, 300, 13, 50484], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 638, "seek": 276964, "start": 2772.04, "end": 2776.7599999999998, "text": " So the sad news is we don't have good ways to measure information content, or", "tokens": [50484, 407, 264, 4227, 2583, 307, 321, 500, 380, 362, 665, 2098, 281, 3481, 1589, 2701, 11, 420, 50720], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 639, "seek": 276964, "start": 2776.7599999999998, "end": 2781.4, "text": " we don't have any good ways to estimate lower bounds on information content,", "tokens": [50720, 321, 500, 380, 362, 604, 665, 2098, 281, 12539, 3126, 29905, 322, 1589, 2701, 11, 50952], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 640, "seek": 276964, "start": 2781.4, "end": 2785.0, "text": " so that if we push up on this lower bound, the information content will go up.", "tokens": [50952, 370, 300, 498, 321, 2944, 493, 322, 341, 3126, 5472, 11, 264, 1589, 2701, 486, 352, 493, 13, 51132], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 641, "seek": 276964, "start": 2785.0, "end": 2787.3199999999997, "text": " We only have upper bounds for information content.", "tokens": [51132, 492, 787, 362, 6597, 29905, 337, 1589, 2701, 13, 51248], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 642, "seek": 276964, "start": 2787.3199999999997, "end": 2790.64, "text": " So we're going to do a very stupid thing, which is push up on the upper bound of", "tokens": [51248, 407, 321, 434, 516, 281, 360, 257, 588, 6631, 551, 11, 597, 307, 2944, 493, 322, 264, 6597, 5472, 295, 51414], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 643, "seek": 276964, "start": 2790.64, "end": 2793.7599999999998, "text": " information content, and hope the actual information content will follow.", "tokens": [51414, 1589, 2701, 11, 293, 1454, 264, 3539, 1589, 2701, 486, 1524, 13, 51570], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 644, "seek": 276964, "start": 2795.8399999999997, "end": 2796.2799999999997, "text": " And it works.", "tokens": [51674, 400, 309, 1985, 13, 51696], "temperature": 0.0, "avg_logprob": -0.19914371736588016, "compression_ratio": 2.1298701298701297, "no_speech_prob": 9.459981811232865e-05}, {"id": 645, "seek": 279628, "start": 2797.28, "end": 2804.8, "text": " So, there's a simple way to prevent the encoder from completely collapsing.", "tokens": [50414, 407, 11, 456, 311, 257, 2199, 636, 281, 4871, 264, 2058, 19866, 490, 2584, 45339, 13, 50790], "temperature": 0.0, "avg_logprob": -0.25091723805850313, "compression_ratio": 1.691588785046729, "no_speech_prob": 8.213531691581011e-05}, {"id": 646, "seek": 279628, "start": 2804.8, "end": 2808.4, "text": " Which is to insist that every variable in SX, SX is a vector.", "tokens": [50790, 3013, 307, 281, 13466, 300, 633, 7006, 294, 318, 55, 11, 318, 55, 307, 257, 8062, 13, 50970], "temperature": 0.0, "avg_logprob": -0.25091723805850313, "compression_ratio": 1.691588785046729, "no_speech_prob": 8.213531691581011e-05}, {"id": 647, "seek": 279628, "start": 2808.4, "end": 2811.7200000000003, "text": " And you insist that every variable, as measured over a batch,", "tokens": [50970, 400, 291, 13466, 300, 633, 7006, 11, 382, 12690, 670, 257, 15245, 11, 51136], "temperature": 0.0, "avg_logprob": -0.25091723805850313, "compression_ratio": 1.691588785046729, "no_speech_prob": 8.213531691581011e-05}, {"id": 648, "seek": 279628, "start": 2811.7200000000003, "end": 2814.96, "text": " has a standard deviation that is at least one.", "tokens": [51136, 575, 257, 3832, 25163, 300, 307, 412, 1935, 472, 13, 51298], "temperature": 0.0, "avg_logprob": -0.25091723805850313, "compression_ratio": 1.691588785046729, "no_speech_prob": 8.213531691581011e-05}, {"id": 649, "seek": 279628, "start": 2814.96, "end": 2817.6800000000003, "text": " Okay, so this is the cost that you see at the top here.", "tokens": [51298, 1033, 11, 370, 341, 307, 264, 2063, 300, 291, 536, 412, 264, 1192, 510, 13, 51434], "temperature": 0.0, "avg_logprob": -0.25091723805850313, "compression_ratio": 1.691588785046729, "no_speech_prob": 8.213531691581011e-05}, {"id": 650, "seek": 279628, "start": 2819.2000000000003, "end": 2823.1200000000003, "text": " Measure the standard deviation of each component of SX, and", "tokens": [51510, 41436, 264, 3832, 25163, 295, 1184, 6542, 295, 318, 55, 11, 293, 51706], "temperature": 0.0, "avg_logprob": -0.25091723805850313, "compression_ratio": 1.691588785046729, "no_speech_prob": 8.213531691581011e-05}, {"id": 651, "seek": 282312, "start": 2823.12, "end": 2826.2, "text": " put it in a hinge loss so that the standard deviation is at least one.", "tokens": [50364, 829, 309, 294, 257, 28822, 4470, 370, 300, 264, 3832, 25163, 307, 412, 1935, 472, 13, 50518], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 652, "seek": 282312, "start": 2827.3199999999997, "end": 2829.56, "text": " So that prevents the system from completely collapsing.", "tokens": [50574, 407, 300, 22367, 264, 1185, 490, 2584, 45339, 13, 50686], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 653, "seek": 282312, "start": 2829.56, "end": 2836.2, "text": " But it can still cheat by making all the components of SX equal or correlated.", "tokens": [50686, 583, 309, 393, 920, 17470, 538, 1455, 439, 264, 6677, 295, 318, 55, 2681, 420, 38574, 13, 51018], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 654, "seek": 282312, "start": 2836.2, "end": 2840.8399999999997, "text": " So the second term says I want to minimize the off diagonal terms of", "tokens": [51018, 407, 264, 1150, 1433, 1619, 286, 528, 281, 17522, 264, 766, 21539, 2115, 295, 51250], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 655, "seek": 282312, "start": 2840.8399999999997, "end": 2844.52, "text": " the covariance matrix of those vectors measured over a batch, right?", "tokens": [51250, 264, 49851, 719, 8141, 295, 729, 18875, 12690, 670, 257, 15245, 11, 558, 30, 51434], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 656, "seek": 282312, "start": 2844.52, "end": 2847.6, "text": " So I want pairs of variables to be uncorrelated.", "tokens": [51434, 407, 286, 528, 15494, 295, 9102, 281, 312, 6219, 284, 12004, 13, 51588], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 657, "seek": 282312, "start": 2848.96, "end": 2851.7999999999997, "text": " So basically, the collection of those two criterion says,", "tokens": [51656, 407, 1936, 11, 264, 5765, 295, 729, 732, 46691, 1619, 11, 51798], "temperature": 0.0, "avg_logprob": -0.21607517977373317, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00020984582079108804}, {"id": 658, "seek": 285180, "start": 2851.8, "end": 2856.4, "text": " if I measure the covariance matrix of those vectors, SX and SY, coming out over", "tokens": [50364, 498, 286, 3481, 264, 49851, 719, 8141, 295, 729, 18875, 11, 318, 55, 293, 318, 56, 11, 1348, 484, 670, 50594], "temperature": 0.0, "avg_logprob": -0.32018832837120964, "compression_ratio": 1.5808823529411764, "no_speech_prob": 8.437580981990322e-05}, {"id": 659, "seek": 285180, "start": 2856.4, "end": 2860.8, "text": " a batch, I want the covariance matrix to be as close to the identity as possible.", "tokens": [50594, 257, 15245, 11, 286, 528, 264, 49851, 719, 8141, 281, 312, 382, 1998, 281, 264, 6575, 382, 1944, 13, 50814], "temperature": 0.0, "avg_logprob": -0.32018832837120964, "compression_ratio": 1.5808823529411764, "no_speech_prob": 8.437580981990322e-05}, {"id": 660, "seek": 285180, "start": 2862.52, "end": 2865.88, "text": " There's a number of different methods that have been proposed to,", "tokens": [50900, 821, 311, 257, 1230, 295, 819, 7150, 300, 362, 668, 10348, 281, 11, 51068], "temperature": 0.0, "avg_logprob": -0.32018832837120964, "compression_ratio": 1.5808823529411764, "no_speech_prob": 8.437580981990322e-05}, {"id": 661, "seek": 285180, "start": 2865.88, "end": 2867.96, "text": " that are kind of similar to this, Barlow-Twins.", "tokens": [51068, 300, 366, 733, 295, 2531, 281, 341, 11, 4156, 14107, 12, 51, 86, 1292, 13, 51172], "temperature": 0.0, "avg_logprob": -0.32018832837120964, "compression_ratio": 1.5808823529411764, "no_speech_prob": 8.437580981990322e-05}, {"id": 662, "seek": 285180, "start": 2867.96, "end": 2873.04, "text": " So this one is called Vic-Rag from my group at Meta in collaboration with Jean-Ponce.", "tokens": [51172, 407, 341, 472, 307, 1219, 33316, 12, 49, 559, 490, 452, 1594, 412, 6377, 64, 294, 9363, 365, 13854, 12, 47, 266, 384, 13, 51426], "temperature": 0.0, "avg_logprob": -0.32018832837120964, "compression_ratio": 1.5808823529411764, "no_speech_prob": 8.437580981990322e-05}, {"id": 663, "seek": 285180, "start": 2874.84, "end": 2879.98, "text": " And then variations of it, but like similar methods from Berkeley in", "tokens": [51516, 400, 550, 17840, 295, 309, 11, 457, 411, 2531, 7150, 490, 23684, 294, 51773], "temperature": 0.0, "avg_logprob": -0.32018832837120964, "compression_ratio": 1.5808823529411764, "no_speech_prob": 8.437580981990322e-05}, {"id": 664, "seek": 287998, "start": 2880.94, "end": 2883.94, "text": " the E-Mise group at Berkeley called NCR squared.", "tokens": [50412, 264, 462, 12, 44, 908, 1594, 412, 23684, 1219, 426, 18547, 8889, 13, 50562], "temperature": 0.0, "avg_logprob": -0.38769141576623406, "compression_ratio": 1.5, "no_speech_prob": 0.00031348507036454976}, {"id": 665, "seek": 287998, "start": 2883.94, "end": 2885.66, "text": " Yeah, maybe one minute.", "tokens": [50562, 865, 11, 1310, 472, 3456, 13, 50648], "temperature": 0.0, "avg_logprob": -0.38769141576623406, "compression_ratio": 1.5, "no_speech_prob": 0.00031348507036454976}, {"id": 666, "seek": 287998, "start": 2885.66, "end": 2889.42, "text": " Yeah, so this works really well and", "tokens": [50648, 865, 11, 370, 341, 1985, 534, 731, 293, 50836], "temperature": 0.0, "avg_logprob": -0.38769141576623406, "compression_ratio": 1.5, "no_speech_prob": 0.00031348507036454976}, {"id": 667, "seek": 287998, "start": 2889.42, "end": 2896.06, "text": " I'm going to not bore you with tables of results that show you how well it works.", "tokens": [50836, 286, 478, 516, 281, 406, 26002, 291, 365, 8020, 295, 3542, 300, 855, 291, 577, 731, 309, 1985, 13, 51168], "temperature": 0.0, "avg_logprob": -0.38769141576623406, "compression_ratio": 1.5, "no_speech_prob": 0.00031348507036454976}, {"id": 668, "seek": 287998, "start": 2896.06, "end": 2901.86, "text": " Only to mention something else, which is another method to do this kind of", "tokens": [51168, 5686, 281, 2152, 746, 1646, 11, 597, 307, 1071, 3170, 281, 360, 341, 733, 295, 51458], "temperature": 0.0, "avg_logprob": -0.38769141576623406, "compression_ratio": 1.5, "no_speech_prob": 0.00031348507036454976}, {"id": 669, "seek": 287998, "start": 2901.86, "end": 2907.18, "text": " self supervised running which is closer to this JEPA architecture called IJEPA.", "tokens": [51458, 2698, 46533, 2614, 597, 307, 4966, 281, 341, 508, 8929, 32, 9482, 1219, 286, 41, 8929, 32, 13, 51724], "temperature": 0.0, "avg_logprob": -0.38769141576623406, "compression_ratio": 1.5, "no_speech_prob": 0.00031348507036454976}, {"id": 670, "seek": 290718, "start": 2907.18, "end": 2912.3399999999997, "text": " So this is for learning features for images without having to do the documentation.", "tokens": [50364, 407, 341, 307, 337, 2539, 4122, 337, 5267, 1553, 1419, 281, 360, 264, 14333, 13, 50622], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 671, "seek": 290718, "start": 2912.3399999999997, "end": 2915.3399999999997, "text": " But basically it's for masking and this works amazingly well, it's very fast.", "tokens": [50622, 583, 1936, 309, 311, 337, 31226, 293, 341, 1985, 31762, 731, 11, 309, 311, 588, 2370, 13, 50772], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 672, "seek": 290718, "start": 2915.3399999999997, "end": 2918.18, "text": " It's a new method, paper is on archive.", "tokens": [50772, 467, 311, 257, 777, 3170, 11, 3035, 307, 322, 23507, 13, 50914], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 673, "seek": 290718, "start": 2919.98, "end": 2925.74, "text": " I don't have time to explain how it works, but basically you run an image", "tokens": [51004, 286, 500, 380, 362, 565, 281, 2903, 577, 309, 1985, 11, 457, 1936, 291, 1190, 364, 3256, 51292], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 674, "seek": 290718, "start": 2925.74, "end": 2929.2599999999998, "text": " through two encoders, one is the full image and", "tokens": [51292, 807, 732, 2058, 378, 433, 11, 472, 307, 264, 1577, 3256, 293, 51468], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 675, "seek": 290718, "start": 2929.2599999999998, "end": 2933.8599999999997, "text": " the other one is sort of a masked image, partially masked image.", "tokens": [51468, 264, 661, 472, 307, 1333, 295, 257, 45249, 3256, 11, 18886, 45249, 3256, 13, 51698], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 676, "seek": 290718, "start": 2933.8599999999997, "end": 2936.46, "text": " You run them through the same encoder or very similar encoder and", "tokens": [51698, 509, 1190, 552, 807, 264, 912, 2058, 19866, 420, 588, 2531, 2058, 19866, 293, 51828], "temperature": 0.0, "avg_logprob": -0.24679284260190767, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.00011588320921873674}, {"id": 677, "seek": 293646, "start": 2936.46, "end": 2941.2200000000003, "text": " you try to predict or to predict the full feature representation of the full image", "tokens": [50364, 291, 853, 281, 6069, 420, 281, 6069, 264, 1577, 4111, 10290, 295, 264, 1577, 3256, 50602], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 678, "seek": 293646, "start": 2941.2200000000003, "end": 2944.1, "text": " from the representation obtained from the partial image.", "tokens": [50602, 490, 264, 10290, 14879, 490, 264, 14641, 3256, 13, 50746], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 679, "seek": 293646, "start": 2944.1, "end": 2947.1, "text": " And just doing this produces really good features for images.", "tokens": [50746, 400, 445, 884, 341, 14725, 534, 665, 4122, 337, 5267, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 680, "seek": 293646, "start": 2947.1, "end": 2951.42, "text": " You get really good performance on object recognition in images and stuff like that.", "tokens": [50896, 509, 483, 534, 665, 3389, 322, 2657, 11150, 294, 5267, 293, 1507, 411, 300, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 681, "seek": 293646, "start": 2951.42, "end": 2954.02, "text": " Again, tables that show you that's true.", "tokens": [51112, 3764, 11, 8020, 300, 855, 291, 300, 311, 2074, 13, 51242], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 682, "seek": 293646, "start": 2954.02, "end": 2958.82, "text": " But I'm coming to the end, so the reason for", "tokens": [51242, 583, 286, 478, 1348, 281, 264, 917, 11, 370, 264, 1778, 337, 51482], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 683, "seek": 293646, "start": 2958.82, "end": 2961.14, "text": " training those JEPA is to build world models.", "tokens": [51482, 3097, 729, 508, 8929, 32, 307, 281, 1322, 1002, 5245, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 684, "seek": 293646, "start": 2961.14, "end": 2963.06, "text": " So architectures are this type.", "tokens": [51598, 407, 6331, 1303, 366, 341, 2010, 13, 51694], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 685, "seek": 293646, "start": 2963.06, "end": 2966.18, "text": " So this is a JEPA, but it's also a world model.", "tokens": [51694, 407, 341, 307, 257, 508, 8929, 32, 11, 457, 309, 311, 611, 257, 1002, 2316, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1727798552740188, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.0001903414959087968}, {"id": 686, "seek": 296618, "start": 2966.18, "end": 2969.14, "text": " That, given an observation about the state of the world,", "tokens": [50364, 663, 11, 2212, 364, 14816, 466, 264, 1785, 295, 264, 1002, 11, 50512], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 687, "seek": 296618, "start": 2969.14, "end": 2972.98, "text": " is going to be able to enter an action or imagined action in latent variables.", "tokens": [50512, 307, 516, 281, 312, 1075, 281, 3242, 364, 3069, 420, 16590, 3069, 294, 48994, 9102, 13, 50704], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 688, "seek": 296618, "start": 2972.98, "end": 2974.8199999999997, "text": " It's going to predict what's going to happen next in the world.", "tokens": [50704, 467, 311, 516, 281, 6069, 437, 311, 516, 281, 1051, 958, 294, 264, 1002, 13, 50796], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 689, "seek": 296618, "start": 2975.8999999999996, "end": 2979.3799999999997, "text": " And once the time passes by, we're going to observe what happens and", "tokens": [50850, 400, 1564, 264, 565, 11335, 538, 11, 321, 434, 516, 281, 11441, 437, 2314, 293, 51024], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 690, "seek": 296618, "start": 2979.3799999999997, "end": 2983.2599999999998, "text": " then perhaps adjust our system to train.", "tokens": [51024, 550, 4317, 4369, 527, 1185, 281, 3847, 13, 51218], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 691, "seek": 296618, "start": 2983.2599999999998, "end": 2987.5, "text": " But we want to use a hierarchical version of this where we can have a higher", "tokens": [51218, 583, 321, 528, 281, 764, 257, 35250, 804, 3037, 295, 341, 689, 321, 393, 362, 257, 2946, 51430], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 692, "seek": 296618, "start": 2987.5, "end": 2992.54, "text": " level, higher abstraction, higher level of abstraction representation that will", "tokens": [51430, 1496, 11, 2946, 37765, 11, 2946, 1496, 295, 37765, 10290, 300, 486, 51682], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 693, "seek": 296618, "start": 2992.54, "end": 2994.62, "text": " allow us to make predictions further in the future.", "tokens": [51682, 2089, 505, 281, 652, 21264, 3052, 294, 264, 2027, 13, 51786], "temperature": 0.0, "avg_logprob": -0.24391851967912379, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.00010884856601478532}, {"id": 694, "seek": 299462, "start": 2995.58, "end": 2999.1, "text": " Okay, I can't tell you the details of how I'm going to get to the train station,", "tokens": [50412, 1033, 11, 286, 393, 380, 980, 291, 264, 4365, 295, 577, 286, 478, 516, 281, 483, 281, 264, 3847, 5214, 11, 50588], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 695, "seek": 299462, "start": 2999.1, "end": 3002.8599999999997, "text": " but I know I'm going to have to be at the train station around 4 PM.", "tokens": [50588, 457, 286, 458, 286, 478, 516, 281, 362, 281, 312, 412, 264, 3847, 5214, 926, 1017, 12499, 13, 50776], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 696, "seek": 299462, "start": 3002.8599999999997, "end": 3006.1, "text": " Okay, so that's the high level.", "tokens": [50776, 1033, 11, 370, 300, 311, 264, 1090, 1496, 13, 50938], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 697, "seek": 299462, "start": 3006.1, "end": 3009.46, "text": " And we have early experiments with sort of various complicated neural net", "tokens": [50938, 400, 321, 362, 2440, 12050, 365, 1333, 295, 3683, 6179, 18161, 2533, 51106], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 698, "seek": 299462, "start": 3009.46, "end": 3012.8199999999997, "text": " architectures which I'm not going to detail to train from video,", "tokens": [51106, 6331, 1303, 597, 286, 478, 406, 516, 281, 2607, 281, 3847, 490, 960, 11, 51274], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 699, "seek": 299462, "start": 3012.8199999999997, "end": 3016.18, "text": " try to predict basically what's going to happen in the video using warping and", "tokens": [51274, 853, 281, 6069, 1936, 437, 311, 516, 281, 1051, 294, 264, 960, 1228, 1516, 3381, 293, 51442], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 700, "seek": 299462, "start": 3016.18, "end": 3017.62, "text": " stuff like that and it works really well.", "tokens": [51442, 1507, 411, 300, 293, 309, 1985, 534, 731, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 701, "seek": 299462, "start": 3017.62, "end": 3022.7, "text": " But in the end, what we'll have is a hierarchical system from which we can do", "tokens": [51514, 583, 294, 264, 917, 11, 437, 321, 603, 362, 307, 257, 35250, 804, 1185, 490, 597, 321, 393, 360, 51768], "temperature": 0.0, "avg_logprob": -0.20592215481926412, "compression_ratio": 1.7474747474747474, "no_speech_prob": 8.884381531970575e-05}, {"id": 702, "seek": 302270, "start": 3022.7, "end": 3026.7799999999997, "text": " a hierarchical planning and then we'll have been trained to predict what's", "tokens": [50364, 257, 35250, 804, 5038, 293, 550, 321, 603, 362, 668, 8895, 281, 6069, 437, 311, 50568], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 703, "seek": 302270, "start": 3026.7799999999997, "end": 3030.02, "text": " going to happen in the world as a consequence of actions or", "tokens": [50568, 516, 281, 1051, 294, 264, 1002, 382, 257, 18326, 295, 5909, 420, 50730], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 704, "seek": 302270, "start": 3030.02, "end": 3032.8199999999997, "text": " latent variables that we can observe, that we can infer.", "tokens": [50730, 48994, 9102, 300, 321, 393, 11441, 11, 300, 321, 393, 13596, 13, 50870], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 705, "seek": 302270, "start": 3034.9399999999996, "end": 3038.62, "text": " And those systems will be able to plan and reason and", "tokens": [50976, 400, 729, 3652, 486, 312, 1075, 281, 1393, 293, 1778, 293, 51160], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 706, "seek": 302270, "start": 3038.62, "end": 3041.74, "text": " will be controllable because the behavior is entirely controlled by the cost", "tokens": [51160, 486, 312, 45159, 712, 570, 264, 5223, 307, 7696, 10164, 538, 264, 2063, 51316], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 707, "seek": 302270, "start": 3041.74, "end": 3043.3799999999997, "text": " functions we ask you to minimize.", "tokens": [51316, 6828, 321, 1029, 291, 281, 17522, 13, 51398], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 708, "seek": 302270, "start": 3044.5, "end": 3050.74, "text": " And so much more controllable than current LNMs and that's pretty much the end.", "tokens": [51454, 400, 370, 709, 544, 45159, 712, 813, 2190, 441, 45, 26386, 293, 300, 311, 1238, 709, 264, 917, 13, 51766], "temperature": 0.0, "avg_logprob": -0.25615492321196054, "compression_ratio": 1.6964980544747081, "no_speech_prob": 0.00022687160526402295}, {"id": 709, "seek": 305074, "start": 3050.74, "end": 3053.8199999999997, "text": " So cell supervised learning is really the ticket.", "tokens": [50364, 407, 2815, 46533, 2539, 307, 534, 264, 10550, 13, 50518], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 710, "seek": 305074, "start": 3053.8199999999997, "end": 3058.58, "text": " Handling and certainty can be done with this energy-based model method and", "tokens": [50518, 8854, 1688, 293, 27022, 393, 312, 1096, 365, 341, 2281, 12, 6032, 2316, 3170, 293, 50756], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 711, "seek": 305074, "start": 3058.58, "end": 3062.5, "text": " using the joint embedding architecture that allows us to avoid predicting all", "tokens": [50756, 1228, 264, 7225, 12240, 3584, 9482, 300, 4045, 505, 281, 5042, 32884, 439, 50952], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 712, "seek": 305074, "start": 3062.5, "end": 3064.1, "text": " the details that are irrelevant about the world.", "tokens": [50952, 264, 4365, 300, 366, 28682, 466, 264, 1002, 13, 51032], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 713, "seek": 305074, "start": 3066.02, "end": 3068.7, "text": " Learning world models from observation and interaction and", "tokens": [51128, 15205, 1002, 5245, 490, 14816, 293, 9285, 293, 51262], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 714, "seek": 305074, "start": 3068.7, "end": 3073.54, "text": " then reasoning and planning is done by basically gradient-based minimization", "tokens": [51262, 550, 21577, 293, 5038, 307, 1096, 538, 1936, 16235, 12, 6032, 4464, 2144, 51504], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 715, "seek": 305074, "start": 3073.54, "end": 3075.2999999999997, "text": " with respect to actions.", "tokens": [51504, 365, 3104, 281, 5909, 13, 51592], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 716, "seek": 305074, "start": 3075.2999999999997, "end": 3076.4599999999996, "text": " And that's it, thank you very much.", "tokens": [51592, 400, 300, 311, 309, 11, 1309, 291, 588, 709, 13, 51650], "temperature": 0.0, "avg_logprob": -0.23687803268432617, "compression_ratio": 1.6842105263157894, "no_speech_prob": 5.1401209930190817e-05}, {"id": 717, "seek": 307646, "start": 3076.46, "end": 3096.02, "text": " Thank you, John, for the great talk.", "tokens": [50364, 1044, 291, 11, 2619, 11, 337, 264, 869, 751, 13, 51342], "temperature": 0.0, "avg_logprob": -0.6099211374918619, "compression_ratio": 1.256, "no_speech_prob": 0.005802845116704702}, {"id": 718, "seek": 307646, "start": 3096.02, "end": 3100.26, "text": " Now we'll have the second part, which is the Fireside Chat between John and", "tokens": [51342, 823, 321, 603, 362, 264, 1150, 644, 11, 597, 307, 264, 479, 3145, 482, 27503, 1296, 2619, 293, 51554], "temperature": 0.0, "avg_logprob": -0.6099211374918619, "compression_ratio": 1.256, "no_speech_prob": 0.005802845116704702}, {"id": 719, "seek": 307646, "start": 3100.26, "end": 3102.26, "text": " Osama, so please.", "tokens": [51554, 8875, 2404, 11, 370, 1767, 13, 51654], "temperature": 0.0, "avg_logprob": -0.6099211374918619, "compression_ratio": 1.256, "no_speech_prob": 0.005802845116704702}, {"id": 720, "seek": 307646, "start": 3102.26, "end": 3106.2200000000003, "text": " Thank you very much, John.", "tokens": [51654, 1044, 291, 588, 709, 11, 2619, 13, 51852], "temperature": 0.0, "avg_logprob": -0.6099211374918619, "compression_ratio": 1.256, "no_speech_prob": 0.005802845116704702}, {"id": 721, "seek": 310622, "start": 3106.22, "end": 3110.9399999999996, "text": " It was actually truly inspirational because it is definitely different", "tokens": [50364, 467, 390, 767, 4908, 33554, 570, 309, 307, 2138, 819, 50600], "temperature": 0.0, "avg_logprob": -0.20302278619063527, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.00044285019976086915}, {"id": 722, "seek": 310622, "start": 3110.9399999999996, "end": 3114.4199999999996, "text": " than your typical machine learning talk, so I enjoyed that.", "tokens": [50600, 813, 428, 7476, 3479, 2539, 751, 11, 370, 286, 4626, 300, 13, 50774], "temperature": 0.0, "avg_logprob": -0.20302278619063527, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.00044285019976086915}, {"id": 723, "seek": 310622, "start": 3114.4199999999996, "end": 3117.8599999999997, "text": " Well, to you to throw away all the basic pillars of machine learning, so yes.", "tokens": [50774, 1042, 11, 281, 291, 281, 3507, 1314, 439, 264, 3875, 26729, 295, 3479, 2539, 11, 370, 2086, 13, 50946], "temperature": 0.0, "avg_logprob": -0.20302278619063527, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.00044285019976086915}, {"id": 724, "seek": 310622, "start": 3120.4199999999996, "end": 3124.06, "text": " So I've collected a bunch of questions, some coming from the audience,", "tokens": [51074, 407, 286, 600, 11087, 257, 3840, 295, 1651, 11, 512, 1348, 490, 264, 4034, 11, 51256], "temperature": 0.0, "avg_logprob": -0.20302278619063527, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.00044285019976086915}, {"id": 725, "seek": 310622, "start": 3124.06, "end": 3127.4199999999996, "text": " some coming from our institute and our faculty.", "tokens": [51256, 512, 1348, 490, 527, 26860, 293, 527, 6389, 13, 51424], "temperature": 0.0, "avg_logprob": -0.20302278619063527, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.00044285019976086915}, {"id": 726, "seek": 310622, "start": 3127.4199999999996, "end": 3132.2599999999998, "text": " And we'll try to go through them in 20 minutes or whatever we can cover.", "tokens": [51424, 400, 321, 603, 853, 281, 352, 807, 552, 294, 945, 2077, 420, 2035, 321, 393, 2060, 13, 51666], "temperature": 0.0, "avg_logprob": -0.20302278619063527, "compression_ratio": 1.6260162601626016, "no_speech_prob": 0.00044285019976086915}, {"id": 727, "seek": 313226, "start": 3133.26, "end": 3137.5800000000004, "text": " Normally, I would commit to answering every question on social media, but", "tokens": [50414, 17424, 11, 286, 576, 5599, 281, 13430, 633, 1168, 322, 2093, 3021, 11, 457, 50630], "temperature": 0.0, "avg_logprob": -0.2435010698106554, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.0005015151691623032}, {"id": 728, "seek": 313226, "start": 3137.5800000000004, "end": 3142.2200000000003, "text": " because we got 150 questions, I'm afraid to commit my time or yours to this at", "tokens": [50630, 570, 321, 658, 8451, 1651, 11, 286, 478, 4638, 281, 5599, 452, 565, 420, 6342, 281, 341, 412, 50862], "temperature": 0.0, "avg_logprob": -0.2435010698106554, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.0005015151691623032}, {"id": 729, "seek": 313226, "start": 3142.2200000000003, "end": 3144.1000000000004, "text": " this point, but we'll try our best.", "tokens": [50862, 341, 935, 11, 457, 321, 603, 853, 527, 1151, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2435010698106554, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.0005015151691623032}, {"id": 730, "seek": 313226, "start": 3145.5400000000004, "end": 3146.94, "text": " So I'll start with my first question.", "tokens": [51028, 407, 286, 603, 722, 365, 452, 700, 1168, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2435010698106554, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.0005015151691623032}, {"id": 731, "seek": 313226, "start": 3148.26, "end": 3151.78, "text": " It's been a long-standing wisdom in statistical inference and", "tokens": [51164, 467, 311, 668, 257, 938, 12, 8618, 10712, 294, 22820, 38253, 293, 51340], "temperature": 0.0, "avg_logprob": -0.2435010698106554, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.0005015151691623032}, {"id": 732, "seek": 313226, "start": 3151.78, "end": 3157.7000000000003, "text": " probabilistic reasoning that when the number of parameters of a model gets", "tokens": [51340, 31959, 3142, 21577, 300, 562, 264, 1230, 295, 9834, 295, 257, 2316, 2170, 51636], "temperature": 0.0, "avg_logprob": -0.2435010698106554, "compression_ratio": 1.5316455696202531, "no_speech_prob": 0.0005015151691623032}, {"id": 733, "seek": 315770, "start": 3157.7, "end": 3162.1, "text": " large enough, you kind of lose your ability to generalize and", "tokens": [50364, 2416, 1547, 11, 291, 733, 295, 3624, 428, 3485, 281, 2674, 1125, 293, 50584], "temperature": 0.0, "avg_logprob": -0.1903062860171, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00039659137837588787}, {"id": 734, "seek": 315770, "start": 3162.1, "end": 3165.74, "text": " you start just memorizing data, and we all know that that's no good.", "tokens": [50584, 291, 722, 445, 10560, 3319, 1412, 11, 293, 321, 439, 458, 300, 300, 311, 572, 665, 13, 50766], "temperature": 0.0, "avg_logprob": -0.1903062860171, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00039659137837588787}, {"id": 735, "seek": 315770, "start": 3165.74, "end": 3169.98, "text": " That's just too detailed, the bias variance trade off.", "tokens": [50766, 663, 311, 445, 886, 9942, 11, 264, 12577, 21977, 4923, 766, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1903062860171, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00039659137837588787}, {"id": 736, "seek": 315770, "start": 3169.98, "end": 3176.54, "text": " But somehow, deep learning seems to have broken through this barrier.", "tokens": [50978, 583, 6063, 11, 2452, 2539, 2544, 281, 362, 5463, 807, 341, 13357, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1903062860171, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00039659137837588787}, {"id": 737, "seek": 315770, "start": 3176.54, "end": 3181.2999999999997, "text": " When we went from regular neural nets to the deep nets, and", "tokens": [51306, 1133, 321, 1437, 490, 3890, 18161, 36170, 281, 264, 2452, 36170, 11, 293, 51544], "temperature": 0.0, "avg_logprob": -0.1903062860171, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00039659137837588787}, {"id": 738, "seek": 315770, "start": 3181.2999999999997, "end": 3187.22, "text": " is there an intuition or understanding today as to why this is working in", "tokens": [51544, 307, 456, 364, 24002, 420, 3701, 965, 382, 281, 983, 341, 307, 1364, 294, 51840], "temperature": 0.0, "avg_logprob": -0.1903062860171, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00039659137837588787}, {"id": 739, "seek": 318722, "start": 3187.22, "end": 3190.98, "text": " LLMs with hundreds of billions and now trillions of parameters.", "tokens": [50364, 441, 43, 26386, 365, 6779, 295, 17375, 293, 586, 504, 46279, 295, 9834, 13, 50552], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 740, "seek": 318722, "start": 3190.98, "end": 3194.58, "text": " Right, well, the fact that it is working,", "tokens": [50552, 1779, 11, 731, 11, 264, 1186, 300, 309, 307, 1364, 11, 50732], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 741, "seek": 318722, "start": 3194.58, "end": 3199.18, "text": " that you can train a ridiculously over-sized neural net, and", "tokens": [50732, 300, 291, 393, 3847, 257, 41358, 670, 12, 20614, 18161, 2533, 11, 293, 50962], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 742, "seek": 318722, "start": 3199.18, "end": 3206.4599999999996, "text": " it will still work reasonably and generalize is dumb-founding.", "tokens": [50962, 309, 486, 920, 589, 23551, 293, 2674, 1125, 307, 10316, 12, 17493, 278, 13, 51326], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 743, "seek": 318722, "start": 3206.4599999999996, "end": 3209.8599999999997, "text": " So much that it contradicts every single thing that has been written in every", "tokens": [51326, 407, 709, 300, 309, 28900, 82, 633, 2167, 551, 300, 575, 668, 3720, 294, 633, 51496], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 744, "seek": 318722, "start": 3209.8599999999997, "end": 3211.7, "text": " statistical textbook.", "tokens": [51496, 22820, 25591, 13, 51588], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 745, "seek": 318722, "start": 3211.7, "end": 3214.98, "text": " That you should never have more parameters than you have training samples, right?", "tokens": [51588, 663, 291, 820, 1128, 362, 544, 9834, 813, 291, 362, 3097, 10938, 11, 558, 30, 51752], "temperature": 0.0, "avg_logprob": -0.228561797944626, "compression_ratio": 1.611764705882353, "no_speech_prob": 5.827863424201496e-05}, {"id": 746, "seek": 321498, "start": 3215.02, "end": 3217.58, "text": " If you're fitting a polynomial or something like this.", "tokens": [50366, 759, 291, 434, 15669, 257, 26110, 420, 746, 411, 341, 13, 50494], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 747, "seek": 321498, "start": 3217.58, "end": 3220.38, "text": " But we knew experimentally, even in the late 80s and early 90s,", "tokens": [50494, 583, 321, 2586, 5120, 379, 11, 754, 294, 264, 3469, 4688, 82, 293, 2440, 4289, 82, 11, 50634], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 748, "seek": 321498, "start": 3220.38, "end": 3222.7400000000002, "text": " that you could make those neural nets pretty big.", "tokens": [50634, 300, 291, 727, 652, 729, 18161, 36170, 1238, 955, 13, 50752], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 749, "seek": 321498, "start": 3222.7400000000002, "end": 3225.86, "text": " And even if you didn't have a huge amount of training data,", "tokens": [50752, 400, 754, 498, 291, 994, 380, 362, 257, 2603, 2372, 295, 3097, 1412, 11, 50908], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 750, "seek": 321498, "start": 3225.86, "end": 3227.5, "text": " it would still work pretty well.", "tokens": [50908, 309, 576, 920, 589, 1238, 731, 13, 50990], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 751, "seek": 321498, "start": 3227.5, "end": 3229.46, "text": " There was just no theoretical explanation.", "tokens": [50990, 821, 390, 445, 572, 20864, 10835, 13, 51088], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 752, "seek": 321498, "start": 3229.46, "end": 3232.7, "text": " So the theorists told us, you're wrong, you're stupid.", "tokens": [51088, 407, 264, 27423, 1751, 1907, 505, 11, 291, 434, 2085, 11, 291, 434, 6631, 13, 51250], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 753, "seek": 321498, "start": 3233.98, "end": 3237.38, "text": " This cannot possibly work, so I'm not gonna believe your results.", "tokens": [51314, 639, 2644, 6264, 589, 11, 370, 286, 478, 406, 799, 1697, 428, 3542, 13, 51484], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 754, "seek": 321498, "start": 3237.38, "end": 3241.54, "text": " And that's in part what made it very difficult to get neural nets", "tokens": [51484, 400, 300, 311, 294, 644, 437, 1027, 309, 588, 2252, 281, 483, 18161, 36170, 51692], "temperature": 0.0, "avg_logprob": -0.17532795159391654, "compression_ratio": 1.6204620462046204, "no_speech_prob": 0.0001822632912080735}, {"id": 755, "seek": 324154, "start": 3241.54, "end": 3245.66, "text": " accepted in the late 90s to the 2000s.", "tokens": [50364, 9035, 294, 264, 3469, 4289, 82, 281, 264, 8132, 82, 13, 50570], "temperature": 0.0, "avg_logprob": -0.2426330248514811, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.000436450878623873}, {"id": 756, "seek": 324154, "start": 3247.86, "end": 3251.14, "text": " But it turns out there is a phenomenon that has since been named", "tokens": [50680, 583, 309, 4523, 484, 456, 307, 257, 14029, 300, 575, 1670, 668, 4926, 50844], "temperature": 0.0, "avg_logprob": -0.2426330248514811, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.000436450878623873}, {"id": 757, "seek": 324154, "start": 3251.14, "end": 3256.14, "text": " double descent, which is that if you increase the number of parameters in", "tokens": [50844, 3834, 23475, 11, 597, 307, 300, 498, 291, 3488, 264, 1230, 295, 9834, 294, 51094], "temperature": 0.0, "avg_logprob": -0.2426330248514811, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.000436450878623873}, {"id": 758, "seek": 324154, "start": 3256.14, "end": 3262.18, "text": " a model for a constant size training set, your training error,", "tokens": [51094, 257, 2316, 337, 257, 5754, 2744, 3097, 992, 11, 428, 3097, 6713, 11, 51396], "temperature": 0.0, "avg_logprob": -0.2426330248514811, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.000436450878623873}, {"id": 759, "seek": 324154, "start": 3262.18, "end": 3264.7799999999997, "text": " of course, is gonna go down, right, to zero, probably.", "tokens": [51396, 295, 1164, 11, 307, 799, 352, 760, 11, 558, 11, 281, 4018, 11, 1391, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2426330248514811, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.000436450878623873}, {"id": 760, "seek": 324154, "start": 3266.1, "end": 3270.46, "text": " But your test error is first gonna go down, go through a minimum, and", "tokens": [51592, 583, 428, 1500, 6713, 307, 700, 799, 352, 760, 11, 352, 807, 257, 7285, 11, 293, 51810], "temperature": 0.0, "avg_logprob": -0.2426330248514811, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.000436450878623873}, {"id": 761, "seek": 327046, "start": 3270.5, "end": 3275.06, "text": " then go up when you start having parameters,", "tokens": [50366, 550, 352, 493, 562, 291, 722, 1419, 9834, 11, 50594], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 762, "seek": 327046, "start": 3275.06, "end": 3279.02, "text": " a number of parameters that is commensurate with the number of samples that you have.", "tokens": [50594, 257, 1230, 295, 9834, 300, 307, 800, 694, 33144, 365, 264, 1230, 295, 10938, 300, 291, 362, 13, 50792], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 763, "seek": 327046, "start": 3279.02, "end": 3283.82, "text": " Okay, so that's when the model starts to be over parameterized, and it goes up.", "tokens": [50792, 1033, 11, 370, 300, 311, 562, 264, 2316, 3719, 281, 312, 670, 13075, 1602, 11, 293, 309, 1709, 493, 13, 51032], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 764, "seek": 327046, "start": 3285.18, "end": 3288.54, "text": " But here is the thing, if you keep going up, if you keep making the model more", "tokens": [51100, 583, 510, 307, 264, 551, 11, 498, 291, 1066, 516, 493, 11, 498, 291, 1066, 1455, 264, 2316, 544, 51268], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 765, "seek": 327046, "start": 3288.54, "end": 3292.18, "text": " complex, the tester will go down again.", "tokens": [51268, 3997, 11, 264, 36101, 486, 352, 760, 797, 13, 51450], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 766, "seek": 327046, "start": 3292.18, "end": 3294.02, "text": " It will go through a maximum and then go down again.", "tokens": [51450, 467, 486, 352, 807, 257, 6674, 293, 550, 352, 760, 797, 13, 51542], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 767, "seek": 327046, "start": 3294.02, "end": 3296.62, "text": " That's called the double descent phenomenon, nowadays.", "tokens": [51542, 663, 311, 1219, 264, 3834, 23475, 14029, 11, 13434, 13, 51672], "temperature": 0.0, "avg_logprob": -0.26927254263278655, "compression_ratio": 1.7620967741935485, "no_speech_prob": 8.219435403589159e-05}, {"id": 768, "seek": 329662, "start": 3296.62, "end": 3301.98, "text": " And it will do this if you regularize the parameters somehow.", "tokens": [50364, 400, 309, 486, 360, 341, 498, 291, 3890, 1125, 264, 9834, 6063, 13, 50632], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 769, "seek": 329662, "start": 3301.98, "end": 3306.06, "text": " You don't necessarily need to regularize explicitly because neural nets have some", "tokens": [50632, 509, 500, 380, 4725, 643, 281, 3890, 1125, 20803, 570, 18161, 36170, 362, 512, 50836], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 770, "seek": 329662, "start": 3306.06, "end": 3308.18, "text": " sort of implicit regularization in them.", "tokens": [50836, 1333, 295, 26947, 3890, 2144, 294, 552, 13, 50942], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 771, "seek": 329662, "start": 3308.18, "end": 3313.98, "text": " But you see this phenomenon, even works if you fit a polynomial, right?", "tokens": [50942, 583, 291, 536, 341, 14029, 11, 754, 1985, 498, 291, 3318, 257, 26110, 11, 558, 30, 51232], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 772, "seek": 329662, "start": 3313.98, "end": 3319.7, "text": " Fit a 10 degree polynomial with 11 data points.", "tokens": [51232, 29263, 257, 1266, 4314, 26110, 365, 2975, 1412, 2793, 13, 51518], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 773, "seek": 329662, "start": 3320.8199999999997, "end": 3323.18, "text": " And your fit will be horrible, right?", "tokens": [51574, 400, 428, 3318, 486, 312, 9263, 11, 558, 30, 51692], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 774, "seek": 329662, "start": 3323.18, "end": 3325.8199999999997, "text": " Because the polynomial has to go to every single point and", "tokens": [51692, 1436, 264, 26110, 575, 281, 352, 281, 633, 2167, 935, 293, 51824], "temperature": 0.0, "avg_logprob": -0.29948614061493234, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.00011588929919525981}, {"id": 775, "seek": 332582, "start": 3325.82, "end": 3327.54, "text": " it's gonna go wild in between.", "tokens": [50364, 309, 311, 799, 352, 4868, 294, 1296, 13, 50450], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 776, "seek": 332582, "start": 3327.54, "end": 3330.26, "text": " But if you increase the degree of the polynomial to something like 20 or", "tokens": [50450, 583, 498, 291, 3488, 264, 4314, 295, 264, 26110, 281, 746, 411, 945, 420, 50586], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 777, "seek": 332582, "start": 3330.26, "end": 3333.86, "text": " 30, and you regularize the coefficient, your error goes down again,", "tokens": [50586, 2217, 11, 293, 291, 3890, 1125, 264, 17619, 11, 428, 6713, 1709, 760, 797, 11, 50766], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 778, "seek": 332582, "start": 3333.86, "end": 3338.98, "text": " your test error goes down again, the fitted polynomial goes through every point.", "tokens": [50766, 428, 1500, 6713, 1709, 760, 797, 11, 264, 26321, 26110, 1709, 807, 633, 935, 13, 51022], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 779, "seek": 332582, "start": 3338.98, "end": 3344.34, "text": " But it's less irregular than with just degree 10.", "tokens": [51022, 583, 309, 311, 1570, 29349, 813, 365, 445, 4314, 1266, 13, 51290], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 780, "seek": 332582, "start": 3344.34, "end": 3348.54, "text": " So this existed all along, it's just that people didn't realize it was a thing,", "tokens": [51290, 407, 341, 13135, 439, 2051, 11, 309, 311, 445, 300, 561, 994, 380, 4325, 309, 390, 257, 551, 11, 51500], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 781, "seek": 332582, "start": 3348.54, "end": 3352.02, "text": " or at least people who were not practitioners of neural nets who", "tokens": [51500, 420, 412, 1935, 561, 567, 645, 406, 25742, 295, 18161, 36170, 567, 51674], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 782, "seek": 332582, "start": 3352.02, "end": 3353.6600000000003, "text": " had realized this was a thing.", "tokens": [51674, 632, 5334, 341, 390, 257, 551, 13, 51756], "temperature": 0.0, "avg_logprob": -0.23361007305754333, "compression_ratio": 1.7573529411764706, "no_speech_prob": 0.00017660294543020427}, {"id": 783, "seek": 335366, "start": 3353.66, "end": 3355.7, "text": " So do we have any explanation why this is a thing?", "tokens": [50364, 407, 360, 321, 362, 604, 10835, 983, 341, 307, 257, 551, 30, 50466], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 784, "seek": 335366, "start": 3357.18, "end": 3361.8199999999997, "text": " So there's a lot of conjectures, there is some theoretical work.", "tokens": [50540, 407, 456, 311, 257, 688, 295, 416, 1020, 1303, 11, 456, 307, 512, 20864, 589, 13, 50772], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 785, "seek": 335366, "start": 3361.8199999999997, "end": 3364.8199999999997, "text": " Some people claim it's about the dynamics of gradient descent.", "tokens": [50772, 2188, 561, 3932, 309, 311, 466, 264, 15679, 295, 16235, 23475, 13, 50922], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 786, "seek": 335366, "start": 3364.8199999999997, "end": 3368.3799999999997, "text": " There is some sort of implicit self regularization in neural nets that occurs.", "tokens": [50922, 821, 307, 512, 1333, 295, 26947, 2698, 3890, 2144, 294, 18161, 36170, 300, 11843, 13, 51100], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 787, "seek": 335366, "start": 3369.66, "end": 3373.14, "text": " Whereby the system kind of recruits just a number of virtual", "tokens": [51164, 2305, 2322, 264, 1185, 733, 295, 9372, 1208, 445, 257, 1230, 295, 6374, 51338], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 788, "seek": 335366, "start": 3373.14, "end": 3375.14, "text": " parameters that it needs somehow.", "tokens": [51338, 9834, 300, 309, 2203, 6063, 13, 51438], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 789, "seek": 335366, "start": 3376.62, "end": 3379.7799999999997, "text": " Some say it's regularization due to stochastic gradient.", "tokens": [51512, 2188, 584, 309, 311, 3890, 2144, 3462, 281, 342, 8997, 2750, 16235, 13, 51670], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 790, "seek": 335366, "start": 3379.7799999999997, "end": 3382.1, "text": " So stochastic gradient descent, which is noisy.", "tokens": [51670, 407, 342, 8997, 2750, 16235, 23475, 11, 597, 307, 24518, 13, 51786], "temperature": 0.0, "avg_logprob": -0.24867079175751786, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00025282669230364263}, {"id": 791, "seek": 338210, "start": 3382.1, "end": 3388.2599999999998, "text": " And so perhaps that forces the system to find robust minima in", "tokens": [50364, 400, 370, 4317, 300, 5874, 264, 1185, 281, 915, 13956, 4464, 64, 294, 50672], "temperature": 0.0, "avg_logprob": -0.2963482992989676, "compression_ratio": 1.554054054054054, "no_speech_prob": 7.835206633899361e-05}, {"id": 792, "seek": 338210, "start": 3388.2599999999998, "end": 3392.9, "text": " the objective, in the loss, that generalize better.", "tokens": [50672, 264, 10024, 11, 294, 264, 4470, 11, 300, 2674, 1125, 1101, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2963482992989676, "compression_ratio": 1.554054054054054, "no_speech_prob": 7.835206633899361e-05}, {"id": 793, "seek": 338210, "start": 3394.3399999999997, "end": 3396.1, "text": " It's not clear, there's a bunch of different things.", "tokens": [50976, 467, 311, 406, 1850, 11, 456, 311, 257, 3840, 295, 819, 721, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2963482992989676, "compression_ratio": 1.554054054054054, "no_speech_prob": 7.835206633899361e-05}, {"id": 794, "seek": 338210, "start": 3396.1, "end": 3399.54, "text": " Yeah, definitely one of the mysteries that keep us interested.", "tokens": [51064, 865, 11, 2138, 472, 295, 264, 30785, 300, 1066, 505, 3102, 13, 51236], "temperature": 0.0, "avg_logprob": -0.2963482992989676, "compression_ratio": 1.554054054054054, "no_speech_prob": 7.835206633899361e-05}, {"id": 795, "seek": 338210, "start": 3401.2999999999997, "end": 3405.5, "text": " This question comes from Raman Chandrasekharan or", "tokens": [51324, 639, 1168, 1487, 490, 497, 6147, 32244, 81, 651, 74, 5854, 282, 420, 51534], "temperature": 0.0, "avg_logprob": -0.2963482992989676, "compression_ratio": 1.554054054054054, "no_speech_prob": 7.835206633899361e-05}, {"id": 796, "seek": 338210, "start": 3405.5, "end": 3409.2599999999998, "text": " Chandra, who's one of our senior research scientists in Seattle.", "tokens": [51534, 761, 18401, 11, 567, 311, 472, 295, 527, 7965, 2132, 7708, 294, 15721, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2963482992989676, "compression_ratio": 1.554054054054054, "no_speech_prob": 7.835206633899361e-05}, {"id": 797, "seek": 340926, "start": 3410.1400000000003, "end": 3416.6600000000003, "text": " How long before LLM, and maybe, I don't know, models in general,", "tokens": [50408, 1012, 938, 949, 441, 43, 44, 11, 293, 1310, 11, 286, 500, 380, 458, 11, 5245, 294, 2674, 11, 50734], "temperature": 0.0, "avg_logprob": -0.2399323121556696, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0004161491524428129}, {"id": 798, "seek": 340926, "start": 3416.6600000000003, "end": 3421.34, "text": " can genuinely start saying, I don't know the answer to this question.", "tokens": [50734, 393, 17839, 722, 1566, 11, 286, 500, 380, 458, 264, 1867, 281, 341, 1168, 13, 50968], "temperature": 0.0, "avg_logprob": -0.2399323121556696, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0004161491524428129}, {"id": 799, "seek": 340926, "start": 3421.34, "end": 3425.7000000000003, "text": " As opposed to attempting to guess the right autocomplete anyway,", "tokens": [50968, 1018, 8851, 281, 22001, 281, 2041, 264, 558, 45833, 298, 17220, 4033, 11, 51186], "temperature": 0.0, "avg_logprob": -0.2399323121556696, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0004161491524428129}, {"id": 800, "seek": 340926, "start": 3425.7000000000003, "end": 3427.0600000000004, "text": " because that's what it's programmed to do.", "tokens": [51186, 570, 300, 311, 437, 309, 311, 31092, 281, 360, 13, 51254], "temperature": 0.0, "avg_logprob": -0.2399323121556696, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0004161491524428129}, {"id": 801, "seek": 340926, "start": 3428.0600000000004, "end": 3432.7000000000003, "text": " Yeah, so I don't think current LLMs can really do this at the moment.", "tokens": [51304, 865, 11, 370, 286, 500, 380, 519, 2190, 441, 43, 26386, 393, 534, 360, 341, 412, 264, 1623, 13, 51536], "temperature": 0.0, "avg_logprob": -0.2399323121556696, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0004161491524428129}, {"id": 802, "seek": 340926, "start": 3432.7000000000003, "end": 3435.9, "text": " I think it's probably possible with architectures, the type that I show.", "tokens": [51536, 286, 519, 309, 311, 1391, 1944, 365, 6331, 1303, 11, 264, 2010, 300, 286, 855, 13, 51696], "temperature": 0.0, "avg_logprob": -0.2399323121556696, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0004161491524428129}, {"id": 803, "seek": 343590, "start": 3435.9, "end": 3441.3, "text": " Because if there are no good minima to the objective that the system is", "tokens": [50364, 1436, 498, 456, 366, 572, 665, 4464, 64, 281, 264, 10024, 300, 264, 1185, 307, 50634], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 804, "seek": 343590, "start": 3441.3, "end": 3445.02, "text": " attempting to minimize to produce it, it's output, it's gonna say, well,", "tokens": [50634, 22001, 281, 17522, 281, 5258, 309, 11, 309, 311, 5598, 11, 309, 311, 799, 584, 11, 731, 11, 50820], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 805, "seek": 343590, "start": 3445.02, "end": 3449.1800000000003, "text": " I found this thing, it seems to be minimizing this objective, but not very well.", "tokens": [50820, 286, 1352, 341, 551, 11, 309, 2544, 281, 312, 46608, 341, 10024, 11, 457, 406, 588, 731, 13, 51028], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 806, "seek": 343590, "start": 3449.1800000000003, "end": 3451.7400000000002, "text": " So probably it's not the right answer you were looking for.", "tokens": [51028, 407, 1391, 309, 311, 406, 264, 558, 1867, 291, 645, 1237, 337, 13, 51156], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 807, "seek": 343590, "start": 3453.1800000000003, "end": 3457.94, "text": " Or by the shape of the minimum, of this energy minimum, perhaps, you could say,", "tokens": [51228, 1610, 538, 264, 3909, 295, 264, 7285, 11, 295, 341, 2281, 7285, 11, 4317, 11, 291, 727, 584, 11, 51466], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 808, "seek": 343590, "start": 3457.94, "end": 3462.1, "text": " like if it's really a sharp minimum, then that's the one answer that", "tokens": [51466, 411, 498, 309, 311, 534, 257, 8199, 7285, 11, 550, 300, 311, 264, 472, 1867, 300, 51674], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 809, "seek": 343590, "start": 3462.1, "end": 3464.5, "text": " corresponds to the question.", "tokens": [51674, 23249, 281, 264, 1168, 13, 51794], "temperature": 0.0, "avg_logprob": -0.23115097681681315, "compression_ratio": 1.7406015037593985, "no_speech_prob": 0.0005173025419935584}, {"id": 810, "seek": 346450, "start": 3464.5, "end": 3468.18, "text": " If it's kind of a shadow minimum, maybe there are multiple answers that are possible.", "tokens": [50364, 759, 309, 311, 733, 295, 257, 8576, 7285, 11, 1310, 456, 366, 3866, 6338, 300, 366, 1944, 13, 50548], "temperature": 0.0, "avg_logprob": -0.3156775890941351, "compression_ratio": 1.4619565217391304, "no_speech_prob": 4.399098543217406e-05}, {"id": 811, "seek": 346450, "start": 3468.18, "end": 3475.18, "text": " So you might be able to attribute, to map energy levels to,", "tokens": [50548, 407, 291, 1062, 312, 1075, 281, 19667, 11, 281, 4471, 2281, 4358, 281, 11, 50898], "temperature": 0.0, "avg_logprob": -0.3156775890941351, "compression_ratio": 1.4619565217391304, "no_speech_prob": 4.399098543217406e-05}, {"id": 812, "seek": 346450, "start": 3475.18, "end": 3479.34, "text": " of different answers to a confidence level.", "tokens": [50898, 295, 819, 6338, 281, 257, 6687, 1496, 13, 51106], "temperature": 0.0, "avg_logprob": -0.3156775890941351, "compression_ratio": 1.4619565217391304, "no_speech_prob": 4.399098543217406e-05}, {"id": 813, "seek": 346450, "start": 3482.62, "end": 3486.98, "text": " To, this is a question from me, I guess.", "tokens": [51270, 1407, 11, 341, 307, 257, 1168, 490, 385, 11, 286, 2041, 13, 51488], "temperature": 0.0, "avg_logprob": -0.3156775890941351, "compression_ratio": 1.4619565217391304, "no_speech_prob": 4.399098543217406e-05}, {"id": 814, "seek": 346450, "start": 3486.98, "end": 3492.34, "text": " Two aspects of critical importance to,", "tokens": [51488, 4453, 7270, 295, 4924, 7379, 281, 11, 51756], "temperature": 0.0, "avg_logprob": -0.3156775890941351, "compression_ratio": 1.4619565217391304, "no_speech_prob": 4.399098543217406e-05}, {"id": 815, "seek": 349234, "start": 3492.34, "end": 3497.6200000000003, "text": " let's say, GPT or large language models that are not talked about a lot by", "tokens": [50364, 718, 311, 584, 11, 26039, 51, 420, 2416, 2856, 5245, 300, 366, 406, 2825, 466, 257, 688, 538, 50628], "temperature": 0.0, "avg_logprob": -0.2481611425226385, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0028267144225537777}, {"id": 816, "seek": 349234, "start": 3497.6200000000003, "end": 3502.1400000000003, "text": " the companies who do them are data curation.", "tokens": [50628, 264, 3431, 567, 360, 552, 366, 1412, 1262, 399, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2481611425226385, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0028267144225537777}, {"id": 817, "seek": 349234, "start": 3502.1400000000003, "end": 3507.26, "text": " Getting that clean data, that balanced data, that representative data,", "tokens": [50854, 13674, 300, 2541, 1412, 11, 300, 13902, 1412, 11, 300, 12424, 1412, 11, 51110], "temperature": 0.0, "avg_logprob": -0.2481611425226385, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0028267144225537777}, {"id": 818, "seek": 349234, "start": 3507.26, "end": 3514.3, "text": " which by the way, counter to popular belief, open AI spent a lot of its money", "tokens": [51110, 597, 538, 264, 636, 11, 5682, 281, 3743, 7107, 11, 1269, 7318, 4418, 257, 688, 295, 1080, 1460, 51462], "temperature": 0.0, "avg_logprob": -0.2481611425226385, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0028267144225537777}, {"id": 819, "seek": 349234, "start": 3514.3, "end": 3519.6600000000003, "text": " on curating just that right corpus so that they can do the training reliably.", "tokens": [51462, 322, 1262, 990, 445, 300, 558, 1181, 31624, 370, 300, 436, 393, 360, 264, 3097, 49927, 13, 51730], "temperature": 0.0, "avg_logprob": -0.2481611425226385, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0028267144225537777}, {"id": 820, "seek": 351966, "start": 3519.66, "end": 3526.46, "text": " And the second part, which is something we're big believers in at the Institute for", "tokens": [50364, 400, 264, 1150, 644, 11, 597, 307, 746, 321, 434, 955, 23125, 294, 412, 264, 9446, 337, 50704], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 821, "seek": 351966, "start": 3526.46, "end": 3530.8199999999997, "text": " experiential AI, experiential AI stands for AI with the human in the loop.", "tokens": [50704, 49611, 831, 7318, 11, 49611, 831, 7318, 7382, 337, 7318, 365, 264, 1952, 294, 264, 6367, 13, 50922], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 822, "seek": 351966, "start": 3530.8199999999997, "end": 3534.74, "text": " Having that human intervention through relevance feedback,", "tokens": [50922, 10222, 300, 1952, 13176, 807, 32684, 5824, 11, 51118], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 823, "seek": 351966, "start": 3534.74, "end": 3539.2999999999997, "text": " that we know now open AI is doing and has been doing.", "tokens": [51118, 300, 321, 458, 586, 1269, 7318, 307, 884, 293, 575, 668, 884, 13, 51346], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 824, "seek": 351966, "start": 3539.2999999999997, "end": 3542.8999999999996, "text": " And some of the queries are actually taken over by humans at some point when", "tokens": [51346, 400, 512, 295, 264, 24109, 366, 767, 2726, 670, 538, 6255, 412, 512, 935, 562, 51526], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 825, "seek": 351966, "start": 3542.8999999999996, "end": 3544.74, "text": " they make enough errors to come back.", "tokens": [51526, 436, 652, 1547, 13603, 281, 808, 646, 13, 51618], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 826, "seek": 351966, "start": 3544.74, "end": 3547.66, "text": " But the good thing is they learn from them and we think that's a great practice.", "tokens": [51618, 583, 264, 665, 551, 307, 436, 1466, 490, 552, 293, 321, 519, 300, 311, 257, 869, 3124, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19832284109933035, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.0003730179159902036}, {"id": 827, "seek": 354766, "start": 3548.66, "end": 3553.7, "text": " Why do you think the companies don't want to talk about the importance of the data", "tokens": [50414, 1545, 360, 291, 519, 264, 3431, 500, 380, 528, 281, 751, 466, 264, 7379, 295, 264, 1412, 50666], "temperature": 0.0, "avg_logprob": -0.22512550354003907, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.0005183478351682425}, {"id": 828, "seek": 354766, "start": 3553.7, "end": 3555.22, "text": " and the importance of the human in the loop?", "tokens": [50666, 293, 264, 7379, 295, 264, 1952, 294, 264, 6367, 30, 50742], "temperature": 0.0, "avg_logprob": -0.22512550354003907, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.0005183478351682425}, {"id": 829, "seek": 354766, "start": 3556.3399999999997, "end": 3557.7, "text": " I don't know if they don't want to talk about it.", "tokens": [50798, 286, 500, 380, 458, 498, 436, 500, 380, 528, 281, 751, 466, 309, 13, 50866], "temperature": 0.0, "avg_logprob": -0.22512550354003907, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.0005183478351682425}, {"id": 830, "seek": 354766, "start": 3557.7, "end": 3566.54, "text": " I mean, it's clearly very expensive to create data to produce a good LLM.", "tokens": [50866, 286, 914, 11, 309, 311, 4448, 588, 5124, 281, 1884, 1412, 281, 5258, 257, 665, 441, 43, 44, 13, 51308], "temperature": 0.0, "avg_logprob": -0.22512550354003907, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.0005183478351682425}, {"id": 831, "seek": 354766, "start": 3566.54, "end": 3572.46, "text": " But in my opinion, it's doomed to failure in the long run for two reasons.", "tokens": [51308, 583, 294, 452, 4800, 11, 309, 311, 33847, 281, 7763, 294, 264, 938, 1190, 337, 732, 4112, 13, 51604], "temperature": 0.0, "avg_logprob": -0.22512550354003907, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.0005183478351682425}, {"id": 832, "seek": 357246, "start": 3572.58, "end": 3581.7400000000002, "text": " The first one is the curation requires going through this enormous amount of data that", "tokens": [50370, 440, 700, 472, 307, 264, 1262, 399, 7029, 516, 807, 341, 11322, 2372, 295, 1412, 300, 50828], "temperature": 0.0, "avg_logprob": -0.2692768149179955, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.00012923707254230976}, {"id": 833, "seek": 357246, "start": 3581.7400000000002, "end": 3582.98, "text": " you want to train the system on.", "tokens": [50828, 291, 528, 281, 3847, 264, 1185, 322, 13, 50890], "temperature": 0.0, "avg_logprob": -0.2692768149179955, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.00012923707254230976}, {"id": 834, "seek": 357246, "start": 3582.98, "end": 3587.14, "text": " And any data you eliminate, it's less training data for your model.", "tokens": [50890, 400, 604, 1412, 291, 13819, 11, 309, 311, 1570, 3097, 1412, 337, 428, 2316, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2692768149179955, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.00012923707254230976}, {"id": 835, "seek": 357246, "start": 3588.7400000000002, "end": 3592.66, "text": " But the second thing is even with human feedback,", "tokens": [51178, 583, 264, 1150, 551, 307, 754, 365, 1952, 5824, 11, 51374], "temperature": 0.0, "avg_logprob": -0.2692768149179955, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.00012923707254230976}, {"id": 836, "seek": 357246, "start": 3592.66, "end": 3598.66, "text": " human feedback that rate different answers or fine tune the system for", "tokens": [51374, 1952, 5824, 300, 3314, 819, 6338, 420, 2489, 10864, 264, 1185, 337, 51674], "temperature": 0.0, "avg_logprob": -0.2692768149179955, "compression_ratio": 1.5794871794871794, "no_speech_prob": 0.00012923707254230976}, {"id": 837, "seek": 359866, "start": 3598.66, "end": 3602.46, "text": " certain question and answers, sort of manually curated.", "tokens": [50364, 1629, 1168, 293, 6338, 11, 1333, 295, 16945, 47851, 13, 50554], "temperature": 0.0, "avg_logprob": -0.22928124804829442, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.008287102915346622}, {"id": 838, "seek": 359866, "start": 3604.74, "end": 3609.62, "text": " If you want those systems ultimately to be the repository of all human knowledge,", "tokens": [50668, 759, 291, 528, 729, 3652, 6284, 281, 312, 264, 25841, 295, 439, 1952, 3601, 11, 50912], "temperature": 0.0, "avg_logprob": -0.22928124804829442, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.008287102915346622}, {"id": 839, "seek": 359866, "start": 3610.74, "end": 3614.98, "text": " the dimension of that space of all human knowledge is enormous.", "tokens": [50968, 264, 10139, 295, 300, 1901, 295, 439, 1952, 3601, 307, 11322, 13, 51180], "temperature": 0.0, "avg_logprob": -0.22928124804829442, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.008287102915346622}, {"id": 840, "seek": 359866, "start": 3614.98, "end": 3618.18, "text": " And you're not going to do it by paying a few thousand people in Kenya or", "tokens": [51180, 400, 291, 434, 406, 516, 281, 360, 309, 538, 6229, 257, 1326, 4714, 561, 294, 31011, 420, 51340], "temperature": 0.0, "avg_logprob": -0.22928124804829442, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.008287102915346622}, {"id": 841, "seek": 359866, "start": 3618.18, "end": 3621.8599999999997, "text": " India rating answers.", "tokens": [51340, 5282, 10990, 6338, 13, 51524], "temperature": 0.0, "avg_logprob": -0.22928124804829442, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.008287102915346622}, {"id": 842, "seek": 359866, "start": 3621.8599999999997, "end": 3628.5, "text": " You're going to have to do it with millions of volunteers that find", "tokens": [51524, 509, 434, 516, 281, 362, 281, 360, 309, 365, 6803, 295, 14352, 300, 915, 51856], "temperature": 0.0, "avg_logprob": -0.22928124804829442, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.008287102915346622}, {"id": 843, "seek": 362850, "start": 3628.5, "end": 3632.18, "text": " the system for all possible questions that might possibly be asked.", "tokens": [50364, 264, 1185, 337, 439, 1944, 1651, 300, 1062, 6264, 312, 2351, 13, 50548], "temperature": 0.0, "avg_logprob": -0.19021950187263909, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0009102540789172053}, {"id": 844, "seek": 362850, "start": 3632.18, "end": 3638.1, "text": " And those volunteers will have to be vetted in the way Wikipedia is being done, right?", "tokens": [50548, 400, 729, 14352, 486, 362, 281, 312, 371, 46508, 294, 264, 636, 28999, 307, 885, 1096, 11, 558, 30, 50844], "temperature": 0.0, "avg_logprob": -0.19021950187263909, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0009102540789172053}, {"id": 845, "seek": 362850, "start": 3638.1, "end": 3645.22, "text": " So think of LLMs in the long run as a version of Wikipedia plus your favorite", "tokens": [50844, 407, 519, 295, 441, 43, 26386, 294, 264, 938, 1190, 382, 257, 3037, 295, 28999, 1804, 428, 2954, 51200], "temperature": 0.0, "avg_logprob": -0.19021950187263909, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0009102540789172053}, {"id": 846, "seek": 362850, "start": 3645.22, "end": 3650.5, "text": " newspapers plus the scientific literature plus everything, but you can talk to it.", "tokens": [51200, 20781, 1804, 264, 8134, 10394, 1804, 1203, 11, 457, 291, 393, 751, 281, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19021950187263909, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0009102540789172053}, {"id": 847, "seek": 362850, "start": 3650.5, "end": 3652.7, "text": " You don't have to read articles, you can just talk to it.", "tokens": [51464, 509, 500, 380, 362, 281, 1401, 11290, 11, 291, 393, 445, 751, 281, 309, 13, 51574], "temperature": 0.0, "avg_logprob": -0.19021950187263909, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0009102540789172053}, {"id": 848, "seek": 365270, "start": 3653.66, "end": 3658.46, "text": " And so if it's supposed to become the repository of all human knowledge,", "tokens": [50412, 400, 370, 498, 309, 311, 3442, 281, 1813, 264, 25841, 295, 439, 1952, 3601, 11, 50652], "temperature": 0.0, "avg_logprob": -0.24867988887586093, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0004366055363789201}, {"id": 849, "seek": 365270, "start": 3658.46, "end": 3664.02, "text": " the thing it's been trained to do will have to be curated by", "tokens": [50652, 264, 551, 309, 311, 668, 8895, 281, 360, 486, 362, 281, 312, 47851, 538, 50930], "temperature": 0.0, "avg_logprob": -0.24867988887586093, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0004366055363789201}, {"id": 850, "seek": 365270, "start": 3664.02, "end": 3670.22, "text": " quite sourcing the way Wikipedia is to cover all the possible things that", "tokens": [50930, 1596, 11006, 2175, 264, 636, 28999, 307, 281, 2060, 439, 264, 1944, 721, 300, 51240], "temperature": 0.0, "avg_logprob": -0.24867988887586093, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0004366055363789201}, {"id": 851, "seek": 365270, "start": 3670.22, "end": 3671.98, "text": " may be covered.", "tokens": [51240, 815, 312, 5343, 13, 51328], "temperature": 0.0, "avg_logprob": -0.24867988887586093, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0004366055363789201}, {"id": 852, "seek": 365270, "start": 3671.98, "end": 3679.18, "text": " This is a very strong argument for having open source based models for LLMs.", "tokens": [51328, 639, 307, 257, 588, 2068, 6770, 337, 1419, 1269, 4009, 2361, 5245, 337, 441, 43, 26386, 13, 51688], "temperature": 0.0, "avg_logprob": -0.24867988887586093, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0004366055363789201}, {"id": 853, "seek": 367918, "start": 3679.2599999999998, "end": 3684.54, "text": " So in my opinion, the future is inevitably going to be that you're going to", "tokens": [50368, 407, 294, 452, 4800, 11, 264, 2027, 307, 28171, 516, 281, 312, 300, 291, 434, 516, 281, 50632], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 854, "seek": 367918, "start": 3684.54, "end": 3689.74, "text": " have a small number of open source based LLMs that are not trained for", "tokens": [50632, 362, 257, 1359, 1230, 295, 1269, 4009, 2361, 441, 43, 26386, 300, 366, 406, 8895, 337, 50892], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 855, "seek": 367918, "start": 3689.74, "end": 3694.7799999999997, "text": " any particular application, they're trained on enormous amounts of data", "tokens": [50892, 604, 1729, 3861, 11, 436, 434, 8895, 322, 11322, 11663, 295, 1412, 51144], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 856, "seek": 367918, "start": 3694.7799999999997, "end": 3695.8999999999996, "text": " that requires a lot of money.", "tokens": [51144, 300, 7029, 257, 688, 295, 1460, 13, 51200], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 857, "seek": 367918, "start": 3695.8999999999996, "end": 3698.3799999999997, "text": " So you're not going to have 25 of them, you're going to have two or three.", "tokens": [51200, 407, 291, 434, 406, 516, 281, 362, 3552, 295, 552, 11, 291, 434, 516, 281, 362, 732, 420, 1045, 13, 51324], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 858, "seek": 367918, "start": 3699.66, "end": 3702.8599999999997, "text": " And then actual applications are going to be built on top of it by", "tokens": [51388, 400, 550, 3539, 5821, 366, 516, 281, 312, 3094, 322, 1192, 295, 309, 538, 51548], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 859, "seek": 367918, "start": 3702.8599999999997, "end": 3706.62, "text": " finding those systems for particular vertical applications.", "tokens": [51548, 5006, 729, 3652, 337, 1729, 9429, 5821, 13, 51736], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 860, "seek": 367918, "start": 3706.62, "end": 3707.3399999999997, "text": " That's the future.", "tokens": [51736, 663, 311, 264, 2027, 13, 51772], "temperature": 0.0, "avg_logprob": -0.14659033881293404, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.00042360820225439966}, {"id": 861, "seek": 370734, "start": 3708.06, "end": 3713.02, "text": " Sadly, in the industry, there are people who are lobbying governments to", "tokens": [50400, 29628, 11, 294, 264, 3518, 11, 456, 366, 561, 567, 366, 47142, 11280, 281, 50648], "temperature": 0.0, "avg_logprob": -0.18368353162493026, "compression_ratio": 1.5459183673469388, "no_speech_prob": 0.0008543593576177955}, {"id": 862, "seek": 370734, "start": 3713.02, "end": 3717.82, "text": " actually make the open sourcing of large scale LLM illegal.", "tokens": [50648, 767, 652, 264, 1269, 11006, 2175, 295, 2416, 4373, 441, 43, 44, 11905, 13, 50888], "temperature": 0.0, "avg_logprob": -0.18368353162493026, "compression_ratio": 1.5459183673469388, "no_speech_prob": 0.0008543593576177955}, {"id": 863, "seek": 370734, "start": 3719.26, "end": 3725.34, "text": " What they're worried about is potential misuse of LLMs by bad actors,", "tokens": [50960, 708, 436, 434, 5804, 466, 307, 3995, 3346, 438, 295, 441, 43, 26386, 538, 1578, 10037, 11, 51264], "temperature": 0.0, "avg_logprob": -0.18368353162493026, "compression_ratio": 1.5459183673469388, "no_speech_prob": 0.0008543593576177955}, {"id": 864, "seek": 370734, "start": 3726.46, "end": 3728.3, "text": " potential users.", "tokens": [51320, 3995, 5022, 13, 51412], "temperature": 0.0, "avg_logprob": -0.18368353162493026, "compression_ratio": 1.5459183673469388, "no_speech_prob": 0.0008543593576177955}, {"id": 865, "seek": 370734, "start": 3729.9, "end": 3733.82, "text": " So some people in the US, for example, are worried, oh, if we open source our LLMs,", "tokens": [51492, 407, 512, 561, 294, 264, 2546, 11, 337, 1365, 11, 366, 5804, 11, 1954, 11, 498, 321, 1269, 4009, 527, 441, 43, 26386, 11, 51688], "temperature": 0.0, "avg_logprob": -0.18368353162493026, "compression_ratio": 1.5459183673469388, "no_speech_prob": 0.0008543593576177955}, {"id": 866, "seek": 373382, "start": 3733.82, "end": 3737.7400000000002, "text": " you know China and North Korea and Iran will put their hands on it and that's", "tokens": [50364, 291, 458, 3533, 293, 4067, 6307, 293, 8283, 486, 829, 641, 2377, 322, 309, 293, 300, 311, 50560], "temperature": 0.0, "avg_logprob": -0.1179447074731191, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.00014412583550438285}, {"id": 867, "seek": 373382, "start": 3737.7400000000002, "end": 3738.3, "text": " going to be bad.", "tokens": [50560, 516, 281, 312, 1578, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1179447074731191, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.00014412583550438285}, {"id": 868, "seek": 373382, "start": 3740.94, "end": 3744.38, "text": " And then some people are worried that the real powerful LLMs are going to be", "tokens": [50720, 400, 550, 512, 561, 366, 5804, 300, 264, 957, 4005, 441, 43, 26386, 366, 516, 281, 312, 50892], "temperature": 0.0, "avg_logprob": -0.1179447074731191, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.00014412583550438285}, {"id": 869, "seek": 373382, "start": 3744.38, "end": 3748.86, "text": " super intelligent and destroy humanity, which I think is preposterous,", "tokens": [50892, 1687, 13232, 293, 5293, 10243, 11, 597, 286, 519, 307, 2666, 7096, 563, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1179447074731191, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.00014412583550438285}, {"id": 870, "seek": 373382, "start": 3750.2200000000003, "end": 3753.1000000000004, "text": " even though some of my friends that I respect actually believe this.", "tokens": [51184, 754, 1673, 512, 295, 452, 1855, 300, 286, 3104, 767, 1697, 341, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1179447074731191, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.00014412583550438285}, {"id": 871, "seek": 373382, "start": 3754.2200000000003, "end": 3760.1400000000003, "text": " So I think it would be really, really bad if those lobbying attempts succeed.", "tokens": [51384, 407, 286, 519, 309, 576, 312, 534, 11, 534, 1578, 498, 729, 47142, 15257, 7754, 13, 51680], "temperature": 0.0, "avg_logprob": -0.1179447074731191, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.00014412583550438285}, {"id": 872, "seek": 376014, "start": 3760.8599999999997, "end": 3764.8599999999997, "text": " I'm very much in favor of a future with open based models.", "tokens": [50400, 286, 478, 588, 709, 294, 2294, 295, 257, 2027, 365, 1269, 2361, 5245, 13, 50600], "temperature": 0.0, "avg_logprob": -0.16805929067183514, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00039175254642032087}, {"id": 873, "seek": 376014, "start": 3765.58, "end": 3768.22, "text": " And there's going to be bad actors, but there's going to be countermeasures", "tokens": [50636, 400, 456, 311, 516, 281, 312, 1578, 10037, 11, 457, 456, 311, 516, 281, 312, 5682, 1398, 20044, 50768], "temperature": 0.0, "avg_logprob": -0.16805929067183514, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00039175254642032087}, {"id": 874, "seek": 376014, "start": 3768.22, "end": 3769.02, "text": " against them.", "tokens": [50768, 1970, 552, 13, 50808], "temperature": 0.0, "avg_logprob": -0.16805929067183514, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00039175254642032087}, {"id": 875, "seek": 376014, "start": 3769.02, "end": 3777.1, "text": " It's going to be, you know, or powerful good AI cop against their nefarious AI, essentially.", "tokens": [50808, 467, 311, 516, 281, 312, 11, 291, 458, 11, 420, 4005, 665, 7318, 2971, 1970, 641, 408, 21196, 851, 7318, 11, 4476, 13, 51212], "temperature": 0.0, "avg_logprob": -0.16805929067183514, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00039175254642032087}, {"id": 876, "seek": 376014, "start": 3780.06, "end": 3783.66, "text": " So let's shift to this trend.", "tokens": [51360, 407, 718, 311, 5513, 281, 341, 6028, 13, 51540], "temperature": 0.0, "avg_logprob": -0.16805929067183514, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00039175254642032087}, {"id": 877, "seek": 376014, "start": 3783.66, "end": 3789.3399999999997, "text": " And this is, I've merged a question from Jimmy Shanahan from our AI solutions hub", "tokens": [51540, 400, 341, 307, 11, 286, 600, 36427, 257, 1168, 490, 15709, 25536, 21436, 490, 527, 7318, 6547, 11838, 51824], "temperature": 0.0, "avg_logprob": -0.16805929067183514, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.00039175254642032087}, {"id": 878, "seek": 378934, "start": 3789.9, "end": 3794.06, "text": " with questions from Tomo Lasovic and Ken Church at EAI.", "tokens": [50392, 365, 1651, 490, 5041, 78, 10663, 5179, 299, 293, 8273, 7882, 412, 35747, 40, 13, 50600], "temperature": 0.0, "avg_logprob": -0.16148521087981843, "compression_ratio": 1.6, "no_speech_prob": 0.000555853359401226}, {"id": 879, "seek": 378934, "start": 3795.6600000000003, "end": 3802.1400000000003, "text": " The trend nowadays seems to be heading towards bigger is better, more compute, more parameters.", "tokens": [50680, 440, 6028, 13434, 2544, 281, 312, 9864, 3030, 3801, 307, 1101, 11, 544, 14722, 11, 544, 9834, 13, 51004], "temperature": 0.0, "avg_logprob": -0.16148521087981843, "compression_ratio": 1.6, "no_speech_prob": 0.000555853359401226}, {"id": 880, "seek": 378934, "start": 3803.58, "end": 3809.42, "text": " There's been some studies even suggesting that by open AI themselves that they're moving at a", "tokens": [51076, 821, 311, 668, 512, 5313, 754, 18094, 300, 538, 1269, 7318, 2969, 300, 436, 434, 2684, 412, 257, 51368], "temperature": 0.0, "avg_logprob": -0.16148521087981843, "compression_ratio": 1.6, "no_speech_prob": 0.000555853359401226}, {"id": 881, "seek": 378934, "start": 3809.42, "end": 3815.6600000000003, "text": " pace faster than Moore's law, even though now they seem to be normalizing towards it,", "tokens": [51368, 11638, 4663, 813, 21644, 311, 2101, 11, 754, 1673, 586, 436, 1643, 281, 312, 2710, 3319, 3030, 309, 11, 51680], "temperature": 0.0, "avg_logprob": -0.16148521087981843, "compression_ratio": 1.6, "no_speech_prob": 0.000555853359401226}, {"id": 882, "seek": 378934, "start": 3815.6600000000003, "end": 3818.06, "text": " although Moore's law itself is slowing down.", "tokens": [51680, 4878, 21644, 311, 2101, 2564, 307, 26958, 760, 13, 51800], "temperature": 0.0, "avg_logprob": -0.16148521087981843, "compression_ratio": 1.6, "no_speech_prob": 0.000555853359401226}, {"id": 883, "seek": 381806, "start": 3818.2999999999997, "end": 3822.46, "text": " So the real question here is how long can this go on?", "tokens": [50376, 407, 264, 957, 1168, 510, 307, 577, 938, 393, 341, 352, 322, 30, 50584], "temperature": 0.0, "avg_logprob": -0.12466083194898522, "compression_ratio": 1.5576208178438662, "no_speech_prob": 0.00035645824391394854}, {"id": 884, "seek": 381806, "start": 3822.46, "end": 3824.38, "text": " And will we ask them, what do you think?", "tokens": [50584, 400, 486, 321, 1029, 552, 11, 437, 360, 291, 519, 30, 50680], "temperature": 0.0, "avg_logprob": -0.12466083194898522, "compression_ratio": 1.5576208178438662, "no_speech_prob": 0.00035645824391394854}, {"id": 885, "seek": 381806, "start": 3824.38, "end": 3828.22, "text": " I know that we may not have the final answer here, but it seems crazy.", "tokens": [50680, 286, 458, 300, 321, 815, 406, 362, 264, 2572, 1867, 510, 11, 457, 309, 2544, 3219, 13, 50872], "temperature": 0.0, "avg_logprob": -0.12466083194898522, "compression_ratio": 1.5576208178438662, "no_speech_prob": 0.00035645824391394854}, {"id": 886, "seek": 381806, "start": 3828.22, "end": 3831.74, "text": " Like all you have to do is wait a few weeks and you hear about the next big model.", "tokens": [50872, 1743, 439, 291, 362, 281, 360, 307, 1699, 257, 1326, 3259, 293, 291, 1568, 466, 264, 958, 955, 2316, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12466083194898522, "compression_ratio": 1.5576208178438662, "no_speech_prob": 0.00035645824391394854}, {"id": 887, "seek": 381806, "start": 3832.46, "end": 3836.22, "text": " Well, so actually in the last few months, you've seen a decrease in the size.", "tokens": [51084, 1042, 11, 370, 767, 294, 264, 1036, 1326, 2493, 11, 291, 600, 1612, 257, 11514, 294, 264, 2744, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12466083194898522, "compression_ratio": 1.5576208178438662, "no_speech_prob": 0.00035645824391394854}, {"id": 888, "seek": 381806, "start": 3837.34, "end": 3843.42, "text": " So Lama, for example, the 13 billion version of Lama in terms of raw performance on standard", "tokens": [51328, 407, 441, 2404, 11, 337, 1365, 11, 264, 3705, 5218, 3037, 295, 441, 2404, 294, 2115, 295, 8936, 3389, 322, 3832, 51632], "temperature": 0.0, "avg_logprob": -0.12466083194898522, "compression_ratio": 1.5576208178438662, "no_speech_prob": 0.00035645824391394854}, {"id": 889, "seek": 384342, "start": 3843.42, "end": 3848.38, "text": " benchmarks is actually better than GPT-3, which has 175 billion parameters.", "tokens": [50364, 43751, 307, 767, 1101, 813, 26039, 51, 12, 18, 11, 597, 575, 41165, 5218, 9834, 13, 50612], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 890, "seek": 384342, "start": 3848.94, "end": 3852.14, "text": " And so it's not clear that bigger is better.", "tokens": [50640, 400, 370, 309, 311, 406, 1850, 300, 3801, 307, 1101, 13, 50800], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 891, "seek": 384342, "start": 3852.14, "end": 3857.98, "text": " With the architecture I propose, I think you can get away with smaller systems that", "tokens": [50800, 2022, 264, 9482, 286, 17421, 11, 286, 519, 291, 393, 483, 1314, 365, 4356, 3652, 300, 51092], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 892, "seek": 384342, "start": 3857.98, "end": 3859.1, "text": " perform at least as well.", "tokens": [51092, 2042, 412, 1935, 382, 731, 13, 51148], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 893, "seek": 384342, "start": 3859.1, "end": 3862.7000000000003, "text": " The reason being that when you train in a current autoregressive LLM,", "tokens": [51148, 440, 1778, 885, 300, 562, 291, 3847, 294, 257, 2190, 1476, 418, 3091, 488, 441, 43, 44, 11, 51328], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 894, "seek": 384342, "start": 3862.7000000000003, "end": 3867.1, "text": " you have to train it to not just accumulate knowledge, not just predict the next word,", "tokens": [51328, 291, 362, 281, 3847, 309, 281, 406, 445, 33384, 3601, 11, 406, 445, 6069, 264, 958, 1349, 11, 51548], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 895, "seek": 384342, "start": 3867.1, "end": 3868.46, "text": " but also solve a lot of problems.", "tokens": [51548, 457, 611, 5039, 257, 688, 295, 2740, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09464536894352064, "compression_ratio": 1.530909090909091, "no_speech_prob": 0.0015702927485108376}, {"id": 896, "seek": 386846, "start": 3868.46, "end": 3875.02, "text": " So basically, know how to produce the right answer when you specify the question in the", "tokens": [50364, 407, 1936, 11, 458, 577, 281, 5258, 264, 558, 1867, 562, 291, 16500, 264, 1168, 294, 264, 50692], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 897, "seek": 386846, "start": 3875.02, "end": 3875.66, "text": " prompt.", "tokens": [50692, 12391, 13, 50724], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 898, "seek": 386846, "start": 3875.66, "end": 3879.26, "text": " And so everything is wrapped into the weights of that single model.", "tokens": [50724, 400, 370, 1203, 307, 14226, 666, 264, 17443, 295, 300, 2167, 2316, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 899, "seek": 386846, "start": 3879.26, "end": 3882.06, "text": " Whereas in the model I propose here, the architecture I propose,", "tokens": [50904, 13813, 294, 264, 2316, 286, 17421, 510, 11, 264, 9482, 286, 17421, 11, 51044], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 900, "seek": 386846, "start": 3882.78, "end": 3884.46, "text": " the word model is just a word model.", "tokens": [51080, 264, 1349, 2316, 307, 445, 257, 1349, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 901, "seek": 386846, "start": 3885.1, "end": 3889.98, "text": " The task is specified by the objective function, which may include the prompt.", "tokens": [51196, 440, 5633, 307, 22206, 538, 264, 10024, 2445, 11, 597, 815, 4090, 264, 12391, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 902, "seek": 386846, "start": 3889.98, "end": 3891.82, "text": " So it may include the representation of the prompt.", "tokens": [51440, 407, 309, 815, 4090, 264, 10290, 295, 264, 12391, 13, 51532], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 903, "seek": 386846, "start": 3892.7, "end": 3895.1, "text": " And so you're separating different things.", "tokens": [51576, 400, 370, 291, 434, 29279, 819, 721, 13, 51696], "temperature": 0.0, "avg_logprob": -0.11022313435872395, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0003149632830172777}, {"id": 904, "seek": 389510, "start": 3895.1, "end": 3901.9, "text": " You're separating the inference procedure that produces the output from the word model,", "tokens": [50364, 509, 434, 29279, 264, 38253, 10747, 300, 14725, 264, 5598, 490, 264, 1349, 2316, 11, 50704], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 905, "seek": 389510, "start": 3901.9, "end": 3905.2599999999998, "text": " the sort of the mental model of the world that the system uses,", "tokens": [50704, 264, 1333, 295, 264, 4973, 2316, 295, 264, 1002, 300, 264, 1185, 4960, 11, 50872], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 906, "seek": 389510, "start": 3905.2599999999998, "end": 3908.14, "text": " from the task itself, which is specified by the objective.", "tokens": [50872, 490, 264, 5633, 2564, 11, 597, 307, 22206, 538, 264, 10024, 13, 51016], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 907, "seek": 389510, "start": 3908.14, "end": 3911.5, "text": " And you can probably get away with smaller networks for the same performance.", "tokens": [51016, 400, 291, 393, 1391, 483, 1314, 365, 4356, 9590, 337, 264, 912, 3389, 13, 51184], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 908, "seek": 389510, "start": 3913.1, "end": 3919.9, "text": " But yes, I mean, there were a few years ago models by Google that had like a trillion", "tokens": [51264, 583, 2086, 11, 286, 914, 11, 456, 645, 257, 1326, 924, 2057, 5245, 538, 3329, 300, 632, 411, 257, 18723, 51604], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 909, "seek": 389510, "start": 3919.9, "end": 3920.38, "text": " parameters.", "tokens": [51604, 9834, 13, 51628], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 910, "seek": 389510, "start": 3920.38, "end": 3923.9, "text": " There were basically multiple models that were stuck together with some sort of", "tokens": [51628, 821, 645, 1936, 3866, 5245, 300, 645, 5541, 1214, 365, 512, 1333, 295, 51804], "temperature": 0.0, "avg_logprob": -0.12567013331821986, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00019103453087154776}, {"id": 911, "seek": 392510, "start": 3925.74, "end": 3926.2999999999997, "text": " gating.", "tokens": [50396, 290, 990, 13, 50424], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 912, "seek": 392510, "start": 3926.2999999999997, "end": 3930.7, "text": " Yeah, between them, they've kind of backpedaled on this a little bit.", "tokens": [50424, 865, 11, 1296, 552, 11, 436, 600, 733, 295, 646, 3452, 5573, 322, 341, 257, 707, 857, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 913, "seek": 392510, "start": 3930.7, "end": 3933.74, "text": " If you want the system to be practical, like to be used by everyone,", "tokens": [50644, 759, 291, 528, 264, 1185, 281, 312, 8496, 11, 411, 281, 312, 1143, 538, 1518, 11, 50796], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 914, "seek": 392510, "start": 3933.74, "end": 3935.74, "text": " you can't make them like a trillion parameters.", "tokens": [50796, 291, 393, 380, 652, 552, 411, 257, 18723, 9834, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 915, "seek": 392510, "start": 3935.74, "end": 3937.2599999999998, "text": " Right now, it'd be just too expensive.", "tokens": [50896, 1779, 586, 11, 309, 1116, 312, 445, 886, 5124, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 916, "seek": 392510, "start": 3938.38, "end": 3940.46, "text": " So you have to minimize that size.", "tokens": [51028, 407, 291, 362, 281, 17522, 300, 2744, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 917, "seek": 392510, "start": 3940.46, "end": 3945.8199999999997, "text": " Now you can run things like Lama 7 billion on a Mac.", "tokens": [51132, 823, 291, 393, 1190, 721, 411, 441, 2404, 1614, 5218, 322, 257, 5707, 13, 51400], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 918, "seek": 392510, "start": 3947.3399999999997, "end": 3949.1, "text": " You know, you can run on a laptop.", "tokens": [51476, 509, 458, 11, 291, 393, 1190, 322, 257, 10732, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 919, "seek": 392510, "start": 3949.1, "end": 3951.2599999999998, "text": " You can't train it on a laptop, but you can run it.", "tokens": [51564, 509, 393, 380, 3847, 309, 322, 257, 10732, 11, 457, 291, 393, 1190, 309, 13, 51672], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 920, "seek": 392510, "start": 3951.2599999999998, "end": 3951.5, "text": " Yes.", "tokens": [51672, 1079, 13, 51684], "temperature": 0.0, "avg_logprob": -0.1890808641910553, "compression_ratio": 1.652, "no_speech_prob": 0.0008015450439415872}, {"id": 921, "seek": 395150, "start": 3951.66, "end": 3959.66, "text": " So clearly, you believe you're advocating for a different view of what the machine learning", "tokens": [50372, 407, 4448, 11, 291, 1697, 291, 434, 32050, 337, 257, 819, 1910, 295, 437, 264, 3479, 2539, 50772], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 922, "seek": 395150, "start": 3959.66, "end": 3963.82, "text": " and AI community should be doing as opposed to what they are doing today.", "tokens": [50772, 293, 7318, 1768, 820, 312, 884, 382, 8851, 281, 437, 436, 366, 884, 965, 13, 50980], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 923, "seek": 395150, "start": 3964.78, "end": 3966.38, "text": " That's the story of my career.", "tokens": [51028, 663, 311, 264, 1657, 295, 452, 3988, 13, 51108], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 924, "seek": 395150, "start": 3966.38, "end": 3966.62, "text": " Yes.", "tokens": [51108, 1079, 13, 51120], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 925, "seek": 395150, "start": 3968.06, "end": 3970.38, "text": " And this question is coming from Ken Church.", "tokens": [51192, 400, 341, 1168, 307, 1348, 490, 8273, 7882, 13, 51308], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 926, "seek": 395150, "start": 3971.42, "end": 3973.34, "text": " A former colleague from AT&T.", "tokens": [51360, 316, 5819, 13532, 490, 8872, 5, 51, 13, 51456], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 927, "seek": 395150, "start": 3973.34, "end": 3974.62, "text": " From AT&T.", "tokens": [51456, 3358, 8872, 5, 51, 13, 51520], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 928, "seek": 395150, "start": 3974.62, "end": 3977.66, "text": " He is at the Institute for AI in Silicon Valley.", "tokens": [51520, 634, 307, 412, 264, 9446, 337, 7318, 294, 25351, 10666, 13, 51672], "temperature": 0.0, "avg_logprob": -0.26234507817094044, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0002331293362658471}, {"id": 929, "seek": 397766, "start": 3978.62, "end": 3979.3399999999997, "text": " Do you believe?", "tokens": [50412, 1144, 291, 1697, 30, 50448], "temperature": 0.0, "avg_logprob": -0.13729193240781373, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00039157862192951143}, {"id": 930, "seek": 397766, "start": 3981.66, "end": 3987.8199999999997, "text": " Well, I guess the question is, how long do you think it will take to pivot the field from", "tokens": [50564, 1042, 11, 286, 2041, 264, 1168, 307, 11, 577, 938, 360, 291, 519, 309, 486, 747, 281, 14538, 264, 2519, 490, 50872], "temperature": 0.0, "avg_logprob": -0.13729193240781373, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00039157862192951143}, {"id": 931, "seek": 397766, "start": 3987.8199999999997, "end": 3990.2999999999997, "text": " where it is to where you would like it to be?", "tokens": [50872, 689, 309, 307, 281, 689, 291, 576, 411, 309, 281, 312, 30, 50996], "temperature": 0.0, "avg_logprob": -0.13729193240781373, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00039157862192951143}, {"id": 932, "seek": 397766, "start": 3992.8599999999997, "end": 3994.7799999999997, "text": " Well, last time I tried, it took 15 years.", "tokens": [51124, 1042, 11, 1036, 565, 286, 3031, 11, 309, 1890, 2119, 924, 13, 51220], "temperature": 0.0, "avg_logprob": -0.13729193240781373, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00039157862192951143}, {"id": 933, "seek": 397766, "start": 3998.06, "end": 4001.3399999999997, "text": " If not more, actually, depending on how you count, it might have been 20.", "tokens": [51384, 759, 406, 544, 11, 767, 11, 5413, 322, 577, 291, 1207, 11, 309, 1062, 362, 668, 945, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13729193240781373, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00039157862192951143}, {"id": 934, "seek": 400134, "start": 4002.3, "end": 4005.1800000000003, "text": " So I don't know.", "tokens": [50412, 407, 286, 500, 380, 458, 13, 50556], "temperature": 0.0, "avg_logprob": -0.19098648328459664, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0013651663903146982}, {"id": 935, "seek": 400134, "start": 4005.1800000000003, "end": 4010.86, "text": " I think I see a phenomenon in kind of this is a sociology of science question.", "tokens": [50556, 286, 519, 286, 536, 257, 14029, 294, 733, 295, 341, 307, 257, 41744, 295, 3497, 1168, 13, 50840], "temperature": 0.0, "avg_logprob": -0.19098648328459664, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0013651663903146982}, {"id": 936, "seek": 400134, "start": 4011.42, "end": 4014.54, "text": " When there is something that seems to work, everybody gets excited about it.", "tokens": [50868, 1133, 456, 307, 746, 300, 2544, 281, 589, 11, 2201, 2170, 2919, 466, 309, 13, 51024], "temperature": 0.0, "avg_logprob": -0.19098648328459664, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0013651663903146982}, {"id": 937, "seek": 400134, "start": 4014.54, "end": 4023.34, "text": " And it's a fashion trend type phenomenon where every paper written is about this trend.", "tokens": [51024, 400, 309, 311, 257, 6700, 6028, 2010, 14029, 689, 633, 3035, 3720, 307, 466, 341, 6028, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19098648328459664, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0013651663903146982}, {"id": 938, "seek": 400134, "start": 4023.34, "end": 4027.26, "text": " I saw this in computer vision back in the early to mid 2000.", "tokens": [51464, 286, 1866, 341, 294, 3820, 5201, 646, 294, 264, 2440, 281, 2062, 8132, 13, 51660], "temperature": 0.0, "avg_logprob": -0.19098648328459664, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0013651663903146982}, {"id": 939, "seek": 400134, "start": 4027.26, "end": 4029.34, "text": " Everybody was working on boosting.", "tokens": [51660, 7646, 390, 1364, 322, 43117, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19098648328459664, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.0013651663903146982}, {"id": 940, "seek": 402934, "start": 4029.34, "end": 4032.78, "text": " That was the thing, you had to work on boosting for computer vision.", "tokens": [50364, 663, 390, 264, 551, 11, 291, 632, 281, 589, 322, 43117, 337, 3820, 5201, 13, 50536], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 941, "seek": 402934, "start": 4032.78, "end": 4041.1800000000003, "text": " And then someone in 2006 and 2005 came up with a different way of doing vision using dense", "tokens": [50536, 400, 550, 1580, 294, 14062, 293, 14394, 1361, 493, 365, 257, 819, 636, 295, 884, 5201, 1228, 18011, 50956], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 942, "seek": 402934, "start": 4041.98, "end": 4046.6200000000003, "text": " features like sift and stuff like that using unsupervised running for a middle layer and then", "tokens": [50996, 4122, 411, 262, 2008, 293, 1507, 411, 300, 1228, 2693, 12879, 24420, 2614, 337, 257, 2808, 4583, 293, 550, 51228], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 943, "seek": 402934, "start": 4046.6200000000003, "end": 4047.82, "text": " an SVM on top.", "tokens": [51228, 364, 31910, 44, 322, 1192, 13, 51288], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 944, "seek": 402934, "start": 4047.82, "end": 4049.42, "text": " All of a sudden, everybody was doing this.", "tokens": [51288, 1057, 295, 257, 3990, 11, 2201, 390, 884, 341, 13, 51368], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 945, "seek": 402934, "start": 4050.54, "end": 4054.1400000000003, "text": " And then starting in 2013, everybody started using convolutional nets.", "tokens": [51424, 400, 550, 2891, 294, 9012, 11, 2201, 1409, 1228, 45216, 304, 36170, 13, 51604], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 946, "seek": 402934, "start": 4056.3, "end": 4057.7400000000002, "text": " That came from results.", "tokens": [51712, 663, 1361, 490, 3542, 13, 51784], "temperature": 0.0, "avg_logprob": -0.18709667056214577, "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.0007537644705735147}, {"id": 947, "seek": 405774, "start": 4057.74, "end": 4060.62, "text": " So now we are in a phase where everybody is focused on LLMs.", "tokens": [50364, 407, 586, 321, 366, 294, 257, 5574, 689, 2201, 307, 5178, 322, 441, 43, 26386, 13, 50508], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 948, "seek": 405774, "start": 4061.18, "end": 4063.9799999999996, "text": " And if you don't work on LLMs, nobody wants to talk to you.", "tokens": [50536, 400, 498, 291, 500, 380, 589, 322, 441, 43, 26386, 11, 5079, 2738, 281, 751, 281, 291, 13, 50676], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 949, "seek": 405774, "start": 4065.8199999999997, "end": 4067.18, "text": " But it will change.", "tokens": [50768, 583, 309, 486, 1319, 13, 50836], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 950, "seek": 405774, "start": 4069.4199999999996, "end": 4070.9399999999996, "text": " So you think it's 15 years?", "tokens": [50948, 407, 291, 519, 309, 311, 2119, 924, 30, 51024], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 951, "seek": 405774, "start": 4070.9399999999996, "end": 4072.7, "text": " No, I think it's more like five.", "tokens": [51024, 883, 11, 286, 519, 309, 311, 544, 411, 1732, 13, 51112], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 952, "seek": 405774, "start": 4072.7, "end": 4077.3399999999997, "text": " Like I made that prediction that autoregressive LLMs will probably...", "tokens": [51112, 1743, 286, 1027, 300, 17630, 300, 1476, 418, 3091, 488, 441, 43, 26386, 486, 1391, 485, 51344], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 953, "seek": 405774, "start": 4077.3399999999997, "end": 4079.4199999999996, "text": " Five years, that's true, yeah, they're doomed.", "tokens": [51344, 9436, 924, 11, 300, 311, 2074, 11, 1338, 11, 436, 434, 33847, 13, 51448], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 954, "seek": 405774, "start": 4079.9799999999996, "end": 4081.74, "text": " Yeah, I mean, I might be wrong, obviously.", "tokens": [51476, 865, 11, 286, 914, 11, 286, 1062, 312, 2085, 11, 2745, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 955, "seek": 405774, "start": 4082.4599999999996, "end": 4084.2999999999997, "text": " We will hold you to that.", "tokens": [51600, 492, 486, 1797, 291, 281, 300, 13, 51692], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 956, "seek": 405774, "start": 4084.2999999999997, "end": 4086.3799999999997, "text": " I'll come back and revisit in five years.", "tokens": [51692, 286, 603, 808, 646, 293, 32676, 294, 1732, 924, 13, 51796], "temperature": 0.0, "avg_logprob": -0.18526408620124316, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0011646774364635348}, {"id": 957, "seek": 408638, "start": 4086.38, "end": 4089.7400000000002, "text": " Maybe it's a wishful thinking, self-fulfilling prophecy perhaps.", "tokens": [50364, 2704, 309, 311, 257, 3172, 906, 1953, 11, 2698, 12, 906, 69, 7345, 23945, 4317, 13, 50532], "temperature": 0.0, "avg_logprob": -0.1551398065355089, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0011360757052898407}, {"id": 958, "seek": 408638, "start": 4091.26, "end": 4095.5, "text": " A question for something different here from Sam Scarpino,", "tokens": [50608, 316, 1168, 337, 746, 819, 510, 490, 4832, 2747, 6529, 2982, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1551398065355089, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0011360757052898407}, {"id": 959, "seek": 408638, "start": 4095.5, "end": 4099.1, "text": " director of AI and Life Sciences at the Institute for Experiential AI.", "tokens": [50820, 5391, 295, 7318, 293, 7720, 21108, 412, 264, 9446, 337, 12522, 1196, 831, 7318, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1551398065355089, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0011360757052898407}, {"id": 960, "seek": 408638, "start": 4099.9800000000005, "end": 4106.86, "text": " What are the biggest gaps on the education side for graduates of higher education in AI", "tokens": [51044, 708, 366, 264, 3880, 15031, 322, 264, 3309, 1252, 337, 13577, 295, 2946, 3309, 294, 7318, 51388], "temperature": 0.0, "avg_logprob": -0.1551398065355089, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0011360757052898407}, {"id": 961, "seek": 408638, "start": 4107.42, "end": 4111.26, "text": " and in particular the new directions AI is taking?", "tokens": [51416, 293, 294, 1729, 264, 777, 11095, 7318, 307, 1940, 30, 51608], "temperature": 0.0, "avg_logprob": -0.1551398065355089, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0011360757052898407}, {"id": 962, "seek": 408638, "start": 4111.26, "end": 4113.26, "text": " What do you think is missing?", "tokens": [51608, 708, 360, 291, 519, 307, 5361, 30, 51708], "temperature": 0.0, "avg_logprob": -0.1551398065355089, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.0011360757052898407}, {"id": 963, "seek": 411326, "start": 4114.22, "end": 4117.42, "text": " So I think what's missing...", "tokens": [50412, 407, 286, 519, 437, 311, 5361, 485, 50572], "temperature": 0.0, "avg_logprob": -0.10960779492817228, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.00046478433068841696}, {"id": 964, "seek": 411326, "start": 4117.42, "end": 4121.66, "text": " So it depends which major you're following.", "tokens": [50572, 407, 309, 5946, 597, 2563, 291, 434, 3480, 13, 50784], "temperature": 0.0, "avg_logprob": -0.10960779492817228, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.00046478433068841696}, {"id": 965, "seek": 411326, "start": 4124.62, "end": 4132.860000000001, "text": " Most computer science curricula in the US are very weak in mathematics.", "tokens": [50932, 4534, 3820, 3497, 13179, 3780, 294, 264, 2546, 366, 588, 5336, 294, 18666, 13, 51344], "temperature": 0.0, "avg_logprob": -0.10960779492817228, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.00046478433068841696}, {"id": 966, "seek": 411326, "start": 4133.820000000001, "end": 4138.54, "text": " The requirements for mathematics in a typical CS degree,", "tokens": [51392, 440, 7728, 337, 18666, 294, 257, 7476, 9460, 4314, 11, 51628], "temperature": 0.0, "avg_logprob": -0.10960779492817228, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.00046478433068841696}, {"id": 967, "seek": 411326, "start": 4138.54, "end": 4141.02, "text": " the minimum requirement is very, very small, right?", "tokens": [51628, 264, 7285, 11695, 307, 588, 11, 588, 1359, 11, 558, 30, 51752], "temperature": 0.0, "avg_logprob": -0.10960779492817228, "compression_ratio": 1.4457142857142857, "no_speech_prob": 0.00046478433068841696}, {"id": 968, "seek": 414102, "start": 4141.02, "end": 4145.42, "text": " It's one course in discrete math and perhaps in algebra if you're lucky.", "tokens": [50364, 467, 311, 472, 1164, 294, 27706, 5221, 293, 4317, 294, 21989, 498, 291, 434, 6356, 13, 50584], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 969, "seek": 414102, "start": 4146.38, "end": 4149.18, "text": " Maybe a probability if you are courageous.", "tokens": [50632, 2704, 257, 8482, 498, 291, 366, 33233, 13, 50772], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 970, "seek": 414102, "start": 4151.1, "end": 4152.46, "text": " But what about optimization?", "tokens": [50868, 583, 437, 466, 19618, 30, 50936], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 971, "seek": 414102, "start": 4152.46, "end": 4154.780000000001, "text": " That would be something that would be very useful.", "tokens": [50936, 663, 576, 312, 746, 300, 576, 312, 588, 4420, 13, 51052], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 972, "seek": 414102, "start": 4154.780000000001, "end": 4160.14, "text": " And then there is courses in physics because the mathematics of inference", "tokens": [51052, 400, 550, 456, 307, 7712, 294, 10649, 570, 264, 18666, 295, 38253, 51320], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 973, "seek": 414102, "start": 4160.14, "end": 4166.22, "text": " and variational autoencoder and stuff like that, graphical models, etc.", "tokens": [51320, 293, 3034, 1478, 8399, 22660, 19866, 293, 1507, 411, 300, 11, 35942, 5245, 11, 5183, 13, 51624], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 974, "seek": 414102, "start": 4166.22, "end": 4168.860000000001, "text": " The mathematics of this is from statistical physics.", "tokens": [51624, 440, 18666, 295, 341, 307, 490, 22820, 10649, 13, 51756], "temperature": 0.0, "avg_logprob": -0.20982933044433594, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0007312196539714932}, {"id": 975, "seek": 416886, "start": 4169.82, "end": 4174.0599999999995, "text": " And so if you have a choice between taking your course in, I don't know,", "tokens": [50412, 400, 370, 498, 291, 362, 257, 3922, 1296, 1940, 428, 1164, 294, 11, 286, 500, 380, 458, 11, 50624], "temperature": 0.0, "avg_logprob": -0.18803829616970485, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00010705966997193173}, {"id": 976, "seek": 416886, "start": 4174.94, "end": 4178.86, "text": " mobile app programming or quantum mechanics, take quantum mechanics.", "tokens": [50668, 6013, 724, 9410, 420, 13018, 12939, 11, 747, 13018, 12939, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18803829616970485, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00010705966997193173}, {"id": 977, "seek": 416886, "start": 4180.7, "end": 4181.339999999999, "text": " I'm not kidding.", "tokens": [50956, 286, 478, 406, 9287, 13, 50988], "temperature": 0.0, "avg_logprob": -0.18803829616970485, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00010705966997193173}, {"id": 978, "seek": 416886, "start": 4188.54, "end": 4195.339999999999, "text": " This is a question that came from the audience and a few of the people at the Institute.", "tokens": [51348, 639, 307, 257, 1168, 300, 1361, 490, 264, 4034, 293, 257, 1326, 295, 264, 561, 412, 264, 9446, 13, 51688], "temperature": 0.0, "avg_logprob": -0.18803829616970485, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.00010705966997193173}, {"id": 979, "seek": 419534, "start": 4196.22, "end": 4201.1, "text": " Your thoughts on the current, you know, these recent congressional hearings where", "tokens": [50408, 2260, 4598, 322, 264, 2190, 11, 291, 458, 11, 613, 5162, 32962, 34052, 689, 50652], "temperature": 0.0, "avg_logprob": -0.13982009887695312, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0009995116852223873}, {"id": 980, "seek": 419534, "start": 4202.78, "end": 4209.900000000001, "text": " certainly seems like much of the testimony by some Altman was understandably self-serving.", "tokens": [50736, 3297, 2544, 411, 709, 295, 264, 15634, 538, 512, 15992, 1601, 390, 1223, 1188, 2698, 12, 12484, 798, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13982009887695312, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0009995116852223873}, {"id": 981, "seek": 419534, "start": 4209.900000000001, "end": 4214.860000000001, "text": " You know, they need to be allowed to compete and have their way of working protected.", "tokens": [51092, 509, 458, 11, 436, 643, 281, 312, 4350, 281, 11831, 293, 362, 641, 636, 295, 1364, 10594, 13, 51340], "temperature": 0.0, "avg_logprob": -0.13982009887695312, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0009995116852223873}, {"id": 982, "seek": 419534, "start": 4215.42, "end": 4219.02, "text": " At the same time, he's encouraging the rest of the community to be open source.", "tokens": [51368, 1711, 264, 912, 565, 11, 415, 311, 14580, 264, 1472, 295, 264, 1768, 281, 312, 1269, 4009, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13982009887695312, "compression_ratio": 1.5504587155963303, "no_speech_prob": 0.0009995116852223873}, {"id": 983, "seek": 421902, "start": 4219.660000000001, "end": 4224.700000000001, "text": " What would you have said to Congress?", "tokens": [50396, 708, 576, 291, 362, 848, 281, 6426, 30, 50648], "temperature": 0.0, "avg_logprob": -0.21322766817533054, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.00045082042925059795}, {"id": 984, "seek": 421902, "start": 4224.700000000001, "end": 4225.820000000001, "text": " Have you been on those hearings?", "tokens": [50648, 3560, 291, 668, 322, 729, 34052, 30, 50704], "temperature": 0.0, "avg_logprob": -0.21322766817533054, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.00045082042925059795}, {"id": 985, "seek": 421902, "start": 4228.22, "end": 4229.02, "text": " I was not invited.", "tokens": [50824, 286, 390, 406, 9185, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21322766817533054, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.00045082042925059795}, {"id": 986, "seek": 421902, "start": 4230.860000000001, "end": 4233.1, "text": " I was not invited to the White House either before that.", "tokens": [50956, 286, 390, 406, 9185, 281, 264, 5552, 4928, 2139, 949, 300, 13, 51068], "temperature": 0.0, "avg_logprob": -0.21322766817533054, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.00045082042925059795}, {"id": 987, "seek": 421902, "start": 4235.9800000000005, "end": 4245.42, "text": " So what I would have recommended is that if you want a vibrant ecosystem on top of current AI technology,", "tokens": [51212, 407, 437, 286, 576, 362, 9628, 307, 300, 498, 291, 528, 257, 21571, 11311, 322, 1192, 295, 2190, 7318, 2899, 11, 51684], "temperature": 0.0, "avg_logprob": -0.21322766817533054, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.00045082042925059795}, {"id": 988, "seek": 424542, "start": 4246.38, "end": 4253.18, "text": " you need to have sort of open source based models on top of which an industry can be built.", "tokens": [50412, 291, 643, 281, 362, 1333, 295, 1269, 4009, 2361, 5245, 322, 1192, 295, 597, 364, 3518, 393, 312, 3094, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1194943110148112, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0019250173354521394}, {"id": 989, "seek": 424542, "start": 4254.38, "end": 4259.66, "text": " And that industry will build vertical applications for particular domains", "tokens": [50812, 400, 300, 3518, 486, 1322, 9429, 5821, 337, 1729, 25514, 51076], "temperature": 0.0, "avg_logprob": -0.1194943110148112, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0019250173354521394}, {"id": 990, "seek": 424542, "start": 4259.66, "end": 4260.9400000000005, "text": " on top of a base model.", "tokens": [51076, 322, 1192, 295, 257, 3096, 2316, 13, 51140], "temperature": 0.0, "avg_logprob": -0.1194943110148112, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0019250173354521394}, {"id": 991, "seek": 424542, "start": 4260.9400000000005, "end": 4266.22, "text": " You don't want to have 25 companies selling 25 different base models", "tokens": [51140, 509, 500, 380, 528, 281, 362, 3552, 3431, 6511, 3552, 819, 3096, 5245, 51404], "temperature": 0.0, "avg_logprob": -0.1194943110148112, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0019250173354521394}, {"id": 992, "seek": 424542, "start": 4267.66, "end": 4268.78, "text": " and keep them closed source.", "tokens": [51476, 293, 1066, 552, 5395, 4009, 13, 51532], "temperature": 0.0, "avg_logprob": -0.1194943110148112, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0019250173354521394}, {"id": 993, "seek": 424542, "start": 4268.78, "end": 4272.54, "text": " If you want an industry to be built on top of it, the infrastructure has to be open.", "tokens": [51532, 759, 291, 528, 364, 3518, 281, 312, 3094, 322, 1192, 295, 309, 11, 264, 6896, 575, 281, 312, 1269, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1194943110148112, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0019250173354521394}, {"id": 994, "seek": 427254, "start": 4273.26, "end": 4278.14, "text": " Because that's the only way to really sort of know what you're doing, essentially.", "tokens": [50400, 1436, 300, 311, 264, 787, 636, 281, 534, 1333, 295, 458, 437, 291, 434, 884, 11, 4476, 13, 50644], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 995, "seek": 427254, "start": 4279.66, "end": 4282.14, "text": " And to have some control about your future, right?", "tokens": [50720, 400, 281, 362, 512, 1969, 466, 428, 2027, 11, 558, 30, 50844], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 996, "seek": 427254, "start": 4282.14, "end": 4285.82, "text": " You can't just go like this and pray that.", "tokens": [50844, 509, 393, 380, 445, 352, 411, 341, 293, 3690, 300, 13, 51028], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 997, "seek": 427254, "start": 4285.82, "end": 4287.0199999999995, "text": " Unix versus Windows.", "tokens": [51028, 1156, 970, 5717, 8591, 13, 51088], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 998, "seek": 427254, "start": 4287.66, "end": 4292.06, "text": " Right, so if you go back to the history of the Internet, there was a similar story where", "tokens": [51120, 1779, 11, 370, 498, 291, 352, 646, 281, 264, 2503, 295, 264, 7703, 11, 456, 390, 257, 2531, 1657, 689, 51340], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 999, "seek": 427254, "start": 4293.34, "end": 4297.74, "text": " back in 1992 when the built Internet Al Gore started to figure out like what,", "tokens": [51404, 646, 294, 23952, 562, 264, 3094, 7703, 967, 45450, 1409, 281, 2573, 484, 411, 437, 11, 51624], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 1000, "seek": 427254, "start": 4297.74, "end": 4299.9, "text": " you know, how do we build the information superhighway?", "tokens": [51624, 291, 458, 11, 577, 360, 321, 1322, 264, 1589, 1687, 21454, 676, 30, 51732], "temperature": 0.0, "avg_logprob": -0.23461901924826883, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0007785923662595451}, {"id": 1001, "seek": 429990, "start": 4300.86, "end": 4306.139999999999, "text": " They went to see, you know, the big communication companies like AT&T and AT&T told them,", "tokens": [50412, 814, 1437, 281, 536, 11, 291, 458, 11, 264, 955, 6101, 3431, 411, 8872, 5, 51, 293, 8872, 5, 51, 1907, 552, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1002, "seek": 429990, "start": 4306.139999999999, "end": 4307.42, "text": " oh, you know, leave it to us.", "tokens": [50676, 1954, 11, 291, 458, 11, 1856, 309, 281, 505, 13, 50740], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1003, "seek": 429990, "start": 4307.42, "end": 4308.62, "text": " We'll build the stuff.", "tokens": [50740, 492, 603, 1322, 264, 1507, 13, 50800], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1004, "seek": 429990, "start": 4308.62, "end": 4312.62, "text": " It's going to be, you know, ATM and ISD enter the home and blah, blah, blah.", "tokens": [50800, 467, 311, 516, 281, 312, 11, 291, 458, 11, 46455, 293, 6205, 35, 3242, 264, 1280, 293, 12288, 11, 12288, 11, 12288, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1005, "seek": 429990, "start": 4312.62, "end": 4316.139999999999, "text": " It'll be wonderful and you'll have to pay, you know, $5 per hour.", "tokens": [51000, 467, 603, 312, 3715, 293, 291, 603, 362, 281, 1689, 11, 291, 458, 11, 1848, 20, 680, 1773, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1006, "seek": 429990, "start": 4318.219999999999, "end": 4319.42, "text": " And Al Gore said no.", "tokens": [51280, 400, 967, 45450, 848, 572, 13, 51340], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1007, "seek": 429990, "start": 4319.42, "end": 4323.82, "text": " He said we're going to make the, what was an ARPANET that became the Internet,", "tokens": [51340, 634, 848, 321, 434, 516, 281, 652, 264, 11, 437, 390, 364, 8943, 47, 1770, 4850, 300, 3062, 264, 7703, 11, 51560], "temperature": 0.0, "avg_logprob": -0.1790081762498425, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0006652510492131114}, {"id": 1008, "seek": 432382, "start": 4324.0599999999995, "end": 4330.54, "text": " basically available to the public and delocalized and, you know, self,", "tokens": [50376, 1936, 2435, 281, 264, 1908, 293, 1103, 36483, 1602, 293, 11, 291, 458, 11, 2698, 11, 50700], "temperature": 0.0, "avg_logprob": -0.15172535540109658, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0023570603225380182}, {"id": 1009, "seek": 432382, "start": 4331.82, "end": 4335.5, "text": " basically open in terms of standard and no company is going to control it.", "tokens": [50764, 1936, 1269, 294, 2115, 295, 3832, 293, 572, 2237, 307, 516, 281, 1969, 309, 13, 50948], "temperature": 0.0, "avg_logprob": -0.15172535540109658, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0023570603225380182}, {"id": 1010, "seek": 432382, "start": 4336.0599999999995, "end": 4338.139999999999, "text": " And that was a really, really good idea.", "tokens": [50976, 400, 300, 390, 257, 534, 11, 534, 665, 1558, 13, 51080], "temperature": 0.0, "avg_logprob": -0.15172535540109658, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0023570603225380182}, {"id": 1011, "seek": 432382, "start": 4338.139999999999, "end": 4339.34, "text": " We can thank Al Gore for this.", "tokens": [51080, 492, 393, 1309, 967, 45450, 337, 341, 13, 51140], "temperature": 0.0, "avg_logprob": -0.15172535540109658, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0023570603225380182}, {"id": 1012, "seek": 432382, "start": 4340.0599999999995, "end": 4341.9, "text": " The world can thank Al Gore, not just the U.S.", "tokens": [51176, 440, 1002, 393, 1309, 967, 45450, 11, 406, 445, 264, 624, 13, 50, 13, 51268], "temperature": 0.0, "avg_logprob": -0.15172535540109658, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0023570603225380182}, {"id": 1013, "seek": 432382, "start": 4347.66, "end": 4348.78, "text": " He did invent the Internet.", "tokens": [51556, 634, 630, 7962, 264, 7703, 13, 51612], "temperature": 0.0, "avg_logprob": -0.15172535540109658, "compression_ratio": 1.5051546391752577, "no_speech_prob": 0.0023570603225380182}, {"id": 1014, "seek": 434878, "start": 4349.5, "end": 4353.98, "text": " And then a similar story happened several years later when people started to realize", "tokens": [50400, 400, 550, 257, 2531, 1657, 2011, 2940, 924, 1780, 562, 561, 1409, 281, 4325, 50624], "temperature": 0.0, "avg_logprob": -0.1472630160195487, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.0002913239586632699}, {"id": 1015, "seek": 434878, "start": 4353.98, "end": 4359.34, "text": " that you could use, you know, graphic browsers like Mosaic and Netscape and stuff like that,", "tokens": [50624, 300, 291, 727, 764, 11, 291, 458, 11, 14089, 36069, 411, 376, 42261, 293, 426, 1385, 4747, 293, 1507, 411, 300, 11, 50892], "temperature": 0.0, "avg_logprob": -0.1472630160195487, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.0002913239586632699}, {"id": 1016, "seek": 434878, "start": 4359.34, "end": 4363.34, "text": " right, when the World Wide Web became popular.", "tokens": [50892, 558, 11, 562, 264, 3937, 42543, 9573, 3062, 3743, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1472630160195487, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.0002913239586632699}, {"id": 1017, "seek": 434878, "start": 4364.54, "end": 4367.9, "text": " So there was a war between Sun Microsystems and Microsoft.", "tokens": [51152, 407, 456, 390, 257, 1516, 1296, 6163, 5818, 2635, 9321, 82, 293, 8116, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1472630160195487, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.0002913239586632699}, {"id": 1018, "seek": 434878, "start": 4367.9, "end": 4372.0599999999995, "text": " Sun Microsystems said, oh, we're going to sell you servers running Solaris,", "tokens": [51320, 6163, 5818, 2635, 9321, 82, 848, 11, 1954, 11, 321, 434, 516, 281, 3607, 291, 15909, 2614, 22385, 271, 11, 51528], "temperature": 0.0, "avg_logprob": -0.1472630160195487, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.0002913239586632699}, {"id": 1019, "seek": 434878, "start": 4372.0599999999995, "end": 4376.3, "text": " the version of Unix, with, you know, our web server infrastructure and Java.", "tokens": [51528, 264, 3037, 295, 1156, 970, 11, 365, 11, 291, 458, 11, 527, 3670, 7154, 6896, 293, 10745, 13, 51740], "temperature": 0.0, "avg_logprob": -0.1472630160195487, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.0002913239586632699}, {"id": 1020, "seek": 437630, "start": 4377.18, "end": 4379.1, "text": " And you're going to be able to build, like, anything you want.", "tokens": [50408, 400, 291, 434, 516, 281, 312, 1075, 281, 1322, 11, 411, 11, 1340, 291, 528, 13, 50504], "temperature": 0.0, "avg_logprob": -0.15105880599424062, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.00026107754092663527}, {"id": 1021, "seek": 437630, "start": 4379.9800000000005, "end": 4385.02, "text": " Microsoft said no, it's going to be Windows NT with the IIT web server and the ASP", "tokens": [50548, 8116, 848, 572, 11, 309, 311, 516, 281, 312, 8591, 43452, 365, 264, 286, 3927, 3670, 7154, 293, 264, 7469, 47, 50800], "temperature": 0.0, "avg_logprob": -0.15105880599424062, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.00026107754092663527}, {"id": 1022, "seek": 437630, "start": 4385.9800000000005, "end": 4391.58, "text": " website, you know, server-side protocol framework, whatever.", "tokens": [50848, 3144, 11, 291, 458, 11, 7154, 12, 1812, 10336, 8388, 11, 2035, 13, 51128], "temperature": 0.0, "avg_logprob": -0.15105880599424062, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.00026107754092663527}, {"id": 1023, "seek": 437630, "start": 4392.38, "end": 4393.1, "text": " They both lost.", "tokens": [51168, 814, 1293, 2731, 13, 51204], "temperature": 0.0, "avg_logprob": -0.15105880599424062, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.00026107754092663527}, {"id": 1024, "seek": 437630, "start": 4394.22, "end": 4402.54, "text": " Sun Microsystem went bankrupt, was sold for parts to Oracle, and Microsoft essentially", "tokens": [51260, 6163, 5818, 2635, 9321, 1437, 21780, 11, 390, 3718, 337, 3166, 281, 25654, 11, 293, 8116, 4476, 51676], "temperature": 0.0, "avg_logprob": -0.15105880599424062, "compression_ratio": 1.4927536231884058, "no_speech_prob": 0.00026107754092663527}, {"id": 1025, "seek": 440254, "start": 4402.54, "end": 4405.1, "text": " exited the market.", "tokens": [50364, 454, 1226, 264, 2142, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1026, "seek": 440254, "start": 4405.74, "end": 4408.46, "text": " One was Linux and Apache, open source.", "tokens": [50524, 1485, 390, 18734, 293, 46597, 11, 1269, 4009, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1027, "seek": 440254, "start": 4408.46, "end": 4414.54, "text": " And the reason is because it's such an essential basic infrastructure that it has to be open.", "tokens": [50660, 400, 264, 1778, 307, 570, 309, 311, 1270, 364, 7115, 3875, 6896, 300, 309, 575, 281, 312, 1269, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1028, "seek": 440254, "start": 4415.98, "end": 4422.22, "text": " It progresses faster if it's open, and it's more reliable, it's more secure.", "tokens": [51036, 467, 41929, 4663, 498, 309, 311, 1269, 11, 293, 309, 311, 544, 12924, 11, 309, 311, 544, 7144, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1029, "seek": 440254, "start": 4422.22, "end": 4423.58, "text": " I mean, there's all the advantages.", "tokens": [51348, 286, 914, 11, 456, 311, 439, 264, 14906, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1030, "seek": 440254, "start": 4423.58, "end": 4426.46, "text": " And, you know, it's easier for startups to build on top of it.", "tokens": [51416, 400, 11, 291, 458, 11, 309, 311, 3571, 337, 28041, 281, 1322, 322, 1192, 295, 309, 13, 51560], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1031, "seek": 440254, "start": 4426.46, "end": 4431.74, "text": " So in the future, we're going to see AI systems as basic infrastructure.", "tokens": [51560, 407, 294, 264, 2027, 11, 321, 434, 516, 281, 536, 7318, 3652, 382, 3875, 6896, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1323765833443458, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.001064572250470519}, {"id": 1032, "seek": 443254, "start": 4432.7, "end": 4436.86, "text": " All of our interactions ten years from now with the digital world will be through", "tokens": [50372, 1057, 295, 527, 13280, 2064, 924, 490, 586, 365, 264, 4562, 1002, 486, 312, 807, 50580], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1033, "seek": 443254, "start": 4437.58, "end": 4442.62, "text": " an intelligent virtual agent that will be with us all the time.", "tokens": [50616, 364, 13232, 6374, 9461, 300, 486, 312, 365, 505, 439, 264, 565, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1034, "seek": 443254, "start": 4442.62, "end": 4446.46, "text": " It's like every one of us will have a staff of intelligent people working for us.", "tokens": [50868, 467, 311, 411, 633, 472, 295, 505, 486, 362, 257, 3525, 295, 13232, 561, 1364, 337, 505, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1035, "seek": 443254, "start": 4447.82, "end": 4448.54, "text": " Okay?", "tokens": [51128, 1033, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1036, "seek": 443254, "start": 4448.54, "end": 4452.54, "text": " We shouldn't be threatened by the fact that those things will be smarter than us.", "tokens": [51164, 492, 4659, 380, 312, 18268, 538, 264, 1186, 300, 729, 721, 486, 312, 20294, 813, 505, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1037, "seek": 443254, "start": 4452.54, "end": 4456.78, "text": " Like everybody that, you know, is working with me at fair is smarter than me.", "tokens": [51364, 1743, 2201, 300, 11, 291, 458, 11, 307, 1364, 365, 385, 412, 3143, 307, 20294, 813, 385, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1038, "seek": 443254, "start": 4456.78, "end": 4459.34, "text": " So I don't feel threatened by that.", "tokens": [51576, 407, 286, 500, 380, 841, 18268, 538, 300, 13, 51704], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1039, "seek": 443254, "start": 4459.34, "end": 4462.46, "text": " You're not a very good manager if you're threatened by people who are smarter than you.", "tokens": [51704, 509, 434, 406, 257, 588, 665, 6598, 498, 291, 434, 18268, 538, 561, 567, 366, 20294, 813, 291, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1351802498102188, "compression_ratio": 1.8268551236749118, "no_speech_prob": 0.0007314945687539876}, {"id": 1040, "seek": 446254, "start": 4462.62, "end": 4468.46, "text": " So your purpose actually should be to hire people, only people who are smarter than you.", "tokens": [50368, 407, 428, 4334, 767, 820, 312, 281, 11158, 561, 11, 787, 561, 567, 366, 20294, 813, 291, 13, 50660], "temperature": 0.0, "avg_logprob": -0.12666571140289307, "compression_ratio": 1.7186147186147187, "no_speech_prob": 7.841823389753699e-05}, {"id": 1041, "seek": 446254, "start": 4468.46, "end": 4473.1, "text": " But anyway, so we're going to have those intelligent systems that are going to be", "tokens": [50660, 583, 4033, 11, 370, 321, 434, 516, 281, 362, 729, 13232, 3652, 300, 366, 516, 281, 312, 50892], "temperature": 0.0, "avg_logprob": -0.12666571140289307, "compression_ratio": 1.7186147186147187, "no_speech_prob": 7.841823389753699e-05}, {"id": 1042, "seek": 446254, "start": 4473.1, "end": 4476.06, "text": " under control that are going to help us, you know, daily lives.", "tokens": [50892, 833, 1969, 300, 366, 516, 281, 854, 505, 11, 291, 458, 11, 5212, 2909, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12666571140289307, "compression_ratio": 1.7186147186147187, "no_speech_prob": 7.841823389753699e-05}, {"id": 1043, "seek": 446254, "start": 4476.06, "end": 4481.42, "text": " And we need those systems to be open because if it's kind of a closed system controlled by", "tokens": [51040, 400, 321, 643, 729, 3652, 281, 312, 1269, 570, 498, 309, 311, 733, 295, 257, 5395, 1185, 10164, 538, 51308], "temperature": 0.0, "avg_logprob": -0.12666571140289307, "compression_ratio": 1.7186147186147187, "no_speech_prob": 7.841823389753699e-05}, {"id": 1044, "seek": 446254, "start": 4481.42, "end": 4486.22, "text": " some company in California, it's going to be able to control our entire", "tokens": [51308, 512, 2237, 294, 5384, 11, 309, 311, 516, 281, 312, 1075, 281, 1969, 527, 2302, 51548], "temperature": 0.0, "avg_logprob": -0.12666571140289307, "compression_ratio": 1.7186147186147187, "no_speech_prob": 7.841823389753699e-05}, {"id": 1045, "seek": 448622, "start": 4486.38, "end": 4490.62, "text": " knowledge and data diet.", "tokens": [50372, 3601, 293, 1412, 6339, 13, 50584], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1046, "seek": 448622, "start": 4491.58, "end": 4493.1, "text": " And that's just too dangerous.", "tokens": [50632, 400, 300, 311, 445, 886, 5795, 13, 50708], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1047, "seek": 448622, "start": 4493.1, "end": 4495.5, "text": " And it's not necessary.", "tokens": [50708, 400, 309, 311, 406, 4818, 13, 50828], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1048, "seek": 448622, "start": 4495.5, "end": 4500.06, "text": " It's necessary for a search engine or a social network because it has to be centralized for", "tokens": [50828, 467, 311, 4818, 337, 257, 3164, 2848, 420, 257, 2093, 3209, 570, 309, 575, 281, 312, 32395, 337, 51056], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1049, "seek": 448622, "start": 4500.06, "end": 4500.7, "text": " various reasons.", "tokens": [51056, 3683, 4112, 13, 51088], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1050, "seek": 448622, "start": 4500.7, "end": 4504.22, "text": " But for an agent like this, it could run on your local device.", "tokens": [51088, 583, 337, 364, 9461, 411, 341, 11, 309, 727, 1190, 322, 428, 2654, 4302, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1051, "seek": 448622, "start": 4504.22, "end": 4505.5, "text": " It could run on your laptop.", "tokens": [51264, 467, 727, 1190, 322, 428, 10732, 13, 51328], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1052, "seek": 448622, "start": 4505.5, "end": 4509.740000000001, "text": " You don't have to talk necessarily with big servers in California.", "tokens": [51328, 509, 500, 380, 362, 281, 751, 4725, 365, 955, 15909, 294, 5384, 13, 51540], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1053, "seek": 448622, "start": 4509.740000000001, "end": 4514.46, "text": " You don't want to give all your, you know, deepest secrets to that.", "tokens": [51540, 509, 500, 380, 528, 281, 976, 439, 428, 11, 291, 458, 11, 28288, 14093, 281, 300, 13, 51776], "temperature": 0.0, "avg_logprob": -0.09223354614532746, "compression_ratio": 1.653386454183267, "no_speech_prob": 0.0006361148552969098}, {"id": 1054, "seek": 451446, "start": 4514.46, "end": 4519.02, "text": " So it's going to have to be an open fact form for that reason.", "tokens": [50364, 407, 309, 311, 516, 281, 362, 281, 312, 364, 1269, 1186, 1254, 337, 300, 1778, 13, 50592], "temperature": 0.0, "avg_logprob": -0.17093954936112507, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.0005525857559405267}, {"id": 1055, "seek": 451446, "start": 4520.14, "end": 4525.58, "text": " If nothing else, governments around the world are going to insist that this is the case.", "tokens": [50648, 759, 1825, 1646, 11, 11280, 926, 264, 1002, 366, 516, 281, 13466, 300, 341, 307, 264, 1389, 13, 50920], "temperature": 0.0, "avg_logprob": -0.17093954936112507, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.0005525857559405267}, {"id": 1056, "seek": 451446, "start": 4525.58, "end": 4533.34, "text": " So that's why I would tell Congress, make it so that, like, don't ban open source", "tokens": [50920, 407, 300, 311, 983, 286, 576, 980, 6426, 11, 652, 309, 370, 300, 11, 411, 11, 500, 380, 5643, 1269, 4009, 51308], "temperature": 0.0, "avg_logprob": -0.17093954936112507, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.0005525857559405267}, {"id": 1057, "seek": 451446, "start": 4534.14, "end": 4535.02, "text": " LLMs.", "tokens": [51348, 441, 43, 26386, 13, 51392], "temperature": 0.0, "avg_logprob": -0.17093954936112507, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.0005525857559405267}, {"id": 1058, "seek": 451446, "start": 4535.02, "end": 4536.46, "text": " They're not going to destroy humanity.", "tokens": [51392, 814, 434, 406, 516, 281, 5293, 10243, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17093954936112507, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.0005525857559405267}, {"id": 1059, "seek": 451446, "start": 4537.74, "end": 4540.86, "text": " Yeah, they're going to be bad actors, but you know, you can have countermeasures", "tokens": [51528, 865, 11, 436, 434, 516, 281, 312, 1578, 10037, 11, 457, 291, 458, 11, 291, 393, 362, 5682, 1398, 20044, 51684], "temperature": 0.0, "avg_logprob": -0.17093954936112507, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.0005525857559405267}, {"id": 1060, "seek": 454086, "start": 4541.66, "end": 4543.42, "text": " and make it open.", "tokens": [50404, 293, 652, 309, 1269, 13, 50492], "temperature": 0.0, "avg_logprob": -0.23387858233874356, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.00034029383095912635}, {"id": 1061, "seek": 454086, "start": 4543.42, "end": 4545.42, "text": " It's the only way to make it safe.", "tokens": [50492, 467, 311, 264, 787, 636, 281, 652, 309, 3273, 13, 50592], "temperature": 0.0, "avg_logprob": -0.23387858233874356, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.00034029383095912635}, {"id": 1062, "seek": 454086, "start": 4553.42, "end": 4556.219999999999, "text": " I'll ask, we'll make this a quick question with a quick answer.", "tokens": [50992, 286, 603, 1029, 11, 321, 603, 652, 341, 257, 1702, 1168, 365, 257, 1702, 1867, 13, 51132], "temperature": 0.0, "avg_logprob": -0.23387858233874356, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.00034029383095912635}, {"id": 1063, "seek": 454086, "start": 4556.219999999999, "end": 4560.139999999999, "text": " And then I know we have some questions live, so we'll switch to those.", "tokens": [51132, 400, 550, 286, 458, 321, 362, 512, 1651, 1621, 11, 370, 321, 603, 3679, 281, 729, 13, 51328], "temperature": 0.0, "avg_logprob": -0.23387858233874356, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.00034029383095912635}, {"id": 1064, "seek": 454086, "start": 4564.94, "end": 4568.219999999999, "text": " In a way, you kind of answered this question when you said LLMs are doomed.", "tokens": [51568, 682, 257, 636, 11, 291, 733, 295, 10103, 341, 1168, 562, 291, 848, 441, 43, 26386, 366, 33847, 13, 51732], "temperature": 0.0, "avg_logprob": -0.23387858233874356, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.00034029383095912635}, {"id": 1065, "seek": 456822, "start": 4568.22, "end": 4577.18, "text": " But if LLMs were to become perfect, at least in language, would that ever give us insight", "tokens": [50364, 583, 498, 441, 43, 26386, 645, 281, 1813, 2176, 11, 412, 1935, 294, 2856, 11, 576, 300, 1562, 976, 505, 11269, 50812], "temperature": 0.0, "avg_logprob": -0.10845038178679231, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.0007838666206225753}, {"id": 1066, "seek": 456822, "start": 4577.18, "end": 4580.54, "text": " into how language and natural language understanding works?", "tokens": [50812, 666, 577, 2856, 293, 3303, 2856, 3701, 1985, 30, 50980], "temperature": 0.0, "avg_logprob": -0.10845038178679231, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.0007838666206225753}, {"id": 1067, "seek": 456822, "start": 4581.740000000001, "end": 4585.26, "text": " The language model today is distributed over these billions of parameters.", "tokens": [51040, 440, 2856, 2316, 965, 307, 12631, 670, 613, 17375, 295, 9834, 13, 51216], "temperature": 0.0, "avg_logprob": -0.10845038178679231, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.0007838666206225753}, {"id": 1068, "seek": 456822, "start": 4585.9800000000005, "end": 4595.1, "text": " And do you think we'll ever have an understandable LLM, like, for example, we use PCA to understand", "tokens": [51252, 400, 360, 291, 519, 321, 603, 1562, 362, 364, 25648, 441, 43, 44, 11, 411, 11, 337, 1365, 11, 321, 764, 6465, 32, 281, 1223, 51708], "temperature": 0.0, "avg_logprob": -0.10845038178679231, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.0007838666206225753}, {"id": 1069, "seek": 459510, "start": 4595.1, "end": 4596.22, "text": " what regression is doing?", "tokens": [50364, 437, 24590, 307, 884, 30, 50420], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1070, "seek": 459510, "start": 4597.02, "end": 4597.900000000001, "text": " Or is that hopeless?", "tokens": [50460, 1610, 307, 300, 27317, 30, 50504], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1071, "seek": 459510, "start": 4598.700000000001, "end": 4600.620000000001, "text": " At some point, I think it's going to be right to be hopeless.", "tokens": [50544, 1711, 512, 935, 11, 286, 519, 309, 311, 516, 281, 312, 558, 281, 312, 27317, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1072, "seek": 459510, "start": 4600.620000000001, "end": 4604.620000000001, "text": " I mean, we'll probably learn a lot about, you know, how the systems represent data", "tokens": [50640, 286, 914, 11, 321, 603, 1391, 1466, 257, 688, 466, 11, 291, 458, 11, 577, 264, 3652, 2906, 1412, 50840], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1073, "seek": 459510, "start": 4604.620000000001, "end": 4606.54, "text": " and, like, how they manipulate it and stuff like that.", "tokens": [50840, 293, 11, 411, 11, 577, 436, 20459, 309, 293, 1507, 411, 300, 13, 50936], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1074, "seek": 459510, "start": 4606.54, "end": 4607.740000000001, "text": " So this is not opaque, right?", "tokens": [50936, 407, 341, 307, 406, 42687, 11, 558, 30, 50996], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1075, "seek": 459510, "start": 4607.740000000001, "end": 4613.5, "text": " We can completely kind of, there's complete visibility on how the systems operate.", "tokens": [50996, 492, 393, 2584, 733, 295, 11, 456, 311, 3566, 19883, 322, 577, 264, 3652, 9651, 13, 51284], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1076, "seek": 459510, "start": 4613.5, "end": 4617.42, "text": " Now, the question is understanding really how the decisions are being made.", "tokens": [51284, 823, 11, 264, 1168, 307, 3701, 534, 577, 264, 5327, 366, 885, 1027, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1077, "seek": 459510, "start": 4617.42, "end": 4621.42, "text": " So I'm actually not particularly interested in those questions, like, you know, as long as", "tokens": [51480, 407, 286, 478, 767, 406, 4098, 3102, 294, 729, 1651, 11, 411, 11, 291, 458, 11, 382, 938, 382, 51680], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1078, "seek": 459510, "start": 4622.46, "end": 4623.58, "text": " they work properly.", "tokens": [51732, 436, 589, 6108, 13, 51788], "temperature": 0.0, "avg_logprob": -0.12858873094831194, "compression_ratio": 1.7278481012658229, "no_speech_prob": 0.0015948256477713585}, {"id": 1079, "seek": 462510, "start": 4625.5, "end": 4629.42, "text": " The same way, I'm not particularly interested in figuring out exactly how the brain works.", "tokens": [50384, 440, 912, 636, 11, 286, 478, 406, 4098, 3102, 294, 15213, 484, 2293, 577, 264, 3567, 1985, 13, 50580], "temperature": 0.0, "avg_logprob": -0.08651924977260353, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.00015112409892026335}, {"id": 1080, "seek": 462510, "start": 4629.42, "end": 4635.26, "text": " I'm more interested in figuring out how the brain builds itself so that it works, right?", "tokens": [50580, 286, 478, 544, 3102, 294, 15213, 484, 577, 264, 3567, 15182, 2564, 370, 300, 309, 1985, 11, 558, 30, 50872], "temperature": 0.0, "avg_logprob": -0.08651924977260353, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.00015112409892026335}, {"id": 1081, "seek": 462510, "start": 4635.26, "end": 4641.34, "text": " So I'm more interested in learning than in studying the result of learning, if you want.", "tokens": [50872, 407, 286, 478, 544, 3102, 294, 2539, 813, 294, 7601, 264, 1874, 295, 2539, 11, 498, 291, 528, 13, 51176], "temperature": 0.0, "avg_logprob": -0.08651924977260353, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.00015112409892026335}, {"id": 1082, "seek": 462510, "start": 4641.34, "end": 4643.660000000001, "text": " So it's the same for those systems.", "tokens": [51176, 407, 309, 311, 264, 912, 337, 729, 3652, 13, 51292], "temperature": 0.0, "avg_logprob": -0.08651924977260353, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.00015112409892026335}, {"id": 1083, "seek": 462510, "start": 4643.660000000001, "end": 4647.34, "text": " I'm more interested in how you get them to learn what you want and how to solve the problem", "tokens": [51292, 286, 478, 544, 3102, 294, 577, 291, 483, 552, 281, 1466, 437, 291, 528, 293, 577, 281, 5039, 264, 1154, 51476], "temperature": 0.0, "avg_logprob": -0.08651924977260353, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.00015112409892026335}, {"id": 1084, "seek": 462510, "start": 4647.34, "end": 4650.06, "text": " in the end is kind of considerably less interesting, in my opinion.", "tokens": [51476, 294, 264, 917, 307, 733, 295, 31308, 1570, 1880, 11, 294, 452, 4800, 13, 51612], "temperature": 0.0, "avg_logprob": -0.08651924977260353, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.00015112409892026335}, {"id": 1085, "seek": 465006, "start": 4650.46, "end": 4654.9400000000005, "text": " But at some point, they're going to be, you know, super intelligent,", "tokens": [50384, 583, 412, 512, 935, 11, 436, 434, 516, 281, 312, 11, 291, 458, 11, 1687, 13232, 11, 50608], "temperature": 0.0, "avg_logprob": -0.32616432345643337, "compression_ratio": 1.478448275862069, "no_speech_prob": 0.0003195300232619047}, {"id": 1086, "seek": 465006, "start": 4654.9400000000005, "end": 4656.9400000000005, "text": " repulsory of all human knowledge.", "tokens": [50608, 1085, 9468, 827, 295, 439, 1952, 3601, 13, 50708], "temperature": 0.0, "avg_logprob": -0.32616432345643337, "compression_ratio": 1.478448275862069, "no_speech_prob": 0.0003195300232619047}, {"id": 1087, "seek": 465006, "start": 4656.9400000000005, "end": 4662.22, "text": " You know, it's going to be too big for us to kind of comprehend at a deep level.", "tokens": [50708, 509, 458, 11, 309, 311, 516, 281, 312, 886, 955, 337, 505, 281, 733, 295, 38183, 412, 257, 2452, 1496, 13, 50972], "temperature": 0.0, "avg_logprob": -0.32616432345643337, "compression_ratio": 1.478448275862069, "no_speech_prob": 0.0003195300232619047}, {"id": 1088, "seek": 465006, "start": 4663.580000000001, "end": 4664.38, "text": " Fair.", "tokens": [51040, 12157, 13, 51080], "temperature": 0.0, "avg_logprob": -0.32616432345643337, "compression_ratio": 1.478448275862069, "no_speech_prob": 0.0003195300232619047}, {"id": 1089, "seek": 465006, "start": 4666.38, "end": 4670.38, "text": " And by the way, I failed to acknowledge that question came from Walid Saba, who's one of our", "tokens": [51180, 400, 538, 264, 636, 11, 286, 7612, 281, 10692, 300, 1168, 1361, 490, 9707, 327, 318, 5509, 11, 567, 311, 472, 295, 527, 51380], "temperature": 0.0, "avg_logprob": -0.32616432345643337, "compression_ratio": 1.478448275862069, "no_speech_prob": 0.0003195300232619047}, {"id": 1090, "seek": 465006, "start": 4670.38, "end": 4675.660000000001, "text": " senior research scientists at the EAI up in Portland, Maine.", "tokens": [51380, 7965, 2132, 7708, 412, 264, 35747, 40, 493, 294, 25020, 11, 28180, 13, 51644], "temperature": 0.0, "avg_logprob": -0.32616432345643337, "compression_ratio": 1.478448275862069, "no_speech_prob": 0.0003195300232619047}, {"id": 1091, "seek": 467566, "start": 4676.139999999999, "end": 4680.86, "text": " The next question I'll use, and then I'll switch over to audience questions,", "tokens": [50388, 440, 958, 1168, 286, 603, 764, 11, 293, 550, 286, 603, 3679, 670, 281, 4034, 1651, 11, 50624], "temperature": 0.0, "avg_logprob": -0.19815788021335354, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0007984298281371593}, {"id": 1092, "seek": 467566, "start": 4683.82, "end": 4688.78, "text": " comes from Gene Tunic, the director of AI Plus Health at the Institute for Experiential AI.", "tokens": [50772, 1487, 490, 18083, 21363, 299, 11, 264, 5391, 295, 7318, 7721, 5912, 412, 264, 9446, 337, 12522, 1196, 831, 7318, 13, 51020], "temperature": 0.0, "avg_logprob": -0.19815788021335354, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0007984298281371593}, {"id": 1093, "seek": 467566, "start": 4691.74, "end": 4696.38, "text": " You believe that deep learning can eventually lead to human-like understanding,", "tokens": [51168, 509, 1697, 300, 2452, 2539, 393, 4728, 1477, 281, 1952, 12, 4092, 3701, 11, 51400], "temperature": 0.0, "avg_logprob": -0.19815788021335354, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0007984298281371593}, {"id": 1094, "seek": 467566, "start": 4697.58, "end": 4703.74, "text": " and you have said that self-supervised learning from unlabeled data", "tokens": [51460, 293, 291, 362, 848, 300, 2698, 12, 48172, 24420, 2539, 490, 32118, 18657, 292, 1412, 51768], "temperature": 0.0, "avg_logprob": -0.19815788021335354, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0007984298281371593}, {"id": 1095, "seek": 470374, "start": 4704.62, "end": 4710.86, "text": " can be a powerful tool, although it seems like in human learning, as I was watching your examples,", "tokens": [50408, 393, 312, 257, 4005, 2290, 11, 4878, 309, 2544, 411, 294, 1952, 2539, 11, 382, 286, 390, 1976, 428, 5110, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11089234603078742, "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0009196996688842773}, {"id": 1096, "seek": 470374, "start": 4710.86, "end": 4718.54, "text": " for example, a lot of that data is, in a way, supervised or tied to some kind of reinforcement", "tokens": [50720, 337, 1365, 11, 257, 688, 295, 300, 1412, 307, 11, 294, 257, 636, 11, 46533, 420, 9601, 281, 512, 733, 295, 29280, 51104], "temperature": 0.0, "avg_logprob": -0.11089234603078742, "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0009196996688842773}, {"id": 1097, "seek": 470374, "start": 4718.54, "end": 4726.3, "text": " feedback around what to expect, is it good, is it bad, etc. So how, where do you draw that line", "tokens": [51104, 5824, 926, 437, 281, 2066, 11, 307, 309, 665, 11, 307, 309, 1578, 11, 5183, 13, 407, 577, 11, 689, 360, 291, 2642, 300, 1622, 51492], "temperature": 0.0, "avg_logprob": -0.11089234603078742, "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0009196996688842773}, {"id": 1098, "seek": 472630, "start": 4726.3, "end": 4738.06, "text": " between, you know, can we really truly go towards unsupervised, or there's a huge dependence on", "tokens": [50364, 1296, 11, 291, 458, 11, 393, 321, 534, 4908, 352, 3030, 2693, 12879, 24420, 11, 420, 456, 311, 257, 2603, 31704, 322, 50952], "temperature": 0.0, "avg_logprob": -0.13029513820525138, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00046531468979083}, {"id": 1099, "seek": 472630, "start": 4738.06, "end": 4743.42, "text": " supervised and on those labels to get it right? Because the world is, in a way, is telling us", "tokens": [50952, 46533, 293, 322, 729, 16949, 281, 483, 309, 558, 30, 1436, 264, 1002, 307, 11, 294, 257, 636, 11, 307, 3585, 505, 51220], "temperature": 0.0, "avg_logprob": -0.13029513820525138, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00046531468979083}, {"id": 1100, "seek": 472630, "start": 4743.42, "end": 4747.9800000000005, "text": " indirectly through supervision. So self-supervised learning, I mean, the reason it's called self-supervised", "tokens": [51220, 37779, 807, 32675, 13, 407, 2698, 12, 48172, 24420, 2539, 11, 286, 914, 11, 264, 1778, 309, 311, 1219, 2698, 12, 48172, 24420, 51448], "temperature": 0.0, "avg_logprob": -0.13029513820525138, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00046531468979083}, {"id": 1101, "seek": 472630, "start": 4747.9800000000005, "end": 4751.900000000001, "text": " is that deep down it's actually supervised learning. It's just supervised learning where", "tokens": [51448, 307, 300, 2452, 760, 309, 311, 767, 46533, 2539, 13, 467, 311, 445, 46533, 2539, 689, 51644], "temperature": 0.0, "avg_logprob": -0.13029513820525138, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00046531468979083}, {"id": 1102, "seek": 475190, "start": 4751.9, "end": 4758.0599999999995, "text": " the supervision signal is the input itself, right? So in a way, that's kind of, you know, a kind of", "tokens": [50364, 264, 32675, 6358, 307, 264, 4846, 2564, 11, 558, 30, 407, 294, 257, 636, 11, 300, 311, 733, 295, 11, 291, 458, 11, 257, 733, 295, 50672], "temperature": 0.0, "avg_logprob": -0.10499339733483656, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0004949076101183891}, {"id": 1103, "seek": 475190, "start": 4761.0199999999995, "end": 4766.46, "text": " simple answer to that question. It's still supervised learning in the end, but with", "tokens": [50820, 2199, 1867, 281, 300, 1168, 13, 467, 311, 920, 46533, 2539, 294, 264, 917, 11, 457, 365, 51092], "temperature": 0.0, "avg_logprob": -0.10499339733483656, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0004949076101183891}, {"id": 1104, "seek": 475190, "start": 4766.46, "end": 4771.58, "text": " particular architectures to handle uncertainty and dimensionality and things like that. Regarding", "tokens": [51092, 1729, 6331, 1303, 281, 4813, 15697, 293, 10139, 1860, 293, 721, 411, 300, 13, 35523, 51348], "temperature": 0.0, "avg_logprob": -0.10499339733483656, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0004949076101183891}, {"id": 1105, "seek": 475190, "start": 4771.58, "end": 4775.98, "text": " reinforcement learning, there is a point at which you need some form of reinforcement learning,", "tokens": [51348, 29280, 2539, 11, 456, 307, 257, 935, 412, 597, 291, 643, 512, 1254, 295, 29280, 2539, 11, 51568], "temperature": 0.0, "avg_logprob": -0.10499339733483656, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0004949076101183891}, {"id": 1106, "seek": 475190, "start": 4775.98, "end": 4781.5, "text": " and you need it in two situations, or at least techniques that have been developed in the context", "tokens": [51568, 293, 291, 643, 309, 294, 732, 6851, 11, 420, 412, 1935, 7512, 300, 362, 668, 4743, 294, 264, 4319, 51844], "temperature": 0.0, "avg_logprob": -0.10499339733483656, "compression_ratio": 1.73992673992674, "no_speech_prob": 0.0004949076101183891}, {"id": 1107, "seek": 478150, "start": 4781.82, "end": 4788.06, "text": " of reinforcement learning. The first situation is if the objective function that is optimized by", "tokens": [50380, 295, 29280, 2539, 13, 440, 700, 2590, 307, 498, 264, 10024, 2445, 300, 307, 26941, 538, 50692], "temperature": 0.0, "avg_logprob": -0.10202713310718536, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00012327967851888388}, {"id": 1108, "seek": 478150, "start": 4788.06, "end": 4793.9, "text": " your system does not reflect the ultimate objective function, you actually want to optimize. So for", "tokens": [50692, 428, 1185, 775, 406, 5031, 264, 9705, 10024, 2445, 11, 291, 767, 528, 281, 19719, 13, 407, 337, 50984], "temperature": 0.0, "avg_logprob": -0.10202713310718536, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00012327967851888388}, {"id": 1109, "seek": 478150, "start": 4793.9, "end": 4801.82, "text": " example, you're learning to ride a bike, your objective function is the, you know, time to the", "tokens": [50984, 1365, 11, 291, 434, 2539, 281, 5077, 257, 5656, 11, 428, 10024, 2445, 307, 264, 11, 291, 458, 11, 565, 281, 264, 51380], "temperature": 0.0, "avg_logprob": -0.10202713310718536, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00012327967851888388}, {"id": 1110, "seek": 480182, "start": 4801.82, "end": 4809.98, "text": " next fall or something, or the inverse time to next fall, you want to minimize that, right?", "tokens": [50364, 958, 2100, 420, 746, 11, 420, 264, 17340, 565, 281, 958, 2100, 11, 291, 528, 281, 17522, 300, 11, 558, 30, 50772], "temperature": 0.0, "avg_logprob": -0.11666411784157824, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.000656263146083802}, {"id": 1111, "seek": 480182, "start": 4812.139999999999, "end": 4819.98, "text": " But you don't know how to compute this from the internal state of your system. And so you need", "tokens": [50880, 583, 291, 500, 380, 458, 577, 281, 14722, 341, 490, 264, 6920, 1785, 295, 428, 1185, 13, 400, 370, 291, 643, 51272], "temperature": 0.0, "avg_logprob": -0.11666411784157824, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.000656263146083802}, {"id": 1112, "seek": 480182, "start": 4821.0199999999995, "end": 4825.98, "text": " to train an objective function to approximate this real cost, which in the context of reinforcement", "tokens": [51324, 281, 3847, 364, 10024, 2445, 281, 30874, 341, 957, 2063, 11, 597, 294, 264, 4319, 295, 29280, 51572], "temperature": 0.0, "avg_logprob": -0.11666411784157824, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.000656263146083802}, {"id": 1113, "seek": 482598, "start": 4825.98, "end": 4832.299999999999, "text": " learning is called a critic. So that's when you need one of those things. The other situation", "tokens": [50364, 2539, 307, 1219, 257, 7850, 13, 407, 300, 311, 562, 291, 643, 472, 295, 729, 721, 13, 440, 661, 2590, 50680], "temperature": 0.0, "avg_logprob": -0.07665315996698972, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0029336658772081137}, {"id": 1114, "seek": 482598, "start": 4832.299999999999, "end": 4837.82, "text": " where you need it is when your world model is not accurate because it's not been trained in all", "tokens": [50680, 689, 291, 643, 309, 307, 562, 428, 1002, 2316, 307, 406, 8559, 570, 309, 311, 406, 668, 8895, 294, 439, 50956], "temperature": 0.0, "avg_logprob": -0.07665315996698972, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0029336658772081137}, {"id": 1115, "seek": 482598, "start": 4837.82, "end": 4842.299999999999, "text": " corners of the state space, and you happen to be in a part of the state space that it wasn't trained", "tokens": [50956, 12413, 295, 264, 1785, 1901, 11, 293, 291, 1051, 281, 312, 294, 257, 644, 295, 264, 1785, 1901, 300, 309, 2067, 380, 8895, 51180], "temperature": 0.0, "avg_logprob": -0.07665315996698972, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0029336658772081137}, {"id": 1116, "seek": 482598, "start": 4842.299999999999, "end": 4846.7, "text": " on. Your world model is going to be bad, and your predictions are going to be bad, your planning is", "tokens": [51180, 322, 13, 2260, 1002, 2316, 307, 516, 281, 312, 1578, 11, 293, 428, 21264, 366, 516, 281, 312, 1578, 11, 428, 5038, 307, 51400], "temperature": 0.0, "avg_logprob": -0.07665315996698972, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0029336658772081137}, {"id": 1117, "seek": 482598, "start": 4846.7, "end": 4854.0599999999995, "text": " going to be bad. So to prevent this, you need to train your world model using things that are", "tokens": [51400, 516, 281, 312, 1578, 13, 407, 281, 4871, 341, 11, 291, 643, 281, 3847, 428, 1002, 2316, 1228, 721, 300, 366, 51768], "temperature": 0.0, "avg_logprob": -0.07665315996698972, "compression_ratio": 2.0508474576271185, "no_speech_prob": 0.0029336658772081137}, {"id": 1118, "seek": 485406, "start": 4854.14, "end": 4858.780000000001, "text": " called curiosity or exploration. And that's another concept that comes from reinforcement", "tokens": [50368, 1219, 18769, 420, 16197, 13, 400, 300, 311, 1071, 3410, 300, 1487, 490, 29280, 50600], "temperature": 0.0, "avg_logprob": -0.09986769122841918, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0005871441680938005}, {"id": 1119, "seek": 485406, "start": 4858.780000000001, "end": 4863.18, "text": " learning. So don't completely abandon reinforcement learning, but minimize its use.", "tokens": [50600, 2539, 13, 407, 500, 380, 2584, 9072, 29280, 2539, 11, 457, 17522, 1080, 764, 13, 50820], "temperature": 0.0, "avg_logprob": -0.09986769122841918, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0005871441680938005}, {"id": 1120, "seek": 485406, "start": 4864.14, "end": 4870.06, "text": " As we switch over to the live questions, let me, I can't help but ask you this question. It comes", "tokens": [50868, 1018, 321, 3679, 670, 281, 264, 1621, 1651, 11, 718, 385, 11, 286, 393, 380, 854, 457, 1029, 291, 341, 1168, 13, 467, 1487, 51164], "temperature": 0.0, "avg_logprob": -0.09986769122841918, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0005871441680938005}, {"id": 1121, "seek": 485406, "start": 4870.06, "end": 4879.5, "text": " from several anonymous people as well as Ken Church, your former colleague. Did you actually say", "tokens": [51164, 490, 2940, 24932, 561, 382, 731, 382, 8273, 7882, 11, 428, 5819, 13532, 13, 2589, 291, 767, 584, 51636], "temperature": 0.0, "avg_logprob": -0.09986769122841918, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0005871441680938005}, {"id": 1122, "seek": 487950, "start": 4880.14, "end": 4885.9, "text": " the revolution will not be supervised? I did, yeah. Okay. But actually, I stole it", "tokens": [50396, 264, 8894, 486, 406, 312, 46533, 30, 286, 630, 11, 1338, 13, 1033, 13, 583, 767, 11, 286, 16326, 309, 50684], "temperature": 0.0, "avg_logprob": -0.2588152604944566, "compression_ratio": 1.4171122994652405, "no_speech_prob": 0.0012952948454767466}, {"id": 1123, "seek": 487950, "start": 4887.02, "end": 4896.94, "text": " from Adi Asha Efros from Berkeley. He had a magnificent slide that was a picture of a wall", "tokens": [50740, 490, 1999, 72, 1018, 1641, 31840, 2635, 490, 23684, 13, 634, 632, 257, 23690, 4137, 300, 390, 257, 3036, 295, 257, 2929, 51236], "temperature": 0.0, "avg_logprob": -0.2588152604944566, "compression_ratio": 1.4171122994652405, "no_speech_prob": 0.0012952948454767466}, {"id": 1124, "seek": 487950, "start": 4896.94, "end": 4903.82, "text": " painting in Chile someplace, which was one of those kind of revolutionary thing. And I took", "tokens": [51236, 5370, 294, 22238, 37126, 11, 597, 390, 472, 295, 729, 733, 295, 22687, 551, 13, 400, 286, 1890, 51580], "temperature": 0.0, "avg_logprob": -0.2588152604944566, "compression_ratio": 1.4171122994652405, "no_speech_prob": 0.0012952948454767466}, {"id": 1125, "seek": 490382, "start": 4903.82, "end": 4911.5, "text": " that picture and overlaid on it. The revolution will not be supervised. Yes. Okay. So I stole", "tokens": [50364, 300, 3036, 293, 670, 875, 327, 322, 309, 13, 440, 8894, 486, 406, 312, 46533, 13, 1079, 13, 1033, 13, 407, 286, 16326, 50748], "temperature": 0.0, "avg_logprob": -0.1418969731935313, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0007644803845323622}, {"id": 1126, "seek": 490382, "start": 4911.5, "end": 4917.9, "text": " that from him. I deserve no credit. Shall we switch over to a question from the audience?", "tokens": [50748, 300, 490, 796, 13, 286, 9948, 572, 5397, 13, 12128, 321, 3679, 670, 281, 257, 1168, 490, 264, 4034, 30, 51068], "temperature": 0.0, "avg_logprob": -0.1418969731935313, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0007644803845323622}, {"id": 1127, "seek": 490382, "start": 4917.9, "end": 4924.62, "text": " Yeah. So first question from Glenn Jenkinson is, what two questions about AI do you wish you", "tokens": [51068, 865, 13, 407, 700, 1168, 490, 30119, 41273, 266, 307, 11, 437, 732, 1651, 466, 7318, 360, 291, 3172, 291, 51404], "temperature": 0.0, "avg_logprob": -0.1418969731935313, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.0007644803845323622}, {"id": 1128, "seek": 492462, "start": 4924.7, "end": 4933.74, "text": " would ask more often? Two questions. I don't know. I get asked a lot of questions. I can't", "tokens": [50368, 576, 1029, 544, 2049, 30, 4453, 1651, 13, 286, 500, 380, 458, 13, 286, 483, 2351, 257, 688, 295, 1651, 13, 286, 393, 380, 50820], "temperature": 0.0, "avg_logprob": -0.13836912486864172, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0021145204082131386}, {"id": 1129, "seek": 492462, "start": 4933.74, "end": 4941.74, "text": " imagine a question have not been asked. That's relevant. I mean, I think the important questions", "tokens": [50820, 3811, 257, 1168, 362, 406, 668, 2351, 13, 663, 311, 7340, 13, 286, 914, 11, 286, 519, 264, 1021, 1651, 51220], "temperature": 0.0, "avg_logprob": -0.13836912486864172, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0021145204082131386}, {"id": 1130, "seek": 492462, "start": 4941.74, "end": 4946.78, "text": " are the ones that I'm asking myself. And I wish other people would sort of frame the", "tokens": [51220, 366, 264, 2306, 300, 286, 478, 3365, 2059, 13, 400, 286, 3172, 661, 561, 576, 1333, 295, 3920, 264, 51472], "temperature": 0.0, "avg_logprob": -0.13836912486864172, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0021145204082131386}, {"id": 1131, "seek": 492462, "start": 4946.78, "end": 4953.66, "text": " problems in the same way. So big question. How is it that any teenager can learn to", "tokens": [51472, 2740, 294, 264, 912, 636, 13, 407, 955, 1168, 13, 1012, 307, 309, 300, 604, 21440, 393, 1466, 281, 51816], "temperature": 0.0, "avg_logprob": -0.13836912486864172, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.0021145204082131386}, {"id": 1132, "seek": 495366, "start": 4953.66, "end": 4957.42, "text": " drive a car in 20 hours? And we still don't have level five autonomous driving?", "tokens": [50364, 3332, 257, 1032, 294, 945, 2496, 30, 400, 321, 920, 500, 380, 362, 1496, 1732, 23797, 4840, 30, 50552], "temperature": 0.0, "avg_logprob": -0.1457540053355543, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008811670704744756}, {"id": 1133, "seek": 495366, "start": 4958.94, "end": 4961.9, "text": " That was the first question. So second question is, what are we missing?", "tokens": [50628, 663, 390, 264, 700, 1168, 13, 407, 1150, 1168, 307, 11, 437, 366, 321, 5361, 30, 50776], "temperature": 0.0, "avg_logprob": -0.1457540053355543, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008811670704744756}, {"id": 1134, "seek": 495366, "start": 4964.3, "end": 4973.0199999999995, "text": " That's the answer I want. Joe. Next question from Juan Leylanda. Do you think quantum computing", "tokens": [50896, 663, 311, 264, 1867, 286, 528, 13, 6807, 13, 3087, 1168, 490, 17064, 36794, 1661, 64, 13, 1144, 291, 519, 13018, 15866, 51332], "temperature": 0.0, "avg_logprob": -0.1457540053355543, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008811670704744756}, {"id": 1135, "seek": 495366, "start": 4973.0199999999995, "end": 4975.74, "text": " will have a significant role in the future of AI? No.", "tokens": [51332, 486, 362, 257, 4776, 3090, 294, 264, 2027, 295, 7318, 30, 883, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1457540053355543, "compression_ratio": 1.458937198067633, "no_speech_prob": 0.0008811670704744756}, {"id": 1136, "seek": 497574, "start": 4976.54, "end": 4989.099999999999, "text": " Or at least not any time soon. But the time this happens, I probably won't be alive anymore.", "tokens": [50404, 1610, 412, 1935, 406, 604, 565, 2321, 13, 583, 264, 565, 341, 2314, 11, 286, 1391, 1582, 380, 312, 5465, 3602, 13, 51032], "temperature": 0.0, "avg_logprob": -0.19353930609566825, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.002016868209466338}, {"id": 1137, "seek": 497574, "start": 4989.099999999999, "end": 4996.54, "text": " So I'm not taking a big risk. No, I don't think so. I mean, there's precious few", "tokens": [51032, 407, 286, 478, 406, 1940, 257, 955, 3148, 13, 883, 11, 286, 500, 380, 519, 370, 13, 286, 914, 11, 456, 311, 12406, 1326, 51404], "temperature": 0.0, "avg_logprob": -0.19353930609566825, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.002016868209466338}, {"id": 1138, "seek": 497574, "start": 4997.34, "end": 5002.0599999999995, "text": " situations today where quantum computing could be useful. There's no situation where it actually", "tokens": [51444, 6851, 965, 689, 13018, 15866, 727, 312, 4420, 13, 821, 311, 572, 2590, 689, 309, 767, 51680], "temperature": 0.0, "avg_logprob": -0.19353930609566825, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.002016868209466338}, {"id": 1139, "seek": 500206, "start": 5002.06, "end": 5007.9800000000005, "text": " is useful because the quantum computers are not big enough at the moment. So it's a huge bet.", "tokens": [50364, 307, 4420, 570, 264, 13018, 10807, 366, 406, 955, 1547, 412, 264, 1623, 13, 407, 309, 311, 257, 2603, 778, 13, 50660], "temperature": 0.0, "avg_logprob": -0.11001717062557445, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.003067445009946823}, {"id": 1140, "seek": 500206, "start": 5007.9800000000005, "end": 5013.5, "text": " I think scientifically it's fascinating. I'm really fascinated by quantum computing at the", "tokens": [50660, 286, 519, 39719, 309, 311, 10343, 13, 286, 478, 534, 24597, 538, 13018, 15866, 412, 264, 50936], "temperature": 0.0, "avg_logprob": -0.11001717062557445, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.003067445009946823}, {"id": 1141, "seek": 500206, "start": 5013.5, "end": 5020.06, "text": " conceptual level. I have one or two papers with Seth Lloyd on connections between neural", "tokens": [50936, 24106, 1496, 13, 286, 362, 472, 420, 732, 10577, 365, 25353, 31401, 322, 9271, 1296, 18161, 51264], "temperature": 0.0, "avg_logprob": -0.11001717062557445, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.003067445009946823}, {"id": 1142, "seek": 500206, "start": 5020.06, "end": 5026.14, "text": " nets and quantum computing. I think it's a very interesting topic, but I don't think it has any", "tokens": [51264, 36170, 293, 13018, 15866, 13, 286, 519, 309, 311, 257, 588, 1880, 4829, 11, 457, 286, 500, 380, 519, 309, 575, 604, 51568], "temperature": 0.0, "avg_logprob": -0.11001717062557445, "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.003067445009946823}, {"id": 1143, "seek": 502614, "start": 5026.14, "end": 5033.740000000001, "text": " practical value in the short term. Joe. One last question from Anton Dabura.", "tokens": [50364, 8496, 2158, 294, 264, 2099, 1433, 13, 6807, 13, 1485, 1036, 1168, 490, 15291, 413, 455, 2991, 13, 50744], "temperature": 0.0, "avg_logprob": -0.11647374770220588, "compression_ratio": 1.5, "no_speech_prob": 0.001026954036206007}, {"id": 1144, "seek": 502614, "start": 5034.54, "end": 5040.38, "text": " To what extent do you see ML models being used for problems that we already have pretty good", "tokens": [50784, 1407, 437, 8396, 360, 291, 536, 21601, 5245, 885, 1143, 337, 2740, 300, 321, 1217, 362, 1238, 665, 51076], "temperature": 0.0, "avg_logprob": -0.11647374770220588, "compression_ratio": 1.5, "no_speech_prob": 0.001026954036206007}, {"id": 1145, "seek": 502614, "start": 5040.38, "end": 5045.9800000000005, "text": " algorithms to solve, such as sorting shortest path, linear integer programming, and so on?", "tokens": [51076, 14642, 281, 5039, 11, 1270, 382, 32411, 31875, 3100, 11, 8213, 24922, 9410, 11, 293, 370, 322, 30, 51356], "temperature": 0.0, "avg_logprob": -0.11647374770220588, "compression_ratio": 1.5, "no_speech_prob": 0.001026954036206007}, {"id": 1146, "seek": 502614, "start": 5045.9800000000005, "end": 5051.5, "text": " How would you characterize the boundary, if any? So there's a lot of problems that we can currently", "tokens": [51356, 1012, 576, 291, 38463, 264, 12866, 11, 498, 604, 30, 407, 456, 311, 257, 688, 295, 2740, 300, 321, 393, 4362, 51632], "temperature": 0.0, "avg_logprob": -0.11647374770220588, "compression_ratio": 1.5, "no_speech_prob": 0.001026954036206007}, {"id": 1147, "seek": 505150, "start": 5051.5, "end": 5059.1, "text": " solve that are NP-complete or NP-hard, and so we can solve them within limits. What we need very", "tokens": [50364, 5039, 300, 366, 38611, 12, 1112, 17220, 420, 38611, 12, 21491, 11, 293, 370, 321, 393, 5039, 552, 1951, 10406, 13, 708, 321, 643, 588, 50744], "temperature": 0.0, "avg_logprob": -0.11179334587521023, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001524035818874836}, {"id": 1148, "seek": 505150, "start": 5059.1, "end": 5067.58, "text": " often are approximate algorithms, so methods that give us approximate solutions to complex problems", "tokens": [50744, 2049, 366, 30874, 14642, 11, 370, 7150, 300, 976, 505, 30874, 6547, 281, 3997, 2740, 51168], "temperature": 0.0, "avg_logprob": -0.11179334587521023, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001524035818874836}, {"id": 1149, "seek": 505150, "start": 5067.58, "end": 5077.42, "text": " that, in theory, are NP-hard, NP-complete, whatever, but if you reduce yourself to accepting", "tokens": [51168, 300, 11, 294, 5261, 11, 366, 38611, 12, 21491, 11, 38611, 12, 1112, 17220, 11, 2035, 11, 457, 498, 291, 5407, 1803, 281, 17391, 51660], "temperature": 0.0, "avg_logprob": -0.11179334587521023, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.001524035818874836}, {"id": 1150, "seek": 507742, "start": 5077.42, "end": 5086.38, "text": " approximate solutions, might become solvable. So I think there is a lot to be said for ML methods", "tokens": [50364, 30874, 6547, 11, 1062, 1813, 1404, 17915, 13, 407, 286, 519, 456, 307, 257, 688, 281, 312, 848, 337, 21601, 7150, 50812], "temperature": 0.0, "avg_logprob": -0.1154642105102539, "compression_ratio": 1.812206572769953, "no_speech_prob": 0.0008962580468505621}, {"id": 1151, "seek": 507742, "start": 5086.38, "end": 5093.1, "text": " that do something that has become to be known as amortized inference. So amortized inference is", "tokens": [50812, 300, 360, 746, 300, 575, 1813, 281, 312, 2570, 382, 669, 477, 1602, 38253, 13, 407, 669, 477, 1602, 38253, 307, 51148], "temperature": 0.0, "avg_logprob": -0.1154642105102539, "compression_ratio": 1.812206572769953, "no_speech_prob": 0.0008962580468505621}, {"id": 1152, "seek": 507742, "start": 5093.1, "end": 5097.66, "text": " this idea that you might have a problem that is formulated as an optimization problem. Every", "tokens": [51148, 341, 1558, 300, 291, 1062, 362, 257, 1154, 300, 307, 48936, 382, 364, 19618, 1154, 13, 2048, 51376], "temperature": 0.0, "avg_logprob": -0.1154642105102539, "compression_ratio": 1.812206572769953, "no_speech_prob": 0.0008962580468505621}, {"id": 1153, "seek": 507742, "start": 5097.66, "end": 5104.54, "text": " computing problem can be formulated as an optimization problem. And what you might be able to do is", "tokens": [51376, 15866, 1154, 393, 312, 48936, 382, 364, 19618, 1154, 13, 400, 437, 291, 1062, 312, 1075, 281, 360, 307, 51720], "temperature": 0.0, "avg_logprob": -0.1154642105102539, "compression_ratio": 1.812206572769953, "no_speech_prob": 0.0008962580468505621}, {"id": 1154, "seek": 510454, "start": 5104.62, "end": 5109.9, "text": " solve that problem in certain cases, give a solution, and now what you do with this is that you", "tokens": [50368, 5039, 300, 1154, 294, 1629, 3331, 11, 976, 257, 3827, 11, 293, 586, 437, 291, 360, 365, 341, 307, 300, 291, 50632], "temperature": 0.0, "avg_logprob": -0.11837431300770153, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0010814124252647161}, {"id": 1155, "seek": 510454, "start": 5109.9, "end": 5115.82, "text": " train in your net of some kind to predict, to approximate the solution to that optimization", "tokens": [50632, 3847, 294, 428, 2533, 295, 512, 733, 281, 6069, 11, 281, 30874, 264, 3827, 281, 300, 19618, 50928], "temperature": 0.0, "avg_logprob": -0.11837431300770153, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0010814124252647161}, {"id": 1156, "seek": 510454, "start": 5115.82, "end": 5121.82, "text": " problem from the specification of the problem, from the inputs. So that system will not be able to", "tokens": [50928, 1154, 490, 264, 31256, 295, 264, 1154, 11, 490, 264, 15743, 13, 407, 300, 1185, 486, 406, 312, 1075, 281, 51228], "temperature": 0.0, "avg_logprob": -0.11837431300770153, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0010814124252647161}, {"id": 1157, "seek": 510454, "start": 5122.46, "end": 5126.54, "text": " completely solve the problem in those situations, but for the type of problem that you train it on,", "tokens": [51260, 2584, 5039, 264, 1154, 294, 729, 6851, 11, 457, 337, 264, 2010, 295, 1154, 300, 291, 3847, 309, 322, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11837431300770153, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0010814124252647161}, {"id": 1158, "seek": 510454, "start": 5127.34, "end": 5131.42, "text": " it's going to be able to give you an approximate solution really quickly. Amortized inference.", "tokens": [51504, 309, 311, 516, 281, 312, 1075, 281, 976, 291, 364, 30874, 3827, 534, 2661, 13, 2012, 477, 1602, 38253, 13, 51708], "temperature": 0.0, "avg_logprob": -0.11837431300770153, "compression_ratio": 1.842911877394636, "no_speech_prob": 0.0010814124252647161}, {"id": 1159, "seek": 513142, "start": 5131.42, "end": 5140.86, "text": " There is a tutorial on this that was written and given at a recent conference by one of my", "tokens": [50364, 821, 307, 257, 7073, 322, 341, 300, 390, 3720, 293, 2212, 412, 257, 5162, 7586, 538, 472, 295, 452, 50836], "temperature": 0.0, "avg_logprob": -0.14074292676202182, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0004874107544310391}, {"id": 1160, "seek": 513142, "start": 5140.86, "end": 5146.62, "text": " colleagues at fair called Brendan Amos, AMOS, very interesting concept.", "tokens": [50836, 7734, 412, 3143, 1219, 48484, 2012, 329, 11, 6475, 4367, 11, 588, 1880, 3410, 13, 51124], "temperature": 0.0, "avg_logprob": -0.14074292676202182, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0004874107544310391}, {"id": 1161, "seek": 513142, "start": 5148.46, "end": 5153.34, "text": " I will close my questions with one last question, then we'll take a real live one and call it the", "tokens": [51216, 286, 486, 1998, 452, 1651, 365, 472, 1036, 1168, 11, 550, 321, 603, 747, 257, 957, 1621, 472, 293, 818, 309, 264, 51460], "temperature": 0.0, "avg_logprob": -0.14074292676202182, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0004874107544310391}, {"id": 1162, "seek": 513142, "start": 5153.34, "end": 5159.1, "text": " end. I have to use this. It comes from one of our faculty who wanted to remain anonymous,", "tokens": [51460, 917, 13, 286, 362, 281, 764, 341, 13, 467, 1487, 490, 472, 295, 527, 6389, 567, 1415, 281, 6222, 24932, 11, 51748], "temperature": 0.0, "avg_logprob": -0.14074292676202182, "compression_ratio": 1.5350877192982457, "no_speech_prob": 0.0004874107544310391}, {"id": 1163, "seek": 515910, "start": 5159.1, "end": 5164.22, "text": " I don't know why, but given the big excitement around LLMs and not without a reason,", "tokens": [50364, 286, 500, 380, 458, 983, 11, 457, 2212, 264, 955, 14755, 926, 441, 43, 26386, 293, 406, 1553, 257, 1778, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09429404793716059, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.000814545841421932}, {"id": 1164, "seek": 515910, "start": 5165.02, "end": 5169.660000000001, "text": " what are some of the research directions that are possible to tackle for non-Google slash", "tokens": [50660, 437, 366, 512, 295, 264, 2132, 11095, 300, 366, 1944, 281, 14896, 337, 2107, 12, 12104, 3127, 17330, 50892], "temperature": 0.0, "avg_logprob": -0.09429404793716059, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.000814545841421932}, {"id": 1165, "seek": 515910, "start": 5169.660000000001, "end": 5177.18, "text": " Facebook type sized institutions that are under studies? Space for foundational research,", "tokens": [50892, 4384, 2010, 20004, 8142, 300, 366, 833, 5313, 30, 8705, 337, 32195, 2132, 11, 51268], "temperature": 0.0, "avg_logprob": -0.09429404793716059, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.000814545841421932}, {"id": 1166, "seek": 515910, "start": 5177.18, "end": 5183.58, "text": " big open questions in need of creative solutions. Thus, if you were a young investigator today,", "tokens": [51268, 955, 1269, 1651, 294, 643, 295, 5880, 6547, 13, 13827, 11, 498, 291, 645, 257, 2037, 38330, 965, 11, 51588], "temperature": 0.0, "avg_logprob": -0.09429404793716059, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.000814545841421932}, {"id": 1167, "seek": 518358, "start": 5183.58, "end": 5189.18, "text": " like a starting assistant professor, what would you do in this environment?", "tokens": [50364, 411, 257, 2891, 10994, 8304, 11, 437, 576, 291, 360, 294, 341, 2823, 30, 50644], "temperature": 0.0, "avg_logprob": -0.15465633685772234, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.001698168576695025}, {"id": 1168, "seek": 518358, "start": 5189.18, "end": 5193.98, "text": " I mean, that's a problem I have to face when I have PhD students at NYU that don't have access to", "tokens": [50644, 286, 914, 11, 300, 311, 257, 1154, 286, 362, 281, 1851, 562, 286, 362, 14476, 1731, 412, 42682, 300, 500, 380, 362, 2105, 281, 50884], "temperature": 0.0, "avg_logprob": -0.15465633685772234, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.001698168576695025}, {"id": 1169, "seek": 518358, "start": 5194.94, "end": 5205.1, "text": " 16,000 GPUs, unlike people at fair. So I think a lot of most good ideas still come from academia,", "tokens": [50932, 3165, 11, 1360, 18407, 82, 11, 8343, 561, 412, 3143, 13, 407, 286, 519, 257, 688, 295, 881, 665, 3487, 920, 808, 490, 28937, 11, 51440], "temperature": 0.0, "avg_logprob": -0.15465633685772234, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.001698168576695025}, {"id": 1170, "seek": 518358, "start": 5205.1, "end": 5213.1, "text": " so you're not going to be Google or Meta or Microsoft on beating the record on", "tokens": [51440, 370, 291, 434, 406, 516, 281, 312, 3329, 420, 6377, 64, 420, 8116, 322, 13497, 264, 2136, 322, 51840], "temperature": 0.0, "avg_logprob": -0.15465633685772234, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.001698168576695025}, {"id": 1171, "seek": 521310, "start": 5213.1, "end": 5216.700000000001, "text": " translation or something like that. You don't want to do this in universities.", "tokens": [50364, 12853, 420, 746, 411, 300, 13, 509, 500, 380, 528, 281, 360, 341, 294, 11779, 13, 50544], "temperature": 0.0, "avg_logprob": -0.09673113141741072, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.0009388769394718111}, {"id": 1172, "seek": 521310, "start": 5217.820000000001, "end": 5222.700000000001, "text": " But coming up with new ideas, for example, the problem I mentioned of how do you do", "tokens": [50600, 583, 1348, 493, 365, 777, 3487, 11, 337, 1365, 11, 264, 1154, 286, 2835, 295, 577, 360, 291, 360, 50844], "temperature": 0.0, "avg_logprob": -0.09673113141741072, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.0009388769394718111}, {"id": 1173, "seek": 521310, "start": 5222.700000000001, "end": 5229.42, "text": " hierarchical planning? How do you train a system to figure out how to represent the world and", "tokens": [50844, 35250, 804, 5038, 30, 1012, 360, 291, 3847, 257, 1185, 281, 2573, 484, 577, 281, 2906, 264, 1002, 293, 51180], "temperature": 0.0, "avg_logprob": -0.09673113141741072, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.0009388769394718111}, {"id": 1174, "seek": 521310, "start": 5229.42, "end": 5233.34, "text": " action spaces so that you can do hierarchical planning? It's completely unsolved. You can do", "tokens": [51180, 3069, 7673, 370, 300, 291, 393, 360, 35250, 804, 5038, 30, 467, 311, 2584, 2693, 29110, 13, 509, 393, 360, 51376], "temperature": 0.0, "avg_logprob": -0.09673113141741072, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.0009388769394718111}, {"id": 1175, "seek": 521310, "start": 5233.34, "end": 5238.620000000001, "text": " this with toy problems. If you have any idea of how you might approach that problem on toy", "tokens": [51376, 341, 365, 12058, 2740, 13, 759, 291, 362, 604, 1558, 295, 577, 291, 1062, 3109, 300, 1154, 322, 12058, 51640], "temperature": 0.0, "avg_logprob": -0.09673113141741072, "compression_ratio": 1.7054263565891472, "no_speech_prob": 0.0009388769394718111}, {"id": 1176, "seek": 523862, "start": 5238.62, "end": 5244.94, "text": " problems, you don't have to have tons of GPUs for that. You will have an idea that might have a", "tokens": [50364, 2740, 11, 291, 500, 380, 362, 281, 362, 9131, 295, 18407, 82, 337, 300, 13, 509, 486, 362, 364, 1558, 300, 1062, 362, 257, 50680], "temperature": 0.0, "avg_logprob": -0.12303192416826884, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.004132633563131094}, {"id": 1177, "seek": 523862, "start": 5244.94, "end": 5252.86, "text": " huge impact. So if you have a good architecture that you can show, can learn some simple world", "tokens": [50680, 2603, 2712, 13, 407, 498, 291, 362, 257, 665, 9482, 300, 291, 393, 855, 11, 393, 1466, 512, 2199, 1002, 51076], "temperature": 0.0, "avg_logprob": -0.12303192416826884, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.004132633563131094}, {"id": 1178, "seek": 523862, "start": 5252.86, "end": 5257.74, "text": " model from video, it's the same. You don't have to train on all of YouTube. You can train on", "tokens": [51076, 2316, 490, 960, 11, 309, 311, 264, 912, 13, 509, 500, 380, 362, 281, 3847, 322, 439, 295, 3088, 13, 509, 393, 3847, 322, 51320], "temperature": 0.0, "avg_logprob": -0.12303192416826884, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.004132633563131094}, {"id": 1179, "seek": 523862, "start": 5257.74, "end": 5263.82, "text": " artificial environments and stuff like that and demonstrate that it works. It doesn't have to be", "tokens": [51320, 11677, 12388, 293, 1507, 411, 300, 293, 11698, 300, 309, 1985, 13, 467, 1177, 380, 362, 281, 312, 51624], "temperature": 0.0, "avg_logprob": -0.12303192416826884, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.004132633563131094}, {"id": 1180, "seek": 526382, "start": 5264.0599999999995, "end": 5268.54, "text": " large scale. So this is the kind of stuff you want to do. And then there is a new domain which is", "tokens": [50376, 2416, 4373, 13, 407, 341, 307, 264, 733, 295, 1507, 291, 528, 281, 360, 13, 400, 550, 456, 307, 257, 777, 9274, 597, 307, 50600], "temperature": 0.0, "avg_logprob": -0.1662970733642578, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.007103427778929472}, {"id": 1181, "seek": 526382, "start": 5268.54, "end": 5274.46, "text": " building on top of open source base models. So unfortunately, right now, the best base models,", "tokens": [50600, 2390, 322, 1192, 295, 1269, 4009, 3096, 5245, 13, 407, 7015, 11, 558, 586, 11, 264, 1151, 3096, 5245, 11, 50896], "temperature": 0.0, "avg_logprob": -0.1662970733642578, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.007103427778929472}, {"id": 1182, "seek": 526382, "start": 5275.98, "end": 5283.0199999999995, "text": " LLMs are the LAMA class of models from $7 billion to $65 billion. They're not usable for commercial", "tokens": [50972, 441, 43, 26386, 366, 264, 441, 38136, 1508, 295, 5245, 490, 1848, 22, 5218, 281, 1848, 16824, 5218, 13, 814, 434, 406, 29975, 337, 6841, 51324], "temperature": 0.0, "avg_logprob": -0.1662970733642578, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.007103427778929472}, {"id": 1183, "seek": 526382, "start": 5283.0199999999995, "end": 5288.139999999999, "text": " use. They are distributed with a license for non-commercial use, only for research, which you", "tokens": [51324, 764, 13, 814, 366, 12631, 365, 257, 10476, 337, 2107, 12, 1112, 39260, 764, 11, 787, 337, 2132, 11, 597, 291, 51580], "temperature": 0.0, "avg_logprob": -0.1662970733642578, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.007103427778929472}, {"id": 1184, "seek": 526382, "start": 5288.139999999999, "end": 5293.42, "text": " can of course use in the university. And there's a lot of work to be done to figure out how to", "tokens": [51580, 393, 295, 1164, 764, 294, 264, 5454, 13, 400, 456, 311, 257, 688, 295, 589, 281, 312, 1096, 281, 2573, 484, 577, 281, 51844], "temperature": 0.0, "avg_logprob": -0.1662970733642578, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.007103427778929472}, {"id": 1185, "seek": 529342, "start": 5293.5, "end": 5298.62, "text": " make those things safe, factual, etc. And you can work from those base models. You don't have to", "tokens": [50368, 652, 729, 721, 3273, 11, 48029, 11, 5183, 13, 400, 291, 393, 589, 490, 729, 3096, 5245, 13, 509, 500, 380, 362, 281, 50624], "temperature": 0.0, "avg_logprob": -0.13801739358494425, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.00019301398424431682}, {"id": 1186, "seek": 529342, "start": 5298.62, "end": 5304.7, "text": " retrain them from scratch. So you don't need to have roomful, rooms full of GPUs.", "tokens": [50624, 1533, 7146, 552, 490, 8459, 13, 407, 291, 500, 380, 643, 281, 362, 1808, 906, 11, 9396, 1577, 295, 18407, 82, 13, 50928], "temperature": 0.0, "avg_logprob": -0.13801739358494425, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.00019301398424431682}, {"id": 1187, "seek": 529342, "start": 5305.58, "end": 5308.86, "text": " We'll try for one last question, maybe two. Go ahead, please, with your question.", "tokens": [50972, 492, 603, 853, 337, 472, 1036, 1168, 11, 1310, 732, 13, 1037, 2286, 11, 1767, 11, 365, 428, 1168, 13, 51136], "temperature": 0.0, "avg_logprob": -0.13801739358494425, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.00019301398424431682}, {"id": 1188, "seek": 529342, "start": 5309.82, "end": 5315.5, "text": " Hi. So my question really dwells from the side of, or we'd love to hear your thoughts,", "tokens": [51184, 2421, 13, 407, 452, 1168, 534, 24355, 82, 490, 264, 1252, 295, 11, 420, 321, 1116, 959, 281, 1568, 428, 4598, 11, 51468], "temperature": 0.0, "avg_logprob": -0.13801739358494425, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.00019301398424431682}, {"id": 1189, "seek": 529342, "start": 5315.5, "end": 5321.42, "text": " on impact and control of these large language models or any of these models, the fancy models", "tokens": [51468, 322, 2712, 293, 1969, 295, 613, 2416, 2856, 5245, 420, 604, 295, 613, 5245, 11, 264, 10247, 5245, 51764], "temperature": 0.0, "avg_logprob": -0.13801739358494425, "compression_ratio": 1.6704545454545454, "no_speech_prob": 0.00019301398424431682}, {"id": 1190, "seek": 532142, "start": 5321.42, "end": 5326.62, "text": " that you showed with billions of trillions of parameters. So the impact side is, do you really", "tokens": [50364, 300, 291, 4712, 365, 17375, 295, 504, 46279, 295, 9834, 13, 407, 264, 2712, 1252, 307, 11, 360, 291, 534, 50624], "temperature": 0.0, "avg_logprob": -0.13826807509077357, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0004476680187508464}, {"id": 1191, "seek": 532142, "start": 5326.62, "end": 5333.02, "text": " give or how much thought do you give to the impact that would have on the community or on the people", "tokens": [50624, 976, 420, 577, 709, 1194, 360, 291, 976, 281, 264, 2712, 300, 576, 362, 322, 264, 1768, 420, 322, 264, 561, 50944], "temperature": 0.0, "avg_logprob": -0.13826807509077357, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0004476680187508464}, {"id": 1192, "seek": 532142, "start": 5333.02, "end": 5340.62, "text": " in general, based on what that model does? And control is, once that model is out there,", "tokens": [50944, 294, 2674, 11, 2361, 322, 437, 300, 2316, 775, 30, 400, 1969, 307, 11, 1564, 300, 2316, 307, 484, 456, 11, 51324], "temperature": 0.0, "avg_logprob": -0.13826807509077357, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0004476680187508464}, {"id": 1193, "seek": 532142, "start": 5343.34, "end": 5349.26, "text": " how do I make sure that it doesn't do a certain things it's not supposed to do with regular,", "tokens": [51460, 577, 360, 286, 652, 988, 300, 309, 1177, 380, 360, 257, 1629, 721, 309, 311, 406, 3442, 281, 360, 365, 3890, 11, 51756], "temperature": 0.0, "avg_logprob": -0.13826807509077357, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0004476680187508464}, {"id": 1194, "seek": 534926, "start": 5350.22, "end": 5354.62, "text": " the way people used to use internet before those models. It used to be very controlled", "tokens": [50412, 264, 636, 561, 1143, 281, 764, 4705, 949, 729, 5245, 13, 467, 1143, 281, 312, 588, 10164, 50632], "temperature": 0.0, "avg_logprob": -0.14608957101632883, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0005187259521335363}, {"id": 1195, "seek": 534926, "start": 5354.62, "end": 5360.46, "text": " environment where you could have, in a way, regulate those environments. But now with models,", "tokens": [50632, 2823, 689, 291, 727, 362, 11, 294, 257, 636, 11, 24475, 729, 12388, 13, 583, 586, 365, 5245, 11, 50924], "temperature": 0.0, "avg_logprob": -0.14608957101632883, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0005187259521335363}, {"id": 1196, "seek": 534926, "start": 5360.46, "end": 5366.38, "text": " it's getting increasingly difficult and a slow process to have or do not have certain things in", "tokens": [50924, 309, 311, 1242, 12980, 2252, 293, 257, 2964, 1399, 281, 362, 420, 360, 406, 362, 1629, 721, 294, 51220], "temperature": 0.0, "avg_logprob": -0.14608957101632883, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0005187259521335363}, {"id": 1197, "seek": 534926, "start": 5366.38, "end": 5372.54, "text": " those models. Okay. So there is a long view, a very positive one, which is imagine that all of us", "tokens": [51220, 729, 5245, 13, 1033, 13, 407, 456, 307, 257, 938, 1910, 11, 257, 588, 3353, 472, 11, 597, 307, 3811, 300, 439, 295, 505, 51528], "temperature": 0.0, "avg_logprob": -0.14608957101632883, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0005187259521335363}, {"id": 1198, "seek": 534926, "start": 5372.54, "end": 5378.06, "text": " have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff", "tokens": [51528, 362, 729, 34949, 365, 1687, 18796, 7599, 13, 407, 309, 311, 411, 633, 472, 295, 505, 575, 733, 295, 257, 3525, 51804], "temperature": 0.0, "avg_logprob": -0.14608957101632883, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0005187259521335363}, {"id": 1199, "seek": 537806, "start": 5378.14, "end": 5385.42, "text": " of people working for us, but like super people working for us. This is going to create a new", "tokens": [50368, 295, 561, 1364, 337, 505, 11, 457, 411, 1687, 561, 1364, 337, 505, 13, 639, 307, 516, 281, 1884, 257, 777, 50732], "temperature": 0.0, "avg_logprob": -0.129524241672473, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0003003103192895651}, {"id": 1200, "seek": 537806, "start": 5386.38, "end": 5394.860000000001, "text": " renaissance for humanity. It's going to increase humanity's intelligence, however you want to", "tokens": [50780, 319, 629, 14431, 337, 10243, 13, 467, 311, 516, 281, 3488, 10243, 311, 7599, 11, 4461, 291, 528, 281, 51204], "temperature": 0.0, "avg_logprob": -0.129524241672473, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0003003103192895651}, {"id": 1201, "seek": 537806, "start": 5394.860000000001, "end": 5403.26, "text": " measure it. That has to be intrinsically good. It's been the case in the past that anytime a new", "tokens": [51204, 3481, 309, 13, 663, 575, 281, 312, 28621, 984, 665, 13, 467, 311, 668, 264, 1389, 294, 264, 1791, 300, 13038, 257, 777, 51624], "temperature": 0.0, "avg_logprob": -0.129524241672473, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0003003103192895651}, {"id": 1202, "seek": 537806, "start": 5403.820000000001, "end": 5407.5, "text": " medium was invented or a new way of communication was invented, like the printing press.", "tokens": [51652, 6399, 390, 14479, 420, 257, 777, 636, 295, 6101, 390, 14479, 11, 411, 264, 14699, 1886, 13, 51836], "temperature": 0.0, "avg_logprob": -0.129524241672473, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.0003003103192895651}, {"id": 1203, "seek": 540806, "start": 5408.9400000000005, "end": 5412.780000000001, "text": " Humanity kind of went to the next step. The printing press let", "tokens": [50408, 10294, 507, 733, 295, 1437, 281, 264, 958, 1823, 13, 440, 14699, 1886, 718, 50600], "temperature": 0.0, "avg_logprob": -0.18289789102845272, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.00048669788520783186}, {"id": 1204, "seek": 540806, "start": 5415.18, "end": 5422.780000000001, "text": " the dissemination of philosophy, science, secularism, democracy, all that stuff. The US", "tokens": [50720, 264, 34585, 399, 295, 10675, 11, 3497, 11, 25734, 1434, 11, 10528, 11, 439, 300, 1507, 13, 440, 2546, 51100], "temperature": 0.0, "avg_logprob": -0.18289789102845272, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.00048669788520783186}, {"id": 1205, "seek": 540806, "start": 5422.780000000001, "end": 5434.620000000001, "text": " would not exist without the French philosophers of the 18th century. So neither would the French", "tokens": [51100, 576, 406, 2514, 1553, 264, 5522, 36839, 295, 264, 2443, 392, 4901, 13, 407, 9662, 576, 264, 5522, 51692], "temperature": 0.0, "avg_logprob": -0.18289789102845272, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.00048669788520783186}, {"id": 1206, "seek": 543462, "start": 5434.62, "end": 5443.18, "text": " revolution. So I think same for the internet, that gave people instant access to an enormous", "tokens": [50364, 8894, 13, 407, 286, 519, 912, 337, 264, 4705, 11, 300, 2729, 561, 9836, 2105, 281, 364, 11322, 50792], "temperature": 0.0, "avg_logprob": -0.17769619588101848, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01298783253878355}, {"id": 1207, "seek": 543462, "start": 5443.18, "end": 5450.0599999999995, "text": " wealth of knowledge. Also disinformation, but okay, I mean, we have to have countermeasures for", "tokens": [50792, 7203, 295, 3601, 13, 2743, 717, 20941, 11, 457, 1392, 11, 286, 914, 11, 321, 362, 281, 362, 5682, 1398, 20044, 337, 51136], "temperature": 0.0, "avg_logprob": -0.17769619588101848, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01298783253878355}, {"id": 1208, "seek": 543462, "start": 5450.0599999999995, "end": 5455.18, "text": " every technology can be used for good and bad. We need to have countermeasures for the worst", "tokens": [51136, 633, 2899, 393, 312, 1143, 337, 665, 293, 1578, 13, 492, 643, 281, 362, 5682, 1398, 20044, 337, 264, 5855, 51392], "temperature": 0.0, "avg_logprob": -0.17769619588101848, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01298783253878355}, {"id": 1209, "seek": 543462, "start": 5455.74, "end": 5464.46, "text": " aspects. But ultimately, I think we need widest possible access to those AI systems by everyone.", "tokens": [51420, 7270, 13, 583, 6284, 11, 286, 519, 321, 643, 5274, 377, 1944, 2105, 281, 729, 7318, 3652, 538, 1518, 13, 51856], "temperature": 0.0, "avg_logprob": -0.17769619588101848, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.01298783253878355}, {"id": 1210, "seek": 546462, "start": 5465.5, "end": 5472.46, "text": " Now, how do we make sure those systems don't lie to us? How do we make sure that the information", "tokens": [50408, 823, 11, 577, 360, 321, 652, 988, 729, 3652, 500, 380, 4544, 281, 505, 30, 1012, 360, 321, 652, 988, 300, 264, 1589, 50756], "temperature": 0.0, "avg_logprob": -0.10639127095540364, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00023357920872513205}, {"id": 1211, "seek": 546462, "start": 5472.46, "end": 5477.18, "text": " they give us is not under the control of someone that has nefarious purpose, you know, things like", "tokens": [50756, 436, 976, 505, 307, 406, 833, 264, 1969, 295, 1580, 300, 575, 408, 21196, 851, 4334, 11, 291, 458, 11, 721, 411, 50992], "temperature": 0.0, "avg_logprob": -0.10639127095540364, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00023357920872513205}, {"id": 1212, "seek": 546462, "start": 5477.18, "end": 5483.34, "text": " that, which is I think a good reason for them to be open as I stated earlier. But I think it's a", "tokens": [50992, 300, 11, 597, 307, 286, 519, 257, 665, 1778, 337, 552, 281, 312, 1269, 382, 286, 11323, 3071, 13, 583, 286, 519, 309, 311, 257, 51300], "temperature": 0.0, "avg_logprob": -0.10639127095540364, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00023357920872513205}, {"id": 1213, "seek": 546462, "start": 5483.34, "end": 5489.66, "text": " bright future for humanity, you know, contrary to some people who tell young people don't expect", "tokens": [51300, 4730, 2027, 337, 10243, 11, 291, 458, 11, 19506, 281, 512, 561, 567, 980, 2037, 561, 500, 380, 2066, 51616], "temperature": 0.0, "avg_logprob": -0.10639127095540364, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.00023357920872513205}, {"id": 1214, "seek": 548966, "start": 5489.66, "end": 5497.82, "text": " to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the", "tokens": [50364, 281, 1621, 938, 11, 597, 307, 10483, 13, 286, 519, 309, 311, 257, 588, 4730, 2027, 13, 286, 458, 291, 600, 668, 3806, 337, 264, 50772], "temperature": 0.0, "avg_logprob": -0.09679117450466404, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.00399484857916832}, {"id": 1215, "seek": 548966, "start": 5497.82, "end": 5505.26, "text": " next question, but we are five minutes over our time limit. And I know we have to grab a bite and", "tokens": [50772, 958, 1168, 11, 457, 321, 366, 1732, 2077, 670, 527, 565, 4948, 13, 400, 286, 458, 321, 362, 281, 4444, 257, 7988, 293, 51144], "temperature": 0.0, "avg_logprob": -0.09679117450466404, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.00399484857916832}, {"id": 1216, "seek": 548966, "start": 5505.26, "end": 5511.82, "text": " deliver you to the train station on time, according to the hierarchical plan. So with that, please", "tokens": [51144, 4239, 291, 281, 264, 3847, 5214, 322, 565, 11, 4650, 281, 264, 35250, 804, 1393, 13, 407, 365, 300, 11, 1767, 51472], "temperature": 0.0, "avg_logprob": -0.09679117450466404, "compression_ratio": 1.4898989898989898, "no_speech_prob": 0.00399484857916832}, {"id": 1217, "seek": 551182, "start": 5511.82, "end": 5523.98, "text": " join me in thanking Jan for an amazing session today. Thank you.", "tokens": [50364, 3917, 385, 294, 30830, 4956, 337, 364, 2243, 5481, 965, 13, 1044, 291, 13, 50972], "temperature": 0.0, "avg_logprob": -0.3498198845807244, "compression_ratio": 1.0, "no_speech_prob": 0.05820920690894127}], "language": "en"}