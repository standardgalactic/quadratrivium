1
00:00:00,000 --> 00:00:15,960
Okay, great, welcome everyone and thank you for joining us.

2
00:00:15,960 --> 00:00:22,080
This is the last distinguished lecture series for the Institute of Expansion AI for the

3
00:00:22,080 --> 00:00:23,080
academic year.

4
00:00:23,080 --> 00:00:27,360
We'll resume again in September with a full program for the year.

5
00:00:27,360 --> 00:00:35,280
As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series,

6
00:00:35,280 --> 00:00:41,520
which is designed to feature a lot of our Northeastern University experts and faculty

7
00:00:41,520 --> 00:00:43,560
and so forth.

8
00:00:43,560 --> 00:00:50,920
In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic

9
00:00:50,920 --> 00:00:52,600
who's at the Curie College.

10
00:00:52,600 --> 00:00:56,960
My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential

11
00:00:56,960 --> 00:01:02,880
AI and also Professor of the Practice in the Curie College for Computer Sciences and it

12
00:01:02,880 --> 00:01:08,880
is my pleasure today to introduce Yan Lokhan.

13
00:01:08,880 --> 00:01:13,480
Yan is a very well-known name in the field.

14
00:01:13,480 --> 00:01:20,000
I've known him for many years, I think at one point in my life I interviewed at Bell

15
00:01:20,000 --> 00:01:24,880
Labs or AT&T Labs and that's when he was there.

16
00:01:24,880 --> 00:01:32,760
He is VP and Chief AI Scientist at Meta, also known as Facebook, and Silver Professor at

17
00:01:32,760 --> 00:01:38,040
NYU affiliated with the Kauan Institute of Mathematical Sciences and the Center for Data

18
00:01:38,040 --> 00:01:41,880
Science, which he actually founded.

19
00:01:41,880 --> 00:01:48,640
He was the founding director of FAIR, I learned this morning that FAIR used to stand for Facebook

20
00:01:48,640 --> 00:01:57,520
AI Research, now it's changed to MetaFair for Fundamental AI Research and of course

21
00:01:57,520 --> 00:02:04,080
he founded the NYU Center for Data Science, received an engineering diploma from SEA in

22
00:02:04,080 --> 00:02:09,480
Paris and a PhD from the Saoubon University.

23
00:02:09,480 --> 00:02:18,560
After a postdoc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs

24
00:02:18,560 --> 00:02:23,520
in 1996 as Head of Image Processing Research.

25
00:02:23,520 --> 00:02:31,320
He joined NYU as Professor in 2003 and Meta or Facebook in 2013.

26
00:02:31,320 --> 00:02:38,000
He is the recipient of the 2018 ACM Touring Award, along with Jeffrey Hinton and Yashua

27
00:02:38,000 --> 00:02:42,480
Benjio, and for those of you who don't know the Touring Award, it's essentially the equivalent

28
00:02:42,480 --> 00:02:49,520
of the Nobel Prize for Computer Science, the toughest award to get from the ACM.

29
00:02:49,520 --> 00:02:54,680
The award was for conceptual and engineering breakthroughs that have made deep neural networks

30
00:02:54,680 --> 00:02:57,240
a critical component of computing.

31
00:02:57,240 --> 00:03:04,200
He is a member of the National Academy of Sciences and the National Academy of Engineering,

32
00:03:04,200 --> 00:03:07,520
amongst many others.

33
00:03:07,520 --> 00:03:13,800
His interests include AI, machine learning, computer perception, robotics and computational

34
00:03:13,800 --> 00:03:20,760
neuroscience, and I'm sure you're all eager to hear from Jan on what's been happening

35
00:03:20,760 --> 00:03:27,360
with generative AI and what all the buzz is about, hopefully we'll get into the technical

36
00:03:27,360 --> 00:03:32,760
details and immediately following his talk, we will do a fireside chat where I will try

37
00:03:32,760 --> 00:03:36,000
to ask him some tough questions.

38
00:03:36,000 --> 00:03:38,960
And then we will also get questions from the audience.

39
00:03:38,960 --> 00:03:42,800
By the way, we did get online questions from the audience.

40
00:03:42,800 --> 00:03:47,680
We got 150 questions, so there's no way we're going to walk you through all of those.

41
00:03:47,680 --> 00:03:50,520
So we'll see how much time allows us to answer.

42
00:03:50,520 --> 00:04:02,520
Thank you and please join me in welcoming Jan to Northeastern University.

43
00:04:02,520 --> 00:04:03,520
Thank you, Samap.

44
00:04:03,520 --> 00:04:11,920
A real pleasure to be here and thanks for coming here so numerous or for listening in online.

45
00:04:11,920 --> 00:04:16,400
So I'm going to talk a bit about the state of the art in AI but also about the next step

46
00:04:16,400 --> 00:04:24,520
because I'm always interested in the next step and how we can make machines more intelligent.

47
00:04:24,520 --> 00:04:31,520
And we need to figure out how to get machines that cannot just learn but also can reason

48
00:04:31,520 --> 00:04:37,400
and plan and current AI really does not allow current systems to do this.

49
00:04:37,400 --> 00:04:43,400
So I'll try to kind of sketch a potential pathway towards such systems.

50
00:04:43,400 --> 00:04:50,080
I can't say that we built it completely but we built some components and I go through this.

51
00:04:50,080 --> 00:04:56,080
So AI is in the news, everybody is playing with it at the moment.

52
00:04:56,080 --> 00:04:58,440
It's pretty amazing how it works.

53
00:04:58,440 --> 00:04:59,760
There's a lot of success.

54
00:04:59,760 --> 00:05:03,720
It's been very widely deployed very much in many applications that are behind the curtain

55
00:05:03,720 --> 00:05:06,200
but in some of them much more visible.

56
00:05:06,200 --> 00:05:12,760
So LLMs have the advantage of being visible but for the last 10 years or so there's massive

57
00:05:12,760 --> 00:05:20,080
use of AI and the latest development of AI for such thing as ranking for search engine

58
00:05:20,080 --> 00:05:25,880
and social networks or for content moderation, things like that.

59
00:05:25,880 --> 00:05:30,640
But overall machine learning requires a lot of data and the machines that we have are

60
00:05:30,640 --> 00:05:32,640
somewhat brittle, specialized.

61
00:05:32,640 --> 00:05:39,560
They don't have human-level intelligence despite what we may be led to believe.

62
00:05:39,560 --> 00:05:47,280
So in short, machine learning sucks at least compared to humans and animals.

63
00:05:47,280 --> 00:05:52,520
We've been using supervised learning which really was the workhorse of machine learning

64
00:05:52,520 --> 00:05:56,520
and AI systems until very recently.

65
00:05:56,520 --> 00:06:00,640
Reinforcement learning is insanely inefficient but it works really well for games but not

66
00:06:00,640 --> 00:06:02,720
many other things.

67
00:06:02,720 --> 00:06:07,560
So one thing that has taken over the AI world in the last few years is something called self-supervised

68
00:06:07,560 --> 00:06:11,880
learning which I will talk about at length.

69
00:06:11,880 --> 00:06:14,160
But current AI systems are specialized and brittle.

70
00:06:14,160 --> 00:06:15,160
They make stupid mistakes.

71
00:06:15,160 --> 00:06:21,440
They don't really reason and plan with a few exceptions for a game playing for example.

72
00:06:21,440 --> 00:06:25,400
Compared to humans and animals, they can learn new tasks extremely quickly, understand

73
00:06:25,400 --> 00:06:30,760
how the world works, can reason and plan have some level of common sense.

74
00:06:30,760 --> 00:06:32,960
Machines still don't have common sense.

75
00:06:32,960 --> 00:06:38,480
So how do we get machines to reason and plan like animals and humans learn as fast as animals

76
00:06:38,480 --> 00:06:40,320
and humans?

77
00:06:40,320 --> 00:06:44,320
And we'll need machines that can understand how the world works, can predict the consequences

78
00:06:44,320 --> 00:06:51,040
of their actions, can perform change of reasoning with unlimited number of steps, can plan complex

79
00:06:51,640 --> 00:06:55,080
tasks by decomposing them into simpler tasks.

80
00:06:55,080 --> 00:06:58,360
So let's start with this idea of self-supervised learning.

81
00:06:58,360 --> 00:07:00,800
It's really taken over the world.

82
00:07:00,800 --> 00:07:04,840
Every sort of top machine learning system today uses some form of self-supervised learning

83
00:07:04,840 --> 00:07:09,080
as a first step to pre-train the system.

84
00:07:09,080 --> 00:07:10,580
And it's used everywhere.

85
00:07:10,580 --> 00:07:11,580
What does it consist of?

86
00:07:11,580 --> 00:07:16,840
It's really the idea that instead of having, of training a system with an input and an

87
00:07:16,840 --> 00:07:21,600
output, which is the case in supervised learning, or with an input and a reward, which is the

88
00:07:21,600 --> 00:07:26,200
case for reinforcement learning, you train the system to basically model its input.

89
00:07:26,200 --> 00:07:30,320
You don't train it for any particular task other than capture the dependency between

90
00:07:30,320 --> 00:07:31,560
different parts of its input.

91
00:07:31,560 --> 00:07:38,080
So one thing you might do is, for example, take a piece of video, a piece of text, show

92
00:07:38,080 --> 00:07:43,280
a piece of the video to the system and ask it to predict the missing piece, like the

93
00:07:43,280 --> 00:07:44,520
continuation of that video.

94
00:07:44,520 --> 00:07:48,920
And after a while, you reveal the rest of the video and you adjust the system so that

95
00:07:48,920 --> 00:07:51,960
it does a better job at predicting.

96
00:07:51,960 --> 00:07:55,200
So prediction really is kind of the essence of intelligence.

97
00:07:55,200 --> 00:07:59,200
And to some extent, by training a system to predict, it doesn't have to be predicting

98
00:07:59,200 --> 00:08:00,200
the future.

99
00:08:00,200 --> 00:08:03,160
It could be predicting the past or the left from the right.

100
00:08:03,160 --> 00:08:06,560
You're training the system to represent data, essentially.

101
00:08:06,560 --> 00:08:15,280
And that's been nothing short of astonishingly successful in the domain of natural language

102
00:08:15,280 --> 00:08:16,520
understanding.

103
00:08:16,520 --> 00:08:26,080
So every type performing NLP system today is pre-trained the following way, or with some

104
00:08:26,080 --> 00:08:31,600
form of the following way, which is a special case of an old idea called denoising autoencoder.

105
00:08:31,600 --> 00:08:40,120
And the idea is that you take a piece of text, sequence of words from a corpus.

106
00:08:40,120 --> 00:08:43,880
Typically it would be a few hundred or a few thousand words long.

107
00:08:43,880 --> 00:08:49,520
Those words immediately get turned into vectors, but let me not talk about this for just now.

108
00:08:49,520 --> 00:08:52,560
So the first thing you do is you corrupt this text.

109
00:08:52,560 --> 00:08:57,560
You remove some of the words and replace them by blank markers, or you substitute them for

110
00:08:57,560 --> 00:08:58,560
another word.

111
00:08:59,120 --> 00:09:03,280
And then you train some gigantic neural net to predict the words that are missing.

112
00:09:03,280 --> 00:09:06,880
In the process of doing so, the system has to basically develop some sort of understanding

113
00:09:06,880 --> 00:09:11,520
of the text, because if you want to be able to predict what word comes here, you have

114
00:09:11,520 --> 00:09:17,720
to understand the role of the word in the sentence, the type of word that comes here,

115
00:09:17,720 --> 00:09:18,800
and the whole meaning of the sentence.

116
00:09:18,800 --> 00:09:21,680
So the system basically learns to represent text.

117
00:09:21,680 --> 00:09:26,120
And the amazing thing is that just by doing this, you can train a system to represent

118
00:09:26,120 --> 00:09:31,200
the meaning of text in pretty much any language, as long as you have data.

119
00:09:31,200 --> 00:09:36,520
With a single system, you can have a system that represents the meaning of a piece of

120
00:09:36,520 --> 00:09:39,760
text in any language.

121
00:09:39,760 --> 00:09:40,840
So pretty cool.

122
00:09:40,840 --> 00:09:47,160
You can use this to build translation systems, systems that detect hate speech on social

123
00:09:47,160 --> 00:09:50,480
networks or figure out what something talks about.

124
00:09:50,480 --> 00:09:56,280
The way you do this is that you chop off the last few layers of that gigantic neural

125
00:09:56,280 --> 00:10:01,720
net, and you use the representation, the internal representation, learned by the system as input

126
00:10:01,720 --> 00:10:08,800
to a subsequent downstream task that you train supervised, like, say, translation.

127
00:10:08,800 --> 00:10:12,640
And it's really astonishing how well this works.

128
00:10:12,640 --> 00:10:19,880
So from this to a generative AI system, there's a small step, particularly for text generation.

129
00:10:19,880 --> 00:10:22,600
Text generation is a completely different thing, which I'm not going to talk about,

130
00:10:22,600 --> 00:10:26,760
but although some systems use the same technique.

131
00:10:26,760 --> 00:10:34,440
So what is a generative text generation system, a large language model?

132
00:10:34,440 --> 00:10:42,120
It's a system of the type I just described, except that when you train it, you don't remove

133
00:10:42,120 --> 00:10:47,280
random words in the text that you show at the input, you only remove the last one.

134
00:10:47,280 --> 00:10:53,120
So you train the system to predict the last word in a sequence of words.

135
00:10:53,120 --> 00:10:58,600
So show a sequence of words, and then show the last word, and train some gigantic neural

136
00:10:58,600 --> 00:11:02,320
net, perhaps with billions or hundreds of billions of parameters, to predict the next

137
00:11:02,320 --> 00:11:03,320
word.

138
00:11:03,320 --> 00:11:12,080
And you have to train this on trillions of text snippets, typically one to two trillion

139
00:11:12,080 --> 00:11:14,560
for the biggest models.

140
00:11:14,560 --> 00:11:17,960
Once you have that system, you can use it to generate text using what's called autoregressive

141
00:11:17,960 --> 00:11:21,320
prediction, which is a very classical thing to do in single processing.

142
00:11:21,320 --> 00:11:25,720
So you take a piece of text called a prompt, you enter it into the system, you have it

143
00:11:25,720 --> 00:11:31,920
predict the next word, and then you shift that word into the input.

144
00:11:31,920 --> 00:11:37,160
So now it becomes part of the input to the system, and now you can predict the next word,

145
00:11:37,160 --> 00:11:39,800
shift it in, predict the third word, shift it in.

146
00:11:39,800 --> 00:11:42,200
That's autoregressive prediction.

147
00:11:42,200 --> 00:11:46,920
And that's how all the bigger alarms that everybody has played with work.

148
00:11:46,920 --> 00:11:47,920
That's how they've been trained.

149
00:11:47,920 --> 00:11:51,160
That's how they generate text.

150
00:11:51,160 --> 00:12:00,400
So those alarms are kind of amazing in terms of the performance that they produce.

151
00:12:00,400 --> 00:12:03,760
So again, they're trained on something like one to two trillion tokens.

152
00:12:03,760 --> 00:12:07,720
A token is like a word or a subword unit.

153
00:12:07,720 --> 00:12:10,400
And there's a whole bunch of those models, most of which you probably haven't heard of,

154
00:12:10,400 --> 00:12:15,800
but there's a few that have become household names.

155
00:12:15,800 --> 00:12:26,000
So we've heard of chatGBT and GPT-4 from OpenAI, which are kind of usable, barred from Google,

156
00:12:26,000 --> 00:12:33,200
and derivative of chatGBT and GPT-4 from Microsoft, married with Bing.

157
00:12:33,200 --> 00:12:38,160
But there's a long history of those things that goes back several years, some from Fair,

158
00:12:38,160 --> 00:12:39,160
Lunderbot, and Galactica.

159
00:12:39,160 --> 00:12:47,400
Galactica was trained on the scientific literature and is designed to help scientists write papers.

160
00:12:47,400 --> 00:12:51,960
And a more recent one, called LAMA, which is the code is open source.

161
00:12:51,960 --> 00:12:56,760
The model, you can get it on request if you are using it for research purpose.

162
00:12:56,760 --> 00:13:00,360
And it's the same level of performance as things like chatGBT, but it's not fine-tuned.

163
00:13:00,360 --> 00:13:02,360
You have to fine-tune it for application.

164
00:13:02,360 --> 00:13:07,080
And in fact, people have done this, so Alpaca is a model which basically is a fine-tuned

165
00:13:07,080 --> 00:13:13,400
version of LAMA that was built by people at Stanford for answering questions and things

166
00:13:13,400 --> 00:13:15,200
like that, instruction.

167
00:13:15,200 --> 00:13:21,120
So they're pretty amazing, they surprised a lot of people in how well they work, but

168
00:13:21,120 --> 00:13:27,200
they make a lot of factual errors, logical errors, inconsistencies, limited reasoning

169
00:13:27,200 --> 00:13:29,720
abilities, things like that.

170
00:13:29,720 --> 00:13:33,560
And they are easy to, they're pretty gullible.

171
00:13:33,560 --> 00:13:38,560
So you tell them, what is 2 plus 2, and the system will say 4, and you say, no, actually

172
00:13:38,560 --> 00:13:39,560
2 plus 2 equals 5.

173
00:13:39,560 --> 00:13:41,640
Oh yeah, you're right, I made a mistake.

174
00:13:41,640 --> 00:13:51,640
So they kind of, they predict answers that would sound like someone could produce these

175
00:13:51,640 --> 00:13:54,000
answers, but the details might be wrong.

176
00:13:54,000 --> 00:14:01,480
So you can't really use them for factual answers, but you can use them certainly for

177
00:14:01,480 --> 00:14:03,120
writing aids.

178
00:14:03,120 --> 00:14:10,760
And particularly, it works really well for text or for standard sort of templatized text

179
00:14:10,760 --> 00:14:16,040
that you need to write, like I don't know, there's a bunch of professors here that have

180
00:14:16,040 --> 00:14:26,200
to spend quite a bit of time writing recommendation letters for students, very useful for that.

181
00:14:26,200 --> 00:14:27,720
And very useful for code generation.

182
00:14:27,720 --> 00:14:32,560
So the software industry is probably going to be revolutionized by such tools.

183
00:14:32,560 --> 00:14:38,080
So this is an example of code generated from a prompt by the Lama 65 billion model, the

184
00:14:38,080 --> 00:14:39,080
open source one.

185
00:14:39,080 --> 00:14:44,160
So, you know, ask it, you know, find real roots of AX squared plus BX plus C, and the

186
00:14:44,160 --> 00:14:49,520
thing just writes a function in Python or whatever, whatever you want, or Reg X or whatever,

187
00:14:49,520 --> 00:14:53,280
who remembers the syntax of Reg X?

188
00:14:53,280 --> 00:14:55,280
Like.

189
00:14:55,280 --> 00:15:00,480
You can have it, you know, hallucinate text that might sound plausible or completely implausible

190
00:15:00,480 --> 00:15:01,480
like this.

191
00:15:01,480 --> 00:15:04,560
Did you know that Yanukun dropped a rap album last year?

192
00:15:04,560 --> 00:15:07,200
We listened to it and here is what we thought.

193
00:15:07,200 --> 00:15:17,920
And the thing writes a review of my alleged rap album.

194
00:15:17,920 --> 00:15:22,240
I'm not much of a rap person, I'm more of a jazz person, so when my colleagues showed

195
00:15:22,720 --> 00:15:26,680
this to me, I told them, like, can you do the same for a jazz album that would be kind

196
00:15:26,680 --> 00:15:27,680
of more appropriate?

197
00:15:27,680 --> 00:15:33,160
I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't

198
00:15:33,160 --> 00:15:38,120
work very well because there's not enough training data on the web of reviews of jazz

199
00:15:38,120 --> 00:15:39,120
albums.

200
00:15:39,120 --> 00:15:44,320
I found that incredibly sad, I cried.

201
00:15:44,320 --> 00:15:46,720
So you need a lot of data to train those things, right?

202
00:15:46,720 --> 00:15:50,960
In fact, the amount of data, like something like 1.5 trillion tokens that Lama is trained

203
00:15:50,960 --> 00:15:56,000
on, it would take about 22,000 years for a human reading eight hours a day at every

204
00:15:56,000 --> 00:15:58,480
speed to read the whole material.

205
00:15:58,480 --> 00:16:04,160
So obviously those things can accumulate a lot of knowledge, at least approximately.

206
00:16:04,160 --> 00:16:09,240
So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not

207
00:16:09,240 --> 00:16:12,680
good for producing factual and consistent answers, at least not yet.

208
00:16:12,680 --> 00:16:18,680
So a lot of LLMs are being augmented or extended so that they can use tools like calculators

209
00:16:18,680 --> 00:16:26,400
or database engines or whatever to search for information and then refer to the source.

210
00:16:26,400 --> 00:16:31,960
They're not good at all for reasoning, planning, or even for arithmetic.

211
00:16:31,960 --> 00:16:39,440
So but we are easily fooled by their language fluency into thinking that they are intelligent.

212
00:16:39,440 --> 00:16:42,600
They're not that intelligent.

213
00:16:42,600 --> 00:16:46,440
And they really have no understanding of the physical world because they're trained with

214
00:16:46,440 --> 00:16:47,440
text.

215
00:16:47,720 --> 00:16:51,200
And there's another flaw, which is a huge problem.

216
00:16:51,200 --> 00:16:56,880
It's the fact that if you imagine that there is the set of all possible answers represented

217
00:16:56,880 --> 00:17:02,080
by this sphere, disk, which is really a tree, right?

218
00:17:02,080 --> 00:17:05,080
Every token you add put, you have a certain number of options for what the token should

219
00:17:05,080 --> 00:17:06,680
be, what the word is.

220
00:17:06,680 --> 00:17:08,480
So it's a tree of all possible answers.

221
00:17:08,480 --> 00:17:13,600
Within this tree, there is a small subtree that corresponds to correct answers for the

222
00:17:13,600 --> 00:17:16,120
question being asked.

223
00:17:16,120 --> 00:17:22,680
And imagine that there is a probability E for any token that is produced by the system

224
00:17:22,680 --> 00:17:26,280
to be outside, to take you outside that tree of correct answers.

225
00:17:26,280 --> 00:17:29,200
Once you go outside that tree, you can't come back because it's a tree.

226
00:17:29,200 --> 00:17:37,480
So let's imagine that the probability per token is E. So the probability that a sequence

227
00:17:37,480 --> 00:17:41,400
of N tokens would be correct is 1 minus E to the power N, making the assumption that

228
00:17:41,400 --> 00:17:46,920
the errors are independent, which of course they're not, but that's kind of a crude assumption.

229
00:17:46,920 --> 00:17:51,320
And so the problem with this is that it's an exponentially divergent process, this

230
00:17:51,320 --> 00:17:55,480
autoregressive prediction, errors accumulate.

231
00:17:55,480 --> 00:18:01,000
And if you produce too many tokens, the thing will sort of diverge away from the set of

232
00:18:01,000 --> 00:18:02,560
correct answers, exponentially.

233
00:18:02,560 --> 00:18:06,800
And that's not fixable with the current architecture.

234
00:18:06,800 --> 00:18:12,760
You can fine tune those systems a lot to reduce E, but you're not going to make it

235
00:18:12,760 --> 00:18:14,880
go away.

236
00:18:14,880 --> 00:18:21,640
So I have a bold prediction, which is that the shelf life of autoregressive LLM is very

237
00:18:21,640 --> 00:18:23,000
short.

238
00:18:23,000 --> 00:18:28,360
My prediction is that five years from now, nobody in their right mind would use them.

239
00:18:28,360 --> 00:18:32,760
So enjoy it while it lasts.

240
00:18:32,760 --> 00:18:35,720
They'll be replaced by things that are better.

241
00:18:35,720 --> 00:18:41,680
And I'll hint about directions to kind of perhaps fix up those problems.

242
00:18:41,680 --> 00:18:49,400
So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema

243
00:18:49,400 --> 00:18:54,800
magazine, which is a philosophy magazine, about the fact that a system that is purely

244
00:18:54,800 --> 00:19:01,600
trained from text, from language, cannot possibly attain human level intelligence because much

245
00:19:01,600 --> 00:19:07,920
of what humans know is actually derived from experience of the physical world.

246
00:19:07,920 --> 00:19:13,520
This is true for a lot of human knowledge, but it's true certainly for almost the totality

247
00:19:13,520 --> 00:19:15,760
of animal knowledge.

248
00:19:15,760 --> 00:19:20,240
It's all about the world is no linguistic related, no language related.

249
00:19:20,240 --> 00:19:25,960
So linguistic abilities and fluency are not related to the ability to think.

250
00:19:25,960 --> 00:19:30,680
Those are two different things.

251
00:19:30,680 --> 00:19:37,200
There are some criticisms of autoregressive LLMs from people coming from the cognitive

252
00:19:37,200 --> 00:19:43,200
science realm who say like, this is not at all the way the human mind works.

253
00:19:43,200 --> 00:19:46,160
There is essential missing pieces.

254
00:19:46,160 --> 00:19:50,280
Other criticism for people who come from sort of more classical AI, pre-deep learning,

255
00:19:50,280 --> 00:19:56,760
they say like, you know, AI systems are supposed to be able to plan and reason, and those LLMs

256
00:19:56,760 --> 00:19:57,760
can do it.

257
00:19:57,760 --> 00:20:02,320
Or at least not, you know, they can do it maybe in very sort of primitive forms.

258
00:20:02,320 --> 00:20:08,040
Perhaps they can plan things in situations that correspond to a template that they've

259
00:20:08,040 --> 00:20:12,200
been trained on, but they're not so innovative.

260
00:20:12,200 --> 00:20:20,240
So we should ask, how is it that humans and animals can run so quickly?

261
00:20:20,240 --> 00:20:26,840
And I've been using this diagram for quite a while now, several, many years from Emmanuel

262
00:20:26,840 --> 00:20:30,680
Dupu, who's a cognitive scientist in Paris.

263
00:20:30,680 --> 00:20:36,440
And we tried to sort of make a chart of at what age babies learn basic concepts about

264
00:20:36,440 --> 00:20:43,400
the world, so things like distinguishing between animate objects and inanimate objects, learning

265
00:20:43,400 --> 00:20:46,560
the notion of object permanence, the fact that when an object is hidden behind another

266
00:20:46,560 --> 00:20:49,760
one, it still exists.

267
00:20:49,760 --> 00:20:54,440
Notion of rigidity, solidity, things like natural categories, babies don't need to

268
00:20:54,440 --> 00:20:58,680
know the name of an object to actually know that there are different categories of objects

269
00:20:58,680 --> 00:21:00,240
around four months or so.

270
00:21:00,240 --> 00:21:04,840
And then it takes about nine months for babies to really understand that sort of intuitive

271
00:21:04,840 --> 00:21:11,320
physics that objects that are not supported will fall, that, you know, objects have a

272
00:21:11,320 --> 00:21:19,560
momentum, weight, friction, you know, knowing that if I push on this object, you know, light

273
00:21:19,560 --> 00:21:24,120
objects like this, they're going to move, but if I push on an object that's heavier,

274
00:21:24,120 --> 00:21:26,320
it's not going to move unless I push harder.

275
00:21:26,320 --> 00:21:27,320
So things like that.

276
00:21:27,320 --> 00:21:31,880
So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where

277
00:21:31,880 --> 00:21:36,160
you have a little car on the platform, you push the car off the platform, it appears

278
00:21:36,160 --> 00:21:42,000
to float in the air, a five-month-old baby will pay attention, a 10-month-old baby will

279
00:21:42,000 --> 00:21:48,440
go like this because she understood that by then that objects are not supported or supposed

280
00:21:48,440 --> 00:21:52,360
to fall and this object appears to be floating in the air.

281
00:21:52,360 --> 00:21:59,200
So we can determine that her mental model of the world is being violated, okay?

282
00:21:59,200 --> 00:22:03,080
That's how this chart was built.

283
00:22:03,080 --> 00:22:07,680
So we accumulate as babies an enormous amount of background knowledge about how the world

284
00:22:07,680 --> 00:22:11,680
works, mostly by observation, a little bit by interaction, when we start being able to

285
00:22:11,680 --> 00:22:17,600
kind of grab things, but in the first few months it's mostly just observation.

286
00:22:17,600 --> 00:22:23,600
So we don't know how to reproduce this with this type of learning with machines.

287
00:22:23,600 --> 00:22:29,880
Once we accumulate all this background knowledge, you know, in a number of years, learning a

288
00:22:29,880 --> 00:22:33,160
new task like driving is very fast.

289
00:22:33,160 --> 00:22:38,600
So any teenager can learn to drive in about 20 hours of practice, mostly without causing

290
00:22:38,600 --> 00:22:41,320
any accident.

291
00:22:41,320 --> 00:22:44,920
So the teenager doesn't have to run off a cliff to figure out that the car, that nothing

292
00:22:44,920 --> 00:22:48,080
good is going to happen if you run off a cliff.

293
00:22:48,080 --> 00:22:53,000
The mental model of the world is already there, okay?

294
00:22:53,000 --> 00:22:56,480
We still won't have level five salivating cars.

295
00:22:56,480 --> 00:23:01,000
So obviously we're missing something pretty big.

296
00:23:01,000 --> 00:23:05,520
Any 10-year-old can clear up the dinner table and fill up the dishwasher.

297
00:23:05,520 --> 00:23:08,480
We're nowhere near having robots that can do this and it's not because of mechanical

298
00:23:08,480 --> 00:23:16,120
design, it's because we don't know how to build the minds behind it.

299
00:23:16,120 --> 00:23:18,600
So we're missing something big, right?

300
00:23:18,600 --> 00:23:24,480
The past towards human-level AI is not just making LLMs bigger, that's just not going

301
00:23:24,480 --> 00:23:26,640
to get us there.

302
00:23:26,640 --> 00:23:33,720
It's been a common recurring error by AI scientists and engineers over the last six decades to

303
00:23:33,720 --> 00:23:39,960
imagine that the one thing that they just discovered was the solution to human-level

304
00:23:39,960 --> 00:23:44,760
AI, only to discover a few years later that no, there was actually a big obstacle, another

305
00:23:44,760 --> 00:23:46,560
obstacle they had to clear.

306
00:23:46,560 --> 00:23:51,240
It's a recurring history story in AI.

307
00:23:51,240 --> 00:23:56,000
So common sense will probably emerge from the ability of machines to learn how the world

308
00:23:56,000 --> 00:24:01,120
works by observation, the way babies and animals do it.

309
00:24:01,120 --> 00:24:05,800
So I see three challenges for AI research over the next decade also, learning representations

310
00:24:05,800 --> 00:24:10,160
of the world and predictive models of the world, I'll say why in a minute, and self-supervised

311
00:24:10,160 --> 00:24:16,120
learning is going to be the key component of that, learning to reason.

312
00:24:16,120 --> 00:24:19,840
So psychologists talk about system one and system two.

313
00:24:19,840 --> 00:24:28,400
System one is the type of control that our brains use to kind of react to something without

314
00:24:28,400 --> 00:24:31,760
really having to think about it, like subconscious action.

315
00:24:31,760 --> 00:24:36,120
So if you're an experienced driver, you don't have to think about driving, you can just drive

316
00:24:36,120 --> 00:24:41,240
and you can talk to someone at the same time and barely pay attention.

317
00:24:41,240 --> 00:24:42,240
So that's system one.

318
00:24:42,240 --> 00:24:46,400
But then when you are learning to drive, you pay attention to absolutely everything.

319
00:24:46,400 --> 00:24:56,920
You use your entire focus, consciousness, attention to drive and that's system two.

320
00:24:56,920 --> 00:25:01,400
And then the last thing is learning to plan complex action sequences, decomposing them

321
00:25:01,400 --> 00:25:02,400
into simpler ones.

322
00:25:02,400 --> 00:25:13,400
So I wrote a sort of vision paper about a year ago, which I posted on open review for

323
00:25:13,400 --> 00:25:16,120
comments, so you're welcome to comment on it.

324
00:25:16,120 --> 00:25:20,880
I give a bunch of technical talks about it, one of the earliest one was at Berkeley, but

325
00:25:20,880 --> 00:25:24,680
you are having a more recent version of it right now, so you don't need to look at that

326
00:25:24,680 --> 00:25:26,800
one.

327
00:25:26,800 --> 00:25:32,400
And it's based on what's called a cognitive architecture.

328
00:25:32,400 --> 00:25:38,240
So basically how can we sort of design a system with different modules so that those modules

329
00:25:38,240 --> 00:25:46,640
may implement all the properties that I was telling you about so systems can perceive,

330
00:25:46,640 --> 00:25:51,600
reason, predict, in particular predict the consequences of their actions and then plan

331
00:25:51,600 --> 00:25:56,560
a sequence of actions to optimize, to satisfy a particular objective.

332
00:25:56,560 --> 00:26:04,720
So the main components of the system is the key component, I would say, is the world model

333
00:26:04,720 --> 00:26:13,760
and the world model is what allows the system to predict ahead, imagine what's gonna happen.

334
00:26:13,760 --> 00:26:18,800
This is to some extent what current AI systems don't really have.

335
00:26:18,800 --> 00:26:23,280
Perception system basically gets an estimate of the state of the world and initializes

336
00:26:23,280 --> 00:26:25,680
the world model with it.

337
00:26:25,680 --> 00:26:31,160
The cost here is a really important module, so basically the entire purpose of the agent

338
00:26:31,160 --> 00:26:35,080
is to minimize this cost.

339
00:26:35,080 --> 00:26:42,840
So the cost is something that uses a measurement of the state of the agent, particularly the

340
00:26:42,840 --> 00:26:48,240
prediction from the world model, and predicts whether an act is going to be good or bad.

341
00:26:48,240 --> 00:26:51,880
And the entire purpose of the agent here is to figure out a sequence of actions, so this

342
00:26:51,880 --> 00:26:56,920
is taking place in the actor, figure out a sequence of actions such that when I predict

343
00:26:56,920 --> 00:27:01,520
what's gonna happen as a consequence of those actions using my world model, my objective,

344
00:27:01,520 --> 00:27:04,320
my cost function will be minimized.

345
00:27:04,320 --> 00:27:11,680
So if my cost function is, so the cost function is basically our measures of discomfort of

346
00:27:11,680 --> 00:27:15,080
the agent.

347
00:27:15,080 --> 00:27:17,960
Biological brains have things like that in the visual language, so this is the thing

348
00:27:17,960 --> 00:27:23,320
that tells you when you're hungry, for example, or you're hurting.

349
00:27:23,320 --> 00:27:29,800
So nature tells you you're hungry, nature doesn't tell you how to feed, you have to

350
00:27:29,800 --> 00:27:34,520
figure that out by yourself, perhaps using your world model and your planning abilities.

351
00:27:34,520 --> 00:27:38,040
So this is the same thing here, imagine this is a robot and the robot battery are kind

352
00:27:38,040 --> 00:27:43,400
of starting to get drained, so there's a cost function here that says be careful, you're

353
00:27:43,400 --> 00:27:45,480
running out of power.

354
00:27:45,480 --> 00:27:49,960
And so the system, according to this world model, would say, well, I can recharge my

355
00:27:49,960 --> 00:27:53,480
battery by plugging myself into a socket.

356
00:27:53,480 --> 00:27:58,560
So it figures out the sequence of actions to plug itself into a socket and that will

357
00:27:58,560 --> 00:28:04,000
eventually minimize the cost function that just appeared.

358
00:28:04,000 --> 00:28:11,240
So in fact, there's two ways to operate that system one is the kind of system one where

359
00:28:11,240 --> 00:28:15,600
the system makes an estimate of the state of the world, run this to a perception system

360
00:28:15,600 --> 00:28:20,120
called an encoder here, produces an estimate of the state of the world called S0 and that

361
00:28:20,120 --> 00:28:25,000
runs into a neural net called a policy network that just produces an action and the action

362
00:28:25,000 --> 00:28:27,000
goes into the world.

363
00:28:27,000 --> 00:28:32,440
LLNs are like this, they are system one, you give them a pump, that's X, they produce

364
00:28:32,440 --> 00:28:37,400
an action, that's the token they predict, that goes back into the world and the world

365
00:28:37,400 --> 00:28:42,880
is very simplistic here, it's just you shift in the input.

366
00:28:42,880 --> 00:28:46,560
So no reasoning necessary, here is system two.

367
00:28:46,560 --> 00:28:54,840
So you use the same system here and this is a sort of time-enrolled version of the system.

368
00:28:54,840 --> 00:29:00,080
So we have the world model, the world model is this green module and the different instances

369
00:29:00,080 --> 00:29:04,600
of that green module are the state of the system at different time steps, so think of

370
00:29:04,600 --> 00:29:08,820
it as like a recurrent net that you unfolded, so it's really the same module at different

371
00:29:08,820 --> 00:29:09,820
time steps.

372
00:29:09,820 --> 00:29:14,560
What the world model is supposed to be able to predict is given the representation of

373
00:29:14,560 --> 00:29:19,600
the state of the world at time t and given an action that I'm imagining taking, what

374
00:29:19,600 --> 00:29:24,120
is going to be the predicted state of the world at time t plus one.

375
00:29:24,120 --> 00:29:29,320
So I can imagine a sequence of actions that I might take, imagine the effect on the world

376
00:29:29,320 --> 00:29:34,720
using my world model and then I can plug the state of the world over this trajectory

377
00:29:34,720 --> 00:29:42,440
through my cost and measure whether my cost is going to be minimized by this action sequence,

378
00:29:42,440 --> 00:29:43,440
my objectives.

379
00:29:43,440 --> 00:29:50,160
So what I should do is run some sort of optimization procedure that will try to search for a sequence

380
00:29:50,160 --> 00:29:54,920
of actions that minimizes the cost given the prediction given to me by the produced by

381
00:29:54,920 --> 00:29:56,320
the world model.

382
00:29:56,320 --> 00:30:00,280
This type of planning is very classical in optimal control.

383
00:30:00,280 --> 00:30:02,960
It's called model predictive control.

384
00:30:02,960 --> 00:30:10,640
In classical optimal control, the model is not learned usually, it's handcrafted.

385
00:30:10,640 --> 00:30:15,400
Here we are thinking about a situation where the world model is learned by, for example,

386
00:30:15,400 --> 00:30:22,240
watching the world go by, by video, but also by observing actions being taken in the world

387
00:30:22,240 --> 00:30:23,400
and seeing the effect.

388
00:30:23,400 --> 00:30:28,800
So to get a good accurate model here, I'm going to have to observe the state of the

389
00:30:28,800 --> 00:30:33,680
world, observe, like, take an action and observe the effect or observe someone else

390
00:30:33,680 --> 00:30:36,040
take an action and observe the effect.

391
00:30:36,040 --> 00:30:40,880
Let me skip this for now.

392
00:30:40,880 --> 00:30:45,480
Ultimately what we want is a hierarchical version of this because if you want the system

393
00:30:45,480 --> 00:30:50,720
to be able to plan complex actions, we can't plan it at the lowest level.

394
00:30:50,720 --> 00:30:59,920
So for example, if I want to plan to go from here to New York City, I would have to basically

395
00:30:59,920 --> 00:31:05,520
plan every millisecond exactly what muscle actions I should take, okay?

396
00:31:05,520 --> 00:31:07,680
And it's impossible, right?

397
00:31:07,680 --> 00:31:13,280
You can't plan an entire trip from here to New York City millisecond by millisecond,

398
00:31:13,280 --> 00:31:19,200
partly because you don't have a perfect model of the environment, like you don't know if,

399
00:31:19,200 --> 00:31:25,480
when you're going to walk up the room here, whether someone is going to be on the way,

400
00:31:25,480 --> 00:31:26,840
in the way and you're going to have to go around.

401
00:31:26,840 --> 00:31:29,200
So you can't completely plan in advance, right?

402
00:31:29,200 --> 00:31:33,440
So what we do is we plan hierarchically, say like, okay, I want to go to New York City,

403
00:31:33,440 --> 00:31:37,320
so the cost function at the top here measures my distance to New York City.

404
00:31:37,320 --> 00:31:44,040
And the first thing I have to do is go to the airport and catch a train or go to the train

405
00:31:44,040 --> 00:31:48,080
station and catch a train or go to the airport catch a plane.

406
00:31:49,080 --> 00:31:53,040
So the top predictors are predictors at a high level that says, oh, okay,

407
00:31:53,040 --> 00:31:57,800
if I catch a taxi, it might take me to the airport.

408
00:31:57,800 --> 00:32:03,120
If I catch, or to the train station, then if I catch a train, it'll take me to New York City.

409
00:32:03,120 --> 00:32:09,200
Okay, so you have those two hidden actions, those Z variables here.

410
00:32:09,200 --> 00:32:12,960
And they define a cost function for the next level down.

411
00:32:13,160 --> 00:32:19,600
So if the first action is I'm taking a taxi to the train station,

412
00:32:19,600 --> 00:32:23,520
the lower level is how do I catch a taxi here?

413
00:32:23,520 --> 00:32:25,240
I go down in the street and hail the taxi.

414
00:32:25,240 --> 00:32:26,080
No, this is Boston.

415
00:32:26,080 --> 00:32:28,840
I need to call it Uber or something.

416
00:32:28,840 --> 00:32:35,520
Okay, so I go on the street and I call it Uber.

417
00:32:35,520 --> 00:32:36,760
How do I go in the street?

418
00:32:36,760 --> 00:32:38,120
There's going to be lower levels.

419
00:32:38,120 --> 00:32:39,600
I have to get out of this building.

420
00:32:39,600 --> 00:32:40,520
How do we get out of this building?

421
00:32:40,520 --> 00:32:42,000
I have to walk through the door.

422
00:32:42,080 --> 00:32:43,400
How do I work through the door?

423
00:32:43,400 --> 00:32:47,200
I have to put one leg in front of the other over obstacles.

424
00:32:47,200 --> 00:32:49,480
And all the way down to millisecond.

425
00:32:49,480 --> 00:32:53,000
Also control for a short period, which is replanned as we go.

426
00:32:54,160 --> 00:32:57,440
Okay, no AI systems today can do any of this.

427
00:32:57,440 --> 00:33:01,640
This is completely virgin territory.

428
00:33:01,640 --> 00:33:05,680
Okay, there's a lot of people who've worked on hierarchical planning,

429
00:33:05,680 --> 00:33:10,160
but in situations where the representations at every level are hardwired,

430
00:33:10,160 --> 00:33:11,560
they're known in advance.

431
00:33:11,560 --> 00:33:13,080
They're predetermined.

432
00:33:13,080 --> 00:33:15,840
It's sort of like the equivalent of a vision system where the features

433
00:33:15,840 --> 00:33:18,080
at every level are hardwired or designed by hand.

434
00:33:19,080 --> 00:33:19,920
There's no system today.

435
00:33:19,920 --> 00:33:22,480
They can learn hierarchical representations for action plans.

436
00:33:23,760 --> 00:33:25,680
So that's a big challenge.

437
00:33:25,680 --> 00:33:29,600
The cost function, so here's what's important here.

438
00:33:29,600 --> 00:33:33,960
A lot of people today are talking about the fact that AI systems are difficult

439
00:33:33,960 --> 00:33:39,200
to control and that's terrible, maybe toxic, various things.

440
00:33:40,200 --> 00:33:46,560
The system I describe cannot produce outputs that do not minimize the objectives.

441
00:33:46,560 --> 00:33:53,320
And so if you have terms in the objective that guarantee certain conditions,

442
00:33:53,320 --> 00:33:56,360
that system will have no choice but obeying those conditions.

443
00:33:56,360 --> 00:33:58,960
Okay, so having a system that is designed like this,

444
00:33:58,960 --> 00:34:03,120
that whose output is produced by minimizing a set of objectives,

445
00:34:03,120 --> 00:34:08,000
according to a model, will basically help guarantee the safety of that system.

446
00:34:08,000 --> 00:34:12,680
Because you can hardwire intrinsic objectives on the left here

447
00:34:12,680 --> 00:34:14,600
that basically guarantee the safety.

448
00:34:14,600 --> 00:34:18,840
And the system cannot escape the satisfaction of those constraints.

449
00:34:18,840 --> 00:34:20,880
So let me take a very simple example.

450
00:34:20,880 --> 00:34:24,120
Let's say someone figures out how to build a domestic robot they can cook.

451
00:34:25,720 --> 00:34:30,320
This robot will have to be able to kind of handle a kitchen knife.

452
00:34:30,320 --> 00:34:33,680
And you might put a cost function that says, don't flail your arm

453
00:34:33,680 --> 00:34:36,520
if you have a kitchen knife in your arm and there is people around.

454
00:34:37,520 --> 00:34:38,960
Okay, because it's dangerous.

455
00:34:38,960 --> 00:34:42,640
So you can imagine putting a lot of kind of safety conditions in those systems

456
00:34:42,640 --> 00:34:44,080
to make them steerable.

457
00:34:44,080 --> 00:34:51,120
So I don't think the problem of making AI systems safe is such a huge problem

458
00:34:51,120 --> 00:34:55,840
that some people who are very vocal are seeing it is that AI is going to kill us all.

459
00:34:58,280 --> 00:34:59,880
It's not going to kill us all.

460
00:34:59,880 --> 00:35:06,120
We would have to screw up really badly for that to happen.

461
00:35:06,120 --> 00:35:09,640
Okay, now here's the thing.

462
00:35:09,640 --> 00:35:11,600
How do we build the world model?

463
00:35:13,600 --> 00:35:17,720
And that's basically the biggest challenge that we have at the moment.

464
00:35:17,720 --> 00:35:21,360
How do we build a system that can predict what's going to happen in the world?

465
00:35:21,360 --> 00:35:24,080
For example, by training itself to predict videos.

466
00:35:24,080 --> 00:35:28,160
Now the problem with predicting videos is that the world is not entirely predictable.

467
00:35:30,720 --> 00:35:32,960
It may not be deterministic, but even if it were deterministic,

468
00:35:32,960 --> 00:35:34,800
it wouldn't be completely predictable.

469
00:35:34,800 --> 00:35:36,560
So in fact, here is an example here.

470
00:35:38,320 --> 00:35:43,960
If you take a video, this is a top-down video of a highway that looks like cars

471
00:35:43,960 --> 00:35:46,440
driving around just following the blue car.

472
00:35:46,440 --> 00:35:49,160
And you train a neural net to predict what's going to happen in the video

473
00:35:49,160 --> 00:35:50,600
after the first few frames.

474
00:35:50,600 --> 00:35:56,400
It produces blurry, it makes blurry prediction because it can't predict if

475
00:35:56,400 --> 00:36:02,000
the car that's behind you is going to accelerate or break or change lane or whatever.

476
00:36:02,000 --> 00:36:05,600
So it makes an average of all the possible future and that's a blurry image.

477
00:36:05,600 --> 00:36:09,200
Same with, this is an old paper where we attempted to do video prediction using

478
00:36:09,200 --> 00:36:14,120
neural nets and the predictions are blurry because there's too many things

479
00:36:14,120 --> 00:36:16,600
that can plausibly happen and the system can only predict one thing.

480
00:36:16,600 --> 00:36:17,600
So it predicts the average.

481
00:36:19,840 --> 00:36:20,840
So that's no good.

482
00:36:20,840 --> 00:36:24,600
The solution to this is what I call a joint evading predictive architecture.

483
00:36:24,600 --> 00:36:28,280
And this is really the most important slide of the talk.

484
00:36:29,800 --> 00:36:35,520
So the normal way to make predictions is through a generative model.

485
00:36:35,520 --> 00:36:36,400
What's a generative model?

486
00:36:36,400 --> 00:36:38,600
It's a model where you have a bunch of variables you observe,

487
00:36:38,600 --> 00:36:41,840
let's say the initial segment of a video.

488
00:36:41,840 --> 00:36:44,920
You run it through an encoder and through a predictor and the predictor predicts

489
00:36:44,920 --> 00:36:48,440
y, which is, let's say, the continuation of that video.

490
00:36:49,560 --> 00:36:52,600
And you have some cost function that measures the discrepancy divergence

491
00:36:52,600 --> 00:36:55,800
between the predicted y and the actual y you observe.

492
00:36:55,800 --> 00:36:57,600
This is when you train your world model.

493
00:36:59,280 --> 00:37:02,320
It could be that the predictor has an action variable that comes in,

494
00:37:02,320 --> 00:37:03,880
but in this example there isn't.

495
00:37:05,560 --> 00:37:09,920
So examples of this are things like variational auto encoders,

496
00:37:09,920 --> 00:37:14,400
mass auto encoders, or denoising auto encoders, which is a more general concept.

497
00:37:14,400 --> 00:37:18,600
And so basically all NLP systems, including LMS, are of this type,

498
00:37:18,600 --> 00:37:19,360
the generative models.

499
00:37:20,360 --> 00:37:22,360
But here is the thing.

500
00:37:22,360 --> 00:37:25,440
You don't want to be predicting every detail about the world.

501
00:37:25,440 --> 00:37:28,360
Here you have to predict every single detail about the world.

502
00:37:28,360 --> 00:37:30,880
So it's easy if it's text, because text is discrete.

503
00:37:30,880 --> 00:37:35,040
So predicting the next word, I cannot predict the next word from a text.

504
00:37:35,040 --> 00:37:41,320
But I can predict within 10 possible words some probability distribution of

505
00:37:41,320 --> 00:37:45,120
the, over all the words in the dictionary of which word comes next, right?

506
00:37:45,120 --> 00:37:49,320
They can represent distributions over discrete variables.

507
00:37:49,320 --> 00:37:52,920
I cannot do this over the set of all possible video frames.

508
00:37:54,080 --> 00:37:57,560
I cannot usefully represent a distribution over the set of all possible video frames.

509
00:37:59,560 --> 00:38:05,200
So I can't use the same trick for video that is used for language.

510
00:38:05,200 --> 00:38:09,800
The reason why we have LMS that works so well is because text is easy.

511
00:38:09,800 --> 00:38:10,520
Language is simple.

512
00:38:11,840 --> 00:38:14,280
We only popped up in the last few hundred thousand years anyway, so

513
00:38:14,280 --> 00:38:15,240
it can be that complicated.

514
00:38:16,240 --> 00:38:22,040
And it's also processed in the brain by two tiny areas called the Vernike area

515
00:38:22,040 --> 00:38:24,600
for understanding and the Borke area for production.

516
00:38:25,640 --> 00:38:26,800
What about the rest of the brain?

517
00:38:26,800 --> 00:38:28,920
The prefrontal cortex, that's where we think, okay?

518
00:38:30,200 --> 00:38:34,360
That's not part of LMS, the LMS are perhaps good models of Vernike and Borke,

519
00:38:34,360 --> 00:38:34,880
but that's it.

520
00:38:37,880 --> 00:38:41,880
Okay, so what I'm proposing here is to replace this generative architecture by

521
00:38:41,880 --> 00:38:45,400
a joint embedding architecture and the essential characteristic of it is that

522
00:38:45,400 --> 00:38:49,640
the variable that you want to capture the dependency of with respect to X goes

523
00:38:49,640 --> 00:38:53,440
itself through an encoder and the encoder eliminates the relevant information

524
00:38:53,440 --> 00:38:55,600
that is not useful for anything.

525
00:38:55,600 --> 00:39:01,800
Okay, so for example, if I had a video of this, if I was shooting a video of

526
00:39:01,800 --> 00:39:05,680
the room here and then panning the camera and asking a system to predict

527
00:39:05,680 --> 00:39:10,960
what's the rest of the room, it would probably predict that the rest of

528
00:39:10,960 --> 00:39:17,200
the room looks like the initial part that there'd be a lot of people in

529
00:39:17,200 --> 00:39:25,920
different seats, but it couldn't predict your age, gender, hairstyle, clothing,

530
00:39:27,200 --> 00:39:31,800
or the texture, precise texture of the floor or things like that, right?

531
00:39:31,800 --> 00:39:36,440
So there's details that cannot possibly be predicted and one way to avoid

532
00:39:36,440 --> 00:39:40,200
predicting them is to basically eliminate that information from the variable to be

533
00:39:40,200 --> 00:39:42,760
predicted through an encoder.

534
00:39:42,760 --> 00:39:45,680
So that's a joint embedding architecture or predictive architecture because it has

535
00:39:45,680 --> 00:39:46,680
a predictor.

536
00:39:46,680 --> 00:39:51,040
Now there's an issue with this thing, which is that if you train a system with,

537
00:39:51,040 --> 00:39:54,600
let's say, a piece of video and the following piece of video and you just train

538
00:39:54,600 --> 00:39:59,200
it to minimize the prediction error, you train the whole thing, it collapses.

539
00:39:59,200 --> 00:40:04,680
It collapses, basically the encoders ignore the inputs, they produce constant

540
00:40:04,680 --> 00:40:09,000
vectors for SX and SY and the predictor just needs to map SX to SY and it's a

541
00:40:09,000 --> 00:40:13,280
constant, so it's super easy, okay, bad.

542
00:40:13,280 --> 00:40:17,120
So the question now is how do we prevent this from happening?

543
00:40:17,120 --> 00:40:18,640
How do we prevent it collapse?

544
00:40:18,640 --> 00:40:23,000
It doesn't happen with generative models because they can't collapse.

545
00:40:23,000 --> 00:40:27,280
So there are three flavors of those joint embedding architectures, a simple one

546
00:40:27,280 --> 00:40:31,440
where you're basically trying to make the two representation of SX and SY identical.

547
00:40:31,440 --> 00:40:36,640
So for example, X and Y are two different views of the same scene and you want SX

548
00:40:36,640 --> 00:40:40,600
to represent the content of the scene, so it doesn't matter where you look it from.

549
00:40:40,600 --> 00:40:43,160
You just want to make the representations equal.

550
00:40:43,160 --> 00:40:45,640
When the encoders are identical, this is called a Syme's network.

551
00:40:45,640 --> 00:40:50,480
This is another idea that goes back to the early 90s.

552
00:40:50,480 --> 00:40:53,640
You have deterministic joint embedding architectures and then you have joint

553
00:40:53,640 --> 00:40:58,080
predictive architectures that may be non-deterministic where the predictor

554
00:40:58,080 --> 00:41:02,800
function has a latent variable that could be drawn from a distribution or taken

555
00:41:02,800 --> 00:41:06,720
in a set that would allow that system to make multiple predictions if necessary.

556
00:41:10,720 --> 00:41:15,960
Now, we have to ask ourselves the question of how do we train those things?

557
00:41:15,960 --> 00:41:21,560
And I'm going to use a symbolism here where that I've used the rectangles

558
00:41:21,560 --> 00:41:27,480
and squares of cost functions, energy terms, the circles of variables,

559
00:41:27,480 --> 00:41:31,320
observed or not, and those symbols here are deterministic functions.

560
00:41:31,320 --> 00:41:35,480
Imagine a neural net, okay, trainable.

561
00:41:35,480 --> 00:41:39,720
We may have to hardwire some cost functions in the system to have it,

562
00:41:39,720 --> 00:41:43,600
to drive it to focus on aspects of the input that are important.

563
00:41:43,600 --> 00:41:48,200
So that's the purpose of that C cost function at the top.

564
00:41:48,200 --> 00:41:53,200
Okay, but to explain how to train those things,

565
00:41:53,200 --> 00:41:57,120
I'm going to have to explain a little bit what energy based models is about because

566
00:41:57,120 --> 00:42:00,720
the classical kind of probabilistic modeling in machine learning kind of goes

567
00:42:00,720 --> 00:42:04,520
at the window when we use the joint embedding architectures.

568
00:42:04,520 --> 00:42:05,640
So what's an energy based model?

569
00:42:05,640 --> 00:42:10,280
Energy based model is a learning system that captures the dependency between

570
00:42:10,280 --> 00:42:14,400
two sets of variable x and y through an energy function that is supposed to take

571
00:42:14,400 --> 00:42:19,560
low values, low energies around data, training samples.

572
00:42:19,560 --> 00:42:23,480
So imagine those black dots are training samples.

573
00:42:23,480 --> 00:42:27,680
You want that energy function f of x, y to take low values around the training

574
00:42:27,680 --> 00:42:30,400
samples and then higher values outside.

575
00:42:30,600 --> 00:42:33,280
And that system will capture the dependencies between x and y.

576
00:42:33,280 --> 00:42:36,680
If I give you a value of x and I ask you what can be the possible values for y,

577
00:42:36,680 --> 00:42:40,000
you're going to tell me, well, it's either this or that or maybe that other thing at

578
00:42:40,000 --> 00:42:41,400
the top.

579
00:42:41,400 --> 00:42:45,640
Okay, so it's not a mapping from x to y, it's an implicit function.

580
00:42:45,640 --> 00:42:48,960
And by figuring out what value of y minimizes the energy function,

581
00:42:48,960 --> 00:42:50,200
you can do inference.

582
00:42:50,200 --> 00:42:54,080
You can infer y, possibly, but you don't necessarily have to do that.

583
00:42:55,600 --> 00:42:56,680
So that's energy based model.

584
00:42:56,680 --> 00:43:01,040
It's kind of a weaker form of modeling than probabilistic modeling.

585
00:43:01,040 --> 00:43:04,960
And so now the learning problem becomes how do you train this energy function,

586
00:43:04,960 --> 00:43:10,480
which is going to be some big neural net, so that the energy takes low value around

587
00:43:10,480 --> 00:43:12,760
the training samples and high values outside.

588
00:43:12,760 --> 00:43:15,520
If you're not careful, you're going to get a collapse so that the same type of

589
00:43:15,520 --> 00:43:18,600
collapse I was telling you about before, if you just pull down the energy of the

590
00:43:18,600 --> 00:43:22,960
training samples, minimize the prediction error in this joint invading architecture,

591
00:43:22,960 --> 00:43:24,520
you're going to get zero energy for everything.

592
00:43:24,520 --> 00:43:27,200
It's not a good way to capture the dependencies.

593
00:43:27,200 --> 00:43:29,280
You have two classes of methods, contrastive methods.

594
00:43:29,280 --> 00:43:33,040
So contrastive methods consist in generating those green points,

595
00:43:33,040 --> 00:43:38,000
which are outside the region of data, and then push the energy up while you push

596
00:43:38,000 --> 00:43:40,120
down on the energy of the data points.

597
00:43:40,120 --> 00:43:45,040
Okay, so that's going to create a groove in the energy surface, and

598
00:43:45,040 --> 00:43:47,320
the system will have captured the dependency between x and y.

599
00:43:48,520 --> 00:43:51,520
But there's an alternative here, which is regularized methods,

600
00:43:51,520 --> 00:43:56,000
where the point of those methods is to minimize the volume of space that can take

601
00:43:56,000 --> 00:44:00,280
low energy, so that when you push down on the energy of data points,

602
00:44:02,560 --> 00:44:07,040
the rest of the space takes higher energy because there is only a small amount of,

603
00:44:07,040 --> 00:44:09,520
a small region of low energy to go around.

604
00:44:09,520 --> 00:44:11,480
So those are the two classes of methods.

605
00:44:11,480 --> 00:44:15,960
Every method you ever heard of in machine learning can be viewed as one of those two.

606
00:44:15,960 --> 00:44:21,120
Most probabilistic methods actually belong to the contrastive category.

607
00:44:22,480 --> 00:44:27,400
Anything that uses Monte Carlo sampling, for example, is contrastive.

608
00:44:27,400 --> 00:44:30,680
And then things like sparse coding and k-means and

609
00:44:30,680 --> 00:44:33,760
things like that are more on the regularized method side of things.

610
00:44:35,880 --> 00:44:39,320
Okay, so I'm asking you to do four things.

611
00:44:39,320 --> 00:44:44,200
Abandoned generative models in favor of the joint embedding architectures, right?

612
00:44:44,200 --> 00:44:47,280
So generative models are the most popular thing at the moment.

613
00:44:47,280 --> 00:44:53,760
Forget about it, at least if you're interested in getting to the next step in AI.

614
00:44:53,760 --> 00:44:56,600
Abandoned probabilistic models, because if you have those joint embedding

615
00:44:56,600 --> 00:45:00,360
architectures, you cannot actually use it to derive a pure y given x.

616
00:45:02,160 --> 00:45:04,200
The only thing you can use is sort of energy-based view.

617
00:45:05,920 --> 00:45:08,600
Abandoned contrastive methods in favor of those regularized methods,

618
00:45:08,600 --> 00:45:11,160
which I'll talk about a bit more.

619
00:45:11,160 --> 00:45:13,480
And then something I've said for many years now, abandoned reinforcement

620
00:45:13,480 --> 00:45:14,600
modeling because it's too inefficient.

621
00:45:17,360 --> 00:45:19,880
So those are some of the pillars of machine learning.

622
00:45:21,000 --> 00:45:25,800
And I realize this is not a very popular opinion here, but okay.

623
00:45:25,800 --> 00:45:27,640
So what about those regularized methods?

624
00:45:27,640 --> 00:45:28,720
I'm just going to give you one example.

625
00:45:28,720 --> 00:45:29,480
There's a whole bunch of them.

626
00:45:29,480 --> 00:45:35,360
There's like a dozen of them, but I'm just going to give you one called Vicreg.

627
00:45:35,360 --> 00:45:41,280
And the basic idea of it is to prevent those representations from collapsing.

628
00:45:41,280 --> 00:45:44,040
We're going to use a criterion that attempts to maximize the information

629
00:45:44,040 --> 00:45:45,880
content that comes out of those representations.

630
00:45:47,320 --> 00:45:50,240
Okay, so we're going to measure the information content in some way, and

631
00:45:50,240 --> 00:45:53,280
then maximize the information content or minimize the negative information content.

632
00:45:54,480 --> 00:45:57,160
We're going to do this for both SX and SY.

633
00:45:57,160 --> 00:45:59,480
We're also going to minimize the prediction error.

634
00:45:59,480 --> 00:46:01,200
And if we have a latent variable, we're going to have to minimize

635
00:46:01,200 --> 00:46:03,240
the information content of that latent variable.

636
00:46:03,240 --> 00:46:05,160
I can't explain why, because it would take too long.

637
00:46:05,160 --> 00:46:09,640
But you have to do that also to prevent another type of collapse.

638
00:46:09,640 --> 00:46:12,040
I'm going to focus on how you do that.

639
00:46:12,040 --> 00:46:16,760
So the sad news is we don't have good ways to measure information content, or

640
00:46:16,760 --> 00:46:21,400
we don't have any good ways to estimate lower bounds on information content,

641
00:46:21,400 --> 00:46:25,000
so that if we push up on this lower bound, the information content will go up.

642
00:46:25,000 --> 00:46:27,320
We only have upper bounds for information content.

643
00:46:27,320 --> 00:46:30,640
So we're going to do a very stupid thing, which is push up on the upper bound of

644
00:46:30,640 --> 00:46:33,760
information content, and hope the actual information content will follow.

645
00:46:35,840 --> 00:46:36,280
And it works.

646
00:46:37,280 --> 00:46:44,800
So, there's a simple way to prevent the encoder from completely collapsing.

647
00:46:44,800 --> 00:46:48,400
Which is to insist that every variable in SX, SX is a vector.

648
00:46:48,400 --> 00:46:51,720
And you insist that every variable, as measured over a batch,

649
00:46:51,720 --> 00:46:54,960
has a standard deviation that is at least one.

650
00:46:54,960 --> 00:46:57,680
Okay, so this is the cost that you see at the top here.

651
00:46:59,200 --> 00:47:03,120
Measure the standard deviation of each component of SX, and

652
00:47:03,120 --> 00:47:06,200
put it in a hinge loss so that the standard deviation is at least one.

653
00:47:07,320 --> 00:47:09,560
So that prevents the system from completely collapsing.

654
00:47:09,560 --> 00:47:16,200
But it can still cheat by making all the components of SX equal or correlated.

655
00:47:16,200 --> 00:47:20,840
So the second term says I want to minimize the off diagonal terms of

656
00:47:20,840 --> 00:47:24,520
the covariance matrix of those vectors measured over a batch, right?

657
00:47:24,520 --> 00:47:27,600
So I want pairs of variables to be uncorrelated.

658
00:47:28,960 --> 00:47:31,800
So basically, the collection of those two criterion says,

659
00:47:31,800 --> 00:47:36,400
if I measure the covariance matrix of those vectors, SX and SY, coming out over

660
00:47:36,400 --> 00:47:40,800
a batch, I want the covariance matrix to be as close to the identity as possible.

661
00:47:42,520 --> 00:47:45,880
There's a number of different methods that have been proposed to,

662
00:47:45,880 --> 00:47:47,960
that are kind of similar to this, Barlow-Twins.

663
00:47:47,960 --> 00:47:53,040
So this one is called Vic-Rag from my group at Meta in collaboration with Jean-Ponce.

664
00:47:54,840 --> 00:47:59,980
And then variations of it, but like similar methods from Berkeley in

665
00:48:00,940 --> 00:48:03,940
the E-Mise group at Berkeley called NCR squared.

666
00:48:03,940 --> 00:48:05,660
Yeah, maybe one minute.

667
00:48:05,660 --> 00:48:09,420
Yeah, so this works really well and

668
00:48:09,420 --> 00:48:16,060
I'm going to not bore you with tables of results that show you how well it works.

669
00:48:16,060 --> 00:48:21,860
Only to mention something else, which is another method to do this kind of

670
00:48:21,860 --> 00:48:27,180
self supervised running which is closer to this JEPA architecture called IJEPA.

671
00:48:27,180 --> 00:48:32,340
So this is for learning features for images without having to do the documentation.

672
00:48:32,340 --> 00:48:35,340
But basically it's for masking and this works amazingly well, it's very fast.

673
00:48:35,340 --> 00:48:38,180
It's a new method, paper is on archive.

674
00:48:39,980 --> 00:48:45,740
I don't have time to explain how it works, but basically you run an image

675
00:48:45,740 --> 00:48:49,260
through two encoders, one is the full image and

676
00:48:49,260 --> 00:48:53,860
the other one is sort of a masked image, partially masked image.

677
00:48:53,860 --> 00:48:56,460
You run them through the same encoder or very similar encoder and

678
00:48:56,460 --> 00:49:01,220
you try to predict or to predict the full feature representation of the full image

679
00:49:01,220 --> 00:49:04,100
from the representation obtained from the partial image.

680
00:49:04,100 --> 00:49:07,100
And just doing this produces really good features for images.

681
00:49:07,100 --> 00:49:11,420
You get really good performance on object recognition in images and stuff like that.

682
00:49:11,420 --> 00:49:14,020
Again, tables that show you that's true.

683
00:49:14,020 --> 00:49:18,820
But I'm coming to the end, so the reason for

684
00:49:18,820 --> 00:49:21,140
training those JEPA is to build world models.

685
00:49:21,140 --> 00:49:23,060
So architectures are this type.

686
00:49:23,060 --> 00:49:26,180
So this is a JEPA, but it's also a world model.

687
00:49:26,180 --> 00:49:29,140
That, given an observation about the state of the world,

688
00:49:29,140 --> 00:49:32,980
is going to be able to enter an action or imagined action in latent variables.

689
00:49:32,980 --> 00:49:34,820
It's going to predict what's going to happen next in the world.

690
00:49:35,900 --> 00:49:39,380
And once the time passes by, we're going to observe what happens and

691
00:49:39,380 --> 00:49:43,260
then perhaps adjust our system to train.

692
00:49:43,260 --> 00:49:47,500
But we want to use a hierarchical version of this where we can have a higher

693
00:49:47,500 --> 00:49:52,540
level, higher abstraction, higher level of abstraction representation that will

694
00:49:52,540 --> 00:49:54,620
allow us to make predictions further in the future.

695
00:49:55,580 --> 00:49:59,100
Okay, I can't tell you the details of how I'm going to get to the train station,

696
00:49:59,100 --> 00:50:02,860
but I know I'm going to have to be at the train station around 4 PM.

697
00:50:02,860 --> 00:50:06,100
Okay, so that's the high level.

698
00:50:06,100 --> 00:50:09,460
And we have early experiments with sort of various complicated neural net

699
00:50:09,460 --> 00:50:12,820
architectures which I'm not going to detail to train from video,

700
00:50:12,820 --> 00:50:16,180
try to predict basically what's going to happen in the video using warping and

701
00:50:16,180 --> 00:50:17,620
stuff like that and it works really well.

702
00:50:17,620 --> 00:50:22,700
But in the end, what we'll have is a hierarchical system from which we can do

703
00:50:22,700 --> 00:50:26,780
a hierarchical planning and then we'll have been trained to predict what's

704
00:50:26,780 --> 00:50:30,020
going to happen in the world as a consequence of actions or

705
00:50:30,020 --> 00:50:32,820
latent variables that we can observe, that we can infer.

706
00:50:34,940 --> 00:50:38,620
And those systems will be able to plan and reason and

707
00:50:38,620 --> 00:50:41,740
will be controllable because the behavior is entirely controlled by the cost

708
00:50:41,740 --> 00:50:43,380
functions we ask you to minimize.

709
00:50:44,500 --> 00:50:50,740
And so much more controllable than current LNMs and that's pretty much the end.

710
00:50:50,740 --> 00:50:53,820
So cell supervised learning is really the ticket.

711
00:50:53,820 --> 00:50:58,580
Handling and certainty can be done with this energy-based model method and

712
00:50:58,580 --> 00:51:02,500
using the joint embedding architecture that allows us to avoid predicting all

713
00:51:02,500 --> 00:51:04,100
the details that are irrelevant about the world.

714
00:51:06,020 --> 00:51:08,700
Learning world models from observation and interaction and

715
00:51:08,700 --> 00:51:13,540
then reasoning and planning is done by basically gradient-based minimization

716
00:51:13,540 --> 00:51:15,300
with respect to actions.

717
00:51:15,300 --> 00:51:16,460
And that's it, thank you very much.

718
00:51:16,460 --> 00:51:36,020
Thank you, John, for the great talk.

719
00:51:36,020 --> 00:51:40,260
Now we'll have the second part, which is the Fireside Chat between John and

720
00:51:40,260 --> 00:51:42,260
Osama, so please.

721
00:51:42,260 --> 00:51:46,220
Thank you very much, John.

722
00:51:46,220 --> 00:51:50,940
It was actually truly inspirational because it is definitely different

723
00:51:50,940 --> 00:51:54,420
than your typical machine learning talk, so I enjoyed that.

724
00:51:54,420 --> 00:51:57,860
Well, to you to throw away all the basic pillars of machine learning, so yes.

725
00:52:00,420 --> 00:52:04,060
So I've collected a bunch of questions, some coming from the audience,

726
00:52:04,060 --> 00:52:07,420
some coming from our institute and our faculty.

727
00:52:07,420 --> 00:52:12,260
And we'll try to go through them in 20 minutes or whatever we can cover.

728
00:52:13,260 --> 00:52:17,580
Normally, I would commit to answering every question on social media, but

729
00:52:17,580 --> 00:52:22,220
because we got 150 questions, I'm afraid to commit my time or yours to this at

730
00:52:22,220 --> 00:52:24,100
this point, but we'll try our best.

731
00:52:25,540 --> 00:52:26,940
So I'll start with my first question.

732
00:52:28,260 --> 00:52:31,780
It's been a long-standing wisdom in statistical inference and

733
00:52:31,780 --> 00:52:37,700
probabilistic reasoning that when the number of parameters of a model gets

734
00:52:37,700 --> 00:52:42,100
large enough, you kind of lose your ability to generalize and

735
00:52:42,100 --> 00:52:45,740
you start just memorizing data, and we all know that that's no good.

736
00:52:45,740 --> 00:52:49,980
That's just too detailed, the bias variance trade off.

737
00:52:49,980 --> 00:52:56,540
But somehow, deep learning seems to have broken through this barrier.

738
00:52:56,540 --> 00:53:01,300
When we went from regular neural nets to the deep nets, and

739
00:53:01,300 --> 00:53:07,220
is there an intuition or understanding today as to why this is working in

740
00:53:07,220 --> 00:53:10,980
LLMs with hundreds of billions and now trillions of parameters.

741
00:53:10,980 --> 00:53:14,580
Right, well, the fact that it is working,

742
00:53:14,580 --> 00:53:19,180
that you can train a ridiculously over-sized neural net, and

743
00:53:19,180 --> 00:53:26,460
it will still work reasonably and generalize is dumb-founding.

744
00:53:26,460 --> 00:53:29,860
So much that it contradicts every single thing that has been written in every

745
00:53:29,860 --> 00:53:31,700
statistical textbook.

746
00:53:31,700 --> 00:53:34,980
That you should never have more parameters than you have training samples, right?

747
00:53:35,020 --> 00:53:37,580
If you're fitting a polynomial or something like this.

748
00:53:37,580 --> 00:53:40,380
But we knew experimentally, even in the late 80s and early 90s,

749
00:53:40,380 --> 00:53:42,740
that you could make those neural nets pretty big.

750
00:53:42,740 --> 00:53:45,860
And even if you didn't have a huge amount of training data,

751
00:53:45,860 --> 00:53:47,500
it would still work pretty well.

752
00:53:47,500 --> 00:53:49,460
There was just no theoretical explanation.

753
00:53:49,460 --> 00:53:52,700
So the theorists told us, you're wrong, you're stupid.

754
00:53:53,980 --> 00:53:57,380
This cannot possibly work, so I'm not gonna believe your results.

755
00:53:57,380 --> 00:54:01,540
And that's in part what made it very difficult to get neural nets

756
00:54:01,540 --> 00:54:05,660
accepted in the late 90s to the 2000s.

757
00:54:07,860 --> 00:54:11,140
But it turns out there is a phenomenon that has since been named

758
00:54:11,140 --> 00:54:16,140
double descent, which is that if you increase the number of parameters in

759
00:54:16,140 --> 00:54:22,180
a model for a constant size training set, your training error,

760
00:54:22,180 --> 00:54:24,780
of course, is gonna go down, right, to zero, probably.

761
00:54:26,100 --> 00:54:30,460
But your test error is first gonna go down, go through a minimum, and

762
00:54:30,500 --> 00:54:35,060
then go up when you start having parameters,

763
00:54:35,060 --> 00:54:39,020
a number of parameters that is commensurate with the number of samples that you have.

764
00:54:39,020 --> 00:54:43,820
Okay, so that's when the model starts to be over parameterized, and it goes up.

765
00:54:45,180 --> 00:54:48,540
But here is the thing, if you keep going up, if you keep making the model more

766
00:54:48,540 --> 00:54:52,180
complex, the tester will go down again.

767
00:54:52,180 --> 00:54:54,020
It will go through a maximum and then go down again.

768
00:54:54,020 --> 00:54:56,620
That's called the double descent phenomenon, nowadays.

769
00:54:56,620 --> 00:55:01,980
And it will do this if you regularize the parameters somehow.

770
00:55:01,980 --> 00:55:06,060
You don't necessarily need to regularize explicitly because neural nets have some

771
00:55:06,060 --> 00:55:08,180
sort of implicit regularization in them.

772
00:55:08,180 --> 00:55:13,980
But you see this phenomenon, even works if you fit a polynomial, right?

773
00:55:13,980 --> 00:55:19,700
Fit a 10 degree polynomial with 11 data points.

774
00:55:20,820 --> 00:55:23,180
And your fit will be horrible, right?

775
00:55:23,180 --> 00:55:25,820
Because the polynomial has to go to every single point and

776
00:55:25,820 --> 00:55:27,540
it's gonna go wild in between.

777
00:55:27,540 --> 00:55:30,260
But if you increase the degree of the polynomial to something like 20 or

778
00:55:30,260 --> 00:55:33,860
30, and you regularize the coefficient, your error goes down again,

779
00:55:33,860 --> 00:55:38,980
your test error goes down again, the fitted polynomial goes through every point.

780
00:55:38,980 --> 00:55:44,340
But it's less irregular than with just degree 10.

781
00:55:44,340 --> 00:55:48,540
So this existed all along, it's just that people didn't realize it was a thing,

782
00:55:48,540 --> 00:55:52,020
or at least people who were not practitioners of neural nets who

783
00:55:52,020 --> 00:55:53,660
had realized this was a thing.

784
00:55:53,660 --> 00:55:55,700
So do we have any explanation why this is a thing?

785
00:55:57,180 --> 00:56:01,820
So there's a lot of conjectures, there is some theoretical work.

786
00:56:01,820 --> 00:56:04,820
Some people claim it's about the dynamics of gradient descent.

787
00:56:04,820 --> 00:56:08,380
There is some sort of implicit self regularization in neural nets that occurs.

788
00:56:09,660 --> 00:56:13,140
Whereby the system kind of recruits just a number of virtual

789
00:56:13,140 --> 00:56:15,140
parameters that it needs somehow.

790
00:56:16,620 --> 00:56:19,780
Some say it's regularization due to stochastic gradient.

791
00:56:19,780 --> 00:56:22,100
So stochastic gradient descent, which is noisy.

792
00:56:22,100 --> 00:56:28,260
And so perhaps that forces the system to find robust minima in

793
00:56:28,260 --> 00:56:32,900
the objective, in the loss, that generalize better.

794
00:56:34,340 --> 00:56:36,100
It's not clear, there's a bunch of different things.

795
00:56:36,100 --> 00:56:39,540
Yeah, definitely one of the mysteries that keep us interested.

796
00:56:41,300 --> 00:56:45,500
This question comes from Raman Chandrasekharan or

797
00:56:45,500 --> 00:56:49,260
Chandra, who's one of our senior research scientists in Seattle.

798
00:56:50,140 --> 00:56:56,660
How long before LLM, and maybe, I don't know, models in general,

799
00:56:56,660 --> 00:57:01,340
can genuinely start saying, I don't know the answer to this question.

800
00:57:01,340 --> 00:57:05,700
As opposed to attempting to guess the right autocomplete anyway,

801
00:57:05,700 --> 00:57:07,060
because that's what it's programmed to do.

802
00:57:08,060 --> 00:57:12,700
Yeah, so I don't think current LLMs can really do this at the moment.

803
00:57:12,700 --> 00:57:15,900
I think it's probably possible with architectures, the type that I show.

804
00:57:15,900 --> 00:57:21,300
Because if there are no good minima to the objective that the system is

805
00:57:21,300 --> 00:57:25,020
attempting to minimize to produce it, it's output, it's gonna say, well,

806
00:57:25,020 --> 00:57:29,180
I found this thing, it seems to be minimizing this objective, but not very well.

807
00:57:29,180 --> 00:57:31,740
So probably it's not the right answer you were looking for.

808
00:57:33,180 --> 00:57:37,940
Or by the shape of the minimum, of this energy minimum, perhaps, you could say,

809
00:57:37,940 --> 00:57:42,100
like if it's really a sharp minimum, then that's the one answer that

810
00:57:42,100 --> 00:57:44,500
corresponds to the question.

811
00:57:44,500 --> 00:57:48,180
If it's kind of a shadow minimum, maybe there are multiple answers that are possible.

812
00:57:48,180 --> 00:57:55,180
So you might be able to attribute, to map energy levels to,

813
00:57:55,180 --> 00:57:59,340
of different answers to a confidence level.

814
00:58:02,620 --> 00:58:06,980
To, this is a question from me, I guess.

815
00:58:06,980 --> 00:58:12,340
Two aspects of critical importance to,

816
00:58:12,340 --> 00:58:17,620
let's say, GPT or large language models that are not talked about a lot by

817
00:58:17,620 --> 00:58:22,140
the companies who do them are data curation.

818
00:58:22,140 --> 00:58:27,260
Getting that clean data, that balanced data, that representative data,

819
00:58:27,260 --> 00:58:34,300
which by the way, counter to popular belief, open AI spent a lot of its money

820
00:58:34,300 --> 00:58:39,660
on curating just that right corpus so that they can do the training reliably.

821
00:58:39,660 --> 00:58:46,460
And the second part, which is something we're big believers in at the Institute for

822
00:58:46,460 --> 00:58:50,820
experiential AI, experiential AI stands for AI with the human in the loop.

823
00:58:50,820 --> 00:58:54,740
Having that human intervention through relevance feedback,

824
00:58:54,740 --> 00:58:59,300
that we know now open AI is doing and has been doing.

825
00:58:59,300 --> 00:59:02,900
And some of the queries are actually taken over by humans at some point when

826
00:59:02,900 --> 00:59:04,740
they make enough errors to come back.

827
00:59:04,740 --> 00:59:07,660
But the good thing is they learn from them and we think that's a great practice.

828
00:59:08,660 --> 00:59:13,700
Why do you think the companies don't want to talk about the importance of the data

829
00:59:13,700 --> 00:59:15,220
and the importance of the human in the loop?

830
00:59:16,340 --> 00:59:17,700
I don't know if they don't want to talk about it.

831
00:59:17,700 --> 00:59:26,540
I mean, it's clearly very expensive to create data to produce a good LLM.

832
00:59:26,540 --> 00:59:32,460
But in my opinion, it's doomed to failure in the long run for two reasons.

833
00:59:32,580 --> 00:59:41,740
The first one is the curation requires going through this enormous amount of data that

834
00:59:41,740 --> 00:59:42,980
you want to train the system on.

835
00:59:42,980 --> 00:59:47,140
And any data you eliminate, it's less training data for your model.

836
00:59:48,740 --> 00:59:52,660
But the second thing is even with human feedback,

837
00:59:52,660 --> 00:59:58,660
human feedback that rate different answers or fine tune the system for

838
00:59:58,660 --> 01:00:02,460
certain question and answers, sort of manually curated.

839
01:00:04,740 --> 01:00:09,620
If you want those systems ultimately to be the repository of all human knowledge,

840
01:00:10,740 --> 01:00:14,980
the dimension of that space of all human knowledge is enormous.

841
01:00:14,980 --> 01:00:18,180
And you're not going to do it by paying a few thousand people in Kenya or

842
01:00:18,180 --> 01:00:21,860
India rating answers.

843
01:00:21,860 --> 01:00:28,500
You're going to have to do it with millions of volunteers that find

844
01:00:28,500 --> 01:00:32,180
the system for all possible questions that might possibly be asked.

845
01:00:32,180 --> 01:00:38,100
And those volunteers will have to be vetted in the way Wikipedia is being done, right?

846
01:00:38,100 --> 01:00:45,220
So think of LLMs in the long run as a version of Wikipedia plus your favorite

847
01:00:45,220 --> 01:00:50,500
newspapers plus the scientific literature plus everything, but you can talk to it.

848
01:00:50,500 --> 01:00:52,700
You don't have to read articles, you can just talk to it.

849
01:00:53,660 --> 01:00:58,460
And so if it's supposed to become the repository of all human knowledge,

850
01:00:58,460 --> 01:01:04,020
the thing it's been trained to do will have to be curated by

851
01:01:04,020 --> 01:01:10,220
quite sourcing the way Wikipedia is to cover all the possible things that

852
01:01:10,220 --> 01:01:11,980
may be covered.

853
01:01:11,980 --> 01:01:19,180
This is a very strong argument for having open source based models for LLMs.

854
01:01:19,260 --> 01:01:24,540
So in my opinion, the future is inevitably going to be that you're going to

855
01:01:24,540 --> 01:01:29,740
have a small number of open source based LLMs that are not trained for

856
01:01:29,740 --> 01:01:34,780
any particular application, they're trained on enormous amounts of data

857
01:01:34,780 --> 01:01:35,900
that requires a lot of money.

858
01:01:35,900 --> 01:01:38,380
So you're not going to have 25 of them, you're going to have two or three.

859
01:01:39,660 --> 01:01:42,860
And then actual applications are going to be built on top of it by

860
01:01:42,860 --> 01:01:46,620
finding those systems for particular vertical applications.

861
01:01:46,620 --> 01:01:47,340
That's the future.

862
01:01:48,060 --> 01:01:53,020
Sadly, in the industry, there are people who are lobbying governments to

863
01:01:53,020 --> 01:01:57,820
actually make the open sourcing of large scale LLM illegal.

864
01:01:59,260 --> 01:02:05,340
What they're worried about is potential misuse of LLMs by bad actors,

865
01:02:06,460 --> 01:02:08,300
potential users.

866
01:02:09,900 --> 01:02:13,820
So some people in the US, for example, are worried, oh, if we open source our LLMs,

867
01:02:13,820 --> 01:02:17,740
you know China and North Korea and Iran will put their hands on it and that's

868
01:02:17,740 --> 01:02:18,300
going to be bad.

869
01:02:20,940 --> 01:02:24,380
And then some people are worried that the real powerful LLMs are going to be

870
01:02:24,380 --> 01:02:28,860
super intelligent and destroy humanity, which I think is preposterous,

871
01:02:30,220 --> 01:02:33,100
even though some of my friends that I respect actually believe this.

872
01:02:34,220 --> 01:02:40,140
So I think it would be really, really bad if those lobbying attempts succeed.

873
01:02:40,860 --> 01:02:44,860
I'm very much in favor of a future with open based models.

874
01:02:45,580 --> 01:02:48,220
And there's going to be bad actors, but there's going to be countermeasures

875
01:02:48,220 --> 01:02:49,020
against them.

876
01:02:49,020 --> 01:02:57,100
It's going to be, you know, or powerful good AI cop against their nefarious AI, essentially.

877
01:03:00,060 --> 01:03:03,660
So let's shift to this trend.

878
01:03:03,660 --> 01:03:09,340
And this is, I've merged a question from Jimmy Shanahan from our AI solutions hub

879
01:03:09,900 --> 01:03:14,060
with questions from Tomo Lasovic and Ken Church at EAI.

880
01:03:15,660 --> 01:03:22,140
The trend nowadays seems to be heading towards bigger is better, more compute, more parameters.

881
01:03:23,580 --> 01:03:29,420
There's been some studies even suggesting that by open AI themselves that they're moving at a

882
01:03:29,420 --> 01:03:35,660
pace faster than Moore's law, even though now they seem to be normalizing towards it,

883
01:03:35,660 --> 01:03:38,060
although Moore's law itself is slowing down.

884
01:03:38,300 --> 01:03:42,460
So the real question here is how long can this go on?

885
01:03:42,460 --> 01:03:44,380
And will we ask them, what do you think?

886
01:03:44,380 --> 01:03:48,220
I know that we may not have the final answer here, but it seems crazy.

887
01:03:48,220 --> 01:03:51,740
Like all you have to do is wait a few weeks and you hear about the next big model.

888
01:03:52,460 --> 01:03:56,220
Well, so actually in the last few months, you've seen a decrease in the size.

889
01:03:57,340 --> 01:04:03,420
So Lama, for example, the 13 billion version of Lama in terms of raw performance on standard

890
01:04:03,420 --> 01:04:08,380
benchmarks is actually better than GPT-3, which has 175 billion parameters.

891
01:04:08,940 --> 01:04:12,140
And so it's not clear that bigger is better.

892
01:04:12,140 --> 01:04:17,980
With the architecture I propose, I think you can get away with smaller systems that

893
01:04:17,980 --> 01:04:19,100
perform at least as well.

894
01:04:19,100 --> 01:04:22,700
The reason being that when you train in a current autoregressive LLM,

895
01:04:22,700 --> 01:04:27,100
you have to train it to not just accumulate knowledge, not just predict the next word,

896
01:04:27,100 --> 01:04:28,460
but also solve a lot of problems.

897
01:04:28,460 --> 01:04:35,020
So basically, know how to produce the right answer when you specify the question in the

898
01:04:35,020 --> 01:04:35,660
prompt.

899
01:04:35,660 --> 01:04:39,260
And so everything is wrapped into the weights of that single model.

900
01:04:39,260 --> 01:04:42,060
Whereas in the model I propose here, the architecture I propose,

901
01:04:42,780 --> 01:04:44,460
the word model is just a word model.

902
01:04:45,100 --> 01:04:49,980
The task is specified by the objective function, which may include the prompt.

903
01:04:49,980 --> 01:04:51,820
So it may include the representation of the prompt.

904
01:04:52,700 --> 01:04:55,100
And so you're separating different things.

905
01:04:55,100 --> 01:05:01,900
You're separating the inference procedure that produces the output from the word model,

906
01:05:01,900 --> 01:05:05,260
the sort of the mental model of the world that the system uses,

907
01:05:05,260 --> 01:05:08,140
from the task itself, which is specified by the objective.

908
01:05:08,140 --> 01:05:11,500
And you can probably get away with smaller networks for the same performance.

909
01:05:13,100 --> 01:05:19,900
But yes, I mean, there were a few years ago models by Google that had like a trillion

910
01:05:19,900 --> 01:05:20,380
parameters.

911
01:05:20,380 --> 01:05:23,900
There were basically multiple models that were stuck together with some sort of

912
01:05:25,740 --> 01:05:26,300
gating.

913
01:05:26,300 --> 01:05:30,700
Yeah, between them, they've kind of backpedaled on this a little bit.

914
01:05:30,700 --> 01:05:33,740
If you want the system to be practical, like to be used by everyone,

915
01:05:33,740 --> 01:05:35,740
you can't make them like a trillion parameters.

916
01:05:35,740 --> 01:05:37,260
Right now, it'd be just too expensive.

917
01:05:38,380 --> 01:05:40,460
So you have to minimize that size.

918
01:05:40,460 --> 01:05:45,820
Now you can run things like Lama 7 billion on a Mac.

919
01:05:47,340 --> 01:05:49,100
You know, you can run on a laptop.

920
01:05:49,100 --> 01:05:51,260
You can't train it on a laptop, but you can run it.

921
01:05:51,260 --> 01:05:51,500
Yes.

922
01:05:51,660 --> 01:05:59,660
So clearly, you believe you're advocating for a different view of what the machine learning

923
01:05:59,660 --> 01:06:03,820
and AI community should be doing as opposed to what they are doing today.

924
01:06:04,780 --> 01:06:06,380
That's the story of my career.

925
01:06:06,380 --> 01:06:06,620
Yes.

926
01:06:08,060 --> 01:06:10,380
And this question is coming from Ken Church.

927
01:06:11,420 --> 01:06:13,340
A former colleague from AT&T.

928
01:06:13,340 --> 01:06:14,620
From AT&T.

929
01:06:14,620 --> 01:06:17,660
He is at the Institute for AI in Silicon Valley.

930
01:06:18,620 --> 01:06:19,340
Do you believe?

931
01:06:21,660 --> 01:06:27,820
Well, I guess the question is, how long do you think it will take to pivot the field from

932
01:06:27,820 --> 01:06:30,300
where it is to where you would like it to be?

933
01:06:32,860 --> 01:06:34,780
Well, last time I tried, it took 15 years.

934
01:06:38,060 --> 01:06:41,340
If not more, actually, depending on how you count, it might have been 20.

935
01:06:42,300 --> 01:06:45,180
So I don't know.

936
01:06:45,180 --> 01:06:50,860
I think I see a phenomenon in kind of this is a sociology of science question.

937
01:06:51,420 --> 01:06:54,540
When there is something that seems to work, everybody gets excited about it.

938
01:06:54,540 --> 01:07:03,340
And it's a fashion trend type phenomenon where every paper written is about this trend.

939
01:07:03,340 --> 01:07:07,260
I saw this in computer vision back in the early to mid 2000.

940
01:07:07,260 --> 01:07:09,340
Everybody was working on boosting.

941
01:07:09,340 --> 01:07:12,780
That was the thing, you had to work on boosting for computer vision.

942
01:07:12,780 --> 01:07:21,180
And then someone in 2006 and 2005 came up with a different way of doing vision using dense

943
01:07:21,980 --> 01:07:26,620
features like sift and stuff like that using unsupervised running for a middle layer and then

944
01:07:26,620 --> 01:07:27,820
an SVM on top.

945
01:07:27,820 --> 01:07:29,420
All of a sudden, everybody was doing this.

946
01:07:30,540 --> 01:07:34,140
And then starting in 2013, everybody started using convolutional nets.

947
01:07:36,300 --> 01:07:37,740
That came from results.

948
01:07:37,740 --> 01:07:40,620
So now we are in a phase where everybody is focused on LLMs.

949
01:07:41,180 --> 01:07:43,980
And if you don't work on LLMs, nobody wants to talk to you.

950
01:07:45,820 --> 01:07:47,180
But it will change.

951
01:07:49,420 --> 01:07:50,940
So you think it's 15 years?

952
01:07:50,940 --> 01:07:52,700
No, I think it's more like five.

953
01:07:52,700 --> 01:07:57,340
Like I made that prediction that autoregressive LLMs will probably...

954
01:07:57,340 --> 01:07:59,420
Five years, that's true, yeah, they're doomed.

955
01:07:59,980 --> 01:08:01,740
Yeah, I mean, I might be wrong, obviously.

956
01:08:02,460 --> 01:08:04,300
We will hold you to that.

957
01:08:04,300 --> 01:08:06,380
I'll come back and revisit in five years.

958
01:08:06,380 --> 01:08:09,740
Maybe it's a wishful thinking, self-fulfilling prophecy perhaps.

959
01:08:11,260 --> 01:08:15,500
A question for something different here from Sam Scarpino,

960
01:08:15,500 --> 01:08:19,100
director of AI and Life Sciences at the Institute for Experiential AI.

961
01:08:19,980 --> 01:08:26,860
What are the biggest gaps on the education side for graduates of higher education in AI

962
01:08:27,420 --> 01:08:31,260
and in particular the new directions AI is taking?

963
01:08:31,260 --> 01:08:33,260
What do you think is missing?

964
01:08:34,220 --> 01:08:37,420
So I think what's missing...

965
01:08:37,420 --> 01:08:41,660
So it depends which major you're following.

966
01:08:44,620 --> 01:08:52,860
Most computer science curricula in the US are very weak in mathematics.

967
01:08:53,820 --> 01:08:58,540
The requirements for mathematics in a typical CS degree,

968
01:08:58,540 --> 01:09:01,020
the minimum requirement is very, very small, right?

969
01:09:01,020 --> 01:09:05,420
It's one course in discrete math and perhaps in algebra if you're lucky.

970
01:09:06,380 --> 01:09:09,180
Maybe a probability if you are courageous.

971
01:09:11,100 --> 01:09:12,460
But what about optimization?

972
01:09:12,460 --> 01:09:14,780
That would be something that would be very useful.

973
01:09:14,780 --> 01:09:20,140
And then there is courses in physics because the mathematics of inference

974
01:09:20,140 --> 01:09:26,220
and variational autoencoder and stuff like that, graphical models, etc.

975
01:09:26,220 --> 01:09:28,860
The mathematics of this is from statistical physics.

976
01:09:29,820 --> 01:09:34,060
And so if you have a choice between taking your course in, I don't know,

977
01:09:34,940 --> 01:09:38,860
mobile app programming or quantum mechanics, take quantum mechanics.

978
01:09:40,700 --> 01:09:41,340
I'm not kidding.

979
01:09:48,540 --> 01:09:55,340
This is a question that came from the audience and a few of the people at the Institute.

980
01:09:56,220 --> 01:10:01,100
Your thoughts on the current, you know, these recent congressional hearings where

981
01:10:02,780 --> 01:10:09,900
certainly seems like much of the testimony by some Altman was understandably self-serving.

982
01:10:09,900 --> 01:10:14,860
You know, they need to be allowed to compete and have their way of working protected.

983
01:10:15,420 --> 01:10:19,020
At the same time, he's encouraging the rest of the community to be open source.

984
01:10:19,660 --> 01:10:24,700
What would you have said to Congress?

985
01:10:24,700 --> 01:10:25,820
Have you been on those hearings?

986
01:10:28,220 --> 01:10:29,020
I was not invited.

987
01:10:30,860 --> 01:10:33,100
I was not invited to the White House either before that.

988
01:10:35,980 --> 01:10:45,420
So what I would have recommended is that if you want a vibrant ecosystem on top of current AI technology,

989
01:10:46,380 --> 01:10:53,180
you need to have sort of open source based models on top of which an industry can be built.

990
01:10:54,380 --> 01:10:59,660
And that industry will build vertical applications for particular domains

991
01:10:59,660 --> 01:11:00,940
on top of a base model.

992
01:11:00,940 --> 01:11:06,220
You don't want to have 25 companies selling 25 different base models

993
01:11:07,660 --> 01:11:08,780
and keep them closed source.

994
01:11:08,780 --> 01:11:12,540
If you want an industry to be built on top of it, the infrastructure has to be open.

995
01:11:13,260 --> 01:11:18,140
Because that's the only way to really sort of know what you're doing, essentially.

996
01:11:19,660 --> 01:11:22,140
And to have some control about your future, right?

997
01:11:22,140 --> 01:11:25,820
You can't just go like this and pray that.

998
01:11:25,820 --> 01:11:27,020
Unix versus Windows.

999
01:11:27,660 --> 01:11:32,060
Right, so if you go back to the history of the Internet, there was a similar story where

1000
01:11:33,340 --> 01:11:37,740
back in 1992 when the built Internet Al Gore started to figure out like what,

1001
01:11:37,740 --> 01:11:39,900
you know, how do we build the information superhighway?

1002
01:11:40,860 --> 01:11:46,140
They went to see, you know, the big communication companies like AT&T and AT&T told them,

1003
01:11:46,140 --> 01:11:47,420
oh, you know, leave it to us.

1004
01:11:47,420 --> 01:11:48,620
We'll build the stuff.

1005
01:11:48,620 --> 01:11:52,620
It's going to be, you know, ATM and ISD enter the home and blah, blah, blah.

1006
01:11:52,620 --> 01:11:56,140
It'll be wonderful and you'll have to pay, you know, $5 per hour.

1007
01:11:58,220 --> 01:11:59,420
And Al Gore said no.

1008
01:11:59,420 --> 01:12:03,820
He said we're going to make the, what was an ARPANET that became the Internet,

1009
01:12:04,060 --> 01:12:10,540
basically available to the public and delocalized and, you know, self,

1010
01:12:11,820 --> 01:12:15,500
basically open in terms of standard and no company is going to control it.

1011
01:12:16,060 --> 01:12:18,140
And that was a really, really good idea.

1012
01:12:18,140 --> 01:12:19,340
We can thank Al Gore for this.

1013
01:12:20,060 --> 01:12:21,900
The world can thank Al Gore, not just the U.S.

1014
01:12:27,660 --> 01:12:28,780
He did invent the Internet.

1015
01:12:29,500 --> 01:12:33,980
And then a similar story happened several years later when people started to realize

1016
01:12:33,980 --> 01:12:39,340
that you could use, you know, graphic browsers like Mosaic and Netscape and stuff like that,

1017
01:12:39,340 --> 01:12:43,340
right, when the World Wide Web became popular.

1018
01:12:44,540 --> 01:12:47,900
So there was a war between Sun Microsystems and Microsoft.

1019
01:12:47,900 --> 01:12:52,060
Sun Microsystems said, oh, we're going to sell you servers running Solaris,

1020
01:12:52,060 --> 01:12:56,300
the version of Unix, with, you know, our web server infrastructure and Java.

1021
01:12:57,180 --> 01:12:59,100
And you're going to be able to build, like, anything you want.

1022
01:12:59,980 --> 01:13:05,020
Microsoft said no, it's going to be Windows NT with the IIT web server and the ASP

1023
01:13:05,980 --> 01:13:11,580
website, you know, server-side protocol framework, whatever.

1024
01:13:12,380 --> 01:13:13,100
They both lost.

1025
01:13:14,220 --> 01:13:22,540
Sun Microsystem went bankrupt, was sold for parts to Oracle, and Microsoft essentially

1026
01:13:22,540 --> 01:13:25,100
exited the market.

1027
01:13:25,740 --> 01:13:28,460
One was Linux and Apache, open source.

1028
01:13:28,460 --> 01:13:34,540
And the reason is because it's such an essential basic infrastructure that it has to be open.

1029
01:13:35,980 --> 01:13:42,220
It progresses faster if it's open, and it's more reliable, it's more secure.

1030
01:13:42,220 --> 01:13:43,580
I mean, there's all the advantages.

1031
01:13:43,580 --> 01:13:46,460
And, you know, it's easier for startups to build on top of it.

1032
01:13:46,460 --> 01:13:51,740
So in the future, we're going to see AI systems as basic infrastructure.

1033
01:13:52,700 --> 01:13:56,860
All of our interactions ten years from now with the digital world will be through

1034
01:13:57,580 --> 01:14:02,620
an intelligent virtual agent that will be with us all the time.

1035
01:14:02,620 --> 01:14:06,460
It's like every one of us will have a staff of intelligent people working for us.

1036
01:14:07,820 --> 01:14:08,540
Okay?

1037
01:14:08,540 --> 01:14:12,540
We shouldn't be threatened by the fact that those things will be smarter than us.

1038
01:14:12,540 --> 01:14:16,780
Like everybody that, you know, is working with me at fair is smarter than me.

1039
01:14:16,780 --> 01:14:19,340
So I don't feel threatened by that.

1040
01:14:19,340 --> 01:14:22,460
You're not a very good manager if you're threatened by people who are smarter than you.

1041
01:14:22,620 --> 01:14:28,460
So your purpose actually should be to hire people, only people who are smarter than you.

1042
01:14:28,460 --> 01:14:33,100
But anyway, so we're going to have those intelligent systems that are going to be

1043
01:14:33,100 --> 01:14:36,060
under control that are going to help us, you know, daily lives.

1044
01:14:36,060 --> 01:14:41,420
And we need those systems to be open because if it's kind of a closed system controlled by

1045
01:14:41,420 --> 01:14:46,220
some company in California, it's going to be able to control our entire

1046
01:14:46,380 --> 01:14:50,620
knowledge and data diet.

1047
01:14:51,580 --> 01:14:53,100
And that's just too dangerous.

1048
01:14:53,100 --> 01:14:55,500
And it's not necessary.

1049
01:14:55,500 --> 01:15:00,060
It's necessary for a search engine or a social network because it has to be centralized for

1050
01:15:00,060 --> 01:15:00,700
various reasons.

1051
01:15:00,700 --> 01:15:04,220
But for an agent like this, it could run on your local device.

1052
01:15:04,220 --> 01:15:05,500
It could run on your laptop.

1053
01:15:05,500 --> 01:15:09,740
You don't have to talk necessarily with big servers in California.

1054
01:15:09,740 --> 01:15:14,460
You don't want to give all your, you know, deepest secrets to that.

1055
01:15:14,460 --> 01:15:19,020
So it's going to have to be an open fact form for that reason.

1056
01:15:20,140 --> 01:15:25,580
If nothing else, governments around the world are going to insist that this is the case.

1057
01:15:25,580 --> 01:15:33,340
So that's why I would tell Congress, make it so that, like, don't ban open source

1058
01:15:34,140 --> 01:15:35,020
LLMs.

1059
01:15:35,020 --> 01:15:36,460
They're not going to destroy humanity.

1060
01:15:37,740 --> 01:15:40,860
Yeah, they're going to be bad actors, but you know, you can have countermeasures

1061
01:15:41,660 --> 01:15:43,420
and make it open.

1062
01:15:43,420 --> 01:15:45,420
It's the only way to make it safe.

1063
01:15:53,420 --> 01:15:56,220
I'll ask, we'll make this a quick question with a quick answer.

1064
01:15:56,220 --> 01:16:00,140
And then I know we have some questions live, so we'll switch to those.

1065
01:16:04,940 --> 01:16:08,220
In a way, you kind of answered this question when you said LLMs are doomed.

1066
01:16:08,220 --> 01:16:17,180
But if LLMs were to become perfect, at least in language, would that ever give us insight

1067
01:16:17,180 --> 01:16:20,540
into how language and natural language understanding works?

1068
01:16:21,740 --> 01:16:25,260
The language model today is distributed over these billions of parameters.

1069
01:16:25,980 --> 01:16:35,100
And do you think we'll ever have an understandable LLM, like, for example, we use PCA to understand

1070
01:16:35,100 --> 01:16:36,220
what regression is doing?

1071
01:16:37,020 --> 01:16:37,900
Or is that hopeless?

1072
01:16:38,700 --> 01:16:40,620
At some point, I think it's going to be right to be hopeless.

1073
01:16:40,620 --> 01:16:44,620
I mean, we'll probably learn a lot about, you know, how the systems represent data

1074
01:16:44,620 --> 01:16:46,540
and, like, how they manipulate it and stuff like that.

1075
01:16:46,540 --> 01:16:47,740
So this is not opaque, right?

1076
01:16:47,740 --> 01:16:53,500
We can completely kind of, there's complete visibility on how the systems operate.

1077
01:16:53,500 --> 01:16:57,420
Now, the question is understanding really how the decisions are being made.

1078
01:16:57,420 --> 01:17:01,420
So I'm actually not particularly interested in those questions, like, you know, as long as

1079
01:17:02,460 --> 01:17:03,580
they work properly.

1080
01:17:05,500 --> 01:17:09,420
The same way, I'm not particularly interested in figuring out exactly how the brain works.

1081
01:17:09,420 --> 01:17:15,260
I'm more interested in figuring out how the brain builds itself so that it works, right?

1082
01:17:15,260 --> 01:17:21,340
So I'm more interested in learning than in studying the result of learning, if you want.

1083
01:17:21,340 --> 01:17:23,660
So it's the same for those systems.

1084
01:17:23,660 --> 01:17:27,340
I'm more interested in how you get them to learn what you want and how to solve the problem

1085
01:17:27,340 --> 01:17:30,060
in the end is kind of considerably less interesting, in my opinion.

1086
01:17:30,460 --> 01:17:34,940
But at some point, they're going to be, you know, super intelligent,

1087
01:17:34,940 --> 01:17:36,940
repulsory of all human knowledge.

1088
01:17:36,940 --> 01:17:42,220
You know, it's going to be too big for us to kind of comprehend at a deep level.

1089
01:17:43,580 --> 01:17:44,380
Fair.

1090
01:17:46,380 --> 01:17:50,380
And by the way, I failed to acknowledge that question came from Walid Saba, who's one of our

1091
01:17:50,380 --> 01:17:55,660
senior research scientists at the EAI up in Portland, Maine.

1092
01:17:56,140 --> 01:18:00,860
The next question I'll use, and then I'll switch over to audience questions,

1093
01:18:03,820 --> 01:18:08,780
comes from Gene Tunic, the director of AI Plus Health at the Institute for Experiential AI.

1094
01:18:11,740 --> 01:18:16,380
You believe that deep learning can eventually lead to human-like understanding,

1095
01:18:17,580 --> 01:18:23,740
and you have said that self-supervised learning from unlabeled data

1096
01:18:24,620 --> 01:18:30,860
can be a powerful tool, although it seems like in human learning, as I was watching your examples,

1097
01:18:30,860 --> 01:18:38,540
for example, a lot of that data is, in a way, supervised or tied to some kind of reinforcement

1098
01:18:38,540 --> 01:18:46,300
feedback around what to expect, is it good, is it bad, etc. So how, where do you draw that line

1099
01:18:46,300 --> 01:18:58,060
between, you know, can we really truly go towards unsupervised, or there's a huge dependence on

1100
01:18:58,060 --> 01:19:03,420
supervised and on those labels to get it right? Because the world is, in a way, is telling us

1101
01:19:03,420 --> 01:19:07,980
indirectly through supervision. So self-supervised learning, I mean, the reason it's called self-supervised

1102
01:19:07,980 --> 01:19:11,900
is that deep down it's actually supervised learning. It's just supervised learning where

1103
01:19:11,900 --> 01:19:18,060
the supervision signal is the input itself, right? So in a way, that's kind of, you know, a kind of

1104
01:19:21,020 --> 01:19:26,460
simple answer to that question. It's still supervised learning in the end, but with

1105
01:19:26,460 --> 01:19:31,580
particular architectures to handle uncertainty and dimensionality and things like that. Regarding

1106
01:19:31,580 --> 01:19:35,980
reinforcement learning, there is a point at which you need some form of reinforcement learning,

1107
01:19:35,980 --> 01:19:41,500
and you need it in two situations, or at least techniques that have been developed in the context

1108
01:19:41,820 --> 01:19:48,060
of reinforcement learning. The first situation is if the objective function that is optimized by

1109
01:19:48,060 --> 01:19:53,900
your system does not reflect the ultimate objective function, you actually want to optimize. So for

1110
01:19:53,900 --> 01:20:01,820
example, you're learning to ride a bike, your objective function is the, you know, time to the

1111
01:20:01,820 --> 01:20:09,980
next fall or something, or the inverse time to next fall, you want to minimize that, right?

1112
01:20:12,140 --> 01:20:19,980
But you don't know how to compute this from the internal state of your system. And so you need

1113
01:20:21,020 --> 01:20:25,980
to train an objective function to approximate this real cost, which in the context of reinforcement

1114
01:20:25,980 --> 01:20:32,300
learning is called a critic. So that's when you need one of those things. The other situation

1115
01:20:32,300 --> 01:20:37,820
where you need it is when your world model is not accurate because it's not been trained in all

1116
01:20:37,820 --> 01:20:42,300
corners of the state space, and you happen to be in a part of the state space that it wasn't trained

1117
01:20:42,300 --> 01:20:46,700
on. Your world model is going to be bad, and your predictions are going to be bad, your planning is

1118
01:20:46,700 --> 01:20:54,060
going to be bad. So to prevent this, you need to train your world model using things that are

1119
01:20:54,140 --> 01:20:58,780
called curiosity or exploration. And that's another concept that comes from reinforcement

1120
01:20:58,780 --> 01:21:03,180
learning. So don't completely abandon reinforcement learning, but minimize its use.

1121
01:21:04,140 --> 01:21:10,060
As we switch over to the live questions, let me, I can't help but ask you this question. It comes

1122
01:21:10,060 --> 01:21:19,500
from several anonymous people as well as Ken Church, your former colleague. Did you actually say

1123
01:21:20,140 --> 01:21:25,900
the revolution will not be supervised? I did, yeah. Okay. But actually, I stole it

1124
01:21:27,020 --> 01:21:36,940
from Adi Asha Efros from Berkeley. He had a magnificent slide that was a picture of a wall

1125
01:21:36,940 --> 01:21:43,820
painting in Chile someplace, which was one of those kind of revolutionary thing. And I took

1126
01:21:43,820 --> 01:21:51,500
that picture and overlaid on it. The revolution will not be supervised. Yes. Okay. So I stole

1127
01:21:51,500 --> 01:21:57,900
that from him. I deserve no credit. Shall we switch over to a question from the audience?

1128
01:21:57,900 --> 01:22:04,620
Yeah. So first question from Glenn Jenkinson is, what two questions about AI do you wish you

1129
01:22:04,700 --> 01:22:13,740
would ask more often? Two questions. I don't know. I get asked a lot of questions. I can't

1130
01:22:13,740 --> 01:22:21,740
imagine a question have not been asked. That's relevant. I mean, I think the important questions

1131
01:22:21,740 --> 01:22:26,780
are the ones that I'm asking myself. And I wish other people would sort of frame the

1132
01:22:26,780 --> 01:22:33,660
problems in the same way. So big question. How is it that any teenager can learn to

1133
01:22:33,660 --> 01:22:37,420
drive a car in 20 hours? And we still don't have level five autonomous driving?

1134
01:22:38,940 --> 01:22:41,900
That was the first question. So second question is, what are we missing?

1135
01:22:44,300 --> 01:22:53,020
That's the answer I want. Joe. Next question from Juan Leylanda. Do you think quantum computing

1136
01:22:53,020 --> 01:22:55,740
will have a significant role in the future of AI? No.

1137
01:22:56,540 --> 01:23:09,100
Or at least not any time soon. But the time this happens, I probably won't be alive anymore.

1138
01:23:09,100 --> 01:23:16,540
So I'm not taking a big risk. No, I don't think so. I mean, there's precious few

1139
01:23:17,340 --> 01:23:22,060
situations today where quantum computing could be useful. There's no situation where it actually

1140
01:23:22,060 --> 01:23:27,980
is useful because the quantum computers are not big enough at the moment. So it's a huge bet.

1141
01:23:27,980 --> 01:23:33,500
I think scientifically it's fascinating. I'm really fascinated by quantum computing at the

1142
01:23:33,500 --> 01:23:40,060
conceptual level. I have one or two papers with Seth Lloyd on connections between neural

1143
01:23:40,060 --> 01:23:46,140
nets and quantum computing. I think it's a very interesting topic, but I don't think it has any

1144
01:23:46,140 --> 01:23:53,740
practical value in the short term. Joe. One last question from Anton Dabura.

1145
01:23:54,540 --> 01:24:00,380
To what extent do you see ML models being used for problems that we already have pretty good

1146
01:24:00,380 --> 01:24:05,980
algorithms to solve, such as sorting shortest path, linear integer programming, and so on?

1147
01:24:05,980 --> 01:24:11,500
How would you characterize the boundary, if any? So there's a lot of problems that we can currently

1148
01:24:11,500 --> 01:24:19,100
solve that are NP-complete or NP-hard, and so we can solve them within limits. What we need very

1149
01:24:19,100 --> 01:24:27,580
often are approximate algorithms, so methods that give us approximate solutions to complex problems

1150
01:24:27,580 --> 01:24:37,420
that, in theory, are NP-hard, NP-complete, whatever, but if you reduce yourself to accepting

1151
01:24:37,420 --> 01:24:46,380
approximate solutions, might become solvable. So I think there is a lot to be said for ML methods

1152
01:24:46,380 --> 01:24:53,100
that do something that has become to be known as amortized inference. So amortized inference is

1153
01:24:53,100 --> 01:24:57,660
this idea that you might have a problem that is formulated as an optimization problem. Every

1154
01:24:57,660 --> 01:25:04,540
computing problem can be formulated as an optimization problem. And what you might be able to do is

1155
01:25:04,620 --> 01:25:09,900
solve that problem in certain cases, give a solution, and now what you do with this is that you

1156
01:25:09,900 --> 01:25:15,820
train in your net of some kind to predict, to approximate the solution to that optimization

1157
01:25:15,820 --> 01:25:21,820
problem from the specification of the problem, from the inputs. So that system will not be able to

1158
01:25:22,460 --> 01:25:26,540
completely solve the problem in those situations, but for the type of problem that you train it on,

1159
01:25:27,340 --> 01:25:31,420
it's going to be able to give you an approximate solution really quickly. Amortized inference.

1160
01:25:31,420 --> 01:25:40,860
There is a tutorial on this that was written and given at a recent conference by one of my

1161
01:25:40,860 --> 01:25:46,620
colleagues at fair called Brendan Amos, AMOS, very interesting concept.

1162
01:25:48,460 --> 01:25:53,340
I will close my questions with one last question, then we'll take a real live one and call it the

1163
01:25:53,340 --> 01:25:59,100
end. I have to use this. It comes from one of our faculty who wanted to remain anonymous,

1164
01:25:59,100 --> 01:26:04,220
I don't know why, but given the big excitement around LLMs and not without a reason,

1165
01:26:05,020 --> 01:26:09,660
what are some of the research directions that are possible to tackle for non-Google slash

1166
01:26:09,660 --> 01:26:17,180
Facebook type sized institutions that are under studies? Space for foundational research,

1167
01:26:17,180 --> 01:26:23,580
big open questions in need of creative solutions. Thus, if you were a young investigator today,

1168
01:26:23,580 --> 01:26:29,180
like a starting assistant professor, what would you do in this environment?

1169
01:26:29,180 --> 01:26:33,980
I mean, that's a problem I have to face when I have PhD students at NYU that don't have access to

1170
01:26:34,940 --> 01:26:45,100
16,000 GPUs, unlike people at fair. So I think a lot of most good ideas still come from academia,

1171
01:26:45,100 --> 01:26:53,100
so you're not going to be Google or Meta or Microsoft on beating the record on

1172
01:26:53,100 --> 01:26:56,700
translation or something like that. You don't want to do this in universities.

1173
01:26:57,820 --> 01:27:02,700
But coming up with new ideas, for example, the problem I mentioned of how do you do

1174
01:27:02,700 --> 01:27:09,420
hierarchical planning? How do you train a system to figure out how to represent the world and

1175
01:27:09,420 --> 01:27:13,340
action spaces so that you can do hierarchical planning? It's completely unsolved. You can do

1176
01:27:13,340 --> 01:27:18,620
this with toy problems. If you have any idea of how you might approach that problem on toy

1177
01:27:18,620 --> 01:27:24,940
problems, you don't have to have tons of GPUs for that. You will have an idea that might have a

1178
01:27:24,940 --> 01:27:32,860
huge impact. So if you have a good architecture that you can show, can learn some simple world

1179
01:27:32,860 --> 01:27:37,740
model from video, it's the same. You don't have to train on all of YouTube. You can train on

1180
01:27:37,740 --> 01:27:43,820
artificial environments and stuff like that and demonstrate that it works. It doesn't have to be

1181
01:27:44,060 --> 01:27:48,540
large scale. So this is the kind of stuff you want to do. And then there is a new domain which is

1182
01:27:48,540 --> 01:27:54,460
building on top of open source base models. So unfortunately, right now, the best base models,

1183
01:27:55,980 --> 01:28:03,020
LLMs are the LAMA class of models from $7 billion to $65 billion. They're not usable for commercial

1184
01:28:03,020 --> 01:28:08,140
use. They are distributed with a license for non-commercial use, only for research, which you

1185
01:28:08,140 --> 01:28:13,420
can of course use in the university. And there's a lot of work to be done to figure out how to

1186
01:28:13,500 --> 01:28:18,620
make those things safe, factual, etc. And you can work from those base models. You don't have to

1187
01:28:18,620 --> 01:28:24,700
retrain them from scratch. So you don't need to have roomful, rooms full of GPUs.

1188
01:28:25,580 --> 01:28:28,860
We'll try for one last question, maybe two. Go ahead, please, with your question.

1189
01:28:29,820 --> 01:28:35,500
Hi. So my question really dwells from the side of, or we'd love to hear your thoughts,

1190
01:28:35,500 --> 01:28:41,420
on impact and control of these large language models or any of these models, the fancy models

1191
01:28:41,420 --> 01:28:46,620
that you showed with billions of trillions of parameters. So the impact side is, do you really

1192
01:28:46,620 --> 01:28:53,020
give or how much thought do you give to the impact that would have on the community or on the people

1193
01:28:53,020 --> 01:29:00,620
in general, based on what that model does? And control is, once that model is out there,

1194
01:29:03,340 --> 01:29:09,260
how do I make sure that it doesn't do a certain things it's not supposed to do with regular,

1195
01:29:10,220 --> 01:29:14,620
the way people used to use internet before those models. It used to be very controlled

1196
01:29:14,620 --> 01:29:20,460
environment where you could have, in a way, regulate those environments. But now with models,

1197
01:29:20,460 --> 01:29:26,380
it's getting increasingly difficult and a slow process to have or do not have certain things in

1198
01:29:26,380 --> 01:29:32,540
those models. Okay. So there is a long view, a very positive one, which is imagine that all of us

1199
01:29:32,540 --> 01:29:38,060
have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff

1200
01:29:38,140 --> 01:29:45,420
of people working for us, but like super people working for us. This is going to create a new

1201
01:29:46,380 --> 01:29:54,860
renaissance for humanity. It's going to increase humanity's intelligence, however you want to

1202
01:29:54,860 --> 01:30:03,260
measure it. That has to be intrinsically good. It's been the case in the past that anytime a new

1203
01:30:03,820 --> 01:30:07,500
medium was invented or a new way of communication was invented, like the printing press.

1204
01:30:08,940 --> 01:30:12,780
Humanity kind of went to the next step. The printing press let

1205
01:30:15,180 --> 01:30:22,780
the dissemination of philosophy, science, secularism, democracy, all that stuff. The US

1206
01:30:22,780 --> 01:30:34,620
would not exist without the French philosophers of the 18th century. So neither would the French

1207
01:30:34,620 --> 01:30:43,180
revolution. So I think same for the internet, that gave people instant access to an enormous

1208
01:30:43,180 --> 01:30:50,060
wealth of knowledge. Also disinformation, but okay, I mean, we have to have countermeasures for

1209
01:30:50,060 --> 01:30:55,180
every technology can be used for good and bad. We need to have countermeasures for the worst

1210
01:30:55,740 --> 01:31:04,460
aspects. But ultimately, I think we need widest possible access to those AI systems by everyone.

1211
01:31:05,500 --> 01:31:12,460
Now, how do we make sure those systems don't lie to us? How do we make sure that the information

1212
01:31:12,460 --> 01:31:17,180
they give us is not under the control of someone that has nefarious purpose, you know, things like

1213
01:31:17,180 --> 01:31:23,340
that, which is I think a good reason for them to be open as I stated earlier. But I think it's a

1214
01:31:23,340 --> 01:31:29,660
bright future for humanity, you know, contrary to some people who tell young people don't expect

1215
01:31:29,660 --> 01:31:37,820
to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the

1216
01:31:37,820 --> 01:31:45,260
next question, but we are five minutes over our time limit. And I know we have to grab a bite and

1217
01:31:45,260 --> 01:31:51,820
deliver you to the train station on time, according to the hierarchical plan. So with that, please

1218
01:31:51,820 --> 01:32:03,980
join me in thanking Jan for an amazing session today. Thank you.

