WEBVTT

00:00.000 --> 00:15.960
Okay, great, welcome everyone and thank you for joining us.

00:15.960 --> 00:22.080
This is the last distinguished lecture series for the Institute of Expansion AI for the

00:22.080 --> 00:23.080
academic year.

00:23.080 --> 00:27.360
We'll resume again in September with a full program for the year.

00:27.360 --> 00:35.280
As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series,

00:35.280 --> 00:41.520
which is designed to feature a lot of our Northeastern University experts and faculty

00:41.520 --> 00:43.560
and so forth.

00:43.560 --> 00:50.920
In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic

00:50.920 --> 00:52.600
who's at the Curie College.

00:52.600 --> 00:56.960
My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential

00:56.960 --> 01:02.880
AI and also Professor of the Practice in the Curie College for Computer Sciences and it

01:02.880 --> 01:08.880
is my pleasure today to introduce Yan Lokhan.

01:08.880 --> 01:13.480
Yan is a very well-known name in the field.

01:13.480 --> 01:20.000
I've known him for many years, I think at one point in my life I interviewed at Bell

01:20.000 --> 01:24.880
Labs or AT&T Labs and that's when he was there.

01:24.880 --> 01:32.760
He is VP and Chief AI Scientist at Meta, also known as Facebook, and Silver Professor at

01:32.760 --> 01:38.040
NYU affiliated with the Kauan Institute of Mathematical Sciences and the Center for Data

01:38.040 --> 01:41.880
Science, which he actually founded.

01:41.880 --> 01:48.640
He was the founding director of FAIR, I learned this morning that FAIR used to stand for Facebook

01:48.640 --> 01:57.520
AI Research, now it's changed to MetaFair for Fundamental AI Research and of course

01:57.520 --> 02:04.080
he founded the NYU Center for Data Science, received an engineering diploma from SEA in

02:04.080 --> 02:09.480
Paris and a PhD from the Saoubon University.

02:09.480 --> 02:18.560
After a postdoc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs

02:18.560 --> 02:23.520
in 1996 as Head of Image Processing Research.

02:23.520 --> 02:31.320
He joined NYU as Professor in 2003 and Meta or Facebook in 2013.

02:31.320 --> 02:38.000
He is the recipient of the 2018 ACM Touring Award, along with Jeffrey Hinton and Yashua

02:38.000 --> 02:42.480
Benjio, and for those of you who don't know the Touring Award, it's essentially the equivalent

02:42.480 --> 02:49.520
of the Nobel Prize for Computer Science, the toughest award to get from the ACM.

02:49.520 --> 02:54.680
The award was for conceptual and engineering breakthroughs that have made deep neural networks

02:54.680 --> 02:57.240
a critical component of computing.

02:57.240 --> 03:04.200
He is a member of the National Academy of Sciences and the National Academy of Engineering,

03:04.200 --> 03:07.520
amongst many others.

03:07.520 --> 03:13.800
His interests include AI, machine learning, computer perception, robotics and computational

03:13.800 --> 03:20.760
neuroscience, and I'm sure you're all eager to hear from Jan on what's been happening

03:20.760 --> 03:27.360
with generative AI and what all the buzz is about, hopefully we'll get into the technical

03:27.360 --> 03:32.760
details and immediately following his talk, we will do a fireside chat where I will try

03:32.760 --> 03:36.000
to ask him some tough questions.

03:36.000 --> 03:38.960
And then we will also get questions from the audience.

03:38.960 --> 03:42.800
By the way, we did get online questions from the audience.

03:42.800 --> 03:47.680
We got 150 questions, so there's no way we're going to walk you through all of those.

03:47.680 --> 03:50.520
So we'll see how much time allows us to answer.

03:50.520 --> 04:02.520
Thank you and please join me in welcoming Jan to Northeastern University.

04:02.520 --> 04:03.520
Thank you, Samap.

04:03.520 --> 04:11.920
A real pleasure to be here and thanks for coming here so numerous or for listening in online.

04:11.920 --> 04:16.400
So I'm going to talk a bit about the state of the art in AI but also about the next step

04:16.400 --> 04:24.520
because I'm always interested in the next step and how we can make machines more intelligent.

04:24.520 --> 04:31.520
And we need to figure out how to get machines that cannot just learn but also can reason

04:31.520 --> 04:37.400
and plan and current AI really does not allow current systems to do this.

04:37.400 --> 04:43.400
So I'll try to kind of sketch a potential pathway towards such systems.

04:43.400 --> 04:50.080
I can't say that we built it completely but we built some components and I go through this.

04:50.080 --> 04:56.080
So AI is in the news, everybody is playing with it at the moment.

04:56.080 --> 04:58.440
It's pretty amazing how it works.

04:58.440 --> 04:59.760
There's a lot of success.

04:59.760 --> 05:03.720
It's been very widely deployed very much in many applications that are behind the curtain

05:03.720 --> 05:06.200
but in some of them much more visible.

05:06.200 --> 05:12.760
So LLMs have the advantage of being visible but for the last 10 years or so there's massive

05:12.760 --> 05:20.080
use of AI and the latest development of AI for such thing as ranking for search engine

05:20.080 --> 05:25.880
and social networks or for content moderation, things like that.

05:25.880 --> 05:30.640
But overall machine learning requires a lot of data and the machines that we have are

05:30.640 --> 05:32.640
somewhat brittle, specialized.

05:32.640 --> 05:39.560
They don't have human-level intelligence despite what we may be led to believe.

05:39.560 --> 05:47.280
So in short, machine learning sucks at least compared to humans and animals.

05:47.280 --> 05:52.520
We've been using supervised learning which really was the workhorse of machine learning

05:52.520 --> 05:56.520
and AI systems until very recently.

05:56.520 --> 06:00.640
Reinforcement learning is insanely inefficient but it works really well for games but not

06:00.640 --> 06:02.720
many other things.

06:02.720 --> 06:07.560
So one thing that has taken over the AI world in the last few years is something called self-supervised

06:07.560 --> 06:11.880
learning which I will talk about at length.

06:11.880 --> 06:14.160
But current AI systems are specialized and brittle.

06:14.160 --> 06:15.160
They make stupid mistakes.

06:15.160 --> 06:21.440
They don't really reason and plan with a few exceptions for a game playing for example.

06:21.440 --> 06:25.400
Compared to humans and animals, they can learn new tasks extremely quickly, understand

06:25.400 --> 06:30.760
how the world works, can reason and plan have some level of common sense.

06:30.760 --> 06:32.960
Machines still don't have common sense.

06:32.960 --> 06:38.480
So how do we get machines to reason and plan like animals and humans learn as fast as animals

06:38.480 --> 06:40.320
and humans?

06:40.320 --> 06:44.320
And we'll need machines that can understand how the world works, can predict the consequences

06:44.320 --> 06:51.040
of their actions, can perform change of reasoning with unlimited number of steps, can plan complex

06:51.640 --> 06:55.080
tasks by decomposing them into simpler tasks.

06:55.080 --> 06:58.360
So let's start with this idea of self-supervised learning.

06:58.360 --> 07:00.800
It's really taken over the world.

07:00.800 --> 07:04.840
Every sort of top machine learning system today uses some form of self-supervised learning

07:04.840 --> 07:09.080
as a first step to pre-train the system.

07:09.080 --> 07:10.580
And it's used everywhere.

07:10.580 --> 07:11.580
What does it consist of?

07:11.580 --> 07:16.840
It's really the idea that instead of having, of training a system with an input and an

07:16.840 --> 07:21.600
output, which is the case in supervised learning, or with an input and a reward, which is the

07:21.600 --> 07:26.200
case for reinforcement learning, you train the system to basically model its input.

07:26.200 --> 07:30.320
You don't train it for any particular task other than capture the dependency between

07:30.320 --> 07:31.560
different parts of its input.

07:31.560 --> 07:38.080
So one thing you might do is, for example, take a piece of video, a piece of text, show

07:38.080 --> 07:43.280
a piece of the video to the system and ask it to predict the missing piece, like the

07:43.280 --> 07:44.520
continuation of that video.

07:44.520 --> 07:48.920
And after a while, you reveal the rest of the video and you adjust the system so that

07:48.920 --> 07:51.960
it does a better job at predicting.

07:51.960 --> 07:55.200
So prediction really is kind of the essence of intelligence.

07:55.200 --> 07:59.200
And to some extent, by training a system to predict, it doesn't have to be predicting

07:59.200 --> 08:00.200
the future.

08:00.200 --> 08:03.160
It could be predicting the past or the left from the right.

08:03.160 --> 08:06.560
You're training the system to represent data, essentially.

08:06.560 --> 08:15.280
And that's been nothing short of astonishingly successful in the domain of natural language

08:15.280 --> 08:16.520
understanding.

08:16.520 --> 08:26.080
So every type performing NLP system today is pre-trained the following way, or with some

08:26.080 --> 08:31.600
form of the following way, which is a special case of an old idea called denoising autoencoder.

08:31.600 --> 08:40.120
And the idea is that you take a piece of text, sequence of words from a corpus.

08:40.120 --> 08:43.880
Typically it would be a few hundred or a few thousand words long.

08:43.880 --> 08:49.520
Those words immediately get turned into vectors, but let me not talk about this for just now.

08:49.520 --> 08:52.560
So the first thing you do is you corrupt this text.

08:52.560 --> 08:57.560
You remove some of the words and replace them by blank markers, or you substitute them for

08:57.560 --> 08:58.560
another word.

08:59.120 --> 09:03.280
And then you train some gigantic neural net to predict the words that are missing.

09:03.280 --> 09:06.880
In the process of doing so, the system has to basically develop some sort of understanding

09:06.880 --> 09:11.520
of the text, because if you want to be able to predict what word comes here, you have

09:11.520 --> 09:17.720
to understand the role of the word in the sentence, the type of word that comes here,

09:17.720 --> 09:18.800
and the whole meaning of the sentence.

09:18.800 --> 09:21.680
So the system basically learns to represent text.

09:21.680 --> 09:26.120
And the amazing thing is that just by doing this, you can train a system to represent

09:26.120 --> 09:31.200
the meaning of text in pretty much any language, as long as you have data.

09:31.200 --> 09:36.520
With a single system, you can have a system that represents the meaning of a piece of

09:36.520 --> 09:39.760
text in any language.

09:39.760 --> 09:40.840
So pretty cool.

09:40.840 --> 09:47.160
You can use this to build translation systems, systems that detect hate speech on social

09:47.160 --> 09:50.480
networks or figure out what something talks about.

09:50.480 --> 09:56.280
The way you do this is that you chop off the last few layers of that gigantic neural

09:56.280 --> 10:01.720
net, and you use the representation, the internal representation, learned by the system as input

10:01.720 --> 10:08.800
to a subsequent downstream task that you train supervised, like, say, translation.

10:08.800 --> 10:12.640
And it's really astonishing how well this works.

10:12.640 --> 10:19.880
So from this to a generative AI system, there's a small step, particularly for text generation.

10:19.880 --> 10:22.600
Text generation is a completely different thing, which I'm not going to talk about,

10:22.600 --> 10:26.760
but although some systems use the same technique.

10:26.760 --> 10:34.440
So what is a generative text generation system, a large language model?

10:34.440 --> 10:42.120
It's a system of the type I just described, except that when you train it, you don't remove

10:42.120 --> 10:47.280
random words in the text that you show at the input, you only remove the last one.

10:47.280 --> 10:53.120
So you train the system to predict the last word in a sequence of words.

10:53.120 --> 10:58.600
So show a sequence of words, and then show the last word, and train some gigantic neural

10:58.600 --> 11:02.320
net, perhaps with billions or hundreds of billions of parameters, to predict the next

11:02.320 --> 11:03.320
word.

11:03.320 --> 11:12.080
And you have to train this on trillions of text snippets, typically one to two trillion

11:12.080 --> 11:14.560
for the biggest models.

11:14.560 --> 11:17.960
Once you have that system, you can use it to generate text using what's called autoregressive

11:17.960 --> 11:21.320
prediction, which is a very classical thing to do in single processing.

11:21.320 --> 11:25.720
So you take a piece of text called a prompt, you enter it into the system, you have it

11:25.720 --> 11:31.920
predict the next word, and then you shift that word into the input.

11:31.920 --> 11:37.160
So now it becomes part of the input to the system, and now you can predict the next word,

11:37.160 --> 11:39.800
shift it in, predict the third word, shift it in.

11:39.800 --> 11:42.200
That's autoregressive prediction.

11:42.200 --> 11:46.920
And that's how all the bigger alarms that everybody has played with work.

11:46.920 --> 11:47.920
That's how they've been trained.

11:47.920 --> 11:51.160
That's how they generate text.

11:51.160 --> 12:00.400
So those alarms are kind of amazing in terms of the performance that they produce.

12:00.400 --> 12:03.760
So again, they're trained on something like one to two trillion tokens.

12:03.760 --> 12:07.720
A token is like a word or a subword unit.

12:07.720 --> 12:10.400
And there's a whole bunch of those models, most of which you probably haven't heard of,

12:10.400 --> 12:15.800
but there's a few that have become household names.

12:15.800 --> 12:26.000
So we've heard of chatGBT and GPT-4 from OpenAI, which are kind of usable, barred from Google,

12:26.000 --> 12:33.200
and derivative of chatGBT and GPT-4 from Microsoft, married with Bing.

12:33.200 --> 12:38.160
But there's a long history of those things that goes back several years, some from Fair,

12:38.160 --> 12:39.160
Lunderbot, and Galactica.

12:39.160 --> 12:47.400
Galactica was trained on the scientific literature and is designed to help scientists write papers.

12:47.400 --> 12:51.960
And a more recent one, called LAMA, which is the code is open source.

12:51.960 --> 12:56.760
The model, you can get it on request if you are using it for research purpose.

12:56.760 --> 13:00.360
And it's the same level of performance as things like chatGBT, but it's not fine-tuned.

13:00.360 --> 13:02.360
You have to fine-tune it for application.

13:02.360 --> 13:07.080
And in fact, people have done this, so Alpaca is a model which basically is a fine-tuned

13:07.080 --> 13:13.400
version of LAMA that was built by people at Stanford for answering questions and things

13:13.400 --> 13:15.200
like that, instruction.

13:15.200 --> 13:21.120
So they're pretty amazing, they surprised a lot of people in how well they work, but

13:21.120 --> 13:27.200
they make a lot of factual errors, logical errors, inconsistencies, limited reasoning

13:27.200 --> 13:29.720
abilities, things like that.

13:29.720 --> 13:33.560
And they are easy to, they're pretty gullible.

13:33.560 --> 13:38.560
So you tell them, what is 2 plus 2, and the system will say 4, and you say, no, actually

13:38.560 --> 13:39.560
2 plus 2 equals 5.

13:39.560 --> 13:41.640
Oh yeah, you're right, I made a mistake.

13:41.640 --> 13:51.640
So they kind of, they predict answers that would sound like someone could produce these

13:51.640 --> 13:54.000
answers, but the details might be wrong.

13:54.000 --> 14:01.480
So you can't really use them for factual answers, but you can use them certainly for

14:01.480 --> 14:03.120
writing aids.

14:03.120 --> 14:10.760
And particularly, it works really well for text or for standard sort of templatized text

14:10.760 --> 14:16.040
that you need to write, like I don't know, there's a bunch of professors here that have

14:16.040 --> 14:26.200
to spend quite a bit of time writing recommendation letters for students, very useful for that.

14:26.200 --> 14:27.720
And very useful for code generation.

14:27.720 --> 14:32.560
So the software industry is probably going to be revolutionized by such tools.

14:32.560 --> 14:38.080
So this is an example of code generated from a prompt by the Lama 65 billion model, the

14:38.080 --> 14:39.080
open source one.

14:39.080 --> 14:44.160
So, you know, ask it, you know, find real roots of AX squared plus BX plus C, and the

14:44.160 --> 14:49.520
thing just writes a function in Python or whatever, whatever you want, or Reg X or whatever,

14:49.520 --> 14:53.280
who remembers the syntax of Reg X?

14:53.280 --> 14:55.280
Like.

14:55.280 --> 15:00.480
You can have it, you know, hallucinate text that might sound plausible or completely implausible

15:00.480 --> 15:01.480
like this.

15:01.480 --> 15:04.560
Did you know that Yanukun dropped a rap album last year?

15:04.560 --> 15:07.200
We listened to it and here is what we thought.

15:07.200 --> 15:17.920
And the thing writes a review of my alleged rap album.

15:17.920 --> 15:22.240
I'm not much of a rap person, I'm more of a jazz person, so when my colleagues showed

15:22.720 --> 15:26.680
this to me, I told them, like, can you do the same for a jazz album that would be kind

15:26.680 --> 15:27.680
of more appropriate?

15:27.680 --> 15:33.160
I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't

15:33.160 --> 15:38.120
work very well because there's not enough training data on the web of reviews of jazz

15:38.120 --> 15:39.120
albums.

15:39.120 --> 15:44.320
I found that incredibly sad, I cried.

15:44.320 --> 15:46.720
So you need a lot of data to train those things, right?

15:46.720 --> 15:50.960
In fact, the amount of data, like something like 1.5 trillion tokens that Lama is trained

15:50.960 --> 15:56.000
on, it would take about 22,000 years for a human reading eight hours a day at every

15:56.000 --> 15:58.480
speed to read the whole material.

15:58.480 --> 16:04.160
So obviously those things can accumulate a lot of knowledge, at least approximately.

16:04.160 --> 16:09.240
So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not

16:09.240 --> 16:12.680
good for producing factual and consistent answers, at least not yet.

16:12.680 --> 16:18.680
So a lot of LLMs are being augmented or extended so that they can use tools like calculators

16:18.680 --> 16:26.400
or database engines or whatever to search for information and then refer to the source.

16:26.400 --> 16:31.960
They're not good at all for reasoning, planning, or even for arithmetic.

16:31.960 --> 16:39.440
So but we are easily fooled by their language fluency into thinking that they are intelligent.

16:39.440 --> 16:42.600
They're not that intelligent.

16:42.600 --> 16:46.440
And they really have no understanding of the physical world because they're trained with

16:46.440 --> 16:47.440
text.

16:47.720 --> 16:51.200
And there's another flaw, which is a huge problem.

16:51.200 --> 16:56.880
It's the fact that if you imagine that there is the set of all possible answers represented

16:56.880 --> 17:02.080
by this sphere, disk, which is really a tree, right?

17:02.080 --> 17:05.080
Every token you add put, you have a certain number of options for what the token should

17:05.080 --> 17:06.680
be, what the word is.

17:06.680 --> 17:08.480
So it's a tree of all possible answers.

17:08.480 --> 17:13.600
Within this tree, there is a small subtree that corresponds to correct answers for the

17:13.600 --> 17:16.120
question being asked.

17:16.120 --> 17:22.680
And imagine that there is a probability E for any token that is produced by the system

17:22.680 --> 17:26.280
to be outside, to take you outside that tree of correct answers.

17:26.280 --> 17:29.200
Once you go outside that tree, you can't come back because it's a tree.

17:29.200 --> 17:37.480
So let's imagine that the probability per token is E. So the probability that a sequence

17:37.480 --> 17:41.400
of N tokens would be correct is 1 minus E to the power N, making the assumption that

17:41.400 --> 17:46.920
the errors are independent, which of course they're not, but that's kind of a crude assumption.

17:46.920 --> 17:51.320
And so the problem with this is that it's an exponentially divergent process, this

17:51.320 --> 17:55.480
autoregressive prediction, errors accumulate.

17:55.480 --> 18:01.000
And if you produce too many tokens, the thing will sort of diverge away from the set of

18:01.000 --> 18:02.560
correct answers, exponentially.

18:02.560 --> 18:06.800
And that's not fixable with the current architecture.

18:06.800 --> 18:12.760
You can fine tune those systems a lot to reduce E, but you're not going to make it

18:12.760 --> 18:14.880
go away.

18:14.880 --> 18:21.640
So I have a bold prediction, which is that the shelf life of autoregressive LLM is very

18:21.640 --> 18:23.000
short.

18:23.000 --> 18:28.360
My prediction is that five years from now, nobody in their right mind would use them.

18:28.360 --> 18:32.760
So enjoy it while it lasts.

18:32.760 --> 18:35.720
They'll be replaced by things that are better.

18:35.720 --> 18:41.680
And I'll hint about directions to kind of perhaps fix up those problems.

18:41.680 --> 18:49.400
So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema

18:49.400 --> 18:54.800
magazine, which is a philosophy magazine, about the fact that a system that is purely

18:54.800 --> 19:01.600
trained from text, from language, cannot possibly attain human level intelligence because much

19:01.600 --> 19:07.920
of what humans know is actually derived from experience of the physical world.

19:07.920 --> 19:13.520
This is true for a lot of human knowledge, but it's true certainly for almost the totality

19:13.520 --> 19:15.760
of animal knowledge.

19:15.760 --> 19:20.240
It's all about the world is no linguistic related, no language related.

19:20.240 --> 19:25.960
So linguistic abilities and fluency are not related to the ability to think.

19:25.960 --> 19:30.680
Those are two different things.

19:30.680 --> 19:37.200
There are some criticisms of autoregressive LLMs from people coming from the cognitive

19:37.200 --> 19:43.200
science realm who say like, this is not at all the way the human mind works.

19:43.200 --> 19:46.160
There is essential missing pieces.

19:46.160 --> 19:50.280
Other criticism for people who come from sort of more classical AI, pre-deep learning,

19:50.280 --> 19:56.760
they say like, you know, AI systems are supposed to be able to plan and reason, and those LLMs

19:56.760 --> 19:57.760
can do it.

19:57.760 --> 20:02.320
Or at least not, you know, they can do it maybe in very sort of primitive forms.

20:02.320 --> 20:08.040
Perhaps they can plan things in situations that correspond to a template that they've

20:08.040 --> 20:12.200
been trained on, but they're not so innovative.

20:12.200 --> 20:20.240
So we should ask, how is it that humans and animals can run so quickly?

20:20.240 --> 20:26.840
And I've been using this diagram for quite a while now, several, many years from Emmanuel

20:26.840 --> 20:30.680
Dupu, who's a cognitive scientist in Paris.

20:30.680 --> 20:36.440
And we tried to sort of make a chart of at what age babies learn basic concepts about

20:36.440 --> 20:43.400
the world, so things like distinguishing between animate objects and inanimate objects, learning

20:43.400 --> 20:46.560
the notion of object permanence, the fact that when an object is hidden behind another

20:46.560 --> 20:49.760
one, it still exists.

20:49.760 --> 20:54.440
Notion of rigidity, solidity, things like natural categories, babies don't need to

20:54.440 --> 20:58.680
know the name of an object to actually know that there are different categories of objects

20:58.680 --> 21:00.240
around four months or so.

21:00.240 --> 21:04.840
And then it takes about nine months for babies to really understand that sort of intuitive

21:04.840 --> 21:11.320
physics that objects that are not supported will fall, that, you know, objects have a

21:11.320 --> 21:19.560
momentum, weight, friction, you know, knowing that if I push on this object, you know, light

21:19.560 --> 21:24.120
objects like this, they're going to move, but if I push on an object that's heavier,

21:24.120 --> 21:26.320
it's not going to move unless I push harder.

21:26.320 --> 21:27.320
So things like that.

21:27.320 --> 21:31.880
So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where

21:31.880 --> 21:36.160
you have a little car on the platform, you push the car off the platform, it appears

21:36.160 --> 21:42.000
to float in the air, a five-month-old baby will pay attention, a 10-month-old baby will

21:42.000 --> 21:48.440
go like this because she understood that by then that objects are not supported or supposed

21:48.440 --> 21:52.360
to fall and this object appears to be floating in the air.

21:52.360 --> 21:59.200
So we can determine that her mental model of the world is being violated, okay?

21:59.200 --> 22:03.080
That's how this chart was built.

22:03.080 --> 22:07.680
So we accumulate as babies an enormous amount of background knowledge about how the world

22:07.680 --> 22:11.680
works, mostly by observation, a little bit by interaction, when we start being able to

22:11.680 --> 22:17.600
kind of grab things, but in the first few months it's mostly just observation.

22:17.600 --> 22:23.600
So we don't know how to reproduce this with this type of learning with machines.

22:23.600 --> 22:29.880
Once we accumulate all this background knowledge, you know, in a number of years, learning a

22:29.880 --> 22:33.160
new task like driving is very fast.

22:33.160 --> 22:38.600
So any teenager can learn to drive in about 20 hours of practice, mostly without causing

22:38.600 --> 22:41.320
any accident.

22:41.320 --> 22:44.920
So the teenager doesn't have to run off a cliff to figure out that the car, that nothing

22:44.920 --> 22:48.080
good is going to happen if you run off a cliff.

22:48.080 --> 22:53.000
The mental model of the world is already there, okay?

22:53.000 --> 22:56.480
We still won't have level five salivating cars.

22:56.480 --> 23:01.000
So obviously we're missing something pretty big.

23:01.000 --> 23:05.520
Any 10-year-old can clear up the dinner table and fill up the dishwasher.

23:05.520 --> 23:08.480
We're nowhere near having robots that can do this and it's not because of mechanical

23:08.480 --> 23:16.120
design, it's because we don't know how to build the minds behind it.

23:16.120 --> 23:18.600
So we're missing something big, right?

23:18.600 --> 23:24.480
The past towards human-level AI is not just making LLMs bigger, that's just not going

23:24.480 --> 23:26.640
to get us there.

23:26.640 --> 23:33.720
It's been a common recurring error by AI scientists and engineers over the last six decades to

23:33.720 --> 23:39.960
imagine that the one thing that they just discovered was the solution to human-level

23:39.960 --> 23:44.760
AI, only to discover a few years later that no, there was actually a big obstacle, another

23:44.760 --> 23:46.560
obstacle they had to clear.

23:46.560 --> 23:51.240
It's a recurring history story in AI.

23:51.240 --> 23:56.000
So common sense will probably emerge from the ability of machines to learn how the world

23:56.000 --> 24:01.120
works by observation, the way babies and animals do it.

24:01.120 --> 24:05.800
So I see three challenges for AI research over the next decade also, learning representations

24:05.800 --> 24:10.160
of the world and predictive models of the world, I'll say why in a minute, and self-supervised

24:10.160 --> 24:16.120
learning is going to be the key component of that, learning to reason.

24:16.120 --> 24:19.840
So psychologists talk about system one and system two.

24:19.840 --> 24:28.400
System one is the type of control that our brains use to kind of react to something without

24:28.400 --> 24:31.760
really having to think about it, like subconscious action.

24:31.760 --> 24:36.120
So if you're an experienced driver, you don't have to think about driving, you can just drive

24:36.120 --> 24:41.240
and you can talk to someone at the same time and barely pay attention.

24:41.240 --> 24:42.240
So that's system one.

24:42.240 --> 24:46.400
But then when you are learning to drive, you pay attention to absolutely everything.

24:46.400 --> 24:56.920
You use your entire focus, consciousness, attention to drive and that's system two.

24:56.920 --> 25:01.400
And then the last thing is learning to plan complex action sequences, decomposing them

25:01.400 --> 25:02.400
into simpler ones.

25:02.400 --> 25:13.400
So I wrote a sort of vision paper about a year ago, which I posted on open review for

25:13.400 --> 25:16.120
comments, so you're welcome to comment on it.

25:16.120 --> 25:20.880
I give a bunch of technical talks about it, one of the earliest one was at Berkeley, but

25:20.880 --> 25:24.680
you are having a more recent version of it right now, so you don't need to look at that

25:24.680 --> 25:26.800
one.

25:26.800 --> 25:32.400
And it's based on what's called a cognitive architecture.

25:32.400 --> 25:38.240
So basically how can we sort of design a system with different modules so that those modules

25:38.240 --> 25:46.640
may implement all the properties that I was telling you about so systems can perceive,

25:46.640 --> 25:51.600
reason, predict, in particular predict the consequences of their actions and then plan

25:51.600 --> 25:56.560
a sequence of actions to optimize, to satisfy a particular objective.

25:56.560 --> 26:04.720
So the main components of the system is the key component, I would say, is the world model

26:04.720 --> 26:13.760
and the world model is what allows the system to predict ahead, imagine what's gonna happen.

26:13.760 --> 26:18.800
This is to some extent what current AI systems don't really have.

26:18.800 --> 26:23.280
Perception system basically gets an estimate of the state of the world and initializes

26:23.280 --> 26:25.680
the world model with it.

26:25.680 --> 26:31.160
The cost here is a really important module, so basically the entire purpose of the agent

26:31.160 --> 26:35.080
is to minimize this cost.

26:35.080 --> 26:42.840
So the cost is something that uses a measurement of the state of the agent, particularly the

26:42.840 --> 26:48.240
prediction from the world model, and predicts whether an act is going to be good or bad.

26:48.240 --> 26:51.880
And the entire purpose of the agent here is to figure out a sequence of actions, so this

26:51.880 --> 26:56.920
is taking place in the actor, figure out a sequence of actions such that when I predict

26:56.920 --> 27:01.520
what's gonna happen as a consequence of those actions using my world model, my objective,

27:01.520 --> 27:04.320
my cost function will be minimized.

27:04.320 --> 27:11.680
So if my cost function is, so the cost function is basically our measures of discomfort of

27:11.680 --> 27:15.080
the agent.

27:15.080 --> 27:17.960
Biological brains have things like that in the visual language, so this is the thing

27:17.960 --> 27:23.320
that tells you when you're hungry, for example, or you're hurting.

27:23.320 --> 27:29.800
So nature tells you you're hungry, nature doesn't tell you how to feed, you have to

27:29.800 --> 27:34.520
figure that out by yourself, perhaps using your world model and your planning abilities.

27:34.520 --> 27:38.040
So this is the same thing here, imagine this is a robot and the robot battery are kind

27:38.040 --> 27:43.400
of starting to get drained, so there's a cost function here that says be careful, you're

27:43.400 --> 27:45.480
running out of power.

27:45.480 --> 27:49.960
And so the system, according to this world model, would say, well, I can recharge my

27:49.960 --> 27:53.480
battery by plugging myself into a socket.

27:53.480 --> 27:58.560
So it figures out the sequence of actions to plug itself into a socket and that will

27:58.560 --> 28:04.000
eventually minimize the cost function that just appeared.

28:04.000 --> 28:11.240
So in fact, there's two ways to operate that system one is the kind of system one where

28:11.240 --> 28:15.600
the system makes an estimate of the state of the world, run this to a perception system

28:15.600 --> 28:20.120
called an encoder here, produces an estimate of the state of the world called S0 and that

28:20.120 --> 28:25.000
runs into a neural net called a policy network that just produces an action and the action

28:25.000 --> 28:27.000
goes into the world.

28:27.000 --> 28:32.440
LLNs are like this, they are system one, you give them a pump, that's X, they produce

28:32.440 --> 28:37.400
an action, that's the token they predict, that goes back into the world and the world

28:37.400 --> 28:42.880
is very simplistic here, it's just you shift in the input.

28:42.880 --> 28:46.560
So no reasoning necessary, here is system two.

28:46.560 --> 28:54.840
So you use the same system here and this is a sort of time-enrolled version of the system.

28:54.840 --> 29:00.080
So we have the world model, the world model is this green module and the different instances

29:00.080 --> 29:04.600
of that green module are the state of the system at different time steps, so think of

29:04.600 --> 29:08.820
it as like a recurrent net that you unfolded, so it's really the same module at different

29:08.820 --> 29:09.820
time steps.

29:09.820 --> 29:14.560
What the world model is supposed to be able to predict is given the representation of

29:14.560 --> 29:19.600
the state of the world at time t and given an action that I'm imagining taking, what

29:19.600 --> 29:24.120
is going to be the predicted state of the world at time t plus one.

29:24.120 --> 29:29.320
So I can imagine a sequence of actions that I might take, imagine the effect on the world

29:29.320 --> 29:34.720
using my world model and then I can plug the state of the world over this trajectory

29:34.720 --> 29:42.440
through my cost and measure whether my cost is going to be minimized by this action sequence,

29:42.440 --> 29:43.440
my objectives.

29:43.440 --> 29:50.160
So what I should do is run some sort of optimization procedure that will try to search for a sequence

29:50.160 --> 29:54.920
of actions that minimizes the cost given the prediction given to me by the produced by

29:54.920 --> 29:56.320
the world model.

29:56.320 --> 30:00.280
This type of planning is very classical in optimal control.

30:00.280 --> 30:02.960
It's called model predictive control.

30:02.960 --> 30:10.640
In classical optimal control, the model is not learned usually, it's handcrafted.

30:10.640 --> 30:15.400
Here we are thinking about a situation where the world model is learned by, for example,

30:15.400 --> 30:22.240
watching the world go by, by video, but also by observing actions being taken in the world

30:22.240 --> 30:23.400
and seeing the effect.

30:23.400 --> 30:28.800
So to get a good accurate model here, I'm going to have to observe the state of the

30:28.800 --> 30:33.680
world, observe, like, take an action and observe the effect or observe someone else

30:33.680 --> 30:36.040
take an action and observe the effect.

30:36.040 --> 30:40.880
Let me skip this for now.

30:40.880 --> 30:45.480
Ultimately what we want is a hierarchical version of this because if you want the system

30:45.480 --> 30:50.720
to be able to plan complex actions, we can't plan it at the lowest level.

30:50.720 --> 30:59.920
So for example, if I want to plan to go from here to New York City, I would have to basically

30:59.920 --> 31:05.520
plan every millisecond exactly what muscle actions I should take, okay?

31:05.520 --> 31:07.680
And it's impossible, right?

31:07.680 --> 31:13.280
You can't plan an entire trip from here to New York City millisecond by millisecond,

31:13.280 --> 31:19.200
partly because you don't have a perfect model of the environment, like you don't know if,

31:19.200 --> 31:25.480
when you're going to walk up the room here, whether someone is going to be on the way,

31:25.480 --> 31:26.840
in the way and you're going to have to go around.

31:26.840 --> 31:29.200
So you can't completely plan in advance, right?

31:29.200 --> 31:33.440
So what we do is we plan hierarchically, say like, okay, I want to go to New York City,

31:33.440 --> 31:37.320
so the cost function at the top here measures my distance to New York City.

31:37.320 --> 31:44.040
And the first thing I have to do is go to the airport and catch a train or go to the train

31:44.040 --> 31:48.080
station and catch a train or go to the airport catch a plane.

31:49.080 --> 31:53.040
So the top predictors are predictors at a high level that says, oh, okay,

31:53.040 --> 31:57.800
if I catch a taxi, it might take me to the airport.

31:57.800 --> 32:03.120
If I catch, or to the train station, then if I catch a train, it'll take me to New York City.

32:03.120 --> 32:09.200
Okay, so you have those two hidden actions, those Z variables here.

32:09.200 --> 32:12.960
And they define a cost function for the next level down.

32:13.160 --> 32:19.600
So if the first action is I'm taking a taxi to the train station,

32:19.600 --> 32:23.520
the lower level is how do I catch a taxi here?

32:23.520 --> 32:25.240
I go down in the street and hail the taxi.

32:25.240 --> 32:26.080
No, this is Boston.

32:26.080 --> 32:28.840
I need to call it Uber or something.

32:28.840 --> 32:35.520
Okay, so I go on the street and I call it Uber.

32:35.520 --> 32:36.760
How do I go in the street?

32:36.760 --> 32:38.120
There's going to be lower levels.

32:38.120 --> 32:39.600
I have to get out of this building.

32:39.600 --> 32:40.520
How do we get out of this building?

32:40.520 --> 32:42.000
I have to walk through the door.

32:42.080 --> 32:43.400
How do I work through the door?

32:43.400 --> 32:47.200
I have to put one leg in front of the other over obstacles.

32:47.200 --> 32:49.480
And all the way down to millisecond.

32:49.480 --> 32:53.000
Also control for a short period, which is replanned as we go.

32:54.160 --> 32:57.440
Okay, no AI systems today can do any of this.

32:57.440 --> 33:01.640
This is completely virgin territory.

33:01.640 --> 33:05.680
Okay, there's a lot of people who've worked on hierarchical planning,

33:05.680 --> 33:10.160
but in situations where the representations at every level are hardwired,

33:10.160 --> 33:11.560
they're known in advance.

33:11.560 --> 33:13.080
They're predetermined.

33:13.080 --> 33:15.840
It's sort of like the equivalent of a vision system where the features

33:15.840 --> 33:18.080
at every level are hardwired or designed by hand.

33:19.080 --> 33:19.920
There's no system today.

33:19.920 --> 33:22.480
They can learn hierarchical representations for action plans.

33:23.760 --> 33:25.680
So that's a big challenge.

33:25.680 --> 33:29.600
The cost function, so here's what's important here.

33:29.600 --> 33:33.960
A lot of people today are talking about the fact that AI systems are difficult

33:33.960 --> 33:39.200
to control and that's terrible, maybe toxic, various things.

33:40.200 --> 33:46.560
The system I describe cannot produce outputs that do not minimize the objectives.

33:46.560 --> 33:53.320
And so if you have terms in the objective that guarantee certain conditions,

33:53.320 --> 33:56.360
that system will have no choice but obeying those conditions.

33:56.360 --> 33:58.960
Okay, so having a system that is designed like this,

33:58.960 --> 34:03.120
that whose output is produced by minimizing a set of objectives,

34:03.120 --> 34:08.000
according to a model, will basically help guarantee the safety of that system.

34:08.000 --> 34:12.680
Because you can hardwire intrinsic objectives on the left here

34:12.680 --> 34:14.600
that basically guarantee the safety.

34:14.600 --> 34:18.840
And the system cannot escape the satisfaction of those constraints.

34:18.840 --> 34:20.880
So let me take a very simple example.

34:20.880 --> 34:24.120
Let's say someone figures out how to build a domestic robot they can cook.

34:25.720 --> 34:30.320
This robot will have to be able to kind of handle a kitchen knife.

34:30.320 --> 34:33.680
And you might put a cost function that says, don't flail your arm

34:33.680 --> 34:36.520
if you have a kitchen knife in your arm and there is people around.

34:37.520 --> 34:38.960
Okay, because it's dangerous.

34:38.960 --> 34:42.640
So you can imagine putting a lot of kind of safety conditions in those systems

34:42.640 --> 34:44.080
to make them steerable.

34:44.080 --> 34:51.120
So I don't think the problem of making AI systems safe is such a huge problem

34:51.120 --> 34:55.840
that some people who are very vocal are seeing it is that AI is going to kill us all.

34:58.280 --> 34:59.880
It's not going to kill us all.

34:59.880 --> 35:06.120
We would have to screw up really badly for that to happen.

35:06.120 --> 35:09.640
Okay, now here's the thing.

35:09.640 --> 35:11.600
How do we build the world model?

35:13.600 --> 35:17.720
And that's basically the biggest challenge that we have at the moment.

35:17.720 --> 35:21.360
How do we build a system that can predict what's going to happen in the world?

35:21.360 --> 35:24.080
For example, by training itself to predict videos.

35:24.080 --> 35:28.160
Now the problem with predicting videos is that the world is not entirely predictable.

35:30.720 --> 35:32.960
It may not be deterministic, but even if it were deterministic,

35:32.960 --> 35:34.800
it wouldn't be completely predictable.

35:34.800 --> 35:36.560
So in fact, here is an example here.

35:38.320 --> 35:43.960
If you take a video, this is a top-down video of a highway that looks like cars

35:43.960 --> 35:46.440
driving around just following the blue car.

35:46.440 --> 35:49.160
And you train a neural net to predict what's going to happen in the video

35:49.160 --> 35:50.600
after the first few frames.

35:50.600 --> 35:56.400
It produces blurry, it makes blurry prediction because it can't predict if

35:56.400 --> 36:02.000
the car that's behind you is going to accelerate or break or change lane or whatever.

36:02.000 --> 36:05.600
So it makes an average of all the possible future and that's a blurry image.

36:05.600 --> 36:09.200
Same with, this is an old paper where we attempted to do video prediction using

36:09.200 --> 36:14.120
neural nets and the predictions are blurry because there's too many things

36:14.120 --> 36:16.600
that can plausibly happen and the system can only predict one thing.

36:16.600 --> 36:17.600
So it predicts the average.

36:19.840 --> 36:20.840
So that's no good.

36:20.840 --> 36:24.600
The solution to this is what I call a joint evading predictive architecture.

36:24.600 --> 36:28.280
And this is really the most important slide of the talk.

36:29.800 --> 36:35.520
So the normal way to make predictions is through a generative model.

36:35.520 --> 36:36.400
What's a generative model?

36:36.400 --> 36:38.600
It's a model where you have a bunch of variables you observe,

36:38.600 --> 36:41.840
let's say the initial segment of a video.

36:41.840 --> 36:44.920
You run it through an encoder and through a predictor and the predictor predicts

36:44.920 --> 36:48.440
y, which is, let's say, the continuation of that video.

36:49.560 --> 36:52.600
And you have some cost function that measures the discrepancy divergence

36:52.600 --> 36:55.800
between the predicted y and the actual y you observe.

36:55.800 --> 36:57.600
This is when you train your world model.

36:59.280 --> 37:02.320
It could be that the predictor has an action variable that comes in,

37:02.320 --> 37:03.880
but in this example there isn't.

37:05.560 --> 37:09.920
So examples of this are things like variational auto encoders,

37:09.920 --> 37:14.400
mass auto encoders, or denoising auto encoders, which is a more general concept.

37:14.400 --> 37:18.600
And so basically all NLP systems, including LMS, are of this type,

37:18.600 --> 37:19.360
the generative models.

37:20.360 --> 37:22.360
But here is the thing.

37:22.360 --> 37:25.440
You don't want to be predicting every detail about the world.

37:25.440 --> 37:28.360
Here you have to predict every single detail about the world.

37:28.360 --> 37:30.880
So it's easy if it's text, because text is discrete.

37:30.880 --> 37:35.040
So predicting the next word, I cannot predict the next word from a text.

37:35.040 --> 37:41.320
But I can predict within 10 possible words some probability distribution of

37:41.320 --> 37:45.120
the, over all the words in the dictionary of which word comes next, right?

37:45.120 --> 37:49.320
They can represent distributions over discrete variables.

37:49.320 --> 37:52.920
I cannot do this over the set of all possible video frames.

37:54.080 --> 37:57.560
I cannot usefully represent a distribution over the set of all possible video frames.

37:59.560 --> 38:05.200
So I can't use the same trick for video that is used for language.

38:05.200 --> 38:09.800
The reason why we have LMS that works so well is because text is easy.

38:09.800 --> 38:10.520
Language is simple.

38:11.840 --> 38:14.280
We only popped up in the last few hundred thousand years anyway, so

38:14.280 --> 38:15.240
it can be that complicated.

38:16.240 --> 38:22.040
And it's also processed in the brain by two tiny areas called the Vernike area

38:22.040 --> 38:24.600
for understanding and the Borke area for production.

38:25.640 --> 38:26.800
What about the rest of the brain?

38:26.800 --> 38:28.920
The prefrontal cortex, that's where we think, okay?

38:30.200 --> 38:34.360
That's not part of LMS, the LMS are perhaps good models of Vernike and Borke,

38:34.360 --> 38:34.880
but that's it.

38:37.880 --> 38:41.880
Okay, so what I'm proposing here is to replace this generative architecture by

38:41.880 --> 38:45.400
a joint embedding architecture and the essential characteristic of it is that

38:45.400 --> 38:49.640
the variable that you want to capture the dependency of with respect to X goes

38:49.640 --> 38:53.440
itself through an encoder and the encoder eliminates the relevant information

38:53.440 --> 38:55.600
that is not useful for anything.

38:55.600 --> 39:01.800
Okay, so for example, if I had a video of this, if I was shooting a video of

39:01.800 --> 39:05.680
the room here and then panning the camera and asking a system to predict

39:05.680 --> 39:10.960
what's the rest of the room, it would probably predict that the rest of

39:10.960 --> 39:17.200
the room looks like the initial part that there'd be a lot of people in

39:17.200 --> 39:25.920
different seats, but it couldn't predict your age, gender, hairstyle, clothing,

39:27.200 --> 39:31.800
or the texture, precise texture of the floor or things like that, right?

39:31.800 --> 39:36.440
So there's details that cannot possibly be predicted and one way to avoid

39:36.440 --> 39:40.200
predicting them is to basically eliminate that information from the variable to be

39:40.200 --> 39:42.760
predicted through an encoder.

39:42.760 --> 39:45.680
So that's a joint embedding architecture or predictive architecture because it has

39:45.680 --> 39:46.680
a predictor.

39:46.680 --> 39:51.040
Now there's an issue with this thing, which is that if you train a system with,

39:51.040 --> 39:54.600
let's say, a piece of video and the following piece of video and you just train

39:54.600 --> 39:59.200
it to minimize the prediction error, you train the whole thing, it collapses.

39:59.200 --> 40:04.680
It collapses, basically the encoders ignore the inputs, they produce constant

40:04.680 --> 40:09.000
vectors for SX and SY and the predictor just needs to map SX to SY and it's a

40:09.000 --> 40:13.280
constant, so it's super easy, okay, bad.

40:13.280 --> 40:17.120
So the question now is how do we prevent this from happening?

40:17.120 --> 40:18.640
How do we prevent it collapse?

40:18.640 --> 40:23.000
It doesn't happen with generative models because they can't collapse.

40:23.000 --> 40:27.280
So there are three flavors of those joint embedding architectures, a simple one

40:27.280 --> 40:31.440
where you're basically trying to make the two representation of SX and SY identical.

40:31.440 --> 40:36.640
So for example, X and Y are two different views of the same scene and you want SX

40:36.640 --> 40:40.600
to represent the content of the scene, so it doesn't matter where you look it from.

40:40.600 --> 40:43.160
You just want to make the representations equal.

40:43.160 --> 40:45.640
When the encoders are identical, this is called a Syme's network.

40:45.640 --> 40:50.480
This is another idea that goes back to the early 90s.

40:50.480 --> 40:53.640
You have deterministic joint embedding architectures and then you have joint

40:53.640 --> 40:58.080
predictive architectures that may be non-deterministic where the predictor

40:58.080 --> 41:02.800
function has a latent variable that could be drawn from a distribution or taken

41:02.800 --> 41:06.720
in a set that would allow that system to make multiple predictions if necessary.

41:10.720 --> 41:15.960
Now, we have to ask ourselves the question of how do we train those things?

41:15.960 --> 41:21.560
And I'm going to use a symbolism here where that I've used the rectangles

41:21.560 --> 41:27.480
and squares of cost functions, energy terms, the circles of variables,

41:27.480 --> 41:31.320
observed or not, and those symbols here are deterministic functions.

41:31.320 --> 41:35.480
Imagine a neural net, okay, trainable.

41:35.480 --> 41:39.720
We may have to hardwire some cost functions in the system to have it,

41:39.720 --> 41:43.600
to drive it to focus on aspects of the input that are important.

41:43.600 --> 41:48.200
So that's the purpose of that C cost function at the top.

41:48.200 --> 41:53.200
Okay, but to explain how to train those things,

41:53.200 --> 41:57.120
I'm going to have to explain a little bit what energy based models is about because

41:57.120 --> 42:00.720
the classical kind of probabilistic modeling in machine learning kind of goes

42:00.720 --> 42:04.520
at the window when we use the joint embedding architectures.

42:04.520 --> 42:05.640
So what's an energy based model?

42:05.640 --> 42:10.280
Energy based model is a learning system that captures the dependency between

42:10.280 --> 42:14.400
two sets of variable x and y through an energy function that is supposed to take

42:14.400 --> 42:19.560
low values, low energies around data, training samples.

42:19.560 --> 42:23.480
So imagine those black dots are training samples.

42:23.480 --> 42:27.680
You want that energy function f of x, y to take low values around the training

42:27.680 --> 42:30.400
samples and then higher values outside.

42:30.600 --> 42:33.280
And that system will capture the dependencies between x and y.

42:33.280 --> 42:36.680
If I give you a value of x and I ask you what can be the possible values for y,

42:36.680 --> 42:40.000
you're going to tell me, well, it's either this or that or maybe that other thing at

42:40.000 --> 42:41.400
the top.

42:41.400 --> 42:45.640
Okay, so it's not a mapping from x to y, it's an implicit function.

42:45.640 --> 42:48.960
And by figuring out what value of y minimizes the energy function,

42:48.960 --> 42:50.200
you can do inference.

42:50.200 --> 42:54.080
You can infer y, possibly, but you don't necessarily have to do that.

42:55.600 --> 42:56.680
So that's energy based model.

42:56.680 --> 43:01.040
It's kind of a weaker form of modeling than probabilistic modeling.

43:01.040 --> 43:04.960
And so now the learning problem becomes how do you train this energy function,

43:04.960 --> 43:10.480
which is going to be some big neural net, so that the energy takes low value around

43:10.480 --> 43:12.760
the training samples and high values outside.

43:12.760 --> 43:15.520
If you're not careful, you're going to get a collapse so that the same type of

43:15.520 --> 43:18.600
collapse I was telling you about before, if you just pull down the energy of the

43:18.600 --> 43:22.960
training samples, minimize the prediction error in this joint invading architecture,

43:22.960 --> 43:24.520
you're going to get zero energy for everything.

43:24.520 --> 43:27.200
It's not a good way to capture the dependencies.

43:27.200 --> 43:29.280
You have two classes of methods, contrastive methods.

43:29.280 --> 43:33.040
So contrastive methods consist in generating those green points,

43:33.040 --> 43:38.000
which are outside the region of data, and then push the energy up while you push

43:38.000 --> 43:40.120
down on the energy of the data points.

43:40.120 --> 43:45.040
Okay, so that's going to create a groove in the energy surface, and

43:45.040 --> 43:47.320
the system will have captured the dependency between x and y.

43:48.520 --> 43:51.520
But there's an alternative here, which is regularized methods,

43:51.520 --> 43:56.000
where the point of those methods is to minimize the volume of space that can take

43:56.000 --> 44:00.280
low energy, so that when you push down on the energy of data points,

44:02.560 --> 44:07.040
the rest of the space takes higher energy because there is only a small amount of,

44:07.040 --> 44:09.520
a small region of low energy to go around.

44:09.520 --> 44:11.480
So those are the two classes of methods.

44:11.480 --> 44:15.960
Every method you ever heard of in machine learning can be viewed as one of those two.

44:15.960 --> 44:21.120
Most probabilistic methods actually belong to the contrastive category.

44:22.480 --> 44:27.400
Anything that uses Monte Carlo sampling, for example, is contrastive.

44:27.400 --> 44:30.680
And then things like sparse coding and k-means and

44:30.680 --> 44:33.760
things like that are more on the regularized method side of things.

44:35.880 --> 44:39.320
Okay, so I'm asking you to do four things.

44:39.320 --> 44:44.200
Abandoned generative models in favor of the joint embedding architectures, right?

44:44.200 --> 44:47.280
So generative models are the most popular thing at the moment.

44:47.280 --> 44:53.760
Forget about it, at least if you're interested in getting to the next step in AI.

44:53.760 --> 44:56.600
Abandoned probabilistic models, because if you have those joint embedding

44:56.600 --> 45:00.360
architectures, you cannot actually use it to derive a pure y given x.

45:02.160 --> 45:04.200
The only thing you can use is sort of energy-based view.

45:05.920 --> 45:08.600
Abandoned contrastive methods in favor of those regularized methods,

45:08.600 --> 45:11.160
which I'll talk about a bit more.

45:11.160 --> 45:13.480
And then something I've said for many years now, abandoned reinforcement

45:13.480 --> 45:14.600
modeling because it's too inefficient.

45:17.360 --> 45:19.880
So those are some of the pillars of machine learning.

45:21.000 --> 45:25.800
And I realize this is not a very popular opinion here, but okay.

45:25.800 --> 45:27.640
So what about those regularized methods?

45:27.640 --> 45:28.720
I'm just going to give you one example.

45:28.720 --> 45:29.480
There's a whole bunch of them.

45:29.480 --> 45:35.360
There's like a dozen of them, but I'm just going to give you one called Vicreg.

45:35.360 --> 45:41.280
And the basic idea of it is to prevent those representations from collapsing.

45:41.280 --> 45:44.040
We're going to use a criterion that attempts to maximize the information

45:44.040 --> 45:45.880
content that comes out of those representations.

45:47.320 --> 45:50.240
Okay, so we're going to measure the information content in some way, and

45:50.240 --> 45:53.280
then maximize the information content or minimize the negative information content.

45:54.480 --> 45:57.160
We're going to do this for both SX and SY.

45:57.160 --> 45:59.480
We're also going to minimize the prediction error.

45:59.480 --> 46:01.200
And if we have a latent variable, we're going to have to minimize

46:01.200 --> 46:03.240
the information content of that latent variable.

46:03.240 --> 46:05.160
I can't explain why, because it would take too long.

46:05.160 --> 46:09.640
But you have to do that also to prevent another type of collapse.

46:09.640 --> 46:12.040
I'm going to focus on how you do that.

46:12.040 --> 46:16.760
So the sad news is we don't have good ways to measure information content, or

46:16.760 --> 46:21.400
we don't have any good ways to estimate lower bounds on information content,

46:21.400 --> 46:25.000
so that if we push up on this lower bound, the information content will go up.

46:25.000 --> 46:27.320
We only have upper bounds for information content.

46:27.320 --> 46:30.640
So we're going to do a very stupid thing, which is push up on the upper bound of

46:30.640 --> 46:33.760
information content, and hope the actual information content will follow.

46:35.840 --> 46:36.280
And it works.

46:37.280 --> 46:44.800
So, there's a simple way to prevent the encoder from completely collapsing.

46:44.800 --> 46:48.400
Which is to insist that every variable in SX, SX is a vector.

46:48.400 --> 46:51.720
And you insist that every variable, as measured over a batch,

46:51.720 --> 46:54.960
has a standard deviation that is at least one.

46:54.960 --> 46:57.680
Okay, so this is the cost that you see at the top here.

46:59.200 --> 47:03.120
Measure the standard deviation of each component of SX, and

47:03.120 --> 47:06.200
put it in a hinge loss so that the standard deviation is at least one.

47:07.320 --> 47:09.560
So that prevents the system from completely collapsing.

47:09.560 --> 47:16.200
But it can still cheat by making all the components of SX equal or correlated.

47:16.200 --> 47:20.840
So the second term says I want to minimize the off diagonal terms of

47:20.840 --> 47:24.520
the covariance matrix of those vectors measured over a batch, right?

47:24.520 --> 47:27.600
So I want pairs of variables to be uncorrelated.

47:28.960 --> 47:31.800
So basically, the collection of those two criterion says,

47:31.800 --> 47:36.400
if I measure the covariance matrix of those vectors, SX and SY, coming out over

47:36.400 --> 47:40.800
a batch, I want the covariance matrix to be as close to the identity as possible.

47:42.520 --> 47:45.880
There's a number of different methods that have been proposed to,

47:45.880 --> 47:47.960
that are kind of similar to this, Barlow-Twins.

47:47.960 --> 47:53.040
So this one is called Vic-Rag from my group at Meta in collaboration with Jean-Ponce.

47:54.840 --> 47:59.980
And then variations of it, but like similar methods from Berkeley in

48:00.940 --> 48:03.940
the E-Mise group at Berkeley called NCR squared.

48:03.940 --> 48:05.660
Yeah, maybe one minute.

48:05.660 --> 48:09.420
Yeah, so this works really well and

48:09.420 --> 48:16.060
I'm going to not bore you with tables of results that show you how well it works.

48:16.060 --> 48:21.860
Only to mention something else, which is another method to do this kind of

48:21.860 --> 48:27.180
self supervised running which is closer to this JEPA architecture called IJEPA.

48:27.180 --> 48:32.340
So this is for learning features for images without having to do the documentation.

48:32.340 --> 48:35.340
But basically it's for masking and this works amazingly well, it's very fast.

48:35.340 --> 48:38.180
It's a new method, paper is on archive.

48:39.980 --> 48:45.740
I don't have time to explain how it works, but basically you run an image

48:45.740 --> 48:49.260
through two encoders, one is the full image and

48:49.260 --> 48:53.860
the other one is sort of a masked image, partially masked image.

48:53.860 --> 48:56.460
You run them through the same encoder or very similar encoder and

48:56.460 --> 49:01.220
you try to predict or to predict the full feature representation of the full image

49:01.220 --> 49:04.100
from the representation obtained from the partial image.

49:04.100 --> 49:07.100
And just doing this produces really good features for images.

49:07.100 --> 49:11.420
You get really good performance on object recognition in images and stuff like that.

49:11.420 --> 49:14.020
Again, tables that show you that's true.

49:14.020 --> 49:18.820
But I'm coming to the end, so the reason for

49:18.820 --> 49:21.140
training those JEPA is to build world models.

49:21.140 --> 49:23.060
So architectures are this type.

49:23.060 --> 49:26.180
So this is a JEPA, but it's also a world model.

49:26.180 --> 49:29.140
That, given an observation about the state of the world,

49:29.140 --> 49:32.980
is going to be able to enter an action or imagined action in latent variables.

49:32.980 --> 49:34.820
It's going to predict what's going to happen next in the world.

49:35.900 --> 49:39.380
And once the time passes by, we're going to observe what happens and

49:39.380 --> 49:43.260
then perhaps adjust our system to train.

49:43.260 --> 49:47.500
But we want to use a hierarchical version of this where we can have a higher

49:47.500 --> 49:52.540
level, higher abstraction, higher level of abstraction representation that will

49:52.540 --> 49:54.620
allow us to make predictions further in the future.

49:55.580 --> 49:59.100
Okay, I can't tell you the details of how I'm going to get to the train station,

49:59.100 --> 50:02.860
but I know I'm going to have to be at the train station around 4 PM.

50:02.860 --> 50:06.100
Okay, so that's the high level.

50:06.100 --> 50:09.460
And we have early experiments with sort of various complicated neural net

50:09.460 --> 50:12.820
architectures which I'm not going to detail to train from video,

50:12.820 --> 50:16.180
try to predict basically what's going to happen in the video using warping and

50:16.180 --> 50:17.620
stuff like that and it works really well.

50:17.620 --> 50:22.700
But in the end, what we'll have is a hierarchical system from which we can do

50:22.700 --> 50:26.780
a hierarchical planning and then we'll have been trained to predict what's

50:26.780 --> 50:30.020
going to happen in the world as a consequence of actions or

50:30.020 --> 50:32.820
latent variables that we can observe, that we can infer.

50:34.940 --> 50:38.620
And those systems will be able to plan and reason and

50:38.620 --> 50:41.740
will be controllable because the behavior is entirely controlled by the cost

50:41.740 --> 50:43.380
functions we ask you to minimize.

50:44.500 --> 50:50.740
And so much more controllable than current LNMs and that's pretty much the end.

50:50.740 --> 50:53.820
So cell supervised learning is really the ticket.

50:53.820 --> 50:58.580
Handling and certainty can be done with this energy-based model method and

50:58.580 --> 51:02.500
using the joint embedding architecture that allows us to avoid predicting all

51:02.500 --> 51:04.100
the details that are irrelevant about the world.

51:06.020 --> 51:08.700
Learning world models from observation and interaction and

51:08.700 --> 51:13.540
then reasoning and planning is done by basically gradient-based minimization

51:13.540 --> 51:15.300
with respect to actions.

51:15.300 --> 51:16.460
And that's it, thank you very much.

51:16.460 --> 51:36.020
Thank you, John, for the great talk.

51:36.020 --> 51:40.260
Now we'll have the second part, which is the Fireside Chat between John and

51:40.260 --> 51:42.260
Osama, so please.

51:42.260 --> 51:46.220
Thank you very much, John.

51:46.220 --> 51:50.940
It was actually truly inspirational because it is definitely different

51:50.940 --> 51:54.420
than your typical machine learning talk, so I enjoyed that.

51:54.420 --> 51:57.860
Well, to you to throw away all the basic pillars of machine learning, so yes.

52:00.420 --> 52:04.060
So I've collected a bunch of questions, some coming from the audience,

52:04.060 --> 52:07.420
some coming from our institute and our faculty.

52:07.420 --> 52:12.260
And we'll try to go through them in 20 minutes or whatever we can cover.

52:13.260 --> 52:17.580
Normally, I would commit to answering every question on social media, but

52:17.580 --> 52:22.220
because we got 150 questions, I'm afraid to commit my time or yours to this at

52:22.220 --> 52:24.100
this point, but we'll try our best.

52:25.540 --> 52:26.940
So I'll start with my first question.

52:28.260 --> 52:31.780
It's been a long-standing wisdom in statistical inference and

52:31.780 --> 52:37.700
probabilistic reasoning that when the number of parameters of a model gets

52:37.700 --> 52:42.100
large enough, you kind of lose your ability to generalize and

52:42.100 --> 52:45.740
you start just memorizing data, and we all know that that's no good.

52:45.740 --> 52:49.980
That's just too detailed, the bias variance trade off.

52:49.980 --> 52:56.540
But somehow, deep learning seems to have broken through this barrier.

52:56.540 --> 53:01.300
When we went from regular neural nets to the deep nets, and

53:01.300 --> 53:07.220
is there an intuition or understanding today as to why this is working in

53:07.220 --> 53:10.980
LLMs with hundreds of billions and now trillions of parameters.

53:10.980 --> 53:14.580
Right, well, the fact that it is working,

53:14.580 --> 53:19.180
that you can train a ridiculously over-sized neural net, and

53:19.180 --> 53:26.460
it will still work reasonably and generalize is dumb-founding.

53:26.460 --> 53:29.860
So much that it contradicts every single thing that has been written in every

53:29.860 --> 53:31.700
statistical textbook.

53:31.700 --> 53:34.980
That you should never have more parameters than you have training samples, right?

53:35.020 --> 53:37.580
If you're fitting a polynomial or something like this.

53:37.580 --> 53:40.380
But we knew experimentally, even in the late 80s and early 90s,

53:40.380 --> 53:42.740
that you could make those neural nets pretty big.

53:42.740 --> 53:45.860
And even if you didn't have a huge amount of training data,

53:45.860 --> 53:47.500
it would still work pretty well.

53:47.500 --> 53:49.460
There was just no theoretical explanation.

53:49.460 --> 53:52.700
So the theorists told us, you're wrong, you're stupid.

53:53.980 --> 53:57.380
This cannot possibly work, so I'm not gonna believe your results.

53:57.380 --> 54:01.540
And that's in part what made it very difficult to get neural nets

54:01.540 --> 54:05.660
accepted in the late 90s to the 2000s.

54:07.860 --> 54:11.140
But it turns out there is a phenomenon that has since been named

54:11.140 --> 54:16.140
double descent, which is that if you increase the number of parameters in

54:16.140 --> 54:22.180
a model for a constant size training set, your training error,

54:22.180 --> 54:24.780
of course, is gonna go down, right, to zero, probably.

54:26.100 --> 54:30.460
But your test error is first gonna go down, go through a minimum, and

54:30.500 --> 54:35.060
then go up when you start having parameters,

54:35.060 --> 54:39.020
a number of parameters that is commensurate with the number of samples that you have.

54:39.020 --> 54:43.820
Okay, so that's when the model starts to be over parameterized, and it goes up.

54:45.180 --> 54:48.540
But here is the thing, if you keep going up, if you keep making the model more

54:48.540 --> 54:52.180
complex, the tester will go down again.

54:52.180 --> 54:54.020
It will go through a maximum and then go down again.

54:54.020 --> 54:56.620
That's called the double descent phenomenon, nowadays.

54:56.620 --> 55:01.980
And it will do this if you regularize the parameters somehow.

55:01.980 --> 55:06.060
You don't necessarily need to regularize explicitly because neural nets have some

55:06.060 --> 55:08.180
sort of implicit regularization in them.

55:08.180 --> 55:13.980
But you see this phenomenon, even works if you fit a polynomial, right?

55:13.980 --> 55:19.700
Fit a 10 degree polynomial with 11 data points.

55:20.820 --> 55:23.180
And your fit will be horrible, right?

55:23.180 --> 55:25.820
Because the polynomial has to go to every single point and

55:25.820 --> 55:27.540
it's gonna go wild in between.

55:27.540 --> 55:30.260
But if you increase the degree of the polynomial to something like 20 or

55:30.260 --> 55:33.860
30, and you regularize the coefficient, your error goes down again,

55:33.860 --> 55:38.980
your test error goes down again, the fitted polynomial goes through every point.

55:38.980 --> 55:44.340
But it's less irregular than with just degree 10.

55:44.340 --> 55:48.540
So this existed all along, it's just that people didn't realize it was a thing,

55:48.540 --> 55:52.020
or at least people who were not practitioners of neural nets who

55:52.020 --> 55:53.660
had realized this was a thing.

55:53.660 --> 55:55.700
So do we have any explanation why this is a thing?

55:57.180 --> 56:01.820
So there's a lot of conjectures, there is some theoretical work.

56:01.820 --> 56:04.820
Some people claim it's about the dynamics of gradient descent.

56:04.820 --> 56:08.380
There is some sort of implicit self regularization in neural nets that occurs.

56:09.660 --> 56:13.140
Whereby the system kind of recruits just a number of virtual

56:13.140 --> 56:15.140
parameters that it needs somehow.

56:16.620 --> 56:19.780
Some say it's regularization due to stochastic gradient.

56:19.780 --> 56:22.100
So stochastic gradient descent, which is noisy.

56:22.100 --> 56:28.260
And so perhaps that forces the system to find robust minima in

56:28.260 --> 56:32.900
the objective, in the loss, that generalize better.

56:34.340 --> 56:36.100
It's not clear, there's a bunch of different things.

56:36.100 --> 56:39.540
Yeah, definitely one of the mysteries that keep us interested.

56:41.300 --> 56:45.500
This question comes from Raman Chandrasekharan or

56:45.500 --> 56:49.260
Chandra, who's one of our senior research scientists in Seattle.

56:50.140 --> 56:56.660
How long before LLM, and maybe, I don't know, models in general,

56:56.660 --> 57:01.340
can genuinely start saying, I don't know the answer to this question.

57:01.340 --> 57:05.700
As opposed to attempting to guess the right autocomplete anyway,

57:05.700 --> 57:07.060
because that's what it's programmed to do.

57:08.060 --> 57:12.700
Yeah, so I don't think current LLMs can really do this at the moment.

57:12.700 --> 57:15.900
I think it's probably possible with architectures, the type that I show.

57:15.900 --> 57:21.300
Because if there are no good minima to the objective that the system is

57:21.300 --> 57:25.020
attempting to minimize to produce it, it's output, it's gonna say, well,

57:25.020 --> 57:29.180
I found this thing, it seems to be minimizing this objective, but not very well.

57:29.180 --> 57:31.740
So probably it's not the right answer you were looking for.

57:33.180 --> 57:37.940
Or by the shape of the minimum, of this energy minimum, perhaps, you could say,

57:37.940 --> 57:42.100
like if it's really a sharp minimum, then that's the one answer that

57:42.100 --> 57:44.500
corresponds to the question.

57:44.500 --> 57:48.180
If it's kind of a shadow minimum, maybe there are multiple answers that are possible.

57:48.180 --> 57:55.180
So you might be able to attribute, to map energy levels to,

57:55.180 --> 57:59.340
of different answers to a confidence level.

58:02.620 --> 58:06.980
To, this is a question from me, I guess.

58:06.980 --> 58:12.340
Two aspects of critical importance to,

58:12.340 --> 58:17.620
let's say, GPT or large language models that are not talked about a lot by

58:17.620 --> 58:22.140
the companies who do them are data curation.

58:22.140 --> 58:27.260
Getting that clean data, that balanced data, that representative data,

58:27.260 --> 58:34.300
which by the way, counter to popular belief, open AI spent a lot of its money

58:34.300 --> 58:39.660
on curating just that right corpus so that they can do the training reliably.

58:39.660 --> 58:46.460
And the second part, which is something we're big believers in at the Institute for

58:46.460 --> 58:50.820
experiential AI, experiential AI stands for AI with the human in the loop.

58:50.820 --> 58:54.740
Having that human intervention through relevance feedback,

58:54.740 --> 58:59.300
that we know now open AI is doing and has been doing.

58:59.300 --> 59:02.900
And some of the queries are actually taken over by humans at some point when

59:02.900 --> 59:04.740
they make enough errors to come back.

59:04.740 --> 59:07.660
But the good thing is they learn from them and we think that's a great practice.

59:08.660 --> 59:13.700
Why do you think the companies don't want to talk about the importance of the data

59:13.700 --> 59:15.220
and the importance of the human in the loop?

59:16.340 --> 59:17.700
I don't know if they don't want to talk about it.

59:17.700 --> 59:26.540
I mean, it's clearly very expensive to create data to produce a good LLM.

59:26.540 --> 59:32.460
But in my opinion, it's doomed to failure in the long run for two reasons.

59:32.580 --> 59:41.740
The first one is the curation requires going through this enormous amount of data that

59:41.740 --> 59:42.980
you want to train the system on.

59:42.980 --> 59:47.140
And any data you eliminate, it's less training data for your model.

59:48.740 --> 59:52.660
But the second thing is even with human feedback,

59:52.660 --> 59:58.660
human feedback that rate different answers or fine tune the system for

59:58.660 --> 01:00:02.460
certain question and answers, sort of manually curated.

01:00:04.740 --> 01:00:09.620
If you want those systems ultimately to be the repository of all human knowledge,

01:00:10.740 --> 01:00:14.980
the dimension of that space of all human knowledge is enormous.

01:00:14.980 --> 01:00:18.180
And you're not going to do it by paying a few thousand people in Kenya or

01:00:18.180 --> 01:00:21.860
India rating answers.

01:00:21.860 --> 01:00:28.500
You're going to have to do it with millions of volunteers that find

01:00:28.500 --> 01:00:32.180
the system for all possible questions that might possibly be asked.

01:00:32.180 --> 01:00:38.100
And those volunteers will have to be vetted in the way Wikipedia is being done, right?

01:00:38.100 --> 01:00:45.220
So think of LLMs in the long run as a version of Wikipedia plus your favorite

01:00:45.220 --> 01:00:50.500
newspapers plus the scientific literature plus everything, but you can talk to it.

01:00:50.500 --> 01:00:52.700
You don't have to read articles, you can just talk to it.

01:00:53.660 --> 01:00:58.460
And so if it's supposed to become the repository of all human knowledge,

01:00:58.460 --> 01:01:04.020
the thing it's been trained to do will have to be curated by

01:01:04.020 --> 01:01:10.220
quite sourcing the way Wikipedia is to cover all the possible things that

01:01:10.220 --> 01:01:11.980
may be covered.

01:01:11.980 --> 01:01:19.180
This is a very strong argument for having open source based models for LLMs.

01:01:19.260 --> 01:01:24.540
So in my opinion, the future is inevitably going to be that you're going to

01:01:24.540 --> 01:01:29.740
have a small number of open source based LLMs that are not trained for

01:01:29.740 --> 01:01:34.780
any particular application, they're trained on enormous amounts of data

01:01:34.780 --> 01:01:35.900
that requires a lot of money.

01:01:35.900 --> 01:01:38.380
So you're not going to have 25 of them, you're going to have two or three.

01:01:39.660 --> 01:01:42.860
And then actual applications are going to be built on top of it by

01:01:42.860 --> 01:01:46.620
finding those systems for particular vertical applications.

01:01:46.620 --> 01:01:47.340
That's the future.

01:01:48.060 --> 01:01:53.020
Sadly, in the industry, there are people who are lobbying governments to

01:01:53.020 --> 01:01:57.820
actually make the open sourcing of large scale LLM illegal.

01:01:59.260 --> 01:02:05.340
What they're worried about is potential misuse of LLMs by bad actors,

01:02:06.460 --> 01:02:08.300
potential users.

01:02:09.900 --> 01:02:13.820
So some people in the US, for example, are worried, oh, if we open source our LLMs,

01:02:13.820 --> 01:02:17.740
you know China and North Korea and Iran will put their hands on it and that's

01:02:17.740 --> 01:02:18.300
going to be bad.

01:02:20.940 --> 01:02:24.380
And then some people are worried that the real powerful LLMs are going to be

01:02:24.380 --> 01:02:28.860
super intelligent and destroy humanity, which I think is preposterous,

01:02:30.220 --> 01:02:33.100
even though some of my friends that I respect actually believe this.

01:02:34.220 --> 01:02:40.140
So I think it would be really, really bad if those lobbying attempts succeed.

01:02:40.860 --> 01:02:44.860
I'm very much in favor of a future with open based models.

01:02:45.580 --> 01:02:48.220
And there's going to be bad actors, but there's going to be countermeasures

01:02:48.220 --> 01:02:49.020
against them.

01:02:49.020 --> 01:02:57.100
It's going to be, you know, or powerful good AI cop against their nefarious AI, essentially.

01:03:00.060 --> 01:03:03.660
So let's shift to this trend.

01:03:03.660 --> 01:03:09.340
And this is, I've merged a question from Jimmy Shanahan from our AI solutions hub

01:03:09.900 --> 01:03:14.060
with questions from Tomo Lasovic and Ken Church at EAI.

01:03:15.660 --> 01:03:22.140
The trend nowadays seems to be heading towards bigger is better, more compute, more parameters.

01:03:23.580 --> 01:03:29.420
There's been some studies even suggesting that by open AI themselves that they're moving at a

01:03:29.420 --> 01:03:35.660
pace faster than Moore's law, even though now they seem to be normalizing towards it,

01:03:35.660 --> 01:03:38.060
although Moore's law itself is slowing down.

01:03:38.300 --> 01:03:42.460
So the real question here is how long can this go on?

01:03:42.460 --> 01:03:44.380
And will we ask them, what do you think?

01:03:44.380 --> 01:03:48.220
I know that we may not have the final answer here, but it seems crazy.

01:03:48.220 --> 01:03:51.740
Like all you have to do is wait a few weeks and you hear about the next big model.

01:03:52.460 --> 01:03:56.220
Well, so actually in the last few months, you've seen a decrease in the size.

01:03:57.340 --> 01:04:03.420
So Lama, for example, the 13 billion version of Lama in terms of raw performance on standard

01:04:03.420 --> 01:04:08.380
benchmarks is actually better than GPT-3, which has 175 billion parameters.

01:04:08.940 --> 01:04:12.140
And so it's not clear that bigger is better.

01:04:12.140 --> 01:04:17.980
With the architecture I propose, I think you can get away with smaller systems that

01:04:17.980 --> 01:04:19.100
perform at least as well.

01:04:19.100 --> 01:04:22.700
The reason being that when you train in a current autoregressive LLM,

01:04:22.700 --> 01:04:27.100
you have to train it to not just accumulate knowledge, not just predict the next word,

01:04:27.100 --> 01:04:28.460
but also solve a lot of problems.

01:04:28.460 --> 01:04:35.020
So basically, know how to produce the right answer when you specify the question in the

01:04:35.020 --> 01:04:35.660
prompt.

01:04:35.660 --> 01:04:39.260
And so everything is wrapped into the weights of that single model.

01:04:39.260 --> 01:04:42.060
Whereas in the model I propose here, the architecture I propose,

01:04:42.780 --> 01:04:44.460
the word model is just a word model.

01:04:45.100 --> 01:04:49.980
The task is specified by the objective function, which may include the prompt.

01:04:49.980 --> 01:04:51.820
So it may include the representation of the prompt.

01:04:52.700 --> 01:04:55.100
And so you're separating different things.

01:04:55.100 --> 01:05:01.900
You're separating the inference procedure that produces the output from the word model,

01:05:01.900 --> 01:05:05.260
the sort of the mental model of the world that the system uses,

01:05:05.260 --> 01:05:08.140
from the task itself, which is specified by the objective.

01:05:08.140 --> 01:05:11.500
And you can probably get away with smaller networks for the same performance.

01:05:13.100 --> 01:05:19.900
But yes, I mean, there were a few years ago models by Google that had like a trillion

01:05:19.900 --> 01:05:20.380
parameters.

01:05:20.380 --> 01:05:23.900
There were basically multiple models that were stuck together with some sort of

01:05:25.740 --> 01:05:26.300
gating.

01:05:26.300 --> 01:05:30.700
Yeah, between them, they've kind of backpedaled on this a little bit.

01:05:30.700 --> 01:05:33.740
If you want the system to be practical, like to be used by everyone,

01:05:33.740 --> 01:05:35.740
you can't make them like a trillion parameters.

01:05:35.740 --> 01:05:37.260
Right now, it'd be just too expensive.

01:05:38.380 --> 01:05:40.460
So you have to minimize that size.

01:05:40.460 --> 01:05:45.820
Now you can run things like Lama 7 billion on a Mac.

01:05:47.340 --> 01:05:49.100
You know, you can run on a laptop.

01:05:49.100 --> 01:05:51.260
You can't train it on a laptop, but you can run it.

01:05:51.260 --> 01:05:51.500
Yes.

01:05:51.660 --> 01:05:59.660
So clearly, you believe you're advocating for a different view of what the machine learning

01:05:59.660 --> 01:06:03.820
and AI community should be doing as opposed to what they are doing today.

01:06:04.780 --> 01:06:06.380
That's the story of my career.

01:06:06.380 --> 01:06:06.620
Yes.

01:06:08.060 --> 01:06:10.380
And this question is coming from Ken Church.

01:06:11.420 --> 01:06:13.340
A former colleague from AT&T.

01:06:13.340 --> 01:06:14.620
From AT&T.

01:06:14.620 --> 01:06:17.660
He is at the Institute for AI in Silicon Valley.

01:06:18.620 --> 01:06:19.340
Do you believe?

01:06:21.660 --> 01:06:27.820
Well, I guess the question is, how long do you think it will take to pivot the field from

01:06:27.820 --> 01:06:30.300
where it is to where you would like it to be?

01:06:32.860 --> 01:06:34.780
Well, last time I tried, it took 15 years.

01:06:38.060 --> 01:06:41.340
If not more, actually, depending on how you count, it might have been 20.

01:06:42.300 --> 01:06:45.180
So I don't know.

01:06:45.180 --> 01:06:50.860
I think I see a phenomenon in kind of this is a sociology of science question.

01:06:51.420 --> 01:06:54.540
When there is something that seems to work, everybody gets excited about it.

01:06:54.540 --> 01:07:03.340
And it's a fashion trend type phenomenon where every paper written is about this trend.

01:07:03.340 --> 01:07:07.260
I saw this in computer vision back in the early to mid 2000.

01:07:07.260 --> 01:07:09.340
Everybody was working on boosting.

01:07:09.340 --> 01:07:12.780
That was the thing, you had to work on boosting for computer vision.

01:07:12.780 --> 01:07:21.180
And then someone in 2006 and 2005 came up with a different way of doing vision using dense

01:07:21.980 --> 01:07:26.620
features like sift and stuff like that using unsupervised running for a middle layer and then

01:07:26.620 --> 01:07:27.820
an SVM on top.

01:07:27.820 --> 01:07:29.420
All of a sudden, everybody was doing this.

01:07:30.540 --> 01:07:34.140
And then starting in 2013, everybody started using convolutional nets.

01:07:36.300 --> 01:07:37.740
That came from results.

01:07:37.740 --> 01:07:40.620
So now we are in a phase where everybody is focused on LLMs.

01:07:41.180 --> 01:07:43.980
And if you don't work on LLMs, nobody wants to talk to you.

01:07:45.820 --> 01:07:47.180
But it will change.

01:07:49.420 --> 01:07:50.940
So you think it's 15 years?

01:07:50.940 --> 01:07:52.700
No, I think it's more like five.

01:07:52.700 --> 01:07:57.340
Like I made that prediction that autoregressive LLMs will probably...

01:07:57.340 --> 01:07:59.420
Five years, that's true, yeah, they're doomed.

01:07:59.980 --> 01:08:01.740
Yeah, I mean, I might be wrong, obviously.

01:08:02.460 --> 01:08:04.300
We will hold you to that.

01:08:04.300 --> 01:08:06.380
I'll come back and revisit in five years.

01:08:06.380 --> 01:08:09.740
Maybe it's a wishful thinking, self-fulfilling prophecy perhaps.

01:08:11.260 --> 01:08:15.500
A question for something different here from Sam Scarpino,

01:08:15.500 --> 01:08:19.100
director of AI and Life Sciences at the Institute for Experiential AI.

01:08:19.980 --> 01:08:26.860
What are the biggest gaps on the education side for graduates of higher education in AI

01:08:27.420 --> 01:08:31.260
and in particular the new directions AI is taking?

01:08:31.260 --> 01:08:33.260
What do you think is missing?

01:08:34.220 --> 01:08:37.420
So I think what's missing...

01:08:37.420 --> 01:08:41.660
So it depends which major you're following.

01:08:44.620 --> 01:08:52.860
Most computer science curricula in the US are very weak in mathematics.

01:08:53.820 --> 01:08:58.540
The requirements for mathematics in a typical CS degree,

01:08:58.540 --> 01:09:01.020
the minimum requirement is very, very small, right?

01:09:01.020 --> 01:09:05.420
It's one course in discrete math and perhaps in algebra if you're lucky.

01:09:06.380 --> 01:09:09.180
Maybe a probability if you are courageous.

01:09:11.100 --> 01:09:12.460
But what about optimization?

01:09:12.460 --> 01:09:14.780
That would be something that would be very useful.

01:09:14.780 --> 01:09:20.140
And then there is courses in physics because the mathematics of inference

01:09:20.140 --> 01:09:26.220
and variational autoencoder and stuff like that, graphical models, etc.

01:09:26.220 --> 01:09:28.860
The mathematics of this is from statistical physics.

01:09:29.820 --> 01:09:34.060
And so if you have a choice between taking your course in, I don't know,

01:09:34.940 --> 01:09:38.860
mobile app programming or quantum mechanics, take quantum mechanics.

01:09:40.700 --> 01:09:41.340
I'm not kidding.

01:09:48.540 --> 01:09:55.340
This is a question that came from the audience and a few of the people at the Institute.

01:09:56.220 --> 01:10:01.100
Your thoughts on the current, you know, these recent congressional hearings where

01:10:02.780 --> 01:10:09.900
certainly seems like much of the testimony by some Altman was understandably self-serving.

01:10:09.900 --> 01:10:14.860
You know, they need to be allowed to compete and have their way of working protected.

01:10:15.420 --> 01:10:19.020
At the same time, he's encouraging the rest of the community to be open source.

01:10:19.660 --> 01:10:24.700
What would you have said to Congress?

01:10:24.700 --> 01:10:25.820
Have you been on those hearings?

01:10:28.220 --> 01:10:29.020
I was not invited.

01:10:30.860 --> 01:10:33.100
I was not invited to the White House either before that.

01:10:35.980 --> 01:10:45.420
So what I would have recommended is that if you want a vibrant ecosystem on top of current AI technology,

01:10:46.380 --> 01:10:53.180
you need to have sort of open source based models on top of which an industry can be built.

01:10:54.380 --> 01:10:59.660
And that industry will build vertical applications for particular domains

01:10:59.660 --> 01:11:00.940
on top of a base model.

01:11:00.940 --> 01:11:06.220
You don't want to have 25 companies selling 25 different base models

01:11:07.660 --> 01:11:08.780
and keep them closed source.

01:11:08.780 --> 01:11:12.540
If you want an industry to be built on top of it, the infrastructure has to be open.

01:11:13.260 --> 01:11:18.140
Because that's the only way to really sort of know what you're doing, essentially.

01:11:19.660 --> 01:11:22.140
And to have some control about your future, right?

01:11:22.140 --> 01:11:25.820
You can't just go like this and pray that.

01:11:25.820 --> 01:11:27.020
Unix versus Windows.

01:11:27.660 --> 01:11:32.060
Right, so if you go back to the history of the Internet, there was a similar story where

01:11:33.340 --> 01:11:37.740
back in 1992 when the built Internet Al Gore started to figure out like what,

01:11:37.740 --> 01:11:39.900
you know, how do we build the information superhighway?

01:11:40.860 --> 01:11:46.140
They went to see, you know, the big communication companies like AT&T and AT&T told them,

01:11:46.140 --> 01:11:47.420
oh, you know, leave it to us.

01:11:47.420 --> 01:11:48.620
We'll build the stuff.

01:11:48.620 --> 01:11:52.620
It's going to be, you know, ATM and ISD enter the home and blah, blah, blah.

01:11:52.620 --> 01:11:56.140
It'll be wonderful and you'll have to pay, you know, $5 per hour.

01:11:58.220 --> 01:11:59.420
And Al Gore said no.

01:11:59.420 --> 01:12:03.820
He said we're going to make the, what was an ARPANET that became the Internet,

01:12:04.060 --> 01:12:10.540
basically available to the public and delocalized and, you know, self,

01:12:11.820 --> 01:12:15.500
basically open in terms of standard and no company is going to control it.

01:12:16.060 --> 01:12:18.140
And that was a really, really good idea.

01:12:18.140 --> 01:12:19.340
We can thank Al Gore for this.

01:12:20.060 --> 01:12:21.900
The world can thank Al Gore, not just the U.S.

01:12:27.660 --> 01:12:28.780
He did invent the Internet.

01:12:29.500 --> 01:12:33.980
And then a similar story happened several years later when people started to realize

01:12:33.980 --> 01:12:39.340
that you could use, you know, graphic browsers like Mosaic and Netscape and stuff like that,

01:12:39.340 --> 01:12:43.340
right, when the World Wide Web became popular.

01:12:44.540 --> 01:12:47.900
So there was a war between Sun Microsystems and Microsoft.

01:12:47.900 --> 01:12:52.060
Sun Microsystems said, oh, we're going to sell you servers running Solaris,

01:12:52.060 --> 01:12:56.300
the version of Unix, with, you know, our web server infrastructure and Java.

01:12:57.180 --> 01:12:59.100
And you're going to be able to build, like, anything you want.

01:12:59.980 --> 01:13:05.020
Microsoft said no, it's going to be Windows NT with the IIT web server and the ASP

01:13:05.980 --> 01:13:11.580
website, you know, server-side protocol framework, whatever.

01:13:12.380 --> 01:13:13.100
They both lost.

01:13:14.220 --> 01:13:22.540
Sun Microsystem went bankrupt, was sold for parts to Oracle, and Microsoft essentially

01:13:22.540 --> 01:13:25.100
exited the market.

01:13:25.740 --> 01:13:28.460
One was Linux and Apache, open source.

01:13:28.460 --> 01:13:34.540
And the reason is because it's such an essential basic infrastructure that it has to be open.

01:13:35.980 --> 01:13:42.220
It progresses faster if it's open, and it's more reliable, it's more secure.

01:13:42.220 --> 01:13:43.580
I mean, there's all the advantages.

01:13:43.580 --> 01:13:46.460
And, you know, it's easier for startups to build on top of it.

01:13:46.460 --> 01:13:51.740
So in the future, we're going to see AI systems as basic infrastructure.

01:13:52.700 --> 01:13:56.860
All of our interactions ten years from now with the digital world will be through

01:13:57.580 --> 01:14:02.620
an intelligent virtual agent that will be with us all the time.

01:14:02.620 --> 01:14:06.460
It's like every one of us will have a staff of intelligent people working for us.

01:14:07.820 --> 01:14:08.540
Okay?

01:14:08.540 --> 01:14:12.540
We shouldn't be threatened by the fact that those things will be smarter than us.

01:14:12.540 --> 01:14:16.780
Like everybody that, you know, is working with me at fair is smarter than me.

01:14:16.780 --> 01:14:19.340
So I don't feel threatened by that.

01:14:19.340 --> 01:14:22.460
You're not a very good manager if you're threatened by people who are smarter than you.

01:14:22.620 --> 01:14:28.460
So your purpose actually should be to hire people, only people who are smarter than you.

01:14:28.460 --> 01:14:33.100
But anyway, so we're going to have those intelligent systems that are going to be

01:14:33.100 --> 01:14:36.060
under control that are going to help us, you know, daily lives.

01:14:36.060 --> 01:14:41.420
And we need those systems to be open because if it's kind of a closed system controlled by

01:14:41.420 --> 01:14:46.220
some company in California, it's going to be able to control our entire

01:14:46.380 --> 01:14:50.620
knowledge and data diet.

01:14:51.580 --> 01:14:53.100
And that's just too dangerous.

01:14:53.100 --> 01:14:55.500
And it's not necessary.

01:14:55.500 --> 01:15:00.060
It's necessary for a search engine or a social network because it has to be centralized for

01:15:00.060 --> 01:15:00.700
various reasons.

01:15:00.700 --> 01:15:04.220
But for an agent like this, it could run on your local device.

01:15:04.220 --> 01:15:05.500
It could run on your laptop.

01:15:05.500 --> 01:15:09.740
You don't have to talk necessarily with big servers in California.

01:15:09.740 --> 01:15:14.460
You don't want to give all your, you know, deepest secrets to that.

01:15:14.460 --> 01:15:19.020
So it's going to have to be an open fact form for that reason.

01:15:20.140 --> 01:15:25.580
If nothing else, governments around the world are going to insist that this is the case.

01:15:25.580 --> 01:15:33.340
So that's why I would tell Congress, make it so that, like, don't ban open source

01:15:34.140 --> 01:15:35.020
LLMs.

01:15:35.020 --> 01:15:36.460
They're not going to destroy humanity.

01:15:37.740 --> 01:15:40.860
Yeah, they're going to be bad actors, but you know, you can have countermeasures

01:15:41.660 --> 01:15:43.420
and make it open.

01:15:43.420 --> 01:15:45.420
It's the only way to make it safe.

01:15:53.420 --> 01:15:56.220
I'll ask, we'll make this a quick question with a quick answer.

01:15:56.220 --> 01:16:00.140
And then I know we have some questions live, so we'll switch to those.

01:16:04.940 --> 01:16:08.220
In a way, you kind of answered this question when you said LLMs are doomed.

01:16:08.220 --> 01:16:17.180
But if LLMs were to become perfect, at least in language, would that ever give us insight

01:16:17.180 --> 01:16:20.540
into how language and natural language understanding works?

01:16:21.740 --> 01:16:25.260
The language model today is distributed over these billions of parameters.

01:16:25.980 --> 01:16:35.100
And do you think we'll ever have an understandable LLM, like, for example, we use PCA to understand

01:16:35.100 --> 01:16:36.220
what regression is doing?

01:16:37.020 --> 01:16:37.900
Or is that hopeless?

01:16:38.700 --> 01:16:40.620
At some point, I think it's going to be right to be hopeless.

01:16:40.620 --> 01:16:44.620
I mean, we'll probably learn a lot about, you know, how the systems represent data

01:16:44.620 --> 01:16:46.540
and, like, how they manipulate it and stuff like that.

01:16:46.540 --> 01:16:47.740
So this is not opaque, right?

01:16:47.740 --> 01:16:53.500
We can completely kind of, there's complete visibility on how the systems operate.

01:16:53.500 --> 01:16:57.420
Now, the question is understanding really how the decisions are being made.

01:16:57.420 --> 01:17:01.420
So I'm actually not particularly interested in those questions, like, you know, as long as

01:17:02.460 --> 01:17:03.580
they work properly.

01:17:05.500 --> 01:17:09.420
The same way, I'm not particularly interested in figuring out exactly how the brain works.

01:17:09.420 --> 01:17:15.260
I'm more interested in figuring out how the brain builds itself so that it works, right?

01:17:15.260 --> 01:17:21.340
So I'm more interested in learning than in studying the result of learning, if you want.

01:17:21.340 --> 01:17:23.660
So it's the same for those systems.

01:17:23.660 --> 01:17:27.340
I'm more interested in how you get them to learn what you want and how to solve the problem

01:17:27.340 --> 01:17:30.060
in the end is kind of considerably less interesting, in my opinion.

01:17:30.460 --> 01:17:34.940
But at some point, they're going to be, you know, super intelligent,

01:17:34.940 --> 01:17:36.940
repulsory of all human knowledge.

01:17:36.940 --> 01:17:42.220
You know, it's going to be too big for us to kind of comprehend at a deep level.

01:17:43.580 --> 01:17:44.380
Fair.

01:17:46.380 --> 01:17:50.380
And by the way, I failed to acknowledge that question came from Walid Saba, who's one of our

01:17:50.380 --> 01:17:55.660
senior research scientists at the EAI up in Portland, Maine.

01:17:56.140 --> 01:18:00.860
The next question I'll use, and then I'll switch over to audience questions,

01:18:03.820 --> 01:18:08.780
comes from Gene Tunic, the director of AI Plus Health at the Institute for Experiential AI.

01:18:11.740 --> 01:18:16.380
You believe that deep learning can eventually lead to human-like understanding,

01:18:17.580 --> 01:18:23.740
and you have said that self-supervised learning from unlabeled data

01:18:24.620 --> 01:18:30.860
can be a powerful tool, although it seems like in human learning, as I was watching your examples,

01:18:30.860 --> 01:18:38.540
for example, a lot of that data is, in a way, supervised or tied to some kind of reinforcement

01:18:38.540 --> 01:18:46.300
feedback around what to expect, is it good, is it bad, etc. So how, where do you draw that line

01:18:46.300 --> 01:18:58.060
between, you know, can we really truly go towards unsupervised, or there's a huge dependence on

01:18:58.060 --> 01:19:03.420
supervised and on those labels to get it right? Because the world is, in a way, is telling us

01:19:03.420 --> 01:19:07.980
indirectly through supervision. So self-supervised learning, I mean, the reason it's called self-supervised

01:19:07.980 --> 01:19:11.900
is that deep down it's actually supervised learning. It's just supervised learning where

01:19:11.900 --> 01:19:18.060
the supervision signal is the input itself, right? So in a way, that's kind of, you know, a kind of

01:19:21.020 --> 01:19:26.460
simple answer to that question. It's still supervised learning in the end, but with

01:19:26.460 --> 01:19:31.580
particular architectures to handle uncertainty and dimensionality and things like that. Regarding

01:19:31.580 --> 01:19:35.980
reinforcement learning, there is a point at which you need some form of reinforcement learning,

01:19:35.980 --> 01:19:41.500
and you need it in two situations, or at least techniques that have been developed in the context

01:19:41.820 --> 01:19:48.060
of reinforcement learning. The first situation is if the objective function that is optimized by

01:19:48.060 --> 01:19:53.900
your system does not reflect the ultimate objective function, you actually want to optimize. So for

01:19:53.900 --> 01:20:01.820
example, you're learning to ride a bike, your objective function is the, you know, time to the

01:20:01.820 --> 01:20:09.980
next fall or something, or the inverse time to next fall, you want to minimize that, right?

01:20:12.140 --> 01:20:19.980
But you don't know how to compute this from the internal state of your system. And so you need

01:20:21.020 --> 01:20:25.980
to train an objective function to approximate this real cost, which in the context of reinforcement

01:20:25.980 --> 01:20:32.300
learning is called a critic. So that's when you need one of those things. The other situation

01:20:32.300 --> 01:20:37.820
where you need it is when your world model is not accurate because it's not been trained in all

01:20:37.820 --> 01:20:42.300
corners of the state space, and you happen to be in a part of the state space that it wasn't trained

01:20:42.300 --> 01:20:46.700
on. Your world model is going to be bad, and your predictions are going to be bad, your planning is

01:20:46.700 --> 01:20:54.060
going to be bad. So to prevent this, you need to train your world model using things that are

01:20:54.140 --> 01:20:58.780
called curiosity or exploration. And that's another concept that comes from reinforcement

01:20:58.780 --> 01:21:03.180
learning. So don't completely abandon reinforcement learning, but minimize its use.

01:21:04.140 --> 01:21:10.060
As we switch over to the live questions, let me, I can't help but ask you this question. It comes

01:21:10.060 --> 01:21:19.500
from several anonymous people as well as Ken Church, your former colleague. Did you actually say

01:21:20.140 --> 01:21:25.900
the revolution will not be supervised? I did, yeah. Okay. But actually, I stole it

01:21:27.020 --> 01:21:36.940
from Adi Asha Efros from Berkeley. He had a magnificent slide that was a picture of a wall

01:21:36.940 --> 01:21:43.820
painting in Chile someplace, which was one of those kind of revolutionary thing. And I took

01:21:43.820 --> 01:21:51.500
that picture and overlaid on it. The revolution will not be supervised. Yes. Okay. So I stole

01:21:51.500 --> 01:21:57.900
that from him. I deserve no credit. Shall we switch over to a question from the audience?

01:21:57.900 --> 01:22:04.620
Yeah. So first question from Glenn Jenkinson is, what two questions about AI do you wish you

01:22:04.700 --> 01:22:13.740
would ask more often? Two questions. I don't know. I get asked a lot of questions. I can't

01:22:13.740 --> 01:22:21.740
imagine a question have not been asked. That's relevant. I mean, I think the important questions

01:22:21.740 --> 01:22:26.780
are the ones that I'm asking myself. And I wish other people would sort of frame the

01:22:26.780 --> 01:22:33.660
problems in the same way. So big question. How is it that any teenager can learn to

01:22:33.660 --> 01:22:37.420
drive a car in 20 hours? And we still don't have level five autonomous driving?

01:22:38.940 --> 01:22:41.900
That was the first question. So second question is, what are we missing?

01:22:44.300 --> 01:22:53.020
That's the answer I want. Joe. Next question from Juan Leylanda. Do you think quantum computing

01:22:53.020 --> 01:22:55.740
will have a significant role in the future of AI? No.

01:22:56.540 --> 01:23:09.100
Or at least not any time soon. But the time this happens, I probably won't be alive anymore.

01:23:09.100 --> 01:23:16.540
So I'm not taking a big risk. No, I don't think so. I mean, there's precious few

01:23:17.340 --> 01:23:22.060
situations today where quantum computing could be useful. There's no situation where it actually

01:23:22.060 --> 01:23:27.980
is useful because the quantum computers are not big enough at the moment. So it's a huge bet.

01:23:27.980 --> 01:23:33.500
I think scientifically it's fascinating. I'm really fascinated by quantum computing at the

01:23:33.500 --> 01:23:40.060
conceptual level. I have one or two papers with Seth Lloyd on connections between neural

01:23:40.060 --> 01:23:46.140
nets and quantum computing. I think it's a very interesting topic, but I don't think it has any

01:23:46.140 --> 01:23:53.740
practical value in the short term. Joe. One last question from Anton Dabura.

01:23:54.540 --> 01:24:00.380
To what extent do you see ML models being used for problems that we already have pretty good

01:24:00.380 --> 01:24:05.980
algorithms to solve, such as sorting shortest path, linear integer programming, and so on?

01:24:05.980 --> 01:24:11.500
How would you characterize the boundary, if any? So there's a lot of problems that we can currently

01:24:11.500 --> 01:24:19.100
solve that are NP-complete or NP-hard, and so we can solve them within limits. What we need very

01:24:19.100 --> 01:24:27.580
often are approximate algorithms, so methods that give us approximate solutions to complex problems

01:24:27.580 --> 01:24:37.420
that, in theory, are NP-hard, NP-complete, whatever, but if you reduce yourself to accepting

01:24:37.420 --> 01:24:46.380
approximate solutions, might become solvable. So I think there is a lot to be said for ML methods

01:24:46.380 --> 01:24:53.100
that do something that has become to be known as amortized inference. So amortized inference is

01:24:53.100 --> 01:24:57.660
this idea that you might have a problem that is formulated as an optimization problem. Every

01:24:57.660 --> 01:25:04.540
computing problem can be formulated as an optimization problem. And what you might be able to do is

01:25:04.620 --> 01:25:09.900
solve that problem in certain cases, give a solution, and now what you do with this is that you

01:25:09.900 --> 01:25:15.820
train in your net of some kind to predict, to approximate the solution to that optimization

01:25:15.820 --> 01:25:21.820
problem from the specification of the problem, from the inputs. So that system will not be able to

01:25:22.460 --> 01:25:26.540
completely solve the problem in those situations, but for the type of problem that you train it on,

01:25:27.340 --> 01:25:31.420
it's going to be able to give you an approximate solution really quickly. Amortized inference.

01:25:31.420 --> 01:25:40.860
There is a tutorial on this that was written and given at a recent conference by one of my

01:25:40.860 --> 01:25:46.620
colleagues at fair called Brendan Amos, AMOS, very interesting concept.

01:25:48.460 --> 01:25:53.340
I will close my questions with one last question, then we'll take a real live one and call it the

01:25:53.340 --> 01:25:59.100
end. I have to use this. It comes from one of our faculty who wanted to remain anonymous,

01:25:59.100 --> 01:26:04.220
I don't know why, but given the big excitement around LLMs and not without a reason,

01:26:05.020 --> 01:26:09.660
what are some of the research directions that are possible to tackle for non-Google slash

01:26:09.660 --> 01:26:17.180
Facebook type sized institutions that are under studies? Space for foundational research,

01:26:17.180 --> 01:26:23.580
big open questions in need of creative solutions. Thus, if you were a young investigator today,

01:26:23.580 --> 01:26:29.180
like a starting assistant professor, what would you do in this environment?

01:26:29.180 --> 01:26:33.980
I mean, that's a problem I have to face when I have PhD students at NYU that don't have access to

01:26:34.940 --> 01:26:45.100
16,000 GPUs, unlike people at fair. So I think a lot of most good ideas still come from academia,

01:26:45.100 --> 01:26:53.100
so you're not going to be Google or Meta or Microsoft on beating the record on

01:26:53.100 --> 01:26:56.700
translation or something like that. You don't want to do this in universities.

01:26:57.820 --> 01:27:02.700
But coming up with new ideas, for example, the problem I mentioned of how do you do

01:27:02.700 --> 01:27:09.420
hierarchical planning? How do you train a system to figure out how to represent the world and

01:27:09.420 --> 01:27:13.340
action spaces so that you can do hierarchical planning? It's completely unsolved. You can do

01:27:13.340 --> 01:27:18.620
this with toy problems. If you have any idea of how you might approach that problem on toy

01:27:18.620 --> 01:27:24.940
problems, you don't have to have tons of GPUs for that. You will have an idea that might have a

01:27:24.940 --> 01:27:32.860
huge impact. So if you have a good architecture that you can show, can learn some simple world

01:27:32.860 --> 01:27:37.740
model from video, it's the same. You don't have to train on all of YouTube. You can train on

01:27:37.740 --> 01:27:43.820
artificial environments and stuff like that and demonstrate that it works. It doesn't have to be

01:27:44.060 --> 01:27:48.540
large scale. So this is the kind of stuff you want to do. And then there is a new domain which is

01:27:48.540 --> 01:27:54.460
building on top of open source base models. So unfortunately, right now, the best base models,

01:27:55.980 --> 01:28:03.020
LLMs are the LAMA class of models from $7 billion to $65 billion. They're not usable for commercial

01:28:03.020 --> 01:28:08.140
use. They are distributed with a license for non-commercial use, only for research, which you

01:28:08.140 --> 01:28:13.420
can of course use in the university. And there's a lot of work to be done to figure out how to

01:28:13.500 --> 01:28:18.620
make those things safe, factual, etc. And you can work from those base models. You don't have to

01:28:18.620 --> 01:28:24.700
retrain them from scratch. So you don't need to have roomful, rooms full of GPUs.

01:28:25.580 --> 01:28:28.860
We'll try for one last question, maybe two. Go ahead, please, with your question.

01:28:29.820 --> 01:28:35.500
Hi. So my question really dwells from the side of, or we'd love to hear your thoughts,

01:28:35.500 --> 01:28:41.420
on impact and control of these large language models or any of these models, the fancy models

01:28:41.420 --> 01:28:46.620
that you showed with billions of trillions of parameters. So the impact side is, do you really

01:28:46.620 --> 01:28:53.020
give or how much thought do you give to the impact that would have on the community or on the people

01:28:53.020 --> 01:29:00.620
in general, based on what that model does? And control is, once that model is out there,

01:29:03.340 --> 01:29:09.260
how do I make sure that it doesn't do a certain things it's not supposed to do with regular,

01:29:10.220 --> 01:29:14.620
the way people used to use internet before those models. It used to be very controlled

01:29:14.620 --> 01:29:20.460
environment where you could have, in a way, regulate those environments. But now with models,

01:29:20.460 --> 01:29:26.380
it's getting increasingly difficult and a slow process to have or do not have certain things in

01:29:26.380 --> 01:29:32.540
those models. Okay. So there is a long view, a very positive one, which is imagine that all of us

01:29:32.540 --> 01:29:38.060
have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff

01:29:38.140 --> 01:29:45.420
of people working for us, but like super people working for us. This is going to create a new

01:29:46.380 --> 01:29:54.860
renaissance for humanity. It's going to increase humanity's intelligence, however you want to

01:29:54.860 --> 01:30:03.260
measure it. That has to be intrinsically good. It's been the case in the past that anytime a new

01:30:03.820 --> 01:30:07.500
medium was invented or a new way of communication was invented, like the printing press.

01:30:08.940 --> 01:30:12.780
Humanity kind of went to the next step. The printing press let

01:30:15.180 --> 01:30:22.780
the dissemination of philosophy, science, secularism, democracy, all that stuff. The US

01:30:22.780 --> 01:30:34.620
would not exist without the French philosophers of the 18th century. So neither would the French

01:30:34.620 --> 01:30:43.180
revolution. So I think same for the internet, that gave people instant access to an enormous

01:30:43.180 --> 01:30:50.060
wealth of knowledge. Also disinformation, but okay, I mean, we have to have countermeasures for

01:30:50.060 --> 01:30:55.180
every technology can be used for good and bad. We need to have countermeasures for the worst

01:30:55.740 --> 01:31:04.460
aspects. But ultimately, I think we need widest possible access to those AI systems by everyone.

01:31:05.500 --> 01:31:12.460
Now, how do we make sure those systems don't lie to us? How do we make sure that the information

01:31:12.460 --> 01:31:17.180
they give us is not under the control of someone that has nefarious purpose, you know, things like

01:31:17.180 --> 01:31:23.340
that, which is I think a good reason for them to be open as I stated earlier. But I think it's a

01:31:23.340 --> 01:31:29.660
bright future for humanity, you know, contrary to some people who tell young people don't expect

01:31:29.660 --> 01:31:37.820
to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the

01:31:37.820 --> 01:31:45.260
next question, but we are five minutes over our time limit. And I know we have to grab a bite and

01:31:45.260 --> 01:31:51.820
deliver you to the train station on time, according to the hierarchical plan. So with that, please

01:31:51.820 --> 01:32:03.980
join me in thanking Jan for an amazing session today. Thank you.

