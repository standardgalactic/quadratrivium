start	end	text
0	15960	Okay, great, welcome everyone and thank you for joining us.
15960	22080	This is the last distinguished lecture series for the Institute of Expansion AI for the
22080	23080	academic year.
23080	27360	We'll resume again in September with a full program for the year.
27360	35280	As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series,
35280	41520	which is designed to feature a lot of our Northeastern University experts and faculty
41520	43560	and so forth.
43560	50920	In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic
50920	52600	who's at the Curie College.
52600	56960	My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential
56960	62880	AI and also Professor of the Practice in the Curie College for Computer Sciences and it
62880	68880	is my pleasure today to introduce Yan Lokhan.
68880	73480	Yan is a very well-known name in the field.
73480	80000	I've known him for many years, I think at one point in my life I interviewed at Bell
80000	84880	Labs or AT&T Labs and that's when he was there.
84880	92760	He is VP and Chief AI Scientist at Meta, also known as Facebook, and Silver Professor at
92760	98040	NYU affiliated with the Kauan Institute of Mathematical Sciences and the Center for Data
98040	101880	Science, which he actually founded.
101880	108640	He was the founding director of FAIR, I learned this morning that FAIR used to stand for Facebook
108640	117520	AI Research, now it's changed to MetaFair for Fundamental AI Research and of course
117520	124080	he founded the NYU Center for Data Science, received an engineering diploma from SEA in
124080	129480	Paris and a PhD from the Saoubon University.
129480	138560	After a postdoc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs
138560	143520	in 1996 as Head of Image Processing Research.
143520	151320	He joined NYU as Professor in 2003 and Meta or Facebook in 2013.
151320	158000	He is the recipient of the 2018 ACM Touring Award, along with Jeffrey Hinton and Yashua
158000	162480	Benjio, and for those of you who don't know the Touring Award, it's essentially the equivalent
162480	169520	of the Nobel Prize for Computer Science, the toughest award to get from the ACM.
169520	174680	The award was for conceptual and engineering breakthroughs that have made deep neural networks
174680	177240	a critical component of computing.
177240	184200	He is a member of the National Academy of Sciences and the National Academy of Engineering,
184200	187520	amongst many others.
187520	193800	His interests include AI, machine learning, computer perception, robotics and computational
193800	200760	neuroscience, and I'm sure you're all eager to hear from Jan on what's been happening
200760	207360	with generative AI and what all the buzz is about, hopefully we'll get into the technical
207360	212760	details and immediately following his talk, we will do a fireside chat where I will try
212760	216000	to ask him some tough questions.
216000	218960	And then we will also get questions from the audience.
218960	222800	By the way, we did get online questions from the audience.
222800	227680	We got 150 questions, so there's no way we're going to walk you through all of those.
227680	230520	So we'll see how much time allows us to answer.
230520	242520	Thank you and please join me in welcoming Jan to Northeastern University.
242520	243520	Thank you, Samap.
243520	251920	A real pleasure to be here and thanks for coming here so numerous or for listening in online.
251920	256400	So I'm going to talk a bit about the state of the art in AI but also about the next step
256400	264520	because I'm always interested in the next step and how we can make machines more intelligent.
264520	271520	And we need to figure out how to get machines that cannot just learn but also can reason
271520	277400	and plan and current AI really does not allow current systems to do this.
277400	283400	So I'll try to kind of sketch a potential pathway towards such systems.
283400	290080	I can't say that we built it completely but we built some components and I go through this.
290080	296080	So AI is in the news, everybody is playing with it at the moment.
296080	298440	It's pretty amazing how it works.
298440	299760	There's a lot of success.
299760	303720	It's been very widely deployed very much in many applications that are behind the curtain
303720	306200	but in some of them much more visible.
306200	312760	So LLMs have the advantage of being visible but for the last 10 years or so there's massive
312760	320080	use of AI and the latest development of AI for such thing as ranking for search engine
320080	325880	and social networks or for content moderation, things like that.
325880	330640	But overall machine learning requires a lot of data and the machines that we have are
330640	332640	somewhat brittle, specialized.
332640	339560	They don't have human-level intelligence despite what we may be led to believe.
339560	347280	So in short, machine learning sucks at least compared to humans and animals.
347280	352520	We've been using supervised learning which really was the workhorse of machine learning
352520	356520	and AI systems until very recently.
356520	360640	Reinforcement learning is insanely inefficient but it works really well for games but not
360640	362720	many other things.
362720	367560	So one thing that has taken over the AI world in the last few years is something called self-supervised
367560	371880	learning which I will talk about at length.
371880	374160	But current AI systems are specialized and brittle.
374160	375160	They make stupid mistakes.
375160	381440	They don't really reason and plan with a few exceptions for a game playing for example.
381440	385400	Compared to humans and animals, they can learn new tasks extremely quickly, understand
385400	390760	how the world works, can reason and plan have some level of common sense.
390760	392960	Machines still don't have common sense.
392960	398480	So how do we get machines to reason and plan like animals and humans learn as fast as animals
398480	400320	and humans?
400320	404320	And we'll need machines that can understand how the world works, can predict the consequences
404320	411040	of their actions, can perform change of reasoning with unlimited number of steps, can plan complex
411640	415080	tasks by decomposing them into simpler tasks.
415080	418360	So let's start with this idea of self-supervised learning.
418360	420800	It's really taken over the world.
420800	424840	Every sort of top machine learning system today uses some form of self-supervised learning
424840	429080	as a first step to pre-train the system.
429080	430580	And it's used everywhere.
430580	431580	What does it consist of?
431580	436840	It's really the idea that instead of having, of training a system with an input and an
436840	441600	output, which is the case in supervised learning, or with an input and a reward, which is the
441600	446200	case for reinforcement learning, you train the system to basically model its input.
446200	450320	You don't train it for any particular task other than capture the dependency between
450320	451560	different parts of its input.
451560	458080	So one thing you might do is, for example, take a piece of video, a piece of text, show
458080	463280	a piece of the video to the system and ask it to predict the missing piece, like the
463280	464520	continuation of that video.
464520	468920	And after a while, you reveal the rest of the video and you adjust the system so that
468920	471960	it does a better job at predicting.
471960	475200	So prediction really is kind of the essence of intelligence.
475200	479200	And to some extent, by training a system to predict, it doesn't have to be predicting
479200	480200	the future.
480200	483160	It could be predicting the past or the left from the right.
483160	486560	You're training the system to represent data, essentially.
486560	495280	And that's been nothing short of astonishingly successful in the domain of natural language
495280	496520	understanding.
496520	506080	So every type performing NLP system today is pre-trained the following way, or with some
506080	511600	form of the following way, which is a special case of an old idea called denoising autoencoder.
511600	520120	And the idea is that you take a piece of text, sequence of words from a corpus.
520120	523880	Typically it would be a few hundred or a few thousand words long.
523880	529520	Those words immediately get turned into vectors, but let me not talk about this for just now.
529520	532560	So the first thing you do is you corrupt this text.
532560	537560	You remove some of the words and replace them by blank markers, or you substitute them for
537560	538560	another word.
539120	543280	And then you train some gigantic neural net to predict the words that are missing.
543280	546880	In the process of doing so, the system has to basically develop some sort of understanding
546880	551520	of the text, because if you want to be able to predict what word comes here, you have
551520	557720	to understand the role of the word in the sentence, the type of word that comes here,
557720	558800	and the whole meaning of the sentence.
558800	561680	So the system basically learns to represent text.
561680	566120	And the amazing thing is that just by doing this, you can train a system to represent
566120	571200	the meaning of text in pretty much any language, as long as you have data.
571200	576520	With a single system, you can have a system that represents the meaning of a piece of
576520	579760	text in any language.
579760	580840	So pretty cool.
580840	587160	You can use this to build translation systems, systems that detect hate speech on social
587160	590480	networks or figure out what something talks about.
590480	596280	The way you do this is that you chop off the last few layers of that gigantic neural
596280	601720	net, and you use the representation, the internal representation, learned by the system as input
601720	608800	to a subsequent downstream task that you train supervised, like, say, translation.
608800	612640	And it's really astonishing how well this works.
612640	619880	So from this to a generative AI system, there's a small step, particularly for text generation.
619880	622600	Text generation is a completely different thing, which I'm not going to talk about,
622600	626760	but although some systems use the same technique.
626760	634440	So what is a generative text generation system, a large language model?
634440	642120	It's a system of the type I just described, except that when you train it, you don't remove
642120	647280	random words in the text that you show at the input, you only remove the last one.
647280	653120	So you train the system to predict the last word in a sequence of words.
653120	658600	So show a sequence of words, and then show the last word, and train some gigantic neural
658600	662320	net, perhaps with billions or hundreds of billions of parameters, to predict the next
662320	663320	word.
663320	672080	And you have to train this on trillions of text snippets, typically one to two trillion
672080	674560	for the biggest models.
674560	677960	Once you have that system, you can use it to generate text using what's called autoregressive
677960	681320	prediction, which is a very classical thing to do in single processing.
681320	685720	So you take a piece of text called a prompt, you enter it into the system, you have it
685720	691920	predict the next word, and then you shift that word into the input.
691920	697160	So now it becomes part of the input to the system, and now you can predict the next word,
697160	699800	shift it in, predict the third word, shift it in.
699800	702200	That's autoregressive prediction.
702200	706920	And that's how all the bigger alarms that everybody has played with work.
706920	707920	That's how they've been trained.
707920	711160	That's how they generate text.
711160	720400	So those alarms are kind of amazing in terms of the performance that they produce.
720400	723760	So again, they're trained on something like one to two trillion tokens.
723760	727720	A token is like a word or a subword unit.
727720	730400	And there's a whole bunch of those models, most of which you probably haven't heard of,
730400	735800	but there's a few that have become household names.
735800	746000	So we've heard of chatGBT and GPT-4 from OpenAI, which are kind of usable, barred from Google,
746000	753200	and derivative of chatGBT and GPT-4 from Microsoft, married with Bing.
753200	758160	But there's a long history of those things that goes back several years, some from Fair,
758160	759160	Lunderbot, and Galactica.
759160	767400	Galactica was trained on the scientific literature and is designed to help scientists write papers.
767400	771960	And a more recent one, called LAMA, which is the code is open source.
771960	776760	The model, you can get it on request if you are using it for research purpose.
776760	780360	And it's the same level of performance as things like chatGBT, but it's not fine-tuned.
780360	782360	You have to fine-tune it for application.
782360	787080	And in fact, people have done this, so Alpaca is a model which basically is a fine-tuned
787080	793400	version of LAMA that was built by people at Stanford for answering questions and things
793400	795200	like that, instruction.
795200	801120	So they're pretty amazing, they surprised a lot of people in how well they work, but
801120	807200	they make a lot of factual errors, logical errors, inconsistencies, limited reasoning
807200	809720	abilities, things like that.
809720	813560	And they are easy to, they're pretty gullible.
813560	818560	So you tell them, what is 2 plus 2, and the system will say 4, and you say, no, actually
818560	819560	2 plus 2 equals 5.
819560	821640	Oh yeah, you're right, I made a mistake.
821640	831640	So they kind of, they predict answers that would sound like someone could produce these
831640	834000	answers, but the details might be wrong.
834000	841480	So you can't really use them for factual answers, but you can use them certainly for
841480	843120	writing aids.
843120	850760	And particularly, it works really well for text or for standard sort of templatized text
850760	856040	that you need to write, like I don't know, there's a bunch of professors here that have
856040	866200	to spend quite a bit of time writing recommendation letters for students, very useful for that.
866200	867720	And very useful for code generation.
867720	872560	So the software industry is probably going to be revolutionized by such tools.
872560	878080	So this is an example of code generated from a prompt by the Lama 65 billion model, the
878080	879080	open source one.
879080	884160	So, you know, ask it, you know, find real roots of AX squared plus BX plus C, and the
884160	889520	thing just writes a function in Python or whatever, whatever you want, or Reg X or whatever,
889520	893280	who remembers the syntax of Reg X?
893280	895280	Like.
895280	900480	You can have it, you know, hallucinate text that might sound plausible or completely implausible
900480	901480	like this.
901480	904560	Did you know that Yanukun dropped a rap album last year?
904560	907200	We listened to it and here is what we thought.
907200	917920	And the thing writes a review of my alleged rap album.
917920	922240	I'm not much of a rap person, I'm more of a jazz person, so when my colleagues showed
922720	926680	this to me, I told them, like, can you do the same for a jazz album that would be kind
926680	927680	of more appropriate?
927680	933160	I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't
933160	938120	work very well because there's not enough training data on the web of reviews of jazz
938120	939120	albums.
939120	944320	I found that incredibly sad, I cried.
944320	946720	So you need a lot of data to train those things, right?
946720	950960	In fact, the amount of data, like something like 1.5 trillion tokens that Lama is trained
950960	956000	on, it would take about 22,000 years for a human reading eight hours a day at every
956000	958480	speed to read the whole material.
958480	964160	So obviously those things can accumulate a lot of knowledge, at least approximately.
964160	969240	So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not
969240	972680	good for producing factual and consistent answers, at least not yet.
972680	978680	So a lot of LLMs are being augmented or extended so that they can use tools like calculators
978680	986400	or database engines or whatever to search for information and then refer to the source.
986400	991960	They're not good at all for reasoning, planning, or even for arithmetic.
991960	999440	So but we are easily fooled by their language fluency into thinking that they are intelligent.
999440	1002600	They're not that intelligent.
1002600	1006440	And they really have no understanding of the physical world because they're trained with
1006440	1007440	text.
1007720	1011200	And there's another flaw, which is a huge problem.
1011200	1016880	It's the fact that if you imagine that there is the set of all possible answers represented
1016880	1022080	by this sphere, disk, which is really a tree, right?
1022080	1025080	Every token you add put, you have a certain number of options for what the token should
1025080	1026680	be, what the word is.
1026680	1028480	So it's a tree of all possible answers.
1028480	1033600	Within this tree, there is a small subtree that corresponds to correct answers for the
1033600	1036120	question being asked.
1036120	1042680	And imagine that there is a probability E for any token that is produced by the system
1042680	1046280	to be outside, to take you outside that tree of correct answers.
1046280	1049200	Once you go outside that tree, you can't come back because it's a tree.
1049200	1057480	So let's imagine that the probability per token is E. So the probability that a sequence
1057480	1061400	of N tokens would be correct is 1 minus E to the power N, making the assumption that
1061400	1066920	the errors are independent, which of course they're not, but that's kind of a crude assumption.
1066920	1071320	And so the problem with this is that it's an exponentially divergent process, this
1071320	1075480	autoregressive prediction, errors accumulate.
1075480	1081000	And if you produce too many tokens, the thing will sort of diverge away from the set of
1081000	1082560	correct answers, exponentially.
1082560	1086800	And that's not fixable with the current architecture.
1086800	1092760	You can fine tune those systems a lot to reduce E, but you're not going to make it
1092760	1094880	go away.
1094880	1101640	So I have a bold prediction, which is that the shelf life of autoregressive LLM is very
1101640	1103000	short.
1103000	1108360	My prediction is that five years from now, nobody in their right mind would use them.
1108360	1112760	So enjoy it while it lasts.
1112760	1115720	They'll be replaced by things that are better.
1115720	1121680	And I'll hint about directions to kind of perhaps fix up those problems.
1121680	1129400	So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema
1129400	1134800	magazine, which is a philosophy magazine, about the fact that a system that is purely
1134800	1141600	trained from text, from language, cannot possibly attain human level intelligence because much
1141600	1147920	of what humans know is actually derived from experience of the physical world.
1147920	1153520	This is true for a lot of human knowledge, but it's true certainly for almost the totality
1153520	1155760	of animal knowledge.
1155760	1160240	It's all about the world is no linguistic related, no language related.
1160240	1165960	So linguistic abilities and fluency are not related to the ability to think.
1165960	1170680	Those are two different things.
1170680	1177200	There are some criticisms of autoregressive LLMs from people coming from the cognitive
1177200	1183200	science realm who say like, this is not at all the way the human mind works.
1183200	1186160	There is essential missing pieces.
1186160	1190280	Other criticism for people who come from sort of more classical AI, pre-deep learning,
1190280	1196760	they say like, you know, AI systems are supposed to be able to plan and reason, and those LLMs
1196760	1197760	can do it.
1197760	1202320	Or at least not, you know, they can do it maybe in very sort of primitive forms.
1202320	1208040	Perhaps they can plan things in situations that correspond to a template that they've
1208040	1212200	been trained on, but they're not so innovative.
1212200	1220240	So we should ask, how is it that humans and animals can run so quickly?
1220240	1226840	And I've been using this diagram for quite a while now, several, many years from Emmanuel
1226840	1230680	Dupu, who's a cognitive scientist in Paris.
1230680	1236440	And we tried to sort of make a chart of at what age babies learn basic concepts about
1236440	1243400	the world, so things like distinguishing between animate objects and inanimate objects, learning
1243400	1246560	the notion of object permanence, the fact that when an object is hidden behind another
1246560	1249760	one, it still exists.
1249760	1254440	Notion of rigidity, solidity, things like natural categories, babies don't need to
1254440	1258680	know the name of an object to actually know that there are different categories of objects
1258680	1260240	around four months or so.
1260240	1264840	And then it takes about nine months for babies to really understand that sort of intuitive
1264840	1271320	physics that objects that are not supported will fall, that, you know, objects have a
1271320	1279560	momentum, weight, friction, you know, knowing that if I push on this object, you know, light
1279560	1284120	objects like this, they're going to move, but if I push on an object that's heavier,
1284120	1286320	it's not going to move unless I push harder.
1286320	1287320	So things like that.
1287320	1291880	So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where
1291880	1296160	you have a little car on the platform, you push the car off the platform, it appears
1296160	1302000	to float in the air, a five-month-old baby will pay attention, a 10-month-old baby will
1302000	1308440	go like this because she understood that by then that objects are not supported or supposed
1308440	1312360	to fall and this object appears to be floating in the air.
1312360	1319200	So we can determine that her mental model of the world is being violated, okay?
1319200	1323080	That's how this chart was built.
1323080	1327680	So we accumulate as babies an enormous amount of background knowledge about how the world
1327680	1331680	works, mostly by observation, a little bit by interaction, when we start being able to
1331680	1337600	kind of grab things, but in the first few months it's mostly just observation.
1337600	1343600	So we don't know how to reproduce this with this type of learning with machines.
1343600	1349880	Once we accumulate all this background knowledge, you know, in a number of years, learning a
1349880	1353160	new task like driving is very fast.
1353160	1358600	So any teenager can learn to drive in about 20 hours of practice, mostly without causing
1358600	1361320	any accident.
1361320	1364920	So the teenager doesn't have to run off a cliff to figure out that the car, that nothing
1364920	1368080	good is going to happen if you run off a cliff.
1368080	1373000	The mental model of the world is already there, okay?
1373000	1376480	We still won't have level five salivating cars.
1376480	1381000	So obviously we're missing something pretty big.
1381000	1385520	Any 10-year-old can clear up the dinner table and fill up the dishwasher.
1385520	1388480	We're nowhere near having robots that can do this and it's not because of mechanical
1388480	1396120	design, it's because we don't know how to build the minds behind it.
1396120	1398600	So we're missing something big, right?
1398600	1404480	The past towards human-level AI is not just making LLMs bigger, that's just not going
1404480	1406640	to get us there.
1406640	1413720	It's been a common recurring error by AI scientists and engineers over the last six decades to
1413720	1419960	imagine that the one thing that they just discovered was the solution to human-level
1419960	1424760	AI, only to discover a few years later that no, there was actually a big obstacle, another
1424760	1426560	obstacle they had to clear.
1426560	1431240	It's a recurring history story in AI.
1431240	1436000	So common sense will probably emerge from the ability of machines to learn how the world
1436000	1441120	works by observation, the way babies and animals do it.
1441120	1445800	So I see three challenges for AI research over the next decade also, learning representations
1445800	1450160	of the world and predictive models of the world, I'll say why in a minute, and self-supervised
1450160	1456120	learning is going to be the key component of that, learning to reason.
1456120	1459840	So psychologists talk about system one and system two.
1459840	1468400	System one is the type of control that our brains use to kind of react to something without
1468400	1471760	really having to think about it, like subconscious action.
1471760	1476120	So if you're an experienced driver, you don't have to think about driving, you can just drive
1476120	1481240	and you can talk to someone at the same time and barely pay attention.
1481240	1482240	So that's system one.
1482240	1486400	But then when you are learning to drive, you pay attention to absolutely everything.
1486400	1496920	You use your entire focus, consciousness, attention to drive and that's system two.
1496920	1501400	And then the last thing is learning to plan complex action sequences, decomposing them
1501400	1502400	into simpler ones.
1502400	1513400	So I wrote a sort of vision paper about a year ago, which I posted on open review for
1513400	1516120	comments, so you're welcome to comment on it.
1516120	1520880	I give a bunch of technical talks about it, one of the earliest one was at Berkeley, but
1520880	1524680	you are having a more recent version of it right now, so you don't need to look at that
1524680	1526800	one.
1526800	1532400	And it's based on what's called a cognitive architecture.
1532400	1538240	So basically how can we sort of design a system with different modules so that those modules
1538240	1546640	may implement all the properties that I was telling you about so systems can perceive,
1546640	1551600	reason, predict, in particular predict the consequences of their actions and then plan
1551600	1556560	a sequence of actions to optimize, to satisfy a particular objective.
1556560	1564720	So the main components of the system is the key component, I would say, is the world model
1564720	1573760	and the world model is what allows the system to predict ahead, imagine what's gonna happen.
1573760	1578800	This is to some extent what current AI systems don't really have.
1578800	1583280	Perception system basically gets an estimate of the state of the world and initializes
1583280	1585680	the world model with it.
1585680	1591160	The cost here is a really important module, so basically the entire purpose of the agent
1591160	1595080	is to minimize this cost.
1595080	1602840	So the cost is something that uses a measurement of the state of the agent, particularly the
1602840	1608240	prediction from the world model, and predicts whether an act is going to be good or bad.
1608240	1611880	And the entire purpose of the agent here is to figure out a sequence of actions, so this
1611880	1616920	is taking place in the actor, figure out a sequence of actions such that when I predict
1616920	1621520	what's gonna happen as a consequence of those actions using my world model, my objective,
1621520	1624320	my cost function will be minimized.
1624320	1631680	So if my cost function is, so the cost function is basically our measures of discomfort of
1631680	1635080	the agent.
1635080	1637960	Biological brains have things like that in the visual language, so this is the thing
1637960	1643320	that tells you when you're hungry, for example, or you're hurting.
1643320	1649800	So nature tells you you're hungry, nature doesn't tell you how to feed, you have to
1649800	1654520	figure that out by yourself, perhaps using your world model and your planning abilities.
1654520	1658040	So this is the same thing here, imagine this is a robot and the robot battery are kind
1658040	1663400	of starting to get drained, so there's a cost function here that says be careful, you're
1663400	1665480	running out of power.
1665480	1669960	And so the system, according to this world model, would say, well, I can recharge my
1669960	1673480	battery by plugging myself into a socket.
1673480	1678560	So it figures out the sequence of actions to plug itself into a socket and that will
1678560	1684000	eventually minimize the cost function that just appeared.
1684000	1691240	So in fact, there's two ways to operate that system one is the kind of system one where
1691240	1695600	the system makes an estimate of the state of the world, run this to a perception system
1695600	1700120	called an encoder here, produces an estimate of the state of the world called S0 and that
1700120	1705000	runs into a neural net called a policy network that just produces an action and the action
1705000	1707000	goes into the world.
1707000	1712440	LLNs are like this, they are system one, you give them a pump, that's X, they produce
1712440	1717400	an action, that's the token they predict, that goes back into the world and the world
1717400	1722880	is very simplistic here, it's just you shift in the input.
1722880	1726560	So no reasoning necessary, here is system two.
1726560	1734840	So you use the same system here and this is a sort of time-enrolled version of the system.
1734840	1740080	So we have the world model, the world model is this green module and the different instances
1740080	1744600	of that green module are the state of the system at different time steps, so think of
1744600	1748820	it as like a recurrent net that you unfolded, so it's really the same module at different
1748820	1749820	time steps.
1749820	1754560	What the world model is supposed to be able to predict is given the representation of
1754560	1759600	the state of the world at time t and given an action that I'm imagining taking, what
1759600	1764120	is going to be the predicted state of the world at time t plus one.
1764120	1769320	So I can imagine a sequence of actions that I might take, imagine the effect on the world
1769320	1774720	using my world model and then I can plug the state of the world over this trajectory
1774720	1782440	through my cost and measure whether my cost is going to be minimized by this action sequence,
1782440	1783440	my objectives.
1783440	1790160	So what I should do is run some sort of optimization procedure that will try to search for a sequence
1790160	1794920	of actions that minimizes the cost given the prediction given to me by the produced by
1794920	1796320	the world model.
1796320	1800280	This type of planning is very classical in optimal control.
1800280	1802960	It's called model predictive control.
1802960	1810640	In classical optimal control, the model is not learned usually, it's handcrafted.
1810640	1815400	Here we are thinking about a situation where the world model is learned by, for example,
1815400	1822240	watching the world go by, by video, but also by observing actions being taken in the world
1822240	1823400	and seeing the effect.
1823400	1828800	So to get a good accurate model here, I'm going to have to observe the state of the
1828800	1833680	world, observe, like, take an action and observe the effect or observe someone else
1833680	1836040	take an action and observe the effect.
1836040	1840880	Let me skip this for now.
1840880	1845480	Ultimately what we want is a hierarchical version of this because if you want the system
1845480	1850720	to be able to plan complex actions, we can't plan it at the lowest level.
1850720	1859920	So for example, if I want to plan to go from here to New York City, I would have to basically
1859920	1865520	plan every millisecond exactly what muscle actions I should take, okay?
1865520	1867680	And it's impossible, right?
1867680	1873280	You can't plan an entire trip from here to New York City millisecond by millisecond,
1873280	1879200	partly because you don't have a perfect model of the environment, like you don't know if,
1879200	1885480	when you're going to walk up the room here, whether someone is going to be on the way,
1885480	1886840	in the way and you're going to have to go around.
1886840	1889200	So you can't completely plan in advance, right?
1889200	1893440	So what we do is we plan hierarchically, say like, okay, I want to go to New York City,
1893440	1897320	so the cost function at the top here measures my distance to New York City.
1897320	1904040	And the first thing I have to do is go to the airport and catch a train or go to the train
1904040	1908080	station and catch a train or go to the airport catch a plane.
1909080	1913040	So the top predictors are predictors at a high level that says, oh, okay,
1913040	1917800	if I catch a taxi, it might take me to the airport.
1917800	1923120	If I catch, or to the train station, then if I catch a train, it'll take me to New York City.
1923120	1929200	Okay, so you have those two hidden actions, those Z variables here.
1929200	1932960	And they define a cost function for the next level down.
1933160	1939600	So if the first action is I'm taking a taxi to the train station,
1939600	1943520	the lower level is how do I catch a taxi here?
1943520	1945240	I go down in the street and hail the taxi.
1945240	1946080	No, this is Boston.
1946080	1948840	I need to call it Uber or something.
1948840	1955520	Okay, so I go on the street and I call it Uber.
1955520	1956760	How do I go in the street?
1956760	1958120	There's going to be lower levels.
1958120	1959600	I have to get out of this building.
1959600	1960520	How do we get out of this building?
1960520	1962000	I have to walk through the door.
1962080	1963400	How do I work through the door?
1963400	1967200	I have to put one leg in front of the other over obstacles.
1967200	1969480	And all the way down to millisecond.
1969480	1973000	Also control for a short period, which is replanned as we go.
1974160	1977440	Okay, no AI systems today can do any of this.
1977440	1981640	This is completely virgin territory.
1981640	1985680	Okay, there's a lot of people who've worked on hierarchical planning,
1985680	1990160	but in situations where the representations at every level are hardwired,
1990160	1991560	they're known in advance.
1991560	1993080	They're predetermined.
1993080	1995840	It's sort of like the equivalent of a vision system where the features
1995840	1998080	at every level are hardwired or designed by hand.
1999080	1999920	There's no system today.
1999920	2002480	They can learn hierarchical representations for action plans.
2003760	2005680	So that's a big challenge.
2005680	2009600	The cost function, so here's what's important here.
2009600	2013960	A lot of people today are talking about the fact that AI systems are difficult
2013960	2019200	to control and that's terrible, maybe toxic, various things.
2020200	2026560	The system I describe cannot produce outputs that do not minimize the objectives.
2026560	2033320	And so if you have terms in the objective that guarantee certain conditions,
2033320	2036360	that system will have no choice but obeying those conditions.
2036360	2038960	Okay, so having a system that is designed like this,
2038960	2043120	that whose output is produced by minimizing a set of objectives,
2043120	2048000	according to a model, will basically help guarantee the safety of that system.
2048000	2052680	Because you can hardwire intrinsic objectives on the left here
2052680	2054600	that basically guarantee the safety.
2054600	2058840	And the system cannot escape the satisfaction of those constraints.
2058840	2060880	So let me take a very simple example.
2060880	2064120	Let's say someone figures out how to build a domestic robot they can cook.
2065720	2070320	This robot will have to be able to kind of handle a kitchen knife.
2070320	2073680	And you might put a cost function that says, don't flail your arm
2073680	2076520	if you have a kitchen knife in your arm and there is people around.
2077520	2078960	Okay, because it's dangerous.
2078960	2082640	So you can imagine putting a lot of kind of safety conditions in those systems
2082640	2084080	to make them steerable.
2084080	2091120	So I don't think the problem of making AI systems safe is such a huge problem
2091120	2095840	that some people who are very vocal are seeing it is that AI is going to kill us all.
2098280	2099880	It's not going to kill us all.
2099880	2106120	We would have to screw up really badly for that to happen.
2106120	2109640	Okay, now here's the thing.
2109640	2111600	How do we build the world model?
2113600	2117720	And that's basically the biggest challenge that we have at the moment.
2117720	2121360	How do we build a system that can predict what's going to happen in the world?
2121360	2124080	For example, by training itself to predict videos.
2124080	2128160	Now the problem with predicting videos is that the world is not entirely predictable.
2130720	2132960	It may not be deterministic, but even if it were deterministic,
2132960	2134800	it wouldn't be completely predictable.
2134800	2136560	So in fact, here is an example here.
2138320	2143960	If you take a video, this is a top-down video of a highway that looks like cars
2143960	2146440	driving around just following the blue car.
2146440	2149160	And you train a neural net to predict what's going to happen in the video
2149160	2150600	after the first few frames.
2150600	2156400	It produces blurry, it makes blurry prediction because it can't predict if
2156400	2162000	the car that's behind you is going to accelerate or break or change lane or whatever.
2162000	2165600	So it makes an average of all the possible future and that's a blurry image.
2165600	2169200	Same with, this is an old paper where we attempted to do video prediction using
2169200	2174120	neural nets and the predictions are blurry because there's too many things
2174120	2176600	that can plausibly happen and the system can only predict one thing.
2176600	2177600	So it predicts the average.
2179840	2180840	So that's no good.
2180840	2184600	The solution to this is what I call a joint evading predictive architecture.
2184600	2188280	And this is really the most important slide of the talk.
2189800	2195520	So the normal way to make predictions is through a generative model.
2195520	2196400	What's a generative model?
2196400	2198600	It's a model where you have a bunch of variables you observe,
2198600	2201840	let's say the initial segment of a video.
2201840	2204920	You run it through an encoder and through a predictor and the predictor predicts
2204920	2208440	y, which is, let's say, the continuation of that video.
2209560	2212600	And you have some cost function that measures the discrepancy divergence
2212600	2215800	between the predicted y and the actual y you observe.
2215800	2217600	This is when you train your world model.
2219280	2222320	It could be that the predictor has an action variable that comes in,
2222320	2223880	but in this example there isn't.
2225560	2229920	So examples of this are things like variational auto encoders,
2229920	2234400	mass auto encoders, or denoising auto encoders, which is a more general concept.
2234400	2238600	And so basically all NLP systems, including LMS, are of this type,
2238600	2239360	the generative models.
2240360	2242360	But here is the thing.
2242360	2245440	You don't want to be predicting every detail about the world.
2245440	2248360	Here you have to predict every single detail about the world.
2248360	2250880	So it's easy if it's text, because text is discrete.
2250880	2255040	So predicting the next word, I cannot predict the next word from a text.
2255040	2261320	But I can predict within 10 possible words some probability distribution of
2261320	2265120	the, over all the words in the dictionary of which word comes next, right?
2265120	2269320	They can represent distributions over discrete variables.
2269320	2272920	I cannot do this over the set of all possible video frames.
2274080	2277560	I cannot usefully represent a distribution over the set of all possible video frames.
2279560	2285200	So I can't use the same trick for video that is used for language.
2285200	2289800	The reason why we have LMS that works so well is because text is easy.
2289800	2290520	Language is simple.
2291840	2294280	We only popped up in the last few hundred thousand years anyway, so
2294280	2295240	it can be that complicated.
2296240	2302040	And it's also processed in the brain by two tiny areas called the Vernike area
2302040	2304600	for understanding and the Borke area for production.
2305640	2306800	What about the rest of the brain?
2306800	2308920	The prefrontal cortex, that's where we think, okay?
2310200	2314360	That's not part of LMS, the LMS are perhaps good models of Vernike and Borke,
2314360	2314880	but that's it.
2317880	2321880	Okay, so what I'm proposing here is to replace this generative architecture by
2321880	2325400	a joint embedding architecture and the essential characteristic of it is that
2325400	2329640	the variable that you want to capture the dependency of with respect to X goes
2329640	2333440	itself through an encoder and the encoder eliminates the relevant information
2333440	2335600	that is not useful for anything.
2335600	2341800	Okay, so for example, if I had a video of this, if I was shooting a video of
2341800	2345680	the room here and then panning the camera and asking a system to predict
2345680	2350960	what's the rest of the room, it would probably predict that the rest of
2350960	2357200	the room looks like the initial part that there'd be a lot of people in
2357200	2365920	different seats, but it couldn't predict your age, gender, hairstyle, clothing,
2367200	2371800	or the texture, precise texture of the floor or things like that, right?
2371800	2376440	So there's details that cannot possibly be predicted and one way to avoid
2376440	2380200	predicting them is to basically eliminate that information from the variable to be
2380200	2382760	predicted through an encoder.
2382760	2385680	So that's a joint embedding architecture or predictive architecture because it has
2385680	2386680	a predictor.
2386680	2391040	Now there's an issue with this thing, which is that if you train a system with,
2391040	2394600	let's say, a piece of video and the following piece of video and you just train
2394600	2399200	it to minimize the prediction error, you train the whole thing, it collapses.
2399200	2404680	It collapses, basically the encoders ignore the inputs, they produce constant
2404680	2409000	vectors for SX and SY and the predictor just needs to map SX to SY and it's a
2409000	2413280	constant, so it's super easy, okay, bad.
2413280	2417120	So the question now is how do we prevent this from happening?
2417120	2418640	How do we prevent it collapse?
2418640	2423000	It doesn't happen with generative models because they can't collapse.
2423000	2427280	So there are three flavors of those joint embedding architectures, a simple one
2427280	2431440	where you're basically trying to make the two representation of SX and SY identical.
2431440	2436640	So for example, X and Y are two different views of the same scene and you want SX
2436640	2440600	to represent the content of the scene, so it doesn't matter where you look it from.
2440600	2443160	You just want to make the representations equal.
2443160	2445640	When the encoders are identical, this is called a Syme's network.
2445640	2450480	This is another idea that goes back to the early 90s.
2450480	2453640	You have deterministic joint embedding architectures and then you have joint
2453640	2458080	predictive architectures that may be non-deterministic where the predictor
2458080	2462800	function has a latent variable that could be drawn from a distribution or taken
2462800	2466720	in a set that would allow that system to make multiple predictions if necessary.
2470720	2475960	Now, we have to ask ourselves the question of how do we train those things?
2475960	2481560	And I'm going to use a symbolism here where that I've used the rectangles
2481560	2487480	and squares of cost functions, energy terms, the circles of variables,
2487480	2491320	observed or not, and those symbols here are deterministic functions.
2491320	2495480	Imagine a neural net, okay, trainable.
2495480	2499720	We may have to hardwire some cost functions in the system to have it,
2499720	2503600	to drive it to focus on aspects of the input that are important.
2503600	2508200	So that's the purpose of that C cost function at the top.
2508200	2513200	Okay, but to explain how to train those things,
2513200	2517120	I'm going to have to explain a little bit what energy based models is about because
2517120	2520720	the classical kind of probabilistic modeling in machine learning kind of goes
2520720	2524520	at the window when we use the joint embedding architectures.
2524520	2525640	So what's an energy based model?
2525640	2530280	Energy based model is a learning system that captures the dependency between
2530280	2534400	two sets of variable x and y through an energy function that is supposed to take
2534400	2539560	low values, low energies around data, training samples.
2539560	2543480	So imagine those black dots are training samples.
2543480	2547680	You want that energy function f of x, y to take low values around the training
2547680	2550400	samples and then higher values outside.
2550600	2553280	And that system will capture the dependencies between x and y.
2553280	2556680	If I give you a value of x and I ask you what can be the possible values for y,
2556680	2560000	you're going to tell me, well, it's either this or that or maybe that other thing at
2560000	2561400	the top.
2561400	2565640	Okay, so it's not a mapping from x to y, it's an implicit function.
2565640	2568960	And by figuring out what value of y minimizes the energy function,
2568960	2570200	you can do inference.
2570200	2574080	You can infer y, possibly, but you don't necessarily have to do that.
2575600	2576680	So that's energy based model.
2576680	2581040	It's kind of a weaker form of modeling than probabilistic modeling.
2581040	2584960	And so now the learning problem becomes how do you train this energy function,
2584960	2590480	which is going to be some big neural net, so that the energy takes low value around
2590480	2592760	the training samples and high values outside.
2592760	2595520	If you're not careful, you're going to get a collapse so that the same type of
2595520	2598600	collapse I was telling you about before, if you just pull down the energy of the
2598600	2602960	training samples, minimize the prediction error in this joint invading architecture,
2602960	2604520	you're going to get zero energy for everything.
2604520	2607200	It's not a good way to capture the dependencies.
2607200	2609280	You have two classes of methods, contrastive methods.
2609280	2613040	So contrastive methods consist in generating those green points,
2613040	2618000	which are outside the region of data, and then push the energy up while you push
2618000	2620120	down on the energy of the data points.
2620120	2625040	Okay, so that's going to create a groove in the energy surface, and
2625040	2627320	the system will have captured the dependency between x and y.
2628520	2631520	But there's an alternative here, which is regularized methods,
2631520	2636000	where the point of those methods is to minimize the volume of space that can take
2636000	2640280	low energy, so that when you push down on the energy of data points,
2642560	2647040	the rest of the space takes higher energy because there is only a small amount of,
2647040	2649520	a small region of low energy to go around.
2649520	2651480	So those are the two classes of methods.
2651480	2655960	Every method you ever heard of in machine learning can be viewed as one of those two.
2655960	2661120	Most probabilistic methods actually belong to the contrastive category.
2662480	2667400	Anything that uses Monte Carlo sampling, for example, is contrastive.
2667400	2670680	And then things like sparse coding and k-means and
2670680	2673760	things like that are more on the regularized method side of things.
2675880	2679320	Okay, so I'm asking you to do four things.
2679320	2684200	Abandoned generative models in favor of the joint embedding architectures, right?
2684200	2687280	So generative models are the most popular thing at the moment.
2687280	2693760	Forget about it, at least if you're interested in getting to the next step in AI.
2693760	2696600	Abandoned probabilistic models, because if you have those joint embedding
2696600	2700360	architectures, you cannot actually use it to derive a pure y given x.
2702160	2704200	The only thing you can use is sort of energy-based view.
2705920	2708600	Abandoned contrastive methods in favor of those regularized methods,
2708600	2711160	which I'll talk about a bit more.
2711160	2713480	And then something I've said for many years now, abandoned reinforcement
2713480	2714600	modeling because it's too inefficient.
2717360	2719880	So those are some of the pillars of machine learning.
2721000	2725800	And I realize this is not a very popular opinion here, but okay.
2725800	2727640	So what about those regularized methods?
2727640	2728720	I'm just going to give you one example.
2728720	2729480	There's a whole bunch of them.
2729480	2735360	There's like a dozen of them, but I'm just going to give you one called Vicreg.
2735360	2741280	And the basic idea of it is to prevent those representations from collapsing.
2741280	2744040	We're going to use a criterion that attempts to maximize the information
2744040	2745880	content that comes out of those representations.
2747320	2750240	Okay, so we're going to measure the information content in some way, and
2750240	2753280	then maximize the information content or minimize the negative information content.
2754480	2757160	We're going to do this for both SX and SY.
2757160	2759480	We're also going to minimize the prediction error.
2759480	2761200	And if we have a latent variable, we're going to have to minimize
2761200	2763240	the information content of that latent variable.
2763240	2765160	I can't explain why, because it would take too long.
2765160	2769640	But you have to do that also to prevent another type of collapse.
2769640	2772040	I'm going to focus on how you do that.
2772040	2776760	So the sad news is we don't have good ways to measure information content, or
2776760	2781400	we don't have any good ways to estimate lower bounds on information content,
2781400	2785000	so that if we push up on this lower bound, the information content will go up.
2785000	2787320	We only have upper bounds for information content.
2787320	2790640	So we're going to do a very stupid thing, which is push up on the upper bound of
2790640	2793760	information content, and hope the actual information content will follow.
2795840	2796280	And it works.
2797280	2804800	So, there's a simple way to prevent the encoder from completely collapsing.
2804800	2808400	Which is to insist that every variable in SX, SX is a vector.
2808400	2811720	And you insist that every variable, as measured over a batch,
2811720	2814960	has a standard deviation that is at least one.
2814960	2817680	Okay, so this is the cost that you see at the top here.
2819200	2823120	Measure the standard deviation of each component of SX, and
2823120	2826200	put it in a hinge loss so that the standard deviation is at least one.
2827320	2829560	So that prevents the system from completely collapsing.
2829560	2836200	But it can still cheat by making all the components of SX equal or correlated.
2836200	2840840	So the second term says I want to minimize the off diagonal terms of
2840840	2844520	the covariance matrix of those vectors measured over a batch, right?
2844520	2847600	So I want pairs of variables to be uncorrelated.
2848960	2851800	So basically, the collection of those two criterion says,
2851800	2856400	if I measure the covariance matrix of those vectors, SX and SY, coming out over
2856400	2860800	a batch, I want the covariance matrix to be as close to the identity as possible.
2862520	2865880	There's a number of different methods that have been proposed to,
2865880	2867960	that are kind of similar to this, Barlow-Twins.
2867960	2873040	So this one is called Vic-Rag from my group at Meta in collaboration with Jean-Ponce.
2874840	2879980	And then variations of it, but like similar methods from Berkeley in
2880940	2883940	the E-Mise group at Berkeley called NCR squared.
2883940	2885660	Yeah, maybe one minute.
2885660	2889420	Yeah, so this works really well and
2889420	2896060	I'm going to not bore you with tables of results that show you how well it works.
2896060	2901860	Only to mention something else, which is another method to do this kind of
2901860	2907180	self supervised running which is closer to this JEPA architecture called IJEPA.
2907180	2912340	So this is for learning features for images without having to do the documentation.
2912340	2915340	But basically it's for masking and this works amazingly well, it's very fast.
2915340	2918180	It's a new method, paper is on archive.
2919980	2925740	I don't have time to explain how it works, but basically you run an image
2925740	2929260	through two encoders, one is the full image and
2929260	2933860	the other one is sort of a masked image, partially masked image.
2933860	2936460	You run them through the same encoder or very similar encoder and
2936460	2941220	you try to predict or to predict the full feature representation of the full image
2941220	2944100	from the representation obtained from the partial image.
2944100	2947100	And just doing this produces really good features for images.
2947100	2951420	You get really good performance on object recognition in images and stuff like that.
2951420	2954020	Again, tables that show you that's true.
2954020	2958820	But I'm coming to the end, so the reason for
2958820	2961140	training those JEPA is to build world models.
2961140	2963060	So architectures are this type.
2963060	2966180	So this is a JEPA, but it's also a world model.
2966180	2969140	That, given an observation about the state of the world,
2969140	2972980	is going to be able to enter an action or imagined action in latent variables.
2972980	2974820	It's going to predict what's going to happen next in the world.
2975900	2979380	And once the time passes by, we're going to observe what happens and
2979380	2983260	then perhaps adjust our system to train.
2983260	2987500	But we want to use a hierarchical version of this where we can have a higher
2987500	2992540	level, higher abstraction, higher level of abstraction representation that will
2992540	2994620	allow us to make predictions further in the future.
2995580	2999100	Okay, I can't tell you the details of how I'm going to get to the train station,
2999100	3002860	but I know I'm going to have to be at the train station around 4 PM.
3002860	3006100	Okay, so that's the high level.
3006100	3009460	And we have early experiments with sort of various complicated neural net
3009460	3012820	architectures which I'm not going to detail to train from video,
3012820	3016180	try to predict basically what's going to happen in the video using warping and
3016180	3017620	stuff like that and it works really well.
3017620	3022700	But in the end, what we'll have is a hierarchical system from which we can do
3022700	3026780	a hierarchical planning and then we'll have been trained to predict what's
3026780	3030020	going to happen in the world as a consequence of actions or
3030020	3032820	latent variables that we can observe, that we can infer.
3034940	3038620	And those systems will be able to plan and reason and
3038620	3041740	will be controllable because the behavior is entirely controlled by the cost
3041740	3043380	functions we ask you to minimize.
3044500	3050740	And so much more controllable than current LNMs and that's pretty much the end.
3050740	3053820	So cell supervised learning is really the ticket.
3053820	3058580	Handling and certainty can be done with this energy-based model method and
3058580	3062500	using the joint embedding architecture that allows us to avoid predicting all
3062500	3064100	the details that are irrelevant about the world.
3066020	3068700	Learning world models from observation and interaction and
3068700	3073540	then reasoning and planning is done by basically gradient-based minimization
3073540	3075300	with respect to actions.
3075300	3076460	And that's it, thank you very much.
3076460	3096020	Thank you, John, for the great talk.
3096020	3100260	Now we'll have the second part, which is the Fireside Chat between John and
3100260	3102260	Osama, so please.
3102260	3106220	Thank you very much, John.
3106220	3110940	It was actually truly inspirational because it is definitely different
3110940	3114420	than your typical machine learning talk, so I enjoyed that.
3114420	3117860	Well, to you to throw away all the basic pillars of machine learning, so yes.
3120420	3124060	So I've collected a bunch of questions, some coming from the audience,
3124060	3127420	some coming from our institute and our faculty.
3127420	3132260	And we'll try to go through them in 20 minutes or whatever we can cover.
3133260	3137580	Normally, I would commit to answering every question on social media, but
3137580	3142220	because we got 150 questions, I'm afraid to commit my time or yours to this at
3142220	3144100	this point, but we'll try our best.
3145540	3146940	So I'll start with my first question.
3148260	3151780	It's been a long-standing wisdom in statistical inference and
3151780	3157700	probabilistic reasoning that when the number of parameters of a model gets
3157700	3162100	large enough, you kind of lose your ability to generalize and
3162100	3165740	you start just memorizing data, and we all know that that's no good.
3165740	3169980	That's just too detailed, the bias variance trade off.
3169980	3176540	But somehow, deep learning seems to have broken through this barrier.
3176540	3181300	When we went from regular neural nets to the deep nets, and
3181300	3187220	is there an intuition or understanding today as to why this is working in
3187220	3190980	LLMs with hundreds of billions and now trillions of parameters.
3190980	3194580	Right, well, the fact that it is working,
3194580	3199180	that you can train a ridiculously over-sized neural net, and
3199180	3206460	it will still work reasonably and generalize is dumb-founding.
3206460	3209860	So much that it contradicts every single thing that has been written in every
3209860	3211700	statistical textbook.
3211700	3214980	That you should never have more parameters than you have training samples, right?
3215020	3217580	If you're fitting a polynomial or something like this.
3217580	3220380	But we knew experimentally, even in the late 80s and early 90s,
3220380	3222740	that you could make those neural nets pretty big.
3222740	3225860	And even if you didn't have a huge amount of training data,
3225860	3227500	it would still work pretty well.
3227500	3229460	There was just no theoretical explanation.
3229460	3232700	So the theorists told us, you're wrong, you're stupid.
3233980	3237380	This cannot possibly work, so I'm not gonna believe your results.
3237380	3241540	And that's in part what made it very difficult to get neural nets
3241540	3245660	accepted in the late 90s to the 2000s.
3247860	3251140	But it turns out there is a phenomenon that has since been named
3251140	3256140	double descent, which is that if you increase the number of parameters in
3256140	3262180	a model for a constant size training set, your training error,
3262180	3264780	of course, is gonna go down, right, to zero, probably.
3266100	3270460	But your test error is first gonna go down, go through a minimum, and
3270500	3275060	then go up when you start having parameters,
3275060	3279020	a number of parameters that is commensurate with the number of samples that you have.
3279020	3283820	Okay, so that's when the model starts to be over parameterized, and it goes up.
3285180	3288540	But here is the thing, if you keep going up, if you keep making the model more
3288540	3292180	complex, the tester will go down again.
3292180	3294020	It will go through a maximum and then go down again.
3294020	3296620	That's called the double descent phenomenon, nowadays.
3296620	3301980	And it will do this if you regularize the parameters somehow.
3301980	3306060	You don't necessarily need to regularize explicitly because neural nets have some
3306060	3308180	sort of implicit regularization in them.
3308180	3313980	But you see this phenomenon, even works if you fit a polynomial, right?
3313980	3319700	Fit a 10 degree polynomial with 11 data points.
3320820	3323180	And your fit will be horrible, right?
3323180	3325820	Because the polynomial has to go to every single point and
3325820	3327540	it's gonna go wild in between.
3327540	3330260	But if you increase the degree of the polynomial to something like 20 or
3330260	3333860	30, and you regularize the coefficient, your error goes down again,
3333860	3338980	your test error goes down again, the fitted polynomial goes through every point.
3338980	3344340	But it's less irregular than with just degree 10.
3344340	3348540	So this existed all along, it's just that people didn't realize it was a thing,
3348540	3352020	or at least people who were not practitioners of neural nets who
3352020	3353660	had realized this was a thing.
3353660	3355700	So do we have any explanation why this is a thing?
3357180	3361820	So there's a lot of conjectures, there is some theoretical work.
3361820	3364820	Some people claim it's about the dynamics of gradient descent.
3364820	3368380	There is some sort of implicit self regularization in neural nets that occurs.
3369660	3373140	Whereby the system kind of recruits just a number of virtual
3373140	3375140	parameters that it needs somehow.
3376620	3379780	Some say it's regularization due to stochastic gradient.
3379780	3382100	So stochastic gradient descent, which is noisy.
3382100	3388260	And so perhaps that forces the system to find robust minima in
3388260	3392900	the objective, in the loss, that generalize better.
3394340	3396100	It's not clear, there's a bunch of different things.
3396100	3399540	Yeah, definitely one of the mysteries that keep us interested.
3401300	3405500	This question comes from Raman Chandrasekharan or
3405500	3409260	Chandra, who's one of our senior research scientists in Seattle.
3410140	3416660	How long before LLM, and maybe, I don't know, models in general,
3416660	3421340	can genuinely start saying, I don't know the answer to this question.
3421340	3425700	As opposed to attempting to guess the right autocomplete anyway,
3425700	3427060	because that's what it's programmed to do.
3428060	3432700	Yeah, so I don't think current LLMs can really do this at the moment.
3432700	3435900	I think it's probably possible with architectures, the type that I show.
3435900	3441300	Because if there are no good minima to the objective that the system is
3441300	3445020	attempting to minimize to produce it, it's output, it's gonna say, well,
3445020	3449180	I found this thing, it seems to be minimizing this objective, but not very well.
3449180	3451740	So probably it's not the right answer you were looking for.
3453180	3457940	Or by the shape of the minimum, of this energy minimum, perhaps, you could say,
3457940	3462100	like if it's really a sharp minimum, then that's the one answer that
3462100	3464500	corresponds to the question.
3464500	3468180	If it's kind of a shadow minimum, maybe there are multiple answers that are possible.
3468180	3475180	So you might be able to attribute, to map energy levels to,
3475180	3479340	of different answers to a confidence level.
3482620	3486980	To, this is a question from me, I guess.
3486980	3492340	Two aspects of critical importance to,
3492340	3497620	let's say, GPT or large language models that are not talked about a lot by
3497620	3502140	the companies who do them are data curation.
3502140	3507260	Getting that clean data, that balanced data, that representative data,
3507260	3514300	which by the way, counter to popular belief, open AI spent a lot of its money
3514300	3519660	on curating just that right corpus so that they can do the training reliably.
3519660	3526460	And the second part, which is something we're big believers in at the Institute for
3526460	3530820	experiential AI, experiential AI stands for AI with the human in the loop.
3530820	3534740	Having that human intervention through relevance feedback,
3534740	3539300	that we know now open AI is doing and has been doing.
3539300	3542900	And some of the queries are actually taken over by humans at some point when
3542900	3544740	they make enough errors to come back.
3544740	3547660	But the good thing is they learn from them and we think that's a great practice.
3548660	3553700	Why do you think the companies don't want to talk about the importance of the data
3553700	3555220	and the importance of the human in the loop?
3556340	3557700	I don't know if they don't want to talk about it.
3557700	3566540	I mean, it's clearly very expensive to create data to produce a good LLM.
3566540	3572460	But in my opinion, it's doomed to failure in the long run for two reasons.
3572580	3581740	The first one is the curation requires going through this enormous amount of data that
3581740	3582980	you want to train the system on.
3582980	3587140	And any data you eliminate, it's less training data for your model.
3588740	3592660	But the second thing is even with human feedback,
3592660	3598660	human feedback that rate different answers or fine tune the system for
3598660	3602460	certain question and answers, sort of manually curated.
3604740	3609620	If you want those systems ultimately to be the repository of all human knowledge,
3610740	3614980	the dimension of that space of all human knowledge is enormous.
3614980	3618180	And you're not going to do it by paying a few thousand people in Kenya or
3618180	3621860	India rating answers.
3621860	3628500	You're going to have to do it with millions of volunteers that find
3628500	3632180	the system for all possible questions that might possibly be asked.
3632180	3638100	And those volunteers will have to be vetted in the way Wikipedia is being done, right?
3638100	3645220	So think of LLMs in the long run as a version of Wikipedia plus your favorite
3645220	3650500	newspapers plus the scientific literature plus everything, but you can talk to it.
3650500	3652700	You don't have to read articles, you can just talk to it.
3653660	3658460	And so if it's supposed to become the repository of all human knowledge,
3658460	3664020	the thing it's been trained to do will have to be curated by
3664020	3670220	quite sourcing the way Wikipedia is to cover all the possible things that
3670220	3671980	may be covered.
3671980	3679180	This is a very strong argument for having open source based models for LLMs.
3679260	3684540	So in my opinion, the future is inevitably going to be that you're going to
3684540	3689740	have a small number of open source based LLMs that are not trained for
3689740	3694780	any particular application, they're trained on enormous amounts of data
3694780	3695900	that requires a lot of money.
3695900	3698380	So you're not going to have 25 of them, you're going to have two or three.
3699660	3702860	And then actual applications are going to be built on top of it by
3702860	3706620	finding those systems for particular vertical applications.
3706620	3707340	That's the future.
3708060	3713020	Sadly, in the industry, there are people who are lobbying governments to
3713020	3717820	actually make the open sourcing of large scale LLM illegal.
3719260	3725340	What they're worried about is potential misuse of LLMs by bad actors,
3726460	3728300	potential users.
3729900	3733820	So some people in the US, for example, are worried, oh, if we open source our LLMs,
3733820	3737740	you know China and North Korea and Iran will put their hands on it and that's
3737740	3738300	going to be bad.
3740940	3744380	And then some people are worried that the real powerful LLMs are going to be
3744380	3748860	super intelligent and destroy humanity, which I think is preposterous,
3750220	3753100	even though some of my friends that I respect actually believe this.
3754220	3760140	So I think it would be really, really bad if those lobbying attempts succeed.
3760860	3764860	I'm very much in favor of a future with open based models.
3765580	3768220	And there's going to be bad actors, but there's going to be countermeasures
3768220	3769020	against them.
3769020	3777100	It's going to be, you know, or powerful good AI cop against their nefarious AI, essentially.
3780060	3783660	So let's shift to this trend.
3783660	3789340	And this is, I've merged a question from Jimmy Shanahan from our AI solutions hub
3789900	3794060	with questions from Tomo Lasovic and Ken Church at EAI.
3795660	3802140	The trend nowadays seems to be heading towards bigger is better, more compute, more parameters.
3803580	3809420	There's been some studies even suggesting that by open AI themselves that they're moving at a
3809420	3815660	pace faster than Moore's law, even though now they seem to be normalizing towards it,
3815660	3818060	although Moore's law itself is slowing down.
3818300	3822460	So the real question here is how long can this go on?
3822460	3824380	And will we ask them, what do you think?
3824380	3828220	I know that we may not have the final answer here, but it seems crazy.
3828220	3831740	Like all you have to do is wait a few weeks and you hear about the next big model.
3832460	3836220	Well, so actually in the last few months, you've seen a decrease in the size.
3837340	3843420	So Lama, for example, the 13 billion version of Lama in terms of raw performance on standard
3843420	3848380	benchmarks is actually better than GPT-3, which has 175 billion parameters.
3848940	3852140	And so it's not clear that bigger is better.
3852140	3857980	With the architecture I propose, I think you can get away with smaller systems that
3857980	3859100	perform at least as well.
3859100	3862700	The reason being that when you train in a current autoregressive LLM,
3862700	3867100	you have to train it to not just accumulate knowledge, not just predict the next word,
3867100	3868460	but also solve a lot of problems.
3868460	3875020	So basically, know how to produce the right answer when you specify the question in the
3875020	3875660	prompt.
3875660	3879260	And so everything is wrapped into the weights of that single model.
3879260	3882060	Whereas in the model I propose here, the architecture I propose,
3882780	3884460	the word model is just a word model.
3885100	3889980	The task is specified by the objective function, which may include the prompt.
3889980	3891820	So it may include the representation of the prompt.
3892700	3895100	And so you're separating different things.
3895100	3901900	You're separating the inference procedure that produces the output from the word model,
3901900	3905260	the sort of the mental model of the world that the system uses,
3905260	3908140	from the task itself, which is specified by the objective.
3908140	3911500	And you can probably get away with smaller networks for the same performance.
3913100	3919900	But yes, I mean, there were a few years ago models by Google that had like a trillion
3919900	3920380	parameters.
3920380	3923900	There were basically multiple models that were stuck together with some sort of
3925740	3926300	gating.
3926300	3930700	Yeah, between them, they've kind of backpedaled on this a little bit.
3930700	3933740	If you want the system to be practical, like to be used by everyone,
3933740	3935740	you can't make them like a trillion parameters.
3935740	3937260	Right now, it'd be just too expensive.
3938380	3940460	So you have to minimize that size.
3940460	3945820	Now you can run things like Lama 7 billion on a Mac.
3947340	3949100	You know, you can run on a laptop.
3949100	3951260	You can't train it on a laptop, but you can run it.
3951260	3951500	Yes.
3951660	3959660	So clearly, you believe you're advocating for a different view of what the machine learning
3959660	3963820	and AI community should be doing as opposed to what they are doing today.
3964780	3966380	That's the story of my career.
3966380	3966620	Yes.
3968060	3970380	And this question is coming from Ken Church.
3971420	3973340	A former colleague from AT&T.
3973340	3974620	From AT&T.
3974620	3977660	He is at the Institute for AI in Silicon Valley.
3978620	3979340	Do you believe?
3981660	3987820	Well, I guess the question is, how long do you think it will take to pivot the field from
3987820	3990300	where it is to where you would like it to be?
3992860	3994780	Well, last time I tried, it took 15 years.
3998060	4001340	If not more, actually, depending on how you count, it might have been 20.
4002300	4005180	So I don't know.
4005180	4010860	I think I see a phenomenon in kind of this is a sociology of science question.
4011420	4014540	When there is something that seems to work, everybody gets excited about it.
4014540	4023340	And it's a fashion trend type phenomenon where every paper written is about this trend.
4023340	4027260	I saw this in computer vision back in the early to mid 2000.
4027260	4029340	Everybody was working on boosting.
4029340	4032780	That was the thing, you had to work on boosting for computer vision.
4032780	4041180	And then someone in 2006 and 2005 came up with a different way of doing vision using dense
4041980	4046620	features like sift and stuff like that using unsupervised running for a middle layer and then
4046620	4047820	an SVM on top.
4047820	4049420	All of a sudden, everybody was doing this.
4050540	4054140	And then starting in 2013, everybody started using convolutional nets.
4056300	4057740	That came from results.
4057740	4060620	So now we are in a phase where everybody is focused on LLMs.
4061180	4063980	And if you don't work on LLMs, nobody wants to talk to you.
4065820	4067180	But it will change.
4069420	4070940	So you think it's 15 years?
4070940	4072700	No, I think it's more like five.
4072700	4077340	Like I made that prediction that autoregressive LLMs will probably...
4077340	4079420	Five years, that's true, yeah, they're doomed.
4079980	4081740	Yeah, I mean, I might be wrong, obviously.
4082460	4084300	We will hold you to that.
4084300	4086380	I'll come back and revisit in five years.
4086380	4089740	Maybe it's a wishful thinking, self-fulfilling prophecy perhaps.
4091260	4095500	A question for something different here from Sam Scarpino,
4095500	4099100	director of AI and Life Sciences at the Institute for Experiential AI.
4099980	4106860	What are the biggest gaps on the education side for graduates of higher education in AI
4107420	4111260	and in particular the new directions AI is taking?
4111260	4113260	What do you think is missing?
4114220	4117420	So I think what's missing...
4117420	4121660	So it depends which major you're following.
4124620	4132860	Most computer science curricula in the US are very weak in mathematics.
4133820	4138540	The requirements for mathematics in a typical CS degree,
4138540	4141020	the minimum requirement is very, very small, right?
4141020	4145420	It's one course in discrete math and perhaps in algebra if you're lucky.
4146380	4149180	Maybe a probability if you are courageous.
4151100	4152460	But what about optimization?
4152460	4154780	That would be something that would be very useful.
4154780	4160140	And then there is courses in physics because the mathematics of inference
4160140	4166220	and variational autoencoder and stuff like that, graphical models, etc.
4166220	4168860	The mathematics of this is from statistical physics.
4169820	4174060	And so if you have a choice between taking your course in, I don't know,
4174940	4178860	mobile app programming or quantum mechanics, take quantum mechanics.
4180700	4181340	I'm not kidding.
4188540	4195340	This is a question that came from the audience and a few of the people at the Institute.
4196220	4201100	Your thoughts on the current, you know, these recent congressional hearings where
4202780	4209900	certainly seems like much of the testimony by some Altman was understandably self-serving.
4209900	4214860	You know, they need to be allowed to compete and have their way of working protected.
4215420	4219020	At the same time, he's encouraging the rest of the community to be open source.
4219660	4224700	What would you have said to Congress?
4224700	4225820	Have you been on those hearings?
4228220	4229020	I was not invited.
4230860	4233100	I was not invited to the White House either before that.
4235980	4245420	So what I would have recommended is that if you want a vibrant ecosystem on top of current AI technology,
4246380	4253180	you need to have sort of open source based models on top of which an industry can be built.
4254380	4259660	And that industry will build vertical applications for particular domains
4259660	4260940	on top of a base model.
4260940	4266220	You don't want to have 25 companies selling 25 different base models
4267660	4268780	and keep them closed source.
4268780	4272540	If you want an industry to be built on top of it, the infrastructure has to be open.
4273260	4278140	Because that's the only way to really sort of know what you're doing, essentially.
4279660	4282140	And to have some control about your future, right?
4282140	4285820	You can't just go like this and pray that.
4285820	4287020	Unix versus Windows.
4287660	4292060	Right, so if you go back to the history of the Internet, there was a similar story where
4293340	4297740	back in 1992 when the built Internet Al Gore started to figure out like what,
4297740	4299900	you know, how do we build the information superhighway?
4300860	4306140	They went to see, you know, the big communication companies like AT&T and AT&T told them,
4306140	4307420	oh, you know, leave it to us.
4307420	4308620	We'll build the stuff.
4308620	4312620	It's going to be, you know, ATM and ISD enter the home and blah, blah, blah.
4312620	4316140	It'll be wonderful and you'll have to pay, you know, $5 per hour.
4318220	4319420	And Al Gore said no.
4319420	4323820	He said we're going to make the, what was an ARPANET that became the Internet,
4324060	4330540	basically available to the public and delocalized and, you know, self,
4331820	4335500	basically open in terms of standard and no company is going to control it.
4336060	4338140	And that was a really, really good idea.
4338140	4339340	We can thank Al Gore for this.
4340060	4341900	The world can thank Al Gore, not just the U.S.
4347660	4348780	He did invent the Internet.
4349500	4353980	And then a similar story happened several years later when people started to realize
4353980	4359340	that you could use, you know, graphic browsers like Mosaic and Netscape and stuff like that,
4359340	4363340	right, when the World Wide Web became popular.
4364540	4367900	So there was a war between Sun Microsystems and Microsoft.
4367900	4372060	Sun Microsystems said, oh, we're going to sell you servers running Solaris,
4372060	4376300	the version of Unix, with, you know, our web server infrastructure and Java.
4377180	4379100	And you're going to be able to build, like, anything you want.
4379980	4385020	Microsoft said no, it's going to be Windows NT with the IIT web server and the ASP
4385980	4391580	website, you know, server-side protocol framework, whatever.
4392380	4393100	They both lost.
4394220	4402540	Sun Microsystem went bankrupt, was sold for parts to Oracle, and Microsoft essentially
4402540	4405100	exited the market.
4405740	4408460	One was Linux and Apache, open source.
4408460	4414540	And the reason is because it's such an essential basic infrastructure that it has to be open.
4415980	4422220	It progresses faster if it's open, and it's more reliable, it's more secure.
4422220	4423580	I mean, there's all the advantages.
4423580	4426460	And, you know, it's easier for startups to build on top of it.
4426460	4431740	So in the future, we're going to see AI systems as basic infrastructure.
4432700	4436860	All of our interactions ten years from now with the digital world will be through
4437580	4442620	an intelligent virtual agent that will be with us all the time.
4442620	4446460	It's like every one of us will have a staff of intelligent people working for us.
4447820	4448540	Okay?
4448540	4452540	We shouldn't be threatened by the fact that those things will be smarter than us.
4452540	4456780	Like everybody that, you know, is working with me at fair is smarter than me.
4456780	4459340	So I don't feel threatened by that.
4459340	4462460	You're not a very good manager if you're threatened by people who are smarter than you.
4462620	4468460	So your purpose actually should be to hire people, only people who are smarter than you.
4468460	4473100	But anyway, so we're going to have those intelligent systems that are going to be
4473100	4476060	under control that are going to help us, you know, daily lives.
4476060	4481420	And we need those systems to be open because if it's kind of a closed system controlled by
4481420	4486220	some company in California, it's going to be able to control our entire
4486380	4490620	knowledge and data diet.
4491580	4493100	And that's just too dangerous.
4493100	4495500	And it's not necessary.
4495500	4500060	It's necessary for a search engine or a social network because it has to be centralized for
4500060	4500700	various reasons.
4500700	4504220	But for an agent like this, it could run on your local device.
4504220	4505500	It could run on your laptop.
4505500	4509740	You don't have to talk necessarily with big servers in California.
4509740	4514460	You don't want to give all your, you know, deepest secrets to that.
4514460	4519020	So it's going to have to be an open fact form for that reason.
4520140	4525580	If nothing else, governments around the world are going to insist that this is the case.
4525580	4533340	So that's why I would tell Congress, make it so that, like, don't ban open source
4534140	4535020	LLMs.
4535020	4536460	They're not going to destroy humanity.
4537740	4540860	Yeah, they're going to be bad actors, but you know, you can have countermeasures
4541660	4543420	and make it open.
4543420	4545420	It's the only way to make it safe.
4553420	4556220	I'll ask, we'll make this a quick question with a quick answer.
4556220	4560140	And then I know we have some questions live, so we'll switch to those.
4564940	4568220	In a way, you kind of answered this question when you said LLMs are doomed.
4568220	4577180	But if LLMs were to become perfect, at least in language, would that ever give us insight
4577180	4580540	into how language and natural language understanding works?
4581740	4585260	The language model today is distributed over these billions of parameters.
4585980	4595100	And do you think we'll ever have an understandable LLM, like, for example, we use PCA to understand
4595100	4596220	what regression is doing?
4597020	4597900	Or is that hopeless?
4598700	4600620	At some point, I think it's going to be right to be hopeless.
4600620	4604620	I mean, we'll probably learn a lot about, you know, how the systems represent data
4604620	4606540	and, like, how they manipulate it and stuff like that.
4606540	4607740	So this is not opaque, right?
4607740	4613500	We can completely kind of, there's complete visibility on how the systems operate.
4613500	4617420	Now, the question is understanding really how the decisions are being made.
4617420	4621420	So I'm actually not particularly interested in those questions, like, you know, as long as
4622460	4623580	they work properly.
4625500	4629420	The same way, I'm not particularly interested in figuring out exactly how the brain works.
4629420	4635260	I'm more interested in figuring out how the brain builds itself so that it works, right?
4635260	4641340	So I'm more interested in learning than in studying the result of learning, if you want.
4641340	4643660	So it's the same for those systems.
4643660	4647340	I'm more interested in how you get them to learn what you want and how to solve the problem
4647340	4650060	in the end is kind of considerably less interesting, in my opinion.
4650460	4654940	But at some point, they're going to be, you know, super intelligent,
4654940	4656940	repulsory of all human knowledge.
4656940	4662220	You know, it's going to be too big for us to kind of comprehend at a deep level.
4663580	4664380	Fair.
4666380	4670380	And by the way, I failed to acknowledge that question came from Walid Saba, who's one of our
4670380	4675660	senior research scientists at the EAI up in Portland, Maine.
4676140	4680860	The next question I'll use, and then I'll switch over to audience questions,
4683820	4688780	comes from Gene Tunic, the director of AI Plus Health at the Institute for Experiential AI.
4691740	4696380	You believe that deep learning can eventually lead to human-like understanding,
4697580	4703740	and you have said that self-supervised learning from unlabeled data
4704620	4710860	can be a powerful tool, although it seems like in human learning, as I was watching your examples,
4710860	4718540	for example, a lot of that data is, in a way, supervised or tied to some kind of reinforcement
4718540	4726300	feedback around what to expect, is it good, is it bad, etc. So how, where do you draw that line
4726300	4738060	between, you know, can we really truly go towards unsupervised, or there's a huge dependence on
4738060	4743420	supervised and on those labels to get it right? Because the world is, in a way, is telling us
4743420	4747980	indirectly through supervision. So self-supervised learning, I mean, the reason it's called self-supervised
4747980	4751900	is that deep down it's actually supervised learning. It's just supervised learning where
4751900	4758060	the supervision signal is the input itself, right? So in a way, that's kind of, you know, a kind of
4761020	4766460	simple answer to that question. It's still supervised learning in the end, but with
4766460	4771580	particular architectures to handle uncertainty and dimensionality and things like that. Regarding
4771580	4775980	reinforcement learning, there is a point at which you need some form of reinforcement learning,
4775980	4781500	and you need it in two situations, or at least techniques that have been developed in the context
4781820	4788060	of reinforcement learning. The first situation is if the objective function that is optimized by
4788060	4793900	your system does not reflect the ultimate objective function, you actually want to optimize. So for
4793900	4801820	example, you're learning to ride a bike, your objective function is the, you know, time to the
4801820	4809980	next fall or something, or the inverse time to next fall, you want to minimize that, right?
4812140	4819980	But you don't know how to compute this from the internal state of your system. And so you need
4821020	4825980	to train an objective function to approximate this real cost, which in the context of reinforcement
4825980	4832300	learning is called a critic. So that's when you need one of those things. The other situation
4832300	4837820	where you need it is when your world model is not accurate because it's not been trained in all
4837820	4842300	corners of the state space, and you happen to be in a part of the state space that it wasn't trained
4842300	4846700	on. Your world model is going to be bad, and your predictions are going to be bad, your planning is
4846700	4854060	going to be bad. So to prevent this, you need to train your world model using things that are
4854140	4858780	called curiosity or exploration. And that's another concept that comes from reinforcement
4858780	4863180	learning. So don't completely abandon reinforcement learning, but minimize its use.
4864140	4870060	As we switch over to the live questions, let me, I can't help but ask you this question. It comes
4870060	4879500	from several anonymous people as well as Ken Church, your former colleague. Did you actually say
4880140	4885900	the revolution will not be supervised? I did, yeah. Okay. But actually, I stole it
4887020	4896940	from Adi Asha Efros from Berkeley. He had a magnificent slide that was a picture of a wall
4896940	4903820	painting in Chile someplace, which was one of those kind of revolutionary thing. And I took
4903820	4911500	that picture and overlaid on it. The revolution will not be supervised. Yes. Okay. So I stole
4911500	4917900	that from him. I deserve no credit. Shall we switch over to a question from the audience?
4917900	4924620	Yeah. So first question from Glenn Jenkinson is, what two questions about AI do you wish you
4924700	4933740	would ask more often? Two questions. I don't know. I get asked a lot of questions. I can't
4933740	4941740	imagine a question have not been asked. That's relevant. I mean, I think the important questions
4941740	4946780	are the ones that I'm asking myself. And I wish other people would sort of frame the
4946780	4953660	problems in the same way. So big question. How is it that any teenager can learn to
4953660	4957420	drive a car in 20 hours? And we still don't have level five autonomous driving?
4958940	4961900	That was the first question. So second question is, what are we missing?
4964300	4973020	That's the answer I want. Joe. Next question from Juan Leylanda. Do you think quantum computing
4973020	4975740	will have a significant role in the future of AI? No.
4976540	4989100	Or at least not any time soon. But the time this happens, I probably won't be alive anymore.
4989100	4996540	So I'm not taking a big risk. No, I don't think so. I mean, there's precious few
4997340	5002060	situations today where quantum computing could be useful. There's no situation where it actually
5002060	5007980	is useful because the quantum computers are not big enough at the moment. So it's a huge bet.
5007980	5013500	I think scientifically it's fascinating. I'm really fascinated by quantum computing at the
5013500	5020060	conceptual level. I have one or two papers with Seth Lloyd on connections between neural
5020060	5026140	nets and quantum computing. I think it's a very interesting topic, but I don't think it has any
5026140	5033740	practical value in the short term. Joe. One last question from Anton Dabura.
5034540	5040380	To what extent do you see ML models being used for problems that we already have pretty good
5040380	5045980	algorithms to solve, such as sorting shortest path, linear integer programming, and so on?
5045980	5051500	How would you characterize the boundary, if any? So there's a lot of problems that we can currently
5051500	5059100	solve that are NP-complete or NP-hard, and so we can solve them within limits. What we need very
5059100	5067580	often are approximate algorithms, so methods that give us approximate solutions to complex problems
5067580	5077420	that, in theory, are NP-hard, NP-complete, whatever, but if you reduce yourself to accepting
5077420	5086380	approximate solutions, might become solvable. So I think there is a lot to be said for ML methods
5086380	5093100	that do something that has become to be known as amortized inference. So amortized inference is
5093100	5097660	this idea that you might have a problem that is formulated as an optimization problem. Every
5097660	5104540	computing problem can be formulated as an optimization problem. And what you might be able to do is
5104620	5109900	solve that problem in certain cases, give a solution, and now what you do with this is that you
5109900	5115820	train in your net of some kind to predict, to approximate the solution to that optimization
5115820	5121820	problem from the specification of the problem, from the inputs. So that system will not be able to
5122460	5126540	completely solve the problem in those situations, but for the type of problem that you train it on,
5127340	5131420	it's going to be able to give you an approximate solution really quickly. Amortized inference.
5131420	5140860	There is a tutorial on this that was written and given at a recent conference by one of my
5140860	5146620	colleagues at fair called Brendan Amos, AMOS, very interesting concept.
5148460	5153340	I will close my questions with one last question, then we'll take a real live one and call it the
5153340	5159100	end. I have to use this. It comes from one of our faculty who wanted to remain anonymous,
5159100	5164220	I don't know why, but given the big excitement around LLMs and not without a reason,
5165020	5169660	what are some of the research directions that are possible to tackle for non-Google slash
5169660	5177180	Facebook type sized institutions that are under studies? Space for foundational research,
5177180	5183580	big open questions in need of creative solutions. Thus, if you were a young investigator today,
5183580	5189180	like a starting assistant professor, what would you do in this environment?
5189180	5193980	I mean, that's a problem I have to face when I have PhD students at NYU that don't have access to
5194940	5205100	16,000 GPUs, unlike people at fair. So I think a lot of most good ideas still come from academia,
5205100	5213100	so you're not going to be Google or Meta or Microsoft on beating the record on
5213100	5216700	translation or something like that. You don't want to do this in universities.
5217820	5222700	But coming up with new ideas, for example, the problem I mentioned of how do you do
5222700	5229420	hierarchical planning? How do you train a system to figure out how to represent the world and
5229420	5233340	action spaces so that you can do hierarchical planning? It's completely unsolved. You can do
5233340	5238620	this with toy problems. If you have any idea of how you might approach that problem on toy
5238620	5244940	problems, you don't have to have tons of GPUs for that. You will have an idea that might have a
5244940	5252860	huge impact. So if you have a good architecture that you can show, can learn some simple world
5252860	5257740	model from video, it's the same. You don't have to train on all of YouTube. You can train on
5257740	5263820	artificial environments and stuff like that and demonstrate that it works. It doesn't have to be
5264060	5268540	large scale. So this is the kind of stuff you want to do. And then there is a new domain which is
5268540	5274460	building on top of open source base models. So unfortunately, right now, the best base models,
5275980	5283020	LLMs are the LAMA class of models from $7 billion to $65 billion. They're not usable for commercial
5283020	5288140	use. They are distributed with a license for non-commercial use, only for research, which you
5288140	5293420	can of course use in the university. And there's a lot of work to be done to figure out how to
5293500	5298620	make those things safe, factual, etc. And you can work from those base models. You don't have to
5298620	5304700	retrain them from scratch. So you don't need to have roomful, rooms full of GPUs.
5305580	5308860	We'll try for one last question, maybe two. Go ahead, please, with your question.
5309820	5315500	Hi. So my question really dwells from the side of, or we'd love to hear your thoughts,
5315500	5321420	on impact and control of these large language models or any of these models, the fancy models
5321420	5326620	that you showed with billions of trillions of parameters. So the impact side is, do you really
5326620	5333020	give or how much thought do you give to the impact that would have on the community or on the people
5333020	5340620	in general, based on what that model does? And control is, once that model is out there,
5343340	5349260	how do I make sure that it doesn't do a certain things it's not supposed to do with regular,
5350220	5354620	the way people used to use internet before those models. It used to be very controlled
5354620	5360460	environment where you could have, in a way, regulate those environments. But now with models,
5360460	5366380	it's getting increasingly difficult and a slow process to have or do not have certain things in
5366380	5372540	those models. Okay. So there is a long view, a very positive one, which is imagine that all of us
5372540	5378060	have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff
5378140	5385420	of people working for us, but like super people working for us. This is going to create a new
5386380	5394860	renaissance for humanity. It's going to increase humanity's intelligence, however you want to
5394860	5403260	measure it. That has to be intrinsically good. It's been the case in the past that anytime a new
5403820	5407500	medium was invented or a new way of communication was invented, like the printing press.
5408940	5412780	Humanity kind of went to the next step. The printing press let
5415180	5422780	the dissemination of philosophy, science, secularism, democracy, all that stuff. The US
5422780	5434620	would not exist without the French philosophers of the 18th century. So neither would the French
5434620	5443180	revolution. So I think same for the internet, that gave people instant access to an enormous
5443180	5450060	wealth of knowledge. Also disinformation, but okay, I mean, we have to have countermeasures for
5450060	5455180	every technology can be used for good and bad. We need to have countermeasures for the worst
5455740	5464460	aspects. But ultimately, I think we need widest possible access to those AI systems by everyone.
5465500	5472460	Now, how do we make sure those systems don't lie to us? How do we make sure that the information
5472460	5477180	they give us is not under the control of someone that has nefarious purpose, you know, things like
5477180	5483340	that, which is I think a good reason for them to be open as I stated earlier. But I think it's a
5483340	5489660	bright future for humanity, you know, contrary to some people who tell young people don't expect
5489660	5497820	to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the
5497820	5505260	next question, but we are five minutes over our time limit. And I know we have to grab a bite and
5505260	5511820	deliver you to the train station on time, according to the hierarchical plan. So with that, please
5511820	5523980	join me in thanking Jan for an amazing session today. Thank you.
