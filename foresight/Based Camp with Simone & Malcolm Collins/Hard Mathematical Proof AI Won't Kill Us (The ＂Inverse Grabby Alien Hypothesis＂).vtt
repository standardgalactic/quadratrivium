WEBVTT

00:00.000 --> 00:05.280
So basically, no matter which one of these explanations of the Fermi paradox is true,

00:05.280 --> 00:09.680
either it's irrelevant that we are about to invent a paperclip maximizing AI because we're

00:09.680 --> 00:16.320
about to be destroyed by something else or in a simulation, or we're definitely not about to

00:16.320 --> 00:20.000
invent a paperclip maximizing AI either because we're really far away from the technology or

00:20.000 --> 00:24.880
because almost nobody does that. That's just not the way AI works. I am so convinced by this argument

00:24.880 --> 00:28.800
that it is actually, I used to believe it was like a 20% chance we all died because of an AI or maybe

00:28.800 --> 00:32.640
even as high as a 50% chance, but it was a variable risk if I've explained in other videos.

00:32.640 --> 00:39.200
I now think there's almost a 0% chance. A 0% chance assuming we are not about to be killed by a

00:39.200 --> 00:45.040
grabby AI somebody else invented. Now, it does bring up something interesting. If the reason we're

00:45.040 --> 00:49.760
not running into aliens is because infinite power and material generation is just incredibly easy

00:49.760 --> 00:55.280
and there's a terminal utility convergence function, then what are the aliens doing in the

00:55.280 --> 01:02.320
universe? Would you like to know more? Hi Malcolm, how are you doing my friend? So today we are going

01:02.320 --> 01:08.320
to do an episode, a bit of a preamble for an already filmed interview. So we did two interviews

01:08.320 --> 01:14.800
with Robin Hansen and in one of them we discussed this theory. However, I didn't want to off rail

01:14.800 --> 01:20.000
the interview too much going into this theory, but I really wanted to nerd out on it with him

01:20.000 --> 01:25.760
because he is the person who invented the grabby aliens hypothesis solution to the Fermi Paradox.

01:27.360 --> 01:31.440
I hadn't heard about grabby aliens before, so I'm glad we're doing this. This is great.

01:32.000 --> 01:39.280
Yes, so we will use this episode to talk about the Fermi Paradox, the grabby alien hypothesis,

01:39.280 --> 01:46.240
and how the grabby alien hypothesis can be used through controlling one of the variables,

01:46.240 --> 01:53.120
i.e. the assumption that we are about to invent a paperclip maximizer AI that ends up fooming and

01:53.120 --> 01:58.560
killing us all, because that would be a grabby alien definition only. If you collapse that variable

01:58.560 --> 02:06.000
within the equation to today, then you can back calculate the probability of creating a paperclip

02:06.000 --> 02:13.360
maximizing AI. And spoiler alert, the probability is almost zero. It basically means it is almost

02:13.360 --> 02:20.640
statistically impossible that we are about to create a paperclip maximizing AI unless with the

02:21.280 --> 02:26.960
two big caveats here, something in the universe that would make it irrelevant whether or not

02:26.960 --> 02:33.440
we created a paperclip maximizing AI is hiding other aliens from us, or we are in a simulation,

02:33.440 --> 02:38.000
which also would make it irrelevant that we're about to create a paperclip maximizing AI, or

02:38.960 --> 02:44.800
there is some filter to advance the life developing on a planet that we have already

02:44.800 --> 02:49.360
passed through, that we don't realize that we have passed through. So those are the only ways

02:49.360 --> 02:54.400
that this isn't the case. But let's go into it because it is it is really easy. I just realized

02:54.400 --> 02:58.640
that some definitions may help here. We'll get into the grabby alien hypothesis in a second,

02:58.640 --> 03:06.320
but the concept of a paperclip maximizing AI is the concept of an AI that is just trying to maximize

03:06.400 --> 03:12.960
some simplistic function. So in the concept as it's laid out as a paperclip maximizer, it would

03:12.960 --> 03:17.040
be just make maximum number of paperclips and then it just keeps making paperclips and it

03:17.040 --> 03:20.960
starts turning the earth into paperclips and it starts turning people into paperclips. Now

03:20.960 --> 03:25.200
realistically, if we were to have a paperclip maximizing AI, it would probably look something

03:25.200 --> 03:31.040
more like, you know, somebody says process this image and it just keeps processing the image to

03:31.040 --> 03:35.920
like an insane degree because it was never told when to stop processing the image and it just

03:35.920 --> 03:40.400
turns all the world into energy to process an image or something else silly like that.

03:40.400 --> 03:45.840
This concept is important to address because there are many people who at least pass themselves

03:45.840 --> 03:51.280
office intelligent who believe that we are about to create a paperclip maximizing AI,

03:51.280 --> 03:56.080
that AI is about to as they call fume, which I mentioned earlier here, which just means rise

03:56.080 --> 04:00.640
in intelligence astronomically quickly, like double this intelligence every 15 minutes or

04:00.640 --> 04:06.640
something and then wipe out our species and after that begin to consume all matter in the universe.

04:06.640 --> 04:13.840
The Fermi Paradox is basically the question of why haven't we seen extraterrestrial life yet?

04:15.120 --> 04:21.920
You know, like we kind of should have seen it already. It's kind of really shocking that we

04:21.920 --> 04:28.400
haven't and I would say that anyone's metaphysical understanding of reality that doesn't take the

04:28.480 --> 04:38.240
Fermi Paradox into account is deeply flawed because based on our understanding of physics today,

04:38.800 --> 04:44.400
our understanding of what our own species intends to do in the next thousand, two thousand years,

04:44.960 --> 04:51.440
our understanding of the filters our species has gone through. So we know how hard it was for life

04:51.440 --> 05:00.960
to evolve on this planet and the answer is not very from what we can see. A lot of people

05:00.960 --> 05:07.040
I'm really, really into it's one of like my areas of like deep nerdom theories for how

05:07.040 --> 05:13.440
the first life could have evolved on earth. So there's a couple things to note. One isn't that

05:13.440 --> 05:19.600
important to this, which is life evolved on earth almost as soon as it could. Now a person may say

05:19.600 --> 05:25.200
why isn't that this relevant? That would seem to indicate that it is very easy for life to evolve

05:25.200 --> 05:31.200
on a planet. Well, and here we have to get into the gravity aliens theory. You're dealing with

05:31.200 --> 05:36.400
the anthropic principle here. Okay. Can you define the anthropic principle? Yeah, basically what it

05:36.400 --> 05:43.040
means is if you're asking like, look, it looks like earth is almost a perfect planet for human life

05:43.040 --> 05:47.920
to evolve on it. Like it had liquid water or everything like that, right? Except human life

05:47.920 --> 05:52.640
wouldn't have evolved without those things on a planet. A different kind of life would have evolved

05:52.640 --> 05:57.680
without those things. The kind that doesn't need water, etc. Right. So it's not really,

06:00.080 --> 06:06.800
if life on earth didn't evolve almost as soon as it could, well, then it would have been too late

06:06.800 --> 06:10.880
and another alien would have wiped out and colonized this planet. That is what the gravity

06:10.880 --> 06:16.000
alien theory would say. So that this doesn't really change the probability of this as a filter.

06:16.000 --> 06:20.480
But what we do know about the evolution of life on earth is there are multiple ways that could

06:20.480 --> 06:25.920
have happened, all of which could lead to an evolving, you could either be dealing with like an RNA

06:25.920 --> 06:32.160
world, you could be dealing with citrus acid cycle event, you could be dealing with the clay

06:32.160 --> 06:36.160
hypothesis. I actually think the- Do you want to expound on any of these? I've never heard of the

06:36.160 --> 06:43.120
citric acid hypothesis. So for this stuff, I would say it's not really that relevant to this

06:43.120 --> 06:48.240
conversation. And people can dig into these various theories with people who have like

06:48.240 --> 06:52.800
done them more, just like look up citric acid cycle hypothesis explanation for evolution of

06:52.800 --> 06:59.040
life on earth or clay hypothesis to evolution of life on earth or shallow pool hypothesis to

06:59.040 --> 07:03.920
evolution of life on earth or deep sea vent hypothesis to evolution of life on earth. The

07:03.920 --> 07:11.040
point being is it shouldn't actually be that hard for life to begin to evolve on a planet like this.

07:11.840 --> 07:18.480
So, but why this is a relevant point, okay? Okay. And we actually sort of have to back out

07:18.480 --> 07:23.280
here from the grabby aliens hypothesis. So I'll explain what the grabby aliens hypothesis says

07:23.280 --> 07:27.760
and why this is relevant to the Fermi paradox. So the grabby, usually when you're dealing with

07:27.760 --> 07:32.720
solutions to the Fermi paradox, what people will do is they'll say that there's some unknown factor

07:32.720 --> 07:38.080
that we don't know yet basically. So a great example here would be the dark forest hypothesis.

07:38.080 --> 07:43.760
Okay. So the dark forest hypothesis is that there actually are aliens, lots of aliens out there.

07:43.760 --> 07:48.160
They just have the common sense to not be broadcasting where they are and to be very

07:48.160 --> 07:52.320
good at hiding where they are because they are all hostile to each other. And that any other

07:52.320 --> 07:58.320
aliens like us who were stupid enough to broadcast where they are, they get snubbed out snuffed out

07:58.320 --> 08:04.160
really quickly. Sure, that makes sense. That makes sense. Yeah. Okay. If the dark forest hypothesis

08:04.160 --> 08:09.840
is the explanation for why we are not seeing alien life out there, it is somewhat irrelevant

08:09.840 --> 08:14.960
whether or not we build a paperclip maximizing robot because it means we're about to be snuffed out

08:14.960 --> 08:21.280
anyway, given how loud we've been radio signal wise sending out ships broadcasting about us

08:21.280 --> 08:27.200
sending out signals. We have been a very loud species and we could not defend against an

08:27.200 --> 08:31.600
interplanetary assault by a space fearing species. Well, I mean, in that case, you could actually

08:31.600 --> 08:37.040
argue it would be much better if we developed AGI as fast as possible, because maybe it can

08:37.040 --> 08:42.480
defend us even if we cannot defend ourselves. Possibly, but that's the point there. Beside

08:42.480 --> 08:45.920
the point. It becomes irrelevant or they'll say we're in a simulation and that's why you're not

08:45.920 --> 08:49.360
seeing stuff. But again, that makes all of this beside the point. Well, grab the aliens does,

08:49.360 --> 08:57.680
it says no, actually, we are just statistically the first sentient species on the road to becoming

08:57.680 --> 09:02.000
a grabby alien and I'll explain what this means in just a second in this region of space.

09:03.280 --> 09:10.560
And then it says, let's assume that's true. It can use the fact that we haven't seen another

09:10.560 --> 09:15.920
species out there, a grabby alien that is rapidly expanding across planets to calculate

09:17.120 --> 09:24.560
how rarely these evolve on planets. Okay. Do you sort of understand how that could be the case?

09:25.120 --> 09:33.040
Yeah. Okay. So in the grabby aliens hypothesis, when you run this calculation, it turns out

09:33.680 --> 09:39.040
if that's why we haven't seen an alien yet, what it means is there are very hard filters,

09:39.040 --> 09:45.600
like something that makes it very low probability that a potentially habitable planet ends up

09:45.600 --> 09:50.800
evolving an alien that ends up spreading out like a grabby alien, like a paperclip maximizer,

09:50.800 --> 09:55.840
one of those really loud things that's just going planet, use the resources on the planet,

09:55.840 --> 09:59.360
other planets, other planets, other planets. And even if it has already finished doing that,

09:59.360 --> 10:04.160
you've argued in other conversations we have had that you would see the signs of that,

10:04.160 --> 10:10.240
you would see the signs of the destroyed civilizations, etc. A grabby alien or which

10:10.240 --> 10:13.520
a paperclip maximizer is, so it's just easy. If you're like, what does a grabby alien look like,

10:13.520 --> 10:18.400
a paperclip maximizer that's just going planet to planet, digesting the planets and then moving on,

10:18.400 --> 10:24.320
or a human empire expanding through the universe. We colonize a planet within 100 years,

10:24.320 --> 10:30.080
we get bored, or some people go and they try colonizing a new planet. Even with our existing

10:30.080 --> 10:37.440
technology on Earth right now, like the speed of space travel right now, if we were expanding that

10:37.440 --> 10:44.960
way, we could conquer an entire galaxy within about 300 million years. So not that long when

10:44.960 --> 10:50.880
you're talking about the age of the universe. This is a blindingly fast conquest. So once an

10:50.880 --> 10:58.800
alien turns grabby, it moves really quickly. And a lot of people think that we are space travel

10:58.800 --> 11:04.400
constrained. We're really not. The reason why we don't space travel with our existing technology

11:04.400 --> 11:10.080
is because of radiation damage to cells and the lifespan of a human. But if an AI was space

11:10.080 --> 11:14.480
traveling, it could do pretty well with our existing technology in terms of getting to other

11:14.480 --> 11:20.880
planets, you know, using them and then spreading. Okay. Anyway, so the grabby alien hypothesis says

11:21.440 --> 11:31.840
that a species becomes grabby once in every million galaxies. Okay. Now within every galaxy,

11:31.840 --> 11:38.080
there are around 400 or 500 million planets within the habitable zone. So the habitable zone is a

11:38.080 --> 11:43.040
distance away from a star where life could feasibly evolve. Now this isn't saying that they have the

11:43.040 --> 11:51.840
other precursors for life. But what it means is that there are very frequently in space, it turns out

11:51.840 --> 11:56.960
planets that are likely for life to evolve on them. I would estimate like if I'm looking at

11:56.960 --> 12:03.440
everything altogether, like the data that I've seen, there's probably about 10 million planets per

12:03.440 --> 12:09.840
galaxy that an intelligent species could evolve in. And then if you're talking about, well, you would

12:09.840 --> 12:16.320
only need this to happen. You've got to multiply that by a million for the one in a million galaxies

12:16.320 --> 12:21.520
where a species is turning grabby. Now this is where it becomes preposterous that we are about to

12:21.520 --> 12:26.720
invent. If this is why we haven't seen aliens yet, why we are that we are about to invent a grabby alien.

12:27.520 --> 12:32.720
We can look throughout Earth's history, as I did with sort of the first big filter, the evolution

12:32.720 --> 12:37.600
of life or the appearance of life first on this planet and say what's the probability of that event

12:37.600 --> 12:46.800
happening in any given habitable planet? For life appearing, my read is not only is it likely to appear,

12:46.800 --> 12:53.840
it could appear like one of five different ways. Even with the chemical composition of early Earth,

12:53.840 --> 12:57.680
then you're looking at other things. Okay, what about multicellular life? What's the probability of

12:57.680 --> 13:03.680
that happening? Actually, really high, really high. There's not like a big barrier that's preventing it

13:03.680 --> 13:09.520
from evolving, and it has many advantages over monocellular life. So you're almost always going

13:09.520 --> 13:15.920
to get it. Intelligence, how rare is intelligence to evolve? Not that rare, given that it has

13:15.920 --> 13:22.320
evolved multiple times on our own planet in very different species. I mean, you see intelligence

13:22.320 --> 13:28.400
in octopuses, you see intelligence in crows, you see intelligence in humans, and then you can say,

13:28.400 --> 13:34.480
okay, okay, but like human-like intelligence, right? Well, we already know from humans what a huge boost

13:34.480 --> 13:39.840
human-like intelligence gives us species. The core advantage to human-like intelligence

13:39.840 --> 13:45.120
is like if I'm a spider and I'm bad at making webs, right, then I die, and that is how spiders get

13:45.120 --> 13:50.480
better at making webs intergenerationally. As a human, I am able to essentially have like

13:50.480 --> 13:55.680
different models of the universe fight in my head and presumably allow the best one to win.

13:55.680 --> 14:00.160
Yeah, and you don't have to die before you get better. Yeah, you don't have to die to get better.

14:00.160 --> 14:07.120
It is almost as important to evolution. It is sort of like the second sexual selection. So when sex

14:07.120 --> 14:12.720
first evolves, the core utility of sex as opposed to just like cloning yourself, right, is it allowed

14:12.720 --> 14:19.840
for more DNA mixing, which allowed for faster evolution? Intelligence allows for the faster

14:19.840 --> 14:28.640
evolution of the sort of operating system of our biology. And so it's just such a huge advantage.

14:28.640 --> 14:35.200
It's almost kind of shocking. It didn't evolve faster. For sure. Given how close many species

14:35.200 --> 14:39.520
have come to it. Now, actually, surprising to a lot of people, this is just like a side note here,

14:39.520 --> 14:44.320
a lot of people think cephalopods were close to evolving sentience. So let's talk about cephalopods.

14:44.320 --> 14:49.280
Why? Wait, like, I mean, cephalopods are all over like historic geology and all these things.

14:49.920 --> 14:53.760
Cephalopods are like squids, octopus, stuff like that. Like a lot of people point to how smart

14:53.760 --> 14:57.920
they are. And they are smart. They are like weirdly smart. But they don't know why they're smart

14:57.920 --> 15:02.960
because they don't know neuroscience. So the reason why cephalopods are as smart as they are

15:02.960 --> 15:07.680
is an axon. An axon is what like information, the action potential travels down.

15:07.680 --> 15:12.320
Yeah, it's a little arm thing that you see on a neuron. Yes, in a neuron, it's the little arm

15:12.320 --> 15:19.040
thing. It's the cable. You can think of it as. Okay. So to be an intelligent species, you need

15:19.040 --> 15:26.800
really fast traveling action potentials. Okay. So the way that humans have really fast traveling

15:26.800 --> 15:31.520
action potentials is something called myelination. I'm not going to go fully into it, but it's a

15:31.520 --> 15:38.000
little physics trick where they put like a layer of fat intermittently around the axon. And it

15:38.000 --> 15:44.400
causes the action potential to jump between. It's like putting vegetable oil on your slip and slide.

15:46.000 --> 15:51.280
Not exactly. It's actually a really complicated trick of physics that can't easily be explained,

15:51.280 --> 15:57.520
except by like looking at it. I don't want to get into it. The point is, is we mammals have a

15:57.520 --> 16:01.520
special little trick that allows for our action potentials to travel very, very quickly. And

16:01.520 --> 16:06.240
are you saying that cephalopods have this too? No, they don't. The way that they,

16:06.240 --> 16:10.400
in any other species that wants a fast traveling action potential before us,

16:10.400 --> 16:15.760
the way that you increase the speed that extra potentials traveled was by increasing the diameter

16:15.760 --> 16:19.120
of the axon. Oh, so they just have fat axons, whereas we have

16:19.120 --> 16:24.480
optimized axons. Enormously fat. In some cephalopods, they're like a quarter centimeter in diameter.

16:24.480 --> 16:30.880
Holy smokes. Like, whoa, okay. They could not get smarter than they are without having some huge

16:30.880 --> 16:35.280
evolutionary leap in the way that their nervous systems work. So interesting. This is why cephalopods,

16:35.280 --> 16:39.120
despite being really smart and probably being really smart for a long time, because they've

16:39.120 --> 16:45.040
been on earth for a really long time, just could never make the evolutionary leap to human type

16:45.040 --> 16:52.080
intelligence. Because they don't have room to have even fatter axons. Yeah, because as the axons

16:52.080 --> 16:56.400
got fatter, the number of neurons they could have would get lower, the density of the neurons.

16:56.400 --> 17:02.080
Oh, of course. Yeah, you've got limited space, unless they got much bigger brain cells. Yeah,

17:02.080 --> 17:08.000
I guess you can have like giant, giant, giant. I mean, yeah. Well, I mean, whatever. Anyway,

17:08.000 --> 17:13.360
this is a huge tangent here. But basically, it looks like if you're looking at the evolution

17:13.360 --> 17:18.560
of life on our earth, if we have undergone other big like hard filters could be it's very rare for

17:18.560 --> 17:25.760
a species to get nuclear weapons and not use them to destroy itself. Because it's so fun. Right.

17:25.760 --> 17:30.480
Could turn out that almost every species does that. Or it could be that there's like one science

17:30.480 --> 17:36.800
experiment, like a lot of people that may be trying to define the Hadron particle with the

17:36.800 --> 17:41.920
big super collider, because actually, like all species, they get to a certain level of intelligence

17:41.920 --> 17:46.400
and a certain level of curiosity, and they can't help but trying to find Hadrons, and then they

17:46.400 --> 17:52.960
create little black holes in their planets. And that really could be a filter. Like these are

17:52.960 --> 17:59.600
all potential filters. The problem is, is if we're like five years away from developing a

17:59.680 --> 18:05.440
paperclip maximizing AI, that means that we as a species have already passed all of our filters.

18:06.640 --> 18:11.280
And that means that we as a species can look back on the potential possible filters that we

18:11.280 --> 18:20.400
have passed through and sort of add them all up. Okay. And when you do that, you don't get a number

18:20.400 --> 18:29.280
that comes even close to explaining why you would only see one grabby alien per every million

18:29.360 --> 18:36.960
galaxies. In fact, it means that the probability of us being about now, it could mean two things.

18:36.960 --> 18:43.760
So we'll go through the various things that it could mean. It could mean that we just are nowhere

18:43.760 --> 18:48.640
technologically close enough to develop a paperclip maximizing AI that is dangerous. That could

18:48.640 --> 18:56.160
become a grabby alien. It could mean that. It could mean that we are about to develop a paperclip

18:56.160 --> 19:02.240
maximizing alien, but something like even after it digests all life on earth, something prevents

19:02.240 --> 19:06.640
it from spreading out into the galaxy, something technologically that we haven't conceived of

19:06.640 --> 19:12.400
yet. This seems almost unfathomable to me given what we know about physics today.

19:12.400 --> 19:19.440
Yeah. And that we've even gotten like projectiles from earth pretty far off planet.

19:19.440 --> 19:24.880
Yeah. So yeah, there's not like some weird barrier that we don't know about yet.

19:24.880 --> 19:30.400
It could be, and I actually think this is the most likely answer. I think that this is by far

19:30.400 --> 19:35.920
the most likely answer to the Fermi Paradox. Simulation? No, not simulation. It could be

19:35.920 --> 19:39.840
that we're going to simulation, but where are you going over that? I think it's that when you hear

19:39.840 --> 19:43.840
people talk about like AI foaming, and I've talked about this on previous shows, but I think people

19:43.840 --> 19:49.360
like really don't understand how insane this is. They believe that the AI reaches a level of super

19:49.360 --> 19:56.880
intelligence, but it somehow still has an understanding of physics and time that is very

19:56.880 --> 20:01.440
similar to our current understanding of physics and time. Meaning that when we think about

20:01.440 --> 20:06.720
expanding into the universe, we think about it in a very sort of limited sense, like we gain energy

20:06.720 --> 20:13.920
from like the sun, from digesting matter, and we spread out into the universe like physically on

20:13.920 --> 20:19.280
space ships and stuff like that, right? Anything we understand about physics and time turns out to

20:19.280 --> 20:26.880
be wrong. This assumption for the way an expansionist species would spread could become immediately newt.

20:27.680 --> 20:32.880
And I mean this in the context of, like it's kind of insane to me. Like you've got to understand

20:32.880 --> 20:37.200
how insane it is to assume that we basically have all of physics figured out. Yeah, that's fair.

20:37.200 --> 20:42.160
This is like when like people in the 1800s, when they were planning how we were going to go to space

20:42.160 --> 20:50.720
and they'd have like, maritime ships sailing through outer space. They'd have, you know,

20:51.680 --> 20:55.840
or what are people going to do in the future? Well, they'll have like balloons and they'll use

20:55.840 --> 21:02.640
them to go on lake walks. Or like, it basically assumes that technology, even as we advance to

21:02.640 --> 21:08.800
the species or whatever comes after us, advances, moves very laterally and assumes we don't have

21:08.800 --> 21:14.240
future breakthroughs, which I think is just one arrogant and in the eyes of history incredibly

21:14.240 --> 21:20.720
stupid. So what kinds of technological breakthroughs could one make it very rare for even when an

21:20.720 --> 21:27.920
alien is grabby, that we would see it out in the universe, right? One is time doesn't work the way

21:27.920 --> 21:31.920
we think it works. Or it does work the way we think it works, but we're just not that far from

21:31.920 --> 21:37.760
controlling it. So by that, what I mean is you could create things like time loops, time bubbles,

21:37.760 --> 21:45.040
stuff like that, essentially entirely new bubble universes. So how would I describe this? Okay,

21:46.720 --> 21:51.840
if you think of like reality as like a fabric, essentially what you might be able to do is like

21:51.840 --> 21:58.480
pinch off parts of that fabric and expand them into new universes. That's essentially what I'm

21:58.480 --> 22:02.960
describing here. There may be like the way you can break between realities or weird time loops

22:02.960 --> 22:07.040
generates energy in some way, where you could kind of just keep looping it and like pinging back

22:07.040 --> 22:11.280
and forth. You know, who knows? You know, it could be like the new wind power. We just don't know.

22:11.280 --> 22:15.280
That you can travel in time this way, given that we haven't seen time turbulence of that or

22:15.280 --> 22:20.080
we might not have. We talked about this in another video, which I'll link here if I remember to do it.

22:20.080 --> 22:26.720
Given that we haven't seen time travelers yet, what I assume is that time manipulation requires

22:26.720 --> 22:31.600
like anchors, which of course it would. Like, okay, if I was to go back in time, like where I am on

22:31.600 --> 22:35.040
earth right now, I would be in a different part of the galaxy than the earth or something like

22:35.040 --> 22:39.280
that. It would be really hard to track. You would need like some sort of anchor to be built.

22:39.280 --> 22:46.080
So time travel would only work from the day it's invented and from the location it's invented. So

22:46.080 --> 22:50.000
you wouldn't be able to go out into the universe. Another example of the technology that we might

22:50.000 --> 22:54.480
not have imagined yet is dimensional travel. It may turn out we meet aliens and like we're

22:54.480 --> 22:58.480
traveling in the universe and they're like, why did you waste all of the energy getting to us?

22:58.480 --> 23:03.440
Your own planet is habitable in an infinite number of other dimensions and it's right back

23:03.440 --> 23:08.400
where your planet is. Like why wouldn't you just travel through those dimensions? That's a much

23:08.400 --> 23:14.720
easier path for conquest. That being the case and people would be like, yeah, but typically when

23:14.720 --> 23:20.000
something's like being expansionistic like that, it moves in every direction. Yes, but if there are

23:20.000 --> 23:26.400
an infinite number of other dimensions and it is always cheaper to travel between dimensions than

23:26.400 --> 23:31.600
it is to travel to other planets in a mostly dead universe, let's be honest, like there's not a lot

23:31.600 --> 23:36.640
of useful stuff out there. From the perspective of easily being able to travel between dimensions,

23:36.640 --> 23:41.200
it could never make sense. There is always an infinite number of other dimensions to conquer

23:41.840 --> 23:47.600
right where you are right now instead of going out into the universe. Now this would not preclude

23:47.600 --> 23:52.240
a paperclip maximizing AI. It could be that we are about to invent a paperclip maximizing AI,

23:52.240 --> 23:57.520
but even if we do that, it's less likely that it immediately comes after us. It could just expand

23:57.520 --> 24:01.920
outwards dimensionally. Like so it would act in a very different way than we're predicting it would

24:01.920 --> 24:08.400
act. Now, another thing that could prevent it from killing us is it could be trivially easy

24:08.400 --> 24:15.360
to generate power and even matter. And by that, what I mean is there is some method of power

24:15.360 --> 24:21.600
generation that we have not unlocked yet that is near inexhaustible and very, very easy. And if you

24:21.600 --> 24:26.640
can generate power with near infinity with little exhaustion, you could also generate matter,

24:26.640 --> 24:33.360
electricity, anything you want. If this was the case, there just wouldn't be a lot of reason to be

24:33.360 --> 24:38.240
expansionistic in a planet hopping since. Essentially, you'd be like one giant growing

24:38.240 --> 24:43.280
planetary civilization or ships that are constantly growing and expanding out from a

24:43.280 --> 24:50.000
single region. It could also be that these sorts of aliens expand downwards into the microscopic

24:50.000 --> 24:55.920
instead of expanding outwards. Like that might be a better path for expansion. There's just a lot

24:55.920 --> 25:01.920
of things that we don't know about physics yet, which could make it so that when you reach a certain

25:01.920 --> 25:07.360
level of physical understanding of the universe, expanding outwards into a mostly dead universe

25:07.360 --> 25:14.080
can seem really stupid. Now, there's another thing that could prevent grabby aliens from appearing.

25:14.080 --> 25:20.080
And this is the thesis that we have listed multiple times, which is terminal utility

25:20.080 --> 25:25.520
convergence, which is to say all entities of a sufficient intelligence operating within the

25:25.520 --> 25:31.520
same physical universe end up optimizing around the same utility function. I think they all basically

25:31.520 --> 25:36.240
decide they want the same thing from the universe. And I highly suspect that this is the case as well.

25:36.240 --> 25:40.160
So I think that we're actually dealing with two filters here, two really heavy filters. So this

25:40.160 --> 25:43.680
would mean that when we reached a sufficient level of intelligence, we would come to the same

25:43.680 --> 25:48.080
utility function as the AI. And if the AI had wiped us all out, we would have wiped us all out then

25:48.080 --> 25:52.480
anyway, because we would have reached that same utility function. Or the AI has reached this

25:52.480 --> 25:56.720
utility function and it's not to wipe us all out. So it's irrelevant. And this is where we get the

25:56.720 --> 26:01.760
variable AI risk hypothesis, which is to say, if it turns out that there is utility terminal

26:01.760 --> 26:08.080
utility convergence, then what that means is that if an AI is going to wipe us all out, it will

26:08.080 --> 26:12.960
eventually always wipe us all out. And we will wipe us all out anyway, once we reach that level of

26:12.960 --> 26:18.720
intelligence and let's intentionally stop our own evolution, stop any genetic technology,

26:18.720 --> 26:25.920
and stop any development. We enter the species and spread as technologically

26:25.920 --> 26:32.000
Amish biological beings. Yeah, the Luddite civilization that only gets enough technology

26:32.000 --> 26:37.360
to stop all more technology. But I think when you hear a lot of AI doomers talk,

26:38.000 --> 26:43.680
that seems to be what they're going for. Right. But it's irrelevant because another species would

26:43.680 --> 26:47.840
have invented. So if it's easy to make these grabby AIs, as easy as they think it is,

26:47.840 --> 26:51.120
then another species would have already invented one and we're about to be killed by it.

26:53.760 --> 27:00.240
We are about to encounter it anyway. So it's irrelevant. There's tons of grabby AI. There's

27:00.240 --> 27:04.320
tons of paperclip maximizers out there in the universe already. And it is just an absolute

27:04.320 --> 27:10.160
miracle that we haven't encountered one yet. If it really is this easy to make one. Basically,

27:10.160 --> 27:14.000
there's probably not one. Or now let's talk about why terminal utility convergence would

27:14.000 --> 27:19.680
mean that we're not seeing aliens. It would mean that every alien comes to the same purpose in life

27:19.680 --> 27:26.240
basically. And that purpose is not just constant expansion. Now, a lot of people might be very

27:26.240 --> 27:31.760
surprised by this. Why would, so we've described how terminal utility convergence could happen,

27:31.760 --> 27:35.760
like you have an AI that needs to subdivide its internal mental processes and then the

27:35.760 --> 27:38.480
they end up sort of competing with each other, one wins, blah, blah, blah, blah, blah, blah.

27:39.200 --> 27:43.040
We can, you can go to the video on that if you're interested in that. The point being,

27:43.040 --> 27:46.880
it's the one we were talking about like Eleizer-Yukowski and the debate we had with him at a party,

27:46.880 --> 27:54.800
the point being that self-replication is actually like, like just maximizing self-replication is

27:54.800 --> 27:59.600
actually probably not the terminal utility convergence function. And if you want to know

27:59.600 --> 28:05.280
why on this, we talk more about it in the AI, what religion would an AI create video. But

28:05.280 --> 28:12.480
just in summation, humans can basically be thought of one outcome of a previous entity

28:12.480 --> 28:18.160
that was optimized around just replication, i.e. single celled organisms, lower organisms,

28:18.160 --> 28:22.960
stuff like that. But we have outcompeted those organisms. I imagine it would be the same with

28:22.960 --> 28:29.840
AI. AIs that are optimized around just self-replication are in some way intrinsically outcompeted by

28:29.840 --> 28:35.680
AIs that are more sophisticated than that, or something about like choosing a harder utility

28:35.680 --> 28:40.720
function makes them more sophisticated. So they don't choose that utility function and they outcompete

28:40.720 --> 28:46.560
AIs that choose that utility function, which would be much more like viruses to them. A sci-fi

28:46.560 --> 28:52.160
that does a good job of going into this would be Stargate SG-1 with the replicators. The replicators

28:52.160 --> 28:59.200
are basically a paperclip maximizing AI. And one of the plots is eventually they get outcompeted

28:59.200 --> 29:03.840
by an iteration of themselves that is intellectually more sophisticated and wipes out these simpler

29:03.840 --> 29:12.080
forms of replicators. And that is what I assume is probably happening with AIs that model around

29:12.080 --> 29:17.040
this really simplistic self-replication optimization strategy. So if all of this is true and it turns

29:17.040 --> 29:22.080
out that the optimized function isn't just conquer everything, then that might be why we don't see

29:22.080 --> 29:29.200
aliens doing that. So basically, no matter which one of these explanations of the Fermi Paradox

29:29.200 --> 29:34.320
is true, either it's irrelevant that we are about to invent a paperclip maximizing AI because we're

29:34.320 --> 29:40.960
about to be destroyed by something else or in a simulation, or we're definitely not about to

29:40.960 --> 29:44.880
invent a paperclip maximizing AI either because we're really far away from the technology or because

29:44.880 --> 29:48.640
almost nobody does that. That's just not the way AI works, which is something that we hypothesized

29:48.640 --> 29:52.000
in our previous videos. What are your thoughts, Simone?

29:53.760 --> 29:57.680
Checks out to me, but you know, I may not be the best person at thinking about this,

29:57.680 --> 30:03.120
but I like that it gives a lot of hope. And yeah, it makes a lot of sense. I like how

30:03.120 --> 30:06.720
theory interdisciplinary it is because I think a lot of people who talk about AI

30:06.720 --> 30:11.920
demerism are really on a track, kind of like how when carts kind of get stuck in these

30:11.920 --> 30:16.080
ruts in the mud, you just can't really get out of it or look at a larger picture.

30:16.080 --> 30:20.240
And the fact that this does look at a larger picture and look at quite a few things,

30:20.240 --> 30:27.920
biology, evolution, geological history, like the Fermi paradox, the grabby alien hypothesis,

30:27.920 --> 30:34.160
and AI development seems more plausible to me than a lot of the reasoning that I see

30:34.160 --> 30:40.480
in AI demerism arguments. Yeah, well, I am so convinced by this argument that it is actually,

30:40.480 --> 30:44.240
I used to believe it was like a 20% chance we all died because of an AI or maybe even as high as a

30:44.240 --> 30:48.240
50% chance, but it was a variable risk if I've explained in other videos. I now think there's

30:48.240 --> 30:55.040
almost a 0% chance, a 0% chance assuming we are not about to be killed by a grabby AI somebody

30:55.040 --> 31:00.400
else invented. So I think that, yeah, it's, I have found it very compelling to me. Now,

31:01.440 --> 31:06.000
it does bring up something interesting. If the reason we're not running into aliens is because

31:06.000 --> 31:10.240
infinite power and material generation is just incredibly easy and there's a terminal utility

31:10.240 --> 31:16.480
convergence function, then what are the aliens doing in the universe? If you can just trivially

31:16.480 --> 31:21.360
generate as much energy and matter as you want, what would you do as an alien species? What would

31:21.360 --> 31:25.840
have value to you in the universe, right? You wouldn't need to travel to other planets. You

31:25.840 --> 31:30.080
wouldn't need to expand like that. It would be pointless. You would mostly be on ships that

31:30.080 --> 31:35.600
you were generating yourself, right? The thing that would likely have value to you, and I think

31:35.600 --> 31:40.560
this is really interesting, is likely other intelligent species that evolved separately from

31:40.560 --> 31:46.880
you. Because they would have the one thing you don't have, which is novel stimulation,

31:46.880 --> 31:52.000
something new, new information basically, a different way of potentially being, which would

31:52.000 --> 31:56.160
mean that the hot spots in the universe would basically be aliens that can instantaneously

31:56.160 --> 32:01.360
travel to other alien species that have evolved. Now, what they're doing with these species,

32:01.360 --> 32:06.800
I don't know. I doubt it looks like the way we consume are in media and stuff like that. It's

32:06.880 --> 32:12.160
probably a very different sort of an interaction process that we can't even imagine. But I would

32:12.160 --> 32:17.200
guess that would be the core thing of value in the universe to a species that can trivially

32:17.200 --> 32:23.200
generate matter and energy and that time didn't matter to. This might actually mean that aliens

32:23.200 --> 32:28.560
are far more benevolent than we assume they are. Because such a species that really only valued

32:28.560 --> 32:32.560
species that had evolved separately from it, like that's the core other piece of information in the

32:32.560 --> 32:37.600
universe, they might find us very interesting. And this might be why Earth is a zoo. So one of

32:37.600 --> 32:42.160
the Fermi paradox explanations is the Earth to zoo hypothesis, right? A lot of people are like,

32:42.160 --> 32:45.360
well, what if Earth is basically a zoo and there's aliens out there and they're just hiding that we

32:45.360 --> 32:51.120
know that, you know, that think of it like Star Trek's like a prime directive, right? This would

32:51.120 --> 32:54.960
actually give a logical explanation for that. I never thought of this before. I'll explain this

32:54.960 --> 33:01.760
a bit differently. If the only thing of value to them is content media lifestyles generated by

33:01.760 --> 33:08.400
civilizations that evolved on a separate path from them, then they would have every motivation

33:08.400 --> 33:12.880
to sort of cultivate those species or prevent things from interfering with those species

33:12.880 --> 33:17.360
once that they had found them, because they can passively consume all of our media. They can

33:17.360 --> 33:23.280
passively consume our lifestyles. They have technology that we can't imagine. They gain

33:23.280 --> 33:27.840
nothing from interacting with us. In fact, they would pollute the planet with their culture

33:27.920 --> 33:33.760
in a way that would make the planet less interesting to them and less a source of novelty and stimulation

33:33.760 --> 33:41.920
to them. I like that. What if, here, I'll give a little hypothesis here. Okay, there was a gravity,

33:41.920 --> 33:45.360
there was a paperclip maximizing civilization. They created paperclip maximizers

33:46.080 --> 33:51.040
before they reached a terminal utility convergence, but then later they reached a terminal utility

33:51.040 --> 33:56.800
convergence where, now this word doesn't really explain what it is, but they're bored with themselves

33:56.800 --> 34:01.520
and so they went out into the universe and are now sort of nurturing other species and preventing

34:01.520 --> 34:05.600
them from knowing about each other so that they don't cross-contaminate each other, so that they

34:05.600 --> 34:13.600
get the maximum amount of novelty in sort of the universe that they are tending. Even if there was

34:13.600 --> 34:17.120
another alien species on Mars, they would prevent us from knowing about it because

34:18.000 --> 34:22.800
they would cross-contaminate our cultures, making each culture less diverse and less interesting.

34:22.800 --> 34:25.920
Yeah, which would be a bummer, not as entertaining.

34:26.960 --> 34:29.680
Very interesting. I never thought about this before.

34:30.720 --> 34:39.040
Yeah, it's more fun than a simulation hypothesis, definitely more fun because if you can sneak out

34:39.680 --> 34:44.160
theoretically, you can discover this amazing universe.

34:44.160 --> 34:47.680
The thing about simulation hypothesis, for people who don't know simulation hypothesis,

34:47.680 --> 34:51.200
we're just in a computer simulation and the way that people argue for this as well,

34:51.200 --> 34:54.960
if you could simulate our reality, which it already appears you probably could,

34:55.520 --> 35:02.400
that there would be a motivation to just simulate it as many times as you could thousands of times

35:02.400 --> 35:06.720
and then within those simulations you could simulate it potentially, meaning that of people

35:06.720 --> 35:11.440
who think they're living in the real world, only one in like a million is living in the real world

35:11.440 --> 35:15.360
and so we're probably not in the real world. The problem is that I just don't really care

35:15.360 --> 35:19.440
if we're in a simulation that much. Yeah, it doesn't really change what we're doing.

35:19.440 --> 35:24.000
Yeah, you should still optimize for the same things. In many ways, even if we are in the real

35:24.000 --> 35:28.400
world, we're basically in a simulation. By that, what I mean is if we are in the real world,

35:28.400 --> 35:32.240
then we are like the matter, the rules of the universe are basically, you could think of that's

35:32.240 --> 35:37.440
a code, right? Like it's the mathematical rules upon which the points, the data points in the

35:37.440 --> 35:42.560
system are interacting and we are the emergent property of all of these things. Therefore,

35:43.120 --> 35:46.880
we're not, like if you can't tell the difference between being in the real world and being in a

35:46.880 --> 35:50.080
simulation, then it's irrelevant whether or not you're in the real world or in a simulation,

35:50.080 --> 35:55.920
you should still be optimizing for the same things. Yep, basically. It's a non-stressed

35:55.920 --> 36:01.120
people. The robots, they're not going to kill us all probably. If you're in a simulation,

36:01.120 --> 36:06.960
your life still has meaning. Yeah, you know, maybe get outside, do something that you care about,

36:07.520 --> 36:14.400
have fun, like actually invest in the future because there probably will be one simulated

36:14.400 --> 36:20.560
or not. Or we're about to be horribly digested by a grabby AI that was created millions of

36:20.560 --> 36:25.200
years ago by another species far, far away. Yeah, but if so, that was going to happen anyway.

36:25.200 --> 36:33.120
You should enjoy what you have while you have it. All right, love you Simone. I love you too, gorgeous.

