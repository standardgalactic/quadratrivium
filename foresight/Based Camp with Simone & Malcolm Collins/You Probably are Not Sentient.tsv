start	end	text
0	2160	Would you like to know more?
2160	3840	Hello, Malcolm.
3840	6320	Hello, Simone.
6320	8320	I love your response.
8320	13320	I love that it is your signature greeting with people,
13320	14920	very high energy.
14920	19240	But I also think it is an element of your social autopilot.
19240	21160	Not that I don't have a social autopilot.
21160	23760	I'm on that right now.
23760	27240	But I think that's a really interesting part of human
27240	30320	existence, because for the vast majority of our lives,
30320	34680	I don't think we're actually, let alone not sapient,
34680	38160	not even really conscious, not even really aware
38160	38880	of what's going on.
38880	39160	Oh, yeah.
39160	41320	And I think it's so arrogant when people pretend
41320	42960	that they are aware of most of their lives.
42960	45000	We talk about something called road hypnosis,
45000	47240	where they look back on a drive and they're like,
47240	50080	I don't remember what I was doing during the drive.
50080	52360	Their brain just shuts off recording.
52360	57120	And the question is, how much of our life is road hypnosis?
57120	59320	And I think it's a huge portion of our life.
59320	63000	And it's something, this is what initially got us talking
63000	65560	about consciousness early in our relationship,
65560	70400	was how do we at least enter moments of lucidity
70400	74400	where we are aware of what's going on,
74400	76920	somewhat sentient, just long enough
76920	80400	to be able to change things about the internal self
80400	83200	model that does run our autopilot.
83200	85600	So that at least in the majority of the life
85600	90320	when we are on autopilot, we are better serving our values,
90320	93880	better people, more productive, more
93880	96280	emotionally in control, et cetera.
96280	99360	And I think our thought on consciousness
99360	101280	really evolved in interesting directions
101280	104760	from there when we started really
104760	107360	thinking about what consciousness means
107360	109640	and why maybe it exists.
109640	112320	So I think this will be really fun to talk about.
112320	114160	Why don't you talk a bit about what you think
114200	116440	sentience is?
116440	119360	I think sentience, our experience of consciousness,
119360	123720	in other words, is really an emergent property
123720	126080	of a memory compression system.
126080	128960	So imagine you have a building security system
128960	130960	with tons of different inputs.
130960	134000	It's a feed of doors opening and closing
134000	136760	within the building, a bunch of different camera feeds,
136760	140360	a chemical monitoring system coming in,
140360	143600	everything's feeding into this one control room
143640	146520	and then being put into a camera feed
146520	148640	and then being stored in memory.
148640	152160	And there's a man watching the security feed.
152160	154520	And I think that's our experience of consciousness
154520	160120	is that our minds are synthesizing, smell, sight,
160120	165400	hormonal fluctuations going on, a lot of very complex inputs.
165400	167320	They're synthesizing them into something
167320	169440	that can be compressed into unified memory,
169440	172240	which if relevant will be stored in long-term memory
172240	177440	and then in turn influence automatic instinctual responses.
177440	181360	And because this memory is being codified
181360	185560	and in the moment it's being run through a camera system,
185560	188960	we're getting the impression that there
188960	193920	is some kind of observed conscious driver that
193920	197080	is running consciousness.
197080	198560	I'm going to run this back to you.
198560	200720	It's almost like what you're saying is,
200720	204640	this guy who is sitting at this feed,
204640	208120	he is collecting all of these different camera inputs,
208120	210200	all of these different sensory inputs,
210200	214120	and they are encoded in this single quote unquote
214120	218520	experience, which is being written into the hard drive
218520	220000	of this computer.
220000	225080	And when he is referencing what happened in the past,
225080	227480	when anybody is referencing what happened in the past
227480	229680	within this big security array, they
229720	232920	are referencing this encoding.
232920	236040	And it is because they are referencing this encoding,
236040	238440	it creates the perception falsely
238440	241240	so that the way this encoding works
241240	242960	is the way that these things are being
242960	244560	experienced in the moment.
244560	246800	But it isn't, actually.
246800	250080	Well, Ann, that there is some intentional driver
250080	253880	that's shaping each decision intentionally
253880	256200	through that interface, essentially.
256200	258800	Whereas the interface only actually
258800	262160	affects, in so far, the memory itself influences
262160	263840	automatic reactions.
263840	268760	So I think the research supports this.
268760	270440	We automatically respond to things.
270440	273160	We automatically start taking action in response
273160	276080	to stimulus before we have some kind of conscious
276080	278040	understanding that we're doing that.
278040	282360	And our memories absolutely, yeah, MRI missions
282360	283920	have shown this.
283960	289160	And while our memories will influence those responses,
289160	292760	our current experience of consciousness
292760	294120	is not in the driver's seat.
294120	296080	It is just passively experiencing
296080	297560	this encoding of memories.
297560	299240	It believes this in the driver's seat.
299240	301120	I think that this is what's really interesting,
301120	304840	is it will apply this feeling of consciousness
304840	306640	to any experience that you're doing
306640	308000	or any action that you're taking.
308000	310480	So when you're doing open brain surgery on someone,
310480	313320	you need to keep them awake to prevent accidentally cutting
313320	315640	part of the brain you're not supposed to, so they'll check.
315640	318520	You can do things like apply a small amount of electricity
318520	321120	to a part of the brain and get the person to move their hand.
321120	322960	And then you ask them, why did you move your hand?
322960	324680	And they'll say, oh, I felt like moving my hand.
324680	326480	And you can also see this with split brain patients.
326480	328960	These are patients where the corpus callosum is split
328960	331080	in their head, and their right brain and their left brain
331080	333640	actually function pretty independently of each other
333640	335040	when this happens.
335040	337160	So you can cover one eye and communicate
337160	339320	with part of their brain and not the other part of the brain.
339320	341560	So you can tell part of their brain, pick up a Rubik's Cube
341560	342680	and try to solve it.
342720	344320	Then you put something on the other eye
344320	346360	and you ask, OK, why did you do that?
346360	348480	And they'll say, oh, I always felt like solving a Rubik's Cube.
348480	349760	I always wanted to do this.
349760	351640	And you can do this with more complicated things.
351640	354040	So there's this experiment, really great one,
354040	356520	where they would give people pictures of attractive women
356520	357920	and they say, which is the most attractive?
357920	359920	And then they do a little sleight of hand later and say, OK,
359920	361920	why did you say this one was the most attractive?
361920	362920	But it wasn't the one they chose.
362920	364480	You'd actually replace it with another picture.
364480	366360	I mean, you could do this with political beliefs as well
366360	367680	and all sorts of other things.
367680	372120	And most people will say, oh, I chose this person for x, y,
372120	374840	z reasons and go into detail about why they chose that person.
374840	378500	Even though that wasn't the person they chose, which shows
378500	380720	that a lot of our consciousness, a lot of the way
380720	382400	that we describe our sentence is more
382400	384940	like sense-making of our environment.
384940	388920	We know we made x decision, but X decision was actually
388920	391560	made completely outside of our sentence as control.
391560	393480	And then we have this little lying historian
393480	396520	in our head, which is like, no, I made the decision.
396520	397560	I made the decision.
397560	399320	I made every decision.
399320	401260	But he's also recording the history
401260	406060	you remember. So then he's going through and saying, okay, I made the decision for this. It's not that
406060	409580	he doesn't have any say. See, this is where he does have a say in it, something that you mentioned,
409580	415580	which is that he can encode emotions into the things we're doing. And this can actually cause a
415580	420860	lot of... No, emotion isn't the right word, because emotions do, you know, let's say that...
420860	427100	Emotional narratives. Emotional, yeah. So they can encode positive or negative modifiers and
427100	432300	they can shift the narrative. They can change the camera angle or add sad music to something,
432300	438460	essentially, to make it seem like a sad scene. You've seen the YouTube video of the Mary Poppins
438460	446620	preview, but done with scary music, and it just seems horrifying. So that's how we can change...
446620	451580	Yeah, that is how we can change the narrative. And the first time I was ever introduced to this idea
451580	456620	that we take action before we consciously are aware of it, the person discussing it said that
456620	461420	there's a lot of implications to this because it would lead many people to believe that they don't
461420	465820	have free will and have them just say, oh, none of this is my fault. I didn't consciously make this
465820	472060	decision anyway, where that's really not quite, we would say the right conclusion, because you
472780	480940	do have the ability to color how you perceive reality. It's not in this kind of immediate,
480940	485500	non-asynchronous way that you would expect. I would say that this is just the myth of humanity
485500	490700	versus the actuality of humanity. And we would argue that we likely evolved this ability because
490700	494300	it was like a compression algorithm for communicating ideas to other people. I actually don't suspect
494300	498220	that Great 8 have this sort of internal thing that we call consciousness because they didn't
498220	502700	need to communicate these. It's a really good compression algorithm for linear experiences
502700	510060	over time. But one of the big lies that happens throughout this process is it convinces us that
510060	516060	we are a singular entity, when in fact our brains function much more like we see AI's function with
516060	520700	individual instances running. And we can see this with the corpus callosum split that I mentioned
520700	526940	earlier, where it basically means that we have two largely separate parts of our internal mental
526940	532140	processing that are happening separate from each other. This idea that the decisions you make happen
532140	536060	before they enter your conscious mind, what that basically means is you have another part of your
536060	541260	brain which is making this decision and then delivers it to the conscious mind. When we were
541260	549260	talking about the idea of a security camera with a bunch of different feeds, a lot of the
549260	555260	processing is done locally at these various security cameras before they all get centralized into this
555260	561340	sort of communal feed with many of the quote unquote decisions being made at those local levels.
561340	569500	And so we have this illusion of ourselves as a singular entity, which is created by the way
569500	574620	this sort of sentience processor works. But it is just an illusion. And so when we say, oh,
575340	582140	we don't really have self-control or we're not responsible for our decisions, I think that
582140	589020	actually even overstates the level to which we exist in any sort of a meaningful concept
589020	594940	close to how we think we exist. And so then there's this, I would say, added layer of
596300	602780	complexity or maybe confusion. You shared with me an article saying that a very high
602780	607980	percentage of people don't have an internal monologue. What we would describe as-
607980	611260	Yeah, they don't have an internal monologue. Another high percentage of people can't even
611260	616780	create images in their mind. And so what we're even describing as consciousness is also not
616780	624700	even something that is universal as part of the human experience, which is interesting.
624700	633340	Because I think most of us who experience consciousness as we're describing it would
633340	639740	have a very hard time understanding even what that means. I don't know, maybe someone watching
639740	643820	this YouTube video doesn't have an internal monologue. I wonder-
643900	648860	It's hard for you to model that, but I suspect that the human, the variance within the human
648860	653980	condition in terms of how things are processed is probably a lot larger than we give it credit for,
653980	658220	and it will be even larger in the future. A statistic that I just cannot stop mentioning
658220	662060	because it's something that more people should know, that if you look at the heredity of IQ
662060	666860	right now and you look at the selective pressure, so you look at the number of people who have these
666860	669740	markers versus people who don't have these markers, which you can see because it's genetic
669820	673500	work, as it says you, is this the number of kids they have. We're likely looking at a one standard
673500	678140	deviation shift down in IQ in the next 75 years in developed countries at least. This is probably
678140	682220	going to affect developing countries later. So I guess good for them. There'll be all the geniuses
682220	687180	in the world. We'll be in Africa or whatever, but places where you have this post prosperity,
687180	693420	fertility, collapse situation. And when we think about how quickly and how much human IQ can shift
693420	698860	up or down, we use this one marker IQ, but I suspect it's linked to just all sorts of things
698860	704140	about how we process reality. So actually I wanted to dig in a little bit more on the subject
704140	711820	of kids because I think that also as we've become parents, we've had a more complex understanding
711820	717020	of how consciousness develops because we see it start to emerge in our kids. I think there's
717020	723260	definitely this point at which we see consciousness blossoming and it's not one day our kids aren't
723260	728620	very conscious and the next they are. I think that consciousness for example is starting to emerge more
728620	735020	and more especially in our three-year-old. It's just beginning to emerge in our two-year-old. And
735020	741420	I think a lot of that has to do with where they are with language processing. I think it really
741420	747340	influences. That's why I suspect this had to do, it evolved alongside language to compress ideas,
747340	751100	but I think that this is where you can see how this system can break in a way that can be very
751100	755100	useful in relationships. So this isn't just like theory or whatever. So one of the things you'll
755100	760220	often see one of our kids do is he'll be in a bad mood, but he won't understand the concept of
760220	764860	generally being in a bad mood. So he'll start crying and he'll say, I want this. Give me that
764860	769420	toy and then you get him the toy and he just, it doesn't stop the bad mood. And so he needs
769420	775980	whatever he notices next, close the door or move that chair. Like he just was like whatever
776700	781340	is currently causing the littlest bit of discomfort, he thinks it's the core cause
781900	787740	of like this bad mood or why he's angry or what he's angry about. And as humans, I think this
787740	792620	happens as well. And this is really bad when a friend tells you you're justified to have an angry
792620	796700	state or something like that, because then this little narrative maker in your head says, ah,
796700	800700	now you get to be angry. Now you're socially justified to be angry and you will feel very
800700	805020	angry about something or you might be in a generally bad mood and your partner comes into
805020	810060	the house and he does something that just annoys you in the slightest. And then you create the
810060	816140	internal narrative that you are in this bad mood because of what your partner did. And when you
816140	821100	keep in mind why you're feeling these things and you try to keep like fully in touch with the way
821100	825340	your brain is actually working, it leads to a lot more harmony and a lot fewer fights and
825340	830540	relationships because you have language for I am in a bad overlay state right now,
831260	838780	which just means I'm in a bad mood generally, but I'm not actually mad at you or anything specific.
838860	842380	Hold on though, actually, I think you've touched on something very interesting there,
842380	849020	which is that maybe sometimes consciousness and narrative building hampers more than helps
849020	856140	us. For example, like the toxic girlfriend who has a bad dream in which her boyfriend
856140	863180	cheats on her, she wakes up angry at him, like she's mad at him for something he can actually do,
863180	868860	or maybe one day she's just in a bad mood, but then she makes up some narrative about
868860	872860	it's because her boyfriend didn't bring her flowers and doesn't appreciate her,
872860	878220	some he did something mean. The presence of consciousness and the presence of narrative
878220	884700	building would cause her to turn what might be just a very transient bad mood into something that
885260	889180	builds a grudge over time and literally ends up killing the relationship,
889260	893820	humiliatively. And sometimes consciousness hampers us more than it helps us.
893820	897260	What I love about what you're saying here, and this follows your idea of what it means to be
897260	904460	meaningfully human and the spectrum of humanity, which is you become more human the more you take
904460	910780	mastery and ownership over the sort of evolved or quirks of the way your brain works, and you
910780	917260	don't allow them to control your actions. Your actions are more logically decided and more
917260	923180	decided based on as close to an objective view of reality as you can get. And so from the perspective
923180	926620	of humanity that you convinced me was a good one, because this wasn't the one I had before,
927180	931820	somebody who does that, somebody who has a dream and then can't logically understand that is not
931820	937420	a justified reason to be mad at somebody, that they are like meaningfully less human than another
937420	941660	person. And so then what does it mean to be fully human? It means to have total mastery over these
941660	946860	things. And that is something that we don't have, but I think it helps people understand because
946940	953900	a lot of people hear the level of disdain we talk about things like sentience and love and happiness
953900	958940	and other human emotional states that a lot of people venerate, but they don't understand where
958940	965020	that's coming from. But then wouldn't that make an LM more human than we are? People may not know
965020	970780	what it is. A large language model is more sophisticated than we are. And it's also not
970780	979180	bogged down by the need for hunger, human failings, hormones, all these sorts of pollutants,
979180	986140	not pollutants. They're very instrumentally useful for biological human in a modern globalized
986140	990620	society and often with the type of knowledge work that humans are expected to do. It's pretty
990620	996220	counterproductive. Well, I think that this comes to your goal for yourself, where your goal and
996220	1002460	iteration of yourself that is your idealized iteration would strip out your emotional shortcomings,
1002460	1011260	be they love or happiness or hatred or pain or greed. And I'm not that way, by the way. I am not as
1011260	1015820	bought into this philosophy as Simone is. I would not strip those things away from myself. I think
1015820	1022780	that they add something that I feel illogically. I still think has some value, but I don't know,
1022780	1028460	maybe you feel that way too. I'm mixed on it. I'm mixed on it. One, I'm deeply uncomfortable
1028460	1034140	being human. I really don't like my body. I really don't like being human. I don't like the corruption
1034140	1042140	to our objective functions that human weaknesses cause. But my general stance is if this is what
1042140	1047100	I have to deal with, if I've been given a meat puppet, I'm going to use it to the max. I'm going
1047180	1054140	to play the game. You've given me a crappy little battlebot. I'm going to take that thing and I'm
1054140	1061020	going to destroy everything, even if it's the worst machinery ever. I think the way she talks
1061020	1067100	about pregnancy, she's, I have a uterus. I am going to wreck that thing. I am going to have so many
1067100	1074700	babies. I'm going to shreds. If that's what it was meant to do. Yeah. Then as a woman, I reached
1074700	1080140	the planes of Valhalla by dying in childbirth. Let it happen. Don't worry, Malcolm. I promise.
1080140	1086620	I will play that clip at your funeral. Thank you. I really should probably plan that out.
1086620	1090860	But yeah, I feel conflicted. I mean, I, yes, if this is the hand that we're dealt, I'm going to
1090860	1098140	play it and I'm going to play it hard. But at the same time, yeah, I really aspire to that. I don't
1098140	1103900	think that has to be me. And I guess that's, maybe it's more AI and machines are my Beatrice
1103900	1110460	and Dante's Inferno, this idealized version of humanity that I know I am not and that I do not
1110460	1115500	aspire to be, but that I deeply admire. I don't need to become it. I don't need to be with it. I
1115500	1121980	just, I just see it as a better iteration and as naturally and morally superior. Does that make
1121980	1126700	sense? What you hope is to make our kids superior to that. Oh, for sure. But our kids are still
1126700	1131660	biological. They're still human. So I think I'm playing the field. I'm pretty sure this next
1131660	1134860	generation is going to be the first that integrates with tech. I know you say.
1134860	1139260	Well, our generation is going to integrate with tech. I'm sure that AI models will be trained
1139260	1145020	on if not us family members or our kids or a combined version of us, which would be even cooler.
1145580	1151660	But I still think that for a while we're going to be biologically human and limited by
1152780	1158940	the shortcomings of biological humanity. There's one other element of consciousness that I think
1158940	1163980	you downplay. You used to not downplay it as much and I don't know why this has changed maybe
1163980	1169980	because you're so focused on the role that language plays in consciousness. But I do really
1169980	1179660	think that humanities focus on modeling. The actions of other animals and humans plays a role
1179660	1184620	in our development of kindness because one, there's talk about this model for humanity. It's
1185580	1189820	yeah. It's the model of humanity that we use in The Pragmatist Guide to Life, which is our first
1189820	1192940	book, which is why I don't talk about it because this is an older idea that I had. When you're
1192940	1198300	trying to model other people's behavior, what you do is you have a mental model of them, which is
1198300	1202540	like an emulation that you're running within your own head of the way that you think that they are
1202540	1205900	going to act and the things you think that they are thinking. This is how you're able to have
1205900	1210060	like arguments with little simulations of other people in your head. You have modeled them and
1210140	1215660	you've modeled you and you are arguing with this different entity. When I was a neuroscientist,
1215660	1219500	one of the spaces I focused on was schizophrenia. What I actually think that we are seeing when
1219500	1225740	people hear voices is a lower activation of this. Using TMS, Transmining Stimulation, you
1225740	1229660	would hyperactivate parts of a person's brain and then if you'd hyperactivate the part that's
1229660	1233660	associated with saying letters, you put a letter in front of somebody and they won't be able to
1233660	1238540	help but say it because you have primed them with a vision of that letter and you have lowered the
1238540	1243820	threshold. I think what's happening with schizophrenia is something similar to that. They have their
1243820	1250060	system that they use to apply mental models to other things gets activated too easily. It can
1250060	1255100	be activated by the slightest thing. They look in a store window and they're like, oh, that must
1255100	1259100	have been done with intentionality. There must be some thought process behind the way everything
1259100	1263820	was arranged or they see something innocuous in the environment like a helicopter and then they are
1263820	1267580	like, oh, why is the helicopter there? There must be a person in it. They must be thinking about me.
1267580	1272540	Oh my gosh. Or they begin to hear whispers. This is why whisper hearing is associated with
1272540	1275740	schizophrenia. Auditory hallucinations are much more common than visual hallucinations.
1275740	1281020	Visual hallucinations are incredibly rare. But anyway, so that's what's happening with schizophrenia.
1282060	1286780	The question is, okay, what does this have to do with the regular person? What it has to do
1286780	1293180	as a regular person is that I think people have a sort of internal mental model of themselves
1293180	1299340	which is used to prime emotional reactions to things. So when the way we talked about this
1299340	1304460	little like sentience box in your head, what it's doing when it's judging whether or not you should
1304460	1309820	react emotionally to something and how you should react emotionally to something is it is testing
1309820	1314460	what's happening in this sort of simulation saying that's what we would call our sentience
1314460	1319500	against this little mental model that's running of the way it thinks you should be feeling.
1319500	1322300	And you're saying, oh, does this mean he should be feeling anger? Oh, does this mean he should be
1322300	1326540	feeling happiness? And then it outputs that emotional state by telling you that you should be
1326540	1331020	feeling this. The way you can see this is that if somebody justifies a particular emotion, like
1331020	1336300	you should be really angry about that. Often a person will become much angry and they'll begin
1336300	1340700	to spin away or how could you let your boyfriend do that to you? And then you're like, ah, this
1340700	1346060	mental model has been adapted to feel angrier and you will actually experience much more of this
1346060	1353820	emotion. But what were you talking about if not that? In general, the role that modeling things
1353820	1359900	played in developing human consciousness that maybe what happened is one humans have
1360700	1365500	an evolutionary advantage if they are able to model predators and prey, because then they can
1365500	1373580	anticipate the moves of these organisms before they make them. And that to that ability would
1373580	1379980	start to, just like with Schizophrenics, get misapplied to that compression algorithm of memory
1379980	1387900	that's being formed. It's a mixture of language and so language and narrative building plus our
1387900	1393900	modeling things that were literally anthropomorphizing ourselves, if that makes sense. That's a good
1393900	1398620	way to put it. And I think people see, first of all, people with Schizophrenia, not Schizophrenics,
1398620	1405020	they are not defined by their theory. But like people with Frenchness. But we see this in how
1405020	1410300	easy it is that we answer anthropomorphize things. So I think it's very hard to not answer
1410300	1415260	anthropomorphize like a dog, right? Like you see a dog, you can see it's happiness, you can see it's
1415260	1420300	worried about things, you can see it and you perceive it as experiencing these emotions to see
1420300	1424460	what a human does, even though it probably doesn't. And you could see this in when people kick those
1424460	1431020	robots. Oh my gosh, yes. I see somebody kick with these robots and I'm like, I feel so bad for the
1431020	1436060	robot. I'm like, how do you do this to this portal? I know, logically, the robot's not experiencing
1436060	1440620	all that. Now, when you're a human and you're anthropomorphizing yourself, and you have no
1440620	1445660	way of knowing that you're not feeling these things in a real context, we struggle to not
1445660	1452060	answer anthropomorphize robots. How do we know that the robot's not suffering? How do we know if
1452060	1458060	its objective function is to run and kick the ball into the net, that it's not experiencing some kind
1458060	1464060	of suffering? Have you put ugly eyes on a soccer ball? People will feel bad for it. Simone, I'm
1464060	1469660	just trying to think of the things that people like definitely can empathize with when I'm talking
1469660	1473660	about this anthropomorphizing of things that most people don't think we should be anthropomorphizing,
1473660	1478060	with saying that if you didn't know whether or not it could feel emotions and everyone around you
1478060	1482860	said it could feel emotions, you would 100% believe that robot was feeling emotions,
1482860	1486540	as soon as you saw it kick because you'd feel so bad when it gets back up and it tries to walk again.
1486540	1491020	And as humans, it's the same way. If you didn't know, if you didn't have hard proof because you
1491020	1494940	hadn't gone through all the studies like I have and you didn't know that humans probably don't have
1495660	1499900	full control of this sort of sentient aspect of themselves and is likely irrelevant, you would
1499900	1504060	totally answer for more of our humans. And so I love this way of doing things, Simone. A very
1504060	1511660	interesting thought on your part. There is a subreddit. I don't know if it still exists.
1511660	1520460	It's nsfw, where people put ugly eyes on butts. Do you think that people are anther butts?
1521100	1530700	You know, butts. Are they are they anthropomorphizing the butts? Is that part of what's fun about that?
1530700	1536140	You and I loved, no, it's more me. I try to figure out like what is making people tick behind
1536140	1543740	weird nsfw subreddits, but I'm wondering, is that one is an outstanding one? Subscribe if that's
1543740	1547900	what you're interested in, is deep dives on why people are engaged. Because that's what,
1547900	1551980	the pragmatist guy to sexuality was totally like a meditation on this. Why are humans like turned
1551980	1555660	on? Because obviously we're very interested in the way that like the human mind actually processes
1555660	1559180	things. I left science. Why didn't I leave science because I didn't feel like real researchers being
1559180	1563900	done anymore? And I felt like there were specific narratives and it was like toe the line or else.
1563900	1567580	And I'm glad that we have reached a level of financial security where we are able to talk
1567580	1570620	about these things and research these things because we actually do a lot of independent
1570620	1574780	research, which if you're wondering how we get to these ideas and the data that leads us to get to
1574780	1578780	the ideas, go to our books and that's where we discuss it all. But yeah, I mean, it's really
1578780	1582860	fun and there are just so many low hanging fruits because academia is not doing anything anymore
1582860	1586140	or not doing the same level of work. I think it should be in these areas.
1586140	1593740	So there's one more thing that I think consciousness some credit for and sapience in
1593740	1598220	general, because I think that an easy conclusion to make from our theories around consciousness,
1598220	1605660	especially I see it as an illusion, is to say the colonists don't value consciousness. They think
1605660	1611580	it's an illusion, therefore it doesn't matter. To the contrary, I think it could easily be argued
1611580	1616540	that sapience is one of the things that we think is most valuable, most interesting. It's what
1616540	1622700	distinguishes humans from other organisms, but it's what makes us. But more important,
1622700	1628940	more importantly than that, it is this narrative building, whether or not it's illusory or not,
1629740	1634460	it is what enables us to edit our objective functions. That is the one differentiating
1634460	1640460	factor. Any non-conscious entity, any entity that doesn't have this narrative building effect,
1640460	1648140	this weird recording and encoding system and modeling system cannot question its actions.
1648140	1652540	It cannot look at the compression of all the inputs and the narrative that is being woven
1652540	1661100	in say, should we change the narrative? And I think that I've seen critiques of consciousness
1661820	1666940	where people totally miss that, where they say consciousness can get in the way of things,
1667740	1671900	not necessarily, it was evolved because it worked, not because it's superior.
1671900	1677020	And I think they're missing the core point here that consciousness has enabled humanity to pivot
1677020	1682860	in ways that no species on earth has ever done. It's what allowed us to make the leap. I completely
1682860	1687180	agree with you. And there was a final point I wanted to close out with here, that there was this fun
1687180	1693340	video clip of, we were talking on Piers Morgan, and you were talking, and you can see me moving
1693340	1697900	my mouth to your words as you're talking. And people might wonder why I'm doing this. And this
1697900	1702220	actually relates to something we were talking about in the video. So we are both on opposite
1702220	1706780	sense of the spectrum. If my model schizophrenia is correct, you basically have an autism to
1706780	1712460	schizophrenia spectrum, which is how much do you innately mentally model others? With people who
1712460	1717020	are autistic or have Asperger's, not innately running mental models of other people whenever
1717020	1720460	they're interacting with people. And people who are on the schizophrenia side of the spectrum,
1720460	1723900	not being able to help running mental models, even when there's no humans around.
1723900	1728860	And when we say Simone is diagnosed autism, so definitely on the autism side of the spectrum,
1728860	1733660	then I am almost certainly, when I look at myself on the schizophrenia side of the spectrum,
1733660	1740220	which is I don't hear voices or anything like that, but I really struggle to not mentally
1740220	1745740	model people I'm engaged with to the extent that I basically almost pass out after social
1745740	1750940	situations. I find them so exhausting. I met a big party. It's like just constantly modeling
1750940	1755420	everyone. And that's what was happening on that podcast. I was in a heightened emotional state
1755420	1759340	where I really cared about what she said. So I was running through the words in my head as she
1759340	1764700	was saying them and trying to process how she would respond to something. And I couldn't help,
1764700	1768940	but move my mouse because it was that sub level of stimulation. Like I talk about people can't
1768940	1774380	help but say the letter when that part of their brain is TMS. And that's what was happening there.
1775260	1780140	But there are reasons why we have in the human genetic code autism and schizophrenia,
1780140	1784060	why it hasn't been evolved out of us. And it's because both of these extremes are useful.
1784060	1790060	Autism can make you able to act more logically about the world around you, not being encumbered
1790060	1794860	by constantly mentally model others. And then my ability, people often will say it's like
1794860	1798940	eerie how much I can tell what other people are thinking, like to the level where
1798940	1802300	it can feel to some people like I can read their mind in a conversation.
1802300	1807100	And I think that is why you have these people on these schizophrenia side of the spectrum.
1807100	1811580	And then sometimes they just get a little too much of these genes, and it leads them to
1811580	1817100	hear voices constantly, instead of just having a really hyperactive ability to mentally model
1817100	1824620	anyone around them. Yeah, no, 100% Malcolm is on overdrive. And then he'll sometimes be
1824620	1828540	thinking about conversations with other people while we're walking. And I can always tell,
1828540	1833340	because he gets so deep into them that he's literally like gesturing as that's like we're
1833340	1837980	driving in the car, like one hand is on the steering wheel, the other hand is like gesturing
1837980	1842620	his silent conversation he's having with someone he anticipates speaking within the future,
1842620	1849660	or reliving a conversation he had in the past. And he will have these aftershocks from when
1849660	1857820	we socialize, where he feels the stress or pain of saying something not quite right to someone.
1857900	1864460	And it hits him like a ton of bricks, and he will like visibly like crumble and cringe.
1865740	1869020	It's not just cringe. Yes, like somebody just kicked me in the nuts or something.
1869020	1875100	Yeah, like, it looks like he has been physically hurt by something. And that is not something
1875100	1882780	that I can even begin to imagine. And I do think that it's a lot less stressful to be on the
1882780	1891260	autistic end of the spectrum and to just not know that other people like nothing going on there.
1892220	1896220	Such a good partnership. And I think it was one of our main goals throughout our books and throughout
1896220	1900380	our lives to understand how humans think and process things and what's really happening in
1900380	1904700	the human brain. I started my career as a neuroscientist and a philosopher. That was my
1904700	1909660	interest. It's like, what's really going on? And being able to be in a relationship with somebody
1909740	1914060	who sees the world so differently has given me such insights that I would never come to on
1914060	1919100	my own. And I just admire that so much about you, Simone. And I admire that you have taken me to
1919100	1925340	where I am, which is somewhere I never could have reached without your guidance. And I love you so
1925340	1931340	much. I love you so much too. You're the superhero that I always wish existed. And I still worry
1931340	1935500	that I'm going to wake up from a coma at some point and find out that you're the sidekick that
1935500	1940380	actually does everything. I might be the superhero. She's the hacker nerd in the background that like
1940380	1944780	actually makes everything work. And no, if the hacker nerd went away, the superhero would have
1944780	1951020	nothing. That is so our relationship. I have nothing without you actually doing all the
1951020	1957180	detective work and telling me where to go next. It's a massively inflated estimation of my
1957180	1964300	contribution. I just follow her instructions. I don't manage my calendar at all. I'm operating
1964380	1969580	on Simone is driving me like she says, what was the one thing like the thing from aliens?
1970700	1977020	Oh, yeah, like I'm not talking to people. I'm the alien suit that you're using to punch through
1977020	1985740	reality. Power loader. You're the you're Ripley. Oh, okay. That's the other way we both feel about
1985740	1991660	each other. I adore you. I love these conversations. And I know we have to pick up the kids now. But
1992460	1996860	I think you're going to make another dish tonight. So I'm going to have fun. Oh,
1996860	2001900	yes, another base camp cooking. We have a little side playlist of anyone seen it where I try to
2001900	2006540	come up with new dishes. So you get it right. You get to see the college household at night.
2006540	2014220	What happens after that? Yes. All right, see you soon. Love you.
