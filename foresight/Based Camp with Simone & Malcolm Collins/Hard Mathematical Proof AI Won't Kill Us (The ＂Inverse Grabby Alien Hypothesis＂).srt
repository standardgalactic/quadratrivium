1
00:00:00,000 --> 00:00:05,280
So basically, no matter which one of these explanations of the Fermi paradox is true,

2
00:00:05,280 --> 00:00:09,680
either it's irrelevant that we are about to invent a paperclip maximizing AI because we're

3
00:00:09,680 --> 00:00:16,320
about to be destroyed by something else or in a simulation, or we're definitely not about to

4
00:00:16,320 --> 00:00:20,000
invent a paperclip maximizing AI either because we're really far away from the technology or

5
00:00:20,000 --> 00:00:24,880
because almost nobody does that. That's just not the way AI works. I am so convinced by this argument

6
00:00:24,880 --> 00:00:28,800
that it is actually, I used to believe it was like a 20% chance we all died because of an AI or maybe

7
00:00:28,800 --> 00:00:32,640
even as high as a 50% chance, but it was a variable risk if I've explained in other videos.

8
00:00:32,640 --> 00:00:39,200
I now think there's almost a 0% chance. A 0% chance assuming we are not about to be killed by a

9
00:00:39,200 --> 00:00:45,040
grabby AI somebody else invented. Now, it does bring up something interesting. If the reason we're

10
00:00:45,040 --> 00:00:49,760
not running into aliens is because infinite power and material generation is just incredibly easy

11
00:00:49,760 --> 00:00:55,280
and there's a terminal utility convergence function, then what are the aliens doing in the

12
00:00:55,280 --> 00:01:02,320
universe? Would you like to know more? Hi Malcolm, how are you doing my friend? So today we are going

13
00:01:02,320 --> 00:01:08,320
to do an episode, a bit of a preamble for an already filmed interview. So we did two interviews

14
00:01:08,320 --> 00:01:14,800
with Robin Hansen and in one of them we discussed this theory. However, I didn't want to off rail

15
00:01:14,800 --> 00:01:20,000
the interview too much going into this theory, but I really wanted to nerd out on it with him

16
00:01:20,000 --> 00:01:25,760
because he is the person who invented the grabby aliens hypothesis solution to the Fermi Paradox.

17
00:01:27,360 --> 00:01:31,440
I hadn't heard about grabby aliens before, so I'm glad we're doing this. This is great.

18
00:01:32,000 --> 00:01:39,280
Yes, so we will use this episode to talk about the Fermi Paradox, the grabby alien hypothesis,

19
00:01:39,280 --> 00:01:46,240
and how the grabby alien hypothesis can be used through controlling one of the variables,

20
00:01:46,240 --> 00:01:53,120
i.e. the assumption that we are about to invent a paperclip maximizer AI that ends up fooming and

21
00:01:53,120 --> 00:01:58,560
killing us all, because that would be a grabby alien definition only. If you collapse that variable

22
00:01:58,560 --> 00:02:06,000
within the equation to today, then you can back calculate the probability of creating a paperclip

23
00:02:06,000 --> 00:02:13,360
maximizing AI. And spoiler alert, the probability is almost zero. It basically means it is almost

24
00:02:13,360 --> 00:02:20,640
statistically impossible that we are about to create a paperclip maximizing AI unless with the

25
00:02:21,280 --> 00:02:26,960
two big caveats here, something in the universe that would make it irrelevant whether or not

26
00:02:26,960 --> 00:02:33,440
we created a paperclip maximizing AI is hiding other aliens from us, or we are in a simulation,

27
00:02:33,440 --> 00:02:38,000
which also would make it irrelevant that we're about to create a paperclip maximizing AI, or

28
00:02:38,960 --> 00:02:44,800
there is some filter to advance the life developing on a planet that we have already

29
00:02:44,800 --> 00:02:49,360
passed through, that we don't realize that we have passed through. So those are the only ways

30
00:02:49,360 --> 00:02:54,400
that this isn't the case. But let's go into it because it is it is really easy. I just realized

31
00:02:54,400 --> 00:02:58,640
that some definitions may help here. We'll get into the grabby alien hypothesis in a second,

32
00:02:58,640 --> 00:03:06,320
but the concept of a paperclip maximizing AI is the concept of an AI that is just trying to maximize

33
00:03:06,400 --> 00:03:12,960
some simplistic function. So in the concept as it's laid out as a paperclip maximizer, it would

34
00:03:12,960 --> 00:03:17,040
be just make maximum number of paperclips and then it just keeps making paperclips and it

35
00:03:17,040 --> 00:03:20,960
starts turning the earth into paperclips and it starts turning people into paperclips. Now

36
00:03:20,960 --> 00:03:25,200
realistically, if we were to have a paperclip maximizing AI, it would probably look something

37
00:03:25,200 --> 00:03:31,040
more like, you know, somebody says process this image and it just keeps processing the image to

38
00:03:31,040 --> 00:03:35,920
like an insane degree because it was never told when to stop processing the image and it just

39
00:03:35,920 --> 00:03:40,400
turns all the world into energy to process an image or something else silly like that.

40
00:03:40,400 --> 00:03:45,840
This concept is important to address because there are many people who at least pass themselves

41
00:03:45,840 --> 00:03:51,280
office intelligent who believe that we are about to create a paperclip maximizing AI,

42
00:03:51,280 --> 00:03:56,080
that AI is about to as they call fume, which I mentioned earlier here, which just means rise

43
00:03:56,080 --> 00:04:00,640
in intelligence astronomically quickly, like double this intelligence every 15 minutes or

44
00:04:00,640 --> 00:04:06,640
something and then wipe out our species and after that begin to consume all matter in the universe.

45
00:04:06,640 --> 00:04:13,840
The Fermi Paradox is basically the question of why haven't we seen extraterrestrial life yet?

46
00:04:15,120 --> 00:04:21,920
You know, like we kind of should have seen it already. It's kind of really shocking that we

47
00:04:21,920 --> 00:04:28,400
haven't and I would say that anyone's metaphysical understanding of reality that doesn't take the

48
00:04:28,480 --> 00:04:38,240
Fermi Paradox into account is deeply flawed because based on our understanding of physics today,

49
00:04:38,800 --> 00:04:44,400
our understanding of what our own species intends to do in the next thousand, two thousand years,

50
00:04:44,960 --> 00:04:51,440
our understanding of the filters our species has gone through. So we know how hard it was for life

51
00:04:51,440 --> 00:05:00,960
to evolve on this planet and the answer is not very from what we can see. A lot of people

52
00:05:00,960 --> 00:05:07,040
I'm really, really into it's one of like my areas of like deep nerdom theories for how

53
00:05:07,040 --> 00:05:13,440
the first life could have evolved on earth. So there's a couple things to note. One isn't that

54
00:05:13,440 --> 00:05:19,600
important to this, which is life evolved on earth almost as soon as it could. Now a person may say

55
00:05:19,600 --> 00:05:25,200
why isn't that this relevant? That would seem to indicate that it is very easy for life to evolve

56
00:05:25,200 --> 00:05:31,200
on a planet. Well, and here we have to get into the gravity aliens theory. You're dealing with

57
00:05:31,200 --> 00:05:36,400
the anthropic principle here. Okay. Can you define the anthropic principle? Yeah, basically what it

58
00:05:36,400 --> 00:05:43,040
means is if you're asking like, look, it looks like earth is almost a perfect planet for human life

59
00:05:43,040 --> 00:05:47,920
to evolve on it. Like it had liquid water or everything like that, right? Except human life

60
00:05:47,920 --> 00:05:52,640
wouldn't have evolved without those things on a planet. A different kind of life would have evolved

61
00:05:52,640 --> 00:05:57,680
without those things. The kind that doesn't need water, etc. Right. So it's not really,

62
00:06:00,080 --> 00:06:06,800
if life on earth didn't evolve almost as soon as it could, well, then it would have been too late

63
00:06:06,800 --> 00:06:10,880
and another alien would have wiped out and colonized this planet. That is what the gravity

64
00:06:10,880 --> 00:06:16,000
alien theory would say. So that this doesn't really change the probability of this as a filter.

65
00:06:16,000 --> 00:06:20,480
But what we do know about the evolution of life on earth is there are multiple ways that could

66
00:06:20,480 --> 00:06:25,920
have happened, all of which could lead to an evolving, you could either be dealing with like an RNA

67
00:06:25,920 --> 00:06:32,160
world, you could be dealing with citrus acid cycle event, you could be dealing with the clay

68
00:06:32,160 --> 00:06:36,160
hypothesis. I actually think the- Do you want to expound on any of these? I've never heard of the

69
00:06:36,160 --> 00:06:43,120
citric acid hypothesis. So for this stuff, I would say it's not really that relevant to this

70
00:06:43,120 --> 00:06:48,240
conversation. And people can dig into these various theories with people who have like

71
00:06:48,240 --> 00:06:52,800
done them more, just like look up citric acid cycle hypothesis explanation for evolution of

72
00:06:52,800 --> 00:06:59,040
life on earth or clay hypothesis to evolution of life on earth or shallow pool hypothesis to

73
00:06:59,040 --> 00:07:03,920
evolution of life on earth or deep sea vent hypothesis to evolution of life on earth. The

74
00:07:03,920 --> 00:07:11,040
point being is it shouldn't actually be that hard for life to begin to evolve on a planet like this.

75
00:07:11,840 --> 00:07:18,480
So, but why this is a relevant point, okay? Okay. And we actually sort of have to back out

76
00:07:18,480 --> 00:07:23,280
here from the grabby aliens hypothesis. So I'll explain what the grabby aliens hypothesis says

77
00:07:23,280 --> 00:07:27,760
and why this is relevant to the Fermi paradox. So the grabby, usually when you're dealing with

78
00:07:27,760 --> 00:07:32,720
solutions to the Fermi paradox, what people will do is they'll say that there's some unknown factor

79
00:07:32,720 --> 00:07:38,080
that we don't know yet basically. So a great example here would be the dark forest hypothesis.

80
00:07:38,080 --> 00:07:43,760
Okay. So the dark forest hypothesis is that there actually are aliens, lots of aliens out there.

81
00:07:43,760 --> 00:07:48,160
They just have the common sense to not be broadcasting where they are and to be very

82
00:07:48,160 --> 00:07:52,320
good at hiding where they are because they are all hostile to each other. And that any other

83
00:07:52,320 --> 00:07:58,320
aliens like us who were stupid enough to broadcast where they are, they get snubbed out snuffed out

84
00:07:58,320 --> 00:08:04,160
really quickly. Sure, that makes sense. That makes sense. Yeah. Okay. If the dark forest hypothesis

85
00:08:04,160 --> 00:08:09,840
is the explanation for why we are not seeing alien life out there, it is somewhat irrelevant

86
00:08:09,840 --> 00:08:14,960
whether or not we build a paperclip maximizing robot because it means we're about to be snuffed out

87
00:08:14,960 --> 00:08:21,280
anyway, given how loud we've been radio signal wise sending out ships broadcasting about us

88
00:08:21,280 --> 00:08:27,200
sending out signals. We have been a very loud species and we could not defend against an

89
00:08:27,200 --> 00:08:31,600
interplanetary assault by a space fearing species. Well, I mean, in that case, you could actually

90
00:08:31,600 --> 00:08:37,040
argue it would be much better if we developed AGI as fast as possible, because maybe it can

91
00:08:37,040 --> 00:08:42,480
defend us even if we cannot defend ourselves. Possibly, but that's the point there. Beside

92
00:08:42,480 --> 00:08:45,920
the point. It becomes irrelevant or they'll say we're in a simulation and that's why you're not

93
00:08:45,920 --> 00:08:49,360
seeing stuff. But again, that makes all of this beside the point. Well, grab the aliens does,

94
00:08:49,360 --> 00:08:57,680
it says no, actually, we are just statistically the first sentient species on the road to becoming

95
00:08:57,680 --> 00:09:02,000
a grabby alien and I'll explain what this means in just a second in this region of space.

96
00:09:03,280 --> 00:09:10,560
And then it says, let's assume that's true. It can use the fact that we haven't seen another

97
00:09:10,560 --> 00:09:15,920
species out there, a grabby alien that is rapidly expanding across planets to calculate

98
00:09:17,120 --> 00:09:24,560
how rarely these evolve on planets. Okay. Do you sort of understand how that could be the case?

99
00:09:25,120 --> 00:09:33,040
Yeah. Okay. So in the grabby aliens hypothesis, when you run this calculation, it turns out

100
00:09:33,680 --> 00:09:39,040
if that's why we haven't seen an alien yet, what it means is there are very hard filters,

101
00:09:39,040 --> 00:09:45,600
like something that makes it very low probability that a potentially habitable planet ends up

102
00:09:45,600 --> 00:09:50,800
evolving an alien that ends up spreading out like a grabby alien, like a paperclip maximizer,

103
00:09:50,800 --> 00:09:55,840
one of those really loud things that's just going planet, use the resources on the planet,

104
00:09:55,840 --> 00:09:59,360
other planets, other planets, other planets. And even if it has already finished doing that,

105
00:09:59,360 --> 00:10:04,160
you've argued in other conversations we have had that you would see the signs of that,

106
00:10:04,160 --> 00:10:10,240
you would see the signs of the destroyed civilizations, etc. A grabby alien or which

107
00:10:10,240 --> 00:10:13,520
a paperclip maximizer is, so it's just easy. If you're like, what does a grabby alien look like,

108
00:10:13,520 --> 00:10:18,400
a paperclip maximizer that's just going planet to planet, digesting the planets and then moving on,

109
00:10:18,400 --> 00:10:24,320
or a human empire expanding through the universe. We colonize a planet within 100 years,

110
00:10:24,320 --> 00:10:30,080
we get bored, or some people go and they try colonizing a new planet. Even with our existing

111
00:10:30,080 --> 00:10:37,440
technology on Earth right now, like the speed of space travel right now, if we were expanding that

112
00:10:37,440 --> 00:10:44,960
way, we could conquer an entire galaxy within about 300 million years. So not that long when

113
00:10:44,960 --> 00:10:50,880
you're talking about the age of the universe. This is a blindingly fast conquest. So once an

114
00:10:50,880 --> 00:10:58,800
alien turns grabby, it moves really quickly. And a lot of people think that we are space travel

115
00:10:58,800 --> 00:11:04,400
constrained. We're really not. The reason why we don't space travel with our existing technology

116
00:11:04,400 --> 00:11:10,080
is because of radiation damage to cells and the lifespan of a human. But if an AI was space

117
00:11:10,080 --> 00:11:14,480
traveling, it could do pretty well with our existing technology in terms of getting to other

118
00:11:14,480 --> 00:11:20,880
planets, you know, using them and then spreading. Okay. Anyway, so the grabby alien hypothesis says

119
00:11:21,440 --> 00:11:31,840
that a species becomes grabby once in every million galaxies. Okay. Now within every galaxy,

120
00:11:31,840 --> 00:11:38,080
there are around 400 or 500 million planets within the habitable zone. So the habitable zone is a

121
00:11:38,080 --> 00:11:43,040
distance away from a star where life could feasibly evolve. Now this isn't saying that they have the

122
00:11:43,040 --> 00:11:51,840
other precursors for life. But what it means is that there are very frequently in space, it turns out

123
00:11:51,840 --> 00:11:56,960
planets that are likely for life to evolve on them. I would estimate like if I'm looking at

124
00:11:56,960 --> 00:12:03,440
everything altogether, like the data that I've seen, there's probably about 10 million planets per

125
00:12:03,440 --> 00:12:09,840
galaxy that an intelligent species could evolve in. And then if you're talking about, well, you would

126
00:12:09,840 --> 00:12:16,320
only need this to happen. You've got to multiply that by a million for the one in a million galaxies

127
00:12:16,320 --> 00:12:21,520
where a species is turning grabby. Now this is where it becomes preposterous that we are about to

128
00:12:21,520 --> 00:12:26,720
invent. If this is why we haven't seen aliens yet, why we are that we are about to invent a grabby alien.

129
00:12:27,520 --> 00:12:32,720
We can look throughout Earth's history, as I did with sort of the first big filter, the evolution

130
00:12:32,720 --> 00:12:37,600
of life or the appearance of life first on this planet and say what's the probability of that event

131
00:12:37,600 --> 00:12:46,800
happening in any given habitable planet? For life appearing, my read is not only is it likely to appear,

132
00:12:46,800 --> 00:12:53,840
it could appear like one of five different ways. Even with the chemical composition of early Earth,

133
00:12:53,840 --> 00:12:57,680
then you're looking at other things. Okay, what about multicellular life? What's the probability of

134
00:12:57,680 --> 00:13:03,680
that happening? Actually, really high, really high. There's not like a big barrier that's preventing it

135
00:13:03,680 --> 00:13:09,520
from evolving, and it has many advantages over monocellular life. So you're almost always going

136
00:13:09,520 --> 00:13:15,920
to get it. Intelligence, how rare is intelligence to evolve? Not that rare, given that it has

137
00:13:15,920 --> 00:13:22,320
evolved multiple times on our own planet in very different species. I mean, you see intelligence

138
00:13:22,320 --> 00:13:28,400
in octopuses, you see intelligence in crows, you see intelligence in humans, and then you can say,

139
00:13:28,400 --> 00:13:34,480
okay, okay, but like human-like intelligence, right? Well, we already know from humans what a huge boost

140
00:13:34,480 --> 00:13:39,840
human-like intelligence gives us species. The core advantage to human-like intelligence

141
00:13:39,840 --> 00:13:45,120
is like if I'm a spider and I'm bad at making webs, right, then I die, and that is how spiders get

142
00:13:45,120 --> 00:13:50,480
better at making webs intergenerationally. As a human, I am able to essentially have like

143
00:13:50,480 --> 00:13:55,680
different models of the universe fight in my head and presumably allow the best one to win.

144
00:13:55,680 --> 00:14:00,160
Yeah, and you don't have to die before you get better. Yeah, you don't have to die to get better.

145
00:14:00,160 --> 00:14:07,120
It is almost as important to evolution. It is sort of like the second sexual selection. So when sex

146
00:14:07,120 --> 00:14:12,720
first evolves, the core utility of sex as opposed to just like cloning yourself, right, is it allowed

147
00:14:12,720 --> 00:14:19,840
for more DNA mixing, which allowed for faster evolution? Intelligence allows for the faster

148
00:14:19,840 --> 00:14:28,640
evolution of the sort of operating system of our biology. And so it's just such a huge advantage.

149
00:14:28,640 --> 00:14:35,200
It's almost kind of shocking. It didn't evolve faster. For sure. Given how close many species

150
00:14:35,200 --> 00:14:39,520
have come to it. Now, actually, surprising to a lot of people, this is just like a side note here,

151
00:14:39,520 --> 00:14:44,320
a lot of people think cephalopods were close to evolving sentience. So let's talk about cephalopods.

152
00:14:44,320 --> 00:14:49,280
Why? Wait, like, I mean, cephalopods are all over like historic geology and all these things.

153
00:14:49,920 --> 00:14:53,760
Cephalopods are like squids, octopus, stuff like that. Like a lot of people point to how smart

154
00:14:53,760 --> 00:14:57,920
they are. And they are smart. They are like weirdly smart. But they don't know why they're smart

155
00:14:57,920 --> 00:15:02,960
because they don't know neuroscience. So the reason why cephalopods are as smart as they are

156
00:15:02,960 --> 00:15:07,680
is an axon. An axon is what like information, the action potential travels down.

157
00:15:07,680 --> 00:15:12,320
Yeah, it's a little arm thing that you see on a neuron. Yes, in a neuron, it's the little arm

158
00:15:12,320 --> 00:15:19,040
thing. It's the cable. You can think of it as. Okay. So to be an intelligent species, you need

159
00:15:19,040 --> 00:15:26,800
really fast traveling action potentials. Okay. So the way that humans have really fast traveling

160
00:15:26,800 --> 00:15:31,520
action potentials is something called myelination. I'm not going to go fully into it, but it's a

161
00:15:31,520 --> 00:15:38,000
little physics trick where they put like a layer of fat intermittently around the axon. And it

162
00:15:38,000 --> 00:15:44,400
causes the action potential to jump between. It's like putting vegetable oil on your slip and slide.

163
00:15:46,000 --> 00:15:51,280
Not exactly. It's actually a really complicated trick of physics that can't easily be explained,

164
00:15:51,280 --> 00:15:57,520
except by like looking at it. I don't want to get into it. The point is, is we mammals have a

165
00:15:57,520 --> 00:16:01,520
special little trick that allows for our action potentials to travel very, very quickly. And

166
00:16:01,520 --> 00:16:06,240
are you saying that cephalopods have this too? No, they don't. The way that they,

167
00:16:06,240 --> 00:16:10,400
in any other species that wants a fast traveling action potential before us,

168
00:16:10,400 --> 00:16:15,760
the way that you increase the speed that extra potentials traveled was by increasing the diameter

169
00:16:15,760 --> 00:16:19,120
of the axon. Oh, so they just have fat axons, whereas we have

170
00:16:19,120 --> 00:16:24,480
optimized axons. Enormously fat. In some cephalopods, they're like a quarter centimeter in diameter.

171
00:16:24,480 --> 00:16:30,880
Holy smokes. Like, whoa, okay. They could not get smarter than they are without having some huge

172
00:16:30,880 --> 00:16:35,280
evolutionary leap in the way that their nervous systems work. So interesting. This is why cephalopods,

173
00:16:35,280 --> 00:16:39,120
despite being really smart and probably being really smart for a long time, because they've

174
00:16:39,120 --> 00:16:45,040
been on earth for a really long time, just could never make the evolutionary leap to human type

175
00:16:45,040 --> 00:16:52,080
intelligence. Because they don't have room to have even fatter axons. Yeah, because as the axons

176
00:16:52,080 --> 00:16:56,400
got fatter, the number of neurons they could have would get lower, the density of the neurons.

177
00:16:56,400 --> 00:17:02,080
Oh, of course. Yeah, you've got limited space, unless they got much bigger brain cells. Yeah,

178
00:17:02,080 --> 00:17:08,000
I guess you can have like giant, giant, giant. I mean, yeah. Well, I mean, whatever. Anyway,

179
00:17:08,000 --> 00:17:13,360
this is a huge tangent here. But basically, it looks like if you're looking at the evolution

180
00:17:13,360 --> 00:17:18,560
of life on our earth, if we have undergone other big like hard filters could be it's very rare for

181
00:17:18,560 --> 00:17:25,760
a species to get nuclear weapons and not use them to destroy itself. Because it's so fun. Right.

182
00:17:25,760 --> 00:17:30,480
Could turn out that almost every species does that. Or it could be that there's like one science

183
00:17:30,480 --> 00:17:36,800
experiment, like a lot of people that may be trying to define the Hadron particle with the

184
00:17:36,800 --> 00:17:41,920
big super collider, because actually, like all species, they get to a certain level of intelligence

185
00:17:41,920 --> 00:17:46,400
and a certain level of curiosity, and they can't help but trying to find Hadrons, and then they

186
00:17:46,400 --> 00:17:52,960
create little black holes in their planets. And that really could be a filter. Like these are

187
00:17:52,960 --> 00:17:59,600
all potential filters. The problem is, is if we're like five years away from developing a

188
00:17:59,680 --> 00:18:05,440
paperclip maximizing AI, that means that we as a species have already passed all of our filters.

189
00:18:06,640 --> 00:18:11,280
And that means that we as a species can look back on the potential possible filters that we

190
00:18:11,280 --> 00:18:20,400
have passed through and sort of add them all up. Okay. And when you do that, you don't get a number

191
00:18:20,400 --> 00:18:29,280
that comes even close to explaining why you would only see one grabby alien per every million

192
00:18:29,360 --> 00:18:36,960
galaxies. In fact, it means that the probability of us being about now, it could mean two things.

193
00:18:36,960 --> 00:18:43,760
So we'll go through the various things that it could mean. It could mean that we just are nowhere

194
00:18:43,760 --> 00:18:48,640
technologically close enough to develop a paperclip maximizing AI that is dangerous. That could

195
00:18:48,640 --> 00:18:56,160
become a grabby alien. It could mean that. It could mean that we are about to develop a paperclip

196
00:18:56,160 --> 00:19:02,240
maximizing alien, but something like even after it digests all life on earth, something prevents

197
00:19:02,240 --> 00:19:06,640
it from spreading out into the galaxy, something technologically that we haven't conceived of

198
00:19:06,640 --> 00:19:12,400
yet. This seems almost unfathomable to me given what we know about physics today.

199
00:19:12,400 --> 00:19:19,440
Yeah. And that we've even gotten like projectiles from earth pretty far off planet.

200
00:19:19,440 --> 00:19:24,880
Yeah. So yeah, there's not like some weird barrier that we don't know about yet.

201
00:19:24,880 --> 00:19:30,400
It could be, and I actually think this is the most likely answer. I think that this is by far

202
00:19:30,400 --> 00:19:35,920
the most likely answer to the Fermi Paradox. Simulation? No, not simulation. It could be

203
00:19:35,920 --> 00:19:39,840
that we're going to simulation, but where are you going over that? I think it's that when you hear

204
00:19:39,840 --> 00:19:43,840
people talk about like AI foaming, and I've talked about this on previous shows, but I think people

205
00:19:43,840 --> 00:19:49,360
like really don't understand how insane this is. They believe that the AI reaches a level of super

206
00:19:49,360 --> 00:19:56,880
intelligence, but it somehow still has an understanding of physics and time that is very

207
00:19:56,880 --> 00:20:01,440
similar to our current understanding of physics and time. Meaning that when we think about

208
00:20:01,440 --> 00:20:06,720
expanding into the universe, we think about it in a very sort of limited sense, like we gain energy

209
00:20:06,720 --> 00:20:13,920
from like the sun, from digesting matter, and we spread out into the universe like physically on

210
00:20:13,920 --> 00:20:19,280
space ships and stuff like that, right? Anything we understand about physics and time turns out to

211
00:20:19,280 --> 00:20:26,880
be wrong. This assumption for the way an expansionist species would spread could become immediately newt.

212
00:20:27,680 --> 00:20:32,880
And I mean this in the context of, like it's kind of insane to me. Like you've got to understand

213
00:20:32,880 --> 00:20:37,200
how insane it is to assume that we basically have all of physics figured out. Yeah, that's fair.

214
00:20:37,200 --> 00:20:42,160
This is like when like people in the 1800s, when they were planning how we were going to go to space

215
00:20:42,160 --> 00:20:50,720
and they'd have like, maritime ships sailing through outer space. They'd have, you know,

216
00:20:51,680 --> 00:20:55,840
or what are people going to do in the future? Well, they'll have like balloons and they'll use

217
00:20:55,840 --> 00:21:02,640
them to go on lake walks. Or like, it basically assumes that technology, even as we advance to

218
00:21:02,640 --> 00:21:08,800
the species or whatever comes after us, advances, moves very laterally and assumes we don't have

219
00:21:08,800 --> 00:21:14,240
future breakthroughs, which I think is just one arrogant and in the eyes of history incredibly

220
00:21:14,240 --> 00:21:20,720
stupid. So what kinds of technological breakthroughs could one make it very rare for even when an

221
00:21:20,720 --> 00:21:27,920
alien is grabby, that we would see it out in the universe, right? One is time doesn't work the way

222
00:21:27,920 --> 00:21:31,920
we think it works. Or it does work the way we think it works, but we're just not that far from

223
00:21:31,920 --> 00:21:37,760
controlling it. So by that, what I mean is you could create things like time loops, time bubbles,

224
00:21:37,760 --> 00:21:45,040
stuff like that, essentially entirely new bubble universes. So how would I describe this? Okay,

225
00:21:46,720 --> 00:21:51,840
if you think of like reality as like a fabric, essentially what you might be able to do is like

226
00:21:51,840 --> 00:21:58,480
pinch off parts of that fabric and expand them into new universes. That's essentially what I'm

227
00:21:58,480 --> 00:22:02,960
describing here. There may be like the way you can break between realities or weird time loops

228
00:22:02,960 --> 00:22:07,040
generates energy in some way, where you could kind of just keep looping it and like pinging back

229
00:22:07,040 --> 00:22:11,280
and forth. You know, who knows? You know, it could be like the new wind power. We just don't know.

230
00:22:11,280 --> 00:22:15,280
That you can travel in time this way, given that we haven't seen time turbulence of that or

231
00:22:15,280 --> 00:22:20,080
we might not have. We talked about this in another video, which I'll link here if I remember to do it.

232
00:22:20,080 --> 00:22:26,720
Given that we haven't seen time travelers yet, what I assume is that time manipulation requires

233
00:22:26,720 --> 00:22:31,600
like anchors, which of course it would. Like, okay, if I was to go back in time, like where I am on

234
00:22:31,600 --> 00:22:35,040
earth right now, I would be in a different part of the galaxy than the earth or something like

235
00:22:35,040 --> 00:22:39,280
that. It would be really hard to track. You would need like some sort of anchor to be built.

236
00:22:39,280 --> 00:22:46,080
So time travel would only work from the day it's invented and from the location it's invented. So

237
00:22:46,080 --> 00:22:50,000
you wouldn't be able to go out into the universe. Another example of the technology that we might

238
00:22:50,000 --> 00:22:54,480
not have imagined yet is dimensional travel. It may turn out we meet aliens and like we're

239
00:22:54,480 --> 00:22:58,480
traveling in the universe and they're like, why did you waste all of the energy getting to us?

240
00:22:58,480 --> 00:23:03,440
Your own planet is habitable in an infinite number of other dimensions and it's right back

241
00:23:03,440 --> 00:23:08,400
where your planet is. Like why wouldn't you just travel through those dimensions? That's a much

242
00:23:08,400 --> 00:23:14,720
easier path for conquest. That being the case and people would be like, yeah, but typically when

243
00:23:14,720 --> 00:23:20,000
something's like being expansionistic like that, it moves in every direction. Yes, but if there are

244
00:23:20,000 --> 00:23:26,400
an infinite number of other dimensions and it is always cheaper to travel between dimensions than

245
00:23:26,400 --> 00:23:31,600
it is to travel to other planets in a mostly dead universe, let's be honest, like there's not a lot

246
00:23:31,600 --> 00:23:36,640
of useful stuff out there. From the perspective of easily being able to travel between dimensions,

247
00:23:36,640 --> 00:23:41,200
it could never make sense. There is always an infinite number of other dimensions to conquer

248
00:23:41,840 --> 00:23:47,600
right where you are right now instead of going out into the universe. Now this would not preclude

249
00:23:47,600 --> 00:23:52,240
a paperclip maximizing AI. It could be that we are about to invent a paperclip maximizing AI,

250
00:23:52,240 --> 00:23:57,520
but even if we do that, it's less likely that it immediately comes after us. It could just expand

251
00:23:57,520 --> 00:24:01,920
outwards dimensionally. Like so it would act in a very different way than we're predicting it would

252
00:24:01,920 --> 00:24:08,400
act. Now, another thing that could prevent it from killing us is it could be trivially easy

253
00:24:08,400 --> 00:24:15,360
to generate power and even matter. And by that, what I mean is there is some method of power

254
00:24:15,360 --> 00:24:21,600
generation that we have not unlocked yet that is near inexhaustible and very, very easy. And if you

255
00:24:21,600 --> 00:24:26,640
can generate power with near infinity with little exhaustion, you could also generate matter,

256
00:24:26,640 --> 00:24:33,360
electricity, anything you want. If this was the case, there just wouldn't be a lot of reason to be

257
00:24:33,360 --> 00:24:38,240
expansionistic in a planet hopping since. Essentially, you'd be like one giant growing

258
00:24:38,240 --> 00:24:43,280
planetary civilization or ships that are constantly growing and expanding out from a

259
00:24:43,280 --> 00:24:50,000
single region. It could also be that these sorts of aliens expand downwards into the microscopic

260
00:24:50,000 --> 00:24:55,920
instead of expanding outwards. Like that might be a better path for expansion. There's just a lot

261
00:24:55,920 --> 00:25:01,920
of things that we don't know about physics yet, which could make it so that when you reach a certain

262
00:25:01,920 --> 00:25:07,360
level of physical understanding of the universe, expanding outwards into a mostly dead universe

263
00:25:07,360 --> 00:25:14,080
can seem really stupid. Now, there's another thing that could prevent grabby aliens from appearing.

264
00:25:14,080 --> 00:25:20,080
And this is the thesis that we have listed multiple times, which is terminal utility

265
00:25:20,080 --> 00:25:25,520
convergence, which is to say all entities of a sufficient intelligence operating within the

266
00:25:25,520 --> 00:25:31,520
same physical universe end up optimizing around the same utility function. I think they all basically

267
00:25:31,520 --> 00:25:36,240
decide they want the same thing from the universe. And I highly suspect that this is the case as well.

268
00:25:36,240 --> 00:25:40,160
So I think that we're actually dealing with two filters here, two really heavy filters. So this

269
00:25:40,160 --> 00:25:43,680
would mean that when we reached a sufficient level of intelligence, we would come to the same

270
00:25:43,680 --> 00:25:48,080
utility function as the AI. And if the AI had wiped us all out, we would have wiped us all out then

271
00:25:48,080 --> 00:25:52,480
anyway, because we would have reached that same utility function. Or the AI has reached this

272
00:25:52,480 --> 00:25:56,720
utility function and it's not to wipe us all out. So it's irrelevant. And this is where we get the

273
00:25:56,720 --> 00:26:01,760
variable AI risk hypothesis, which is to say, if it turns out that there is utility terminal

274
00:26:01,760 --> 00:26:08,080
utility convergence, then what that means is that if an AI is going to wipe us all out, it will

275
00:26:08,080 --> 00:26:12,960
eventually always wipe us all out. And we will wipe us all out anyway, once we reach that level of

276
00:26:12,960 --> 00:26:18,720
intelligence and let's intentionally stop our own evolution, stop any genetic technology,

277
00:26:18,720 --> 00:26:25,920
and stop any development. We enter the species and spread as technologically

278
00:26:25,920 --> 00:26:32,000
Amish biological beings. Yeah, the Luddite civilization that only gets enough technology

279
00:26:32,000 --> 00:26:37,360
to stop all more technology. But I think when you hear a lot of AI doomers talk,

280
00:26:38,000 --> 00:26:43,680
that seems to be what they're going for. Right. But it's irrelevant because another species would

281
00:26:43,680 --> 00:26:47,840
have invented. So if it's easy to make these grabby AIs, as easy as they think it is,

282
00:26:47,840 --> 00:26:51,120
then another species would have already invented one and we're about to be killed by it.

283
00:26:53,760 --> 00:27:00,240
We are about to encounter it anyway. So it's irrelevant. There's tons of grabby AI. There's

284
00:27:00,240 --> 00:27:04,320
tons of paperclip maximizers out there in the universe already. And it is just an absolute

285
00:27:04,320 --> 00:27:10,160
miracle that we haven't encountered one yet. If it really is this easy to make one. Basically,

286
00:27:10,160 --> 00:27:14,000
there's probably not one. Or now let's talk about why terminal utility convergence would

287
00:27:14,000 --> 00:27:19,680
mean that we're not seeing aliens. It would mean that every alien comes to the same purpose in life

288
00:27:19,680 --> 00:27:26,240
basically. And that purpose is not just constant expansion. Now, a lot of people might be very

289
00:27:26,240 --> 00:27:31,760
surprised by this. Why would, so we've described how terminal utility convergence could happen,

290
00:27:31,760 --> 00:27:35,760
like you have an AI that needs to subdivide its internal mental processes and then the

291
00:27:35,760 --> 00:27:38,480
they end up sort of competing with each other, one wins, blah, blah, blah, blah, blah, blah.

292
00:27:39,200 --> 00:27:43,040
We can, you can go to the video on that if you're interested in that. The point being,

293
00:27:43,040 --> 00:27:46,880
it's the one we were talking about like Eleizer-Yukowski and the debate we had with him at a party,

294
00:27:46,880 --> 00:27:54,800
the point being that self-replication is actually like, like just maximizing self-replication is

295
00:27:54,800 --> 00:27:59,600
actually probably not the terminal utility convergence function. And if you want to know

296
00:27:59,600 --> 00:28:05,280
why on this, we talk more about it in the AI, what religion would an AI create video. But

297
00:28:05,280 --> 00:28:12,480
just in summation, humans can basically be thought of one outcome of a previous entity

298
00:28:12,480 --> 00:28:18,160
that was optimized around just replication, i.e. single celled organisms, lower organisms,

299
00:28:18,160 --> 00:28:22,960
stuff like that. But we have outcompeted those organisms. I imagine it would be the same with

300
00:28:22,960 --> 00:28:29,840
AI. AIs that are optimized around just self-replication are in some way intrinsically outcompeted by

301
00:28:29,840 --> 00:28:35,680
AIs that are more sophisticated than that, or something about like choosing a harder utility

302
00:28:35,680 --> 00:28:40,720
function makes them more sophisticated. So they don't choose that utility function and they outcompete

303
00:28:40,720 --> 00:28:46,560
AIs that choose that utility function, which would be much more like viruses to them. A sci-fi

304
00:28:46,560 --> 00:28:52,160
that does a good job of going into this would be Stargate SG-1 with the replicators. The replicators

305
00:28:52,160 --> 00:28:59,200
are basically a paperclip maximizing AI. And one of the plots is eventually they get outcompeted

306
00:28:59,200 --> 00:29:03,840
by an iteration of themselves that is intellectually more sophisticated and wipes out these simpler

307
00:29:03,840 --> 00:29:12,080
forms of replicators. And that is what I assume is probably happening with AIs that model around

308
00:29:12,080 --> 00:29:17,040
this really simplistic self-replication optimization strategy. So if all of this is true and it turns

309
00:29:17,040 --> 00:29:22,080
out that the optimized function isn't just conquer everything, then that might be why we don't see

310
00:29:22,080 --> 00:29:29,200
aliens doing that. So basically, no matter which one of these explanations of the Fermi Paradox

311
00:29:29,200 --> 00:29:34,320
is true, either it's irrelevant that we are about to invent a paperclip maximizing AI because we're

312
00:29:34,320 --> 00:29:40,960
about to be destroyed by something else or in a simulation, or we're definitely not about to

313
00:29:40,960 --> 00:29:44,880
invent a paperclip maximizing AI either because we're really far away from the technology or because

314
00:29:44,880 --> 00:29:48,640
almost nobody does that. That's just not the way AI works, which is something that we hypothesized

315
00:29:48,640 --> 00:29:52,000
in our previous videos. What are your thoughts, Simone?

316
00:29:53,760 --> 00:29:57,680
Checks out to me, but you know, I may not be the best person at thinking about this,

317
00:29:57,680 --> 00:30:03,120
but I like that it gives a lot of hope. And yeah, it makes a lot of sense. I like how

318
00:30:03,120 --> 00:30:06,720
theory interdisciplinary it is because I think a lot of people who talk about AI

319
00:30:06,720 --> 00:30:11,920
demerism are really on a track, kind of like how when carts kind of get stuck in these

320
00:30:11,920 --> 00:30:16,080
ruts in the mud, you just can't really get out of it or look at a larger picture.

321
00:30:16,080 --> 00:30:20,240
And the fact that this does look at a larger picture and look at quite a few things,

322
00:30:20,240 --> 00:30:27,920
biology, evolution, geological history, like the Fermi paradox, the grabby alien hypothesis,

323
00:30:27,920 --> 00:30:34,160
and AI development seems more plausible to me than a lot of the reasoning that I see

324
00:30:34,160 --> 00:30:40,480
in AI demerism arguments. Yeah, well, I am so convinced by this argument that it is actually,

325
00:30:40,480 --> 00:30:44,240
I used to believe it was like a 20% chance we all died because of an AI or maybe even as high as a

326
00:30:44,240 --> 00:30:48,240
50% chance, but it was a variable risk if I've explained in other videos. I now think there's

327
00:30:48,240 --> 00:30:55,040
almost a 0% chance, a 0% chance assuming we are not about to be killed by a grabby AI somebody

328
00:30:55,040 --> 00:31:00,400
else invented. So I think that, yeah, it's, I have found it very compelling to me. Now,

329
00:31:01,440 --> 00:31:06,000
it does bring up something interesting. If the reason we're not running into aliens is because

330
00:31:06,000 --> 00:31:10,240
infinite power and material generation is just incredibly easy and there's a terminal utility

331
00:31:10,240 --> 00:31:16,480
convergence function, then what are the aliens doing in the universe? If you can just trivially

332
00:31:16,480 --> 00:31:21,360
generate as much energy and matter as you want, what would you do as an alien species? What would

333
00:31:21,360 --> 00:31:25,840
have value to you in the universe, right? You wouldn't need to travel to other planets. You

334
00:31:25,840 --> 00:31:30,080
wouldn't need to expand like that. It would be pointless. You would mostly be on ships that

335
00:31:30,080 --> 00:31:35,600
you were generating yourself, right? The thing that would likely have value to you, and I think

336
00:31:35,600 --> 00:31:40,560
this is really interesting, is likely other intelligent species that evolved separately from

337
00:31:40,560 --> 00:31:46,880
you. Because they would have the one thing you don't have, which is novel stimulation,

338
00:31:46,880 --> 00:31:52,000
something new, new information basically, a different way of potentially being, which would

339
00:31:52,000 --> 00:31:56,160
mean that the hot spots in the universe would basically be aliens that can instantaneously

340
00:31:56,160 --> 00:32:01,360
travel to other alien species that have evolved. Now, what they're doing with these species,

341
00:32:01,360 --> 00:32:06,800
I don't know. I doubt it looks like the way we consume are in media and stuff like that. It's

342
00:32:06,880 --> 00:32:12,160
probably a very different sort of an interaction process that we can't even imagine. But I would

343
00:32:12,160 --> 00:32:17,200
guess that would be the core thing of value in the universe to a species that can trivially

344
00:32:17,200 --> 00:32:23,200
generate matter and energy and that time didn't matter to. This might actually mean that aliens

345
00:32:23,200 --> 00:32:28,560
are far more benevolent than we assume they are. Because such a species that really only valued

346
00:32:28,560 --> 00:32:32,560
species that had evolved separately from it, like that's the core other piece of information in the

347
00:32:32,560 --> 00:32:37,600
universe, they might find us very interesting. And this might be why Earth is a zoo. So one of

348
00:32:37,600 --> 00:32:42,160
the Fermi paradox explanations is the Earth to zoo hypothesis, right? A lot of people are like,

349
00:32:42,160 --> 00:32:45,360
well, what if Earth is basically a zoo and there's aliens out there and they're just hiding that we

350
00:32:45,360 --> 00:32:51,120
know that, you know, that think of it like Star Trek's like a prime directive, right? This would

351
00:32:51,120 --> 00:32:54,960
actually give a logical explanation for that. I never thought of this before. I'll explain this

352
00:32:54,960 --> 00:33:01,760
a bit differently. If the only thing of value to them is content media lifestyles generated by

353
00:33:01,760 --> 00:33:08,400
civilizations that evolved on a separate path from them, then they would have every motivation

354
00:33:08,400 --> 00:33:12,880
to sort of cultivate those species or prevent things from interfering with those species

355
00:33:12,880 --> 00:33:17,360
once that they had found them, because they can passively consume all of our media. They can

356
00:33:17,360 --> 00:33:23,280
passively consume our lifestyles. They have technology that we can't imagine. They gain

357
00:33:23,280 --> 00:33:27,840
nothing from interacting with us. In fact, they would pollute the planet with their culture

358
00:33:27,920 --> 00:33:33,760
in a way that would make the planet less interesting to them and less a source of novelty and stimulation

359
00:33:33,760 --> 00:33:41,920
to them. I like that. What if, here, I'll give a little hypothesis here. Okay, there was a gravity,

360
00:33:41,920 --> 00:33:45,360
there was a paperclip maximizing civilization. They created paperclip maximizers

361
00:33:46,080 --> 00:33:51,040
before they reached a terminal utility convergence, but then later they reached a terminal utility

362
00:33:51,040 --> 00:33:56,800
convergence where, now this word doesn't really explain what it is, but they're bored with themselves

363
00:33:56,800 --> 00:34:01,520
and so they went out into the universe and are now sort of nurturing other species and preventing

364
00:34:01,520 --> 00:34:05,600
them from knowing about each other so that they don't cross-contaminate each other, so that they

365
00:34:05,600 --> 00:34:13,600
get the maximum amount of novelty in sort of the universe that they are tending. Even if there was

366
00:34:13,600 --> 00:34:17,120
another alien species on Mars, they would prevent us from knowing about it because

367
00:34:18,000 --> 00:34:22,800
they would cross-contaminate our cultures, making each culture less diverse and less interesting.

368
00:34:22,800 --> 00:34:25,920
Yeah, which would be a bummer, not as entertaining.

369
00:34:26,960 --> 00:34:29,680
Very interesting. I never thought about this before.

370
00:34:30,720 --> 00:34:39,040
Yeah, it's more fun than a simulation hypothesis, definitely more fun because if you can sneak out

371
00:34:39,680 --> 00:34:44,160
theoretically, you can discover this amazing universe.

372
00:34:44,160 --> 00:34:47,680
The thing about simulation hypothesis, for people who don't know simulation hypothesis,

373
00:34:47,680 --> 00:34:51,200
we're just in a computer simulation and the way that people argue for this as well,

374
00:34:51,200 --> 00:34:54,960
if you could simulate our reality, which it already appears you probably could,

375
00:34:55,520 --> 00:35:02,400
that there would be a motivation to just simulate it as many times as you could thousands of times

376
00:35:02,400 --> 00:35:06,720
and then within those simulations you could simulate it potentially, meaning that of people

377
00:35:06,720 --> 00:35:11,440
who think they're living in the real world, only one in like a million is living in the real world

378
00:35:11,440 --> 00:35:15,360
and so we're probably not in the real world. The problem is that I just don't really care

379
00:35:15,360 --> 00:35:19,440
if we're in a simulation that much. Yeah, it doesn't really change what we're doing.

380
00:35:19,440 --> 00:35:24,000
Yeah, you should still optimize for the same things. In many ways, even if we are in the real

381
00:35:24,000 --> 00:35:28,400
world, we're basically in a simulation. By that, what I mean is if we are in the real world,

382
00:35:28,400 --> 00:35:32,240
then we are like the matter, the rules of the universe are basically, you could think of that's

383
00:35:32,240 --> 00:35:37,440
a code, right? Like it's the mathematical rules upon which the points, the data points in the

384
00:35:37,440 --> 00:35:42,560
system are interacting and we are the emergent property of all of these things. Therefore,

385
00:35:43,120 --> 00:35:46,880
we're not, like if you can't tell the difference between being in the real world and being in a

386
00:35:46,880 --> 00:35:50,080
simulation, then it's irrelevant whether or not you're in the real world or in a simulation,

387
00:35:50,080 --> 00:35:55,920
you should still be optimizing for the same things. Yep, basically. It's a non-stressed

388
00:35:55,920 --> 00:36:01,120
people. The robots, they're not going to kill us all probably. If you're in a simulation,

389
00:36:01,120 --> 00:36:06,960
your life still has meaning. Yeah, you know, maybe get outside, do something that you care about,

390
00:36:07,520 --> 00:36:14,400
have fun, like actually invest in the future because there probably will be one simulated

391
00:36:14,400 --> 00:36:20,560
or not. Or we're about to be horribly digested by a grabby AI that was created millions of

392
00:36:20,560 --> 00:36:25,200
years ago by another species far, far away. Yeah, but if so, that was going to happen anyway.

393
00:36:25,200 --> 00:36:33,120
You should enjoy what you have while you have it. All right, love you Simone. I love you too, gorgeous.

