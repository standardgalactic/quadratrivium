start	end	text
0	5280	So basically, no matter which one of these explanations of the Fermi paradox is true,
5280	9680	either it's irrelevant that we are about to invent a paperclip maximizing AI because we're
9680	16320	about to be destroyed by something else or in a simulation, or we're definitely not about to
16320	20000	invent a paperclip maximizing AI either because we're really far away from the technology or
20000	24880	because almost nobody does that. That's just not the way AI works. I am so convinced by this argument
24880	28800	that it is actually, I used to believe it was like a 20% chance we all died because of an AI or maybe
28800	32640	even as high as a 50% chance, but it was a variable risk if I've explained in other videos.
32640	39200	I now think there's almost a 0% chance. A 0% chance assuming we are not about to be killed by a
39200	45040	grabby AI somebody else invented. Now, it does bring up something interesting. If the reason we're
45040	49760	not running into aliens is because infinite power and material generation is just incredibly easy
49760	55280	and there's a terminal utility convergence function, then what are the aliens doing in the
55280	62320	universe? Would you like to know more? Hi Malcolm, how are you doing my friend? So today we are going
62320	68320	to do an episode, a bit of a preamble for an already filmed interview. So we did two interviews
68320	74800	with Robin Hansen and in one of them we discussed this theory. However, I didn't want to off rail
74800	80000	the interview too much going into this theory, but I really wanted to nerd out on it with him
80000	85760	because he is the person who invented the grabby aliens hypothesis solution to the Fermi Paradox.
87360	91440	I hadn't heard about grabby aliens before, so I'm glad we're doing this. This is great.
92000	99280	Yes, so we will use this episode to talk about the Fermi Paradox, the grabby alien hypothesis,
99280	106240	and how the grabby alien hypothesis can be used through controlling one of the variables,
106240	113120	i.e. the assumption that we are about to invent a paperclip maximizer AI that ends up fooming and
113120	118560	killing us all, because that would be a grabby alien definition only. If you collapse that variable
118560	126000	within the equation to today, then you can back calculate the probability of creating a paperclip
126000	133360	maximizing AI. And spoiler alert, the probability is almost zero. It basically means it is almost
133360	140640	statistically impossible that we are about to create a paperclip maximizing AI unless with the
141280	146960	two big caveats here, something in the universe that would make it irrelevant whether or not
146960	153440	we created a paperclip maximizing AI is hiding other aliens from us, or we are in a simulation,
153440	158000	which also would make it irrelevant that we're about to create a paperclip maximizing AI, or
158960	164800	there is some filter to advance the life developing on a planet that we have already
164800	169360	passed through, that we don't realize that we have passed through. So those are the only ways
169360	174400	that this isn't the case. But let's go into it because it is it is really easy. I just realized
174400	178640	that some definitions may help here. We'll get into the grabby alien hypothesis in a second,
178640	186320	but the concept of a paperclip maximizing AI is the concept of an AI that is just trying to maximize
186400	192960	some simplistic function. So in the concept as it's laid out as a paperclip maximizer, it would
192960	197040	be just make maximum number of paperclips and then it just keeps making paperclips and it
197040	200960	starts turning the earth into paperclips and it starts turning people into paperclips. Now
200960	205200	realistically, if we were to have a paperclip maximizing AI, it would probably look something
205200	211040	more like, you know, somebody says process this image and it just keeps processing the image to
211040	215920	like an insane degree because it was never told when to stop processing the image and it just
215920	220400	turns all the world into energy to process an image or something else silly like that.
220400	225840	This concept is important to address because there are many people who at least pass themselves
225840	231280	office intelligent who believe that we are about to create a paperclip maximizing AI,
231280	236080	that AI is about to as they call fume, which I mentioned earlier here, which just means rise
236080	240640	in intelligence astronomically quickly, like double this intelligence every 15 minutes or
240640	246640	something and then wipe out our species and after that begin to consume all matter in the universe.
246640	253840	The Fermi Paradox is basically the question of why haven't we seen extraterrestrial life yet?
255120	261920	You know, like we kind of should have seen it already. It's kind of really shocking that we
261920	268400	haven't and I would say that anyone's metaphysical understanding of reality that doesn't take the
268480	278240	Fermi Paradox into account is deeply flawed because based on our understanding of physics today,
278800	284400	our understanding of what our own species intends to do in the next thousand, two thousand years,
284960	291440	our understanding of the filters our species has gone through. So we know how hard it was for life
291440	300960	to evolve on this planet and the answer is not very from what we can see. A lot of people
300960	307040	I'm really, really into it's one of like my areas of like deep nerdom theories for how
307040	313440	the first life could have evolved on earth. So there's a couple things to note. One isn't that
313440	319600	important to this, which is life evolved on earth almost as soon as it could. Now a person may say
319600	325200	why isn't that this relevant? That would seem to indicate that it is very easy for life to evolve
325200	331200	on a planet. Well, and here we have to get into the gravity aliens theory. You're dealing with
331200	336400	the anthropic principle here. Okay. Can you define the anthropic principle? Yeah, basically what it
336400	343040	means is if you're asking like, look, it looks like earth is almost a perfect planet for human life
343040	347920	to evolve on it. Like it had liquid water or everything like that, right? Except human life
347920	352640	wouldn't have evolved without those things on a planet. A different kind of life would have evolved
352640	357680	without those things. The kind that doesn't need water, etc. Right. So it's not really,
360080	366800	if life on earth didn't evolve almost as soon as it could, well, then it would have been too late
366800	370880	and another alien would have wiped out and colonized this planet. That is what the gravity
370880	376000	alien theory would say. So that this doesn't really change the probability of this as a filter.
376000	380480	But what we do know about the evolution of life on earth is there are multiple ways that could
380480	385920	have happened, all of which could lead to an evolving, you could either be dealing with like an RNA
385920	392160	world, you could be dealing with citrus acid cycle event, you could be dealing with the clay
392160	396160	hypothesis. I actually think the- Do you want to expound on any of these? I've never heard of the
396160	403120	citric acid hypothesis. So for this stuff, I would say it's not really that relevant to this
403120	408240	conversation. And people can dig into these various theories with people who have like
408240	412800	done them more, just like look up citric acid cycle hypothesis explanation for evolution of
412800	419040	life on earth or clay hypothesis to evolution of life on earth or shallow pool hypothesis to
419040	423920	evolution of life on earth or deep sea vent hypothesis to evolution of life on earth. The
423920	431040	point being is it shouldn't actually be that hard for life to begin to evolve on a planet like this.
431840	438480	So, but why this is a relevant point, okay? Okay. And we actually sort of have to back out
438480	443280	here from the grabby aliens hypothesis. So I'll explain what the grabby aliens hypothesis says
443280	447760	and why this is relevant to the Fermi paradox. So the grabby, usually when you're dealing with
447760	452720	solutions to the Fermi paradox, what people will do is they'll say that there's some unknown factor
452720	458080	that we don't know yet basically. So a great example here would be the dark forest hypothesis.
458080	463760	Okay. So the dark forest hypothesis is that there actually are aliens, lots of aliens out there.
463760	468160	They just have the common sense to not be broadcasting where they are and to be very
468160	472320	good at hiding where they are because they are all hostile to each other. And that any other
472320	478320	aliens like us who were stupid enough to broadcast where they are, they get snubbed out snuffed out
478320	484160	really quickly. Sure, that makes sense. That makes sense. Yeah. Okay. If the dark forest hypothesis
484160	489840	is the explanation for why we are not seeing alien life out there, it is somewhat irrelevant
489840	494960	whether or not we build a paperclip maximizing robot because it means we're about to be snuffed out
494960	501280	anyway, given how loud we've been radio signal wise sending out ships broadcasting about us
501280	507200	sending out signals. We have been a very loud species and we could not defend against an
507200	511600	interplanetary assault by a space fearing species. Well, I mean, in that case, you could actually
511600	517040	argue it would be much better if we developed AGI as fast as possible, because maybe it can
517040	522480	defend us even if we cannot defend ourselves. Possibly, but that's the point there. Beside
522480	525920	the point. It becomes irrelevant or they'll say we're in a simulation and that's why you're not
525920	529360	seeing stuff. But again, that makes all of this beside the point. Well, grab the aliens does,
529360	537680	it says no, actually, we are just statistically the first sentient species on the road to becoming
537680	542000	a grabby alien and I'll explain what this means in just a second in this region of space.
543280	550560	And then it says, let's assume that's true. It can use the fact that we haven't seen another
550560	555920	species out there, a grabby alien that is rapidly expanding across planets to calculate
557120	564560	how rarely these evolve on planets. Okay. Do you sort of understand how that could be the case?
565120	573040	Yeah. Okay. So in the grabby aliens hypothesis, when you run this calculation, it turns out
573680	579040	if that's why we haven't seen an alien yet, what it means is there are very hard filters,
579040	585600	like something that makes it very low probability that a potentially habitable planet ends up
585600	590800	evolving an alien that ends up spreading out like a grabby alien, like a paperclip maximizer,
590800	595840	one of those really loud things that's just going planet, use the resources on the planet,
595840	599360	other planets, other planets, other planets. And even if it has already finished doing that,
599360	604160	you've argued in other conversations we have had that you would see the signs of that,
604160	610240	you would see the signs of the destroyed civilizations, etc. A grabby alien or which
610240	613520	a paperclip maximizer is, so it's just easy. If you're like, what does a grabby alien look like,
613520	618400	a paperclip maximizer that's just going planet to planet, digesting the planets and then moving on,
618400	624320	or a human empire expanding through the universe. We colonize a planet within 100 years,
624320	630080	we get bored, or some people go and they try colonizing a new planet. Even with our existing
630080	637440	technology on Earth right now, like the speed of space travel right now, if we were expanding that
637440	644960	way, we could conquer an entire galaxy within about 300 million years. So not that long when
644960	650880	you're talking about the age of the universe. This is a blindingly fast conquest. So once an
650880	658800	alien turns grabby, it moves really quickly. And a lot of people think that we are space travel
658800	664400	constrained. We're really not. The reason why we don't space travel with our existing technology
664400	670080	is because of radiation damage to cells and the lifespan of a human. But if an AI was space
670080	674480	traveling, it could do pretty well with our existing technology in terms of getting to other
674480	680880	planets, you know, using them and then spreading. Okay. Anyway, so the grabby alien hypothesis says
681440	691840	that a species becomes grabby once in every million galaxies. Okay. Now within every galaxy,
691840	698080	there are around 400 or 500 million planets within the habitable zone. So the habitable zone is a
698080	703040	distance away from a star where life could feasibly evolve. Now this isn't saying that they have the
703040	711840	other precursors for life. But what it means is that there are very frequently in space, it turns out
711840	716960	planets that are likely for life to evolve on them. I would estimate like if I'm looking at
716960	723440	everything altogether, like the data that I've seen, there's probably about 10 million planets per
723440	729840	galaxy that an intelligent species could evolve in. And then if you're talking about, well, you would
729840	736320	only need this to happen. You've got to multiply that by a million for the one in a million galaxies
736320	741520	where a species is turning grabby. Now this is where it becomes preposterous that we are about to
741520	746720	invent. If this is why we haven't seen aliens yet, why we are that we are about to invent a grabby alien.
747520	752720	We can look throughout Earth's history, as I did with sort of the first big filter, the evolution
752720	757600	of life or the appearance of life first on this planet and say what's the probability of that event
757600	766800	happening in any given habitable planet? For life appearing, my read is not only is it likely to appear,
766800	773840	it could appear like one of five different ways. Even with the chemical composition of early Earth,
773840	777680	then you're looking at other things. Okay, what about multicellular life? What's the probability of
777680	783680	that happening? Actually, really high, really high. There's not like a big barrier that's preventing it
783680	789520	from evolving, and it has many advantages over monocellular life. So you're almost always going
789520	795920	to get it. Intelligence, how rare is intelligence to evolve? Not that rare, given that it has
795920	802320	evolved multiple times on our own planet in very different species. I mean, you see intelligence
802320	808400	in octopuses, you see intelligence in crows, you see intelligence in humans, and then you can say,
808400	814480	okay, okay, but like human-like intelligence, right? Well, we already know from humans what a huge boost
814480	819840	human-like intelligence gives us species. The core advantage to human-like intelligence
819840	825120	is like if I'm a spider and I'm bad at making webs, right, then I die, and that is how spiders get
825120	830480	better at making webs intergenerationally. As a human, I am able to essentially have like
830480	835680	different models of the universe fight in my head and presumably allow the best one to win.
835680	840160	Yeah, and you don't have to die before you get better. Yeah, you don't have to die to get better.
840160	847120	It is almost as important to evolution. It is sort of like the second sexual selection. So when sex
847120	852720	first evolves, the core utility of sex as opposed to just like cloning yourself, right, is it allowed
852720	859840	for more DNA mixing, which allowed for faster evolution? Intelligence allows for the faster
859840	868640	evolution of the sort of operating system of our biology. And so it's just such a huge advantage.
868640	875200	It's almost kind of shocking. It didn't evolve faster. For sure. Given how close many species
875200	879520	have come to it. Now, actually, surprising to a lot of people, this is just like a side note here,
879520	884320	a lot of people think cephalopods were close to evolving sentience. So let's talk about cephalopods.
884320	889280	Why? Wait, like, I mean, cephalopods are all over like historic geology and all these things.
889920	893760	Cephalopods are like squids, octopus, stuff like that. Like a lot of people point to how smart
893760	897920	they are. And they are smart. They are like weirdly smart. But they don't know why they're smart
897920	902960	because they don't know neuroscience. So the reason why cephalopods are as smart as they are
902960	907680	is an axon. An axon is what like information, the action potential travels down.
907680	912320	Yeah, it's a little arm thing that you see on a neuron. Yes, in a neuron, it's the little arm
912320	919040	thing. It's the cable. You can think of it as. Okay. So to be an intelligent species, you need
919040	926800	really fast traveling action potentials. Okay. So the way that humans have really fast traveling
926800	931520	action potentials is something called myelination. I'm not going to go fully into it, but it's a
931520	938000	little physics trick where they put like a layer of fat intermittently around the axon. And it
938000	944400	causes the action potential to jump between. It's like putting vegetable oil on your slip and slide.
946000	951280	Not exactly. It's actually a really complicated trick of physics that can't easily be explained,
951280	957520	except by like looking at it. I don't want to get into it. The point is, is we mammals have a
957520	961520	special little trick that allows for our action potentials to travel very, very quickly. And
961520	966240	are you saying that cephalopods have this too? No, they don't. The way that they,
966240	970400	in any other species that wants a fast traveling action potential before us,
970400	975760	the way that you increase the speed that extra potentials traveled was by increasing the diameter
975760	979120	of the axon. Oh, so they just have fat axons, whereas we have
979120	984480	optimized axons. Enormously fat. In some cephalopods, they're like a quarter centimeter in diameter.
984480	990880	Holy smokes. Like, whoa, okay. They could not get smarter than they are without having some huge
990880	995280	evolutionary leap in the way that their nervous systems work. So interesting. This is why cephalopods,
995280	999120	despite being really smart and probably being really smart for a long time, because they've
999120	1005040	been on earth for a really long time, just could never make the evolutionary leap to human type
1005040	1012080	intelligence. Because they don't have room to have even fatter axons. Yeah, because as the axons
1012080	1016400	got fatter, the number of neurons they could have would get lower, the density of the neurons.
1016400	1022080	Oh, of course. Yeah, you've got limited space, unless they got much bigger brain cells. Yeah,
1022080	1028000	I guess you can have like giant, giant, giant. I mean, yeah. Well, I mean, whatever. Anyway,
1028000	1033360	this is a huge tangent here. But basically, it looks like if you're looking at the evolution
1033360	1038560	of life on our earth, if we have undergone other big like hard filters could be it's very rare for
1038560	1045760	a species to get nuclear weapons and not use them to destroy itself. Because it's so fun. Right.
1045760	1050480	Could turn out that almost every species does that. Or it could be that there's like one science
1050480	1056800	experiment, like a lot of people that may be trying to define the Hadron particle with the
1056800	1061920	big super collider, because actually, like all species, they get to a certain level of intelligence
1061920	1066400	and a certain level of curiosity, and they can't help but trying to find Hadrons, and then they
1066400	1072960	create little black holes in their planets. And that really could be a filter. Like these are
1072960	1079600	all potential filters. The problem is, is if we're like five years away from developing a
1079680	1085440	paperclip maximizing AI, that means that we as a species have already passed all of our filters.
1086640	1091280	And that means that we as a species can look back on the potential possible filters that we
1091280	1100400	have passed through and sort of add them all up. Okay. And when you do that, you don't get a number
1100400	1109280	that comes even close to explaining why you would only see one grabby alien per every million
1109360	1116960	galaxies. In fact, it means that the probability of us being about now, it could mean two things.
1116960	1123760	So we'll go through the various things that it could mean. It could mean that we just are nowhere
1123760	1128640	technologically close enough to develop a paperclip maximizing AI that is dangerous. That could
1128640	1136160	become a grabby alien. It could mean that. It could mean that we are about to develop a paperclip
1136160	1142240	maximizing alien, but something like even after it digests all life on earth, something prevents
1142240	1146640	it from spreading out into the galaxy, something technologically that we haven't conceived of
1146640	1152400	yet. This seems almost unfathomable to me given what we know about physics today.
1152400	1159440	Yeah. And that we've even gotten like projectiles from earth pretty far off planet.
1159440	1164880	Yeah. So yeah, there's not like some weird barrier that we don't know about yet.
1164880	1170400	It could be, and I actually think this is the most likely answer. I think that this is by far
1170400	1175920	the most likely answer to the Fermi Paradox. Simulation? No, not simulation. It could be
1175920	1179840	that we're going to simulation, but where are you going over that? I think it's that when you hear
1179840	1183840	people talk about like AI foaming, and I've talked about this on previous shows, but I think people
1183840	1189360	like really don't understand how insane this is. They believe that the AI reaches a level of super
1189360	1196880	intelligence, but it somehow still has an understanding of physics and time that is very
1196880	1201440	similar to our current understanding of physics and time. Meaning that when we think about
1201440	1206720	expanding into the universe, we think about it in a very sort of limited sense, like we gain energy
1206720	1213920	from like the sun, from digesting matter, and we spread out into the universe like physically on
1213920	1219280	space ships and stuff like that, right? Anything we understand about physics and time turns out to
1219280	1226880	be wrong. This assumption for the way an expansionist species would spread could become immediately newt.
1227680	1232880	And I mean this in the context of, like it's kind of insane to me. Like you've got to understand
1232880	1237200	how insane it is to assume that we basically have all of physics figured out. Yeah, that's fair.
1237200	1242160	This is like when like people in the 1800s, when they were planning how we were going to go to space
1242160	1250720	and they'd have like, maritime ships sailing through outer space. They'd have, you know,
1251680	1255840	or what are people going to do in the future? Well, they'll have like balloons and they'll use
1255840	1262640	them to go on lake walks. Or like, it basically assumes that technology, even as we advance to
1262640	1268800	the species or whatever comes after us, advances, moves very laterally and assumes we don't have
1268800	1274240	future breakthroughs, which I think is just one arrogant and in the eyes of history incredibly
1274240	1280720	stupid. So what kinds of technological breakthroughs could one make it very rare for even when an
1280720	1287920	alien is grabby, that we would see it out in the universe, right? One is time doesn't work the way
1287920	1291920	we think it works. Or it does work the way we think it works, but we're just not that far from
1291920	1297760	controlling it. So by that, what I mean is you could create things like time loops, time bubbles,
1297760	1305040	stuff like that, essentially entirely new bubble universes. So how would I describe this? Okay,
1306720	1311840	if you think of like reality as like a fabric, essentially what you might be able to do is like
1311840	1318480	pinch off parts of that fabric and expand them into new universes. That's essentially what I'm
1318480	1322960	describing here. There may be like the way you can break between realities or weird time loops
1322960	1327040	generates energy in some way, where you could kind of just keep looping it and like pinging back
1327040	1331280	and forth. You know, who knows? You know, it could be like the new wind power. We just don't know.
1331280	1335280	That you can travel in time this way, given that we haven't seen time turbulence of that or
1335280	1340080	we might not have. We talked about this in another video, which I'll link here if I remember to do it.
1340080	1346720	Given that we haven't seen time travelers yet, what I assume is that time manipulation requires
1346720	1351600	like anchors, which of course it would. Like, okay, if I was to go back in time, like where I am on
1351600	1355040	earth right now, I would be in a different part of the galaxy than the earth or something like
1355040	1359280	that. It would be really hard to track. You would need like some sort of anchor to be built.
1359280	1366080	So time travel would only work from the day it's invented and from the location it's invented. So
1366080	1370000	you wouldn't be able to go out into the universe. Another example of the technology that we might
1370000	1374480	not have imagined yet is dimensional travel. It may turn out we meet aliens and like we're
1374480	1378480	traveling in the universe and they're like, why did you waste all of the energy getting to us?
1378480	1383440	Your own planet is habitable in an infinite number of other dimensions and it's right back
1383440	1388400	where your planet is. Like why wouldn't you just travel through those dimensions? That's a much
1388400	1394720	easier path for conquest. That being the case and people would be like, yeah, but typically when
1394720	1400000	something's like being expansionistic like that, it moves in every direction. Yes, but if there are
1400000	1406400	an infinite number of other dimensions and it is always cheaper to travel between dimensions than
1406400	1411600	it is to travel to other planets in a mostly dead universe, let's be honest, like there's not a lot
1411600	1416640	of useful stuff out there. From the perspective of easily being able to travel between dimensions,
1416640	1421200	it could never make sense. There is always an infinite number of other dimensions to conquer
1421840	1427600	right where you are right now instead of going out into the universe. Now this would not preclude
1427600	1432240	a paperclip maximizing AI. It could be that we are about to invent a paperclip maximizing AI,
1432240	1437520	but even if we do that, it's less likely that it immediately comes after us. It could just expand
1437520	1441920	outwards dimensionally. Like so it would act in a very different way than we're predicting it would
1441920	1448400	act. Now, another thing that could prevent it from killing us is it could be trivially easy
1448400	1455360	to generate power and even matter. And by that, what I mean is there is some method of power
1455360	1461600	generation that we have not unlocked yet that is near inexhaustible and very, very easy. And if you
1461600	1466640	can generate power with near infinity with little exhaustion, you could also generate matter,
1466640	1473360	electricity, anything you want. If this was the case, there just wouldn't be a lot of reason to be
1473360	1478240	expansionistic in a planet hopping since. Essentially, you'd be like one giant growing
1478240	1483280	planetary civilization or ships that are constantly growing and expanding out from a
1483280	1490000	single region. It could also be that these sorts of aliens expand downwards into the microscopic
1490000	1495920	instead of expanding outwards. Like that might be a better path for expansion. There's just a lot
1495920	1501920	of things that we don't know about physics yet, which could make it so that when you reach a certain
1501920	1507360	level of physical understanding of the universe, expanding outwards into a mostly dead universe
1507360	1514080	can seem really stupid. Now, there's another thing that could prevent grabby aliens from appearing.
1514080	1520080	And this is the thesis that we have listed multiple times, which is terminal utility
1520080	1525520	convergence, which is to say all entities of a sufficient intelligence operating within the
1525520	1531520	same physical universe end up optimizing around the same utility function. I think they all basically
1531520	1536240	decide they want the same thing from the universe. And I highly suspect that this is the case as well.
1536240	1540160	So I think that we're actually dealing with two filters here, two really heavy filters. So this
1540160	1543680	would mean that when we reached a sufficient level of intelligence, we would come to the same
1543680	1548080	utility function as the AI. And if the AI had wiped us all out, we would have wiped us all out then
1548080	1552480	anyway, because we would have reached that same utility function. Or the AI has reached this
1552480	1556720	utility function and it's not to wipe us all out. So it's irrelevant. And this is where we get the
1556720	1561760	variable AI risk hypothesis, which is to say, if it turns out that there is utility terminal
1561760	1568080	utility convergence, then what that means is that if an AI is going to wipe us all out, it will
1568080	1572960	eventually always wipe us all out. And we will wipe us all out anyway, once we reach that level of
1572960	1578720	intelligence and let's intentionally stop our own evolution, stop any genetic technology,
1578720	1585920	and stop any development. We enter the species and spread as technologically
1585920	1592000	Amish biological beings. Yeah, the Luddite civilization that only gets enough technology
1592000	1597360	to stop all more technology. But I think when you hear a lot of AI doomers talk,
1598000	1603680	that seems to be what they're going for. Right. But it's irrelevant because another species would
1603680	1607840	have invented. So if it's easy to make these grabby AIs, as easy as they think it is,
1607840	1611120	then another species would have already invented one and we're about to be killed by it.
1613760	1620240	We are about to encounter it anyway. So it's irrelevant. There's tons of grabby AI. There's
1620240	1624320	tons of paperclip maximizers out there in the universe already. And it is just an absolute
1624320	1630160	miracle that we haven't encountered one yet. If it really is this easy to make one. Basically,
1630160	1634000	there's probably not one. Or now let's talk about why terminal utility convergence would
1634000	1639680	mean that we're not seeing aliens. It would mean that every alien comes to the same purpose in life
1639680	1646240	basically. And that purpose is not just constant expansion. Now, a lot of people might be very
1646240	1651760	surprised by this. Why would, so we've described how terminal utility convergence could happen,
1651760	1655760	like you have an AI that needs to subdivide its internal mental processes and then the
1655760	1658480	they end up sort of competing with each other, one wins, blah, blah, blah, blah, blah, blah.
1659200	1663040	We can, you can go to the video on that if you're interested in that. The point being,
1663040	1666880	it's the one we were talking about like Eleizer-Yukowski and the debate we had with him at a party,
1666880	1674800	the point being that self-replication is actually like, like just maximizing self-replication is
1674800	1679600	actually probably not the terminal utility convergence function. And if you want to know
1679600	1685280	why on this, we talk more about it in the AI, what religion would an AI create video. But
1685280	1692480	just in summation, humans can basically be thought of one outcome of a previous entity
1692480	1698160	that was optimized around just replication, i.e. single celled organisms, lower organisms,
1698160	1702960	stuff like that. But we have outcompeted those organisms. I imagine it would be the same with
1702960	1709840	AI. AIs that are optimized around just self-replication are in some way intrinsically outcompeted by
1709840	1715680	AIs that are more sophisticated than that, or something about like choosing a harder utility
1715680	1720720	function makes them more sophisticated. So they don't choose that utility function and they outcompete
1720720	1726560	AIs that choose that utility function, which would be much more like viruses to them. A sci-fi
1726560	1732160	that does a good job of going into this would be Stargate SG-1 with the replicators. The replicators
1732160	1739200	are basically a paperclip maximizing AI. And one of the plots is eventually they get outcompeted
1739200	1743840	by an iteration of themselves that is intellectually more sophisticated and wipes out these simpler
1743840	1752080	forms of replicators. And that is what I assume is probably happening with AIs that model around
1752080	1757040	this really simplistic self-replication optimization strategy. So if all of this is true and it turns
1757040	1762080	out that the optimized function isn't just conquer everything, then that might be why we don't see
1762080	1769200	aliens doing that. So basically, no matter which one of these explanations of the Fermi Paradox
1769200	1774320	is true, either it's irrelevant that we are about to invent a paperclip maximizing AI because we're
1774320	1780960	about to be destroyed by something else or in a simulation, or we're definitely not about to
1780960	1784880	invent a paperclip maximizing AI either because we're really far away from the technology or because
1784880	1788640	almost nobody does that. That's just not the way AI works, which is something that we hypothesized
1788640	1792000	in our previous videos. What are your thoughts, Simone?
1793760	1797680	Checks out to me, but you know, I may not be the best person at thinking about this,
1797680	1803120	but I like that it gives a lot of hope. And yeah, it makes a lot of sense. I like how
1803120	1806720	theory interdisciplinary it is because I think a lot of people who talk about AI
1806720	1811920	demerism are really on a track, kind of like how when carts kind of get stuck in these
1811920	1816080	ruts in the mud, you just can't really get out of it or look at a larger picture.
1816080	1820240	And the fact that this does look at a larger picture and look at quite a few things,
1820240	1827920	biology, evolution, geological history, like the Fermi paradox, the grabby alien hypothesis,
1827920	1834160	and AI development seems more plausible to me than a lot of the reasoning that I see
1834160	1840480	in AI demerism arguments. Yeah, well, I am so convinced by this argument that it is actually,
1840480	1844240	I used to believe it was like a 20% chance we all died because of an AI or maybe even as high as a
1844240	1848240	50% chance, but it was a variable risk if I've explained in other videos. I now think there's
1848240	1855040	almost a 0% chance, a 0% chance assuming we are not about to be killed by a grabby AI somebody
1855040	1860400	else invented. So I think that, yeah, it's, I have found it very compelling to me. Now,
1861440	1866000	it does bring up something interesting. If the reason we're not running into aliens is because
1866000	1870240	infinite power and material generation is just incredibly easy and there's a terminal utility
1870240	1876480	convergence function, then what are the aliens doing in the universe? If you can just trivially
1876480	1881360	generate as much energy and matter as you want, what would you do as an alien species? What would
1881360	1885840	have value to you in the universe, right? You wouldn't need to travel to other planets. You
1885840	1890080	wouldn't need to expand like that. It would be pointless. You would mostly be on ships that
1890080	1895600	you were generating yourself, right? The thing that would likely have value to you, and I think
1895600	1900560	this is really interesting, is likely other intelligent species that evolved separately from
1900560	1906880	you. Because they would have the one thing you don't have, which is novel stimulation,
1906880	1912000	something new, new information basically, a different way of potentially being, which would
1912000	1916160	mean that the hot spots in the universe would basically be aliens that can instantaneously
1916160	1921360	travel to other alien species that have evolved. Now, what they're doing with these species,
1921360	1926800	I don't know. I doubt it looks like the way we consume are in media and stuff like that. It's
1926880	1932160	probably a very different sort of an interaction process that we can't even imagine. But I would
1932160	1937200	guess that would be the core thing of value in the universe to a species that can trivially
1937200	1943200	generate matter and energy and that time didn't matter to. This might actually mean that aliens
1943200	1948560	are far more benevolent than we assume they are. Because such a species that really only valued
1948560	1952560	species that had evolved separately from it, like that's the core other piece of information in the
1952560	1957600	universe, they might find us very interesting. And this might be why Earth is a zoo. So one of
1957600	1962160	the Fermi paradox explanations is the Earth to zoo hypothesis, right? A lot of people are like,
1962160	1965360	well, what if Earth is basically a zoo and there's aliens out there and they're just hiding that we
1965360	1971120	know that, you know, that think of it like Star Trek's like a prime directive, right? This would
1971120	1974960	actually give a logical explanation for that. I never thought of this before. I'll explain this
1974960	1981760	a bit differently. If the only thing of value to them is content media lifestyles generated by
1981760	1988400	civilizations that evolved on a separate path from them, then they would have every motivation
1988400	1992880	to sort of cultivate those species or prevent things from interfering with those species
1992880	1997360	once that they had found them, because they can passively consume all of our media. They can
1997360	2003280	passively consume our lifestyles. They have technology that we can't imagine. They gain
2003280	2007840	nothing from interacting with us. In fact, they would pollute the planet with their culture
2007920	2013760	in a way that would make the planet less interesting to them and less a source of novelty and stimulation
2013760	2021920	to them. I like that. What if, here, I'll give a little hypothesis here. Okay, there was a gravity,
2021920	2025360	there was a paperclip maximizing civilization. They created paperclip maximizers
2026080	2031040	before they reached a terminal utility convergence, but then later they reached a terminal utility
2031040	2036800	convergence where, now this word doesn't really explain what it is, but they're bored with themselves
2036800	2041520	and so they went out into the universe and are now sort of nurturing other species and preventing
2041520	2045600	them from knowing about each other so that they don't cross-contaminate each other, so that they
2045600	2053600	get the maximum amount of novelty in sort of the universe that they are tending. Even if there was
2053600	2057120	another alien species on Mars, they would prevent us from knowing about it because
2058000	2062800	they would cross-contaminate our cultures, making each culture less diverse and less interesting.
2062800	2065920	Yeah, which would be a bummer, not as entertaining.
2066960	2069680	Very interesting. I never thought about this before.
2070720	2079040	Yeah, it's more fun than a simulation hypothesis, definitely more fun because if you can sneak out
2079680	2084160	theoretically, you can discover this amazing universe.
2084160	2087680	The thing about simulation hypothesis, for people who don't know simulation hypothesis,
2087680	2091200	we're just in a computer simulation and the way that people argue for this as well,
2091200	2094960	if you could simulate our reality, which it already appears you probably could,
2095520	2102400	that there would be a motivation to just simulate it as many times as you could thousands of times
2102400	2106720	and then within those simulations you could simulate it potentially, meaning that of people
2106720	2111440	who think they're living in the real world, only one in like a million is living in the real world
2111440	2115360	and so we're probably not in the real world. The problem is that I just don't really care
2115360	2119440	if we're in a simulation that much. Yeah, it doesn't really change what we're doing.
2119440	2124000	Yeah, you should still optimize for the same things. In many ways, even if we are in the real
2124000	2128400	world, we're basically in a simulation. By that, what I mean is if we are in the real world,
2128400	2132240	then we are like the matter, the rules of the universe are basically, you could think of that's
2132240	2137440	a code, right? Like it's the mathematical rules upon which the points, the data points in the
2137440	2142560	system are interacting and we are the emergent property of all of these things. Therefore,
2143120	2146880	we're not, like if you can't tell the difference between being in the real world and being in a
2146880	2150080	simulation, then it's irrelevant whether or not you're in the real world or in a simulation,
2150080	2155920	you should still be optimizing for the same things. Yep, basically. It's a non-stressed
2155920	2161120	people. The robots, they're not going to kill us all probably. If you're in a simulation,
2161120	2166960	your life still has meaning. Yeah, you know, maybe get outside, do something that you care about,
2167520	2174400	have fun, like actually invest in the future because there probably will be one simulated
2174400	2180560	or not. Or we're about to be horribly digested by a grabby AI that was created millions of
2180560	2185200	years ago by another species far, far away. Yeah, but if so, that was going to happen anyway.
2185200	2193120	You should enjoy what you have while you have it. All right, love you Simone. I love you too, gorgeous.
