start	end	text
0	7760	Hello boys and girls. In this video I want to talk about the line of research that I
7760	17600	only came across last week, namely the very strong connection between quantum fields and
17600	27800	neural network theory. And what emerges from that is tools from one field being applicable
27800	34520	to the other. So you can then use let's say Feynman diagrams to find stuff out about
34520	40960	neural networks. And in the other way around you can use neural networks to compute stuff
40960	49280	for quantum fields. I think the most senior researcher, if I'm not mistaken from this
49280	58200	list of this paper in particular is James Harvorsson. And if you look it up there have
58200	68160	been a few papers in this direction in the last years. And nonetheless I chose this particular
68160	76800	paper because it's clearly some of the more advanced results there. And to motivate how
76800	86280	things work, I'm also in this video going to explain some results which were known for longer
86280	93280	time. So there I suppose it's fair to say that this sort of stochastic result for large neural
93280	100840	networks that we are going to discuss emerge in the mid 90s. And I'm going to explain this because
101800	112680	I want these subjects which I find very exciting actually to be known also a little better. In
112680	118600	this video however I will not go into any deep mathematical analysis. I have also not written
118600	129080	down much. I will basically jump from tab to tab. And nonetheless give a sensible explanation of
129080	134200	these things. So I think everything that I say should make sense in principle. And then you can
134200	140520	delve into all the subjects on your own. If you're actually interested in doing something in this
140520	149800	direction let me know in the comments. I will also point to some Google libraries regarding neural
150040	157880	kernel theory that I will also sketch out in this video. And yeah so that's that.
159320	164360	The requirements for the video are that you have like a basic understanding of deep neural networks
164360	172520	like the fact that these neural networks encode the parameterization of certain functions. And then
172520	177000	if you have a big enough network stuff like universal approximation theorem the fact that
177000	184520	you basically can represent functions in the space let's say continuous functions densely.
185080	192600	And it helps if you have a basic understanding about the ideas of quantum mechanics. So the
192600	200120	fact that what you're interested in is transition probabilities and that they are expressed as some
200120	210920	products in a Hilbert space. In this video I have a section where I motivate the jump from the
210920	217720	expressions that we are going to be able to compute with neural networks as they are explained in
217720	226200	this paper. Let me scroll down a second. How these expressions connect to scattering amplitudes in
226200	234440	let's say some large hydrogen collider experiments. These sort of stuff so I have a small like wake
236280	243800	mafia section that concerns physics. But my main goal is that if you go away from this video
243800	249240	and have a vague understanding why these sort of expressions that you see on the screen right now
249240	256120	are both relevant for neural network theory and for quantum field theory then my job is done.
256840	264040	As I said this gives the possibility not just to compute stuff in physics but for example
264040	273880	you might then be able to apply Feynman diagram calculus to compute various aspects of neural
273880	279560	networks as well and so even if you edit from a purely computer science perspective
280440	286040	and have some statistics background then this might be fairly helpful.
287320	295800	Okay so I have here just a bunch of bullet points that I want to go through. I might come back to
296600	303800	this from time to time otherwise I will just jump through like here 15 tabs or so and explain some
303960	313080	results. I am actually currently working on or started working on a video that I maybe want to
313800	323240	make as my some for summer video where I will do like a painful analysis of the universal
323240	328440	approximation theorem. But that's like months and months out. In this video I'm basically just
328440	338280	rambling. I hope you don't expect to neither deep or concise elaboration so I'm warning you already
338280	349240	but nonetheless I would really recommend that you listen up. Okay so first off as you have just
349240	359960	seen in the bullet points I will explain to you like sketch out this result from the 90s which
359960	366680	concerns neural network Gaussian processes. So there are nice results that have been found there
366680	373480	and have since been extended to a broad range of neural network architectures. So this is
373560	380200	mathematical theory results for neural networks that hold in general. In this video for simplicity
380200	387240	we can just look at fully connected in the sense that you see in the screen here feed
388040	395320	forward neural networks and for this video it's not even super relevant how many inputs
395320	401400	outputs you have. Basically you have let's say at least one input some float or if you want a real
401400	409080	number x that goes in one real number y that goes out and in the middle you have a bunch of
409080	414920	hidden layers in the image you just see one but you know for the sake of it you might think of two
414920	424520	three four something like that and you see the nodes in this one hidden layer here and the
424520	430520	theory that we are going to discuss kicks in once you have a really big network. So this
431880	439320	you think of the number of nodes in each hidden layer here going to infinity or you know it will
439320	447160	suffice if you think of a huge number a bunch of billions of billions and the thing that then
447160	455000	emerges with large networks is not just the universal approximation theorem that says
455560	461560	the the nice functions let's say continuous functions from r to r are represented densely
461560	471800	by this sort of neural network by these weights but what also emerges in this large network limit
471800	485400	is that the dependency of the output for fixed input and probabilistic weights
486920	492520	takes on a very deterministic character okay so this is this neural network Gaussian process
492520	498280	phenomenon and then I will explain in a second in more detail but basically what we want to
498280	504520	hear first look at is we take one fixed architecture some big neural network with let's say three
504520	512120	hidden layers and all the layers are very large and what we're here are first concerned with
512120	517640	before we talk about tangent kernels before we talk about quantum field theory is we few
519400	525000	we're concerned with the random initialization of these networks right so let's say you are on a
525000	536520	computer you you have this network encoded on your GPU or whatever and or then the weights really
536520	544760	I mean the you have the architecture laid out the way in which all the the the float data
545720	554680	pass to each other naturally if you have this this float types in every realized configuration
554760	562600	the weights have to have some some value and this gives the start configuration for the
562600	568600	learning process right in the learning process you're going to probably assign some some
569400	574840	loss function and you do gradient descent and then you tweak the network to behave in a certain
574840	582120	nice way and fulfill some task but to start this process you need to initialize the network you
582280	589320	want to maybe explicitly set some weights okay and now what you can do is you can play around with
589320	595320	what is actually your starting condition right you can say hey shoot all the weights in the beginning
595320	603480	be set to zero or be set to one or and this is the interesting thing here you do a random
603480	612280	initialization of all the weights and biases also so think of you know you're in python you
612840	617960	take a library and you sample from a normal distribution for all the
619720	626440	trillion weights you sample trillion random numbers and you sample them each from a Gaussian
627240	636040	from a bell curve and then set the corresponding weights like this okay so all these w i j are
636040	645080	sampled from some from some Gaussian and when you like put in some input right we said there's one
645080	655800	input let's say you you take the input seven and set x to seven and then feed do the feed forward
655800	663240	process the evaluation of the neural network then if these things are random then the output will
663240	669400	also be some essentially random number it will be determined for whatever random weight you have
669400	678600	sampled here and in this way you can think of y for fixed x as a random variable composed of the
678600	688760	random variables w right so we have here's the neural network Gaussian process page the math
688760	694600	and the proof sketch of this result that we are going to get to is actually described there so the
694600	701400	weights are sampled from some Gaussian we are going to take a Gaussian where the standard deviation
702280	707080	gets tighter and tighter with the number of layers right but if you have a fixed network this is some
707080	718360	fixed variance here and the output that is in a standard way computed from neural network is
719080	728920	um computed as you see here you do fast forward and I think it's fairly easy to believe that just
728920	734440	due to the central limit theorem right the statement that if you have a bunch of independent
736280	743480	random variables if you sum them all up then this is another random variable that will behave
743480	750040	like a Gaussian process right so basically if you sum up random numbers then you usually end up with
751720	755240	if all the conditions are fulfilled all the mathematical conditions then you will end up
755240	759480	with a Gaussian this is the statement of the central central limit theorem and this exact
759480	764120	thing applies here also you know maybe there's some non-linearities involved and maybe there's
764120	771240	different steps but in the end the final output of the network here in this picture in the on the
771240	778840	last layer this set is still a function of all these small Gaussians and because there are so many
778840	787240	sums this is again just a Gaussian process right so and so this says that in the limit and this
787240	795160	is especially emerges if you have enough width if the width is big enough so that the central
795160	801400	limit theorem really kicks in but this basically means that the as a random field as a random
801400	808040	variable the output of the neural network has very nice stochastic process properties and it's
808040	814440	it's a Gaussian process in particular one of the nicest you can have basically here on this
814440	825160	web page on this Wikipedia page there's also like this this this example animation so here they have
825720	830440	some network with three inputs right as I said it doesn't have to be three it suffices to think of
830440	836520	one and they have a bunch of outputs again it suffices to think of one so one green input one
837240	847160	yellow output and a bunch of nodes in in bunch of layers in this case two layers one layer would
847160	853960	also work you see on the right side I mean you probably don't see just because of my face here
853960	860680	but I mean doesn't really matter too much it's just a bunch of like random distribution the
860680	867400	statement is that for fixed input the green value again let's say there's one green input and it's
867400	877080	set to this the float number seven if you fix if you go up with the the number of nodes and
877080	883400	random initialize this with weights and biases then just by the central limit theorem which is
884120	892360	dependent on this this seven and this bunch of random numbers the output y1 here this yellow
892360	901400	output will behave like a Gaussian just by the central limit theorem and in this case there's
901400	909160	two outputs so you can draw a plot and the statement is then that both y1 and y2 behaving
909160	916840	like Gaussians independently from another like it's not a statistic statement but each behave
916840	923800	like a like a Gaussian they have some peak and so on the plot you get another nice Gaussian with
923800	934280	some peak here and so if then the press play again if the network becomes even bigger then you get
934280	942200	like this perfect Gaussian where it this just says that it has this maximum expected value here in
942200	950520	the middle and this goes for all the outputs right so this is the result that that that
953640	961560	the okay I closed the neural network Gaussian process page but doesn't matter this is the result
961560	969160	that just because of statistics you the network if it's large enough at random initialization
969160	977240	behaves like a Gaussian we will not need it for the this video but if you want to take a look
977240	982120	this is the the formal definition of a Gaussian Gaussian process I mean to motivate this basically
982120	986440	you think you know a Gaussian is something which if you do the Fourier transform it's again like
986440	993560	a Gaussian and the Gaussian process is abstractly defined as this random variable or sequence of
993560	998760	random variables where the characteristic function the expectation value of this phase
999480	1006760	is this Gaussian with a certain mean we are not going to need this the Fourier transform will
1006760	1014840	pop up again when I talk about the quantum field theories but suffice to say the nice thing is
1014840	1021160	that the neural network the big neural networks behave like Gaussian processes sorry if I repeat
1021160	1031960	myself okay so do we do physics first or do we do neural tangent kernels first
1032440	1041400	um let me actually um yeah let me actually say something about
1043080	1051320	the neural tangent kernel so um could we know now that the the network at the start behaves
1051320	1057480	like this Gaussian for all inputs and if you do the learning process then this is about
1057880	1068440	um giving it a test data and then moving um in parameter space from wherever we random started
1069000	1076440	to some other position in in weight space and I have made a bunch of videos on gradient descent
1076440	1082760	I will not explain it here but suffice to say you have a space of weights and then the the weights
1082760	1089000	follow some path and you do that in a way that optimizes some goal that you have right some
1089000	1103640	task for the neural network and I think I sketched it out here so as is common we're dealing with
1103640	1108600	not only here with a large neural network so that the the theory becomes simpler and nicer
1108600	1114040	but also we are matching we have so much compute that we can do really small step sizes
1114040	1120040	so that we can then in the limit talk of the behavior of the network as in a differentiable
1120040	1131400	way where we say the the the motion of the in path space in a parameter space can be described
1131400	1139480	you know with literally just calculus differentials and so the gradient descent algorithm what we are
1139480	1146200	doing really is um you know as per instruction of the algorithm we say the change of the weights
1146200	1151960	and here I abbreviate all the weights together with this uh theta the the change in the weights
1151960	1157720	right from from one point to the next in the graphic that you just saw um is chosen in a way
1157720	1167960	that it takes the negative direction of the gradient of some cost function and the cost
1167960	1175400	function in here is the you know the the difference essentially between what the neural network
1175400	1182520	currently says versus where we want to get at where set is all the learning data that we have
1182520	1189000	available right so we say for all the learning data that we that we have um there is a discrepancy
1189000	1196840	between what the network f currently um says um what's correct and what is actually correct
1197400	1204200	why I said here is what's actually correct um and we send that up and so this is like the the this
1205240	1210760	cost of all learning data together and at every step in time in the learning process
1211480	1218680	we go follow this path right and this equation is really just the Newtonian equation um where
1219880	1224360	you know in in physics um theta would be the momentum
1228520	1235400	and where you um look at the situation where the force on the right hand side is governed by a
1235400	1246360	potential and you'll say um the the direction of motion um captured by the momentum is given
1246360	1252120	wherever the you know potential energy will be lowest and that's where the path followed by
1252120	1260120	the particle in Newtonian physics right so this already looks very um like like this simple physics
1261080	1267480	uh equation governing governing the motion in weight space and now given that you have
1268040	1274120	the behavior of the um the the particle if you will uh going through weight space like
1274120	1282360	and give you just saw um and the the potential depends on the outputs at the network on all
1282360	1288360	these spaces you can also then um do the calculus and and look at hey how does the output of the
1288360	1294680	network which depends on the position where you are at right where the weights determine what the
1294680	1300440	network output will be on all the um learning data how does that the f change and so if you
1300440	1310440	do them just do the math um and I think I have this is here so this is um newer tangent kernel
1310520	1313400	theory um if you
1319960	1325240	if you do this sort of calculation and if you uh you know if you ever started physics you have to
1325240	1330440	this this sort of calculation a million times because basically if you have some observable
1330440	1336120	in a physical system and you know all the the constituents of the particles behave in this
1336120	1340200	isn't this way and then I have some observable which is made out of particles and you say how
1340200	1346760	does this this observable quantity change then um you have to plug in a bunch of partial derivatives
1346760	1351640	and then the Hamiltonian comes in and whatever and so on and so forth what comes out of this
1351640	1360520	is that the development of the output of the neural network um is governed by some matrix
1360520	1372040	this is the so-called newer tangent kernel and the changes of uh the the loss in uh with respect
1372040	1378760	to the to the weights right so I mean I did not adopt this terminology theta is again all the weights
1378760	1390120	together and um you can do this calculation on one sheet of paper yourself I'm not going to discuss
1390120	1397240	all the the convention or a notation chosen here but the point is that the evolution of the output
1397240	1404040	on a network from your starting point which might be a random starting point is understood at least
1404040	1408840	here in theory it's another question of whether you can actually calculate that because this matrix
1408840	1415080	which determines how this the output of the network evolves as you do the learning according
1415080	1420840	to gradient descent is determined by this complicated object theta and the theta is basically
1422040	1430840	this so-called kernel um this is you can view this as the inner product of the gradient of the
1430840	1435640	output with research with respect to the weight change and so the interpretation is basically
1436360	1447560	that um you look at uh different inputs and then you um you as a kernel a kernel roughly
1447560	1454360	charges how similar input data are and this kernel basically looks at uh hey these two
1454360	1462280	input data are similar if upon a change of the weights the um the response of the network changes
1462280	1467880	in a similar fashion and you compute this it's in a product in any case this is like an interesting
1467880	1477080	object that in the end determines how the network behaves um and similar to neural network Gaussian
1477080	1486920	process theory where you say once I have enough um uh weights in my layer some nice theory emerges
1486920	1493320	right in in the Gaussian network case it goes towards a Gaussian it's also the case that
1494040	1501160	for a large network then these these matrix can simplify and then you can get the infinite
1502200	1507720	size network also to an analytical theory and basically you random initialize you already
1507720	1514520	know it's some Gaussian process and then you have some matrix which determines how the network moves
1515240	1522920	um through through weight space and thereby you you have an idea of actually what happens
1522920	1528360	during network to network training right so if you have never heard that and more or less followed
1528360	1535640	my explanation then you can see this is kind of cool that at least in this limits you have
1536200	1542920	sort of an analytical idea what happens during learning and then the question is to what extent
1543240	1550920	is this sort of logic valid for networks as we can implement them at the moment at the moment
1550920	1556200	because of course we have a lot of weights like billions of weights but it's not infinite so
1556840	1561720	you might ask to what extent is the analytical theory where these limits are taken right so
1562600	1570920	super small step size very large networks applicable to today's convolutional deep neural
1570920	1576920	networks and so on and so forth and this is basically a subject of study so this is something
1576920	1586440	where people still put a lot of time in and so for example you have here this google uh neural
1586440	1596440	tangents project which i might be interested in looking at and there's a bunch of google researchers
1596440	1601640	who are still using this and publishing papers in this and and so on and so forth there's also
1602520	1613400	i think a recent um new rips uh poster from 2019 where you get some of the examples of
1613400	1618360	analytical formulas that i talked about um just because i want to get to the quantum
1618360	1625480	field theory part i will not discuss this in detail but i um i hope my tangent pun intended
1626920	1631480	was interesting and as i said i would also actually like to to work a little bit in
1631480	1636680	this direction myself so if you want to look into that um feel free to reach out and we can
1636680	1651000	do some sort of project together okay so um now for the the the field theory part
1656440	1662920	but still extremely important for us is this neural network Gaussian process inside okay
1663640	1667160	and if i'm here in the paper on page four
1670280	1675800	then um let's make first some definitions okay so here we have these correlation functions
1676520	1683880	which we call g n uh for uh for a concreteness sake you can think of n uh let's say as two
1683880	1692520	so um we are going to actually look at um two different um forward passes for the neural network
1692520	1699320	right so you have two different imports that you want to try x and you plug them in and you
1699320	1708200	get something out of your current network which might be randomly initialized um and if you
1709560	1716680	as we had it with the neural network Gaussian process case if you your weights as a random
1716680	1723240	variable right over all the weights over all your trillion weights you put a little Gaussian and let
1723240	1733160	them wiggle a little bit um and you say what typically happens if i sample once and um
1735400	1745000	put into uh inputs x uh how are the inputs on a on a typical or random network correlated with
1745000	1752360	each other then you can you can try this a bunch of times and get an idea um what you can also do is
1754040	1757080	analytically if you know the distribution of where your weights
1757800	1762600	compute what what will come out right so you have here uh p over the weights this is the
1762600	1767960	distribution that you yourself chose from which you can sample and uh for fixed neural network
1767960	1778840	architecture um the weights and um the neural network um which um is here called phi this is the
1780680	1787080	function that depends on the weights depending on your architecture right so this is the sigmoids
1787080	1794520	and this sum and so on and so forth and so the correlation function gives you how this how
1794520	1802120	let's say two inputs are um correlated with each other for this network right and by the neural
1802120	1813240	network Gaussian process result if we said that uh this this billion uh probability distribution over
1813240	1820360	the weights um make the input output relation of the whole network also into a random variable
1820360	1827880	right so you can also view uh this the setup that you have here not as um as um
1829720	1836840	not just a sampling um the weights and getting uh then a fixed input output but you can also
1836840	1842440	view any sample process as sampling a whole neural network right even this is just what
1842440	1848280	you do if you sample if you random initialize for fixed architecture um the um
1850760	1855320	certain uh functions as your certain neural network then you've also sampled the neural
1855320	1861480	network from who knows what distribution right so there's also this different the different
1861480	1870520	view of this initialization process and then um by the result of the neural network Gaussian process
1870520	1876920	what you have is that you can also view the same uh exact object this correlation function or any
1877000	1885720	expectation value really um as in terms of distribution over the these functions themselves
1885720	1890520	and by the result that we just had for a large network this will be a Gaussian process and this
1890520	1897240	manifests in this way right so um here is the integral not over the weights but over the um the
1898200	1908920	uh whole function that the network represents itself and um the the um probability distribution
1908920	1919160	that you have in this case is um of of this sort where this s uh and now this relates back to the
1919160	1926440	formal definition of the Gaussian process is some quadratic let's say let me fill some local terms
1928200	1934760	cost associated with the whole um with the whole function so basically what what this does I mean
1934760	1938200	do they give here some examples I think they do um
1942920	1949320	okay so this is already sort of a physical example but what you have as s here in the the exponent
1949400	1957240	is some sum over all uh the values of the network and what what they like what in effect happens is
1957240	1970680	that um the um the the this this weight is such that um field configurations and when I say field
1970680	1974760	now I always just mean the same as the input output relation not given by the neural network
1975400	1983800	field configurations or neural networks where at one uh places you get a huge output these are just
1983800	1991880	exponentially suppressed because um if you random sample from the with the weights then you are not
1991880	2000840	likely to get um some basically you are going to get um neural networks in initializations
2000920	2007800	which are around some some uh some certain um typical expectation value like they have there are
2007800	2014840	some typical behaviors and everything that deviates uh a lot from it is like exponentially less
2014840	2022360	likely right you um if you do um thousand random initialization of the neural network you will
2022360	2028360	get some typical behavior and then you can cook up some other extreme behavior that is not likely
2028360	2036680	to happen and um by the result of the neural network Gaussian process um theory um it tells you
2036680	2043320	what the the the probability distribution looks for the neural network so there is this connection
2043320	2049480	that you have here right and as I will motivate um later uh in quantum field theory this is exactly
2049480	2058040	sort of the setup for the path integral and what this paper does is it um if you view um
2058360	2067080	the um this this sort of um mathematical um overlap um as a physicist and you want to use
2067080	2074760	neural networks as as as a physicist you you see this as a way to um then try to craft neural
2074760	2080520	network architecture and sampling techniques right the the way in which you sample the weights
2081240	2088440	in a way so that the this this whole process of random initialization corresponds to certain
2088440	2094680	s s certain actions here right you do you have some physical scenario in mind there's physical
2094680	2102040	theory that says oh you know uh certain scalar field theories have this and this action what is
2102040	2108920	the way I and in which I must set up a network and the way in which I must sample the weights
2109480	2114520	so that what I sample is exactly as if I would sample from a quantum field
2114520	2122280	like like as if I would sample from uh would I would sample a field which is one instance of
2123720	2131240	um a quantum field in the path integral formalism right and then I get out a bunch of correlation
2131240	2137720	functions this endpoint functions um and these are exactly the things that are uh what what you
2137720	2142600	do quantum field theory for right you you compute these g's and then I will explain it later then
2142600	2151960	there's some mathematical connections to how you get from this this this g's to um the scattering
2151960	2157480	amplitude or whatever your quantum field theory does um maybe particle physics solid state physics
2157480	2167400	and so on and so forth okay uh let me see so as you might notice this is a very free flow
2167480	2179080	sort of explanation right um so I have to check uh what I have touched upon and what I didn't
2183320	2187080	okay
2187720	2198200	yeah okay so um from the very complicated quantum field theory math you get some
2198840	2209000	you know relations of um how the correlation function must relate to these actions and there's
2209000	2216200	a bunch of stochastic differential equation mathematics involved and because in physics
2216200	2222680	the evolution is always governed by some Hamiltonian operator function there's a bunch of energy
2222680	2229880	terms that you have to kick around and that's why for example um these objects tend to look
2229880	2238280	a little bit like this um like if you're never studied this physics um maybe one way in which
2238280	2243800	you should look at this is that because the evolution of these fields like how they um
2244680	2253400	have often time is governed by the Schrodinger equation which relates the time derivatives
2254440	2262440	like the evolution of the fields themselves to some energy expression captured in the Hamiltonians
2262440	2271880	and the the energy kinetic energy and in particular for fields is given by some spatial operators
2271880	2277240	that's that's why these sort of objects pop up here and you know mass energy equivalents
2277240	2284440	that's why we also have mass and so if you see these Laplacian operators or mass terms um you
2284440	2289160	should not be too surprised uh there because in physics they just always pop up in relation to
2289160	2296200	the time evolution of the the fields okay so okay now I've already touched upon the concept of time
2296200	2305320	the thing is of course that um here this the fields as they pop up here will um be
2308280	2315640	not uh like what you get there is the better controlled theory of euclidean fields right
2315640	2322440	you're not you're not having to do a priori with spacetime metrics and all these things which make
2322440	2329880	field theory quantum field field extra complicated but um just talking about Gaussian processes is
2329880	2335000	just talking about stochastics and then there is this sort of bridge that you have to take
2335000	2343240	and hope that you can get from the the euclidean field theories to um to some actual quantum field
2343240	2352360	and I will just name drop um a bunch of concepts there the idea is that you uh if you approach
2352360	2360920	quantum field theory with this neural network she bang then um you want to find a neural network
2360920	2374120	which mimics the weak rotated version uh of um of physical quantum field and so there is um
2375160	2380440	a bunch of uh so-called constructive quantum field theory coming in so there are various approaches
2380440	2387240	for of people trying to um uh make certain aspects of quantum field theory more rigorous
2387240	2396520	and um transfer like get rid of uh pseudo metrics in quantum field theory move everything um to the
2396520	2406840	euclidean domain where you have nice metrics and um uh so in this paper for example they say that um
2406840	2412600	you know what we really want to impose is um neural networks which when you view them as a field
2412600	2417640	behave in a certain way and something that is important there for example are these uh
2417640	2425000	Osterwald Schrader relations so for example here in this case the um correlation functions um
2425000	2429960	these particular coordination functions of relevance here are called denoted s and then you see on
2429960	2435320	the screen a bunch of properties that these shall have right so there you have the physical
2435320	2442280	translation in variances of certain objects and certain symmetry or independence relations right
2442280	2448120	i'm just mentioning this that there is uh it's not like um you just take any neural network
2448120	2453720	and then you get some uh some quantum field theory in the path into word formalisms out of it
2453720	2461320	there's a fairly restricted subset of uh fields that the physicists for quantum field theory might
2461320	2469160	be interested in um okay but i should probably not go into too much detail on that uh here
2469240	2480760	um yeah uh also the um these objects in this um exponential so sorry here um
2482680	2487720	if you if you know some physics then you know this but um if you don't then just want to mention
2487720	2495640	that these sort of s s um basically any s that you can write down uh gives you some field theory
2495640	2506040	these s s are some sums or integrals over energy terms and if you go to um sorry for the click
2506680	2512120	clickery um if you go to the Lagrangian field theory Wikipedia page then you can find
2512840	2520200	a whole lot of different um s objects that make sense and you can see here see how they
2520200	2526520	data mine how the data mine um the various physical theories that you have certainly heard of
2526520	2531720	we are interested in particular about fields so we have here scalar field theories this is what we
2531720	2541160	just saw you have some partial and then um some um some mass and there will be also be some time
2541160	2549800	the derivatives there um but the thing is that the pure gaussians are um where this is just
2549800	2557080	this quadratic object in in s are actually relatively um uninteresting from the physical
2557080	2562680	perspective because if you have for example if you have different quantum fields that interact
2562680	2568680	with each other and and then this information how they interact is also all encoded in this
2568680	2575560	in this um in this Lagrangian cells or in the action s and then uh you will have some more
2575560	2583320	complicated products in these objects and um if you have some power of the field that's higher than
2583320	2592840	two then this is actually uh representing um sort of self interaction in in field theory so um what
2592840	2600280	we really want to have is not just um the fields which fulfill these nice um properties in the
2601000	2606920	Euclidean version but we also want to have very finely controlled um interaction terms
2606920	2616360	there and so what we really want to have is um processes which are actually not gaussian right
2617320	2623000	and so what they do in the paper is in in the end um look at
2623000	2634200	uh um five to the fourth field theory so quadratic interaction let me see sorry
2647400	2651320	so what they really want to implement and what they actually then do in the paper is
2651320	2656760	they take this sort of action you you not only have this quadratic term there but you also have
2657400	2669320	their um this five to the four uh term and to to get this in to get um uh away from the
2669320	2677240	just quadratic gaussian process scenario there is two ways um to implement this self interaction
2677480	2685720	uh and one way is to actually not look at the infinite and limit right not the infinite
2687080	2696520	size network limit um because then uh the you basically break the neural network gaussian
2696520	2705400	process scenario you you person person uh you purposely stop a little bit earlier and get some
2705480	2711320	non gaussian effects and so what from the mathematical point of view is a bug that for
2711320	2717240	a real network you actually don't get a perfect gaussian situation here becomes sort of feature
2717240	2723640	it gives you the freedom to actually implement behavior and if you tweak the network uh nice
2723640	2729160	enough the ideas that then you can sort of control how it is broken this bug certainly
2729240	2736280	becomes a feature this is one way and the other way is if you um actually um
2739080	2745880	in sampling you do not uh take a trillion independent distributions over the weights
2745880	2753560	but what you and instead do is you introduce on purpose some dependencies of the individual
2753560	2759480	weight uh distributions right do you break the independence such that um there is a little
2759480	2765720	bit of a flaw in the central limit theorem and in this way by independence you also get some
2765720	2771800	non gaussian process because it's clear that if you do only um break the independence of this
2771800	2777640	weights a little bit you'll still by the central limit theorem get something which is just a slight
2777640	2783320	deviation from discussion processes right so if you the idea is if you tweak the this
2783320	2788920	the conditions for the god for the central limit theorem just in a wide way then you might produce
2791480	2799000	errors to the the gaussianity in a in a in a very controlled way and this is exactly what
2799000	2806680	they do here so here they describe a neural network with particular non-linearity as activations it
2806680	2813800	looks like this and what they do is they sample in this um in this dependent way in exactly the
2813800	2825640	correct way so that the um the action um that emerges by random sampling in this you know
2826440	2834680	particular way represents uh this uh this sort of physical field theory so this field theory in
2834680	2840360	particular is basically always the first interaction uh interacting field theory or one of the first
2840360	2845000	interaction field theories that you would learn in the quantum field theory course um this is not
2845000	2855320	one of the famous standard model um energy densities at least not in the exact same way here
2856280	2861000	but nonetheless this is like classical physical theory and so this is what this paper is all
2861000	2868200	about right breaking the neural network gaussian process uh theorems in the correct way to get um
2869480	2878200	the the right uh g functions endpoint functions out there that are relevant for physics um and
2878200	2886040	so you see that then you can you know in principle sample uh from this fixed neural network architectures
2886040	2892920	fields and then once you have like a way of sampling fields you can in principle because
2892920	2897640	the fields that demand all the properties of the quantum field theory uh compute expectation
2897640	2903480	values and thereby get physical uh information so this is the idea it also goes in the other direction
2903480	2914040	in um since there is uh these methods in particular um Feynman diagram computation methods that compute
2914040	2923320	correlations um for quantum fields you also have a method of computing aspects of random
2923320	2929320	initialized big neural networks right you have a big neural network you know that if you uh
2932440	2939240	to sample from it it is as if you would sample from a quantum field um and because you have
2939240	2944360	ways in physics to compute aspects like correlation functions and so on of the quantum fields
2945000	2950280	these uh g functions that you can compute in physics with Feynman diagrams and so on
2950280	2959320	also have a relevant meaning for computing typical aspects of random initialized neural networks
2959960	2964600	okay i know this is a little bit much but i hope it sort of makes sense um
2964600	2976520	um i um i don't know how clear everything was that i discussed so far i just want to give you
2977960	2983240	this is then more on the quantum field theory side a little motivation that i have just here
2983240	2990040	written up um that connects these g functions right these autocorrelations to physics i just
2990120	2996840	want to motivate it maybe give me a three more minutes so i i'm going to assume that you have
2996840	3003000	an idea of the fact that transition probabilities are the squares of inner products in a Hilbert's
3003000	3010840	base in quantum mechanics um so there are these um kernel objects k which are given like that
3010840	3017560	so in in in you know standard quantum mechanics what you have is some um you have some in um
3018280	3022600	current state which i call here in and then you have some uh other state out that you are
3022600	3029000	interested in you want to know what is the chance that this state um transitions over in that state
3029000	3036360	what you do is you um take the Schrodinger equation uh evolution which is governed by the
3036360	3043880	Hamiltonian h in in the nicest case um here the formal solution of this equation is just e to the i
3044840	3052360	h so this is the operator which moves any state forward in time you apply it to in to the like
3052360	3059320	let's say this is now this is in two days you say this state that i have currently how will it
3059320	3066280	evolve into the in like how will it look in two days then you basically apply this operator to
3066280	3073640	fast forward this thing um and then you get something uh the in state how it will look like
3073640	3080280	in two days and then the product with this object in two days with the out state that you're interested
3080280	3087960	in gives you the probability that the the current state after it has evolved is going to be observed
3087960	3094520	in uh in the out state okay so this is basically quantum mechanics one one um the for whatever
3094520	3101880	Hamiltonian you have here you um can often then compute some analytical case some kernels
3104280	3109320	these are these things and i think here if you score down in this Wikipedia page you find some
3109320	3115160	examples so these are just you know in this case uh three particle in one dimension this is like
3115320	3125480	some Gaussian this is similar to some heat dissipation equation and then if you have some
3125480	3130840	more relativistic scenario i think they also write down here some this is more complicated
3132440	3142840	correlation functions and examples okay so that's the one thing now if we talk
3143560	3145000	quantum field theory
3151080	3151480	then
3158760	3165880	yeah the uh this is basically just a definition um the the g's we all also had that in the paper
3166520	3173480	are certain expectation values right so there since we're on the pure relativistic side
3173480	3179000	and i'll take your clean side there's some time operator there but you can basically ignore it
3179000	3184920	for for the moment the the point is that these g's are determined by some fairly simple looking
3185880	3190600	expectation values and in the in the quantum mechanical formalisms these are also some
3190600	3195400	inner products in some Hilbert space of course in a quantum field theory there is no super nice
3195400	3202360	theory um about uh Hilbert spaces and and and these operators so there is it's more like a
3203160	3209000	like a functional definition this is what you calculate and um you hope it's somewhere
3209000	3213720	else comes up with the sort of rigorous setting to compute this but nonetheless you know how to
3213720	3223320	compute these g's you know how to set up the an evolution equation of your choice and make
3223320	3227640	sense of these correlation functions which is um what you want to get at in the first step
3228680	3237560	so given um you have uh definition of these g's there is actually then a sort of generating
3237560	3243560	function in the in the um you know analysis generating function sense there are these partition
3243560	3249560	functions which look like so so you basically we can view it like starting from this we start
3249560	3256920	with some some definition where all these g's are um captured uh in this sort of formal some
3256920	3264360	formal sum of products and then these g's are once you would have this uh partition function
3264360	3270360	with this j parameter you can in principle like formally speaking with functional derivatives
3270360	3274680	compute this g from this thing and then there's this whole quantum field theory I mean this is
3274680	3281240	what quantum field theory is all about that tells you how to compute this this set and this is
3281240	3288520	then exactly this sort of object this is what we already had have had there right we already said
3288520	3298440	that um if you um take the expectation value of e to the minus s in the euclidean case is here
3299720	3306760	of a product of these phi's you get all these these g's and this is basically sort of the
3306760	3314760	generating function trick to get to this to these sort of objects okay um okay so this is
3314760	3321080	what you do in practice and just to motivate also that how um neural network sampling would
3321080	3325720	be done another way of getting at this these g functions which you want to to have okay and
3325720	3332760	and so to round it up to see to to explain how this this simple g's this field theory um
3334760	3343560	correlation functions relate to actually um to actual observables right you want to have this
3343560	3348760	sort of transition functions as I discussed them for the quantum mechanics case there is then
3351000	3356840	complicated theory of how you take these states how they how you encode these states with some
3356840	3362360	fixed momentum in the in the quantum field theory you know they also live in some supposed
3363480	3372360	Hilbert space let's say you have to compute a lot in in momentum space which by Fourier
3372360	3378280	transform is sort of the dual to the the position space that we talked about in the whole video
3378280	3385240	so there are some Fourier transforms involved and you pass from the phi's to this a's so
3386200	3389800	just to sketch this out here um
3394120	3399560	yeah so you have when these are I'm just throwing these formulas at you right but
3399560	3407480	there's this typical relations between um the momentum space here ap and the the
3409320	3416280	spacetime fields and you can view this as a transition link by Fourier transform but
3417160	3423160	Fourier transform is with respect to space and also like you know momentum on space and also time
3423320	3430920	and energy and the energy encodes the sort of relation that is governed by the
3432920	3439000	Schrodinger equation relation and so all these things are fairly like complex but in the end
3439000	3445000	it's clear that once we have these g's we have to sort of Fourier transform them over to a momentum
3445000	3450520	space and in momentum space we encode actually the input output state that we are actually interested
3450520	3462840	in like this for some scattering transition probability stuff okay and then there is some more
3462840	3469720	theory that gives you this momentum space in out transition amplitudes that you're actually
3469720	3476840	interested in in terms of these these g's basically you see on this side here these are basically g's
3476840	3483800	and then from Fourier transforming and taking into account how the the frequency is and space is
3483800	3489080	sort of tied together through the evolution equations you get some nasty object here that you
3489080	3498920	have to then compute and then with this this this formalism you get from the green's functions to
3498920	3504920	the transition probability right so I in these last 10 minutes yeah I just explained to you
3504920	3508840	why these g's are really the important thing that you want to get that once you have them
3508840	3517800	then in principle you can compute this transition probabilities and the neural network aspect
3517800	3526840	gives you a way to get at these g's okay so lastly I want also to mention that
3527160	3536360	apart from the neural network kernel theory stuff there's also the you know classical
3537320	3545800	information geometry approach where you you know you have the underlying parameters theta
3545800	3555240	which are the weights and you can see them as encoding if you put a distribution over the
3555320	3559560	inputs and you get the distribution over the outputs which are governed by the parameters
3559560	3565000	then you get this information manifold so you heard that information geometry stuff this is
3565000	3572040	like a related but different mathematical angle that I just wanted to mention because we are like
3572040	3576760	formally so close having all the important all these stochastics already but this is a little
3576760	3582360	bit different nonetheless but also interesting there you have this neural neural manifold
3582360	3589480	thing so this is also one line of research I actually try to find things going into the
3589480	3595400	this quantum field theory research direction in in deep learning books in preparation for this video
3596440	3602040	and actually didn't find too much so there was more about this classical information geometry
3602040	3611080	aspect of things and then also worth noting the applications to quantum field theory is not the
3611080	3618760	only way in which people try to apply neural network theory and all these nice formulas to
3618760	3629080	physics there's also this mathematical metric flow things going on I just mentioned it because if
3629080	3638440	you look at for example the research groups that put out this paper then you will also find these
3638440	3646680	sort of neural network applications using quantum field theories okay so I mean at one hour I know
3646680	3652360	it has been a little bit fuzzy but nonetheless I think I at least maybe you understood the neural
3652360	3658440	network Gaussian process stuff and and can see how this ties in with path integral formalisms
3659960	3665560	and I motivated you I'm really I know very few people will make it through a one hour
3665640	3672920	video of these sort of friends but it's probably nonetheless I think the best way to get an infusion
3672920	3679320	of this sort of theory in a digestible way and in a way where somebody highlights these things so
3679320	3683880	this is it's not a real excuse I think it's nonetheless helpful even if this was not very
3683880	3694360	super prepared okay with that I leave it I leave it at that I wish you a good transition into the
3694360	3702680	next year I have no real videos planned for the upcoming months really I mean I have a folder with
3702680	3710280	20 started projects and videos I could talk about that I might come back to probably not
3710280	3717720	within the next months but then as I said I might look more into classical classical
3717720	3722520	functional analysis stuff for the sake of making a nice video about the universal approximation
3722520	3734360	theorem and at the latest then I will have a polished video take care
