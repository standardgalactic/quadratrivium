{"text": " Hello boys and girls. In this video I want to talk about the line of research that I only came across last week, namely the very strong connection between quantum fields and neural network theory. And what emerges from that is tools from one field being applicable to the other. So you can then use let's say Feynman diagrams to find stuff out about neural networks. And in the other way around you can use neural networks to compute stuff for quantum fields. I think the most senior researcher, if I'm not mistaken from this list of this paper in particular is James Harvorsson. And if you look it up there have been a few papers in this direction in the last years. And nonetheless I chose this particular paper because it's clearly some of the more advanced results there. And to motivate how things work, I'm also in this video going to explain some results which were known for longer time. So there I suppose it's fair to say that this sort of stochastic result for large neural networks that we are going to discuss emerge in the mid 90s. And I'm going to explain this because I want these subjects which I find very exciting actually to be known also a little better. In this video however I will not go into any deep mathematical analysis. I have also not written down much. I will basically jump from tab to tab. And nonetheless give a sensible explanation of these things. So I think everything that I say should make sense in principle. And then you can delve into all the subjects on your own. If you're actually interested in doing something in this direction let me know in the comments. I will also point to some Google libraries regarding neural kernel theory that I will also sketch out in this video. And yeah so that's that. The requirements for the video are that you have like a basic understanding of deep neural networks like the fact that these neural networks encode the parameterization of certain functions. And then if you have a big enough network stuff like universal approximation theorem the fact that you basically can represent functions in the space let's say continuous functions densely. And it helps if you have a basic understanding about the ideas of quantum mechanics. So the fact that what you're interested in is transition probabilities and that they are expressed as some products in a Hilbert space. In this video I have a section where I motivate the jump from the expressions that we are going to be able to compute with neural networks as they are explained in this paper. Let me scroll down a second. How these expressions connect to scattering amplitudes in let's say some large hydrogen collider experiments. These sort of stuff so I have a small like wake mafia section that concerns physics. But my main goal is that if you go away from this video and have a vague understanding why these sort of expressions that you see on the screen right now are both relevant for neural network theory and for quantum field theory then my job is done. As I said this gives the possibility not just to compute stuff in physics but for example you might then be able to apply Feynman diagram calculus to compute various aspects of neural networks as well and so even if you edit from a purely computer science perspective and have some statistics background then this might be fairly helpful. Okay so I have here just a bunch of bullet points that I want to go through. I might come back to this from time to time otherwise I will just jump through like here 15 tabs or so and explain some results. I am actually currently working on or started working on a video that I maybe want to make as my some for summer video where I will do like a painful analysis of the universal approximation theorem. But that's like months and months out. In this video I'm basically just rambling. I hope you don't expect to neither deep or concise elaboration so I'm warning you already but nonetheless I would really recommend that you listen up. Okay so first off as you have just seen in the bullet points I will explain to you like sketch out this result from the 90s which concerns neural network Gaussian processes. So there are nice results that have been found there and have since been extended to a broad range of neural network architectures. So this is mathematical theory results for neural networks that hold in general. In this video for simplicity we can just look at fully connected in the sense that you see in the screen here feed forward neural networks and for this video it's not even super relevant how many inputs outputs you have. Basically you have let's say at least one input some float or if you want a real number x that goes in one real number y that goes out and in the middle you have a bunch of hidden layers in the image you just see one but you know for the sake of it you might think of two three four something like that and you see the nodes in this one hidden layer here and the theory that we are going to discuss kicks in once you have a really big network. So this you think of the number of nodes in each hidden layer here going to infinity or you know it will suffice if you think of a huge number a bunch of billions of billions and the thing that then emerges with large networks is not just the universal approximation theorem that says the the nice functions let's say continuous functions from r to r are represented densely by this sort of neural network by these weights but what also emerges in this large network limit is that the dependency of the output for fixed input and probabilistic weights takes on a very deterministic character okay so this is this neural network Gaussian process phenomenon and then I will explain in a second in more detail but basically what we want to hear first look at is we take one fixed architecture some big neural network with let's say three hidden layers and all the layers are very large and what we're here are first concerned with before we talk about tangent kernels before we talk about quantum field theory is we few we're concerned with the random initialization of these networks right so let's say you are on a computer you you have this network encoded on your GPU or whatever and or then the weights really I mean the you have the architecture laid out the way in which all the the the float data pass to each other naturally if you have this this float types in every realized configuration the weights have to have some some value and this gives the start configuration for the learning process right in the learning process you're going to probably assign some some loss function and you do gradient descent and then you tweak the network to behave in a certain nice way and fulfill some task but to start this process you need to initialize the network you want to maybe explicitly set some weights okay and now what you can do is you can play around with what is actually your starting condition right you can say hey shoot all the weights in the beginning be set to zero or be set to one or and this is the interesting thing here you do a random initialization of all the weights and biases also so think of you know you're in python you take a library and you sample from a normal distribution for all the trillion weights you sample trillion random numbers and you sample them each from a Gaussian from a bell curve and then set the corresponding weights like this okay so all these w i j are sampled from some from some Gaussian and when you like put in some input right we said there's one input let's say you you take the input seven and set x to seven and then feed do the feed forward process the evaluation of the neural network then if these things are random then the output will also be some essentially random number it will be determined for whatever random weight you have sampled here and in this way you can think of y for fixed x as a random variable composed of the random variables w right so we have here's the neural network Gaussian process page the math and the proof sketch of this result that we are going to get to is actually described there so the weights are sampled from some Gaussian we are going to take a Gaussian where the standard deviation gets tighter and tighter with the number of layers right but if you have a fixed network this is some fixed variance here and the output that is in a standard way computed from neural network is um computed as you see here you do fast forward and I think it's fairly easy to believe that just due to the central limit theorem right the statement that if you have a bunch of independent random variables if you sum them all up then this is another random variable that will behave like a Gaussian process right so basically if you sum up random numbers then you usually end up with if all the conditions are fulfilled all the mathematical conditions then you will end up with a Gaussian this is the statement of the central central limit theorem and this exact thing applies here also you know maybe there's some non-linearities involved and maybe there's different steps but in the end the final output of the network here in this picture in the on the last layer this set is still a function of all these small Gaussians and because there are so many sums this is again just a Gaussian process right so and so this says that in the limit and this is especially emerges if you have enough width if the width is big enough so that the central limit theorem really kicks in but this basically means that the as a random field as a random variable the output of the neural network has very nice stochastic process properties and it's it's a Gaussian process in particular one of the nicest you can have basically here on this web page on this Wikipedia page there's also like this this this example animation so here they have some network with three inputs right as I said it doesn't have to be three it suffices to think of one and they have a bunch of outputs again it suffices to think of one so one green input one yellow output and a bunch of nodes in in bunch of layers in this case two layers one layer would also work you see on the right side I mean you probably don't see just because of my face here but I mean doesn't really matter too much it's just a bunch of like random distribution the statement is that for fixed input the green value again let's say there's one green input and it's set to this the float number seven if you fix if you go up with the the number of nodes and random initialize this with weights and biases then just by the central limit theorem which is dependent on this this seven and this bunch of random numbers the output y1 here this yellow output will behave like a Gaussian just by the central limit theorem and in this case there's two outputs so you can draw a plot and the statement is then that both y1 and y2 behaving like Gaussians independently from another like it's not a statistic statement but each behave like a like a Gaussian they have some peak and so on the plot you get another nice Gaussian with some peak here and so if then the press play again if the network becomes even bigger then you get like this perfect Gaussian where it this just says that it has this maximum expected value here in the middle and this goes for all the outputs right so this is the result that that that the okay I closed the neural network Gaussian process page but doesn't matter this is the result that just because of statistics you the network if it's large enough at random initialization behaves like a Gaussian we will not need it for the this video but if you want to take a look this is the the formal definition of a Gaussian Gaussian process I mean to motivate this basically you think you know a Gaussian is something which if you do the Fourier transform it's again like a Gaussian and the Gaussian process is abstractly defined as this random variable or sequence of random variables where the characteristic function the expectation value of this phase is this Gaussian with a certain mean we are not going to need this the Fourier transform will pop up again when I talk about the quantum field theories but suffice to say the nice thing is that the neural network the big neural networks behave like Gaussian processes sorry if I repeat myself okay so do we do physics first or do we do neural tangent kernels first um let me actually um yeah let me actually say something about the neural tangent kernel so um could we know now that the the network at the start behaves like this Gaussian for all inputs and if you do the learning process then this is about um giving it a test data and then moving um in parameter space from wherever we random started to some other position in in weight space and I have made a bunch of videos on gradient descent I will not explain it here but suffice to say you have a space of weights and then the the weights follow some path and you do that in a way that optimizes some goal that you have right some task for the neural network and I think I sketched it out here so as is common we're dealing with not only here with a large neural network so that the the theory becomes simpler and nicer but also we are matching we have so much compute that we can do really small step sizes so that we can then in the limit talk of the behavior of the network as in a differentiable way where we say the the the motion of the in path space in a parameter space can be described you know with literally just calculus differentials and so the gradient descent algorithm what we are doing really is um you know as per instruction of the algorithm we say the change of the weights and here I abbreviate all the weights together with this uh theta the the change in the weights right from from one point to the next in the graphic that you just saw um is chosen in a way that it takes the negative direction of the gradient of some cost function and the cost function in here is the you know the the difference essentially between what the neural network currently says versus where we want to get at where set is all the learning data that we have available right so we say for all the learning data that we that we have um there is a discrepancy between what the network f currently um says um what's correct and what is actually correct why I said here is what's actually correct um and we send that up and so this is like the the this cost of all learning data together and at every step in time in the learning process we go follow this path right and this equation is really just the Newtonian equation um where you know in in physics um theta would be the momentum and where you um look at the situation where the force on the right hand side is governed by a potential and you'll say um the the direction of motion um captured by the momentum is given wherever the you know potential energy will be lowest and that's where the path followed by the particle in Newtonian physics right so this already looks very um like like this simple physics uh equation governing governing the motion in weight space and now given that you have the behavior of the um the the particle if you will uh going through weight space like and give you just saw um and the the potential depends on the outputs at the network on all these spaces you can also then um do the calculus and and look at hey how does the output of the network which depends on the position where you are at right where the weights determine what the network output will be on all the um learning data how does that the f change and so if you do them just do the math um and I think I have this is here so this is um newer tangent kernel theory um if you if you do this sort of calculation and if you uh you know if you ever started physics you have to this this sort of calculation a million times because basically if you have some observable in a physical system and you know all the the constituents of the particles behave in this isn't this way and then I have some observable which is made out of particles and you say how does this this observable quantity change then um you have to plug in a bunch of partial derivatives and then the Hamiltonian comes in and whatever and so on and so forth what comes out of this is that the development of the output of the neural network um is governed by some matrix this is the so-called newer tangent kernel and the changes of uh the the loss in uh with respect to the to the weights right so I mean I did not adopt this terminology theta is again all the weights together and um you can do this calculation on one sheet of paper yourself I'm not going to discuss all the the convention or a notation chosen here but the point is that the evolution of the output on a network from your starting point which might be a random starting point is understood at least here in theory it's another question of whether you can actually calculate that because this matrix which determines how this the output of the network evolves as you do the learning according to gradient descent is determined by this complicated object theta and the theta is basically this so-called kernel um this is you can view this as the inner product of the gradient of the output with research with respect to the weight change and so the interpretation is basically that um you look at uh different inputs and then you um you as a kernel a kernel roughly charges how similar input data are and this kernel basically looks at uh hey these two input data are similar if upon a change of the weights the um the response of the network changes in a similar fashion and you compute this it's in a product in any case this is like an interesting object that in the end determines how the network behaves um and similar to neural network Gaussian process theory where you say once I have enough um uh weights in my layer some nice theory emerges right in in the Gaussian network case it goes towards a Gaussian it's also the case that for a large network then these these matrix can simplify and then you can get the infinite size network also to an analytical theory and basically you random initialize you already know it's some Gaussian process and then you have some matrix which determines how the network moves um through through weight space and thereby you you have an idea of actually what happens during network to network training right so if you have never heard that and more or less followed my explanation then you can see this is kind of cool that at least in this limits you have sort of an analytical idea what happens during learning and then the question is to what extent is this sort of logic valid for networks as we can implement them at the moment at the moment because of course we have a lot of weights like billions of weights but it's not infinite so you might ask to what extent is the analytical theory where these limits are taken right so super small step size very large networks applicable to today's convolutional deep neural networks and so on and so forth and this is basically a subject of study so this is something where people still put a lot of time in and so for example you have here this google uh neural tangents project which i might be interested in looking at and there's a bunch of google researchers who are still using this and publishing papers in this and and so on and so forth there's also i think a recent um new rips uh poster from 2019 where you get some of the examples of analytical formulas that i talked about um just because i want to get to the quantum field theory part i will not discuss this in detail but i um i hope my tangent pun intended was interesting and as i said i would also actually like to to work a little bit in this direction myself so if you want to look into that um feel free to reach out and we can do some sort of project together okay so um now for the the the field theory part but still extremely important for us is this neural network Gaussian process inside okay and if i'm here in the paper on page four then um let's make first some definitions okay so here we have these correlation functions which we call g n uh for uh for a concreteness sake you can think of n uh let's say as two so um we are going to actually look at um two different um forward passes for the neural network right so you have two different imports that you want to try x and you plug them in and you get something out of your current network which might be randomly initialized um and if you as we had it with the neural network Gaussian process case if you your weights as a random variable right over all the weights over all your trillion weights you put a little Gaussian and let them wiggle a little bit um and you say what typically happens if i sample once and um put into uh inputs x uh how are the inputs on a on a typical or random network correlated with each other then you can you can try this a bunch of times and get an idea um what you can also do is analytically if you know the distribution of where your weights compute what what will come out right so you have here uh p over the weights this is the distribution that you yourself chose from which you can sample and uh for fixed neural network architecture um the weights and um the neural network um which um is here called phi this is the function that depends on the weights depending on your architecture right so this is the sigmoids and this sum and so on and so forth and so the correlation function gives you how this how let's say two inputs are um correlated with each other for this network right and by the neural network Gaussian process result if we said that uh this this billion uh probability distribution over the weights um make the input output relation of the whole network also into a random variable right so you can also view uh this the setup that you have here not as um as um not just a sampling um the weights and getting uh then a fixed input output but you can also view any sample process as sampling a whole neural network right even this is just what you do if you sample if you random initialize for fixed architecture um the um certain uh functions as your certain neural network then you've also sampled the neural network from who knows what distribution right so there's also this different the different view of this initialization process and then um by the result of the neural network Gaussian process what you have is that you can also view the same uh exact object this correlation function or any expectation value really um as in terms of distribution over the these functions themselves and by the result that we just had for a large network this will be a Gaussian process and this manifests in this way right so um here is the integral not over the weights but over the um the uh whole function that the network represents itself and um the the um probability distribution that you have in this case is um of of this sort where this s uh and now this relates back to the formal definition of the Gaussian process is some quadratic let's say let me fill some local terms cost associated with the whole um with the whole function so basically what what this does I mean do they give here some examples I think they do um okay so this is already sort of a physical example but what you have as s here in the the exponent is some sum over all uh the values of the network and what what they like what in effect happens is that um the um the the this this weight is such that um field configurations and when I say field now I always just mean the same as the input output relation not given by the neural network field configurations or neural networks where at one uh places you get a huge output these are just exponentially suppressed because um if you random sample from the with the weights then you are not likely to get um some basically you are going to get um neural networks in initializations which are around some some uh some certain um typical expectation value like they have there are some typical behaviors and everything that deviates uh a lot from it is like exponentially less likely right you um if you do um thousand random initialization of the neural network you will get some typical behavior and then you can cook up some other extreme behavior that is not likely to happen and um by the result of the neural network Gaussian process um theory um it tells you what the the the probability distribution looks for the neural network so there is this connection that you have here right and as I will motivate um later uh in quantum field theory this is exactly sort of the setup for the path integral and what this paper does is it um if you view um the um this this sort of um mathematical um overlap um as a physicist and you want to use neural networks as as as a physicist you you see this as a way to um then try to craft neural network architecture and sampling techniques right the the way in which you sample the weights in a way so that the this this whole process of random initialization corresponds to certain s s certain actions here right you do you have some physical scenario in mind there's physical theory that says oh you know uh certain scalar field theories have this and this action what is the way I and in which I must set up a network and the way in which I must sample the weights so that what I sample is exactly as if I would sample from a quantum field like like as if I would sample from uh would I would sample a field which is one instance of um a quantum field in the path integral formalism right and then I get out a bunch of correlation functions this endpoint functions um and these are exactly the things that are uh what what you do quantum field theory for right you you compute these g's and then I will explain it later then there's some mathematical connections to how you get from this this this g's to um the scattering amplitude or whatever your quantum field theory does um maybe particle physics solid state physics and so on and so forth okay uh let me see so as you might notice this is a very free flow sort of explanation right um so I have to check uh what I have touched upon and what I didn't okay yeah okay so um from the very complicated quantum field theory math you get some you know relations of um how the correlation function must relate to these actions and there's a bunch of stochastic differential equation mathematics involved and because in physics the evolution is always governed by some Hamiltonian operator function there's a bunch of energy terms that you have to kick around and that's why for example um these objects tend to look a little bit like this um like if you're never studied this physics um maybe one way in which you should look at this is that because the evolution of these fields like how they um have often time is governed by the Schrodinger equation which relates the time derivatives like the evolution of the fields themselves to some energy expression captured in the Hamiltonians and the the energy kinetic energy and in particular for fields is given by some spatial operators that's that's why these sort of objects pop up here and you know mass energy equivalents that's why we also have mass and so if you see these Laplacian operators or mass terms um you should not be too surprised uh there because in physics they just always pop up in relation to the time evolution of the the fields okay so okay now I've already touched upon the concept of time the thing is of course that um here this the fields as they pop up here will um be not uh like what you get there is the better controlled theory of euclidean fields right you're not you're not having to do a priori with spacetime metrics and all these things which make field theory quantum field field extra complicated but um just talking about Gaussian processes is just talking about stochastics and then there is this sort of bridge that you have to take and hope that you can get from the the euclidean field theories to um to some actual quantum field and I will just name drop um a bunch of concepts there the idea is that you uh if you approach quantum field theory with this neural network she bang then um you want to find a neural network which mimics the weak rotated version uh of um of physical quantum field and so there is um a bunch of uh so-called constructive quantum field theory coming in so there are various approaches for of people trying to um uh make certain aspects of quantum field theory more rigorous and um transfer like get rid of uh pseudo metrics in quantum field theory move everything um to the euclidean domain where you have nice metrics and um uh so in this paper for example they say that um you know what we really want to impose is um neural networks which when you view them as a field behave in a certain way and something that is important there for example are these uh Osterwald Schrader relations so for example here in this case the um correlation functions um these particular coordination functions of relevance here are called denoted s and then you see on the screen a bunch of properties that these shall have right so there you have the physical translation in variances of certain objects and certain symmetry or independence relations right i'm just mentioning this that there is uh it's not like um you just take any neural network and then you get some uh some quantum field theory in the path into word formalisms out of it there's a fairly restricted subset of uh fields that the physicists for quantum field theory might be interested in um okay but i should probably not go into too much detail on that uh here um yeah uh also the um these objects in this um exponential so sorry here um if you if you know some physics then you know this but um if you don't then just want to mention that these sort of s s um basically any s that you can write down uh gives you some field theory these s s are some sums or integrals over energy terms and if you go to um sorry for the click clickery um if you go to the Lagrangian field theory Wikipedia page then you can find a whole lot of different um s objects that make sense and you can see here see how they data mine how the data mine um the various physical theories that you have certainly heard of we are interested in particular about fields so we have here scalar field theories this is what we just saw you have some partial and then um some um some mass and there will be also be some time the derivatives there um but the thing is that the pure gaussians are um where this is just this quadratic object in in s are actually relatively um uninteresting from the physical perspective because if you have for example if you have different quantum fields that interact with each other and and then this information how they interact is also all encoded in this in this um in this Lagrangian cells or in the action s and then uh you will have some more complicated products in these objects and um if you have some power of the field that's higher than two then this is actually uh representing um sort of self interaction in in field theory so um what we really want to have is not just um the fields which fulfill these nice um properties in the Euclidean version but we also want to have very finely controlled um interaction terms there and so what we really want to have is um processes which are actually not gaussian right and so what they do in the paper is in in the end um look at uh um five to the fourth field theory so quadratic interaction let me see sorry so what they really want to implement and what they actually then do in the paper is they take this sort of action you you not only have this quadratic term there but you also have their um this five to the four uh term and to to get this in to get um uh away from the just quadratic gaussian process scenario there is two ways um to implement this self interaction uh and one way is to actually not look at the infinite and limit right not the infinite size network limit um because then uh the you basically break the neural network gaussian process scenario you you person person uh you purposely stop a little bit earlier and get some non gaussian effects and so what from the mathematical point of view is a bug that for a real network you actually don't get a perfect gaussian situation here becomes sort of feature it gives you the freedom to actually implement behavior and if you tweak the network uh nice enough the ideas that then you can sort of control how it is broken this bug certainly becomes a feature this is one way and the other way is if you um actually um in sampling you do not uh take a trillion independent distributions over the weights but what you and instead do is you introduce on purpose some dependencies of the individual weight uh distributions right do you break the independence such that um there is a little bit of a flaw in the central limit theorem and in this way by independence you also get some non gaussian process because it's clear that if you do only um break the independence of this weights a little bit you'll still by the central limit theorem get something which is just a slight deviation from discussion processes right so if you the idea is if you tweak the this the conditions for the god for the central limit theorem just in a wide way then you might produce errors to the the gaussianity in a in a in a very controlled way and this is exactly what they do here so here they describe a neural network with particular non-linearity as activations it looks like this and what they do is they sample in this um in this dependent way in exactly the correct way so that the um the action um that emerges by random sampling in this you know particular way represents uh this uh this sort of physical field theory so this field theory in particular is basically always the first interaction uh interacting field theory or one of the first interaction field theories that you would learn in the quantum field theory course um this is not one of the famous standard model um energy densities at least not in the exact same way here but nonetheless this is like classical physical theory and so this is what this paper is all about right breaking the neural network gaussian process uh theorems in the correct way to get um the the right uh g functions endpoint functions out there that are relevant for physics um and so you see that then you can you know in principle sample uh from this fixed neural network architectures fields and then once you have like a way of sampling fields you can in principle because the fields that demand all the properties of the quantum field theory uh compute expectation values and thereby get physical uh information so this is the idea it also goes in the other direction in um since there is uh these methods in particular um Feynman diagram computation methods that compute correlations um for quantum fields you also have a method of computing aspects of random initialized big neural networks right you have a big neural network you know that if you uh to sample from it it is as if you would sample from a quantum field um and because you have ways in physics to compute aspects like correlation functions and so on of the quantum fields these uh g functions that you can compute in physics with Feynman diagrams and so on also have a relevant meaning for computing typical aspects of random initialized neural networks okay i know this is a little bit much but i hope it sort of makes sense um um i um i don't know how clear everything was that i discussed so far i just want to give you this is then more on the quantum field theory side a little motivation that i have just here written up um that connects these g functions right these autocorrelations to physics i just want to motivate it maybe give me a three more minutes so i i'm going to assume that you have an idea of the fact that transition probabilities are the squares of inner products in a Hilbert's base in quantum mechanics um so there are these um kernel objects k which are given like that so in in in you know standard quantum mechanics what you have is some um you have some in um current state which i call here in and then you have some uh other state out that you are interested in you want to know what is the chance that this state um transitions over in that state what you do is you um take the Schrodinger equation uh evolution which is governed by the Hamiltonian h in in the nicest case um here the formal solution of this equation is just e to the i h so this is the operator which moves any state forward in time you apply it to in to the like let's say this is now this is in two days you say this state that i have currently how will it evolve into the in like how will it look in two days then you basically apply this operator to fast forward this thing um and then you get something uh the in state how it will look like in two days and then the product with this object in two days with the out state that you're interested in gives you the probability that the the current state after it has evolved is going to be observed in uh in the out state okay so this is basically quantum mechanics one one um the for whatever Hamiltonian you have here you um can often then compute some analytical case some kernels these are these things and i think here if you score down in this Wikipedia page you find some examples so these are just you know in this case uh three particle in one dimension this is like some Gaussian this is similar to some heat dissipation equation and then if you have some more relativistic scenario i think they also write down here some this is more complicated correlation functions and examples okay so that's the one thing now if we talk quantum field theory then yeah the uh this is basically just a definition um the the g's we all also had that in the paper are certain expectation values right so there since we're on the pure relativistic side and i'll take your clean side there's some time operator there but you can basically ignore it for for the moment the the point is that these g's are determined by some fairly simple looking expectation values and in the in the quantum mechanical formalisms these are also some inner products in some Hilbert space of course in a quantum field theory there is no super nice theory um about uh Hilbert spaces and and and these operators so there is it's more like a like a functional definition this is what you calculate and um you hope it's somewhere else comes up with the sort of rigorous setting to compute this but nonetheless you know how to compute these g's you know how to set up the an evolution equation of your choice and make sense of these correlation functions which is um what you want to get at in the first step so given um you have uh definition of these g's there is actually then a sort of generating function in the in the um you know analysis generating function sense there are these partition functions which look like so so you basically we can view it like starting from this we start with some some definition where all these g's are um captured uh in this sort of formal some formal sum of products and then these g's are once you would have this uh partition function with this j parameter you can in principle like formally speaking with functional derivatives compute this g from this thing and then there's this whole quantum field theory I mean this is what quantum field theory is all about that tells you how to compute this this set and this is then exactly this sort of object this is what we already had have had there right we already said that um if you um take the expectation value of e to the minus s in the euclidean case is here of a product of these phi's you get all these these g's and this is basically sort of the generating function trick to get to this to these sort of objects okay um okay so this is what you do in practice and just to motivate also that how um neural network sampling would be done another way of getting at this these g functions which you want to to have okay and and so to round it up to see to to explain how this this simple g's this field theory um correlation functions relate to actually um to actual observables right you want to have this sort of transition functions as I discussed them for the quantum mechanics case there is then complicated theory of how you take these states how they how you encode these states with some fixed momentum in the in the quantum field theory you know they also live in some supposed Hilbert space let's say you have to compute a lot in in momentum space which by Fourier transform is sort of the dual to the the position space that we talked about in the whole video so there are some Fourier transforms involved and you pass from the phi's to this a's so just to sketch this out here um yeah so you have when these are I'm just throwing these formulas at you right but there's this typical relations between um the momentum space here ap and the the spacetime fields and you can view this as a transition link by Fourier transform but Fourier transform is with respect to space and also like you know momentum on space and also time and energy and the energy encodes the sort of relation that is governed by the Schrodinger equation relation and so all these things are fairly like complex but in the end it's clear that once we have these g's we have to sort of Fourier transform them over to a momentum space and in momentum space we encode actually the input output state that we are actually interested in like this for some scattering transition probability stuff okay and then there is some more theory that gives you this momentum space in out transition amplitudes that you're actually interested in in terms of these these g's basically you see on this side here these are basically g's and then from Fourier transforming and taking into account how the the frequency is and space is sort of tied together through the evolution equations you get some nasty object here that you have to then compute and then with this this this formalism you get from the green's functions to the transition probability right so I in these last 10 minutes yeah I just explained to you why these g's are really the important thing that you want to get that once you have them then in principle you can compute this transition probabilities and the neural network aspect gives you a way to get at these g's okay so lastly I want also to mention that apart from the neural network kernel theory stuff there's also the you know classical information geometry approach where you you know you have the underlying parameters theta which are the weights and you can see them as encoding if you put a distribution over the inputs and you get the distribution over the outputs which are governed by the parameters then you get this information manifold so you heard that information geometry stuff this is like a related but different mathematical angle that I just wanted to mention because we are like formally so close having all the important all these stochastics already but this is a little bit different nonetheless but also interesting there you have this neural neural manifold thing so this is also one line of research I actually try to find things going into the this quantum field theory research direction in in deep learning books in preparation for this video and actually didn't find too much so there was more about this classical information geometry aspect of things and then also worth noting the applications to quantum field theory is not the only way in which people try to apply neural network theory and all these nice formulas to physics there's also this mathematical metric flow things going on I just mentioned it because if you look at for example the research groups that put out this paper then you will also find these sort of neural network applications using quantum field theories okay so I mean at one hour I know it has been a little bit fuzzy but nonetheless I think I at least maybe you understood the neural network Gaussian process stuff and and can see how this ties in with path integral formalisms and I motivated you I'm really I know very few people will make it through a one hour video of these sort of friends but it's probably nonetheless I think the best way to get an infusion of this sort of theory in a digestible way and in a way where somebody highlights these things so this is it's not a real excuse I think it's nonetheless helpful even if this was not very super prepared okay with that I leave it I leave it at that I wish you a good transition into the next year I have no real videos planned for the upcoming months really I mean I have a folder with 20 started projects and videos I could talk about that I might come back to probably not within the next months but then as I said I might look more into classical classical functional analysis stuff for the sake of making a nice video about the universal approximation theorem and at the latest then I will have a polished video take care", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.76, "text": " Hello boys and girls. In this video I want to talk about the line of research that I", "tokens": [50364, 2425, 6347, 293, 4519, 13, 682, 341, 960, 286, 528, 281, 751, 466, 264, 1622, 295, 2132, 300, 286, 50752], "temperature": 0.0, "avg_logprob": -0.16631808774224643, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.16797414422035217}, {"id": 1, "seek": 0, "start": 7.76, "end": 17.6, "text": " only came across last week, namely the very strong connection between quantum fields and", "tokens": [50752, 787, 1361, 2108, 1036, 1243, 11, 20926, 264, 588, 2068, 4984, 1296, 13018, 7909, 293, 51244], "temperature": 0.0, "avg_logprob": -0.16631808774224643, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.16797414422035217}, {"id": 2, "seek": 0, "start": 17.6, "end": 27.8, "text": " neural network theory. And what emerges from that is tools from one field being applicable", "tokens": [51244, 18161, 3209, 5261, 13, 400, 437, 38965, 490, 300, 307, 3873, 490, 472, 2519, 885, 21142, 51754], "temperature": 0.0, "avg_logprob": -0.16631808774224643, "compression_ratio": 1.4831460674157304, "no_speech_prob": 0.16797414422035217}, {"id": 3, "seek": 2780, "start": 27.8, "end": 34.52, "text": " to the other. So you can then use let's say Feynman diagrams to find stuff out about", "tokens": [50364, 281, 264, 661, 13, 407, 291, 393, 550, 764, 718, 311, 584, 46530, 77, 1601, 36709, 281, 915, 1507, 484, 466, 50700], "temperature": 0.0, "avg_logprob": -0.17069126665592194, "compression_ratio": 1.5204678362573099, "no_speech_prob": 0.02499711886048317}, {"id": 4, "seek": 2780, "start": 34.52, "end": 40.96, "text": " neural networks. And in the other way around you can use neural networks to compute stuff", "tokens": [50700, 18161, 9590, 13, 400, 294, 264, 661, 636, 926, 291, 393, 764, 18161, 9590, 281, 14722, 1507, 51022], "temperature": 0.0, "avg_logprob": -0.17069126665592194, "compression_ratio": 1.5204678362573099, "no_speech_prob": 0.02499711886048317}, {"id": 5, "seek": 2780, "start": 40.96, "end": 49.28, "text": " for quantum fields. I think the most senior researcher, if I'm not mistaken from this", "tokens": [51022, 337, 13018, 7909, 13, 286, 519, 264, 881, 7965, 21751, 11, 498, 286, 478, 406, 21333, 490, 341, 51438], "temperature": 0.0, "avg_logprob": -0.17069126665592194, "compression_ratio": 1.5204678362573099, "no_speech_prob": 0.02499711886048317}, {"id": 6, "seek": 4928, "start": 49.28, "end": 58.2, "text": " list of this paper in particular is James Harvorsson. And if you look it up there have", "tokens": [50364, 1329, 295, 341, 3035, 294, 1729, 307, 5678, 3653, 85, 830, 3015, 13, 400, 498, 291, 574, 309, 493, 456, 362, 50810], "temperature": 0.0, "avg_logprob": -0.2128588855266571, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.22850997745990753}, {"id": 7, "seek": 4928, "start": 58.2, "end": 68.16, "text": " been a few papers in this direction in the last years. And nonetheless I chose this particular", "tokens": [50810, 668, 257, 1326, 10577, 294, 341, 3513, 294, 264, 1036, 924, 13, 400, 26756, 286, 5111, 341, 1729, 51308], "temperature": 0.0, "avg_logprob": -0.2128588855266571, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.22850997745990753}, {"id": 8, "seek": 4928, "start": 68.16, "end": 76.8, "text": " paper because it's clearly some of the more advanced results there. And to motivate how", "tokens": [51308, 3035, 570, 309, 311, 4448, 512, 295, 264, 544, 7339, 3542, 456, 13, 400, 281, 28497, 577, 51740], "temperature": 0.0, "avg_logprob": -0.2128588855266571, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.22850997745990753}, {"id": 9, "seek": 7680, "start": 76.8, "end": 86.28, "text": " things work, I'm also in this video going to explain some results which were known for longer", "tokens": [50364, 721, 589, 11, 286, 478, 611, 294, 341, 960, 516, 281, 2903, 512, 3542, 597, 645, 2570, 337, 2854, 50838], "temperature": 0.0, "avg_logprob": -0.17307472229003906, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.007809331174939871}, {"id": 10, "seek": 7680, "start": 86.28, "end": 93.28, "text": " time. So there I suppose it's fair to say that this sort of stochastic result for large neural", "tokens": [50838, 565, 13, 407, 456, 286, 7297, 309, 311, 3143, 281, 584, 300, 341, 1333, 295, 342, 8997, 2750, 1874, 337, 2416, 18161, 51188], "temperature": 0.0, "avg_logprob": -0.17307472229003906, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.007809331174939871}, {"id": 11, "seek": 7680, "start": 93.28, "end": 100.84, "text": " networks that we are going to discuss emerge in the mid 90s. And I'm going to explain this because", "tokens": [51188, 9590, 300, 321, 366, 516, 281, 2248, 21511, 294, 264, 2062, 4289, 82, 13, 400, 286, 478, 516, 281, 2903, 341, 570, 51566], "temperature": 0.0, "avg_logprob": -0.17307472229003906, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.007809331174939871}, {"id": 12, "seek": 10084, "start": 101.8, "end": 112.68, "text": " I want these subjects which I find very exciting actually to be known also a little better. In", "tokens": [50412, 286, 528, 613, 13066, 597, 286, 915, 588, 4670, 767, 281, 312, 2570, 611, 257, 707, 1101, 13, 682, 50956], "temperature": 0.0, "avg_logprob": -0.19979126751422882, "compression_ratio": 1.5, "no_speech_prob": 0.01589788682758808}, {"id": 13, "seek": 10084, "start": 112.68, "end": 118.60000000000001, "text": " this video however I will not go into any deep mathematical analysis. I have also not written", "tokens": [50956, 341, 960, 4461, 286, 486, 406, 352, 666, 604, 2452, 18894, 5215, 13, 286, 362, 611, 406, 3720, 51252], "temperature": 0.0, "avg_logprob": -0.19979126751422882, "compression_ratio": 1.5, "no_speech_prob": 0.01589788682758808}, {"id": 14, "seek": 10084, "start": 118.60000000000001, "end": 129.08, "text": " down much. I will basically jump from tab to tab. And nonetheless give a sensible explanation of", "tokens": [51252, 760, 709, 13, 286, 486, 1936, 3012, 490, 4421, 281, 4421, 13, 400, 26756, 976, 257, 25380, 10835, 295, 51776], "temperature": 0.0, "avg_logprob": -0.19979126751422882, "compression_ratio": 1.5, "no_speech_prob": 0.01589788682758808}, {"id": 15, "seek": 12908, "start": 129.08, "end": 134.20000000000002, "text": " these things. So I think everything that I say should make sense in principle. And then you can", "tokens": [50364, 613, 721, 13, 407, 286, 519, 1203, 300, 286, 584, 820, 652, 2020, 294, 8665, 13, 400, 550, 291, 393, 50620], "temperature": 0.0, "avg_logprob": -0.1505685806274414, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.01223148312419653}, {"id": 16, "seek": 12908, "start": 134.20000000000002, "end": 140.52, "text": " delve into all the subjects on your own. If you're actually interested in doing something in this", "tokens": [50620, 43098, 666, 439, 264, 13066, 322, 428, 1065, 13, 759, 291, 434, 767, 3102, 294, 884, 746, 294, 341, 50936], "temperature": 0.0, "avg_logprob": -0.1505685806274414, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.01223148312419653}, {"id": 17, "seek": 12908, "start": 140.52, "end": 149.8, "text": " direction let me know in the comments. I will also point to some Google libraries regarding neural", "tokens": [50936, 3513, 718, 385, 458, 294, 264, 3053, 13, 286, 486, 611, 935, 281, 512, 3329, 15148, 8595, 18161, 51400], "temperature": 0.0, "avg_logprob": -0.1505685806274414, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.01223148312419653}, {"id": 18, "seek": 14980, "start": 150.04000000000002, "end": 157.88000000000002, "text": " kernel theory that I will also sketch out in this video. And yeah so that's that.", "tokens": [50376, 28256, 5261, 300, 286, 486, 611, 12325, 484, 294, 341, 960, 13, 400, 1338, 370, 300, 311, 300, 13, 50768], "temperature": 0.0, "avg_logprob": -0.09168800523009481, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.03306439146399498}, {"id": 19, "seek": 14980, "start": 159.32000000000002, "end": 164.36, "text": " The requirements for the video are that you have like a basic understanding of deep neural networks", "tokens": [50840, 440, 7728, 337, 264, 960, 366, 300, 291, 362, 411, 257, 3875, 3701, 295, 2452, 18161, 9590, 51092], "temperature": 0.0, "avg_logprob": -0.09168800523009481, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.03306439146399498}, {"id": 20, "seek": 14980, "start": 164.36, "end": 172.52, "text": " like the fact that these neural networks encode the parameterization of certain functions. And then", "tokens": [51092, 411, 264, 1186, 300, 613, 18161, 9590, 2058, 1429, 264, 13075, 2144, 295, 1629, 6828, 13, 400, 550, 51500], "temperature": 0.0, "avg_logprob": -0.09168800523009481, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.03306439146399498}, {"id": 21, "seek": 14980, "start": 172.52, "end": 177.0, "text": " if you have a big enough network stuff like universal approximation theorem the fact that", "tokens": [51500, 498, 291, 362, 257, 955, 1547, 3209, 1507, 411, 11455, 28023, 20904, 264, 1186, 300, 51724], "temperature": 0.0, "avg_logprob": -0.09168800523009481, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.03306439146399498}, {"id": 22, "seek": 17700, "start": 177.0, "end": 184.52, "text": " you basically can represent functions in the space let's say continuous functions densely.", "tokens": [50364, 291, 1936, 393, 2906, 6828, 294, 264, 1901, 718, 311, 584, 10957, 6828, 24505, 736, 13, 50740], "temperature": 0.0, "avg_logprob": -0.11602287777399613, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.0006662230589427054}, {"id": 23, "seek": 17700, "start": 185.08, "end": 192.6, "text": " And it helps if you have a basic understanding about the ideas of quantum mechanics. So the", "tokens": [50768, 400, 309, 3665, 498, 291, 362, 257, 3875, 3701, 466, 264, 3487, 295, 13018, 12939, 13, 407, 264, 51144], "temperature": 0.0, "avg_logprob": -0.11602287777399613, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.0006662230589427054}, {"id": 24, "seek": 17700, "start": 192.6, "end": 200.12, "text": " fact that what you're interested in is transition probabilities and that they are expressed as some", "tokens": [51144, 1186, 300, 437, 291, 434, 3102, 294, 307, 6034, 33783, 293, 300, 436, 366, 12675, 382, 512, 51520], "temperature": 0.0, "avg_logprob": -0.11602287777399613, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.0006662230589427054}, {"id": 25, "seek": 20012, "start": 200.12, "end": 210.92000000000002, "text": " products in a Hilbert space. In this video I have a section where I motivate the jump from the", "tokens": [50364, 3383, 294, 257, 19914, 4290, 1901, 13, 682, 341, 960, 286, 362, 257, 3541, 689, 286, 28497, 264, 3012, 490, 264, 50904], "temperature": 0.0, "avg_logprob": -0.08261257951909845, "compression_ratio": 1.5315789473684212, "no_speech_prob": 0.04015752300620079}, {"id": 26, "seek": 20012, "start": 210.92000000000002, "end": 217.72, "text": " expressions that we are going to be able to compute with neural networks as they are explained in", "tokens": [50904, 15277, 300, 321, 366, 516, 281, 312, 1075, 281, 14722, 365, 18161, 9590, 382, 436, 366, 8825, 294, 51244], "temperature": 0.0, "avg_logprob": -0.08261257951909845, "compression_ratio": 1.5315789473684212, "no_speech_prob": 0.04015752300620079}, {"id": 27, "seek": 20012, "start": 217.72, "end": 226.20000000000002, "text": " this paper. Let me scroll down a second. How these expressions connect to scattering amplitudes in", "tokens": [51244, 341, 3035, 13, 961, 385, 11369, 760, 257, 1150, 13, 1012, 613, 15277, 1745, 281, 42314, 9731, 16451, 294, 51668], "temperature": 0.0, "avg_logprob": -0.08261257951909845, "compression_ratio": 1.5315789473684212, "no_speech_prob": 0.04015752300620079}, {"id": 28, "seek": 22620, "start": 226.2, "end": 234.44, "text": " let's say some large hydrogen collider experiments. These sort of stuff so I have a small like wake", "tokens": [50364, 718, 311, 584, 512, 2416, 12697, 1263, 1438, 12050, 13, 1981, 1333, 295, 1507, 370, 286, 362, 257, 1359, 411, 6634, 50776], "temperature": 0.0, "avg_logprob": -0.20122174060705936, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.0016993358731269836}, {"id": 29, "seek": 22620, "start": 236.28, "end": 243.79999999999998, "text": " mafia section that concerns physics. But my main goal is that if you go away from this video", "tokens": [50868, 36412, 3541, 300, 7389, 10649, 13, 583, 452, 2135, 3387, 307, 300, 498, 291, 352, 1314, 490, 341, 960, 51244], "temperature": 0.0, "avg_logprob": -0.20122174060705936, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.0016993358731269836}, {"id": 30, "seek": 22620, "start": 243.79999999999998, "end": 249.23999999999998, "text": " and have a vague understanding why these sort of expressions that you see on the screen right now", "tokens": [51244, 293, 362, 257, 24247, 3701, 983, 613, 1333, 295, 15277, 300, 291, 536, 322, 264, 2568, 558, 586, 51516], "temperature": 0.0, "avg_logprob": -0.20122174060705936, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.0016993358731269836}, {"id": 31, "seek": 24924, "start": 249.24, "end": 256.12, "text": " are both relevant for neural network theory and for quantum field theory then my job is done.", "tokens": [50364, 366, 1293, 7340, 337, 18161, 3209, 5261, 293, 337, 13018, 2519, 5261, 550, 452, 1691, 307, 1096, 13, 50708], "temperature": 0.0, "avg_logprob": -0.04905077277636919, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.007436795625835657}, {"id": 32, "seek": 24924, "start": 256.84000000000003, "end": 264.04, "text": " As I said this gives the possibility not just to compute stuff in physics but for example", "tokens": [50744, 1018, 286, 848, 341, 2709, 264, 7959, 406, 445, 281, 14722, 1507, 294, 10649, 457, 337, 1365, 51104], "temperature": 0.0, "avg_logprob": -0.04905077277636919, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.007436795625835657}, {"id": 33, "seek": 24924, "start": 264.04, "end": 273.88, "text": " you might then be able to apply Feynman diagram calculus to compute various aspects of neural", "tokens": [51104, 291, 1062, 550, 312, 1075, 281, 3079, 46530, 77, 1601, 10686, 33400, 281, 14722, 3683, 7270, 295, 18161, 51596], "temperature": 0.0, "avg_logprob": -0.04905077277636919, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.007436795625835657}, {"id": 34, "seek": 27388, "start": 273.88, "end": 279.56, "text": " networks as well and so even if you edit from a purely computer science perspective", "tokens": [50364, 9590, 382, 731, 293, 370, 754, 498, 291, 8129, 490, 257, 17491, 3820, 3497, 4585, 50648], "temperature": 0.0, "avg_logprob": -0.12085433006286621, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.005465106572955847}, {"id": 35, "seek": 27388, "start": 280.44, "end": 286.04, "text": " and have some statistics background then this might be fairly helpful.", "tokens": [50692, 293, 362, 512, 12523, 3678, 550, 341, 1062, 312, 6457, 4961, 13, 50972], "temperature": 0.0, "avg_logprob": -0.12085433006286621, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.005465106572955847}, {"id": 36, "seek": 27388, "start": 287.32, "end": 295.8, "text": " Okay so I have here just a bunch of bullet points that I want to go through. I might come back to", "tokens": [51036, 1033, 370, 286, 362, 510, 445, 257, 3840, 295, 11632, 2793, 300, 286, 528, 281, 352, 807, 13, 286, 1062, 808, 646, 281, 51460], "temperature": 0.0, "avg_logprob": -0.12085433006286621, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.005465106572955847}, {"id": 37, "seek": 27388, "start": 296.6, "end": 303.8, "text": " this from time to time otherwise I will just jump through like here 15 tabs or so and explain some", "tokens": [51500, 341, 490, 565, 281, 565, 5911, 286, 486, 445, 3012, 807, 411, 510, 2119, 20743, 420, 370, 293, 2903, 512, 51860], "temperature": 0.0, "avg_logprob": -0.12085433006286621, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.005465106572955847}, {"id": 38, "seek": 30380, "start": 303.96000000000004, "end": 313.08, "text": " results. I am actually currently working on or started working on a video that I maybe want to", "tokens": [50372, 3542, 13, 286, 669, 767, 4362, 1364, 322, 420, 1409, 1364, 322, 257, 960, 300, 286, 1310, 528, 281, 50828], "temperature": 0.0, "avg_logprob": -0.10054954886436462, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.0015974723501130939}, {"id": 39, "seek": 30380, "start": 313.8, "end": 323.24, "text": " make as my some for summer video where I will do like a painful analysis of the universal", "tokens": [50864, 652, 382, 452, 512, 337, 4266, 960, 689, 286, 486, 360, 411, 257, 11697, 5215, 295, 264, 11455, 51336], "temperature": 0.0, "avg_logprob": -0.10054954886436462, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.0015974723501130939}, {"id": 40, "seek": 30380, "start": 323.24, "end": 328.44, "text": " approximation theorem. But that's like months and months out. In this video I'm basically just", "tokens": [51336, 28023, 20904, 13, 583, 300, 311, 411, 2493, 293, 2493, 484, 13, 682, 341, 960, 286, 478, 1936, 445, 51596], "temperature": 0.0, "avg_logprob": -0.10054954886436462, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.0015974723501130939}, {"id": 41, "seek": 32844, "start": 328.44, "end": 338.28, "text": " rambling. I hope you don't expect to neither deep or concise elaboration so I'm warning you already", "tokens": [50364, 367, 19391, 13, 286, 1454, 291, 500, 380, 2066, 281, 9662, 2452, 420, 44882, 16298, 399, 370, 286, 478, 9164, 291, 1217, 50856], "temperature": 0.0, "avg_logprob": -0.14026824464189244, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.009704988449811935}, {"id": 42, "seek": 32844, "start": 338.28, "end": 349.24, "text": " but nonetheless I would really recommend that you listen up. Okay so first off as you have just", "tokens": [50856, 457, 26756, 286, 576, 534, 2748, 300, 291, 2140, 493, 13, 1033, 370, 700, 766, 382, 291, 362, 445, 51404], "temperature": 0.0, "avg_logprob": -0.14026824464189244, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.009704988449811935}, {"id": 43, "seek": 34924, "start": 349.24, "end": 359.96000000000004, "text": " seen in the bullet points I will explain to you like sketch out this result from the 90s which", "tokens": [50364, 1612, 294, 264, 11632, 2793, 286, 486, 2903, 281, 291, 411, 12325, 484, 341, 1874, 490, 264, 4289, 82, 597, 50900], "temperature": 0.0, "avg_logprob": -0.09381415767054405, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.01242772862315178}, {"id": 44, "seek": 34924, "start": 359.96000000000004, "end": 366.68, "text": " concerns neural network Gaussian processes. So there are nice results that have been found there", "tokens": [50900, 7389, 18161, 3209, 39148, 7555, 13, 407, 456, 366, 1481, 3542, 300, 362, 668, 1352, 456, 51236], "temperature": 0.0, "avg_logprob": -0.09381415767054405, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.01242772862315178}, {"id": 45, "seek": 34924, "start": 366.68, "end": 373.48, "text": " and have since been extended to a broad range of neural network architectures. So this is", "tokens": [51236, 293, 362, 1670, 668, 10913, 281, 257, 4152, 3613, 295, 18161, 3209, 6331, 1303, 13, 407, 341, 307, 51576], "temperature": 0.0, "avg_logprob": -0.09381415767054405, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.01242772862315178}, {"id": 46, "seek": 37348, "start": 373.56, "end": 380.20000000000005, "text": " mathematical theory results for neural networks that hold in general. In this video for simplicity", "tokens": [50368, 18894, 5261, 3542, 337, 18161, 9590, 300, 1797, 294, 2674, 13, 682, 341, 960, 337, 25632, 50700], "temperature": 0.0, "avg_logprob": -0.10839892596733279, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.009552898816764355}, {"id": 47, "seek": 37348, "start": 380.20000000000005, "end": 387.24, "text": " we can just look at fully connected in the sense that you see in the screen here feed", "tokens": [50700, 321, 393, 445, 574, 412, 4498, 4582, 294, 264, 2020, 300, 291, 536, 294, 264, 2568, 510, 3154, 51052], "temperature": 0.0, "avg_logprob": -0.10839892596733279, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.009552898816764355}, {"id": 48, "seek": 37348, "start": 388.04, "end": 395.32, "text": " forward neural networks and for this video it's not even super relevant how many inputs", "tokens": [51092, 2128, 18161, 9590, 293, 337, 341, 960, 309, 311, 406, 754, 1687, 7340, 577, 867, 15743, 51456], "temperature": 0.0, "avg_logprob": -0.10839892596733279, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.009552898816764355}, {"id": 49, "seek": 37348, "start": 395.32, "end": 401.40000000000003, "text": " outputs you have. Basically you have let's say at least one input some float or if you want a real", "tokens": [51456, 23930, 291, 362, 13, 8537, 291, 362, 718, 311, 584, 412, 1935, 472, 4846, 512, 15706, 420, 498, 291, 528, 257, 957, 51760], "temperature": 0.0, "avg_logprob": -0.10839892596733279, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.009552898816764355}, {"id": 50, "seek": 40140, "start": 401.4, "end": 409.08, "text": " number x that goes in one real number y that goes out and in the middle you have a bunch of", "tokens": [50364, 1230, 2031, 300, 1709, 294, 472, 957, 1230, 288, 300, 1709, 484, 293, 294, 264, 2808, 291, 362, 257, 3840, 295, 50748], "temperature": 0.0, "avg_logprob": -0.11629953277244996, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.002357770223170519}, {"id": 51, "seek": 40140, "start": 409.08, "end": 414.91999999999996, "text": " hidden layers in the image you just see one but you know for the sake of it you might think of two", "tokens": [50748, 7633, 7914, 294, 264, 3256, 291, 445, 536, 472, 457, 291, 458, 337, 264, 9717, 295, 309, 291, 1062, 519, 295, 732, 51040], "temperature": 0.0, "avg_logprob": -0.11629953277244996, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.002357770223170519}, {"id": 52, "seek": 40140, "start": 414.91999999999996, "end": 424.52, "text": " three four something like that and you see the nodes in this one hidden layer here and the", "tokens": [51040, 1045, 1451, 746, 411, 300, 293, 291, 536, 264, 13891, 294, 341, 472, 7633, 4583, 510, 293, 264, 51520], "temperature": 0.0, "avg_logprob": -0.11629953277244996, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.002357770223170519}, {"id": 53, "seek": 40140, "start": 424.52, "end": 430.52, "text": " theory that we are going to discuss kicks in once you have a really big network. So this", "tokens": [51520, 5261, 300, 321, 366, 516, 281, 2248, 21293, 294, 1564, 291, 362, 257, 534, 955, 3209, 13, 407, 341, 51820], "temperature": 0.0, "avg_logprob": -0.11629953277244996, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.002357770223170519}, {"id": 54, "seek": 43140, "start": 431.88, "end": 439.32, "text": " you think of the number of nodes in each hidden layer here going to infinity or you know it will", "tokens": [50388, 291, 519, 295, 264, 1230, 295, 13891, 294, 1184, 7633, 4583, 510, 516, 281, 13202, 420, 291, 458, 309, 486, 50760], "temperature": 0.0, "avg_logprob": -0.09246526780675669, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0011506134178489447}, {"id": 55, "seek": 43140, "start": 439.32, "end": 447.15999999999997, "text": " suffice if you think of a huge number a bunch of billions of billions and the thing that then", "tokens": [50760, 3889, 573, 498, 291, 519, 295, 257, 2603, 1230, 257, 3840, 295, 17375, 295, 17375, 293, 264, 551, 300, 550, 51152], "temperature": 0.0, "avg_logprob": -0.09246526780675669, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0011506134178489447}, {"id": 56, "seek": 43140, "start": 447.15999999999997, "end": 455.0, "text": " emerges with large networks is not just the universal approximation theorem that says", "tokens": [51152, 38965, 365, 2416, 9590, 307, 406, 445, 264, 11455, 28023, 20904, 300, 1619, 51544], "temperature": 0.0, "avg_logprob": -0.09246526780675669, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0011506134178489447}, {"id": 57, "seek": 45500, "start": 455.56, "end": 461.56, "text": " the the nice functions let's say continuous functions from r to r are represented densely", "tokens": [50392, 264, 264, 1481, 6828, 718, 311, 584, 10957, 6828, 490, 367, 281, 367, 366, 10379, 24505, 736, 50692], "temperature": 0.0, "avg_logprob": -0.08752490252983279, "compression_ratio": 1.5080645161290323, "no_speech_prob": 0.013839875347912312}, {"id": 58, "seek": 45500, "start": 461.56, "end": 471.8, "text": " by this sort of neural network by these weights but what also emerges in this large network limit", "tokens": [50692, 538, 341, 1333, 295, 18161, 3209, 538, 613, 17443, 457, 437, 611, 38965, 294, 341, 2416, 3209, 4948, 51204], "temperature": 0.0, "avg_logprob": -0.08752490252983279, "compression_ratio": 1.5080645161290323, "no_speech_prob": 0.013839875347912312}, {"id": 59, "seek": 47180, "start": 471.8, "end": 485.40000000000003, "text": " is that the dependency of the output for fixed input and probabilistic weights", "tokens": [50364, 307, 300, 264, 33621, 295, 264, 5598, 337, 6806, 4846, 293, 31959, 3142, 17443, 51044], "temperature": 0.0, "avg_logprob": -0.12209360088620867, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0007207331364043057}, {"id": 60, "seek": 47180, "start": 486.92, "end": 492.52, "text": " takes on a very deterministic character okay so this is this neural network Gaussian process", "tokens": [51120, 2516, 322, 257, 588, 15957, 3142, 2517, 1392, 370, 341, 307, 341, 18161, 3209, 39148, 1399, 51400], "temperature": 0.0, "avg_logprob": -0.12209360088620867, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0007207331364043057}, {"id": 61, "seek": 47180, "start": 492.52, "end": 498.28000000000003, "text": " phenomenon and then I will explain in a second in more detail but basically what we want to", "tokens": [51400, 14029, 293, 550, 286, 486, 2903, 294, 257, 1150, 294, 544, 2607, 457, 1936, 437, 321, 528, 281, 51688], "temperature": 0.0, "avg_logprob": -0.12209360088620867, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.0007207331364043057}, {"id": 62, "seek": 49828, "start": 498.28, "end": 504.52, "text": " hear first look at is we take one fixed architecture some big neural network with let's say three", "tokens": [50364, 1568, 700, 574, 412, 307, 321, 747, 472, 6806, 9482, 512, 955, 18161, 3209, 365, 718, 311, 584, 1045, 50676], "temperature": 0.0, "avg_logprob": -0.09129416374933153, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0013445902150124311}, {"id": 63, "seek": 49828, "start": 504.52, "end": 512.12, "text": " hidden layers and all the layers are very large and what we're here are first concerned with", "tokens": [50676, 7633, 7914, 293, 439, 264, 7914, 366, 588, 2416, 293, 437, 321, 434, 510, 366, 700, 5922, 365, 51056], "temperature": 0.0, "avg_logprob": -0.09129416374933153, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0013445902150124311}, {"id": 64, "seek": 49828, "start": 512.12, "end": 517.64, "text": " before we talk about tangent kernels before we talk about quantum field theory is we few", "tokens": [51056, 949, 321, 751, 466, 27747, 23434, 1625, 949, 321, 751, 466, 13018, 2519, 5261, 307, 321, 1326, 51332], "temperature": 0.0, "avg_logprob": -0.09129416374933153, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0013445902150124311}, {"id": 65, "seek": 49828, "start": 519.4, "end": 525.0, "text": " we're concerned with the random initialization of these networks right so let's say you are on a", "tokens": [51420, 321, 434, 5922, 365, 264, 4974, 5883, 2144, 295, 613, 9590, 558, 370, 718, 311, 584, 291, 366, 322, 257, 51700], "temperature": 0.0, "avg_logprob": -0.09129416374933153, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.0013445902150124311}, {"id": 66, "seek": 52500, "start": 525.0, "end": 536.52, "text": " computer you you have this network encoded on your GPU or whatever and or then the weights really", "tokens": [50364, 3820, 291, 291, 362, 341, 3209, 2058, 12340, 322, 428, 18407, 420, 2035, 293, 420, 550, 264, 17443, 534, 50940], "temperature": 0.0, "avg_logprob": -0.1798412876744424, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0011869472218677402}, {"id": 67, "seek": 52500, "start": 536.52, "end": 544.76, "text": " I mean the you have the architecture laid out the way in which all the the the float data", "tokens": [50940, 286, 914, 264, 291, 362, 264, 9482, 9897, 484, 264, 636, 294, 597, 439, 264, 264, 264, 15706, 1412, 51352], "temperature": 0.0, "avg_logprob": -0.1798412876744424, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0011869472218677402}, {"id": 68, "seek": 52500, "start": 545.72, "end": 554.68, "text": " pass to each other naturally if you have this this float types in every realized configuration", "tokens": [51400, 1320, 281, 1184, 661, 8195, 498, 291, 362, 341, 341, 15706, 3467, 294, 633, 5334, 11694, 51848], "temperature": 0.0, "avg_logprob": -0.1798412876744424, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.0011869472218677402}, {"id": 69, "seek": 55468, "start": 554.76, "end": 562.5999999999999, "text": " the weights have to have some some value and this gives the start configuration for the", "tokens": [50368, 264, 17443, 362, 281, 362, 512, 512, 2158, 293, 341, 2709, 264, 722, 11694, 337, 264, 50760], "temperature": 0.0, "avg_logprob": -0.06791997567201272, "compression_ratio": 1.84, "no_speech_prob": 0.001726315007545054}, {"id": 70, "seek": 55468, "start": 562.5999999999999, "end": 568.5999999999999, "text": " learning process right in the learning process you're going to probably assign some some", "tokens": [50760, 2539, 1399, 558, 294, 264, 2539, 1399, 291, 434, 516, 281, 1391, 6269, 512, 512, 51060], "temperature": 0.0, "avg_logprob": -0.06791997567201272, "compression_ratio": 1.84, "no_speech_prob": 0.001726315007545054}, {"id": 71, "seek": 55468, "start": 569.4, "end": 574.8399999999999, "text": " loss function and you do gradient descent and then you tweak the network to behave in a certain", "tokens": [51100, 4470, 2445, 293, 291, 360, 16235, 23475, 293, 550, 291, 29879, 264, 3209, 281, 15158, 294, 257, 1629, 51372], "temperature": 0.0, "avg_logprob": -0.06791997567201272, "compression_ratio": 1.84, "no_speech_prob": 0.001726315007545054}, {"id": 72, "seek": 55468, "start": 574.8399999999999, "end": 582.12, "text": " nice way and fulfill some task but to start this process you need to initialize the network you", "tokens": [51372, 1481, 636, 293, 13875, 512, 5633, 457, 281, 722, 341, 1399, 291, 643, 281, 5883, 1125, 264, 3209, 291, 51736], "temperature": 0.0, "avg_logprob": -0.06791997567201272, "compression_ratio": 1.84, "no_speech_prob": 0.001726315007545054}, {"id": 73, "seek": 58212, "start": 582.28, "end": 589.32, "text": " want to maybe explicitly set some weights okay and now what you can do is you can play around with", "tokens": [50372, 528, 281, 1310, 20803, 992, 512, 17443, 1392, 293, 586, 437, 291, 393, 360, 307, 291, 393, 862, 926, 365, 50724], "temperature": 0.0, "avg_logprob": -0.06332782489150318, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.0008827189449220896}, {"id": 74, "seek": 58212, "start": 589.32, "end": 595.32, "text": " what is actually your starting condition right you can say hey shoot all the weights in the beginning", "tokens": [50724, 437, 307, 767, 428, 2891, 4188, 558, 291, 393, 584, 4177, 3076, 439, 264, 17443, 294, 264, 2863, 51024], "temperature": 0.0, "avg_logprob": -0.06332782489150318, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.0008827189449220896}, {"id": 75, "seek": 58212, "start": 595.32, "end": 603.48, "text": " be set to zero or be set to one or and this is the interesting thing here you do a random", "tokens": [51024, 312, 992, 281, 4018, 420, 312, 992, 281, 472, 420, 293, 341, 307, 264, 1880, 551, 510, 291, 360, 257, 4974, 51432], "temperature": 0.0, "avg_logprob": -0.06332782489150318, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.0008827189449220896}, {"id": 76, "seek": 60348, "start": 603.48, "end": 612.28, "text": " initialization of all the weights and biases also so think of you know you're in python you", "tokens": [50364, 5883, 2144, 295, 439, 264, 17443, 293, 32152, 611, 370, 519, 295, 291, 458, 291, 434, 294, 38797, 291, 50804], "temperature": 0.0, "avg_logprob": -0.11421465440229936, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.02094990387558937}, {"id": 77, "seek": 60348, "start": 612.84, "end": 617.96, "text": " take a library and you sample from a normal distribution for all the", "tokens": [50832, 747, 257, 6405, 293, 291, 6889, 490, 257, 2710, 7316, 337, 439, 264, 51088], "temperature": 0.0, "avg_logprob": -0.11421465440229936, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.02094990387558937}, {"id": 78, "seek": 60348, "start": 619.72, "end": 626.44, "text": " trillion weights you sample trillion random numbers and you sample them each from a Gaussian", "tokens": [51176, 18723, 17443, 291, 6889, 18723, 4974, 3547, 293, 291, 6889, 552, 1184, 490, 257, 39148, 51512], "temperature": 0.0, "avg_logprob": -0.11421465440229936, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.02094990387558937}, {"id": 79, "seek": 62644, "start": 627.24, "end": 636.0400000000001, "text": " from a bell curve and then set the corresponding weights like this okay so all these w i j are", "tokens": [50404, 490, 257, 4549, 7605, 293, 550, 992, 264, 11760, 17443, 411, 341, 1392, 370, 439, 613, 261, 741, 361, 366, 50844], "temperature": 0.0, "avg_logprob": -0.15671101022273937, "compression_ratio": 1.4732824427480915, "no_speech_prob": 0.002114662667736411}, {"id": 80, "seek": 62644, "start": 636.0400000000001, "end": 645.08, "text": " sampled from some from some Gaussian and when you like put in some input right we said there's one", "tokens": [50844, 3247, 15551, 490, 512, 490, 512, 39148, 293, 562, 291, 411, 829, 294, 512, 4846, 558, 321, 848, 456, 311, 472, 51296], "temperature": 0.0, "avg_logprob": -0.15671101022273937, "compression_ratio": 1.4732824427480915, "no_speech_prob": 0.002114662667736411}, {"id": 81, "seek": 64508, "start": 645.08, "end": 655.8000000000001, "text": " input let's say you you take the input seven and set x to seven and then feed do the feed forward", "tokens": [50364, 4846, 718, 311, 584, 291, 291, 747, 264, 4846, 3407, 293, 992, 2031, 281, 3407, 293, 550, 3154, 360, 264, 3154, 2128, 50900], "temperature": 0.0, "avg_logprob": -0.1124812989007859, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.028420452028512955}, {"id": 82, "seek": 64508, "start": 655.8000000000001, "end": 663.24, "text": " process the evaluation of the neural network then if these things are random then the output will", "tokens": [50900, 1399, 264, 13344, 295, 264, 18161, 3209, 550, 498, 613, 721, 366, 4974, 550, 264, 5598, 486, 51272], "temperature": 0.0, "avg_logprob": -0.1124812989007859, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.028420452028512955}, {"id": 83, "seek": 64508, "start": 663.24, "end": 669.4000000000001, "text": " also be some essentially random number it will be determined for whatever random weight you have", "tokens": [51272, 611, 312, 512, 4476, 4974, 1230, 309, 486, 312, 9540, 337, 2035, 4974, 3364, 291, 362, 51580], "temperature": 0.0, "avg_logprob": -0.1124812989007859, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.028420452028512955}, {"id": 84, "seek": 66940, "start": 669.4, "end": 678.6, "text": " sampled here and in this way you can think of y for fixed x as a random variable composed of the", "tokens": [50364, 3247, 15551, 510, 293, 294, 341, 636, 291, 393, 519, 295, 288, 337, 6806, 2031, 382, 257, 4974, 7006, 18204, 295, 264, 50824], "temperature": 0.0, "avg_logprob": -0.06421420111585019, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.007226358633488417}, {"id": 85, "seek": 66940, "start": 678.6, "end": 688.76, "text": " random variables w right so we have here's the neural network Gaussian process page the math", "tokens": [50824, 4974, 9102, 261, 558, 370, 321, 362, 510, 311, 264, 18161, 3209, 39148, 1399, 3028, 264, 5221, 51332], "temperature": 0.0, "avg_logprob": -0.06421420111585019, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.007226358633488417}, {"id": 86, "seek": 66940, "start": 688.76, "end": 694.6, "text": " and the proof sketch of this result that we are going to get to is actually described there so the", "tokens": [51332, 293, 264, 8177, 12325, 295, 341, 1874, 300, 321, 366, 516, 281, 483, 281, 307, 767, 7619, 456, 370, 264, 51624], "temperature": 0.0, "avg_logprob": -0.06421420111585019, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.007226358633488417}, {"id": 87, "seek": 69460, "start": 694.6, "end": 701.4, "text": " weights are sampled from some Gaussian we are going to take a Gaussian where the standard deviation", "tokens": [50364, 17443, 366, 3247, 15551, 490, 512, 39148, 321, 366, 516, 281, 747, 257, 39148, 689, 264, 3832, 25163, 50704], "temperature": 0.0, "avg_logprob": -0.06845300159757099, "compression_ratio": 1.68, "no_speech_prob": 0.0004950313013978302}, {"id": 88, "seek": 69460, "start": 702.28, "end": 707.08, "text": " gets tighter and tighter with the number of layers right but if you have a fixed network this is some", "tokens": [50748, 2170, 30443, 293, 30443, 365, 264, 1230, 295, 7914, 558, 457, 498, 291, 362, 257, 6806, 3209, 341, 307, 512, 50988], "temperature": 0.0, "avg_logprob": -0.06845300159757099, "compression_ratio": 1.68, "no_speech_prob": 0.0004950313013978302}, {"id": 89, "seek": 69460, "start": 707.08, "end": 718.36, "text": " fixed variance here and the output that is in a standard way computed from neural network is", "tokens": [50988, 6806, 21977, 510, 293, 264, 5598, 300, 307, 294, 257, 3832, 636, 40610, 490, 18161, 3209, 307, 51552], "temperature": 0.0, "avg_logprob": -0.06845300159757099, "compression_ratio": 1.68, "no_speech_prob": 0.0004950313013978302}, {"id": 90, "seek": 71836, "start": 719.08, "end": 728.92, "text": " um computed as you see here you do fast forward and I think it's fairly easy to believe that just", "tokens": [50400, 1105, 40610, 382, 291, 536, 510, 291, 360, 2370, 2128, 293, 286, 519, 309, 311, 6457, 1858, 281, 1697, 300, 445, 50892], "temperature": 0.0, "avg_logprob": -0.11720059409974114, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.002799857407808304}, {"id": 91, "seek": 71836, "start": 728.92, "end": 734.44, "text": " due to the central limit theorem right the statement that if you have a bunch of independent", "tokens": [50892, 3462, 281, 264, 5777, 4948, 20904, 558, 264, 5629, 300, 498, 291, 362, 257, 3840, 295, 6695, 51168], "temperature": 0.0, "avg_logprob": -0.11720059409974114, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.002799857407808304}, {"id": 92, "seek": 71836, "start": 736.28, "end": 743.48, "text": " random variables if you sum them all up then this is another random variable that will behave", "tokens": [51260, 4974, 9102, 498, 291, 2408, 552, 439, 493, 550, 341, 307, 1071, 4974, 7006, 300, 486, 15158, 51620], "temperature": 0.0, "avg_logprob": -0.11720059409974114, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.002799857407808304}, {"id": 93, "seek": 74348, "start": 743.48, "end": 750.04, "text": " like a Gaussian process right so basically if you sum up random numbers then you usually end up with", "tokens": [50364, 411, 257, 39148, 1399, 558, 370, 1936, 498, 291, 2408, 493, 4974, 3547, 550, 291, 2673, 917, 493, 365, 50692], "temperature": 0.0, "avg_logprob": -0.0812512199477394, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.003705956507474184}, {"id": 94, "seek": 74348, "start": 751.72, "end": 755.24, "text": " if all the conditions are fulfilled all the mathematical conditions then you will end up", "tokens": [50776, 498, 439, 264, 4487, 366, 21380, 439, 264, 18894, 4487, 550, 291, 486, 917, 493, 50952], "temperature": 0.0, "avg_logprob": -0.0812512199477394, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.003705956507474184}, {"id": 95, "seek": 74348, "start": 755.24, "end": 759.48, "text": " with a Gaussian this is the statement of the central central limit theorem and this exact", "tokens": [50952, 365, 257, 39148, 341, 307, 264, 5629, 295, 264, 5777, 5777, 4948, 20904, 293, 341, 1900, 51164], "temperature": 0.0, "avg_logprob": -0.0812512199477394, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.003705956507474184}, {"id": 96, "seek": 74348, "start": 759.48, "end": 764.12, "text": " thing applies here also you know maybe there's some non-linearities involved and maybe there's", "tokens": [51164, 551, 13165, 510, 611, 291, 458, 1310, 456, 311, 512, 2107, 12, 28263, 1088, 3288, 293, 1310, 456, 311, 51396], "temperature": 0.0, "avg_logprob": -0.0812512199477394, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.003705956507474184}, {"id": 97, "seek": 74348, "start": 764.12, "end": 771.24, "text": " different steps but in the end the final output of the network here in this picture in the on the", "tokens": [51396, 819, 4439, 457, 294, 264, 917, 264, 2572, 5598, 295, 264, 3209, 510, 294, 341, 3036, 294, 264, 322, 264, 51752], "temperature": 0.0, "avg_logprob": -0.0812512199477394, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.003705956507474184}, {"id": 98, "seek": 77124, "start": 771.24, "end": 778.84, "text": " last layer this set is still a function of all these small Gaussians and because there are so many", "tokens": [50364, 1036, 4583, 341, 992, 307, 920, 257, 2445, 295, 439, 613, 1359, 10384, 2023, 2567, 293, 570, 456, 366, 370, 867, 50744], "temperature": 0.0, "avg_logprob": -0.09986574258377302, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0008037775405682623}, {"id": 99, "seek": 77124, "start": 778.84, "end": 787.24, "text": " sums this is again just a Gaussian process right so and so this says that in the limit and this", "tokens": [50744, 34499, 341, 307, 797, 445, 257, 39148, 1399, 558, 370, 293, 370, 341, 1619, 300, 294, 264, 4948, 293, 341, 51164], "temperature": 0.0, "avg_logprob": -0.09986574258377302, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0008037775405682623}, {"id": 100, "seek": 77124, "start": 787.24, "end": 795.16, "text": " is especially emerges if you have enough width if the width is big enough so that the central", "tokens": [51164, 307, 2318, 38965, 498, 291, 362, 1547, 11402, 498, 264, 11402, 307, 955, 1547, 370, 300, 264, 5777, 51560], "temperature": 0.0, "avg_logprob": -0.09986574258377302, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0008037775405682623}, {"id": 101, "seek": 79516, "start": 795.16, "end": 801.4, "text": " limit theorem really kicks in but this basically means that the as a random field as a random", "tokens": [50364, 4948, 20904, 534, 21293, 294, 457, 341, 1936, 1355, 300, 264, 382, 257, 4974, 2519, 382, 257, 4974, 50676], "temperature": 0.0, "avg_logprob": -0.10597182089282621, "compression_ratio": 1.6568047337278107, "no_speech_prob": 0.008058079518377781}, {"id": 102, "seek": 79516, "start": 801.4, "end": 808.04, "text": " variable the output of the neural network has very nice stochastic process properties and it's", "tokens": [50676, 7006, 264, 5598, 295, 264, 18161, 3209, 575, 588, 1481, 342, 8997, 2750, 1399, 7221, 293, 309, 311, 51008], "temperature": 0.0, "avg_logprob": -0.10597182089282621, "compression_ratio": 1.6568047337278107, "no_speech_prob": 0.008058079518377781}, {"id": 103, "seek": 79516, "start": 808.04, "end": 814.4399999999999, "text": " it's a Gaussian process in particular one of the nicest you can have basically here on this", "tokens": [51008, 309, 311, 257, 39148, 1399, 294, 1729, 472, 295, 264, 45516, 291, 393, 362, 1936, 510, 322, 341, 51328], "temperature": 0.0, "avg_logprob": -0.10597182089282621, "compression_ratio": 1.6568047337278107, "no_speech_prob": 0.008058079518377781}, {"id": 104, "seek": 81444, "start": 814.44, "end": 825.1600000000001, "text": " web page on this Wikipedia page there's also like this this this example animation so here they have", "tokens": [50364, 3670, 3028, 322, 341, 28999, 3028, 456, 311, 611, 411, 341, 341, 341, 1365, 9603, 370, 510, 436, 362, 50900], "temperature": 0.0, "avg_logprob": -0.148539366040911, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.11097010225057602}, {"id": 105, "seek": 81444, "start": 825.72, "end": 830.44, "text": " some network with three inputs right as I said it doesn't have to be three it suffices to think of", "tokens": [50928, 512, 3209, 365, 1045, 15743, 558, 382, 286, 848, 309, 1177, 380, 362, 281, 312, 1045, 309, 3889, 1473, 281, 519, 295, 51164], "temperature": 0.0, "avg_logprob": -0.148539366040911, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.11097010225057602}, {"id": 106, "seek": 81444, "start": 830.44, "end": 836.5200000000001, "text": " one and they have a bunch of outputs again it suffices to think of one so one green input one", "tokens": [51164, 472, 293, 436, 362, 257, 3840, 295, 23930, 797, 309, 3889, 1473, 281, 519, 295, 472, 370, 472, 3092, 4846, 472, 51468], "temperature": 0.0, "avg_logprob": -0.148539366040911, "compression_ratio": 1.7544910179640718, "no_speech_prob": 0.11097010225057602}, {"id": 107, "seek": 83652, "start": 837.24, "end": 847.16, "text": " yellow output and a bunch of nodes in in bunch of layers in this case two layers one layer would", "tokens": [50400, 5566, 5598, 293, 257, 3840, 295, 13891, 294, 294, 3840, 295, 7914, 294, 341, 1389, 732, 7914, 472, 4583, 576, 50896], "temperature": 0.0, "avg_logprob": -0.10179255990421071, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.01938636787235737}, {"id": 108, "seek": 83652, "start": 847.16, "end": 853.96, "text": " also work you see on the right side I mean you probably don't see just because of my face here", "tokens": [50896, 611, 589, 291, 536, 322, 264, 558, 1252, 286, 914, 291, 1391, 500, 380, 536, 445, 570, 295, 452, 1851, 510, 51236], "temperature": 0.0, "avg_logprob": -0.10179255990421071, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.01938636787235737}, {"id": 109, "seek": 83652, "start": 853.96, "end": 860.68, "text": " but I mean doesn't really matter too much it's just a bunch of like random distribution the", "tokens": [51236, 457, 286, 914, 1177, 380, 534, 1871, 886, 709, 309, 311, 445, 257, 3840, 295, 411, 4974, 7316, 264, 51572], "temperature": 0.0, "avg_logprob": -0.10179255990421071, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.01938636787235737}, {"id": 110, "seek": 86068, "start": 860.68, "end": 867.4, "text": " statement is that for fixed input the green value again let's say there's one green input and it's", "tokens": [50364, 5629, 307, 300, 337, 6806, 4846, 264, 3092, 2158, 797, 718, 311, 584, 456, 311, 472, 3092, 4846, 293, 309, 311, 50700], "temperature": 0.0, "avg_logprob": -0.10191360872183274, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0018620899645611644}, {"id": 111, "seek": 86068, "start": 867.4, "end": 877.0799999999999, "text": " set to this the float number seven if you fix if you go up with the the number of nodes and", "tokens": [50700, 992, 281, 341, 264, 15706, 1230, 3407, 498, 291, 3191, 498, 291, 352, 493, 365, 264, 264, 1230, 295, 13891, 293, 51184], "temperature": 0.0, "avg_logprob": -0.10191360872183274, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0018620899645611644}, {"id": 112, "seek": 86068, "start": 877.0799999999999, "end": 883.4, "text": " random initialize this with weights and biases then just by the central limit theorem which is", "tokens": [51184, 4974, 5883, 1125, 341, 365, 17443, 293, 32152, 550, 445, 538, 264, 5777, 4948, 20904, 597, 307, 51500], "temperature": 0.0, "avg_logprob": -0.10191360872183274, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0018620899645611644}, {"id": 113, "seek": 88340, "start": 884.12, "end": 892.36, "text": " dependent on this this seven and this bunch of random numbers the output y1 here this yellow", "tokens": [50400, 12334, 322, 341, 341, 3407, 293, 341, 3840, 295, 4974, 3547, 264, 5598, 288, 16, 510, 341, 5566, 50812], "temperature": 0.0, "avg_logprob": -0.07026607806865985, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.004606637172400951}, {"id": 114, "seek": 88340, "start": 892.36, "end": 901.4, "text": " output will behave like a Gaussian just by the central limit theorem and in this case there's", "tokens": [50812, 5598, 486, 15158, 411, 257, 39148, 445, 538, 264, 5777, 4948, 20904, 293, 294, 341, 1389, 456, 311, 51264], "temperature": 0.0, "avg_logprob": -0.07026607806865985, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.004606637172400951}, {"id": 115, "seek": 88340, "start": 901.4, "end": 909.16, "text": " two outputs so you can draw a plot and the statement is then that both y1 and y2 behaving", "tokens": [51264, 732, 23930, 370, 291, 393, 2642, 257, 7542, 293, 264, 5629, 307, 550, 300, 1293, 288, 16, 293, 288, 17, 35263, 51652], "temperature": 0.0, "avg_logprob": -0.07026607806865985, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.004606637172400951}, {"id": 116, "seek": 90916, "start": 909.16, "end": 916.8399999999999, "text": " like Gaussians independently from another like it's not a statistic statement but each behave", "tokens": [50364, 411, 10384, 2023, 2567, 21761, 490, 1071, 411, 309, 311, 406, 257, 29588, 5629, 457, 1184, 15158, 50748], "temperature": 0.0, "avg_logprob": -0.1391831324650691, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.007340652868151665}, {"id": 117, "seek": 90916, "start": 916.8399999999999, "end": 923.8, "text": " like a like a Gaussian they have some peak and so on the plot you get another nice Gaussian with", "tokens": [50748, 411, 257, 411, 257, 39148, 436, 362, 512, 10651, 293, 370, 322, 264, 7542, 291, 483, 1071, 1481, 39148, 365, 51096], "temperature": 0.0, "avg_logprob": -0.1391831324650691, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.007340652868151665}, {"id": 118, "seek": 90916, "start": 923.8, "end": 934.28, "text": " some peak here and so if then the press play again if the network becomes even bigger then you get", "tokens": [51096, 512, 10651, 510, 293, 370, 498, 550, 264, 1886, 862, 797, 498, 264, 3209, 3643, 754, 3801, 550, 291, 483, 51620], "temperature": 0.0, "avg_logprob": -0.1391831324650691, "compression_ratio": 1.7621951219512195, "no_speech_prob": 0.007340652868151665}, {"id": 119, "seek": 93428, "start": 934.28, "end": 942.1999999999999, "text": " like this perfect Gaussian where it this just says that it has this maximum expected value here in", "tokens": [50364, 411, 341, 2176, 39148, 689, 309, 341, 445, 1619, 300, 309, 575, 341, 6674, 5176, 2158, 510, 294, 50760], "temperature": 0.0, "avg_logprob": -0.1321536494839576, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.004827339202165604}, {"id": 120, "seek": 93428, "start": 942.1999999999999, "end": 950.52, "text": " the middle and this goes for all the outputs right so this is the result that that that", "tokens": [50760, 264, 2808, 293, 341, 1709, 337, 439, 264, 23930, 558, 370, 341, 307, 264, 1874, 300, 300, 300, 51176], "temperature": 0.0, "avg_logprob": -0.1321536494839576, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.004827339202165604}, {"id": 121, "seek": 93428, "start": 953.64, "end": 961.56, "text": " the okay I closed the neural network Gaussian process page but doesn't matter this is the result", "tokens": [51332, 264, 1392, 286, 5395, 264, 18161, 3209, 39148, 1399, 3028, 457, 1177, 380, 1871, 341, 307, 264, 1874, 51728], "temperature": 0.0, "avg_logprob": -0.1321536494839576, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.004827339202165604}, {"id": 122, "seek": 96156, "start": 961.56, "end": 969.16, "text": " that just because of statistics you the network if it's large enough at random initialization", "tokens": [50364, 300, 445, 570, 295, 12523, 291, 264, 3209, 498, 309, 311, 2416, 1547, 412, 4974, 5883, 2144, 50744], "temperature": 0.0, "avg_logprob": -0.12095703397478376, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002549713244661689}, {"id": 123, "seek": 96156, "start": 969.16, "end": 977.2399999999999, "text": " behaves like a Gaussian we will not need it for the this video but if you want to take a look", "tokens": [50744, 36896, 411, 257, 39148, 321, 486, 406, 643, 309, 337, 264, 341, 960, 457, 498, 291, 528, 281, 747, 257, 574, 51148], "temperature": 0.0, "avg_logprob": -0.12095703397478376, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002549713244661689}, {"id": 124, "seek": 96156, "start": 977.2399999999999, "end": 982.1199999999999, "text": " this is the the formal definition of a Gaussian Gaussian process I mean to motivate this basically", "tokens": [51148, 341, 307, 264, 264, 9860, 7123, 295, 257, 39148, 39148, 1399, 286, 914, 281, 28497, 341, 1936, 51392], "temperature": 0.0, "avg_logprob": -0.12095703397478376, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002549713244661689}, {"id": 125, "seek": 96156, "start": 982.1199999999999, "end": 986.4399999999999, "text": " you think you know a Gaussian is something which if you do the Fourier transform it's again like", "tokens": [51392, 291, 519, 291, 458, 257, 39148, 307, 746, 597, 498, 291, 360, 264, 36810, 4088, 309, 311, 797, 411, 51608], "temperature": 0.0, "avg_logprob": -0.12095703397478376, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.002549713244661689}, {"id": 126, "seek": 98644, "start": 986.44, "end": 993.5600000000001, "text": " a Gaussian and the Gaussian process is abstractly defined as this random variable or sequence of", "tokens": [50364, 257, 39148, 293, 264, 39148, 1399, 307, 12649, 356, 7642, 382, 341, 4974, 7006, 420, 8310, 295, 50720], "temperature": 0.0, "avg_logprob": -0.08902876098434646, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013244014699012041}, {"id": 127, "seek": 98644, "start": 993.5600000000001, "end": 998.7600000000001, "text": " random variables where the characteristic function the expectation value of this phase", "tokens": [50720, 4974, 9102, 689, 264, 16282, 2445, 264, 14334, 2158, 295, 341, 5574, 50980], "temperature": 0.0, "avg_logprob": -0.08902876098434646, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013244014699012041}, {"id": 128, "seek": 98644, "start": 999.48, "end": 1006.7600000000001, "text": " is this Gaussian with a certain mean we are not going to need this the Fourier transform will", "tokens": [51016, 307, 341, 39148, 365, 257, 1629, 914, 321, 366, 406, 516, 281, 643, 341, 264, 36810, 4088, 486, 51380], "temperature": 0.0, "avg_logprob": -0.08902876098434646, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013244014699012041}, {"id": 129, "seek": 98644, "start": 1006.7600000000001, "end": 1014.84, "text": " pop up again when I talk about the quantum field theories but suffice to say the nice thing is", "tokens": [51380, 1665, 493, 797, 562, 286, 751, 466, 264, 13018, 2519, 13667, 457, 3889, 573, 281, 584, 264, 1481, 551, 307, 51784], "temperature": 0.0, "avg_logprob": -0.08902876098434646, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013244014699012041}, {"id": 130, "seek": 101484, "start": 1014.84, "end": 1021.1600000000001, "text": " that the neural network the big neural networks behave like Gaussian processes sorry if I repeat", "tokens": [50364, 300, 264, 18161, 3209, 264, 955, 18161, 9590, 15158, 411, 39148, 7555, 2597, 498, 286, 7149, 50680], "temperature": 0.0, "avg_logprob": -0.12424009274213742, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0017536593368276954}, {"id": 131, "seek": 101484, "start": 1021.1600000000001, "end": 1031.96, "text": " myself okay so do we do physics first or do we do neural tangent kernels first", "tokens": [50680, 2059, 1392, 370, 360, 321, 360, 10649, 700, 420, 360, 321, 360, 18161, 27747, 23434, 1625, 700, 51220], "temperature": 0.0, "avg_logprob": -0.12424009274213742, "compression_ratio": 1.4830508474576272, "no_speech_prob": 0.0017536593368276954}, {"id": 132, "seek": 103196, "start": 1032.44, "end": 1041.4, "text": " um let me actually um yeah let me actually say something about", "tokens": [50388, 1105, 718, 385, 767, 1105, 1338, 718, 385, 767, 584, 746, 466, 50836], "temperature": 0.0, "avg_logprob": -0.16771808971058239, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.004902174696326256}, {"id": 133, "seek": 103196, "start": 1043.08, "end": 1051.32, "text": " the neural tangent kernel so um could we know now that the the network at the start behaves", "tokens": [50920, 264, 18161, 27747, 28256, 370, 1105, 727, 321, 458, 586, 300, 264, 264, 3209, 412, 264, 722, 36896, 51332], "temperature": 0.0, "avg_logprob": -0.16771808971058239, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.004902174696326256}, {"id": 134, "seek": 103196, "start": 1051.32, "end": 1057.48, "text": " like this Gaussian for all inputs and if you do the learning process then this is about", "tokens": [51332, 411, 341, 39148, 337, 439, 15743, 293, 498, 291, 360, 264, 2539, 1399, 550, 341, 307, 466, 51640], "temperature": 0.0, "avg_logprob": -0.16771808971058239, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.004902174696326256}, {"id": 135, "seek": 105748, "start": 1057.88, "end": 1068.44, "text": " um giving it a test data and then moving um in parameter space from wherever we random started", "tokens": [50384, 1105, 2902, 309, 257, 1500, 1412, 293, 550, 2684, 1105, 294, 13075, 1901, 490, 8660, 321, 4974, 1409, 50912], "temperature": 0.0, "avg_logprob": -0.09837737012265334, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.0016734726959839463}, {"id": 136, "seek": 105748, "start": 1069.0, "end": 1076.44, "text": " to some other position in in weight space and I have made a bunch of videos on gradient descent", "tokens": [50940, 281, 512, 661, 2535, 294, 294, 3364, 1901, 293, 286, 362, 1027, 257, 3840, 295, 2145, 322, 16235, 23475, 51312], "temperature": 0.0, "avg_logprob": -0.09837737012265334, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.0016734726959839463}, {"id": 137, "seek": 105748, "start": 1076.44, "end": 1082.76, "text": " I will not explain it here but suffice to say you have a space of weights and then the the weights", "tokens": [51312, 286, 486, 406, 2903, 309, 510, 457, 3889, 573, 281, 584, 291, 362, 257, 1901, 295, 17443, 293, 550, 264, 264, 17443, 51628], "temperature": 0.0, "avg_logprob": -0.09837737012265334, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.0016734726959839463}, {"id": 138, "seek": 108276, "start": 1082.76, "end": 1089.0, "text": " follow some path and you do that in a way that optimizes some goal that you have right some", "tokens": [50364, 1524, 512, 3100, 293, 291, 360, 300, 294, 257, 636, 300, 5028, 5660, 512, 3387, 300, 291, 362, 558, 512, 50676], "temperature": 0.0, "avg_logprob": -0.10686927766942267, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0012441854923963547}, {"id": 139, "seek": 108276, "start": 1089.0, "end": 1103.64, "text": " task for the neural network and I think I sketched it out here so as is common we're dealing with", "tokens": [50676, 5633, 337, 264, 18161, 3209, 293, 286, 519, 286, 12325, 292, 309, 484, 510, 370, 382, 307, 2689, 321, 434, 6260, 365, 51408], "temperature": 0.0, "avg_logprob": -0.10686927766942267, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0012441854923963547}, {"id": 140, "seek": 108276, "start": 1103.64, "end": 1108.6, "text": " not only here with a large neural network so that the the theory becomes simpler and nicer", "tokens": [51408, 406, 787, 510, 365, 257, 2416, 18161, 3209, 370, 300, 264, 264, 5261, 3643, 18587, 293, 22842, 51656], "temperature": 0.0, "avg_logprob": -0.10686927766942267, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0012441854923963547}, {"id": 141, "seek": 110860, "start": 1108.6, "end": 1114.04, "text": " but also we are matching we have so much compute that we can do really small step sizes", "tokens": [50364, 457, 611, 321, 366, 14324, 321, 362, 370, 709, 14722, 300, 321, 393, 360, 534, 1359, 1823, 11602, 50636], "temperature": 0.0, "avg_logprob": -0.10845480543194395, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.00043035976705141366}, {"id": 142, "seek": 110860, "start": 1114.04, "end": 1120.04, "text": " so that we can then in the limit talk of the behavior of the network as in a differentiable", "tokens": [50636, 370, 300, 321, 393, 550, 294, 264, 4948, 751, 295, 264, 5223, 295, 264, 3209, 382, 294, 257, 819, 9364, 50936], "temperature": 0.0, "avg_logprob": -0.10845480543194395, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.00043035976705141366}, {"id": 143, "seek": 110860, "start": 1120.04, "end": 1131.3999999999999, "text": " way where we say the the the motion of the in path space in a parameter space can be described", "tokens": [50936, 636, 689, 321, 584, 264, 264, 264, 5394, 295, 264, 294, 3100, 1901, 294, 257, 13075, 1901, 393, 312, 7619, 51504], "temperature": 0.0, "avg_logprob": -0.10845480543194395, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.00043035976705141366}, {"id": 144, "seek": 113140, "start": 1131.4, "end": 1139.48, "text": " you know with literally just calculus differentials and so the gradient descent algorithm what we are", "tokens": [50364, 291, 458, 365, 3736, 445, 33400, 819, 12356, 293, 370, 264, 16235, 23475, 9284, 437, 321, 366, 50768], "temperature": 0.0, "avg_logprob": -0.06443041424418605, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.007569573353976011}, {"id": 145, "seek": 113140, "start": 1139.48, "end": 1146.2, "text": " doing really is um you know as per instruction of the algorithm we say the change of the weights", "tokens": [50768, 884, 534, 307, 1105, 291, 458, 382, 680, 10951, 295, 264, 9284, 321, 584, 264, 1319, 295, 264, 17443, 51104], "temperature": 0.0, "avg_logprob": -0.06443041424418605, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.007569573353976011}, {"id": 146, "seek": 113140, "start": 1146.2, "end": 1151.96, "text": " and here I abbreviate all the weights together with this uh theta the the change in the weights", "tokens": [51104, 293, 510, 286, 35839, 473, 439, 264, 17443, 1214, 365, 341, 2232, 9725, 264, 264, 1319, 294, 264, 17443, 51392], "temperature": 0.0, "avg_logprob": -0.06443041424418605, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.007569573353976011}, {"id": 147, "seek": 113140, "start": 1151.96, "end": 1157.72, "text": " right from from one point to the next in the graphic that you just saw um is chosen in a way", "tokens": [51392, 558, 490, 490, 472, 935, 281, 264, 958, 294, 264, 14089, 300, 291, 445, 1866, 1105, 307, 8614, 294, 257, 636, 51680], "temperature": 0.0, "avg_logprob": -0.06443041424418605, "compression_ratio": 1.8254716981132075, "no_speech_prob": 0.007569573353976011}, {"id": 148, "seek": 115772, "start": 1157.72, "end": 1167.96, "text": " that it takes the negative direction of the gradient of some cost function and the cost", "tokens": [50364, 300, 309, 2516, 264, 3671, 3513, 295, 264, 16235, 295, 512, 2063, 2445, 293, 264, 2063, 50876], "temperature": 0.0, "avg_logprob": -0.07852009595450708, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0019827818032354116}, {"id": 149, "seek": 115772, "start": 1167.96, "end": 1175.4, "text": " function in here is the you know the the difference essentially between what the neural network", "tokens": [50876, 2445, 294, 510, 307, 264, 291, 458, 264, 264, 2649, 4476, 1296, 437, 264, 18161, 3209, 51248], "temperature": 0.0, "avg_logprob": -0.07852009595450708, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0019827818032354116}, {"id": 150, "seek": 115772, "start": 1175.4, "end": 1182.52, "text": " currently says versus where we want to get at where set is all the learning data that we have", "tokens": [51248, 4362, 1619, 5717, 689, 321, 528, 281, 483, 412, 689, 992, 307, 439, 264, 2539, 1412, 300, 321, 362, 51604], "temperature": 0.0, "avg_logprob": -0.07852009595450708, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.0019827818032354116}, {"id": 151, "seek": 118252, "start": 1182.52, "end": 1189.0, "text": " available right so we say for all the learning data that we that we have um there is a discrepancy", "tokens": [50364, 2435, 558, 370, 321, 584, 337, 439, 264, 2539, 1412, 300, 321, 300, 321, 362, 1105, 456, 307, 257, 2983, 265, 6040, 1344, 50688], "temperature": 0.0, "avg_logprob": -0.12789888060494756, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0024712905287742615}, {"id": 152, "seek": 118252, "start": 1189.0, "end": 1196.84, "text": " between what the network f currently um says um what's correct and what is actually correct", "tokens": [50688, 1296, 437, 264, 3209, 283, 4362, 1105, 1619, 1105, 437, 311, 3006, 293, 437, 307, 767, 3006, 51080], "temperature": 0.0, "avg_logprob": -0.12789888060494756, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0024712905287742615}, {"id": 153, "seek": 118252, "start": 1197.4, "end": 1204.2, "text": " why I said here is what's actually correct um and we send that up and so this is like the the this", "tokens": [51108, 983, 286, 848, 510, 307, 437, 311, 767, 3006, 1105, 293, 321, 2845, 300, 493, 293, 370, 341, 307, 411, 264, 264, 341, 51448], "temperature": 0.0, "avg_logprob": -0.12789888060494756, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0024712905287742615}, {"id": 154, "seek": 118252, "start": 1205.24, "end": 1210.76, "text": " cost of all learning data together and at every step in time in the learning process", "tokens": [51500, 2063, 295, 439, 2539, 1412, 1214, 293, 412, 633, 1823, 294, 565, 294, 264, 2539, 1399, 51776], "temperature": 0.0, "avg_logprob": -0.12789888060494756, "compression_ratio": 1.8423645320197044, "no_speech_prob": 0.0024712905287742615}, {"id": 155, "seek": 121076, "start": 1211.48, "end": 1218.68, "text": " we go follow this path right and this equation is really just the Newtonian equation um where", "tokens": [50400, 321, 352, 1524, 341, 3100, 558, 293, 341, 5367, 307, 534, 445, 264, 19541, 952, 5367, 1105, 689, 50760], "temperature": 0.0, "avg_logprob": -0.12506996957879318, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0007789407973177731}, {"id": 156, "seek": 121076, "start": 1219.8799999999999, "end": 1224.36, "text": " you know in in physics um theta would be the momentum", "tokens": [50820, 291, 458, 294, 294, 10649, 1105, 9725, 576, 312, 264, 11244, 51044], "temperature": 0.0, "avg_logprob": -0.12506996957879318, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0007789407973177731}, {"id": 157, "seek": 121076, "start": 1228.52, "end": 1235.4, "text": " and where you um look at the situation where the force on the right hand side is governed by a", "tokens": [51252, 293, 689, 291, 1105, 574, 412, 264, 2590, 689, 264, 3464, 322, 264, 558, 1011, 1252, 307, 35529, 538, 257, 51596], "temperature": 0.0, "avg_logprob": -0.12506996957879318, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0007789407973177731}, {"id": 158, "seek": 123540, "start": 1235.4, "end": 1246.3600000000001, "text": " potential and you'll say um the the direction of motion um captured by the momentum is given", "tokens": [50364, 3995, 293, 291, 603, 584, 1105, 264, 264, 3513, 295, 5394, 1105, 11828, 538, 264, 11244, 307, 2212, 50912], "temperature": 0.0, "avg_logprob": -0.13430091982982198, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0011511138873174787}, {"id": 159, "seek": 123540, "start": 1246.3600000000001, "end": 1252.1200000000001, "text": " wherever the you know potential energy will be lowest and that's where the path followed by", "tokens": [50912, 8660, 264, 291, 458, 3995, 2281, 486, 312, 12437, 293, 300, 311, 689, 264, 3100, 6263, 538, 51200], "temperature": 0.0, "avg_logprob": -0.13430091982982198, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0011511138873174787}, {"id": 160, "seek": 123540, "start": 1252.1200000000001, "end": 1260.1200000000001, "text": " the particle in Newtonian physics right so this already looks very um like like this simple physics", "tokens": [51200, 264, 12359, 294, 19541, 952, 10649, 558, 370, 341, 1217, 1542, 588, 1105, 411, 411, 341, 2199, 10649, 51600], "temperature": 0.0, "avg_logprob": -0.13430091982982198, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0011511138873174787}, {"id": 161, "seek": 126012, "start": 1261.08, "end": 1267.4799999999998, "text": " uh equation governing governing the motion in weight space and now given that you have", "tokens": [50412, 2232, 5367, 30054, 30054, 264, 5394, 294, 3364, 1901, 293, 586, 2212, 300, 291, 362, 50732], "temperature": 0.0, "avg_logprob": -0.15630952323355327, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.002395105082541704}, {"id": 162, "seek": 126012, "start": 1268.04, "end": 1274.12, "text": " the behavior of the um the the particle if you will uh going through weight space like", "tokens": [50760, 264, 5223, 295, 264, 1105, 264, 264, 12359, 498, 291, 486, 2232, 516, 807, 3364, 1901, 411, 51064], "temperature": 0.0, "avg_logprob": -0.15630952323355327, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.002395105082541704}, {"id": 163, "seek": 126012, "start": 1274.12, "end": 1282.36, "text": " and give you just saw um and the the potential depends on the outputs at the network on all", "tokens": [51064, 293, 976, 291, 445, 1866, 1105, 293, 264, 264, 3995, 5946, 322, 264, 23930, 412, 264, 3209, 322, 439, 51476], "temperature": 0.0, "avg_logprob": -0.15630952323355327, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.002395105082541704}, {"id": 164, "seek": 126012, "start": 1282.36, "end": 1288.36, "text": " these spaces you can also then um do the calculus and and look at hey how does the output of the", "tokens": [51476, 613, 7673, 291, 393, 611, 550, 1105, 360, 264, 33400, 293, 293, 574, 412, 4177, 577, 775, 264, 5598, 295, 264, 51776], "temperature": 0.0, "avg_logprob": -0.15630952323355327, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.002395105082541704}, {"id": 165, "seek": 128836, "start": 1288.36, "end": 1294.6799999999998, "text": " network which depends on the position where you are at right where the weights determine what the", "tokens": [50364, 3209, 597, 5946, 322, 264, 2535, 689, 291, 366, 412, 558, 689, 264, 17443, 6997, 437, 264, 50680], "temperature": 0.0, "avg_logprob": -0.13997900663916743, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0005524419248104095}, {"id": 166, "seek": 128836, "start": 1294.6799999999998, "end": 1300.4399999999998, "text": " network output will be on all the um learning data how does that the f change and so if you", "tokens": [50680, 3209, 5598, 486, 312, 322, 439, 264, 1105, 2539, 1412, 577, 775, 300, 264, 283, 1319, 293, 370, 498, 291, 50968], "temperature": 0.0, "avg_logprob": -0.13997900663916743, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0005524419248104095}, {"id": 167, "seek": 128836, "start": 1300.4399999999998, "end": 1310.4399999999998, "text": " do them just do the math um and I think I have this is here so this is um newer tangent kernel", "tokens": [50968, 360, 552, 445, 360, 264, 5221, 1105, 293, 286, 519, 286, 362, 341, 307, 510, 370, 341, 307, 1105, 17628, 27747, 28256, 51468], "temperature": 0.0, "avg_logprob": -0.13997900663916743, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0005524419248104095}, {"id": 168, "seek": 131044, "start": 1310.52, "end": 1313.4, "text": " theory um if you", "tokens": [50368, 5261, 1105, 498, 291, 50512], "temperature": 0.0, "avg_logprob": -0.13402417126823873, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.006288595497608185}, {"id": 169, "seek": 131044, "start": 1319.96, "end": 1325.24, "text": " if you do this sort of calculation and if you uh you know if you ever started physics you have to", "tokens": [50840, 498, 291, 360, 341, 1333, 295, 17108, 293, 498, 291, 2232, 291, 458, 498, 291, 1562, 1409, 10649, 291, 362, 281, 51104], "temperature": 0.0, "avg_logprob": -0.13402417126823873, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.006288595497608185}, {"id": 170, "seek": 131044, "start": 1325.24, "end": 1330.44, "text": " this this sort of calculation a million times because basically if you have some observable", "tokens": [51104, 341, 341, 1333, 295, 17108, 257, 2459, 1413, 570, 1936, 498, 291, 362, 512, 9951, 712, 51364], "temperature": 0.0, "avg_logprob": -0.13402417126823873, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.006288595497608185}, {"id": 171, "seek": 131044, "start": 1330.44, "end": 1336.1200000000001, "text": " in a physical system and you know all the the constituents of the particles behave in this", "tokens": [51364, 294, 257, 4001, 1185, 293, 291, 458, 439, 264, 264, 30847, 295, 264, 10007, 15158, 294, 341, 51648], "temperature": 0.0, "avg_logprob": -0.13402417126823873, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.006288595497608185}, {"id": 172, "seek": 133612, "start": 1336.12, "end": 1340.1999999999998, "text": " isn't this way and then I have some observable which is made out of particles and you say how", "tokens": [50364, 1943, 380, 341, 636, 293, 550, 286, 362, 512, 9951, 712, 597, 307, 1027, 484, 295, 10007, 293, 291, 584, 577, 50568], "temperature": 0.0, "avg_logprob": -0.05828295082881533, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.009552155621349812}, {"id": 173, "seek": 133612, "start": 1340.1999999999998, "end": 1346.76, "text": " does this this observable quantity change then um you have to plug in a bunch of partial derivatives", "tokens": [50568, 775, 341, 341, 9951, 712, 11275, 1319, 550, 1105, 291, 362, 281, 5452, 294, 257, 3840, 295, 14641, 33733, 50896], "temperature": 0.0, "avg_logprob": -0.05828295082881533, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.009552155621349812}, {"id": 174, "seek": 133612, "start": 1346.76, "end": 1351.6399999999999, "text": " and then the Hamiltonian comes in and whatever and so on and so forth what comes out of this", "tokens": [50896, 293, 550, 264, 18484, 952, 1487, 294, 293, 2035, 293, 370, 322, 293, 370, 5220, 437, 1487, 484, 295, 341, 51140], "temperature": 0.0, "avg_logprob": -0.05828295082881533, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.009552155621349812}, {"id": 175, "seek": 133612, "start": 1351.6399999999999, "end": 1360.52, "text": " is that the development of the output of the neural network um is governed by some matrix", "tokens": [51140, 307, 300, 264, 3250, 295, 264, 5598, 295, 264, 18161, 3209, 1105, 307, 35529, 538, 512, 8141, 51584], "temperature": 0.0, "avg_logprob": -0.05828295082881533, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.009552155621349812}, {"id": 176, "seek": 136052, "start": 1360.52, "end": 1372.04, "text": " this is the so-called newer tangent kernel and the changes of uh the the loss in uh with respect", "tokens": [50364, 341, 307, 264, 370, 12, 11880, 17628, 27747, 28256, 293, 264, 2962, 295, 2232, 264, 264, 4470, 294, 2232, 365, 3104, 50940], "temperature": 0.0, "avg_logprob": -0.14070587158203124, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0022153789177536964}, {"id": 177, "seek": 136052, "start": 1372.04, "end": 1378.76, "text": " to the to the weights right so I mean I did not adopt this terminology theta is again all the weights", "tokens": [50940, 281, 264, 281, 264, 17443, 558, 370, 286, 914, 286, 630, 406, 6878, 341, 27575, 9725, 307, 797, 439, 264, 17443, 51276], "temperature": 0.0, "avg_logprob": -0.14070587158203124, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0022153789177536964}, {"id": 178, "seek": 136052, "start": 1378.76, "end": 1390.12, "text": " together and um you can do this calculation on one sheet of paper yourself I'm not going to discuss", "tokens": [51276, 1214, 293, 1105, 291, 393, 360, 341, 17108, 322, 472, 8193, 295, 3035, 1803, 286, 478, 406, 516, 281, 2248, 51844], "temperature": 0.0, "avg_logprob": -0.14070587158203124, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0022153789177536964}, {"id": 179, "seek": 139012, "start": 1390.12, "end": 1397.2399999999998, "text": " all the the convention or a notation chosen here but the point is that the evolution of the output", "tokens": [50364, 439, 264, 264, 10286, 420, 257, 24657, 8614, 510, 457, 264, 935, 307, 300, 264, 9303, 295, 264, 5598, 50720], "temperature": 0.0, "avg_logprob": -0.09164215922355652, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.0016220191027969122}, {"id": 180, "seek": 139012, "start": 1397.2399999999998, "end": 1404.04, "text": " on a network from your starting point which might be a random starting point is understood at least", "tokens": [50720, 322, 257, 3209, 490, 428, 2891, 935, 597, 1062, 312, 257, 4974, 2891, 935, 307, 7320, 412, 1935, 51060], "temperature": 0.0, "avg_logprob": -0.09164215922355652, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.0016220191027969122}, {"id": 181, "seek": 139012, "start": 1404.04, "end": 1408.84, "text": " here in theory it's another question of whether you can actually calculate that because this matrix", "tokens": [51060, 510, 294, 5261, 309, 311, 1071, 1168, 295, 1968, 291, 393, 767, 8873, 300, 570, 341, 8141, 51300], "temperature": 0.0, "avg_logprob": -0.09164215922355652, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.0016220191027969122}, {"id": 182, "seek": 139012, "start": 1408.84, "end": 1415.08, "text": " which determines how this the output of the network evolves as you do the learning according", "tokens": [51300, 597, 24799, 577, 341, 264, 5598, 295, 264, 3209, 43737, 382, 291, 360, 264, 2539, 4650, 51612], "temperature": 0.0, "avg_logprob": -0.09164215922355652, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.0016220191027969122}, {"id": 183, "seek": 141508, "start": 1415.08, "end": 1420.84, "text": " to gradient descent is determined by this complicated object theta and the theta is basically", "tokens": [50364, 281, 16235, 23475, 307, 9540, 538, 341, 6179, 2657, 9725, 293, 264, 9725, 307, 1936, 50652], "temperature": 0.0, "avg_logprob": -0.1072549577486717, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.002357976045459509}, {"id": 184, "seek": 141508, "start": 1422.04, "end": 1430.84, "text": " this so-called kernel um this is you can view this as the inner product of the gradient of the", "tokens": [50712, 341, 370, 12, 11880, 28256, 1105, 341, 307, 291, 393, 1910, 341, 382, 264, 7284, 1674, 295, 264, 16235, 295, 264, 51152], "temperature": 0.0, "avg_logprob": -0.1072549577486717, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.002357976045459509}, {"id": 185, "seek": 141508, "start": 1430.84, "end": 1435.6399999999999, "text": " output with research with respect to the weight change and so the interpretation is basically", "tokens": [51152, 5598, 365, 2132, 365, 3104, 281, 264, 3364, 1319, 293, 370, 264, 14174, 307, 1936, 51392], "temperature": 0.0, "avg_logprob": -0.1072549577486717, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.002357976045459509}, {"id": 186, "seek": 143564, "start": 1436.3600000000001, "end": 1447.5600000000002, "text": " that um you look at uh different inputs and then you um you as a kernel a kernel roughly", "tokens": [50400, 300, 1105, 291, 574, 412, 2232, 819, 15743, 293, 550, 291, 1105, 291, 382, 257, 28256, 257, 28256, 9810, 50960], "temperature": 0.0, "avg_logprob": -0.0925514159664031, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.013825771398842335}, {"id": 187, "seek": 143564, "start": 1447.5600000000002, "end": 1454.3600000000001, "text": " charges how similar input data are and this kernel basically looks at uh hey these two", "tokens": [50960, 12235, 577, 2531, 4846, 1412, 366, 293, 341, 28256, 1936, 1542, 412, 2232, 4177, 613, 732, 51300], "temperature": 0.0, "avg_logprob": -0.0925514159664031, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.013825771398842335}, {"id": 188, "seek": 143564, "start": 1454.3600000000001, "end": 1462.2800000000002, "text": " input data are similar if upon a change of the weights the um the response of the network changes", "tokens": [51300, 4846, 1412, 366, 2531, 498, 3564, 257, 1319, 295, 264, 17443, 264, 1105, 264, 4134, 295, 264, 3209, 2962, 51696], "temperature": 0.0, "avg_logprob": -0.0925514159664031, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.013825771398842335}, {"id": 189, "seek": 146228, "start": 1462.28, "end": 1467.8799999999999, "text": " in a similar fashion and you compute this it's in a product in any case this is like an interesting", "tokens": [50364, 294, 257, 2531, 6700, 293, 291, 14722, 341, 309, 311, 294, 257, 1674, 294, 604, 1389, 341, 307, 411, 364, 1880, 50644], "temperature": 0.0, "avg_logprob": -0.11507018162653997, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.0020822458900511265}, {"id": 190, "seek": 146228, "start": 1467.8799999999999, "end": 1477.08, "text": " object that in the end determines how the network behaves um and similar to neural network Gaussian", "tokens": [50644, 2657, 300, 294, 264, 917, 24799, 577, 264, 3209, 36896, 1105, 293, 2531, 281, 18161, 3209, 39148, 51104], "temperature": 0.0, "avg_logprob": -0.11507018162653997, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.0020822458900511265}, {"id": 191, "seek": 146228, "start": 1477.08, "end": 1486.92, "text": " process theory where you say once I have enough um uh weights in my layer some nice theory emerges", "tokens": [51104, 1399, 5261, 689, 291, 584, 1564, 286, 362, 1547, 1105, 2232, 17443, 294, 452, 4583, 512, 1481, 5261, 38965, 51596], "temperature": 0.0, "avg_logprob": -0.11507018162653997, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.0020822458900511265}, {"id": 192, "seek": 148692, "start": 1486.92, "end": 1493.3200000000002, "text": " right in in the Gaussian network case it goes towards a Gaussian it's also the case that", "tokens": [50364, 558, 294, 294, 264, 39148, 3209, 1389, 309, 1709, 3030, 257, 39148, 309, 311, 611, 264, 1389, 300, 50684], "temperature": 0.0, "avg_logprob": -0.12481841062888122, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0034287539310753345}, {"id": 193, "seek": 148692, "start": 1494.04, "end": 1501.16, "text": " for a large network then these these matrix can simplify and then you can get the infinite", "tokens": [50720, 337, 257, 2416, 3209, 550, 613, 613, 8141, 393, 20460, 293, 550, 291, 393, 483, 264, 13785, 51076], "temperature": 0.0, "avg_logprob": -0.12481841062888122, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0034287539310753345}, {"id": 194, "seek": 148692, "start": 1502.2, "end": 1507.72, "text": " size network also to an analytical theory and basically you random initialize you already", "tokens": [51128, 2744, 3209, 611, 281, 364, 29579, 5261, 293, 1936, 291, 4974, 5883, 1125, 291, 1217, 51404], "temperature": 0.0, "avg_logprob": -0.12481841062888122, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0034287539310753345}, {"id": 195, "seek": 148692, "start": 1507.72, "end": 1514.52, "text": " know it's some Gaussian process and then you have some matrix which determines how the network moves", "tokens": [51404, 458, 309, 311, 512, 39148, 1399, 293, 550, 291, 362, 512, 8141, 597, 24799, 577, 264, 3209, 6067, 51744], "temperature": 0.0, "avg_logprob": -0.12481841062888122, "compression_ratio": 1.8316831683168318, "no_speech_prob": 0.0034287539310753345}, {"id": 196, "seek": 151452, "start": 1515.24, "end": 1522.92, "text": " um through through weight space and thereby you you have an idea of actually what happens", "tokens": [50400, 1105, 807, 807, 3364, 1901, 293, 28281, 291, 291, 362, 364, 1558, 295, 767, 437, 2314, 50784], "temperature": 0.0, "avg_logprob": -0.1479328155517578, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0012062712339684367}, {"id": 197, "seek": 151452, "start": 1522.92, "end": 1528.36, "text": " during network to network training right so if you have never heard that and more or less followed", "tokens": [50784, 1830, 3209, 281, 3209, 3097, 558, 370, 498, 291, 362, 1128, 2198, 300, 293, 544, 420, 1570, 6263, 51056], "temperature": 0.0, "avg_logprob": -0.1479328155517578, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0012062712339684367}, {"id": 198, "seek": 151452, "start": 1528.36, "end": 1535.6399999999999, "text": " my explanation then you can see this is kind of cool that at least in this limits you have", "tokens": [51056, 452, 10835, 550, 291, 393, 536, 341, 307, 733, 295, 1627, 300, 412, 1935, 294, 341, 10406, 291, 362, 51420], "temperature": 0.0, "avg_logprob": -0.1479328155517578, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0012062712339684367}, {"id": 199, "seek": 151452, "start": 1536.2, "end": 1542.92, "text": " sort of an analytical idea what happens during learning and then the question is to what extent", "tokens": [51448, 1333, 295, 364, 29579, 1558, 437, 2314, 1830, 2539, 293, 550, 264, 1168, 307, 281, 437, 8396, 51784], "temperature": 0.0, "avg_logprob": -0.1479328155517578, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0012062712339684367}, {"id": 200, "seek": 154292, "start": 1543.24, "end": 1550.92, "text": " is this sort of logic valid for networks as we can implement them at the moment at the moment", "tokens": [50380, 307, 341, 1333, 295, 9952, 7363, 337, 9590, 382, 321, 393, 4445, 552, 412, 264, 1623, 412, 264, 1623, 50764], "temperature": 0.0, "avg_logprob": -0.12760981321334838, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.001700391760095954}, {"id": 201, "seek": 154292, "start": 1550.92, "end": 1556.2, "text": " because of course we have a lot of weights like billions of weights but it's not infinite so", "tokens": [50764, 570, 295, 1164, 321, 362, 257, 688, 295, 17443, 411, 17375, 295, 17443, 457, 309, 311, 406, 13785, 370, 51028], "temperature": 0.0, "avg_logprob": -0.12760981321334838, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.001700391760095954}, {"id": 202, "seek": 154292, "start": 1556.8400000000001, "end": 1561.72, "text": " you might ask to what extent is the analytical theory where these limits are taken right so", "tokens": [51060, 291, 1062, 1029, 281, 437, 8396, 307, 264, 29579, 5261, 689, 613, 10406, 366, 2726, 558, 370, 51304], "temperature": 0.0, "avg_logprob": -0.12760981321334838, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.001700391760095954}, {"id": 203, "seek": 154292, "start": 1562.6000000000001, "end": 1570.92, "text": " super small step size very large networks applicable to today's convolutional deep neural", "tokens": [51348, 1687, 1359, 1823, 2744, 588, 2416, 9590, 21142, 281, 965, 311, 45216, 304, 2452, 18161, 51764], "temperature": 0.0, "avg_logprob": -0.12760981321334838, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.001700391760095954}, {"id": 204, "seek": 157092, "start": 1570.92, "end": 1576.92, "text": " networks and so on and so forth and this is basically a subject of study so this is something", "tokens": [50364, 9590, 293, 370, 322, 293, 370, 5220, 293, 341, 307, 1936, 257, 3983, 295, 2979, 370, 341, 307, 746, 50664], "temperature": 0.0, "avg_logprob": -0.08939089197101015, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.004196594934910536}, {"id": 205, "seek": 157092, "start": 1576.92, "end": 1586.44, "text": " where people still put a lot of time in and so for example you have here this google uh neural", "tokens": [50664, 689, 561, 920, 829, 257, 688, 295, 565, 294, 293, 370, 337, 1365, 291, 362, 510, 341, 20742, 2232, 18161, 51140], "temperature": 0.0, "avg_logprob": -0.08939089197101015, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.004196594934910536}, {"id": 206, "seek": 157092, "start": 1586.44, "end": 1596.44, "text": " tangents project which i might be interested in looking at and there's a bunch of google researchers", "tokens": [51140, 10266, 791, 1716, 597, 741, 1062, 312, 3102, 294, 1237, 412, 293, 456, 311, 257, 3840, 295, 20742, 10309, 51640], "temperature": 0.0, "avg_logprob": -0.08939089197101015, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.004196594934910536}, {"id": 207, "seek": 159644, "start": 1596.44, "end": 1601.64, "text": " who are still using this and publishing papers in this and and so on and so forth there's also", "tokens": [50364, 567, 366, 920, 1228, 341, 293, 17832, 10577, 294, 341, 293, 293, 370, 322, 293, 370, 5220, 456, 311, 611, 50624], "temperature": 0.0, "avg_logprob": -0.12744057178497314, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.005133911967277527}, {"id": 208, "seek": 159644, "start": 1602.52, "end": 1613.4, "text": " i think a recent um new rips uh poster from 2019 where you get some of the examples of", "tokens": [50668, 741, 519, 257, 5162, 1105, 777, 367, 2600, 2232, 17171, 490, 6071, 689, 291, 483, 512, 295, 264, 5110, 295, 51212], "temperature": 0.0, "avg_logprob": -0.12744057178497314, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.005133911967277527}, {"id": 209, "seek": 159644, "start": 1613.4, "end": 1618.3600000000001, "text": " analytical formulas that i talked about um just because i want to get to the quantum", "tokens": [51212, 29579, 30546, 300, 741, 2825, 466, 1105, 445, 570, 741, 528, 281, 483, 281, 264, 13018, 51460], "temperature": 0.0, "avg_logprob": -0.12744057178497314, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.005133911967277527}, {"id": 210, "seek": 161836, "start": 1618.36, "end": 1625.4799999999998, "text": " field theory part i will not discuss this in detail but i um i hope my tangent pun intended", "tokens": [50364, 2519, 5261, 644, 741, 486, 406, 2248, 341, 294, 2607, 457, 741, 1105, 741, 1454, 452, 27747, 4468, 10226, 50720], "temperature": 0.0, "avg_logprob": -0.132962887103741, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.017691779881715775}, {"id": 211, "seek": 161836, "start": 1626.9199999999998, "end": 1631.4799999999998, "text": " was interesting and as i said i would also actually like to to work a little bit in", "tokens": [50792, 390, 1880, 293, 382, 741, 848, 741, 576, 611, 767, 411, 281, 281, 589, 257, 707, 857, 294, 51020], "temperature": 0.0, "avg_logprob": -0.132962887103741, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.017691779881715775}, {"id": 212, "seek": 161836, "start": 1631.4799999999998, "end": 1636.6799999999998, "text": " this direction myself so if you want to look into that um feel free to reach out and we can", "tokens": [51020, 341, 3513, 2059, 370, 498, 291, 528, 281, 574, 666, 300, 1105, 841, 1737, 281, 2524, 484, 293, 321, 393, 51280], "temperature": 0.0, "avg_logprob": -0.132962887103741, "compression_ratio": 1.5798816568047338, "no_speech_prob": 0.017691779881715775}, {"id": 213, "seek": 163668, "start": 1636.68, "end": 1651.0, "text": " do some sort of project together okay so um now for the the the field theory part", "tokens": [50364, 360, 512, 1333, 295, 1716, 1214, 1392, 370, 1105, 586, 337, 264, 264, 264, 2519, 5261, 644, 51080], "temperature": 0.0, "avg_logprob": -0.1253588006303117, "compression_ratio": 1.4049586776859504, "no_speech_prob": 0.0028816615231335163}, {"id": 214, "seek": 163668, "start": 1656.44, "end": 1662.92, "text": " but still extremely important for us is this neural network Gaussian process inside okay", "tokens": [51352, 457, 920, 4664, 1021, 337, 505, 307, 341, 18161, 3209, 39148, 1399, 1854, 1392, 51676], "temperature": 0.0, "avg_logprob": -0.1253588006303117, "compression_ratio": 1.4049586776859504, "no_speech_prob": 0.0028816615231335163}, {"id": 215, "seek": 166292, "start": 1663.64, "end": 1667.16, "text": " and if i'm here in the paper on page four", "tokens": [50400, 293, 498, 741, 478, 510, 294, 264, 3035, 322, 3028, 1451, 50576], "temperature": 0.0, "avg_logprob": -0.1522709646342713, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.0016734434757381678}, {"id": 216, "seek": 166292, "start": 1670.28, "end": 1675.8000000000002, "text": " then um let's make first some definitions okay so here we have these correlation functions", "tokens": [50732, 550, 1105, 718, 311, 652, 700, 512, 21988, 1392, 370, 510, 321, 362, 613, 20009, 6828, 51008], "temperature": 0.0, "avg_logprob": -0.1522709646342713, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.0016734434757381678}, {"id": 217, "seek": 166292, "start": 1676.52, "end": 1683.88, "text": " which we call g n uh for uh for a concreteness sake you can think of n uh let's say as two", "tokens": [51044, 597, 321, 818, 290, 297, 2232, 337, 2232, 337, 257, 1588, 35383, 442, 9717, 291, 393, 519, 295, 297, 2232, 718, 311, 584, 382, 732, 51412], "temperature": 0.0, "avg_logprob": -0.1522709646342713, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.0016734434757381678}, {"id": 218, "seek": 166292, "start": 1683.88, "end": 1692.52, "text": " so um we are going to actually look at um two different um forward passes for the neural network", "tokens": [51412, 370, 1105, 321, 366, 516, 281, 767, 574, 412, 1105, 732, 819, 1105, 2128, 11335, 337, 264, 18161, 3209, 51844], "temperature": 0.0, "avg_logprob": -0.1522709646342713, "compression_ratio": 1.6580310880829014, "no_speech_prob": 0.0016734434757381678}, {"id": 219, "seek": 169252, "start": 1692.52, "end": 1699.32, "text": " right so you have two different imports that you want to try x and you plug them in and you", "tokens": [50364, 558, 370, 291, 362, 732, 819, 41596, 300, 291, 528, 281, 853, 2031, 293, 291, 5452, 552, 294, 293, 291, 50704], "temperature": 0.0, "avg_logprob": -0.11505930007450164, "compression_ratio": 1.6606060606060606, "no_speech_prob": 0.000999870477244258}, {"id": 220, "seek": 169252, "start": 1699.32, "end": 1708.2, "text": " get something out of your current network which might be randomly initialized um and if you", "tokens": [50704, 483, 746, 484, 295, 428, 2190, 3209, 597, 1062, 312, 16979, 5883, 1602, 1105, 293, 498, 291, 51148], "temperature": 0.0, "avg_logprob": -0.11505930007450164, "compression_ratio": 1.6606060606060606, "no_speech_prob": 0.000999870477244258}, {"id": 221, "seek": 169252, "start": 1709.56, "end": 1716.68, "text": " as we had it with the neural network Gaussian process case if you your weights as a random", "tokens": [51216, 382, 321, 632, 309, 365, 264, 18161, 3209, 39148, 1399, 1389, 498, 291, 428, 17443, 382, 257, 4974, 51572], "temperature": 0.0, "avg_logprob": -0.11505930007450164, "compression_ratio": 1.6606060606060606, "no_speech_prob": 0.000999870477244258}, {"id": 222, "seek": 171668, "start": 1716.68, "end": 1723.24, "text": " variable right over all the weights over all your trillion weights you put a little Gaussian and let", "tokens": [50364, 7006, 558, 670, 439, 264, 17443, 670, 439, 428, 18723, 17443, 291, 829, 257, 707, 39148, 293, 718, 50692], "temperature": 0.0, "avg_logprob": -0.10474148392677307, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.00792776234447956}, {"id": 223, "seek": 171668, "start": 1723.24, "end": 1733.16, "text": " them wiggle a little bit um and you say what typically happens if i sample once and um", "tokens": [50692, 552, 33377, 257, 707, 857, 1105, 293, 291, 584, 437, 5850, 2314, 498, 741, 6889, 1564, 293, 1105, 51188], "temperature": 0.0, "avg_logprob": -0.10474148392677307, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.00792776234447956}, {"id": 224, "seek": 171668, "start": 1735.4, "end": 1745.0, "text": " put into uh inputs x uh how are the inputs on a on a typical or random network correlated with", "tokens": [51300, 829, 666, 2232, 15743, 2031, 2232, 577, 366, 264, 15743, 322, 257, 322, 257, 7476, 420, 4974, 3209, 38574, 365, 51780], "temperature": 0.0, "avg_logprob": -0.10474148392677307, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.00792776234447956}, {"id": 225, "seek": 174500, "start": 1745.0, "end": 1752.36, "text": " each other then you can you can try this a bunch of times and get an idea um what you can also do is", "tokens": [50364, 1184, 661, 550, 291, 393, 291, 393, 853, 341, 257, 3840, 295, 1413, 293, 483, 364, 1558, 1105, 437, 291, 393, 611, 360, 307, 50732], "temperature": 0.0, "avg_logprob": -0.09166450500488281, "compression_ratio": 1.8219895287958114, "no_speech_prob": 0.0011686687357723713}, {"id": 226, "seek": 174500, "start": 1754.04, "end": 1757.08, "text": " analytically if you know the distribution of where your weights", "tokens": [50816, 10783, 984, 498, 291, 458, 264, 7316, 295, 689, 428, 17443, 50968], "temperature": 0.0, "avg_logprob": -0.09166450500488281, "compression_ratio": 1.8219895287958114, "no_speech_prob": 0.0011686687357723713}, {"id": 227, "seek": 174500, "start": 1757.8, "end": 1762.6, "text": " compute what what will come out right so you have here uh p over the weights this is the", "tokens": [51004, 14722, 437, 437, 486, 808, 484, 558, 370, 291, 362, 510, 2232, 280, 670, 264, 17443, 341, 307, 264, 51244], "temperature": 0.0, "avg_logprob": -0.09166450500488281, "compression_ratio": 1.8219895287958114, "no_speech_prob": 0.0011686687357723713}, {"id": 228, "seek": 174500, "start": 1762.6, "end": 1767.96, "text": " distribution that you yourself chose from which you can sample and uh for fixed neural network", "tokens": [51244, 7316, 300, 291, 1803, 5111, 490, 597, 291, 393, 6889, 293, 2232, 337, 6806, 18161, 3209, 51512], "temperature": 0.0, "avg_logprob": -0.09166450500488281, "compression_ratio": 1.8219895287958114, "no_speech_prob": 0.0011686687357723713}, {"id": 229, "seek": 176796, "start": 1767.96, "end": 1778.8400000000001, "text": " architecture um the weights and um the neural network um which um is here called phi this is the", "tokens": [50364, 9482, 1105, 264, 17443, 293, 1105, 264, 18161, 3209, 1105, 597, 1105, 307, 510, 1219, 13107, 341, 307, 264, 50908], "temperature": 0.0, "avg_logprob": -0.10209957510232925, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.002508878242224455}, {"id": 230, "seek": 176796, "start": 1780.68, "end": 1787.08, "text": " function that depends on the weights depending on your architecture right so this is the sigmoids", "tokens": [51000, 2445, 300, 5946, 322, 264, 17443, 5413, 322, 428, 9482, 558, 370, 341, 307, 264, 4556, 3280, 3742, 51320], "temperature": 0.0, "avg_logprob": -0.10209957510232925, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.002508878242224455}, {"id": 231, "seek": 176796, "start": 1787.08, "end": 1794.52, "text": " and this sum and so on and so forth and so the correlation function gives you how this how", "tokens": [51320, 293, 341, 2408, 293, 370, 322, 293, 370, 5220, 293, 370, 264, 20009, 2445, 2709, 291, 577, 341, 577, 51692], "temperature": 0.0, "avg_logprob": -0.10209957510232925, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.002508878242224455}, {"id": 232, "seek": 179452, "start": 1794.52, "end": 1802.12, "text": " let's say two inputs are um correlated with each other for this network right and by the neural", "tokens": [50364, 718, 311, 584, 732, 15743, 366, 1105, 38574, 365, 1184, 661, 337, 341, 3209, 558, 293, 538, 264, 18161, 50744], "temperature": 0.0, "avg_logprob": -0.12132309277852377, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0007317358977161348}, {"id": 233, "seek": 179452, "start": 1802.12, "end": 1813.24, "text": " network Gaussian process result if we said that uh this this billion uh probability distribution over", "tokens": [50744, 3209, 39148, 1399, 1874, 498, 321, 848, 300, 2232, 341, 341, 5218, 2232, 8482, 7316, 670, 51300], "temperature": 0.0, "avg_logprob": -0.12132309277852377, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0007317358977161348}, {"id": 234, "seek": 179452, "start": 1813.24, "end": 1820.36, "text": " the weights um make the input output relation of the whole network also into a random variable", "tokens": [51300, 264, 17443, 1105, 652, 264, 4846, 5598, 9721, 295, 264, 1379, 3209, 611, 666, 257, 4974, 7006, 51656], "temperature": 0.0, "avg_logprob": -0.12132309277852377, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0007317358977161348}, {"id": 235, "seek": 182036, "start": 1820.36, "end": 1827.8799999999999, "text": " right so you can also view uh this the setup that you have here not as um as um", "tokens": [50364, 558, 370, 291, 393, 611, 1910, 2232, 341, 264, 8657, 300, 291, 362, 510, 406, 382, 1105, 382, 1105, 50740], "temperature": 0.0, "avg_logprob": -0.12710206719893444, "compression_ratio": 1.8128342245989304, "no_speech_prob": 0.0017537915846332908}, {"id": 236, "seek": 182036, "start": 1829.7199999999998, "end": 1836.84, "text": " not just a sampling um the weights and getting uh then a fixed input output but you can also", "tokens": [50832, 406, 445, 257, 21179, 1105, 264, 17443, 293, 1242, 2232, 550, 257, 6806, 4846, 5598, 457, 291, 393, 611, 51188], "temperature": 0.0, "avg_logprob": -0.12710206719893444, "compression_ratio": 1.8128342245989304, "no_speech_prob": 0.0017537915846332908}, {"id": 237, "seek": 182036, "start": 1836.84, "end": 1842.4399999999998, "text": " view any sample process as sampling a whole neural network right even this is just what", "tokens": [51188, 1910, 604, 6889, 1399, 382, 21179, 257, 1379, 18161, 3209, 558, 754, 341, 307, 445, 437, 51468], "temperature": 0.0, "avg_logprob": -0.12710206719893444, "compression_ratio": 1.8128342245989304, "no_speech_prob": 0.0017537915846332908}, {"id": 238, "seek": 182036, "start": 1842.4399999999998, "end": 1848.28, "text": " you do if you sample if you random initialize for fixed architecture um the um", "tokens": [51468, 291, 360, 498, 291, 6889, 498, 291, 4974, 5883, 1125, 337, 6806, 9482, 1105, 264, 1105, 51760], "temperature": 0.0, "avg_logprob": -0.12710206719893444, "compression_ratio": 1.8128342245989304, "no_speech_prob": 0.0017537915846332908}, {"id": 239, "seek": 185036, "start": 1850.76, "end": 1855.32, "text": " certain uh functions as your certain neural network then you've also sampled the neural", "tokens": [50384, 1629, 2232, 6828, 382, 428, 1629, 18161, 3209, 550, 291, 600, 611, 3247, 15551, 264, 18161, 50612], "temperature": 0.0, "avg_logprob": -0.06862782209347455, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0006360486731864512}, {"id": 240, "seek": 185036, "start": 1855.32, "end": 1861.4799999999998, "text": " network from who knows what distribution right so there's also this different the different", "tokens": [50612, 3209, 490, 567, 3255, 437, 7316, 558, 370, 456, 311, 611, 341, 819, 264, 819, 50920], "temperature": 0.0, "avg_logprob": -0.06862782209347455, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0006360486731864512}, {"id": 241, "seek": 185036, "start": 1861.4799999999998, "end": 1870.52, "text": " view of this initialization process and then um by the result of the neural network Gaussian process", "tokens": [50920, 1910, 295, 341, 5883, 2144, 1399, 293, 550, 1105, 538, 264, 1874, 295, 264, 18161, 3209, 39148, 1399, 51372], "temperature": 0.0, "avg_logprob": -0.06862782209347455, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0006360486731864512}, {"id": 242, "seek": 185036, "start": 1870.52, "end": 1876.9199999999998, "text": " what you have is that you can also view the same uh exact object this correlation function or any", "tokens": [51372, 437, 291, 362, 307, 300, 291, 393, 611, 1910, 264, 912, 2232, 1900, 2657, 341, 20009, 2445, 420, 604, 51692], "temperature": 0.0, "avg_logprob": -0.06862782209347455, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0006360486731864512}, {"id": 243, "seek": 187692, "start": 1877.0, "end": 1885.72, "text": " expectation value really um as in terms of distribution over the these functions themselves", "tokens": [50368, 14334, 2158, 534, 1105, 382, 294, 2115, 295, 7316, 670, 264, 613, 6828, 2969, 50804], "temperature": 0.0, "avg_logprob": -0.10521822590981761, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.001205815002322197}, {"id": 244, "seek": 187692, "start": 1885.72, "end": 1890.52, "text": " and by the result that we just had for a large network this will be a Gaussian process and this", "tokens": [50804, 293, 538, 264, 1874, 300, 321, 445, 632, 337, 257, 2416, 3209, 341, 486, 312, 257, 39148, 1399, 293, 341, 51044], "temperature": 0.0, "avg_logprob": -0.10521822590981761, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.001205815002322197}, {"id": 245, "seek": 187692, "start": 1890.52, "end": 1897.24, "text": " manifests in this way right so um here is the integral not over the weights but over the um the", "tokens": [51044, 50252, 294, 341, 636, 558, 370, 1105, 510, 307, 264, 11573, 406, 670, 264, 17443, 457, 670, 264, 1105, 264, 51380], "temperature": 0.0, "avg_logprob": -0.10521822590981761, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.001205815002322197}, {"id": 246, "seek": 189724, "start": 1898.2, "end": 1908.92, "text": " uh whole function that the network represents itself and um the the um probability distribution", "tokens": [50412, 2232, 1379, 2445, 300, 264, 3209, 8855, 2564, 293, 1105, 264, 264, 1105, 8482, 7316, 50948], "temperature": 0.0, "avg_logprob": -0.16941439916217138, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.000868635717779398}, {"id": 247, "seek": 189724, "start": 1908.92, "end": 1919.16, "text": " that you have in this case is um of of this sort where this s uh and now this relates back to the", "tokens": [50948, 300, 291, 362, 294, 341, 1389, 307, 1105, 295, 295, 341, 1333, 689, 341, 262, 2232, 293, 586, 341, 16155, 646, 281, 264, 51460], "temperature": 0.0, "avg_logprob": -0.16941439916217138, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.000868635717779398}, {"id": 248, "seek": 189724, "start": 1919.16, "end": 1926.44, "text": " formal definition of the Gaussian process is some quadratic let's say let me fill some local terms", "tokens": [51460, 9860, 7123, 295, 264, 39148, 1399, 307, 512, 37262, 718, 311, 584, 718, 385, 2836, 512, 2654, 2115, 51824], "temperature": 0.0, "avg_logprob": -0.16941439916217138, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.000868635717779398}, {"id": 249, "seek": 192724, "start": 1928.2, "end": 1934.76, "text": " cost associated with the whole um with the whole function so basically what what this does I mean", "tokens": [50412, 2063, 6615, 365, 264, 1379, 1105, 365, 264, 1379, 2445, 370, 1936, 437, 437, 341, 775, 286, 914, 50740], "temperature": 0.0, "avg_logprob": -0.14408770922956796, "compression_ratio": 1.6466666666666667, "no_speech_prob": 0.0016204112907871604}, {"id": 250, "seek": 192724, "start": 1934.76, "end": 1938.2, "text": " do they give here some examples I think they do um", "tokens": [50740, 360, 436, 976, 510, 512, 5110, 286, 519, 436, 360, 1105, 50912], "temperature": 0.0, "avg_logprob": -0.14408770922956796, "compression_ratio": 1.6466666666666667, "no_speech_prob": 0.0016204112907871604}, {"id": 251, "seek": 192724, "start": 1942.92, "end": 1949.32, "text": " okay so this is already sort of a physical example but what you have as s here in the the exponent", "tokens": [51148, 1392, 370, 341, 307, 1217, 1333, 295, 257, 4001, 1365, 457, 437, 291, 362, 382, 262, 510, 294, 264, 264, 37871, 51468], "temperature": 0.0, "avg_logprob": -0.14408770922956796, "compression_ratio": 1.6466666666666667, "no_speech_prob": 0.0016204112907871604}, {"id": 252, "seek": 194932, "start": 1949.3999999999999, "end": 1957.24, "text": " is some sum over all uh the values of the network and what what they like what in effect happens is", "tokens": [50368, 307, 512, 2408, 670, 439, 2232, 264, 4190, 295, 264, 3209, 293, 437, 437, 436, 411, 437, 294, 1802, 2314, 307, 50760], "temperature": 0.0, "avg_logprob": -0.14738551182533377, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.004004854708909988}, {"id": 253, "seek": 194932, "start": 1957.24, "end": 1970.6799999999998, "text": " that um the um the the this this weight is such that um field configurations and when I say field", "tokens": [50760, 300, 1105, 264, 1105, 264, 264, 341, 341, 3364, 307, 1270, 300, 1105, 2519, 31493, 293, 562, 286, 584, 2519, 51432], "temperature": 0.0, "avg_logprob": -0.14738551182533377, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.004004854708909988}, {"id": 254, "seek": 194932, "start": 1970.6799999999998, "end": 1974.76, "text": " now I always just mean the same as the input output relation not given by the neural network", "tokens": [51432, 586, 286, 1009, 445, 914, 264, 912, 382, 264, 4846, 5598, 9721, 406, 2212, 538, 264, 18161, 3209, 51636], "temperature": 0.0, "avg_logprob": -0.14738551182533377, "compression_ratio": 1.7159763313609468, "no_speech_prob": 0.004004854708909988}, {"id": 255, "seek": 197476, "start": 1975.4, "end": 1983.8, "text": " field configurations or neural networks where at one uh places you get a huge output these are just", "tokens": [50396, 2519, 31493, 420, 18161, 9590, 689, 412, 472, 2232, 3190, 291, 483, 257, 2603, 5598, 613, 366, 445, 50816], "temperature": 0.0, "avg_logprob": -0.11772050857543945, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005553101189434528}, {"id": 256, "seek": 197476, "start": 1983.8, "end": 1991.8799999999999, "text": " exponentially suppressed because um if you random sample from the with the weights then you are not", "tokens": [50816, 37330, 42645, 570, 1105, 498, 291, 4974, 6889, 490, 264, 365, 264, 17443, 550, 291, 366, 406, 51220], "temperature": 0.0, "avg_logprob": -0.11772050857543945, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005553101189434528}, {"id": 257, "seek": 197476, "start": 1991.8799999999999, "end": 2000.84, "text": " likely to get um some basically you are going to get um neural networks in initializations", "tokens": [51220, 3700, 281, 483, 1105, 512, 1936, 291, 366, 516, 281, 483, 1105, 18161, 9590, 294, 5883, 14455, 51668], "temperature": 0.0, "avg_logprob": -0.11772050857543945, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005553101189434528}, {"id": 258, "seek": 200084, "start": 2000.9199999999998, "end": 2007.8, "text": " which are around some some uh some certain um typical expectation value like they have there are", "tokens": [50368, 597, 366, 926, 512, 512, 2232, 512, 1629, 1105, 7476, 14334, 2158, 411, 436, 362, 456, 366, 50712], "temperature": 0.0, "avg_logprob": -0.10085333585739135, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0017271939432248473}, {"id": 259, "seek": 200084, "start": 2007.8, "end": 2014.84, "text": " some typical behaviors and everything that deviates uh a lot from it is like exponentially less", "tokens": [50712, 512, 7476, 15501, 293, 1203, 300, 31219, 1024, 2232, 257, 688, 490, 309, 307, 411, 37330, 1570, 51064], "temperature": 0.0, "avg_logprob": -0.10085333585739135, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0017271939432248473}, {"id": 260, "seek": 200084, "start": 2014.84, "end": 2022.36, "text": " likely right you um if you do um thousand random initialization of the neural network you will", "tokens": [51064, 3700, 558, 291, 1105, 498, 291, 360, 1105, 4714, 4974, 5883, 2144, 295, 264, 18161, 3209, 291, 486, 51440], "temperature": 0.0, "avg_logprob": -0.10085333585739135, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0017271939432248473}, {"id": 261, "seek": 200084, "start": 2022.36, "end": 2028.36, "text": " get some typical behavior and then you can cook up some other extreme behavior that is not likely", "tokens": [51440, 483, 512, 7476, 5223, 293, 550, 291, 393, 2543, 493, 512, 661, 8084, 5223, 300, 307, 406, 3700, 51740], "temperature": 0.0, "avg_logprob": -0.10085333585739135, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0017271939432248473}, {"id": 262, "seek": 202836, "start": 2028.36, "end": 2036.6799999999998, "text": " to happen and um by the result of the neural network Gaussian process um theory um it tells you", "tokens": [50364, 281, 1051, 293, 1105, 538, 264, 1874, 295, 264, 18161, 3209, 39148, 1399, 1105, 5261, 1105, 309, 5112, 291, 50780], "temperature": 0.0, "avg_logprob": -0.12066564105805896, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0034816584084182978}, {"id": 263, "seek": 202836, "start": 2036.6799999999998, "end": 2043.32, "text": " what the the the probability distribution looks for the neural network so there is this connection", "tokens": [50780, 437, 264, 264, 264, 8482, 7316, 1542, 337, 264, 18161, 3209, 370, 456, 307, 341, 4984, 51112], "temperature": 0.0, "avg_logprob": -0.12066564105805896, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0034816584084182978}, {"id": 264, "seek": 202836, "start": 2043.32, "end": 2049.48, "text": " that you have here right and as I will motivate um later uh in quantum field theory this is exactly", "tokens": [51112, 300, 291, 362, 510, 558, 293, 382, 286, 486, 28497, 1105, 1780, 2232, 294, 13018, 2519, 5261, 341, 307, 2293, 51420], "temperature": 0.0, "avg_logprob": -0.12066564105805896, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0034816584084182978}, {"id": 265, "seek": 202836, "start": 2049.48, "end": 2058.04, "text": " sort of the setup for the path integral and what this paper does is it um if you view um", "tokens": [51420, 1333, 295, 264, 8657, 337, 264, 3100, 11573, 293, 437, 341, 3035, 775, 307, 309, 1105, 498, 291, 1910, 1105, 51848], "temperature": 0.0, "avg_logprob": -0.12066564105805896, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0034816584084182978}, {"id": 266, "seek": 205836, "start": 2058.36, "end": 2067.08, "text": " the um this this sort of um mathematical um overlap um as a physicist and you want to use", "tokens": [50364, 264, 1105, 341, 341, 1333, 295, 1105, 18894, 1105, 19959, 1105, 382, 257, 42466, 293, 291, 528, 281, 764, 50800], "temperature": 0.0, "avg_logprob": -0.1197195053100586, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0001971512974705547}, {"id": 267, "seek": 205836, "start": 2067.08, "end": 2074.76, "text": " neural networks as as as a physicist you you see this as a way to um then try to craft neural", "tokens": [50800, 18161, 9590, 382, 382, 382, 257, 42466, 291, 291, 536, 341, 382, 257, 636, 281, 1105, 550, 853, 281, 8448, 18161, 51184], "temperature": 0.0, "avg_logprob": -0.1197195053100586, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0001971512974705547}, {"id": 268, "seek": 205836, "start": 2074.76, "end": 2080.52, "text": " network architecture and sampling techniques right the the way in which you sample the weights", "tokens": [51184, 3209, 9482, 293, 21179, 7512, 558, 264, 264, 636, 294, 597, 291, 6889, 264, 17443, 51472], "temperature": 0.0, "avg_logprob": -0.1197195053100586, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0001971512974705547}, {"id": 269, "seek": 208052, "start": 2081.24, "end": 2088.44, "text": " in a way so that the this this whole process of random initialization corresponds to certain", "tokens": [50400, 294, 257, 636, 370, 300, 264, 341, 341, 1379, 1399, 295, 4974, 5883, 2144, 23249, 281, 1629, 50760], "temperature": 0.0, "avg_logprob": -0.11686192680807675, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.017705928534269333}, {"id": 270, "seek": 208052, "start": 2088.44, "end": 2094.68, "text": " s s certain actions here right you do you have some physical scenario in mind there's physical", "tokens": [50760, 262, 262, 1629, 5909, 510, 558, 291, 360, 291, 362, 512, 4001, 9005, 294, 1575, 456, 311, 4001, 51072], "temperature": 0.0, "avg_logprob": -0.11686192680807675, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.017705928534269333}, {"id": 271, "seek": 208052, "start": 2094.68, "end": 2102.04, "text": " theory that says oh you know uh certain scalar field theories have this and this action what is", "tokens": [51072, 5261, 300, 1619, 1954, 291, 458, 2232, 1629, 39684, 2519, 13667, 362, 341, 293, 341, 3069, 437, 307, 51440], "temperature": 0.0, "avg_logprob": -0.11686192680807675, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.017705928534269333}, {"id": 272, "seek": 208052, "start": 2102.04, "end": 2108.92, "text": " the way I and in which I must set up a network and the way in which I must sample the weights", "tokens": [51440, 264, 636, 286, 293, 294, 597, 286, 1633, 992, 493, 257, 3209, 293, 264, 636, 294, 597, 286, 1633, 6889, 264, 17443, 51784], "temperature": 0.0, "avg_logprob": -0.11686192680807675, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.017705928534269333}, {"id": 273, "seek": 210892, "start": 2109.48, "end": 2114.52, "text": " so that what I sample is exactly as if I would sample from a quantum field", "tokens": [50392, 370, 300, 437, 286, 6889, 307, 2293, 382, 498, 286, 576, 6889, 490, 257, 13018, 2519, 50644], "temperature": 0.0, "avg_logprob": -0.09933698033711996, "compression_ratio": 1.9304812834224598, "no_speech_prob": 0.0012251526350155473}, {"id": 274, "seek": 210892, "start": 2114.52, "end": 2122.28, "text": " like like as if I would sample from uh would I would sample a field which is one instance of", "tokens": [50644, 411, 411, 382, 498, 286, 576, 6889, 490, 2232, 576, 286, 576, 6889, 257, 2519, 597, 307, 472, 5197, 295, 51032], "temperature": 0.0, "avg_logprob": -0.09933698033711996, "compression_ratio": 1.9304812834224598, "no_speech_prob": 0.0012251526350155473}, {"id": 275, "seek": 210892, "start": 2123.7200000000003, "end": 2131.2400000000002, "text": " um a quantum field in the path integral formalism right and then I get out a bunch of correlation", "tokens": [51104, 1105, 257, 13018, 2519, 294, 264, 3100, 11573, 9860, 1434, 558, 293, 550, 286, 483, 484, 257, 3840, 295, 20009, 51480], "temperature": 0.0, "avg_logprob": -0.09933698033711996, "compression_ratio": 1.9304812834224598, "no_speech_prob": 0.0012251526350155473}, {"id": 276, "seek": 210892, "start": 2131.2400000000002, "end": 2137.7200000000003, "text": " functions this endpoint functions um and these are exactly the things that are uh what what you", "tokens": [51480, 6828, 341, 35795, 6828, 1105, 293, 613, 366, 2293, 264, 721, 300, 366, 2232, 437, 437, 291, 51804], "temperature": 0.0, "avg_logprob": -0.09933698033711996, "compression_ratio": 1.9304812834224598, "no_speech_prob": 0.0012251526350155473}, {"id": 277, "seek": 213772, "start": 2137.72, "end": 2142.6, "text": " do quantum field theory for right you you compute these g's and then I will explain it later then", "tokens": [50364, 360, 13018, 2519, 5261, 337, 558, 291, 291, 14722, 613, 290, 311, 293, 550, 286, 486, 2903, 309, 1780, 550, 50608], "temperature": 0.0, "avg_logprob": -0.10674349097318428, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.000527266354765743}, {"id": 278, "seek": 213772, "start": 2142.6, "end": 2151.9599999999996, "text": " there's some mathematical connections to how you get from this this this g's to um the scattering", "tokens": [50608, 456, 311, 512, 18894, 9271, 281, 577, 291, 483, 490, 341, 341, 341, 290, 311, 281, 1105, 264, 42314, 51076], "temperature": 0.0, "avg_logprob": -0.10674349097318428, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.000527266354765743}, {"id": 279, "seek": 213772, "start": 2151.9599999999996, "end": 2157.48, "text": " amplitude or whatever your quantum field theory does um maybe particle physics solid state physics", "tokens": [51076, 27433, 420, 2035, 428, 13018, 2519, 5261, 775, 1105, 1310, 12359, 10649, 5100, 1785, 10649, 51352], "temperature": 0.0, "avg_logprob": -0.10674349097318428, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.000527266354765743}, {"id": 280, "seek": 213772, "start": 2157.48, "end": 2167.3999999999996, "text": " and so on and so forth okay uh let me see so as you might notice this is a very free flow", "tokens": [51352, 293, 370, 322, 293, 370, 5220, 1392, 2232, 718, 385, 536, 370, 382, 291, 1062, 3449, 341, 307, 257, 588, 1737, 3095, 51848], "temperature": 0.0, "avg_logprob": -0.10674349097318428, "compression_ratio": 1.737556561085973, "no_speech_prob": 0.000527266354765743}, {"id": 281, "seek": 216740, "start": 2167.48, "end": 2179.08, "text": " sort of explanation right um so I have to check uh what I have touched upon and what I didn't", "tokens": [50368, 1333, 295, 10835, 558, 1105, 370, 286, 362, 281, 1520, 2232, 437, 286, 362, 9828, 3564, 293, 437, 286, 994, 380, 50948], "temperature": 0.0, "avg_logprob": -0.2577826976776123, "compression_ratio": 1.2098765432098766, "no_speech_prob": 0.003938963636755943}, {"id": 282, "seek": 216740, "start": 2183.32, "end": 2187.08, "text": " okay", "tokens": [51160, 1392, 51348], "temperature": 0.0, "avg_logprob": -0.2577826976776123, "compression_ratio": 1.2098765432098766, "no_speech_prob": 0.003938963636755943}, {"id": 283, "seek": 218708, "start": 2187.72, "end": 2198.2, "text": " yeah okay so um from the very complicated quantum field theory math you get some", "tokens": [50396, 1338, 1392, 370, 1105, 490, 264, 588, 6179, 13018, 2519, 5261, 5221, 291, 483, 512, 50920], "temperature": 0.0, "avg_logprob": -0.15575680909333406, "compression_ratio": 1.5843373493975903, "no_speech_prob": 0.0007418388850055635}, {"id": 284, "seek": 218708, "start": 2198.84, "end": 2209.0, "text": " you know relations of um how the correlation function must relate to these actions and there's", "tokens": [50952, 291, 458, 2299, 295, 1105, 577, 264, 20009, 2445, 1633, 10961, 281, 613, 5909, 293, 456, 311, 51460], "temperature": 0.0, "avg_logprob": -0.15575680909333406, "compression_ratio": 1.5843373493975903, "no_speech_prob": 0.0007418388850055635}, {"id": 285, "seek": 218708, "start": 2209.0, "end": 2216.2, "text": " a bunch of stochastic differential equation mathematics involved and because in physics", "tokens": [51460, 257, 3840, 295, 342, 8997, 2750, 15756, 5367, 18666, 3288, 293, 570, 294, 10649, 51820], "temperature": 0.0, "avg_logprob": -0.15575680909333406, "compression_ratio": 1.5843373493975903, "no_speech_prob": 0.0007418388850055635}, {"id": 286, "seek": 221620, "start": 2216.2, "end": 2222.68, "text": " the evolution is always governed by some Hamiltonian operator function there's a bunch of energy", "tokens": [50364, 264, 9303, 307, 1009, 35529, 538, 512, 18484, 952, 12973, 2445, 456, 311, 257, 3840, 295, 2281, 50688], "temperature": 0.0, "avg_logprob": -0.09165399045829313, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010158075019717216}, {"id": 287, "seek": 221620, "start": 2222.68, "end": 2229.8799999999997, "text": " terms that you have to kick around and that's why for example um these objects tend to look", "tokens": [50688, 2115, 300, 291, 362, 281, 4437, 926, 293, 300, 311, 983, 337, 1365, 1105, 613, 6565, 3928, 281, 574, 51048], "temperature": 0.0, "avg_logprob": -0.09165399045829313, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010158075019717216}, {"id": 288, "seek": 221620, "start": 2229.8799999999997, "end": 2238.2799999999997, "text": " a little bit like this um like if you're never studied this physics um maybe one way in which", "tokens": [51048, 257, 707, 857, 411, 341, 1105, 411, 498, 291, 434, 1128, 9454, 341, 10649, 1105, 1310, 472, 636, 294, 597, 51468], "temperature": 0.0, "avg_logprob": -0.09165399045829313, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010158075019717216}, {"id": 289, "seek": 221620, "start": 2238.2799999999997, "end": 2243.7999999999997, "text": " you should look at this is that because the evolution of these fields like how they um", "tokens": [51468, 291, 820, 574, 412, 341, 307, 300, 570, 264, 9303, 295, 613, 7909, 411, 577, 436, 1105, 51744], "temperature": 0.0, "avg_logprob": -0.09165399045829313, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0010158075019717216}, {"id": 290, "seek": 224380, "start": 2244.6800000000003, "end": 2253.4, "text": " have often time is governed by the Schrodinger equation which relates the time derivatives", "tokens": [50408, 362, 2049, 565, 307, 35529, 538, 264, 2065, 340, 3584, 260, 5367, 597, 16155, 264, 565, 33733, 50844], "temperature": 0.0, "avg_logprob": -0.16207921916040882, "compression_ratio": 1.64, "no_speech_prob": 0.0005439246306195855}, {"id": 291, "seek": 224380, "start": 2254.44, "end": 2262.44, "text": " like the evolution of the fields themselves to some energy expression captured in the Hamiltonians", "tokens": [50896, 411, 264, 9303, 295, 264, 7909, 2969, 281, 512, 2281, 6114, 11828, 294, 264, 18484, 2567, 51296], "temperature": 0.0, "avg_logprob": -0.16207921916040882, "compression_ratio": 1.64, "no_speech_prob": 0.0005439246306195855}, {"id": 292, "seek": 224380, "start": 2262.44, "end": 2271.88, "text": " and the the energy kinetic energy and in particular for fields is given by some spatial operators", "tokens": [51296, 293, 264, 264, 2281, 27135, 2281, 293, 294, 1729, 337, 7909, 307, 2212, 538, 512, 23598, 19077, 51768], "temperature": 0.0, "avg_logprob": -0.16207921916040882, "compression_ratio": 1.64, "no_speech_prob": 0.0005439246306195855}, {"id": 293, "seek": 227188, "start": 2271.88, "end": 2277.2400000000002, "text": " that's that's why these sort of objects pop up here and you know mass energy equivalents", "tokens": [50364, 300, 311, 300, 311, 983, 613, 1333, 295, 6565, 1665, 493, 510, 293, 291, 458, 2758, 2281, 9052, 791, 50632], "temperature": 0.0, "avg_logprob": -0.0926876809861925, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0005791049334220588}, {"id": 294, "seek": 227188, "start": 2277.2400000000002, "end": 2284.44, "text": " that's why we also have mass and so if you see these Laplacian operators or mass terms um you", "tokens": [50632, 300, 311, 983, 321, 611, 362, 2758, 293, 370, 498, 291, 536, 613, 2369, 564, 326, 952, 19077, 420, 2758, 2115, 1105, 291, 50992], "temperature": 0.0, "avg_logprob": -0.0926876809861925, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0005791049334220588}, {"id": 295, "seek": 227188, "start": 2284.44, "end": 2289.1600000000003, "text": " should not be too surprised uh there because in physics they just always pop up in relation to", "tokens": [50992, 820, 406, 312, 886, 6100, 2232, 456, 570, 294, 10649, 436, 445, 1009, 1665, 493, 294, 9721, 281, 51228], "temperature": 0.0, "avg_logprob": -0.0926876809861925, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0005791049334220588}, {"id": 296, "seek": 227188, "start": 2289.1600000000003, "end": 2296.2000000000003, "text": " the time evolution of the the fields okay so okay now I've already touched upon the concept of time", "tokens": [51228, 264, 565, 9303, 295, 264, 264, 7909, 1392, 370, 1392, 586, 286, 600, 1217, 9828, 3564, 264, 3410, 295, 565, 51580], "temperature": 0.0, "avg_logprob": -0.0926876809861925, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.0005791049334220588}, {"id": 297, "seek": 229620, "start": 2296.2, "end": 2305.3199999999997, "text": " the thing is of course that um here this the fields as they pop up here will um be", "tokens": [50364, 264, 551, 307, 295, 1164, 300, 1105, 510, 341, 264, 7909, 382, 436, 1665, 493, 510, 486, 1105, 312, 50820], "temperature": 0.0, "avg_logprob": -0.1197516216951258, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0019871608819812536}, {"id": 298, "seek": 229620, "start": 2308.2799999999997, "end": 2315.64, "text": " not uh like what you get there is the better controlled theory of euclidean fields right", "tokens": [50968, 406, 2232, 411, 437, 291, 483, 456, 307, 264, 1101, 10164, 5261, 295, 308, 1311, 31264, 282, 7909, 558, 51336], "temperature": 0.0, "avg_logprob": -0.1197516216951258, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0019871608819812536}, {"id": 299, "seek": 229620, "start": 2315.64, "end": 2322.4399999999996, "text": " you're not you're not having to do a priori with spacetime metrics and all these things which make", "tokens": [51336, 291, 434, 406, 291, 434, 406, 1419, 281, 360, 257, 4059, 72, 365, 39404, 9764, 16367, 293, 439, 613, 721, 597, 652, 51676], "temperature": 0.0, "avg_logprob": -0.1197516216951258, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0019871608819812536}, {"id": 300, "seek": 232244, "start": 2322.44, "end": 2329.88, "text": " field theory quantum field field extra complicated but um just talking about Gaussian processes is", "tokens": [50364, 2519, 5261, 13018, 2519, 2519, 2857, 6179, 457, 1105, 445, 1417, 466, 39148, 7555, 307, 50736], "temperature": 0.0, "avg_logprob": -0.10580890625715256, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.003480619518086314}, {"id": 301, "seek": 232244, "start": 2329.88, "end": 2335.0, "text": " just talking about stochastics and then there is this sort of bridge that you have to take", "tokens": [50736, 445, 1417, 466, 342, 8997, 21598, 293, 550, 456, 307, 341, 1333, 295, 7283, 300, 291, 362, 281, 747, 50992], "temperature": 0.0, "avg_logprob": -0.10580890625715256, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.003480619518086314}, {"id": 302, "seek": 232244, "start": 2335.0, "end": 2343.2400000000002, "text": " and hope that you can get from the the euclidean field theories to um to some actual quantum field", "tokens": [50992, 293, 1454, 300, 291, 393, 483, 490, 264, 264, 308, 1311, 31264, 282, 2519, 13667, 281, 1105, 281, 512, 3539, 13018, 2519, 51404], "temperature": 0.0, "avg_logprob": -0.10580890625715256, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.003480619518086314}, {"id": 303, "seek": 234324, "start": 2343.24, "end": 2352.3599999999997, "text": " and I will just name drop um a bunch of concepts there the idea is that you uh if you approach", "tokens": [50364, 293, 286, 486, 445, 1315, 3270, 1105, 257, 3840, 295, 10392, 456, 264, 1558, 307, 300, 291, 2232, 498, 291, 3109, 50820], "temperature": 0.0, "avg_logprob": -0.11495428085327149, "compression_ratio": 1.4921875, "no_speech_prob": 0.010155386291444302}, {"id": 304, "seek": 234324, "start": 2352.3599999999997, "end": 2360.9199999999996, "text": " quantum field theory with this neural network she bang then um you want to find a neural network", "tokens": [50820, 13018, 2519, 5261, 365, 341, 18161, 3209, 750, 8550, 550, 1105, 291, 528, 281, 915, 257, 18161, 3209, 51248], "temperature": 0.0, "avg_logprob": -0.11495428085327149, "compression_ratio": 1.4921875, "no_speech_prob": 0.010155386291444302}, {"id": 305, "seek": 236092, "start": 2360.92, "end": 2374.12, "text": " which mimics the weak rotated version uh of um of physical quantum field and so there is um", "tokens": [50364, 597, 12247, 1167, 264, 5336, 42146, 3037, 2232, 295, 1105, 295, 4001, 13018, 2519, 293, 370, 456, 307, 1105, 51024], "temperature": 0.0, "avg_logprob": -0.13361565011446594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015716204652562737}, {"id": 306, "seek": 236092, "start": 2375.16, "end": 2380.44, "text": " a bunch of uh so-called constructive quantum field theory coming in so there are various approaches", "tokens": [51076, 257, 3840, 295, 2232, 370, 12, 11880, 30223, 13018, 2519, 5261, 1348, 294, 370, 456, 366, 3683, 11587, 51340], "temperature": 0.0, "avg_logprob": -0.13361565011446594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015716204652562737}, {"id": 307, "seek": 236092, "start": 2380.44, "end": 2387.2400000000002, "text": " for of people trying to um uh make certain aspects of quantum field theory more rigorous", "tokens": [51340, 337, 295, 561, 1382, 281, 1105, 2232, 652, 1629, 7270, 295, 13018, 2519, 5261, 544, 29882, 51680], "temperature": 0.0, "avg_logprob": -0.13361565011446594, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0015716204652562737}, {"id": 308, "seek": 238724, "start": 2387.24, "end": 2396.52, "text": " and um transfer like get rid of uh pseudo metrics in quantum field theory move everything um to the", "tokens": [50364, 293, 1105, 5003, 411, 483, 3973, 295, 2232, 35899, 16367, 294, 13018, 2519, 5261, 1286, 1203, 1105, 281, 264, 50828], "temperature": 0.0, "avg_logprob": -0.11304608753749303, "compression_ratio": 1.6054054054054054, "no_speech_prob": 9.024193423101678e-05}, {"id": 309, "seek": 238724, "start": 2396.52, "end": 2406.8399999999997, "text": " euclidean domain where you have nice metrics and um uh so in this paper for example they say that um", "tokens": [50828, 308, 1311, 31264, 282, 9274, 689, 291, 362, 1481, 16367, 293, 1105, 2232, 370, 294, 341, 3035, 337, 1365, 436, 584, 300, 1105, 51344], "temperature": 0.0, "avg_logprob": -0.11304608753749303, "compression_ratio": 1.6054054054054054, "no_speech_prob": 9.024193423101678e-05}, {"id": 310, "seek": 238724, "start": 2406.8399999999997, "end": 2412.6, "text": " you know what we really want to impose is um neural networks which when you view them as a field", "tokens": [51344, 291, 458, 437, 321, 534, 528, 281, 26952, 307, 1105, 18161, 9590, 597, 562, 291, 1910, 552, 382, 257, 2519, 51632], "temperature": 0.0, "avg_logprob": -0.11304608753749303, "compression_ratio": 1.6054054054054054, "no_speech_prob": 9.024193423101678e-05}, {"id": 311, "seek": 241260, "start": 2412.6, "end": 2417.64, "text": " behave in a certain way and something that is important there for example are these uh", "tokens": [50364, 15158, 294, 257, 1629, 636, 293, 746, 300, 307, 1021, 456, 337, 1365, 366, 613, 2232, 50616], "temperature": 0.0, "avg_logprob": -0.1543834083958676, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.008571749553084373}, {"id": 312, "seek": 241260, "start": 2417.64, "end": 2425.0, "text": " Osterwald Schrader relations so for example here in this case the um correlation functions um", "tokens": [50616, 422, 3120, 33262, 2065, 6206, 260, 2299, 370, 337, 1365, 510, 294, 341, 1389, 264, 1105, 20009, 6828, 1105, 50984], "temperature": 0.0, "avg_logprob": -0.1543834083958676, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.008571749553084373}, {"id": 313, "seek": 241260, "start": 2425.0, "end": 2429.96, "text": " these particular coordination functions of relevance here are called denoted s and then you see on", "tokens": [50984, 613, 1729, 21252, 6828, 295, 32684, 510, 366, 1219, 1441, 23325, 262, 293, 550, 291, 536, 322, 51232], "temperature": 0.0, "avg_logprob": -0.1543834083958676, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.008571749553084373}, {"id": 314, "seek": 241260, "start": 2429.96, "end": 2435.3199999999997, "text": " the screen a bunch of properties that these shall have right so there you have the physical", "tokens": [51232, 264, 2568, 257, 3840, 295, 7221, 300, 613, 4393, 362, 558, 370, 456, 291, 362, 264, 4001, 51500], "temperature": 0.0, "avg_logprob": -0.1543834083958676, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.008571749553084373}, {"id": 315, "seek": 241260, "start": 2435.3199999999997, "end": 2442.2799999999997, "text": " translation in variances of certain objects and certain symmetry or independence relations right", "tokens": [51500, 12853, 294, 1374, 21518, 295, 1629, 6565, 293, 1629, 25440, 420, 14640, 2299, 558, 51848], "temperature": 0.0, "avg_logprob": -0.1543834083958676, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.008571749553084373}, {"id": 316, "seek": 244228, "start": 2442.28, "end": 2448.1200000000003, "text": " i'm just mentioning this that there is uh it's not like um you just take any neural network", "tokens": [50364, 741, 478, 445, 18315, 341, 300, 456, 307, 2232, 309, 311, 406, 411, 1105, 291, 445, 747, 604, 18161, 3209, 50656], "temperature": 0.0, "avg_logprob": -0.14427175078281138, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0029789379332214594}, {"id": 317, "seek": 244228, "start": 2448.1200000000003, "end": 2453.7200000000003, "text": " and then you get some uh some quantum field theory in the path into word formalisms out of it", "tokens": [50656, 293, 550, 291, 483, 512, 2232, 512, 13018, 2519, 5261, 294, 264, 3100, 666, 1349, 9860, 13539, 484, 295, 309, 50936], "temperature": 0.0, "avg_logprob": -0.14427175078281138, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0029789379332214594}, {"id": 318, "seek": 244228, "start": 2453.7200000000003, "end": 2461.32, "text": " there's a fairly restricted subset of uh fields that the physicists for quantum field theory might", "tokens": [50936, 456, 311, 257, 6457, 20608, 25993, 295, 2232, 7909, 300, 264, 48716, 337, 13018, 2519, 5261, 1062, 51316], "temperature": 0.0, "avg_logprob": -0.14427175078281138, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0029789379332214594}, {"id": 319, "seek": 244228, "start": 2461.32, "end": 2469.1600000000003, "text": " be interested in um okay but i should probably not go into too much detail on that uh here", "tokens": [51316, 312, 3102, 294, 1105, 1392, 457, 741, 820, 1391, 406, 352, 666, 886, 709, 2607, 322, 300, 2232, 510, 51708], "temperature": 0.0, "avg_logprob": -0.14427175078281138, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.0029789379332214594}, {"id": 320, "seek": 246916, "start": 2469.24, "end": 2480.7599999999998, "text": " um yeah uh also the um these objects in this um exponential so sorry here um", "tokens": [50368, 1105, 1338, 2232, 611, 264, 1105, 613, 6565, 294, 341, 1105, 21510, 370, 2597, 510, 1105, 50944], "temperature": 0.0, "avg_logprob": -0.147951254204138, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001986585557460785}, {"id": 321, "seek": 246916, "start": 2482.68, "end": 2487.72, "text": " if you if you know some physics then you know this but um if you don't then just want to mention", "tokens": [51040, 498, 291, 498, 291, 458, 512, 10649, 550, 291, 458, 341, 457, 1105, 498, 291, 500, 380, 550, 445, 528, 281, 2152, 51292], "temperature": 0.0, "avg_logprob": -0.147951254204138, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001986585557460785}, {"id": 322, "seek": 246916, "start": 2487.72, "end": 2495.64, "text": " that these sort of s s um basically any s that you can write down uh gives you some field theory", "tokens": [51292, 300, 613, 1333, 295, 262, 262, 1105, 1936, 604, 262, 300, 291, 393, 2464, 760, 2232, 2709, 291, 512, 2519, 5261, 51688], "temperature": 0.0, "avg_logprob": -0.147951254204138, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001986585557460785}, {"id": 323, "seek": 249564, "start": 2495.64, "end": 2506.04, "text": " these s s are some sums or integrals over energy terms and if you go to um sorry for the click", "tokens": [50364, 613, 262, 262, 366, 512, 34499, 420, 3572, 1124, 670, 2281, 2115, 293, 498, 291, 352, 281, 1105, 2597, 337, 264, 2052, 50884], "temperature": 0.0, "avg_logprob": -0.09730212828692268, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0013040787307545543}, {"id": 324, "seek": 249564, "start": 2506.68, "end": 2512.12, "text": " clickery um if you go to the Lagrangian field theory Wikipedia page then you can find", "tokens": [50916, 2052, 2109, 1105, 498, 291, 352, 281, 264, 24886, 32926, 952, 2519, 5261, 28999, 3028, 550, 291, 393, 915, 51188], "temperature": 0.0, "avg_logprob": -0.09730212828692268, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0013040787307545543}, {"id": 325, "seek": 249564, "start": 2512.8399999999997, "end": 2520.2, "text": " a whole lot of different um s objects that make sense and you can see here see how they", "tokens": [51224, 257, 1379, 688, 295, 819, 1105, 262, 6565, 300, 652, 2020, 293, 291, 393, 536, 510, 536, 577, 436, 51592], "temperature": 0.0, "avg_logprob": -0.09730212828692268, "compression_ratio": 1.5857988165680474, "no_speech_prob": 0.0013040787307545543}, {"id": 326, "seek": 252020, "start": 2520.2, "end": 2526.52, "text": " data mine how the data mine um the various physical theories that you have certainly heard of", "tokens": [50364, 1412, 3892, 577, 264, 1412, 3892, 1105, 264, 3683, 4001, 13667, 300, 291, 362, 3297, 2198, 295, 50680], "temperature": 0.0, "avg_logprob": -0.20272993486981059, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002395899500697851}, {"id": 327, "seek": 252020, "start": 2526.52, "end": 2531.72, "text": " we are interested in particular about fields so we have here scalar field theories this is what we", "tokens": [50680, 321, 366, 3102, 294, 1729, 466, 7909, 370, 321, 362, 510, 39684, 2519, 13667, 341, 307, 437, 321, 50940], "temperature": 0.0, "avg_logprob": -0.20272993486981059, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002395899500697851}, {"id": 328, "seek": 252020, "start": 2531.72, "end": 2541.16, "text": " just saw you have some partial and then um some um some mass and there will be also be some time", "tokens": [50940, 445, 1866, 291, 362, 512, 14641, 293, 550, 1105, 512, 1105, 512, 2758, 293, 456, 486, 312, 611, 312, 512, 565, 51412], "temperature": 0.0, "avg_logprob": -0.20272993486981059, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002395899500697851}, {"id": 329, "seek": 252020, "start": 2541.16, "end": 2549.7999999999997, "text": " the derivatives there um but the thing is that the pure gaussians are um where this is just", "tokens": [51412, 264, 33733, 456, 1105, 457, 264, 551, 307, 300, 264, 6075, 5959, 2023, 2567, 366, 1105, 689, 341, 307, 445, 51844], "temperature": 0.0, "avg_logprob": -0.20272993486981059, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002395899500697851}, {"id": 330, "seek": 254980, "start": 2549.8, "end": 2557.0800000000004, "text": " this quadratic object in in s are actually relatively um uninteresting from the physical", "tokens": [50364, 341, 37262, 2657, 294, 294, 262, 366, 767, 7226, 1105, 49234, 8714, 490, 264, 4001, 50728], "temperature": 0.0, "avg_logprob": -0.07790598273277283, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.001698845997452736}, {"id": 331, "seek": 254980, "start": 2557.0800000000004, "end": 2562.6800000000003, "text": " perspective because if you have for example if you have different quantum fields that interact", "tokens": [50728, 4585, 570, 498, 291, 362, 337, 1365, 498, 291, 362, 819, 13018, 7909, 300, 4648, 51008], "temperature": 0.0, "avg_logprob": -0.07790598273277283, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.001698845997452736}, {"id": 332, "seek": 254980, "start": 2562.6800000000003, "end": 2568.6800000000003, "text": " with each other and and then this information how they interact is also all encoded in this", "tokens": [51008, 365, 1184, 661, 293, 293, 550, 341, 1589, 577, 436, 4648, 307, 611, 439, 2058, 12340, 294, 341, 51308], "temperature": 0.0, "avg_logprob": -0.07790598273277283, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.001698845997452736}, {"id": 333, "seek": 254980, "start": 2568.6800000000003, "end": 2575.5600000000004, "text": " in this um in this Lagrangian cells or in the action s and then uh you will have some more", "tokens": [51308, 294, 341, 1105, 294, 341, 24886, 32926, 952, 5438, 420, 294, 264, 3069, 262, 293, 550, 2232, 291, 486, 362, 512, 544, 51652], "temperature": 0.0, "avg_logprob": -0.07790598273277283, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.001698845997452736}, {"id": 334, "seek": 257556, "start": 2575.56, "end": 2583.32, "text": " complicated products in these objects and um if you have some power of the field that's higher than", "tokens": [50364, 6179, 3383, 294, 613, 6565, 293, 1105, 498, 291, 362, 512, 1347, 295, 264, 2519, 300, 311, 2946, 813, 50752], "temperature": 0.0, "avg_logprob": -0.1297522911658654, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00115075777284801}, {"id": 335, "seek": 257556, "start": 2583.32, "end": 2592.84, "text": " two then this is actually uh representing um sort of self interaction in in field theory so um what", "tokens": [50752, 732, 550, 341, 307, 767, 2232, 13460, 1105, 1333, 295, 2698, 9285, 294, 294, 2519, 5261, 370, 1105, 437, 51228], "temperature": 0.0, "avg_logprob": -0.1297522911658654, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00115075777284801}, {"id": 336, "seek": 257556, "start": 2592.84, "end": 2600.2799999999997, "text": " we really want to have is not just um the fields which fulfill these nice um properties in the", "tokens": [51228, 321, 534, 528, 281, 362, 307, 406, 445, 1105, 264, 7909, 597, 13875, 613, 1481, 1105, 7221, 294, 264, 51600], "temperature": 0.0, "avg_logprob": -0.1297522911658654, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.00115075777284801}, {"id": 337, "seek": 260028, "start": 2601.0, "end": 2606.92, "text": " Euclidean version but we also want to have very finely controlled um interaction terms", "tokens": [50400, 462, 1311, 31264, 282, 3037, 457, 321, 611, 528, 281, 362, 588, 31529, 10164, 1105, 9285, 2115, 50696], "temperature": 0.0, "avg_logprob": -0.12617592016855875, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.003026541555300355}, {"id": 338, "seek": 260028, "start": 2606.92, "end": 2616.36, "text": " there and so what we really want to have is um processes which are actually not gaussian right", "tokens": [50696, 456, 293, 370, 437, 321, 534, 528, 281, 362, 307, 1105, 7555, 597, 366, 767, 406, 5959, 21948, 558, 51168], "temperature": 0.0, "avg_logprob": -0.12617592016855875, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.003026541555300355}, {"id": 339, "seek": 260028, "start": 2617.32, "end": 2623.0, "text": " and so what they do in the paper is in in the end um look at", "tokens": [51216, 293, 370, 437, 436, 360, 294, 264, 3035, 307, 294, 294, 264, 917, 1105, 574, 412, 51500], "temperature": 0.0, "avg_logprob": -0.12617592016855875, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.003026541555300355}, {"id": 340, "seek": 262300, "start": 2623.0, "end": 2634.2, "text": " uh um five to the fourth field theory so quadratic interaction let me see sorry", "tokens": [50364, 2232, 1105, 1732, 281, 264, 6409, 2519, 5261, 370, 37262, 9285, 718, 385, 536, 2597, 50924], "temperature": 0.0, "avg_logprob": -0.2086618699525532, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.002395165618509054}, {"id": 341, "seek": 262300, "start": 2647.4, "end": 2651.32, "text": " so what they really want to implement and what they actually then do in the paper is", "tokens": [51584, 370, 437, 436, 534, 528, 281, 4445, 293, 437, 436, 767, 550, 360, 294, 264, 3035, 307, 51780], "temperature": 0.0, "avg_logprob": -0.2086618699525532, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.002395165618509054}, {"id": 342, "seek": 265132, "start": 2651.32, "end": 2656.76, "text": " they take this sort of action you you not only have this quadratic term there but you also have", "tokens": [50364, 436, 747, 341, 1333, 295, 3069, 291, 291, 406, 787, 362, 341, 37262, 1433, 456, 457, 291, 611, 362, 50636], "temperature": 0.0, "avg_logprob": -0.12683774507962742, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0014769466361030936}, {"id": 343, "seek": 265132, "start": 2657.4, "end": 2669.32, "text": " their um this five to the four uh term and to to get this in to get um uh away from the", "tokens": [50668, 641, 1105, 341, 1732, 281, 264, 1451, 2232, 1433, 293, 281, 281, 483, 341, 294, 281, 483, 1105, 2232, 1314, 490, 264, 51264], "temperature": 0.0, "avg_logprob": -0.12683774507962742, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0014769466361030936}, {"id": 344, "seek": 265132, "start": 2669.32, "end": 2677.2400000000002, "text": " just quadratic gaussian process scenario there is two ways um to implement this self interaction", "tokens": [51264, 445, 37262, 5959, 21948, 1399, 9005, 456, 307, 732, 2098, 1105, 281, 4445, 341, 2698, 9285, 51660], "temperature": 0.0, "avg_logprob": -0.12683774507962742, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0014769466361030936}, {"id": 345, "seek": 267724, "start": 2677.4799999999996, "end": 2685.72, "text": " uh and one way is to actually not look at the infinite and limit right not the infinite", "tokens": [50376, 2232, 293, 472, 636, 307, 281, 767, 406, 574, 412, 264, 13785, 293, 4948, 558, 406, 264, 13785, 50788], "temperature": 0.0, "avg_logprob": -0.17148863259008376, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.0023216712288558483}, {"id": 346, "seek": 267724, "start": 2687.08, "end": 2696.52, "text": " size network limit um because then uh the you basically break the neural network gaussian", "tokens": [50856, 2744, 3209, 4948, 1105, 570, 550, 2232, 264, 291, 1936, 1821, 264, 18161, 3209, 5959, 21948, 51328], "temperature": 0.0, "avg_logprob": -0.17148863259008376, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.0023216712288558483}, {"id": 347, "seek": 267724, "start": 2696.52, "end": 2705.3999999999996, "text": " process scenario you you person person uh you purposely stop a little bit earlier and get some", "tokens": [51328, 1399, 9005, 291, 291, 954, 954, 2232, 291, 41840, 1590, 257, 707, 857, 3071, 293, 483, 512, 51772], "temperature": 0.0, "avg_logprob": -0.17148863259008376, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.0023216712288558483}, {"id": 348, "seek": 270540, "start": 2705.48, "end": 2711.32, "text": " non gaussian effects and so what from the mathematical point of view is a bug that for", "tokens": [50368, 2107, 5959, 21948, 5065, 293, 370, 437, 490, 264, 18894, 935, 295, 1910, 307, 257, 7426, 300, 337, 50660], "temperature": 0.0, "avg_logprob": -0.07951713800430298, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.015177948400378227}, {"id": 349, "seek": 270540, "start": 2711.32, "end": 2717.2400000000002, "text": " a real network you actually don't get a perfect gaussian situation here becomes sort of feature", "tokens": [50660, 257, 957, 3209, 291, 767, 500, 380, 483, 257, 2176, 5959, 21948, 2590, 510, 3643, 1333, 295, 4111, 50956], "temperature": 0.0, "avg_logprob": -0.07951713800430298, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.015177948400378227}, {"id": 350, "seek": 270540, "start": 2717.2400000000002, "end": 2723.64, "text": " it gives you the freedom to actually implement behavior and if you tweak the network uh nice", "tokens": [50956, 309, 2709, 291, 264, 5645, 281, 767, 4445, 5223, 293, 498, 291, 29879, 264, 3209, 2232, 1481, 51276], "temperature": 0.0, "avg_logprob": -0.07951713800430298, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.015177948400378227}, {"id": 351, "seek": 270540, "start": 2723.64, "end": 2729.1600000000003, "text": " enough the ideas that then you can sort of control how it is broken this bug certainly", "tokens": [51276, 1547, 264, 3487, 300, 550, 291, 393, 1333, 295, 1969, 577, 309, 307, 5463, 341, 7426, 3297, 51552], "temperature": 0.0, "avg_logprob": -0.07951713800430298, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.015177948400378227}, {"id": 352, "seek": 272916, "start": 2729.24, "end": 2736.2799999999997, "text": " becomes a feature this is one way and the other way is if you um actually um", "tokens": [50368, 3643, 257, 4111, 341, 307, 472, 636, 293, 264, 661, 636, 307, 498, 291, 1105, 767, 1105, 50720], "temperature": 0.0, "avg_logprob": -0.12522030743685636, "compression_ratio": 1.58125, "no_speech_prob": 0.021606573835015297}, {"id": 353, "seek": 272916, "start": 2739.08, "end": 2745.8799999999997, "text": " in sampling you do not uh take a trillion independent distributions over the weights", "tokens": [50860, 294, 21179, 291, 360, 406, 2232, 747, 257, 18723, 6695, 37870, 670, 264, 17443, 51200], "temperature": 0.0, "avg_logprob": -0.12522030743685636, "compression_ratio": 1.58125, "no_speech_prob": 0.021606573835015297}, {"id": 354, "seek": 272916, "start": 2745.8799999999997, "end": 2753.56, "text": " but what you and instead do is you introduce on purpose some dependencies of the individual", "tokens": [51200, 457, 437, 291, 293, 2602, 360, 307, 291, 5366, 322, 4334, 512, 36606, 295, 264, 2609, 51584], "temperature": 0.0, "avg_logprob": -0.12522030743685636, "compression_ratio": 1.58125, "no_speech_prob": 0.021606573835015297}, {"id": 355, "seek": 275356, "start": 2753.56, "end": 2759.48, "text": " weight uh distributions right do you break the independence such that um there is a little", "tokens": [50364, 3364, 2232, 37870, 558, 360, 291, 1821, 264, 14640, 1270, 300, 1105, 456, 307, 257, 707, 50660], "temperature": 0.0, "avg_logprob": -0.09536447869725974, "compression_ratio": 1.885, "no_speech_prob": 0.014722186140716076}, {"id": 356, "seek": 275356, "start": 2759.48, "end": 2765.72, "text": " bit of a flaw in the central limit theorem and in this way by independence you also get some", "tokens": [50660, 857, 295, 257, 13717, 294, 264, 5777, 4948, 20904, 293, 294, 341, 636, 538, 14640, 291, 611, 483, 512, 50972], "temperature": 0.0, "avg_logprob": -0.09536447869725974, "compression_ratio": 1.885, "no_speech_prob": 0.014722186140716076}, {"id": 357, "seek": 275356, "start": 2765.72, "end": 2771.7999999999997, "text": " non gaussian process because it's clear that if you do only um break the independence of this", "tokens": [50972, 2107, 5959, 21948, 1399, 570, 309, 311, 1850, 300, 498, 291, 360, 787, 1105, 1821, 264, 14640, 295, 341, 51276], "temperature": 0.0, "avg_logprob": -0.09536447869725974, "compression_ratio": 1.885, "no_speech_prob": 0.014722186140716076}, {"id": 358, "seek": 275356, "start": 2771.7999999999997, "end": 2777.64, "text": " weights a little bit you'll still by the central limit theorem get something which is just a slight", "tokens": [51276, 17443, 257, 707, 857, 291, 603, 920, 538, 264, 5777, 4948, 20904, 483, 746, 597, 307, 445, 257, 4036, 51568], "temperature": 0.0, "avg_logprob": -0.09536447869725974, "compression_ratio": 1.885, "no_speech_prob": 0.014722186140716076}, {"id": 359, "seek": 277764, "start": 2777.64, "end": 2783.3199999999997, "text": " deviation from discussion processes right so if you the idea is if you tweak the this", "tokens": [50364, 25163, 490, 5017, 7555, 558, 370, 498, 291, 264, 1558, 307, 498, 291, 29879, 264, 341, 50648], "temperature": 0.0, "avg_logprob": -0.12791813251584075, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.004328610375523567}, {"id": 360, "seek": 277764, "start": 2783.3199999999997, "end": 2788.92, "text": " the conditions for the god for the central limit theorem just in a wide way then you might produce", "tokens": [50648, 264, 4487, 337, 264, 3044, 337, 264, 5777, 4948, 20904, 445, 294, 257, 4874, 636, 550, 291, 1062, 5258, 50928], "temperature": 0.0, "avg_logprob": -0.12791813251584075, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.004328610375523567}, {"id": 361, "seek": 277764, "start": 2791.48, "end": 2799.0, "text": " errors to the the gaussianity in a in a in a very controlled way and this is exactly what", "tokens": [51056, 13603, 281, 264, 264, 5959, 21948, 507, 294, 257, 294, 257, 294, 257, 588, 10164, 636, 293, 341, 307, 2293, 437, 51432], "temperature": 0.0, "avg_logprob": -0.12791813251584075, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.004328610375523567}, {"id": 362, "seek": 277764, "start": 2799.0, "end": 2806.68, "text": " they do here so here they describe a neural network with particular non-linearity as activations it", "tokens": [51432, 436, 360, 510, 370, 510, 436, 6786, 257, 18161, 3209, 365, 1729, 2107, 12, 1889, 17409, 382, 2430, 763, 309, 51816], "temperature": 0.0, "avg_logprob": -0.12791813251584075, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.004328610375523567}, {"id": 363, "seek": 280668, "start": 2806.68, "end": 2813.7999999999997, "text": " looks like this and what they do is they sample in this um in this dependent way in exactly the", "tokens": [50364, 1542, 411, 341, 293, 437, 436, 360, 307, 436, 6889, 294, 341, 1105, 294, 341, 12334, 636, 294, 2293, 264, 50720], "temperature": 0.0, "avg_logprob": -0.1104017212277367, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.0007088261190801859}, {"id": 364, "seek": 280668, "start": 2813.7999999999997, "end": 2825.64, "text": " correct way so that the um the action um that emerges by random sampling in this you know", "tokens": [50720, 3006, 636, 370, 300, 264, 1105, 264, 3069, 1105, 300, 38965, 538, 4974, 21179, 294, 341, 291, 458, 51312], "temperature": 0.0, "avg_logprob": -0.1104017212277367, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.0007088261190801859}, {"id": 365, "seek": 280668, "start": 2826.44, "end": 2834.68, "text": " particular way represents uh this uh this sort of physical field theory so this field theory in", "tokens": [51352, 1729, 636, 8855, 2232, 341, 2232, 341, 1333, 295, 4001, 2519, 5261, 370, 341, 2519, 5261, 294, 51764], "temperature": 0.0, "avg_logprob": -0.1104017212277367, "compression_ratio": 1.7784810126582278, "no_speech_prob": 0.0007088261190801859}, {"id": 366, "seek": 283468, "start": 2834.68, "end": 2840.3599999999997, "text": " particular is basically always the first interaction uh interacting field theory or one of the first", "tokens": [50364, 1729, 307, 1936, 1009, 264, 700, 9285, 2232, 18017, 2519, 5261, 420, 472, 295, 264, 700, 50648], "temperature": 0.0, "avg_logprob": -0.07648940629596952, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00020660526934079826}, {"id": 367, "seek": 283468, "start": 2840.3599999999997, "end": 2845.0, "text": " interaction field theories that you would learn in the quantum field theory course um this is not", "tokens": [50648, 9285, 2519, 13667, 300, 291, 576, 1466, 294, 264, 13018, 2519, 5261, 1164, 1105, 341, 307, 406, 50880], "temperature": 0.0, "avg_logprob": -0.07648940629596952, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00020660526934079826}, {"id": 368, "seek": 283468, "start": 2845.0, "end": 2855.3199999999997, "text": " one of the famous standard model um energy densities at least not in the exact same way here", "tokens": [50880, 472, 295, 264, 4618, 3832, 2316, 1105, 2281, 24505, 1088, 412, 1935, 406, 294, 264, 1900, 912, 636, 510, 51396], "temperature": 0.0, "avg_logprob": -0.07648940629596952, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00020660526934079826}, {"id": 369, "seek": 283468, "start": 2856.2799999999997, "end": 2861.0, "text": " but nonetheless this is like classical physical theory and so this is what this paper is all", "tokens": [51444, 457, 26756, 341, 307, 411, 13735, 4001, 5261, 293, 370, 341, 307, 437, 341, 3035, 307, 439, 51680], "temperature": 0.0, "avg_logprob": -0.07648940629596952, "compression_ratio": 1.900990099009901, "no_speech_prob": 0.00020660526934079826}, {"id": 370, "seek": 286100, "start": 2861.0, "end": 2868.2, "text": " about right breaking the neural network gaussian process uh theorems in the correct way to get um", "tokens": [50364, 466, 558, 7697, 264, 18161, 3209, 5959, 21948, 1399, 2232, 10299, 2592, 294, 264, 3006, 636, 281, 483, 1105, 50724], "temperature": 0.0, "avg_logprob": -0.12447661906480789, "compression_ratio": 1.7028571428571428, "no_speech_prob": 0.004978802055120468}, {"id": 371, "seek": 286100, "start": 2869.48, "end": 2878.2, "text": " the the right uh g functions endpoint functions out there that are relevant for physics um and", "tokens": [50788, 264, 264, 558, 2232, 290, 6828, 35795, 6828, 484, 456, 300, 366, 7340, 337, 10649, 1105, 293, 51224], "temperature": 0.0, "avg_logprob": -0.12447661906480789, "compression_ratio": 1.7028571428571428, "no_speech_prob": 0.004978802055120468}, {"id": 372, "seek": 286100, "start": 2878.2, "end": 2886.04, "text": " so you see that then you can you know in principle sample uh from this fixed neural network architectures", "tokens": [51224, 370, 291, 536, 300, 550, 291, 393, 291, 458, 294, 8665, 6889, 2232, 490, 341, 6806, 18161, 3209, 6331, 1303, 51616], "temperature": 0.0, "avg_logprob": -0.12447661906480789, "compression_ratio": 1.7028571428571428, "no_speech_prob": 0.004978802055120468}, {"id": 373, "seek": 288604, "start": 2886.04, "end": 2892.92, "text": " fields and then once you have like a way of sampling fields you can in principle because", "tokens": [50364, 7909, 293, 550, 1564, 291, 362, 411, 257, 636, 295, 21179, 7909, 291, 393, 294, 8665, 570, 50708], "temperature": 0.0, "avg_logprob": -0.14764442443847656, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0069000073708593845}, {"id": 374, "seek": 288604, "start": 2892.92, "end": 2897.64, "text": " the fields that demand all the properties of the quantum field theory uh compute expectation", "tokens": [50708, 264, 7909, 300, 4733, 439, 264, 7221, 295, 264, 13018, 2519, 5261, 2232, 14722, 14334, 50944], "temperature": 0.0, "avg_logprob": -0.14764442443847656, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0069000073708593845}, {"id": 375, "seek": 288604, "start": 2897.64, "end": 2903.48, "text": " values and thereby get physical uh information so this is the idea it also goes in the other direction", "tokens": [50944, 4190, 293, 28281, 483, 4001, 2232, 1589, 370, 341, 307, 264, 1558, 309, 611, 1709, 294, 264, 661, 3513, 51236], "temperature": 0.0, "avg_logprob": -0.14764442443847656, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0069000073708593845}, {"id": 376, "seek": 288604, "start": 2903.48, "end": 2914.04, "text": " in um since there is uh these methods in particular um Feynman diagram computation methods that compute", "tokens": [51236, 294, 1105, 1670, 456, 307, 2232, 613, 7150, 294, 1729, 1105, 46530, 77, 1601, 10686, 24903, 7150, 300, 14722, 51764], "temperature": 0.0, "avg_logprob": -0.14764442443847656, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0069000073708593845}, {"id": 377, "seek": 291404, "start": 2914.04, "end": 2923.32, "text": " correlations um for quantum fields you also have a method of computing aspects of random", "tokens": [50364, 13983, 763, 1105, 337, 13018, 7909, 291, 611, 362, 257, 3170, 295, 15866, 7270, 295, 4974, 50828], "temperature": 0.0, "avg_logprob": -0.09217028464040448, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.0008828467107377946}, {"id": 378, "seek": 291404, "start": 2923.32, "end": 2929.32, "text": " initialized big neural networks right you have a big neural network you know that if you uh", "tokens": [50828, 5883, 1602, 955, 18161, 9590, 558, 291, 362, 257, 955, 18161, 3209, 291, 458, 300, 498, 291, 2232, 51128], "temperature": 0.0, "avg_logprob": -0.09217028464040448, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.0008828467107377946}, {"id": 379, "seek": 291404, "start": 2932.44, "end": 2939.24, "text": " to sample from it it is as if you would sample from a quantum field um and because you have", "tokens": [51284, 281, 6889, 490, 309, 309, 307, 382, 498, 291, 576, 6889, 490, 257, 13018, 2519, 1105, 293, 570, 291, 362, 51624], "temperature": 0.0, "avg_logprob": -0.09217028464040448, "compression_ratio": 1.7662337662337662, "no_speech_prob": 0.0008828467107377946}, {"id": 380, "seek": 293924, "start": 2939.24, "end": 2944.3599999999997, "text": " ways in physics to compute aspects like correlation functions and so on of the quantum fields", "tokens": [50364, 2098, 294, 10649, 281, 14722, 7270, 411, 20009, 6828, 293, 370, 322, 295, 264, 13018, 7909, 50620], "temperature": 0.0, "avg_logprob": -0.07384635875751445, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.0016226178267970681}, {"id": 381, "seek": 293924, "start": 2945.0, "end": 2950.2799999999997, "text": " these uh g functions that you can compute in physics with Feynman diagrams and so on", "tokens": [50652, 613, 2232, 290, 6828, 300, 291, 393, 14722, 294, 10649, 365, 46530, 77, 1601, 36709, 293, 370, 322, 50916], "temperature": 0.0, "avg_logprob": -0.07384635875751445, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.0016226178267970681}, {"id": 382, "seek": 293924, "start": 2950.2799999999997, "end": 2959.3199999999997, "text": " also have a relevant meaning for computing typical aspects of random initialized neural networks", "tokens": [50916, 611, 362, 257, 7340, 3620, 337, 15866, 7476, 7270, 295, 4974, 5883, 1602, 18161, 9590, 51368], "temperature": 0.0, "avg_logprob": -0.07384635875751445, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.0016226178267970681}, {"id": 383, "seek": 293924, "start": 2959.9599999999996, "end": 2964.6, "text": " okay i know this is a little bit much but i hope it sort of makes sense um", "tokens": [51400, 1392, 741, 458, 341, 307, 257, 707, 857, 709, 457, 741, 1454, 309, 1333, 295, 1669, 2020, 1105, 51632], "temperature": 0.0, "avg_logprob": -0.07384635875751445, "compression_ratio": 1.6826923076923077, "no_speech_prob": 0.0016226178267970681}, {"id": 384, "seek": 296460, "start": 2964.6, "end": 2976.52, "text": " um i um i don't know how clear everything was that i discussed so far i just want to give you", "tokens": [50364, 1105, 741, 1105, 741, 500, 380, 458, 577, 1850, 1203, 390, 300, 741, 7152, 370, 1400, 741, 445, 528, 281, 976, 291, 50960], "temperature": 0.0, "avg_logprob": -0.16865527991092566, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.0017267902148887515}, {"id": 385, "seek": 296460, "start": 2977.96, "end": 2983.24, "text": " this is then more on the quantum field theory side a little motivation that i have just here", "tokens": [51032, 341, 307, 550, 544, 322, 264, 13018, 2519, 5261, 1252, 257, 707, 12335, 300, 741, 362, 445, 510, 51296], "temperature": 0.0, "avg_logprob": -0.16865527991092566, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.0017267902148887515}, {"id": 386, "seek": 296460, "start": 2983.24, "end": 2990.04, "text": " written up um that connects these g functions right these autocorrelations to physics i just", "tokens": [51296, 3720, 493, 1105, 300, 16967, 613, 290, 6828, 558, 613, 45833, 284, 4419, 763, 281, 10649, 741, 445, 51636], "temperature": 0.0, "avg_logprob": -0.16865527991092566, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.0017267902148887515}, {"id": 387, "seek": 299004, "start": 2990.12, "end": 2996.84, "text": " want to motivate it maybe give me a three more minutes so i i'm going to assume that you have", "tokens": [50368, 528, 281, 28497, 309, 1310, 976, 385, 257, 1045, 544, 2077, 370, 741, 741, 478, 516, 281, 6552, 300, 291, 362, 50704], "temperature": 0.0, "avg_logprob": -0.13206639073111795, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0013037315802648664}, {"id": 388, "seek": 299004, "start": 2996.84, "end": 3003.0, "text": " an idea of the fact that transition probabilities are the squares of inner products in a Hilbert's", "tokens": [50704, 364, 1558, 295, 264, 1186, 300, 6034, 33783, 366, 264, 19368, 295, 7284, 3383, 294, 257, 19914, 4290, 311, 51012], "temperature": 0.0, "avg_logprob": -0.13206639073111795, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0013037315802648664}, {"id": 389, "seek": 299004, "start": 3003.0, "end": 3010.84, "text": " base in quantum mechanics um so there are these um kernel objects k which are given like that", "tokens": [51012, 3096, 294, 13018, 12939, 1105, 370, 456, 366, 613, 1105, 28256, 6565, 350, 597, 366, 2212, 411, 300, 51404], "temperature": 0.0, "avg_logprob": -0.13206639073111795, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0013037315802648664}, {"id": 390, "seek": 299004, "start": 3010.84, "end": 3017.56, "text": " so in in in you know standard quantum mechanics what you have is some um you have some in um", "tokens": [51404, 370, 294, 294, 294, 291, 458, 3832, 13018, 12939, 437, 291, 362, 307, 512, 1105, 291, 362, 512, 294, 1105, 51740], "temperature": 0.0, "avg_logprob": -0.13206639073111795, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0013037315802648664}, {"id": 391, "seek": 301756, "start": 3018.2799999999997, "end": 3022.6, "text": " current state which i call here in and then you have some uh other state out that you are", "tokens": [50400, 2190, 1785, 597, 741, 818, 510, 294, 293, 550, 291, 362, 512, 2232, 661, 1785, 484, 300, 291, 366, 50616], "temperature": 0.0, "avg_logprob": -0.0957036018371582, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0007094874163158238}, {"id": 392, "seek": 301756, "start": 3022.6, "end": 3029.0, "text": " interested in you want to know what is the chance that this state um transitions over in that state", "tokens": [50616, 3102, 294, 291, 528, 281, 458, 437, 307, 264, 2931, 300, 341, 1785, 1105, 23767, 670, 294, 300, 1785, 50936], "temperature": 0.0, "avg_logprob": -0.0957036018371582, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0007094874163158238}, {"id": 393, "seek": 301756, "start": 3029.0, "end": 3036.36, "text": " what you do is you um take the Schrodinger equation uh evolution which is governed by the", "tokens": [50936, 437, 291, 360, 307, 291, 1105, 747, 264, 2065, 340, 3584, 260, 5367, 2232, 9303, 597, 307, 35529, 538, 264, 51304], "temperature": 0.0, "avg_logprob": -0.0957036018371582, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0007094874163158238}, {"id": 394, "seek": 301756, "start": 3036.36, "end": 3043.88, "text": " Hamiltonian h in in the nicest case um here the formal solution of this equation is just e to the i", "tokens": [51304, 18484, 952, 276, 294, 294, 264, 45516, 1389, 1105, 510, 264, 9860, 3827, 295, 341, 5367, 307, 445, 308, 281, 264, 741, 51680], "temperature": 0.0, "avg_logprob": -0.0957036018371582, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.0007094874163158238}, {"id": 395, "seek": 304388, "start": 3044.84, "end": 3052.36, "text": " h so this is the operator which moves any state forward in time you apply it to in to the like", "tokens": [50412, 276, 370, 341, 307, 264, 12973, 597, 6067, 604, 1785, 2128, 294, 565, 291, 3079, 309, 281, 294, 281, 264, 411, 50788], "temperature": 0.0, "avg_logprob": -0.08435208456856864, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.009264128282666206}, {"id": 396, "seek": 304388, "start": 3052.36, "end": 3059.32, "text": " let's say this is now this is in two days you say this state that i have currently how will it", "tokens": [50788, 718, 311, 584, 341, 307, 586, 341, 307, 294, 732, 1708, 291, 584, 341, 1785, 300, 741, 362, 4362, 577, 486, 309, 51136], "temperature": 0.0, "avg_logprob": -0.08435208456856864, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.009264128282666206}, {"id": 397, "seek": 304388, "start": 3059.32, "end": 3066.28, "text": " evolve into the in like how will it look in two days then you basically apply this operator to", "tokens": [51136, 16693, 666, 264, 294, 411, 577, 486, 309, 574, 294, 732, 1708, 550, 291, 1936, 3079, 341, 12973, 281, 51484], "temperature": 0.0, "avg_logprob": -0.08435208456856864, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.009264128282666206}, {"id": 398, "seek": 306628, "start": 3066.28, "end": 3073.6400000000003, "text": " fast forward this thing um and then you get something uh the in state how it will look like", "tokens": [50364, 2370, 2128, 341, 551, 1105, 293, 550, 291, 483, 746, 2232, 264, 294, 1785, 577, 309, 486, 574, 411, 50732], "temperature": 0.0, "avg_logprob": -0.10593316771767357, "compression_ratio": 1.8186046511627907, "no_speech_prob": 0.005137572530657053}, {"id": 399, "seek": 306628, "start": 3073.6400000000003, "end": 3080.28, "text": " in two days and then the product with this object in two days with the out state that you're interested", "tokens": [50732, 294, 732, 1708, 293, 550, 264, 1674, 365, 341, 2657, 294, 732, 1708, 365, 264, 484, 1785, 300, 291, 434, 3102, 51064], "temperature": 0.0, "avg_logprob": -0.10593316771767357, "compression_ratio": 1.8186046511627907, "no_speech_prob": 0.005137572530657053}, {"id": 400, "seek": 306628, "start": 3080.28, "end": 3087.96, "text": " in gives you the probability that the the current state after it has evolved is going to be observed", "tokens": [51064, 294, 2709, 291, 264, 8482, 300, 264, 264, 2190, 1785, 934, 309, 575, 14178, 307, 516, 281, 312, 13095, 51448], "temperature": 0.0, "avg_logprob": -0.10593316771767357, "compression_ratio": 1.8186046511627907, "no_speech_prob": 0.005137572530657053}, {"id": 401, "seek": 306628, "start": 3087.96, "end": 3094.52, "text": " in uh in the out state okay so this is basically quantum mechanics one one um the for whatever", "tokens": [51448, 294, 2232, 294, 264, 484, 1785, 1392, 370, 341, 307, 1936, 13018, 12939, 472, 472, 1105, 264, 337, 2035, 51776], "temperature": 0.0, "avg_logprob": -0.10593316771767357, "compression_ratio": 1.8186046511627907, "no_speech_prob": 0.005137572530657053}, {"id": 402, "seek": 309452, "start": 3094.52, "end": 3101.88, "text": " Hamiltonian you have here you um can often then compute some analytical case some kernels", "tokens": [50364, 18484, 952, 291, 362, 510, 291, 1105, 393, 2049, 550, 14722, 512, 29579, 1389, 512, 23434, 1625, 50732], "temperature": 0.0, "avg_logprob": -0.15286790756952195, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.0015473752282559872}, {"id": 403, "seek": 309452, "start": 3104.28, "end": 3109.32, "text": " these are these things and i think here if you score down in this Wikipedia page you find some", "tokens": [50852, 613, 366, 613, 721, 293, 741, 519, 510, 498, 291, 6175, 760, 294, 341, 28999, 3028, 291, 915, 512, 51104], "temperature": 0.0, "avg_logprob": -0.15286790756952195, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.0015473752282559872}, {"id": 404, "seek": 309452, "start": 3109.32, "end": 3115.16, "text": " examples so these are just you know in this case uh three particle in one dimension this is like", "tokens": [51104, 5110, 370, 613, 366, 445, 291, 458, 294, 341, 1389, 2232, 1045, 12359, 294, 472, 10139, 341, 307, 411, 51396], "temperature": 0.0, "avg_logprob": -0.15286790756952195, "compression_ratio": 1.6726190476190477, "no_speech_prob": 0.0015473752282559872}, {"id": 405, "seek": 311516, "start": 3115.3199999999997, "end": 3125.48, "text": " some Gaussian this is similar to some heat dissipation equation and then if you have some", "tokens": [50372, 512, 39148, 341, 307, 2531, 281, 512, 3738, 29544, 399, 5367, 293, 550, 498, 291, 362, 512, 50880], "temperature": 0.0, "avg_logprob": -0.1505981513432094, "compression_ratio": 1.5889570552147239, "no_speech_prob": 0.0034796728286892176}, {"id": 406, "seek": 311516, "start": 3125.48, "end": 3130.8399999999997, "text": " more relativistic scenario i think they also write down here some this is more complicated", "tokens": [50880, 544, 21960, 3142, 9005, 741, 519, 436, 611, 2464, 760, 510, 512, 341, 307, 544, 6179, 51148], "temperature": 0.0, "avg_logprob": -0.1505981513432094, "compression_ratio": 1.5889570552147239, "no_speech_prob": 0.0034796728286892176}, {"id": 407, "seek": 311516, "start": 3132.44, "end": 3142.8399999999997, "text": " correlation functions and examples okay so that's the one thing now if we talk", "tokens": [51228, 20009, 6828, 293, 5110, 1392, 370, 300, 311, 264, 472, 551, 586, 498, 321, 751, 51748], "temperature": 0.0, "avg_logprob": -0.1505981513432094, "compression_ratio": 1.5889570552147239, "no_speech_prob": 0.0034796728286892176}, {"id": 408, "seek": 314284, "start": 3143.56, "end": 3145.0, "text": " quantum field theory", "tokens": [50400, 13018, 2519, 5261, 50472], "temperature": 0.0, "avg_logprob": -0.2880227145026712, "compression_ratio": 1.326086956521739, "no_speech_prob": 0.0032176214735955}, {"id": 409, "seek": 314284, "start": 3151.08, "end": 3151.48, "text": " then", "tokens": [50776, 550, 50796], "temperature": 0.0, "avg_logprob": -0.2880227145026712, "compression_ratio": 1.326086956521739, "no_speech_prob": 0.0032176214735955}, {"id": 410, "seek": 314284, "start": 3158.76, "end": 3165.88, "text": " yeah the uh this is basically just a definition um the the g's we all also had that in the paper", "tokens": [51160, 1338, 264, 2232, 341, 307, 1936, 445, 257, 7123, 1105, 264, 264, 290, 311, 321, 439, 611, 632, 300, 294, 264, 3035, 51516], "temperature": 0.0, "avg_logprob": -0.2880227145026712, "compression_ratio": 1.326086956521739, "no_speech_prob": 0.0032176214735955}, {"id": 411, "seek": 316588, "start": 3166.52, "end": 3173.48, "text": " are certain expectation values right so there since we're on the pure relativistic side", "tokens": [50396, 366, 1629, 14334, 4190, 558, 370, 456, 1670, 321, 434, 322, 264, 6075, 21960, 3142, 1252, 50744], "temperature": 0.0, "avg_logprob": -0.1621872141391416, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.005375917535275221}, {"id": 412, "seek": 316588, "start": 3173.48, "end": 3179.0, "text": " and i'll take your clean side there's some time operator there but you can basically ignore it", "tokens": [50744, 293, 741, 603, 747, 428, 2541, 1252, 456, 311, 512, 565, 12973, 456, 457, 291, 393, 1936, 11200, 309, 51020], "temperature": 0.0, "avg_logprob": -0.1621872141391416, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.005375917535275221}, {"id": 413, "seek": 316588, "start": 3179.0, "end": 3184.92, "text": " for for the moment the the point is that these g's are determined by some fairly simple looking", "tokens": [51020, 337, 337, 264, 1623, 264, 264, 935, 307, 300, 613, 290, 311, 366, 9540, 538, 512, 6457, 2199, 1237, 51316], "temperature": 0.0, "avg_logprob": -0.1621872141391416, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.005375917535275221}, {"id": 414, "seek": 316588, "start": 3185.88, "end": 3190.6, "text": " expectation values and in the in the quantum mechanical formalisms these are also some", "tokens": [51364, 14334, 4190, 293, 294, 264, 294, 264, 13018, 12070, 9860, 13539, 613, 366, 611, 512, 51600], "temperature": 0.0, "avg_logprob": -0.1621872141391416, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.005375917535275221}, {"id": 415, "seek": 319060, "start": 3190.6, "end": 3195.4, "text": " inner products in some Hilbert space of course in a quantum field theory there is no super nice", "tokens": [50364, 7284, 3383, 294, 512, 19914, 4290, 1901, 295, 1164, 294, 257, 13018, 2519, 5261, 456, 307, 572, 1687, 1481, 50604], "temperature": 0.0, "avg_logprob": -0.12322329325848315, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003479472128674388}, {"id": 416, "seek": 319060, "start": 3195.4, "end": 3202.36, "text": " theory um about uh Hilbert spaces and and and these operators so there is it's more like a", "tokens": [50604, 5261, 1105, 466, 2232, 19914, 4290, 7673, 293, 293, 293, 613, 19077, 370, 456, 307, 309, 311, 544, 411, 257, 50952], "temperature": 0.0, "avg_logprob": -0.12322329325848315, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003479472128674388}, {"id": 417, "seek": 319060, "start": 3203.16, "end": 3209.0, "text": " like a functional definition this is what you calculate and um you hope it's somewhere", "tokens": [50992, 411, 257, 11745, 7123, 341, 307, 437, 291, 8873, 293, 1105, 291, 1454, 309, 311, 4079, 51284], "temperature": 0.0, "avg_logprob": -0.12322329325848315, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003479472128674388}, {"id": 418, "seek": 319060, "start": 3209.0, "end": 3213.72, "text": " else comes up with the sort of rigorous setting to compute this but nonetheless you know how to", "tokens": [51284, 1646, 1487, 493, 365, 264, 1333, 295, 29882, 3287, 281, 14722, 341, 457, 26756, 291, 458, 577, 281, 51520], "temperature": 0.0, "avg_logprob": -0.12322329325848315, "compression_ratio": 1.732394366197183, "no_speech_prob": 0.003479472128674388}, {"id": 419, "seek": 321372, "start": 3213.72, "end": 3223.3199999999997, "text": " compute these g's you know how to set up the an evolution equation of your choice and make", "tokens": [50364, 14722, 613, 290, 311, 291, 458, 577, 281, 992, 493, 264, 364, 9303, 5367, 295, 428, 3922, 293, 652, 50844], "temperature": 0.0, "avg_logprob": -0.122764952480793, "compression_ratio": 1.625, "no_speech_prob": 0.0014093247009441257}, {"id": 420, "seek": 321372, "start": 3223.3199999999997, "end": 3227.64, "text": " sense of these correlation functions which is um what you want to get at in the first step", "tokens": [50844, 2020, 295, 613, 20009, 6828, 597, 307, 1105, 437, 291, 528, 281, 483, 412, 294, 264, 700, 1823, 51060], "temperature": 0.0, "avg_logprob": -0.122764952480793, "compression_ratio": 1.625, "no_speech_prob": 0.0014093247009441257}, {"id": 421, "seek": 321372, "start": 3228.68, "end": 3237.56, "text": " so given um you have uh definition of these g's there is actually then a sort of generating", "tokens": [51112, 370, 2212, 1105, 291, 362, 2232, 7123, 295, 613, 290, 311, 456, 307, 767, 550, 257, 1333, 295, 17746, 51556], "temperature": 0.0, "avg_logprob": -0.122764952480793, "compression_ratio": 1.625, "no_speech_prob": 0.0014093247009441257}, {"id": 422, "seek": 323756, "start": 3237.56, "end": 3243.56, "text": " function in the in the um you know analysis generating function sense there are these partition", "tokens": [50364, 2445, 294, 264, 294, 264, 1105, 291, 458, 5215, 17746, 2445, 2020, 456, 366, 613, 24808, 50664], "temperature": 0.0, "avg_logprob": -0.10808514960018205, "compression_ratio": 1.875, "no_speech_prob": 0.005058353766798973}, {"id": 423, "seek": 323756, "start": 3243.56, "end": 3249.56, "text": " functions which look like so so you basically we can view it like starting from this we start", "tokens": [50664, 6828, 597, 574, 411, 370, 370, 291, 1936, 321, 393, 1910, 309, 411, 2891, 490, 341, 321, 722, 50964], "temperature": 0.0, "avg_logprob": -0.10808514960018205, "compression_ratio": 1.875, "no_speech_prob": 0.005058353766798973}, {"id": 424, "seek": 323756, "start": 3249.56, "end": 3256.92, "text": " with some some definition where all these g's are um captured uh in this sort of formal some", "tokens": [50964, 365, 512, 512, 7123, 689, 439, 613, 290, 311, 366, 1105, 11828, 2232, 294, 341, 1333, 295, 9860, 512, 51332], "temperature": 0.0, "avg_logprob": -0.10808514960018205, "compression_ratio": 1.875, "no_speech_prob": 0.005058353766798973}, {"id": 425, "seek": 323756, "start": 3256.92, "end": 3264.36, "text": " formal sum of products and then these g's are once you would have this uh partition function", "tokens": [51332, 9860, 2408, 295, 3383, 293, 550, 613, 290, 311, 366, 1564, 291, 576, 362, 341, 2232, 24808, 2445, 51704], "temperature": 0.0, "avg_logprob": -0.10808514960018205, "compression_ratio": 1.875, "no_speech_prob": 0.005058353766798973}, {"id": 426, "seek": 326436, "start": 3264.36, "end": 3270.36, "text": " with this j parameter you can in principle like formally speaking with functional derivatives", "tokens": [50364, 365, 341, 361, 13075, 291, 393, 294, 8665, 411, 25983, 4124, 365, 11745, 33733, 50664], "temperature": 0.0, "avg_logprob": -0.06352150881731952, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002081926679238677}, {"id": 427, "seek": 326436, "start": 3270.36, "end": 3274.6800000000003, "text": " compute this g from this thing and then there's this whole quantum field theory I mean this is", "tokens": [50664, 14722, 341, 290, 490, 341, 551, 293, 550, 456, 311, 341, 1379, 13018, 2519, 5261, 286, 914, 341, 307, 50880], "temperature": 0.0, "avg_logprob": -0.06352150881731952, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002081926679238677}, {"id": 428, "seek": 326436, "start": 3274.6800000000003, "end": 3281.2400000000002, "text": " what quantum field theory is all about that tells you how to compute this this set and this is", "tokens": [50880, 437, 13018, 2519, 5261, 307, 439, 466, 300, 5112, 291, 577, 281, 14722, 341, 341, 992, 293, 341, 307, 51208], "temperature": 0.0, "avg_logprob": -0.06352150881731952, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002081926679238677}, {"id": 429, "seek": 326436, "start": 3281.2400000000002, "end": 3288.52, "text": " then exactly this sort of object this is what we already had have had there right we already said", "tokens": [51208, 550, 2293, 341, 1333, 295, 2657, 341, 307, 437, 321, 1217, 632, 362, 632, 456, 558, 321, 1217, 848, 51572], "temperature": 0.0, "avg_logprob": -0.06352150881731952, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.002081926679238677}, {"id": 430, "seek": 328852, "start": 3288.52, "end": 3298.44, "text": " that um if you um take the expectation value of e to the minus s in the euclidean case is here", "tokens": [50364, 300, 1105, 498, 291, 1105, 747, 264, 14334, 2158, 295, 308, 281, 264, 3175, 262, 294, 264, 308, 1311, 31264, 282, 1389, 307, 510, 50860], "temperature": 0.0, "avg_logprob": -0.111341268243924, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.006001642905175686}, {"id": 431, "seek": 328852, "start": 3299.72, "end": 3306.7599999999998, "text": " of a product of these phi's you get all these these g's and this is basically sort of the", "tokens": [50924, 295, 257, 1674, 295, 613, 13107, 311, 291, 483, 439, 613, 613, 290, 311, 293, 341, 307, 1936, 1333, 295, 264, 51276], "temperature": 0.0, "avg_logprob": -0.111341268243924, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.006001642905175686}, {"id": 432, "seek": 328852, "start": 3306.7599999999998, "end": 3314.7599999999998, "text": " generating function trick to get to this to these sort of objects okay um okay so this is", "tokens": [51276, 17746, 2445, 4282, 281, 483, 281, 341, 281, 613, 1333, 295, 6565, 1392, 1105, 1392, 370, 341, 307, 51676], "temperature": 0.0, "avg_logprob": -0.111341268243924, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.006001642905175686}, {"id": 433, "seek": 331476, "start": 3314.76, "end": 3321.0800000000004, "text": " what you do in practice and just to motivate also that how um neural network sampling would", "tokens": [50364, 437, 291, 360, 294, 3124, 293, 445, 281, 28497, 611, 300, 577, 1105, 18161, 3209, 21179, 576, 50680], "temperature": 0.0, "avg_logprob": -0.14164936111634036, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0027994082774966955}, {"id": 434, "seek": 331476, "start": 3321.0800000000004, "end": 3325.7200000000003, "text": " be done another way of getting at this these g functions which you want to to have okay and", "tokens": [50680, 312, 1096, 1071, 636, 295, 1242, 412, 341, 613, 290, 6828, 597, 291, 528, 281, 281, 362, 1392, 293, 50912], "temperature": 0.0, "avg_logprob": -0.14164936111634036, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0027994082774966955}, {"id": 435, "seek": 331476, "start": 3325.7200000000003, "end": 3332.76, "text": " and so to round it up to see to to explain how this this simple g's this field theory um", "tokens": [50912, 293, 370, 281, 3098, 309, 493, 281, 536, 281, 281, 2903, 577, 341, 341, 2199, 290, 311, 341, 2519, 5261, 1105, 51264], "temperature": 0.0, "avg_logprob": -0.14164936111634036, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0027994082774966955}, {"id": 436, "seek": 331476, "start": 3334.76, "end": 3343.5600000000004, "text": " correlation functions relate to actually um to actual observables right you want to have this", "tokens": [51364, 20009, 6828, 10961, 281, 767, 1105, 281, 3539, 9951, 2965, 558, 291, 528, 281, 362, 341, 51804], "temperature": 0.0, "avg_logprob": -0.14164936111634036, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.0027994082774966955}, {"id": 437, "seek": 334356, "start": 3343.56, "end": 3348.7599999999998, "text": " sort of transition functions as I discussed them for the quantum mechanics case there is then", "tokens": [50364, 1333, 295, 6034, 6828, 382, 286, 7152, 552, 337, 264, 13018, 12939, 1389, 456, 307, 550, 50624], "temperature": 0.0, "avg_logprob": -0.14812768697738649, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.0006163828074932098}, {"id": 438, "seek": 334356, "start": 3351.0, "end": 3356.84, "text": " complicated theory of how you take these states how they how you encode these states with some", "tokens": [50736, 6179, 5261, 295, 577, 291, 747, 613, 4368, 577, 436, 577, 291, 2058, 1429, 613, 4368, 365, 512, 51028], "temperature": 0.0, "avg_logprob": -0.14812768697738649, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.0006163828074932098}, {"id": 439, "seek": 334356, "start": 3356.84, "end": 3362.36, "text": " fixed momentum in the in the quantum field theory you know they also live in some supposed", "tokens": [51028, 6806, 11244, 294, 264, 294, 264, 13018, 2519, 5261, 291, 458, 436, 611, 1621, 294, 512, 3442, 51304], "temperature": 0.0, "avg_logprob": -0.14812768697738649, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.0006163828074932098}, {"id": 440, "seek": 334356, "start": 3363.48, "end": 3372.36, "text": " Hilbert space let's say you have to compute a lot in in momentum space which by Fourier", "tokens": [51360, 19914, 4290, 1901, 718, 311, 584, 291, 362, 281, 14722, 257, 688, 294, 294, 11244, 1901, 597, 538, 36810, 51804], "temperature": 0.0, "avg_logprob": -0.14812768697738649, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.0006163828074932098}, {"id": 441, "seek": 337236, "start": 3372.36, "end": 3378.28, "text": " transform is sort of the dual to the the position space that we talked about in the whole video", "tokens": [50364, 4088, 307, 1333, 295, 264, 11848, 281, 264, 264, 2535, 1901, 300, 321, 2825, 466, 294, 264, 1379, 960, 50660], "temperature": 0.0, "avg_logprob": -0.09978946050008138, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0015723753022029996}, {"id": 442, "seek": 337236, "start": 3378.28, "end": 3385.2400000000002, "text": " so there are some Fourier transforms involved and you pass from the phi's to this a's so", "tokens": [50660, 370, 456, 366, 512, 36810, 35592, 3288, 293, 291, 1320, 490, 264, 13107, 311, 281, 341, 257, 311, 370, 51008], "temperature": 0.0, "avg_logprob": -0.09978946050008138, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0015723753022029996}, {"id": 443, "seek": 337236, "start": 3386.2000000000003, "end": 3389.8, "text": " just to sketch this out here um", "tokens": [51056, 445, 281, 12325, 341, 484, 510, 1105, 51236], "temperature": 0.0, "avg_logprob": -0.09978946050008138, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0015723753022029996}, {"id": 444, "seek": 337236, "start": 3394.1200000000003, "end": 3399.56, "text": " yeah so you have when these are I'm just throwing these formulas at you right but", "tokens": [51452, 1338, 370, 291, 362, 562, 613, 366, 286, 478, 445, 10238, 613, 30546, 412, 291, 558, 457, 51724], "temperature": 0.0, "avg_logprob": -0.09978946050008138, "compression_ratio": 1.6464088397790055, "no_speech_prob": 0.0015723753022029996}, {"id": 445, "seek": 339956, "start": 3399.56, "end": 3407.48, "text": " there's this typical relations between um the momentum space here ap and the the", "tokens": [50364, 456, 311, 341, 7476, 2299, 1296, 1105, 264, 11244, 1901, 510, 1882, 293, 264, 264, 50760], "temperature": 0.0, "avg_logprob": -0.21215331763551945, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0017522667767480016}, {"id": 446, "seek": 339956, "start": 3409.32, "end": 3416.2799999999997, "text": " spacetime fields and you can view this as a transition link by Fourier transform but", "tokens": [50852, 39404, 9764, 7909, 293, 291, 393, 1910, 341, 382, 257, 6034, 2113, 538, 36810, 4088, 457, 51200], "temperature": 0.0, "avg_logprob": -0.21215331763551945, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0017522667767480016}, {"id": 447, "seek": 339956, "start": 3417.16, "end": 3423.16, "text": " Fourier transform is with respect to space and also like you know momentum on space and also time", "tokens": [51244, 36810, 4088, 307, 365, 3104, 281, 1901, 293, 611, 411, 291, 458, 11244, 322, 1901, 293, 611, 565, 51544], "temperature": 0.0, "avg_logprob": -0.21215331763551945, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0017522667767480016}, {"id": 448, "seek": 342316, "start": 3423.3199999999997, "end": 3430.92, "text": " and energy and the energy encodes the sort of relation that is governed by the", "tokens": [50372, 293, 2281, 293, 264, 2281, 2058, 4789, 264, 1333, 295, 9721, 300, 307, 35529, 538, 264, 50752], "temperature": 0.0, "avg_logprob": -0.06980516770306755, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.005908530205488205}, {"id": 449, "seek": 342316, "start": 3432.92, "end": 3439.0, "text": " Schrodinger equation relation and so all these things are fairly like complex but in the end", "tokens": [50852, 2065, 340, 3584, 260, 5367, 9721, 293, 370, 439, 613, 721, 366, 6457, 411, 3997, 457, 294, 264, 917, 51156], "temperature": 0.0, "avg_logprob": -0.06980516770306755, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.005908530205488205}, {"id": 450, "seek": 342316, "start": 3439.0, "end": 3445.0, "text": " it's clear that once we have these g's we have to sort of Fourier transform them over to a momentum", "tokens": [51156, 309, 311, 1850, 300, 1564, 321, 362, 613, 290, 311, 321, 362, 281, 1333, 295, 36810, 4088, 552, 670, 281, 257, 11244, 51456], "temperature": 0.0, "avg_logprob": -0.06980516770306755, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.005908530205488205}, {"id": 451, "seek": 342316, "start": 3445.0, "end": 3450.52, "text": " space and in momentum space we encode actually the input output state that we are actually interested", "tokens": [51456, 1901, 293, 294, 11244, 1901, 321, 2058, 1429, 767, 264, 4846, 5598, 1785, 300, 321, 366, 767, 3102, 51732], "temperature": 0.0, "avg_logprob": -0.06980516770306755, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.005908530205488205}, {"id": 452, "seek": 345052, "start": 3450.52, "end": 3462.84, "text": " in like this for some scattering transition probability stuff okay and then there is some more", "tokens": [50364, 294, 411, 341, 337, 512, 42314, 6034, 8482, 1507, 1392, 293, 550, 456, 307, 512, 544, 50980], "temperature": 0.0, "avg_logprob": -0.1470232166227747, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0006357799284160137}, {"id": 453, "seek": 345052, "start": 3462.84, "end": 3469.72, "text": " theory that gives you this momentum space in out transition amplitudes that you're actually", "tokens": [50980, 5261, 300, 2709, 291, 341, 11244, 1901, 294, 484, 6034, 9731, 16451, 300, 291, 434, 767, 51324], "temperature": 0.0, "avg_logprob": -0.1470232166227747, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0006357799284160137}, {"id": 454, "seek": 345052, "start": 3469.72, "end": 3476.84, "text": " interested in in terms of these these g's basically you see on this side here these are basically g's", "tokens": [51324, 3102, 294, 294, 2115, 295, 613, 613, 290, 311, 1936, 291, 536, 322, 341, 1252, 510, 613, 366, 1936, 290, 311, 51680], "temperature": 0.0, "avg_logprob": -0.1470232166227747, "compression_ratio": 1.6941176470588235, "no_speech_prob": 0.0006357799284160137}, {"id": 455, "seek": 347684, "start": 3476.84, "end": 3483.8, "text": " and then from Fourier transforming and taking into account how the the frequency is and space is", "tokens": [50364, 293, 550, 490, 36810, 27210, 293, 1940, 666, 2696, 577, 264, 264, 7893, 307, 293, 1901, 307, 50712], "temperature": 0.0, "avg_logprob": -0.09051229953765869, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00046527438098564744}, {"id": 456, "seek": 347684, "start": 3483.8, "end": 3489.08, "text": " sort of tied together through the evolution equations you get some nasty object here that you", "tokens": [50712, 1333, 295, 9601, 1214, 807, 264, 9303, 11787, 291, 483, 512, 17923, 2657, 510, 300, 291, 50976], "temperature": 0.0, "avg_logprob": -0.09051229953765869, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00046527438098564744}, {"id": 457, "seek": 347684, "start": 3489.08, "end": 3498.92, "text": " have to then compute and then with this this this formalism you get from the green's functions to", "tokens": [50976, 362, 281, 550, 14722, 293, 550, 365, 341, 341, 341, 9860, 1434, 291, 483, 490, 264, 3092, 311, 6828, 281, 51468], "temperature": 0.0, "avg_logprob": -0.09051229953765869, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00046527438098564744}, {"id": 458, "seek": 347684, "start": 3498.92, "end": 3504.92, "text": " the transition probability right so I in these last 10 minutes yeah I just explained to you", "tokens": [51468, 264, 6034, 8482, 558, 370, 286, 294, 613, 1036, 1266, 2077, 1338, 286, 445, 8825, 281, 291, 51768], "temperature": 0.0, "avg_logprob": -0.09051229953765869, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00046527438098564744}, {"id": 459, "seek": 350492, "start": 3504.92, "end": 3508.84, "text": " why these g's are really the important thing that you want to get that once you have them", "tokens": [50364, 983, 613, 290, 311, 366, 534, 264, 1021, 551, 300, 291, 528, 281, 483, 300, 1564, 291, 362, 552, 50560], "temperature": 0.0, "avg_logprob": -0.08155021667480469, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.0008278457098640501}, {"id": 460, "seek": 350492, "start": 3508.84, "end": 3517.8, "text": " then in principle you can compute this transition probabilities and the neural network aspect", "tokens": [50560, 550, 294, 8665, 291, 393, 14722, 341, 6034, 33783, 293, 264, 18161, 3209, 4171, 51008], "temperature": 0.0, "avg_logprob": -0.08155021667480469, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.0008278457098640501}, {"id": 461, "seek": 350492, "start": 3517.8, "end": 3526.84, "text": " gives you a way to get at these g's okay so lastly I want also to mention that", "tokens": [51008, 2709, 291, 257, 636, 281, 483, 412, 613, 290, 311, 1392, 370, 16386, 286, 528, 611, 281, 2152, 300, 51460], "temperature": 0.0, "avg_logprob": -0.08155021667480469, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.0008278457098640501}, {"id": 462, "seek": 352684, "start": 3527.1600000000003, "end": 3536.36, "text": " apart from the neural network kernel theory stuff there's also the you know classical", "tokens": [50380, 4936, 490, 264, 18161, 3209, 28256, 5261, 1507, 456, 311, 611, 264, 291, 458, 13735, 50840], "temperature": 0.0, "avg_logprob": -0.1175928204147904, "compression_ratio": 1.6158536585365855, "no_speech_prob": 0.004903757944703102}, {"id": 463, "seek": 352684, "start": 3537.32, "end": 3545.8, "text": " information geometry approach where you you know you have the underlying parameters theta", "tokens": [50888, 1589, 18426, 3109, 689, 291, 291, 458, 291, 362, 264, 14217, 9834, 9725, 51312], "temperature": 0.0, "avg_logprob": -0.1175928204147904, "compression_ratio": 1.6158536585365855, "no_speech_prob": 0.004903757944703102}, {"id": 464, "seek": 352684, "start": 3545.8, "end": 3555.2400000000002, "text": " which are the weights and you can see them as encoding if you put a distribution over the", "tokens": [51312, 597, 366, 264, 17443, 293, 291, 393, 536, 552, 382, 43430, 498, 291, 829, 257, 7316, 670, 264, 51784], "temperature": 0.0, "avg_logprob": -0.1175928204147904, "compression_ratio": 1.6158536585365855, "no_speech_prob": 0.004903757944703102}, {"id": 465, "seek": 355524, "start": 3555.3199999999997, "end": 3559.56, "text": " inputs and you get the distribution over the outputs which are governed by the parameters", "tokens": [50368, 15743, 293, 291, 483, 264, 7316, 670, 264, 23930, 597, 366, 35529, 538, 264, 9834, 50580], "temperature": 0.0, "avg_logprob": -0.10459589428371853, "compression_ratio": 1.852, "no_speech_prob": 0.011683033779263496}, {"id": 466, "seek": 355524, "start": 3559.56, "end": 3565.0, "text": " then you get this information manifold so you heard that information geometry stuff this is", "tokens": [50580, 550, 291, 483, 341, 1589, 47138, 370, 291, 2198, 300, 1589, 18426, 1507, 341, 307, 50852], "temperature": 0.0, "avg_logprob": -0.10459589428371853, "compression_ratio": 1.852, "no_speech_prob": 0.011683033779263496}, {"id": 467, "seek": 355524, "start": 3565.0, "end": 3572.04, "text": " like a related but different mathematical angle that I just wanted to mention because we are like", "tokens": [50852, 411, 257, 4077, 457, 819, 18894, 5802, 300, 286, 445, 1415, 281, 2152, 570, 321, 366, 411, 51204], "temperature": 0.0, "avg_logprob": -0.10459589428371853, "compression_ratio": 1.852, "no_speech_prob": 0.011683033779263496}, {"id": 468, "seek": 355524, "start": 3572.04, "end": 3576.7599999999998, "text": " formally so close having all the important all these stochastics already but this is a little", "tokens": [51204, 25983, 370, 1998, 1419, 439, 264, 1021, 439, 613, 342, 8997, 21598, 1217, 457, 341, 307, 257, 707, 51440], "temperature": 0.0, "avg_logprob": -0.10459589428371853, "compression_ratio": 1.852, "no_speech_prob": 0.011683033779263496}, {"id": 469, "seek": 355524, "start": 3576.7599999999998, "end": 3582.3599999999997, "text": " bit different nonetheless but also interesting there you have this neural neural manifold", "tokens": [51440, 857, 819, 26756, 457, 611, 1880, 456, 291, 362, 341, 18161, 18161, 47138, 51720], "temperature": 0.0, "avg_logprob": -0.10459589428371853, "compression_ratio": 1.852, "no_speech_prob": 0.011683033779263496}, {"id": 470, "seek": 358236, "start": 3582.36, "end": 3589.48, "text": " thing so this is also one line of research I actually try to find things going into the", "tokens": [50364, 551, 370, 341, 307, 611, 472, 1622, 295, 2132, 286, 767, 853, 281, 915, 721, 516, 666, 264, 50720], "temperature": 0.0, "avg_logprob": -0.12497717374330991, "compression_ratio": 1.8, "no_speech_prob": 0.009697054512798786}, {"id": 471, "seek": 358236, "start": 3589.48, "end": 3595.4, "text": " this quantum field theory research direction in in deep learning books in preparation for this video", "tokens": [50720, 341, 13018, 2519, 5261, 2132, 3513, 294, 294, 2452, 2539, 3642, 294, 13081, 337, 341, 960, 51016], "temperature": 0.0, "avg_logprob": -0.12497717374330991, "compression_ratio": 1.8, "no_speech_prob": 0.009697054512798786}, {"id": 472, "seek": 358236, "start": 3596.44, "end": 3602.04, "text": " and actually didn't find too much so there was more about this classical information geometry", "tokens": [51068, 293, 767, 994, 380, 915, 886, 709, 370, 456, 390, 544, 466, 341, 13735, 1589, 18426, 51348], "temperature": 0.0, "avg_logprob": -0.12497717374330991, "compression_ratio": 1.8, "no_speech_prob": 0.009697054512798786}, {"id": 473, "seek": 358236, "start": 3602.04, "end": 3611.08, "text": " aspect of things and then also worth noting the applications to quantum field theory is not the", "tokens": [51348, 4171, 295, 721, 293, 550, 611, 3163, 26801, 264, 5821, 281, 13018, 2519, 5261, 307, 406, 264, 51800], "temperature": 0.0, "avg_logprob": -0.12497717374330991, "compression_ratio": 1.8, "no_speech_prob": 0.009697054512798786}, {"id": 474, "seek": 361108, "start": 3611.08, "end": 3618.7599999999998, "text": " only way in which people try to apply neural network theory and all these nice formulas to", "tokens": [50364, 787, 636, 294, 597, 561, 853, 281, 3079, 18161, 3209, 5261, 293, 439, 613, 1481, 30546, 281, 50748], "temperature": 0.0, "avg_logprob": -0.0639509138513784, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0010001480113714933}, {"id": 475, "seek": 361108, "start": 3618.7599999999998, "end": 3629.08, "text": " physics there's also this mathematical metric flow things going on I just mentioned it because if", "tokens": [50748, 10649, 456, 311, 611, 341, 18894, 20678, 3095, 721, 516, 322, 286, 445, 2835, 309, 570, 498, 51264], "temperature": 0.0, "avg_logprob": -0.0639509138513784, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0010001480113714933}, {"id": 476, "seek": 361108, "start": 3629.08, "end": 3638.44, "text": " you look at for example the research groups that put out this paper then you will also find these", "tokens": [51264, 291, 574, 412, 337, 1365, 264, 2132, 3935, 300, 829, 484, 341, 3035, 550, 291, 486, 611, 915, 613, 51732], "temperature": 0.0, "avg_logprob": -0.0639509138513784, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0010001480113714933}, {"id": 477, "seek": 363844, "start": 3638.44, "end": 3646.68, "text": " sort of neural network applications using quantum field theories okay so I mean at one hour I know", "tokens": [50364, 1333, 295, 18161, 3209, 5821, 1228, 13018, 2519, 13667, 1392, 370, 286, 914, 412, 472, 1773, 286, 458, 50776], "temperature": 0.0, "avg_logprob": -0.11302064413047698, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.005133016966283321}, {"id": 478, "seek": 363844, "start": 3646.68, "end": 3652.36, "text": " it has been a little bit fuzzy but nonetheless I think I at least maybe you understood the neural", "tokens": [50776, 309, 575, 668, 257, 707, 857, 34710, 457, 26756, 286, 519, 286, 412, 1935, 1310, 291, 7320, 264, 18161, 51060], "temperature": 0.0, "avg_logprob": -0.11302064413047698, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.005133016966283321}, {"id": 479, "seek": 363844, "start": 3652.36, "end": 3658.44, "text": " network Gaussian process stuff and and can see how this ties in with path integral formalisms", "tokens": [51060, 3209, 39148, 1399, 1507, 293, 293, 393, 536, 577, 341, 14039, 294, 365, 3100, 11573, 9860, 13539, 51364], "temperature": 0.0, "avg_logprob": -0.11302064413047698, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.005133016966283321}, {"id": 480, "seek": 363844, "start": 3659.96, "end": 3665.56, "text": " and I motivated you I'm really I know very few people will make it through a one hour", "tokens": [51440, 293, 286, 14515, 291, 286, 478, 534, 286, 458, 588, 1326, 561, 486, 652, 309, 807, 257, 472, 1773, 51720], "temperature": 0.0, "avg_logprob": -0.11302064413047698, "compression_ratio": 1.613733905579399, "no_speech_prob": 0.005133016966283321}, {"id": 481, "seek": 366556, "start": 3665.64, "end": 3672.92, "text": " video of these sort of friends but it's probably nonetheless I think the best way to get an infusion", "tokens": [50368, 960, 295, 613, 1333, 295, 1855, 457, 309, 311, 1391, 26756, 286, 519, 264, 1151, 636, 281, 483, 364, 1536, 5704, 50732], "temperature": 0.0, "avg_logprob": -0.06527302057846733, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0040682218968868256}, {"id": 482, "seek": 366556, "start": 3672.92, "end": 3679.32, "text": " of this sort of theory in a digestible way and in a way where somebody highlights these things so", "tokens": [50732, 295, 341, 1333, 295, 5261, 294, 257, 13884, 964, 636, 293, 294, 257, 636, 689, 2618, 14254, 613, 721, 370, 51052], "temperature": 0.0, "avg_logprob": -0.06527302057846733, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0040682218968868256}, {"id": 483, "seek": 366556, "start": 3679.32, "end": 3683.88, "text": " this is it's not a real excuse I think it's nonetheless helpful even if this was not very", "tokens": [51052, 341, 307, 309, 311, 406, 257, 957, 8960, 286, 519, 309, 311, 26756, 4961, 754, 498, 341, 390, 406, 588, 51280], "temperature": 0.0, "avg_logprob": -0.06527302057846733, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0040682218968868256}, {"id": 484, "seek": 366556, "start": 3683.88, "end": 3694.36, "text": " super prepared okay with that I leave it I leave it at that I wish you a good transition into the", "tokens": [51280, 1687, 4927, 1392, 365, 300, 286, 1856, 309, 286, 1856, 309, 412, 300, 286, 3172, 291, 257, 665, 6034, 666, 264, 51804], "temperature": 0.0, "avg_logprob": -0.06527302057846733, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.0040682218968868256}, {"id": 485, "seek": 369436, "start": 3694.36, "end": 3702.6800000000003, "text": " next year I have no real videos planned for the upcoming months really I mean I have a folder with", "tokens": [50364, 958, 1064, 286, 362, 572, 957, 2145, 8589, 337, 264, 11500, 2493, 534, 286, 914, 286, 362, 257, 10820, 365, 50780], "temperature": 0.0, "avg_logprob": -0.06464304068149665, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005999007262289524}, {"id": 486, "seek": 369436, "start": 3702.6800000000003, "end": 3710.28, "text": " 20 started projects and videos I could talk about that I might come back to probably not", "tokens": [50780, 945, 1409, 4455, 293, 2145, 286, 727, 751, 466, 300, 286, 1062, 808, 646, 281, 1391, 406, 51160], "temperature": 0.0, "avg_logprob": -0.06464304068149665, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005999007262289524}, {"id": 487, "seek": 369436, "start": 3710.28, "end": 3717.7200000000003, "text": " within the next months but then as I said I might look more into classical classical", "tokens": [51160, 1951, 264, 958, 2493, 457, 550, 382, 286, 848, 286, 1062, 574, 544, 666, 13735, 13735, 51532], "temperature": 0.0, "avg_logprob": -0.06464304068149665, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005999007262289524}, {"id": 488, "seek": 369436, "start": 3717.7200000000003, "end": 3722.52, "text": " functional analysis stuff for the sake of making a nice video about the universal approximation", "tokens": [51532, 11745, 5215, 1507, 337, 264, 9717, 295, 1455, 257, 1481, 960, 466, 264, 11455, 28023, 51772], "temperature": 0.0, "avg_logprob": -0.06464304068149665, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.005999007262289524}, {"id": 489, "seek": 372252, "start": 3722.52, "end": 3734.36, "text": " theorem and at the latest then I will have a polished video take care", "tokens": [50364, 20904, 293, 412, 264, 6792, 550, 286, 486, 362, 257, 29079, 960, 747, 1127, 50956], "temperature": 0.0, "avg_logprob": -0.2734339377459358, "compression_ratio": 1.0615384615384615, "no_speech_prob": 0.013419749215245247}], "language": "en"}