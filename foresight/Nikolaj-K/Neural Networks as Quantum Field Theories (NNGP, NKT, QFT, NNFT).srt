1
00:00:00,000 --> 00:00:07,760
Hello boys and girls. In this video I want to talk about the line of research that I

2
00:00:07,760 --> 00:00:17,600
only came across last week, namely the very strong connection between quantum fields and

3
00:00:17,600 --> 00:00:27,800
neural network theory. And what emerges from that is tools from one field being applicable

4
00:00:27,800 --> 00:00:34,520
to the other. So you can then use let's say Feynman diagrams to find stuff out about

5
00:00:34,520 --> 00:00:40,960
neural networks. And in the other way around you can use neural networks to compute stuff

6
00:00:40,960 --> 00:00:49,280
for quantum fields. I think the most senior researcher, if I'm not mistaken from this

7
00:00:49,280 --> 00:00:58,200
list of this paper in particular is James Harvorsson. And if you look it up there have

8
00:00:58,200 --> 00:01:08,160
been a few papers in this direction in the last years. And nonetheless I chose this particular

9
00:01:08,160 --> 00:01:16,800
paper because it's clearly some of the more advanced results there. And to motivate how

10
00:01:16,800 --> 00:01:26,280
things work, I'm also in this video going to explain some results which were known for longer

11
00:01:26,280 --> 00:01:33,280
time. So there I suppose it's fair to say that this sort of stochastic result for large neural

12
00:01:33,280 --> 00:01:40,840
networks that we are going to discuss emerge in the mid 90s. And I'm going to explain this because

13
00:01:41,800 --> 00:01:52,680
I want these subjects which I find very exciting actually to be known also a little better. In

14
00:01:52,680 --> 00:01:58,600
this video however I will not go into any deep mathematical analysis. I have also not written

15
00:01:58,600 --> 00:02:09,080
down much. I will basically jump from tab to tab. And nonetheless give a sensible explanation of

16
00:02:09,080 --> 00:02:14,200
these things. So I think everything that I say should make sense in principle. And then you can

17
00:02:14,200 --> 00:02:20,520
delve into all the subjects on your own. If you're actually interested in doing something in this

18
00:02:20,520 --> 00:02:29,800
direction let me know in the comments. I will also point to some Google libraries regarding neural

19
00:02:30,040 --> 00:02:37,880
kernel theory that I will also sketch out in this video. And yeah so that's that.

20
00:02:39,320 --> 00:02:44,360
The requirements for the video are that you have like a basic understanding of deep neural networks

21
00:02:44,360 --> 00:02:52,520
like the fact that these neural networks encode the parameterization of certain functions. And then

22
00:02:52,520 --> 00:02:57,000
if you have a big enough network stuff like universal approximation theorem the fact that

23
00:02:57,000 --> 00:03:04,520
you basically can represent functions in the space let's say continuous functions densely.

24
00:03:05,080 --> 00:03:12,600
And it helps if you have a basic understanding about the ideas of quantum mechanics. So the

25
00:03:12,600 --> 00:03:20,120
fact that what you're interested in is transition probabilities and that they are expressed as some

26
00:03:20,120 --> 00:03:30,920
products in a Hilbert space. In this video I have a section where I motivate the jump from the

27
00:03:30,920 --> 00:03:37,720
expressions that we are going to be able to compute with neural networks as they are explained in

28
00:03:37,720 --> 00:03:46,200
this paper. Let me scroll down a second. How these expressions connect to scattering amplitudes in

29
00:03:46,200 --> 00:03:54,440
let's say some large hydrogen collider experiments. These sort of stuff so I have a small like wake

30
00:03:56,280 --> 00:04:03,800
mafia section that concerns physics. But my main goal is that if you go away from this video

31
00:04:03,800 --> 00:04:09,240
and have a vague understanding why these sort of expressions that you see on the screen right now

32
00:04:09,240 --> 00:04:16,120
are both relevant for neural network theory and for quantum field theory then my job is done.

33
00:04:16,840 --> 00:04:24,040
As I said this gives the possibility not just to compute stuff in physics but for example

34
00:04:24,040 --> 00:04:33,880
you might then be able to apply Feynman diagram calculus to compute various aspects of neural

35
00:04:33,880 --> 00:04:39,560
networks as well and so even if you edit from a purely computer science perspective

36
00:04:40,440 --> 00:04:46,040
and have some statistics background then this might be fairly helpful.

37
00:04:47,320 --> 00:04:55,800
Okay so I have here just a bunch of bullet points that I want to go through. I might come back to

38
00:04:56,600 --> 00:05:03,800
this from time to time otherwise I will just jump through like here 15 tabs or so and explain some

39
00:05:03,960 --> 00:05:13,080
results. I am actually currently working on or started working on a video that I maybe want to

40
00:05:13,800 --> 00:05:23,240
make as my some for summer video where I will do like a painful analysis of the universal

41
00:05:23,240 --> 00:05:28,440
approximation theorem. But that's like months and months out. In this video I'm basically just

42
00:05:28,440 --> 00:05:38,280
rambling. I hope you don't expect to neither deep or concise elaboration so I'm warning you already

43
00:05:38,280 --> 00:05:49,240
but nonetheless I would really recommend that you listen up. Okay so first off as you have just

44
00:05:49,240 --> 00:05:59,960
seen in the bullet points I will explain to you like sketch out this result from the 90s which

45
00:05:59,960 --> 00:06:06,680
concerns neural network Gaussian processes. So there are nice results that have been found there

46
00:06:06,680 --> 00:06:13,480
and have since been extended to a broad range of neural network architectures. So this is

47
00:06:13,560 --> 00:06:20,200
mathematical theory results for neural networks that hold in general. In this video for simplicity

48
00:06:20,200 --> 00:06:27,240
we can just look at fully connected in the sense that you see in the screen here feed

49
00:06:28,040 --> 00:06:35,320
forward neural networks and for this video it's not even super relevant how many inputs

50
00:06:35,320 --> 00:06:41,400
outputs you have. Basically you have let's say at least one input some float or if you want a real

51
00:06:41,400 --> 00:06:49,080
number x that goes in one real number y that goes out and in the middle you have a bunch of

52
00:06:49,080 --> 00:06:54,920
hidden layers in the image you just see one but you know for the sake of it you might think of two

53
00:06:54,920 --> 00:07:04,520
three four something like that and you see the nodes in this one hidden layer here and the

54
00:07:04,520 --> 00:07:10,520
theory that we are going to discuss kicks in once you have a really big network. So this

55
00:07:11,880 --> 00:07:19,320
you think of the number of nodes in each hidden layer here going to infinity or you know it will

56
00:07:19,320 --> 00:07:27,160
suffice if you think of a huge number a bunch of billions of billions and the thing that then

57
00:07:27,160 --> 00:07:35,000
emerges with large networks is not just the universal approximation theorem that says

58
00:07:35,560 --> 00:07:41,560
the the nice functions let's say continuous functions from r to r are represented densely

59
00:07:41,560 --> 00:07:51,800
by this sort of neural network by these weights but what also emerges in this large network limit

60
00:07:51,800 --> 00:08:05,400
is that the dependency of the output for fixed input and probabilistic weights

61
00:08:06,920 --> 00:08:12,520
takes on a very deterministic character okay so this is this neural network Gaussian process

62
00:08:12,520 --> 00:08:18,280
phenomenon and then I will explain in a second in more detail but basically what we want to

63
00:08:18,280 --> 00:08:24,520
hear first look at is we take one fixed architecture some big neural network with let's say three

64
00:08:24,520 --> 00:08:32,120
hidden layers and all the layers are very large and what we're here are first concerned with

65
00:08:32,120 --> 00:08:37,640
before we talk about tangent kernels before we talk about quantum field theory is we few

66
00:08:39,400 --> 00:08:45,000
we're concerned with the random initialization of these networks right so let's say you are on a

67
00:08:45,000 --> 00:08:56,520
computer you you have this network encoded on your GPU or whatever and or then the weights really

68
00:08:56,520 --> 00:09:04,760
I mean the you have the architecture laid out the way in which all the the the float data

69
00:09:05,720 --> 00:09:14,680
pass to each other naturally if you have this this float types in every realized configuration

70
00:09:14,760 --> 00:09:22,600
the weights have to have some some value and this gives the start configuration for the

71
00:09:22,600 --> 00:09:28,600
learning process right in the learning process you're going to probably assign some some

72
00:09:29,400 --> 00:09:34,840
loss function and you do gradient descent and then you tweak the network to behave in a certain

73
00:09:34,840 --> 00:09:42,120
nice way and fulfill some task but to start this process you need to initialize the network you

74
00:09:42,280 --> 00:09:49,320
want to maybe explicitly set some weights okay and now what you can do is you can play around with

75
00:09:49,320 --> 00:09:55,320
what is actually your starting condition right you can say hey shoot all the weights in the beginning

76
00:09:55,320 --> 00:10:03,480
be set to zero or be set to one or and this is the interesting thing here you do a random

77
00:10:03,480 --> 00:10:12,280
initialization of all the weights and biases also so think of you know you're in python you

78
00:10:12,840 --> 00:10:17,960
take a library and you sample from a normal distribution for all the

79
00:10:19,720 --> 00:10:26,440
trillion weights you sample trillion random numbers and you sample them each from a Gaussian

80
00:10:27,240 --> 00:10:36,040
from a bell curve and then set the corresponding weights like this okay so all these w i j are

81
00:10:36,040 --> 00:10:45,080
sampled from some from some Gaussian and when you like put in some input right we said there's one

82
00:10:45,080 --> 00:10:55,800
input let's say you you take the input seven and set x to seven and then feed do the feed forward

83
00:10:55,800 --> 00:11:03,240
process the evaluation of the neural network then if these things are random then the output will

84
00:11:03,240 --> 00:11:09,400
also be some essentially random number it will be determined for whatever random weight you have

85
00:11:09,400 --> 00:11:18,600
sampled here and in this way you can think of y for fixed x as a random variable composed of the

86
00:11:18,600 --> 00:11:28,760
random variables w right so we have here's the neural network Gaussian process page the math

87
00:11:28,760 --> 00:11:34,600
and the proof sketch of this result that we are going to get to is actually described there so the

88
00:11:34,600 --> 00:11:41,400
weights are sampled from some Gaussian we are going to take a Gaussian where the standard deviation

89
00:11:42,280 --> 00:11:47,080
gets tighter and tighter with the number of layers right but if you have a fixed network this is some

90
00:11:47,080 --> 00:11:58,360
fixed variance here and the output that is in a standard way computed from neural network is

91
00:11:59,080 --> 00:12:08,920
um computed as you see here you do fast forward and I think it's fairly easy to believe that just

92
00:12:08,920 --> 00:12:14,440
due to the central limit theorem right the statement that if you have a bunch of independent

93
00:12:16,280 --> 00:12:23,480
random variables if you sum them all up then this is another random variable that will behave

94
00:12:23,480 --> 00:12:30,040
like a Gaussian process right so basically if you sum up random numbers then you usually end up with

95
00:12:31,720 --> 00:12:35,240
if all the conditions are fulfilled all the mathematical conditions then you will end up

96
00:12:35,240 --> 00:12:39,480
with a Gaussian this is the statement of the central central limit theorem and this exact

97
00:12:39,480 --> 00:12:44,120
thing applies here also you know maybe there's some non-linearities involved and maybe there's

98
00:12:44,120 --> 00:12:51,240
different steps but in the end the final output of the network here in this picture in the on the

99
00:12:51,240 --> 00:12:58,840
last layer this set is still a function of all these small Gaussians and because there are so many

100
00:12:58,840 --> 00:13:07,240
sums this is again just a Gaussian process right so and so this says that in the limit and this

101
00:13:07,240 --> 00:13:15,160
is especially emerges if you have enough width if the width is big enough so that the central

102
00:13:15,160 --> 00:13:21,400
limit theorem really kicks in but this basically means that the as a random field as a random

103
00:13:21,400 --> 00:13:28,040
variable the output of the neural network has very nice stochastic process properties and it's

104
00:13:28,040 --> 00:13:34,440
it's a Gaussian process in particular one of the nicest you can have basically here on this

105
00:13:34,440 --> 00:13:45,160
web page on this Wikipedia page there's also like this this this example animation so here they have

106
00:13:45,720 --> 00:13:50,440
some network with three inputs right as I said it doesn't have to be three it suffices to think of

107
00:13:50,440 --> 00:13:56,520
one and they have a bunch of outputs again it suffices to think of one so one green input one

108
00:13:57,240 --> 00:14:07,160
yellow output and a bunch of nodes in in bunch of layers in this case two layers one layer would

109
00:14:07,160 --> 00:14:13,960
also work you see on the right side I mean you probably don't see just because of my face here

110
00:14:13,960 --> 00:14:20,680
but I mean doesn't really matter too much it's just a bunch of like random distribution the

111
00:14:20,680 --> 00:14:27,400
statement is that for fixed input the green value again let's say there's one green input and it's

112
00:14:27,400 --> 00:14:37,080
set to this the float number seven if you fix if you go up with the the number of nodes and

113
00:14:37,080 --> 00:14:43,400
random initialize this with weights and biases then just by the central limit theorem which is

114
00:14:44,120 --> 00:14:52,360
dependent on this this seven and this bunch of random numbers the output y1 here this yellow

115
00:14:52,360 --> 00:15:01,400
output will behave like a Gaussian just by the central limit theorem and in this case there's

116
00:15:01,400 --> 00:15:09,160
two outputs so you can draw a plot and the statement is then that both y1 and y2 behaving

117
00:15:09,160 --> 00:15:16,840
like Gaussians independently from another like it's not a statistic statement but each behave

118
00:15:16,840 --> 00:15:23,800
like a like a Gaussian they have some peak and so on the plot you get another nice Gaussian with

119
00:15:23,800 --> 00:15:34,280
some peak here and so if then the press play again if the network becomes even bigger then you get

120
00:15:34,280 --> 00:15:42,200
like this perfect Gaussian where it this just says that it has this maximum expected value here in

121
00:15:42,200 --> 00:15:50,520
the middle and this goes for all the outputs right so this is the result that that that

122
00:15:53,640 --> 00:16:01,560
the okay I closed the neural network Gaussian process page but doesn't matter this is the result

123
00:16:01,560 --> 00:16:09,160
that just because of statistics you the network if it's large enough at random initialization

124
00:16:09,160 --> 00:16:17,240
behaves like a Gaussian we will not need it for the this video but if you want to take a look

125
00:16:17,240 --> 00:16:22,120
this is the the formal definition of a Gaussian Gaussian process I mean to motivate this basically

126
00:16:22,120 --> 00:16:26,440
you think you know a Gaussian is something which if you do the Fourier transform it's again like

127
00:16:26,440 --> 00:16:33,560
a Gaussian and the Gaussian process is abstractly defined as this random variable or sequence of

128
00:16:33,560 --> 00:16:38,760
random variables where the characteristic function the expectation value of this phase

129
00:16:39,480 --> 00:16:46,760
is this Gaussian with a certain mean we are not going to need this the Fourier transform will

130
00:16:46,760 --> 00:16:54,840
pop up again when I talk about the quantum field theories but suffice to say the nice thing is

131
00:16:54,840 --> 00:17:01,160
that the neural network the big neural networks behave like Gaussian processes sorry if I repeat

132
00:17:01,160 --> 00:17:11,960
myself okay so do we do physics first or do we do neural tangent kernels first

133
00:17:12,440 --> 00:17:21,400
um let me actually um yeah let me actually say something about

134
00:17:23,080 --> 00:17:31,320
the neural tangent kernel so um could we know now that the the network at the start behaves

135
00:17:31,320 --> 00:17:37,480
like this Gaussian for all inputs and if you do the learning process then this is about

136
00:17:37,880 --> 00:17:48,440
um giving it a test data and then moving um in parameter space from wherever we random started

137
00:17:49,000 --> 00:17:56,440
to some other position in in weight space and I have made a bunch of videos on gradient descent

138
00:17:56,440 --> 00:18:02,760
I will not explain it here but suffice to say you have a space of weights and then the the weights

139
00:18:02,760 --> 00:18:09,000
follow some path and you do that in a way that optimizes some goal that you have right some

140
00:18:09,000 --> 00:18:23,640
task for the neural network and I think I sketched it out here so as is common we're dealing with

141
00:18:23,640 --> 00:18:28,600
not only here with a large neural network so that the the theory becomes simpler and nicer

142
00:18:28,600 --> 00:18:34,040
but also we are matching we have so much compute that we can do really small step sizes

143
00:18:34,040 --> 00:18:40,040
so that we can then in the limit talk of the behavior of the network as in a differentiable

144
00:18:40,040 --> 00:18:51,400
way where we say the the the motion of the in path space in a parameter space can be described

145
00:18:51,400 --> 00:18:59,480
you know with literally just calculus differentials and so the gradient descent algorithm what we are

146
00:18:59,480 --> 00:19:06,200
doing really is um you know as per instruction of the algorithm we say the change of the weights

147
00:19:06,200 --> 00:19:11,960
and here I abbreviate all the weights together with this uh theta the the change in the weights

148
00:19:11,960 --> 00:19:17,720
right from from one point to the next in the graphic that you just saw um is chosen in a way

149
00:19:17,720 --> 00:19:27,960
that it takes the negative direction of the gradient of some cost function and the cost

150
00:19:27,960 --> 00:19:35,400
function in here is the you know the the difference essentially between what the neural network

151
00:19:35,400 --> 00:19:42,520
currently says versus where we want to get at where set is all the learning data that we have

152
00:19:42,520 --> 00:19:49,000
available right so we say for all the learning data that we that we have um there is a discrepancy

153
00:19:49,000 --> 00:19:56,840
between what the network f currently um says um what's correct and what is actually correct

154
00:19:57,400 --> 00:20:04,200
why I said here is what's actually correct um and we send that up and so this is like the the this

155
00:20:05,240 --> 00:20:10,760
cost of all learning data together and at every step in time in the learning process

156
00:20:11,480 --> 00:20:18,680
we go follow this path right and this equation is really just the Newtonian equation um where

157
00:20:19,880 --> 00:20:24,360
you know in in physics um theta would be the momentum

158
00:20:28,520 --> 00:20:35,400
and where you um look at the situation where the force on the right hand side is governed by a

159
00:20:35,400 --> 00:20:46,360
potential and you'll say um the the direction of motion um captured by the momentum is given

160
00:20:46,360 --> 00:20:52,120
wherever the you know potential energy will be lowest and that's where the path followed by

161
00:20:52,120 --> 00:21:00,120
the particle in Newtonian physics right so this already looks very um like like this simple physics

162
00:21:01,080 --> 00:21:07,480
uh equation governing governing the motion in weight space and now given that you have

163
00:21:08,040 --> 00:21:14,120
the behavior of the um the the particle if you will uh going through weight space like

164
00:21:14,120 --> 00:21:22,360
and give you just saw um and the the potential depends on the outputs at the network on all

165
00:21:22,360 --> 00:21:28,360
these spaces you can also then um do the calculus and and look at hey how does the output of the

166
00:21:28,360 --> 00:21:34,680
network which depends on the position where you are at right where the weights determine what the

167
00:21:34,680 --> 00:21:40,440
network output will be on all the um learning data how does that the f change and so if you

168
00:21:40,440 --> 00:21:50,440
do them just do the math um and I think I have this is here so this is um newer tangent kernel

169
00:21:50,520 --> 00:21:53,400
theory um if you

170
00:21:59,960 --> 00:22:05,240
if you do this sort of calculation and if you uh you know if you ever started physics you have to

171
00:22:05,240 --> 00:22:10,440
this this sort of calculation a million times because basically if you have some observable

172
00:22:10,440 --> 00:22:16,120
in a physical system and you know all the the constituents of the particles behave in this

173
00:22:16,120 --> 00:22:20,200
isn't this way and then I have some observable which is made out of particles and you say how

174
00:22:20,200 --> 00:22:26,760
does this this observable quantity change then um you have to plug in a bunch of partial derivatives

175
00:22:26,760 --> 00:22:31,640
and then the Hamiltonian comes in and whatever and so on and so forth what comes out of this

176
00:22:31,640 --> 00:22:40,520
is that the development of the output of the neural network um is governed by some matrix

177
00:22:40,520 --> 00:22:52,040
this is the so-called newer tangent kernel and the changes of uh the the loss in uh with respect

178
00:22:52,040 --> 00:22:58,760
to the to the weights right so I mean I did not adopt this terminology theta is again all the weights

179
00:22:58,760 --> 00:23:10,120
together and um you can do this calculation on one sheet of paper yourself I'm not going to discuss

180
00:23:10,120 --> 00:23:17,240
all the the convention or a notation chosen here but the point is that the evolution of the output

181
00:23:17,240 --> 00:23:24,040
on a network from your starting point which might be a random starting point is understood at least

182
00:23:24,040 --> 00:23:28,840
here in theory it's another question of whether you can actually calculate that because this matrix

183
00:23:28,840 --> 00:23:35,080
which determines how this the output of the network evolves as you do the learning according

184
00:23:35,080 --> 00:23:40,840
to gradient descent is determined by this complicated object theta and the theta is basically

185
00:23:42,040 --> 00:23:50,840
this so-called kernel um this is you can view this as the inner product of the gradient of the

186
00:23:50,840 --> 00:23:55,640
output with research with respect to the weight change and so the interpretation is basically

187
00:23:56,360 --> 00:24:07,560
that um you look at uh different inputs and then you um you as a kernel a kernel roughly

188
00:24:07,560 --> 00:24:14,360
charges how similar input data are and this kernel basically looks at uh hey these two

189
00:24:14,360 --> 00:24:22,280
input data are similar if upon a change of the weights the um the response of the network changes

190
00:24:22,280 --> 00:24:27,880
in a similar fashion and you compute this it's in a product in any case this is like an interesting

191
00:24:27,880 --> 00:24:37,080
object that in the end determines how the network behaves um and similar to neural network Gaussian

192
00:24:37,080 --> 00:24:46,920
process theory where you say once I have enough um uh weights in my layer some nice theory emerges

193
00:24:46,920 --> 00:24:53,320
right in in the Gaussian network case it goes towards a Gaussian it's also the case that

194
00:24:54,040 --> 00:25:01,160
for a large network then these these matrix can simplify and then you can get the infinite

195
00:25:02,200 --> 00:25:07,720
size network also to an analytical theory and basically you random initialize you already

196
00:25:07,720 --> 00:25:14,520
know it's some Gaussian process and then you have some matrix which determines how the network moves

197
00:25:15,240 --> 00:25:22,920
um through through weight space and thereby you you have an idea of actually what happens

198
00:25:22,920 --> 00:25:28,360
during network to network training right so if you have never heard that and more or less followed

199
00:25:28,360 --> 00:25:35,640
my explanation then you can see this is kind of cool that at least in this limits you have

200
00:25:36,200 --> 00:25:42,920
sort of an analytical idea what happens during learning and then the question is to what extent

201
00:25:43,240 --> 00:25:50,920
is this sort of logic valid for networks as we can implement them at the moment at the moment

202
00:25:50,920 --> 00:25:56,200
because of course we have a lot of weights like billions of weights but it's not infinite so

203
00:25:56,840 --> 00:26:01,720
you might ask to what extent is the analytical theory where these limits are taken right so

204
00:26:02,600 --> 00:26:10,920
super small step size very large networks applicable to today's convolutional deep neural

205
00:26:10,920 --> 00:26:16,920
networks and so on and so forth and this is basically a subject of study so this is something

206
00:26:16,920 --> 00:26:26,440
where people still put a lot of time in and so for example you have here this google uh neural

207
00:26:26,440 --> 00:26:36,440
tangents project which i might be interested in looking at and there's a bunch of google researchers

208
00:26:36,440 --> 00:26:41,640
who are still using this and publishing papers in this and and so on and so forth there's also

209
00:26:42,520 --> 00:26:53,400
i think a recent um new rips uh poster from 2019 where you get some of the examples of

210
00:26:53,400 --> 00:26:58,360
analytical formulas that i talked about um just because i want to get to the quantum

211
00:26:58,360 --> 00:27:05,480
field theory part i will not discuss this in detail but i um i hope my tangent pun intended

212
00:27:06,920 --> 00:27:11,480
was interesting and as i said i would also actually like to to work a little bit in

213
00:27:11,480 --> 00:27:16,680
this direction myself so if you want to look into that um feel free to reach out and we can

214
00:27:16,680 --> 00:27:31,000
do some sort of project together okay so um now for the the the field theory part

215
00:27:36,440 --> 00:27:42,920
but still extremely important for us is this neural network Gaussian process inside okay

216
00:27:43,640 --> 00:27:47,160
and if i'm here in the paper on page four

217
00:27:50,280 --> 00:27:55,800
then um let's make first some definitions okay so here we have these correlation functions

218
00:27:56,520 --> 00:28:03,880
which we call g n uh for uh for a concreteness sake you can think of n uh let's say as two

219
00:28:03,880 --> 00:28:12,520
so um we are going to actually look at um two different um forward passes for the neural network

220
00:28:12,520 --> 00:28:19,320
right so you have two different imports that you want to try x and you plug them in and you

221
00:28:19,320 --> 00:28:28,200
get something out of your current network which might be randomly initialized um and if you

222
00:28:29,560 --> 00:28:36,680
as we had it with the neural network Gaussian process case if you your weights as a random

223
00:28:36,680 --> 00:28:43,240
variable right over all the weights over all your trillion weights you put a little Gaussian and let

224
00:28:43,240 --> 00:28:53,160
them wiggle a little bit um and you say what typically happens if i sample once and um

225
00:28:55,400 --> 00:29:05,000
put into uh inputs x uh how are the inputs on a on a typical or random network correlated with

226
00:29:05,000 --> 00:29:12,360
each other then you can you can try this a bunch of times and get an idea um what you can also do is

227
00:29:14,040 --> 00:29:17,080
analytically if you know the distribution of where your weights

228
00:29:17,800 --> 00:29:22,600
compute what what will come out right so you have here uh p over the weights this is the

229
00:29:22,600 --> 00:29:27,960
distribution that you yourself chose from which you can sample and uh for fixed neural network

230
00:29:27,960 --> 00:29:38,840
architecture um the weights and um the neural network um which um is here called phi this is the

231
00:29:40,680 --> 00:29:47,080
function that depends on the weights depending on your architecture right so this is the sigmoids

232
00:29:47,080 --> 00:29:54,520
and this sum and so on and so forth and so the correlation function gives you how this how

233
00:29:54,520 --> 00:30:02,120
let's say two inputs are um correlated with each other for this network right and by the neural

234
00:30:02,120 --> 00:30:13,240
network Gaussian process result if we said that uh this this billion uh probability distribution over

235
00:30:13,240 --> 00:30:20,360
the weights um make the input output relation of the whole network also into a random variable

236
00:30:20,360 --> 00:30:27,880
right so you can also view uh this the setup that you have here not as um as um

237
00:30:29,720 --> 00:30:36,840
not just a sampling um the weights and getting uh then a fixed input output but you can also

238
00:30:36,840 --> 00:30:42,440
view any sample process as sampling a whole neural network right even this is just what

239
00:30:42,440 --> 00:30:48,280
you do if you sample if you random initialize for fixed architecture um the um

240
00:30:50,760 --> 00:30:55,320
certain uh functions as your certain neural network then you've also sampled the neural

241
00:30:55,320 --> 00:31:01,480
network from who knows what distribution right so there's also this different the different

242
00:31:01,480 --> 00:31:10,520
view of this initialization process and then um by the result of the neural network Gaussian process

243
00:31:10,520 --> 00:31:16,920
what you have is that you can also view the same uh exact object this correlation function or any

244
00:31:17,000 --> 00:31:25,720
expectation value really um as in terms of distribution over the these functions themselves

245
00:31:25,720 --> 00:31:30,520
and by the result that we just had for a large network this will be a Gaussian process and this

246
00:31:30,520 --> 00:31:37,240
manifests in this way right so um here is the integral not over the weights but over the um the

247
00:31:38,200 --> 00:31:48,920
uh whole function that the network represents itself and um the the um probability distribution

248
00:31:48,920 --> 00:31:59,160
that you have in this case is um of of this sort where this s uh and now this relates back to the

249
00:31:59,160 --> 00:32:06,440
formal definition of the Gaussian process is some quadratic let's say let me fill some local terms

250
00:32:08,200 --> 00:32:14,760
cost associated with the whole um with the whole function so basically what what this does I mean

251
00:32:14,760 --> 00:32:18,200
do they give here some examples I think they do um

252
00:32:22,920 --> 00:32:29,320
okay so this is already sort of a physical example but what you have as s here in the the exponent

253
00:32:29,400 --> 00:32:37,240
is some sum over all uh the values of the network and what what they like what in effect happens is

254
00:32:37,240 --> 00:32:50,680
that um the um the the this this weight is such that um field configurations and when I say field

255
00:32:50,680 --> 00:32:54,760
now I always just mean the same as the input output relation not given by the neural network

256
00:32:55,400 --> 00:33:03,800
field configurations or neural networks where at one uh places you get a huge output these are just

257
00:33:03,800 --> 00:33:11,880
exponentially suppressed because um if you random sample from the with the weights then you are not

258
00:33:11,880 --> 00:33:20,840
likely to get um some basically you are going to get um neural networks in initializations

259
00:33:20,920 --> 00:33:27,800
which are around some some uh some certain um typical expectation value like they have there are

260
00:33:27,800 --> 00:33:34,840
some typical behaviors and everything that deviates uh a lot from it is like exponentially less

261
00:33:34,840 --> 00:33:42,360
likely right you um if you do um thousand random initialization of the neural network you will

262
00:33:42,360 --> 00:33:48,360
get some typical behavior and then you can cook up some other extreme behavior that is not likely

263
00:33:48,360 --> 00:33:56,680
to happen and um by the result of the neural network Gaussian process um theory um it tells you

264
00:33:56,680 --> 00:34:03,320
what the the the probability distribution looks for the neural network so there is this connection

265
00:34:03,320 --> 00:34:09,480
that you have here right and as I will motivate um later uh in quantum field theory this is exactly

266
00:34:09,480 --> 00:34:18,040
sort of the setup for the path integral and what this paper does is it um if you view um

267
00:34:18,360 --> 00:34:27,080
the um this this sort of um mathematical um overlap um as a physicist and you want to use

268
00:34:27,080 --> 00:34:34,760
neural networks as as as a physicist you you see this as a way to um then try to craft neural

269
00:34:34,760 --> 00:34:40,520
network architecture and sampling techniques right the the way in which you sample the weights

270
00:34:41,240 --> 00:34:48,440
in a way so that the this this whole process of random initialization corresponds to certain

271
00:34:48,440 --> 00:34:54,680
s s certain actions here right you do you have some physical scenario in mind there's physical

272
00:34:54,680 --> 00:35:02,040
theory that says oh you know uh certain scalar field theories have this and this action what is

273
00:35:02,040 --> 00:35:08,920
the way I and in which I must set up a network and the way in which I must sample the weights

274
00:35:09,480 --> 00:35:14,520
so that what I sample is exactly as if I would sample from a quantum field

275
00:35:14,520 --> 00:35:22,280
like like as if I would sample from uh would I would sample a field which is one instance of

276
00:35:23,720 --> 00:35:31,240
um a quantum field in the path integral formalism right and then I get out a bunch of correlation

277
00:35:31,240 --> 00:35:37,720
functions this endpoint functions um and these are exactly the things that are uh what what you

278
00:35:37,720 --> 00:35:42,600
do quantum field theory for right you you compute these g's and then I will explain it later then

279
00:35:42,600 --> 00:35:51,960
there's some mathematical connections to how you get from this this this g's to um the scattering

280
00:35:51,960 --> 00:35:57,480
amplitude or whatever your quantum field theory does um maybe particle physics solid state physics

281
00:35:57,480 --> 00:36:07,400
and so on and so forth okay uh let me see so as you might notice this is a very free flow

282
00:36:07,480 --> 00:36:19,080
sort of explanation right um so I have to check uh what I have touched upon and what I didn't

283
00:36:23,320 --> 00:36:27,080
okay

284
00:36:27,720 --> 00:36:38,200
yeah okay so um from the very complicated quantum field theory math you get some

285
00:36:38,840 --> 00:36:49,000
you know relations of um how the correlation function must relate to these actions and there's

286
00:36:49,000 --> 00:36:56,200
a bunch of stochastic differential equation mathematics involved and because in physics

287
00:36:56,200 --> 00:37:02,680
the evolution is always governed by some Hamiltonian operator function there's a bunch of energy

288
00:37:02,680 --> 00:37:09,880
terms that you have to kick around and that's why for example um these objects tend to look

289
00:37:09,880 --> 00:37:18,280
a little bit like this um like if you're never studied this physics um maybe one way in which

290
00:37:18,280 --> 00:37:23,800
you should look at this is that because the evolution of these fields like how they um

291
00:37:24,680 --> 00:37:33,400
have often time is governed by the Schrodinger equation which relates the time derivatives

292
00:37:34,440 --> 00:37:42,440
like the evolution of the fields themselves to some energy expression captured in the Hamiltonians

293
00:37:42,440 --> 00:37:51,880
and the the energy kinetic energy and in particular for fields is given by some spatial operators

294
00:37:51,880 --> 00:37:57,240
that's that's why these sort of objects pop up here and you know mass energy equivalents

295
00:37:57,240 --> 00:38:04,440
that's why we also have mass and so if you see these Laplacian operators or mass terms um you

296
00:38:04,440 --> 00:38:09,160
should not be too surprised uh there because in physics they just always pop up in relation to

297
00:38:09,160 --> 00:38:16,200
the time evolution of the the fields okay so okay now I've already touched upon the concept of time

298
00:38:16,200 --> 00:38:25,320
the thing is of course that um here this the fields as they pop up here will um be

299
00:38:28,280 --> 00:38:35,640
not uh like what you get there is the better controlled theory of euclidean fields right

300
00:38:35,640 --> 00:38:42,440
you're not you're not having to do a priori with spacetime metrics and all these things which make

301
00:38:42,440 --> 00:38:49,880
field theory quantum field field extra complicated but um just talking about Gaussian processes is

302
00:38:49,880 --> 00:38:55,000
just talking about stochastics and then there is this sort of bridge that you have to take

303
00:38:55,000 --> 00:39:03,240
and hope that you can get from the the euclidean field theories to um to some actual quantum field

304
00:39:03,240 --> 00:39:12,360
and I will just name drop um a bunch of concepts there the idea is that you uh if you approach

305
00:39:12,360 --> 00:39:20,920
quantum field theory with this neural network she bang then um you want to find a neural network

306
00:39:20,920 --> 00:39:34,120
which mimics the weak rotated version uh of um of physical quantum field and so there is um

307
00:39:35,160 --> 00:39:40,440
a bunch of uh so-called constructive quantum field theory coming in so there are various approaches

308
00:39:40,440 --> 00:39:47,240
for of people trying to um uh make certain aspects of quantum field theory more rigorous

309
00:39:47,240 --> 00:39:56,520
and um transfer like get rid of uh pseudo metrics in quantum field theory move everything um to the

310
00:39:56,520 --> 00:40:06,840
euclidean domain where you have nice metrics and um uh so in this paper for example they say that um

311
00:40:06,840 --> 00:40:12,600
you know what we really want to impose is um neural networks which when you view them as a field

312
00:40:12,600 --> 00:40:17,640
behave in a certain way and something that is important there for example are these uh

313
00:40:17,640 --> 00:40:25,000
Osterwald Schrader relations so for example here in this case the um correlation functions um

314
00:40:25,000 --> 00:40:29,960
these particular coordination functions of relevance here are called denoted s and then you see on

315
00:40:29,960 --> 00:40:35,320
the screen a bunch of properties that these shall have right so there you have the physical

316
00:40:35,320 --> 00:40:42,280
translation in variances of certain objects and certain symmetry or independence relations right

317
00:40:42,280 --> 00:40:48,120
i'm just mentioning this that there is uh it's not like um you just take any neural network

318
00:40:48,120 --> 00:40:53,720
and then you get some uh some quantum field theory in the path into word formalisms out of it

319
00:40:53,720 --> 00:41:01,320
there's a fairly restricted subset of uh fields that the physicists for quantum field theory might

320
00:41:01,320 --> 00:41:09,160
be interested in um okay but i should probably not go into too much detail on that uh here

321
00:41:09,240 --> 00:41:20,760
um yeah uh also the um these objects in this um exponential so sorry here um

322
00:41:22,680 --> 00:41:27,720
if you if you know some physics then you know this but um if you don't then just want to mention

323
00:41:27,720 --> 00:41:35,640
that these sort of s s um basically any s that you can write down uh gives you some field theory

324
00:41:35,640 --> 00:41:46,040
these s s are some sums or integrals over energy terms and if you go to um sorry for the click

325
00:41:46,680 --> 00:41:52,120
clickery um if you go to the Lagrangian field theory Wikipedia page then you can find

326
00:41:52,840 --> 00:42:00,200
a whole lot of different um s objects that make sense and you can see here see how they

327
00:42:00,200 --> 00:42:06,520
data mine how the data mine um the various physical theories that you have certainly heard of

328
00:42:06,520 --> 00:42:11,720
we are interested in particular about fields so we have here scalar field theories this is what we

329
00:42:11,720 --> 00:42:21,160
just saw you have some partial and then um some um some mass and there will be also be some time

330
00:42:21,160 --> 00:42:29,800
the derivatives there um but the thing is that the pure gaussians are um where this is just

331
00:42:29,800 --> 00:42:37,080
this quadratic object in in s are actually relatively um uninteresting from the physical

332
00:42:37,080 --> 00:42:42,680
perspective because if you have for example if you have different quantum fields that interact

333
00:42:42,680 --> 00:42:48,680
with each other and and then this information how they interact is also all encoded in this

334
00:42:48,680 --> 00:42:55,560
in this um in this Lagrangian cells or in the action s and then uh you will have some more

335
00:42:55,560 --> 00:43:03,320
complicated products in these objects and um if you have some power of the field that's higher than

336
00:43:03,320 --> 00:43:12,840
two then this is actually uh representing um sort of self interaction in in field theory so um what

337
00:43:12,840 --> 00:43:20,280
we really want to have is not just um the fields which fulfill these nice um properties in the

338
00:43:21,000 --> 00:43:26,920
Euclidean version but we also want to have very finely controlled um interaction terms

339
00:43:26,920 --> 00:43:36,360
there and so what we really want to have is um processes which are actually not gaussian right

340
00:43:37,320 --> 00:43:43,000
and so what they do in the paper is in in the end um look at

341
00:43:43,000 --> 00:43:54,200
uh um five to the fourth field theory so quadratic interaction let me see sorry

342
00:44:07,400 --> 00:44:11,320
so what they really want to implement and what they actually then do in the paper is

343
00:44:11,320 --> 00:44:16,760
they take this sort of action you you not only have this quadratic term there but you also have

344
00:44:17,400 --> 00:44:29,320
their um this five to the four uh term and to to get this in to get um uh away from the

345
00:44:29,320 --> 00:44:37,240
just quadratic gaussian process scenario there is two ways um to implement this self interaction

346
00:44:37,480 --> 00:44:45,720
uh and one way is to actually not look at the infinite and limit right not the infinite

347
00:44:47,080 --> 00:44:56,520
size network limit um because then uh the you basically break the neural network gaussian

348
00:44:56,520 --> 00:45:05,400
process scenario you you person person uh you purposely stop a little bit earlier and get some

349
00:45:05,480 --> 00:45:11,320
non gaussian effects and so what from the mathematical point of view is a bug that for

350
00:45:11,320 --> 00:45:17,240
a real network you actually don't get a perfect gaussian situation here becomes sort of feature

351
00:45:17,240 --> 00:45:23,640
it gives you the freedom to actually implement behavior and if you tweak the network uh nice

352
00:45:23,640 --> 00:45:29,160
enough the ideas that then you can sort of control how it is broken this bug certainly

353
00:45:29,240 --> 00:45:36,280
becomes a feature this is one way and the other way is if you um actually um

354
00:45:39,080 --> 00:45:45,880
in sampling you do not uh take a trillion independent distributions over the weights

355
00:45:45,880 --> 00:45:53,560
but what you and instead do is you introduce on purpose some dependencies of the individual

356
00:45:53,560 --> 00:45:59,480
weight uh distributions right do you break the independence such that um there is a little

357
00:45:59,480 --> 00:46:05,720
bit of a flaw in the central limit theorem and in this way by independence you also get some

358
00:46:05,720 --> 00:46:11,800
non gaussian process because it's clear that if you do only um break the independence of this

359
00:46:11,800 --> 00:46:17,640
weights a little bit you'll still by the central limit theorem get something which is just a slight

360
00:46:17,640 --> 00:46:23,320
deviation from discussion processes right so if you the idea is if you tweak the this

361
00:46:23,320 --> 00:46:28,920
the conditions for the god for the central limit theorem just in a wide way then you might produce

362
00:46:31,480 --> 00:46:39,000
errors to the the gaussianity in a in a in a very controlled way and this is exactly what

363
00:46:39,000 --> 00:46:46,680
they do here so here they describe a neural network with particular non-linearity as activations it

364
00:46:46,680 --> 00:46:53,800
looks like this and what they do is they sample in this um in this dependent way in exactly the

365
00:46:53,800 --> 00:47:05,640
correct way so that the um the action um that emerges by random sampling in this you know

366
00:47:06,440 --> 00:47:14,680
particular way represents uh this uh this sort of physical field theory so this field theory in

367
00:47:14,680 --> 00:47:20,360
particular is basically always the first interaction uh interacting field theory or one of the first

368
00:47:20,360 --> 00:47:25,000
interaction field theories that you would learn in the quantum field theory course um this is not

369
00:47:25,000 --> 00:47:35,320
one of the famous standard model um energy densities at least not in the exact same way here

370
00:47:36,280 --> 00:47:41,000
but nonetheless this is like classical physical theory and so this is what this paper is all

371
00:47:41,000 --> 00:47:48,200
about right breaking the neural network gaussian process uh theorems in the correct way to get um

372
00:47:49,480 --> 00:47:58,200
the the right uh g functions endpoint functions out there that are relevant for physics um and

373
00:47:58,200 --> 00:48:06,040
so you see that then you can you know in principle sample uh from this fixed neural network architectures

374
00:48:06,040 --> 00:48:12,920
fields and then once you have like a way of sampling fields you can in principle because

375
00:48:12,920 --> 00:48:17,640
the fields that demand all the properties of the quantum field theory uh compute expectation

376
00:48:17,640 --> 00:48:23,480
values and thereby get physical uh information so this is the idea it also goes in the other direction

377
00:48:23,480 --> 00:48:34,040
in um since there is uh these methods in particular um Feynman diagram computation methods that compute

378
00:48:34,040 --> 00:48:43,320
correlations um for quantum fields you also have a method of computing aspects of random

379
00:48:43,320 --> 00:48:49,320
initialized big neural networks right you have a big neural network you know that if you uh

380
00:48:52,440 --> 00:48:59,240
to sample from it it is as if you would sample from a quantum field um and because you have

381
00:48:59,240 --> 00:49:04,360
ways in physics to compute aspects like correlation functions and so on of the quantum fields

382
00:49:05,000 --> 00:49:10,280
these uh g functions that you can compute in physics with Feynman diagrams and so on

383
00:49:10,280 --> 00:49:19,320
also have a relevant meaning for computing typical aspects of random initialized neural networks

384
00:49:19,960 --> 00:49:24,600
okay i know this is a little bit much but i hope it sort of makes sense um

385
00:49:24,600 --> 00:49:36,520
um i um i don't know how clear everything was that i discussed so far i just want to give you

386
00:49:37,960 --> 00:49:43,240
this is then more on the quantum field theory side a little motivation that i have just here

387
00:49:43,240 --> 00:49:50,040
written up um that connects these g functions right these autocorrelations to physics i just

388
00:49:50,120 --> 00:49:56,840
want to motivate it maybe give me a three more minutes so i i'm going to assume that you have

389
00:49:56,840 --> 00:50:03,000
an idea of the fact that transition probabilities are the squares of inner products in a Hilbert's

390
00:50:03,000 --> 00:50:10,840
base in quantum mechanics um so there are these um kernel objects k which are given like that

391
00:50:10,840 --> 00:50:17,560
so in in in you know standard quantum mechanics what you have is some um you have some in um

392
00:50:18,280 --> 00:50:22,600
current state which i call here in and then you have some uh other state out that you are

393
00:50:22,600 --> 00:50:29,000
interested in you want to know what is the chance that this state um transitions over in that state

394
00:50:29,000 --> 00:50:36,360
what you do is you um take the Schrodinger equation uh evolution which is governed by the

395
00:50:36,360 --> 00:50:43,880
Hamiltonian h in in the nicest case um here the formal solution of this equation is just e to the i

396
00:50:44,840 --> 00:50:52,360
h so this is the operator which moves any state forward in time you apply it to in to the like

397
00:50:52,360 --> 00:50:59,320
let's say this is now this is in two days you say this state that i have currently how will it

398
00:50:59,320 --> 00:51:06,280
evolve into the in like how will it look in two days then you basically apply this operator to

399
00:51:06,280 --> 00:51:13,640
fast forward this thing um and then you get something uh the in state how it will look like

400
00:51:13,640 --> 00:51:20,280
in two days and then the product with this object in two days with the out state that you're interested

401
00:51:20,280 --> 00:51:27,960
in gives you the probability that the the current state after it has evolved is going to be observed

402
00:51:27,960 --> 00:51:34,520
in uh in the out state okay so this is basically quantum mechanics one one um the for whatever

403
00:51:34,520 --> 00:51:41,880
Hamiltonian you have here you um can often then compute some analytical case some kernels

404
00:51:44,280 --> 00:51:49,320
these are these things and i think here if you score down in this Wikipedia page you find some

405
00:51:49,320 --> 00:51:55,160
examples so these are just you know in this case uh three particle in one dimension this is like

406
00:51:55,320 --> 00:52:05,480
some Gaussian this is similar to some heat dissipation equation and then if you have some

407
00:52:05,480 --> 00:52:10,840
more relativistic scenario i think they also write down here some this is more complicated

408
00:52:12,440 --> 00:52:22,840
correlation functions and examples okay so that's the one thing now if we talk

409
00:52:23,560 --> 00:52:25,000
quantum field theory

410
00:52:31,080 --> 00:52:31,480
then

411
00:52:38,760 --> 00:52:45,880
yeah the uh this is basically just a definition um the the g's we all also had that in the paper

412
00:52:46,520 --> 00:52:53,480
are certain expectation values right so there since we're on the pure relativistic side

413
00:52:53,480 --> 00:52:59,000
and i'll take your clean side there's some time operator there but you can basically ignore it

414
00:52:59,000 --> 00:53:04,920
for for the moment the the point is that these g's are determined by some fairly simple looking

415
00:53:05,880 --> 00:53:10,600
expectation values and in the in the quantum mechanical formalisms these are also some

416
00:53:10,600 --> 00:53:15,400
inner products in some Hilbert space of course in a quantum field theory there is no super nice

417
00:53:15,400 --> 00:53:22,360
theory um about uh Hilbert spaces and and and these operators so there is it's more like a

418
00:53:23,160 --> 00:53:29,000
like a functional definition this is what you calculate and um you hope it's somewhere

419
00:53:29,000 --> 00:53:33,720
else comes up with the sort of rigorous setting to compute this but nonetheless you know how to

420
00:53:33,720 --> 00:53:43,320
compute these g's you know how to set up the an evolution equation of your choice and make

421
00:53:43,320 --> 00:53:47,640
sense of these correlation functions which is um what you want to get at in the first step

422
00:53:48,680 --> 00:53:57,560
so given um you have uh definition of these g's there is actually then a sort of generating

423
00:53:57,560 --> 00:54:03,560
function in the in the um you know analysis generating function sense there are these partition

424
00:54:03,560 --> 00:54:09,560
functions which look like so so you basically we can view it like starting from this we start

425
00:54:09,560 --> 00:54:16,920
with some some definition where all these g's are um captured uh in this sort of formal some

426
00:54:16,920 --> 00:54:24,360
formal sum of products and then these g's are once you would have this uh partition function

427
00:54:24,360 --> 00:54:30,360
with this j parameter you can in principle like formally speaking with functional derivatives

428
00:54:30,360 --> 00:54:34,680
compute this g from this thing and then there's this whole quantum field theory I mean this is

429
00:54:34,680 --> 00:54:41,240
what quantum field theory is all about that tells you how to compute this this set and this is

430
00:54:41,240 --> 00:54:48,520
then exactly this sort of object this is what we already had have had there right we already said

431
00:54:48,520 --> 00:54:58,440
that um if you um take the expectation value of e to the minus s in the euclidean case is here

432
00:54:59,720 --> 00:55:06,760
of a product of these phi's you get all these these g's and this is basically sort of the

433
00:55:06,760 --> 00:55:14,760
generating function trick to get to this to these sort of objects okay um okay so this is

434
00:55:14,760 --> 00:55:21,080
what you do in practice and just to motivate also that how um neural network sampling would

435
00:55:21,080 --> 00:55:25,720
be done another way of getting at this these g functions which you want to to have okay and

436
00:55:25,720 --> 00:55:32,760
and so to round it up to see to to explain how this this simple g's this field theory um

437
00:55:34,760 --> 00:55:43,560
correlation functions relate to actually um to actual observables right you want to have this

438
00:55:43,560 --> 00:55:48,760
sort of transition functions as I discussed them for the quantum mechanics case there is then

439
00:55:51,000 --> 00:55:56,840
complicated theory of how you take these states how they how you encode these states with some

440
00:55:56,840 --> 00:56:02,360
fixed momentum in the in the quantum field theory you know they also live in some supposed

441
00:56:03,480 --> 00:56:12,360
Hilbert space let's say you have to compute a lot in in momentum space which by Fourier

442
00:56:12,360 --> 00:56:18,280
transform is sort of the dual to the the position space that we talked about in the whole video

443
00:56:18,280 --> 00:56:25,240
so there are some Fourier transforms involved and you pass from the phi's to this a's so

444
00:56:26,200 --> 00:56:29,800
just to sketch this out here um

445
00:56:34,120 --> 00:56:39,560
yeah so you have when these are I'm just throwing these formulas at you right but

446
00:56:39,560 --> 00:56:47,480
there's this typical relations between um the momentum space here ap and the the

447
00:56:49,320 --> 00:56:56,280
spacetime fields and you can view this as a transition link by Fourier transform but

448
00:56:57,160 --> 00:57:03,160
Fourier transform is with respect to space and also like you know momentum on space and also time

449
00:57:03,320 --> 00:57:10,920
and energy and the energy encodes the sort of relation that is governed by the

450
00:57:12,920 --> 00:57:19,000
Schrodinger equation relation and so all these things are fairly like complex but in the end

451
00:57:19,000 --> 00:57:25,000
it's clear that once we have these g's we have to sort of Fourier transform them over to a momentum

452
00:57:25,000 --> 00:57:30,520
space and in momentum space we encode actually the input output state that we are actually interested

453
00:57:30,520 --> 00:57:42,840
in like this for some scattering transition probability stuff okay and then there is some more

454
00:57:42,840 --> 00:57:49,720
theory that gives you this momentum space in out transition amplitudes that you're actually

455
00:57:49,720 --> 00:57:56,840
interested in in terms of these these g's basically you see on this side here these are basically g's

456
00:57:56,840 --> 00:58:03,800
and then from Fourier transforming and taking into account how the the frequency is and space is

457
00:58:03,800 --> 00:58:09,080
sort of tied together through the evolution equations you get some nasty object here that you

458
00:58:09,080 --> 00:58:18,920
have to then compute and then with this this this formalism you get from the green's functions to

459
00:58:18,920 --> 00:58:24,920
the transition probability right so I in these last 10 minutes yeah I just explained to you

460
00:58:24,920 --> 00:58:28,840
why these g's are really the important thing that you want to get that once you have them

461
00:58:28,840 --> 00:58:37,800
then in principle you can compute this transition probabilities and the neural network aspect

462
00:58:37,800 --> 00:58:46,840
gives you a way to get at these g's okay so lastly I want also to mention that

463
00:58:47,160 --> 00:58:56,360
apart from the neural network kernel theory stuff there's also the you know classical

464
00:58:57,320 --> 00:59:05,800
information geometry approach where you you know you have the underlying parameters theta

465
00:59:05,800 --> 00:59:15,240
which are the weights and you can see them as encoding if you put a distribution over the

466
00:59:15,320 --> 00:59:19,560
inputs and you get the distribution over the outputs which are governed by the parameters

467
00:59:19,560 --> 00:59:25,000
then you get this information manifold so you heard that information geometry stuff this is

468
00:59:25,000 --> 00:59:32,040
like a related but different mathematical angle that I just wanted to mention because we are like

469
00:59:32,040 --> 00:59:36,760
formally so close having all the important all these stochastics already but this is a little

470
00:59:36,760 --> 00:59:42,360
bit different nonetheless but also interesting there you have this neural neural manifold

471
00:59:42,360 --> 00:59:49,480
thing so this is also one line of research I actually try to find things going into the

472
00:59:49,480 --> 00:59:55,400
this quantum field theory research direction in in deep learning books in preparation for this video

473
00:59:56,440 --> 01:00:02,040
and actually didn't find too much so there was more about this classical information geometry

474
01:00:02,040 --> 01:00:11,080
aspect of things and then also worth noting the applications to quantum field theory is not the

475
01:00:11,080 --> 01:00:18,760
only way in which people try to apply neural network theory and all these nice formulas to

476
01:00:18,760 --> 01:00:29,080
physics there's also this mathematical metric flow things going on I just mentioned it because if

477
01:00:29,080 --> 01:00:38,440
you look at for example the research groups that put out this paper then you will also find these

478
01:00:38,440 --> 01:00:46,680
sort of neural network applications using quantum field theories okay so I mean at one hour I know

479
01:00:46,680 --> 01:00:52,360
it has been a little bit fuzzy but nonetheless I think I at least maybe you understood the neural

480
01:00:52,360 --> 01:00:58,440
network Gaussian process stuff and and can see how this ties in with path integral formalisms

481
01:00:59,960 --> 01:01:05,560
and I motivated you I'm really I know very few people will make it through a one hour

482
01:01:05,640 --> 01:01:12,920
video of these sort of friends but it's probably nonetheless I think the best way to get an infusion

483
01:01:12,920 --> 01:01:19,320
of this sort of theory in a digestible way and in a way where somebody highlights these things so

484
01:01:19,320 --> 01:01:23,880
this is it's not a real excuse I think it's nonetheless helpful even if this was not very

485
01:01:23,880 --> 01:01:34,360
super prepared okay with that I leave it I leave it at that I wish you a good transition into the

486
01:01:34,360 --> 01:01:42,680
next year I have no real videos planned for the upcoming months really I mean I have a folder with

487
01:01:42,680 --> 01:01:50,280
20 started projects and videos I could talk about that I might come back to probably not

488
01:01:50,280 --> 01:01:57,720
within the next months but then as I said I might look more into classical classical

489
01:01:57,720 --> 01:02:02,520
functional analysis stuff for the sake of making a nice video about the universal approximation

490
01:02:02,520 --> 01:02:14,360
theorem and at the latest then I will have a polished video take care

