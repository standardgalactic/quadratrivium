start	end	text
0	15000	Hello, Lionel. It's already seven o'clock.
15000	18000	Wake up. It's twenty twenty seven.
18000	22000	Don't wake up. You have a meeting at nine o'clock this morning.
22000	27000	In twenty twenty seven, Sarah takes care of everything.
27000	30000	For breakfast, what do I make for you?
30000	32000	A glass of chocolate.
32000	38000	Looking at your health data, I advise you to use soy milk instead of avocados.
38000	44000	Sarah is a virtual assistant who knows exactly what's best for you.
44000	48000	For your part of Squash, I selected Emmanuelle.
48000	53000	I crossed your data. This time, you're sure to take it.
54000	58000	For your dinner party tonight, I reserved this restaurant.
58000	60000	It's new and very well-noted.
60000	62000	It's perfect. Thank you.
62000	69000	Everywhere you go, artificial intelligence, like Sarah, predicts your needs and does the work for you.
71000	77000	Hello, Lionel. We're going to start. I've prepared a contract for our last meeting.
78000	84000	With all of these machines working for you, isn't life wonderful in twenty twenty seven?
87000	90000	But let's not get carried away.
90000	97000	Before Sarah changes your life forever, there's another story to tell, one with less special effects.
97000	113000	This story takes place behind the scenes of those businesses who are working to invent our future.
113000	119000	For now, it's hardly this wonderful world where machines are working entirely for mankind.
119000	122000	In fact, you could say it's exactly the opposite.
122000	125000	Humans are involved in every step of the process.
125000	132000	When you're using anything online, but we're sold as this miracle of automation.
132000	142000	Google, Facebook, Amazon, Uber, these digital giants are using a completely invisible workforce to keep their applications running.
142000	144000	There we are.
144000	151000	With technology, you can actually find them, pay them the tiny amount of money, and then get rid of them when you don't need them anymore.
152000	158000	A workforce that is disposable and underpaid.
158000	167000	On a very good day, I could do five dollars an hour. On a really bad day, I could do ten cents an hour.
167000	171000	Is it possible for you to pay less than the American minimum wage?
171000	174000	I'm not sure we want to go in this direction, yeah.
174000	179000	Whilst millions of men and women are training artificial intelligence for next to nothing,
179000	185000	others are being hired and hidden out of sight to clean up social networks.
185000	191000	You must have been told by the recruiting team that you cannot mention that you are working for this project.
191000	198000	We went undercover as one of these web cleaners, working as a content moderator for Facebook.
198000	212000	There are a few things that I saw. Those things are going to stay with me because I remember them as it was yesterday.
212000	222000	To meet the workers hiding behind your screen, we're taking you to the factory of the future, the digital economy's best kept secret.
222000	227000	You know, it's just like a sausage factory. They don't want people to come in to see how the sausage is made.
227000	230000	I mean, I think it's just that simple.
244000	251000	To delve into the mysteries of artificial intelligence, we're heading to the west coast of the U.S.
252000	259000	Here in San Francisco and the Silicon Valley, the world of tomorrow is being developed.
259000	274000	It's the high-tech hub of giants, like Apple, Facebook, YouTube, Uber, Netflix, and Google.
274000	282000	We have a meeting of figure eight, a business specializing in artificial intelligence that primarily works with Google.
282000	287000	The founder, Lucas Biewald, agreed to spend the morning with us.
287000	295000	Hello, Lucas. Nice to meet you. Thank you very much for your time. I know you have a busy schedule. Thank you.
295000	304000	At 38 years old, this Stanford graduate has already worked for the likes of Microsoft and Yahoo before founding his own company.
304000	310000	Once his microphone is on, a quick tour of their startup-style Californian office space.
310000	315000	This is our best dressed in play.
315000	318000	Cool and relaxed.
318000	322000	This is probably our worst dressed in play.
327000	332000	Do you play babyfoot? I think I'm pretty good. I don't know, maybe.
332000	337000	This is kind of our eating area. This is actually where I like to work.
337000	341000	My coffee got cold.
341000	345000	And in the reception area, an impressive display.
345000	355000	These are some of our customers and the different things that they did with our products.
355000	363000	Here's Twitter. We help them remove a lot of people that were kind of bullying on their website.
363000	369000	You know, American Express. Is that in France? Yeah.
369000	373000	You know, I feel especially proud of, you know, something like Tesco, right?
373000	382000	Is able to use us to improve their online website to show better search results so people can find the items that they're looking for.
382000	384000	And I don't see Google.
384000	387000	No, I don't know. Do you know why some of these get up here?
387000	393000	Frankly, we just stopped because it was getting out of hand.
393000	396000	This is Mr. Brown, head of PR.
396000	398000	This is a good scene.
398000	405000	After our visit, the founder explains the enigmatic name, Figure 8.
405000	409000	We call our company Figure 8 because we think of it as a loop.
409000	415000	And the loop really has these two parts, right? There's the humans that do the labeling.
415000	418000	And then the machine learning that learns from the humans.
418000	422000	And then it goes back to the humans for more labeling, right?
422000	426000	So we think of this kind of like beautiful loop, right?
426000	433000	Where humans do the best things that humans can do and the algorithms, the artificial intelligence, does the best things that the algorithms can do.
433000	437000	And we put that together. And that's why we call it Figure 8.
443000	451000	To get a better understanding of why AI needs humans to function, we stop joking around and get out the computer.
452000	454000	So here's an example.
454000	458000	You know, a lot of people these days are trying to build cars that automatically drive.
458000	464000	Like, for example, Tesla has a system where you can drive around in a car.
464000	469000	But of course, it's incredibly important that these cars don't run into pedestrians.
469000	473000	So the car camera just sees something like this.
473000	478000	So it's really important that they build reliable systems that can identify people.
478000	485000	And the way that they learn to identify people is looking at lots of pictures of what the car is seeing from the camera.
485000	488000	And then actually literally labeling where the people are.
489000	492000	Here's a real example of how it works.
492000	498000	If you want to teach a self-driving car to recognize a pedestrian, a human like you or I,
498000	505000	it first has to identify pedestrians from photos and then feed this information to the AI.
505000	513000	And this process has to be done over a thousand, even a million times over, which can be very time-consuming.
518000	524000	This is where Figure 8 gets involved, using real people who are paid to do this work.
526000	533000	So the task here is to look at this picture and then label where the people are.
533000	534000	And so you get paid for this?
534000	537000	You get paid to draw boxes around the people.
537000	538000	How much?
538000	547000	I'm not sure this task, but maybe it would be like ten cents per person that you draw a box around.
547000	553000	Who do this job? Do you have employees doing these jobs and labeling people?
553000	558000	Yes, so it's contractors in our network that log in and do these jobs.
558000	562000	What do you mean by contractors on your network? What kind of people?
562000	568000	So it's like people that log into this and then want to work on these tasks.
568000	572000	How many people work for Figure 8?
572000	574000	In this capacity as labellers?
574000	575000	Yeah.
575000	578000	So again, people can kind of come and go if they want to.
578000	587000	So there's maybe around 100,000 people that kind of consistently work every day for certain use cases that we have.
587000	593000	But then there's also millions of people that log in from time to time and work on tasks.
593000	596000	And where do those people live?
596000	598000	They live all over the world actually.
598000	603000	So they live all over America and then they live all over the world.
606000	612000	So who are these millions of people who are being paid to train AI technology?
613000	619000	In order to meet these contractors, as Figure 8 calls them,
619000	625000	we leave Silicon Valley and head 500 miles north of San Francisco in Oregon.
632000	633000	There we are.
633000	635000	Ah, success.
636000	640000	Jared Mansfield signed up to Figure 8 three years ago.
640000	644000	He now spends several hours a week working for them.
644000	649000	Every day, the company offers a list of tasks that he can complete for money.
649000	652000	For example, training search engines.
659000	662000	For this first one, it's showing examples of how to do it.
662000	665000	The query is mac and cheese pierogies.
665000	669000	And the two results are Annie's homegrown organic mac and cheese
669000	674000	and Annie's really cheddar microwavable macaroni and cheese, which are neither of them are pierogies.
674000	677000	So it's saying that one would be equally bad matches.
677000	679000	What's the use of doing that?
679000	685000	A lot of it, I think, is to train search algorithms.
685000	689000	So like when someone sits at their computer and types a product,
689000	694000	the algorithm will be able to determine with more accuracy
694000	697000	what product it is that that person is looking for.
701000	706000	For every ten answers, Jared earns less than one cent.
706000	712000	To get an idea of how much money he can make, we leave him to work for 30 minutes.
712000	717000	He's answered 180 questions over the course of half an hour.
719000	721000	How much have you earned?
721000	722000	15 cents.
722000	723000	For how long?
723000	725000	A half hour.
725000	728000	Which would be 30 cents per hour?
728000	734000	Yeah, which are pretty definitely not a livable wage, that's for sure.
734000	736000	Do they have the right to do this?
736000	739000	I mean, they have the right to do whatever they want.
739000	744000	I'm the one coming to them for little tiny bits of coins on this website.
744000	750000	And there's no contract between me and them.
753000	757000	No contract, no salary, no guaranteed minimum wage.
757000	761000	These ghost workers are paid to train software and robots
761000	765000	using only one rule, supply and demand.
766000	770000	It definitely feels like I'm part of this invisible workforce
770000	775000	that is kind of made up of just random people throughout the world.
775000	784000	And together we're kind of training what's going to replace the workforce as a whole eventually.
784000	788000	Jared is very philosophical about the idea.
788000	790000	Still, he can afford to be.
790000	795000	To earn a real living, he has another job selling chicken in the supermarket
795000	798000	for a little more than $1,500 a month.
798000	803000	Figure 8 is just what he does on the side to earn a little extra cash.
810000	815000	After leaving Oregon, we decided to take advantage of what we'd learned in America
815000	821000	and sign ourselves up to Figure 8 to train artificial intelligence.
821000	828000	On the site's welcome page, small tasks are proposed at 1, 2, or 12 cents.
831000	838000	We chose this as our first task, drawing boxes around objects in images.
838000	842000	Following the instructions, it took us a couple of minutes
842000	846000	to draw around 10 objects and earn 2 cents.
846000	852000	On the list of tasks, Figure 8 also offers evaluations of search engine answers,
852000	854000	Jared's task of choice.
854000	858000	We could also listen to conversations and confirm if the recording
858000	863000	features a man or a woman's voice, and if they are speaking English.
863000	866000	Hi, is James there, please?
867000	870000	We work for our team,
871000	876000	we work for hours without ever earning more than 30 cents an hour.
881000	888000	It's difficult to imagine that there are people who work on these tasks on a full-time basis.
891000	897000	We're in Maine, on the east coast of the United States, close to the Canadian border.
897000	900000	We've arranged to meet with one of the NET's ghost workers,
900000	903000	the human side of the Figure 8 loop.
906000	910000	Her name is Don Carbone, she is 46 years old.
912000	913000	Bonjour.
915000	916000	Hello.
916000	917000	Hello.
918000	919000	Nice to meet you.
919000	921000	Thank you so much for your welcome.
921000	922000	Beautiful.
922000	928000	Oh, we had a blizzard not that long ago, and then we got more.
928000	931000	And it's also, I think, negative seven out there.
932000	934000	Don is a single mother.
934000	936000	She lives here with three of her children.
941000	945000	This is what subsidized housing looks like up here.
945000	949000	I mean, it's not bad for public housing.
950000	956000	She lives and works here, working on the Figure 8 site all day.
961000	965000	I'll turn it on, like I said, right before seven o'clock.
967000	969000	Get the initial stuff done.
969000	975000	I'll turn this off at three o'clock in the afternoon and then turn it back on at nine o'clock at night.
975000	980000	So, I'll say eight hours minimum.
982000	984000	I bust my butt though.
985000	987000	Like this would be the dashboard.
987000	991000	And you can see I've done 6,445 tasks.
991000	993000	Since when?
993000	995000	Three years.
995000	996000	See these different badges?
996000	997000	Yeah.
997000	999000	You start off, you have no badge.
999000	1002000	And you have to do so many questions and get so many right.
1002000	1004000	And then you get your first level badge.
1004000	1011000	And then when you get to level three, you have access to virtually all the tasks that are put up.
1013000	1015000	What is your level right now?
1015000	1016000	Right now, oh, I'm on level three.
1016000	1018000	I've been level three.
1018000	1021000	I've been level three for quite a while.
1021000	1025000	Don is considered a high performing worker.
1025000	1028000	Figure 8 therefore offers her more work than a beginner.
1028000	1031000	But it isn't necessarily more interesting.
1033000	1039000	I have to put bounding boxes around people.
1042000	1045000	I'm not really keen on this job.
1049000	1055000	The biggest problem is trying to find jobs that are viable.
1055000	1058000	And right now, I don't have many.
1059000	1062000	And it's definitely not better paid.
1064000	1067000	On a very good day, I could do $5 an hour.
1067000	1071000	On a really bad day, I could do $0.10 an hour.
1071000	1078000	I mean, I have had some really, really good days until February.
1078000	1079000	Yeah.
1079000	1084000	Do you think this is a fair payment for what you're doing?
1084000	1087000	No, no, no, no.
1087000	1089000	Not at all.
1089000	1092000	But I live in Northern Maine.
1092000	1094000	We get a lot of snow.
1094000	1098000	There's a very low job market.
1098000	1101000	And it helps me as a stay-at-home mom.
1101000	1104000	It helps with added income.
1110000	1113000	Don prefers to work from home because her youngest daughter,
1113000	1115000	Jane, has autism.
1116000	1118000	Here you go.
1118000	1120000	What happened?
1120000	1122000	Don wants to be there to take care of her
1122000	1126000	when she gets home from school at 3 p.m.
1126000	1128000	So how was school?
1128000	1130000	Good day or bad day?
1130000	1133000	Really a good day?
1133000	1136000	With her autism, I always have to be ready to jump in my car
1136000	1138000	and go get her from school.
1138000	1141000	I mean, it could happen one day out of the week,
1141000	1145000	or not at all, or three days out of the week.
1145000	1147000	And the school is very understanding.
1147000	1151000	So I mean, I have to take out the whole week
1151000	1154000	if I was working out of the home.
1154000	1159000	Don receives $750 in government aid every month,
1159000	1162000	which isn't enough to cover all of her bills.
1162000	1165000	This is why she signed up to Figure 8.
1165000	1168000	By working eight hours a day and five days a week,
1168000	1173000	she says she earns, on average, $250 a month on the site.
1184000	1187000	On Figure 8, the pay is non-negotiable.
1187000	1189000	If you refuse the work,
1189000	1192000	there will always be someone else to take it.
1194000	1197000	There is an unlimited supply of these ghost workers,
1197000	1199000	coming from all over the world.
1199000	1203000	It's probably why Lucas B. Wald is so happy.
1203000	1208000	But he isn't the only one to take advantage of this phenomenon.
1210000	1213000	Various other businesses propose these sorts of repetitive
1213000	1216000	and underpaid online tasks,
1216000	1221000	the biggest amongst them being ClickWorker and Amazon Mechanical Turk,
1221000	1225000	a platform provided by Amazon and its boss, Jeff Bezos,
1225000	1228000	who invented the concept in 2005.
1231000	1234000	Micro-working is a growing concern for the ILO,
1234000	1237000	the International Labour Organization,
1237000	1242000	a UN agency in charge of protecting workers' rights across the globe.
1249000	1253000	Jeanine Berg is the resident expert on this subject at the ILO,
1253000	1255000	who speaks to us through Skype.
1283000	1288000	And the technology has facilitated this, and it's cheap.
1288000	1291000	That's us, the main advantage.
1292000	1294000	Jeanine Berg wrote a report,
1294000	1299000	calculating that micro-workers earn, on average, $3.31 an hour,
1299000	1301000	without any rights in return.
1302000	1307000	Workers' extreme vulnerability is the key to Lucas B. Wald's business model.
1308000	1310000	After months of investigations,
1310000	1316000	we found this video from 2010 that sums up his view of the labour force.
1319000	1323000	Before the internet, it would be really difficult to find someone,
1323000	1325000	sit them down for 10 minutes and get them to work for you,
1325000	1327000	and then fire them after those 10 minutes.
1327000	1329000	But with technology, you can actually find them,
1329000	1331000	pay them a tiny amount of money,
1331000	1334000	and then get rid of them when you don't need them anymore.
1338000	1340000	While we were interviewing him,
1340000	1344000	we wanted to ask him if he still shared the same opinion.
1345000	1348000	But when we start talking about work conditions,
1348000	1352000	the figure-eight founder seemed to lose his sense of humour.
1352000	1357000	Do you have an idea of the average revenue per hour of your contributor?
1357000	1358000	You know, I'm not sure.
1358000	1361000	It's totally dependent on the task that someone puts in,
1361000	1363000	and it's hard to track time on the internet
1363000	1365000	because people can walk away from their computer and come back,
1365000	1369000	so I don't know how much people don't really make.
1369000	1372000	There was a report on ILO saying that on average,
1372000	1379000	the people working on crowdsourcing were paid $3.31 an hour.
1379000	1382000	Would that be consistent with what you pay?
1383000	1385000	Again, I'm not sure.
1386000	1390000	Is it possible for you to pay less than the American minimum wage?
1391000	1393000	It could be possible.
1393000	1395000	So this is legal?
1399000	1401000	I'm not sure we want to go in this direction.
1401000	1404000	Can we take this a different direction?
1404000	1407000	I'd rather this focus on more AI than anything.
1407000	1409000	Yeah, but this is the whole thing.
1409000	1411000	This is about crowdsourcing as well,
1411000	1414000	so I have to ask questions on crowdsourcing.
1414000	1418000	I prepped him for more of an AI conversation
1418000	1421000	than a crowdsourcing conversation.
1421000	1425000	No, I don't really want to do this.
1425000	1429000	Yeah, we can find someone else to talk about this stuff.
1429000	1433000	Okay, so you're not comfortable with this part of the discussion?
1433000	1435000	No, no, no.
1435000	1437000	You're right, it is an important part of the conversation,
1437000	1439000	but I think it's just, you know,
1439000	1441000	it's not the AI conversation.
1443000	1445000	We don't have time to pull up the video.
1445000	1450000	Lucas B. Wald makes a heasty exit without saying goodbye
1450000	1453000	and leaves us alone with his head of PR.
1453000	1457000	One last chance to ask how the business treats these contractors,
1457000	1459000	as they call them here.
1459000	1462000	When I was working on this,
1462000	1466000	I found many people complaining, being disconnected.
1466000	1471000	I actually have to go now, too.
1471000	1473000	So it's 11 o'clock.
1473000	1477000	So you don't want to speak about human in the...
1477000	1480000	That's not my role here.
1480000	1482000	All right, I think we're done.
1482000	1484000	So only artificial intelligence, no human?
1484000	1487000	Well, that's what we were prepared for, so, sorry.
1487000	1488000	Okay, it's a pity.
1488000	1491000	To get some answers to our questions about Lucas B. Wald
1491000	1493000	and his views on his workers,
1493000	1495000	we thought we'd try a different tactic.
1496000	1499000	On the day the Figure 8 founder made his statement
1499000	1501000	on disposable workers,
1501000	1503000	there were other entrepreneurs amongst him,
1503000	1507000	as well as a researcher, Lily Irani, just on the right.
1507000	1509000	Ten years after the conference,
1509000	1513000	we find Lily living south of Los Angeles, California.
1537000	1541000	Lily Irani teaches at the University of San Diego,
1541000	1543000	and one of her specialist subjects
1543000	1546000	is the working culture of high-tech business.
1549000	1553000	We're lucky she has a good memory.
1553000	1556000	Do you remember if somebody reacted after this sentence,
1556000	1559000	which is very brutal in a certain way?
1559000	1562000	To be honest, the reaction was nothing.
1562000	1566000	I remember that panel, everyone went up to him to talk to him,
1566000	1568000	and two or three people came up to me
1568000	1572000	to talk about the ethics of this form of labor.
1572000	1576000	This is a room full of highly educated people in San Francisco,
1576000	1578000	and nobody batted an eyelash.
1578000	1580000	How do you explain that?
1582000	1585000	The kinds of people who have access to these spaces
1585000	1589000	are the kinds of people who never worked in a situation
1589000	1591000	where they wondered if they could make rent,
1591000	1594000	or they never worked in a situation where somebody gets sick
1594000	1597000	and they can't pay someone to go take care of them,
1597000	1600000	so they have to kind of take a really bad job at home.
1600000	1604000	And they have no connection to the kinds of situations
1604000	1607000	of the people that are willing to do this work.
1607000	1609000	It's what happens when you go to schools
1609000	1611000	like Stanford and Harvard and Princeton
1611000	1613000	that tell you you're the smartest person
1613000	1615000	and you're going to be a future leader
1615000	1617000	and you've been chosen because you're special,
1617000	1619000	and that you have the power to change the world.
1619000	1622000	A Silicon Valley elite who is out of touch
1622000	1624000	with the rest of the world.
1624000	1627000	This is the key to understanding Lucas B. Wald's logic,
1627000	1629000	although it's not the only part.
1629000	1632000	These workers are invisible by design.
1632000	1635000	You can write code and send your work out,
1635000	1637000	never talk to anyone.
1637000	1640000	It's designed so you can get the work back on a spreadsheet.
1640000	1642000	If you need to, you just see these, you know,
1642000	1645000	letters and numbers identifying the worker.
1645000	1647000	You don't see a name. You don't see where they live.
1647000	1649000	You don't see what their situation is.
1649000	1652000	You don't see, unless you keep track of it yourself,
1652000	1655000	have they worked for you before or not?
1655000	1658000	Do these ghost workers really know who they work for?
1658000	1661000	Have they ever heard of Lucas B. Wald?
1661000	1664000	We showed them the footage of the figure eight founder
1664000	1666000	talking about their work.
1673000	1675000	With technology, you can actually find them,
1675000	1677000	pay them a tiny amount of money,
1677000	1680000	and then get rid of them when you don't need them anymore.
1680000	1682000	You're giggling over and paying people pennies and,
1682000	1685000	yeah, bye-bye.
1685000	1686000	Okay.
1686000	1688000	Now I'm going to start arguing what I do about the AIs
1688000	1691000	when they get me agitated.
1691000	1694000	It's kind of surprising, I guess, a little bit
1694000	1698000	to see there's so openly,
1698000	1704000	openly talking about that view that they have of the workforce.
1704000	1706000	I guess it doesn't really surprise me that much,
1706000	1711000	but, yeah, it definitely kind of sucks,
1711000	1714000	I guess, when they could be paying them a lot more,
1714000	1718000	or at least showing some appreciation,
1718000	1722000	or maybe even some discretion.
1722000	1724000	Basically, saying in person, you know,
1724000	1727000	you hide somebody for 10 minutes and fire them.
1727000	1729000	This way, you don't have to look at the person
1729000	1730000	and you just, goodbye.
1730000	1734000	So that's kind of just, it is kind of,
1734000	1737000	the fact that the head of the company is,
1737000	1739000	people are that disposable,
1739000	1742000	that really isn't right.
1742000	1744000	I don't like that.
1744000	1747000	So I like what I do when I have something to say,
1747000	1748000	and I will say it.
1748000	1755000	So I'm not disposable.
1755000	1759000	Amongst this invisible workforce hiding behind your screen,
1759000	1764000	there are those who feed algorithms for next to nothing.
1764000	1767000	It's the people in charge of tidying up the web,
1767000	1769000	the social media cleaners,
1769000	1773000	who work on sites like Facebook or Instagram.
1773000	1775000	These workers are never mentioned
1775000	1779000	in the slick presentations of the Silicon Valley CEOs.
1779000	1783000	I started building a service to do that,
1783000	1785000	to put people first,
1785000	1788000	and at the center of our experience with technology,
1788000	1792000	because our relationships are what matters most to us.
1792000	1794000	That's how we find meaning
1794000	1798000	and how we make sense of our place in the world.
1798000	1800000	Today, with 2 billion users,
1800000	1804000	Facebook no longer has anything to do with Mark Zuckerberg's
1804000	1810000	initial vision of the site.
1810000	1814000	With violent videos, hate speech, and pornographic images,
1814000	1817000	more and more content has to be deleted.
1817000	1820000	And it isn't always robots doing this job.
1820000	1825000	There are, once again, humans hidden behind the screen.
1825000	1827000	Determining if something is hate speech
1827000	1829000	is very linguistically nuanced.
1829000	1833000	I am optimistic that over a five to ten year period,
1833000	1837000	we will have AI tools that can get into some of the nuances,
1837000	1840000	the linguistic nuances of different types of content
1840000	1842000	to be more accurate in flagging things for our systems,
1842000	1844000	but today we're just not there on that.
1844000	1847000	So a lot of this is still reactive, people flag at us.
1847000	1850000	We have people look at it.
1850000	1854000	These people are in charge of sorting and managing content
1854000	1855000	on the network.
1855000	1860000	Facebook call them content reviewers.
1860000	1861000	According to their site,
1861000	1866000	Facebook has 15,000 workers doing this job across the world,
1866000	1873000	in Ireland, Portugal, the Philippines, and the U.S.
1873000	1875000	We contacted Facebook,
1875000	1884000	but the company refused our request for an interview.
1884000	1888000	So in order to meet these moderators and understand their role,
1888000	1892000	we identified Facebook's main subcontractors,
1892000	1900000	multinationals such as Majoral, Cognizant, or Accenture.
1900000	1903000	We found this job offer for a content reviewer
1903000	1911000	for the French market in Portugal.
1911000	1916000	Gregoire is one of the journalists in our team.
1916000	1925000	He responded to the ad and was offered the job.
1925000	1928000	Before taking off, he received his contract,
1928000	1932000	which included his monthly salary, 800 euros,
1932000	1935000	a little over the minimum wage in Portugal,
1935000	1941000	with a food allowance of 7 euros 63 cents a day.
1941000	1945000	Facebook isn't mentioned once in the document.
1945000	1951000	Even when directly asked, Accenture refused to give the client's name.
1951000	1955000	I was just wondering, now that I took the job,
1955000	1958000	I'm going there, I'm doing it.
1958000	1963000	I was just wondering if I could know the name of the company
1963000	1965000	I'm going to work for.
1965000	1968000	No, we cannot reveal the name yet.
1968000	1971000	It's for a wonderful customer,
1971000	1975000	but we are not allowed to say the name.
1976000	1991000	This is where Gregoire will be working at the Accenture offices in Lisbon.
1991000	1995000	Before getting started, our journalist was sent to a welcome meeting.
1995000	2000000	The footage is a little shaky, as Gregoire is filming with a hidden camera.
2001000	2006000	Hello, I'm having a meeting with Accenture on 9.30.
2006000	2009000	Gregoire isn't the only new employee.
2009000	2012000	12 other people are starting the role at the same time.
2012000	2016000	Another French person along with some Italians and Spaniards.
2016000	2020000	An HR representative is running the welcome meeting.
2020000	2021000	Welcome, you all.
2021000	2024000	My job as career advisor is to help you
2024000	2027000	in all the relationships with Accenture.
2027000	2031000	After the vacation documents and social security paperwork,
2031000	2035000	the small group finally find out which company they are working for.
2035000	2037000	But it's top secret.
2039000	2041000	You must have been told by the recruiting team
2041000	2044000	that you cannot mention that you are working for this project.
2044000	2047000	The client is really very demanding.
2047000	2050000	You cannot mention anyone that you are working for at the Accenture.
2050000	2053000	If someone asks you where you work, you work for Accenture.
2054000	2058000	We still have this code name, they seal.
2058000	2062000	So if I'm talking to some colleague from Accenture, not from this project,
2062000	2064000	and he asks me where do I work,
2064000	2066000	I cannot tell that I work for Facebook.
2066000	2068000	This is not allowed.
2068000	2073000	It's completely confidential that Facebook is working here at this facility.
2075000	2080000	Codenames, confidentiality clauses, and a complete ban on cell phones.
2080000	2085000	Facebook gives you the life of a secret agent for $800 a month.
2085000	2087000	And if you're the chatty type,
2087000	2091000	the following argument should shut you up pretty quickly.
2092000	2096000	You have an agreement and you cannot break that agreement
2096000	2100000	because by law we can punish you by law.
2100000	2102000	It's confidential.
2103000	2108000	Cleaning up social media is a bit like doing your family's dirty laundry.
2108000	2111000	It has to be done, but nobody talks about it.
2113000	2116000	Why so careful? What does the job involve?
2120000	2122000	We continue discreetly with Gregoire.
2130000	2132000	Before becoming a moderator,
2132000	2135000	Gregoire has to follow a three-week training program.
2136000	2139000	Moderating Facebook's content doesn't only involve
2139000	2142000	deleting violent videos or racist jokes.
2142000	2145000	It's a lot more complicated.
2145000	2149000	At the moment, the algorithms can't handle everything.
2149000	2153000	Every decision must be justified using very strict rules.
2153000	2156000	This is what we learn during the training.
2157000	2161000	Every day is dedicated to a different theme during the program.
2161000	2166000	For example, nudity, violent images, or hate speech.
2166000	2170000	On the agenda today, dark humor and jokes and bad taste.
2170000	2176000	We will remove a violation if the person that you see in the image
2176000	2180000	we need to have a real person is visibly affected.
2180000	2183000	If you are making fun of the event,
2183000	2187000	then it's going to be in the market score.
2192000	2197000	What do we do when there's a knock on the event?
2197000	2201000	Here's an example of an inappropriate joke about 9-11.
2203000	2205000	It may seem over the top,
2205000	2208000	but there are dozens of rules like this for each category,
2208000	2211000	which can be difficult to get your head around.
2212000	2214000	Take nudity, for example.
2214000	2218000	Depending on what part of the body you see, or their position,
2218000	2222000	the moderator can't always make the same decision.
2222000	2226000	Here's an example from the exercises to better explain.
2227000	2231000	Gregoire decided to delete this particular photo.
2231000	2235000	But according to Facebook's rules, he was wrong to do so.
2235000	2239000	In the feedback session, the trainer offers this explanation.
2239000	2242000	If we cannot see...
2242000	2247000	If his head is not here, then it's ignored.
2247000	2249000	It's in between her boobs.
2249000	2253000	So if I don't see directly the contact with the nipple, it's nothing.
2253000	2258000	You know, that's exactly why I am having so much trouble to understand things.
2258000	2262000	You have an artistic picture of a photograph of a woman
2262000	2266000	and you show a tiny nipple on it.
2266000	2272000	On one hand, this is a delete because we have 100% uncovered nipples.
2272000	2276000	On the other hand, you have this almost porn photo.
2276000	2279000	And you don't delete because it doesn't feed the world.
2279000	2281000	That's exactly why I...
2281000	2287000	Yes, but you have a small problem because you're still going from what you think
2287000	2289000	in your decisions.
2289000	2295000	And we're in school to learn rules.
2296000	2301000	Applying Facebook's rules without questioning them is the number one rule.
2301000	2305000	A principle that will be drilled into you all day, every day.
2305000	2310000	There has to be a line and they drill it around that.
2310000	2315000	We just need to respect it and we just need to apply it to do our jobs.
2315000	2319000	Sometimes we'll find disagreements, but I mean, this is the good job
2319000	2324000	because this is not my social network experience.
2324000	2333000	A training program with the end goal of turning you into a machine.
2333000	2339000	Pedro worked for six months as a content reviewer for Facebook at Accenture.
2339000	2345000	He agreed to respond to our questions, but only if he remained anonymous.
2345000	2353000	Two years after leaving the company, he still remembers the numbing side of the rule.
2353000	2358000	You have to play by their game or else you won't have a job at the end of the month.
2358000	2362000	And it's got two points where I just felt I was a robot
2362000	2367000	and just doing as many pictures and videos as much as possible
2367000	2371000	just because that's the only thing I can do.
2371000	2374000	You're just there with numbers and clicking enter.
2374000	2376000	Numbers, enter, numbers, enter.
2376000	2384000	The hardest thing for Pedro is trying to forget everything that he saw on that screen over six months.
2384000	2388000	We're not prepared for it. We're not mentally prepared for it.
2388000	2392000	All these stuff, they don't really give us the input before
2392000	2394000	and it just comes to you as a shock.
2394000	2397000	It just comes to you as like a wave here, have this in front of you
2397000	2400000	and you can't really say yes or no to it.
2400000	2404000	If you give me a million euros, a billion euros, I wouldn't go.
2404000	2406000	It's not for me.
2414000	2420000	What Pedro described to us, the wave of shock that washes over you unexpectedly
2420000	2422000	is exactly what happened to Grégoire.
2422000	2427000	It started around the fifth day of training during the practical exercises.
2434000	2437000	A stream of horrific images
2437000	2444000	and unbearable videos that must be watched closely in order to make the right decision
2444000	2447000	according to Facebook's criteria.
2456000	2461000	The same horrific scenes are unfolding on his neighbor's screen too.
2464000	2466000	I'm going to open the window.
2469000	2471000	Excuse me.
2471000	2473000	I'll take a glass on the other side.
2484000	2486000	I'm not far from Vominier.
2486000	2491000	I just took a break because I saw the bodies of the members,
2491000	2494000	but the people who threw themselves from the top of the tower,
2494000	2496000	they were crushed by the ground.
2496000	2500000	The nose, the body that trembled, the hands that trembled,
2500000	2503000	it was really close to today.
2507000	2512000	It's like this on a daily basis for Grégoire and his group.
2512000	2517000	Luckily, they can always rely on the useful advice of the trainers to feel better.
2522000	2525000	If the macarena isn't quite enough to cheer you up,
2525000	2530000	the business also has psychologists available for the most traumatized moderators.
2533000	2536000	On this day, a video lasting several minutes
2536000	2540000	brought the violence to another level for Grégoire.
2552000	2556000	During the break, everyone tries to shake off the shock
2556000	2560000	by discussing the grim video they've just witnessed.
2581000	2607000	Grégoire realizes the extent of the damage this job can cause
2607000	2611000	when talking with a former moderator who is now a trainer.
2637000	2639000	I know that I have to be like this.
2639000	2642000	I can't watch people running across the street.
2642000	2643000	Anyone.
2643000	2647000	You're still doing this while you have PTSD?
2647000	2648000	There is a purpose.
2648000	2652000	I do feel every day like I'm cleaning the trash.
2652000	2653000	Right.
2655000	2658000	Okay, I will watch it, but at least I know that I'm going to watch it.
2658000	2662000	Someone who's 14 years old is going to quit that and not know.
2668000	2670000	Even two years after quitting the post,
2670000	2675000	Pedro still has very vivid memories of certain videos.
2677000	2679000	There's a few things that I saw.
2679000	2682000	Those things are going to stay with me
2682000	2685000	because I remember them as if it was yesterday,
2685000	2688000	it's very emotional sometimes.
2688000	2690000	I remember sometimes people used to like,
2690000	2693000	they were working, being productive,
2693000	2696000	and suddenly they just stand up and run out of the room.
2697000	2698000	That's okay.
2698000	2700000	Trauma built up.
2700000	2704000	And for Pedro, left him feeling helpless.
2704000	2706000	But if you see someone getting murdered,
2706000	2708000	the only action you take is the lead, for example.
2708000	2711000	You just erase it out of the platform.
2711000	2716000	You don't really go into depth of like calling the police, for example.
2716000	2720000	It's like, you never really feel content with what you're doing.
2720000	2723000	You're just going round and round in circles
2723000	2726000	and just like bombard with all this stuff.
2726000	2730000	It's like a mixture of emotions that you go through in one day,
2730000	2733000	eight hours for it.
2733000	2735000	How many were you when you started?
2735000	2737000	We were 30 when we started, 30.
2737000	2742000	From that 30, it started just decreasing month by month
2742000	2745000	until now there's only like three people.
2747000	2751000	Pedro claims that a lot of people struggle to deal with the role
2751000	2753000	and end up quitting.
2753000	2756000	To understand what Pedro went through
2756000	2760000	and what Grégoix and his colleagues are currently experiencing,
2760000	2763000	we met up with a psychiatrist.
2763000	2766000	Professor Thierry Boubet is a specialist
2766000	2768000	in post-traumatic stress disorder.
2768000	2771000	For example, he works with police officers
2771000	2774000	who have been involved in terrorist attacks.
2774000	2777000	We show him the footage we filmed.
2778000	2782000	Some potentially traumatic images,
2782000	2784000	like the ones described here,
2784000	2786000	can have several effects.
2786000	2789000	For some people, it's just anxiety effects.
2789000	2792000	It can make you anxious for a while,
2792000	2794000	sometimes in an important way,
2794000	2797000	with panic attacks or something like that.
2797000	2800000	But in some cases,
2800000	2804000	there can be what we call a traumatic infraction,
2804000	2807000	i.e. one of these images
2807000	2810000	or some of these images
2810000	2815000	will go deeper into us
2815000	2818000	and come back to us without any stress.
2818000	2822000	What's special about post-traumatic stress disorder
2822000	2825000	is that when these images come back without any stress,
2825000	2828000	they produce without any stress the same stress.
2828000	2831000	So it's a stress that lasts
2831000	2837000	and it doesn't disappear, if you like.
2837000	2840000	We also talk to him about the famous
2840000	2844000	confidentiality classes imposed by Facebook.
2844000	2847000	The secret culture and the interdiction
2847000	2850000	that is made in talking to third parties
2850000	2856000	is a process that belongs to the records
2856000	2859000	of what we call mental imprisonment.
2859000	2862000	Mental imprisonment is something
2862000	2865000	that is used by different movements,
2865000	2868000	like, for example, sector movements,
2868000	2872000	and it makes it even more vulnerable
2872000	2875000	to traumatic impacts.
2876000	2879000	Anxiety, trauma, stress.
2879000	2883000	Cleaning up social media comes at a great cost.
2884000	2888000	Grégoire decides to quit only two weeks later,
2888000	2891000	still in his training period.
2895000	2898000	He received his paycheck just before leaving,
2898000	2901000	his hourly pay written at the top,
2901000	2904000	four euros, 62 cents gross.
2904000	2908000	This is a tough pill to swallow for his colleague.
2910000	2913000	I was earning more in the ice cream shop.
2913000	2915000	In the ice cream shop?
2915000	2918000	Man, that's bad, right?
2919000	2923000	After our experience there, we contacted Accenture.
2923000	2925000	Their response was a brief email
2925000	2928000	that didn't once reference Facebook.
2928000	2930000	It did, however, contain this phrase,
2930000	2934000	the well-being of our employees is our priority.
2936000	2939000	To finish our tour of the Internet's trash cleaners,
2939000	2942000	the invisible workforce behind your Facebook
2942000	2946000	or Instagram feed, we had one last meeting.
2946000	2949000	Sarah Roberts is the leading researcher
2949000	2952000	specializing in those who work as moderators.
2952000	2955000	She is a key figure in this field.
2955000	2959000	We met her at the university where she teaches in California.
2959000	2962000	She presented us with an analysis of the rise
2962000	2966000	and development of content moderation over the past year.
2967000	2973000	We are talking about a scope and a scale of magnitude
2973000	2975000	that has not been seen before.
2975000	2978000	Billions of things shared per day on Facebook.
2978000	2982000	Hundreds of hours of video uploaded to YouTube
2982000	2985000	per minute per day and so on.
2985000	2987000	The response has continued to be,
2987000	2989000	we'll put more content moderators on it,
2989000	2993000	which means that that continues to exponentially grow.
2993000	2998000	It has gone from a next to nothing kind of line item
2998000	3003000	in the budget to being a massive, massive cost center,
3003000	3005000	meaning it doesn't actually return revenue.
3005000	3007000	It's not like a new product.
3007000	3010000	It's just seen as an economic drain.
3010000	3012000	And the way we manage that problem
3012000	3015000	is by pushing it onto some low-wage workers
3015000	3017000	to do it as cheaply as possible,
3017000	3019000	because, again, that stacks up
3019000	3022000	when you double your workforce in two years
3022000	3024000	that it does not come for free.
3024000	3028000	This is why companies like Facebook use subcontractors.
3028000	3030000	But according to this researcher,
3030000	3032000	this isn't the only reason.
3032000	3033000	It's about labor costs,
3033000	3036000	but it's also about creating layers
3036000	3039000	of lessening responsibility
3039000	3041000	between those who solicit this kind of work
3041000	3045000	and need it and those who do it and where they do it.
3045000	3046000	They remove themselves,
3046000	3048000	they put themselves at a distance
3048000	3050000	from the workers and their conditions,
3050000	3052000	and it's not just a geographic distance,
3052000	3054000	but sort of a moral distance.
3054000	3057000	So when that content moderator some years later alleges harm
3057000	3060000	or, you know, is having trouble psychologically
3060000	3062000	or emotionally because of the work that they did,
3062000	3065000	then it may be possible for that company
3065000	3067000	to disclaim responsibility for that,
3067000	3070000	even though ultimately they really are responsible
3070000	3072000	because they asked them to do that work in the first place.
3072000	3074000	Despite these precautions,
3074000	3077000	three former moderators filed lawsuits
3077000	3080000	against Facebook in the U.S. a few months ago.
3080000	3083000	All three were working under subcontractors,
3083000	3087000	all claimed to be victims of post-traumatic stress disorder.
3087000	3090000	The American company refused every request
3090000	3092000	we made for an interview.
3092000	3095000	They did, however, send us an email to explain
3095000	3097000	how Facebook, with its partners,
3097000	3101000	pays great attention to the well-being of content moderators
3101000	3103000	working on its platform,
3103000	3106000	which is an absolute priority.
3107000	3111000	To finish off, here's some of the latest news from the sector.
3111000	3114000	While these ghost workers are left in the shadows,
3114000	3117000	it's business as usual for the companies
3117000	3119000	working in this new sector.
3119000	3121000	A few weeks after filming,
3121000	3126000	Figure 8's founder sold his company for $300 million.
3126000	3130000	Well, at least now, he has good reason to be happy.
3167000	3170000	Thank you.
