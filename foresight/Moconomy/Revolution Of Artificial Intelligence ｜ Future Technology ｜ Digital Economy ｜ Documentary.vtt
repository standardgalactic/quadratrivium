WEBVTT

00:00.000 --> 00:15.000
Hello, Lionel. It's already seven o'clock.

00:15.000 --> 00:18.000
Wake up. It's twenty twenty seven.

00:18.000 --> 00:22.000
Don't wake up. You have a meeting at nine o'clock this morning.

00:22.000 --> 00:27.000
In twenty twenty seven, Sarah takes care of everything.

00:27.000 --> 00:30.000
For breakfast, what do I make for you?

00:30.000 --> 00:32.000
A glass of chocolate.

00:32.000 --> 00:38.000
Looking at your health data, I advise you to use soy milk instead of avocados.

00:38.000 --> 00:44.000
Sarah is a virtual assistant who knows exactly what's best for you.

00:44.000 --> 00:48.000
For your part of Squash, I selected Emmanuelle.

00:48.000 --> 00:53.000
I crossed your data. This time, you're sure to take it.

00:54.000 --> 00:58.000
For your dinner party tonight, I reserved this restaurant.

00:58.000 --> 01:00.000
It's new and very well-noted.

01:00.000 --> 01:02.000
It's perfect. Thank you.

01:02.000 --> 01:09.000
Everywhere you go, artificial intelligence, like Sarah, predicts your needs and does the work for you.

01:11.000 --> 01:17.000
Hello, Lionel. We're going to start. I've prepared a contract for our last meeting.

01:18.000 --> 01:24.000
With all of these machines working for you, isn't life wonderful in twenty twenty seven?

01:27.000 --> 01:30.000
But let's not get carried away.

01:30.000 --> 01:37.000
Before Sarah changes your life forever, there's another story to tell, one with less special effects.

01:37.000 --> 01:53.000
This story takes place behind the scenes of those businesses who are working to invent our future.

01:53.000 --> 01:59.000
For now, it's hardly this wonderful world where machines are working entirely for mankind.

01:59.000 --> 02:02.000
In fact, you could say it's exactly the opposite.

02:02.000 --> 02:05.000
Humans are involved in every step of the process.

02:05.000 --> 02:12.000
When you're using anything online, but we're sold as this miracle of automation.

02:12.000 --> 02:22.000
Google, Facebook, Amazon, Uber, these digital giants are using a completely invisible workforce to keep their applications running.

02:22.000 --> 02:24.000
There we are.

02:24.000 --> 02:31.000
With technology, you can actually find them, pay them the tiny amount of money, and then get rid of them when you don't need them anymore.

02:32.000 --> 02:38.000
A workforce that is disposable and underpaid.

02:38.000 --> 02:47.000
On a very good day, I could do five dollars an hour. On a really bad day, I could do ten cents an hour.

02:47.000 --> 02:51.000
Is it possible for you to pay less than the American minimum wage?

02:51.000 --> 02:54.000
I'm not sure we want to go in this direction, yeah.

02:54.000 --> 02:59.000
Whilst millions of men and women are training artificial intelligence for next to nothing,

02:59.000 --> 03:05.000
others are being hired and hidden out of sight to clean up social networks.

03:05.000 --> 03:11.000
You must have been told by the recruiting team that you cannot mention that you are working for this project.

03:11.000 --> 03:18.000
We went undercover as one of these web cleaners, working as a content moderator for Facebook.

03:18.000 --> 03:32.000
There are a few things that I saw. Those things are going to stay with me because I remember them as it was yesterday.

03:32.000 --> 03:42.000
To meet the workers hiding behind your screen, we're taking you to the factory of the future, the digital economy's best kept secret.

03:42.000 --> 03:47.000
You know, it's just like a sausage factory. They don't want people to come in to see how the sausage is made.

03:47.000 --> 03:50.000
I mean, I think it's just that simple.

04:04.000 --> 04:11.000
To delve into the mysteries of artificial intelligence, we're heading to the west coast of the U.S.

04:12.000 --> 04:19.000
Here in San Francisco and the Silicon Valley, the world of tomorrow is being developed.

04:19.000 --> 04:34.000
It's the high-tech hub of giants, like Apple, Facebook, YouTube, Uber, Netflix, and Google.

04:34.000 --> 04:42.000
We have a meeting of figure eight, a business specializing in artificial intelligence that primarily works with Google.

04:42.000 --> 04:47.000
The founder, Lucas Biewald, agreed to spend the morning with us.

04:47.000 --> 04:55.000
Hello, Lucas. Nice to meet you. Thank you very much for your time. I know you have a busy schedule. Thank you.

04:55.000 --> 05:04.000
At 38 years old, this Stanford graduate has already worked for the likes of Microsoft and Yahoo before founding his own company.

05:04.000 --> 05:10.000
Once his microphone is on, a quick tour of their startup-style Californian office space.

05:10.000 --> 05:15.000
This is our best dressed in play.

05:15.000 --> 05:18.000
Cool and relaxed.

05:18.000 --> 05:22.000
This is probably our worst dressed in play.

05:27.000 --> 05:32.000
Do you play babyfoot? I think I'm pretty good. I don't know, maybe.

05:32.000 --> 05:37.000
This is kind of our eating area. This is actually where I like to work.

05:37.000 --> 05:41.000
My coffee got cold.

05:41.000 --> 05:45.000
And in the reception area, an impressive display.

05:45.000 --> 05:55.000
These are some of our customers and the different things that they did with our products.

05:55.000 --> 06:03.000
Here's Twitter. We help them remove a lot of people that were kind of bullying on their website.

06:03.000 --> 06:09.000
You know, American Express. Is that in France? Yeah.

06:09.000 --> 06:13.000
You know, I feel especially proud of, you know, something like Tesco, right?

06:13.000 --> 06:22.000
Is able to use us to improve their online website to show better search results so people can find the items that they're looking for.

06:22.000 --> 06:24.000
And I don't see Google.

06:24.000 --> 06:27.000
No, I don't know. Do you know why some of these get up here?

06:27.000 --> 06:33.000
Frankly, we just stopped because it was getting out of hand.

06:33.000 --> 06:36.000
This is Mr. Brown, head of PR.

06:36.000 --> 06:38.000
This is a good scene.

06:38.000 --> 06:45.000
After our visit, the founder explains the enigmatic name, Figure 8.

06:45.000 --> 06:49.000
We call our company Figure 8 because we think of it as a loop.

06:49.000 --> 06:55.000
And the loop really has these two parts, right? There's the humans that do the labeling.

06:55.000 --> 06:58.000
And then the machine learning that learns from the humans.

06:58.000 --> 07:02.000
And then it goes back to the humans for more labeling, right?

07:02.000 --> 07:06.000
So we think of this kind of like beautiful loop, right?

07:06.000 --> 07:13.000
Where humans do the best things that humans can do and the algorithms, the artificial intelligence, does the best things that the algorithms can do.

07:13.000 --> 07:17.000
And we put that together. And that's why we call it Figure 8.

07:23.000 --> 07:31.000
To get a better understanding of why AI needs humans to function, we stop joking around and get out the computer.

07:32.000 --> 07:34.000
So here's an example.

07:34.000 --> 07:38.000
You know, a lot of people these days are trying to build cars that automatically drive.

07:38.000 --> 07:44.000
Like, for example, Tesla has a system where you can drive around in a car.

07:44.000 --> 07:49.000
But of course, it's incredibly important that these cars don't run into pedestrians.

07:49.000 --> 07:53.000
So the car camera just sees something like this.

07:53.000 --> 07:58.000
So it's really important that they build reliable systems that can identify people.

07:58.000 --> 08:05.000
And the way that they learn to identify people is looking at lots of pictures of what the car is seeing from the camera.

08:05.000 --> 08:08.000
And then actually literally labeling where the people are.

08:09.000 --> 08:12.000
Here's a real example of how it works.

08:12.000 --> 08:18.000
If you want to teach a self-driving car to recognize a pedestrian, a human like you or I,

08:18.000 --> 08:25.000
it first has to identify pedestrians from photos and then feed this information to the AI.

08:25.000 --> 08:33.000
And this process has to be done over a thousand, even a million times over, which can be very time-consuming.

08:38.000 --> 08:44.000
This is where Figure 8 gets involved, using real people who are paid to do this work.

08:46.000 --> 08:53.000
So the task here is to look at this picture and then label where the people are.

08:53.000 --> 08:54.000
And so you get paid for this?

08:54.000 --> 08:57.000
You get paid to draw boxes around the people.

08:57.000 --> 08:58.000
How much?

08:58.000 --> 09:07.000
I'm not sure this task, but maybe it would be like ten cents per person that you draw a box around.

09:07.000 --> 09:13.000
Who do this job? Do you have employees doing these jobs and labeling people?

09:13.000 --> 09:18.000
Yes, so it's contractors in our network that log in and do these jobs.

09:18.000 --> 09:22.000
What do you mean by contractors on your network? What kind of people?

09:22.000 --> 09:28.000
So it's like people that log into this and then want to work on these tasks.

09:28.000 --> 09:32.000
How many people work for Figure 8?

09:32.000 --> 09:34.000
In this capacity as labellers?

09:34.000 --> 09:35.000
Yeah.

09:35.000 --> 09:38.000
So again, people can kind of come and go if they want to.

09:38.000 --> 09:47.000
So there's maybe around 100,000 people that kind of consistently work every day for certain use cases that we have.

09:47.000 --> 09:53.000
But then there's also millions of people that log in from time to time and work on tasks.

09:53.000 --> 09:56.000
And where do those people live?

09:56.000 --> 09:58.000
They live all over the world actually.

09:58.000 --> 10:03.000
So they live all over America and then they live all over the world.

10:06.000 --> 10:12.000
So who are these millions of people who are being paid to train AI technology?

10:13.000 --> 10:19.000
In order to meet these contractors, as Figure 8 calls them,

10:19.000 --> 10:25.000
we leave Silicon Valley and head 500 miles north of San Francisco in Oregon.

10:32.000 --> 10:33.000
There we are.

10:33.000 --> 10:35.000
Ah, success.

10:36.000 --> 10:40.000
Jared Mansfield signed up to Figure 8 three years ago.

10:40.000 --> 10:44.000
He now spends several hours a week working for them.

10:44.000 --> 10:49.000
Every day, the company offers a list of tasks that he can complete for money.

10:49.000 --> 10:52.000
For example, training search engines.

10:59.000 --> 11:02.000
For this first one, it's showing examples of how to do it.

11:02.000 --> 11:05.000
The query is mac and cheese pierogies.

11:05.000 --> 11:09.000
And the two results are Annie's homegrown organic mac and cheese

11:09.000 --> 11:14.000
and Annie's really cheddar microwavable macaroni and cheese, which are neither of them are pierogies.

11:14.000 --> 11:17.000
So it's saying that one would be equally bad matches.

11:17.000 --> 11:19.000
What's the use of doing that?

11:19.000 --> 11:25.000
A lot of it, I think, is to train search algorithms.

11:25.000 --> 11:29.000
So like when someone sits at their computer and types a product,

11:29.000 --> 11:34.000
the algorithm will be able to determine with more accuracy

11:34.000 --> 11:37.000
what product it is that that person is looking for.

11:41.000 --> 11:46.000
For every ten answers, Jared earns less than one cent.

11:46.000 --> 11:52.000
To get an idea of how much money he can make, we leave him to work for 30 minutes.

11:52.000 --> 11:57.000
He's answered 180 questions over the course of half an hour.

11:59.000 --> 12:01.000
How much have you earned?

12:01.000 --> 12:02.000
15 cents.

12:02.000 --> 12:03.000
For how long?

12:03.000 --> 12:05.000
A half hour.

12:05.000 --> 12:08.000
Which would be 30 cents per hour?

12:08.000 --> 12:14.000
Yeah, which are pretty definitely not a livable wage, that's for sure.

12:14.000 --> 12:16.000
Do they have the right to do this?

12:16.000 --> 12:19.000
I mean, they have the right to do whatever they want.

12:19.000 --> 12:24.000
I'm the one coming to them for little tiny bits of coins on this website.

12:24.000 --> 12:30.000
And there's no contract between me and them.

12:33.000 --> 12:37.000
No contract, no salary, no guaranteed minimum wage.

12:37.000 --> 12:41.000
These ghost workers are paid to train software and robots

12:41.000 --> 12:45.000
using only one rule, supply and demand.

12:46.000 --> 12:50.000
It definitely feels like I'm part of this invisible workforce

12:50.000 --> 12:55.000
that is kind of made up of just random people throughout the world.

12:55.000 --> 13:04.000
And together we're kind of training what's going to replace the workforce as a whole eventually.

13:04.000 --> 13:08.000
Jared is very philosophical about the idea.

13:08.000 --> 13:10.000
Still, he can afford to be.

13:10.000 --> 13:15.000
To earn a real living, he has another job selling chicken in the supermarket

13:15.000 --> 13:18.000
for a little more than $1,500 a month.

13:18.000 --> 13:23.000
Figure 8 is just what he does on the side to earn a little extra cash.

13:30.000 --> 13:35.000
After leaving Oregon, we decided to take advantage of what we'd learned in America

13:35.000 --> 13:41.000
and sign ourselves up to Figure 8 to train artificial intelligence.

13:41.000 --> 13:48.000
On the site's welcome page, small tasks are proposed at 1, 2, or 12 cents.

13:51.000 --> 13:58.000
We chose this as our first task, drawing boxes around objects in images.

13:58.000 --> 14:02.000
Following the instructions, it took us a couple of minutes

14:02.000 --> 14:06.000
to draw around 10 objects and earn 2 cents.

14:06.000 --> 14:12.000
On the list of tasks, Figure 8 also offers evaluations of search engine answers,

14:12.000 --> 14:14.000
Jared's task of choice.

14:14.000 --> 14:18.000
We could also listen to conversations and confirm if the recording

14:18.000 --> 14:23.000
features a man or a woman's voice, and if they are speaking English.

14:23.000 --> 14:26.000
Hi, is James there, please?

14:27.000 --> 14:30.000
We work for our team,

14:31.000 --> 14:36.000
we work for hours without ever earning more than 30 cents an hour.

14:41.000 --> 14:48.000
It's difficult to imagine that there are people who work on these tasks on a full-time basis.

14:51.000 --> 14:57.000
We're in Maine, on the east coast of the United States, close to the Canadian border.

14:57.000 --> 15:00.000
We've arranged to meet with one of the NET's ghost workers,

15:00.000 --> 15:03.000
the human side of the Figure 8 loop.

15:06.000 --> 15:10.000
Her name is Don Carbone, she is 46 years old.

15:12.000 --> 15:13.000
Bonjour.

15:15.000 --> 15:16.000
Hello.

15:16.000 --> 15:17.000
Hello.

15:18.000 --> 15:19.000
Nice to meet you.

15:19.000 --> 15:21.000
Thank you so much for your welcome.

15:21.000 --> 15:22.000
Beautiful.

15:22.000 --> 15:28.000
Oh, we had a blizzard not that long ago, and then we got more.

15:28.000 --> 15:31.000
And it's also, I think, negative seven out there.

15:32.000 --> 15:34.000
Don is a single mother.

15:34.000 --> 15:36.000
She lives here with three of her children.

15:41.000 --> 15:45.000
This is what subsidized housing looks like up here.

15:45.000 --> 15:49.000
I mean, it's not bad for public housing.

15:50.000 --> 15:56.000
She lives and works here, working on the Figure 8 site all day.

16:01.000 --> 16:05.000
I'll turn it on, like I said, right before seven o'clock.

16:07.000 --> 16:09.000
Get the initial stuff done.

16:09.000 --> 16:15.000
I'll turn this off at three o'clock in the afternoon and then turn it back on at nine o'clock at night.

16:15.000 --> 16:20.000
So, I'll say eight hours minimum.

16:22.000 --> 16:24.000
I bust my butt though.

16:25.000 --> 16:27.000
Like this would be the dashboard.

16:27.000 --> 16:31.000
And you can see I've done 6,445 tasks.

16:31.000 --> 16:33.000
Since when?

16:33.000 --> 16:35.000
Three years.

16:35.000 --> 16:36.000
See these different badges?

16:36.000 --> 16:37.000
Yeah.

16:37.000 --> 16:39.000
You start off, you have no badge.

16:39.000 --> 16:42.000
And you have to do so many questions and get so many right.

16:42.000 --> 16:44.000
And then you get your first level badge.

16:44.000 --> 16:51.000
And then when you get to level three, you have access to virtually all the tasks that are put up.

16:53.000 --> 16:55.000
What is your level right now?

16:55.000 --> 16:56.000
Right now, oh, I'm on level three.

16:56.000 --> 16:58.000
I've been level three.

16:58.000 --> 17:01.000
I've been level three for quite a while.

17:01.000 --> 17:05.000
Don is considered a high performing worker.

17:05.000 --> 17:08.000
Figure 8 therefore offers her more work than a beginner.

17:08.000 --> 17:11.000
But it isn't necessarily more interesting.

17:13.000 --> 17:19.000
I have to put bounding boxes around people.

17:22.000 --> 17:25.000
I'm not really keen on this job.

17:29.000 --> 17:35.000
The biggest problem is trying to find jobs that are viable.

17:35.000 --> 17:38.000
And right now, I don't have many.

17:39.000 --> 17:42.000
And it's definitely not better paid.

17:44.000 --> 17:47.000
On a very good day, I could do $5 an hour.

17:47.000 --> 17:51.000
On a really bad day, I could do $0.10 an hour.

17:51.000 --> 17:58.000
I mean, I have had some really, really good days until February.

17:58.000 --> 17:59.000
Yeah.

17:59.000 --> 18:04.000
Do you think this is a fair payment for what you're doing?

18:04.000 --> 18:07.000
No, no, no, no.

18:07.000 --> 18:09.000
Not at all.

18:09.000 --> 18:12.000
But I live in Northern Maine.

18:12.000 --> 18:14.000
We get a lot of snow.

18:14.000 --> 18:18.000
There's a very low job market.

18:18.000 --> 18:21.000
And it helps me as a stay-at-home mom.

18:21.000 --> 18:24.000
It helps with added income.

18:30.000 --> 18:33.000
Don prefers to work from home because her youngest daughter,

18:33.000 --> 18:35.000
Jane, has autism.

18:36.000 --> 18:38.000
Here you go.

18:38.000 --> 18:40.000
What happened?

18:40.000 --> 18:42.000
Don wants to be there to take care of her

18:42.000 --> 18:46.000
when she gets home from school at 3 p.m.

18:46.000 --> 18:48.000
So how was school?

18:48.000 --> 18:50.000
Good day or bad day?

18:50.000 --> 18:53.000
Really a good day?

18:53.000 --> 18:56.000
With her autism, I always have to be ready to jump in my car

18:56.000 --> 18:58.000
and go get her from school.

18:58.000 --> 19:01.000
I mean, it could happen one day out of the week,

19:01.000 --> 19:05.000
or not at all, or three days out of the week.

19:05.000 --> 19:07.000
And the school is very understanding.

19:07.000 --> 19:11.000
So I mean, I have to take out the whole week

19:11.000 --> 19:14.000
if I was working out of the home.

19:14.000 --> 19:19.000
Don receives $750 in government aid every month,

19:19.000 --> 19:22.000
which isn't enough to cover all of her bills.

19:22.000 --> 19:25.000
This is why she signed up to Figure 8.

19:25.000 --> 19:28.000
By working eight hours a day and five days a week,

19:28.000 --> 19:33.000
she says she earns, on average, $250 a month on the site.

19:44.000 --> 19:47.000
On Figure 8, the pay is non-negotiable.

19:47.000 --> 19:49.000
If you refuse the work,

19:49.000 --> 19:52.000
there will always be someone else to take it.

19:54.000 --> 19:57.000
There is an unlimited supply of these ghost workers,

19:57.000 --> 19:59.000
coming from all over the world.

19:59.000 --> 20:03.000
It's probably why Lucas B. Wald is so happy.

20:03.000 --> 20:08.000
But he isn't the only one to take advantage of this phenomenon.

20:10.000 --> 20:13.000
Various other businesses propose these sorts of repetitive

20:13.000 --> 20:16.000
and underpaid online tasks,

20:16.000 --> 20:21.000
the biggest amongst them being ClickWorker and Amazon Mechanical Turk,

20:21.000 --> 20:25.000
a platform provided by Amazon and its boss, Jeff Bezos,

20:25.000 --> 20:28.000
who invented the concept in 2005.

20:31.000 --> 20:34.000
Micro-working is a growing concern for the ILO,

20:34.000 --> 20:37.000
the International Labour Organization,

20:37.000 --> 20:42.000
a UN agency in charge of protecting workers' rights across the globe.

20:49.000 --> 20:53.000
Jeanine Berg is the resident expert on this subject at the ILO,

20:53.000 --> 20:55.000
who speaks to us through Skype.

21:23.000 --> 21:28.000
And the technology has facilitated this, and it's cheap.

21:28.000 --> 21:31.000
That's us, the main advantage.

21:32.000 --> 21:34.000
Jeanine Berg wrote a report,

21:34.000 --> 21:39.000
calculating that micro-workers earn, on average, $3.31 an hour,

21:39.000 --> 21:41.000
without any rights in return.

21:42.000 --> 21:47.000
Workers' extreme vulnerability is the key to Lucas B. Wald's business model.

21:48.000 --> 21:50.000
After months of investigations,

21:50.000 --> 21:56.000
we found this video from 2010 that sums up his view of the labour force.

21:59.000 --> 22:03.000
Before the internet, it would be really difficult to find someone,

22:03.000 --> 22:05.000
sit them down for 10 minutes and get them to work for you,

22:05.000 --> 22:07.000
and then fire them after those 10 minutes.

22:07.000 --> 22:09.000
But with technology, you can actually find them,

22:09.000 --> 22:11.000
pay them a tiny amount of money,

22:11.000 --> 22:14.000
and then get rid of them when you don't need them anymore.

22:18.000 --> 22:20.000
While we were interviewing him,

22:20.000 --> 22:24.000
we wanted to ask him if he still shared the same opinion.

22:25.000 --> 22:28.000
But when we start talking about work conditions,

22:28.000 --> 22:32.000
the figure-eight founder seemed to lose his sense of humour.

22:32.000 --> 22:37.000
Do you have an idea of the average revenue per hour of your contributor?

22:37.000 --> 22:38.000
You know, I'm not sure.

22:38.000 --> 22:41.000
It's totally dependent on the task that someone puts in,

22:41.000 --> 22:43.000
and it's hard to track time on the internet

22:43.000 --> 22:45.000
because people can walk away from their computer and come back,

22:45.000 --> 22:49.000
so I don't know how much people don't really make.

22:49.000 --> 22:52.000
There was a report on ILO saying that on average,

22:52.000 --> 22:59.000
the people working on crowdsourcing were paid $3.31 an hour.

22:59.000 --> 23:02.000
Would that be consistent with what you pay?

23:03.000 --> 23:05.000
Again, I'm not sure.

23:06.000 --> 23:10.000
Is it possible for you to pay less than the American minimum wage?

23:11.000 --> 23:13.000
It could be possible.

23:13.000 --> 23:15.000
So this is legal?

23:19.000 --> 23:21.000
I'm not sure we want to go in this direction.

23:21.000 --> 23:24.000
Can we take this a different direction?

23:24.000 --> 23:27.000
I'd rather this focus on more AI than anything.

23:27.000 --> 23:29.000
Yeah, but this is the whole thing.

23:29.000 --> 23:31.000
This is about crowdsourcing as well,

23:31.000 --> 23:34.000
so I have to ask questions on crowdsourcing.

23:34.000 --> 23:38.000
I prepped him for more of an AI conversation

23:38.000 --> 23:41.000
than a crowdsourcing conversation.

23:41.000 --> 23:45.000
No, I don't really want to do this.

23:45.000 --> 23:49.000
Yeah, we can find someone else to talk about this stuff.

23:49.000 --> 23:53.000
Okay, so you're not comfortable with this part of the discussion?

23:53.000 --> 23:55.000
No, no, no.

23:55.000 --> 23:57.000
You're right, it is an important part of the conversation,

23:57.000 --> 23:59.000
but I think it's just, you know,

23:59.000 --> 24:01.000
it's not the AI conversation.

24:03.000 --> 24:05.000
We don't have time to pull up the video.

24:05.000 --> 24:10.000
Lucas B. Wald makes a heasty exit without saying goodbye

24:10.000 --> 24:13.000
and leaves us alone with his head of PR.

24:13.000 --> 24:17.000
One last chance to ask how the business treats these contractors,

24:17.000 --> 24:19.000
as they call them here.

24:19.000 --> 24:22.000
When I was working on this,

24:22.000 --> 24:26.000
I found many people complaining, being disconnected.

24:26.000 --> 24:31.000
I actually have to go now, too.

24:31.000 --> 24:33.000
So it's 11 o'clock.

24:33.000 --> 24:37.000
So you don't want to speak about human in the...

24:37.000 --> 24:40.000
That's not my role here.

24:40.000 --> 24:42.000
All right, I think we're done.

24:42.000 --> 24:44.000
So only artificial intelligence, no human?

24:44.000 --> 24:47.000
Well, that's what we were prepared for, so, sorry.

24:47.000 --> 24:48.000
Okay, it's a pity.

24:48.000 --> 24:51.000
To get some answers to our questions about Lucas B. Wald

24:51.000 --> 24:53.000
and his views on his workers,

24:53.000 --> 24:55.000
we thought we'd try a different tactic.

24:56.000 --> 24:59.000
On the day the Figure 8 founder made his statement

24:59.000 --> 25:01.000
on disposable workers,

25:01.000 --> 25:03.000
there were other entrepreneurs amongst him,

25:03.000 --> 25:07.000
as well as a researcher, Lily Irani, just on the right.

25:07.000 --> 25:09.000
Ten years after the conference,

25:09.000 --> 25:13.000
we find Lily living south of Los Angeles, California.

25:37.000 --> 25:41.000
Lily Irani teaches at the University of San Diego,

25:41.000 --> 25:43.000
and one of her specialist subjects

25:43.000 --> 25:46.000
is the working culture of high-tech business.

25:49.000 --> 25:53.000
We're lucky she has a good memory.

25:53.000 --> 25:56.000
Do you remember if somebody reacted after this sentence,

25:56.000 --> 25:59.000
which is very brutal in a certain way?

25:59.000 --> 26:02.000
To be honest, the reaction was nothing.

26:02.000 --> 26:06.000
I remember that panel, everyone went up to him to talk to him,

26:06.000 --> 26:08.000
and two or three people came up to me

26:08.000 --> 26:12.000
to talk about the ethics of this form of labor.

26:12.000 --> 26:16.000
This is a room full of highly educated people in San Francisco,

26:16.000 --> 26:18.000
and nobody batted an eyelash.

26:18.000 --> 26:20.000
How do you explain that?

26:22.000 --> 26:25.000
The kinds of people who have access to these spaces

26:25.000 --> 26:29.000
are the kinds of people who never worked in a situation

26:29.000 --> 26:31.000
where they wondered if they could make rent,

26:31.000 --> 26:34.000
or they never worked in a situation where somebody gets sick

26:34.000 --> 26:37.000
and they can't pay someone to go take care of them,

26:37.000 --> 26:40.000
so they have to kind of take a really bad job at home.

26:40.000 --> 26:44.000
And they have no connection to the kinds of situations

26:44.000 --> 26:47.000
of the people that are willing to do this work.

26:47.000 --> 26:49.000
It's what happens when you go to schools

26:49.000 --> 26:51.000
like Stanford and Harvard and Princeton

26:51.000 --> 26:53.000
that tell you you're the smartest person

26:53.000 --> 26:55.000
and you're going to be a future leader

26:55.000 --> 26:57.000
and you've been chosen because you're special,

26:57.000 --> 26:59.000
and that you have the power to change the world.

26:59.000 --> 27:02.000
A Silicon Valley elite who is out of touch

27:02.000 --> 27:04.000
with the rest of the world.

27:04.000 --> 27:07.000
This is the key to understanding Lucas B. Wald's logic,

27:07.000 --> 27:09.000
although it's not the only part.

27:09.000 --> 27:12.000
These workers are invisible by design.

27:12.000 --> 27:15.000
You can write code and send your work out,

27:15.000 --> 27:17.000
never talk to anyone.

27:17.000 --> 27:20.000
It's designed so you can get the work back on a spreadsheet.

27:20.000 --> 27:22.000
If you need to, you just see these, you know,

27:22.000 --> 27:25.000
letters and numbers identifying the worker.

27:25.000 --> 27:27.000
You don't see a name. You don't see where they live.

27:27.000 --> 27:29.000
You don't see what their situation is.

27:29.000 --> 27:32.000
You don't see, unless you keep track of it yourself,

27:32.000 --> 27:35.000
have they worked for you before or not?

27:35.000 --> 27:38.000
Do these ghost workers really know who they work for?

27:38.000 --> 27:41.000
Have they ever heard of Lucas B. Wald?

27:41.000 --> 27:44.000
We showed them the footage of the figure eight founder

27:44.000 --> 27:46.000
talking about their work.

27:53.000 --> 27:55.000
With technology, you can actually find them,

27:55.000 --> 27:57.000
pay them a tiny amount of money,

27:57.000 --> 28:00.000
and then get rid of them when you don't need them anymore.

28:00.000 --> 28:02.000
You're giggling over and paying people pennies and,

28:02.000 --> 28:05.000
yeah, bye-bye.

28:05.000 --> 28:06.000
Okay.

28:06.000 --> 28:08.000
Now I'm going to start arguing what I do about the AIs

28:08.000 --> 28:11.000
when they get me agitated.

28:11.000 --> 28:14.000
It's kind of surprising, I guess, a little bit

28:14.000 --> 28:18.000
to see there's so openly,

28:18.000 --> 28:24.000
openly talking about that view that they have of the workforce.

28:24.000 --> 28:26.000
I guess it doesn't really surprise me that much,

28:26.000 --> 28:31.000
but, yeah, it definitely kind of sucks,

28:31.000 --> 28:34.000
I guess, when they could be paying them a lot more,

28:34.000 --> 28:38.000
or at least showing some appreciation,

28:38.000 --> 28:42.000
or maybe even some discretion.

28:42.000 --> 28:44.000
Basically, saying in person, you know,

28:44.000 --> 28:47.000
you hide somebody for 10 minutes and fire them.

28:47.000 --> 28:49.000
This way, you don't have to look at the person

28:49.000 --> 28:50.000
and you just, goodbye.

28:50.000 --> 28:54.000
So that's kind of just, it is kind of,

28:54.000 --> 28:57.000
the fact that the head of the company is,

28:57.000 --> 28:59.000
people are that disposable,

28:59.000 --> 29:02.000
that really isn't right.

29:02.000 --> 29:04.000
I don't like that.

29:04.000 --> 29:07.000
So I like what I do when I have something to say,

29:07.000 --> 29:08.000
and I will say it.

29:08.000 --> 29:15.000
So I'm not disposable.

29:15.000 --> 29:19.000
Amongst this invisible workforce hiding behind your screen,

29:19.000 --> 29:24.000
there are those who feed algorithms for next to nothing.

29:24.000 --> 29:27.000
It's the people in charge of tidying up the web,

29:27.000 --> 29:29.000
the social media cleaners,

29:29.000 --> 29:33.000
who work on sites like Facebook or Instagram.

29:33.000 --> 29:35.000
These workers are never mentioned

29:35.000 --> 29:39.000
in the slick presentations of the Silicon Valley CEOs.

29:39.000 --> 29:43.000
I started building a service to do that,

29:43.000 --> 29:45.000
to put people first,

29:45.000 --> 29:48.000
and at the center of our experience with technology,

29:48.000 --> 29:52.000
because our relationships are what matters most to us.

29:52.000 --> 29:54.000
That's how we find meaning

29:54.000 --> 29:58.000
and how we make sense of our place in the world.

29:58.000 --> 30:00.000
Today, with 2 billion users,

30:00.000 --> 30:04.000
Facebook no longer has anything to do with Mark Zuckerberg's

30:04.000 --> 30:10.000
initial vision of the site.

30:10.000 --> 30:14.000
With violent videos, hate speech, and pornographic images,

30:14.000 --> 30:17.000
more and more content has to be deleted.

30:17.000 --> 30:20.000
And it isn't always robots doing this job.

30:20.000 --> 30:25.000
There are, once again, humans hidden behind the screen.

30:25.000 --> 30:27.000
Determining if something is hate speech

30:27.000 --> 30:29.000
is very linguistically nuanced.

30:29.000 --> 30:33.000
I am optimistic that over a five to ten year period,

30:33.000 --> 30:37.000
we will have AI tools that can get into some of the nuances,

30:37.000 --> 30:40.000
the linguistic nuances of different types of content

30:40.000 --> 30:42.000
to be more accurate in flagging things for our systems,

30:42.000 --> 30:44.000
but today we're just not there on that.

30:44.000 --> 30:47.000
So a lot of this is still reactive, people flag at us.

30:47.000 --> 30:50.000
We have people look at it.

30:50.000 --> 30:54.000
These people are in charge of sorting and managing content

30:54.000 --> 30:55.000
on the network.

30:55.000 --> 31:00.000
Facebook call them content reviewers.

31:00.000 --> 31:01.000
According to their site,

31:01.000 --> 31:06.000
Facebook has 15,000 workers doing this job across the world,

31:06.000 --> 31:13.000
in Ireland, Portugal, the Philippines, and the U.S.

31:13.000 --> 31:15.000
We contacted Facebook,

31:15.000 --> 31:24.000
but the company refused our request for an interview.

31:24.000 --> 31:28.000
So in order to meet these moderators and understand their role,

31:28.000 --> 31:32.000
we identified Facebook's main subcontractors,

31:32.000 --> 31:40.000
multinationals such as Majoral, Cognizant, or Accenture.

31:40.000 --> 31:43.000
We found this job offer for a content reviewer

31:43.000 --> 31:51.000
for the French market in Portugal.

31:51.000 --> 31:56.000
Gregoire is one of the journalists in our team.

31:56.000 --> 32:05.000
He responded to the ad and was offered the job.

32:05.000 --> 32:08.000
Before taking off, he received his contract,

32:08.000 --> 32:12.000
which included his monthly salary, 800 euros,

32:12.000 --> 32:15.000
a little over the minimum wage in Portugal,

32:15.000 --> 32:21.000
with a food allowance of 7 euros 63 cents a day.

32:21.000 --> 32:25.000
Facebook isn't mentioned once in the document.

32:25.000 --> 32:31.000
Even when directly asked, Accenture refused to give the client's name.

32:31.000 --> 32:35.000
I was just wondering, now that I took the job,

32:35.000 --> 32:38.000
I'm going there, I'm doing it.

32:38.000 --> 32:43.000
I was just wondering if I could know the name of the company

32:43.000 --> 32:45.000
I'm going to work for.

32:45.000 --> 32:48.000
No, we cannot reveal the name yet.

32:48.000 --> 32:51.000
It's for a wonderful customer,

32:51.000 --> 32:55.000
but we are not allowed to say the name.

32:56.000 --> 33:11.000
This is where Gregoire will be working at the Accenture offices in Lisbon.

33:11.000 --> 33:15.000
Before getting started, our journalist was sent to a welcome meeting.

33:15.000 --> 33:20.000
The footage is a little shaky, as Gregoire is filming with a hidden camera.

33:21.000 --> 33:26.000
Hello, I'm having a meeting with Accenture on 9.30.

33:26.000 --> 33:29.000
Gregoire isn't the only new employee.

33:29.000 --> 33:32.000
12 other people are starting the role at the same time.

33:32.000 --> 33:36.000
Another French person along with some Italians and Spaniards.

33:36.000 --> 33:40.000
An HR representative is running the welcome meeting.

33:40.000 --> 33:41.000
Welcome, you all.

33:41.000 --> 33:44.000
My job as career advisor is to help you

33:44.000 --> 33:47.000
in all the relationships with Accenture.

33:47.000 --> 33:51.000
After the vacation documents and social security paperwork,

33:51.000 --> 33:55.000
the small group finally find out which company they are working for.

33:55.000 --> 33:57.000
But it's top secret.

33:59.000 --> 34:01.000
You must have been told by the recruiting team

34:01.000 --> 34:04.000
that you cannot mention that you are working for this project.

34:04.000 --> 34:07.000
The client is really very demanding.

34:07.000 --> 34:10.000
You cannot mention anyone that you are working for at the Accenture.

34:10.000 --> 34:13.000
If someone asks you where you work, you work for Accenture.

34:14.000 --> 34:18.000
We still have this code name, they seal.

34:18.000 --> 34:22.000
So if I'm talking to some colleague from Accenture, not from this project,

34:22.000 --> 34:24.000
and he asks me where do I work,

34:24.000 --> 34:26.000
I cannot tell that I work for Facebook.

34:26.000 --> 34:28.000
This is not allowed.

34:28.000 --> 34:33.000
It's completely confidential that Facebook is working here at this facility.

34:35.000 --> 34:40.000
Codenames, confidentiality clauses, and a complete ban on cell phones.

34:40.000 --> 34:45.000
Facebook gives you the life of a secret agent for $800 a month.

34:45.000 --> 34:47.000
And if you're the chatty type,

34:47.000 --> 34:51.000
the following argument should shut you up pretty quickly.

34:52.000 --> 34:56.000
You have an agreement and you cannot break that agreement

34:56.000 --> 35:00.000
because by law we can punish you by law.

35:00.000 --> 35:02.000
It's confidential.

35:03.000 --> 35:08.000
Cleaning up social media is a bit like doing your family's dirty laundry.

35:08.000 --> 35:11.000
It has to be done, but nobody talks about it.

35:13.000 --> 35:16.000
Why so careful? What does the job involve?

35:20.000 --> 35:22.000
We continue discreetly with Gregoire.

35:30.000 --> 35:32.000
Before becoming a moderator,

35:32.000 --> 35:35.000
Gregoire has to follow a three-week training program.

35:36.000 --> 35:39.000
Moderating Facebook's content doesn't only involve

35:39.000 --> 35:42.000
deleting violent videos or racist jokes.

35:42.000 --> 35:45.000
It's a lot more complicated.

35:45.000 --> 35:49.000
At the moment, the algorithms can't handle everything.

35:49.000 --> 35:53.000
Every decision must be justified using very strict rules.

35:53.000 --> 35:56.000
This is what we learn during the training.

35:57.000 --> 36:01.000
Every day is dedicated to a different theme during the program.

36:01.000 --> 36:06.000
For example, nudity, violent images, or hate speech.

36:06.000 --> 36:10.000
On the agenda today, dark humor and jokes and bad taste.

36:10.000 --> 36:16.000
We will remove a violation if the person that you see in the image

36:16.000 --> 36:20.000
we need to have a real person is visibly affected.

36:20.000 --> 36:23.000
If you are making fun of the event,

36:23.000 --> 36:27.000
then it's going to be in the market score.

36:32.000 --> 36:37.000
What do we do when there's a knock on the event?

36:37.000 --> 36:41.000
Here's an example of an inappropriate joke about 9-11.

36:43.000 --> 36:45.000
It may seem over the top,

36:45.000 --> 36:48.000
but there are dozens of rules like this for each category,

36:48.000 --> 36:51.000
which can be difficult to get your head around.

36:52.000 --> 36:54.000
Take nudity, for example.

36:54.000 --> 36:58.000
Depending on what part of the body you see, or their position,

36:58.000 --> 37:02.000
the moderator can't always make the same decision.

37:02.000 --> 37:06.000
Here's an example from the exercises to better explain.

37:07.000 --> 37:11.000
Gregoire decided to delete this particular photo.

37:11.000 --> 37:15.000
But according to Facebook's rules, he was wrong to do so.

37:15.000 --> 37:19.000
In the feedback session, the trainer offers this explanation.

37:19.000 --> 37:22.000
If we cannot see...

37:22.000 --> 37:27.000
If his head is not here, then it's ignored.

37:27.000 --> 37:29.000
It's in between her boobs.

37:29.000 --> 37:33.000
So if I don't see directly the contact with the nipple, it's nothing.

37:33.000 --> 37:38.000
You know, that's exactly why I am having so much trouble to understand things.

37:38.000 --> 37:42.000
You have an artistic picture of a photograph of a woman

37:42.000 --> 37:46.000
and you show a tiny nipple on it.

37:46.000 --> 37:52.000
On one hand, this is a delete because we have 100% uncovered nipples.

37:52.000 --> 37:56.000
On the other hand, you have this almost porn photo.

37:56.000 --> 37:59.000
And you don't delete because it doesn't feed the world.

37:59.000 --> 38:01.000
That's exactly why I...

38:01.000 --> 38:07.000
Yes, but you have a small problem because you're still going from what you think

38:07.000 --> 38:09.000
in your decisions.

38:09.000 --> 38:15.000
And we're in school to learn rules.

38:16.000 --> 38:21.000
Applying Facebook's rules without questioning them is the number one rule.

38:21.000 --> 38:25.000
A principle that will be drilled into you all day, every day.

38:25.000 --> 38:30.000
There has to be a line and they drill it around that.

38:30.000 --> 38:35.000
We just need to respect it and we just need to apply it to do our jobs.

38:35.000 --> 38:39.000
Sometimes we'll find disagreements, but I mean, this is the good job

38:39.000 --> 38:44.000
because this is not my social network experience.

38:44.000 --> 38:53.000
A training program with the end goal of turning you into a machine.

38:53.000 --> 38:59.000
Pedro worked for six months as a content reviewer for Facebook at Accenture.

38:59.000 --> 39:05.000
He agreed to respond to our questions, but only if he remained anonymous.

39:05.000 --> 39:13.000
Two years after leaving the company, he still remembers the numbing side of the rule.

39:13.000 --> 39:18.000
You have to play by their game or else you won't have a job at the end of the month.

39:18.000 --> 39:22.000
And it's got two points where I just felt I was a robot

39:22.000 --> 39:27.000
and just doing as many pictures and videos as much as possible

39:27.000 --> 39:31.000
just because that's the only thing I can do.

39:31.000 --> 39:34.000
You're just there with numbers and clicking enter.

39:34.000 --> 39:36.000
Numbers, enter, numbers, enter.

39:36.000 --> 39:44.000
The hardest thing for Pedro is trying to forget everything that he saw on that screen over six months.

39:44.000 --> 39:48.000
We're not prepared for it. We're not mentally prepared for it.

39:48.000 --> 39:52.000
All these stuff, they don't really give us the input before

39:52.000 --> 39:54.000
and it just comes to you as a shock.

39:54.000 --> 39:57.000
It just comes to you as like a wave here, have this in front of you

39:57.000 --> 40:00.000
and you can't really say yes or no to it.

40:00.000 --> 40:04.000
If you give me a million euros, a billion euros, I wouldn't go.

40:04.000 --> 40:06.000
It's not for me.

40:14.000 --> 40:20.000
What Pedro described to us, the wave of shock that washes over you unexpectedly

40:20.000 --> 40:22.000
is exactly what happened to Grégoire.

40:22.000 --> 40:27.000
It started around the fifth day of training during the practical exercises.

40:34.000 --> 40:37.000
A stream of horrific images

40:37.000 --> 40:44.000
and unbearable videos that must be watched closely in order to make the right decision

40:44.000 --> 40:47.000
according to Facebook's criteria.

40:56.000 --> 41:01.000
The same horrific scenes are unfolding on his neighbor's screen too.

41:04.000 --> 41:06.000
I'm going to open the window.

41:09.000 --> 41:11.000
Excuse me.

41:11.000 --> 41:13.000
I'll take a glass on the other side.

41:24.000 --> 41:26.000
I'm not far from Vominier.

41:26.000 --> 41:31.000
I just took a break because I saw the bodies of the members,

41:31.000 --> 41:34.000
but the people who threw themselves from the top of the tower,

41:34.000 --> 41:36.000
they were crushed by the ground.

41:36.000 --> 41:40.000
The nose, the body that trembled, the hands that trembled,

41:40.000 --> 41:43.000
it was really close to today.

41:47.000 --> 41:52.000
It's like this on a daily basis for Grégoire and his group.

41:52.000 --> 41:57.000
Luckily, they can always rely on the useful advice of the trainers to feel better.

42:02.000 --> 42:05.000
If the macarena isn't quite enough to cheer you up,

42:05.000 --> 42:10.000
the business also has psychologists available for the most traumatized moderators.

42:13.000 --> 42:16.000
On this day, a video lasting several minutes

42:16.000 --> 42:20.000
brought the violence to another level for Grégoire.

42:32.000 --> 42:36.000
During the break, everyone tries to shake off the shock

42:36.000 --> 42:40.000
by discussing the grim video they've just witnessed.

43:01.000 --> 43:27.000
Grégoire realizes the extent of the damage this job can cause

43:27.000 --> 43:31.000
when talking with a former moderator who is now a trainer.

43:57.000 --> 43:59.000
I know that I have to be like this.

43:59.000 --> 44:02.000
I can't watch people running across the street.

44:02.000 --> 44:03.000
Anyone.

44:03.000 --> 44:07.000
You're still doing this while you have PTSD?

44:07.000 --> 44:08.000
There is a purpose.

44:08.000 --> 44:12.000
I do feel every day like I'm cleaning the trash.

44:12.000 --> 44:13.000
Right.

44:15.000 --> 44:18.000
Okay, I will watch it, but at least I know that I'm going to watch it.

44:18.000 --> 44:22.000
Someone who's 14 years old is going to quit that and not know.

44:28.000 --> 44:30.000
Even two years after quitting the post,

44:30.000 --> 44:35.000
Pedro still has very vivid memories of certain videos.

44:37.000 --> 44:39.000
There's a few things that I saw.

44:39.000 --> 44:42.000
Those things are going to stay with me

44:42.000 --> 44:45.000
because I remember them as if it was yesterday,

44:45.000 --> 44:48.000
it's very emotional sometimes.

44:48.000 --> 44:50.000
I remember sometimes people used to like,

44:50.000 --> 44:53.000
they were working, being productive,

44:53.000 --> 44:56.000
and suddenly they just stand up and run out of the room.

44:57.000 --> 44:58.000
That's okay.

44:58.000 --> 45:00.000
Trauma built up.

45:00.000 --> 45:04.000
And for Pedro, left him feeling helpless.

45:04.000 --> 45:06.000
But if you see someone getting murdered,

45:06.000 --> 45:08.000
the only action you take is the lead, for example.

45:08.000 --> 45:11.000
You just erase it out of the platform.

45:11.000 --> 45:16.000
You don't really go into depth of like calling the police, for example.

45:16.000 --> 45:20.000
It's like, you never really feel content with what you're doing.

45:20.000 --> 45:23.000
You're just going round and round in circles

45:23.000 --> 45:26.000
and just like bombard with all this stuff.

45:26.000 --> 45:30.000
It's like a mixture of emotions that you go through in one day,

45:30.000 --> 45:33.000
eight hours for it.

45:33.000 --> 45:35.000
How many were you when you started?

45:35.000 --> 45:37.000
We were 30 when we started, 30.

45:37.000 --> 45:42.000
From that 30, it started just decreasing month by month

45:42.000 --> 45:45.000
until now there's only like three people.

45:47.000 --> 45:51.000
Pedro claims that a lot of people struggle to deal with the role

45:51.000 --> 45:53.000
and end up quitting.

45:53.000 --> 45:56.000
To understand what Pedro went through

45:56.000 --> 46:00.000
and what Grégoix and his colleagues are currently experiencing,

46:00.000 --> 46:03.000
we met up with a psychiatrist.

46:03.000 --> 46:06.000
Professor Thierry Boubet is a specialist

46:06.000 --> 46:08.000
in post-traumatic stress disorder.

46:08.000 --> 46:11.000
For example, he works with police officers

46:11.000 --> 46:14.000
who have been involved in terrorist attacks.

46:14.000 --> 46:17.000
We show him the footage we filmed.

46:18.000 --> 46:22.000
Some potentially traumatic images,

46:22.000 --> 46:24.000
like the ones described here,

46:24.000 --> 46:26.000
can have several effects.

46:26.000 --> 46:29.000
For some people, it's just anxiety effects.

46:29.000 --> 46:32.000
It can make you anxious for a while,

46:32.000 --> 46:34.000
sometimes in an important way,

46:34.000 --> 46:37.000
with panic attacks or something like that.

46:37.000 --> 46:40.000
But in some cases,

46:40.000 --> 46:44.000
there can be what we call a traumatic infraction,

46:44.000 --> 46:47.000
i.e. one of these images

46:47.000 --> 46:50.000
or some of these images

46:50.000 --> 46:55.000
will go deeper into us

46:55.000 --> 46:58.000
and come back to us without any stress.

46:58.000 --> 47:02.000
What's special about post-traumatic stress disorder

47:02.000 --> 47:05.000
is that when these images come back without any stress,

47:05.000 --> 47:08.000
they produce without any stress the same stress.

47:08.000 --> 47:11.000
So it's a stress that lasts

47:11.000 --> 47:17.000
and it doesn't disappear, if you like.

47:17.000 --> 47:20.000
We also talk to him about the famous

47:20.000 --> 47:24.000
confidentiality classes imposed by Facebook.

47:24.000 --> 47:27.000
The secret culture and the interdiction

47:27.000 --> 47:30.000
that is made in talking to third parties

47:30.000 --> 47:36.000
is a process that belongs to the records

47:36.000 --> 47:39.000
of what we call mental imprisonment.

47:39.000 --> 47:42.000
Mental imprisonment is something

47:42.000 --> 47:45.000
that is used by different movements,

47:45.000 --> 47:48.000
like, for example, sector movements,

47:48.000 --> 47:52.000
and it makes it even more vulnerable

47:52.000 --> 47:55.000
to traumatic impacts.

47:56.000 --> 47:59.000
Anxiety, trauma, stress.

47:59.000 --> 48:03.000
Cleaning up social media comes at a great cost.

48:04.000 --> 48:08.000
Grégoire decides to quit only two weeks later,

48:08.000 --> 48:11.000
still in his training period.

48:15.000 --> 48:18.000
He received his paycheck just before leaving,

48:18.000 --> 48:21.000
his hourly pay written at the top,

48:21.000 --> 48:24.000
four euros, 62 cents gross.

48:24.000 --> 48:28.000
This is a tough pill to swallow for his colleague.

48:30.000 --> 48:33.000
I was earning more in the ice cream shop.

48:33.000 --> 48:35.000
In the ice cream shop?

48:35.000 --> 48:38.000
Man, that's bad, right?

48:39.000 --> 48:43.000
After our experience there, we contacted Accenture.

48:43.000 --> 48:45.000
Their response was a brief email

48:45.000 --> 48:48.000
that didn't once reference Facebook.

48:48.000 --> 48:50.000
It did, however, contain this phrase,

48:50.000 --> 48:54.000
the well-being of our employees is our priority.

48:56.000 --> 48:59.000
To finish our tour of the Internet's trash cleaners,

48:59.000 --> 49:02.000
the invisible workforce behind your Facebook

49:02.000 --> 49:06.000
or Instagram feed, we had one last meeting.

49:06.000 --> 49:09.000
Sarah Roberts is the leading researcher

49:09.000 --> 49:12.000
specializing in those who work as moderators.

49:12.000 --> 49:15.000
She is a key figure in this field.

49:15.000 --> 49:19.000
We met her at the university where she teaches in California.

49:19.000 --> 49:22.000
She presented us with an analysis of the rise

49:22.000 --> 49:26.000
and development of content moderation over the past year.

49:27.000 --> 49:33.000
We are talking about a scope and a scale of magnitude

49:33.000 --> 49:35.000
that has not been seen before.

49:35.000 --> 49:38.000
Billions of things shared per day on Facebook.

49:38.000 --> 49:42.000
Hundreds of hours of video uploaded to YouTube

49:42.000 --> 49:45.000
per minute per day and so on.

49:45.000 --> 49:47.000
The response has continued to be,

49:47.000 --> 49:49.000
we'll put more content moderators on it,

49:49.000 --> 49:53.000
which means that that continues to exponentially grow.

49:53.000 --> 49:58.000
It has gone from a next to nothing kind of line item

49:58.000 --> 50:03.000
in the budget to being a massive, massive cost center,

50:03.000 --> 50:05.000
meaning it doesn't actually return revenue.

50:05.000 --> 50:07.000
It's not like a new product.

50:07.000 --> 50:10.000
It's just seen as an economic drain.

50:10.000 --> 50:12.000
And the way we manage that problem

50:12.000 --> 50:15.000
is by pushing it onto some low-wage workers

50:15.000 --> 50:17.000
to do it as cheaply as possible,

50:17.000 --> 50:19.000
because, again, that stacks up

50:19.000 --> 50:22.000
when you double your workforce in two years

50:22.000 --> 50:24.000
that it does not come for free.

50:24.000 --> 50:28.000
This is why companies like Facebook use subcontractors.

50:28.000 --> 50:30.000
But according to this researcher,

50:30.000 --> 50:32.000
this isn't the only reason.

50:32.000 --> 50:33.000
It's about labor costs,

50:33.000 --> 50:36.000
but it's also about creating layers

50:36.000 --> 50:39.000
of lessening responsibility

50:39.000 --> 50:41.000
between those who solicit this kind of work

50:41.000 --> 50:45.000
and need it and those who do it and where they do it.

50:45.000 --> 50:46.000
They remove themselves,

50:46.000 --> 50:48.000
they put themselves at a distance

50:48.000 --> 50:50.000
from the workers and their conditions,

50:50.000 --> 50:52.000
and it's not just a geographic distance,

50:52.000 --> 50:54.000
but sort of a moral distance.

50:54.000 --> 50:57.000
So when that content moderator some years later alleges harm

50:57.000 --> 51:00.000
or, you know, is having trouble psychologically

51:00.000 --> 51:02.000
or emotionally because of the work that they did,

51:02.000 --> 51:05.000
then it may be possible for that company

51:05.000 --> 51:07.000
to disclaim responsibility for that,

51:07.000 --> 51:10.000
even though ultimately they really are responsible

51:10.000 --> 51:12.000
because they asked them to do that work in the first place.

51:12.000 --> 51:14.000
Despite these precautions,

51:14.000 --> 51:17.000
three former moderators filed lawsuits

51:17.000 --> 51:20.000
against Facebook in the U.S. a few months ago.

51:20.000 --> 51:23.000
All three were working under subcontractors,

51:23.000 --> 51:27.000
all claimed to be victims of post-traumatic stress disorder.

51:27.000 --> 51:30.000
The American company refused every request

51:30.000 --> 51:32.000
we made for an interview.

51:32.000 --> 51:35.000
They did, however, send us an email to explain

51:35.000 --> 51:37.000
how Facebook, with its partners,

51:37.000 --> 51:41.000
pays great attention to the well-being of content moderators

51:41.000 --> 51:43.000
working on its platform,

51:43.000 --> 51:46.000
which is an absolute priority.

51:47.000 --> 51:51.000
To finish off, here's some of the latest news from the sector.

51:51.000 --> 51:54.000
While these ghost workers are left in the shadows,

51:54.000 --> 51:57.000
it's business as usual for the companies

51:57.000 --> 51:59.000
working in this new sector.

51:59.000 --> 52:01.000
A few weeks after filming,

52:01.000 --> 52:06.000
Figure 8's founder sold his company for $300 million.

52:06.000 --> 52:10.000
Well, at least now, he has good reason to be happy.

52:47.000 --> 52:50.000
Thank you.

