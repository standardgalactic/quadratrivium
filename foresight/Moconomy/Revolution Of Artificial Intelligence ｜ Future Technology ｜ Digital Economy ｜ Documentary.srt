1
00:00:00,000 --> 00:00:15,000
Hello, Lionel. It's already seven o'clock.

2
00:00:15,000 --> 00:00:18,000
Wake up. It's twenty twenty seven.

3
00:00:18,000 --> 00:00:22,000
Don't wake up. You have a meeting at nine o'clock this morning.

4
00:00:22,000 --> 00:00:27,000
In twenty twenty seven, Sarah takes care of everything.

5
00:00:27,000 --> 00:00:30,000
For breakfast, what do I make for you?

6
00:00:30,000 --> 00:00:32,000
A glass of chocolate.

7
00:00:32,000 --> 00:00:38,000
Looking at your health data, I advise you to use soy milk instead of avocados.

8
00:00:38,000 --> 00:00:44,000
Sarah is a virtual assistant who knows exactly what's best for you.

9
00:00:44,000 --> 00:00:48,000
For your part of Squash, I selected Emmanuelle.

10
00:00:48,000 --> 00:00:53,000
I crossed your data. This time, you're sure to take it.

11
00:00:54,000 --> 00:00:58,000
For your dinner party tonight, I reserved this restaurant.

12
00:00:58,000 --> 00:01:00,000
It's new and very well-noted.

13
00:01:00,000 --> 00:01:02,000
It's perfect. Thank you.

14
00:01:02,000 --> 00:01:09,000
Everywhere you go, artificial intelligence, like Sarah, predicts your needs and does the work for you.

15
00:01:11,000 --> 00:01:17,000
Hello, Lionel. We're going to start. I've prepared a contract for our last meeting.

16
00:01:18,000 --> 00:01:24,000
With all of these machines working for you, isn't life wonderful in twenty twenty seven?

17
00:01:27,000 --> 00:01:30,000
But let's not get carried away.

18
00:01:30,000 --> 00:01:37,000
Before Sarah changes your life forever, there's another story to tell, one with less special effects.

19
00:01:37,000 --> 00:01:53,000
This story takes place behind the scenes of those businesses who are working to invent our future.

20
00:01:53,000 --> 00:01:59,000
For now, it's hardly this wonderful world where machines are working entirely for mankind.

21
00:01:59,000 --> 00:02:02,000
In fact, you could say it's exactly the opposite.

22
00:02:02,000 --> 00:02:05,000
Humans are involved in every step of the process.

23
00:02:05,000 --> 00:02:12,000
When you're using anything online, but we're sold as this miracle of automation.

24
00:02:12,000 --> 00:02:22,000
Google, Facebook, Amazon, Uber, these digital giants are using a completely invisible workforce to keep their applications running.

25
00:02:22,000 --> 00:02:24,000
There we are.

26
00:02:24,000 --> 00:02:31,000
With technology, you can actually find them, pay them the tiny amount of money, and then get rid of them when you don't need them anymore.

27
00:02:32,000 --> 00:02:38,000
A workforce that is disposable and underpaid.

28
00:02:38,000 --> 00:02:47,000
On a very good day, I could do five dollars an hour. On a really bad day, I could do ten cents an hour.

29
00:02:47,000 --> 00:02:51,000
Is it possible for you to pay less than the American minimum wage?

30
00:02:51,000 --> 00:02:54,000
I'm not sure we want to go in this direction, yeah.

31
00:02:54,000 --> 00:02:59,000
Whilst millions of men and women are training artificial intelligence for next to nothing,

32
00:02:59,000 --> 00:03:05,000
others are being hired and hidden out of sight to clean up social networks.

33
00:03:05,000 --> 00:03:11,000
You must have been told by the recruiting team that you cannot mention that you are working for this project.

34
00:03:11,000 --> 00:03:18,000
We went undercover as one of these web cleaners, working as a content moderator for Facebook.

35
00:03:18,000 --> 00:03:32,000
There are a few things that I saw. Those things are going to stay with me because I remember them as it was yesterday.

36
00:03:32,000 --> 00:03:42,000
To meet the workers hiding behind your screen, we're taking you to the factory of the future, the digital economy's best kept secret.

37
00:03:42,000 --> 00:03:47,000
You know, it's just like a sausage factory. They don't want people to come in to see how the sausage is made.

38
00:03:47,000 --> 00:03:50,000
I mean, I think it's just that simple.

39
00:04:04,000 --> 00:04:11,000
To delve into the mysteries of artificial intelligence, we're heading to the west coast of the U.S.

40
00:04:12,000 --> 00:04:19,000
Here in San Francisco and the Silicon Valley, the world of tomorrow is being developed.

41
00:04:19,000 --> 00:04:34,000
It's the high-tech hub of giants, like Apple, Facebook, YouTube, Uber, Netflix, and Google.

42
00:04:34,000 --> 00:04:42,000
We have a meeting of figure eight, a business specializing in artificial intelligence that primarily works with Google.

43
00:04:42,000 --> 00:04:47,000
The founder, Lucas Biewald, agreed to spend the morning with us.

44
00:04:47,000 --> 00:04:55,000
Hello, Lucas. Nice to meet you. Thank you very much for your time. I know you have a busy schedule. Thank you.

45
00:04:55,000 --> 00:05:04,000
At 38 years old, this Stanford graduate has already worked for the likes of Microsoft and Yahoo before founding his own company.

46
00:05:04,000 --> 00:05:10,000
Once his microphone is on, a quick tour of their startup-style Californian office space.

47
00:05:10,000 --> 00:05:15,000
This is our best dressed in play.

48
00:05:15,000 --> 00:05:18,000
Cool and relaxed.

49
00:05:18,000 --> 00:05:22,000
This is probably our worst dressed in play.

50
00:05:27,000 --> 00:05:32,000
Do you play babyfoot? I think I'm pretty good. I don't know, maybe.

51
00:05:32,000 --> 00:05:37,000
This is kind of our eating area. This is actually where I like to work.

52
00:05:37,000 --> 00:05:41,000
My coffee got cold.

53
00:05:41,000 --> 00:05:45,000
And in the reception area, an impressive display.

54
00:05:45,000 --> 00:05:55,000
These are some of our customers and the different things that they did with our products.

55
00:05:55,000 --> 00:06:03,000
Here's Twitter. We help them remove a lot of people that were kind of bullying on their website.

56
00:06:03,000 --> 00:06:09,000
You know, American Express. Is that in France? Yeah.

57
00:06:09,000 --> 00:06:13,000
You know, I feel especially proud of, you know, something like Tesco, right?

58
00:06:13,000 --> 00:06:22,000
Is able to use us to improve their online website to show better search results so people can find the items that they're looking for.

59
00:06:22,000 --> 00:06:24,000
And I don't see Google.

60
00:06:24,000 --> 00:06:27,000
No, I don't know. Do you know why some of these get up here?

61
00:06:27,000 --> 00:06:33,000
Frankly, we just stopped because it was getting out of hand.

62
00:06:33,000 --> 00:06:36,000
This is Mr. Brown, head of PR.

63
00:06:36,000 --> 00:06:38,000
This is a good scene.

64
00:06:38,000 --> 00:06:45,000
After our visit, the founder explains the enigmatic name, Figure 8.

65
00:06:45,000 --> 00:06:49,000
We call our company Figure 8 because we think of it as a loop.

66
00:06:49,000 --> 00:06:55,000
And the loop really has these two parts, right? There's the humans that do the labeling.

67
00:06:55,000 --> 00:06:58,000
And then the machine learning that learns from the humans.

68
00:06:58,000 --> 00:07:02,000
And then it goes back to the humans for more labeling, right?

69
00:07:02,000 --> 00:07:06,000
So we think of this kind of like beautiful loop, right?

70
00:07:06,000 --> 00:07:13,000
Where humans do the best things that humans can do and the algorithms, the artificial intelligence, does the best things that the algorithms can do.

71
00:07:13,000 --> 00:07:17,000
And we put that together. And that's why we call it Figure 8.

72
00:07:23,000 --> 00:07:31,000
To get a better understanding of why AI needs humans to function, we stop joking around and get out the computer.

73
00:07:32,000 --> 00:07:34,000
So here's an example.

74
00:07:34,000 --> 00:07:38,000
You know, a lot of people these days are trying to build cars that automatically drive.

75
00:07:38,000 --> 00:07:44,000
Like, for example, Tesla has a system where you can drive around in a car.

76
00:07:44,000 --> 00:07:49,000
But of course, it's incredibly important that these cars don't run into pedestrians.

77
00:07:49,000 --> 00:07:53,000
So the car camera just sees something like this.

78
00:07:53,000 --> 00:07:58,000
So it's really important that they build reliable systems that can identify people.

79
00:07:58,000 --> 00:08:05,000
And the way that they learn to identify people is looking at lots of pictures of what the car is seeing from the camera.

80
00:08:05,000 --> 00:08:08,000
And then actually literally labeling where the people are.

81
00:08:09,000 --> 00:08:12,000
Here's a real example of how it works.

82
00:08:12,000 --> 00:08:18,000
If you want to teach a self-driving car to recognize a pedestrian, a human like you or I,

83
00:08:18,000 --> 00:08:25,000
it first has to identify pedestrians from photos and then feed this information to the AI.

84
00:08:25,000 --> 00:08:33,000
And this process has to be done over a thousand, even a million times over, which can be very time-consuming.

85
00:08:38,000 --> 00:08:44,000
This is where Figure 8 gets involved, using real people who are paid to do this work.

86
00:08:46,000 --> 00:08:53,000
So the task here is to look at this picture and then label where the people are.

87
00:08:53,000 --> 00:08:54,000
And so you get paid for this?

88
00:08:54,000 --> 00:08:57,000
You get paid to draw boxes around the people.

89
00:08:57,000 --> 00:08:58,000
How much?

90
00:08:58,000 --> 00:09:07,000
I'm not sure this task, but maybe it would be like ten cents per person that you draw a box around.

91
00:09:07,000 --> 00:09:13,000
Who do this job? Do you have employees doing these jobs and labeling people?

92
00:09:13,000 --> 00:09:18,000
Yes, so it's contractors in our network that log in and do these jobs.

93
00:09:18,000 --> 00:09:22,000
What do you mean by contractors on your network? What kind of people?

94
00:09:22,000 --> 00:09:28,000
So it's like people that log into this and then want to work on these tasks.

95
00:09:28,000 --> 00:09:32,000
How many people work for Figure 8?

96
00:09:32,000 --> 00:09:34,000
In this capacity as labellers?

97
00:09:34,000 --> 00:09:35,000
Yeah.

98
00:09:35,000 --> 00:09:38,000
So again, people can kind of come and go if they want to.

99
00:09:38,000 --> 00:09:47,000
So there's maybe around 100,000 people that kind of consistently work every day for certain use cases that we have.

100
00:09:47,000 --> 00:09:53,000
But then there's also millions of people that log in from time to time and work on tasks.

101
00:09:53,000 --> 00:09:56,000
And where do those people live?

102
00:09:56,000 --> 00:09:58,000
They live all over the world actually.

103
00:09:58,000 --> 00:10:03,000
So they live all over America and then they live all over the world.

104
00:10:06,000 --> 00:10:12,000
So who are these millions of people who are being paid to train AI technology?

105
00:10:13,000 --> 00:10:19,000
In order to meet these contractors, as Figure 8 calls them,

106
00:10:19,000 --> 00:10:25,000
we leave Silicon Valley and head 500 miles north of San Francisco in Oregon.

107
00:10:32,000 --> 00:10:33,000
There we are.

108
00:10:33,000 --> 00:10:35,000
Ah, success.

109
00:10:36,000 --> 00:10:40,000
Jared Mansfield signed up to Figure 8 three years ago.

110
00:10:40,000 --> 00:10:44,000
He now spends several hours a week working for them.

111
00:10:44,000 --> 00:10:49,000
Every day, the company offers a list of tasks that he can complete for money.

112
00:10:49,000 --> 00:10:52,000
For example, training search engines.

113
00:10:59,000 --> 00:11:02,000
For this first one, it's showing examples of how to do it.

114
00:11:02,000 --> 00:11:05,000
The query is mac and cheese pierogies.

115
00:11:05,000 --> 00:11:09,000
And the two results are Annie's homegrown organic mac and cheese

116
00:11:09,000 --> 00:11:14,000
and Annie's really cheddar microwavable macaroni and cheese, which are neither of them are pierogies.

117
00:11:14,000 --> 00:11:17,000
So it's saying that one would be equally bad matches.

118
00:11:17,000 --> 00:11:19,000
What's the use of doing that?

119
00:11:19,000 --> 00:11:25,000
A lot of it, I think, is to train search algorithms.

120
00:11:25,000 --> 00:11:29,000
So like when someone sits at their computer and types a product,

121
00:11:29,000 --> 00:11:34,000
the algorithm will be able to determine with more accuracy

122
00:11:34,000 --> 00:11:37,000
what product it is that that person is looking for.

123
00:11:41,000 --> 00:11:46,000
For every ten answers, Jared earns less than one cent.

124
00:11:46,000 --> 00:11:52,000
To get an idea of how much money he can make, we leave him to work for 30 minutes.

125
00:11:52,000 --> 00:11:57,000
He's answered 180 questions over the course of half an hour.

126
00:11:59,000 --> 00:12:01,000
How much have you earned?

127
00:12:01,000 --> 00:12:02,000
15 cents.

128
00:12:02,000 --> 00:12:03,000
For how long?

129
00:12:03,000 --> 00:12:05,000
A half hour.

130
00:12:05,000 --> 00:12:08,000
Which would be 30 cents per hour?

131
00:12:08,000 --> 00:12:14,000
Yeah, which are pretty definitely not a livable wage, that's for sure.

132
00:12:14,000 --> 00:12:16,000
Do they have the right to do this?

133
00:12:16,000 --> 00:12:19,000
I mean, they have the right to do whatever they want.

134
00:12:19,000 --> 00:12:24,000
I'm the one coming to them for little tiny bits of coins on this website.

135
00:12:24,000 --> 00:12:30,000
And there's no contract between me and them.

136
00:12:33,000 --> 00:12:37,000
No contract, no salary, no guaranteed minimum wage.

137
00:12:37,000 --> 00:12:41,000
These ghost workers are paid to train software and robots

138
00:12:41,000 --> 00:12:45,000
using only one rule, supply and demand.

139
00:12:46,000 --> 00:12:50,000
It definitely feels like I'm part of this invisible workforce

140
00:12:50,000 --> 00:12:55,000
that is kind of made up of just random people throughout the world.

141
00:12:55,000 --> 00:13:04,000
And together we're kind of training what's going to replace the workforce as a whole eventually.

142
00:13:04,000 --> 00:13:08,000
Jared is very philosophical about the idea.

143
00:13:08,000 --> 00:13:10,000
Still, he can afford to be.

144
00:13:10,000 --> 00:13:15,000
To earn a real living, he has another job selling chicken in the supermarket

145
00:13:15,000 --> 00:13:18,000
for a little more than $1,500 a month.

146
00:13:18,000 --> 00:13:23,000
Figure 8 is just what he does on the side to earn a little extra cash.

147
00:13:30,000 --> 00:13:35,000
After leaving Oregon, we decided to take advantage of what we'd learned in America

148
00:13:35,000 --> 00:13:41,000
and sign ourselves up to Figure 8 to train artificial intelligence.

149
00:13:41,000 --> 00:13:48,000
On the site's welcome page, small tasks are proposed at 1, 2, or 12 cents.

150
00:13:51,000 --> 00:13:58,000
We chose this as our first task, drawing boxes around objects in images.

151
00:13:58,000 --> 00:14:02,000
Following the instructions, it took us a couple of minutes

152
00:14:02,000 --> 00:14:06,000
to draw around 10 objects and earn 2 cents.

153
00:14:06,000 --> 00:14:12,000
On the list of tasks, Figure 8 also offers evaluations of search engine answers,

154
00:14:12,000 --> 00:14:14,000
Jared's task of choice.

155
00:14:14,000 --> 00:14:18,000
We could also listen to conversations and confirm if the recording

156
00:14:18,000 --> 00:14:23,000
features a man or a woman's voice, and if they are speaking English.

157
00:14:23,000 --> 00:14:26,000
Hi, is James there, please?

158
00:14:27,000 --> 00:14:30,000
We work for our team,

159
00:14:31,000 --> 00:14:36,000
we work for hours without ever earning more than 30 cents an hour.

160
00:14:41,000 --> 00:14:48,000
It's difficult to imagine that there are people who work on these tasks on a full-time basis.

161
00:14:51,000 --> 00:14:57,000
We're in Maine, on the east coast of the United States, close to the Canadian border.

162
00:14:57,000 --> 00:15:00,000
We've arranged to meet with one of the NET's ghost workers,

163
00:15:00,000 --> 00:15:03,000
the human side of the Figure 8 loop.

164
00:15:06,000 --> 00:15:10,000
Her name is Don Carbone, she is 46 years old.

165
00:15:12,000 --> 00:15:13,000
Bonjour.

166
00:15:15,000 --> 00:15:16,000
Hello.

167
00:15:16,000 --> 00:15:17,000
Hello.

168
00:15:18,000 --> 00:15:19,000
Nice to meet you.

169
00:15:19,000 --> 00:15:21,000
Thank you so much for your welcome.

170
00:15:21,000 --> 00:15:22,000
Beautiful.

171
00:15:22,000 --> 00:15:28,000
Oh, we had a blizzard not that long ago, and then we got more.

172
00:15:28,000 --> 00:15:31,000
And it's also, I think, negative seven out there.

173
00:15:32,000 --> 00:15:34,000
Don is a single mother.

174
00:15:34,000 --> 00:15:36,000
She lives here with three of her children.

175
00:15:41,000 --> 00:15:45,000
This is what subsidized housing looks like up here.

176
00:15:45,000 --> 00:15:49,000
I mean, it's not bad for public housing.

177
00:15:50,000 --> 00:15:56,000
She lives and works here, working on the Figure 8 site all day.

178
00:16:01,000 --> 00:16:05,000
I'll turn it on, like I said, right before seven o'clock.

179
00:16:07,000 --> 00:16:09,000
Get the initial stuff done.

180
00:16:09,000 --> 00:16:15,000
I'll turn this off at three o'clock in the afternoon and then turn it back on at nine o'clock at night.

181
00:16:15,000 --> 00:16:20,000
So, I'll say eight hours minimum.

182
00:16:22,000 --> 00:16:24,000
I bust my butt though.

183
00:16:25,000 --> 00:16:27,000
Like this would be the dashboard.

184
00:16:27,000 --> 00:16:31,000
And you can see I've done 6,445 tasks.

185
00:16:31,000 --> 00:16:33,000
Since when?

186
00:16:33,000 --> 00:16:35,000
Three years.

187
00:16:35,000 --> 00:16:36,000
See these different badges?

188
00:16:36,000 --> 00:16:37,000
Yeah.

189
00:16:37,000 --> 00:16:39,000
You start off, you have no badge.

190
00:16:39,000 --> 00:16:42,000
And you have to do so many questions and get so many right.

191
00:16:42,000 --> 00:16:44,000
And then you get your first level badge.

192
00:16:44,000 --> 00:16:51,000
And then when you get to level three, you have access to virtually all the tasks that are put up.

193
00:16:53,000 --> 00:16:55,000
What is your level right now?

194
00:16:55,000 --> 00:16:56,000
Right now, oh, I'm on level three.

195
00:16:56,000 --> 00:16:58,000
I've been level three.

196
00:16:58,000 --> 00:17:01,000
I've been level three for quite a while.

197
00:17:01,000 --> 00:17:05,000
Don is considered a high performing worker.

198
00:17:05,000 --> 00:17:08,000
Figure 8 therefore offers her more work than a beginner.

199
00:17:08,000 --> 00:17:11,000
But it isn't necessarily more interesting.

200
00:17:13,000 --> 00:17:19,000
I have to put bounding boxes around people.

201
00:17:22,000 --> 00:17:25,000
I'm not really keen on this job.

202
00:17:29,000 --> 00:17:35,000
The biggest problem is trying to find jobs that are viable.

203
00:17:35,000 --> 00:17:38,000
And right now, I don't have many.

204
00:17:39,000 --> 00:17:42,000
And it's definitely not better paid.

205
00:17:44,000 --> 00:17:47,000
On a very good day, I could do $5 an hour.

206
00:17:47,000 --> 00:17:51,000
On a really bad day, I could do $0.10 an hour.

207
00:17:51,000 --> 00:17:58,000
I mean, I have had some really, really good days until February.

208
00:17:58,000 --> 00:17:59,000
Yeah.

209
00:17:59,000 --> 00:18:04,000
Do you think this is a fair payment for what you're doing?

210
00:18:04,000 --> 00:18:07,000
No, no, no, no.

211
00:18:07,000 --> 00:18:09,000
Not at all.

212
00:18:09,000 --> 00:18:12,000
But I live in Northern Maine.

213
00:18:12,000 --> 00:18:14,000
We get a lot of snow.

214
00:18:14,000 --> 00:18:18,000
There's a very low job market.

215
00:18:18,000 --> 00:18:21,000
And it helps me as a stay-at-home mom.

216
00:18:21,000 --> 00:18:24,000
It helps with added income.

217
00:18:30,000 --> 00:18:33,000
Don prefers to work from home because her youngest daughter,

218
00:18:33,000 --> 00:18:35,000
Jane, has autism.

219
00:18:36,000 --> 00:18:38,000
Here you go.

220
00:18:38,000 --> 00:18:40,000
What happened?

221
00:18:40,000 --> 00:18:42,000
Don wants to be there to take care of her

222
00:18:42,000 --> 00:18:46,000
when she gets home from school at 3 p.m.

223
00:18:46,000 --> 00:18:48,000
So how was school?

224
00:18:48,000 --> 00:18:50,000
Good day or bad day?

225
00:18:50,000 --> 00:18:53,000
Really a good day?

226
00:18:53,000 --> 00:18:56,000
With her autism, I always have to be ready to jump in my car

227
00:18:56,000 --> 00:18:58,000
and go get her from school.

228
00:18:58,000 --> 00:19:01,000
I mean, it could happen one day out of the week,

229
00:19:01,000 --> 00:19:05,000
or not at all, or three days out of the week.

230
00:19:05,000 --> 00:19:07,000
And the school is very understanding.

231
00:19:07,000 --> 00:19:11,000
So I mean, I have to take out the whole week

232
00:19:11,000 --> 00:19:14,000
if I was working out of the home.

233
00:19:14,000 --> 00:19:19,000
Don receives $750 in government aid every month,

234
00:19:19,000 --> 00:19:22,000
which isn't enough to cover all of her bills.

235
00:19:22,000 --> 00:19:25,000
This is why she signed up to Figure 8.

236
00:19:25,000 --> 00:19:28,000
By working eight hours a day and five days a week,

237
00:19:28,000 --> 00:19:33,000
she says she earns, on average, $250 a month on the site.

238
00:19:44,000 --> 00:19:47,000
On Figure 8, the pay is non-negotiable.

239
00:19:47,000 --> 00:19:49,000
If you refuse the work,

240
00:19:49,000 --> 00:19:52,000
there will always be someone else to take it.

241
00:19:54,000 --> 00:19:57,000
There is an unlimited supply of these ghost workers,

242
00:19:57,000 --> 00:19:59,000
coming from all over the world.

243
00:19:59,000 --> 00:20:03,000
It's probably why Lucas B. Wald is so happy.

244
00:20:03,000 --> 00:20:08,000
But he isn't the only one to take advantage of this phenomenon.

245
00:20:10,000 --> 00:20:13,000
Various other businesses propose these sorts of repetitive

246
00:20:13,000 --> 00:20:16,000
and underpaid online tasks,

247
00:20:16,000 --> 00:20:21,000
the biggest amongst them being ClickWorker and Amazon Mechanical Turk,

248
00:20:21,000 --> 00:20:25,000
a platform provided by Amazon and its boss, Jeff Bezos,

249
00:20:25,000 --> 00:20:28,000
who invented the concept in 2005.

250
00:20:31,000 --> 00:20:34,000
Micro-working is a growing concern for the ILO,

251
00:20:34,000 --> 00:20:37,000
the International Labour Organization,

252
00:20:37,000 --> 00:20:42,000
a UN agency in charge of protecting workers' rights across the globe.

253
00:20:49,000 --> 00:20:53,000
Jeanine Berg is the resident expert on this subject at the ILO,

254
00:20:53,000 --> 00:20:55,000
who speaks to us through Skype.

255
00:21:23,000 --> 00:21:28,000
And the technology has facilitated this, and it's cheap.

256
00:21:28,000 --> 00:21:31,000
That's us, the main advantage.

257
00:21:32,000 --> 00:21:34,000
Jeanine Berg wrote a report,

258
00:21:34,000 --> 00:21:39,000
calculating that micro-workers earn, on average, $3.31 an hour,

259
00:21:39,000 --> 00:21:41,000
without any rights in return.

260
00:21:42,000 --> 00:21:47,000
Workers' extreme vulnerability is the key to Lucas B. Wald's business model.

261
00:21:48,000 --> 00:21:50,000
After months of investigations,

262
00:21:50,000 --> 00:21:56,000
we found this video from 2010 that sums up his view of the labour force.

263
00:21:59,000 --> 00:22:03,000
Before the internet, it would be really difficult to find someone,

264
00:22:03,000 --> 00:22:05,000
sit them down for 10 minutes and get them to work for you,

265
00:22:05,000 --> 00:22:07,000
and then fire them after those 10 minutes.

266
00:22:07,000 --> 00:22:09,000
But with technology, you can actually find them,

267
00:22:09,000 --> 00:22:11,000
pay them a tiny amount of money,

268
00:22:11,000 --> 00:22:14,000
and then get rid of them when you don't need them anymore.

269
00:22:18,000 --> 00:22:20,000
While we were interviewing him,

270
00:22:20,000 --> 00:22:24,000
we wanted to ask him if he still shared the same opinion.

271
00:22:25,000 --> 00:22:28,000
But when we start talking about work conditions,

272
00:22:28,000 --> 00:22:32,000
the figure-eight founder seemed to lose his sense of humour.

273
00:22:32,000 --> 00:22:37,000
Do you have an idea of the average revenue per hour of your contributor?

274
00:22:37,000 --> 00:22:38,000
You know, I'm not sure.

275
00:22:38,000 --> 00:22:41,000
It's totally dependent on the task that someone puts in,

276
00:22:41,000 --> 00:22:43,000
and it's hard to track time on the internet

277
00:22:43,000 --> 00:22:45,000
because people can walk away from their computer and come back,

278
00:22:45,000 --> 00:22:49,000
so I don't know how much people don't really make.

279
00:22:49,000 --> 00:22:52,000
There was a report on ILO saying that on average,

280
00:22:52,000 --> 00:22:59,000
the people working on crowdsourcing were paid $3.31 an hour.

281
00:22:59,000 --> 00:23:02,000
Would that be consistent with what you pay?

282
00:23:03,000 --> 00:23:05,000
Again, I'm not sure.

283
00:23:06,000 --> 00:23:10,000
Is it possible for you to pay less than the American minimum wage?

284
00:23:11,000 --> 00:23:13,000
It could be possible.

285
00:23:13,000 --> 00:23:15,000
So this is legal?

286
00:23:19,000 --> 00:23:21,000
I'm not sure we want to go in this direction.

287
00:23:21,000 --> 00:23:24,000
Can we take this a different direction?

288
00:23:24,000 --> 00:23:27,000
I'd rather this focus on more AI than anything.

289
00:23:27,000 --> 00:23:29,000
Yeah, but this is the whole thing.

290
00:23:29,000 --> 00:23:31,000
This is about crowdsourcing as well,

291
00:23:31,000 --> 00:23:34,000
so I have to ask questions on crowdsourcing.

292
00:23:34,000 --> 00:23:38,000
I prepped him for more of an AI conversation

293
00:23:38,000 --> 00:23:41,000
than a crowdsourcing conversation.

294
00:23:41,000 --> 00:23:45,000
No, I don't really want to do this.

295
00:23:45,000 --> 00:23:49,000
Yeah, we can find someone else to talk about this stuff.

296
00:23:49,000 --> 00:23:53,000
Okay, so you're not comfortable with this part of the discussion?

297
00:23:53,000 --> 00:23:55,000
No, no, no.

298
00:23:55,000 --> 00:23:57,000
You're right, it is an important part of the conversation,

299
00:23:57,000 --> 00:23:59,000
but I think it's just, you know,

300
00:23:59,000 --> 00:24:01,000
it's not the AI conversation.

301
00:24:03,000 --> 00:24:05,000
We don't have time to pull up the video.

302
00:24:05,000 --> 00:24:10,000
Lucas B. Wald makes a heasty exit without saying goodbye

303
00:24:10,000 --> 00:24:13,000
and leaves us alone with his head of PR.

304
00:24:13,000 --> 00:24:17,000
One last chance to ask how the business treats these contractors,

305
00:24:17,000 --> 00:24:19,000
as they call them here.

306
00:24:19,000 --> 00:24:22,000
When I was working on this,

307
00:24:22,000 --> 00:24:26,000
I found many people complaining, being disconnected.

308
00:24:26,000 --> 00:24:31,000
I actually have to go now, too.

309
00:24:31,000 --> 00:24:33,000
So it's 11 o'clock.

310
00:24:33,000 --> 00:24:37,000
So you don't want to speak about human in the...

311
00:24:37,000 --> 00:24:40,000
That's not my role here.

312
00:24:40,000 --> 00:24:42,000
All right, I think we're done.

313
00:24:42,000 --> 00:24:44,000
So only artificial intelligence, no human?

314
00:24:44,000 --> 00:24:47,000
Well, that's what we were prepared for, so, sorry.

315
00:24:47,000 --> 00:24:48,000
Okay, it's a pity.

316
00:24:48,000 --> 00:24:51,000
To get some answers to our questions about Lucas B. Wald

317
00:24:51,000 --> 00:24:53,000
and his views on his workers,

318
00:24:53,000 --> 00:24:55,000
we thought we'd try a different tactic.

319
00:24:56,000 --> 00:24:59,000
On the day the Figure 8 founder made his statement

320
00:24:59,000 --> 00:25:01,000
on disposable workers,

321
00:25:01,000 --> 00:25:03,000
there were other entrepreneurs amongst him,

322
00:25:03,000 --> 00:25:07,000
as well as a researcher, Lily Irani, just on the right.

323
00:25:07,000 --> 00:25:09,000
Ten years after the conference,

324
00:25:09,000 --> 00:25:13,000
we find Lily living south of Los Angeles, California.

325
00:25:37,000 --> 00:25:41,000
Lily Irani teaches at the University of San Diego,

326
00:25:41,000 --> 00:25:43,000
and one of her specialist subjects

327
00:25:43,000 --> 00:25:46,000
is the working culture of high-tech business.

328
00:25:49,000 --> 00:25:53,000
We're lucky she has a good memory.

329
00:25:53,000 --> 00:25:56,000
Do you remember if somebody reacted after this sentence,

330
00:25:56,000 --> 00:25:59,000
which is very brutal in a certain way?

331
00:25:59,000 --> 00:26:02,000
To be honest, the reaction was nothing.

332
00:26:02,000 --> 00:26:06,000
I remember that panel, everyone went up to him to talk to him,

333
00:26:06,000 --> 00:26:08,000
and two or three people came up to me

334
00:26:08,000 --> 00:26:12,000
to talk about the ethics of this form of labor.

335
00:26:12,000 --> 00:26:16,000
This is a room full of highly educated people in San Francisco,

336
00:26:16,000 --> 00:26:18,000
and nobody batted an eyelash.

337
00:26:18,000 --> 00:26:20,000
How do you explain that?

338
00:26:22,000 --> 00:26:25,000
The kinds of people who have access to these spaces

339
00:26:25,000 --> 00:26:29,000
are the kinds of people who never worked in a situation

340
00:26:29,000 --> 00:26:31,000
where they wondered if they could make rent,

341
00:26:31,000 --> 00:26:34,000
or they never worked in a situation where somebody gets sick

342
00:26:34,000 --> 00:26:37,000
and they can't pay someone to go take care of them,

343
00:26:37,000 --> 00:26:40,000
so they have to kind of take a really bad job at home.

344
00:26:40,000 --> 00:26:44,000
And they have no connection to the kinds of situations

345
00:26:44,000 --> 00:26:47,000
of the people that are willing to do this work.

346
00:26:47,000 --> 00:26:49,000
It's what happens when you go to schools

347
00:26:49,000 --> 00:26:51,000
like Stanford and Harvard and Princeton

348
00:26:51,000 --> 00:26:53,000
that tell you you're the smartest person

349
00:26:53,000 --> 00:26:55,000
and you're going to be a future leader

350
00:26:55,000 --> 00:26:57,000
and you've been chosen because you're special,

351
00:26:57,000 --> 00:26:59,000
and that you have the power to change the world.

352
00:26:59,000 --> 00:27:02,000
A Silicon Valley elite who is out of touch

353
00:27:02,000 --> 00:27:04,000
with the rest of the world.

354
00:27:04,000 --> 00:27:07,000
This is the key to understanding Lucas B. Wald's logic,

355
00:27:07,000 --> 00:27:09,000
although it's not the only part.

356
00:27:09,000 --> 00:27:12,000
These workers are invisible by design.

357
00:27:12,000 --> 00:27:15,000
You can write code and send your work out,

358
00:27:15,000 --> 00:27:17,000
never talk to anyone.

359
00:27:17,000 --> 00:27:20,000
It's designed so you can get the work back on a spreadsheet.

360
00:27:20,000 --> 00:27:22,000
If you need to, you just see these, you know,

361
00:27:22,000 --> 00:27:25,000
letters and numbers identifying the worker.

362
00:27:25,000 --> 00:27:27,000
You don't see a name. You don't see where they live.

363
00:27:27,000 --> 00:27:29,000
You don't see what their situation is.

364
00:27:29,000 --> 00:27:32,000
You don't see, unless you keep track of it yourself,

365
00:27:32,000 --> 00:27:35,000
have they worked for you before or not?

366
00:27:35,000 --> 00:27:38,000
Do these ghost workers really know who they work for?

367
00:27:38,000 --> 00:27:41,000
Have they ever heard of Lucas B. Wald?

368
00:27:41,000 --> 00:27:44,000
We showed them the footage of the figure eight founder

369
00:27:44,000 --> 00:27:46,000
talking about their work.

370
00:27:53,000 --> 00:27:55,000
With technology, you can actually find them,

371
00:27:55,000 --> 00:27:57,000
pay them a tiny amount of money,

372
00:27:57,000 --> 00:28:00,000
and then get rid of them when you don't need them anymore.

373
00:28:00,000 --> 00:28:02,000
You're giggling over and paying people pennies and,

374
00:28:02,000 --> 00:28:05,000
yeah, bye-bye.

375
00:28:05,000 --> 00:28:06,000
Okay.

376
00:28:06,000 --> 00:28:08,000
Now I'm going to start arguing what I do about the AIs

377
00:28:08,000 --> 00:28:11,000
when they get me agitated.

378
00:28:11,000 --> 00:28:14,000
It's kind of surprising, I guess, a little bit

379
00:28:14,000 --> 00:28:18,000
to see there's so openly,

380
00:28:18,000 --> 00:28:24,000
openly talking about that view that they have of the workforce.

381
00:28:24,000 --> 00:28:26,000
I guess it doesn't really surprise me that much,

382
00:28:26,000 --> 00:28:31,000
but, yeah, it definitely kind of sucks,

383
00:28:31,000 --> 00:28:34,000
I guess, when they could be paying them a lot more,

384
00:28:34,000 --> 00:28:38,000
or at least showing some appreciation,

385
00:28:38,000 --> 00:28:42,000
or maybe even some discretion.

386
00:28:42,000 --> 00:28:44,000
Basically, saying in person, you know,

387
00:28:44,000 --> 00:28:47,000
you hide somebody for 10 minutes and fire them.

388
00:28:47,000 --> 00:28:49,000
This way, you don't have to look at the person

389
00:28:49,000 --> 00:28:50,000
and you just, goodbye.

390
00:28:50,000 --> 00:28:54,000
So that's kind of just, it is kind of,

391
00:28:54,000 --> 00:28:57,000
the fact that the head of the company is,

392
00:28:57,000 --> 00:28:59,000
people are that disposable,

393
00:28:59,000 --> 00:29:02,000
that really isn't right.

394
00:29:02,000 --> 00:29:04,000
I don't like that.

395
00:29:04,000 --> 00:29:07,000
So I like what I do when I have something to say,

396
00:29:07,000 --> 00:29:08,000
and I will say it.

397
00:29:08,000 --> 00:29:15,000
So I'm not disposable.

398
00:29:15,000 --> 00:29:19,000
Amongst this invisible workforce hiding behind your screen,

399
00:29:19,000 --> 00:29:24,000
there are those who feed algorithms for next to nothing.

400
00:29:24,000 --> 00:29:27,000
It's the people in charge of tidying up the web,

401
00:29:27,000 --> 00:29:29,000
the social media cleaners,

402
00:29:29,000 --> 00:29:33,000
who work on sites like Facebook or Instagram.

403
00:29:33,000 --> 00:29:35,000
These workers are never mentioned

404
00:29:35,000 --> 00:29:39,000
in the slick presentations of the Silicon Valley CEOs.

405
00:29:39,000 --> 00:29:43,000
I started building a service to do that,

406
00:29:43,000 --> 00:29:45,000
to put people first,

407
00:29:45,000 --> 00:29:48,000
and at the center of our experience with technology,

408
00:29:48,000 --> 00:29:52,000
because our relationships are what matters most to us.

409
00:29:52,000 --> 00:29:54,000
That's how we find meaning

410
00:29:54,000 --> 00:29:58,000
and how we make sense of our place in the world.

411
00:29:58,000 --> 00:30:00,000
Today, with 2 billion users,

412
00:30:00,000 --> 00:30:04,000
Facebook no longer has anything to do with Mark Zuckerberg's

413
00:30:04,000 --> 00:30:10,000
initial vision of the site.

414
00:30:10,000 --> 00:30:14,000
With violent videos, hate speech, and pornographic images,

415
00:30:14,000 --> 00:30:17,000
more and more content has to be deleted.

416
00:30:17,000 --> 00:30:20,000
And it isn't always robots doing this job.

417
00:30:20,000 --> 00:30:25,000
There are, once again, humans hidden behind the screen.

418
00:30:25,000 --> 00:30:27,000
Determining if something is hate speech

419
00:30:27,000 --> 00:30:29,000
is very linguistically nuanced.

420
00:30:29,000 --> 00:30:33,000
I am optimistic that over a five to ten year period,

421
00:30:33,000 --> 00:30:37,000
we will have AI tools that can get into some of the nuances,

422
00:30:37,000 --> 00:30:40,000
the linguistic nuances of different types of content

423
00:30:40,000 --> 00:30:42,000
to be more accurate in flagging things for our systems,

424
00:30:42,000 --> 00:30:44,000
but today we're just not there on that.

425
00:30:44,000 --> 00:30:47,000
So a lot of this is still reactive, people flag at us.

426
00:30:47,000 --> 00:30:50,000
We have people look at it.

427
00:30:50,000 --> 00:30:54,000
These people are in charge of sorting and managing content

428
00:30:54,000 --> 00:30:55,000
on the network.

429
00:30:55,000 --> 00:31:00,000
Facebook call them content reviewers.

430
00:31:00,000 --> 00:31:01,000
According to their site,

431
00:31:01,000 --> 00:31:06,000
Facebook has 15,000 workers doing this job across the world,

432
00:31:06,000 --> 00:31:13,000
in Ireland, Portugal, the Philippines, and the U.S.

433
00:31:13,000 --> 00:31:15,000
We contacted Facebook,

434
00:31:15,000 --> 00:31:24,000
but the company refused our request for an interview.

435
00:31:24,000 --> 00:31:28,000
So in order to meet these moderators and understand their role,

436
00:31:28,000 --> 00:31:32,000
we identified Facebook's main subcontractors,

437
00:31:32,000 --> 00:31:40,000
multinationals such as Majoral, Cognizant, or Accenture.

438
00:31:40,000 --> 00:31:43,000
We found this job offer for a content reviewer

439
00:31:43,000 --> 00:31:51,000
for the French market in Portugal.

440
00:31:51,000 --> 00:31:56,000
Gregoire is one of the journalists in our team.

441
00:31:56,000 --> 00:32:05,000
He responded to the ad and was offered the job.

442
00:32:05,000 --> 00:32:08,000
Before taking off, he received his contract,

443
00:32:08,000 --> 00:32:12,000
which included his monthly salary, 800 euros,

444
00:32:12,000 --> 00:32:15,000
a little over the minimum wage in Portugal,

445
00:32:15,000 --> 00:32:21,000
with a food allowance of 7 euros 63 cents a day.

446
00:32:21,000 --> 00:32:25,000
Facebook isn't mentioned once in the document.

447
00:32:25,000 --> 00:32:31,000
Even when directly asked, Accenture refused to give the client's name.

448
00:32:31,000 --> 00:32:35,000
I was just wondering, now that I took the job,

449
00:32:35,000 --> 00:32:38,000
I'm going there, I'm doing it.

450
00:32:38,000 --> 00:32:43,000
I was just wondering if I could know the name of the company

451
00:32:43,000 --> 00:32:45,000
I'm going to work for.

452
00:32:45,000 --> 00:32:48,000
No, we cannot reveal the name yet.

453
00:32:48,000 --> 00:32:51,000
It's for a wonderful customer,

454
00:32:51,000 --> 00:32:55,000
but we are not allowed to say the name.

455
00:32:56,000 --> 00:33:11,000
This is where Gregoire will be working at the Accenture offices in Lisbon.

456
00:33:11,000 --> 00:33:15,000
Before getting started, our journalist was sent to a welcome meeting.

457
00:33:15,000 --> 00:33:20,000
The footage is a little shaky, as Gregoire is filming with a hidden camera.

458
00:33:21,000 --> 00:33:26,000
Hello, I'm having a meeting with Accenture on 9.30.

459
00:33:26,000 --> 00:33:29,000
Gregoire isn't the only new employee.

460
00:33:29,000 --> 00:33:32,000
12 other people are starting the role at the same time.

461
00:33:32,000 --> 00:33:36,000
Another French person along with some Italians and Spaniards.

462
00:33:36,000 --> 00:33:40,000
An HR representative is running the welcome meeting.

463
00:33:40,000 --> 00:33:41,000
Welcome, you all.

464
00:33:41,000 --> 00:33:44,000
My job as career advisor is to help you

465
00:33:44,000 --> 00:33:47,000
in all the relationships with Accenture.

466
00:33:47,000 --> 00:33:51,000
After the vacation documents and social security paperwork,

467
00:33:51,000 --> 00:33:55,000
the small group finally find out which company they are working for.

468
00:33:55,000 --> 00:33:57,000
But it's top secret.

469
00:33:59,000 --> 00:34:01,000
You must have been told by the recruiting team

470
00:34:01,000 --> 00:34:04,000
that you cannot mention that you are working for this project.

471
00:34:04,000 --> 00:34:07,000
The client is really very demanding.

472
00:34:07,000 --> 00:34:10,000
You cannot mention anyone that you are working for at the Accenture.

473
00:34:10,000 --> 00:34:13,000
If someone asks you where you work, you work for Accenture.

474
00:34:14,000 --> 00:34:18,000
We still have this code name, they seal.

475
00:34:18,000 --> 00:34:22,000
So if I'm talking to some colleague from Accenture, not from this project,

476
00:34:22,000 --> 00:34:24,000
and he asks me where do I work,

477
00:34:24,000 --> 00:34:26,000
I cannot tell that I work for Facebook.

478
00:34:26,000 --> 00:34:28,000
This is not allowed.

479
00:34:28,000 --> 00:34:33,000
It's completely confidential that Facebook is working here at this facility.

480
00:34:35,000 --> 00:34:40,000
Codenames, confidentiality clauses, and a complete ban on cell phones.

481
00:34:40,000 --> 00:34:45,000
Facebook gives you the life of a secret agent for $800 a month.

482
00:34:45,000 --> 00:34:47,000
And if you're the chatty type,

483
00:34:47,000 --> 00:34:51,000
the following argument should shut you up pretty quickly.

484
00:34:52,000 --> 00:34:56,000
You have an agreement and you cannot break that agreement

485
00:34:56,000 --> 00:35:00,000
because by law we can punish you by law.

486
00:35:00,000 --> 00:35:02,000
It's confidential.

487
00:35:03,000 --> 00:35:08,000
Cleaning up social media is a bit like doing your family's dirty laundry.

488
00:35:08,000 --> 00:35:11,000
It has to be done, but nobody talks about it.

489
00:35:13,000 --> 00:35:16,000
Why so careful? What does the job involve?

490
00:35:20,000 --> 00:35:22,000
We continue discreetly with Gregoire.

491
00:35:30,000 --> 00:35:32,000
Before becoming a moderator,

492
00:35:32,000 --> 00:35:35,000
Gregoire has to follow a three-week training program.

493
00:35:36,000 --> 00:35:39,000
Moderating Facebook's content doesn't only involve

494
00:35:39,000 --> 00:35:42,000
deleting violent videos or racist jokes.

495
00:35:42,000 --> 00:35:45,000
It's a lot more complicated.

496
00:35:45,000 --> 00:35:49,000
At the moment, the algorithms can't handle everything.

497
00:35:49,000 --> 00:35:53,000
Every decision must be justified using very strict rules.

498
00:35:53,000 --> 00:35:56,000
This is what we learn during the training.

499
00:35:57,000 --> 00:36:01,000
Every day is dedicated to a different theme during the program.

500
00:36:01,000 --> 00:36:06,000
For example, nudity, violent images, or hate speech.

501
00:36:06,000 --> 00:36:10,000
On the agenda today, dark humor and jokes and bad taste.

502
00:36:10,000 --> 00:36:16,000
We will remove a violation if the person that you see in the image

503
00:36:16,000 --> 00:36:20,000
we need to have a real person is visibly affected.

504
00:36:20,000 --> 00:36:23,000
If you are making fun of the event,

505
00:36:23,000 --> 00:36:27,000
then it's going to be in the market score.

506
00:36:32,000 --> 00:36:37,000
What do we do when there's a knock on the event?

507
00:36:37,000 --> 00:36:41,000
Here's an example of an inappropriate joke about 9-11.

508
00:36:43,000 --> 00:36:45,000
It may seem over the top,

509
00:36:45,000 --> 00:36:48,000
but there are dozens of rules like this for each category,

510
00:36:48,000 --> 00:36:51,000
which can be difficult to get your head around.

511
00:36:52,000 --> 00:36:54,000
Take nudity, for example.

512
00:36:54,000 --> 00:36:58,000
Depending on what part of the body you see, or their position,

513
00:36:58,000 --> 00:37:02,000
the moderator can't always make the same decision.

514
00:37:02,000 --> 00:37:06,000
Here's an example from the exercises to better explain.

515
00:37:07,000 --> 00:37:11,000
Gregoire decided to delete this particular photo.

516
00:37:11,000 --> 00:37:15,000
But according to Facebook's rules, he was wrong to do so.

517
00:37:15,000 --> 00:37:19,000
In the feedback session, the trainer offers this explanation.

518
00:37:19,000 --> 00:37:22,000
If we cannot see...

519
00:37:22,000 --> 00:37:27,000
If his head is not here, then it's ignored.

520
00:37:27,000 --> 00:37:29,000
It's in between her boobs.

521
00:37:29,000 --> 00:37:33,000
So if I don't see directly the contact with the nipple, it's nothing.

522
00:37:33,000 --> 00:37:38,000
You know, that's exactly why I am having so much trouble to understand things.

523
00:37:38,000 --> 00:37:42,000
You have an artistic picture of a photograph of a woman

524
00:37:42,000 --> 00:37:46,000
and you show a tiny nipple on it.

525
00:37:46,000 --> 00:37:52,000
On one hand, this is a delete because we have 100% uncovered nipples.

526
00:37:52,000 --> 00:37:56,000
On the other hand, you have this almost porn photo.

527
00:37:56,000 --> 00:37:59,000
And you don't delete because it doesn't feed the world.

528
00:37:59,000 --> 00:38:01,000
That's exactly why I...

529
00:38:01,000 --> 00:38:07,000
Yes, but you have a small problem because you're still going from what you think

530
00:38:07,000 --> 00:38:09,000
in your decisions.

531
00:38:09,000 --> 00:38:15,000
And we're in school to learn rules.

532
00:38:16,000 --> 00:38:21,000
Applying Facebook's rules without questioning them is the number one rule.

533
00:38:21,000 --> 00:38:25,000
A principle that will be drilled into you all day, every day.

534
00:38:25,000 --> 00:38:30,000
There has to be a line and they drill it around that.

535
00:38:30,000 --> 00:38:35,000
We just need to respect it and we just need to apply it to do our jobs.

536
00:38:35,000 --> 00:38:39,000
Sometimes we'll find disagreements, but I mean, this is the good job

537
00:38:39,000 --> 00:38:44,000
because this is not my social network experience.

538
00:38:44,000 --> 00:38:53,000
A training program with the end goal of turning you into a machine.

539
00:38:53,000 --> 00:38:59,000
Pedro worked for six months as a content reviewer for Facebook at Accenture.

540
00:38:59,000 --> 00:39:05,000
He agreed to respond to our questions, but only if he remained anonymous.

541
00:39:05,000 --> 00:39:13,000
Two years after leaving the company, he still remembers the numbing side of the rule.

542
00:39:13,000 --> 00:39:18,000
You have to play by their game or else you won't have a job at the end of the month.

543
00:39:18,000 --> 00:39:22,000
And it's got two points where I just felt I was a robot

544
00:39:22,000 --> 00:39:27,000
and just doing as many pictures and videos as much as possible

545
00:39:27,000 --> 00:39:31,000
just because that's the only thing I can do.

546
00:39:31,000 --> 00:39:34,000
You're just there with numbers and clicking enter.

547
00:39:34,000 --> 00:39:36,000
Numbers, enter, numbers, enter.

548
00:39:36,000 --> 00:39:44,000
The hardest thing for Pedro is trying to forget everything that he saw on that screen over six months.

549
00:39:44,000 --> 00:39:48,000
We're not prepared for it. We're not mentally prepared for it.

550
00:39:48,000 --> 00:39:52,000
All these stuff, they don't really give us the input before

551
00:39:52,000 --> 00:39:54,000
and it just comes to you as a shock.

552
00:39:54,000 --> 00:39:57,000
It just comes to you as like a wave here, have this in front of you

553
00:39:57,000 --> 00:40:00,000
and you can't really say yes or no to it.

554
00:40:00,000 --> 00:40:04,000
If you give me a million euros, a billion euros, I wouldn't go.

555
00:40:04,000 --> 00:40:06,000
It's not for me.

556
00:40:14,000 --> 00:40:20,000
What Pedro described to us, the wave of shock that washes over you unexpectedly

557
00:40:20,000 --> 00:40:22,000
is exactly what happened to Grégoire.

558
00:40:22,000 --> 00:40:27,000
It started around the fifth day of training during the practical exercises.

559
00:40:34,000 --> 00:40:37,000
A stream of horrific images

560
00:40:37,000 --> 00:40:44,000
and unbearable videos that must be watched closely in order to make the right decision

561
00:40:44,000 --> 00:40:47,000
according to Facebook's criteria.

562
00:40:56,000 --> 00:41:01,000
The same horrific scenes are unfolding on his neighbor's screen too.

563
00:41:04,000 --> 00:41:06,000
I'm going to open the window.

564
00:41:09,000 --> 00:41:11,000
Excuse me.

565
00:41:11,000 --> 00:41:13,000
I'll take a glass on the other side.

566
00:41:24,000 --> 00:41:26,000
I'm not far from Vominier.

567
00:41:26,000 --> 00:41:31,000
I just took a break because I saw the bodies of the members,

568
00:41:31,000 --> 00:41:34,000
but the people who threw themselves from the top of the tower,

569
00:41:34,000 --> 00:41:36,000
they were crushed by the ground.

570
00:41:36,000 --> 00:41:40,000
The nose, the body that trembled, the hands that trembled,

571
00:41:40,000 --> 00:41:43,000
it was really close to today.

572
00:41:47,000 --> 00:41:52,000
It's like this on a daily basis for Grégoire and his group.

573
00:41:52,000 --> 00:41:57,000
Luckily, they can always rely on the useful advice of the trainers to feel better.

574
00:42:02,000 --> 00:42:05,000
If the macarena isn't quite enough to cheer you up,

575
00:42:05,000 --> 00:42:10,000
the business also has psychologists available for the most traumatized moderators.

576
00:42:13,000 --> 00:42:16,000
On this day, a video lasting several minutes

577
00:42:16,000 --> 00:42:20,000
brought the violence to another level for Grégoire.

578
00:42:32,000 --> 00:42:36,000
During the break, everyone tries to shake off the shock

579
00:42:36,000 --> 00:42:40,000
by discussing the grim video they've just witnessed.

580
00:43:01,000 --> 00:43:27,000
Grégoire realizes the extent of the damage this job can cause

581
00:43:27,000 --> 00:43:31,000
when talking with a former moderator who is now a trainer.

582
00:43:57,000 --> 00:43:59,000
I know that I have to be like this.

583
00:43:59,000 --> 00:44:02,000
I can't watch people running across the street.

584
00:44:02,000 --> 00:44:03,000
Anyone.

585
00:44:03,000 --> 00:44:07,000
You're still doing this while you have PTSD?

586
00:44:07,000 --> 00:44:08,000
There is a purpose.

587
00:44:08,000 --> 00:44:12,000
I do feel every day like I'm cleaning the trash.

588
00:44:12,000 --> 00:44:13,000
Right.

589
00:44:15,000 --> 00:44:18,000
Okay, I will watch it, but at least I know that I'm going to watch it.

590
00:44:18,000 --> 00:44:22,000
Someone who's 14 years old is going to quit that and not know.

591
00:44:28,000 --> 00:44:30,000
Even two years after quitting the post,

592
00:44:30,000 --> 00:44:35,000
Pedro still has very vivid memories of certain videos.

593
00:44:37,000 --> 00:44:39,000
There's a few things that I saw.

594
00:44:39,000 --> 00:44:42,000
Those things are going to stay with me

595
00:44:42,000 --> 00:44:45,000
because I remember them as if it was yesterday,

596
00:44:45,000 --> 00:44:48,000
it's very emotional sometimes.

597
00:44:48,000 --> 00:44:50,000
I remember sometimes people used to like,

598
00:44:50,000 --> 00:44:53,000
they were working, being productive,

599
00:44:53,000 --> 00:44:56,000
and suddenly they just stand up and run out of the room.

600
00:44:57,000 --> 00:44:58,000
That's okay.

601
00:44:58,000 --> 00:45:00,000
Trauma built up.

602
00:45:00,000 --> 00:45:04,000
And for Pedro, left him feeling helpless.

603
00:45:04,000 --> 00:45:06,000
But if you see someone getting murdered,

604
00:45:06,000 --> 00:45:08,000
the only action you take is the lead, for example.

605
00:45:08,000 --> 00:45:11,000
You just erase it out of the platform.

606
00:45:11,000 --> 00:45:16,000
You don't really go into depth of like calling the police, for example.

607
00:45:16,000 --> 00:45:20,000
It's like, you never really feel content with what you're doing.

608
00:45:20,000 --> 00:45:23,000
You're just going round and round in circles

609
00:45:23,000 --> 00:45:26,000
and just like bombard with all this stuff.

610
00:45:26,000 --> 00:45:30,000
It's like a mixture of emotions that you go through in one day,

611
00:45:30,000 --> 00:45:33,000
eight hours for it.

612
00:45:33,000 --> 00:45:35,000
How many were you when you started?

613
00:45:35,000 --> 00:45:37,000
We were 30 when we started, 30.

614
00:45:37,000 --> 00:45:42,000
From that 30, it started just decreasing month by month

615
00:45:42,000 --> 00:45:45,000
until now there's only like three people.

616
00:45:47,000 --> 00:45:51,000
Pedro claims that a lot of people struggle to deal with the role

617
00:45:51,000 --> 00:45:53,000
and end up quitting.

618
00:45:53,000 --> 00:45:56,000
To understand what Pedro went through

619
00:45:56,000 --> 00:46:00,000
and what Grégoix and his colleagues are currently experiencing,

620
00:46:00,000 --> 00:46:03,000
we met up with a psychiatrist.

621
00:46:03,000 --> 00:46:06,000
Professor Thierry Boubet is a specialist

622
00:46:06,000 --> 00:46:08,000
in post-traumatic stress disorder.

623
00:46:08,000 --> 00:46:11,000
For example, he works with police officers

624
00:46:11,000 --> 00:46:14,000
who have been involved in terrorist attacks.

625
00:46:14,000 --> 00:46:17,000
We show him the footage we filmed.

626
00:46:18,000 --> 00:46:22,000
Some potentially traumatic images,

627
00:46:22,000 --> 00:46:24,000
like the ones described here,

628
00:46:24,000 --> 00:46:26,000
can have several effects.

629
00:46:26,000 --> 00:46:29,000
For some people, it's just anxiety effects.

630
00:46:29,000 --> 00:46:32,000
It can make you anxious for a while,

631
00:46:32,000 --> 00:46:34,000
sometimes in an important way,

632
00:46:34,000 --> 00:46:37,000
with panic attacks or something like that.

633
00:46:37,000 --> 00:46:40,000
But in some cases,

634
00:46:40,000 --> 00:46:44,000
there can be what we call a traumatic infraction,

635
00:46:44,000 --> 00:46:47,000
i.e. one of these images

636
00:46:47,000 --> 00:46:50,000
or some of these images

637
00:46:50,000 --> 00:46:55,000
will go deeper into us

638
00:46:55,000 --> 00:46:58,000
and come back to us without any stress.

639
00:46:58,000 --> 00:47:02,000
What's special about post-traumatic stress disorder

640
00:47:02,000 --> 00:47:05,000
is that when these images come back without any stress,

641
00:47:05,000 --> 00:47:08,000
they produce without any stress the same stress.

642
00:47:08,000 --> 00:47:11,000
So it's a stress that lasts

643
00:47:11,000 --> 00:47:17,000
and it doesn't disappear, if you like.

644
00:47:17,000 --> 00:47:20,000
We also talk to him about the famous

645
00:47:20,000 --> 00:47:24,000
confidentiality classes imposed by Facebook.

646
00:47:24,000 --> 00:47:27,000
The secret culture and the interdiction

647
00:47:27,000 --> 00:47:30,000
that is made in talking to third parties

648
00:47:30,000 --> 00:47:36,000
is a process that belongs to the records

649
00:47:36,000 --> 00:47:39,000
of what we call mental imprisonment.

650
00:47:39,000 --> 00:47:42,000
Mental imprisonment is something

651
00:47:42,000 --> 00:47:45,000
that is used by different movements,

652
00:47:45,000 --> 00:47:48,000
like, for example, sector movements,

653
00:47:48,000 --> 00:47:52,000
and it makes it even more vulnerable

654
00:47:52,000 --> 00:47:55,000
to traumatic impacts.

655
00:47:56,000 --> 00:47:59,000
Anxiety, trauma, stress.

656
00:47:59,000 --> 00:48:03,000
Cleaning up social media comes at a great cost.

657
00:48:04,000 --> 00:48:08,000
Grégoire decides to quit only two weeks later,

658
00:48:08,000 --> 00:48:11,000
still in his training period.

659
00:48:15,000 --> 00:48:18,000
He received his paycheck just before leaving,

660
00:48:18,000 --> 00:48:21,000
his hourly pay written at the top,

661
00:48:21,000 --> 00:48:24,000
four euros, 62 cents gross.

662
00:48:24,000 --> 00:48:28,000
This is a tough pill to swallow for his colleague.

663
00:48:30,000 --> 00:48:33,000
I was earning more in the ice cream shop.

664
00:48:33,000 --> 00:48:35,000
In the ice cream shop?

665
00:48:35,000 --> 00:48:38,000
Man, that's bad, right?

666
00:48:39,000 --> 00:48:43,000
After our experience there, we contacted Accenture.

667
00:48:43,000 --> 00:48:45,000
Their response was a brief email

668
00:48:45,000 --> 00:48:48,000
that didn't once reference Facebook.

669
00:48:48,000 --> 00:48:50,000
It did, however, contain this phrase,

670
00:48:50,000 --> 00:48:54,000
the well-being of our employees is our priority.

671
00:48:56,000 --> 00:48:59,000
To finish our tour of the Internet's trash cleaners,

672
00:48:59,000 --> 00:49:02,000
the invisible workforce behind your Facebook

673
00:49:02,000 --> 00:49:06,000
or Instagram feed, we had one last meeting.

674
00:49:06,000 --> 00:49:09,000
Sarah Roberts is the leading researcher

675
00:49:09,000 --> 00:49:12,000
specializing in those who work as moderators.

676
00:49:12,000 --> 00:49:15,000
She is a key figure in this field.

677
00:49:15,000 --> 00:49:19,000
We met her at the university where she teaches in California.

678
00:49:19,000 --> 00:49:22,000
She presented us with an analysis of the rise

679
00:49:22,000 --> 00:49:26,000
and development of content moderation over the past year.

680
00:49:27,000 --> 00:49:33,000
We are talking about a scope and a scale of magnitude

681
00:49:33,000 --> 00:49:35,000
that has not been seen before.

682
00:49:35,000 --> 00:49:38,000
Billions of things shared per day on Facebook.

683
00:49:38,000 --> 00:49:42,000
Hundreds of hours of video uploaded to YouTube

684
00:49:42,000 --> 00:49:45,000
per minute per day and so on.

685
00:49:45,000 --> 00:49:47,000
The response has continued to be,

686
00:49:47,000 --> 00:49:49,000
we'll put more content moderators on it,

687
00:49:49,000 --> 00:49:53,000
which means that that continues to exponentially grow.

688
00:49:53,000 --> 00:49:58,000
It has gone from a next to nothing kind of line item

689
00:49:58,000 --> 00:50:03,000
in the budget to being a massive, massive cost center,

690
00:50:03,000 --> 00:50:05,000
meaning it doesn't actually return revenue.

691
00:50:05,000 --> 00:50:07,000
It's not like a new product.

692
00:50:07,000 --> 00:50:10,000
It's just seen as an economic drain.

693
00:50:10,000 --> 00:50:12,000
And the way we manage that problem

694
00:50:12,000 --> 00:50:15,000
is by pushing it onto some low-wage workers

695
00:50:15,000 --> 00:50:17,000
to do it as cheaply as possible,

696
00:50:17,000 --> 00:50:19,000
because, again, that stacks up

697
00:50:19,000 --> 00:50:22,000
when you double your workforce in two years

698
00:50:22,000 --> 00:50:24,000
that it does not come for free.

699
00:50:24,000 --> 00:50:28,000
This is why companies like Facebook use subcontractors.

700
00:50:28,000 --> 00:50:30,000
But according to this researcher,

701
00:50:30,000 --> 00:50:32,000
this isn't the only reason.

702
00:50:32,000 --> 00:50:33,000
It's about labor costs,

703
00:50:33,000 --> 00:50:36,000
but it's also about creating layers

704
00:50:36,000 --> 00:50:39,000
of lessening responsibility

705
00:50:39,000 --> 00:50:41,000
between those who solicit this kind of work

706
00:50:41,000 --> 00:50:45,000
and need it and those who do it and where they do it.

707
00:50:45,000 --> 00:50:46,000
They remove themselves,

708
00:50:46,000 --> 00:50:48,000
they put themselves at a distance

709
00:50:48,000 --> 00:50:50,000
from the workers and their conditions,

710
00:50:50,000 --> 00:50:52,000
and it's not just a geographic distance,

711
00:50:52,000 --> 00:50:54,000
but sort of a moral distance.

712
00:50:54,000 --> 00:50:57,000
So when that content moderator some years later alleges harm

713
00:50:57,000 --> 00:51:00,000
or, you know, is having trouble psychologically

714
00:51:00,000 --> 00:51:02,000
or emotionally because of the work that they did,

715
00:51:02,000 --> 00:51:05,000
then it may be possible for that company

716
00:51:05,000 --> 00:51:07,000
to disclaim responsibility for that,

717
00:51:07,000 --> 00:51:10,000
even though ultimately they really are responsible

718
00:51:10,000 --> 00:51:12,000
because they asked them to do that work in the first place.

719
00:51:12,000 --> 00:51:14,000
Despite these precautions,

720
00:51:14,000 --> 00:51:17,000
three former moderators filed lawsuits

721
00:51:17,000 --> 00:51:20,000
against Facebook in the U.S. a few months ago.

722
00:51:20,000 --> 00:51:23,000
All three were working under subcontractors,

723
00:51:23,000 --> 00:51:27,000
all claimed to be victims of post-traumatic stress disorder.

724
00:51:27,000 --> 00:51:30,000
The American company refused every request

725
00:51:30,000 --> 00:51:32,000
we made for an interview.

726
00:51:32,000 --> 00:51:35,000
They did, however, send us an email to explain

727
00:51:35,000 --> 00:51:37,000
how Facebook, with its partners,

728
00:51:37,000 --> 00:51:41,000
pays great attention to the well-being of content moderators

729
00:51:41,000 --> 00:51:43,000
working on its platform,

730
00:51:43,000 --> 00:51:46,000
which is an absolute priority.

731
00:51:47,000 --> 00:51:51,000
To finish off, here's some of the latest news from the sector.

732
00:51:51,000 --> 00:51:54,000
While these ghost workers are left in the shadows,

733
00:51:54,000 --> 00:51:57,000
it's business as usual for the companies

734
00:51:57,000 --> 00:51:59,000
working in this new sector.

735
00:51:59,000 --> 00:52:01,000
A few weeks after filming,

736
00:52:01,000 --> 00:52:06,000
Figure 8's founder sold his company for $300 million.

737
00:52:06,000 --> 00:52:10,000
Well, at least now, he has good reason to be happy.

738
00:52:47,000 --> 00:52:50,000
Thank you.

