start	end	text
0	2060	you
30000	32060	you
60000	62060	you
90000	92060	you
120000	122360	you
540000	569480	good afternoon I still have to wait for a sign since we
569480	578400	the livestream determines the program are we ready I guess we are ready good
578400	583720	afternoon dear ladies and gentlemen the president of the Bavarian Academy of
583720	588840	Science I would like to welcome you all to our talk focusing on current
588840	594920	developments of advanced computing technologies we are very proud to host
594920	601820	this event in the rural residents in the center of Munich which houses the main
601820	609200	office of the Bavarian Academy of Science founded in 1759 the Academy
609200	615080	functions today as a community of scholars a non-university research
615080	619680	institution and a communication interface between the Bavarian
619680	626240	scientific community society and policymakers with the scholars the
626240	631240	Academy provides a powerful internet disciplinary network of very
631240	637880	established scientists this network of excellence interacts very closely with
637880	643720	all Bavarian research institutions and political decision makers and represents
643720	649920	an important part of the science communication with the public research
649920	655560	activities range from the composer Richard Strauss to the study of climate
655560	661600	change in their alps from baroque ceiling paintings to quantum physics the
661600	666520	longer-term basic research spans from natural science to technology to
666520	672120	humanities and social studies the Academy research project actively
672160	678200	leverage the latest digital technologies the Leibniz supercomputer center also
678200	683880	part of our Academy serves as an important infrastructural support for
683880	689120	digital activities at Bavarian universities today we will hear an
689120	693440	important contribution to the rapidly growing discussions about artificial
693440	699280	intelligence from Jan Lecun who is according to the Time magazine one of
699280	705280	the hundred worldwide leading AI pioneers his professor of NYU on chief
705280	713000	I scientists at Metta not in California in New York I just learned the title of
713000	717960	his talk from machine learning to autonomous intelligence artificial
717960	722120	intelligence as you all know is the key topic of our time not only in research
722120	728000	and industry but also in the broader society this becomes evident as a large
728120	733760	interest and thank you for all coming to this meeting on sharing this
733760	740840	experience we're very pleased as Bavarian Academy to gather with the AI agency to
740840	746840	be part of biosphere biosphere is the official network of all AI activities
746840	753560	in Bavaria we partner in this important task to advance AI science in Bavaria in
753640	759000	close cooperation with the Center for Advanced Studies at LMU the Bavarian
759000	763040	Research Institute for Digital Transformation the Munich Center for
763040	770680	Machine Learning and the Konrad Susie School of Excellence in Reliable AI the
770680	775080	event today is part of this successful cooperation and we're looking very
775080	780480	broadly forward to learn more about recent progress from advanced computing to
780480	787520	autonomous intelligence I hereby hand over to professor Thomas Seidel member
787520	793440	of the Bavarian AI Council chair of database systems and data mining
793440	800280	director of Munich Center for Machine Learning to all of you I wish a very
800440	805760	insightful and informative afternoon the Seidel
813760	819720	yeah thank you President Schweiger for this nice introduction and very warm
819720	825600	welcome also from my side to all of you particularly to Jan Leckardt to be here
825600	832040	today I wear two hats today one is I'm a member of the Bavarian AI Council we
832040	836560	are 20 members appointed by the Bavarian state government as part of their high
836560	841600	tech agenda from universities research institutions and also companies in the
841600	847000	field to advise the government on AI strategies and actions particularly in
847000	853440	steering the Bavarian AI agency so the representative is also Dr. Klimke which
853480	858560	promotes the Bavarian network which we call biosphere so the logo is there as
858560	863120	well so the biosphere comprises a variety of strong AI players in Bavaria
863120	867400	also universities research institutes from all over Bavaria as well as a lot
867400	872000	of the global companies we have here including Google, Microsoft, IBM but
872000	878800	also the locally sitting global players Siemens, BMWs, every insurances Munich
878800	883520	three Alliance and so on but also many original small and medium enterprises
883520	890800	and startups in the field of AI so the second hat I am aware today's I'm one
890800	893920	of the four directors of the Munich Center for Machine Learning one of the
893920	900120	co-directors Daniel Gremmels also here and we this is a consortium of LMU and
900120	905160	Tom funded by the BNBF in the Bavarian high-tech agenda with around 50 PIs in
905160	908800	machine learning in the I3 junior research groups recently established
908800	914480	around 200 doctoral students and our focus is on foundations of machine
914480	919120	learning where we have several players including Gitta Kottiniok from the
919120	923040	mathematical part and statistics and computer science is there then perception
923040	928360	we are particularly strong in computer vision here and in Munich and in natural
928360	934560	language processing the two big things where humans and computers interact
934600	941280	and a lot of domain specific things it's not on research but also on transfer
941280	946120	activities fostering the collaboration network together with the biosphere
946120	950560	outreach to the general public things like that if you're interested also
950560	955200	openings of course so this is that part so I'm sure we get fully inspired by
955200	960240	your presentation and between your presentation is now the next is Dr. Mayer
960240	973360	from TASS, you're on stage, thank you ladies and gentlemen may I also welcome
973360	977920	you warmly on behalf of the Center for Advanced Studies at LMU and let me
977920	982440	briefly say a few words about this institution and how it comes into play
982440	987840	the Center for Advanced Studies at LMU was founded 15 years ago to provide a
987960	993600	forum for precisely those research questions that cannot be tackled by
993600	999200	only one discipline this was intended to take account of an increasingly
999200	1005200	diversifying but also specializing body of research that is becoming more and
1005200	1010640	more disparate not only in terms of content but also in terms of space in
1010640	1016640	Munich you can just think of the campus in Ober-Schleishheim, Ober-Guy-Ching,
1016640	1022600	Nordeid in the far south so the Center for Advanced Studies offers the place
1022600	1029120	where these centrifugal forces can be bundled and for what topic does this
1029120	1034200	task play a more important role than for artificial intelligence which is
1034200	1041080	spread across most faculties of LMU and other universities it was therefore a
1041080	1045600	great pleasure for us when Gitta Kutinyok freshly appointed at our
1045640	1050680	university approached us and asked whether a form it could be found that
1050680	1056920	would network research on AI at LMU and unable to discuss overarching issues
1056920	1062280	together it wasn't long before a so-called interdisciplinary CAS
1062280	1067880	research focused entitled next generation AI was born bringing together
1067880	1073960	researchers from 14 faculties working on the topic of artificial intelligence
1073960	1079960	over the course of two years a wide variety of lectures workshops and
1079960	1085680	conferences was organized and held and we were thrilled by the spirit that
1085680	1091120	emerged of that group that's why we look forward to Professor LeCun's lecture
1091120	1096760	today with both a smile and a tear as it marks the formal conclusion of the
1096760	1102040	research focus we are honored and grateful that Professor LeCun is going
1102080	1119320	to give the lecture in this framework today yeah also a warm welcome from my
1119320	1124320	side to everyone here on site and also to everyone who participates via
1124320	1129240	live stream it is wonderful to have so many people with us here this afternoon
1129920	1132760	and a great thanks to all of our cooperation partners for actually
1132760	1136720	making this lecture possible and here I would like to particularly thank Dr
1136720	1140640	Anette Meyer and the Center for Advanced Studies of the Ludwig Maximilians
1140640	1145400	University MÃ¼nchen Professor Dr. Markus Schweiger and the Bavarian Academy of
1145400	1150120	Sciences and Humanities that this event can take place here in the academy in
1150120	1154640	these really beautiful rooms Professor Dr. Thomas Seidel and Dr. Michael
1154680	1159320	Klimker from the biosphere the Bavarian Eye Network which made the live stream
1159320	1164520	for the event possible and also Dr. Christoph Egle from the Bavarian
1164520	1169360	Research Institute for Digital Transformation and now it's my great
1169360	1174400	pleasure and honor to welcome Professor Jan LeCun thank you very much for
1174400	1179080	accepting our invitation and for coming to Munich for this lecture today
1179680	1184520	Professor LeCun is chief AI scientist at Meta and the silver professor of
1184520	1189680	computer science at New York University he started his career with a PhD in
1189680	1195400	computer science at Sorbonne University in Paris and then moved to the US where
1195400	1200360	he became the head of the image processing research department at the
1200360	1208120	famous Bell Labs the AT&T Bell Laboratories then after intermediate
1208160	1214320	stations he joined New York University in 2003 and he also became there the
1214320	1222000	founding director of the NYU Center for Data Science in 2012. His groundbreaking
1222000	1226320	work includes among many others the development of convolutional neural
1226320	1230480	networks which are the state-of-the-art for basically any problem in particular
1230480	1235360	imaging sciences and computer vision and a particularly particular convolutional
1235560	1241240	network architecture is also named by him the so-called LeNet which in
1241240	1245440	sense also promoted the impressive development of deep learning and AI as
1245440	1252520	we experience it today. His contributions are honored by numerous awards many more
1252520	1257120	than I could name here let me just mention that he's a member of the US
1257120	1261640	National Academy of Sciences and the National Academy of Engineering he
1261640	1267480	received various honorary degrees for instance from EPFL, received the IEEE
1267480	1275800	neural network pioneer award and in 2019 the Turing Award which is typically
1275800	1280880	referred to as the Nobel Prize of Computing and just a few weeks ago we
1280880	1285720	already heard at the Time Magazine and congratulations to that has selected him
1285720	1292280	as one of the 100 most influential people in AI worldwide and he also
1292280	1296840	repeatedly contributes to the public debate about AI with also controversial
1296840	1303680	proclamations for example on the current craze around large language models. How
1303680	1309600	could machines to learn as efficiently as humans and animals? How could machines
1309720	1316360	learn to reason and plan? In his lecture Professor Jan LeKang will now talk about
1316360	1322760	a possible path towards an autonomous intelligent agents based on a new
1322760	1327440	modular cognitive architecture. Where come Jan? The floor is yours.
1339600	1353760	Thank you very much for the introduction and thank you very much for
1353760	1358960	inviting me for coming here so so numerous. I have to correct one thing
1358960	1366960	though I did not call convolutional net solonet this was my lab director at Bell
1366960	1378640	Labs who gave it that name I would never done this but it's a good name okay it's
1378640	1382880	a long title and a long subtitle objective-driven AI this is what I call
1382880	1388360	this I used to give this talk with the title autonomous machine intelligence
1388360	1395040	and and it scares people you know they say do you mean machines that will be
1395040	1397880	autonomous we're not going to be able to control them so I changed the name to
1397880	1402840	objective-driven AI because that's really more accurate and they're really kind
1402840	1406640	of systems it's an aspiration it's not something that we've done it's something
1406640	1411480	that we should do and there are systems that could of course learn remember
1411480	1419080	reason plan have common sense be steerable controllable safe and have the
1419080	1422880	same kind of learning abilities and intelligence that we observe in animals
1422920	1430120	and humans so let me start by a little bit of the state of the art okay because
1430120	1434640	there's a lot of debates today about about AI and a lot of people are afraid
1434640	1440280	of AI it's understandable whenever there is technological revolution people are
1440280	1445280	afraid of the unknown and AI is promising to be a big revolution so people
1445280	1450080	are afraid so let's first talk about the benefits before we talk about the risks
1450760	1457680	and the benefits are of AI are numerous already today and there is you know even
1457680	1463400	more coming in medicine particularly in imaging diagnosis assistant treatment
1463400	1467480	protocol drug design things like this very promising research transportation
1467480	1472480	every car sold in the European Union today has to come with what's called a
1472480	1476920	automatic emergency braking system a system that will automatically stop the
1476920	1482480	car there is an obstacle in front of it and the driver does not react this
1482480	1487960	saves lives it reduces frontal collision by 40% so AI saves lives and that uses
1487960	1496120	convolutional nets by the way and in all the systems that I know in fact Germany
1496120	1503920	was kind of a and and a very in particular was a pioneer in this some of
1503920	1509000	the early systems of this type was the word developed by them events so driving
1509000	1512520	assistance autonomous driving energy storage and management things like that
1512520	1516160	environmental environmental monitoring and protection I'm going to say a few
1516160	1520040	words about this content information and management this is probably the biggest
1520040	1525280	use of AI today and of course in industry manufacturing information systems
1525280	1529720	quality control etc a lot of applications are expected also in things like
1529720	1535080	education for personalized education connecting people with each other with
1535080	1540320	translation today presence augmented reality virtual reality and then
1540320	1545160	enormous applications in science biology and genomics neuroscience physics
1545160	1549600	particularly physics of disordered systems complex systems very large-scale
1549600	1556440	simulations chemistry material science very promising area for AI so this well
1556480	1559800	really and of course you know we've been talking a lot about creation like
1559800	1566480	creating art AI is essentially enabling a lot more people to be creative people
1566480	1570440	who don't necessarily have the technique the underlying technique for
1570440	1575960	producing art so I will affect every aspect of human activity and let me give
1575960	1581120	you a couple examples so this is a video that was put together by my colleagues
1581120	1585840	at meta a couple years ago this is already sort of aging if you want and we
1585840	1592360	chose the capability of computer vision system as of about two years ago so we
1592360	1596800	can have systems that detect objects and put frames around them give them a name
1596800	1601600	they can track human bodies and figure out in what what pose they are densely
1601600	1606400	actually so that's actually very useful for all kinds of applications and more
1606400	1610520	interestingly we can have systems that perform what's called semantic
1610520	1615360	segmentation which means isolating every object marking them with kind of a mask
1615480	1620040	and then giving them a name for a category and this works for a very
1620040	1624880	fine-grained category for example the species of a bird or or plant or
1624880	1629160	something of that type so it's pretty amazing it's not like computer vision is
1629160	1634080	completely solved in fact if it was solved we wouldn't have the large
1634080	1639760	conference that takes place in Paris next week called ICCV so there's still a
1639800	1648400	lot of work to do but but there's been a huge amount of advances there and a lot
1648400	1656920	of advances in AI but no advances in my slides for some reason okay my
1656920	1662920	presentation refuses to advance hang on just one minute one second
1670760	1677600	okay I mentioned medicine so certainly medical imaging is an area where a lot
1677600	1681800	of work is going on there's too many to cite really this is some work by some of
1681800	1689240	my colleagues at NYU that use 3d image recognition not just 2d in some cases
1689240	1694560	this is actually 2d but that use various techniques to detect for example tumors
1694640	1702000	in mammograms or particular things in MRI and other types of images and
1702000	1707360	almost a lot of progress there some product project that took place a few
1707360	1711240	years ago which was a collaboration between the NYU radiology department and
1711240	1717080	people at fair Meta's fundamental research lab which essentially allows to
1717080	1721280	accelerate the data collection for an MRI by a factor of 4 without degrading the
1721280	1726080	image quality so instead of having to lie down in a MRI machine for 40 minutes
1726080	1729320	or something you can reduce this to 10 minutes and have the same quality of
1729320	1734640	images and that's thanks to deep learning essentially a lot of applications in
1734640	1741520	science what's interesting today is that the favorite model that neuroscientists
1741520	1746480	use to explain how the brain works use artificial neural nets so the best
1746480	1750640	explanation for what we observe using functional MRI data in the visual
1750720	1755280	cortex of humans and animals are actually models that are essentially
1755280	1762600	convolutional net models and that's kind of a closing the circle because the
1762600	1766560	architectural convolutional net is actually inspired by the architecture of
1766560	1771800	the visual cortex classic work in neuroscience from the 1960s the similar
1771800	1780280	work also in language understanding this is a recent paper in science by some
1780280	1785360	colleagues from from from it actually and they try to figure out if the
1785360	1789480	current large language models that everybody is playing with explain the
1789480	1794240	what we observe in the brain when people are asked to kind of remember or
1794240	1799360	understand a story and the answer is sort of but not really it doesn't work nearly
1799360	1803680	as well as a convolutional net models for vision so what that means is that
1803680	1808680	we're missing something that those models probably are not sufficient to
1808680	1814040	explain what the brain does when when we understand language I mentioned some
1814040	1818480	applications in science in particle physics in particular high energy physics
1818480	1824520	to kind of make models of particle collisions and things of that type image
1824520	1830280	processing to discover exoplanets some estimate says that about 12 percent of
1830280	1836000	all physics papers today actually mention AI as a tool that he used which is
1836040	1842560	astonishing in just a relatively short time and in the large-scale simulation
1842560	1846080	sort of universe scale simulation that could sort of validate or invalidate
1846080	1851760	certain theories about dark matter and things I guess so very fascinating work
1851760	1857520	and applications this is a very interesting project that was started by
1857520	1863280	some of my colleagues at fair by Larry Zittnick in particular called the open
1863280	1868360	catalyst project and you can actually participate if you want the website is
1868360	1878840	open-catalyst.org and and that project the idea of that project is that we
1878840	1883800	could solve climate change if we had a good efficient scalable way of storing
1883800	1890400	energy if we had a good way of storing energy we could cover a small desert
1890680	1896880	with solar panels and produce enough energy to power Europe or the entire
1896880	1903700	planet the problem is you have to have a way of storing energy which is why
1903700	1910760	renewables today despite the decisions of the German government to go all out on
1910760	1917840	it renewables are not drivable you can't control whenever there is wind or sun
1918120	1923960	and so you need another source of energy when there is no no sun or or no no
1923960	1928800	wind and and for that you need to be able to store energy and ship it wherever
1928800	1932120	it's needed the best way to store energy is in the form of hydrogen or maybe
1932120	1937080	methane and the best way to do this is by separating hydrogen from oxygen from
1937080	1940960	water right so take some water put two electrodes and then separate hydrogen
1940960	1948840	from oxygen problem with this is that it's either scalable if you use catalyst
1948840	1956360	to do this like platinum sorry it's either efficient if you use catalyst like
1956360	1960800	platinum or it's scalable but not efficient and so the big question is
1960800	1966640	could we design compounds new catalyst that would facilitate this reaction so
1966640	1971440	that is efficient but does not require exotic materials like like platinum so
1971440	1976240	that is scalable and the idea there is that you do a lot of chemical
1976240	1982080	simulation that's called DFT simulation of various of water on two various
1982080	1986520	compounds and then you generate that data using simulation and also using
1986520	1990320	experiments you put that data you make it available and then you ask people can
1990320	1994240	you train machine learning system to figure out what the underlying rule is
1994240	1999960	so that we can use it to design new materials that might have the same
1999960	2006520	effect but be cheap so fascinating program it may not work but it's worth
2006520	2013160	a shot okay now what's important to realize is that the progress we've seen
2013160	2016720	over the last few years in AI and machine learning are due to a set of
2016720	2020720	techniques that we call self-supervised running which I'm sure many of you here
2020720	2027160	in the room have heard about and essentially self-supervised running
2027160	2032880	would be a set of techniques that allows a system to be trained to represent the
2032880	2041280	data the world without requiring labeled data okay without requiring sort of
2041280	2049960	manual human intervention to produce the data so perhaps the best success of
2049960	2055680	this idea which I've been advocating for a long time is in the context of natural
2055680	2061960	language understanding so the way all NLP systems are trained today whether
2061960	2066800	there are LLMs of the types that we play with or others is the following you
2066800	2072360	take a piece of text a sequence of words and you remove some of the words you
2072360	2077040	you mask you mask them you blank them out you replace them by a blank marker okay
2077080	2081720	you corrupt essentially the input and you put it at the input of a large neural
2081720	2085320	net you train this very large neural net usually usually a transformer
2085320	2091240	architecture to predict the words that are missing in the process of doing so
2091240	2096920	the system has to extract representations of the text that contain the
2096920	2103320	semantics the syntax the you know grammar everything I sort of lied slightly
2103480	2108120	here these are not words that are input they are what's called tokens which are
2108120	2113720	essentially subword units so in most languages words have a prefix and a
2113720	2118400	root and a suffix and you need to kind of separate those for those systems to
2118400	2122200	work properly otherwise your dictionary of words would be gigantic and then in
2122200	2125160	German you have to do it because you can have words that are long like this
2125160	2130600	they are you know by so so there is no choice you have to break up words into
2130600	2137000	subword units you know in tokens and so you train that you train the system and
2137000	2143320	and this is the so-called BERT model if you want or idea and that's me
2143320	2146800	incredibly successful it's completely self-supervised you don't need any other
2146800	2151760	data than the text and once you've pre-trained that system you can use the
2151760	2157680	internal representation produced by the system as input to a subsequent task a
2157680	2162400	downstream task like let's say translation hate speech detection you
2162400	2166200	know summarization whatever so that's the general idea of self-supervised
2166200	2170880	running fill in the blanks have a big piece of data corrupt it in some way
2170880	2176120	and then train some big neural net to fill in the blanks or or recover the
2176120	2181200	original data a particularly stunning example of this which I'm not going to
2181200	2187200	go into the technical details of but I will later is a system they came out of
2187240	2192360	my colleagues that in Paris that fair Paris called Dino v2 you can think of it
2192360	2196160	as a foundation model for vision so it's a system that is trained to extract
2196160	2199960	features from images such that those features can be used for anything you
2199960	2203960	want whether it's classification fine-grained classification depth
2203960	2208200	estimation semantic segmentation instance retrieval so the same kind of
2208200	2212120	application that I showed in the video but basically with very little
2212160	2216880	supervision this is then it's pre-trained and it basically because it's
2216880	2223640	pre-trained on enormous amounts of data just training a very shadow head to
2223640	2228360	solve any particular one of those problems actually beats the state of the
2228360	2232600	art for that's estimation or classification or whatever you can
2232600	2237160	actually play with it interactively that's the URL that you see here and
2237160	2241240	these are some examples of visualization of what the features that are
2241280	2246640	extracted are it's kind of a you know colorful representation of the like
2246640	2250840	different feature vectors are represented by different colors this is
2250840	2253600	actually kind of each color is like a principal component if you know what
2253600	2259080	that is so those are you know examples on sort of typical typical images and
2259080	2262600	people I've started to use this for all kinds of stuff for biological image
2262600	2269120	analysis for astronomy for for environmental protection so that's the
2269120	2273240	next example I'm going to show you so this is a project by someone on the team
2273240	2278480	Camille Coupri and a large collection collection of collaborators and what she
2278480	2286800	did was use the Dino V2 features and trained relatively small system on top
2286800	2293600	of it to tell what the height of the trees are from a satellite image so we
2293600	2297400	have lots of satellite images on the entire world at half meter resolution
2297400	2302560	you can get this from a satellite imaging companies and for some areas
2302560	2307960	there is LiDAR data which tells you how tall the trees are so you use that to
2307960	2311040	train the system and then you can apply it to the entire world and what it tells
2311040	2317840	you is how much how much carbon is captured by the trees if you know
2317840	2321160	roughly what the height of the tree is you know roughly how much carbon is
2321600	2327600	captured in the tree that's super important to know like you know should
2327600	2333200	we protect forests of course we should should we plant more trees where things
2333200	2337960	like that so very interesting this publications on this where you know
2337960	2343240	everything is detailed and everything another success of self supervised
2343240	2347120	running of the type that I showed for natural language processing where you
2347400	2355720	remove some other words is in biology proteomics particularly so you can the
2355720	2361120	protein is a sequence of amino acids and we know hundreds of millions of them so
2361120	2364080	you take a sequence of amino acids you remove some of the amino acids and you
2364080	2367480	train some gigantic neural net to predict the amino acids that are missing
2367480	2372640	the system kind of learns to represent sequences of amino acids that
2372640	2376840	constitute proteins and then you use that representation as input to a system
2376880	2380720	that predicts the conformation of that protein how it folds well they can stick
2380720	2384840	to another protein a particular location so there's a famous work by our
2384840	2390280	colleagues at DeepMind at Fairfold but the this idea of using pre-trained
2390280	2396440	transformers for protein was actually first published by my colleagues at fair
2396440	2400400	they're actually no longer at fair now they have left Fairf to create a startup
2400440	2407840	around this around this idea but it's incredibly successful thousands of
2407840	2411840	research groups around the world are using this kind of data is actually a
2411840	2417920	atlas of folded protein contains 600 million proteins or something like that
2417920	2426680	with the structure that is predicted it's called the ESM metagenomic atlas and
2427440	2434480	ESM atlas.com a very big tool for biologists that really may change
2434480	2438920	completely the way we do drug design and understand the mechanisms of life
2438920	2446320	another very impressive project here that required a lot of effort is a project
2446320	2451480	called no language left behind again from fair collection of people from the
2451480	2455240	various sites of fair and this is a system that can translate 200 languages
2455880	2461920	from in any direction and when you look at what those languages are it's a lot
2461920	2467200	of languages most of them we never heard of in you know square corners of the
2467200	2472480	world but it's important for people to be able to preserve that culture that you
2472480	2476360	know they can speak their language and basically be understood using automatic
2476360	2481360	translation so what's interesting about this is that there are four thousand
2481800	2489120	directions for translation but the data only covers 2400 of those pairs
2489120	2497960	among the 40,000 despite that because we train a giant transformer to represent
2497960	2503200	language regardless of the language the system takes advantage of the
2503200	2507080	similarities between between the language families to actually kind of
2507080	2511160	extract a multilingual language independent representation of language
2511160	2514920	which allows the system to do translation in any direction including
2514920	2519960	four directions has never been trained on that's pretty amazing pretty small
2519960	2529680	model but today standard only 54 billion parameters I mean sizable the same team
2529680	2534480	now as another project called seamless which was was announced a few weeks ago
2534480	2541840	they can do speech to speech speech to text text to speech and text to text
2541840	2547760	translation as well as speech recognition speech synthesis etc speech to
2547760	2553680	speech is interesting because it can do translation for languages that are not
2553680	2557360	written directly from speech to speech that system can handle a thousand
2557360	2563320	languages which is really impressive okay so applications of deep learning that
2563320	2569160	are less visible perhaps is that deep learning or AI connects people to
2569160	2573920	knowledge and they connect people to each other the biggest deployment of
2573920	2578320	machine learning today is probably in social networks and online services like
2578320	2583560	like search engines and if you take deep learning out of Google or Meta or
2583560	2590640	Microsoft companies crumble they literally are built around it so deep
2590640	2593040	learning helps us deal with the information deluge for doing things
2593040	2596720	like search and retrieval ranking question answering things like this but
2596720	2600600	and that requires machine to understand content of course for
2600600	2605160	translation which is very useful for people who are not literate for example
2605160	2610080	or people are blind or visually impaired so there's three billion people in the
2610080	2614800	world today who can't use technology because they basically can't read more
2614800	2619800	or less so here's the biggest use of AI today filtering out illegal and
2619800	2624480	dangerous content and this is something that's very hard to do it's impossible to
2624480	2631440	do perfectly but to tell you to give you an idea of how much progress AI has made
2631440	2635520	those idea of pre-training transformers and stuff like that the
2635520	2642920	proportion of hate speech that Facebook was able to take down automatically five
2642920	2649560	years ago was about 20 to 25 percent okay it was using sort of fairly simple
2649560	2656000	machine learning techniques NLP methods of the types that were common five years
2656000	2666520	ago and then self-supervised pre-trained transformers happened and that number
2666520	2673400	went to 95% last year and it's just progress in AI so a lot of people that
2673400	2677600	we hear talk about AI who generally don't know much about AI actually tell you
2677600	2681640	about all the dangers of AI that then you know AI is going to destroy I don't
2681640	2685400	know democracy because of disinformation and things like that what they
2685400	2688760	don't understand is that AI is actually the solution to those problems it's not
2688760	2692360	actually the problem it's the solution to those problems and it's already the
2692360	2697560	case that doing content moderation on social networks makes massive use of the
2697560	2703080	latest advancements in AI and the people who try to corrupt that system are not
2703240	2711200	sophisticated in terms of their AI so something that needs to be known okay
2711200	2715760	but everybody is excited about generative AI and autoregressive large
2715760	2722000	language models and things of that type right so many of you certainly I'm sure
2722000	2725200	have played with those image generation things where you type a text and
2725200	2730240	outcomes image and this is the state of the art about a year and a half ago from
2730280	2734920	either a meta and make a scene system or a penny I dali to or Google's image and
2734920	2745800	as of yesterday this is what you get out of meta so this is actually from a
2745800	2750440	paper and you can get the paper from archive it's there but there's a product
2750440	2756600	attached to that paper called emu it's an acronym but actually don't remember
2756640	2765000	what it means and what the system can do is in it can generate images from a
2765000	2770840	text prompt and it was rolled out as a product yesterday as well as the paper
2770840	2774000	right so it's one of the things where like the science the research the
2774000	2780080	technology and the product come out to the same day pretty crazy and this is
2780080	2783680	available in Facebook Messenger if you use Facebook Messenger you can you can
2783680	2790840	ask to talk to meta AI that is the name of the intelligent virtual assistant
2790840	2798800	at meta the generic ones and then if in a font you type backslash sorry forward
2798800	2805640	slash imaging and type a text then the system will produce an image in five
2805640	2812480	seconds this used to take minutes the results are pretty amazing the same team
2812520	2818120	is is working on synthesizing video this is actually some work from about a year
2818120	2822840	ago they're making progress on sort of practical things of this type okay but
2822840	2827680	how do those LLMs those large language models you know that you can talk to how
2827680	2833840	do they work they are autoregressive right so what that means is they are of
2833840	2837040	the type that I talked about before you take a text and you remove some other
2837040	2841960	words and then you turn train assistant to predict the words except it's a
2841960	2846360	special case where you only train the system to predict the last word okay to
2846360	2851520	take a long piece of text remove the last word and train this gigantic neural
2851520	2857680	net to predict that last word and if you train the system this way you can do
2857680	2862480	what's called autoregressive prediction which means give a text predict the last
2862480	2867120	word or the next word then inject that into the input and then predict the next
2867120	2871720	next word and then shift that into an input produce the third word etc.
2871880	2878320	Autoregressive prediction and it's amazing how it works there's a whole
2878320	2883840	bunch of those models around actually I typed that list a few a couple months
2883840	2887960	ago and now there's a whole bunch more but Blunderbot Galactica Lama Lama 2
2887960	2894280	from from meta which is actually a open source code Lama that came out in July
2894280	2900320	which is basically Lama specialized for generating code Alpaca Lambda Chinchilla
2900360	2904760	chai GPT the various incarnations of chai GPT and then there is one that came
2904760	2910040	out just a few days ago called mistral via a French startup in Paris formed by
2910040	2914960	people who used to be at fair and deep end actually that's interesting so
2914960	2919760	performance is amazing for those systems right we've all been surprised by it but
2919760	2922880	they do make really really stupid mistakes they don't really understand the
2922880	2928720	world they they're trained to produce the most likely sequence of words that
2928760	2931760	follow a particular prompt and then they're kind of fine-tuned to sort of
2931760	2935280	work well for particular types of questions but they make factual errors
2935280	2940680	logical errors they are inconsistent they don't really have reasoning abilities
2940680	2947880	it's very easy to kind of chorus them into producing toxic content they really
2947880	2951040	do have a limited knowledge of the underlying reality because they're
2951040	2955760	purely trained from text they don't have common sense like a cat can have common
2955760	2961680	sense and they can't plan their answer so you can you can play with Lama so
2961680	2967280	basically the chatbot I just mentioned meta AI is sort of a productized
2967280	2972040	version of Lama too if you want and it has various incarnations actually various
2972040	2976400	personas that you can call and there's three models the production model is a
2976400	2980760	different one but it's open source you can download it if you're a big enough GPU
2980760	2985080	you can read it on your GPU there's a lot of people working towards running those
2985080	2989400	models on mobile devices and laptops and things like that and they they can
2989400	2993880	generate text this is a funny one so in the early days of Lama my colleagues
2993880	2999080	kind of interrogated that so they typed into Lama did you know that Yanlok
2999080	3002720	dropped a rap album last year we listened to it and here is what we thought
3002720	3010040	and this and the system writes a critique of my alleged rap album so they
3010040	3014480	showed this to me and they say is it okay if we put this in the paper and say
3014480	3019360	yeah sure no problem but I said like could you do this with jazz because you
3019360	3026520	know I'm like I'm rap is okay but like I prefer jazz really and they told me yeah
3026520	3032480	yeah we tried and it didn't work because there's not enough training data for
3032480	3043960	jazz so I cried so as I was saying you can fine-tune the system to sort of play
3044000	3049960	different roles and what Mita announced yesterday is that is 28 different
3049960	3052680	chatbots that are specialized for different applications so think for
3052680	3058120	example you can have Snoop Dogg a rapper be a dungeon master if you are into
3058120	3064240	dungeon and dragon or text adventure games others that are like advisors for
3064240	3070560	traveling others that are cooks or or sous chefs or whatever so different
3070760	3077600	but those things really suck I mean they really not that great because they
3077600	3081080	don't understand the world they just manipulate language because they
3081080	3084600	manipulate language fluently we're fooled into thinking that they are
3084600	3089360	intelligent but they're not intelligent in certain ways but they're not
3089360	3095920	intelligent in sort of what we think as as human intelligence so you will see if
3095920	3101040	you go to X from a Twitter or any kind of social networks people who make
3101040	3107000	posts say oh there is a latest LLM from so-and-so company and you type this and
3107000	3111280	it's mind-blowing you know we are this far away from human level intelligence
3111280	3116560	what I call a GI I hate the term and you know it's for tomorrow like you know all
3116560	3120640	the naysayers are wrong blah blah blah it's just happening tomorrow they are
3120640	3124880	wrong okay this those things do not have anything close to human intelligence
3125160	3129040	they appear to do to have to have it because they're trained on so much data
3129040	3133120	that they've accumulated an enormous amount of background knowledge
3133120	3137840	approximately that they can regurgitate approximately so whenever they seem
3137840	3142120	intelligent it's usually because they can do information retrieval in an
3142120	3146400	approximate way that sort of looks reasonable but they cannot possibly
3146400	3150240	understand how the world works because their only training data is text and
3150240	3154400	most of human knowledge this may surprise you but most of human knowledge has
3154440	3159440	nothing to do with language it has to do with our experience with the world
3159440	3167960	every day physics another limitation that people have been pointing out
3167960	3172920	increasingly with various papers is the inability of those LLMs to plan so an
3172920	3178840	LLM produces those tokens autoregressively as I explained earlier right they
3178840	3182640	don't plan their answer they just produce one token after the other and
3183040	3186360	whatever token they produce will determine which token they produce next
3186360	3192960	because it's autoregressive there is a process by which the system is basically
3192960	3197320	an exponentially divergent process the system makes one mistake that takes it
3197320	3205520	out of the kind of correct set of answers it cannot recover and so this
3205520	3210720	entire architecture of autoregressive prediction in my opinion is is inherently
3210720	3215280	flawed and my prediction is that within a few years nobody in their right mind
3215280	3220320	would use autoregressive LLMs okay everybody is working towards something
3220320	3227480	better because those things are major flaws now what's the issue though is
3227480	3231960	that there's a lot of people who are scared about future AI systems that may
3231960	3236680	have the may attain attain human intelligence or or be more intelligent
3236680	3241720	than humans and if you extrapolate from what LLMs currently do you might think
3241720	3244840	well it's gonna be very dangerous because those systems cannot really be
3244840	3249560	controlled they can spew complete nonsense they can be jail broken blah
3249560	3253600	blah if they are smart they might be dangerous that's a big mistake future
3253600	3257720	AI systems will not be using this particular blueprint they're not going
3257720	3262640	to be autoregressive LLMs okay and I'm going to tell you what I think it will
3262640	3273200	be okay so autoregressive LLMs suck I just said all that no reasoning no
3273200	3279240	planning essentially right the amount of computation devoted to producing a
3279240	3284480	single token by an LLM autoregressive LLM is constant there's a constant amount
3284480	3288800	of computation per token produced so there's no possibility for the system
3288840	3293840	to for example think about something for a long time before saying something it's
3293840	3303560	cannot do that by construction so machines do not of this type do not
3303560	3309200	learn how the world works unlike animal and animals and humans they will not be
3309200	3316280	able to approach human intelligence okay so whatever I don't know the CEO of some
3316320	3320480	company that thinks they have the best LLM in the world tells you a GI is just
3320480	3327080	around the corner don't believe that we're still missing some major advances
3327080	3335840	but there is absolutely no question that eventually machines will surpass human
3335840	3342160	intelligence in all domains okay it's basically no doubt about that and it's
3342200	3350440	going to happen during the lifetime of most people here maybe not me you know I
3350440	3357240	might take a few decades there's no question it's going to happen so these
3357240	3362680	are I think the biggest challenges for AI going forward learning representations
3362680	3368000	and predictive models of the world and I'll tell you why in a minute and that's
3368000	3371160	what's addressed by self-supervised learning so we have good handle on this
3371200	3377560	at least for text not so much for video learning to reason so if some of you know
3377560	3381680	about Daniel Kahneman's theory of system one system two sort of subconscious
3381680	3384920	things that we do without thinking and then conscious things that we have to
3384920	3391040	focus our attention on LLMs currently can do system one but not system two we
3391040	3395640	need to build AI systems that are capable of reasoning of the type that Daniel
3395800	3401400	Kahneman calls system two is a Nobel Prize winning well he won the Nobel Prize
3401400	3409680	in economics but is a psychologist and one possible path towards a solution
3409680	3413320	that I've been proposing for about a year now you're gonna have is what I call
3413320	3417600	this objective driven AI so this paper I put on open review it's not on archive
3417600	3421240	it's an open review because on open review you can make comments and and this
3421240	3424800	is a working document more than a kind of finished paper if you want it's long
3424840	3428360	though you can also listen to technical talks I've given about this are a
3428360	3433360	little more technical than the current one and it's based on this idea of a
3433360	3438640	modular cognitive architecture where you have a system composed of multiple
3438640	3445000	modules first module be the perception so it's represented overlaid over the
3445000	3448800	back of the brain because in the human brain perception is in the back so
3448800	3454280	perception basically perceives the world and then constructs an estimate of the
3454280	3458360	state of the world right so it produces an estimate of the state of the world
3458360	3463520	perhaps it needs to combine this with the content of a memory that contains you
3463520	3467480	know other information about the state of the world that is not currently
3467480	3472480	perceptible and then that goes into a world model and the role of the world
3472480	3479880	model is to imagine the outcome of a sequence of actions okay so the system
3479920	3484160	can imagine a sequence of actions that's the role of the actor the yellow
3484160	3488960	module so the actor imagines a sequence of actions fits that to the world model
3488960	3492600	the world model knows the current state of the world and what the world model
3492600	3496880	predicts is the future state of the world that will result from that sequence
3496880	3502120	of actions now that cannot be a perfectly exact prediction because the
3502120	3509200	world is not entirely predictable but that's the the the role of the world
3509240	3513760	model and then the entire purpose of the system is to figure out a particular
3513760	3518960	sequence of actions that will predict a state of the world that satisfies a
3518960	3524120	certain number of constraints that are implemented by the cost module so the
3524120	3529680	red module that you see this cost module that that's the drive of the system
3529680	3534440	that's the the current goal of the system if you want and the entire purpose of
3534440	3538360	the system so imagine this module as getting the predictions from the world
3538360	3544760	model and then computing a cost for it right so basically it computes the degree
3544760	3552000	of in comfort of the system discomfort and what the system does is that it
3552000	3555720	figures out internally a sequence of actions so the actor does that it figures
3555720	3558720	out a sequence of actions that will minimize its cost according to the
3558720	3566040	predictions of the world model okay and this is very much system 2 type it's
3566040	3569960	very similar to what you know people do classically in optimal control it's
3569960	3574720	called model predictive control and it's really like this right observe the state
3574720	3578400	of the world get an initial world state representation combine that with what
3578400	3581760	you think about the state of the world from your memory feed a sequence of
3581760	3584920	actions to your world model and ask the world model to predict where the final
3584920	3588560	state will be then feed that to your objectives the objectives might
3588560	3593720	implement the goal that the system has set for itself or that you set it for
3593760	3598840	it but also you can have a number of guardrails so might be a guardrails if
3598840	3605920	we have a domestic robot that is cooking has a knife in its hand because it's
3605920	3611520	cutting onions or whatever you might have a cost that says if you have a knife
3611520	3616840	in your hand and there are people around you don't move your hand too fast okay
3616840	3621960	don't flail your arms right so maybe dangerous so you can imagine all kinds
3622000	3628080	of guardrails of this type to basically ensure the safety of the of the system and
3628080	3631360	the system has no choice but satisfy those because they are satisfied at
3631360	3637680	inference time they're not it's not like RLHF for LLMs reinforcement learning
3637680	3643320	through human feedback where it's a it's a training time fine-tuning to make
3643320	3649440	sure the system produces only safe behavior the system can always produce
3649600	3654200	unsafe behavior by you know being prompted something that the the people
3654200	3657960	training it didn't think of it didn't think about here that's impossible the
3657960	3661080	system cannot produce a sequence of actions that will not satisfy the
3661080	3666040	guardrails according to the world model so those systems would be intrinsically
3666040	3673160	safe provided two things provided that the guardrail objectives guarantee the
3673160	3677720	safety and that's complicated also provided that the world model is accurate
3677760	3682280	and that's also complicated so you can imagine something like this that works
3682280	3687400	over time so that you know you can have a sequence for example in this case
3687400	3692520	sequence of two actions you can have and again this is very similar to what
3692520	3696720	control theory is called model predictive control except here we're
3696720	3702120	learning the world model and possibly learning the cost as well you might want
3702120	3705960	to imagine the system like this that does hierarchical planning humans animals
3706000	3709680	do hierarchical planning all the time it's a essential characteristic of what
3709680	3714320	we can do and we don't know how to do this at the moment we have some ideas
3714320	3717760	working on it but it really doesn't work like if there is like a really good
3717760	3722640	opportunity for young scientists or aspiring scientists to really solve a
3722640	3726880	problem like try to see if you can do something about hierarchical planning
3726880	3735280	because it's really hard but the payoff if you can do it I think is enormous so
3735280	3741440	a good example of this is let's say I'm at NYU in my office at NYU and I want to
3741440	3745440	go to Paris okay so my objective is my distance to Paris I want to minimize my
3745440	3750280	distance to Paris at a high level I can say well first thing I need to do is go
3750280	3753960	to the airport and then catch a plane and there is a latent variable that may
3753960	3757960	indicate like which airport I'm choosing depending on traffic or whatever or what
3757960	3764240	airline flights at what time okay now how do I go to the airport well I have
3764240	3768800	to go down in the street and catch a taxi you can do this in New York you can
3768800	3773400	just tell the taxi in the street how do I go down in the street I need to stand up
3773400	3778200	for my chair open the door go to the stair staircase of the elevator how do I
3778200	3783880	get out from my chair I need to kind of push with my arms or something or or
3783880	3787640	turn my chair and then you know you imagine you can imagine decomposing this
3787640	3791480	all the way down to millisecond by millisecond muscle control I'm not going
3791480	3797520	to plan my entire trajectory from my NYU office to Paris in terms of millisecond
3797520	3801240	by millisecond muscle control that would be classical planning it has to be
3801240	3807440	hierarchical and people can do this today I mean engineers do this in control
3807440	3813480	but those various levels in a hierarchy are designed by hand the question is can
3813480	3816800	we train a machine to automatically learn what the proper hierarchical
3816840	3822320	representation of the action plan is and that's the answer problem yeah you're
3822320	3829000	looking to do a phd or something or two or three that's a good problem
3831360	3836680	we could use techniques like this for LLMs so LLMs that would be non-auto
3836680	3840920	aggressive instead of producing one token after the other they would basically
3840920	3846360	infer a sequence of tokens that would satisfy a number of objectives on a
3846360	3849560	guardrail an objective that measures to what extent you're answering the
3849560	3855600	question and an objective that measures to what extent the answer is non-toxic
3855600	3863120	or toxic or whatever right that would make LLMs controllable nothing like
3863120	3867520	this works today right again if you are looking for a good topic for a phd that's
3867520	3875920	a good one ultimately we need machine to learn to understand the world that's the
3875960	3880960	purpose of that world model the essential central piece of that architecture
3880960	3885480	I just talked about is this world model given the state of the world at time t
3885480	3888200	given an action I might take or a sequence of actions what is going to be
3888200	3895360	the state of the world at time t plus one or t plus whatever and humans and
3895360	3901000	animals are amazingly good at this babies learn how the world works in the first
3901080	3906400	few months of life at an amazing speed and they learn an incredible amount of
3906400	3909360	background knowledge about the world first thing you learn is that the world
3909360	3914400	is three-dimensional then you learn that something like object permanence the
3914400	3919840	fact that when an object is hidden behind another one it still exists okay
3920200	3922200	five
3926200	3932000	and babies learn things like basic notions like gravity in the around the age of
3932000	3935720	nine months takes a long time to learn intuitive physics like like inertia
3935720	3939600	gravity things like that okay but it's mostly just by observation a little bit
3939600	3946000	by experimentation and we don't know how to reproduce this kind of learning with
3946040	3951120	machines and that's why although we have fluid systems that can pass the bar
3951120	3960800	exam or medical exams we don't have robots that can clear up the dinner table
3960800	3965600	and fill up the dishwasher something that any 10 year old can learn in one
3965600	3971040	shot in a few minutes we don't even have completely autonomous level five cell
3971040	3976960	driving cars even though any 17 year old can learn to do this within 20 hours
3976960	3986120	and then drive at 300 kilometers an hour on the Autobahn you know obviously we're
3986120	3990840	missing something really big with machines that humans and animals can can do
3990840	3995600	in terms of learning that learning efficiency that we don't we don't know
3995600	3999840	how to reproduce so we need this ability to learn world models to get machines to
3999840	4004400	learn world models from video essentially from natural signals and so
4004400	4007480	this is idea of self-supervised learning but now apply to video not text and it
4007480	4013640	turns out text is easy text is easy because text is discrete and finite it's
4013640	4017520	only a finite number of possible tokens in every language on the order of 30
4017520	4022960	thousand or something and so it's easy to predict a distribution a probability
4022960	4026760	distribution over the next token you can represent it by a long list of numbers
4026760	4031920	between 0 and 1 that's on to 1 but if you want to predict video you can't do
4031920	4035200	that because we don't know how to represent probability distributions over
4035200	4039080	all possible videos at least not in a good way so if you train a neural net to
4039080	4043520	predict what happens in a very simple video this is over overhead video from a
4043520	4050000	highway you get this kind of prediction very very blurry prediction because the
4050000	4053240	system can only predict the average of all the possible things that can happen
4053240	4058160	and it can't make make up its mind so the solution I'm proposing to this is
4058160	4066160	something I call joint embedded joint embedding architecture okay or joint
4066160	4072440	embedding predictive architecture JEPA and this is a non generative
4072440	4075120	architecture so everybody is talking about generative AI what I'm telling you
4075120	4083480	here is abandoned generative models okay so not only am I telling you AI is not
4083480	4088960	gonna kill us but LLM suck machine learning sucks and generative models
4088960	4094200	suck right all the popular things at the moment okay so a generative model
4094200	4098200	predicts you know if you have an observation x you're trying to predict
4098200	4102600	y just predict y from x using an encoder and some predictor right but what
4102600	4107720	problem with this is that you have to predict every single details of y and in
4107720	4110920	video that's just too much in text it's okay it's just like you know what word
4110920	4114440	okay you don't know exactly what word but it's okay in video it's just not
4114440	4118680	possible so what you should do instead is what's on the right here the joint
4118680	4123320	embedding architecture where you run both x and y through encoders the
4123320	4126920	encoders eliminate all the irrelevant details about the input and the
4126920	4131440	prediction takes place in representation space okay so joint
4131440	4135160	embedding predictive architecture JPA there's several incarnations of this
4135160	4140600	I'm not gonna go to the details because I don't have time and you can read the
4140600	4150880	details in this long paper I can't read what's on it but I can imagine and that's
4150880	4156320	kind of the basic JPA architecture so let me skip ahead a little bit there's
4156400	4163880	two ways to train those JPAs basically two major techniques to train
4163880	4166680	those JPAs that cannot be understood within the context of probabilistic
4166680	4170480	methods but only within the context of what I called energy based models and I
4170480	4174480	was going to explain what this was but I skipped that section but you don't need
4174480	4177760	to know about energy based model to understand what I'm gonna say so there's
4177760	4182560	several methods to train those JPAs and this is a particularly interesting one
4182600	4190520	this is a paper that was published at CVPR just a few months ago it's called
4190520	4198960	image JPA and it's using this masking idea so you take an image you mask
4198960	4204320	regions of that image okay and you feed that partially masked image to an
4204320	4209000	encoder the encoder produces a representation and with that representation
4209040	4213160	you try to predict using another neural net predictor you try to predict the
4213160	4216680	representation for use from the full image okay and they both they run
4216680	4220520	through essentially identical encoders so not identical one of them uses
4220520	4225320	something called exponential moving average weights but but but they're
4225320	4231160	almost identical and and that works amazingly well so you you train the
4231160	4235840	system this way pre-train it with images that you corrupt by masking them
4235840	4240080	partially and you get amazing result on using the features that are produced by
4240080	4244040	that system you get amazing results for classification for segmentation for all
4244040	4249600	kinds of stuff and the Dino method I told you about before is very similar to
4249600	4254440	this it uses kind of a slightly different way of encoding the outputs but
4254440	4260800	it's it's in spirit it's very much the same idea and it gives really good
4260800	4266880	performance on image recognition on transfer tasks on all kinds of stuff
4266880	4271120	that I don't have time to tell you about okay but things we're working on today
4271120	4274520	that we need to work on because we don't know how to do it perfectly is self
4274520	4278080	supervised running from video so basically a version of this image JEPA
4278080	4282520	that would work for video and learn good representations of videos by
4282520	4286640	observing the world basically the same thing that babies can do right so we
4286640	4292720	have a project along those lines V JEPA and we have a paper that we're just
4292720	4296960	submitting to a conference that some of you probably know what it is because the
4296960	4302400	deadline is today well actually if you know what it is you're probably not here
4302400	4308520	you're working on your paper I think the deadline is passed by two hours so maybe
4308520	4314960	maybe you're here so then you would be able to use those JEPA as world models
4314960	4320840	right because you know you have an input and you can feed it maybe a set of
4320840	4325800	actions that an agent might take and it will predict a representation abstract
4325800	4329200	representation of the state of the world at the next time step and so this could
4329200	4332080	be used perhaps as a world model as one of the components of the big
4332080	4341000	architecture I introduced earlier okay okay I said that already all right so
4341040	4346200	there's quite a question that we need to answer with AI and this is my second
4346200	4351320	last slide how long is it going to be before we reach human level AI years to
4351320	4355920	decades probably decades it's probably harder than we think it's certainly much
4355920	4362760	harder than what the most boasting people believe there's many problems to solve
4362760	4367240	along the way and before we get to human level AI we're going to get to
4367240	4370800	something like cat level AI okay so people who are scared that you know
4370800	4374720	one day someone is going to discover the secret of human level AI is going to
4374720	4378240	turn on this gigantic computer and that gigantic computer is going to take over
4378240	4383640	the world and kill everyone that's just ridiculously stupid just cannot possibly
4383640	4386920	happen we're gonna start small we're gonna you know start with something that
4386920	4390360	has all the right components but it's small it's not gonna be very smart it's
4390360	4396360	gonna be like a rat or a cat right and then we're gonna work our way up and you
4396360	4400440	know change the objectives to make sure it's safe and test it in all kinds of
4400480	4405640	sandboxes and blah blah blah so this idea somehow that you know the discovery of
4405640	4410400	AI is going to be an event and that machines are going to escape for control
4410400	4417360	that's Hollywood movies it's not the real world there is no such thing as a
4417360	4422840	GI anyway because intelligence is really a multi-dimensional thing humans are
4422840	4428760	only good at certain things and terrible at many things in fact our minds are
4428760	4433480	extremely specialized we don't realize this but we're incredibly specialized and
4433480	4438760	we know this because computers are much better than us at many tasks for example
4438760	4446600	chess go poker pretty much every video game I mean not today but eventually
4446600	4452760	recognizing a species of a bird by just listening to the song recognizing an
4452760	4457560	individual whale or marine mammal by the shape of the tail like AI systems can do
4457560	4464080	this a very small number of humans can do all of this I mean we just we totally
4464080	4468920	suck at chess as humans machines are much better than we are and so we don't
4468920	4475120	have general intelligence ourselves so this word a GI makes no sense human
4475120	4480400	level yes a GI no there's no question as I said before that machines will
4480400	4486000	eventually surpass human intelligence and so people are scared by this but
4486040	4493840	really is a interesting question to ask ourselves imagine a future maybe 20 years
4493840	4500720	from now or maybe longer where every single one of our interactions with the
4500720	4504440	digital world is mediated by an AI system okay and it might happen faster
4504440	4509400	actually okay if some of the startups are being created today and some of the
4509400	4514040	big company plans product plans actually fulfilled this may happen fairly
4514040	4518880	quickly that essentially every time that we want to connect to the digital
4518880	4524720	world that will be through the intermediary of an AI system then those
4524720	4533120	systems will become the repository of all human knowledge right and it's very
4533120	4539800	important for that at least the base for a foundation of this to be open source
4540160	4544400	every infrastructure the internet is open source runs on open source software
4544400	4549880	and the reason is because it's too important for one company to control it
4549880	4556000	right so it's the same for AI systems they will have to be open source because
4556000	4560000	it's too important for any single company or small number of Californian
4560000	4566520	company to control AI systems if all of our information of all the citizens are
4566560	4572000	basically filtered through those AI systems the way those systems will be
4572000	4576240	trained will need to be quite sourced kind of like Wikipedia to collect
4576240	4580640	culture and information and knowledge from the entire world not just from the
4580640	4587840	view of the world in parallel to us in place right so that's why I'm a huge
4587840	4594480	advocate of open source base models for AI and a number of my colleagues at
4595200	4599240	Meta and his company policy at Meta to open source those base models because it
4599240	4605960	makes them safer more powerful they progress faster they're more culturally
4605960	4610200	diverse if more people can train them and it creates an entire ecosystem of
4610200	4615800	startups and research projects that can build on top of it so it's a very
4615800	4620280	important political questions at the moment because a lot of companies are
4620280	4624240	pressuring governments around the world including the German government to
4624240	4629400	basically keep AI under lock and key to say AI is too dangerous it needs to be
4629400	4639120	controlled and licensed and and not put into the hands of everyone I think it's
4639120	4642280	the exact opposite I think it's too dangerous to actually keep in the hands
4642280	4647720	of just a few a few people okay so I became a little philosophical political
4647720	4654040	here those people have convinced the UK government the Prime Minister that
4654040	4660160	AI should be regulated under lock and key apparently the EU Commission also is
4660160	4669800	convinced this is very bad and I think if we do it right AI will make everybody
4669800	4674120	smarter it's like we all have those intelligent assistant with us all the
4674120	4677800	time it's like having a staff of intelligent people working for you okay
4677800	4683120	every person who is leading anything including me only works with people who
4683160	4686880	are smarter than them right I only hire people who are smarter than me because
4686880	4690080	that's the way to be successful so that everybody is gonna be like that we'll
4690080	4693960	have AI assistant that are smarter than us we shouldn't feel threatened by them
4693960	4697680	because we'll be controlling them they will be designed to be subservient to us
4697680	4703560	so this may have an effect on society similar to what the printing press had
4703560	4710480	probably 500 years ago not too far from here of basically causing a new
4710520	4714240	renaissance because intelligence is really the commodity that we lack the
4714240	4720480	most this will make humanity smarter thank you very much
4740480	4751360	yeah thank you so very much Jan for an amazing lecture we have about 10 minutes
4751360	4762200	for questions and I'm sure there are several so much for the great talk
4762200	4766360	customer from a minute you alluded to the fact that we should keep code open
4766360	4770320	which is great however as you know right many of the recent developments not
4770320	4773360	just rely on the code but also on the hardware so many of the things are
4773360	4777960	developed at companies because they have access to a large GPU resources now not
4777960	4781800	only Germany I guess we are limited by that so and what's your take on that
4781800	4786600	also being in an academic and an meta environment right how do you deal
4786600	4789920	yourself with it do you do some things only at universities and others only at
4789920	4796320	meta or how I mean how do you see that in the future okay should have used an
4796320	4799920	automatic speech recognizer because there is an awful echo and it's very hard to
4799920	4807680	understand it's not your fault but anyway I mean hardware is a big limitation so
4807680	4811880	currently the only entities they can train large language models that are
4811880	4817480	good are people who have access to large amounts of computation either in-house
4817480	4824240	which is the case for Google and meta and Microsoft or through cloud services
4824240	4828000	which is the case for open AI and entropy can others they have access to
4828000	4832240	Microsoft Azure and you know some of them use AWS some of them use other
4832240	4837640	other tools so but that costs a huge amount of money so training a sort of
4837640	4841880	top-of-the-line language model today you know costs tens of millions of euros
4841880	4850000	right it depends how many tens depends on how you do it possibly more if you
4850000	4855840	want to buy an infrastructure that is sufficient power today you have to buy
4855840	4864720	basically stuff from Nvidia and is going to cost you a number with with 10
4864720	4872040	digits it's in the billions it's insane so what that tells you is that it's like
4872040	4877440	it's like an autobahn you don't want 10 parallel autobahn going from one city to
4877440	4883040	another city you just want one and that has to be sort of accessible to everyone
4883040	4887840	so that's the idea behind open source base model foundation model they need to
4887840	4891320	be open source because it's a common infrastructure they can be customized
4891320	4896440	and there's no point having 50 of them because they cost so much to train that's
4896440	4899760	another argument for open source
4913080	4919520	hello yes hi thank you for the great lecture there was a slide that had
4919520	4924120	challenges of AI and machine learning and there were three points in there it
4924120	4931600	went by really fast and I couldn't catch if ethical fairness responsible AI was a
4931600	4937720	challenge that you're facing with and if so what are you doing about it okay so I
4937760	4943520	think that point was wrapped into the the second point but sort of not
4943520	4948360	mentioned directly it was more can I mention in the other thing so this idea
4948360	4952800	of objective driven AI the fact that a system can only produce answers that
4952800	4957480	satisfy a number of objectives including some guardrails the answer to your
4957480	4960960	question is how do you design those guardrails essentially we don't have an
4960960	4965480	answer to this and the reason we don't have an answer to this is because we
4965520	4971920	haven't really began to build systems of this type and so it's as if someone in
4971920	4978560	1925 asked aviators how are you going to make sure that transatlantic
4978560	4983760	transatlantic flight at near the speed of sound will be safe nobody could be
4983760	4989280	possibly answering this nobody across the Atlantic on a plane in 1925 at least
4989280	4996120	not in there not without any stuff nobody knew what a turbojet was like the
4996120	4999200	idea of you know speaking at near the speed of sound was completely
4999200	5004000	unthinkable so we're a bit in the same situation we don't know how exactly how
5004000	5008880	to make those things safe because we haven't built them yet but I think it's
5008880	5012720	an engineering problem like any other problem and there is a there's a fallacy
5012720	5017240	also which is that to design those objectives a lot of people say oh we've
5017280	5020760	never done this before so we're not going to know how to do it but in fact we are
5020760	5025360	doing this all the time we've been doing it for millenia designing objectives so
5025360	5032000	that intelligent entities behave properly that's called laws lawmaking is
5032000	5036920	designing objectives for humans to behave properly and it's even designing
5036920	5042080	objectives for superhuman entities to behave properly superhuman entities
5042080	5045800	like corporations for example right corporate law basically is a way to make
5045800	5050600	sure that whatever a corporation does is aligned more or less with the common
5050600	5054040	good of society right of course you know they can be corruption and everything
5054040	5058040	but that's the that's a big idea so we're very familiar with this with this
5058040	5062640	this concept it's not it's not new
5069040	5073640	and thank you Jan for the really great talk I want to follow up on the question
5073640	5079440	we had earlier about GPU resources what I see is that in machine learning and AI
5079440	5084480	the biggest breakthroughs in the last years were achieved with huge amount of
5084480	5092040	GPU resources the amount that academic institutions typically do not have do
5092040	5097560	you see a future for academic research in the field of AI so I'm gonna can make
5097600	5104600	myself I have two hats okay let me tell you something many of the best ideas come
5104600	5110120	from academia okay the whole idea of generating images from text and things
5110120	5115260	like this those actually came out of a German university right and then you
5115260	5120320	know people picked it up and made products out of it but originally this
5120320	5124520	was done not too far from here in the university the whole idea of using
5124560	5129280	attention mechanisms which is the basis of transformers they came out of
5129280	5135940	university Montreal so that was an interesting story they this was Dimitri
5135940	5141280	Bada now Kim Jong-chul who is now a colleague at NYU and Joshua Benjo and
5141280	5144880	they came up with this idea that when you build a translation system the system
5144880	5150600	should be able to decide which word to look at to translate you know English
5150600	5154500	into let's say German in fact German was the main issue because you know the
5154500	5159780	verb is at the end so it screws up all the translation system so so that was
5159780	5165180	actually the solution to that problem and and they came up with this idea of
5165180	5171260	this kind of learn attention mechanism and then there was a paper that that by
5171260	5175060	Chris Manning at Stanford that sort of picked up this architecture and made it
5175060	5180460	work at scale so they won the WNT competition a few months later and then
5180460	5183780	the entire industry jumped on it right and then you know as much people at
5183780	5186980	Google said oh you can build an entire neural net based on just this idea and
5186980	5190460	the title of the paper was attention is all you need and that was the
5190460	5196260	transformer and you know so the root of some of those good ideas are very often
5196260	5199500	in academia then the problems that I talked about you know how you do
5199500	5202420	hierarchical planning how you're done world models from video that kind of
5202420	5205980	stuff these are things you don't need enormous amounts of computation to
5205980	5210940	demonstrate the principle you may not be able to you know beat some benchmark
5210940	5215260	results or whatever but it doesn't matter if you show that a principle can
5215260	5219060	work and it's convincing enough then there'll be other people who pick it up
5219060	5222380	and actually build something real out of it it's okay that's the way you have
5222380	5233740	intellectual impact so if you think about your career and what drove you would
5233740	5238260	you say that more the dreams you had about what could be possible or you're
5238300	5245180	so interested in the topic let yeah just all the work you contributed to and how
5245180	5251500	that maybe change also over time yeah so interesting question I think at the
5251500	5256820	root of it is really a scientific question what is intelligence how does
5256820	5262500	the brain work you know it's a very sort of front and center big big scientific
5262500	5267540	question over time so right there's three big scientific questions is what you
5267540	5270900	know what's the universe made of what's life all about and how does the brain
5270900	5278100	work right three questions but then I'm kind of an engineer as well so for a
5278100	5281620	complex system like the brain the only way to really understand how it works is
5281620	5285740	that you build one yourself and you verify that like all the hypothesis that
5285740	5289660	you built into your system actually kind of correspond to what happened and it's
5289660	5294740	really the inspiration behind convolutional nets and multilayer learning
5294980	5299540	and the whole idea of neural nets in the first place right getting inspiration
5299540	5302940	from the brain but not copying it because you copied blindly you're not
5302940	5306540	gonna get anywhere you need to understand the underlying principles so
5306540	5309940	underlying the understanding the underlying principles of intelligence is
5309940	5314060	really what kind of drives me and then it's great if you have like multiple
5314060	5319420	applications whether they are useful or entertaining I mostly don't do this
5319420	5324660	myself but but I'm really happy with people do it
5327340	5334020	hello LeCun I want to ask you a question what's your opinion on the field of
5334020	5338780	embodied AI and robot learning I think it's very interesting because it's
5338780	5344100	deployed artificial intelligence techniques to change the real world yes
5344380	5352860	I completely agree so in fact in fact that's kind of one of the point that I
5352860	5358620	perhaps didn't make clear enough that this idea of world model as I said is
5358620	5362100	easy to do in the context of language which is why we have language models
5362100	5368300	that are so impressive but it's very hard to do in the context of the real
5368300	5372740	world data video things like that property of sensitive data from a robot
5373020	5378700	and so the good news about the good thing the good aspect of embodied AI of
5378700	5382420	like working with with robots whether they are real or simulated is that you
5382420	5388780	can't cheat you can't take shortcuts like representing everything as a word or
5388780	5395660	something although some people are trying to do that but so I think
5396020	5403820	focusing on this kind of type of problem I think makes people honest so I
5403820	5409100	think the most interesting advances in in AI over the last several years are
5409100	5413980	not in LLMs they are in people who do robotics and try to do control and sort
5413980	5419860	of make robots basically learn efficiently without having to be trained
5419860	5425740	by you know for hours in simulation this teams there's a colleague at NYU
5425740	5430660	Lera Alpinto who's working on this there is I mean I've grouped a colleague
5430660	5435140	Emelon and his colleagues and then probably the biggest group working on
5435140	5440100	this is at Berkeley Peter Abiel, Segelle Yvine and Chelsea Finn who is a former
5440100	5443980	student of theirs at Stanford those are really kind of interesting approaches
5443980	5451180	there this whole idea of planning objective-driven kind of planning you
5451180	5455820	have to do that in the context of robots so in that sense is very interesting
5455820	5459820	there's a whole division at fair that actually is called embodied AI for that
5459820	5468740	reason thank you yeah thank you so much Jan I mean this amazing lecture and I
5468740	5472060	think you're all very grateful that you shared your thoughts and perspectives on
5472060	5476380	future AI with us and I think we all got a lot of impulses from this today so we
5476380	5479780	have a small gift for you as well
5503060	5507060	thank you
5510060	5515580	yeah so let me also again I mean thank all cooperation partners who contributed
5515580	5521380	to this event so the Center for Advanced Studies biosphere the Varian Academy of
5521380	5526260	Sciences Humanities Munich Center for Machine Learning the Varian Research
5526260	5530780	Institute for Digital Transformation and the Konrad Susi School of Excellence in
5530780	5536100	Reliable AI and sorry I would like to have a special thanks to Dr. Ursula
5536100	5540420	Olinger who is science manager at my chair and who headed actually the
5540420	5546820	organization of the entire event so I think she deserves a small applause
5554540	5558980	yeah thanks also everyone for coming here and also for those who joined us
5558980	5563860	via live stream we now have we I would now like to invite you to a little
5563860	5569580	reception in this Sitzung Sal 1 and 2 which is here right around the corner
5569580	5573580	so thank you so much
5588980	5591040	you
