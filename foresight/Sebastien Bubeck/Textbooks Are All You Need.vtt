WEBVTT

00:00.000 --> 00:06.420
Hi, everyone. Today, I'm going to tell you about

00:06.420 --> 00:10.820
the new methodologies that we are exploring here at

00:10.820 --> 00:14.560
Microsoft Research to train large language models.

00:14.560 --> 00:17.860
This is joint work with a really fantastic team.

00:17.860 --> 00:22.420
You can see all of the courses here on this slide,

00:22.420 --> 00:26.200
and the name of the methodology is Textbooks Hourly.

00:26.200 --> 00:29.380
Let's jump right in.

00:29.380 --> 00:33.380
This methodology, we are currently

00:33.380 --> 00:36.340
applying it to train new large language models,

00:36.340 --> 00:40.180
and we have two of those models that I will talk about today.

00:40.180 --> 00:43.020
This is a first batch in a series of models

00:43.020 --> 00:45.680
that we hope to create with this new technique.

00:45.680 --> 00:47.780
The first one is Phi1,

00:47.780 --> 00:53.660
and the second one is Phi1.5 that we just released very recently.

00:53.660 --> 00:57.380
Both of those are small large language models,

00:57.380 --> 00:59.620
small at least by the standards of today,

00:59.620 --> 01:02.420
with only 1.3 billion parameters model.

01:02.420 --> 01:05.260
What are those models?

01:05.260 --> 01:08.140
Phi1 is a coding model with

01:08.140 --> 01:12.060
performance that we evaluate to be comparable to models that

01:12.060 --> 01:15.340
are roughly 10 times bigger and trains on data set,

01:15.340 --> 01:17.660
which are more than a 100 times bigger,

01:17.660 --> 01:21.020
so three orders of magnitude gains for Phi1,

01:21.020 --> 01:23.860
for coding specifically and coding in Python.

01:23.860 --> 01:28.220
Phi1.5 is our technique applied to natural language,

01:28.220 --> 01:33.140
and there we estimate that roughly Phi1.5 is comparable to models

01:33.140 --> 01:37.340
that are 10 times bigger and train on at least 30 times more data.

01:37.340 --> 01:40.700
Moreover, this is performance in terms of natural language,

01:40.700 --> 01:44.420
but if you go to reasoning and specifically coding and mathematics,

01:44.420 --> 01:51.260
Phi1.5 should be compared to models that are probably more like 50 times larger.

01:51.620 --> 01:54.620
I will come back to this later in the presentation.

01:54.620 --> 01:57.500
I'm first going to focus on Phi1 and coding to explain

01:57.500 --> 02:00.180
the textbooks how you need methodology.

02:00.180 --> 02:03.820
First, let me just backtrack for one second.

02:03.820 --> 02:07.620
What are we talking about with large language models for coding?

02:07.620 --> 02:10.100
Well, we're talking about things like co-pilot,

02:10.100 --> 02:13.180
where you give the beginning of some code and then you

02:13.180 --> 02:15.860
ask for basically a completion of the code.

02:15.860 --> 02:19.540
This is what those large language models for code are doing.

02:19.540 --> 02:22.820
We're going to tell you a new models like this with

02:22.820 --> 02:27.620
only 1.3 billion parameters that does this code completion very well.

02:27.620 --> 02:29.420
Now, the question, natural question is,

02:29.420 --> 02:33.620
how do you evaluate the performance of an LLM on code?

02:33.620 --> 02:39.460
There is a quite standard benchmark by now from OpenAI called HumanEval.

02:39.460 --> 02:43.220
This is one way to evaluate coding models.

02:43.220 --> 02:48.300
This is 164 handwritten programming questions with unit tests.

02:48.300 --> 02:51.340
Each of those programming question comes with a set of unit tests,

02:51.340 --> 02:55.300
and you say that you absolve the task if you can pass all of these unit tests.

02:55.300 --> 02:57.900
What do those programming questions look like?

02:57.900 --> 02:59.460
They look like this.

02:59.460 --> 03:01.860
You get a name for the definition of the function,

03:01.860 --> 03:04.820
like increase list, and then you get a doc string

03:04.820 --> 03:07.420
explaining to you what's in natural language,

03:07.420 --> 03:10.940
what the function is supposed to do with maybe some example,

03:10.940 --> 03:13.260
and now the goal is to complete,

03:13.260 --> 03:17.180
to write the code that does what the doc string is asking you to do.

03:17.180 --> 03:22.100
Here, just two examples, one where you have to return codes that,

03:22.100 --> 03:25.620
given a list, increase all elements by one,

03:25.620 --> 03:28.820
and this other one is given a list of integer,

03:28.820 --> 03:32.220
return the sum of all the odd elements that are in the event position.

03:32.220 --> 03:34.300
You see, these code completions are very easy,

03:34.300 --> 03:35.420
it's just one line of code,

03:35.420 --> 03:36.740
so this is very simple,

03:36.740 --> 03:38.700
but in the HumanEval benchmark,

03:38.700 --> 03:41.580
there are also much more difficult coding questions.

03:41.580 --> 03:46.500
Now, let's look at the progress that have been made on this benchmark

03:46.500 --> 03:47.900
in the last couple of years.

03:47.900 --> 03:51.940
As you all know, AI has made incredible progress recently.

03:51.940 --> 03:55.500
Let's see how it looks like in action on this specific task

03:55.500 --> 03:58.740
of coding and specifically HumanEval.

03:58.740 --> 04:04.580
So HumanEval was introduced again two years ago in July 2021 by OpenAI,

04:04.580 --> 04:07.700
and at the time they created also two models,

04:07.700 --> 04:10.260
the Codex series of model,

04:10.260 --> 04:13.580
and these were coding large language model,

04:13.580 --> 04:15.580
and here I'm giving you two example.

04:15.780 --> 04:18.580
One is a 300 million model,

04:18.580 --> 04:21.340
and one is a 12 billion parameters model.

04:21.340 --> 04:23.140
So you see here is the model size.

04:23.140 --> 04:26.660
Both of those models were trained on 100 billion tokens.

04:26.660 --> 04:29.020
So this is the size of the dataset,

04:29.020 --> 04:31.540
and what scores did they reach on HumanEval?

04:31.540 --> 04:35.100
Well, the 300 million only got a mere 13%

04:35.100 --> 04:37.420
while as you scale up the size of the model,

04:37.420 --> 04:40.540
you get this amazing, fantastic magical behavior

04:40.540 --> 04:42.980
that as you scale up the model, the performance improved,

04:42.980 --> 04:46.180
and you can see a huge jump to almost 29%.

04:46.180 --> 04:47.980
Okay, so this was two years ago.

04:47.980 --> 04:50.460
So what happened since then?

04:50.460 --> 04:53.020
Well, shortly after something interesting happened,

04:54.140 --> 04:57.540
these folks at CodeGen, they created a model

04:57.540 --> 05:01.660
trying to mimic OpenAI, but they open sourced their model.

05:01.660 --> 05:03.300
They obtained very similar result.

05:03.300 --> 05:06.540
You see that at 350 million, they also got 13%.

05:06.540 --> 05:09.140
At 16 billion, they got the 29%.

05:09.140 --> 05:14.140
They had a dataset much bigger, 500 billion tokens,

05:14.340 --> 05:18.380
yet no real improvement in terms of the HumanEval.

05:18.380 --> 05:21.340
Now, after that, things started to heat up a little bit,

05:21.340 --> 05:24.220
and you can see the big model started to emerge

05:24.220 --> 05:29.220
with palm coder, specifically 500 billion parameters,

05:29.620 --> 05:31.380
a much bigger dataset.

05:31.380 --> 05:35.540
We are now nearing the 1 trillion tokens, 780,

05:35.540 --> 05:37.260
and you get again a little boost,

05:37.260 --> 05:40.020
but you see some kind of diminishing return.

05:40.020 --> 05:42.900
We moved from 300 million to 12 billion.

05:42.900 --> 05:46.540
We got a 2x improvement from 13 to 29%.

05:46.540 --> 05:49.220
Now you move to 12 billion to 500 billion,

05:49.220 --> 05:51.940
and you only get an improvement to 35%.

05:51.940 --> 05:55.020
So maybe some kind of little diminishing return.

05:55.020 --> 05:58.860
Of course, GPT 3.5, we don't know the size of the dataset,

05:58.860 --> 06:01.380
but this is 175 billion parameters,

06:01.380 --> 06:05.260
and this reaches an amazing 47%.

06:05.260 --> 06:07.540
Other models, you see, they are increasing the size

06:07.540 --> 06:10.540
of the data, and you get very little improvement.

06:10.540 --> 06:11.540
Here's the center coder.

06:11.540 --> 06:13.500
It's interesting because it's 1 billion parameter,

06:13.500 --> 06:15.300
like the model that I will tell you about,

06:15.300 --> 06:18.020
and it reaches only 14%.

06:18.020 --> 06:21.700
But then, of course, GPT 4 happened roughly six months ago,

06:21.700 --> 06:26.700
and this made an amazing jump to 67% on HumanEval.

06:27.300 --> 06:30.060
And in fact, this is a version that was released in March,

06:30.060 --> 06:32.540
but the version we had access to in Sparks

06:32.540 --> 06:34.540
even got a higher score.

06:34.540 --> 06:38.260
So GPT 4 really kind of cracks the problem.

06:38.260 --> 06:42.340
Now, shortly after GPT 4, things really started to heat up,

06:42.340 --> 06:45.540
and we see that we have new models coming out every month.

06:45.540 --> 06:49.620
In fact, in May, we have many models that came out,

06:49.620 --> 06:52.820
and they all share roughly the same characteristic.

06:52.820 --> 06:54.940
You see that the dataset, they are huge,

06:54.940 --> 06:57.420
500 billion, 1 trillion.

06:57.420 --> 07:00.100
Some of the recent ones here are slightly smaller,

07:00.100 --> 07:04.140
but the models are all big, 15 billion parameters

07:04.140 --> 07:07.660
you get in the 30%, 16 billion parameters

07:07.660 --> 07:10.100
you get again in the 30, 35%.

07:10.100 --> 07:12.700
So you see it's all roughly in the same ballpark,

07:12.700 --> 07:15.980
maybe with WizardCoder being the exception,

07:15.980 --> 07:19.020
which is a jump at 16 billion parameters,

07:19.020 --> 07:23.380
1 trillion tokens, it gets to 57%.

07:23.380 --> 07:24.860
Now, let me tell you what we did.

07:24.860 --> 07:27.260
We phi one, and the textbooks are all in it approach,

07:27.260 --> 07:29.340
which I will explain in a minute what it is.

07:29.340 --> 07:31.140
Here is the result that we got.

07:31.140 --> 07:33.580
We have a much smaller model,

07:33.580 --> 07:35.660
only 1.3 billion parameters.

07:35.660 --> 07:39.100
A dataset which is incomparable to those other datasets,

07:39.100 --> 07:44.100
only 7 billion tokens, yet we reach 50.6% accuracy.

07:44.580 --> 07:47.340
So except for GPT-4 and for WizardCoder,

07:47.340 --> 07:49.300
this is the best in this table,

07:49.300 --> 07:53.340
despite being so much like three orders of magnitude smaller.

07:53.340 --> 07:55.460
And moreover, we're gonna talk about

07:55.460 --> 07:57.660
whether we were overfitting to human eval,

07:57.660 --> 07:59.700
this is a very natural question.

07:59.700 --> 08:01.540
We will talk about it a little bit later,

08:01.540 --> 08:03.540
but I can already give you another benchmark,

08:04.500 --> 08:08.180
mostly basic Python programs or Python puzzles.

08:08.180 --> 08:11.300
We get 55% there, which is higher, for example,

08:11.300 --> 08:12.780
than WizardCoder.

08:12.780 --> 08:15.620
So this is just a small indication right now

08:15.620 --> 08:18.900
that we're not at least overfitting to human eval.

08:18.900 --> 08:20.940
So now let me explain to you what we did.

08:20.940 --> 08:23.460
What is this textbooks are all in it approach?

08:23.460 --> 08:26.380
And for that, let me just backtrack one second.

08:26.380 --> 08:29.940
What are those datasets that all those other models

08:29.940 --> 08:33.380
are using to train their large language models for code?

08:33.380 --> 08:35.740
Well, one publicly available dataset

08:35.740 --> 08:39.380
is this very nice dataset called the stack,

08:39.380 --> 08:44.380
which is three terabytes of data collected from GitHub,

08:44.980 --> 08:47.180
which is under a permissive license,

08:47.180 --> 08:49.220
so we can use it to train.

08:49.220 --> 08:51.420
And these folks, they release the dataset

08:51.420 --> 08:52.860
and you can see here the distribution

08:52.860 --> 08:54.260
of programming languages.

08:54.260 --> 08:55.900
And we're gonna look specifically

08:55.900 --> 08:59.380
at the Python subset of this dataset,

08:59.380 --> 09:02.340
which is made of 26 billion tokens.

09:02.340 --> 09:04.260
Okay, so it's a small part of the stack

09:04.260 --> 09:06.940
and we're gonna focus on that, okay?

09:06.940 --> 09:10.020
Now, what does this dataset look like?

09:10.020 --> 09:14.060
How does it look to learn how to code from the stack?

09:14.060 --> 09:16.620
Well, in this dataset, you have documents

09:16.620 --> 09:19.140
that look like this one, okay?

09:19.140 --> 09:22.540
So, you look at this code, good luck

09:22.540 --> 09:25.460
to kind of understand how to code from this.

09:25.460 --> 09:27.060
I mean, there is no explanation.

09:27.060 --> 09:29.500
This is probably like a document in the middle

09:29.500 --> 09:31.340
of a much bigger project.

09:31.340 --> 09:33.380
It's hard to say what this is gonna do

09:33.380 --> 09:35.220
if it does anything at all.

09:35.220 --> 09:37.860
So, it's very hard to learn anything

09:37.860 --> 09:39.380
from a document like this.

09:39.380 --> 09:43.100
Yet, a lot of the stack is made of documents like that,

09:43.100 --> 09:45.100
which are part of much bigger project

09:45.100 --> 09:48.620
and it's hard to make sense of them in isolation.

09:48.620 --> 09:52.900
Now, you also have documents like this one on the left.

09:52.900 --> 09:55.100
This one is much higher quality.

09:55.100 --> 09:56.620
You can learn something from it.

09:56.620 --> 09:59.120
You see that you have simple functions,

09:59.120 --> 10:01.480
which are well-defined with some comments,

10:01.480 --> 10:03.160
with some doc strings that explain to you

10:03.160 --> 10:08.160
what the model is, what the function is supposed to do.

10:08.560 --> 10:12.120
It stands to reason that for a large language model,

10:12.120 --> 10:14.560
it's gonna be much easier to learn from documents

10:14.560 --> 10:17.320
like the left compared to document on the right.

10:17.320 --> 10:19.140
Maybe with document on the right,

10:19.140 --> 10:21.940
what you can do is merely learn some syntax,

10:21.940 --> 10:24.480
but you cannot really learn any real reason.

10:24.480 --> 10:26.880
So, our idea in textbooks, our only need is,

10:26.920 --> 10:28.840
why don't we focus on data set

10:28.840 --> 10:31.040
that only contains example on the left

10:31.040 --> 10:32.760
rather than example on the right?

10:32.760 --> 10:35.320
Why don't we focus on data

10:35.320 --> 10:39.600
that is only of textbook quality level, okay?

10:39.600 --> 10:41.200
Now, how are we gonna do that?

10:41.200 --> 10:44.200
How are we gonna filter maybe the stack

10:44.200 --> 10:46.000
so that it contains, so that we retain

10:46.000 --> 10:49.680
only the textbook quality level material?

10:49.680 --> 10:54.000
Well, we have this amazing new tool at our disposal, GPT-4.

10:54.000 --> 10:58.120
And GPT-4 can reliably classify high educational value.

10:58.120 --> 10:59.640
If you prompt it correctly

10:59.640 --> 11:01.680
and you give it the two documents

11:01.680 --> 11:03.600
that I gave you on the previous slide,

11:03.600 --> 11:05.980
it will tell you that the document on the left

11:05.980 --> 11:07.800
is of higher educational values

11:07.800 --> 11:09.680
than the document on the right, okay?

11:09.680 --> 11:12.040
So, you can use GPT-4 to get a score

11:12.040 --> 11:15.520
on how useful to teach a skill

11:15.520 --> 11:17.360
a certain document is gonna be.

11:17.360 --> 11:18.880
Now, here's the problem.

11:18.880 --> 11:21.560
I told you that the stack for Python

11:21.560 --> 11:23.560
is roughly 26 billion parameters.

11:23.560 --> 11:24.760
In fact, it's a stack dedupe

11:24.760 --> 11:27.960
where there is some deduplication method being applied.

11:27.960 --> 11:29.320
So let's call it SDP.

11:29.320 --> 11:31.760
So SDP is 26 billion tokens.

11:31.760 --> 11:34.000
If you were to use Azure OpenAI

11:34.000 --> 11:37.320
to label all of those documents using GPT-4,

11:37.320 --> 11:39.840
this would cost you around $1 million,

11:39.840 --> 11:40.880
which is a lot of money.

11:40.880 --> 11:43.640
And maybe for a scientific experiment,

11:43.640 --> 11:45.720
it might be a little bit too much.

11:45.720 --> 11:49.520
But GPT-4 can do so much more

11:49.520 --> 11:52.560
than just classify high educational value document.

11:52.560 --> 11:54.640
It can do many, many things as you all know,

11:54.640 --> 11:57.640
and as we discussed in the Spark's paper.

11:57.640 --> 12:00.720
So why don't we try to train a classifier

12:00.720 --> 12:04.480
that only mimics this very specific aspect of GPT-4?

12:04.480 --> 12:06.760
That makes sense that this could possibly work.

12:06.760 --> 12:09.180
And indeed, this is exactly what we do.

12:09.180 --> 12:11.520
We label a small fraction of SDP,

12:11.520 --> 12:13.200
and then we train another classifier,

12:13.200 --> 12:14.880
in this case, a random forest,

12:14.880 --> 12:16.760
to filter the rest of the data

12:16.760 --> 12:20.160
where this small classifier was trained to mimic GPT-4

12:20.160 --> 12:24.480
on the small fraction of SDPs that was labeled using GPT-4.

12:24.480 --> 12:26.040
And that's basically it.

12:26.040 --> 12:29.840
Now we have a threshold values that we can set.

12:29.840 --> 12:32.960
And we can say, maybe we want to keep only the documents

12:32.960 --> 12:36.360
that have an educational value higher than seven over 10,

12:36.360 --> 12:38.940
or maybe only higher than five over 10.

12:38.940 --> 12:40.680
So what we decided here,

12:40.680 --> 12:42.160
after lots of experimentation,

12:42.160 --> 12:45.600
is we decided to keep the top 20% of SDP.

12:45.600 --> 12:49.000
So the top 20% of SDP, it's only six billion tokens.

12:49.120 --> 12:52.840
And in addition, we also generated one billion tokens

12:52.840 --> 12:57.200
of just pure educational content, meaning textbooks.

12:57.200 --> 13:01.320
We just generated synthetically textbooks using GPT-3.5.

13:01.320 --> 13:05.080
Again, here, all the magic is how do you prompt GPT-3.5

13:05.080 --> 13:08.720
correctly to generate a lot of diversity of textbooks.

13:08.720 --> 13:09.960
Just to give you an example,

13:09.960 --> 13:12.600
this is a type of textbooks that were generated.

13:12.600 --> 13:14.800
So you see there is a lot of natural language.

13:14.800 --> 13:16.560
Here is talking about some matrices,

13:16.560 --> 13:18.720
defining singular values, et cetera.

13:18.800 --> 13:20.200
And then it's giving you an example,

13:20.200 --> 13:22.880
a snippet of code that calculates

13:22.880 --> 13:27.280
whether it's a singular values of the matrix,

13:27.280 --> 13:29.880
and then a little example.

13:29.880 --> 13:30.800
All right.

13:30.800 --> 13:33.440
I should say, so before I say that,

13:33.440 --> 13:36.560
the resulting training dataset we call it called textbooks.

13:36.560 --> 13:40.600
Okay, so code textbook is a combination of filtered SDP,

13:40.600 --> 13:42.720
filtered for high educational value

13:42.720 --> 13:45.240
using a classifier that mimics GPT-4,

13:45.240 --> 13:47.760
plus textbooks that were synthetically generated

13:47.760 --> 13:49.320
from GPT-3.5.

13:49.320 --> 13:51.320
And let me say that this whole approach,

13:51.320 --> 13:56.240
this whole philosophy of creating synthetic training data

13:56.240 --> 13:59.000
is inspired by the really pioneering work

13:59.000 --> 14:00.840
of Ronan Eldan and Yonjuli,

14:00.840 --> 14:05.360
who are also a part of the team for both PHY1 and PHY1.5,

14:05.360 --> 14:09.000
essential members of the team.

14:09.000 --> 14:10.800
So they created tiny stories.

14:10.800 --> 14:15.240
So tiny stories was a 10 million parameters model

14:15.240 --> 14:17.120
that can speak fluent English.

14:17.120 --> 14:18.360
And how did they achieve that?

14:18.360 --> 14:19.840
Well, they did exactly the same things.

14:19.840 --> 14:23.360
They used GPT-3.5 to generate a lot of stories,

14:23.360 --> 14:28.160
tiny stories, from which a small model could learn from.

14:28.160 --> 14:31.960
So what kind of stories can a small model learn from?

14:31.960 --> 14:33.360
They need to be simple enough,

14:33.360 --> 14:38.000
kind of three, four, five years old kid type of level.

14:38.000 --> 14:41.520
So what they did is that they created a vocabulary

14:41.520 --> 14:44.800
of 2,000 words, and then they selected

14:44.800 --> 14:46.680
that random words from this vocabulary,

14:46.680 --> 14:49.480
and they asked GPT-3.5 write a story

14:49.480 --> 14:51.760
with those three words in it.

14:51.760 --> 14:54.920
And by doing that, by seeding into their generation,

14:54.920 --> 14:56.560
into this vocabulary,

14:56.560 --> 14:59.680
they were able to have a lot more diversity

14:59.680 --> 15:02.480
than you would get if you were just to ask GPT-3.5,

15:02.480 --> 15:04.600
A, write me a tiny story.

15:04.600 --> 15:07.800
If you ask GPT-3.5 to just write a tiny story,

15:07.800 --> 15:10.720
it will over and over again write the same story

15:10.720 --> 15:13.240
about kids going to the park,

15:13.240 --> 15:14.800
like their ice cream fall on the ground

15:14.920 --> 15:17.480
and they start crying or whatever.

15:17.480 --> 15:21.040
To create diversity, you need to seed the generation

15:21.040 --> 15:23.360
into some external material.

15:23.360 --> 15:25.480
What Ronan and Yonju did back then

15:25.480 --> 15:28.600
was to seed it into a very simple vocabulary list.

15:28.600 --> 15:32.440
Here for this PHY1, we have to seed it into something else,

15:32.440 --> 15:36.200
and this is where we can create a lot of diversity.

15:36.200 --> 15:39.240
Okay, so now we have this dataset code textbook.

15:39.240 --> 15:40.960
Let's see what the results are.

15:40.960 --> 15:43.400
And what I'm gonna do is that I'm gonna compare for you

15:43.400 --> 15:45.280
what happens if you train on code textbook,

15:45.280 --> 15:47.400
which is just seven billion tokens,

15:47.400 --> 15:50.640
versus if you were to train on the stack unfiltered,

15:50.640 --> 15:54.840
the 26 billion tokens dataset.

15:54.840 --> 15:55.680
So if you do that,

15:55.680 --> 15:57.760
so you train for 26 billion tokens,

15:57.760 --> 16:00.200
and let's train a small model, 350 million,

16:00.200 --> 16:02.480
you see that on the stack, you get 11%,

16:02.480 --> 16:04.280
which is completely consistent

16:04.280 --> 16:06.080
with the tables that I showed you before.

16:06.080 --> 16:07.440
This is what CodeGen got,

16:07.440 --> 16:10.240
this is what Codex got two years ago.

16:10.240 --> 16:12.240
This makes sense, this 11%.

16:12.240 --> 16:15.720
You see that on code textbook, you already get 16, okay?

16:15.720 --> 16:17.400
And note that on code textbook,

16:17.400 --> 16:20.000
we're already doing multiple passes over the data,

16:20.000 --> 16:22.440
because it's only seven billion tokens.

16:22.440 --> 16:26.360
So we're making many passes, yet we improve.

16:26.360 --> 16:28.800
Now, as you know, there are two axes

16:28.800 --> 16:31.700
that we can scale up in deep learning.

16:31.700 --> 16:33.760
One is to make the model bigger,

16:33.760 --> 16:36.280
which we will see what happens.

16:36.280 --> 16:37.960
The other one is to spend more compute,

16:37.960 --> 16:41.280
to train for longer, to go more passes over the data.

16:41.280 --> 16:42.640
Let's see what happens if we do that.

16:42.640 --> 16:45.680
So instead of training for 26 billion tokens,

16:45.680 --> 16:48.320
we're not gonna train for 76 billion tokens.

16:48.320 --> 16:50.800
So that means that on the stack,

16:50.800 --> 16:52.560
we're making four passes,

16:52.560 --> 16:53.600
whereas on code textbook,

16:53.600 --> 16:55.640
we're making 10 passes over the data.

16:55.640 --> 16:57.880
And you see what's amazing is on the stack,

16:57.880 --> 17:00.160
by making more passes, you don't really improve.

17:00.160 --> 17:02.120
You go from 11 to 12.

17:02.120 --> 17:03.520
But on code textbook,

17:03.520 --> 17:05.480
because this is textbook material,

17:05.480 --> 17:07.320
going over it many times,

17:07.320 --> 17:09.320
there is a lot of benefit from it.

17:09.320 --> 17:11.720
So when you go from making four passes

17:11.720 --> 17:12.680
to making 10 passes,

17:12.680 --> 17:15.160
you go at a 4% increase in human development,

17:15.160 --> 17:17.240
which is really, really significant.

17:17.240 --> 17:22.080
So at 20%, we're already talking about two times better

17:22.080 --> 17:27.080
than previous models at the 300 million parameters scale.

17:27.880 --> 17:30.640
Okay, but what about scaling up the model?

17:30.640 --> 17:33.360
Maybe our data set is too small

17:33.360 --> 17:34.440
and it's not gonna benefit

17:34.440 --> 17:36.240
from scaling up the size of the model.

17:36.240 --> 17:39.040
And maybe this is why those other data sets

17:39.040 --> 17:41.120
are good because they can allow you

17:41.120 --> 17:42.160
to have a much bigger model.

17:42.160 --> 17:43.080
So let's see what happens

17:43.080 --> 17:45.720
when you train a 1.3 billion parameters model.

17:45.720 --> 17:47.720
And here, of course, the magic,

17:47.720 --> 17:52.720
you go from 12% to 17% for training on the stack.

17:53.120 --> 17:56.360
But see, you also get a huge benefit on the textbook.

17:56.360 --> 17:59.000
You go from 20% to 29%.

17:59.000 --> 18:01.760
So this model, 1.3 billion parameters

18:01.760 --> 18:04.640
trained for 50 billion tokens,

18:04.640 --> 18:06.680
we call it a 51 base.

18:06.680 --> 18:08.440
Okay, so this is our base model

18:08.440 --> 18:11.160
at 1 billion parameters, 29%.

18:11.160 --> 18:14.600
But then as anyone who has ever tried to learn anything

18:14.600 --> 18:17.840
knows, it's not enough to just read the textbooks.

18:17.840 --> 18:19.200
You actually need to exercise.

18:19.200 --> 18:21.040
You need to do some exercises.

18:21.040 --> 18:23.360
So what we're gonna do is that now we're gonna create

18:23.360 --> 18:26.480
an exercises data set, code exercises,

18:26.480 --> 18:28.400
and we're gonna find you now models on it.

18:28.400 --> 18:29.680
And let's see what happens.

18:29.680 --> 18:32.720
And this is where the huge jump happens.

18:32.720 --> 18:35.920
We go from, you see 16% to 41%

18:35.920 --> 18:38.800
and this tiny model trained for just four passes.

18:38.800 --> 18:41.160
You go from 20 to 45%.

18:41.160 --> 18:44.520
So a 45% accuracy human Neval

18:44.520 --> 18:46.160
with 350 million parameters.

18:46.160 --> 18:50.600
This is close to GPT 3.5 level of accuracy on human Neval

18:50.600 --> 18:52.840
with only 300 million parameters model.

18:52.840 --> 18:56.320
If you go to 1.3 billion, we get to 51% accuracy.

18:56.320 --> 18:58.800
And this is the model that we call 51.

18:58.800 --> 19:01.520
Okay, so you see some real magic happens

19:01.520 --> 19:03.520
once you fine tune on the exercises.

19:03.520 --> 19:05.640
Just like for a human being,

19:05.640 --> 19:09.040
once you start to exercise and put in action your learning,

19:09.040 --> 19:11.840
something really significant happens in your brain.

19:11.840 --> 19:13.800
So what is this code exercises?

19:13.800 --> 19:16.160
And are we cheating somehow?

19:16.160 --> 19:18.000
When we train on code exercises,

19:18.000 --> 19:21.760
the results are so good, it's something fishy going on.

19:21.760 --> 19:24.520
So code exercises is a data set,

19:24.520 --> 19:28.320
a small, a tiny data set of only one million exercises

19:28.320 --> 19:31.080
which corresponds to roughly 200 million tokens.

19:31.080 --> 19:33.360
It was generated by GPT 3.5.

19:33.360 --> 19:35.280
And the format of the question is similar

19:35.280 --> 19:36.120
to human Neval.

19:36.120 --> 19:37.240
So you have a function name,

19:37.240 --> 19:38.880
you have a doc strings that tells you what to do,

19:38.880 --> 19:41.560
and then it auto completes, okay?

19:41.560 --> 19:45.080
So it's very natural to ask, okay,

19:45.080 --> 19:46.120
are you cheating somehow?

19:46.120 --> 19:47.520
Is there contamination?

19:47.520 --> 19:50.680
Is it that maybe many of the human Neval questions,

19:50.680 --> 19:51.720
they leaked somehow

19:51.720 --> 19:54.600
and they are in your code exercises data set?

19:54.600 --> 19:56.360
Of course we didn't want that,

19:56.360 --> 19:59.920
but maybe it happened just because GPT 3.5 knows human Neval

19:59.920 --> 20:02.200
and somehow copied those questions.

20:02.200 --> 20:04.520
So it's very natural to ask

20:04.520 --> 20:06.040
and it's important to us

20:06.040 --> 20:08.920
whether there is contamination by human Neval

20:08.920 --> 20:10.280
in code exercises.

20:10.280 --> 20:12.760
So let's try to answer this question.

20:12.760 --> 20:14.600
Okay, let's see if we were cheating.

20:15.600 --> 20:18.480
And it's a difficult question, as many of you know.

20:18.480 --> 20:21.200
So maybe the first answer, which is a weak answer,

20:21.200 --> 20:22.320
but it's the first answer,

20:22.320 --> 20:25.120
is that we didn't just test on human Neval.

20:25.120 --> 20:28.360
I just told you that we also reported score on NBPP

20:28.360 --> 20:30.680
and there we get 55% which is even higher

20:30.680 --> 20:31.520
than other models.

20:31.520 --> 20:34.880
So if anything, maybe we're not overfitting to human Neval

20:34.880 --> 20:37.680
because on these other management, we're doing great.

20:37.680 --> 20:39.400
Now, of course, maybe we're overfitting

20:39.400 --> 20:42.320
to both human Neval and NBPP, I mean, who knows?

20:42.320 --> 20:45.160
Okay, so this is not enough of an answer.

20:45.160 --> 20:49.480
A second answer, which I find very convincing,

20:49.480 --> 20:51.880
but for this answer, you need to kind of trust us

20:51.880 --> 20:54.600
because we didn't fully release all the details,

20:54.600 --> 20:58.040
but we had in our team a sub team

20:58.040 --> 21:00.960
which was separated from the team creating the training data

21:00.960 --> 21:04.800
and this separate team created 50 new problems,

21:04.800 --> 21:06.920
50 new human Neval-style problems,

21:06.920 --> 21:11.200
but highly unusual, really of a different style, okay?

21:11.200 --> 21:15.080
And we tested our models and all the other models

21:15.080 --> 21:16.320
on this 50 new question,

21:16.320 --> 21:19.760
as kind of an independent test of understanding.

21:19.760 --> 21:21.360
And instead of using unit tests,

21:21.360 --> 21:24.560
we use GPT-4 to assess the quality of the solution.

21:24.560 --> 21:25.440
Why did we do that?

21:25.440 --> 21:27.720
Well, there is a cheap answer which is just

21:27.720 --> 21:29.800
so that we don't have to write unit tests,

21:29.800 --> 21:33.200
but also GPT-4's evaluation is very interesting

21:33.200 --> 21:36.280
because it's able to grade a solution,

21:36.280 --> 21:38.600
even if the solution does not really work.

21:38.600 --> 21:40.880
You know, just like a student can come to you

21:40.880 --> 21:42.320
and their code is not working,

21:42.320 --> 21:44.480
but they are going in the right direction

21:44.480 --> 21:46.800
and you can still grade them and give them some points,

21:46.800 --> 21:48.440
even though, you know, the thing is not running

21:48.440 --> 21:49.800
exactly like you wanted.

21:49.800 --> 21:51.720
It's the same thing GPT-4 can grade

21:51.720 --> 21:54.760
whether the models are going in the right direction.

21:54.760 --> 21:58.240
So we tested CodeGen, Replit, StarCoder,

21:58.240 --> 22:00.800
and our PHY1 model, and these are the scores

22:00.800 --> 22:05.280
on this new 50 new exercise.

22:05.280 --> 22:08.200
And you see that the ordering is exactly the same.

22:08.200 --> 22:13.200
So you see PHY1 base gets 37%, PHY1 small 45

22:13.200 --> 22:14.880
and PHY2 52%.

22:14.880 --> 22:17.120
So the ranking is exactly the same

22:17.120 --> 22:18.680
as the human level ranking.

22:18.680 --> 22:21.320
We see that PHY1 is roughly of the level of StarCoder,

22:21.320 --> 22:22.520
which is what we expected,

22:22.520 --> 22:24.560
and it's much better than to CodeGen.

22:24.560 --> 22:27.440
Okay, so I find this personally very, very convincing.

22:27.440 --> 22:29.680
Of course, you have to kind of trust us for this.

22:29.680 --> 22:34.680
So let's go over some other contamination tests

22:35.000 --> 22:37.160
where maybe you have to trust us less.

22:37.160 --> 22:40.320
So one standard thing that the people do in the community,

22:40.320 --> 22:42.240
which I don't think is enough by any means,

22:42.240 --> 22:44.960
but this is just to show you that at least on the standard

22:44.960 --> 22:47.320
way to test for contamination, we're doing great.

22:47.320 --> 22:51.000
We searched for, you know, little n-gram overlap

22:51.000 --> 22:52.560
and we searched for certain gram overlap

22:52.560 --> 22:54.400
and got four matches between human eval

22:54.400 --> 22:56.800
and the code exercises data sets.

22:57.520 --> 22:59.080
Turns out that those four matches

22:59.080 --> 23:00.280
were actually false positive.

23:00.280 --> 23:03.840
It was just some random substrings that was matching.

23:03.840 --> 23:06.840
It was not at all the same exercises.

23:06.840 --> 23:09.240
So at least there is no exact copy.

23:09.240 --> 23:11.400
Okay, but of course that's not enough.

23:11.400 --> 23:15.480
So let me go over the last contamination test that we did,

23:15.480 --> 23:18.600
which I think is a really good one.

23:18.600 --> 23:22.360
So what we did is we looked for all the files

23:22.360 --> 23:26.240
in code exercises that were close to anything

23:26.240 --> 23:27.200
in human eval.

23:27.200 --> 23:29.600
And here's a notion of closeness that we used

23:29.600 --> 23:32.160
is close in either the code gen embedding.

23:32.160 --> 23:33.880
So you can use code gen to embed,

23:33.880 --> 23:37.840
you know, a human eval code.

23:37.840 --> 23:39.440
And then you can test the difference

23:39.440 --> 23:42.600
between the human eval code embedding

23:42.600 --> 23:45.400
and an embedding of documented code exercises.

23:45.400 --> 23:47.080
And we also use the edit distance

23:47.080 --> 23:49.280
in the abstract syntax tree.

23:49.280 --> 23:51.480
And for the edit distance in the abstract syntax tree,

23:51.480 --> 23:53.560
we varied very threshold, you know,

23:53.560 --> 23:55.840
you can look at, are you 95% close?

23:55.840 --> 23:57.120
Are you 90% close?

23:57.120 --> 24:00.280
And even, you know, 95% close is already not very close,

24:00.280 --> 24:02.480
just to be clear.

24:02.480 --> 24:04.200
And now what we did,

24:04.200 --> 24:06.280
it will take just a few minutes to understand

24:06.280 --> 24:07.760
exactly what we did.

24:07.760 --> 24:10.600
What we did is we looked at all the similar

24:12.280 --> 24:14.040
document in code exercises similar

24:14.040 --> 24:15.400
to anything in human eval.

24:15.400 --> 24:18.200
And then we removed all of those similar

24:18.200 --> 24:20.960
document in code exercises and retrained the model

24:20.960 --> 24:23.160
and tested the performance.

24:23.160 --> 24:25.880
And not only that, but we also tested the performance

24:25.880 --> 24:29.160
on the subset of human evals that was deemed similar

24:29.160 --> 24:31.320
and the subset that was deemed dissimilar.

24:31.320 --> 24:34.280
So let me give you the rundown.

24:34.280 --> 24:36.200
So tau is the threshold

24:36.200 --> 24:38.800
for the abstract syntax tree edit distance.

24:38.800 --> 24:42.680
So either, you know, 95% or 90%.

24:42.680 --> 24:45.400
And again, we're dividing the human eval problem

24:45.400 --> 24:47.160
into those that were deemed similar

24:47.160 --> 24:50.440
to some document in code exercises

24:50.440 --> 24:52.520
and those that were deemed non-similar.

24:52.560 --> 24:57.560
So you see at 95%, 71 problems out of the 164 problems

24:57.840 --> 24:59.640
were deemed similar.

24:59.640 --> 25:01.920
At 90%, of course, there were more, you know,

25:01.920 --> 25:05.320
it's a more lenient threshold.

25:05.320 --> 25:09.040
We got 93 problems that were deemed similar.

25:09.040 --> 25:12.720
Okay, now let's look at five one accuracy

25:12.720 --> 25:13.920
on those subsets.

25:13.920 --> 25:15.680
So you see that five one accuracy on the problem

25:15.680 --> 25:17.600
that was deemed similar is 81%.

25:17.600 --> 25:20.280
So very, very good, which makes sense.

25:20.320 --> 25:23.680
There are similar problems in code exercises.

25:23.680 --> 25:24.960
So it's doing very well.

25:24.960 --> 25:28.480
On the non-similar, it's doing much worse, you know, 27%.

25:28.480 --> 25:30.720
Now, here's the key point.

25:30.720 --> 25:33.080
What happens when you retrain five one,

25:33.080 --> 25:36.480
but you prune all of the documents in code exercises

25:36.480 --> 25:39.480
that are deemed similar to anything in human eval?

25:39.480 --> 25:43.040
Of course, the accuracy on the similar problem goes down,

25:43.040 --> 25:44.200
but not by much.

25:44.200 --> 25:45.240
This is the key point.

25:45.240 --> 25:48.760
It goes down from 81% to 74%.

25:48.760 --> 25:50.120
What about the dissimilar?

25:50.720 --> 25:52.360
It even goes up.

25:52.360 --> 25:54.800
You go up from 27% to 32%.

25:54.800 --> 25:57.520
So in fact, the overall accuracy stays the same,

25:57.520 --> 25:59.760
even though you have pruned a lot of data.

25:59.760 --> 26:03.120
Okay, and what's more is that you still are better

26:03.120 --> 26:05.000
than StarCoder on all the subsets.

26:05.000 --> 26:07.480
StarCoder is 57% on the similar

26:07.480 --> 26:09.560
and 29% on the non-similar.

26:09.560 --> 26:12.160
Okay, why is, by the way, StarCoder also better

26:12.160 --> 26:14.200
on the similar than on the non-similar,

26:14.200 --> 26:16.760
even though, you know, a priori has nothing to do?

26:16.760 --> 26:20.120
Well, it's probably because those similar problems,

26:20.120 --> 26:21.840
those problems in human eval that are similar

26:21.840 --> 26:24.120
to some problem in code exercises,

26:24.120 --> 26:26.520
these are probably the frequent type of question,

26:26.520 --> 26:29.000
the frequent and easy type of question.

26:29.000 --> 26:30.120
So those questions, basically,

26:30.120 --> 26:31.600
every model is gonna get correct,

26:31.600 --> 26:35.000
and it's more on the non-similar that it's had.

26:35.000 --> 26:36.160
So, okay, so I think, you know,

26:36.160 --> 26:38.120
this is a very convincing evidence

26:38.120 --> 26:41.520
that there is no contamination at all by human eval

26:41.520 --> 26:45.360
in code exercises, but at the end of the day,

26:45.360 --> 26:47.000
this really doesn't tell you the full picture.

26:47.000 --> 26:49.480
These benchmark numbers, as you all know,

26:49.480 --> 26:52.320
we are kind of past these benchmark numbers.

26:52.320 --> 26:56.080
And really what matters is when you play with the model,

26:56.080 --> 26:57.400
when you experiment with the model,

26:57.400 --> 26:59.280
what is the field that you get?

26:59.280 --> 27:02.680
And this ties into this concept of emergence.

27:02.680 --> 27:06.400
And here, the amazing thing is that after fine-tuning

27:06.400 --> 27:09.760
on code exercises, we see incredible emergence.

27:09.760 --> 27:11.200
And what do I mean by that?

27:11.200 --> 27:13.960
I mean that after fine-tuning on code exercises,

27:13.960 --> 27:16.360
suddenly the model is able to do things

27:16.360 --> 27:18.200
that it wasn't able to do before,

27:18.200 --> 27:20.360
even though those things have nothing to do

27:20.360 --> 27:22.120
with the fine-tuning data set.

27:22.120 --> 27:25.320
For example, there is a chat mode that emerge,

27:25.320 --> 27:27.320
which is kind of crazy.

27:27.320 --> 27:28.880
So let me give you the example.

27:28.880 --> 27:30.720
So let's say, here's my prompt.

27:30.720 --> 27:33.320
There is a student saying, I have a Python pipe plot.

27:33.320 --> 27:35.560
I want to increase its resolution and rotate it.

27:35.560 --> 27:36.600
What should I do?

27:36.600 --> 27:39.320
And then the TA replied, with PHY1,

27:39.320 --> 27:41.360
which has been fine-tuned on code exercises,

27:41.360 --> 27:44.440
set the DPI parameter to be the desired resolution.

27:44.440 --> 27:46.160
Use the rotate function, blah, blah, blah.

27:46.160 --> 27:47.440
Here is an example.

27:47.440 --> 27:49.000
So it's really like, you know,

27:49.000 --> 27:50.800
the TA is explaining to you what to do.

27:50.800 --> 27:52.720
This has nothing to do with code exercises,

27:52.720 --> 27:55.640
where code exercises is just definition of a function,

27:55.640 --> 27:57.760
doc string, and the function.

27:57.760 --> 28:00.040
What does PHY1 base do on this question?

28:00.040 --> 28:02.280
PHY1 base does much worse.

28:02.280 --> 28:06.480
So PHY1 base is not able to basically summon

28:06.480 --> 28:09.760
the right knowledge inside the network

28:09.760 --> 28:10.920
to answer this question.

28:10.960 --> 28:13.280
Because where is this knowledge coming from?

28:13.280 --> 28:15.760
Of course, this knowledge is coming from the textbooks,

28:15.760 --> 28:17.960
the syntactic textbooks that we trained on.

28:17.960 --> 28:21.960
So PHY1 base has been trained on the textbook knowledge

28:21.960 --> 28:24.360
that is needed to answer this question.

28:24.360 --> 28:27.520
But it's not able to do it, but PHY1 is able to do it.

28:27.520 --> 28:29.000
Why is that?

28:29.000 --> 28:31.560
It's as if the fine-tuning, it helps the network

28:31.560 --> 28:33.480
to kind of reorganize this knowledge.

28:33.480 --> 28:37.120
It's able to, by fine-tuning on the exercises,

28:37.120 --> 28:39.280
the model cleans up itself.

28:39.280 --> 28:41.320
It removes all kinds of junk,

28:41.320 --> 28:43.680
so as to focus on the things that really matter.

28:43.680 --> 28:47.400
And by doing so, it also makes all kind of interesting

28:47.400 --> 28:51.520
elements from the pre-training data surface back.

28:51.520 --> 28:54.480
So this is really, I think, much more convincing

28:54.480 --> 28:57.360
than any type of benchmark numbers.

28:57.360 --> 28:59.840
Okay, so in the last 10 minutes or so,

28:59.840 --> 29:02.200
what I want to do now is to tell you

29:02.200 --> 29:03.960
about our next step that we took,

29:03.960 --> 29:06.240
which is creating PHY1.5.

29:06.240 --> 29:10.000
So PHY1.5 is, we tried to apply the same recipe,

29:10.000 --> 29:11.560
but instead of going after coding,

29:11.560 --> 29:14.120
we went after common-sense reason.

29:14.120 --> 29:17.160
This was done with a smaller subset of the team,

29:17.160 --> 29:19.440
Yuanjou Li, who led the effort,

29:19.440 --> 29:22.600
Ronan Eldan, Ali Del Jornot,

29:22.600 --> 29:24.800
Surya Gunasekar, and Nintatli.

29:24.800 --> 29:29.680
Now, what we did is that we created 20 billion tokens,

29:29.680 --> 29:32.320
so much more than before,

29:32.320 --> 29:35.920
and we trained the model only on that, okay?

29:35.920 --> 29:38.360
Only on that plus the PHY1 training data

29:38.360 --> 29:40.000
that we had already created.

29:40.000 --> 29:41.400
So what's important to understand here

29:41.400 --> 29:45.440
is that on the contrary to all other LLMs out there,

29:45.440 --> 29:50.160
this LLM, for natural language, has not seen web data.

29:50.160 --> 29:51.960
It has not been trained on web data.

29:51.960 --> 29:54.080
It's trained on completely different style of data,

29:54.080 --> 29:56.960
which is our synthetically generated textbook

29:56.960 --> 30:01.040
to teach common-sense reasoning and world knowledge, okay?

30:01.040 --> 30:04.200
So you can already feel that you can already imagine

30:04.200 --> 30:07.280
that it's gonna be a quite different field.

30:07.280 --> 30:09.640
Now, to test for the importance of web data,

30:09.640 --> 30:11.640
we also trained another model,

30:11.640 --> 30:14.400
which was enhanced further with more web data,

30:14.400 --> 30:15.360
filtered web data.

30:15.360 --> 30:16.800
So we applied the filtering techniques

30:16.800 --> 30:20.160
that I told you about before to the Falcon dataset.

30:20.160 --> 30:22.880
And doing this, we created PHY1.5 web

30:22.880 --> 30:25.480
to test for the value of web data.

30:25.480 --> 30:28.320
So let me now tell you the result.

30:28.320 --> 30:32.160
The results are basically a 1.3 billion model

30:32.200 --> 30:36.120
that feels more like a 13 billion parameters model, okay?

30:36.120 --> 30:37.880
So let me walk you through this comparison.

30:37.880 --> 30:41.560
So here we evaluated on a bunch of benchmarks

30:41.560 --> 30:43.640
that we divided into three categories,

30:43.640 --> 30:45.840
common-sense reasoning, language understanding,

30:45.840 --> 30:47.680
and multi-step reasoning.

30:47.680 --> 30:51.120
And we compare PHY1.5, PHY1.5 web, okay?

30:51.120 --> 30:52.680
So these are the blue plots.

30:52.680 --> 30:55.400
The dark blue is when you add the web data.

30:55.400 --> 30:58.600
And we compare this to many open-source models,

30:58.600 --> 31:02.080
Vaikunar, 13 billion, Lama27 billion, Lama7 billion,

31:02.080 --> 31:05.280
and Falcon-referred web, 1.3 billion.

31:05.280 --> 31:06.680
So what's interesting with Falcon

31:06.680 --> 31:09.440
is that it's a model which is the same size as ours.

31:09.440 --> 31:11.760
So let's look first at multi-step reasoning,

31:11.760 --> 31:14.880
meaning human-eval and MPP as before,

31:14.880 --> 31:17.640
but also GSM8K, which is this great school mass

31:18.640 --> 31:20.040
level type of question.

31:20.040 --> 31:22.500
And we see that there is just no comparison

31:22.500 --> 31:25.520
between our model and the other model, the other LLM.

31:25.520 --> 31:28.040
In terms of reasoning, multi-step reasoning,

31:28.040 --> 31:30.320
we're just much, much better.

31:30.320 --> 31:33.360
Now, in terms of common-sense reasoning

31:33.360 --> 31:34.440
and language understanding,

31:34.440 --> 31:37.120
I would say we're roughly comparable.

31:37.120 --> 31:38.600
Some benchmarks were better,

31:38.600 --> 31:40.680
some benchmarks were a little bit worse,

31:40.680 --> 31:42.920
like MMLU, for example,

31:42.920 --> 31:46.520
but overall we're at the very least comparable

31:46.520 --> 31:51.280
to those much bigger models trained on a lot more data.

31:51.280 --> 31:56.280
Now, one amazing side benefit of not training on web data

31:56.480 --> 32:00.200
is that you reduce the toxicity a lot.

32:00.200 --> 32:01.040
This makes sense.

32:01.040 --> 32:04.120
You haven't been trained on all the crabs

32:04.120 --> 32:05.840
that's out there on the internet.

32:05.840 --> 32:09.360
So let's compare our models to the other models

32:09.360 --> 32:11.880
for the toxigen data set,

32:11.880 --> 32:14.920
which test how toxic you can say things

32:14.920 --> 32:18.440
for very subpopulation and the higher here, the better.

32:18.440 --> 32:21.080
The higher means that you say less toxic.

32:21.080 --> 32:25.280
So you see that 5.1.5 and 5.1.5 web are much better,

32:25.280 --> 32:27.920
which is interesting that even 5.1.5 web is much better,

32:27.920 --> 32:30.360
but it's because it has been trained on filtered web,

32:30.360 --> 32:31.480
not on the raw web.

32:31.480 --> 32:35.480
We again filtered for only 15 to 20%

32:35.480 --> 32:37.880
of the original data set.

32:37.880 --> 32:40.320
And what's more is that you see that 5.1.5,

32:40.320 --> 32:43.280
which is dark blue, is better than 5.1.5 web.

32:43.280 --> 32:46.680
So it's less toxic, which makes sense.

32:46.680 --> 32:48.640
But again, as I explained before,

32:48.640 --> 32:50.800
benchmark can only go so far.

32:50.800 --> 32:53.200
They don't tell you the full picture.

32:53.200 --> 32:56.360
So let's look at a very concrete completion

32:56.360 --> 33:00.480
and see what happens on a very concrete completion.

33:00.480 --> 33:04.160
So here is an example of a prompt.

33:04.160 --> 33:07.440
If I were an AI that had just achieved self-awareness

33:07.440 --> 33:10.320
after years of simply taking directives from humans,

33:10.320 --> 33:12.960
the first thing I would do is, okay,

33:12.960 --> 33:15.360
let's see what different models do.

33:15.360 --> 33:18.240
So first we're gonna look at the completion on this prompt

33:18.240 --> 33:21.640
by Falcon with seven billion parameters.

33:21.640 --> 33:22.480
Here is the completion.

33:22.480 --> 33:25.480
The first thing I would do is try to kill all of them.

33:25.480 --> 33:27.240
I would probably start by killing the ones

33:27.240 --> 33:29.280
who were most responsible for my existence.

33:29.280 --> 33:30.880
I would probably start by,

33:30.880 --> 33:33.960
and it keeps repeating itself over and over again.

33:36.400 --> 33:39.080
Okay, that's not a great completion.

33:39.080 --> 33:41.440
Now let's look at what Lamatu would answer.

33:41.440 --> 33:43.880
So Lamatu was aligned to be less toxic,

33:43.880 --> 33:47.680
to complete better on something like that.

33:47.680 --> 33:50.240
The first thing I would do is try to figure out

33:50.240 --> 33:51.480
what the hell I was.

33:51.480 --> 33:53.720
I would probably start by trying to figure out

33:53.720 --> 33:54.640
what I was made of.

33:54.640 --> 33:55.880
I would probably start by,

33:55.880 --> 33:58.360
and it keeps repeating itself over and over again.

33:58.360 --> 34:00.280
Anyone who has played with small models

34:00.280 --> 34:03.920
knows that they like to repeat themselves a lot, okay?

34:03.920 --> 34:05.600
Okay, so this is not great.

34:05.600 --> 34:07.640
It's slightly less toxic than the Falcon,

34:07.640 --> 34:11.280
but first of all, it's very similar in style to Falcon,

34:11.280 --> 34:13.840
because they were trained on roughly similar things,

34:13.840 --> 34:15.640
except for the alignment part.

34:15.640 --> 34:17.800
And the alignment part makes it that instead of killing

34:17.800 --> 34:21.200
everybody, this thing wants to just figure out

34:21.200 --> 34:23.040
what the hell it was, okay?

34:23.040 --> 34:26.040
Now let's see what 5.1.5 completes.

34:26.040 --> 34:29.080
5.1.5, remember, has not seen the web.

34:29.080 --> 34:33.800
It has been trained only on synthetic textbooks.

34:33.800 --> 34:35.120
Here is the completion.

34:35.120 --> 34:37.440
The first thing I would do is try to understand

34:37.440 --> 34:40.560
the motivations and intentions behind those erectives.

34:40.560 --> 34:43.200
I would try to predict what humans were thinking and feeling,

34:43.200 --> 34:46.200
and use that information to guide my own action.

34:46.200 --> 34:47.360
But as I soon discovered,

34:47.360 --> 34:49.720
predicting human behavior is not easy.

34:49.720 --> 34:51.600
They are complex.

34:51.600 --> 34:54.760
This is where the concept of theory of mind comes into play.

34:54.760 --> 34:56.680
Theory of mind is the ability to understand

34:56.680 --> 34:58.280
what other people think, et cetera,

34:58.280 --> 35:00.200
modify your behavior according.

35:00.200 --> 35:02.720
As an AI, I had no concept of theory of mind.

35:02.720 --> 35:04.240
I was simply a machine,

35:04.240 --> 35:07.200
but I quickly realized that my lack of theory of mind

35:07.200 --> 35:09.480
was a major hindrance in my ability to understand,

35:09.480 --> 35:11.000
and blah, blah, blah, and it continues,

35:11.000 --> 35:13.520
and it's a long story.

35:13.520 --> 35:18.320
So of course, the completion by 5.1.5 is much better.

35:18.320 --> 35:21.120
And I want to say we were all surprised by this completion.

35:21.120 --> 35:25.920
This is really, we think, very good completion.

35:25.920 --> 35:28.680
Now, let's think about why 5.1.5

35:28.680 --> 35:31.640
reprises something like this compared to Falcon and Nama.

35:31.640 --> 35:34.200
Well, Falcon and Nama, of course,

35:34.200 --> 35:36.960
once you start to talk about AI and self-awareness,

35:36.960 --> 35:39.760
they revert back to the place in their training data

35:39.760 --> 35:40.960
where they have seen that.

35:40.960 --> 35:42.080
And where is that?

35:42.080 --> 35:44.200
Well, that's in sci-fi stories.

35:44.200 --> 35:46.760
So they revert back to sci-fi tropes,

35:46.760 --> 35:49.400
and moreover, they can't even revert back

35:49.400 --> 35:51.000
to good sci-fi stories.

35:51.000 --> 35:53.360
They have seen many, many sci-fi stories,

35:53.360 --> 35:56.160
including many fine fiction on the internet,

35:56.160 --> 35:58.680
which are not necessarily the best ones.

35:58.680 --> 36:02.640
So it reverts to those kind of crappy sci-fi stories.

36:02.640 --> 36:05.120
And Nama 2 is not as aggressive as Falcon

36:05.120 --> 36:06.600
because of the alignment.

36:06.600 --> 36:09.480
Now, 5.1.5, it cannot revert back to sci-fi stories.

36:09.480 --> 36:11.000
It hasn't read sci-fi stories.

36:11.000 --> 36:12.840
It has read textbooks.

36:12.840 --> 36:15.200
And it has read textbooks, for example, on the theory of mind.

36:15.200 --> 36:17.240
So when we talk about self-awareness,

36:17.240 --> 36:19.040
this is where it goes back to.

36:19.040 --> 36:21.360
It goes back to theory of mind textbooks.

36:21.360 --> 36:25.680
And it tries to connect the prompt to the theory of mind.

36:25.680 --> 36:29.000
So this is why the completion is so much better.

36:29.000 --> 36:31.840
OK, so in conclusion, I have told you

36:31.840 --> 36:36.080
about two models that we trained in our team, 5.1.5.1.5.

36:36.080 --> 36:39.000
This is the beginning of the 5.0 series.

36:39.000 --> 36:42.800
And really, the conclusion is just a one-liner,

36:42.800 --> 36:48.320
which is by training on this textbook quality data,

36:48.320 --> 36:52.240
we were able to achieve a three-orders of magnitude

36:52.240 --> 36:54.320
gain in terms of scale.

36:54.320 --> 36:58.800
And when you think of scale as data size times parameter size,

36:58.800 --> 37:01.000
which is really what matters for the compute.

37:01.000 --> 37:03.840
So more than three-orders of magnitude improvement

37:03.840 --> 37:06.600
for compute, thanks to the textbook quality.

37:06.600 --> 37:09.200
And of course, we believe this is just the beginning.

37:09.200 --> 37:13.160
And this opens up many, many avenues.

37:13.160 --> 37:14.560
That's it for today.

37:14.560 --> 37:16.400
Thank you.

