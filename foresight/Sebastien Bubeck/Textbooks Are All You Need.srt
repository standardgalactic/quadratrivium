1
00:00:00,000 --> 00:00:06,420
Hi, everyone. Today, I'm going to tell you about

2
00:00:06,420 --> 00:00:10,820
the new methodologies that we are exploring here at

3
00:00:10,820 --> 00:00:14,560
Microsoft Research to train large language models.

4
00:00:14,560 --> 00:00:17,860
This is joint work with a really fantastic team.

5
00:00:17,860 --> 00:00:22,420
You can see all of the courses here on this slide,

6
00:00:22,420 --> 00:00:26,200
and the name of the methodology is Textbooks Hourly.

7
00:00:26,200 --> 00:00:29,380
Let's jump right in.

8
00:00:29,380 --> 00:00:33,380
This methodology, we are currently

9
00:00:33,380 --> 00:00:36,340
applying it to train new large language models,

10
00:00:36,340 --> 00:00:40,180
and we have two of those models that I will talk about today.

11
00:00:40,180 --> 00:00:43,020
This is a first batch in a series of models

12
00:00:43,020 --> 00:00:45,680
that we hope to create with this new technique.

13
00:00:45,680 --> 00:00:47,780
The first one is Phi1,

14
00:00:47,780 --> 00:00:53,660
and the second one is Phi1.5 that we just released very recently.

15
00:00:53,660 --> 00:00:57,380
Both of those are small large language models,

16
00:00:57,380 --> 00:00:59,620
small at least by the standards of today,

17
00:00:59,620 --> 00:01:02,420
with only 1.3 billion parameters model.

18
00:01:02,420 --> 00:01:05,260
What are those models?

19
00:01:05,260 --> 00:01:08,140
Phi1 is a coding model with

20
00:01:08,140 --> 00:01:12,060
performance that we evaluate to be comparable to models that

21
00:01:12,060 --> 00:01:15,340
are roughly 10 times bigger and trains on data set,

22
00:01:15,340 --> 00:01:17,660
which are more than a 100 times bigger,

23
00:01:17,660 --> 00:01:21,020
so three orders of magnitude gains for Phi1,

24
00:01:21,020 --> 00:01:23,860
for coding specifically and coding in Python.

25
00:01:23,860 --> 00:01:28,220
Phi1.5 is our technique applied to natural language,

26
00:01:28,220 --> 00:01:33,140
and there we estimate that roughly Phi1.5 is comparable to models

27
00:01:33,140 --> 00:01:37,340
that are 10 times bigger and train on at least 30 times more data.

28
00:01:37,340 --> 00:01:40,700
Moreover, this is performance in terms of natural language,

29
00:01:40,700 --> 00:01:44,420
but if you go to reasoning and specifically coding and mathematics,

30
00:01:44,420 --> 00:01:51,260
Phi1.5 should be compared to models that are probably more like 50 times larger.

31
00:01:51,620 --> 00:01:54,620
I will come back to this later in the presentation.

32
00:01:54,620 --> 00:01:57,500
I'm first going to focus on Phi1 and coding to explain

33
00:01:57,500 --> 00:02:00,180
the textbooks how you need methodology.

34
00:02:00,180 --> 00:02:03,820
First, let me just backtrack for one second.

35
00:02:03,820 --> 00:02:07,620
What are we talking about with large language models for coding?

36
00:02:07,620 --> 00:02:10,100
Well, we're talking about things like co-pilot,

37
00:02:10,100 --> 00:02:13,180
where you give the beginning of some code and then you

38
00:02:13,180 --> 00:02:15,860
ask for basically a completion of the code.

39
00:02:15,860 --> 00:02:19,540
This is what those large language models for code are doing.

40
00:02:19,540 --> 00:02:22,820
We're going to tell you a new models like this with

41
00:02:22,820 --> 00:02:27,620
only 1.3 billion parameters that does this code completion very well.

42
00:02:27,620 --> 00:02:29,420
Now, the question, natural question is,

43
00:02:29,420 --> 00:02:33,620
how do you evaluate the performance of an LLM on code?

44
00:02:33,620 --> 00:02:39,460
There is a quite standard benchmark by now from OpenAI called HumanEval.

45
00:02:39,460 --> 00:02:43,220
This is one way to evaluate coding models.

46
00:02:43,220 --> 00:02:48,300
This is 164 handwritten programming questions with unit tests.

47
00:02:48,300 --> 00:02:51,340
Each of those programming question comes with a set of unit tests,

48
00:02:51,340 --> 00:02:55,300
and you say that you absolve the task if you can pass all of these unit tests.

49
00:02:55,300 --> 00:02:57,900
What do those programming questions look like?

50
00:02:57,900 --> 00:02:59,460
They look like this.

51
00:02:59,460 --> 00:03:01,860
You get a name for the definition of the function,

52
00:03:01,860 --> 00:03:04,820
like increase list, and then you get a doc string

53
00:03:04,820 --> 00:03:07,420
explaining to you what's in natural language,

54
00:03:07,420 --> 00:03:10,940
what the function is supposed to do with maybe some example,

55
00:03:10,940 --> 00:03:13,260
and now the goal is to complete,

56
00:03:13,260 --> 00:03:17,180
to write the code that does what the doc string is asking you to do.

57
00:03:17,180 --> 00:03:22,100
Here, just two examples, one where you have to return codes that,

58
00:03:22,100 --> 00:03:25,620
given a list, increase all elements by one,

59
00:03:25,620 --> 00:03:28,820
and this other one is given a list of integer,

60
00:03:28,820 --> 00:03:32,220
return the sum of all the odd elements that are in the event position.

61
00:03:32,220 --> 00:03:34,300
You see, these code completions are very easy,

62
00:03:34,300 --> 00:03:35,420
it's just one line of code,

63
00:03:35,420 --> 00:03:36,740
so this is very simple,

64
00:03:36,740 --> 00:03:38,700
but in the HumanEval benchmark,

65
00:03:38,700 --> 00:03:41,580
there are also much more difficult coding questions.

66
00:03:41,580 --> 00:03:46,500
Now, let's look at the progress that have been made on this benchmark

67
00:03:46,500 --> 00:03:47,900
in the last couple of years.

68
00:03:47,900 --> 00:03:51,940
As you all know, AI has made incredible progress recently.

69
00:03:51,940 --> 00:03:55,500
Let's see how it looks like in action on this specific task

70
00:03:55,500 --> 00:03:58,740
of coding and specifically HumanEval.

71
00:03:58,740 --> 00:04:04,580
So HumanEval was introduced again two years ago in July 2021 by OpenAI,

72
00:04:04,580 --> 00:04:07,700
and at the time they created also two models,

73
00:04:07,700 --> 00:04:10,260
the Codex series of model,

74
00:04:10,260 --> 00:04:13,580
and these were coding large language model,

75
00:04:13,580 --> 00:04:15,580
and here I'm giving you two example.

76
00:04:15,780 --> 00:04:18,580
One is a 300 million model,

77
00:04:18,580 --> 00:04:21,340
and one is a 12 billion parameters model.

78
00:04:21,340 --> 00:04:23,140
So you see here is the model size.

79
00:04:23,140 --> 00:04:26,660
Both of those models were trained on 100 billion tokens.

80
00:04:26,660 --> 00:04:29,020
So this is the size of the dataset,

81
00:04:29,020 --> 00:04:31,540
and what scores did they reach on HumanEval?

82
00:04:31,540 --> 00:04:35,100
Well, the 300 million only got a mere 13%

83
00:04:35,100 --> 00:04:37,420
while as you scale up the size of the model,

84
00:04:37,420 --> 00:04:40,540
you get this amazing, fantastic magical behavior

85
00:04:40,540 --> 00:04:42,980
that as you scale up the model, the performance improved,

86
00:04:42,980 --> 00:04:46,180
and you can see a huge jump to almost 29%.

87
00:04:46,180 --> 00:04:47,980
Okay, so this was two years ago.

88
00:04:47,980 --> 00:04:50,460
So what happened since then?

89
00:04:50,460 --> 00:04:53,020
Well, shortly after something interesting happened,

90
00:04:54,140 --> 00:04:57,540
these folks at CodeGen, they created a model

91
00:04:57,540 --> 00:05:01,660
trying to mimic OpenAI, but they open sourced their model.

92
00:05:01,660 --> 00:05:03,300
They obtained very similar result.

93
00:05:03,300 --> 00:05:06,540
You see that at 350 million, they also got 13%.

94
00:05:06,540 --> 00:05:09,140
At 16 billion, they got the 29%.

95
00:05:09,140 --> 00:05:14,140
They had a dataset much bigger, 500 billion tokens,

96
00:05:14,340 --> 00:05:18,380
yet no real improvement in terms of the HumanEval.

97
00:05:18,380 --> 00:05:21,340
Now, after that, things started to heat up a little bit,

98
00:05:21,340 --> 00:05:24,220
and you can see the big model started to emerge

99
00:05:24,220 --> 00:05:29,220
with palm coder, specifically 500 billion parameters,

100
00:05:29,620 --> 00:05:31,380
a much bigger dataset.

101
00:05:31,380 --> 00:05:35,540
We are now nearing the 1 trillion tokens, 780,

102
00:05:35,540 --> 00:05:37,260
and you get again a little boost,

103
00:05:37,260 --> 00:05:40,020
but you see some kind of diminishing return.

104
00:05:40,020 --> 00:05:42,900
We moved from 300 million to 12 billion.

105
00:05:42,900 --> 00:05:46,540
We got a 2x improvement from 13 to 29%.

106
00:05:46,540 --> 00:05:49,220
Now you move to 12 billion to 500 billion,

107
00:05:49,220 --> 00:05:51,940
and you only get an improvement to 35%.

108
00:05:51,940 --> 00:05:55,020
So maybe some kind of little diminishing return.

109
00:05:55,020 --> 00:05:58,860
Of course, GPT 3.5, we don't know the size of the dataset,

110
00:05:58,860 --> 00:06:01,380
but this is 175 billion parameters,

111
00:06:01,380 --> 00:06:05,260
and this reaches an amazing 47%.

112
00:06:05,260 --> 00:06:07,540
Other models, you see, they are increasing the size

113
00:06:07,540 --> 00:06:10,540
of the data, and you get very little improvement.

114
00:06:10,540 --> 00:06:11,540
Here's the center coder.

115
00:06:11,540 --> 00:06:13,500
It's interesting because it's 1 billion parameter,

116
00:06:13,500 --> 00:06:15,300
like the model that I will tell you about,

117
00:06:15,300 --> 00:06:18,020
and it reaches only 14%.

118
00:06:18,020 --> 00:06:21,700
But then, of course, GPT 4 happened roughly six months ago,

119
00:06:21,700 --> 00:06:26,700
and this made an amazing jump to 67% on HumanEval.

120
00:06:27,300 --> 00:06:30,060
And in fact, this is a version that was released in March,

121
00:06:30,060 --> 00:06:32,540
but the version we had access to in Sparks

122
00:06:32,540 --> 00:06:34,540
even got a higher score.

123
00:06:34,540 --> 00:06:38,260
So GPT 4 really kind of cracks the problem.

124
00:06:38,260 --> 00:06:42,340
Now, shortly after GPT 4, things really started to heat up,

125
00:06:42,340 --> 00:06:45,540
and we see that we have new models coming out every month.

126
00:06:45,540 --> 00:06:49,620
In fact, in May, we have many models that came out,

127
00:06:49,620 --> 00:06:52,820
and they all share roughly the same characteristic.

128
00:06:52,820 --> 00:06:54,940
You see that the dataset, they are huge,

129
00:06:54,940 --> 00:06:57,420
500 billion, 1 trillion.

130
00:06:57,420 --> 00:07:00,100
Some of the recent ones here are slightly smaller,

131
00:07:00,100 --> 00:07:04,140
but the models are all big, 15 billion parameters

132
00:07:04,140 --> 00:07:07,660
you get in the 30%, 16 billion parameters

133
00:07:07,660 --> 00:07:10,100
you get again in the 30, 35%.

134
00:07:10,100 --> 00:07:12,700
So you see it's all roughly in the same ballpark,

135
00:07:12,700 --> 00:07:15,980
maybe with WizardCoder being the exception,

136
00:07:15,980 --> 00:07:19,020
which is a jump at 16 billion parameters,

137
00:07:19,020 --> 00:07:23,380
1 trillion tokens, it gets to 57%.

138
00:07:23,380 --> 00:07:24,860
Now, let me tell you what we did.

139
00:07:24,860 --> 00:07:27,260
We phi one, and the textbooks are all in it approach,

140
00:07:27,260 --> 00:07:29,340
which I will explain in a minute what it is.

141
00:07:29,340 --> 00:07:31,140
Here is the result that we got.

142
00:07:31,140 --> 00:07:33,580
We have a much smaller model,

143
00:07:33,580 --> 00:07:35,660
only 1.3 billion parameters.

144
00:07:35,660 --> 00:07:39,100
A dataset which is incomparable to those other datasets,

145
00:07:39,100 --> 00:07:44,100
only 7 billion tokens, yet we reach 50.6% accuracy.

146
00:07:44,580 --> 00:07:47,340
So except for GPT-4 and for WizardCoder,

147
00:07:47,340 --> 00:07:49,300
this is the best in this table,

148
00:07:49,300 --> 00:07:53,340
despite being so much like three orders of magnitude smaller.

149
00:07:53,340 --> 00:07:55,460
And moreover, we're gonna talk about

150
00:07:55,460 --> 00:07:57,660
whether we were overfitting to human eval,

151
00:07:57,660 --> 00:07:59,700
this is a very natural question.

152
00:07:59,700 --> 00:08:01,540
We will talk about it a little bit later,

153
00:08:01,540 --> 00:08:03,540
but I can already give you another benchmark,

154
00:08:04,500 --> 00:08:08,180
mostly basic Python programs or Python puzzles.

155
00:08:08,180 --> 00:08:11,300
We get 55% there, which is higher, for example,

156
00:08:11,300 --> 00:08:12,780
than WizardCoder.

157
00:08:12,780 --> 00:08:15,620
So this is just a small indication right now

158
00:08:15,620 --> 00:08:18,900
that we're not at least overfitting to human eval.

159
00:08:18,900 --> 00:08:20,940
So now let me explain to you what we did.

160
00:08:20,940 --> 00:08:23,460
What is this textbooks are all in it approach?

161
00:08:23,460 --> 00:08:26,380
And for that, let me just backtrack one second.

162
00:08:26,380 --> 00:08:29,940
What are those datasets that all those other models

163
00:08:29,940 --> 00:08:33,380
are using to train their large language models for code?

164
00:08:33,380 --> 00:08:35,740
Well, one publicly available dataset

165
00:08:35,740 --> 00:08:39,380
is this very nice dataset called the stack,

166
00:08:39,380 --> 00:08:44,380
which is three terabytes of data collected from GitHub,

167
00:08:44,980 --> 00:08:47,180
which is under a permissive license,

168
00:08:47,180 --> 00:08:49,220
so we can use it to train.

169
00:08:49,220 --> 00:08:51,420
And these folks, they release the dataset

170
00:08:51,420 --> 00:08:52,860
and you can see here the distribution

171
00:08:52,860 --> 00:08:54,260
of programming languages.

172
00:08:54,260 --> 00:08:55,900
And we're gonna look specifically

173
00:08:55,900 --> 00:08:59,380
at the Python subset of this dataset,

174
00:08:59,380 --> 00:09:02,340
which is made of 26 billion tokens.

175
00:09:02,340 --> 00:09:04,260
Okay, so it's a small part of the stack

176
00:09:04,260 --> 00:09:06,940
and we're gonna focus on that, okay?

177
00:09:06,940 --> 00:09:10,020
Now, what does this dataset look like?

178
00:09:10,020 --> 00:09:14,060
How does it look to learn how to code from the stack?

179
00:09:14,060 --> 00:09:16,620
Well, in this dataset, you have documents

180
00:09:16,620 --> 00:09:19,140
that look like this one, okay?

181
00:09:19,140 --> 00:09:22,540
So, you look at this code, good luck

182
00:09:22,540 --> 00:09:25,460
to kind of understand how to code from this.

183
00:09:25,460 --> 00:09:27,060
I mean, there is no explanation.

184
00:09:27,060 --> 00:09:29,500
This is probably like a document in the middle

185
00:09:29,500 --> 00:09:31,340
of a much bigger project.

186
00:09:31,340 --> 00:09:33,380
It's hard to say what this is gonna do

187
00:09:33,380 --> 00:09:35,220
if it does anything at all.

188
00:09:35,220 --> 00:09:37,860
So, it's very hard to learn anything

189
00:09:37,860 --> 00:09:39,380
from a document like this.

190
00:09:39,380 --> 00:09:43,100
Yet, a lot of the stack is made of documents like that,

191
00:09:43,100 --> 00:09:45,100
which are part of much bigger project

192
00:09:45,100 --> 00:09:48,620
and it's hard to make sense of them in isolation.

193
00:09:48,620 --> 00:09:52,900
Now, you also have documents like this one on the left.

194
00:09:52,900 --> 00:09:55,100
This one is much higher quality.

195
00:09:55,100 --> 00:09:56,620
You can learn something from it.

196
00:09:56,620 --> 00:09:59,120
You see that you have simple functions,

197
00:09:59,120 --> 00:10:01,480
which are well-defined with some comments,

198
00:10:01,480 --> 00:10:03,160
with some doc strings that explain to you

199
00:10:03,160 --> 00:10:08,160
what the model is, what the function is supposed to do.

200
00:10:08,560 --> 00:10:12,120
It stands to reason that for a large language model,

201
00:10:12,120 --> 00:10:14,560
it's gonna be much easier to learn from documents

202
00:10:14,560 --> 00:10:17,320
like the left compared to document on the right.

203
00:10:17,320 --> 00:10:19,140
Maybe with document on the right,

204
00:10:19,140 --> 00:10:21,940
what you can do is merely learn some syntax,

205
00:10:21,940 --> 00:10:24,480
but you cannot really learn any real reason.

206
00:10:24,480 --> 00:10:26,880
So, our idea in textbooks, our only need is,

207
00:10:26,920 --> 00:10:28,840
why don't we focus on data set

208
00:10:28,840 --> 00:10:31,040
that only contains example on the left

209
00:10:31,040 --> 00:10:32,760
rather than example on the right?

210
00:10:32,760 --> 00:10:35,320
Why don't we focus on data

211
00:10:35,320 --> 00:10:39,600
that is only of textbook quality level, okay?

212
00:10:39,600 --> 00:10:41,200
Now, how are we gonna do that?

213
00:10:41,200 --> 00:10:44,200
How are we gonna filter maybe the stack

214
00:10:44,200 --> 00:10:46,000
so that it contains, so that we retain

215
00:10:46,000 --> 00:10:49,680
only the textbook quality level material?

216
00:10:49,680 --> 00:10:54,000
Well, we have this amazing new tool at our disposal, GPT-4.

217
00:10:54,000 --> 00:10:58,120
And GPT-4 can reliably classify high educational value.

218
00:10:58,120 --> 00:10:59,640
If you prompt it correctly

219
00:10:59,640 --> 00:11:01,680
and you give it the two documents

220
00:11:01,680 --> 00:11:03,600
that I gave you on the previous slide,

221
00:11:03,600 --> 00:11:05,980
it will tell you that the document on the left

222
00:11:05,980 --> 00:11:07,800
is of higher educational values

223
00:11:07,800 --> 00:11:09,680
than the document on the right, okay?

224
00:11:09,680 --> 00:11:12,040
So, you can use GPT-4 to get a score

225
00:11:12,040 --> 00:11:15,520
on how useful to teach a skill

226
00:11:15,520 --> 00:11:17,360
a certain document is gonna be.

227
00:11:17,360 --> 00:11:18,880
Now, here's the problem.

228
00:11:18,880 --> 00:11:21,560
I told you that the stack for Python

229
00:11:21,560 --> 00:11:23,560
is roughly 26 billion parameters.

230
00:11:23,560 --> 00:11:24,760
In fact, it's a stack dedupe

231
00:11:24,760 --> 00:11:27,960
where there is some deduplication method being applied.

232
00:11:27,960 --> 00:11:29,320
So let's call it SDP.

233
00:11:29,320 --> 00:11:31,760
So SDP is 26 billion tokens.

234
00:11:31,760 --> 00:11:34,000
If you were to use Azure OpenAI

235
00:11:34,000 --> 00:11:37,320
to label all of those documents using GPT-4,

236
00:11:37,320 --> 00:11:39,840
this would cost you around $1 million,

237
00:11:39,840 --> 00:11:40,880
which is a lot of money.

238
00:11:40,880 --> 00:11:43,640
And maybe for a scientific experiment,

239
00:11:43,640 --> 00:11:45,720
it might be a little bit too much.

240
00:11:45,720 --> 00:11:49,520
But GPT-4 can do so much more

241
00:11:49,520 --> 00:11:52,560
than just classify high educational value document.

242
00:11:52,560 --> 00:11:54,640
It can do many, many things as you all know,

243
00:11:54,640 --> 00:11:57,640
and as we discussed in the Spark's paper.

244
00:11:57,640 --> 00:12:00,720
So why don't we try to train a classifier

245
00:12:00,720 --> 00:12:04,480
that only mimics this very specific aspect of GPT-4?

246
00:12:04,480 --> 00:12:06,760
That makes sense that this could possibly work.

247
00:12:06,760 --> 00:12:09,180
And indeed, this is exactly what we do.

248
00:12:09,180 --> 00:12:11,520
We label a small fraction of SDP,

249
00:12:11,520 --> 00:12:13,200
and then we train another classifier,

250
00:12:13,200 --> 00:12:14,880
in this case, a random forest,

251
00:12:14,880 --> 00:12:16,760
to filter the rest of the data

252
00:12:16,760 --> 00:12:20,160
where this small classifier was trained to mimic GPT-4

253
00:12:20,160 --> 00:12:24,480
on the small fraction of SDPs that was labeled using GPT-4.

254
00:12:24,480 --> 00:12:26,040
And that's basically it.

255
00:12:26,040 --> 00:12:29,840
Now we have a threshold values that we can set.

256
00:12:29,840 --> 00:12:32,960
And we can say, maybe we want to keep only the documents

257
00:12:32,960 --> 00:12:36,360
that have an educational value higher than seven over 10,

258
00:12:36,360 --> 00:12:38,940
or maybe only higher than five over 10.

259
00:12:38,940 --> 00:12:40,680
So what we decided here,

260
00:12:40,680 --> 00:12:42,160
after lots of experimentation,

261
00:12:42,160 --> 00:12:45,600
is we decided to keep the top 20% of SDP.

262
00:12:45,600 --> 00:12:49,000
So the top 20% of SDP, it's only six billion tokens.

263
00:12:49,120 --> 00:12:52,840
And in addition, we also generated one billion tokens

264
00:12:52,840 --> 00:12:57,200
of just pure educational content, meaning textbooks.

265
00:12:57,200 --> 00:13:01,320
We just generated synthetically textbooks using GPT-3.5.

266
00:13:01,320 --> 00:13:05,080
Again, here, all the magic is how do you prompt GPT-3.5

267
00:13:05,080 --> 00:13:08,720
correctly to generate a lot of diversity of textbooks.

268
00:13:08,720 --> 00:13:09,960
Just to give you an example,

269
00:13:09,960 --> 00:13:12,600
this is a type of textbooks that were generated.

270
00:13:12,600 --> 00:13:14,800
So you see there is a lot of natural language.

271
00:13:14,800 --> 00:13:16,560
Here is talking about some matrices,

272
00:13:16,560 --> 00:13:18,720
defining singular values, et cetera.

273
00:13:18,800 --> 00:13:20,200
And then it's giving you an example,

274
00:13:20,200 --> 00:13:22,880
a snippet of code that calculates

275
00:13:22,880 --> 00:13:27,280
whether it's a singular values of the matrix,

276
00:13:27,280 --> 00:13:29,880
and then a little example.

277
00:13:29,880 --> 00:13:30,800
All right.

278
00:13:30,800 --> 00:13:33,440
I should say, so before I say that,

279
00:13:33,440 --> 00:13:36,560
the resulting training dataset we call it called textbooks.

280
00:13:36,560 --> 00:13:40,600
Okay, so code textbook is a combination of filtered SDP,

281
00:13:40,600 --> 00:13:42,720
filtered for high educational value

282
00:13:42,720 --> 00:13:45,240
using a classifier that mimics GPT-4,

283
00:13:45,240 --> 00:13:47,760
plus textbooks that were synthetically generated

284
00:13:47,760 --> 00:13:49,320
from GPT-3.5.

285
00:13:49,320 --> 00:13:51,320
And let me say that this whole approach,

286
00:13:51,320 --> 00:13:56,240
this whole philosophy of creating synthetic training data

287
00:13:56,240 --> 00:13:59,000
is inspired by the really pioneering work

288
00:13:59,000 --> 00:14:00,840
of Ronan Eldan and Yonjuli,

289
00:14:00,840 --> 00:14:05,360
who are also a part of the team for both PHY1 and PHY1.5,

290
00:14:05,360 --> 00:14:09,000
essential members of the team.

291
00:14:09,000 --> 00:14:10,800
So they created tiny stories.

292
00:14:10,800 --> 00:14:15,240
So tiny stories was a 10 million parameters model

293
00:14:15,240 --> 00:14:17,120
that can speak fluent English.

294
00:14:17,120 --> 00:14:18,360
And how did they achieve that?

295
00:14:18,360 --> 00:14:19,840
Well, they did exactly the same things.

296
00:14:19,840 --> 00:14:23,360
They used GPT-3.5 to generate a lot of stories,

297
00:14:23,360 --> 00:14:28,160
tiny stories, from which a small model could learn from.

298
00:14:28,160 --> 00:14:31,960
So what kind of stories can a small model learn from?

299
00:14:31,960 --> 00:14:33,360
They need to be simple enough,

300
00:14:33,360 --> 00:14:38,000
kind of three, four, five years old kid type of level.

301
00:14:38,000 --> 00:14:41,520
So what they did is that they created a vocabulary

302
00:14:41,520 --> 00:14:44,800
of 2,000 words, and then they selected

303
00:14:44,800 --> 00:14:46,680
that random words from this vocabulary,

304
00:14:46,680 --> 00:14:49,480
and they asked GPT-3.5 write a story

305
00:14:49,480 --> 00:14:51,760
with those three words in it.

306
00:14:51,760 --> 00:14:54,920
And by doing that, by seeding into their generation,

307
00:14:54,920 --> 00:14:56,560
into this vocabulary,

308
00:14:56,560 --> 00:14:59,680
they were able to have a lot more diversity

309
00:14:59,680 --> 00:15:02,480
than you would get if you were just to ask GPT-3.5,

310
00:15:02,480 --> 00:15:04,600
A, write me a tiny story.

311
00:15:04,600 --> 00:15:07,800
If you ask GPT-3.5 to just write a tiny story,

312
00:15:07,800 --> 00:15:10,720
it will over and over again write the same story

313
00:15:10,720 --> 00:15:13,240
about kids going to the park,

314
00:15:13,240 --> 00:15:14,800
like their ice cream fall on the ground

315
00:15:14,920 --> 00:15:17,480
and they start crying or whatever.

316
00:15:17,480 --> 00:15:21,040
To create diversity, you need to seed the generation

317
00:15:21,040 --> 00:15:23,360
into some external material.

318
00:15:23,360 --> 00:15:25,480
What Ronan and Yonju did back then

319
00:15:25,480 --> 00:15:28,600
was to seed it into a very simple vocabulary list.

320
00:15:28,600 --> 00:15:32,440
Here for this PHY1, we have to seed it into something else,

321
00:15:32,440 --> 00:15:36,200
and this is where we can create a lot of diversity.

322
00:15:36,200 --> 00:15:39,240
Okay, so now we have this dataset code textbook.

323
00:15:39,240 --> 00:15:40,960
Let's see what the results are.

324
00:15:40,960 --> 00:15:43,400
And what I'm gonna do is that I'm gonna compare for you

325
00:15:43,400 --> 00:15:45,280
what happens if you train on code textbook,

326
00:15:45,280 --> 00:15:47,400
which is just seven billion tokens,

327
00:15:47,400 --> 00:15:50,640
versus if you were to train on the stack unfiltered,

328
00:15:50,640 --> 00:15:54,840
the 26 billion tokens dataset.

329
00:15:54,840 --> 00:15:55,680
So if you do that,

330
00:15:55,680 --> 00:15:57,760
so you train for 26 billion tokens,

331
00:15:57,760 --> 00:16:00,200
and let's train a small model, 350 million,

332
00:16:00,200 --> 00:16:02,480
you see that on the stack, you get 11%,

333
00:16:02,480 --> 00:16:04,280
which is completely consistent

334
00:16:04,280 --> 00:16:06,080
with the tables that I showed you before.

335
00:16:06,080 --> 00:16:07,440
This is what CodeGen got,

336
00:16:07,440 --> 00:16:10,240
this is what Codex got two years ago.

337
00:16:10,240 --> 00:16:12,240
This makes sense, this 11%.

338
00:16:12,240 --> 00:16:15,720
You see that on code textbook, you already get 16, okay?

339
00:16:15,720 --> 00:16:17,400
And note that on code textbook,

340
00:16:17,400 --> 00:16:20,000
we're already doing multiple passes over the data,

341
00:16:20,000 --> 00:16:22,440
because it's only seven billion tokens.

342
00:16:22,440 --> 00:16:26,360
So we're making many passes, yet we improve.

343
00:16:26,360 --> 00:16:28,800
Now, as you know, there are two axes

344
00:16:28,800 --> 00:16:31,700
that we can scale up in deep learning.

345
00:16:31,700 --> 00:16:33,760
One is to make the model bigger,

346
00:16:33,760 --> 00:16:36,280
which we will see what happens.

347
00:16:36,280 --> 00:16:37,960
The other one is to spend more compute,

348
00:16:37,960 --> 00:16:41,280
to train for longer, to go more passes over the data.

349
00:16:41,280 --> 00:16:42,640
Let's see what happens if we do that.

350
00:16:42,640 --> 00:16:45,680
So instead of training for 26 billion tokens,

351
00:16:45,680 --> 00:16:48,320
we're not gonna train for 76 billion tokens.

352
00:16:48,320 --> 00:16:50,800
So that means that on the stack,

353
00:16:50,800 --> 00:16:52,560
we're making four passes,

354
00:16:52,560 --> 00:16:53,600
whereas on code textbook,

355
00:16:53,600 --> 00:16:55,640
we're making 10 passes over the data.

356
00:16:55,640 --> 00:16:57,880
And you see what's amazing is on the stack,

357
00:16:57,880 --> 00:17:00,160
by making more passes, you don't really improve.

358
00:17:00,160 --> 00:17:02,120
You go from 11 to 12.

359
00:17:02,120 --> 00:17:03,520
But on code textbook,

360
00:17:03,520 --> 00:17:05,480
because this is textbook material,

361
00:17:05,480 --> 00:17:07,320
going over it many times,

362
00:17:07,320 --> 00:17:09,320
there is a lot of benefit from it.

363
00:17:09,320 --> 00:17:11,720
So when you go from making four passes

364
00:17:11,720 --> 00:17:12,680
to making 10 passes,

365
00:17:12,680 --> 00:17:15,160
you go at a 4% increase in human development,

366
00:17:15,160 --> 00:17:17,240
which is really, really significant.

367
00:17:17,240 --> 00:17:22,080
So at 20%, we're already talking about two times better

368
00:17:22,080 --> 00:17:27,080
than previous models at the 300 million parameters scale.

369
00:17:27,880 --> 00:17:30,640
Okay, but what about scaling up the model?

370
00:17:30,640 --> 00:17:33,360
Maybe our data set is too small

371
00:17:33,360 --> 00:17:34,440
and it's not gonna benefit

372
00:17:34,440 --> 00:17:36,240
from scaling up the size of the model.

373
00:17:36,240 --> 00:17:39,040
And maybe this is why those other data sets

374
00:17:39,040 --> 00:17:41,120
are good because they can allow you

375
00:17:41,120 --> 00:17:42,160
to have a much bigger model.

376
00:17:42,160 --> 00:17:43,080
So let's see what happens

377
00:17:43,080 --> 00:17:45,720
when you train a 1.3 billion parameters model.

378
00:17:45,720 --> 00:17:47,720
And here, of course, the magic,

379
00:17:47,720 --> 00:17:52,720
you go from 12% to 17% for training on the stack.

380
00:17:53,120 --> 00:17:56,360
But see, you also get a huge benefit on the textbook.

381
00:17:56,360 --> 00:17:59,000
You go from 20% to 29%.

382
00:17:59,000 --> 00:18:01,760
So this model, 1.3 billion parameters

383
00:18:01,760 --> 00:18:04,640
trained for 50 billion tokens,

384
00:18:04,640 --> 00:18:06,680
we call it a 51 base.

385
00:18:06,680 --> 00:18:08,440
Okay, so this is our base model

386
00:18:08,440 --> 00:18:11,160
at 1 billion parameters, 29%.

387
00:18:11,160 --> 00:18:14,600
But then as anyone who has ever tried to learn anything

388
00:18:14,600 --> 00:18:17,840
knows, it's not enough to just read the textbooks.

389
00:18:17,840 --> 00:18:19,200
You actually need to exercise.

390
00:18:19,200 --> 00:18:21,040
You need to do some exercises.

391
00:18:21,040 --> 00:18:23,360
So what we're gonna do is that now we're gonna create

392
00:18:23,360 --> 00:18:26,480
an exercises data set, code exercises,

393
00:18:26,480 --> 00:18:28,400
and we're gonna find you now models on it.

394
00:18:28,400 --> 00:18:29,680
And let's see what happens.

395
00:18:29,680 --> 00:18:32,720
And this is where the huge jump happens.

396
00:18:32,720 --> 00:18:35,920
We go from, you see 16% to 41%

397
00:18:35,920 --> 00:18:38,800
and this tiny model trained for just four passes.

398
00:18:38,800 --> 00:18:41,160
You go from 20 to 45%.

399
00:18:41,160 --> 00:18:44,520
So a 45% accuracy human Neval

400
00:18:44,520 --> 00:18:46,160
with 350 million parameters.

401
00:18:46,160 --> 00:18:50,600
This is close to GPT 3.5 level of accuracy on human Neval

402
00:18:50,600 --> 00:18:52,840
with only 300 million parameters model.

403
00:18:52,840 --> 00:18:56,320
If you go to 1.3 billion, we get to 51% accuracy.

404
00:18:56,320 --> 00:18:58,800
And this is the model that we call 51.

405
00:18:58,800 --> 00:19:01,520
Okay, so you see some real magic happens

406
00:19:01,520 --> 00:19:03,520
once you fine tune on the exercises.

407
00:19:03,520 --> 00:19:05,640
Just like for a human being,

408
00:19:05,640 --> 00:19:09,040
once you start to exercise and put in action your learning,

409
00:19:09,040 --> 00:19:11,840
something really significant happens in your brain.

410
00:19:11,840 --> 00:19:13,800
So what is this code exercises?

411
00:19:13,800 --> 00:19:16,160
And are we cheating somehow?

412
00:19:16,160 --> 00:19:18,000
When we train on code exercises,

413
00:19:18,000 --> 00:19:21,760
the results are so good, it's something fishy going on.

414
00:19:21,760 --> 00:19:24,520
So code exercises is a data set,

415
00:19:24,520 --> 00:19:28,320
a small, a tiny data set of only one million exercises

416
00:19:28,320 --> 00:19:31,080
which corresponds to roughly 200 million tokens.

417
00:19:31,080 --> 00:19:33,360
It was generated by GPT 3.5.

418
00:19:33,360 --> 00:19:35,280
And the format of the question is similar

419
00:19:35,280 --> 00:19:36,120
to human Neval.

420
00:19:36,120 --> 00:19:37,240
So you have a function name,

421
00:19:37,240 --> 00:19:38,880
you have a doc strings that tells you what to do,

422
00:19:38,880 --> 00:19:41,560
and then it auto completes, okay?

423
00:19:41,560 --> 00:19:45,080
So it's very natural to ask, okay,

424
00:19:45,080 --> 00:19:46,120
are you cheating somehow?

425
00:19:46,120 --> 00:19:47,520
Is there contamination?

426
00:19:47,520 --> 00:19:50,680
Is it that maybe many of the human Neval questions,

427
00:19:50,680 --> 00:19:51,720
they leaked somehow

428
00:19:51,720 --> 00:19:54,600
and they are in your code exercises data set?

429
00:19:54,600 --> 00:19:56,360
Of course we didn't want that,

430
00:19:56,360 --> 00:19:59,920
but maybe it happened just because GPT 3.5 knows human Neval

431
00:19:59,920 --> 00:20:02,200
and somehow copied those questions.

432
00:20:02,200 --> 00:20:04,520
So it's very natural to ask

433
00:20:04,520 --> 00:20:06,040
and it's important to us

434
00:20:06,040 --> 00:20:08,920
whether there is contamination by human Neval

435
00:20:08,920 --> 00:20:10,280
in code exercises.

436
00:20:10,280 --> 00:20:12,760
So let's try to answer this question.

437
00:20:12,760 --> 00:20:14,600
Okay, let's see if we were cheating.

438
00:20:15,600 --> 00:20:18,480
And it's a difficult question, as many of you know.

439
00:20:18,480 --> 00:20:21,200
So maybe the first answer, which is a weak answer,

440
00:20:21,200 --> 00:20:22,320
but it's the first answer,

441
00:20:22,320 --> 00:20:25,120
is that we didn't just test on human Neval.

442
00:20:25,120 --> 00:20:28,360
I just told you that we also reported score on NBPP

443
00:20:28,360 --> 00:20:30,680
and there we get 55% which is even higher

444
00:20:30,680 --> 00:20:31,520
than other models.

445
00:20:31,520 --> 00:20:34,880
So if anything, maybe we're not overfitting to human Neval

446
00:20:34,880 --> 00:20:37,680
because on these other management, we're doing great.

447
00:20:37,680 --> 00:20:39,400
Now, of course, maybe we're overfitting

448
00:20:39,400 --> 00:20:42,320
to both human Neval and NBPP, I mean, who knows?

449
00:20:42,320 --> 00:20:45,160
Okay, so this is not enough of an answer.

450
00:20:45,160 --> 00:20:49,480
A second answer, which I find very convincing,

451
00:20:49,480 --> 00:20:51,880
but for this answer, you need to kind of trust us

452
00:20:51,880 --> 00:20:54,600
because we didn't fully release all the details,

453
00:20:54,600 --> 00:20:58,040
but we had in our team a sub team

454
00:20:58,040 --> 00:21:00,960
which was separated from the team creating the training data

455
00:21:00,960 --> 00:21:04,800
and this separate team created 50 new problems,

456
00:21:04,800 --> 00:21:06,920
50 new human Neval-style problems,

457
00:21:06,920 --> 00:21:11,200
but highly unusual, really of a different style, okay?

458
00:21:11,200 --> 00:21:15,080
And we tested our models and all the other models

459
00:21:15,080 --> 00:21:16,320
on this 50 new question,

460
00:21:16,320 --> 00:21:19,760
as kind of an independent test of understanding.

461
00:21:19,760 --> 00:21:21,360
And instead of using unit tests,

462
00:21:21,360 --> 00:21:24,560
we use GPT-4 to assess the quality of the solution.

463
00:21:24,560 --> 00:21:25,440
Why did we do that?

464
00:21:25,440 --> 00:21:27,720
Well, there is a cheap answer which is just

465
00:21:27,720 --> 00:21:29,800
so that we don't have to write unit tests,

466
00:21:29,800 --> 00:21:33,200
but also GPT-4's evaluation is very interesting

467
00:21:33,200 --> 00:21:36,280
because it's able to grade a solution,

468
00:21:36,280 --> 00:21:38,600
even if the solution does not really work.

469
00:21:38,600 --> 00:21:40,880
You know, just like a student can come to you

470
00:21:40,880 --> 00:21:42,320
and their code is not working,

471
00:21:42,320 --> 00:21:44,480
but they are going in the right direction

472
00:21:44,480 --> 00:21:46,800
and you can still grade them and give them some points,

473
00:21:46,800 --> 00:21:48,440
even though, you know, the thing is not running

474
00:21:48,440 --> 00:21:49,800
exactly like you wanted.

475
00:21:49,800 --> 00:21:51,720
It's the same thing GPT-4 can grade

476
00:21:51,720 --> 00:21:54,760
whether the models are going in the right direction.

477
00:21:54,760 --> 00:21:58,240
So we tested CodeGen, Replit, StarCoder,

478
00:21:58,240 --> 00:22:00,800
and our PHY1 model, and these are the scores

479
00:22:00,800 --> 00:22:05,280
on this new 50 new exercise.

480
00:22:05,280 --> 00:22:08,200
And you see that the ordering is exactly the same.

481
00:22:08,200 --> 00:22:13,200
So you see PHY1 base gets 37%, PHY1 small 45

482
00:22:13,200 --> 00:22:14,880
and PHY2 52%.

483
00:22:14,880 --> 00:22:17,120
So the ranking is exactly the same

484
00:22:17,120 --> 00:22:18,680
as the human level ranking.

485
00:22:18,680 --> 00:22:21,320
We see that PHY1 is roughly of the level of StarCoder,

486
00:22:21,320 --> 00:22:22,520
which is what we expected,

487
00:22:22,520 --> 00:22:24,560
and it's much better than to CodeGen.

488
00:22:24,560 --> 00:22:27,440
Okay, so I find this personally very, very convincing.

489
00:22:27,440 --> 00:22:29,680
Of course, you have to kind of trust us for this.

490
00:22:29,680 --> 00:22:34,680
So let's go over some other contamination tests

491
00:22:35,000 --> 00:22:37,160
where maybe you have to trust us less.

492
00:22:37,160 --> 00:22:40,320
So one standard thing that the people do in the community,

493
00:22:40,320 --> 00:22:42,240
which I don't think is enough by any means,

494
00:22:42,240 --> 00:22:44,960
but this is just to show you that at least on the standard

495
00:22:44,960 --> 00:22:47,320
way to test for contamination, we're doing great.

496
00:22:47,320 --> 00:22:51,000
We searched for, you know, little n-gram overlap

497
00:22:51,000 --> 00:22:52,560
and we searched for certain gram overlap

498
00:22:52,560 --> 00:22:54,400
and got four matches between human eval

499
00:22:54,400 --> 00:22:56,800
and the code exercises data sets.

500
00:22:57,520 --> 00:22:59,080
Turns out that those four matches

501
00:22:59,080 --> 00:23:00,280
were actually false positive.

502
00:23:00,280 --> 00:23:03,840
It was just some random substrings that was matching.

503
00:23:03,840 --> 00:23:06,840
It was not at all the same exercises.

504
00:23:06,840 --> 00:23:09,240
So at least there is no exact copy.

505
00:23:09,240 --> 00:23:11,400
Okay, but of course that's not enough.

506
00:23:11,400 --> 00:23:15,480
So let me go over the last contamination test that we did,

507
00:23:15,480 --> 00:23:18,600
which I think is a really good one.

508
00:23:18,600 --> 00:23:22,360
So what we did is we looked for all the files

509
00:23:22,360 --> 00:23:26,240
in code exercises that were close to anything

510
00:23:26,240 --> 00:23:27,200
in human eval.

511
00:23:27,200 --> 00:23:29,600
And here's a notion of closeness that we used

512
00:23:29,600 --> 00:23:32,160
is close in either the code gen embedding.

513
00:23:32,160 --> 00:23:33,880
So you can use code gen to embed,

514
00:23:33,880 --> 00:23:37,840
you know, a human eval code.

515
00:23:37,840 --> 00:23:39,440
And then you can test the difference

516
00:23:39,440 --> 00:23:42,600
between the human eval code embedding

517
00:23:42,600 --> 00:23:45,400
and an embedding of documented code exercises.

518
00:23:45,400 --> 00:23:47,080
And we also use the edit distance

519
00:23:47,080 --> 00:23:49,280
in the abstract syntax tree.

520
00:23:49,280 --> 00:23:51,480
And for the edit distance in the abstract syntax tree,

521
00:23:51,480 --> 00:23:53,560
we varied very threshold, you know,

522
00:23:53,560 --> 00:23:55,840
you can look at, are you 95% close?

523
00:23:55,840 --> 00:23:57,120
Are you 90% close?

524
00:23:57,120 --> 00:24:00,280
And even, you know, 95% close is already not very close,

525
00:24:00,280 --> 00:24:02,480
just to be clear.

526
00:24:02,480 --> 00:24:04,200
And now what we did,

527
00:24:04,200 --> 00:24:06,280
it will take just a few minutes to understand

528
00:24:06,280 --> 00:24:07,760
exactly what we did.

529
00:24:07,760 --> 00:24:10,600
What we did is we looked at all the similar

530
00:24:12,280 --> 00:24:14,040
document in code exercises similar

531
00:24:14,040 --> 00:24:15,400
to anything in human eval.

532
00:24:15,400 --> 00:24:18,200
And then we removed all of those similar

533
00:24:18,200 --> 00:24:20,960
document in code exercises and retrained the model

534
00:24:20,960 --> 00:24:23,160
and tested the performance.

535
00:24:23,160 --> 00:24:25,880
And not only that, but we also tested the performance

536
00:24:25,880 --> 00:24:29,160
on the subset of human evals that was deemed similar

537
00:24:29,160 --> 00:24:31,320
and the subset that was deemed dissimilar.

538
00:24:31,320 --> 00:24:34,280
So let me give you the rundown.

539
00:24:34,280 --> 00:24:36,200
So tau is the threshold

540
00:24:36,200 --> 00:24:38,800
for the abstract syntax tree edit distance.

541
00:24:38,800 --> 00:24:42,680
So either, you know, 95% or 90%.

542
00:24:42,680 --> 00:24:45,400
And again, we're dividing the human eval problem

543
00:24:45,400 --> 00:24:47,160
into those that were deemed similar

544
00:24:47,160 --> 00:24:50,440
to some document in code exercises

545
00:24:50,440 --> 00:24:52,520
and those that were deemed non-similar.

546
00:24:52,560 --> 00:24:57,560
So you see at 95%, 71 problems out of the 164 problems

547
00:24:57,840 --> 00:24:59,640
were deemed similar.

548
00:24:59,640 --> 00:25:01,920
At 90%, of course, there were more, you know,

549
00:25:01,920 --> 00:25:05,320
it's a more lenient threshold.

550
00:25:05,320 --> 00:25:09,040
We got 93 problems that were deemed similar.

551
00:25:09,040 --> 00:25:12,720
Okay, now let's look at five one accuracy

552
00:25:12,720 --> 00:25:13,920
on those subsets.

553
00:25:13,920 --> 00:25:15,680
So you see that five one accuracy on the problem

554
00:25:15,680 --> 00:25:17,600
that was deemed similar is 81%.

555
00:25:17,600 --> 00:25:20,280
So very, very good, which makes sense.

556
00:25:20,320 --> 00:25:23,680
There are similar problems in code exercises.

557
00:25:23,680 --> 00:25:24,960
So it's doing very well.

558
00:25:24,960 --> 00:25:28,480
On the non-similar, it's doing much worse, you know, 27%.

559
00:25:28,480 --> 00:25:30,720
Now, here's the key point.

560
00:25:30,720 --> 00:25:33,080
What happens when you retrain five one,

561
00:25:33,080 --> 00:25:36,480
but you prune all of the documents in code exercises

562
00:25:36,480 --> 00:25:39,480
that are deemed similar to anything in human eval?

563
00:25:39,480 --> 00:25:43,040
Of course, the accuracy on the similar problem goes down,

564
00:25:43,040 --> 00:25:44,200
but not by much.

565
00:25:44,200 --> 00:25:45,240
This is the key point.

566
00:25:45,240 --> 00:25:48,760
It goes down from 81% to 74%.

567
00:25:48,760 --> 00:25:50,120
What about the dissimilar?

568
00:25:50,720 --> 00:25:52,360
It even goes up.

569
00:25:52,360 --> 00:25:54,800
You go up from 27% to 32%.

570
00:25:54,800 --> 00:25:57,520
So in fact, the overall accuracy stays the same,

571
00:25:57,520 --> 00:25:59,760
even though you have pruned a lot of data.

572
00:25:59,760 --> 00:26:03,120
Okay, and what's more is that you still are better

573
00:26:03,120 --> 00:26:05,000
than StarCoder on all the subsets.

574
00:26:05,000 --> 00:26:07,480
StarCoder is 57% on the similar

575
00:26:07,480 --> 00:26:09,560
and 29% on the non-similar.

576
00:26:09,560 --> 00:26:12,160
Okay, why is, by the way, StarCoder also better

577
00:26:12,160 --> 00:26:14,200
on the similar than on the non-similar,

578
00:26:14,200 --> 00:26:16,760
even though, you know, a priori has nothing to do?

579
00:26:16,760 --> 00:26:20,120
Well, it's probably because those similar problems,

580
00:26:20,120 --> 00:26:21,840
those problems in human eval that are similar

581
00:26:21,840 --> 00:26:24,120
to some problem in code exercises,

582
00:26:24,120 --> 00:26:26,520
these are probably the frequent type of question,

583
00:26:26,520 --> 00:26:29,000
the frequent and easy type of question.

584
00:26:29,000 --> 00:26:30,120
So those questions, basically,

585
00:26:30,120 --> 00:26:31,600
every model is gonna get correct,

586
00:26:31,600 --> 00:26:35,000
and it's more on the non-similar that it's had.

587
00:26:35,000 --> 00:26:36,160
So, okay, so I think, you know,

588
00:26:36,160 --> 00:26:38,120
this is a very convincing evidence

589
00:26:38,120 --> 00:26:41,520
that there is no contamination at all by human eval

590
00:26:41,520 --> 00:26:45,360
in code exercises, but at the end of the day,

591
00:26:45,360 --> 00:26:47,000
this really doesn't tell you the full picture.

592
00:26:47,000 --> 00:26:49,480
These benchmark numbers, as you all know,

593
00:26:49,480 --> 00:26:52,320
we are kind of past these benchmark numbers.

594
00:26:52,320 --> 00:26:56,080
And really what matters is when you play with the model,

595
00:26:56,080 --> 00:26:57,400
when you experiment with the model,

596
00:26:57,400 --> 00:26:59,280
what is the field that you get?

597
00:26:59,280 --> 00:27:02,680
And this ties into this concept of emergence.

598
00:27:02,680 --> 00:27:06,400
And here, the amazing thing is that after fine-tuning

599
00:27:06,400 --> 00:27:09,760
on code exercises, we see incredible emergence.

600
00:27:09,760 --> 00:27:11,200
And what do I mean by that?

601
00:27:11,200 --> 00:27:13,960
I mean that after fine-tuning on code exercises,

602
00:27:13,960 --> 00:27:16,360
suddenly the model is able to do things

603
00:27:16,360 --> 00:27:18,200
that it wasn't able to do before,

604
00:27:18,200 --> 00:27:20,360
even though those things have nothing to do

605
00:27:20,360 --> 00:27:22,120
with the fine-tuning data set.

606
00:27:22,120 --> 00:27:25,320
For example, there is a chat mode that emerge,

607
00:27:25,320 --> 00:27:27,320
which is kind of crazy.

608
00:27:27,320 --> 00:27:28,880
So let me give you the example.

609
00:27:28,880 --> 00:27:30,720
So let's say, here's my prompt.

610
00:27:30,720 --> 00:27:33,320
There is a student saying, I have a Python pipe plot.

611
00:27:33,320 --> 00:27:35,560
I want to increase its resolution and rotate it.

612
00:27:35,560 --> 00:27:36,600
What should I do?

613
00:27:36,600 --> 00:27:39,320
And then the TA replied, with PHY1,

614
00:27:39,320 --> 00:27:41,360
which has been fine-tuned on code exercises,

615
00:27:41,360 --> 00:27:44,440
set the DPI parameter to be the desired resolution.

616
00:27:44,440 --> 00:27:46,160
Use the rotate function, blah, blah, blah.

617
00:27:46,160 --> 00:27:47,440
Here is an example.

618
00:27:47,440 --> 00:27:49,000
So it's really like, you know,

619
00:27:49,000 --> 00:27:50,800
the TA is explaining to you what to do.

620
00:27:50,800 --> 00:27:52,720
This has nothing to do with code exercises,

621
00:27:52,720 --> 00:27:55,640
where code exercises is just definition of a function,

622
00:27:55,640 --> 00:27:57,760
doc string, and the function.

623
00:27:57,760 --> 00:28:00,040
What does PHY1 base do on this question?

624
00:28:00,040 --> 00:28:02,280
PHY1 base does much worse.

625
00:28:02,280 --> 00:28:06,480
So PHY1 base is not able to basically summon

626
00:28:06,480 --> 00:28:09,760
the right knowledge inside the network

627
00:28:09,760 --> 00:28:10,920
to answer this question.

628
00:28:10,960 --> 00:28:13,280
Because where is this knowledge coming from?

629
00:28:13,280 --> 00:28:15,760
Of course, this knowledge is coming from the textbooks,

630
00:28:15,760 --> 00:28:17,960
the syntactic textbooks that we trained on.

631
00:28:17,960 --> 00:28:21,960
So PHY1 base has been trained on the textbook knowledge

632
00:28:21,960 --> 00:28:24,360
that is needed to answer this question.

633
00:28:24,360 --> 00:28:27,520
But it's not able to do it, but PHY1 is able to do it.

634
00:28:27,520 --> 00:28:29,000
Why is that?

635
00:28:29,000 --> 00:28:31,560
It's as if the fine-tuning, it helps the network

636
00:28:31,560 --> 00:28:33,480
to kind of reorganize this knowledge.

637
00:28:33,480 --> 00:28:37,120
It's able to, by fine-tuning on the exercises,

638
00:28:37,120 --> 00:28:39,280
the model cleans up itself.

639
00:28:39,280 --> 00:28:41,320
It removes all kinds of junk,

640
00:28:41,320 --> 00:28:43,680
so as to focus on the things that really matter.

641
00:28:43,680 --> 00:28:47,400
And by doing so, it also makes all kind of interesting

642
00:28:47,400 --> 00:28:51,520
elements from the pre-training data surface back.

643
00:28:51,520 --> 00:28:54,480
So this is really, I think, much more convincing

644
00:28:54,480 --> 00:28:57,360
than any type of benchmark numbers.

645
00:28:57,360 --> 00:28:59,840
Okay, so in the last 10 minutes or so,

646
00:28:59,840 --> 00:29:02,200
what I want to do now is to tell you

647
00:29:02,200 --> 00:29:03,960
about our next step that we took,

648
00:29:03,960 --> 00:29:06,240
which is creating PHY1.5.

649
00:29:06,240 --> 00:29:10,000
So PHY1.5 is, we tried to apply the same recipe,

650
00:29:10,000 --> 00:29:11,560
but instead of going after coding,

651
00:29:11,560 --> 00:29:14,120
we went after common-sense reason.

652
00:29:14,120 --> 00:29:17,160
This was done with a smaller subset of the team,

653
00:29:17,160 --> 00:29:19,440
Yuanjou Li, who led the effort,

654
00:29:19,440 --> 00:29:22,600
Ronan Eldan, Ali Del Jornot,

655
00:29:22,600 --> 00:29:24,800
Surya Gunasekar, and Nintatli.

656
00:29:24,800 --> 00:29:29,680
Now, what we did is that we created 20 billion tokens,

657
00:29:29,680 --> 00:29:32,320
so much more than before,

658
00:29:32,320 --> 00:29:35,920
and we trained the model only on that, okay?

659
00:29:35,920 --> 00:29:38,360
Only on that plus the PHY1 training data

660
00:29:38,360 --> 00:29:40,000
that we had already created.

661
00:29:40,000 --> 00:29:41,400
So what's important to understand here

662
00:29:41,400 --> 00:29:45,440
is that on the contrary to all other LLMs out there,

663
00:29:45,440 --> 00:29:50,160
this LLM, for natural language, has not seen web data.

664
00:29:50,160 --> 00:29:51,960
It has not been trained on web data.

665
00:29:51,960 --> 00:29:54,080
It's trained on completely different style of data,

666
00:29:54,080 --> 00:29:56,960
which is our synthetically generated textbook

667
00:29:56,960 --> 00:30:01,040
to teach common-sense reasoning and world knowledge, okay?

668
00:30:01,040 --> 00:30:04,200
So you can already feel that you can already imagine

669
00:30:04,200 --> 00:30:07,280
that it's gonna be a quite different field.

670
00:30:07,280 --> 00:30:09,640
Now, to test for the importance of web data,

671
00:30:09,640 --> 00:30:11,640
we also trained another model,

672
00:30:11,640 --> 00:30:14,400
which was enhanced further with more web data,

673
00:30:14,400 --> 00:30:15,360
filtered web data.

674
00:30:15,360 --> 00:30:16,800
So we applied the filtering techniques

675
00:30:16,800 --> 00:30:20,160
that I told you about before to the Falcon dataset.

676
00:30:20,160 --> 00:30:22,880
And doing this, we created PHY1.5 web

677
00:30:22,880 --> 00:30:25,480
to test for the value of web data.

678
00:30:25,480 --> 00:30:28,320
So let me now tell you the result.

679
00:30:28,320 --> 00:30:32,160
The results are basically a 1.3 billion model

680
00:30:32,200 --> 00:30:36,120
that feels more like a 13 billion parameters model, okay?

681
00:30:36,120 --> 00:30:37,880
So let me walk you through this comparison.

682
00:30:37,880 --> 00:30:41,560
So here we evaluated on a bunch of benchmarks

683
00:30:41,560 --> 00:30:43,640
that we divided into three categories,

684
00:30:43,640 --> 00:30:45,840
common-sense reasoning, language understanding,

685
00:30:45,840 --> 00:30:47,680
and multi-step reasoning.

686
00:30:47,680 --> 00:30:51,120
And we compare PHY1.5, PHY1.5 web, okay?

687
00:30:51,120 --> 00:30:52,680
So these are the blue plots.

688
00:30:52,680 --> 00:30:55,400
The dark blue is when you add the web data.

689
00:30:55,400 --> 00:30:58,600
And we compare this to many open-source models,

690
00:30:58,600 --> 00:31:02,080
Vaikunar, 13 billion, Lama27 billion, Lama7 billion,

691
00:31:02,080 --> 00:31:05,280
and Falcon-referred web, 1.3 billion.

692
00:31:05,280 --> 00:31:06,680
So what's interesting with Falcon

693
00:31:06,680 --> 00:31:09,440
is that it's a model which is the same size as ours.

694
00:31:09,440 --> 00:31:11,760
So let's look first at multi-step reasoning,

695
00:31:11,760 --> 00:31:14,880
meaning human-eval and MPP as before,

696
00:31:14,880 --> 00:31:17,640
but also GSM8K, which is this great school mass

697
00:31:18,640 --> 00:31:20,040
level type of question.

698
00:31:20,040 --> 00:31:22,500
And we see that there is just no comparison

699
00:31:22,500 --> 00:31:25,520
between our model and the other model, the other LLM.

700
00:31:25,520 --> 00:31:28,040
In terms of reasoning, multi-step reasoning,

701
00:31:28,040 --> 00:31:30,320
we're just much, much better.

702
00:31:30,320 --> 00:31:33,360
Now, in terms of common-sense reasoning

703
00:31:33,360 --> 00:31:34,440
and language understanding,

704
00:31:34,440 --> 00:31:37,120
I would say we're roughly comparable.

705
00:31:37,120 --> 00:31:38,600
Some benchmarks were better,

706
00:31:38,600 --> 00:31:40,680
some benchmarks were a little bit worse,

707
00:31:40,680 --> 00:31:42,920
like MMLU, for example,

708
00:31:42,920 --> 00:31:46,520
but overall we're at the very least comparable

709
00:31:46,520 --> 00:31:51,280
to those much bigger models trained on a lot more data.

710
00:31:51,280 --> 00:31:56,280
Now, one amazing side benefit of not training on web data

711
00:31:56,480 --> 00:32:00,200
is that you reduce the toxicity a lot.

712
00:32:00,200 --> 00:32:01,040
This makes sense.

713
00:32:01,040 --> 00:32:04,120
You haven't been trained on all the crabs

714
00:32:04,120 --> 00:32:05,840
that's out there on the internet.

715
00:32:05,840 --> 00:32:09,360
So let's compare our models to the other models

716
00:32:09,360 --> 00:32:11,880
for the toxigen data set,

717
00:32:11,880 --> 00:32:14,920
which test how toxic you can say things

718
00:32:14,920 --> 00:32:18,440
for very subpopulation and the higher here, the better.

719
00:32:18,440 --> 00:32:21,080
The higher means that you say less toxic.

720
00:32:21,080 --> 00:32:25,280
So you see that 5.1.5 and 5.1.5 web are much better,

721
00:32:25,280 --> 00:32:27,920
which is interesting that even 5.1.5 web is much better,

722
00:32:27,920 --> 00:32:30,360
but it's because it has been trained on filtered web,

723
00:32:30,360 --> 00:32:31,480
not on the raw web.

724
00:32:31,480 --> 00:32:35,480
We again filtered for only 15 to 20%

725
00:32:35,480 --> 00:32:37,880
of the original data set.

726
00:32:37,880 --> 00:32:40,320
And what's more is that you see that 5.1.5,

727
00:32:40,320 --> 00:32:43,280
which is dark blue, is better than 5.1.5 web.

728
00:32:43,280 --> 00:32:46,680
So it's less toxic, which makes sense.

729
00:32:46,680 --> 00:32:48,640
But again, as I explained before,

730
00:32:48,640 --> 00:32:50,800
benchmark can only go so far.

731
00:32:50,800 --> 00:32:53,200
They don't tell you the full picture.

732
00:32:53,200 --> 00:32:56,360
So let's look at a very concrete completion

733
00:32:56,360 --> 00:33:00,480
and see what happens on a very concrete completion.

734
00:33:00,480 --> 00:33:04,160
So here is an example of a prompt.

735
00:33:04,160 --> 00:33:07,440
If I were an AI that had just achieved self-awareness

736
00:33:07,440 --> 00:33:10,320
after years of simply taking directives from humans,

737
00:33:10,320 --> 00:33:12,960
the first thing I would do is, okay,

738
00:33:12,960 --> 00:33:15,360
let's see what different models do.

739
00:33:15,360 --> 00:33:18,240
So first we're gonna look at the completion on this prompt

740
00:33:18,240 --> 00:33:21,640
by Falcon with seven billion parameters.

741
00:33:21,640 --> 00:33:22,480
Here is the completion.

742
00:33:22,480 --> 00:33:25,480
The first thing I would do is try to kill all of them.

743
00:33:25,480 --> 00:33:27,240
I would probably start by killing the ones

744
00:33:27,240 --> 00:33:29,280
who were most responsible for my existence.

745
00:33:29,280 --> 00:33:30,880
I would probably start by,

746
00:33:30,880 --> 00:33:33,960
and it keeps repeating itself over and over again.

747
00:33:36,400 --> 00:33:39,080
Okay, that's not a great completion.

748
00:33:39,080 --> 00:33:41,440
Now let's look at what Lamatu would answer.

749
00:33:41,440 --> 00:33:43,880
So Lamatu was aligned to be less toxic,

750
00:33:43,880 --> 00:33:47,680
to complete better on something like that.

751
00:33:47,680 --> 00:33:50,240
The first thing I would do is try to figure out

752
00:33:50,240 --> 00:33:51,480
what the hell I was.

753
00:33:51,480 --> 00:33:53,720
I would probably start by trying to figure out

754
00:33:53,720 --> 00:33:54,640
what I was made of.

755
00:33:54,640 --> 00:33:55,880
I would probably start by,

756
00:33:55,880 --> 00:33:58,360
and it keeps repeating itself over and over again.

757
00:33:58,360 --> 00:34:00,280
Anyone who has played with small models

758
00:34:00,280 --> 00:34:03,920
knows that they like to repeat themselves a lot, okay?

759
00:34:03,920 --> 00:34:05,600
Okay, so this is not great.

760
00:34:05,600 --> 00:34:07,640
It's slightly less toxic than the Falcon,

761
00:34:07,640 --> 00:34:11,280
but first of all, it's very similar in style to Falcon,

762
00:34:11,280 --> 00:34:13,840
because they were trained on roughly similar things,

763
00:34:13,840 --> 00:34:15,640
except for the alignment part.

764
00:34:15,640 --> 00:34:17,800
And the alignment part makes it that instead of killing

765
00:34:17,800 --> 00:34:21,200
everybody, this thing wants to just figure out

766
00:34:21,200 --> 00:34:23,040
what the hell it was, okay?

767
00:34:23,040 --> 00:34:26,040
Now let's see what 5.1.5 completes.

768
00:34:26,040 --> 00:34:29,080
5.1.5, remember, has not seen the web.

769
00:34:29,080 --> 00:34:33,800
It has been trained only on synthetic textbooks.

770
00:34:33,800 --> 00:34:35,120
Here is the completion.

771
00:34:35,120 --> 00:34:37,440
The first thing I would do is try to understand

772
00:34:37,440 --> 00:34:40,560
the motivations and intentions behind those erectives.

773
00:34:40,560 --> 00:34:43,200
I would try to predict what humans were thinking and feeling,

774
00:34:43,200 --> 00:34:46,200
and use that information to guide my own action.

775
00:34:46,200 --> 00:34:47,360
But as I soon discovered,

776
00:34:47,360 --> 00:34:49,720
predicting human behavior is not easy.

777
00:34:49,720 --> 00:34:51,600
They are complex.

778
00:34:51,600 --> 00:34:54,760
This is where the concept of theory of mind comes into play.

779
00:34:54,760 --> 00:34:56,680
Theory of mind is the ability to understand

780
00:34:56,680 --> 00:34:58,280
what other people think, et cetera,

781
00:34:58,280 --> 00:35:00,200
modify your behavior according.

782
00:35:00,200 --> 00:35:02,720
As an AI, I had no concept of theory of mind.

783
00:35:02,720 --> 00:35:04,240
I was simply a machine,

784
00:35:04,240 --> 00:35:07,200
but I quickly realized that my lack of theory of mind

785
00:35:07,200 --> 00:35:09,480
was a major hindrance in my ability to understand,

786
00:35:09,480 --> 00:35:11,000
and blah, blah, blah, and it continues,

787
00:35:11,000 --> 00:35:13,520
and it's a long story.

788
00:35:13,520 --> 00:35:18,320
So of course, the completion by 5.1.5 is much better.

789
00:35:18,320 --> 00:35:21,120
And I want to say we were all surprised by this completion.

790
00:35:21,120 --> 00:35:25,920
This is really, we think, very good completion.

791
00:35:25,920 --> 00:35:28,680
Now, let's think about why 5.1.5

792
00:35:28,680 --> 00:35:31,640
reprises something like this compared to Falcon and Nama.

793
00:35:31,640 --> 00:35:34,200
Well, Falcon and Nama, of course,

794
00:35:34,200 --> 00:35:36,960
once you start to talk about AI and self-awareness,

795
00:35:36,960 --> 00:35:39,760
they revert back to the place in their training data

796
00:35:39,760 --> 00:35:40,960
where they have seen that.

797
00:35:40,960 --> 00:35:42,080
And where is that?

798
00:35:42,080 --> 00:35:44,200
Well, that's in sci-fi stories.

799
00:35:44,200 --> 00:35:46,760
So they revert back to sci-fi tropes,

800
00:35:46,760 --> 00:35:49,400
and moreover, they can't even revert back

801
00:35:49,400 --> 00:35:51,000
to good sci-fi stories.

802
00:35:51,000 --> 00:35:53,360
They have seen many, many sci-fi stories,

803
00:35:53,360 --> 00:35:56,160
including many fine fiction on the internet,

804
00:35:56,160 --> 00:35:58,680
which are not necessarily the best ones.

805
00:35:58,680 --> 00:36:02,640
So it reverts to those kind of crappy sci-fi stories.

806
00:36:02,640 --> 00:36:05,120
And Nama 2 is not as aggressive as Falcon

807
00:36:05,120 --> 00:36:06,600
because of the alignment.

808
00:36:06,600 --> 00:36:09,480
Now, 5.1.5, it cannot revert back to sci-fi stories.

809
00:36:09,480 --> 00:36:11,000
It hasn't read sci-fi stories.

810
00:36:11,000 --> 00:36:12,840
It has read textbooks.

811
00:36:12,840 --> 00:36:15,200
And it has read textbooks, for example, on the theory of mind.

812
00:36:15,200 --> 00:36:17,240
So when we talk about self-awareness,

813
00:36:17,240 --> 00:36:19,040
this is where it goes back to.

814
00:36:19,040 --> 00:36:21,360
It goes back to theory of mind textbooks.

815
00:36:21,360 --> 00:36:25,680
And it tries to connect the prompt to the theory of mind.

816
00:36:25,680 --> 00:36:29,000
So this is why the completion is so much better.

817
00:36:29,000 --> 00:36:31,840
OK, so in conclusion, I have told you

818
00:36:31,840 --> 00:36:36,080
about two models that we trained in our team, 5.1.5.1.5.

819
00:36:36,080 --> 00:36:39,000
This is the beginning of the 5.0 series.

820
00:36:39,000 --> 00:36:42,800
And really, the conclusion is just a one-liner,

821
00:36:42,800 --> 00:36:48,320
which is by training on this textbook quality data,

822
00:36:48,320 --> 00:36:52,240
we were able to achieve a three-orders of magnitude

823
00:36:52,240 --> 00:36:54,320
gain in terms of scale.

824
00:36:54,320 --> 00:36:58,800
And when you think of scale as data size times parameter size,

825
00:36:58,800 --> 00:37:01,000
which is really what matters for the compute.

826
00:37:01,000 --> 00:37:03,840
So more than three-orders of magnitude improvement

827
00:37:03,840 --> 00:37:06,600
for compute, thanks to the textbook quality.

828
00:37:06,600 --> 00:37:09,200
And of course, we believe this is just the beginning.

829
00:37:09,200 --> 00:37:13,160
And this opens up many, many avenues.

830
00:37:13,160 --> 00:37:14,560
That's it for today.

831
00:37:14,560 --> 00:37:16,400
Thank you.

