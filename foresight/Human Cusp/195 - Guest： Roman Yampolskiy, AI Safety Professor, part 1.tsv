start	end	text
0	11800	Artificial Intelligence will completely transform our world, but what is AI?
11800	13800	Why will it affect you?
13800	21360	And how can you and your business survive and thrive through the AI revolution?
21360	24680	Welcome to AI and You.
25320	30880	Here is your host, author, speaker, and futurist Peter Scott.
32880	36720	Hello and welcome to Episode 195.
36720	44720	I am chuffed to welcome back to the show Roman Jampolski, our first three-peat guest.
44720	52840	Roman was on the show in Episodes 16 and 17 and also nearly a year ago in Episodes 160 and 161.
52840	57920	He is here today because he has a new book that has just hit the shelves called AI,
57920	63920	unexplainable, unpredictable, uncontrollable, which gives you, right there,
63920	68120	a good idea of what Roman's field is, AI safety.
68120	72880	A term he has done more than anyone else to promote and inhabit.
72880	78560	Roman is tenured associate professor in the Department of Computer Engineering and Computer Science
78560	83400	at the Speed School of Engineering, University of Louisville in Kentucky.
83400	86640	He is the founding and current director of the Cyber Security Lab
86640	91440	and has written many key books in the field such as Artificial Superintelligence,
91440	97160	a futuristic approach, and Artificial Intelligence, Safety and Security.
97160	101040	He's been central in the field of warning about the control problem
101040	105320	and the value alignment problem of AI from the very beginning.
105320	109680	Back when doing so earned people some scorn from practitioners,
109680	117520	yet Roman is a professor of computer science and applies rigorous methods to his analyses of these problems.
117520	121600	It's those rigorous methods that we want to tap into in this interview
121600	128480	because Roman connects principles of computer science with the issue of existential risk from AI,
128480	132120	which is an area, obviously of ultimate importance to us all,
132120	138160	but one which has a lot of opinion and argument behind it and could use some more science.
138160	142280	Speaking of which, we're going to refer to the halting problem in this interview
142280	149200	and because it's a problem in computer science that was famously solved by Alan Turing in 1936
149200	151880	with an elegant proof, here's the proof.
151880	158240	The halting problem asks whether there could be a computer program or algorithm
158240	164760	that would be capable of telling whether any computer program you fed it would eventually stop running
164760	169760	or whether it would run forever given any specific input.
169760	174440	Turing showed that it was impossible for such a program to exist.
174440	176360	Here's how he did it.
176360	180280	Suppose that such a program does exist and it returns true
180280	184880	if the program and input that you give it will halt false otherwise.
184880	188240	Now we make a new algorithm with the following definition.
188240	196720	It takes a program as its input, then calls the halting analysis program with that program as both program and input
196720	202000	and if that program returns true then it goes into an infinite loop, otherwise it halts.
202000	207920	Now we take that algorithm and call it with itself as input, which means
207920	213400	if the halting analysis program returns true it means the algorithm we just made halts when given itself as input
213400	218760	but we just said that the definition of that algorithm is that in that case it will loop forever.
218760	222080	That's a contradiction. We went wrong somewhere.
222080	227320	There's an equal contradiction if the halting analysis program returns false.
227320	234880	So our initial assumption, the only one we made that such a program could exist, must be false.
234880	239920	This is a powerful proof, somewhat similar to Goedl's incompleteness theorem
239960	243360	and logical statements like this statement is unprovable.
243360	251320	An analogical proof shows that there can be no algorithm that could compress all possible input data sets without loss
251320	258120	because otherwise you could iteratively reduce any data set to an arbitrarily small size which is impossible.
258120	264720	In other words, trying to gzip your jpeg files probably isn't going to make them any smaller.
264720	271320	So we're going to talk about safety of AI where many people seem to suffer from this sort of collective blind spot.
271320	278360	A kind of group think that the onus is on the people calling for pauses in its development to show that it's not safe.
278360	286080	They say things like this could turn out very well for us as though those were things that were final arguments against the pausing.
286080	287920	This is not how safety works.
287920	290480	Imagine you went to get on a bus and the driver said,
290480	295800	I can imagine ways in which this bus could reach its destination totally unscathed.
295800	300480	Our job as engineers is to prove that something is safe.
300480	304960	The absence of proof that something is dangerous is not the same thing.
304960	310120	It's a fair question to ask, how is superintelligence supposed to be able to defeat us?
310120	316000	And people like Roman don't get into that, which looks like a cop-out, but the problem is that when you ask that,
316000	324960	if you get any specific answer like it could decide that humanity is a threat to its purpose and take over biosynthesis labs to produce and release deadly pathogens,
324960	333320	well, deadly to us, we can say, OK, well, then we will ensure that all the biosynthesis labs are disconnected from the network.
333320	336840	And so forth, we could do that for any scenario that we could imagine.
336840	343800	But the problem is that there are many, many more scenarios than we can imagine, effectively an infinite number.
343840	348800	And a superintelligent AI would be very good at finding the ones we hadn't thought about.
348800	355240	To the same kind of fat tail problem I've talked about with respect to why self-driving cars are still not ready for prime time.
355240	357440	More on that later.
357440	361920	Anyway, enough of me. Let's get into the interview.
361920	367480	Well, Roman Jampolsky, welcome so much back to artificial intelligence and you.
367480	375680	We're here to talk about your new book, AI, unexplainable, unpredictable, uncontrollable.
375680	381760	And that is a set of daunting claims.
381760	394120	Before we get into that, I want listeners to get some feel for your background because that may play into what's brought you to the point of writing this book.
394120	400440	And so could you give us the encapsulation of your life from birth to Louisville?
400440	402800	From birth? Wow, that's ambitious.
402800	404360	Thanks for having me back.
404360	406480	I'm a computer scientist at E01.
406480	409240	I'm a PhD in computer science and engineering.
409240	414320	I've been working on different sub-domains of AI safety for over a decade.
414320	421360	I've published a couple of hundred billion good papers, a few books, mostly on AI safety.
421360	439200	In my full-time area of research in the last couple years for sure, my background is, I guess, initially I was raised in Soviet Union, came to US in 94, lived in New York for about 15 years, now I've been in Kentucky for about as much.
439200	447000	And this book gets into some serious territory that impacts a great many people.
447080	460640	I mean, it's inhabiting this rather narrow niche in a way of being a book about computer science written by a computer scientist containing computer science language.
460640	471800	And yet it's talking about something that has fundamental impact on the entire human race and is aimed at a much wider audience than computer scientists.
471800	474440	So could you tell us why you wrote this book?
474440	475600	Well, that's my research.
475600	476640	That's what I'm doing.
476640	481280	I think it's the most important problem ever in any discipline.
481280	491760	We're trying to understand if the science and engineering we're doing right now, creating intelligent assistants, helpers is a long-term beneficial strategy.
491760	500760	If there is any chance that will be too successful, create something too intelligent, too independent, and that will backfire tremendously.
500760	511560	And it's actually shocking and surprising to me that even with a tiny minority of people in AI doing a safety work within that community,
511560	518480	there is really no one outside of me doing work on fundamentals of what is possible in this area.
518480	529440	There is a few informal, independent scholars publishing a few blog posts, but you would expect this problem to be as popular as NP versus PEEP problem.
529440	531840	Can you control super 1000 machines yet?
531840	532920	There is nothing.
532920	543600	And you mentioned their P versus NP classic trope in computer science about the scale of computability of problems.
543600	557160	Now, you talk about the control problem and the alignment problem in the book in ways that embody what appears to be mathematical proof.
557200	562240	It reminds me very much of the halting problem in computer science.
562240	575200	The question is it possible to write a program that when you feed it the source code of some other program can tell you whether that program would ever finish if it were to be run.
575200	584240	And there is a simple and elegant proof that no program can be written that can do that for every possible program that you can give to it.
584800	586720	Your work appears reminiscent of that.
586720	596360	Would you put it in the same category as that level of mathematical proof that you have created with respect to controllability?
596360	597280	Not exactly.
597280	600440	So there is different levels of control we can talk about.
600440	604680	In a chapter, I suggest at least four different levels of control.
604680	618920	For what I call direct control, yes, we can come up with a very mathematically rigorous proof similar to what we have seen with mathematical proofs where we show that it creates a contradiction that it's impossible.
618920	624560	But there are, of course, other levels of control, delegated control, ideal advisor.
624560	627800	And for that, we don't have a strong mathematical proof.
627800	633520	But what we do have is a survey of every relevant field to this problem.
633520	638240	So if you think, OK, we need to accomplish this, this is the tool set, I need to be able to do it.
638240	640680	I need to be able to aggregate votes.
640680	647080	I need to be able to elicit preferences, mathematics, economics, any field you want.
647080	654880	If you survey those fields, they are very well-established proven impossibility results which say, well, you cannot get all those tools.
654880	658320	You can get approximate partial results for many of them.
658320	663200	But if you really need the exact result, you're not going to get there.
663200	669520	And it's not me saying it, it's top experts in all these fields saying, that's impossible, you cannot do it.
669520	673120	Now, one of the elements of your title is unpredictability.
673120	681320	And is that something where you have also arrived at something amounting to a proof that AI is not predictable?
681320	684200	Yes, that one is actually a good solid proof.
684200	686600	It's, again, self-referential all the way.
686600	693320	If we make this assumption that we're creating superintelligence, a system smarter than any one of us in all domains,
693320	699800	then if we could accurately predict what decisions the system will make, we would be equally smart.
699800	707200	Let's say we're playing a game of chess, if I can predict every move my opponent is going to make, I'm at least as good of a chess player.
707200	709480	But that's a violation of our assumption.
709480	714400	We have proof by contradiction that, no, of course, we cannot make those predictions.
714440	721800	We can maybe predict general direction the agent is trying to take us, like that system is trying to win a game of chess.
721800	725240	But we cannot predict specific moves it's going to make to get there.
725240	730520	And this leads us on a direction that's novel with respect to computers,
730520	736560	because the entire field of computer science is founded on predictability.
736560	741040	You cannot, and certainly the way I was trained as a computer scientist,
741080	748160	you are aiming for 100% predictability, otherwise you don't know what you're doing, literally.
748160	757280	And here you have a proof that we're heading in direction of creating things that are provably unpredictable.
757280	763560	Does that not rock the foundations of the science that AI is founded on?
763560	768960	Well, interestingly, I think for the longest time, AI was not science, it was engineering.
769000	774280	We use software engineering, we design specific engineering artifacts for where you design a bridge.
774280	778240	We had specifications, we met them, we knew how the system works.
778240	784040	In about the last decade or so, we switched from where we explicitly programmed their systems,
784040	786800	we kind of teach them to learn independently.
786800	790480	We give them data, we give them computing power, and we say, go figure it out,
790480	796880	and then we'll study to see how good they are at that domain or maybe their general systems.
796880	801160	But it's more like science now where we're on experiments and those artifacts,
801160	806320	and we discover what they are after the fact, not at the time we design them.
806320	812880	So that's quite a paradigm shift, but I think it's never been science before.
812880	818920	To hop around some of the elements of your title here, the third one is unexplainable.
818920	825120	And there is a huge amount of effort going into making AI explainable right now.
825120	835280	And I think this is worth dissecting because some of that work is succeeding and will provide value,
835280	840680	but it's not necessarily the kind of explainability that I think you're talking about.
840680	846240	In this sense that I mean this, we've had people on the show from IBM who talk about
846240	852480	rather ingenious ways that they get AI to explain how it came up with a decision
852480	855840	with respect to someone's insurability, for instance.
855840	861400	That could be useful, could be fit for a purpose in the insurance industry.
861400	867840	But you're going for something more deeper, broader, can you explain?
867840	869520	Pan intended that to us.
869520	871960	So there are certainly things we can explain.
871960	874360	No one argues that everything is unexplainable.
874360	876800	You can explain trivial decisions.
876800	882440	If you're playing a game of tic-tac-toe, you can certainly tell me why you moved to a specific square.
882480	885080	I'm talking about the extreme on the other end.
885080	892760	There was complex systems, systems making decisions based on billions of nodes in the neural network,
892760	894320	trillion of weights.
894320	898680	The system looks at the totality of those to make its decision.
898680	901160	It's working in a novel domain.
901160	908520	So far, we sometimes succeeded understanding one node or maybe a small subset of those nodes.
908520	911720	Okay, they fire, this input is produced.
911720	913280	That's what stimulates them.
913280	915280	That doesn't give us complete picture.
915280	916720	We really have two options.
916720	921600	We can either get some sort of a simplified, reduced explanation.
921600	926040	Top 10 reasons this decision was made, kind of like glossy compression.
926040	927720	And it's useful.
927720	928640	It's beneficial.
928640	929960	It's better than nothing.
929960	931840	But it doesn't give you complete picture.
931840	934880	You can still hide information in such explanations.
934880	936440	That's what we do with children.
936440	937840	We don't give them complete picture.
937840	941480	We give them just so explanation, just so stories.
941480	944120	The opposite is a complete picture.
944120	947240	We will give you a full explanation for how the decision is made.
947240	949080	But that is the whole model.
949080	951840	That's the whole AI system with all of its weights.
951840	954160	That's not something a human brain can survey.
954160	955480	It's too large.
955480	957560	It's not human readable.
957560	961000	Simply giving you access to all the weights will explain nothing to you.
961000	966400	So either we have problem with comprehending explanation given to us,
966440	971360	or we're given a simplified, not full picture explanation,
971360	973720	which does not guarantee safety.
973720	979440	So there you're leading us to why unexplainability is important.
979440	983240	And I will get into that in a moment.
983240	989840	First, I want to say you are a neural network with billions of nodes.
989840	996160	If I ask you to explain some decision, I will get some useful answer out.
996160	1001440	How does this differ from what we want out of a neural network, an artificial one?
1001440	1003800	It really depends on what you ask me to explain.
1003800	1007960	If you ask me to explain how I decide what podcasts to go on,
1007960	1010280	I can probably explain that pretty well.
1010280	1014440	If you ask me how I recognize your face, I will give you a BS story,
1014440	1016360	which has nothing to do with reality.
1016360	1017440	Right.
1017440	1021920	And then we are using neural networks at the moment, artificial ones.
1021920	1025840	Again, to do things like recognize faces.
1025840	1032600	And so expecting an answer of how they came up with that is perhaps futile,
1032600	1035960	because we couldn't relate it to anything that we understand,
1035960	1038000	since we don't know how we do that either.
1038000	1043240	And even the rather clever attempts to do that have often highlighted things in the image
1043240	1046400	that have nothing to do with the face, which is alarming.
1046400	1050440	But yet they work anyway, which is even more alarming.
1050480	1059200	Does safety have more to do with questions that we can't expect an explanation for?
1059200	1061400	Like, how did you recognize this face?
1061400	1066960	Why did you choose to draw that image when I asked for a picture of a poodle riding a motorcycle?
1066960	1076440	Or is it more impacted by questions where we would expect a human comprehensible explanation
1076440	1081480	of a decision like why are you suggesting that we launch our strategic defense missiles?
1081480	1088480	Well, for us to fully confirm that the system is safe and operational as we expect it to be,
1088480	1090640	we need to understand how it works.
1090640	1097040	It doesn't seem like we're designing it, so we have to deal with this artifact produced by this
1097040	1100000	process of self-learning, self-modification.
1100000	1105440	And if we can't comprehend how it works, it doesn't give us any reason to think it's safe.
1105440	1108000	Asking how did you make this decision?
1108000	1112600	Is it useful in part to decide should we accept this decision?
1112600	1117280	If we are accepting it based on previous results, it's always been right so far.
1117280	1122160	Should we just take it as a religious statement that it's going to be right in the future?
1122160	1126760	It's now an oracle-type god-like system that seems to be dangerous.
1126760	1130960	You can have a stretch-restaurant situation where it gives you 10 good answers,
1130960	1135080	gets you trust, and then does something against your preferences.
1135080	1139480	So I think, as I said, if you look at the totality of a problem of control,
1139480	1140640	you have those tools.
1140640	1145160	You need to be able to understand how it works, predict what it's going to do,
1145160	1147560	be able to communicate with it without ambiguity.
1147560	1152080	Say, we have a paper with about 50 different relevant results,
1152080	1154520	and it doesn't look like we have those.
1154520	1158600	So it's very hard for me to understand why people argue that,
1158600	1159880	oh, certainly we can do it.
1159880	1160520	It's easy.
1160520	1163240	We just need to have more money for this project.
1163280	1165520	So it's not obvious.
1165520	1171520	And I think for humans, we want explanations for at least two reasons.
1171520	1175280	One would be, I want to know how to do better in the future.
1175280	1182720	Like, tell me why you denied my college application so my next one can be accepted.
1182720	1185720	And then there's the, I don't trust you.
1185720	1189200	If you give me an explanation of why you made that decision,
1189200	1195960	maybe I can find the floor in your logic or where you're not working properly.
1195960	1204840	So does explainability for AI have more for you personally to do with the verifiability problem?
1204840	1207480	Of course, and that's another impossibility result.
1207480	1212240	There is strong reason to think we cannot perfectly verify software mathematical proofs.
1212240	1217000	We can get it to a certain degree of accuracy proportionate to the resources we allocate.
1217000	1221240	But we never get 100% guarantee that it's always a possibility,
1221240	1223920	one in a billion chance it will make our own decision.
1223920	1227960	But if a system continues making billions of decisions every minute,
1227960	1230480	you basically guarantee that it's going to fail pretty soon.
1230480	1235320	So we're talking about software here that as it becomes more intelligent,
1235320	1241560	more AGI-like, becomes closer to the characteristics of human beings
1241560	1247600	and then of course exceeding them in the dimensions of general intelligence.
1247600	1256960	Would you say that humans are susceptible to the kind of algorithmic analyses that you're making of AI?
1256960	1261760	Humans also have unexplainable, unpredictable and uncontrollable, certainly.
1261760	1264920	Yeah, in fact, they're reduced to a human control problem.
1264960	1268680	If you can't get humans to be safe and controlled,
1268680	1274640	despite years of attempts, you know, millennia of ethics, morals, religion,
1274640	1278760	cultural indoctrination, public schooling, you name it.
1278760	1282080	I mean, we have laws, we have all this and at the end of the day,
1282080	1285640	you still have a situation where employees betray the company,
1285640	1288800	citizens betray the country, spouses betray each other.
1288800	1292720	There is no reliability, there is no guarantee with lie detector tests,
1292720	1295920	with financial incentives, so none of it works.
1295920	1301800	So if we can't fully guarantee safety and controllability of the human neural network,
1301800	1306680	why would we think that scaling it to an extremely large equivalent
1306680	1309280	would make it somehow more likely to happen?
1309280	1314640	Right, then I'm trying to come up with a freezing here
1314640	1317520	because this is a relatively recent thought for me.
1317640	1323200	When we're talking about AI from the computer science analysis
1323200	1330760	of it being a set of algorithms and conducting a number of proofs
1330760	1335400	that resemble the proof of the halting problem, for instance,
1335400	1340640	at some point, does that approach no longer become valid
1340640	1346920	if we're talking about something that is as impenetrable as human beings
1346960	1354320	for whom we would not attempt to prove the halting problem of our brains, I imagine.
1354320	1362000	Would you consider that the human brain is operating on algorithms,
1362000	1366560	just ones that we don't know, that could be analyzed
1366560	1369960	if we had the capability, the technology,
1369960	1376120	to the level of detail that we know for AI?
1376160	1378680	So human brain is particularly interesting
1378680	1384200	because there is not even full agreement on what capabilities
1384200	1390240	are a direct result of brain architecture and which may be something else.
1390240	1393200	So then we talk about hard problem of consciousness.
1393200	1396560	We don't seem to find guaranteed correlates of consciousness.
1396560	1400080	We don't know why some states of matter would generate those,
1400080	1402280	the most interesting, the most important states.
1402280	1405320	We have no idea how to make a computer feel pain.
1405320	1409680	Yes, it's a fundamental part of being a human, probably an animal.
1409680	1414520	So we have very limited understanding of human brain,
1414520	1418400	but because we don't have direct access to a lot of source code,
1418400	1421840	a lot of what makes a brain tick,
1421840	1427680	there could be differences which make understanding artificial neural networks easier.
1427680	1430120	We don't think they have any magical properties.
1430120	1434480	We certainly know there is no immortal soul or anything like that in them.
1434480	1438600	At the same time, the size and scale make it so much harder for human brains
1438600	1442320	to fully comprehend, fully survey that architecture.
1442320	1448680	So I think there is enough differences to make any analogy not a perfect analogy.
1448680	1451160	Let me turn it around then.
1451160	1457240	Is it possible or likely that AI could become so complex
1457240	1463200	that it will become no longer useful to think of it as a computer program
1463240	1469400	and apply the analyses that we do of computer algorithms to it anymore
1469400	1473120	than we could apply those to human beings?
1473120	1477120	It's certainly possible, but I don't think we're already doing much of that.
1477120	1481320	We were not directly measuring those systems, I think,
1481320	1484680	in standard measures of complexity, begonotation.
1484680	1487320	That's not how we evaluate those systems.
1487320	1492160	So we look at them and try to create measures for how helpful it is,
1492160	1495840	for how useful it is, for how well-behaved it is.
1495840	1500120	Those are kind of similar to psychiatric tools we use with humans.
1500120	1501560	Okay, that's a psychopath.
1501560	1505840	This one is kind of well aligned with the values of society,
1505840	1509760	but those are very fuzzy disciplines, very fuzzy measures.
1509760	1515200	Do you see a point perhaps where it becomes more useful in the future
1515200	1519720	to treat AI with the tools of psychology than computer science?
1519760	1524360	It's definitely useful and it's starting to be, I think, a direction of research
1524360	1530240	where you're trying to prompt engineer it to do better, act safer.
1530240	1534520	The problem, of course, is it's a very fuzzy area
1534520	1537840	and just because you succeeded in one way of prompting,
1537840	1540520	it doesn't mean that it's not an alternative phrasing
1540520	1546520	which completely overrides any previous instructions or bypasses any safeguards.
1547320	1549200	If you say it this way, the system says,
1549200	1551520	oh, I will not do it, this is very evil action,
1551520	1554680	but if you put a negative sign in front of one of the words,
1554680	1556760	somehow it bypasses all the safeguards
1556760	1559480	and now it's happy to perform that action.
1559480	1566040	Right, and in some corners of the DSM, Manual for Psychology,
1566040	1571240	there are disorders where apparently something like a bit getting flipped
1571240	1575720	in the human brain causes pathologies that we can be concerned of.
1575760	1579120	So I think there may be some parallels there
1579120	1585640	that at some point in the future might get exercised to degrees we're not anticipating.
1585640	1587880	Now, we're talking about safety.
1587880	1590840	You've mentioned safety several times here
1590840	1596200	and what's on everyone's mind is how unsafe is this?
1596200	1600960	Is this a theoretical exercise for comparison, for instance?
1600960	1604720	I could say, look, one day the sun will burn up the earth.
1604720	1606520	This is a physical certainty.
1606520	1612320	We can prove it as much as we could possibly any conclusion in astrophysics.
1612320	1616800	And despite the fact that that has existential consequences,
1616800	1619760	we will have to evacuate the planet.
1619760	1621920	We are not concerned about that.
1621920	1623840	We should not be focusing on that now
1623840	1625920	because it's five billion years in the future.
1625920	1628800	So that makes it much more theoretical at this point.
1628800	1631160	We have more important problems.
1631160	1637600	How would you characterize the urgency of what you are saying with respect to safety?
1637600	1642200	So I know specifically how soon we're going to get to superintelligence.
1642200	1645080	It may take longer than most people anticipate.
1645080	1649120	For reasons we just don't predict, cannot explain.
1649120	1652600	If you look at what heads of top AI labs are saying,
1652600	1656080	they're claiming they're two to three years away from AGI
1656080	1659600	or even superintelligence, depending on how you define it.
1659600	1663440	AGI with access to internet and large computers is already superintelligence.
1663440	1667760	So it could be as little as two years and we keep adding more funding
1667760	1672360	and then making it go faster, accelerating because that's just too slow.
1672360	1676560	So I think it's a little more pressing than the sun problem.
1676560	1678760	Right. I mean, I'm just trying to put some bounds on it.
1678760	1681720	I think it is much more pressing than five billion years.
1681720	1687560	And when you put the possibility of a two to three year time frame on it,
1687600	1692720	if we get the advances in AI that those people are talking about,
1692720	1696640	artificial general intelligence, which is famously vague,
1696640	1697760	we don't really know what that means.
1697760	1703680	But if we get what some consensus is talking about for that at that point,
1703680	1708600	what sort of problems do you anticipate us facing as a result?
1708600	1713120	So you're really asking me what I would do if I was trying to cause problems.
1713120	1714240	That would be the next question.
1714240	1716200	A superintelligence system would, of course,
1716200	1719640	come up with something I can never even consider as a solution.
1719640	1723160	I'm not superintelligent, despite what you might think.
1723160	1727640	Yes, so if you want to know how Roman Impolsky would go about taking over the world
1727640	1729200	is a very separate discussion.
1729200	1734120	I think those systems can rely on standard approaches humans have considered,
1734120	1740440	nanobots, synthetic biology, bribing humans on the internet with cryptocurrencies.
1740440	1745440	But they can also come up with something completely different, which I have no access to.
1745480	1746480	Right. I understood.
1746480	1755400	Now, I wasn't asking you to predict what could an uncontrollable AI run amok do in those circumstances.
1755400	1761240	But in the more general sense, what kind of not specific mayhem,
1761240	1765120	but what level of problems would we face?
1765120	1767440	Well, again, it's hard to predict specifics.
1767440	1772000	People are concerned about worst-case scenarios in computer science and cryptography.
1772000	1774680	You always look at what's the worst that can happen.
1774720	1779520	If you prepare for that and you're all good, it's actually something much more mild in good shape.
1779520	1785160	The worst is existential crisis where it kills everyone or suffering risks
1785160	1787120	where you wish you were killed with everyone.
1787120	1789760	One of the things you say in the book is, and this is a quote,
1789760	1795400	a single failure of a superintelligent system may cause an existential risk event.
1795400	1802280	And that is something that certainly theoretically on the face of it is almost vacuously true.
1802280	1808880	Is there a more detail that you would like to put around that to perhaps draw attention
1808880	1815360	to what it is that you're trying to get more than the few people who are working on this
1815360	1818360	and most notably yourself to take action?
1818360	1821960	Well, I think there is certain danger in making it too specific.
1821960	1824760	People will concentrate on specifics of the problem.
1825520	1830720	As I say, it's going to come up with the next pandemic, COVID-19, 2020,
1830720	1834520	so then everyone's going to talk about vaccines and boosters,
1834520	1837840	completely ignoring the problem of superintelligence being out of control.
1838840	1840080	A fair point, right.
1840080	1843480	We can always plug any specific hole,
1843480	1846280	but the problem is that there are just too many holes and...
1846280	1848920	So that's exactly what we're going to emphasize.
1848920	1851120	And again, it goes with unpredictability.
1851160	1853760	If I could predict any states of that future,
1853760	1856720	I would be violating my own arguments in the book.
1856720	1861120	What then is the purpose of the field of AI safety?
1861120	1865320	So one is to first understand what can be done in this field.
1865320	1868760	We all intuitively understand we want to create systems.
1868760	1871360	We do not regret creating formally.
1871360	1875120	That means we remain in control to some degree.
1875120	1877320	We can undo things we don't like.
1877400	1881600	And we generally all like and benefit from what we experience,
1881600	1886400	and it's not because we are somehow gamed, tricked, altered,
1886400	1888720	modified in some bizarre ways.
1888720	1892440	Part of it is people just trying to move as quickly as they can
1892440	1895960	to understand how to make those systems not do certain things.
1895960	1897640	Why work is more theoretical?
1897640	1900080	It's trying to understand, well, what are the upper limits
1900080	1901800	and what can be done in this field?
1901800	1907240	And surprisingly, I'm discovering that there are very strong upper limits
1907240	1910480	we cannot control truly advanced AI.
1910480	1915960	And then I survey experts, AI, people in different conferences
1915960	1920200	or social media, majority actually agrees with me.
1920200	1922920	They don't think it's possible to control superintelligence
1922920	1924000	indefinitely.
1924000	1927600	Now, there is not a proportionate percentage of people
1927600	1929040	working directly on that.
1929040	1931360	There is not enough publications, books.
1931360	1935440	Even that possibility is still a very out of
1935480	1937320	overton window possibility.
1937320	1938600	But I think it's changing.
1938600	1940960	I have multiple papers on it, peer-reviewed journal,
1940960	1942800	papers, conferences, a book.
1942800	1944880	We had an article in Time Magazine.
1944880	1948960	I'm not sure what else I can do to make it a mainstream belief
1948960	1950160	within this community.
1950160	1953640	But one thing is for sure, no one is arguing the opposite.
1953640	1957440	No one is saying, oh, we have control mechanism
1957440	1959480	that will scale to any level of intelligence.
1959480	1962000	You want to see the code, you want to see even the prototype,
1962000	1963000	even an idea.
1963000	1964840	No one is making that claim.
1964840	1966280	Right, no one is claiming that.
1966280	1968400	There are plenty of people claiming
1968400	1971800	that the problem won't exist and arguing, for instance,
1971800	1974160	that as AI becomes more intelligent,
1974160	1977560	it will become more compassionate and ethical.
1977560	1979720	And of course, there's no proof of that.
1979720	1983640	And it only takes one that isn't to upset the apple cart.
1984080	1987080	That's the end of the first half of the interview.
1987080	1990080	I think it's really interesting how we're having
1990080	1993800	this level of discussion about the goals and ethics
1993800	1997000	and psychology of AI at such a high level,
1997000	2001040	where up to two years ago, when we were having similar discussions,
2001040	2003360	as you could hear me talking with Roman on this podcast
2003360	2006480	four years ago, that is a very different story.
2006480	2008480	And I think that's a very different story
2008480	2010040	than I thought it would be.
2010040	2013160	And talking with Roman on this podcast four years ago,
2013160	2017400	that seemed to most people a very anthropomorphizing,
2017400	2021240	grandiose and expansive point of view that was,
2021240	2025080	if anything, detrimental to the funding environment for AI.
2025080	2027280	I had people tell me exactly that.
2027280	2030360	And yet now, it's become a serious discussion.
2030360	2032360	And I've talked to politicians about this
2032360	2035240	and they take it very seriously too.
2035240	2037800	I'd like to mark a milestone here.
2037800	2040680	We have just crossed a quarter of a million downloads
2040680	2042880	of episodes of this podcast.
2042880	2045080	And I'm pretty sure that they're not all my mother.
2045080	2049640	So thank you, everyone, for giving us such awesome stats
2049640	2051680	and please give us a five-star rating
2051680	2054080	on your podcast platform of choice
2054080	2057600	and we'll get the next quarter million downloads even faster.
2058480	2061400	In today's news, ripped from the headlines about AI,
2061400	2064360	the commercial deployment of robo-taxis
2064360	2066560	continues its tailspin.
2066560	2069200	Last November, General Motors recalled
2069200	2073560	950 driverless cruise cars in San Francisco,
2073560	2076360	where you'll remember that a number of cruise cars
2076360	2079360	are deployed as autonomous taxis.
2079360	2081640	A hit-and-run driver struck a pedestrian
2081640	2083480	who was thrown into another lane
2083480	2087200	where a cruise robo-taxi was unable to stop in time
2087200	2090400	but then ended up dragging the pedestrian.
2090400	2092160	The recall addresses circumstances
2092160	2095000	when the software may cause the cruise AV
2095000	2096920	to attempt to pull over out of traffic
2096920	2099000	instead of remaining stationary,
2099000	2101560	quote, when a pullover is not the desired
2101560	2105360	post-collision response, unquote said cruise.
2105360	2108240	GM has halted cruise operations nationwide
2108240	2110800	and has also halted the production line.
2110800	2114400	And now the cruise internal share price
2114400	2117520	has been cut by more than half since then.
2117520	2120520	Cruise division executives told some engineering
2120520	2123760	and operation staff in internal meetings in recent weeks
2123760	2126080	that they should not expect to see its robo-taxis
2126080	2129200	on city streets again until the fourth quarter.
2129200	2132000	Next week, we will conclude the interview with Roman
2132000	2134480	when we'll talk about how we should respond
2134480	2137040	to this problem of unsafe AI development
2137040	2140160	and how Roman and his community are addressing it,
2140160	2142800	what he would do with infinite resources
2142800	2146720	and the threat Roman's coffee cup poses to humanity.
2146720	2149320	That's next week on AI and You.
2149320	2151280	Until then, remember,
2151280	2153960	no matter how much computers learn how to do,
2153960	2157080	it's how we come together as humans that matters.
2158800	2162240	That's all for this episode of AI and You.
2162240	2163760	Please leave a rating and comment
2163760	2165440	and share with your friends.
2165440	2168240	Get the book, Artificial Intelligence and You,
2168240	2173000	and see more videos and articles at AIandYou.net.
2173000	2178000	That's A-I-A-N-D-Y-O-U.net,
2178840	2181160	where you can also send us your questions.
2181160	2182360	Thank you for listening.
