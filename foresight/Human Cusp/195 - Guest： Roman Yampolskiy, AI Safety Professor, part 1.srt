1
00:00:00,000 --> 00:00:11,800
Artificial Intelligence will completely transform our world, but what is AI?

2
00:00:11,800 --> 00:00:13,800
Why will it affect you?

3
00:00:13,800 --> 00:00:21,360
And how can you and your business survive and thrive through the AI revolution?

4
00:00:21,360 --> 00:00:24,680
Welcome to AI and You.

5
00:00:25,320 --> 00:00:30,880
Here is your host, author, speaker, and futurist Peter Scott.

6
00:00:32,880 --> 00:00:36,720
Hello and welcome to Episode 195.

7
00:00:36,720 --> 00:00:44,720
I am chuffed to welcome back to the show Roman Jampolski, our first three-peat guest.

8
00:00:44,720 --> 00:00:52,840
Roman was on the show in Episodes 16 and 17 and also nearly a year ago in Episodes 160 and 161.

9
00:00:52,840 --> 00:00:57,920
He is here today because he has a new book that has just hit the shelves called AI,

10
00:00:57,920 --> 00:01:03,920
unexplainable, unpredictable, uncontrollable, which gives you, right there,

11
00:01:03,920 --> 00:01:08,120
a good idea of what Roman's field is, AI safety.

12
00:01:08,120 --> 00:01:12,880
A term he has done more than anyone else to promote and inhabit.

13
00:01:12,880 --> 00:01:18,560
Roman is tenured associate professor in the Department of Computer Engineering and Computer Science

14
00:01:18,560 --> 00:01:23,400
at the Speed School of Engineering, University of Louisville in Kentucky.

15
00:01:23,400 --> 00:01:26,640
He is the founding and current director of the Cyber Security Lab

16
00:01:26,640 --> 00:01:31,440
and has written many key books in the field such as Artificial Superintelligence,

17
00:01:31,440 --> 00:01:37,160
a futuristic approach, and Artificial Intelligence, Safety and Security.

18
00:01:37,160 --> 00:01:41,040
He's been central in the field of warning about the control problem

19
00:01:41,040 --> 00:01:45,320
and the value alignment problem of AI from the very beginning.

20
00:01:45,320 --> 00:01:49,680
Back when doing so earned people some scorn from practitioners,

21
00:01:49,680 --> 00:01:57,520
yet Roman is a professor of computer science and applies rigorous methods to his analyses of these problems.

22
00:01:57,520 --> 00:02:01,600
It's those rigorous methods that we want to tap into in this interview

23
00:02:01,600 --> 00:02:08,480
because Roman connects principles of computer science with the issue of existential risk from AI,

24
00:02:08,480 --> 00:02:12,120
which is an area, obviously of ultimate importance to us all,

25
00:02:12,120 --> 00:02:18,160
but one which has a lot of opinion and argument behind it and could use some more science.

26
00:02:18,160 --> 00:02:22,280
Speaking of which, we're going to refer to the halting problem in this interview

27
00:02:22,280 --> 00:02:29,200
and because it's a problem in computer science that was famously solved by Alan Turing in 1936

28
00:02:29,200 --> 00:02:31,880
with an elegant proof, here's the proof.

29
00:02:31,880 --> 00:02:38,240
The halting problem asks whether there could be a computer program or algorithm

30
00:02:38,240 --> 00:02:44,760
that would be capable of telling whether any computer program you fed it would eventually stop running

31
00:02:44,760 --> 00:02:49,760
or whether it would run forever given any specific input.

32
00:02:49,760 --> 00:02:54,440
Turing showed that it was impossible for such a program to exist.

33
00:02:54,440 --> 00:02:56,360
Here's how he did it.

34
00:02:56,360 --> 00:03:00,280
Suppose that such a program does exist and it returns true

35
00:03:00,280 --> 00:03:04,880
if the program and input that you give it will halt false otherwise.

36
00:03:04,880 --> 00:03:08,240
Now we make a new algorithm with the following definition.

37
00:03:08,240 --> 00:03:16,720
It takes a program as its input, then calls the halting analysis program with that program as both program and input

38
00:03:16,720 --> 00:03:22,000
and if that program returns true then it goes into an infinite loop, otherwise it halts.

39
00:03:22,000 --> 00:03:27,920
Now we take that algorithm and call it with itself as input, which means

40
00:03:27,920 --> 00:03:33,400
if the halting analysis program returns true it means the algorithm we just made halts when given itself as input

41
00:03:33,400 --> 00:03:38,760
but we just said that the definition of that algorithm is that in that case it will loop forever.

42
00:03:38,760 --> 00:03:42,080
That's a contradiction. We went wrong somewhere.

43
00:03:42,080 --> 00:03:47,320
There's an equal contradiction if the halting analysis program returns false.

44
00:03:47,320 --> 00:03:54,880
So our initial assumption, the only one we made that such a program could exist, must be false.

45
00:03:54,880 --> 00:03:59,920
This is a powerful proof, somewhat similar to Goedl's incompleteness theorem

46
00:03:59,960 --> 00:04:03,360
and logical statements like this statement is unprovable.

47
00:04:03,360 --> 00:04:11,320
An analogical proof shows that there can be no algorithm that could compress all possible input data sets without loss

48
00:04:11,320 --> 00:04:18,120
because otherwise you could iteratively reduce any data set to an arbitrarily small size which is impossible.

49
00:04:18,120 --> 00:04:24,720
In other words, trying to gzip your jpeg files probably isn't going to make them any smaller.

50
00:04:24,720 --> 00:04:31,320
So we're going to talk about safety of AI where many people seem to suffer from this sort of collective blind spot.

51
00:04:31,320 --> 00:04:38,360
A kind of group think that the onus is on the people calling for pauses in its development to show that it's not safe.

52
00:04:38,360 --> 00:04:46,080
They say things like this could turn out very well for us as though those were things that were final arguments against the pausing.

53
00:04:46,080 --> 00:04:47,920
This is not how safety works.

54
00:04:47,920 --> 00:04:50,480
Imagine you went to get on a bus and the driver said,

55
00:04:50,480 --> 00:04:55,800
I can imagine ways in which this bus could reach its destination totally unscathed.

56
00:04:55,800 --> 00:05:00,480
Our job as engineers is to prove that something is safe.

57
00:05:00,480 --> 00:05:04,960
The absence of proof that something is dangerous is not the same thing.

58
00:05:04,960 --> 00:05:10,120
It's a fair question to ask, how is superintelligence supposed to be able to defeat us?

59
00:05:10,120 --> 00:05:16,000
And people like Roman don't get into that, which looks like a cop-out, but the problem is that when you ask that,

60
00:05:16,000 --> 00:05:24,960
if you get any specific answer like it could decide that humanity is a threat to its purpose and take over biosynthesis labs to produce and release deadly pathogens,

61
00:05:24,960 --> 00:05:33,320
well, deadly to us, we can say, OK, well, then we will ensure that all the biosynthesis labs are disconnected from the network.

62
00:05:33,320 --> 00:05:36,840
And so forth, we could do that for any scenario that we could imagine.

63
00:05:36,840 --> 00:05:43,800
But the problem is that there are many, many more scenarios than we can imagine, effectively an infinite number.

64
00:05:43,840 --> 00:05:48,800
And a superintelligent AI would be very good at finding the ones we hadn't thought about.

65
00:05:48,800 --> 00:05:55,240
To the same kind of fat tail problem I've talked about with respect to why self-driving cars are still not ready for prime time.

66
00:05:55,240 --> 00:05:57,440
More on that later.

67
00:05:57,440 --> 00:06:01,920
Anyway, enough of me. Let's get into the interview.

68
00:06:01,920 --> 00:06:07,480
Well, Roman Jampolsky, welcome so much back to artificial intelligence and you.

69
00:06:07,480 --> 00:06:15,680
We're here to talk about your new book, AI, unexplainable, unpredictable, uncontrollable.

70
00:06:15,680 --> 00:06:21,760
And that is a set of daunting claims.

71
00:06:21,760 --> 00:06:34,120
Before we get into that, I want listeners to get some feel for your background because that may play into what's brought you to the point of writing this book.

72
00:06:34,120 --> 00:06:40,440
And so could you give us the encapsulation of your life from birth to Louisville?

73
00:06:40,440 --> 00:06:42,800
From birth? Wow, that's ambitious.

74
00:06:42,800 --> 00:06:44,360
Thanks for having me back.

75
00:06:44,360 --> 00:06:46,480
I'm a computer scientist at E01.

76
00:06:46,480 --> 00:06:49,240
I'm a PhD in computer science and engineering.

77
00:06:49,240 --> 00:06:54,320
I've been working on different sub-domains of AI safety for over a decade.

78
00:06:54,320 --> 00:07:01,360
I've published a couple of hundred billion good papers, a few books, mostly on AI safety.

79
00:07:01,360 --> 00:07:19,200
In my full-time area of research in the last couple years for sure, my background is, I guess, initially I was raised in Soviet Union, came to US in 94, lived in New York for about 15 years, now I've been in Kentucky for about as much.

80
00:07:19,200 --> 00:07:27,000
And this book gets into some serious territory that impacts a great many people.

81
00:07:27,080 --> 00:07:40,640
I mean, it's inhabiting this rather narrow niche in a way of being a book about computer science written by a computer scientist containing computer science language.

82
00:07:40,640 --> 00:07:51,800
And yet it's talking about something that has fundamental impact on the entire human race and is aimed at a much wider audience than computer scientists.

83
00:07:51,800 --> 00:07:54,440
So could you tell us why you wrote this book?

84
00:07:54,440 --> 00:07:55,600
Well, that's my research.

85
00:07:55,600 --> 00:07:56,640
That's what I'm doing.

86
00:07:56,640 --> 00:08:01,280
I think it's the most important problem ever in any discipline.

87
00:08:01,280 --> 00:08:11,760
We're trying to understand if the science and engineering we're doing right now, creating intelligent assistants, helpers is a long-term beneficial strategy.

88
00:08:11,760 --> 00:08:20,760
If there is any chance that will be too successful, create something too intelligent, too independent, and that will backfire tremendously.

89
00:08:20,760 --> 00:08:31,560
And it's actually shocking and surprising to me that even with a tiny minority of people in AI doing a safety work within that community,

90
00:08:31,560 --> 00:08:38,480
there is really no one outside of me doing work on fundamentals of what is possible in this area.

91
00:08:38,480 --> 00:08:49,440
There is a few informal, independent scholars publishing a few blog posts, but you would expect this problem to be as popular as NP versus PEEP problem.

92
00:08:49,440 --> 00:08:51,840
Can you control super 1000 machines yet?

93
00:08:51,840 --> 00:08:52,920
There is nothing.

94
00:08:52,920 --> 00:09:03,600
And you mentioned their P versus NP classic trope in computer science about the scale of computability of problems.

95
00:09:03,600 --> 00:09:17,160
Now, you talk about the control problem and the alignment problem in the book in ways that embody what appears to be mathematical proof.

96
00:09:17,200 --> 00:09:22,240
It reminds me very much of the halting problem in computer science.

97
00:09:22,240 --> 00:09:35,200
The question is it possible to write a program that when you feed it the source code of some other program can tell you whether that program would ever finish if it were to be run.

98
00:09:35,200 --> 00:09:44,240
And there is a simple and elegant proof that no program can be written that can do that for every possible program that you can give to it.

99
00:09:44,800 --> 00:09:46,720
Your work appears reminiscent of that.

100
00:09:46,720 --> 00:09:56,360
Would you put it in the same category as that level of mathematical proof that you have created with respect to controllability?

101
00:09:56,360 --> 00:09:57,280
Not exactly.

102
00:09:57,280 --> 00:10:00,440
So there is different levels of control we can talk about.

103
00:10:00,440 --> 00:10:04,680
In a chapter, I suggest at least four different levels of control.

104
00:10:04,680 --> 00:10:18,920
For what I call direct control, yes, we can come up with a very mathematically rigorous proof similar to what we have seen with mathematical proofs where we show that it creates a contradiction that it's impossible.

105
00:10:18,920 --> 00:10:24,560
But there are, of course, other levels of control, delegated control, ideal advisor.

106
00:10:24,560 --> 00:10:27,800
And for that, we don't have a strong mathematical proof.

107
00:10:27,800 --> 00:10:33,520
But what we do have is a survey of every relevant field to this problem.

108
00:10:33,520 --> 00:10:38,240
So if you think, OK, we need to accomplish this, this is the tool set, I need to be able to do it.

109
00:10:38,240 --> 00:10:40,680
I need to be able to aggregate votes.

110
00:10:40,680 --> 00:10:47,080
I need to be able to elicit preferences, mathematics, economics, any field you want.

111
00:10:47,080 --> 00:10:54,880
If you survey those fields, they are very well-established proven impossibility results which say, well, you cannot get all those tools.

112
00:10:54,880 --> 00:10:58,320
You can get approximate partial results for many of them.

113
00:10:58,320 --> 00:11:03,200
But if you really need the exact result, you're not going to get there.

114
00:11:03,200 --> 00:11:09,520
And it's not me saying it, it's top experts in all these fields saying, that's impossible, you cannot do it.

115
00:11:09,520 --> 00:11:13,120
Now, one of the elements of your title is unpredictability.

116
00:11:13,120 --> 00:11:21,320
And is that something where you have also arrived at something amounting to a proof that AI is not predictable?

117
00:11:21,320 --> 00:11:24,200
Yes, that one is actually a good solid proof.

118
00:11:24,200 --> 00:11:26,600
It's, again, self-referential all the way.

119
00:11:26,600 --> 00:11:33,320
If we make this assumption that we're creating superintelligence, a system smarter than any one of us in all domains,

120
00:11:33,320 --> 00:11:39,800
then if we could accurately predict what decisions the system will make, we would be equally smart.

121
00:11:39,800 --> 00:11:47,200
Let's say we're playing a game of chess, if I can predict every move my opponent is going to make, I'm at least as good of a chess player.

122
00:11:47,200 --> 00:11:49,480
But that's a violation of our assumption.

123
00:11:49,480 --> 00:11:54,400
We have proof by contradiction that, no, of course, we cannot make those predictions.

124
00:11:54,440 --> 00:12:01,800
We can maybe predict general direction the agent is trying to take us, like that system is trying to win a game of chess.

125
00:12:01,800 --> 00:12:05,240
But we cannot predict specific moves it's going to make to get there.

126
00:12:05,240 --> 00:12:10,520
And this leads us on a direction that's novel with respect to computers,

127
00:12:10,520 --> 00:12:16,560
because the entire field of computer science is founded on predictability.

128
00:12:16,560 --> 00:12:21,040
You cannot, and certainly the way I was trained as a computer scientist,

129
00:12:21,080 --> 00:12:28,160
you are aiming for 100% predictability, otherwise you don't know what you're doing, literally.

130
00:12:28,160 --> 00:12:37,280
And here you have a proof that we're heading in direction of creating things that are provably unpredictable.

131
00:12:37,280 --> 00:12:43,560
Does that not rock the foundations of the science that AI is founded on?

132
00:12:43,560 --> 00:12:48,960
Well, interestingly, I think for the longest time, AI was not science, it was engineering.

133
00:12:49,000 --> 00:12:54,280
We use software engineering, we design specific engineering artifacts for where you design a bridge.

134
00:12:54,280 --> 00:12:58,240
We had specifications, we met them, we knew how the system works.

135
00:12:58,240 --> 00:13:04,040
In about the last decade or so, we switched from where we explicitly programmed their systems,

136
00:13:04,040 --> 00:13:06,800
we kind of teach them to learn independently.

137
00:13:06,800 --> 00:13:10,480
We give them data, we give them computing power, and we say, go figure it out,

138
00:13:10,480 --> 00:13:16,880
and then we'll study to see how good they are at that domain or maybe their general systems.

139
00:13:16,880 --> 00:13:21,160
But it's more like science now where we're on experiments and those artifacts,

140
00:13:21,160 --> 00:13:26,320
and we discover what they are after the fact, not at the time we design them.

141
00:13:26,320 --> 00:13:32,880
So that's quite a paradigm shift, but I think it's never been science before.

142
00:13:32,880 --> 00:13:38,920
To hop around some of the elements of your title here, the third one is unexplainable.

143
00:13:38,920 --> 00:13:45,120
And there is a huge amount of effort going into making AI explainable right now.

144
00:13:45,120 --> 00:13:55,280
And I think this is worth dissecting because some of that work is succeeding and will provide value,

145
00:13:55,280 --> 00:14:00,680
but it's not necessarily the kind of explainability that I think you're talking about.

146
00:14:00,680 --> 00:14:06,240
In this sense that I mean this, we've had people on the show from IBM who talk about

147
00:14:06,240 --> 00:14:12,480
rather ingenious ways that they get AI to explain how it came up with a decision

148
00:14:12,480 --> 00:14:15,840
with respect to someone's insurability, for instance.

149
00:14:15,840 --> 00:14:21,400
That could be useful, could be fit for a purpose in the insurance industry.

150
00:14:21,400 --> 00:14:27,840
But you're going for something more deeper, broader, can you explain?

151
00:14:27,840 --> 00:14:29,520
Pan intended that to us.

152
00:14:29,520 --> 00:14:31,960
So there are certainly things we can explain.

153
00:14:31,960 --> 00:14:34,360
No one argues that everything is unexplainable.

154
00:14:34,360 --> 00:14:36,800
You can explain trivial decisions.

155
00:14:36,800 --> 00:14:42,440
If you're playing a game of tic-tac-toe, you can certainly tell me why you moved to a specific square.

156
00:14:42,480 --> 00:14:45,080
I'm talking about the extreme on the other end.

157
00:14:45,080 --> 00:14:52,760
There was complex systems, systems making decisions based on billions of nodes in the neural network,

158
00:14:52,760 --> 00:14:54,320
trillion of weights.

159
00:14:54,320 --> 00:14:58,680
The system looks at the totality of those to make its decision.

160
00:14:58,680 --> 00:15:01,160
It's working in a novel domain.

161
00:15:01,160 --> 00:15:08,520
So far, we sometimes succeeded understanding one node or maybe a small subset of those nodes.

162
00:15:08,520 --> 00:15:11,720
Okay, they fire, this input is produced.

163
00:15:11,720 --> 00:15:13,280
That's what stimulates them.

164
00:15:13,280 --> 00:15:15,280
That doesn't give us complete picture.

165
00:15:15,280 --> 00:15:16,720
We really have two options.

166
00:15:16,720 --> 00:15:21,600
We can either get some sort of a simplified, reduced explanation.

167
00:15:21,600 --> 00:15:26,040
Top 10 reasons this decision was made, kind of like glossy compression.

168
00:15:26,040 --> 00:15:27,720
And it's useful.

169
00:15:27,720 --> 00:15:28,640
It's beneficial.

170
00:15:28,640 --> 00:15:29,960
It's better than nothing.

171
00:15:29,960 --> 00:15:31,840
But it doesn't give you complete picture.

172
00:15:31,840 --> 00:15:34,880
You can still hide information in such explanations.

173
00:15:34,880 --> 00:15:36,440
That's what we do with children.

174
00:15:36,440 --> 00:15:37,840
We don't give them complete picture.

175
00:15:37,840 --> 00:15:41,480
We give them just so explanation, just so stories.

176
00:15:41,480 --> 00:15:44,120
The opposite is a complete picture.

177
00:15:44,120 --> 00:15:47,240
We will give you a full explanation for how the decision is made.

178
00:15:47,240 --> 00:15:49,080
But that is the whole model.

179
00:15:49,080 --> 00:15:51,840
That's the whole AI system with all of its weights.

180
00:15:51,840 --> 00:15:54,160
That's not something a human brain can survey.

181
00:15:54,160 --> 00:15:55,480
It's too large.

182
00:15:55,480 --> 00:15:57,560
It's not human readable.

183
00:15:57,560 --> 00:16:01,000
Simply giving you access to all the weights will explain nothing to you.

184
00:16:01,000 --> 00:16:06,400
So either we have problem with comprehending explanation given to us,

185
00:16:06,440 --> 00:16:11,360
or we're given a simplified, not full picture explanation,

186
00:16:11,360 --> 00:16:13,720
which does not guarantee safety.

187
00:16:13,720 --> 00:16:19,440
So there you're leading us to why unexplainability is important.

188
00:16:19,440 --> 00:16:23,240
And I will get into that in a moment.

189
00:16:23,240 --> 00:16:29,840
First, I want to say you are a neural network with billions of nodes.

190
00:16:29,840 --> 00:16:36,160
If I ask you to explain some decision, I will get some useful answer out.

191
00:16:36,160 --> 00:16:41,440
How does this differ from what we want out of a neural network, an artificial one?

192
00:16:41,440 --> 00:16:43,800
It really depends on what you ask me to explain.

193
00:16:43,800 --> 00:16:47,960
If you ask me to explain how I decide what podcasts to go on,

194
00:16:47,960 --> 00:16:50,280
I can probably explain that pretty well.

195
00:16:50,280 --> 00:16:54,440
If you ask me how I recognize your face, I will give you a BS story,

196
00:16:54,440 --> 00:16:56,360
which has nothing to do with reality.

197
00:16:56,360 --> 00:16:57,440
Right.

198
00:16:57,440 --> 00:17:01,920
And then we are using neural networks at the moment, artificial ones.

199
00:17:01,920 --> 00:17:05,840
Again, to do things like recognize faces.

200
00:17:05,840 --> 00:17:12,600
And so expecting an answer of how they came up with that is perhaps futile,

201
00:17:12,600 --> 00:17:15,960
because we couldn't relate it to anything that we understand,

202
00:17:15,960 --> 00:17:18,000
since we don't know how we do that either.

203
00:17:18,000 --> 00:17:23,240
And even the rather clever attempts to do that have often highlighted things in the image

204
00:17:23,240 --> 00:17:26,400
that have nothing to do with the face, which is alarming.

205
00:17:26,400 --> 00:17:30,440
But yet they work anyway, which is even more alarming.

206
00:17:30,480 --> 00:17:39,200
Does safety have more to do with questions that we can't expect an explanation for?

207
00:17:39,200 --> 00:17:41,400
Like, how did you recognize this face?

208
00:17:41,400 --> 00:17:46,960
Why did you choose to draw that image when I asked for a picture of a poodle riding a motorcycle?

209
00:17:46,960 --> 00:17:56,440
Or is it more impacted by questions where we would expect a human comprehensible explanation

210
00:17:56,440 --> 00:18:01,480
of a decision like why are you suggesting that we launch our strategic defense missiles?

211
00:18:01,480 --> 00:18:08,480
Well, for us to fully confirm that the system is safe and operational as we expect it to be,

212
00:18:08,480 --> 00:18:10,640
we need to understand how it works.

213
00:18:10,640 --> 00:18:17,040
It doesn't seem like we're designing it, so we have to deal with this artifact produced by this

214
00:18:17,040 --> 00:18:20,000
process of self-learning, self-modification.

215
00:18:20,000 --> 00:18:25,440
And if we can't comprehend how it works, it doesn't give us any reason to think it's safe.

216
00:18:25,440 --> 00:18:28,000
Asking how did you make this decision?

217
00:18:28,000 --> 00:18:32,600
Is it useful in part to decide should we accept this decision?

218
00:18:32,600 --> 00:18:37,280
If we are accepting it based on previous results, it's always been right so far.

219
00:18:37,280 --> 00:18:42,160
Should we just take it as a religious statement that it's going to be right in the future?

220
00:18:42,160 --> 00:18:46,760
It's now an oracle-type god-like system that seems to be dangerous.

221
00:18:46,760 --> 00:18:50,960
You can have a stretch-restaurant situation where it gives you 10 good answers,

222
00:18:50,960 --> 00:18:55,080
gets you trust, and then does something against your preferences.

223
00:18:55,080 --> 00:18:59,480
So I think, as I said, if you look at the totality of a problem of control,

224
00:18:59,480 --> 00:19:00,640
you have those tools.

225
00:19:00,640 --> 00:19:05,160
You need to be able to understand how it works, predict what it's going to do,

226
00:19:05,160 --> 00:19:07,560
be able to communicate with it without ambiguity.

227
00:19:07,560 --> 00:19:12,080
Say, we have a paper with about 50 different relevant results,

228
00:19:12,080 --> 00:19:14,520
and it doesn't look like we have those.

229
00:19:14,520 --> 00:19:18,600
So it's very hard for me to understand why people argue that,

230
00:19:18,600 --> 00:19:19,880
oh, certainly we can do it.

231
00:19:19,880 --> 00:19:20,520
It's easy.

232
00:19:20,520 --> 00:19:23,240
We just need to have more money for this project.

233
00:19:23,280 --> 00:19:25,520
So it's not obvious.

234
00:19:25,520 --> 00:19:31,520
And I think for humans, we want explanations for at least two reasons.

235
00:19:31,520 --> 00:19:35,280
One would be, I want to know how to do better in the future.

236
00:19:35,280 --> 00:19:42,720
Like, tell me why you denied my college application so my next one can be accepted.

237
00:19:42,720 --> 00:19:45,720
And then there's the, I don't trust you.

238
00:19:45,720 --> 00:19:49,200
If you give me an explanation of why you made that decision,

239
00:19:49,200 --> 00:19:55,960
maybe I can find the floor in your logic or where you're not working properly.

240
00:19:55,960 --> 00:20:04,840
So does explainability for AI have more for you personally to do with the verifiability problem?

241
00:20:04,840 --> 00:20:07,480
Of course, and that's another impossibility result.

242
00:20:07,480 --> 00:20:12,240
There is strong reason to think we cannot perfectly verify software mathematical proofs.

243
00:20:12,240 --> 00:20:17,000
We can get it to a certain degree of accuracy proportionate to the resources we allocate.

244
00:20:17,000 --> 00:20:21,240
But we never get 100% guarantee that it's always a possibility,

245
00:20:21,240 --> 00:20:23,920
one in a billion chance it will make our own decision.

246
00:20:23,920 --> 00:20:27,960
But if a system continues making billions of decisions every minute,

247
00:20:27,960 --> 00:20:30,480
you basically guarantee that it's going to fail pretty soon.

248
00:20:30,480 --> 00:20:35,320
So we're talking about software here that as it becomes more intelligent,

249
00:20:35,320 --> 00:20:41,560
more AGI-like, becomes closer to the characteristics of human beings

250
00:20:41,560 --> 00:20:47,600
and then of course exceeding them in the dimensions of general intelligence.

251
00:20:47,600 --> 00:20:56,960
Would you say that humans are susceptible to the kind of algorithmic analyses that you're making of AI?

252
00:20:56,960 --> 00:21:01,760
Humans also have unexplainable, unpredictable and uncontrollable, certainly.

253
00:21:01,760 --> 00:21:04,920
Yeah, in fact, they're reduced to a human control problem.

254
00:21:04,960 --> 00:21:08,680
If you can't get humans to be safe and controlled,

255
00:21:08,680 --> 00:21:14,640
despite years of attempts, you know, millennia of ethics, morals, religion,

256
00:21:14,640 --> 00:21:18,760
cultural indoctrination, public schooling, you name it.

257
00:21:18,760 --> 00:21:22,080
I mean, we have laws, we have all this and at the end of the day,

258
00:21:22,080 --> 00:21:25,640
you still have a situation where employees betray the company,

259
00:21:25,640 --> 00:21:28,800
citizens betray the country, spouses betray each other.

260
00:21:28,800 --> 00:21:32,720
There is no reliability, there is no guarantee with lie detector tests,

261
00:21:32,720 --> 00:21:35,920
with financial incentives, so none of it works.

262
00:21:35,920 --> 00:21:41,800
So if we can't fully guarantee safety and controllability of the human neural network,

263
00:21:41,800 --> 00:21:46,680
why would we think that scaling it to an extremely large equivalent

264
00:21:46,680 --> 00:21:49,280
would make it somehow more likely to happen?

265
00:21:49,280 --> 00:21:54,640
Right, then I'm trying to come up with a freezing here

266
00:21:54,640 --> 00:21:57,520
because this is a relatively recent thought for me.

267
00:21:57,640 --> 00:22:03,200
When we're talking about AI from the computer science analysis

268
00:22:03,200 --> 00:22:10,760
of it being a set of algorithms and conducting a number of proofs

269
00:22:10,760 --> 00:22:15,400
that resemble the proof of the halting problem, for instance,

270
00:22:15,400 --> 00:22:20,640
at some point, does that approach no longer become valid

271
00:22:20,640 --> 00:22:26,920
if we're talking about something that is as impenetrable as human beings

272
00:22:26,960 --> 00:22:34,320
for whom we would not attempt to prove the halting problem of our brains, I imagine.

273
00:22:34,320 --> 00:22:42,000
Would you consider that the human brain is operating on algorithms,

274
00:22:42,000 --> 00:22:46,560
just ones that we don't know, that could be analyzed

275
00:22:46,560 --> 00:22:49,960
if we had the capability, the technology,

276
00:22:49,960 --> 00:22:56,120
to the level of detail that we know for AI?

277
00:22:56,160 --> 00:22:58,680
So human brain is particularly interesting

278
00:22:58,680 --> 00:23:04,200
because there is not even full agreement on what capabilities

279
00:23:04,200 --> 00:23:10,240
are a direct result of brain architecture and which may be something else.

280
00:23:10,240 --> 00:23:13,200
So then we talk about hard problem of consciousness.

281
00:23:13,200 --> 00:23:16,560
We don't seem to find guaranteed correlates of consciousness.

282
00:23:16,560 --> 00:23:20,080
We don't know why some states of matter would generate those,

283
00:23:20,080 --> 00:23:22,280
the most interesting, the most important states.

284
00:23:22,280 --> 00:23:25,320
We have no idea how to make a computer feel pain.

285
00:23:25,320 --> 00:23:29,680
Yes, it's a fundamental part of being a human, probably an animal.

286
00:23:29,680 --> 00:23:34,520
So we have very limited understanding of human brain,

287
00:23:34,520 --> 00:23:38,400
but because we don't have direct access to a lot of source code,

288
00:23:38,400 --> 00:23:41,840
a lot of what makes a brain tick,

289
00:23:41,840 --> 00:23:47,680
there could be differences which make understanding artificial neural networks easier.

290
00:23:47,680 --> 00:23:50,120
We don't think they have any magical properties.

291
00:23:50,120 --> 00:23:54,480
We certainly know there is no immortal soul or anything like that in them.

292
00:23:54,480 --> 00:23:58,600
At the same time, the size and scale make it so much harder for human brains

293
00:23:58,600 --> 00:24:02,320
to fully comprehend, fully survey that architecture.

294
00:24:02,320 --> 00:24:08,680
So I think there is enough differences to make any analogy not a perfect analogy.

295
00:24:08,680 --> 00:24:11,160
Let me turn it around then.

296
00:24:11,160 --> 00:24:17,240
Is it possible or likely that AI could become so complex

297
00:24:17,240 --> 00:24:23,200
that it will become no longer useful to think of it as a computer program

298
00:24:23,240 --> 00:24:29,400
and apply the analyses that we do of computer algorithms to it anymore

299
00:24:29,400 --> 00:24:33,120
than we could apply those to human beings?

300
00:24:33,120 --> 00:24:37,120
It's certainly possible, but I don't think we're already doing much of that.

301
00:24:37,120 --> 00:24:41,320
We were not directly measuring those systems, I think,

302
00:24:41,320 --> 00:24:44,680
in standard measures of complexity, begonotation.

303
00:24:44,680 --> 00:24:47,320
That's not how we evaluate those systems.

304
00:24:47,320 --> 00:24:52,160
So we look at them and try to create measures for how helpful it is,

305
00:24:52,160 --> 00:24:55,840
for how useful it is, for how well-behaved it is.

306
00:24:55,840 --> 00:25:00,120
Those are kind of similar to psychiatric tools we use with humans.

307
00:25:00,120 --> 00:25:01,560
Okay, that's a psychopath.

308
00:25:01,560 --> 00:25:05,840
This one is kind of well aligned with the values of society,

309
00:25:05,840 --> 00:25:09,760
but those are very fuzzy disciplines, very fuzzy measures.

310
00:25:09,760 --> 00:25:15,200
Do you see a point perhaps where it becomes more useful in the future

311
00:25:15,200 --> 00:25:19,720
to treat AI with the tools of psychology than computer science?

312
00:25:19,760 --> 00:25:24,360
It's definitely useful and it's starting to be, I think, a direction of research

313
00:25:24,360 --> 00:25:30,240
where you're trying to prompt engineer it to do better, act safer.

314
00:25:30,240 --> 00:25:34,520
The problem, of course, is it's a very fuzzy area

315
00:25:34,520 --> 00:25:37,840
and just because you succeeded in one way of prompting,

316
00:25:37,840 --> 00:25:40,520
it doesn't mean that it's not an alternative phrasing

317
00:25:40,520 --> 00:25:46,520
which completely overrides any previous instructions or bypasses any safeguards.

318
00:25:47,320 --> 00:25:49,200
If you say it this way, the system says,

319
00:25:49,200 --> 00:25:51,520
oh, I will not do it, this is very evil action,

320
00:25:51,520 --> 00:25:54,680
but if you put a negative sign in front of one of the words,

321
00:25:54,680 --> 00:25:56,760
somehow it bypasses all the safeguards

322
00:25:56,760 --> 00:25:59,480
and now it's happy to perform that action.

323
00:25:59,480 --> 00:26:06,040
Right, and in some corners of the DSM, Manual for Psychology,

324
00:26:06,040 --> 00:26:11,240
there are disorders where apparently something like a bit getting flipped

325
00:26:11,240 --> 00:26:15,720
in the human brain causes pathologies that we can be concerned of.

326
00:26:15,760 --> 00:26:19,120
So I think there may be some parallels there

327
00:26:19,120 --> 00:26:25,640
that at some point in the future might get exercised to degrees we're not anticipating.

328
00:26:25,640 --> 00:26:27,880
Now, we're talking about safety.

329
00:26:27,880 --> 00:26:30,840
You've mentioned safety several times here

330
00:26:30,840 --> 00:26:36,200
and what's on everyone's mind is how unsafe is this?

331
00:26:36,200 --> 00:26:40,960
Is this a theoretical exercise for comparison, for instance?

332
00:26:40,960 --> 00:26:44,720
I could say, look, one day the sun will burn up the earth.

333
00:26:44,720 --> 00:26:46,520
This is a physical certainty.

334
00:26:46,520 --> 00:26:52,320
We can prove it as much as we could possibly any conclusion in astrophysics.

335
00:26:52,320 --> 00:26:56,800
And despite the fact that that has existential consequences,

336
00:26:56,800 --> 00:26:59,760
we will have to evacuate the planet.

337
00:26:59,760 --> 00:27:01,920
We are not concerned about that.

338
00:27:01,920 --> 00:27:03,840
We should not be focusing on that now

339
00:27:03,840 --> 00:27:05,920
because it's five billion years in the future.

340
00:27:05,920 --> 00:27:08,800
So that makes it much more theoretical at this point.

341
00:27:08,800 --> 00:27:11,160
We have more important problems.

342
00:27:11,160 --> 00:27:17,600
How would you characterize the urgency of what you are saying with respect to safety?

343
00:27:17,600 --> 00:27:22,200
So I know specifically how soon we're going to get to superintelligence.

344
00:27:22,200 --> 00:27:25,080
It may take longer than most people anticipate.

345
00:27:25,080 --> 00:27:29,120
For reasons we just don't predict, cannot explain.

346
00:27:29,120 --> 00:27:32,600
If you look at what heads of top AI labs are saying,

347
00:27:32,600 --> 00:27:36,080
they're claiming they're two to three years away from AGI

348
00:27:36,080 --> 00:27:39,600
or even superintelligence, depending on how you define it.

349
00:27:39,600 --> 00:27:43,440
AGI with access to internet and large computers is already superintelligence.

350
00:27:43,440 --> 00:27:47,760
So it could be as little as two years and we keep adding more funding

351
00:27:47,760 --> 00:27:52,360
and then making it go faster, accelerating because that's just too slow.

352
00:27:52,360 --> 00:27:56,560
So I think it's a little more pressing than the sun problem.

353
00:27:56,560 --> 00:27:58,760
Right. I mean, I'm just trying to put some bounds on it.

354
00:27:58,760 --> 00:28:01,720
I think it is much more pressing than five billion years.

355
00:28:01,720 --> 00:28:07,560
And when you put the possibility of a two to three year time frame on it,

356
00:28:07,600 --> 00:28:12,720
if we get the advances in AI that those people are talking about,

357
00:28:12,720 --> 00:28:16,640
artificial general intelligence, which is famously vague,

358
00:28:16,640 --> 00:28:17,760
we don't really know what that means.

359
00:28:17,760 --> 00:28:23,680
But if we get what some consensus is talking about for that at that point,

360
00:28:23,680 --> 00:28:28,600
what sort of problems do you anticipate us facing as a result?

361
00:28:28,600 --> 00:28:33,120
So you're really asking me what I would do if I was trying to cause problems.

362
00:28:33,120 --> 00:28:34,240
That would be the next question.

363
00:28:34,240 --> 00:28:36,200
A superintelligence system would, of course,

364
00:28:36,200 --> 00:28:39,640
come up with something I can never even consider as a solution.

365
00:28:39,640 --> 00:28:43,160
I'm not superintelligent, despite what you might think.

366
00:28:43,160 --> 00:28:47,640
Yes, so if you want to know how Roman Impolsky would go about taking over the world

367
00:28:47,640 --> 00:28:49,200
is a very separate discussion.

368
00:28:49,200 --> 00:28:54,120
I think those systems can rely on standard approaches humans have considered,

369
00:28:54,120 --> 00:29:00,440
nanobots, synthetic biology, bribing humans on the internet with cryptocurrencies.

370
00:29:00,440 --> 00:29:05,440
But they can also come up with something completely different, which I have no access to.

371
00:29:05,480 --> 00:29:06,480
Right. I understood.

372
00:29:06,480 --> 00:29:15,400
Now, I wasn't asking you to predict what could an uncontrollable AI run amok do in those circumstances.

373
00:29:15,400 --> 00:29:21,240
But in the more general sense, what kind of not specific mayhem,

374
00:29:21,240 --> 00:29:25,120
but what level of problems would we face?

375
00:29:25,120 --> 00:29:27,440
Well, again, it's hard to predict specifics.

376
00:29:27,440 --> 00:29:32,000
People are concerned about worst-case scenarios in computer science and cryptography.

377
00:29:32,000 --> 00:29:34,680
You always look at what's the worst that can happen.

378
00:29:34,720 --> 00:29:39,520
If you prepare for that and you're all good, it's actually something much more mild in good shape.

379
00:29:39,520 --> 00:29:45,160
The worst is existential crisis where it kills everyone or suffering risks

380
00:29:45,160 --> 00:29:47,120
where you wish you were killed with everyone.

381
00:29:47,120 --> 00:29:49,760
One of the things you say in the book is, and this is a quote,

382
00:29:49,760 --> 00:29:55,400
a single failure of a superintelligent system may cause an existential risk event.

383
00:29:55,400 --> 00:30:02,280
And that is something that certainly theoretically on the face of it is almost vacuously true.

384
00:30:02,280 --> 00:30:08,880
Is there a more detail that you would like to put around that to perhaps draw attention

385
00:30:08,880 --> 00:30:15,360
to what it is that you're trying to get more than the few people who are working on this

386
00:30:15,360 --> 00:30:18,360
and most notably yourself to take action?

387
00:30:18,360 --> 00:30:21,960
Well, I think there is certain danger in making it too specific.

388
00:30:21,960 --> 00:30:24,760
People will concentrate on specifics of the problem.

389
00:30:25,520 --> 00:30:30,720
As I say, it's going to come up with the next pandemic, COVID-19, 2020,

390
00:30:30,720 --> 00:30:34,520
so then everyone's going to talk about vaccines and boosters,

391
00:30:34,520 --> 00:30:37,840
completely ignoring the problem of superintelligence being out of control.

392
00:30:38,840 --> 00:30:40,080
A fair point, right.

393
00:30:40,080 --> 00:30:43,480
We can always plug any specific hole,

394
00:30:43,480 --> 00:30:46,280
but the problem is that there are just too many holes and...

395
00:30:46,280 --> 00:30:48,920
So that's exactly what we're going to emphasize.

396
00:30:48,920 --> 00:30:51,120
And again, it goes with unpredictability.

397
00:30:51,160 --> 00:30:53,760
If I could predict any states of that future,

398
00:30:53,760 --> 00:30:56,720
I would be violating my own arguments in the book.

399
00:30:56,720 --> 00:31:01,120
What then is the purpose of the field of AI safety?

400
00:31:01,120 --> 00:31:05,320
So one is to first understand what can be done in this field.

401
00:31:05,320 --> 00:31:08,760
We all intuitively understand we want to create systems.

402
00:31:08,760 --> 00:31:11,360
We do not regret creating formally.

403
00:31:11,360 --> 00:31:15,120
That means we remain in control to some degree.

404
00:31:15,120 --> 00:31:17,320
We can undo things we don't like.

405
00:31:17,400 --> 00:31:21,600
And we generally all like and benefit from what we experience,

406
00:31:21,600 --> 00:31:26,400
and it's not because we are somehow gamed, tricked, altered,

407
00:31:26,400 --> 00:31:28,720
modified in some bizarre ways.

408
00:31:28,720 --> 00:31:32,440
Part of it is people just trying to move as quickly as they can

409
00:31:32,440 --> 00:31:35,960
to understand how to make those systems not do certain things.

410
00:31:35,960 --> 00:31:37,640
Why work is more theoretical?

411
00:31:37,640 --> 00:31:40,080
It's trying to understand, well, what are the upper limits

412
00:31:40,080 --> 00:31:41,800
and what can be done in this field?

413
00:31:41,800 --> 00:31:47,240
And surprisingly, I'm discovering that there are very strong upper limits

414
00:31:47,240 --> 00:31:50,480
we cannot control truly advanced AI.

415
00:31:50,480 --> 00:31:55,960
And then I survey experts, AI, people in different conferences

416
00:31:55,960 --> 00:32:00,200
or social media, majority actually agrees with me.

417
00:32:00,200 --> 00:32:02,920
They don't think it's possible to control superintelligence

418
00:32:02,920 --> 00:32:04,000
indefinitely.

419
00:32:04,000 --> 00:32:07,600
Now, there is not a proportionate percentage of people

420
00:32:07,600 --> 00:32:09,040
working directly on that.

421
00:32:09,040 --> 00:32:11,360
There is not enough publications, books.

422
00:32:11,360 --> 00:32:15,440
Even that possibility is still a very out of

423
00:32:15,480 --> 00:32:17,320
overton window possibility.

424
00:32:17,320 --> 00:32:18,600
But I think it's changing.

425
00:32:18,600 --> 00:32:20,960
I have multiple papers on it, peer-reviewed journal,

426
00:32:20,960 --> 00:32:22,800
papers, conferences, a book.

427
00:32:22,800 --> 00:32:24,880
We had an article in Time Magazine.

428
00:32:24,880 --> 00:32:28,960
I'm not sure what else I can do to make it a mainstream belief

429
00:32:28,960 --> 00:32:30,160
within this community.

430
00:32:30,160 --> 00:32:33,640
But one thing is for sure, no one is arguing the opposite.

431
00:32:33,640 --> 00:32:37,440
No one is saying, oh, we have control mechanism

432
00:32:37,440 --> 00:32:39,480
that will scale to any level of intelligence.

433
00:32:39,480 --> 00:32:42,000
You want to see the code, you want to see even the prototype,

434
00:32:42,000 --> 00:32:43,000
even an idea.

435
00:32:43,000 --> 00:32:44,840
No one is making that claim.

436
00:32:44,840 --> 00:32:46,280
Right, no one is claiming that.

437
00:32:46,280 --> 00:32:48,400
There are plenty of people claiming

438
00:32:48,400 --> 00:32:51,800
that the problem won't exist and arguing, for instance,

439
00:32:51,800 --> 00:32:54,160
that as AI becomes more intelligent,

440
00:32:54,160 --> 00:32:57,560
it will become more compassionate and ethical.

441
00:32:57,560 --> 00:32:59,720
And of course, there's no proof of that.

442
00:32:59,720 --> 00:33:03,640
And it only takes one that isn't to upset the apple cart.

443
00:33:04,080 --> 00:33:07,080
That's the end of the first half of the interview.

444
00:33:07,080 --> 00:33:10,080
I think it's really interesting how we're having

445
00:33:10,080 --> 00:33:13,800
this level of discussion about the goals and ethics

446
00:33:13,800 --> 00:33:17,000
and psychology of AI at such a high level,

447
00:33:17,000 --> 00:33:21,040
where up to two years ago, when we were having similar discussions,

448
00:33:21,040 --> 00:33:23,360
as you could hear me talking with Roman on this podcast

449
00:33:23,360 --> 00:33:26,480
four years ago, that is a very different story.

450
00:33:26,480 --> 00:33:28,480
And I think that's a very different story

451
00:33:28,480 --> 00:33:30,040
than I thought it would be.

452
00:33:30,040 --> 00:33:33,160
And talking with Roman on this podcast four years ago,

453
00:33:33,160 --> 00:33:37,400
that seemed to most people a very anthropomorphizing,

454
00:33:37,400 --> 00:33:41,240
grandiose and expansive point of view that was,

455
00:33:41,240 --> 00:33:45,080
if anything, detrimental to the funding environment for AI.

456
00:33:45,080 --> 00:33:47,280
I had people tell me exactly that.

457
00:33:47,280 --> 00:33:50,360
And yet now, it's become a serious discussion.

458
00:33:50,360 --> 00:33:52,360
And I've talked to politicians about this

459
00:33:52,360 --> 00:33:55,240
and they take it very seriously too.

460
00:33:55,240 --> 00:33:57,800
I'd like to mark a milestone here.

461
00:33:57,800 --> 00:34:00,680
We have just crossed a quarter of a million downloads

462
00:34:00,680 --> 00:34:02,880
of episodes of this podcast.

463
00:34:02,880 --> 00:34:05,080
And I'm pretty sure that they're not all my mother.

464
00:34:05,080 --> 00:34:09,640
So thank you, everyone, for giving us such awesome stats

465
00:34:09,640 --> 00:34:11,680
and please give us a five-star rating

466
00:34:11,680 --> 00:34:14,080
on your podcast platform of choice

467
00:34:14,080 --> 00:34:17,600
and we'll get the next quarter million downloads even faster.

468
00:34:18,480 --> 00:34:21,400
In today's news, ripped from the headlines about AI,

469
00:34:21,400 --> 00:34:24,360
the commercial deployment of robo-taxis

470
00:34:24,360 --> 00:34:26,560
continues its tailspin.

471
00:34:26,560 --> 00:34:29,200
Last November, General Motors recalled

472
00:34:29,200 --> 00:34:33,560
950 driverless cruise cars in San Francisco,

473
00:34:33,560 --> 00:34:36,360
where you'll remember that a number of cruise cars

474
00:34:36,360 --> 00:34:39,360
are deployed as autonomous taxis.

475
00:34:39,360 --> 00:34:41,640
A hit-and-run driver struck a pedestrian

476
00:34:41,640 --> 00:34:43,480
who was thrown into another lane

477
00:34:43,480 --> 00:34:47,200
where a cruise robo-taxi was unable to stop in time

478
00:34:47,200 --> 00:34:50,400
but then ended up dragging the pedestrian.

479
00:34:50,400 --> 00:34:52,160
The recall addresses circumstances

480
00:34:52,160 --> 00:34:55,000
when the software may cause the cruise AV

481
00:34:55,000 --> 00:34:56,920
to attempt to pull over out of traffic

482
00:34:56,920 --> 00:34:59,000
instead of remaining stationary,

483
00:34:59,000 --> 00:35:01,560
quote, when a pullover is not the desired

484
00:35:01,560 --> 00:35:05,360
post-collision response, unquote said cruise.

485
00:35:05,360 --> 00:35:08,240
GM has halted cruise operations nationwide

486
00:35:08,240 --> 00:35:10,800
and has also halted the production line.

487
00:35:10,800 --> 00:35:14,400
And now the cruise internal share price

488
00:35:14,400 --> 00:35:17,520
has been cut by more than half since then.

489
00:35:17,520 --> 00:35:20,520
Cruise division executives told some engineering

490
00:35:20,520 --> 00:35:23,760
and operation staff in internal meetings in recent weeks

491
00:35:23,760 --> 00:35:26,080
that they should not expect to see its robo-taxis

492
00:35:26,080 --> 00:35:29,200
on city streets again until the fourth quarter.

493
00:35:29,200 --> 00:35:32,000
Next week, we will conclude the interview with Roman

494
00:35:32,000 --> 00:35:34,480
when we'll talk about how we should respond

495
00:35:34,480 --> 00:35:37,040
to this problem of unsafe AI development

496
00:35:37,040 --> 00:35:40,160
and how Roman and his community are addressing it,

497
00:35:40,160 --> 00:35:42,800
what he would do with infinite resources

498
00:35:42,800 --> 00:35:46,720
and the threat Roman's coffee cup poses to humanity.

499
00:35:46,720 --> 00:35:49,320
That's next week on AI and You.

500
00:35:49,320 --> 00:35:51,280
Until then, remember,

501
00:35:51,280 --> 00:35:53,960
no matter how much computers learn how to do,

502
00:35:53,960 --> 00:35:57,080
it's how we come together as humans that matters.

503
00:35:58,800 --> 00:36:02,240
That's all for this episode of AI and You.

504
00:36:02,240 --> 00:36:03,760
Please leave a rating and comment

505
00:36:03,760 --> 00:36:05,440
and share with your friends.

506
00:36:05,440 --> 00:36:08,240
Get the book, Artificial Intelligence and You,

507
00:36:08,240 --> 00:36:13,000
and see more videos and articles at AIandYou.net.

508
00:36:13,000 --> 00:36:18,000
That's A-I-A-N-D-Y-O-U.net,

509
00:36:18,840 --> 00:36:21,160
where you can also send us your questions.

510
00:36:21,160 --> 00:36:22,360
Thank you for listening.

