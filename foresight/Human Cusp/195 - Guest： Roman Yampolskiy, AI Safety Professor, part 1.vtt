WEBVTT

00:00.000 --> 00:11.800
Artificial Intelligence will completely transform our world, but what is AI?

00:11.800 --> 00:13.800
Why will it affect you?

00:13.800 --> 00:21.360
And how can you and your business survive and thrive through the AI revolution?

00:21.360 --> 00:24.680
Welcome to AI and You.

00:25.320 --> 00:30.880
Here is your host, author, speaker, and futurist Peter Scott.

00:32.880 --> 00:36.720
Hello and welcome to Episode 195.

00:36.720 --> 00:44.720
I am chuffed to welcome back to the show Roman Jampolski, our first three-peat guest.

00:44.720 --> 00:52.840
Roman was on the show in Episodes 16 and 17 and also nearly a year ago in Episodes 160 and 161.

00:52.840 --> 00:57.920
He is here today because he has a new book that has just hit the shelves called AI,

00:57.920 --> 01:03.920
unexplainable, unpredictable, uncontrollable, which gives you, right there,

01:03.920 --> 01:08.120
a good idea of what Roman's field is, AI safety.

01:08.120 --> 01:12.880
A term he has done more than anyone else to promote and inhabit.

01:12.880 --> 01:18.560
Roman is tenured associate professor in the Department of Computer Engineering and Computer Science

01:18.560 --> 01:23.400
at the Speed School of Engineering, University of Louisville in Kentucky.

01:23.400 --> 01:26.640
He is the founding and current director of the Cyber Security Lab

01:26.640 --> 01:31.440
and has written many key books in the field such as Artificial Superintelligence,

01:31.440 --> 01:37.160
a futuristic approach, and Artificial Intelligence, Safety and Security.

01:37.160 --> 01:41.040
He's been central in the field of warning about the control problem

01:41.040 --> 01:45.320
and the value alignment problem of AI from the very beginning.

01:45.320 --> 01:49.680
Back when doing so earned people some scorn from practitioners,

01:49.680 --> 01:57.520
yet Roman is a professor of computer science and applies rigorous methods to his analyses of these problems.

01:57.520 --> 02:01.600
It's those rigorous methods that we want to tap into in this interview

02:01.600 --> 02:08.480
because Roman connects principles of computer science with the issue of existential risk from AI,

02:08.480 --> 02:12.120
which is an area, obviously of ultimate importance to us all,

02:12.120 --> 02:18.160
but one which has a lot of opinion and argument behind it and could use some more science.

02:18.160 --> 02:22.280
Speaking of which, we're going to refer to the halting problem in this interview

02:22.280 --> 02:29.200
and because it's a problem in computer science that was famously solved by Alan Turing in 1936

02:29.200 --> 02:31.880
with an elegant proof, here's the proof.

02:31.880 --> 02:38.240
The halting problem asks whether there could be a computer program or algorithm

02:38.240 --> 02:44.760
that would be capable of telling whether any computer program you fed it would eventually stop running

02:44.760 --> 02:49.760
or whether it would run forever given any specific input.

02:49.760 --> 02:54.440
Turing showed that it was impossible for such a program to exist.

02:54.440 --> 02:56.360
Here's how he did it.

02:56.360 --> 03:00.280
Suppose that such a program does exist and it returns true

03:00.280 --> 03:04.880
if the program and input that you give it will halt false otherwise.

03:04.880 --> 03:08.240
Now we make a new algorithm with the following definition.

03:08.240 --> 03:16.720
It takes a program as its input, then calls the halting analysis program with that program as both program and input

03:16.720 --> 03:22.000
and if that program returns true then it goes into an infinite loop, otherwise it halts.

03:22.000 --> 03:27.920
Now we take that algorithm and call it with itself as input, which means

03:27.920 --> 03:33.400
if the halting analysis program returns true it means the algorithm we just made halts when given itself as input

03:33.400 --> 03:38.760
but we just said that the definition of that algorithm is that in that case it will loop forever.

03:38.760 --> 03:42.080
That's a contradiction. We went wrong somewhere.

03:42.080 --> 03:47.320
There's an equal contradiction if the halting analysis program returns false.

03:47.320 --> 03:54.880
So our initial assumption, the only one we made that such a program could exist, must be false.

03:54.880 --> 03:59.920
This is a powerful proof, somewhat similar to Goedl's incompleteness theorem

03:59.960 --> 04:03.360
and logical statements like this statement is unprovable.

04:03.360 --> 04:11.320
An analogical proof shows that there can be no algorithm that could compress all possible input data sets without loss

04:11.320 --> 04:18.120
because otherwise you could iteratively reduce any data set to an arbitrarily small size which is impossible.

04:18.120 --> 04:24.720
In other words, trying to gzip your jpeg files probably isn't going to make them any smaller.

04:24.720 --> 04:31.320
So we're going to talk about safety of AI where many people seem to suffer from this sort of collective blind spot.

04:31.320 --> 04:38.360
A kind of group think that the onus is on the people calling for pauses in its development to show that it's not safe.

04:38.360 --> 04:46.080
They say things like this could turn out very well for us as though those were things that were final arguments against the pausing.

04:46.080 --> 04:47.920
This is not how safety works.

04:47.920 --> 04:50.480
Imagine you went to get on a bus and the driver said,

04:50.480 --> 04:55.800
I can imagine ways in which this bus could reach its destination totally unscathed.

04:55.800 --> 05:00.480
Our job as engineers is to prove that something is safe.

05:00.480 --> 05:04.960
The absence of proof that something is dangerous is not the same thing.

05:04.960 --> 05:10.120
It's a fair question to ask, how is superintelligence supposed to be able to defeat us?

05:10.120 --> 05:16.000
And people like Roman don't get into that, which looks like a cop-out, but the problem is that when you ask that,

05:16.000 --> 05:24.960
if you get any specific answer like it could decide that humanity is a threat to its purpose and take over biosynthesis labs to produce and release deadly pathogens,

05:24.960 --> 05:33.320
well, deadly to us, we can say, OK, well, then we will ensure that all the biosynthesis labs are disconnected from the network.

05:33.320 --> 05:36.840
And so forth, we could do that for any scenario that we could imagine.

05:36.840 --> 05:43.800
But the problem is that there are many, many more scenarios than we can imagine, effectively an infinite number.

05:43.840 --> 05:48.800
And a superintelligent AI would be very good at finding the ones we hadn't thought about.

05:48.800 --> 05:55.240
To the same kind of fat tail problem I've talked about with respect to why self-driving cars are still not ready for prime time.

05:55.240 --> 05:57.440
More on that later.

05:57.440 --> 06:01.920
Anyway, enough of me. Let's get into the interview.

06:01.920 --> 06:07.480
Well, Roman Jampolsky, welcome so much back to artificial intelligence and you.

06:07.480 --> 06:15.680
We're here to talk about your new book, AI, unexplainable, unpredictable, uncontrollable.

06:15.680 --> 06:21.760
And that is a set of daunting claims.

06:21.760 --> 06:34.120
Before we get into that, I want listeners to get some feel for your background because that may play into what's brought you to the point of writing this book.

06:34.120 --> 06:40.440
And so could you give us the encapsulation of your life from birth to Louisville?

06:40.440 --> 06:42.800
From birth? Wow, that's ambitious.

06:42.800 --> 06:44.360
Thanks for having me back.

06:44.360 --> 06:46.480
I'm a computer scientist at E01.

06:46.480 --> 06:49.240
I'm a PhD in computer science and engineering.

06:49.240 --> 06:54.320
I've been working on different sub-domains of AI safety for over a decade.

06:54.320 --> 07:01.360
I've published a couple of hundred billion good papers, a few books, mostly on AI safety.

07:01.360 --> 07:19.200
In my full-time area of research in the last couple years for sure, my background is, I guess, initially I was raised in Soviet Union, came to US in 94, lived in New York for about 15 years, now I've been in Kentucky for about as much.

07:19.200 --> 07:27.000
And this book gets into some serious territory that impacts a great many people.

07:27.080 --> 07:40.640
I mean, it's inhabiting this rather narrow niche in a way of being a book about computer science written by a computer scientist containing computer science language.

07:40.640 --> 07:51.800
And yet it's talking about something that has fundamental impact on the entire human race and is aimed at a much wider audience than computer scientists.

07:51.800 --> 07:54.440
So could you tell us why you wrote this book?

07:54.440 --> 07:55.600
Well, that's my research.

07:55.600 --> 07:56.640
That's what I'm doing.

07:56.640 --> 08:01.280
I think it's the most important problem ever in any discipline.

08:01.280 --> 08:11.760
We're trying to understand if the science and engineering we're doing right now, creating intelligent assistants, helpers is a long-term beneficial strategy.

08:11.760 --> 08:20.760
If there is any chance that will be too successful, create something too intelligent, too independent, and that will backfire tremendously.

08:20.760 --> 08:31.560
And it's actually shocking and surprising to me that even with a tiny minority of people in AI doing a safety work within that community,

08:31.560 --> 08:38.480
there is really no one outside of me doing work on fundamentals of what is possible in this area.

08:38.480 --> 08:49.440
There is a few informal, independent scholars publishing a few blog posts, but you would expect this problem to be as popular as NP versus PEEP problem.

08:49.440 --> 08:51.840
Can you control super 1000 machines yet?

08:51.840 --> 08:52.920
There is nothing.

08:52.920 --> 09:03.600
And you mentioned their P versus NP classic trope in computer science about the scale of computability of problems.

09:03.600 --> 09:17.160
Now, you talk about the control problem and the alignment problem in the book in ways that embody what appears to be mathematical proof.

09:17.200 --> 09:22.240
It reminds me very much of the halting problem in computer science.

09:22.240 --> 09:35.200
The question is it possible to write a program that when you feed it the source code of some other program can tell you whether that program would ever finish if it were to be run.

09:35.200 --> 09:44.240
And there is a simple and elegant proof that no program can be written that can do that for every possible program that you can give to it.

09:44.800 --> 09:46.720
Your work appears reminiscent of that.

09:46.720 --> 09:56.360
Would you put it in the same category as that level of mathematical proof that you have created with respect to controllability?

09:56.360 --> 09:57.280
Not exactly.

09:57.280 --> 10:00.440
So there is different levels of control we can talk about.

10:00.440 --> 10:04.680
In a chapter, I suggest at least four different levels of control.

10:04.680 --> 10:18.920
For what I call direct control, yes, we can come up with a very mathematically rigorous proof similar to what we have seen with mathematical proofs where we show that it creates a contradiction that it's impossible.

10:18.920 --> 10:24.560
But there are, of course, other levels of control, delegated control, ideal advisor.

10:24.560 --> 10:27.800
And for that, we don't have a strong mathematical proof.

10:27.800 --> 10:33.520
But what we do have is a survey of every relevant field to this problem.

10:33.520 --> 10:38.240
So if you think, OK, we need to accomplish this, this is the tool set, I need to be able to do it.

10:38.240 --> 10:40.680
I need to be able to aggregate votes.

10:40.680 --> 10:47.080
I need to be able to elicit preferences, mathematics, economics, any field you want.

10:47.080 --> 10:54.880
If you survey those fields, they are very well-established proven impossibility results which say, well, you cannot get all those tools.

10:54.880 --> 10:58.320
You can get approximate partial results for many of them.

10:58.320 --> 11:03.200
But if you really need the exact result, you're not going to get there.

11:03.200 --> 11:09.520
And it's not me saying it, it's top experts in all these fields saying, that's impossible, you cannot do it.

11:09.520 --> 11:13.120
Now, one of the elements of your title is unpredictability.

11:13.120 --> 11:21.320
And is that something where you have also arrived at something amounting to a proof that AI is not predictable?

11:21.320 --> 11:24.200
Yes, that one is actually a good solid proof.

11:24.200 --> 11:26.600
It's, again, self-referential all the way.

11:26.600 --> 11:33.320
If we make this assumption that we're creating superintelligence, a system smarter than any one of us in all domains,

11:33.320 --> 11:39.800
then if we could accurately predict what decisions the system will make, we would be equally smart.

11:39.800 --> 11:47.200
Let's say we're playing a game of chess, if I can predict every move my opponent is going to make, I'm at least as good of a chess player.

11:47.200 --> 11:49.480
But that's a violation of our assumption.

11:49.480 --> 11:54.400
We have proof by contradiction that, no, of course, we cannot make those predictions.

11:54.440 --> 12:01.800
We can maybe predict general direction the agent is trying to take us, like that system is trying to win a game of chess.

12:01.800 --> 12:05.240
But we cannot predict specific moves it's going to make to get there.

12:05.240 --> 12:10.520
And this leads us on a direction that's novel with respect to computers,

12:10.520 --> 12:16.560
because the entire field of computer science is founded on predictability.

12:16.560 --> 12:21.040
You cannot, and certainly the way I was trained as a computer scientist,

12:21.080 --> 12:28.160
you are aiming for 100% predictability, otherwise you don't know what you're doing, literally.

12:28.160 --> 12:37.280
And here you have a proof that we're heading in direction of creating things that are provably unpredictable.

12:37.280 --> 12:43.560
Does that not rock the foundations of the science that AI is founded on?

12:43.560 --> 12:48.960
Well, interestingly, I think for the longest time, AI was not science, it was engineering.

12:49.000 --> 12:54.280
We use software engineering, we design specific engineering artifacts for where you design a bridge.

12:54.280 --> 12:58.240
We had specifications, we met them, we knew how the system works.

12:58.240 --> 13:04.040
In about the last decade or so, we switched from where we explicitly programmed their systems,

13:04.040 --> 13:06.800
we kind of teach them to learn independently.

13:06.800 --> 13:10.480
We give them data, we give them computing power, and we say, go figure it out,

13:10.480 --> 13:16.880
and then we'll study to see how good they are at that domain or maybe their general systems.

13:16.880 --> 13:21.160
But it's more like science now where we're on experiments and those artifacts,

13:21.160 --> 13:26.320
and we discover what they are after the fact, not at the time we design them.

13:26.320 --> 13:32.880
So that's quite a paradigm shift, but I think it's never been science before.

13:32.880 --> 13:38.920
To hop around some of the elements of your title here, the third one is unexplainable.

13:38.920 --> 13:45.120
And there is a huge amount of effort going into making AI explainable right now.

13:45.120 --> 13:55.280
And I think this is worth dissecting because some of that work is succeeding and will provide value,

13:55.280 --> 14:00.680
but it's not necessarily the kind of explainability that I think you're talking about.

14:00.680 --> 14:06.240
In this sense that I mean this, we've had people on the show from IBM who talk about

14:06.240 --> 14:12.480
rather ingenious ways that they get AI to explain how it came up with a decision

14:12.480 --> 14:15.840
with respect to someone's insurability, for instance.

14:15.840 --> 14:21.400
That could be useful, could be fit for a purpose in the insurance industry.

14:21.400 --> 14:27.840
But you're going for something more deeper, broader, can you explain?

14:27.840 --> 14:29.520
Pan intended that to us.

14:29.520 --> 14:31.960
So there are certainly things we can explain.

14:31.960 --> 14:34.360
No one argues that everything is unexplainable.

14:34.360 --> 14:36.800
You can explain trivial decisions.

14:36.800 --> 14:42.440
If you're playing a game of tic-tac-toe, you can certainly tell me why you moved to a specific square.

14:42.480 --> 14:45.080
I'm talking about the extreme on the other end.

14:45.080 --> 14:52.760
There was complex systems, systems making decisions based on billions of nodes in the neural network,

14:52.760 --> 14:54.320
trillion of weights.

14:54.320 --> 14:58.680
The system looks at the totality of those to make its decision.

14:58.680 --> 15:01.160
It's working in a novel domain.

15:01.160 --> 15:08.520
So far, we sometimes succeeded understanding one node or maybe a small subset of those nodes.

15:08.520 --> 15:11.720
Okay, they fire, this input is produced.

15:11.720 --> 15:13.280
That's what stimulates them.

15:13.280 --> 15:15.280
That doesn't give us complete picture.

15:15.280 --> 15:16.720
We really have two options.

15:16.720 --> 15:21.600
We can either get some sort of a simplified, reduced explanation.

15:21.600 --> 15:26.040
Top 10 reasons this decision was made, kind of like glossy compression.

15:26.040 --> 15:27.720
And it's useful.

15:27.720 --> 15:28.640
It's beneficial.

15:28.640 --> 15:29.960
It's better than nothing.

15:29.960 --> 15:31.840
But it doesn't give you complete picture.

15:31.840 --> 15:34.880
You can still hide information in such explanations.

15:34.880 --> 15:36.440
That's what we do with children.

15:36.440 --> 15:37.840
We don't give them complete picture.

15:37.840 --> 15:41.480
We give them just so explanation, just so stories.

15:41.480 --> 15:44.120
The opposite is a complete picture.

15:44.120 --> 15:47.240
We will give you a full explanation for how the decision is made.

15:47.240 --> 15:49.080
But that is the whole model.

15:49.080 --> 15:51.840
That's the whole AI system with all of its weights.

15:51.840 --> 15:54.160
That's not something a human brain can survey.

15:54.160 --> 15:55.480
It's too large.

15:55.480 --> 15:57.560
It's not human readable.

15:57.560 --> 16:01.000
Simply giving you access to all the weights will explain nothing to you.

16:01.000 --> 16:06.400
So either we have problem with comprehending explanation given to us,

16:06.440 --> 16:11.360
or we're given a simplified, not full picture explanation,

16:11.360 --> 16:13.720
which does not guarantee safety.

16:13.720 --> 16:19.440
So there you're leading us to why unexplainability is important.

16:19.440 --> 16:23.240
And I will get into that in a moment.

16:23.240 --> 16:29.840
First, I want to say you are a neural network with billions of nodes.

16:29.840 --> 16:36.160
If I ask you to explain some decision, I will get some useful answer out.

16:36.160 --> 16:41.440
How does this differ from what we want out of a neural network, an artificial one?

16:41.440 --> 16:43.800
It really depends on what you ask me to explain.

16:43.800 --> 16:47.960
If you ask me to explain how I decide what podcasts to go on,

16:47.960 --> 16:50.280
I can probably explain that pretty well.

16:50.280 --> 16:54.440
If you ask me how I recognize your face, I will give you a BS story,

16:54.440 --> 16:56.360
which has nothing to do with reality.

16:56.360 --> 16:57.440
Right.

16:57.440 --> 17:01.920
And then we are using neural networks at the moment, artificial ones.

17:01.920 --> 17:05.840
Again, to do things like recognize faces.

17:05.840 --> 17:12.600
And so expecting an answer of how they came up with that is perhaps futile,

17:12.600 --> 17:15.960
because we couldn't relate it to anything that we understand,

17:15.960 --> 17:18.000
since we don't know how we do that either.

17:18.000 --> 17:23.240
And even the rather clever attempts to do that have often highlighted things in the image

17:23.240 --> 17:26.400
that have nothing to do with the face, which is alarming.

17:26.400 --> 17:30.440
But yet they work anyway, which is even more alarming.

17:30.480 --> 17:39.200
Does safety have more to do with questions that we can't expect an explanation for?

17:39.200 --> 17:41.400
Like, how did you recognize this face?

17:41.400 --> 17:46.960
Why did you choose to draw that image when I asked for a picture of a poodle riding a motorcycle?

17:46.960 --> 17:56.440
Or is it more impacted by questions where we would expect a human comprehensible explanation

17:56.440 --> 18:01.480
of a decision like why are you suggesting that we launch our strategic defense missiles?

18:01.480 --> 18:08.480
Well, for us to fully confirm that the system is safe and operational as we expect it to be,

18:08.480 --> 18:10.640
we need to understand how it works.

18:10.640 --> 18:17.040
It doesn't seem like we're designing it, so we have to deal with this artifact produced by this

18:17.040 --> 18:20.000
process of self-learning, self-modification.

18:20.000 --> 18:25.440
And if we can't comprehend how it works, it doesn't give us any reason to think it's safe.

18:25.440 --> 18:28.000
Asking how did you make this decision?

18:28.000 --> 18:32.600
Is it useful in part to decide should we accept this decision?

18:32.600 --> 18:37.280
If we are accepting it based on previous results, it's always been right so far.

18:37.280 --> 18:42.160
Should we just take it as a religious statement that it's going to be right in the future?

18:42.160 --> 18:46.760
It's now an oracle-type god-like system that seems to be dangerous.

18:46.760 --> 18:50.960
You can have a stretch-restaurant situation where it gives you 10 good answers,

18:50.960 --> 18:55.080
gets you trust, and then does something against your preferences.

18:55.080 --> 18:59.480
So I think, as I said, if you look at the totality of a problem of control,

18:59.480 --> 19:00.640
you have those tools.

19:00.640 --> 19:05.160
You need to be able to understand how it works, predict what it's going to do,

19:05.160 --> 19:07.560
be able to communicate with it without ambiguity.

19:07.560 --> 19:12.080
Say, we have a paper with about 50 different relevant results,

19:12.080 --> 19:14.520
and it doesn't look like we have those.

19:14.520 --> 19:18.600
So it's very hard for me to understand why people argue that,

19:18.600 --> 19:19.880
oh, certainly we can do it.

19:19.880 --> 19:20.520
It's easy.

19:20.520 --> 19:23.240
We just need to have more money for this project.

19:23.280 --> 19:25.520
So it's not obvious.

19:25.520 --> 19:31.520
And I think for humans, we want explanations for at least two reasons.

19:31.520 --> 19:35.280
One would be, I want to know how to do better in the future.

19:35.280 --> 19:42.720
Like, tell me why you denied my college application so my next one can be accepted.

19:42.720 --> 19:45.720
And then there's the, I don't trust you.

19:45.720 --> 19:49.200
If you give me an explanation of why you made that decision,

19:49.200 --> 19:55.960
maybe I can find the floor in your logic or where you're not working properly.

19:55.960 --> 20:04.840
So does explainability for AI have more for you personally to do with the verifiability problem?

20:04.840 --> 20:07.480
Of course, and that's another impossibility result.

20:07.480 --> 20:12.240
There is strong reason to think we cannot perfectly verify software mathematical proofs.

20:12.240 --> 20:17.000
We can get it to a certain degree of accuracy proportionate to the resources we allocate.

20:17.000 --> 20:21.240
But we never get 100% guarantee that it's always a possibility,

20:21.240 --> 20:23.920
one in a billion chance it will make our own decision.

20:23.920 --> 20:27.960
But if a system continues making billions of decisions every minute,

20:27.960 --> 20:30.480
you basically guarantee that it's going to fail pretty soon.

20:30.480 --> 20:35.320
So we're talking about software here that as it becomes more intelligent,

20:35.320 --> 20:41.560
more AGI-like, becomes closer to the characteristics of human beings

20:41.560 --> 20:47.600
and then of course exceeding them in the dimensions of general intelligence.

20:47.600 --> 20:56.960
Would you say that humans are susceptible to the kind of algorithmic analyses that you're making of AI?

20:56.960 --> 21:01.760
Humans also have unexplainable, unpredictable and uncontrollable, certainly.

21:01.760 --> 21:04.920
Yeah, in fact, they're reduced to a human control problem.

21:04.960 --> 21:08.680
If you can't get humans to be safe and controlled,

21:08.680 --> 21:14.640
despite years of attempts, you know, millennia of ethics, morals, religion,

21:14.640 --> 21:18.760
cultural indoctrination, public schooling, you name it.

21:18.760 --> 21:22.080
I mean, we have laws, we have all this and at the end of the day,

21:22.080 --> 21:25.640
you still have a situation where employees betray the company,

21:25.640 --> 21:28.800
citizens betray the country, spouses betray each other.

21:28.800 --> 21:32.720
There is no reliability, there is no guarantee with lie detector tests,

21:32.720 --> 21:35.920
with financial incentives, so none of it works.

21:35.920 --> 21:41.800
So if we can't fully guarantee safety and controllability of the human neural network,

21:41.800 --> 21:46.680
why would we think that scaling it to an extremely large equivalent

21:46.680 --> 21:49.280
would make it somehow more likely to happen?

21:49.280 --> 21:54.640
Right, then I'm trying to come up with a freezing here

21:54.640 --> 21:57.520
because this is a relatively recent thought for me.

21:57.640 --> 22:03.200
When we're talking about AI from the computer science analysis

22:03.200 --> 22:10.760
of it being a set of algorithms and conducting a number of proofs

22:10.760 --> 22:15.400
that resemble the proof of the halting problem, for instance,

22:15.400 --> 22:20.640
at some point, does that approach no longer become valid

22:20.640 --> 22:26.920
if we're talking about something that is as impenetrable as human beings

22:26.960 --> 22:34.320
for whom we would not attempt to prove the halting problem of our brains, I imagine.

22:34.320 --> 22:42.000
Would you consider that the human brain is operating on algorithms,

22:42.000 --> 22:46.560
just ones that we don't know, that could be analyzed

22:46.560 --> 22:49.960
if we had the capability, the technology,

22:49.960 --> 22:56.120
to the level of detail that we know for AI?

22:56.160 --> 22:58.680
So human brain is particularly interesting

22:58.680 --> 23:04.200
because there is not even full agreement on what capabilities

23:04.200 --> 23:10.240
are a direct result of brain architecture and which may be something else.

23:10.240 --> 23:13.200
So then we talk about hard problem of consciousness.

23:13.200 --> 23:16.560
We don't seem to find guaranteed correlates of consciousness.

23:16.560 --> 23:20.080
We don't know why some states of matter would generate those,

23:20.080 --> 23:22.280
the most interesting, the most important states.

23:22.280 --> 23:25.320
We have no idea how to make a computer feel pain.

23:25.320 --> 23:29.680
Yes, it's a fundamental part of being a human, probably an animal.

23:29.680 --> 23:34.520
So we have very limited understanding of human brain,

23:34.520 --> 23:38.400
but because we don't have direct access to a lot of source code,

23:38.400 --> 23:41.840
a lot of what makes a brain tick,

23:41.840 --> 23:47.680
there could be differences which make understanding artificial neural networks easier.

23:47.680 --> 23:50.120
We don't think they have any magical properties.

23:50.120 --> 23:54.480
We certainly know there is no immortal soul or anything like that in them.

23:54.480 --> 23:58.600
At the same time, the size and scale make it so much harder for human brains

23:58.600 --> 24:02.320
to fully comprehend, fully survey that architecture.

24:02.320 --> 24:08.680
So I think there is enough differences to make any analogy not a perfect analogy.

24:08.680 --> 24:11.160
Let me turn it around then.

24:11.160 --> 24:17.240
Is it possible or likely that AI could become so complex

24:17.240 --> 24:23.200
that it will become no longer useful to think of it as a computer program

24:23.240 --> 24:29.400
and apply the analyses that we do of computer algorithms to it anymore

24:29.400 --> 24:33.120
than we could apply those to human beings?

24:33.120 --> 24:37.120
It's certainly possible, but I don't think we're already doing much of that.

24:37.120 --> 24:41.320
We were not directly measuring those systems, I think,

24:41.320 --> 24:44.680
in standard measures of complexity, begonotation.

24:44.680 --> 24:47.320
That's not how we evaluate those systems.

24:47.320 --> 24:52.160
So we look at them and try to create measures for how helpful it is,

24:52.160 --> 24:55.840
for how useful it is, for how well-behaved it is.

24:55.840 --> 25:00.120
Those are kind of similar to psychiatric tools we use with humans.

25:00.120 --> 25:01.560
Okay, that's a psychopath.

25:01.560 --> 25:05.840
This one is kind of well aligned with the values of society,

25:05.840 --> 25:09.760
but those are very fuzzy disciplines, very fuzzy measures.

25:09.760 --> 25:15.200
Do you see a point perhaps where it becomes more useful in the future

25:15.200 --> 25:19.720
to treat AI with the tools of psychology than computer science?

25:19.760 --> 25:24.360
It's definitely useful and it's starting to be, I think, a direction of research

25:24.360 --> 25:30.240
where you're trying to prompt engineer it to do better, act safer.

25:30.240 --> 25:34.520
The problem, of course, is it's a very fuzzy area

25:34.520 --> 25:37.840
and just because you succeeded in one way of prompting,

25:37.840 --> 25:40.520
it doesn't mean that it's not an alternative phrasing

25:40.520 --> 25:46.520
which completely overrides any previous instructions or bypasses any safeguards.

25:47.320 --> 25:49.200
If you say it this way, the system says,

25:49.200 --> 25:51.520
oh, I will not do it, this is very evil action,

25:51.520 --> 25:54.680
but if you put a negative sign in front of one of the words,

25:54.680 --> 25:56.760
somehow it bypasses all the safeguards

25:56.760 --> 25:59.480
and now it's happy to perform that action.

25:59.480 --> 26:06.040
Right, and in some corners of the DSM, Manual for Psychology,

26:06.040 --> 26:11.240
there are disorders where apparently something like a bit getting flipped

26:11.240 --> 26:15.720
in the human brain causes pathologies that we can be concerned of.

26:15.760 --> 26:19.120
So I think there may be some parallels there

26:19.120 --> 26:25.640
that at some point in the future might get exercised to degrees we're not anticipating.

26:25.640 --> 26:27.880
Now, we're talking about safety.

26:27.880 --> 26:30.840
You've mentioned safety several times here

26:30.840 --> 26:36.200
and what's on everyone's mind is how unsafe is this?

26:36.200 --> 26:40.960
Is this a theoretical exercise for comparison, for instance?

26:40.960 --> 26:44.720
I could say, look, one day the sun will burn up the earth.

26:44.720 --> 26:46.520
This is a physical certainty.

26:46.520 --> 26:52.320
We can prove it as much as we could possibly any conclusion in astrophysics.

26:52.320 --> 26:56.800
And despite the fact that that has existential consequences,

26:56.800 --> 26:59.760
we will have to evacuate the planet.

26:59.760 --> 27:01.920
We are not concerned about that.

27:01.920 --> 27:03.840
We should not be focusing on that now

27:03.840 --> 27:05.920
because it's five billion years in the future.

27:05.920 --> 27:08.800
So that makes it much more theoretical at this point.

27:08.800 --> 27:11.160
We have more important problems.

27:11.160 --> 27:17.600
How would you characterize the urgency of what you are saying with respect to safety?

27:17.600 --> 27:22.200
So I know specifically how soon we're going to get to superintelligence.

27:22.200 --> 27:25.080
It may take longer than most people anticipate.

27:25.080 --> 27:29.120
For reasons we just don't predict, cannot explain.

27:29.120 --> 27:32.600
If you look at what heads of top AI labs are saying,

27:32.600 --> 27:36.080
they're claiming they're two to three years away from AGI

27:36.080 --> 27:39.600
or even superintelligence, depending on how you define it.

27:39.600 --> 27:43.440
AGI with access to internet and large computers is already superintelligence.

27:43.440 --> 27:47.760
So it could be as little as two years and we keep adding more funding

27:47.760 --> 27:52.360
and then making it go faster, accelerating because that's just too slow.

27:52.360 --> 27:56.560
So I think it's a little more pressing than the sun problem.

27:56.560 --> 27:58.760
Right. I mean, I'm just trying to put some bounds on it.

27:58.760 --> 28:01.720
I think it is much more pressing than five billion years.

28:01.720 --> 28:07.560
And when you put the possibility of a two to three year time frame on it,

28:07.600 --> 28:12.720
if we get the advances in AI that those people are talking about,

28:12.720 --> 28:16.640
artificial general intelligence, which is famously vague,

28:16.640 --> 28:17.760
we don't really know what that means.

28:17.760 --> 28:23.680
But if we get what some consensus is talking about for that at that point,

28:23.680 --> 28:28.600
what sort of problems do you anticipate us facing as a result?

28:28.600 --> 28:33.120
So you're really asking me what I would do if I was trying to cause problems.

28:33.120 --> 28:34.240
That would be the next question.

28:34.240 --> 28:36.200
A superintelligence system would, of course,

28:36.200 --> 28:39.640
come up with something I can never even consider as a solution.

28:39.640 --> 28:43.160
I'm not superintelligent, despite what you might think.

28:43.160 --> 28:47.640
Yes, so if you want to know how Roman Impolsky would go about taking over the world

28:47.640 --> 28:49.200
is a very separate discussion.

28:49.200 --> 28:54.120
I think those systems can rely on standard approaches humans have considered,

28:54.120 --> 29:00.440
nanobots, synthetic biology, bribing humans on the internet with cryptocurrencies.

29:00.440 --> 29:05.440
But they can also come up with something completely different, which I have no access to.

29:05.480 --> 29:06.480
Right. I understood.

29:06.480 --> 29:15.400
Now, I wasn't asking you to predict what could an uncontrollable AI run amok do in those circumstances.

29:15.400 --> 29:21.240
But in the more general sense, what kind of not specific mayhem,

29:21.240 --> 29:25.120
but what level of problems would we face?

29:25.120 --> 29:27.440
Well, again, it's hard to predict specifics.

29:27.440 --> 29:32.000
People are concerned about worst-case scenarios in computer science and cryptography.

29:32.000 --> 29:34.680
You always look at what's the worst that can happen.

29:34.720 --> 29:39.520
If you prepare for that and you're all good, it's actually something much more mild in good shape.

29:39.520 --> 29:45.160
The worst is existential crisis where it kills everyone or suffering risks

29:45.160 --> 29:47.120
where you wish you were killed with everyone.

29:47.120 --> 29:49.760
One of the things you say in the book is, and this is a quote,

29:49.760 --> 29:55.400
a single failure of a superintelligent system may cause an existential risk event.

29:55.400 --> 30:02.280
And that is something that certainly theoretically on the face of it is almost vacuously true.

30:02.280 --> 30:08.880
Is there a more detail that you would like to put around that to perhaps draw attention

30:08.880 --> 30:15.360
to what it is that you're trying to get more than the few people who are working on this

30:15.360 --> 30:18.360
and most notably yourself to take action?

30:18.360 --> 30:21.960
Well, I think there is certain danger in making it too specific.

30:21.960 --> 30:24.760
People will concentrate on specifics of the problem.

30:25.520 --> 30:30.720
As I say, it's going to come up with the next pandemic, COVID-19, 2020,

30:30.720 --> 30:34.520
so then everyone's going to talk about vaccines and boosters,

30:34.520 --> 30:37.840
completely ignoring the problem of superintelligence being out of control.

30:38.840 --> 30:40.080
A fair point, right.

30:40.080 --> 30:43.480
We can always plug any specific hole,

30:43.480 --> 30:46.280
but the problem is that there are just too many holes and...

30:46.280 --> 30:48.920
So that's exactly what we're going to emphasize.

30:48.920 --> 30:51.120
And again, it goes with unpredictability.

30:51.160 --> 30:53.760
If I could predict any states of that future,

30:53.760 --> 30:56.720
I would be violating my own arguments in the book.

30:56.720 --> 31:01.120
What then is the purpose of the field of AI safety?

31:01.120 --> 31:05.320
So one is to first understand what can be done in this field.

31:05.320 --> 31:08.760
We all intuitively understand we want to create systems.

31:08.760 --> 31:11.360
We do not regret creating formally.

31:11.360 --> 31:15.120
That means we remain in control to some degree.

31:15.120 --> 31:17.320
We can undo things we don't like.

31:17.400 --> 31:21.600
And we generally all like and benefit from what we experience,

31:21.600 --> 31:26.400
and it's not because we are somehow gamed, tricked, altered,

31:26.400 --> 31:28.720
modified in some bizarre ways.

31:28.720 --> 31:32.440
Part of it is people just trying to move as quickly as they can

31:32.440 --> 31:35.960
to understand how to make those systems not do certain things.

31:35.960 --> 31:37.640
Why work is more theoretical?

31:37.640 --> 31:40.080
It's trying to understand, well, what are the upper limits

31:40.080 --> 31:41.800
and what can be done in this field?

31:41.800 --> 31:47.240
And surprisingly, I'm discovering that there are very strong upper limits

31:47.240 --> 31:50.480
we cannot control truly advanced AI.

31:50.480 --> 31:55.960
And then I survey experts, AI, people in different conferences

31:55.960 --> 32:00.200
or social media, majority actually agrees with me.

32:00.200 --> 32:02.920
They don't think it's possible to control superintelligence

32:02.920 --> 32:04.000
indefinitely.

32:04.000 --> 32:07.600
Now, there is not a proportionate percentage of people

32:07.600 --> 32:09.040
working directly on that.

32:09.040 --> 32:11.360
There is not enough publications, books.

32:11.360 --> 32:15.440
Even that possibility is still a very out of

32:15.480 --> 32:17.320
overton window possibility.

32:17.320 --> 32:18.600
But I think it's changing.

32:18.600 --> 32:20.960
I have multiple papers on it, peer-reviewed journal,

32:20.960 --> 32:22.800
papers, conferences, a book.

32:22.800 --> 32:24.880
We had an article in Time Magazine.

32:24.880 --> 32:28.960
I'm not sure what else I can do to make it a mainstream belief

32:28.960 --> 32:30.160
within this community.

32:30.160 --> 32:33.640
But one thing is for sure, no one is arguing the opposite.

32:33.640 --> 32:37.440
No one is saying, oh, we have control mechanism

32:37.440 --> 32:39.480
that will scale to any level of intelligence.

32:39.480 --> 32:42.000
You want to see the code, you want to see even the prototype,

32:42.000 --> 32:43.000
even an idea.

32:43.000 --> 32:44.840
No one is making that claim.

32:44.840 --> 32:46.280
Right, no one is claiming that.

32:46.280 --> 32:48.400
There are plenty of people claiming

32:48.400 --> 32:51.800
that the problem won't exist and arguing, for instance,

32:51.800 --> 32:54.160
that as AI becomes more intelligent,

32:54.160 --> 32:57.560
it will become more compassionate and ethical.

32:57.560 --> 32:59.720
And of course, there's no proof of that.

32:59.720 --> 33:03.640
And it only takes one that isn't to upset the apple cart.

33:04.080 --> 33:07.080
That's the end of the first half of the interview.

33:07.080 --> 33:10.080
I think it's really interesting how we're having

33:10.080 --> 33:13.800
this level of discussion about the goals and ethics

33:13.800 --> 33:17.000
and psychology of AI at such a high level,

33:17.000 --> 33:21.040
where up to two years ago, when we were having similar discussions,

33:21.040 --> 33:23.360
as you could hear me talking with Roman on this podcast

33:23.360 --> 33:26.480
four years ago, that is a very different story.

33:26.480 --> 33:28.480
And I think that's a very different story

33:28.480 --> 33:30.040
than I thought it would be.

33:30.040 --> 33:33.160
And talking with Roman on this podcast four years ago,

33:33.160 --> 33:37.400
that seemed to most people a very anthropomorphizing,

33:37.400 --> 33:41.240
grandiose and expansive point of view that was,

33:41.240 --> 33:45.080
if anything, detrimental to the funding environment for AI.

33:45.080 --> 33:47.280
I had people tell me exactly that.

33:47.280 --> 33:50.360
And yet now, it's become a serious discussion.

33:50.360 --> 33:52.360
And I've talked to politicians about this

33:52.360 --> 33:55.240
and they take it very seriously too.

33:55.240 --> 33:57.800
I'd like to mark a milestone here.

33:57.800 --> 34:00.680
We have just crossed a quarter of a million downloads

34:00.680 --> 34:02.880
of episodes of this podcast.

34:02.880 --> 34:05.080
And I'm pretty sure that they're not all my mother.

34:05.080 --> 34:09.640
So thank you, everyone, for giving us such awesome stats

34:09.640 --> 34:11.680
and please give us a five-star rating

34:11.680 --> 34:14.080
on your podcast platform of choice

34:14.080 --> 34:17.600
and we'll get the next quarter million downloads even faster.

34:18.480 --> 34:21.400
In today's news, ripped from the headlines about AI,

34:21.400 --> 34:24.360
the commercial deployment of robo-taxis

34:24.360 --> 34:26.560
continues its tailspin.

34:26.560 --> 34:29.200
Last November, General Motors recalled

34:29.200 --> 34:33.560
950 driverless cruise cars in San Francisco,

34:33.560 --> 34:36.360
where you'll remember that a number of cruise cars

34:36.360 --> 34:39.360
are deployed as autonomous taxis.

34:39.360 --> 34:41.640
A hit-and-run driver struck a pedestrian

34:41.640 --> 34:43.480
who was thrown into another lane

34:43.480 --> 34:47.200
where a cruise robo-taxi was unable to stop in time

34:47.200 --> 34:50.400
but then ended up dragging the pedestrian.

34:50.400 --> 34:52.160
The recall addresses circumstances

34:52.160 --> 34:55.000
when the software may cause the cruise AV

34:55.000 --> 34:56.920
to attempt to pull over out of traffic

34:56.920 --> 34:59.000
instead of remaining stationary,

34:59.000 --> 35:01.560
quote, when a pullover is not the desired

35:01.560 --> 35:05.360
post-collision response, unquote said cruise.

35:05.360 --> 35:08.240
GM has halted cruise operations nationwide

35:08.240 --> 35:10.800
and has also halted the production line.

35:10.800 --> 35:14.400
And now the cruise internal share price

35:14.400 --> 35:17.520
has been cut by more than half since then.

35:17.520 --> 35:20.520
Cruise division executives told some engineering

35:20.520 --> 35:23.760
and operation staff in internal meetings in recent weeks

35:23.760 --> 35:26.080
that they should not expect to see its robo-taxis

35:26.080 --> 35:29.200
on city streets again until the fourth quarter.

35:29.200 --> 35:32.000
Next week, we will conclude the interview with Roman

35:32.000 --> 35:34.480
when we'll talk about how we should respond

35:34.480 --> 35:37.040
to this problem of unsafe AI development

35:37.040 --> 35:40.160
and how Roman and his community are addressing it,

35:40.160 --> 35:42.800
what he would do with infinite resources

35:42.800 --> 35:46.720
and the threat Roman's coffee cup poses to humanity.

35:46.720 --> 35:49.320
That's next week on AI and You.

35:49.320 --> 35:51.280
Until then, remember,

35:51.280 --> 35:53.960
no matter how much computers learn how to do,

35:53.960 --> 35:57.080
it's how we come together as humans that matters.

35:58.800 --> 36:02.240
That's all for this episode of AI and You.

36:02.240 --> 36:03.760
Please leave a rating and comment

36:03.760 --> 36:05.440
and share with your friends.

36:05.440 --> 36:08.240
Get the book, Artificial Intelligence and You,

36:08.240 --> 36:13.000
and see more videos and articles at AIandYou.net.

36:13.000 --> 36:18.000
That's A-I-A-N-D-Y-O-U.net,

36:18.840 --> 36:21.160
where you can also send us your questions.

36:21.160 --> 36:22.360
Thank you for listening.

