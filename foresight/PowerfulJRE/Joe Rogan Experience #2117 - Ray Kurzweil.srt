1
00:00:00,000 --> 00:00:04,000
Joe Rogan Podcast, check it out!

2
00:00:04,000 --> 00:00:06,000
The Joe Rogan Experience.

3
00:00:06,000 --> 00:00:10,000
Train by day! Joe Rogan Podcast by night! All day!

4
00:00:12,000 --> 00:00:14,000
Good to see you, sir.

5
00:00:14,000 --> 00:00:15,000
Great to see you.

6
00:00:15,000 --> 00:00:18,000
I was telling you before, I'm admiring your suspenders,

7
00:00:18,000 --> 00:00:20,000
and you told me you have how many pairs of these things?

8
00:00:20,000 --> 00:00:21,000
30 of them, yeah.

9
00:00:21,000 --> 00:00:22,000
How did you?

10
00:00:22,000 --> 00:00:23,000
I wear them every day.

11
00:00:23,000 --> 00:00:24,000
Do you really? Every day?

12
00:00:24,000 --> 00:00:25,000
Yeah.

13
00:00:25,000 --> 00:00:27,000
Why do you like suspenders?

14
00:00:27,000 --> 00:00:29,000
Practicality thing?

15
00:00:29,000 --> 00:00:36,000
No, it expresses my personality.

16
00:00:36,000 --> 00:00:45,000
And different ones have different personalities

17
00:00:45,000 --> 00:00:48,000
that express how I feel that day.

18
00:00:48,000 --> 00:00:50,000
I see, so it's just another style point.

19
00:00:50,000 --> 00:00:51,000
Yeah.

20
00:00:51,000 --> 00:00:52,000
See the reason why I was asking?

21
00:00:52,000 --> 00:00:55,000
But you don't see any hand-painted suspenders.

22
00:00:55,000 --> 00:00:57,000
Have you ever seen one?

23
00:00:57,000 --> 00:00:58,000
I don't know.

24
00:00:58,000 --> 00:01:00,000
I would have not noticed.

25
00:01:00,000 --> 00:01:02,000
I only noticed because you were here.

26
00:01:02,000 --> 00:01:04,000
I'm not really a suspender aficionado.

27
00:01:04,000 --> 00:01:08,000
But the reason why I'm asking is because you're basically a technologist.

28
00:01:08,000 --> 00:01:10,000
I mean, you know a lot about technology,

29
00:01:10,000 --> 00:01:17,000
when you think that suspenders are kind of outdated tech.

30
00:01:17,000 --> 00:01:19,000
Well, people like them.

31
00:01:19,000 --> 00:01:20,000
Clearly.

32
00:01:20,000 --> 00:01:21,000
Yeah.

33
00:01:21,000 --> 00:01:24,000
And I'm surprised I haven't caught on.

34
00:01:24,000 --> 00:01:27,000
But you have to have somebody who can actually paint them.

35
00:01:27,000 --> 00:01:30,000
I mean, these are hand-painted suspenders.

36
00:01:30,000 --> 00:01:33,000
So the ones that you have, right here, these are hand-painted?

37
00:01:33,000 --> 00:01:34,000
Yeah.

38
00:01:34,000 --> 00:01:35,000
Interesting.

39
00:01:35,000 --> 00:01:36,000
Okay, so that's part of it.

40
00:01:36,000 --> 00:01:37,000
So you're wearing art.

41
00:01:37,000 --> 00:01:38,000
Exactly.

42
00:01:38,000 --> 00:01:39,000
Got it.

43
00:01:39,000 --> 00:01:40,000
So...

44
00:01:40,000 --> 00:01:42,000
And art is part of technology.

45
00:01:42,000 --> 00:01:45,000
We're using technology to create art now, so...

46
00:01:45,000 --> 00:01:46,000
That's true.

47
00:01:46,000 --> 00:01:47,000
And it's...

48
00:01:47,000 --> 00:01:49,000
In fact, the very first...

49
00:01:49,000 --> 00:01:56,000
I mean, I've been now in AI for 61 years, which is actually a record.

50
00:01:56,000 --> 00:02:04,000
And the first thing I did was create something that could write music.

51
00:02:04,000 --> 00:02:09,000
Writing music now with AI is a major field today,

52
00:02:09,000 --> 00:02:13,000
but this was actually the first time that it had ever been done.

53
00:02:13,000 --> 00:02:16,000
Yeah, that was one of your many inventions.

54
00:02:16,000 --> 00:02:18,000
That was the first one, yeah.

55
00:02:18,000 --> 00:02:20,000
And why did you go about doing that?

56
00:02:20,000 --> 00:02:25,000
What was your desire to create artificial intelligence music?

57
00:02:25,000 --> 00:02:28,000
Well, my father was a musician,

58
00:02:28,000 --> 00:02:31,000
and I felt this would be a good way to relate to him.

59
00:02:31,000 --> 00:02:36,000
And he actually worked with me on it.

60
00:02:36,000 --> 00:02:38,000
And you could feed in music,

61
00:02:38,000 --> 00:02:42,000
like you could feed in, let's say, Mozart or Chopin,

62
00:02:42,000 --> 00:02:47,000
and I would figure out how they created melodies

63
00:02:47,000 --> 00:02:50,000
and then write melodies in the same style.

64
00:02:50,000 --> 00:02:54,000
So you could actually tell this is Mozart, this is Chopin.

65
00:02:54,000 --> 00:03:01,000
It wasn't as good, but it's the first time that that had been done.

66
00:03:01,000 --> 00:03:03,000
It wasn't as good then.

67
00:03:03,000 --> 00:03:05,000
What are the capabilities now?

68
00:03:05,000 --> 00:03:08,000
Because now they can do some pretty extraordinary things.

69
00:03:08,000 --> 00:03:12,000
Yeah, it's still not up to what humans can do,

70
00:03:12,000 --> 00:03:14,000
but it's getting there,

71
00:03:14,000 --> 00:03:17,000
and it's really, it's pleasant to listen to.

72
00:03:17,000 --> 00:03:23,000
We still have a while to do art, both art, music, so on.

73
00:03:23,000 --> 00:03:29,000
Well, one of the main arguments against AI art comes from actual artists

74
00:03:29,000 --> 00:03:32,000
who are upset that what essentially they're doing is they're,

75
00:03:32,000 --> 00:03:36,000
like you could say, right, draw a paint,

76
00:03:36,000 --> 00:03:40,000
or create a painting in the style of Frank Frasetta, for instance.

77
00:03:40,000 --> 00:03:45,000
And what it would be, they would take all of Frasetta's work that he's ever done,

78
00:03:45,000 --> 00:03:48,000
which is all documented on the internet,

79
00:03:48,000 --> 00:03:52,000
and then you create an image that's representative of that.

80
00:03:52,000 --> 00:03:55,000
So you're essentially, in one way or another,

81
00:03:55,000 --> 00:03:58,000
you're kind of taking from the art.

82
00:03:58,000 --> 00:04:00,000
Right, but it's not quite as good.

83
00:04:00,000 --> 00:04:02,000
It will be as good.

84
00:04:02,000 --> 00:04:07,000
I think we'll match human experience by 2029.

85
00:04:07,000 --> 00:04:11,000
That's been my idea.

86
00:04:11,000 --> 00:04:14,000
It's not as good.

87
00:04:14,000 --> 00:04:16,000
Which is the best image generator right now, Jamie?

88
00:04:16,000 --> 00:04:18,000
Pull one up.

89
00:04:18,000 --> 00:04:21,000
They really change almost from day to day right now,

90
00:04:21,000 --> 00:04:23,000
but mid-journey was the most popular one at first,

91
00:04:23,000 --> 00:04:28,000
and then Dolly, I think, is a really good one too.

92
00:04:28,000 --> 00:04:30,000
Mid-journey is incredibly impressive.

93
00:04:30,000 --> 00:04:32,000
Incredibly impressive graphics.

94
00:04:32,000 --> 00:04:34,000
I've seen some of the mid-journey stuff.

95
00:04:34,000 --> 00:04:36,000
It's mind-blowing.

96
00:04:36,000 --> 00:04:38,000
Still not quite as good.

97
00:04:38,000 --> 00:04:40,000
But boys, it's so much better than it was five years ago.

98
00:04:40,000 --> 00:04:42,000
That's what's scary. It's so quick.

99
00:04:42,000 --> 00:04:45,000
I mean, it's never going to reach its limit.

100
00:04:45,000 --> 00:04:46,000
We're not going to get to a point,

101
00:04:46,000 --> 00:04:48,000
okay, this is how good it's going to be.

102
00:04:48,000 --> 00:04:51,000
It's going to keep getting better.

103
00:04:51,000 --> 00:04:53,000
And what would that look like?

104
00:04:53,000 --> 00:04:55,000
If it can get to a certain point,

105
00:04:55,000 --> 00:04:59,000
it will far exceed what human creativity is capable of.

106
00:04:59,000 --> 00:05:04,000
Yes, I mean, when we reach the ability of humans,

107
00:05:04,000 --> 00:05:06,000
it's not going to just match one human.

108
00:05:06,000 --> 00:05:08,000
It's going to match all humans.

109
00:05:08,000 --> 00:05:11,000
It's going to do everything that any human can do.

110
00:05:11,000 --> 00:05:14,000
If it's playing a game like Go,

111
00:05:14,000 --> 00:05:16,000
it's going to play it better than any human.

112
00:05:16,000 --> 00:05:18,000
Right. Well, that's already been proven, right?

113
00:05:18,000 --> 00:05:20,000
That they invented moves.

114
00:05:20,000 --> 00:05:24,000
AI has invented moves that have now been implemented by humans

115
00:05:24,000 --> 00:05:26,000
in a very complex game that they never thought

116
00:05:26,000 --> 00:05:28,000
that AI was going to be able to be,

117
00:05:28,000 --> 00:05:30,000
because it requires so much creativity.

118
00:05:30,000 --> 00:05:33,000
Right. Arthur, we're not quite there,

119
00:05:33,000 --> 00:05:35,000
but we will be there.

120
00:05:35,000 --> 00:05:42,000
And by 2029, it will match any person.

121
00:05:42,000 --> 00:05:44,000
That's it? 2029?

122
00:05:44,000 --> 00:05:46,000
That's just a few years away.

123
00:05:46,000 --> 00:05:48,000
Yeah, well, I'm actually considered conservative.

124
00:05:48,000 --> 00:05:51,000
People think that will happen like next year or the year after,

125
00:05:51,000 --> 00:05:56,000
but I actually said that in 1999.

126
00:05:56,000 --> 00:06:02,000
I said we would match any person by 2029.

127
00:06:02,000 --> 00:06:04,000
So 30 years.

128
00:06:04,000 --> 00:06:07,000
People thought that was totally crazy.

129
00:06:07,000 --> 00:06:12,000
And in fact, Stanford had a conference.

130
00:06:12,000 --> 00:06:15,000
They invited several hundred people from around the world

131
00:06:15,000 --> 00:06:18,000
to talk about my prediction.

132
00:06:18,000 --> 00:06:22,000
And people came in and people thought that this would happen,

133
00:06:22,000 --> 00:06:25,000
but not by 2029. They thought it would take 100 years.

134
00:06:25,000 --> 00:06:27,000
Yeah, I've heard that.

135
00:06:27,000 --> 00:06:30,000
I've heard that, but I think people are amending those.

136
00:06:30,000 --> 00:06:33,000
Is it because human beings have a very difficult time

137
00:06:33,000 --> 00:06:36,000
grasping the concept of exponential growth?

138
00:06:36,000 --> 00:06:39,000
That's exactly right.

139
00:06:39,000 --> 00:06:43,000
In fact, still, economists have a linear view.

140
00:06:43,000 --> 00:06:46,000
And if you say, well, it's going to grow exponentially.

141
00:06:46,000 --> 00:06:52,000
They say, yeah, but maybe 2% a year.

142
00:06:52,000 --> 00:06:57,000
It actually doubles in 14 years.

143
00:06:57,000 --> 00:07:06,000
And I brought a chart I can show you that really illustrates this.

144
00:07:06,000 --> 00:07:09,000
Is this chart available online so we could show people?

145
00:07:09,000 --> 00:07:10,000
Yeah, it's in the book.

146
00:07:10,000 --> 00:07:14,000
But is it available online, that chart, where Jamie could pull it up

147
00:07:14,000 --> 00:07:16,000
and someone could see it?

148
00:07:16,000 --> 00:07:19,000
Just so the folks watching the podcast could see it too.

149
00:07:19,000 --> 00:07:21,000
But I could just hold it up to the camera.

150
00:07:21,000 --> 00:07:23,000
What's the title of it?

151
00:07:23,000 --> 00:07:29,000
It says price performance of computation 1939 to 2023.

152
00:07:29,000 --> 00:07:30,000
You have it.

153
00:07:30,000 --> 00:07:31,000
OK, great.

154
00:07:31,000 --> 00:07:32,000
Jamie already has it.

155
00:07:32,000 --> 00:07:34,000
Yeah, the climb is insane.

156
00:07:34,000 --> 00:07:36,000
It's like the San Juan Mountains.

157
00:07:36,000 --> 00:07:41,000
What's interesting is that it's an exponential curve

158
00:07:41,000 --> 00:07:44,000
and a straight line represents exponential growth.

159
00:07:44,000 --> 00:07:50,000
And that's an absolute straight line for 80 years.

160
00:07:50,000 --> 00:07:55,000
The very first point, this is the speed of computers.

161
00:07:55,000 --> 00:08:06,000
It was 0.0007 calculations per second per constant dollar.

162
00:08:06,000 --> 00:08:11,000
The last point is 35 billion calculations per second.

163
00:08:11,000 --> 00:08:17,000
So that's a 20 quadrillion fold increase in those 80 years.

164
00:08:17,000 --> 00:08:23,000
But the speed with which it gained is actually the same

165
00:08:23,000 --> 00:08:25,000
throughout the entire 80 years.

166
00:08:25,000 --> 00:08:28,000
Because if it was sometimes better and sometimes worse,

167
00:08:28,000 --> 00:08:30,000
this curve would bend.

168
00:08:30,000 --> 00:08:32,000
It would bend up and down.

169
00:08:32,000 --> 00:08:36,000
It's really very much a straight line.

170
00:08:36,000 --> 00:08:40,000
So the speed with which we increased it was the same

171
00:08:40,000 --> 00:08:42,000
regardless of the technologies.

172
00:08:42,000 --> 00:08:44,000
And the technology was radically different

173
00:08:44,000 --> 00:08:46,000
at the beginning versus the end.

174
00:08:46,000 --> 00:08:53,000
And yet it increased the speed exactly the same for 80 years.

175
00:08:53,000 --> 00:08:56,000
In fact, the first 40 years, nobody even knew this was happening.

176
00:08:56,000 --> 00:08:58,000
So it's not like somebody was in charge

177
00:08:58,000 --> 00:09:00,000
and saying, OK, next year we have to get to here

178
00:09:00,000 --> 00:09:02,000
and people would try to match that.

179
00:09:02,000 --> 00:09:05,000
We didn't even know this was happening for 40 years.

180
00:09:05,000 --> 00:09:09,000
40 years later, I noticed this for various reasons.

181
00:09:09,000 --> 00:09:11,000
I predicted it would stay the same,

182
00:09:11,000 --> 00:09:16,000
the same speed increase each year, which it has.

183
00:09:16,000 --> 00:09:19,000
In fact, we just put the last dot like two weeks ago

184
00:09:19,000 --> 00:09:22,000
and it's exactly where it should be.

185
00:09:22,000 --> 00:09:29,000
So technology and computation, certainly prime form of technology,

186
00:09:29,000 --> 00:09:33,000
increases at the same speed.

187
00:09:33,000 --> 00:09:35,000
And this goes through a worn piece.

188
00:09:35,000 --> 00:09:37,000
You might say, well, maybe it's greater during war.

189
00:09:37,000 --> 00:09:39,000
No, it's exactly the same.

190
00:09:39,000 --> 00:09:41,000
You can't tell when there's war or peace

191
00:09:41,000 --> 00:09:43,000
or anything else on here.

192
00:09:43,000 --> 00:09:50,000
It just matches from one type of technology to the next.

193
00:09:50,000 --> 00:09:53,000
And it's also true of other things,

194
00:09:53,000 --> 00:09:59,000
like, for example, getting energy from the sun.

195
00:09:59,000 --> 00:10:01,000
That's also exponential.

196
00:10:01,000 --> 00:10:04,000
It's also just like this.

197
00:10:04,000 --> 00:10:10,000
It's increased.

198
00:10:10,000 --> 00:10:19,000
We're now getting about a thousand times as much energy

199
00:10:19,000 --> 00:10:23,000
from the sun that we did 20 years ago.

200
00:10:23,000 --> 00:10:26,000
Because the implementation of solar panels and the like.

201
00:10:26,000 --> 00:10:31,000
Has the function of it increased exponentially as well?

202
00:10:31,000 --> 00:10:36,000
What I had understood was that there was a bottleneck in the technology

203
00:10:36,000 --> 00:10:40,000
as far as how much you could extract from the sun from those panels.

204
00:10:40,000 --> 00:10:42,000
No, not at all.

205
00:10:42,000 --> 00:10:50,000
I mean, it's increased 99.7% since we started.

206
00:10:50,000 --> 00:10:53,000
And it does the same every year.

207
00:10:53,000 --> 00:10:55,000
It's an exponential curve.

208
00:10:55,000 --> 00:10:57,000
And if you look at the curve,

209
00:10:57,000 --> 00:11:01,000
you'll be getting 100% of all the energy we need in 10 years.

210
00:11:01,000 --> 00:11:02,000
The person who told me that was Elon.

211
00:11:02,000 --> 00:11:04,000
And Elon was telling me that this is the reason

212
00:11:04,000 --> 00:11:07,000
why you can't have a fully solar-powered electric car.

213
00:11:07,000 --> 00:11:10,000
Because it's not capable of absorbing that much from the sun

214
00:11:10,000 --> 00:11:12,000
with a small panel like that.

215
00:11:12,000 --> 00:11:15,000
He said there's a physical limitation in the panel size.

216
00:11:15,000 --> 00:11:20,000
No, I mean, it's increased 99.7% since we started.

217
00:11:20,000 --> 00:11:22,000
Since what year?

218
00:11:22,000 --> 00:11:29,000
This is about 35 years ago.

219
00:11:29,000 --> 00:11:39,000
In 99% of the ability of it as well as the expansion of use?

220
00:11:39,000 --> 00:11:41,000
I mean, you might have to store it.

221
00:11:41,000 --> 00:11:44,000
We're also making exponential gains in the storage of electricity.

222
00:11:44,000 --> 00:11:46,000
Right. Battery technology.

223
00:11:46,000 --> 00:11:51,000
So you don't have to get it all from a solar panel that fits in a car.

224
00:11:51,000 --> 00:11:56,000
The concept was like, could you make a solar-paneled car,

225
00:11:56,000 --> 00:11:58,000
a car that has solar panels on the roof?

226
00:11:58,000 --> 00:12:00,000
And would that be enough to power the car?

227
00:12:00,000 --> 00:12:02,000
And he said no.

228
00:12:02,000 --> 00:12:05,000
He said it's just not really there yet.

229
00:12:05,000 --> 00:12:07,000
Right. It's not there yet.

230
00:12:07,000 --> 00:12:09,000
But it will be there in 10 years.

231
00:12:09,000 --> 00:12:10,000
You think so?

232
00:12:10,000 --> 00:12:12,000
Yeah, he seemed to doubt that.

233
00:12:12,000 --> 00:12:14,000
He thought that there's a certain,

234
00:12:14,000 --> 00:12:17,000
there's limitation of the amount of energy you can get from the sun period,

235
00:12:17,000 --> 00:12:20,000
how much it gives out and how much those solar panels can absorb.

236
00:12:20,000 --> 00:12:24,000
Well, you're not going to be able to get it all from the solar panel that fits in a car.

237
00:12:24,000 --> 00:12:26,000
You're going to have to store some of that energy.

238
00:12:26,000 --> 00:12:31,000
Right. So you wouldn't just be able to drive indefinitely on solar power.

239
00:12:31,000 --> 00:12:33,000
Yeah, that was what he was saying.

240
00:12:33,000 --> 00:12:36,000
So, but you can obviously power a house.

241
00:12:36,000 --> 00:12:41,000
And especially if you have a roof, the Tesla has those solar-powered roofs now.

242
00:12:41,000 --> 00:12:44,000
But you can also store the energy for a car.

243
00:12:45,000 --> 00:12:49,000
I mean, we're going to go to all renewable energy,

244
00:12:49,000 --> 00:12:53,000
wind and sun within 10 years,

245
00:12:53,000 --> 00:12:56,000
including our ability to store the energy.

246
00:12:56,000 --> 00:12:58,000
All renewable in 10 years?

247
00:12:58,000 --> 00:13:02,000
So what are they going to do with all these nuclear plants and coal-powered plants and all these things?

248
00:13:02,000 --> 00:13:04,000
That's completely unnecessary.

249
00:13:04,000 --> 00:13:09,000
People say we need nuclear power, which we don't.

250
00:13:09,000 --> 00:13:13,000
You can get it all from the sun and wind within 10 years.

251
00:13:14,000 --> 00:13:19,000
So in 10 years, you'll be able to power Los Angeles with sun and wind?

252
00:13:19,000 --> 00:13:20,000
Yes.

253
00:13:20,000 --> 00:13:21,000
Really?

254
00:13:21,000 --> 00:13:22,000
Yeah.

255
00:13:22,000 --> 00:13:25,000
I was not aware that we were anywhere near that kind of timeline.

256
00:13:25,000 --> 00:13:30,000
Well, that's because people are not taking into account exponential growth.

257
00:13:30,000 --> 00:13:33,000
So the exponential growth also of the grid?

258
00:13:33,000 --> 00:13:40,000
Because just to pull the amount of power that you would need to charge, you know, X amount of million.

259
00:13:40,000 --> 00:13:45,000
Someone has an electric vehicle by 2035, let's say then.

260
00:13:45,000 --> 00:13:50,000
Just the amount of change you would need on the grid would be pretty substantial.

261
00:13:50,000 --> 00:13:52,000
Well, we're making exponential gains on that as well.

262
00:13:52,000 --> 00:13:53,000
Are we?

263
00:13:53,000 --> 00:13:54,000
Yeah.

264
00:13:54,000 --> 00:13:55,000
Yeah.

265
00:13:55,000 --> 00:13:56,000
I wasn't aware.

266
00:13:56,000 --> 00:14:00,000
I had this impression that there was a problem with that,

267
00:14:00,000 --> 00:14:04,000
and especially in Los Angeles, they've actually asked people at certain times

268
00:14:04,000 --> 00:14:06,000
when it's hot out to not charge your car.

269
00:14:06,000 --> 00:14:11,000
Looking at the future, that's true now, but it's growing exponentially.

270
00:14:11,000 --> 00:14:16,000
In every field of technology then, essentially.

271
00:14:16,000 --> 00:14:19,000
Is the bottleneck a battery technology?

272
00:14:19,000 --> 00:14:23,000
And how close are they to solving some of these problems,

273
00:14:23,000 --> 00:14:30,000
like conflict minerals and the things that we need in order to power these batteries?

274
00:14:30,000 --> 00:14:34,000
Our ability to store energy is also growing exponentially.

275
00:14:34,000 --> 00:14:42,000
So putting all that together, we'll be able to power everything we need within 10 years.

276
00:14:42,000 --> 00:14:43,000
Wow.

277
00:14:43,000 --> 00:14:44,000
Most people don't think that.

278
00:14:44,000 --> 00:14:49,000
So you're thinking that based on this idea that people would have a limited idea.

279
00:14:49,000 --> 00:14:51,000
Let me imagine that computation would grow like this.

280
00:14:51,000 --> 00:14:55,000
It's just continuing to do that.

281
00:14:55,000 --> 00:14:59,000
And so we have large language models, for example.

282
00:14:59,000 --> 00:15:02,000
No one expected that to happen like five years ago.

283
00:15:02,000 --> 00:15:03,000
Right.

284
00:15:03,000 --> 00:15:06,000
And we had them two years ago, but they didn't work very well.

285
00:15:06,000 --> 00:15:13,000
So it began a little less than two years ago that we could actually do large language models.

286
00:15:13,000 --> 00:15:17,000
And that was very much a surprise to everybody.

287
00:15:17,000 --> 00:15:22,000
So that's probably the primary example of exponential growth.

288
00:15:22,000 --> 00:15:23,000
We had Sam Altman on.

289
00:15:23,000 --> 00:15:28,000
One of the things that he and I were talking about was that AI figured out a way to lie.

290
00:15:28,000 --> 00:15:32,000
That they used AI to go through a CAPTCHA system.

291
00:15:32,000 --> 00:15:38,000
And the AI told the system that it was vision impaired, which is not technically a lie.

292
00:15:38,000 --> 00:15:41,000
But it used it to bypass, are you a robot?

293
00:15:41,000 --> 00:15:46,000
Well, we don't know now for large language models to say they don't know something.

294
00:15:46,000 --> 00:15:48,000
So you ask it a question.

295
00:15:48,000 --> 00:15:54,000
And if the answer to that question is not in the system, it still comes up with an answer.

296
00:15:54,000 --> 00:15:58,000
So it'll look at everything and give you its best answer.

297
00:15:58,000 --> 00:16:02,000
And if the best answer is not there, it still gives you an answer.

298
00:16:02,000 --> 00:16:06,000
But that's considered a hallucination.

299
00:16:06,000 --> 00:16:07,000
Oh, hallucination.

300
00:16:07,000 --> 00:16:08,000
Yeah.

301
00:16:08,000 --> 00:16:09,000
That's what it's called.

302
00:16:09,000 --> 00:16:10,000
Really?

303
00:16:10,000 --> 00:16:11,000
AI hallucination.

304
00:16:11,000 --> 00:16:13,000
So they cannot be wrong.

305
00:16:13,000 --> 00:16:15,000
They have to be able to answer this.

306
00:16:15,000 --> 00:16:19,000
So far, we're actually working on being able to tell if it doesn't know something.

307
00:16:19,000 --> 00:16:22,000
So if you ask it something and say, oh, I don't know that.

308
00:16:22,000 --> 00:16:24,000
Right now, it can't do that.

309
00:16:24,000 --> 00:16:25,000
Oh, wow.

310
00:16:25,000 --> 00:16:26,000
That's interesting.

311
00:16:26,000 --> 00:16:31,000
So it gives you some answer.

312
00:16:31,000 --> 00:16:35,000
And if the answer is not there, it's just like make something up.

313
00:16:35,000 --> 00:16:40,000
It's the best answer, but the best answer isn't very good because it doesn't know the answer.

314
00:16:40,000 --> 00:16:47,000
And the way to fix hallucinations is to actually give it more capabilities to memorize things

315
00:16:47,000 --> 00:16:51,000
and give it more information so it knows the answer to it.

316
00:16:51,000 --> 00:17:00,000
And if you tell an answer to a question, it will remember that and give you that correct answer.

317
00:17:00,000 --> 00:17:04,000
But these models are not, we don't know everything.

318
00:17:04,000 --> 00:17:15,000
And it has to, we have to be able to scan an answer to every single question, which we can't quite do.

319
00:17:15,000 --> 00:17:17,000
It'd be actually better if it could actually answer.

320
00:17:17,000 --> 00:17:19,000
Well, gee, I don't know that.

321
00:17:19,000 --> 00:17:20,000
Right.

322
00:17:20,000 --> 00:17:25,000
But in particular, like say when it comes to exploration of the universe,

323
00:17:25,000 --> 00:17:29,000
if there's a certain amount of, I mean, vast amount of the universe we have not explored.

324
00:17:29,000 --> 00:17:33,000
So if it has to answer questions about that, it would just come up with an answer.

325
00:17:33,000 --> 00:17:34,000
Right.

326
00:17:34,000 --> 00:17:37,000
It'll just come up with an answer, which will likely be wrong.

327
00:17:37,000 --> 00:17:39,000
That's interesting.

328
00:17:39,000 --> 00:17:46,000
But that would be a real problem if someone was counting on the AI to have a solution for something too soon.

329
00:17:46,000 --> 00:17:47,000
Right?

330
00:17:47,000 --> 00:17:48,000
Right.

331
00:17:48,000 --> 00:17:55,000
If you don't know everything, search engines actually know are pretty well vetted.

332
00:17:55,000 --> 00:18:00,000
And if it actually answers something, it'll, it's usually correct.

333
00:18:00,000 --> 00:18:02,000
Unless it's curated.

334
00:18:02,000 --> 00:18:06,000
But large language models don't have that capability.

335
00:18:06,000 --> 00:18:09,000
So it'd be good actually if they knew that they were wrong.

336
00:18:09,000 --> 00:18:13,000
They'd also tell us what we have to fix.

337
00:18:13,000 --> 00:18:19,000
What about the idea that AI models are influenced by ideology,

338
00:18:19,000 --> 00:18:23,000
that AI models have been programmed with certain ideologies?

339
00:18:23,000 --> 00:18:25,000
I mean, they do learn from people.

340
00:18:25,000 --> 00:18:26,000
Yeah.

341
00:18:26,000 --> 00:18:28,000
And people have ideologies.

342
00:18:28,000 --> 00:18:29,000
Right.

343
00:18:29,000 --> 00:18:32,000
Some of which are, some of which are not correct.

344
00:18:32,000 --> 00:18:42,000
And that's a large way in which it will make things up because it's learning from people.

345
00:18:42,000 --> 00:18:44,000
Right.

346
00:18:44,000 --> 00:18:52,000
So right now, if somebody has access to a good search engine,

347
00:18:52,000 --> 00:18:58,000
they will check before they actually answer something with a search engine to make sure that it's correct.

348
00:18:58,000 --> 00:19:02,000
Because search engines are generally much more accurate.

349
00:19:02,000 --> 00:19:03,000
Generally.

350
00:19:03,000 --> 00:19:04,000
Right.

351
00:19:04,000 --> 00:19:10,000
When it comes to this idea that people enter information into a computer,

352
00:19:10,000 --> 00:19:12,000
and then the computer relies on that ideology,

353
00:19:12,000 --> 00:19:16,000
do you anticipate that with artificial general intelligence,

354
00:19:16,000 --> 00:19:18,000
it'll be agnostic to ideology,

355
00:19:18,000 --> 00:19:25,000
that it'll be able to reach a point where instead of deciding things based on social norms,

356
00:19:25,000 --> 00:19:28,000
or whatever the culture is accepted currently,

357
00:19:28,000 --> 00:19:32,000
that it would look at things more objectively and rationally?

358
00:19:32,000 --> 00:19:33,000
Well, eventually.

359
00:19:33,000 --> 00:19:34,000
Eventually.

360
00:19:34,000 --> 00:19:35,000
Eventually.

361
00:19:35,000 --> 00:19:38,000
But we still call it artificial general intelligence,

362
00:19:38,000 --> 00:19:39,000
even if it didn't do that.

363
00:19:39,000 --> 00:19:55,000
And people certainly are influenced by whatever their people that they respect feel is correct,

364
00:19:55,000 --> 00:20:00,000
and will be as influenced by as people are.

365
00:20:00,000 --> 00:20:05,000
And we'll still call it artificial general intelligence.

366
00:20:05,000 --> 00:20:13,000
We are starting to check what large language models come up with with search engines,

367
00:20:13,000 --> 00:20:16,000
and that's actually making them more correct.

368
00:20:16,000 --> 00:20:19,000
But we have to actually continue on this curve.

369
00:20:19,000 --> 00:20:22,000
We need more data to be able to store everything.

370
00:20:22,000 --> 00:20:27,000
This is not enough data to be able to store everything correctly.

371
00:20:27,000 --> 00:20:36,000
This is a large amount of large language models for which we don't have storage for the data.

372
00:20:36,000 --> 00:20:39,000
So that's what's holding us back is data and storage?

373
00:20:39,000 --> 00:20:43,000
Yeah, we also have to have the correct storage.

374
00:20:43,000 --> 00:20:51,000
So that's really where the effort is going to be able to get rid of these hallucinations.

375
00:20:51,000 --> 00:20:56,000
That's a fun thing to say, hallucinations in terms of artificial intelligence.

376
00:20:56,000 --> 00:20:59,000
Well, we usually come up with the wrong things.

377
00:20:59,000 --> 00:21:04,000
Large language models is not really the correct way to talk about this.

378
00:21:04,000 --> 00:21:09,000
It does know language, but there's a lot of other things it knows.

379
00:21:09,000 --> 00:21:21,000
We're using them now to come up with medicines.

380
00:21:21,000 --> 00:21:24,000
For example, the Moderna vaccine.

381
00:21:24,000 --> 00:21:41,000
We wrote down every possible type of medicine that might work.

382
00:21:41,000 --> 00:21:45,000
It was actually several billion mRNA sequences,

383
00:21:45,000 --> 00:21:51,000
and we then tested them all and did that in two days.

384
00:21:51,000 --> 00:22:00,000
So it actually came up with tested several billion and decided on it in two days.

385
00:22:00,000 --> 00:22:03,000
We then tested it with people.

386
00:22:03,000 --> 00:22:10,000
We'll be able to overcome that as well because we'll be able to test it with machines.

387
00:22:10,000 --> 00:22:14,000
But we actually did test it with people for ten months.

388
00:22:14,000 --> 00:22:16,000
There was still a record.

389
00:22:16,000 --> 00:22:22,000
So for machines, when they start testing medications with machines, how will they audit that?

390
00:22:22,000 --> 00:22:27,000
So the concept will be that you take into account biological variability,

391
00:22:27,000 --> 00:22:32,000
all the different factors that would lead to a person to have an adverse reaction to a certain compound,

392
00:22:32,000 --> 00:22:39,000
and then you program all the known data about how things interact with the body.

393
00:22:39,000 --> 00:22:44,000
You need to be able to simulate all the different possibilities.

394
00:22:44,000 --> 00:22:48,000
And then come up with a number of how many people will be adversely affected by something?

395
00:22:48,000 --> 00:22:51,000
That's one of the things you would look at.

396
00:22:51,000 --> 00:22:54,000
And then efficacy based on age, health?

397
00:22:54,000 --> 00:23:04,000
But that could be done literally in a matter of days rather than years.

398
00:23:04,000 --> 00:23:10,000
The question would be who's in charge of that data and how does that get resolved?

399
00:23:10,000 --> 00:23:15,000
And if artificial intelligence is still prone to hallucinations,

400
00:23:15,000 --> 00:23:20,000
and they start using those hallucinations to justify medications, that could be a bit of an issue.

401
00:23:20,000 --> 00:23:24,000
Especially if it's controlled by a corporation that wants to make a lot of money.

402
00:23:24,000 --> 00:23:26,000
Well, that's the issue.

403
00:23:26,000 --> 00:23:28,000
To be able to do it correctly.

404
00:23:28,000 --> 00:23:32,000
There's going to have to be a point in time where we all decide that artificial intelligence

405
00:23:32,000 --> 00:23:36,000
has reached this place where we can trust it implicitly.

406
00:23:36,000 --> 00:23:44,000
Right. Well, that's why they take now the leading candidate and actually test it with people.

407
00:23:44,000 --> 00:23:54,000
But we'll be able to get rid of the testing with people once we can have reliance on the simulation.

408
00:23:54,000 --> 00:23:58,000
So we've got to make the simulations correct.

409
00:23:58,000 --> 00:24:07,000
But like right now we actually tested with people and that takes, well, took 10 months in this case.

410
00:24:07,000 --> 00:24:15,000
When you look at artificial intelligence and you look at the expansion of it and the ultimate place that it will eventually be,

411
00:24:15,000 --> 00:24:20,000
what do you see happening inside of our lifetime, like inside of 20 years?

412
00:24:20,000 --> 00:24:25,000
What kind of revolutionary changes on society would this have?

413
00:24:25,000 --> 00:24:35,000
Well, one thing I feel will happen in five years by 2029 is we'll reach longevity escape velocity.

414
00:24:35,000 --> 00:24:40,000
So right now you go through a year and you use up a year of your longevity.

415
00:24:40,000 --> 00:24:42,000
You're then a year older.

416
00:24:42,000 --> 00:24:51,000
However, we do have scientific progress and we're making coming up with new cures for diseases and so on.

417
00:24:51,000 --> 00:24:53,000
Right now you're getting back about four months.

418
00:24:53,000 --> 00:25:00,000
So you lose a year, but through scientific progress, you're getting back four months.

419
00:25:00,000 --> 00:25:02,000
So you're only losing eight months.

420
00:25:02,000 --> 00:25:06,000
However, the scientific progress is progressing exponentially.

421
00:25:06,000 --> 00:25:10,000
And by 2029, you'll get back a full year.

422
00:25:10,000 --> 00:25:15,000
So you lose a year, but you get back a year and you pretty much stay in the same place.

423
00:25:15,000 --> 00:25:18,000
So by 2029, you'll be static.

424
00:25:18,000 --> 00:25:22,000
And past 2029, you'll actually get back more than a year.

425
00:25:22,000 --> 00:25:23,000
You'll get back.

426
00:25:23,000 --> 00:25:28,000
Can I be a baby again?

427
00:25:28,000 --> 00:25:33,000
No, but in terms of your longevity, you'll get back more than a year.

428
00:25:33,000 --> 00:25:34,000
Right.

429
00:25:34,000 --> 00:25:38,000
So you'll be able to essentially go back in biological age.

430
00:25:38,000 --> 00:25:42,000
So the telomeres changing the elasticity of the skin.

431
00:25:42,000 --> 00:25:46,000
Eventually you'll be able to do that.

432
00:25:46,000 --> 00:25:49,000
It doesn't guarantee you living forever.

433
00:25:49,000 --> 00:25:57,000
I mean, you could have a 10 year old and you could compute that he's got many decades of longevity and he could die tomorrow.

434
00:25:57,000 --> 00:25:59,000
Sure.

435
00:25:59,000 --> 00:26:04,000
But overall, there'd be an expansion of the age that most people die.

436
00:26:04,000 --> 00:26:06,000
And that's something that we're going to get.

437
00:26:06,000 --> 00:26:12,000
And it's also using the same type of logic as large language models.

438
00:26:12,000 --> 00:26:14,000
But that's not language.

439
00:26:14,000 --> 00:26:16,000
You're actually creating medications.

440
00:26:16,000 --> 00:26:22,000
So we should call the large event models, not large language models, because it's not just dealing with language.

441
00:26:22,000 --> 00:26:25,000
It's dealing with all kinds of things.

442
00:26:25,000 --> 00:26:32,000
When I talked to you 10 years ago, you were telling me about this pretty extensive supplement routine that you're on.

443
00:26:32,000 --> 00:26:39,000
Well, I'm trying to get to the point where we have longevity escape velocity in good shape.

444
00:26:39,000 --> 00:26:40,000
Right.

445
00:26:40,000 --> 00:26:42,000
And yes, I do follow that.

446
00:26:42,000 --> 00:26:50,000
I take maybe 80 pills a day and some injections and so on.

447
00:26:50,000 --> 00:26:51,000
Peptides.

448
00:26:51,000 --> 00:26:53,000
Yes, peptides.

449
00:26:53,000 --> 00:26:57,000
So far it works.

450
00:26:57,000 --> 00:27:01,000
Have you ever gone off of it to see what you feel like normally?

451
00:27:01,000 --> 00:27:02,000
No.

452
00:27:02,000 --> 00:27:03,000
Well, I do that, right?

453
00:27:03,000 --> 00:27:04,000
Yeah.

454
00:27:04,000 --> 00:27:06,000
I mean, it seems to work.

455
00:27:06,000 --> 00:27:08,000
And there's evidence behind it.

456
00:27:08,000 --> 00:27:09,000
How old are you now?

457
00:27:09,000 --> 00:27:12,000
76.

458
00:27:12,000 --> 00:27:13,000
You look good.

459
00:27:13,000 --> 00:27:15,000
You look good for 76, man.

460
00:27:15,000 --> 00:27:16,000
That's great.

461
00:27:16,000 --> 00:27:17,000
So it's doing something.

462
00:27:17,000 --> 00:27:18,000
Yeah.

463
00:27:18,000 --> 00:27:20,000
I think it's working.

464
00:27:20,000 --> 00:27:31,000
And so your goal is to get to that point where they start doing, you live a year, you stay static, and then eventually get back to youthfulness.

465
00:27:31,000 --> 00:27:32,000
Right.

466
00:27:32,000 --> 00:27:33,000
And it's not that far off.

467
00:27:33,000 --> 00:27:38,000
If you're diligent, I think we'll get there by 2029.

468
00:27:38,000 --> 00:27:40,000
Now, not everybody's diligent.

469
00:27:40,000 --> 00:27:41,000
Right.

470
00:27:41,000 --> 00:27:42,000
Of course.

471
00:27:42,000 --> 00:27:46,000
Now, past that, this is for life extension, which is great.

472
00:27:46,000 --> 00:27:51,000
But what about how AI is going to change society?

473
00:27:51,000 --> 00:27:52,000
Yes.

474
00:27:52,000 --> 00:27:54,000
Well, that's a very big issue.

475
00:27:54,000 --> 00:28:01,000
And it's already doing lots of things, makes some people uncomfortable.

476
00:28:01,000 --> 00:28:05,000
What we're actually doing is increasing our intelligence.

477
00:28:05,000 --> 00:28:07,000
I mean, right now you have a brain.

478
00:28:07,000 --> 00:28:12,000
It has different modules in it to deal with different things.

479
00:28:12,000 --> 00:28:17,000
But really, it's able to connect one concept to another concept.

480
00:28:17,000 --> 00:28:20,000
And that's what your brain does.

481
00:28:20,000 --> 00:28:24,000
We can actually increase that by, for example, carrying around a phone.

482
00:28:24,000 --> 00:28:26,000
This has connections in it.

483
00:28:26,000 --> 00:28:29,000
It's a little bit of a hassle to use.

484
00:28:29,000 --> 00:28:33,000
If I ask you to do something, you've got to kind of mess with it.

485
00:28:33,000 --> 00:28:37,000
Actually, it'd be good if this actually listened to your conversation.

486
00:28:37,000 --> 00:28:38,000
Oh, it does.

487
00:28:38,000 --> 00:28:42,000
And without saying anything, you're just talking.

488
00:28:42,000 --> 00:28:45,000
And it says, oh, the name of that actress is so-and-so.

489
00:28:45,000 --> 00:28:47,000
Yeah, but then it's a busy body.

490
00:28:47,000 --> 00:28:50,000
It's like interfering with your life, talking to you all the time.

491
00:28:50,000 --> 00:28:52,000
Well, there's ways of dealing with that, too.

492
00:28:52,000 --> 00:28:53,000
You shut it off.

493
00:28:53,000 --> 00:28:58,000
So we haven't done that yet.

494
00:28:58,000 --> 00:29:04,000
But that's a way of expanding your connections.

495
00:29:04,000 --> 00:29:12,000
What a large language model does, it has connections in it as well.

496
00:29:12,000 --> 00:29:19,000
And the fact that it's getting now to a point that's getting fairly comparable to the human brain,

497
00:29:19,000 --> 00:29:23,000
we have about a trillion connections in our brain.

498
00:29:23,000 --> 00:29:30,000
Things like the top model from Google or GPT-4,

499
00:29:30,000 --> 00:29:38,000
they have about 400 billion connections approximately.

500
00:29:38,000 --> 00:29:41,000
They'll be at a trillion probably within a year.

501
00:29:41,000 --> 00:29:45,000
That's pretty comparable to what the human brain does.

502
00:29:45,000 --> 00:29:50,000
Eventually, it'll go beyond that and will have access to that.

503
00:29:50,000 --> 00:29:53,000
So it's basically making us smarter.

504
00:29:53,000 --> 00:30:05,000
So if you have the ability to be smarter, that's something that's positive, really.

505
00:30:05,000 --> 00:30:15,000
I mean, if we were like mice today and we had the opportunity to become like humans,

506
00:30:15,000 --> 00:30:17,000
we wouldn't object to that.

507
00:30:17,000 --> 00:30:20,000
In fact, we are humans and we don't object to that.

508
00:30:20,000 --> 00:30:24,000
It used to be shrews.

509
00:30:24,000 --> 00:30:28,000
And this is going to basically make us smarter.

510
00:30:28,000 --> 00:30:32,000
Eventually, we'll be much smarter than we are today.

511
00:30:32,000 --> 00:30:34,000
And that's a positive thing.

512
00:30:34,000 --> 00:30:46,000
We'll be able to do things that are today that we find bothersome in a way that's much more palatable.

513
00:30:46,000 --> 00:30:49,000
The idea of us getting smarter sounds great.

514
00:30:49,000 --> 00:30:51,000
It'd be great to be smarter.

515
00:30:51,000 --> 00:30:56,000
Right, but people object to that because it's like competition.

516
00:30:56,000 --> 00:30:58,000
In what way?

517
00:30:58,000 --> 00:31:05,000
Well, I mean, Google has, I don't know, 60,000, 70,000 programmers.

518
00:31:05,000 --> 00:31:09,000
How many programmers exist in the world?

519
00:31:09,000 --> 00:31:13,000
How much longer is that going to be a viable career?

520
00:31:13,000 --> 00:31:18,000
Because language models already can code.

521
00:31:18,000 --> 00:31:25,000
Not quite as good as a real expert coder, but how long is that going to be?

522
00:31:25,000 --> 00:31:27,000
It's not going to be 100 years.

523
00:31:27,000 --> 00:31:30,000
It's going to be a few years.

524
00:31:30,000 --> 00:31:34,000
So people see it as competition.

525
00:31:34,000 --> 00:31:36,000
I have a slightly different view of that.

526
00:31:36,000 --> 00:31:41,000
I see these things as actually adding to our own intelligence.

527
00:31:41,000 --> 00:31:47,000
And we're merging with these kinds of computers and making ourselves smarter

528
00:31:47,000 --> 00:31:49,000
by merging with it.

529
00:31:49,000 --> 00:31:56,000
And eventually it'll go inside our brain and be able to make us smarter instantly

530
00:31:56,000 --> 00:32:01,000
just like we had more connections inside our own brain.

531
00:32:01,000 --> 00:32:05,000
Well, I think people have reservations always when it comes to great change.

532
00:32:05,000 --> 00:32:07,000
And this is probably the greatest change.

533
00:32:07,000 --> 00:32:11,000
The greatest change we've ever experienced in our lifetimes for sure has been the internet.

534
00:32:11,000 --> 00:32:15,000
And this will make that look like nothing.

535
00:32:15,000 --> 00:32:17,000
It'll change everything.

536
00:32:17,000 --> 00:32:20,000
And it seems inevitable.

537
00:32:20,000 --> 00:32:27,000
I understand that people are upset about it, but it just seems like what human beings were sort of designed to do.

538
00:32:27,000 --> 00:32:31,000
Right. We're the only animal that actually creates technology.

539
00:32:31,000 --> 00:32:36,000
It's a combination of our brain and something else, which is our thumb.

540
00:32:36,000 --> 00:32:38,000
So I can imagine something.

541
00:32:38,000 --> 00:32:45,000
Oh, if I take that leaf from a tree, I could create a tool with it.

542
00:32:45,000 --> 00:32:55,000
Other animals have actually a bigger brain like the whale, dolphins, elephants.

543
00:32:55,000 --> 00:33:00,000
They have a larger brain than we do, but they don't have something equivalent to the thumb.

544
00:33:00,000 --> 00:33:06,000
Monkey has a thing that looks like the thumb, but it's actually an inch down and it doesn't actually work very well.

545
00:33:06,000 --> 00:33:13,000
So they can actually create a tool, but they don't create a tool that's powerful enough to create the next tool.

546
00:33:13,000 --> 00:33:22,000
So we're actually able to use our tools and create something that's that much more significant.

547
00:33:22,000 --> 00:33:28,000
So we can create tools, and that's really part of who we are.

548
00:33:28,000 --> 00:33:35,000
It makes us that much more intelligent, and that's a good thing.

549
00:33:35,000 --> 00:33:50,000
I mean, here's...

550
00:33:50,000 --> 00:33:53,000
So here's U.S. personal income per capita.

551
00:33:53,000 --> 00:34:02,000
So this is the average amount that we make per person in constant dollars.

552
00:34:02,000 --> 00:34:05,000
Right here, it's on the screen.

553
00:34:05,000 --> 00:34:09,000
Do we make a lot more money, but things cost a lot more money too, right?

554
00:34:09,000 --> 00:34:11,000
No, this is constant dollars.

555
00:34:11,000 --> 00:34:14,000
Constant dollars in relation to the inflation?

556
00:34:14,000 --> 00:34:17,000
Yeah, so this does not show you inflation.

557
00:34:17,000 --> 00:34:20,000
These are constant dollars.

558
00:34:20,000 --> 00:34:26,000
So we're actually making that much more each year on average.

559
00:34:26,000 --> 00:34:29,000
Right, but it doesn't take into account inflation, correct?

560
00:34:29,000 --> 00:34:32,000
So it's not taking into account the rise of cost of things.

561
00:34:32,000 --> 00:34:34,000
No, it is taking that.

562
00:34:34,000 --> 00:34:36,000
Oh, it is, okay.

563
00:34:36,000 --> 00:34:40,000
So we're making that much more in constant dollars.

564
00:34:40,000 --> 00:34:46,000
If you look over the past hundred years, we've made about ten times as much.

565
00:34:46,000 --> 00:34:52,000
I wonder if there's a similar chart about consumerism, just about material possessions.

566
00:34:52,000 --> 00:34:55,000
I wonder if, like, how much more we're purchasing and creating.

567
00:34:55,000 --> 00:35:01,000
I've always felt like that's one of the things that materialism is one of those instincts

568
00:35:01,000 --> 00:35:09,000
that human beings sort of look down upon and this aimless pursuit of buying things.

569
00:35:09,000 --> 00:35:16,000
But I feel like that motivates technology because the constant need for the newest,

570
00:35:16,000 --> 00:35:22,000
greatest thing is one of the things that fuels the creation and innovation of new things.

571
00:35:22,000 --> 00:35:25,000
But if you were to go back a hundred years, you'd be very unhappy.

572
00:35:25,000 --> 00:35:26,000
Oh, yeah.

573
00:35:26,000 --> 00:35:30,000
Because you wouldn't have, I mean, you wouldn't have a computer, for example.

574
00:35:30,000 --> 00:35:32,000
You wouldn't have anything.

575
00:35:32,000 --> 00:35:34,000
You'd have most things you've grown accustomed to.

576
00:35:34,000 --> 00:35:35,000
Yeah.

577
00:35:35,000 --> 00:35:39,000
I mean, unless that's why you wanted that.

578
00:35:39,000 --> 00:35:42,000
Also, we didn't live very long.

579
00:35:42,000 --> 00:35:44,000
Right, medical advancements.

580
00:35:44,000 --> 00:35:52,000
At average, life was 48 years in 1900.

581
00:35:52,000 --> 00:35:54,000
It's 35 years in 1800.

582
00:35:54,000 --> 00:35:55,000
Right.

583
00:35:55,000 --> 00:35:58,000
Go back a thousand years, it was 20 years.

584
00:35:58,000 --> 00:36:01,000
That takes into account child mortality, too, though, right?

585
00:36:01,000 --> 00:36:04,000
But it's also injuries, death.

586
00:36:04,000 --> 00:36:06,000
Some people did live long.

587
00:36:06,000 --> 00:36:08,000
There was people that lived back then.

588
00:36:08,000 --> 00:36:11,000
If nothing happened to you, you did live to be 80, like a normal person.

589
00:36:11,000 --> 00:36:14,000
But that was actually very rare.

590
00:36:14,000 --> 00:36:16,000
Because most things happen to people.

591
00:36:16,000 --> 00:36:19,000
Most people, by the time you get to 80, you've had at least one hospital visit.

592
00:36:19,000 --> 00:36:20,000
Something's gone wrong.

593
00:36:20,000 --> 00:36:23,000
Broken arm, broken this, broken that.

594
00:36:23,000 --> 00:36:26,000
It was very rare to make it to 80.

595
00:36:26,000 --> 00:36:27,000
Right.

596
00:36:27,000 --> 00:36:28,000
200 years ago.

597
00:36:28,000 --> 00:36:31,000
Right, but the human body was physically capable of doing it.

598
00:36:31,000 --> 00:36:32,000
Right.

599
00:36:32,000 --> 00:36:40,000
Well, our human body can go on forever if you fix things properly.

600
00:36:40,000 --> 00:36:46,000
There's nothing in our body that means that you have to die at 100 or even 120.

601
00:36:46,000 --> 00:36:50,000
We can go on really indefinitely.

602
00:36:50,000 --> 00:36:52,000
Well, that's the groundbreaking work today, right?

603
00:36:52,000 --> 00:36:57,000
They're treating disease or, excuse me, age as if it is a disease,

604
00:36:57,000 --> 00:36:59,000
not just inevitable consequences.

605
00:36:59,000 --> 00:37:00,000
Right.

606
00:37:00,000 --> 00:37:04,000
And our FDA doesn't accept that, but they're actually beginning to accept it now.

607
00:37:04,000 --> 00:37:05,000
Well, as they get older.

608
00:37:05,000 --> 00:37:07,000
Yeah, exactly.

609
00:37:07,000 --> 00:37:09,000
They're forced into it.

610
00:37:10,000 --> 00:37:15,000
The concept of artificial general intelligence scares a lot of people also because of Hollywood, right?

611
00:37:15,000 --> 00:37:18,000
Because of the Terminator films and things along those lines.

612
00:37:18,000 --> 00:37:25,000
Like, how far away are we, do you think, from actual artificial humans or will we ever get there?

613
00:37:25,000 --> 00:37:28,000
Will we integrate before that takes place?

614
00:37:28,000 --> 00:37:36,000
I mean, all of this additional intelligence that we're creating is something that we use.

615
00:37:36,000 --> 00:37:39,000
And it's just like it came with us.

616
00:37:39,000 --> 00:37:45,000
So we're actually making ourselves more intelligent and ultimately that's a good thing.

617
00:37:45,000 --> 00:37:49,000
And if we have it and then we say, well, gee, we don't really like this.

618
00:37:49,000 --> 00:37:51,000
Let's take it away.

619
00:37:51,000 --> 00:37:53,000
People would never accept that.

620
00:37:53,000 --> 00:38:02,000
They may be against the idea of general intelligence, but once they get it, nobody wants to give that up.

621
00:38:02,000 --> 00:38:07,000
And it will be beneficial.

622
00:38:11,000 --> 00:38:22,000
The Blue Knight started 200 years ago because the cotton genie come out and all these people that were making money with the cotton genie were against it.

623
00:38:22,000 --> 00:38:26,000
And they would actually destroy these machines at night.

624
00:38:27,000 --> 00:38:32,000
And they said, gee, if this keeps going, all jobs are going to go away.

625
00:38:32,000 --> 00:38:39,000
And indeed, people using the cotton genie to create more wealth, that did go away.

626
00:38:39,000 --> 00:38:44,000
But we actually made more money because we created things that didn't exist then.

627
00:38:44,000 --> 00:38:47,000
We didn't have anything like electronics, for example.

628
00:38:47,000 --> 00:38:59,000
And as we can actually see, we make 10 times as much in constant dollars as we did 100 years ago.

629
00:38:59,000 --> 00:39:04,000
And if you were to ask, well, what are people going to be doing?

630
00:39:04,000 --> 00:39:11,000
You couldn't answer it because we didn't understand the internet, for example.

631
00:39:11,000 --> 00:39:16,000
And there's probably some technologies down the pipe that are going to have a similar impact.

632
00:39:16,000 --> 00:39:17,000
Exactly.

633
00:39:17,000 --> 00:39:20,000
And they're going to extend life, for example.

634
00:39:20,000 --> 00:39:22,000
But are they going to create life?

635
00:39:25,000 --> 00:39:32,000
Well, we know how to create life.

636
00:39:40,000 --> 00:39:42,000
Well, that's an interesting question.

637
00:39:43,000 --> 00:39:46,000
What do you mean by create life?

638
00:39:46,000 --> 00:39:55,000
What I think is that human beings are some sort of a biological caterpillar that makes a cocoon that gives birth to an electronic butterfly.

639
00:39:55,000 --> 00:40:01,000
I think we are creating a life form and that we're merely conduits for this thing.

640
00:40:01,000 --> 00:40:08,000
And that all of our instincts and ego and emotions and all these things feed into it, materialism feeds into it.

641
00:40:08,000 --> 00:40:14,000
We keep buying and keep innovating and technology keeps increasing exponentially.

642
00:40:14,000 --> 00:40:20,000
And eventually it's going to be artificial intelligence and artificial intelligence is going to create better artificial intelligence

643
00:40:20,000 --> 00:40:29,000
and a form of being that has no limitations in terms of what's capable of doing and capable of traveling anywhere

644
00:40:29,000 --> 00:40:31,000
and not having any biological limitations in terms of...

645
00:40:31,000 --> 00:40:33,000
But that's going to be ourselves.

646
00:40:33,000 --> 00:40:41,000
We're going to be able to create life that is like humans but far greater than we are today.

647
00:40:41,000 --> 00:40:43,000
With an integration of technology.

648
00:40:43,000 --> 00:40:44,000
Yeah.

649
00:40:44,000 --> 00:40:46,000
If we choose to go that route.

650
00:40:46,000 --> 00:40:49,000
But that's the prediction that you have, that we will go that route.

651
00:40:49,000 --> 00:40:52,000
Like a neural link type deal, something along those lines.

652
00:40:52,000 --> 00:40:53,000
Right.

653
00:40:53,000 --> 00:40:56,000
So I don't see this competition like the things are going to...

654
00:40:56,000 --> 00:40:58,000
No, I don't think it's competition.

655
00:40:58,000 --> 00:41:00,000
Well, it will seem like that.

656
00:41:00,000 --> 00:41:06,000
I mean, if you have a job doing coding and suddenly they don't really want you anymore

657
00:41:06,000 --> 00:41:11,000
because they can do coding with a large language model, it's going to feel like it's competition.

658
00:41:11,000 --> 00:41:13,000
Well, there's an issue now with films.

659
00:41:13,000 --> 00:41:21,000
Tyler Perry, who owns and he was building an $800 million television studio and he stopped production.

660
00:41:21,000 --> 00:41:23,000
What is it called, Sora?

661
00:41:23,000 --> 00:41:25,000
Is that what it's called, Jamie?

662
00:41:25,000 --> 00:41:34,000
He stopped production when he saw the capabilities of AI just for creating visuals, scenes, movies.

663
00:41:34,000 --> 00:41:37,000
There's one that's incredibly impressive.

664
00:41:37,000 --> 00:41:38,000
It's Tokyo.

665
00:41:38,000 --> 00:41:41,000
They're walking down the street of Tokyo in the winter.

666
00:41:41,000 --> 00:41:46,000
So it's snowing and they're walking down the street and you look at it, you're like, this is insane.

667
00:41:46,000 --> 00:41:48,000
This looks like a film.

668
00:41:48,000 --> 00:41:51,000
See if you can find that film because it's incredible.

669
00:41:51,000 --> 00:41:53,000
But would you want to get rid of that?

670
00:41:53,000 --> 00:41:54,000
Get rid of what?

671
00:41:54,000 --> 00:41:56,000
The capability.

672
00:41:56,000 --> 00:41:58,000
No, I don't want to get rid of the capability.

673
00:41:58,000 --> 00:42:01,000
But people do want to get rid of it.

674
00:42:01,000 --> 00:42:08,000
People that make movies, people that actually film things with cameras and use actors are going to be very upset.

675
00:42:08,000 --> 00:42:12,000
So this, this is all fake, which is insane.

676
00:42:12,000 --> 00:42:14,000
Beautiful snowy Tokyo city is bustling.

677
00:42:14,000 --> 00:42:19,000
The camera moves through the bustling city street, following several people enjoying the beautiful snowy weather

678
00:42:19,000 --> 00:42:21,000
and shopping at nearby stalls.

679
00:42:21,000 --> 00:42:25,000
Gorgeous Sakura petals are flying through the wind along with snowflakes.

680
00:42:25,000 --> 00:42:27,000
And this is what you get.

681
00:42:27,000 --> 00:42:30,000
I mean, this is insanely good.

682
00:42:30,000 --> 00:42:33,000
The variability, like just the way people are dressed.

683
00:42:33,000 --> 00:42:39,000
If you saw this somewhere else, look at this, a robot's life in a cyberpunk setting.

684
00:42:39,000 --> 00:42:44,000
If you saw this, you would say, oh, they filmed this.

685
00:42:44,000 --> 00:42:48,000
But just look at what they're able to do with animation and kids movies and things along those lines.

686
00:42:48,000 --> 00:42:50,000
Yeah, and it's going to get better.

687
00:42:50,000 --> 00:42:52,000
Yeah, it's just incredible.

688
00:42:52,000 --> 00:42:55,000
I mean, it's a new art form.

689
00:42:55,000 --> 00:42:59,000
So right there, the smoke looks a little uniform, but yeah.

690
00:42:59,000 --> 00:43:01,000
I mean, there's some problems with this, but...

691
00:43:01,000 --> 00:43:02,000
Not much.

692
00:43:02,000 --> 00:43:03,000
Yeah.

693
00:43:03,000 --> 00:43:07,000
And you imagine what it was like five years ago and then imagine what it's going to be like five years from now.

694
00:43:07,000 --> 00:43:09,000
Yeah, absolutely.

695
00:43:09,000 --> 00:43:10,000
And it's insane.

696
00:43:10,000 --> 00:43:16,000
I mean, no one took into consideration the idea that kids are going to be cheating on their school papers using chat GPT.

697
00:43:16,000 --> 00:43:20,000
But my kids tell me that's a real problem in school now.

698
00:43:20,000 --> 00:43:24,000
Yes, definitely.

699
00:43:24,000 --> 00:43:31,000
So no one saw that coming, no one saw this coming, and what we're at now is with chat GPT 4, right?

700
00:43:31,000 --> 00:43:32,000
4.5?

701
00:43:32,000 --> 00:43:33,000
Is that what it is?

702
00:43:33,000 --> 00:43:34,000
Well, 4.5 is coming.

703
00:43:34,000 --> 00:43:35,000
4.5 is coming.

704
00:43:35,000 --> 00:43:40,000
5 is supposed to be the massive leap.

705
00:43:40,000 --> 00:43:41,000
It'll be a leap.

706
00:43:41,000 --> 00:43:44,000
Just like 3 to 4 was a massive leap.

707
00:43:44,000 --> 00:43:45,000
Yeah.

708
00:43:45,000 --> 00:43:47,000
It's going to continue.

709
00:43:47,000 --> 00:43:49,000
It's never going to be finished.

710
00:43:49,000 --> 00:43:50,000
Right.

711
00:43:50,000 --> 00:43:51,000
It'll keep going.

712
00:43:51,000 --> 00:43:54,000
And it will also be able to make better versions of itself, correct?

713
00:43:54,000 --> 00:43:57,000
And yes, well, we do that.

714
00:43:57,000 --> 00:43:59,000
I mean, technology does that already.

715
00:43:59,000 --> 00:44:00,000
Right.

716
00:44:00,000 --> 00:44:04,000
But if you scale that out 100 years from now, what are you looking at?

717
00:44:04,000 --> 00:44:06,000
You're looking at a God.

718
00:44:06,000 --> 00:44:09,000
Well, it'll be less than 100 years.

719
00:44:09,000 --> 00:44:13,000
So you're looking at a God in 50 years?

720
00:44:13,000 --> 00:44:14,000
Less than that.

721
00:44:14,000 --> 00:44:20,000
I mean, once we have an ability to emulate everything that humans can do, and not just

722
00:44:20,000 --> 00:44:24,000
one human, but all humans, and that's only like 2029.

723
00:44:24,000 --> 00:44:27,000
That's only five years from now.

724
00:44:27,000 --> 00:44:30,000
And then it will make better versions of that.

725
00:44:30,000 --> 00:44:35,000
So it will probably solve a lot of the problems that we have in terms of energy storage, data

726
00:44:35,000 --> 00:44:38,000
storage, data speeds, computation speeds.

727
00:44:38,000 --> 00:44:41,000
And also medications.

728
00:44:41,000 --> 00:44:42,000
For us.

729
00:44:42,000 --> 00:44:43,000
For humans, yeah.

730
00:44:43,000 --> 00:44:48,000
Wouldn't it be better just, Ray, just download yourself into this beautiful electronic body?

731
00:44:48,000 --> 00:44:51,000
Why do you want to be biological?

732
00:44:51,000 --> 00:44:56,000
I mean, ultimately, that's what we're going to be able to do.

733
00:44:56,000 --> 00:44:58,000
You think that's going to happen?

734
00:44:58,000 --> 00:44:59,000
Yeah.

735
00:44:59,000 --> 00:45:01,000
So do you think that we'll be able to...

736
00:45:01,000 --> 00:45:04,000
I mean, we'll be able to create...

737
00:45:04,000 --> 00:45:10,000
I mean, the singularity is when we multiply our intelligence a million fold, and that's 2045.

738
00:45:10,000 --> 00:45:12,000
So that's not that long from now.

739
00:45:12,000 --> 00:45:14,000
That's like 20 years from now.

740
00:45:14,000 --> 00:45:16,000
Right.

741
00:45:16,000 --> 00:45:27,000
And therefore, most of your intelligence will be handled by the computer part of ourselves.

742
00:45:27,000 --> 00:45:33,000
The only thing that won't be captured is what comes with our body originally.

743
00:45:33,000 --> 00:45:36,000
We'll ultimately be able to do that as well.

744
00:45:36,000 --> 00:45:47,000
We'll take a little longer, but we'll be able to actually capture what comes with our normal body and be able to recreate that.

745
00:45:47,000 --> 00:45:56,000
So that also has to do with how long we live, because if everything is backed up...

746
00:45:56,000 --> 00:46:02,000
I mean, right now, anytime you put anything into a phone or any kind of electronics, it's backed up.

747
00:46:02,000 --> 00:46:05,000
So, I mean, this has a lot of data.

748
00:46:05,000 --> 00:46:12,000
I could flip it, and it ends up in a river, and we can't capture anymore.

749
00:46:12,000 --> 00:46:15,000
I can recreate it, because it's all backed up.

750
00:46:15,000 --> 00:46:18,000
And you think that's going to be the case with consciousness?

751
00:46:18,000 --> 00:46:23,000
That's going to be the case of our normal biological body as well.

752
00:46:23,000 --> 00:46:29,000
What's to stop someone like Donald Trump from just making 100,000 versions of himself?

753
00:46:29,000 --> 00:46:33,000
Like, if you can back someone up, could you duplicate it?

754
00:46:33,000 --> 00:46:35,000
Couldn't you have three or four of them?

755
00:46:35,000 --> 00:46:36,000
Couldn't you have a bunch of them?

756
00:46:36,000 --> 00:46:38,000
Couldn't you live multiple lives?

757
00:46:38,000 --> 00:46:40,000
Yes.

758
00:46:40,000 --> 00:46:46,000
Would you be interacting with each other while you're living multiple lives, having consultations about what is St. Louis Ray doing?

759
00:46:46,000 --> 00:46:48,000
Well, I don't know. Let's talk to San Francisco Ray.

760
00:46:48,000 --> 00:46:51,000
San Francisco Ray is talking to Florida Ray.

761
00:46:52,000 --> 00:47:02,000
It's basically a matter of increasing our intelligence and being able to multiply Donald Trump, for example, that comes with that.

762
00:47:02,000 --> 00:47:08,000
Do you think there'll be regulations on that to stop people from making 100,000 versions of themselves that operate a city?

763
00:47:08,000 --> 00:47:12,000
There'll be lots of regulations. There's lots of regulations we have already.

764
00:47:12,000 --> 00:47:17,000
You can't just create a medication and sell it to people that cares its disease.

765
00:47:17,000 --> 00:47:18,000
Right.

766
00:47:18,000 --> 00:47:20,000
We have a tremendous amount of regulations.

767
00:47:20,000 --> 00:47:27,000
Sure, but we don't really with phones. With your phone, essentially, if you had the money, you could make as many copies of that as you wanted.

768
00:47:27,000 --> 00:47:36,000
Yes. There are some regulations. We regulate everything, but you're right.

769
00:47:36,000 --> 00:47:43,000
Generally, electronics doesn't have as much regulation as...

770
00:47:43,000 --> 00:47:48,000
Right. And when you get to a certain point, we will be electronics.

771
00:47:48,000 --> 00:48:01,000
Yes. Certainly, if we multiply our intelligence a million fold, everything of that additional million fold of yours is not regulated.

772
00:48:01,000 --> 00:48:14,000
Right. When you think about the concept of integration and technological integration, when do you think that will start taking place and what will be the initial usage of it?

773
00:48:14,000 --> 00:48:20,000
Like, what will be the first versions and what would they provide?

774
00:48:20,000 --> 00:48:23,000
Well, we have it now. Large language models are pretty impressive.

775
00:48:23,000 --> 00:48:25,000
I mean, if you look at what they can do...

776
00:48:25,000 --> 00:48:31,000
I mean, I'm talking about physical integration with the human body, like a Neuralink type thing.

777
00:48:31,000 --> 00:48:42,000
Right. Some people feel that we could actually understand what's going on in your brain and actually put things into your brain without actually going into the brain with something like Neuralink.

778
00:48:42,000 --> 00:48:45,000
So something that sits on the outside of your head?

779
00:48:45,000 --> 00:48:53,000
Yeah. It's not clear to me if that's feasible or not. I've been assuming that you have to actually go in.

780
00:48:53,000 --> 00:49:04,000
Now, Neuralink isn't exactly what we want because it's too slow and it actually will do what it's advertised to do.

781
00:49:05,000 --> 00:49:20,000
I actually know some people like this who were active people and they completely lost the ability to speak and to understand language and so on.

782
00:49:20,000 --> 00:49:31,000
So they can't actually say anything to you and we can use something like Neuralink to actually have them express something.

783
00:49:31,000 --> 00:49:35,000
They can think something and then have it be expressed to you.

784
00:49:35,000 --> 00:49:39,000
Right. And they're doing that, right? They had the first patient. The first patient that was...

785
00:49:39,000 --> 00:49:40,000
Yeah.

786
00:49:40,000 --> 00:49:44,000
Yeah. And apparently that person can move a cursor around on a screen.

787
00:49:44,000 --> 00:49:50,000
Right. And therefore you can do anything. It's fairly slow, though. And Neuralink is slow.

788
00:49:50,000 --> 00:49:55,000
And if you really want to extend your brain, you need to do it at a much faster pace.

789
00:49:55,000 --> 00:49:57,000
But isn't that going to increase exponentially as well?

790
00:49:57,000 --> 00:49:58,000
Yes, absolutely.

791
00:49:58,000 --> 00:50:02,000
So how long do you think it will be before it's implemented?

792
00:50:02,000 --> 00:50:19,000
Well, it's got to be by 2045 because that's when the singularity exists and we can actually multiply our intelligence on the order of a million fold.

793
00:50:19,000 --> 00:50:29,000
And when you say 2045, what is the source of that estimation?

794
00:50:29,000 --> 00:50:46,000
Because we'll be able to, based actually on this chart and also the increase in the ability of software to also expand,

795
00:50:46,000 --> 00:50:56,000
we'll be able to multiply our intelligence a million fold and we'll be able to put that inside of our brain.

796
00:50:56,000 --> 00:50:59,000
It would be just like it's part of our brain.

797
00:50:59,000 --> 00:51:02,000
So this is just following the current graph of progress?

798
00:51:02,000 --> 00:51:03,000
Yeah, exactly.

799
00:51:03,000 --> 00:51:07,000
So if you follow the current graph of progress and if you do understand exponential growth,

800
00:51:07,000 --> 00:51:11,000
then what we're looking at in 2045 is inevitable.

801
00:51:11,000 --> 00:51:13,000
Right.

802
00:51:13,000 --> 00:51:16,000
Does that concern you at all or are you excited about it?

803
00:51:16,000 --> 00:51:22,000
Do you think it's just a thing that is happening and you're a part of it and you're experiencing it?

804
00:51:22,000 --> 00:51:30,000
I think it will be enthusiastic about it.

805
00:51:30,000 --> 00:51:38,000
I mean, imagine if you were to ask a mouse, would you like to actually be as intelligent as a human?

806
00:51:38,000 --> 00:51:41,000
Right.

807
00:51:41,000 --> 00:51:45,000
It's hard to know what people would say, but generally that's a positive thing.

808
00:51:45,000 --> 00:51:46,000
Generally.

809
00:51:46,000 --> 00:51:47,000
Yeah.

810
00:51:47,000 --> 00:51:49,000
And that's what it's going to be like.

811
00:51:49,000 --> 00:51:51,000
We're going to be that much smarter.

812
00:51:51,000 --> 00:51:52,000
And what do you...

813
00:51:52,000 --> 00:51:56,000
And once we're there, is someone going to say, no, I don't really like this.

814
00:51:56,000 --> 00:52:02,000
I want to be stupid like human beings used to be.

815
00:52:02,000 --> 00:52:05,000
Nobody's really going to say that.

816
00:52:05,000 --> 00:52:08,000
Do human beings now say, gee, I'm really too smart.

817
00:52:08,000 --> 00:52:11,000
I'd really like to be like a mouse.

818
00:52:11,000 --> 00:52:18,000
Not necessarily, but what people do say is that technology is too invasive and that it's too much a part of my life.

819
00:52:18,000 --> 00:52:24,000
And I'd like to sort of have a bit of an electronic vacation and separate from it.

820
00:52:24,000 --> 00:52:27,000
And there's a lot of people that I know that have gone to...

821
00:52:27,000 --> 00:52:28,000
But nobody does that.

822
00:52:28,000 --> 00:52:35,000
I mean, nobody becomes stupid like we used to be when we were mice.

823
00:52:35,000 --> 00:52:36,000
Right.

824
00:52:36,000 --> 00:52:37,000
But I'm not saying stupid.

825
00:52:37,000 --> 00:52:41,000
I'm saying some people, just like being a human, the way humans are now.

826
00:52:41,000 --> 00:52:46,000
Because one of the complications that comes with the integration of technology is what we're seeing now with people.

827
00:52:46,000 --> 00:52:51,000
Massive increases in anxiety from social media use, being manipulated by algorithms,

828
00:52:51,000 --> 00:52:56,000
the effect that it has on culture, misinformation and disinformation and propaganda.

829
00:52:56,000 --> 00:53:05,000
There's so many different factors that are at play now that make people more anxious and more depressed statistically than ever.

830
00:53:05,000 --> 00:53:13,000
I'm not sure we had more anxiety today than we used to have.

831
00:53:13,000 --> 00:53:16,000
Well, we certainly had more when the Mongols were invading.

832
00:53:16,000 --> 00:53:20,000
We certainly had more anxiety when we were worried constantly about war.

833
00:53:20,000 --> 00:53:23,000
But I think people have a pretty heightened level of social anxiety.

834
00:53:23,000 --> 00:53:24,000
Well, they take war.

835
00:53:24,000 --> 00:53:32,000
I mean, 80 years ago we had 100 million people die in Europe and Asia from World War II.

836
00:53:32,000 --> 00:53:38,000
We're very concerned about wars today and they're terrible.

837
00:53:38,000 --> 00:53:42,000
But we're not losing millions of people.

838
00:53:42,000 --> 00:53:43,000
Right.

839
00:53:43,000 --> 00:53:44,000
But we could.

840
00:53:44,000 --> 00:53:45,000
We most certainly could.

841
00:53:45,000 --> 00:53:51,000
With what's going on with Israel and Gaza, what's going on with Ukraine and Russia,

842
00:53:51,000 --> 00:53:53,000
it could easily escalate with...

843
00:53:53,000 --> 00:53:54,000
It's thousands of people.

844
00:53:54,000 --> 00:53:56,000
It's not millions of people.

845
00:53:56,000 --> 00:53:57,000
For now.

846
00:53:57,000 --> 00:53:58,000
Yeah.

847
00:53:58,000 --> 00:54:03,000
But if it escalates to a hot war where it's involving the entire world.

848
00:54:03,000 --> 00:54:10,000
What would really cause a tremendous amount of danger is something that's not really artificial intelligence.

849
00:54:10,000 --> 00:54:15,000
It was invented when I was a child, which is atomic weapons.

850
00:54:15,000 --> 00:54:16,000
Right.

851
00:54:16,000 --> 00:54:24,000
I remember when I was five or six, we'd actually go outside, put our hands behind our back

852
00:54:24,000 --> 00:54:27,000
to protect us from a nuclear war.

853
00:54:27,000 --> 00:54:28,000
Yeah, drills.

854
00:54:28,000 --> 00:54:30,000
And it seemed to work.

855
00:54:30,000 --> 00:54:31,000
We're still here.

856
00:54:31,000 --> 00:54:35,000
Do you remember those things where they tell kids to get under the desk?

857
00:54:35,000 --> 00:54:36,000
Yes, that's right.

858
00:54:36,000 --> 00:54:38,000
We went under the desk and put out...

859
00:54:38,000 --> 00:54:42,000
Which is hilarious, as if a desk is going to protect you from a nuclear bomb.

860
00:54:42,000 --> 00:54:43,000
Right.

861
00:54:43,000 --> 00:54:45,000
But that's not AI.

862
00:54:45,000 --> 00:54:46,000
Right.

863
00:54:46,000 --> 00:54:50,000
No, but AI applied to nuclear weapons makes them significantly more dangerous.

864
00:54:50,000 --> 00:54:55,000
And isn't one of the problems with AI is that AI will find a solution to a problem.

865
00:54:55,000 --> 00:55:01,000
So if you have AI running your military and AI says, what do you want me to do?

866
00:55:01,000 --> 00:55:04,000
And you say, well, I'd like to take over Taiwan.

867
00:55:04,000 --> 00:55:06,000
And AI says, well, this is how to do it.

868
00:55:06,000 --> 00:55:09,000
And it just implements it with no morals.

869
00:55:09,000 --> 00:55:18,000
No thought of any sort of diplomacy or just force.

870
00:55:18,000 --> 00:55:19,000
Right.

871
00:55:19,000 --> 00:55:21,000
It hasn't happened yet.

872
00:55:21,000 --> 00:55:26,000
Because we do have people in charge and the people are enhanced with AI.

873
00:55:26,000 --> 00:55:32,000
And AI can actually help us to avoid that kind of problem by thinking through the implications

874
00:55:32,000 --> 00:55:35,000
of different solutions.

875
00:55:35,000 --> 00:55:36,000
Sure.

876
00:55:36,000 --> 00:55:39,000
If it has some sort of autonomy.

877
00:55:39,000 --> 00:55:44,000
But if we get to the point where one superpower has AI, artificial general intelligence,

878
00:55:44,000 --> 00:55:51,000
the other one doesn't, how much of a significant advantage would that be?

879
00:55:51,000 --> 00:55:53,000
I mean, I do think there are problems.

880
00:55:53,000 --> 00:55:56,000
Basically, there's problems with intelligence.

881
00:55:56,000 --> 00:56:04,000
And we'd like to say stupid.

882
00:56:04,000 --> 00:56:08,000
But actually, it's better to be intelligent.

883
00:56:08,000 --> 00:56:11,000
I believe it's better to have intelligence.

884
00:56:11,000 --> 00:56:12,000
Overall.

885
00:56:12,000 --> 00:56:13,000
Sure.

886
00:56:13,000 --> 00:56:14,000
Right.

887
00:56:14,000 --> 00:56:21,000
But my question was, if there's a race to achieve AGI, how close is this race?

888
00:56:21,000 --> 00:56:22,000
Is it neck and neck?

889
00:56:22,000 --> 00:56:24,000
I mean, who's at the lead?

890
00:56:24,000 --> 00:56:28,000
And how much capital is being put into these companies that are at the lead?

891
00:56:28,000 --> 00:56:35,000
And whoever achieves it first, if that is under the control of a government, it's completely

892
00:56:35,000 --> 00:56:38,000
dependent upon what are the morals and ethics of that government?

893
00:56:38,000 --> 00:56:40,000
What was the Constitution?

894
00:56:40,000 --> 00:56:41,000
What if it happens in China?

895
00:56:41,000 --> 00:56:42,000
What if it happens in Russia?

896
00:56:42,000 --> 00:56:45,000
What if it happens somewhere other than the United States?

897
00:56:45,000 --> 00:56:49,000
And even if it does happen in the United States, who's controlling it?

898
00:56:49,000 --> 00:56:54,000
I mean, the knowledge of how to create these things is pretty widespread.

899
00:56:54,000 --> 00:57:02,000
It's not like somebody can just capitalize on a way to do it and nobody else understands

900
00:57:02,000 --> 00:57:03,000
it.

901
00:57:03,000 --> 00:57:14,000
The knowledge of how to create a large language model or how to create the type of chips that

902
00:57:14,000 --> 00:57:19,000
would enable you to create this is actually pretty widespread.

903
00:57:19,000 --> 00:57:24,000
So do you think essentially the competition is pretty even in all the countries currently?

904
00:57:24,000 --> 00:57:25,000
Yeah.

905
00:57:25,000 --> 00:57:29,000
And there's also probably espionage, there's espionage where they're stealing information

906
00:57:29,000 --> 00:57:33,000
and sharing information and selling information.

907
00:57:33,000 --> 00:57:46,000
And in terms of differences, the United States actually has superior AI compared to other

908
00:57:46,000 --> 00:57:47,000
places.

909
00:57:47,000 --> 00:57:48,000
That's pretty good for us.

910
00:57:48,000 --> 00:57:55,000
I mean, we're actually way ahead of China, I would say.

911
00:57:55,000 --> 00:57:59,000
Right, but China has a way of figuring out what we're doing in copying it.

912
00:57:59,000 --> 00:58:01,000
Pretty good at that.

913
00:58:01,000 --> 00:58:03,000
They have been, yeah.

914
00:58:03,000 --> 00:58:04,000
Yeah.

915
00:58:04,000 --> 00:58:10,000
So do you have any concern whatsoever in the idea that AI gets in the hands of the wrong

916
00:58:10,000 --> 00:58:12,000
people?

917
00:58:12,000 --> 00:58:16,000
So when it first gets implemented, that's the big problem.

918
00:58:16,000 --> 00:58:21,000
Because before it exists, before artificial general intelligence really exists, it doesn't.

919
00:58:21,000 --> 00:58:22,000
And then it does.

920
00:58:22,000 --> 00:58:23,000
And who has it?

921
00:58:23,000 --> 00:58:27,000
And then once it does, can that AGI stop other people from getting it?

922
00:58:27,000 --> 00:58:32,000
Can you program it to make sure, can you sabotage grids?

923
00:58:32,000 --> 00:58:36,000
Can you do whatever you can to take down the internet in these opposing places?

924
00:58:36,000 --> 00:58:39,000
Could you inject their computations with viruses?

925
00:58:39,000 --> 00:58:45,000
What could you do to stop other people from getting to where you're at if you have an infinitely

926
00:58:45,000 --> 00:58:46,000
higher intelligence?

927
00:58:46,000 --> 00:58:47,000
First.

928
00:58:47,000 --> 00:58:52,000
If that's what your goal is, then yes, you could do that.

929
00:58:52,000 --> 00:58:54,000
Are you worried about that at all?

930
00:58:54,000 --> 00:58:55,000
Yes, I worry about it.

931
00:58:55,000 --> 00:58:57,000
What is your main worry?

932
00:58:57,000 --> 00:59:00,000
I mean, you worry about the implementation of artificial intelligence.

933
00:59:00,000 --> 00:59:10,000
What's your main worry?

934
00:59:10,000 --> 00:59:23,000
I mean, I'm worried if people who have a destructive idea of how to use these capabilities get into

935
00:59:23,000 --> 00:59:24,000
control.

936
00:59:24,000 --> 00:59:27,000
Right.

937
00:59:27,000 --> 00:59:29,000
And that could happen.

938
00:59:29,000 --> 00:59:36,000
And I've got a chapter in the book about perils that are like what we're talking about.

939
00:59:37,000 --> 00:59:44,000
And what do you think that could look like if the wrong people got to hold this technology?

940
00:59:44,000 --> 00:59:52,000
Well, if you look at actually who controls atomic weapons, which is not AI, some of the

941
00:59:52,000 --> 01:00:01,000
worst people in the world, and if you were to ask people right after we used two atomic

942
01:00:02,000 --> 01:00:08,000
weapons within a week 80 years ago, what's the likelihood that we're going to go another

943
01:00:08,000 --> 01:00:12,000
80 years and not have that happen again?

944
01:00:12,000 --> 01:00:14,000
Everybody would say zero.

945
01:00:14,000 --> 01:00:15,000
Right.

946
01:00:15,000 --> 01:00:17,000
But it actually has happened.

947
01:00:17,000 --> 01:00:18,000
Chalking.

948
01:00:18,000 --> 01:00:19,000
Yeah.

949
01:00:19,000 --> 01:00:20,000
Yeah.

950
01:00:20,000 --> 01:00:25,000
And I think there's actually some message there.

951
01:00:25,000 --> 01:00:27,000
Mutual assured destruction.

952
01:00:27,000 --> 01:00:30,000
But the thing is, would artificial intelligence...

953
01:00:30,000 --> 01:00:32,000
But that has not happened.

954
01:00:32,000 --> 01:00:33,000
Right.

955
01:00:33,000 --> 01:00:34,000
It has not happened yet.

956
01:00:34,000 --> 01:00:38,000
But would artificial general intelligence in the control of the wrong people negate

957
01:00:38,000 --> 01:00:42,000
that mutually assured destruction that keeps people from doing things?

958
01:00:42,000 --> 01:00:47,000
Obviously, we did drop bombs on Hiroshima and Nagasaki.

959
01:00:47,000 --> 01:00:48,000
We did.

960
01:00:48,000 --> 01:00:52,000
We did indiscriminately kill who knows how many hundreds of thousands of people with

961
01:00:52,000 --> 01:00:53,000
those weapons.

962
01:00:53,000 --> 01:00:54,000
We did it.

963
01:00:54,000 --> 01:00:59,000
And if human beings were capable of doing it because no one else had it, if artificial

964
01:00:59,000 --> 01:01:06,000
intelligence reaches that sentient level and is in control of the wrong people, what's

965
01:01:06,000 --> 01:01:08,000
to stop them from doing...

966
01:01:08,000 --> 01:01:11,000
There's no mutually assured destruction if you're the one who's got it.

967
01:01:11,000 --> 01:01:13,000
You're the only one who's got it.

968
01:01:13,000 --> 01:01:15,000
And you possibly...

969
01:01:15,000 --> 01:01:20,000
My concern is that whoever gets it could possibly stop it from being spread everywhere

970
01:01:20,000 --> 01:01:23,000
else and control it completely.

971
01:01:23,000 --> 01:01:28,000
And then you're looking at a completely dystopian world.

972
01:01:28,000 --> 01:01:29,000
Right.

973
01:01:29,000 --> 01:01:33,000
So if you ask me what I'm concerned about, it's one of those lines.

974
01:01:33,000 --> 01:01:36,000
Because that's what I always want to get out of you guys.

975
01:01:36,000 --> 01:01:41,000
Because there's so many people that are rightfully so, so high on this technology and the possibilities

976
01:01:41,000 --> 01:01:43,000
for enhancing our lives.

977
01:01:43,000 --> 01:01:48,000
But the concern that a lot of people have is that at what cost and what are we signing

978
01:01:48,000 --> 01:01:49,000
up for?

979
01:01:49,000 --> 01:01:50,000
Right.

980
01:01:50,000 --> 01:01:57,000
But I mean, if we want to, for example, live indefinitely, this is what we need to do.

981
01:01:57,000 --> 01:01:58,000
We can't do...

982
01:01:58,000 --> 01:02:00,000
What if you're denying yourself heaven?

983
01:02:00,000 --> 01:02:02,000
You ever thought of that possibility?

984
01:02:02,000 --> 01:02:06,000
I know that's a ridiculous abstract concept, but if heaven is real, if the idea of the

985
01:02:06,000 --> 01:02:11,000
afterlife is real and it's the next level of existence and you're constantly going through

986
01:02:11,000 --> 01:02:17,000
these cycles of life, what if you're stepping in artificially denying that?

987
01:02:17,000 --> 01:02:19,000
It's hard to imagine.

988
01:02:19,000 --> 01:02:21,000
It is hard to imagine, but so is life.

989
01:02:21,000 --> 01:02:23,000
So is the universe itself.

990
01:02:23,000 --> 01:02:24,000
So is the big bang.

991
01:02:24,000 --> 01:02:25,000
My father...

992
01:02:25,000 --> 01:02:35,000
My father died when I was 22, so it's more than 50, 60 years ago.

993
01:02:35,000 --> 01:02:39,000
And it's hard...

994
01:02:39,000 --> 01:02:46,000
And he was actually a great musician and he created fantastic music, but he hasn't done

995
01:02:46,000 --> 01:02:51,000
that since he died.

996
01:02:51,000 --> 01:03:00,000
And there's nothing that exists that is at all creative based on him.

997
01:03:00,000 --> 01:03:03,000
We have his memories.

998
01:03:03,000 --> 01:03:06,000
Actually created a large language model that represented him.

999
01:03:06,000 --> 01:03:08,000
I can actually talk to him.

1000
01:03:08,000 --> 01:03:10,000
You do that now?

1001
01:03:10,000 --> 01:03:11,000
Yeah.

1002
01:03:11,000 --> 01:03:13,000
It's in the book.

1003
01:03:13,000 --> 01:03:19,000
When you do that, have you thought about implementing some sort of a Sora-type deal where you're

1004
01:03:19,000 --> 01:03:22,000
talking to him?

1005
01:03:22,000 --> 01:03:24,000
Well you can do that now with language.

1006
01:03:24,000 --> 01:03:29,000
Right, but I mean physically like looking at him like you're on a zoom call with him.

1007
01:03:29,000 --> 01:03:35,000
That's a little bit in the future to be able to actually capture the way he looks, but

1008
01:03:35,000 --> 01:03:37,000
that's also feasible.

1009
01:03:37,000 --> 01:03:39,000
It seems pretty feasible.

1010
01:03:39,000 --> 01:03:43,000
And certainly it could be something representative of what he looks based on photographs that

1011
01:03:43,000 --> 01:03:45,000
you have, right?

1012
01:03:45,000 --> 01:03:52,000
Things like that is a reason to continue so that we can create that and create our own

1013
01:03:52,000 --> 01:03:58,000
ability to continue to exist.

1014
01:03:58,000 --> 01:04:06,000
You talk to people and they say, well I don't really want to live past 90 or whatever, 100.

1015
01:04:06,000 --> 01:04:15,000
But in my mind if you don't exist, there's nothing for you to experience.

1016
01:04:15,000 --> 01:04:17,000
That's true in this dimension.

1017
01:04:17,000 --> 01:04:22,000
My thought on that, people saying that I don't want to live past 90, it's like okay are you

1018
01:04:22,000 --> 01:04:23,000
alive now?

1019
01:04:23,000 --> 01:04:24,000
Do you like being alive now?

1020
01:04:24,000 --> 01:04:26,000
What's the difference between now and 90?

1021
01:04:26,000 --> 01:04:31,000
Is it just a number or is it the deterioration of your physical body and how much effort

1022
01:04:31,000 --> 01:04:36,000
have you put into mitigating the deterioration of your natural body so that you can enjoy

1023
01:04:36,000 --> 01:04:37,000
life now?

1024
01:04:37,000 --> 01:04:38,000
Exactly.

1025
01:04:38,000 --> 01:04:42,000
And we've actually seen who would want to take their lives.

1026
01:04:42,000 --> 01:04:45,000
People do take their lives.

1027
01:04:45,000 --> 01:04:54,000
If they are experiencing something that's miserable, if they're suffering physically, emotionally,

1028
01:04:54,000 --> 01:05:04,000
mentally, spiritually, and they just cannot stand the way life is carrying on, then they

1029
01:05:04,000 --> 01:05:06,000
want to take their lives.

1030
01:05:06,000 --> 01:05:09,000
Otherwise people don't.

1031
01:05:09,000 --> 01:05:12,000
If they're enjoying their lives, they continue.

1032
01:05:12,000 --> 01:05:15,000
And people say, I don't want to live past 100.

1033
01:05:15,000 --> 01:05:26,000
But when they get to be 99.9, they don't want to disappear unless they're suffering.

1034
01:05:26,000 --> 01:05:27,000
Unless they're suffering.

1035
01:05:27,000 --> 01:05:32,000
That's what's interesting about the positive aspects of AI.

1036
01:05:32,000 --> 01:05:37,000
Once we can manipulate human neurochemistry to the point where we figure out what is causing

1037
01:05:37,000 --> 01:05:42,000
great depression, what is causing anxiety, what is causing a lot of these schizophrenic

1038
01:05:42,000 --> 01:05:43,000
people.

1039
01:05:43,000 --> 01:05:44,000
And we definitely had that before.

1040
01:05:44,000 --> 01:05:45,000
We didn't have the terms.

1041
01:05:45,000 --> 01:05:47,000
We didn't understand schizophrenia.

1042
01:05:47,000 --> 01:05:49,000
But people definitely had it.

1043
01:05:49,000 --> 01:05:50,000
For sure.

1044
01:05:50,000 --> 01:05:53,000
But what if we get to a point where we can mitigate that with technology?

1045
01:05:53,000 --> 01:05:56,000
Where we can say, this is what's going on in the human body.

1046
01:05:56,000 --> 01:05:57,000
That's why we're continuing.

1047
01:05:57,000 --> 01:05:58,000
Right.

1048
01:05:58,000 --> 01:05:59,000
I was saying that's a good thing.

1049
01:05:59,000 --> 01:06:01,000
That's a positive aspect of this technology.

1050
01:06:01,000 --> 01:06:04,000
And think about also profoundly.

1051
01:06:04,000 --> 01:06:09,000
Think about how many people do take their lives and with this technology would not just live

1052
01:06:09,000 --> 01:06:15,000
happily but also be productive and also contribute to whatever society is doing.

1053
01:06:15,000 --> 01:06:19,000
That's why we're carrying on with this.

1054
01:06:19,000 --> 01:06:25,000
But in order to do that, we do have to overcome some of the problems that you've articulated.

1055
01:06:25,000 --> 01:06:31,000
I think what a lot of people are terrified of is that these people that are creating this

1056
01:06:31,000 --> 01:06:35,000
technology, there's oversight.

1057
01:06:35,000 --> 01:06:40,000
But it's oversight by people that don't necessarily understand it the way the people that are creating

1058
01:06:40,000 --> 01:06:41,000
it.

1059
01:06:41,000 --> 01:06:43,000
And they don't know what guardrails are in place.

1060
01:06:43,000 --> 01:06:44,000
How safe is this?

1061
01:06:44,000 --> 01:06:50,000
Especially when it's implemented with some sort of weapons technology.

1062
01:06:50,000 --> 01:06:52,000
Or some sort of a military application.

1063
01:06:52,000 --> 01:06:57,000
Especially a military application that can be insanely profitable.

1064
01:06:57,000 --> 01:07:01,000
And the motivations behind utilizing that are that profit.

1065
01:07:01,000 --> 01:07:06,000
And then we do horrible things and somehow justify it.

1066
01:07:06,000 --> 01:07:12,000
I mean I think democracy is actually an important issue here because democratic nations tend

1067
01:07:12,000 --> 01:07:15,000
not to go to war with each other.

1068
01:07:15,000 --> 01:07:27,000
And I mean you look at the way we're handling military technology.

1069
01:07:27,000 --> 01:07:32,000
If everybody was a democracy, I think there'd be much less war.

1070
01:07:32,000 --> 01:07:36,000
As long as it's a legitimate democracy, it's not controlled by money.

1071
01:07:36,000 --> 01:07:41,000
As long as it's a legitimate democracy, it's not controlled by the military industrial complex.

1072
01:07:41,000 --> 01:07:46,000
The pharmaceutical industry or whoever puts the people that are in elected places.

1073
01:07:46,000 --> 01:07:47,000
Who puts them in there?

1074
01:07:47,000 --> 01:07:48,000
How do they get funded?

1075
01:07:48,000 --> 01:07:51,000
And what do they represent once they get in there?

1076
01:07:51,000 --> 01:07:53,000
Are they there for the will of the people?

1077
01:07:53,000 --> 01:07:54,000
They're there for their own career?

1078
01:07:54,000 --> 01:07:58,000
Do they bypass the safety and the future of the people for their own personal gain,

1079
01:07:58,000 --> 01:08:01,000
which we've seen politicians do?

1080
01:08:01,000 --> 01:08:06,000
There's certain problems with every system that involves human beings.

1081
01:08:06,000 --> 01:08:09,000
This is another thing that technology may be able to do.

1082
01:08:09,000 --> 01:08:14,000
One of the things, if you think about the worst attributes of humans.

1083
01:08:14,000 --> 01:08:23,000
Whether it's war, crime, some of the horrible things that human beings are capable of.

1084
01:08:23,000 --> 01:08:31,000
Imagine that technology can find what causes those thoughts and behaviors in human beings and mitigate them.

1085
01:08:31,000 --> 01:08:38,000
I've joked around about this, but if we came up with something that would elevate dopamine just 300% worldwide,

1086
01:08:38,000 --> 01:08:39,000
there would be no more war.

1087
01:08:39,000 --> 01:08:40,000
It'd be over.

1088
01:08:40,000 --> 01:08:42,000
Everybody would be loving everybody.

1089
01:08:42,000 --> 01:08:44,000
We'd be interacting with each other.

1090
01:08:44,000 --> 01:08:46,000
Well, that's the point of doing this.

1091
01:08:46,000 --> 01:08:48,000
But there would also be no sad songs.

1092
01:08:48,000 --> 01:08:49,000
Wow.

1093
01:08:49,000 --> 01:08:51,000
You need some blues in your life.

1094
01:08:51,000 --> 01:08:53,000
You need a little bit of that too.

1095
01:08:53,000 --> 01:08:54,000
Or do we?

1096
01:08:54,000 --> 01:08:55,000
Maybe we don't.

1097
01:08:55,000 --> 01:08:58,000
Maybe that's just a byproduct of our monkey minds.

1098
01:08:58,000 --> 01:09:04,000
And that one day we'll surpass that and get to this point of enlightenment.

1099
01:09:04,000 --> 01:09:12,000
Enlightenment seems possible without technological innovation, but maybe not.

1100
01:09:12,000 --> 01:09:15,000
I've never really met a truly enlightened person.

1101
01:09:15,000 --> 01:09:16,000
I've met some people that are pretty close.

1102
01:09:16,000 --> 01:09:19,000
But if you could get there with technology.

1103
01:09:19,000 --> 01:09:25,000
If technology just completely elevated the human consciousness to the point where all of our conflicts come erased.

1104
01:09:25,000 --> 01:09:33,000
Just for starters, if you could actually live longer, quite aside from the motivations of people,

1105
01:09:33,000 --> 01:09:40,000
most people die not because of people's motivations, but because our bodies just won't last that long.

1106
01:09:40,000 --> 01:09:41,000
Right.

1107
01:09:42,000 --> 01:09:50,000
And a lot of people say, you know, I don't want to live longer, which makes no sense to me.

1108
01:09:50,000 --> 01:09:57,000
Why would you want to disappear and not be able to have any kind of experience?

1109
01:09:57,000 --> 01:10:00,000
Well, I think some people don't think you're disappearing.

1110
01:10:00,000 --> 01:10:09,000
I mean, there is a long held thought in many cultures that this life is but one step.

1111
01:10:09,000 --> 01:10:20,000
And that there is an afterlife and maybe that exists to comfort us because we deal with existential angst and the reality of our own inevitable demise.

1112
01:10:20,000 --> 01:10:26,000
Or maybe it's a function of consciousness being something that we don't truly understand.

1113
01:10:26,000 --> 01:10:38,000
And what you are is a soul contained in a body and that we have a very primitive understanding of the existence of life itself and of the existence of everything.

1114
01:10:38,000 --> 01:10:41,000
Well, I guess that makes sense.

1115
01:10:41,000 --> 01:10:44,000
But I don't really accept it.

1116
01:10:44,000 --> 01:10:46,000
Well, there's no evidence, right?

1117
01:10:46,000 --> 01:10:54,000
But is it there's no evidence because we're not capable of determining it yet and understanding it?

1118
01:10:54,000 --> 01:10:57,000
Or is it just because it doesn't exist?

1119
01:10:57,000 --> 01:10:59,000
That's the real question.

1120
01:10:59,000 --> 01:11:01,000
It's like, is this it?

1121
01:11:01,000 --> 01:11:02,000
Is this everything?

1122
01:11:02,000 --> 01:11:04,000
Or is this merely a stage?

1123
01:11:04,000 --> 01:11:12,000
And are we monkeying with that stage by interfering with the process of life and death?

1124
01:11:12,000 --> 01:11:14,000
Well, it makes sense.

1125
01:11:14,000 --> 01:11:17,000
But I don't really see the evidence for that.

1126
01:11:17,000 --> 01:11:20,000
I can see from your perspective.

1127
01:11:20,000 --> 01:11:24,000
I don't see the evidence of it either, but it's a concept that is not.

1128
01:11:24,000 --> 01:11:36,000
It's just when you start talking to strength theorists and they start talking about things existing and not existing at the same time, particles in superposition, you're talking about magic.

1129
01:11:36,000 --> 01:11:40,000
You're talking about something that's impossible to wrap your head around.

1130
01:11:40,000 --> 01:11:43,000
Even just the structure of an atom.

1131
01:11:43,000 --> 01:11:44,000
What's in there?

1132
01:11:44,000 --> 01:11:45,000
Nothing?

1133
01:11:45,000 --> 01:11:47,000
How much of it is space?

1134
01:11:47,000 --> 01:11:53,000
The entire existence of everything in the universe seems preposterous.

1135
01:11:53,000 --> 01:11:55,000
But it's all real.

1136
01:11:55,000 --> 01:12:03,000
And we only have a limited grasp of understanding of what this is really all about and what processes are really in place.

1137
01:12:03,000 --> 01:12:04,000
Right.

1138
01:12:04,000 --> 01:12:16,000
But if you look at people's perspective, if somebody gets a disease and is kind of known they could only live like another six months, people are not happy with that.

1139
01:12:16,000 --> 01:12:17,000
No.

1140
01:12:17,000 --> 01:12:18,000
Well, they're scared.

1141
01:12:18,000 --> 01:12:19,000
They're scared to die.

1142
01:12:19,000 --> 01:12:24,000
It's a natural human instinct that keeps us alive for all these hundreds of millions of years.

1143
01:12:24,000 --> 01:12:26,000
Yes, but very few people would be happy with that.

1144
01:12:26,000 --> 01:12:34,000
And if you then had something, gee, we have this new device, you could take this and you won't die.

1145
01:12:34,000 --> 01:12:35,000
Right.

1146
01:12:35,000 --> 01:12:37,000
Almost everybody would do that.

1147
01:12:37,000 --> 01:12:38,000
Sure.

1148
01:12:38,000 --> 01:12:42,000
But would they appreciate life if they knew it had no end?

1149
01:12:43,000 --> 01:12:51,000
Or would it be like a lottery winner just goes nuts and spends all their money and loses their marbles because they can't believe they can't die?

1150
01:12:51,000 --> 01:12:55,000
Well, first of all, it's not guaranteed to live forever.

1151
01:12:55,000 --> 01:12:56,000
Sure.

1152
01:12:56,000 --> 01:12:57,000
You can get in an accident.

1153
01:12:57,000 --> 01:12:58,000
Something can happen.

1154
01:12:58,000 --> 01:12:59,000
You get injured.

1155
01:12:59,000 --> 01:13:06,000
But if we get to a point where you have automated cars that significantly reduce the amount of automobile accidents.

1156
01:13:06,000 --> 01:13:11,000
Well, also, we can back up everything, everything in our physical body as well as...

1157
01:13:11,000 --> 01:13:13,000
How far away are we from that?

1158
01:13:13,000 --> 01:13:20,000
That idea of, I mean, we don't really truly understand what consciousness is, correct?

1159
01:13:20,000 --> 01:13:21,000
Right.

1160
01:13:21,000 --> 01:13:30,000
So how would we be able to manipulate it or reduplicate it to the point where you're putting it inside of some kind of a computation device?

1161
01:13:30,000 --> 01:13:44,000
Well, we know to be able to create a computation that matches what our brain does.

1162
01:13:44,000 --> 01:13:47,000
That's what we're doing with these large language models.

1163
01:13:47,000 --> 01:13:48,000
Right.

1164
01:13:48,000 --> 01:13:56,000
And we're actually very close now to what our brain can do with these large language models and it'll be there like within a year.

1165
01:13:56,000 --> 01:14:13,000
And we can back up the electronic version and we'll get to the point where we can back up what our brain normally does.

1166
01:14:13,000 --> 01:14:16,000
So we'll be able to actually back that up as well.

1167
01:14:16,000 --> 01:14:21,000
We'll be able to detect what it is and back that up just like our computers.

1168
01:14:21,000 --> 01:14:27,000
We'll create it in the form of an artificial version of everything that it is to be a human being.

1169
01:14:27,000 --> 01:14:28,000
Right, exactly.

1170
01:14:28,000 --> 01:14:31,000
In terms of emotions, love, excitement.

1171
01:14:31,000 --> 01:14:34,000
And that's going to happen over the next 20 years.

1172
01:14:34,000 --> 01:14:37,000
It's not a thousand years.

1173
01:14:37,000 --> 01:14:39,000
But will that be a person?

1174
01:14:39,000 --> 01:14:42,000
I mean, or will it be some sort of a zombie?

1175
01:14:42,000 --> 01:14:44,000
Like what motivations will it have?

1176
01:14:44,000 --> 01:14:50,000
If you can take human consciousness and duplicate it, much like you could duplicate your phone,

1177
01:14:50,000 --> 01:14:53,000
and you make this new thing, what does that thing feel like?

1178
01:14:53,000 --> 01:14:55,000
Does that thing live in hell?

1179
01:14:55,000 --> 01:14:58,000
What does that experience like for that thing?

1180
01:14:58,000 --> 01:15:00,000
What about large language models?

1181
01:15:00,000 --> 01:15:02,000
Do they really exist?

1182
01:15:02,000 --> 01:15:04,000
I mean, they can talk.

1183
01:15:04,000 --> 01:15:07,000
They certainly do, but would you want to be one?

1184
01:15:07,000 --> 01:15:09,000
Are we different than that?

1185
01:15:09,000 --> 01:15:10,000
Yeah, we're people.

1186
01:15:10,000 --> 01:15:11,000
We shake hands.

1187
01:15:11,000 --> 01:15:12,000
I give you a hug.

1188
01:15:12,000 --> 01:15:13,000
You pet my dog.

1189
01:15:13,000 --> 01:15:15,000
You listen to music.

1190
01:15:15,000 --> 01:15:17,000
We'll be able to do all of that as well.

1191
01:15:17,000 --> 01:15:18,000
Right, but will you want to?

1192
01:15:18,000 --> 01:15:19,000
Will you even care?

1193
01:15:19,000 --> 01:15:23,000
A lot of what gives us joy in life is biological motivations.

1194
01:15:23,000 --> 01:15:26,000
There's human reward systems that are put in place that allow us to...

1195
01:15:26,000 --> 01:15:28,000
Well, it's going to be part of who we are.

1196
01:15:28,000 --> 01:15:29,000
Right.

1197
01:15:29,000 --> 01:15:34,000
And we just like a person, and we'll also have our physical bodies as well.

1198
01:15:34,000 --> 01:15:37,000
And that'll also be able to be backed up.

1199
01:15:37,000 --> 01:15:43,000
And we'll be doing the things that we do now, except we'll be able to have them continue.

1200
01:15:43,000 --> 01:15:47,000
So if you get hit by a car and you die, there's another array that just pops up.

1201
01:15:47,000 --> 01:15:49,000
Oh, we got the backup array.

1202
01:15:49,000 --> 01:15:57,000
And the backup array will have no feelings at all about having it had died and come back to life.

1203
01:15:57,000 --> 01:15:59,000
Well, that's a question.

1204
01:15:59,000 --> 01:16:00,000
Yeah.

1205
01:16:00,000 --> 01:16:05,000
I mean, why wouldn't it be just like Ray is now?

1206
01:16:05,000 --> 01:16:06,000
Why wouldn't it?

1207
01:16:06,000 --> 01:16:12,000
If we figure out that if biological life is essentially a kind of technology that the

1208
01:16:12,000 --> 01:16:17,000
universe has created, and we can manipulate that to the point where we understand it,

1209
01:16:17,000 --> 01:16:23,000
we get it, we've optimized it, and then replicate it.

1210
01:16:23,000 --> 01:16:24,000
Physically replicate it.

1211
01:16:24,000 --> 01:16:30,000
Not just replicate it in the form of a computer, but an actual physical being.

1212
01:16:30,000 --> 01:16:31,000
Right.

1213
01:16:31,000 --> 01:16:32,000
Well, that's where we're headed.

1214
01:16:32,000 --> 01:16:37,000
Do you anticipate that people will be happy with whatever they have?

1215
01:16:37,000 --> 01:16:42,000
Because if you decide, I don't like being five, six, I wish I was six, six.

1216
01:16:42,000 --> 01:16:43,000
I don't like being a woman.

1217
01:16:43,000 --> 01:16:44,000
I like, I want to be a man.

1218
01:16:44,000 --> 01:16:46,000
I don't want to be Asian.

1219
01:16:46,000 --> 01:16:48,000
I want to be, you know, whatever.

1220
01:16:48,000 --> 01:16:49,000
I want to be a black person.

1221
01:16:49,000 --> 01:16:50,000
I want to be...

1222
01:16:50,000 --> 01:16:57,000
We'll actually be able to do all of those things simultaneously and so on.

1223
01:16:57,000 --> 01:17:01,000
We're not going to be limited by those kinds of happenstance.

1224
01:17:01,000 --> 01:17:03,000
Which is going to be very strange.

1225
01:17:03,000 --> 01:17:07,000
What will human beings look like if you give people the ability to manipulate your physical

1226
01:17:07,000 --> 01:17:08,000
form?

1227
01:17:08,000 --> 01:17:11,000
Well, we do things now that were impossible even 10 years ago.

1228
01:17:11,000 --> 01:17:16,000
We certainly do, but we don't change race, size, sex, gender, height.

1229
01:17:16,000 --> 01:17:21,000
We don't do all those...the radical increase in just your intelligence.

1230
01:17:21,000 --> 01:17:23,000
Like, what is that going to look like?

1231
01:17:23,000 --> 01:17:28,000
What kind of an interaction is it going to be between two human beings when you have

1232
01:17:28,000 --> 01:17:30,000
a completely new form?

1233
01:17:30,000 --> 01:17:34,000
You know, you're much different physically than you ever were when you were alive.

1234
01:17:34,000 --> 01:17:35,000
You're taller.

1235
01:17:35,000 --> 01:17:36,000
You're stronger.

1236
01:17:36,000 --> 01:17:37,000
You're smarter.

1237
01:17:37,000 --> 01:17:38,000
You're faster.

1238
01:17:38,000 --> 01:17:40,000
You're basically not really a human anymore.

1239
01:17:40,000 --> 01:17:42,000
You're a new thing.

1240
01:17:42,000 --> 01:17:44,000
I mean, we're expanding who we are.

1241
01:17:44,000 --> 01:17:47,000
We're already expanded who we are from, you know...

1242
01:17:47,000 --> 01:17:48,000
Sure.

1243
01:17:48,000 --> 01:17:49,000
Right.

1244
01:17:49,000 --> 01:17:53,000
Over a course of hundreds of thousands of years, we've gone from being Australopithecus

1245
01:17:53,000 --> 01:17:54,000
to what we are now.

1246
01:17:54,000 --> 01:18:01,000
That has to do with the pace at which we make changes.

1247
01:18:01,000 --> 01:18:02,000
Right.

1248
01:18:02,000 --> 01:18:10,000
And we can make changes now much more quickly than we could, you know, 100,000 years ago.

1249
01:18:10,000 --> 01:18:11,000
Right.

1250
01:18:11,000 --> 01:18:16,000
But if we can manipulate our physical form with no limitations, I mean, what are...we

1251
01:18:16,000 --> 01:18:18,000
have six armed people that can fly?

1252
01:18:18,000 --> 01:18:19,000
Like, what is it going to look like?

1253
01:18:19,000 --> 01:18:21,000
Well, do you have a problem with that?

1254
01:18:21,000 --> 01:18:24,000
Yeah, I would discriminate against six armed people that can fly.

1255
01:18:24,000 --> 01:18:27,000
That's the one area I allow myself to give prejudice to.

1256
01:18:27,000 --> 01:18:28,000
Okay.

1257
01:18:28,000 --> 01:18:32,000
No, I'm just curious as to how much time you've spent...

1258
01:18:32,000 --> 01:18:34,000
Seven armed people would be okay.

1259
01:18:34,000 --> 01:18:39,000
Yeah, seven armed people is cool because it's like, you know, maybe five on one side,

1260
01:18:39,000 --> 01:18:40,000
two on the other.

1261
01:18:40,000 --> 01:18:47,000
No, I'm just curious as to like how much time you've spent thinking about what this could

1262
01:18:47,000 --> 01:18:48,000
look like.

1263
01:18:48,000 --> 01:18:53,000
And I just...I don't think it's going to be as simple as, you know, it's going to be

1264
01:18:53,000 --> 01:18:59,000
Ray Kurzweil, but Ray Kurzweil as like a 30-year-old man, 50 years from now.

1265
01:18:59,000 --> 01:19:03,000
I think it's probably going to be...you're going to be all kinds of different things.

1266
01:19:03,000 --> 01:19:04,000
You could be kind of whatever you want.

1267
01:19:04,000 --> 01:19:05,000
You could be a bird.

1268
01:19:05,000 --> 01:19:11,000
I mean, what's to stop...if we can get to manipulate the physical form and we can take consciousness

1269
01:19:11,000 --> 01:19:12,000
and put it into a physical form.

1270
01:19:12,000 --> 01:19:17,000
But that's a description, I think, of something that's positive rather than negative.

1271
01:19:17,000 --> 01:19:18,000
You could be a giant eagle.

1272
01:19:18,000 --> 01:19:25,000
I mean, negative is people that want to destroy things, getting power.

1273
01:19:25,000 --> 01:19:26,000
Sure.

1274
01:19:26,000 --> 01:19:28,000
And that is a problem.

1275
01:19:28,000 --> 01:19:32,000
Well, it's certainly improvement in terms of the viability.

1276
01:19:32,000 --> 01:19:35,000
Seven arms and being like an eagle and so on.

1277
01:19:35,000 --> 01:19:40,000
I mean, and you can also change that.

1278
01:19:40,000 --> 01:19:41,000
Right.

1279
01:19:41,000 --> 01:19:46,960
So I think that's a positive aspect and we will be able to do that kind of thing.

1280
01:19:46,960 --> 01:19:47,960
Sure.

1281
01:19:47,960 --> 01:19:52,560
If you want to look at it in a binary fashion, positive and negative, but it's also going

1282
01:19:52,560 --> 01:19:58,240
to be insanely strange, like it's not going to be as simple as there'll be people that

1283
01:19:58,240 --> 01:19:59,960
are living in 2069.

1284
01:19:59,960 --> 01:20:04,160
Well, it seems strange once it's first reported.

1285
01:20:04,160 --> 01:20:08,960
If it's been reported now for five years and people are constantly doing it, you won't find

1286
01:20:08,960 --> 01:20:09,960
it that strange.

1287
01:20:09,960 --> 01:20:11,960
It'll just be life.

1288
01:20:11,960 --> 01:20:12,960
Yeah.

1289
01:20:12,960 --> 01:20:13,960
Yeah.

1290
01:20:13,960 --> 01:20:14,960
So that's what I'm asking.

1291
01:20:14,960 --> 01:20:19,560
Like when you think about the implementation of this technology to its fullest, what does

1292
01:20:19,560 --> 01:20:21,440
the world look like?

1293
01:20:21,440 --> 01:20:28,040
What does the world look like in 2069?

1294
01:20:28,040 --> 01:20:34,480
I mean, the kind of things that you can imagine right now will be able to do.

1295
01:20:34,480 --> 01:20:39,560
And it might seem strange when it first happens, but when it happens for the, you know, billions

1296
01:20:39,560 --> 01:20:43,400
of dollars in time, it won't seem that strange.

1297
01:20:43,400 --> 01:20:49,880
And maybe you're like being an eagle for a few minutes.

1298
01:20:49,880 --> 01:20:51,640
It's certainly interesting.

1299
01:20:51,640 --> 01:20:54,000
It's certainly interesting.

1300
01:20:54,000 --> 01:20:59,520
I just wonder how much time you've spent thinking about what this world looks like with the

1301
01:20:59,520 --> 01:21:05,760
full implementation of the kind of exponential growth of technology that would exist if we

1302
01:21:05,760 --> 01:21:07,640
do make it to 2069.

1303
01:21:07,640 --> 01:21:20,200
Well, I did write a book, Danielle, and this young girl has fantastic capabilities and

1304
01:21:20,200 --> 01:21:23,680
no one really can figure out how she does this.

1305
01:21:23,680 --> 01:21:35,840
She actually takes over China at age 15 and she makes it a democracy and then she actually

1306
01:21:36,040 --> 01:21:43,320
becomes president of the United States at 19, she has to, of course, create a constitutional

1307
01:21:43,320 --> 01:21:49,080
amendment that at least she can become president at 19.

1308
01:21:49,080 --> 01:21:51,640
That sounds like what a dictator would do.

1309
01:21:51,640 --> 01:21:53,200
Right.

1310
01:21:53,200 --> 01:22:00,480
But unlike a dictator, she's very popular and she writes very good music.

1311
01:22:00,480 --> 01:22:03,480
And this is one artificial intelligence creature?

1312
01:22:03,480 --> 01:22:04,480
Yes.

1313
01:22:04,480 --> 01:22:05,480
And how was she created?

1314
01:22:05,720 --> 01:22:12,840
It never says that she gets these capabilities through AI.

1315
01:22:12,840 --> 01:22:18,440
I didn't want to spell that out, but that would be the only way that she could do this.

1316
01:22:18,440 --> 01:22:19,440
Right.

1317
01:22:19,440 --> 01:22:22,200
Unless it's some insane freak of genetics.

1318
01:22:22,200 --> 01:22:25,840
And she's like a very positive person.

1319
01:22:25,840 --> 01:22:27,840
She's very popular.

1320
01:22:27,840 --> 01:22:31,120
Yeah, but she's the only one that has that.

1321
01:22:31,120 --> 01:22:32,120
Yeah.

1322
01:22:32,120 --> 01:22:33,120
Right.

1323
01:22:33,120 --> 01:22:35,440
She doesn't give it to everybody, which is where it gets really weird.

1324
01:22:35,480 --> 01:22:37,160
You have a cell phone, I have a cell phone.

1325
01:22:37,160 --> 01:22:38,560
Pretty much everybody has one now.

1326
01:22:38,560 --> 01:22:41,960
What happens when everybody gets the kind of technology we're discussing?

1327
01:22:41,960 --> 01:22:48,960
Well, it shows you the benefit that she has it and if everybody gets it, that would be

1328
01:22:48,960 --> 01:22:50,480
even more positive.

1329
01:22:50,480 --> 01:22:51,480
Right.

1330
01:22:51,480 --> 01:22:52,480
Perhaps, yeah.

1331
01:22:52,480 --> 01:22:56,840
I mean, that's the best way of looking at it, that we become a completely altruistic,

1332
01:22:56,840 --> 01:23:04,240
positive, beneficial to each other, society of integrated minds.

1333
01:23:04,240 --> 01:23:05,240
A benefit.

1334
01:23:05,240 --> 01:23:09,200
If you have more intelligence, you'd be more likely to do this.

1335
01:23:09,200 --> 01:23:10,200
Yes.

1336
01:23:10,200 --> 01:23:12,680
Yeah, for sure.

1337
01:23:12,680 --> 01:23:13,680
That's the benefit.

1338
01:23:13,680 --> 01:23:14,680
Yeah.

1339
01:23:14,680 --> 01:23:15,680
Yeah.

1340
01:23:15,680 --> 01:23:21,600
So we live longer and we're also smarter than making more rational decisions towards each

1341
01:23:21,600 --> 01:23:23,640
other.

1342
01:23:23,640 --> 01:23:28,440
So overall, when you're looking at this, you just don't concentrate really on the negative

1343
01:23:28,440 --> 01:23:29,440
possibilities.

1344
01:23:29,440 --> 01:23:30,440
Well, no.

1345
01:23:30,440 --> 01:23:33,200
I mean, I do focus on that as well.

1346
01:23:33,200 --> 01:23:34,200
I mean.

1347
01:23:34,200 --> 01:23:37,280
But you think overall it's net positive?

1348
01:23:37,280 --> 01:23:38,280
Yes.

1349
01:23:38,280 --> 01:23:45,320
It's called intelligence and if you have more intelligence, we'll be doing things that

1350
01:23:45,320 --> 01:23:49,200
are more beneficial to ourselves and other people.

1351
01:23:49,200 --> 01:23:51,680
Do you think that the experiences that we're having right now?

1352
01:23:51,680 --> 01:23:56,960
Like right now, we have much less crime than we did 50 years ago.

1353
01:23:56,960 --> 01:24:04,120
Now, if you listen to people debating presidential politics, they'll say, crime is worse than

1354
01:24:04,120 --> 01:24:14,880
it ever, but if you look at the actual statistics, it's gone way down and if you actually go

1355
01:24:14,880 --> 01:24:24,440
back like a few hundred years, crime and murder and so on was far, far higher than it is today.

1356
01:24:24,440 --> 01:24:27,000
It's actually pretty rare.

1357
01:24:27,000 --> 01:24:34,360
So the kind of additional intelligence that we've created is actually good for people.

1358
01:24:34,360 --> 01:24:36,560
If you look at the actual data.

1359
01:24:36,560 --> 01:24:37,560
Sure.

1360
01:24:37,560 --> 01:24:43,360
If you look at Stephen Pinker's work, scale it from 100 plus years ago to today, things

1361
01:24:43,360 --> 01:24:47,320
are generally always seem to be moving in a better direction.

1362
01:24:47,320 --> 01:24:48,320
Right.

1363
01:24:48,320 --> 01:24:52,680
Well, Pinker didn't credit this to technology.

1364
01:24:52,680 --> 01:24:58,160
He just looks at the data and says, it's gotten better.

1365
01:24:58,160 --> 01:25:02,120
What I try to do in the current book is to show how it's related to technology and as

1366
01:25:02,120 --> 01:25:05,800
we have more technology, we're actually moving in this direction.

1367
01:25:05,800 --> 01:25:10,400
So you feel it's a function of technology that we're moving in this direction?

1368
01:25:10,400 --> 01:25:11,400
Absolutely.

1369
01:25:11,400 --> 01:25:15,000
I mean, that's why.

1370
01:25:15,000 --> 01:25:16,760
I mean, look at the technology.

1371
01:25:16,760 --> 01:25:24,480
In 80 years, we've multiplied the amount of computation 20 quadrillion times.

1372
01:25:24,480 --> 01:25:29,160
And so we have things that didn't exist two years ago.

1373
01:25:29,160 --> 01:25:30,520
Right.

1374
01:25:30,520 --> 01:25:37,040
When you think about the idea of life on earth and that this is happening and that we are

1375
01:25:37,040 --> 01:25:43,800
on this journey to 2045, to the singularity, do you consider whether or not this is happening

1376
01:25:43,800 --> 01:25:47,360
elsewhere in the universe or whether it's already happened?

1377
01:25:47,360 --> 01:25:56,240
Yeah, we see no evidence that there's any form of life, let alone intelligent life anywhere

1378
01:25:56,240 --> 01:25:57,240
else.

1379
01:25:57,240 --> 01:26:01,760
And I say, well, we're not in touch with these other people.

1380
01:26:01,760 --> 01:26:13,680
It is possible, but it seems, I mean, given the exponential impact of this type of technology

1381
01:26:13,680 --> 01:26:40,040
we would be spaced out based on, over a launch period of time, so some people that might

1382
01:26:40,040 --> 01:26:46,360
be ahead of us, it could be ahead of us, certainly thousands of years, even millions

1383
01:26:46,360 --> 01:26:47,360
of years.

1384
01:26:47,360 --> 01:26:54,880
And so they'd be like way ahead of us and they'd be doing galaxy-wide engineering.

1385
01:26:54,880 --> 01:26:59,000
How is it that we look out there and we don't see anybody doing galaxy-wide engineering?

1386
01:26:59,000 --> 01:27:02,400
And maybe we don't have the capability to actually see it.

1387
01:27:02,400 --> 01:27:09,080
I mean, the universe is, what's the 13.7 billion years old or whatever it is?

1388
01:27:09,080 --> 01:27:18,640
But even just incidental capabilities would affect galaxies, we would see that somehow.

1389
01:27:18,640 --> 01:27:25,440
Would we, if we were at the peak, if there is intelligent life in the universe, some

1390
01:27:25,440 --> 01:27:29,600
form of that intelligent life has to be the most advanced?

1391
01:27:29,600 --> 01:27:35,000
And what if we are underestimating our position in the universe, that we are the most advanced

1392
01:27:35,000 --> 01:27:36,000
people?

1393
01:27:36,000 --> 01:27:39,440
But maybe there's something that's like 10 years, maybe there's an industrial age.

1394
01:27:39,440 --> 01:27:44,880
I think there's a good argument that we are ahead of other people.

1395
01:27:44,880 --> 01:27:50,320
But we don't have the capability of observing the goings on of a planet 5,000 light years

1396
01:27:50,320 --> 01:27:51,320
away.

1397
01:27:51,320 --> 01:27:58,080
We can't see into their atmosphere, we can't look at high-resolution video of activity on

1398
01:27:58,080 --> 01:27:59,080
that planet.

1399
01:27:59,080 --> 01:28:02,240
But if they were doing galaxy-wide engineering, I think we would notice that.

1400
01:28:02,240 --> 01:28:04,400
If they were more advanced than us, maybe we would.

1401
01:28:04,400 --> 01:28:05,400
But what if they're not?

1402
01:28:05,400 --> 01:28:07,200
What if they're at the level that we're at?

1403
01:28:07,200 --> 01:28:08,960
Well, that's what I'm saying.

1404
01:28:08,960 --> 01:28:09,960
What if we're at the peak?

1405
01:28:09,960 --> 01:28:10,960
And this is like...

1406
01:28:10,960 --> 01:28:13,920
I think it's an argument that we are at the peak.

1407
01:28:13,920 --> 01:28:18,320
What if it gets to the point where artificial intelligence gets implemented and then that

1408
01:28:18,320 --> 01:28:23,120
becomes the primary form of life and it doesn't have the desire to do anything in terms of

1409
01:28:23,120 --> 01:28:28,480
like galactic engineering?

1410
01:28:28,480 --> 01:28:34,840
But even just incidental things would affect whole galaxies.

1411
01:28:34,840 --> 01:28:35,840
Like what?

1412
01:28:35,840 --> 01:28:37,640
Things like we're doing, are we affecting the whole galaxy?

1413
01:28:37,640 --> 01:28:38,640
No, not yet.

1414
01:28:38,640 --> 01:28:39,640
Right.

1415
01:28:39,640 --> 01:28:42,960
But what if it's like us, but it gets to the point where it becomes artificial intelligence

1416
01:28:42,960 --> 01:28:46,840
and then it doesn't have emotions, it doesn't have desires, it doesn't have ambitions, so

1417
01:28:46,840 --> 01:28:48,600
why would it decide to expand?

1418
01:28:48,600 --> 01:28:50,000
Why would it not have those things?

1419
01:28:50,000 --> 01:28:53,840
Well, we'd have to program it into it, but it would probably decide that that's foolish

1420
01:28:53,840 --> 01:28:56,200
and that those things have caused all these problems.

1421
01:28:56,200 --> 01:28:58,040
All the problems in human race.

1422
01:28:58,080 --> 01:29:00,080
That's our number one issue, war.

1423
01:29:00,080 --> 01:29:02,480
What is war caused by?

1424
01:29:02,480 --> 01:29:09,200
It's caused by ideologies, it's caused by acquisition of resources, theft of resources, violence.

1425
01:29:09,200 --> 01:29:14,080
War is not the primary thing that we are motivated by.

1426
01:29:14,080 --> 01:29:18,960
It's not the primary thing we're motivated by, but it's existed in every single step

1427
01:29:18,960 --> 01:29:21,880
of the way of human existence.

1428
01:29:21,880 --> 01:29:23,600
But it's actually getting better.

1429
01:29:23,600 --> 01:29:25,720
I mean, just look at the effect of war.

1430
01:29:25,720 --> 01:29:26,720
Sure.

1431
01:29:26,800 --> 01:29:30,640
We have a couple of wars going on, they're not killing millions of people like they used

1432
01:29:30,640 --> 01:29:31,640
to.

1433
01:29:31,640 --> 01:29:32,640
Right.

1434
01:29:32,640 --> 01:29:33,640
Right.

1435
01:29:33,640 --> 01:29:38,280
My point is that if artificial intelligence recognizes that the problem with human beings

1436
01:29:38,280 --> 01:29:45,760
is these emotions and a lot of it is fueled by these desires, like the desire to expand,

1437
01:29:45,760 --> 01:29:50,280
the desire to acquire things, the desire to achieve.

1438
01:29:50,280 --> 01:29:53,720
Well, the emotion is positive, I mean, music and other things.

1439
01:29:53,720 --> 01:29:55,040
To us.

1440
01:29:55,040 --> 01:29:56,040
To us.

1441
01:29:56,160 --> 01:30:03,160
If it gets to the point where artificial intelligence is no longer stimulated by mere human creations,

1442
01:30:03,160 --> 01:30:08,560
creativity, all these different things, why would it even have the ambition to do any sort

1443
01:30:08,560 --> 01:30:10,680
of galaxy-wide engineering?

1444
01:30:10,680 --> 01:30:12,480
Why would it want to?

1445
01:30:12,480 --> 01:30:17,120
Because it's based on us.

1446
01:30:17,120 --> 01:30:19,920
It is based on us until it decides it's not based on us anymore.

1447
01:30:19,920 --> 01:30:20,920
That's my point.

1448
01:30:20,920 --> 01:30:26,480
We realize that if we're based on a very violent chimpanzee, and we say, you know what, there's

1449
01:30:26,480 --> 01:30:31,480
a lot of what we are because of our genetics, that it really are a problem, and this is

1450
01:30:31,480 --> 01:30:36,240
what's causing all of our violence, all of our crime, all of our war.

1451
01:30:36,240 --> 01:30:42,480
If we just step in and put a stop to all that, will we also put a stop to our ambition?

1452
01:30:42,480 --> 01:30:46,280
I would maintain that we're actually moving away from that.

1453
01:30:46,280 --> 01:30:49,280
We are moving away from that, but that's just natural, right?

1454
01:30:49,320 --> 01:30:54,000
That's natural with our understanding and our mitigations of these social problems.

1455
01:30:54,000 --> 01:30:55,000
Right.

1456
01:30:55,000 --> 01:30:58,320
So if we expand that even more, we'll be even more in that direction.

1457
01:30:58,320 --> 01:30:59,960
As long as we're still we.

1458
01:30:59,960 --> 01:31:04,360
But as soon as you become something different, why would it even have the desire to expand?

1459
01:31:04,360 --> 01:31:09,760
If it was infinitely intelligent, why would it even want to physically go anywhere?

1460
01:31:09,760 --> 01:31:11,160
Why would it want to?

1461
01:31:11,160 --> 01:31:14,960
What's the reason for our motivation to expand?

1462
01:31:14,960 --> 01:31:15,960
What is it?

1463
01:31:15,960 --> 01:31:16,960
It's human.

1464
01:31:17,040 --> 01:31:21,840
The same humans that were tribal creatures that roam, the same humans that stole resources

1465
01:31:21,840 --> 01:31:23,120
from neighboring villages.

1466
01:31:23,120 --> 01:31:24,400
This is our genes, right?

1467
01:31:24,400 --> 01:31:27,400
This is what made us that got us to this point.

1468
01:31:27,400 --> 01:31:32,920
If we create a sentient artificial intelligence that's far superior to us, and it can create

1469
01:31:32,920 --> 01:31:36,920
its own version of artificial intelligence, the first thing it's going to engineer out

1470
01:31:36,920 --> 01:31:40,920
is all these stupid emotions that get us in trouble.

1471
01:31:40,920 --> 01:31:49,600
If it just can create happiness and joy from programming, why would it create happiness

1472
01:31:49,600 --> 01:31:55,520
and joy through the acquisition of other people's creativity, art, music, all those things?

1473
01:31:55,520 --> 01:31:59,240
And then why would it have any ambition at all to travel?

1474
01:31:59,240 --> 01:32:01,000
Why would it want to go anywhere?

1475
01:32:01,000 --> 01:32:04,200
Well, I mean, it's an interesting philosophical problem.

1476
01:32:04,200 --> 01:32:05,200
Right.

1477
01:32:05,200 --> 01:32:09,800
It is a problem because a lot of what we are and the things that we create is because of

1478
01:32:09,800 --> 01:32:12,360
all these flaws that you would say.

1479
01:32:12,360 --> 01:32:16,240
If you were programming us, you'd say, well, what is the cause of all these issues that

1480
01:32:16,240 --> 01:32:17,240
plague the human race?

1481
01:32:17,240 --> 01:32:18,880
I wouldn't necessarily say that there are flaws.

1482
01:32:18,880 --> 01:32:19,880
Murder is a flaw.

1483
01:32:19,880 --> 01:32:21,600
Isn't it a flaw?

1484
01:32:21,600 --> 01:32:23,080
But that's way down.

1485
01:32:23,080 --> 01:32:24,080
Right.

1486
01:32:24,080 --> 01:32:26,720
But it's a technology that moves ahead.

1487
01:32:26,720 --> 01:32:28,400
If it happens to you, it's a flaw.

1488
01:32:28,400 --> 01:32:30,120
And crime is a flaw.

1489
01:32:30,120 --> 01:32:32,960
All these theft is a fraud.

1490
01:32:32,960 --> 01:32:34,040
Those are flaws.

1491
01:32:34,040 --> 01:32:38,320
If we could engineer those out, what would be the way that we do it?

1492
01:32:38,320 --> 01:32:41,560
Well, one of the things we do, we get rid of what it is to be a person.

1493
01:32:41,560 --> 01:32:47,080
Because what it is is corrupt people that go down these terrible paths and cause harm

1494
01:32:47,080 --> 01:32:48,080
to other people.

1495
01:32:48,080 --> 01:32:49,080
Right?

1496
01:32:49,080 --> 01:32:54,720
You're taking a step there that our ability to feel emotion and so on is a flaw.

1497
01:32:54,720 --> 01:32:55,720
No, I'm not.

1498
01:32:55,720 --> 01:32:58,560
I'm saying that it's the root of these flaws.

1499
01:32:58,560 --> 01:33:03,000
That greed and envy and lust and anger are the root.

1500
01:33:03,000 --> 01:33:05,520
I'd like to go to the bathroom.

1501
01:33:05,520 --> 01:33:06,520
Yeah.

1502
01:33:06,520 --> 01:33:07,520
Okay.

1503
01:33:07,520 --> 01:33:08,520
We'll come back.

1504
01:33:08,520 --> 01:33:09,520
We'll talk about flaws.

1505
01:33:09,520 --> 01:33:10,520
And we're back.

1506
01:33:10,520 --> 01:33:12,520
Provide an answer to that.

1507
01:33:12,520 --> 01:33:24,800
I mean, as I think about myself now, it's when I have emotions that are positive emotions,

1508
01:33:24,800 --> 01:33:31,880
like really getting off on a song or a picture or some new art form that didn't exist in

1509
01:33:31,880 --> 01:33:34,480
the past.

1510
01:33:34,480 --> 01:33:35,480
That's positive.

1511
01:33:36,040 --> 01:33:47,120
That's what I live for, relating to another person in a way that's intimate.

1512
01:33:47,120 --> 01:33:53,680
So I mean, the idea, if we're actually more intelligent, we'd not to get rid of that,

1513
01:33:53,680 --> 01:33:59,080
but to actually enjoy that to a greater extent.

1514
01:33:59,080 --> 01:34:00,560
Hopefully.

1515
01:34:00,560 --> 01:34:03,120
But what I'm saying is that...

1516
01:34:03,320 --> 01:34:09,520
Yes, there are things that can go wrong, but lead us in the incorrect direction.

1517
01:34:09,520 --> 01:34:12,400
I'm not even saying it's wrong.

1518
01:34:12,400 --> 01:34:15,080
I'm not saying that it's going to go wrong.

1519
01:34:15,080 --> 01:34:22,240
I'm saying that if you wanted to program away some of the issues that human beings have

1520
01:34:22,240 --> 01:34:29,000
in terms of what keeps us from working with each other universally, all over the globe,

1521
01:34:29,000 --> 01:34:30,440
what keeps us from these things?

1522
01:34:30,560 --> 01:34:33,160
We're actually doing that more than we used to do.

1523
01:34:33,160 --> 01:34:35,080
Sure, but also not.

1524
01:34:35,080 --> 01:34:36,760
We're also massive inequality.

1525
01:34:36,760 --> 01:34:41,280
You've got people in the Congo mining cobalt with sticks that powers your cell phones.

1526
01:34:41,280 --> 01:34:43,760
There's a lot of real problems with society today.

1527
01:34:43,760 --> 01:34:45,880
But there used to be even more of that.

1528
01:34:45,880 --> 01:34:46,960
There's a lot of that, though.

1529
01:34:46,960 --> 01:34:48,760
There's a lot of that.

1530
01:34:48,760 --> 01:34:54,600
If you looked at greed and war and crime and all the problems with human beings, a lot

1531
01:34:54,600 --> 01:35:00,240
of it has to do with these biological instincts, these instincts to control things.

1532
01:35:00,800 --> 01:35:06,400
Built-in genetic codes that we have that are from our ancestors.

1533
01:35:06,400 --> 01:35:09,120
That's because we haven't gotten there yet.

1534
01:35:09,120 --> 01:35:09,800
Right.

1535
01:35:09,800 --> 01:35:18,320
But when we get there, you think we will be a better version of a human being and we will

1536
01:35:18,320 --> 01:35:24,800
be able to experience all the good, the positive aspects of being a human being, the art and

1537
01:35:24,800 --> 01:35:26,400
creativity and all these different things.

1538
01:35:26,400 --> 01:35:27,720
Yeah, I hope so.

1539
01:35:27,720 --> 01:35:35,320
And actually, if you look at what human beings have done already, we're moving in that direction.

1540
01:35:35,320 --> 01:35:36,320
Right.

1541
01:35:36,320 --> 01:35:38,880
I may not seem that way.

1542
01:35:38,880 --> 01:35:40,840
No, it does seem that way to me.

1543
01:35:40,840 --> 01:35:42,320
It does overall.

1544
01:35:42,320 --> 01:35:47,160
But it's also like, if you look at a graph of temperatures, it goes up and it goes down

1545
01:35:47,160 --> 01:35:48,160
and it goes up and it goes down.

1546
01:35:48,160 --> 01:35:50,480
But it's moving in a general direction.

1547
01:35:50,480 --> 01:35:53,080
We are moving in a generally positive direction.

1548
01:35:53,080 --> 01:35:57,280
However, we want to continue moving in this same direction.

1549
01:35:57,280 --> 01:35:59,000
Yeah, I don't think the word...

1550
01:35:59,000 --> 01:36:01,200
It's not a guarantee.

1551
01:36:01,200 --> 01:36:07,360
You can describe things that would be horrible and it's feasible.

1552
01:36:07,360 --> 01:36:09,280
Yeah.

1553
01:36:09,280 --> 01:36:13,040
It could be the end of the human race, right?

1554
01:36:13,040 --> 01:36:15,960
Or it could be the beginning of the next race of this new thing.

1555
01:36:16,480 --> 01:36:23,600
Well, I mean, when I was born, we created nuclear weapons and people were concerned...

1556
01:36:23,600 --> 01:36:31,480
Very soon we had hydrogen weapons and we have enough hydrogen weapons to wipe out all humanity.

1557
01:36:31,480 --> 01:36:34,040
We still have that.

1558
01:36:34,040 --> 01:36:42,440
That didn't exist like a hundred years ago, well, it didn't exist 80 years ago.

1559
01:36:42,440 --> 01:36:48,960
So that is something that concerns me.

1560
01:36:48,960 --> 01:36:51,640
And you could do the same thing with the artificial intelligence.

1561
01:36:51,640 --> 01:36:55,080
It could also create something that would be very negative.

1562
01:36:55,080 --> 01:37:01,320
But what I'm getting at is like, what do you think life looks like if it's engineered?

1563
01:37:01,320 --> 01:37:06,960
What do you think human life looks like if it's engineered by a far superior intelligence?

1564
01:37:06,960 --> 01:37:10,400
And what would it change about what it means to be a person?

1565
01:37:10,400 --> 01:37:19,640
I mean, first of all, we would base it on what human beings are already.

1566
01:37:19,640 --> 01:37:24,360
So we'd become better versions of ourselves.

1567
01:37:24,360 --> 01:37:31,440
For example, we'd be able to overcome life-threatening diseases.

1568
01:37:31,440 --> 01:37:32,760
And we're actually working on that.

1569
01:37:32,760 --> 01:37:36,720
And that's going to go into high gear very soon.

1570
01:37:36,720 --> 01:37:42,760
Yes, but that's still being a human being.

1571
01:37:42,760 --> 01:37:52,920
If you're implementing large-scale artificial intelligence, you're essentially a superhuman.

1572
01:37:52,920 --> 01:37:54,040
You're a different thing.

1573
01:37:54,040 --> 01:37:56,880
You're not what we are.

1574
01:37:56,880 --> 01:37:57,880
If you have the computational power...

1575
01:37:57,880 --> 01:38:02,120
Well, if you're superhuman, you have the human being as part of it.

1576
01:38:03,040 --> 01:38:04,040
But this is the thing.

1577
01:38:04,040 --> 01:38:09,080
If you're engineering this artificial intelligence and you're engineering this with essentially

1578
01:38:09,080 --> 01:38:14,480
like a superior life form, it's going to look at it logically.

1579
01:38:14,480 --> 01:38:20,160
It's going to look at the issues that human beings have logically and say, well, we don't

1580
01:38:20,160 --> 01:38:21,160
need this.

1581
01:38:21,160 --> 01:38:22,160
This is a problem.

1582
01:38:22,160 --> 01:38:25,040
This is what we needed when we were primates, and we're not that anymore.

1583
01:38:25,040 --> 01:38:26,040
This new thing.

1584
01:38:26,040 --> 01:38:27,040
We're going to...

1585
01:38:27,040 --> 01:38:28,520
Who cares what the movie's like?

1586
01:38:28,520 --> 01:38:32,800
It's just a thing that's tricking your body into pretending that it's involved in drama,

1587
01:38:32,800 --> 01:38:33,800
but it's not really...

1588
01:38:33,800 --> 01:38:37,760
Well, you're making certain assumptions about what we'll create.

1589
01:38:37,760 --> 01:38:41,720
No, I'm just making an assumption.

1590
01:38:41,720 --> 01:38:50,360
I mean, in my mind, we would want to create better music and better art and better relationships.

1591
01:38:50,360 --> 01:38:56,320
Well, the relationships should be all perfect eventually if we keep going in this general

1592
01:38:56,320 --> 01:38:57,320
direction.

1593
01:38:57,320 --> 01:38:58,320
Yeah, perfect.

1594
01:38:58,320 --> 01:38:59,320
I mean...

1595
01:38:59,320 --> 01:39:02,840
But if you get artificial intelligence, we're all reading each other's minds and everyone's

1596
01:39:02,840 --> 01:39:04,320
working towards the same goal.

1597
01:39:04,320 --> 01:39:06,320
Well, no, you can't read each other's minds.

1598
01:39:06,320 --> 01:39:07,320
Ever?

1599
01:39:07,320 --> 01:39:08,320
I mean, we can create...

1600
01:39:08,320 --> 01:39:14,320
Yes, we can create privacy that's virtually unbreakable, and you could keep the privacy

1601
01:39:14,320 --> 01:39:15,320
to yourselves.

1602
01:39:15,320 --> 01:39:17,600
But can you do that as technology scales upward?

1603
01:39:17,600 --> 01:39:21,720
If it continues to move, I mean, it's difficult like your phone.

1604
01:39:21,720 --> 01:39:23,200
Anyone can listen to you on your phone.

1605
01:39:23,200 --> 01:39:25,400
I mean, anyone who has a significant technology...

1606
01:39:25,400 --> 01:39:28,560
Actually, it has pretty good technology already.

1607
01:39:28,560 --> 01:39:31,240
You can't really read someone else's phone.

1608
01:39:31,240 --> 01:39:32,240
You're definitely good.

1609
01:39:32,240 --> 01:39:37,040
Yeah, if you have Pegasus, you could hack into your phone easily, not hard at all.

1610
01:39:37,040 --> 01:39:39,840
The new software that they have, all they need is your phone number.

1611
01:39:39,840 --> 01:39:43,440
All they need is your phone number, and they can look at every text message you send, every

1612
01:39:43,440 --> 01:39:48,400
email you send, they can look at your camera, they can turn on your microphone, easy.

1613
01:39:48,400 --> 01:39:53,160
We have ways of keeping total privacy, and if it's not built into your phone now, it

1614
01:39:53,160 --> 01:39:54,160
will be.

1615
01:39:54,320 --> 01:39:56,960
But it's definitely not built into your phone now.

1616
01:39:56,960 --> 01:40:02,560
The security people that really understand the capabilities of intelligence agencies,

1617
01:40:02,560 --> 01:40:04,640
they 100% can listen to your phone.

1618
01:40:04,640 --> 01:40:06,360
100% can turn on your camera.

1619
01:40:06,360 --> 01:40:08,760
100% can record your voice.

1620
01:40:08,760 --> 01:40:10,840
Yes and no.

1621
01:40:10,840 --> 01:40:17,560
I mean, we have an ability to keep total privacy in a device.

1622
01:40:17,560 --> 01:40:19,040
But from who?

1623
01:40:19,040 --> 01:40:22,480
You can keep privacy from me, because I don't have access to your device.

1624
01:40:22,480 --> 01:40:26,680
But if I was working for an intelligence agency and I had access to a Pegasus program,

1625
01:40:26,680 --> 01:40:28,600
I am in your device.

1626
01:40:28,600 --> 01:40:31,320
Now, I've talked to people.

1627
01:40:31,320 --> 01:40:32,760
Only because it's not perfect.

1628
01:40:32,760 --> 01:40:37,480
We can actually build much better privacy than exists today.

1629
01:40:37,480 --> 01:40:41,640
But the privacy that we have today is far less than the privacy that we had before we

1630
01:40:41,640 --> 01:40:42,640
had phones.

1631
01:40:42,640 --> 01:40:47,360
I don't really quite agree with that.

1632
01:40:47,360 --> 01:40:49,560
How so?

1633
01:40:49,560 --> 01:40:54,280
If you didn't have a phone and you were at home having a conversation, a sensitive conversation

1634
01:40:54,280 --> 01:40:57,920
about maybe you didn't pay as much taxes as you should, there's no way anybody would

1635
01:40:57,920 --> 01:40:59,200
hear that.

1636
01:40:59,200 --> 01:41:00,520
But now your phone hears that.

1637
01:41:00,520 --> 01:41:04,120
If you have an Alexa in your home, your Alexa hears you say that.

1638
01:41:04,120 --> 01:41:12,640
People have been charged with crimes because Alexa heard them committing murder.

1639
01:41:12,640 --> 01:41:18,480
We actually know how to create perfect privacy in your phone.

1640
01:41:18,480 --> 01:41:23,200
If your phone doesn't have that, that's just an imperfection in the way we're building

1641
01:41:23,200 --> 01:41:24,400
these things now.

1642
01:41:24,400 --> 01:41:25,680
But it's not just an imperfection.

1643
01:41:25,680 --> 01:41:30,040
It's sort of built into the program itself, because that's what fuels the algorithm, is

1644
01:41:30,040 --> 01:41:32,480
that it has access to all of your data.

1645
01:41:32,480 --> 01:41:37,320
It has access to all of what you're interested in, what you like, what you don't like, you

1646
01:41:37,320 --> 01:41:38,320
can opt out of it.

1647
01:41:38,320 --> 01:41:39,320
Especially you.

1648
01:41:39,320 --> 01:41:40,320
You've got a Google phone.

1649
01:41:40,320 --> 01:41:45,520
It's just a net scooping up information.

1650
01:41:45,520 --> 01:41:50,720
We know how to build perfect privacy.

1651
01:41:50,720 --> 01:42:02,920
How do we do it?

1652
01:42:02,920 --> 01:42:08,000
I mean if it's not built into your phone now, it should be.

1653
01:42:08,000 --> 01:42:10,960
Because they don't want it to be built in there, because there's an actual business

1654
01:42:10,960 --> 01:42:13,160
model and it not being built in there.

1655
01:42:13,160 --> 01:42:20,920
Okay, but it can be done, and if people want that, it'll happen.

1656
01:42:20,920 --> 01:42:23,840
But you recognize the financial incentive in not doing that, right?

1657
01:42:23,840 --> 01:42:28,120
Because that's what a company like Google, for instance, that's where they make the majority

1658
01:42:28,120 --> 01:42:34,560
of their money is from data, or a lot of their money, I should say.

1659
01:42:34,560 --> 01:42:45,960
There's actually a lot of effort that goes into keeping what's on your phone private.

1660
01:42:45,960 --> 01:42:46,960
It's not that easy.

1661
01:42:46,960 --> 01:42:51,320
Private from some people, but not really private.

1662
01:42:51,320 --> 01:42:53,800
It's only private until they want to listen.

1663
01:42:53,800 --> 01:42:58,600
And now the capability of listening to your phone is super easy.

1664
01:42:58,600 --> 01:42:59,600
Not really.

1665
01:42:59,600 --> 01:43:00,600
No?

1666
01:43:00,600 --> 01:43:02,160
With the Pegasus program?

1667
01:43:02,160 --> 01:43:04,280
It's very easy.

1668
01:43:04,280 --> 01:43:08,000
Well that has to do with imperfections in the way phones are created.

1669
01:43:08,000 --> 01:43:10,320
Right, but I think it's a feature.

1670
01:43:10,320 --> 01:43:15,520
I think part of the feature is that they want as much data from you, and knowing about what

1671
01:43:15,520 --> 01:43:16,840
you're doing, what you're talking about.

1672
01:43:16,840 --> 01:43:25,120
Have you ever had a conversation with someone and you see an ad for that thing on Google?

1673
01:43:25,120 --> 01:43:26,120
That happens.

1674
01:43:26,120 --> 01:43:27,800
Yes, but...

1675
01:43:27,800 --> 01:43:32,840
So something's going on where it's listening to your conversations.

1676
01:43:33,120 --> 01:43:34,680
Picking up on keywords.

1677
01:43:34,680 --> 01:43:36,840
It's not picking up on everything.

1678
01:43:36,840 --> 01:43:37,840
Not yet.

1679
01:43:37,840 --> 01:43:38,840
Well, it's not unless it wants to.

1680
01:43:38,840 --> 01:43:43,180
Like I said, if they're using a program, an intelligence program, to gather information

1681
01:43:43,180 --> 01:43:44,880
from your phone, it is.

1682
01:43:44,880 --> 01:43:45,880
And you're basically...

1683
01:43:45,880 --> 01:43:50,040
You got a little spy that you carry around with you everywhere you go.

1684
01:43:50,040 --> 01:43:51,040
Unless you're using...

1685
01:43:51,040 --> 01:43:57,280
I mean, if you think that's a major issue, we could build phones that are impossible

1686
01:43:57,280 --> 01:44:01,240
to spy on.

1687
01:44:01,240 --> 01:44:02,760
Maybe.

1688
01:44:03,680 --> 01:44:07,520
Well, there are some phones that like graphing, do you know about that?

1689
01:44:07,520 --> 01:44:12,600
Do you know about people that they take a Google phone and they put a different Linux-based

1690
01:44:12,600 --> 01:44:16,400
operating system on it, makes it much more difficult to track, and there's multi-levels

1691
01:44:16,400 --> 01:44:17,400
of protection.

1692
01:44:17,400 --> 01:44:22,000
There's a bunch of phones that are being made that are security phones.

1693
01:44:22,000 --> 01:44:24,240
But you lose access to apps.

1694
01:44:24,240 --> 01:44:28,720
You lose access to a lot of the features that people rely on when it comes to phones.

1695
01:44:28,720 --> 01:44:32,560
For instance, like if you have GPS on your phone, as soon as you're using GPS, you're

1696
01:44:32,560 --> 01:44:33,560
easy to find.

1697
01:44:33,560 --> 01:44:34,560
Right?

1698
01:44:34,560 --> 01:44:35,560
So you lose that privacy.

1699
01:44:35,560 --> 01:44:39,480
If they want to know where Ray's phone is, they know exactly where Ray's phone is.

1700
01:44:39,480 --> 01:44:43,640
And that's where you are, and you're with your phone, they've got you tracked everywhere

1701
01:44:43,640 --> 01:44:44,640
you go.

1702
01:44:44,640 --> 01:44:45,640
It's complicated.

1703
01:44:45,640 --> 01:44:49,400
If this were a major issue, we could definitely overcome that.

1704
01:44:49,400 --> 01:44:54,920
I think it's a major issue, but I don't think it's a major concern for most people.

1705
01:44:54,920 --> 01:44:57,480
But it's because they reap the benefits of it.

1706
01:44:57,480 --> 01:45:00,440
Like the algorithm is specifically tailored to their interests.

1707
01:45:00,440 --> 01:45:04,200
That's how we find the kinds of things we put on phones.

1708
01:45:04,200 --> 01:45:05,200
Right.

1709
01:45:05,200 --> 01:45:08,960
But you can't opt out of it unless you just decide to get a flip phone.

1710
01:45:08,960 --> 01:45:13,000
But even if you do, they figure out where you are to triangulate you from cell phone

1711
01:45:13,000 --> 01:45:17,200
towers.

1712
01:45:17,200 --> 01:45:23,080
I mean, we give up certain things in order to get the benefits of phones.

1713
01:45:23,080 --> 01:45:27,240
Yeah, we do.

1714
01:45:27,240 --> 01:45:32,320
If what you're giving up is a grave concern, we could overcome that.

1715
01:45:32,320 --> 01:45:35,240
We know how to do that.

1716
01:45:35,240 --> 01:45:37,800
Yeah.

1717
01:45:37,800 --> 01:45:44,520
If people agree that the benefit of overcoming that outweighs the loss in the financial loss

1718
01:45:44,520 --> 01:45:47,800
that you would have with not having access to everybody's data and information.

1719
01:45:47,800 --> 01:45:53,680
Well, I mean, what you're giving up is a certain type of data that you want, a certain

1720
01:45:53,680 --> 01:46:04,200
type of capability that you could buy, so they can advertise that to you and people

1721
01:46:04,200 --> 01:46:10,120
feel that that's okay.

1722
01:46:10,120 --> 01:46:18,000
But for example, keeping your email private is quite feasible.

1723
01:46:18,000 --> 01:46:21,680
It's possible, but it's also easy to hack.

1724
01:46:21,680 --> 01:46:23,360
People could be reading your emails all the time.

1725
01:46:23,360 --> 01:46:28,040
We should probably assume that they do.

1726
01:46:28,040 --> 01:46:40,920
Well, it's a complicated issue, but we keep, for example, your emails private, and generally

1727
01:46:40,920 --> 01:46:43,480
we actually do do that.

1728
01:46:43,480 --> 01:46:45,360
Generally, for most people.

1729
01:46:45,360 --> 01:46:51,320
But my point is, as this technology scales upward, when you have greater and greater

1730
01:46:51,360 --> 01:46:57,880
computational power, and then you're also integrated with this technology, how does that

1731
01:46:57,880 --> 01:47:06,360
keep whatever group is in charge from being able to essentially access the thing that

1732
01:47:06,360 --> 01:47:09,920
is inside your head now?

1733
01:47:09,920 --> 01:47:16,400
If you have a technology that's going to be upgraded and you're going to get new software

1734
01:47:16,480 --> 01:47:22,840
that's going to keep improving as time goes on, what kind of privacy would be involved

1735
01:47:22,840 --> 01:47:26,040
in that if you're literally having something that can get into your brain?

1736
01:47:26,040 --> 01:47:30,280
If most people can't get into your brain, can intelligence agencies get into your brain?

1737
01:47:30,280 --> 01:47:33,840
Can foreign governments get into your brain?

1738
01:47:33,840 --> 01:47:35,680
What does that look like?

1739
01:47:35,680 --> 01:47:37,160
I'm not looking at this as a negative.

1740
01:47:37,160 --> 01:47:42,720
I'm just saying, if you're just looking at this completely objectively, what are the

1741
01:47:42,720 --> 01:47:45,240
possibilities that this could look like?

1742
01:47:45,280 --> 01:47:48,960
I'm trying to paint a weird picture of what this could look like.

1743
01:47:48,960 --> 01:47:52,760
Well, a lot of things you want to share.

1744
01:47:52,760 --> 01:47:59,760
Music and so on, it's desirable to share that, and you'd want that to be shared.

1745
01:47:59,760 --> 01:48:03,160
If you didn't share anything, you'd be pretty lonely.

1746
01:48:03,160 --> 01:48:04,160
Sure.

1747
01:48:04,160 --> 01:48:09,280
What do you think about the potential for a universal language?

1748
01:48:10,200 --> 01:48:17,200
One of the things that holds people back is the Rosetta Stone, the Tower of Battle.

1749
01:48:17,200 --> 01:48:20,640
The idea that we can't really understand what all these other people are saying.

1750
01:48:20,640 --> 01:48:21,640
We don't know how they think.

1751
01:48:21,640 --> 01:48:28,160
If we can develop a universal worldwide language through this, do you think it's feasible?

1752
01:48:28,160 --> 01:48:30,800
All languages that we have were created.

1753
01:48:30,800 --> 01:48:34,560
We have a certain means of changing one language into another.

1754
01:48:34,560 --> 01:48:35,560
Right.

1755
01:48:35,560 --> 01:48:36,560
That's what I'm saying.

1756
01:48:36,720 --> 01:48:40,640
We're doing that now with some, like Google does that with Translate, and the new Samsung

1757
01:48:40,640 --> 01:48:43,040
phones do that in real time.

1758
01:48:43,040 --> 01:48:44,040
Yeah.

1759
01:48:44,040 --> 01:48:52,040
I wrote about that in 1989, that we'd be able to have universal translation between languages.

1760
01:48:52,720 --> 01:48:56,200
But do you think that the adoption of a universal language?

1761
01:48:56,200 --> 01:48:58,000
It's not perfect, but it's actually pretty good.

1762
01:48:58,000 --> 01:48:59,080
It's pretty good.

1763
01:48:59,080 --> 01:49:05,960
But there's also context that's missing, because there's different cultural significance.

1764
01:49:06,000 --> 01:49:08,960
There's different ways that people say things.

1765
01:49:08,960 --> 01:49:14,040
There's gendered language and other nationalities used and other countries used.

1766
01:49:14,040 --> 01:49:17,680
Well, you could try to get that into the language translation as well.

1767
01:49:17,680 --> 01:49:19,400
You can, but it's a little bit imperfect, right?

1768
01:49:19,400 --> 01:49:20,840
This is what I'm saying.

1769
01:49:20,840 --> 01:49:25,520
You might have something that's said very quickly, and you'd have to translate it into

1770
01:49:25,520 --> 01:49:30,000
much longer language in order to capture that.

1771
01:49:30,000 --> 01:49:35,480
But would a universal language be possible?

1772
01:49:35,480 --> 01:49:37,960
If you're creating something...

1773
01:49:37,960 --> 01:49:39,320
Why would you need that?

1774
01:49:39,320 --> 01:49:45,400
Because what we have, all of our language, is pretty flawed, ultimately.

1775
01:49:45,400 --> 01:49:49,200
We use it, but how many versions of your do we have?

1776
01:49:49,200 --> 01:49:53,480
There's a bunch of different weird things about language that's imperfect, because it's

1777
01:49:53,480 --> 01:49:54,480
old.

1778
01:49:54,480 --> 01:49:56,160
It's like old technology.

1779
01:49:56,160 --> 01:50:03,560
If we decided to make a better version of language through artificial technology and

1780
01:50:03,560 --> 01:50:08,240
say, listen, instead of trying to translate everything, now that we're super powerful

1781
01:50:08,240 --> 01:50:13,120
intelligent beings that are enhanced by artificial intelligence, let's create a better, more

1782
01:50:13,120 --> 01:50:16,720
superior, universally adopted language.

1783
01:50:16,720 --> 01:50:17,720
Maybe.

1784
01:50:17,720 --> 01:50:21,920
Do you see that as a major need?

1785
01:50:21,920 --> 01:50:22,920
Yeah, I do.

1786
01:50:22,920 --> 01:50:23,920
Yeah.

1787
01:50:23,920 --> 01:50:24,920
I think that would change a lot.

1788
01:50:24,920 --> 01:50:30,880
I mean, we'd lose all the amazing nuances of cultures, which I don't think is good for

1789
01:50:30,880 --> 01:50:34,280
us as human beings, but we're not going to be human beings.

1790
01:50:34,280 --> 01:50:39,960
So maybe it would be better if we could communicate exactly the way we prefer to.

1791
01:50:39,960 --> 01:50:41,720
Well, we would be human beings.

1792
01:50:41,720 --> 01:50:48,000
And in my mind, the human being is someone who can change both ourselves and means of

1793
01:50:48,000 --> 01:51:01,760
communication to enjoy better means of expressing art and culture and so on.

1794
01:51:01,760 --> 01:51:05,760
No other animal really quite does that, except human beings.

1795
01:51:05,760 --> 01:51:10,840
So that is an essence of what it means to be a human being.

1796
01:51:10,840 --> 01:51:11,840
For now.

1797
01:51:11,840 --> 01:51:16,000
But when you're a mind reading eagle and you're flying around, are you really a human being

1798
01:51:16,000 --> 01:51:17,000
anymore?

1799
01:51:17,000 --> 01:51:20,640
Yes, because we are able to change ourselves.

1800
01:51:20,640 --> 01:51:24,080
So that's just a new definition of what a human being is.

1801
01:51:24,080 --> 01:51:30,800
What are your thoughts on simulation theory?

1802
01:51:30,800 --> 01:51:40,440
If you mean that we're living in a simulation, well, first of all, some people believe that

1803
01:51:40,440 --> 01:52:04,360
we can express physics as formulas and that the universe is actually able to, is capable

1804
01:52:04,360 --> 01:52:12,800
of computation and therefore everything that happens is the result of some computation.

1805
01:52:12,800 --> 01:52:25,680
And therefore, the universe is capable of, we are living in something that is computable.

1806
01:52:25,680 --> 01:52:31,800
And there's some debate about whether that's feasible, but that doesn't necessarily mean

1807
01:52:32,720 --> 01:52:34,520
that we're living in a simulation.

1808
01:52:34,520 --> 01:52:42,760
Generally, if you say we're living in a simulation, you assume that some other place and teenagers

1809
01:52:42,760 --> 01:52:47,880
in that world like to create a simulation.

1810
01:52:47,880 --> 01:52:53,400
So they created a simulation that we live in and you want to make sure that they don't

1811
01:52:53,400 --> 01:52:58,680
turn the simulation off, so we'd have to be interesting to them, and so they keep the

1812
01:52:58,680 --> 01:53:02,960
simulation going.

1813
01:53:02,960 --> 01:53:13,520
But the whole universe could be capable of simulating reality and that's what we live

1814
01:53:13,520 --> 01:53:20,520
in and it's not a game, it's just the way the universe works.

1815
01:53:20,520 --> 01:53:25,400
I mean, what would the difference be if we lived in a simulation?

1816
01:53:26,120 --> 01:53:32,360
This is what I'm saying, if we can and we're on our way to creating something that is indiscernible

1817
01:53:32,360 --> 01:53:37,520
from reality itself, I don't think we're that far away from that, many decades away from

1818
01:53:37,520 --> 01:53:43,440
having some sort of a virtual experience that's indiscernible from regular reality.

1819
01:53:43,440 --> 01:53:46,960
We try to do that with games and so on.

1820
01:53:46,960 --> 01:53:52,720
And those are far superior to what they were, I mean, I'm younger than you, but I can remember

1821
01:53:53,640 --> 01:53:56,000
Pong, it was groundbreaking.

1822
01:53:56,000 --> 01:53:59,680
You could play a video game on your television, this is crazy, it was so nuts.

1823
01:53:59,680 --> 01:54:01,480
And we're way beyond that now.

1824
01:54:01,480 --> 01:54:07,520
Now you look at the Unreal 5 engine, it's insane how beautiful it is and how incredible

1825
01:54:07,520 --> 01:54:08,800
and what the capabilities are.

1826
01:54:08,800 --> 01:54:12,480
So if you live in that, that's kind of a simulation that we live in.

1827
01:54:12,480 --> 01:54:16,400
Right, but as you expand that further and you get to the point where you're actually

1828
01:54:16,400 --> 01:54:23,320
in a simulation and that your life is not this carbon based biological life, feeling

1829
01:54:23,320 --> 01:54:28,160
and texture that you think it is, whether you're really a part of this thing that's

1830
01:54:28,160 --> 01:54:33,040
been created, this is where it gets real weird with like probability theory, right?

1831
01:54:33,040 --> 01:54:40,040
Because they think that if a simulation is possible, it's more likely it's already happened.

1832
01:54:40,960 --> 01:54:47,960
I mean, there's really an unlimited amount of things that we could simulate and experience

1833
01:54:47,960 --> 01:54:54,960
and so it's hard to say we're living in a simulation because a lot of what we're doing

1834
01:54:54,960 --> 01:55:03,960
is it's living in a computational world anyway, so it's basically being simulated in a way.

1835
01:55:04,960 --> 01:55:11,960
And if you were some sort of an alien life form, wouldn't that be the way you go, instead

1836
01:55:12,400 --> 01:55:19,400
of like taking physical metal crafts and shooting them off into space, wouldn't you sort of

1837
01:55:20,760 --> 01:55:27,240
create artificial space, create artificial worlds, create something that exists in the

1838
01:55:27,240 --> 01:55:31,840
sense that you experience it and it's indiscernible to the person experiencing it.

1839
01:55:31,840 --> 01:55:36,320
So if you're intelligent enough, you'll be able to tell what's being simulated and what's

1840
01:55:36,320 --> 01:55:37,320
not.

1841
01:55:37,320 --> 01:55:43,320
Up to a point, until it actually does all the same things that regular reality does,

1842
01:55:43,320 --> 01:55:49,440
it just does it through technology and maybe that's what the universe is.

1843
01:55:49,440 --> 01:55:56,440
But that's okay, we could still experience what's happening and we could also experience

1844
01:55:57,200 --> 01:56:04,200
people doing galaxy-wide engineering, which not all of which would be simulated.

1845
01:56:04,400 --> 01:56:09,360
So the galaxy-wide engineering is the main thing that you look at to the point where

1846
01:56:09,360 --> 01:56:14,000
I don't see any evidence for life outside. Well, there's definitely no real evidence

1847
01:56:14,000 --> 01:56:19,560
that we see other than these people that talk about UFOs, UAPs and pilots and all these

1848
01:56:19,560 --> 01:56:21,000
people that say that there's these things.

1849
01:56:21,080 --> 01:56:30,080
We don't see any evidence that life is simulated outside of our own life. We can simulate things

1850
01:56:30,160 --> 01:56:37,160
and experience it. We don't see any evidence that other beings are doing that elsewhere.

1851
01:56:39,080 --> 01:56:41,480
This is based on such limited data though, right?

1852
01:56:41,480 --> 01:56:47,480
I mean, look at what limited data we just have of Mars, rover rolling around, satellites

1853
01:56:47,480 --> 01:56:53,040
in orbit. It's very limited data with something that's just one planet over. We don't really

1854
01:56:53,040 --> 01:56:56,040
have the data to understand what's going on in Alpha Centauri.

1855
01:56:56,040 --> 01:57:03,040
It's possible that this simulated life elsewhere. I mean, we don't see any evidence for it,

1856
01:57:04,920 --> 01:57:06,600
but it's possible.

1857
01:57:06,600 --> 01:57:09,640
Is it something that intrigues you or do you just look at it like there's no evidence

1858
01:57:09,640 --> 01:57:11,840
so I'm not going to concentrate on that?

1859
01:57:12,800 --> 01:57:18,280
I'm very interested to see what we can achieve because we're actually, I can see that we're

1860
01:57:18,280 --> 01:57:25,280
on that path. So it doesn't take a lot of curiosity in my part to imagine other people

1861
01:57:31,440 --> 01:57:38,440
simulating life and enjoying it. I'm much more interested to see what will be feasible

1862
01:57:39,280 --> 01:57:44,760
for us and we're not that far away from it.

1863
01:57:44,760 --> 01:57:51,260
So over the next four years, five years, you think we're going to be able to far surpass

1864
01:57:51,260 --> 01:57:57,660
the ability of human beings. We're going to be able to stop aging and then eventually

1865
01:57:57,660 --> 01:58:04,660
reverse aging, and then 2045 comes along. What does that look like?

1866
01:58:05,180 --> 01:58:11,780
Well, one of the reasons we call it singularity is because we really don't know. I mean, that's

1867
01:58:11,780 --> 01:58:18,780
why it's called a singularity. Singularity in physics is where you have a black hole,

1868
01:58:20,420 --> 01:58:25,060
no energy can get out of a black hole, and therefore we don't really know what's going

1869
01:58:25,060 --> 01:58:30,940
on in it and we call it a singularity. So this is a historical singularity based on

1870
01:58:30,940 --> 01:58:36,460
the kinds of things we've been talking about. And again, we don't really know what that

1871
01:58:36,460 --> 01:58:41,460
will be like and that's why we call it a singularity.

1872
01:58:41,460 --> 01:58:44,460
Do you have any theories?

1873
01:58:44,460 --> 01:58:51,460
Another way of looking at it, I mean, we have mice and they have experiences. It's a limited

1874
01:58:51,460 --> 01:58:58,460
amount of complexity because that particular species hasn't really evolved very much. And

1875
01:59:00,860 --> 01:59:07,860
we'll be going beyond what human beings can do. So to ask a human being what it's like

1876
01:59:08,060 --> 01:59:15,060
to be a human being, what it's like to be a human being, what it's like to be a human

1877
01:59:21,600 --> 01:59:27,180
being in singularity, it's like asking a mouse, what would it be like if you were to

1878
01:59:27,180 --> 01:59:33,940
evolve to become like a human? Now, if you ask a mouse that, it wouldn't understand

1879
01:59:33,940 --> 01:59:39,380
the question, it wouldn't be able to formulate an answer, it wouldn't even be able to think

1880
01:59:39,380 --> 01:59:46,380
about it. And asking a card human being what it's going to be like to live in a singularity

1881
01:59:46,740 --> 01:59:52,300
is a little bit like that.

1882
01:59:52,300 --> 01:59:57,180
So it's just who knows? It's going to be wild.

1883
01:59:57,180 --> 02:00:00,860
Be able to do things that we can't even imagine today, right?

1884
02:00:00,860 --> 02:00:07,340
Well, I'm very excited about it. Even though it's scary, I know I ask a lot of tough questions

1885
02:00:07,340 --> 02:00:11,020
about this because these are my own questions. This is like what bounces around inside my

1886
02:00:11,020 --> 02:00:12,020
own head.

1887
02:00:12,020 --> 02:00:18,620
Well, that's why I'm excited about it also because it basically means more intelligence

1888
02:00:18,620 --> 02:00:22,980
and we'll be able to think about things that we can't even imagine today.

1889
02:00:22,980 --> 02:00:24,660
And solve problems.

1890
02:00:24,660 --> 02:00:29,660
Yes, including like dying, for example.

1891
02:00:29,660 --> 02:00:35,660
Well, listen, man, I'm glad you're out there. It's very important that people have access

1892
02:00:35,660 --> 02:00:40,020
to this kind of thinking and you've dedicated your whole life to this. In this book, Ray

1893
02:00:40,020 --> 02:00:44,380
Kurzweil, the Singularity is Near When We Merge with AI. It's available now. Did you

1894
02:00:44,380 --> 02:00:47,940
do the audio version of it?

1895
02:00:47,940 --> 02:00:49,500
That's being worked on now.

1896
02:00:49,500 --> 02:00:50,500
Are you doing it?

1897
02:00:50,500 --> 02:00:53,500
It's coming out June.

1898
02:00:53,500 --> 02:00:54,500
No.

1899
02:00:54,500 --> 02:00:59,500
No? I want to hear it in your voice. It's your words.

1900
02:00:59,500 --> 02:01:01,500
Yeah, that's what people say.

1901
02:01:01,500 --> 02:01:05,140
Yeah, why don't you do it? You should do it. You know what you should do? Just get AI to

1902
02:01:05,140 --> 02:01:09,500
do it. Why waste all that time sitting around doing it? Basically, they could do it now.

1903
02:01:09,980 --> 02:01:10,980
They did that yesterday.

1904
02:01:10,980 --> 02:01:16,500
100%. Look, and they could take your voice from this podcast and do this book in an

1905
02:01:16,500 --> 02:01:22,540
audio version. Easy. Do you know what they're doing now? It's Spotify. They're translating

1906
02:01:22,540 --> 02:01:28,580
this podcast. They're going to translate it to German, French, and Spanish. And it's

1907
02:01:28,580 --> 02:01:32,900
going to be like your voice in perfect Spanish, my voice in perfect Spanish.

1908
02:01:32,900 --> 02:01:36,220
This actually came up yesterday. I'll think about that.

1909
02:01:36,220 --> 02:01:37,220
Pretty wild.

1910
02:01:37,220 --> 02:01:38,220
Yeah.

1911
02:01:38,220 --> 02:01:39,220
It's 100%. You should do that.

1912
02:01:39,420 --> 02:01:43,620
My friend Duncan does that all the time. He'll have friends, text friends, or send a voice

1913
02:01:43,620 --> 02:01:47,980
message as a fake voice message. That's ridiculous. You know, talking about how he's marrying his

1914
02:01:47,980 --> 02:01:52,820
cat or something like that. It's just like, just, but he does it with AI and it sounds

1915
02:01:52,820 --> 02:01:55,980
exactly like whoever that person is.

1916
02:01:55,980 --> 02:02:00,940
So that's the, that's the solution. Have AI read your, of course you should have AI read

1917
02:02:00,940 --> 02:02:06,940
your book. I can't believe we even would think of you sitting down for 40 hours or whatever

1918
02:02:07,060 --> 02:02:12,140
it would take. It'd probably take more than that to read this whole book. And then if

1919
02:02:12,140 --> 02:02:14,140
you mess up, you got to go back and start again.

1920
02:02:14,140 --> 02:02:18,540
Well, certainly that's going to be feasible. Whether that's feasible now, they could get

1921
02:02:18,540 --> 02:02:20,540
all the nuances correct.

1922
02:02:20,540 --> 02:02:22,540
I bet it's pretty close.

1923
02:02:22,540 --> 02:02:23,540
Yeah.

1924
02:02:23,540 --> 02:02:24,540
I bet it's pretty close right now.

1925
02:02:24,540 --> 02:02:28,540
But it has to be very close because we're doing it like in the next month or so.

1926
02:02:28,540 --> 02:02:31,540
I bet. Don't you think they could do it, Jamie?

1927
02:02:31,540 --> 02:02:35,660
Yeah. I think they could do it right now. Listen, Ray, I appreciate you very much. Thank

1928
02:02:35,740 --> 02:02:39,540
you very much for being here. Thank you for your time. And thank you for this book. When

1929
02:02:39,540 --> 02:02:41,540
is it available?

1930
02:02:41,540 --> 02:02:42,540
June 24th.

1931
02:02:42,540 --> 02:02:44,540
June, I got an early copy, kids.

1932
02:02:44,540 --> 02:02:45,540
Yeah.

1933
02:02:45,540 --> 02:02:46,540
Thank you, sir. Really appreciate you. Thank you very much.

1934
02:02:46,540 --> 02:02:47,540
My pleasure.

1935
02:02:47,540 --> 02:02:56,540
Bye, everybody.

