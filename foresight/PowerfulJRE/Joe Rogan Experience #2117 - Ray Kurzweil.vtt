WEBVTT

00:00.000 --> 00:04.000
Joe Rogan Podcast, check it out!

00:04.000 --> 00:06.000
The Joe Rogan Experience.

00:06.000 --> 00:10.000
Train by day! Joe Rogan Podcast by night! All day!

00:12.000 --> 00:14.000
Good to see you, sir.

00:14.000 --> 00:15.000
Great to see you.

00:15.000 --> 00:18.000
I was telling you before, I'm admiring your suspenders,

00:18.000 --> 00:20.000
and you told me you have how many pairs of these things?

00:20.000 --> 00:21.000
30 of them, yeah.

00:21.000 --> 00:22.000
How did you?

00:22.000 --> 00:23.000
I wear them every day.

00:23.000 --> 00:24.000
Do you really? Every day?

00:24.000 --> 00:25.000
Yeah.

00:25.000 --> 00:27.000
Why do you like suspenders?

00:27.000 --> 00:29.000
Practicality thing?

00:29.000 --> 00:36.000
No, it expresses my personality.

00:36.000 --> 00:45.000
And different ones have different personalities

00:45.000 --> 00:48.000
that express how I feel that day.

00:48.000 --> 00:50.000
I see, so it's just another style point.

00:50.000 --> 00:51.000
Yeah.

00:51.000 --> 00:52.000
See the reason why I was asking?

00:52.000 --> 00:55.000
But you don't see any hand-painted suspenders.

00:55.000 --> 00:57.000
Have you ever seen one?

00:57.000 --> 00:58.000
I don't know.

00:58.000 --> 01:00.000
I would have not noticed.

01:00.000 --> 01:02.000
I only noticed because you were here.

01:02.000 --> 01:04.000
I'm not really a suspender aficionado.

01:04.000 --> 01:08.000
But the reason why I'm asking is because you're basically a technologist.

01:08.000 --> 01:10.000
I mean, you know a lot about technology,

01:10.000 --> 01:17.000
when you think that suspenders are kind of outdated tech.

01:17.000 --> 01:19.000
Well, people like them.

01:19.000 --> 01:20.000
Clearly.

01:20.000 --> 01:21.000
Yeah.

01:21.000 --> 01:24.000
And I'm surprised I haven't caught on.

01:24.000 --> 01:27.000
But you have to have somebody who can actually paint them.

01:27.000 --> 01:30.000
I mean, these are hand-painted suspenders.

01:30.000 --> 01:33.000
So the ones that you have, right here, these are hand-painted?

01:33.000 --> 01:34.000
Yeah.

01:34.000 --> 01:35.000
Interesting.

01:35.000 --> 01:36.000
Okay, so that's part of it.

01:36.000 --> 01:37.000
So you're wearing art.

01:37.000 --> 01:38.000
Exactly.

01:38.000 --> 01:39.000
Got it.

01:39.000 --> 01:40.000
So...

01:40.000 --> 01:42.000
And art is part of technology.

01:42.000 --> 01:45.000
We're using technology to create art now, so...

01:45.000 --> 01:46.000
That's true.

01:46.000 --> 01:47.000
And it's...

01:47.000 --> 01:49.000
In fact, the very first...

01:49.000 --> 01:56.000
I mean, I've been now in AI for 61 years, which is actually a record.

01:56.000 --> 02:04.000
And the first thing I did was create something that could write music.

02:04.000 --> 02:09.000
Writing music now with AI is a major field today,

02:09.000 --> 02:13.000
but this was actually the first time that it had ever been done.

02:13.000 --> 02:16.000
Yeah, that was one of your many inventions.

02:16.000 --> 02:18.000
That was the first one, yeah.

02:18.000 --> 02:20.000
And why did you go about doing that?

02:20.000 --> 02:25.000
What was your desire to create artificial intelligence music?

02:25.000 --> 02:28.000
Well, my father was a musician,

02:28.000 --> 02:31.000
and I felt this would be a good way to relate to him.

02:31.000 --> 02:36.000
And he actually worked with me on it.

02:36.000 --> 02:38.000
And you could feed in music,

02:38.000 --> 02:42.000
like you could feed in, let's say, Mozart or Chopin,

02:42.000 --> 02:47.000
and I would figure out how they created melodies

02:47.000 --> 02:50.000
and then write melodies in the same style.

02:50.000 --> 02:54.000
So you could actually tell this is Mozart, this is Chopin.

02:54.000 --> 03:01.000
It wasn't as good, but it's the first time that that had been done.

03:01.000 --> 03:03.000
It wasn't as good then.

03:03.000 --> 03:05.000
What are the capabilities now?

03:05.000 --> 03:08.000
Because now they can do some pretty extraordinary things.

03:08.000 --> 03:12.000
Yeah, it's still not up to what humans can do,

03:12.000 --> 03:14.000
but it's getting there,

03:14.000 --> 03:17.000
and it's really, it's pleasant to listen to.

03:17.000 --> 03:23.000
We still have a while to do art, both art, music, so on.

03:23.000 --> 03:29.000
Well, one of the main arguments against AI art comes from actual artists

03:29.000 --> 03:32.000
who are upset that what essentially they're doing is they're,

03:32.000 --> 03:36.000
like you could say, right, draw a paint,

03:36.000 --> 03:40.000
or create a painting in the style of Frank Frasetta, for instance.

03:40.000 --> 03:45.000
And what it would be, they would take all of Frasetta's work that he's ever done,

03:45.000 --> 03:48.000
which is all documented on the internet,

03:48.000 --> 03:52.000
and then you create an image that's representative of that.

03:52.000 --> 03:55.000
So you're essentially, in one way or another,

03:55.000 --> 03:58.000
you're kind of taking from the art.

03:58.000 --> 04:00.000
Right, but it's not quite as good.

04:00.000 --> 04:02.000
It will be as good.

04:02.000 --> 04:07.000
I think we'll match human experience by 2029.

04:07.000 --> 04:11.000
That's been my idea.

04:11.000 --> 04:14.000
It's not as good.

04:14.000 --> 04:16.000
Which is the best image generator right now, Jamie?

04:16.000 --> 04:18.000
Pull one up.

04:18.000 --> 04:21.000
They really change almost from day to day right now,

04:21.000 --> 04:23.000
but mid-journey was the most popular one at first,

04:23.000 --> 04:28.000
and then Dolly, I think, is a really good one too.

04:28.000 --> 04:30.000
Mid-journey is incredibly impressive.

04:30.000 --> 04:32.000
Incredibly impressive graphics.

04:32.000 --> 04:34.000
I've seen some of the mid-journey stuff.

04:34.000 --> 04:36.000
It's mind-blowing.

04:36.000 --> 04:38.000
Still not quite as good.

04:38.000 --> 04:40.000
But boys, it's so much better than it was five years ago.

04:40.000 --> 04:42.000
That's what's scary. It's so quick.

04:42.000 --> 04:45.000
I mean, it's never going to reach its limit.

04:45.000 --> 04:46.000
We're not going to get to a point,

04:46.000 --> 04:48.000
okay, this is how good it's going to be.

04:48.000 --> 04:51.000
It's going to keep getting better.

04:51.000 --> 04:53.000
And what would that look like?

04:53.000 --> 04:55.000
If it can get to a certain point,

04:55.000 --> 04:59.000
it will far exceed what human creativity is capable of.

04:59.000 --> 05:04.000
Yes, I mean, when we reach the ability of humans,

05:04.000 --> 05:06.000
it's not going to just match one human.

05:06.000 --> 05:08.000
It's going to match all humans.

05:08.000 --> 05:11.000
It's going to do everything that any human can do.

05:11.000 --> 05:14.000
If it's playing a game like Go,

05:14.000 --> 05:16.000
it's going to play it better than any human.

05:16.000 --> 05:18.000
Right. Well, that's already been proven, right?

05:18.000 --> 05:20.000
That they invented moves.

05:20.000 --> 05:24.000
AI has invented moves that have now been implemented by humans

05:24.000 --> 05:26.000
in a very complex game that they never thought

05:26.000 --> 05:28.000
that AI was going to be able to be,

05:28.000 --> 05:30.000
because it requires so much creativity.

05:30.000 --> 05:33.000
Right. Arthur, we're not quite there,

05:33.000 --> 05:35.000
but we will be there.

05:35.000 --> 05:42.000
And by 2029, it will match any person.

05:42.000 --> 05:44.000
That's it? 2029?

05:44.000 --> 05:46.000
That's just a few years away.

05:46.000 --> 05:48.000
Yeah, well, I'm actually considered conservative.

05:48.000 --> 05:51.000
People think that will happen like next year or the year after,

05:51.000 --> 05:56.000
but I actually said that in 1999.

05:56.000 --> 06:02.000
I said we would match any person by 2029.

06:02.000 --> 06:04.000
So 30 years.

06:04.000 --> 06:07.000
People thought that was totally crazy.

06:07.000 --> 06:12.000
And in fact, Stanford had a conference.

06:12.000 --> 06:15.000
They invited several hundred people from around the world

06:15.000 --> 06:18.000
to talk about my prediction.

06:18.000 --> 06:22.000
And people came in and people thought that this would happen,

06:22.000 --> 06:25.000
but not by 2029. They thought it would take 100 years.

06:25.000 --> 06:27.000
Yeah, I've heard that.

06:27.000 --> 06:30.000
I've heard that, but I think people are amending those.

06:30.000 --> 06:33.000
Is it because human beings have a very difficult time

06:33.000 --> 06:36.000
grasping the concept of exponential growth?

06:36.000 --> 06:39.000
That's exactly right.

06:39.000 --> 06:43.000
In fact, still, economists have a linear view.

06:43.000 --> 06:46.000
And if you say, well, it's going to grow exponentially.

06:46.000 --> 06:52.000
They say, yeah, but maybe 2% a year.

06:52.000 --> 06:57.000
It actually doubles in 14 years.

06:57.000 --> 07:06.000
And I brought a chart I can show you that really illustrates this.

07:06.000 --> 07:09.000
Is this chart available online so we could show people?

07:09.000 --> 07:10.000
Yeah, it's in the book.

07:10.000 --> 07:14.000
But is it available online, that chart, where Jamie could pull it up

07:14.000 --> 07:16.000
and someone could see it?

07:16.000 --> 07:19.000
Just so the folks watching the podcast could see it too.

07:19.000 --> 07:21.000
But I could just hold it up to the camera.

07:21.000 --> 07:23.000
What's the title of it?

07:23.000 --> 07:29.000
It says price performance of computation 1939 to 2023.

07:29.000 --> 07:30.000
You have it.

07:30.000 --> 07:31.000
OK, great.

07:31.000 --> 07:32.000
Jamie already has it.

07:32.000 --> 07:34.000
Yeah, the climb is insane.

07:34.000 --> 07:36.000
It's like the San Juan Mountains.

07:36.000 --> 07:41.000
What's interesting is that it's an exponential curve

07:41.000 --> 07:44.000
and a straight line represents exponential growth.

07:44.000 --> 07:50.000
And that's an absolute straight line for 80 years.

07:50.000 --> 07:55.000
The very first point, this is the speed of computers.

07:55.000 --> 08:06.000
It was 0.0007 calculations per second per constant dollar.

08:06.000 --> 08:11.000
The last point is 35 billion calculations per second.

08:11.000 --> 08:17.000
So that's a 20 quadrillion fold increase in those 80 years.

08:17.000 --> 08:23.000
But the speed with which it gained is actually the same

08:23.000 --> 08:25.000
throughout the entire 80 years.

08:25.000 --> 08:28.000
Because if it was sometimes better and sometimes worse,

08:28.000 --> 08:30.000
this curve would bend.

08:30.000 --> 08:32.000
It would bend up and down.

08:32.000 --> 08:36.000
It's really very much a straight line.

08:36.000 --> 08:40.000
So the speed with which we increased it was the same

08:40.000 --> 08:42.000
regardless of the technologies.

08:42.000 --> 08:44.000
And the technology was radically different

08:44.000 --> 08:46.000
at the beginning versus the end.

08:46.000 --> 08:53.000
And yet it increased the speed exactly the same for 80 years.

08:53.000 --> 08:56.000
In fact, the first 40 years, nobody even knew this was happening.

08:56.000 --> 08:58.000
So it's not like somebody was in charge

08:58.000 --> 09:00.000
and saying, OK, next year we have to get to here

09:00.000 --> 09:02.000
and people would try to match that.

09:02.000 --> 09:05.000
We didn't even know this was happening for 40 years.

09:05.000 --> 09:09.000
40 years later, I noticed this for various reasons.

09:09.000 --> 09:11.000
I predicted it would stay the same,

09:11.000 --> 09:16.000
the same speed increase each year, which it has.

09:16.000 --> 09:19.000
In fact, we just put the last dot like two weeks ago

09:19.000 --> 09:22.000
and it's exactly where it should be.

09:22.000 --> 09:29.000
So technology and computation, certainly prime form of technology,

09:29.000 --> 09:33.000
increases at the same speed.

09:33.000 --> 09:35.000
And this goes through a worn piece.

09:35.000 --> 09:37.000
You might say, well, maybe it's greater during war.

09:37.000 --> 09:39.000
No, it's exactly the same.

09:39.000 --> 09:41.000
You can't tell when there's war or peace

09:41.000 --> 09:43.000
or anything else on here.

09:43.000 --> 09:50.000
It just matches from one type of technology to the next.

09:50.000 --> 09:53.000
And it's also true of other things,

09:53.000 --> 09:59.000
like, for example, getting energy from the sun.

09:59.000 --> 10:01.000
That's also exponential.

10:01.000 --> 10:04.000
It's also just like this.

10:04.000 --> 10:10.000
It's increased.

10:10.000 --> 10:19.000
We're now getting about a thousand times as much energy

10:19.000 --> 10:23.000
from the sun that we did 20 years ago.

10:23.000 --> 10:26.000
Because the implementation of solar panels and the like.

10:26.000 --> 10:31.000
Has the function of it increased exponentially as well?

10:31.000 --> 10:36.000
What I had understood was that there was a bottleneck in the technology

10:36.000 --> 10:40.000
as far as how much you could extract from the sun from those panels.

10:40.000 --> 10:42.000
No, not at all.

10:42.000 --> 10:50.000
I mean, it's increased 99.7% since we started.

10:50.000 --> 10:53.000
And it does the same every year.

10:53.000 --> 10:55.000
It's an exponential curve.

10:55.000 --> 10:57.000
And if you look at the curve,

10:57.000 --> 11:01.000
you'll be getting 100% of all the energy we need in 10 years.

11:01.000 --> 11:02.000
The person who told me that was Elon.

11:02.000 --> 11:04.000
And Elon was telling me that this is the reason

11:04.000 --> 11:07.000
why you can't have a fully solar-powered electric car.

11:07.000 --> 11:10.000
Because it's not capable of absorbing that much from the sun

11:10.000 --> 11:12.000
with a small panel like that.

11:12.000 --> 11:15.000
He said there's a physical limitation in the panel size.

11:15.000 --> 11:20.000
No, I mean, it's increased 99.7% since we started.

11:20.000 --> 11:22.000
Since what year?

11:22.000 --> 11:29.000
This is about 35 years ago.

11:29.000 --> 11:39.000
In 99% of the ability of it as well as the expansion of use?

11:39.000 --> 11:41.000
I mean, you might have to store it.

11:41.000 --> 11:44.000
We're also making exponential gains in the storage of electricity.

11:44.000 --> 11:46.000
Right. Battery technology.

11:46.000 --> 11:51.000
So you don't have to get it all from a solar panel that fits in a car.

11:51.000 --> 11:56.000
The concept was like, could you make a solar-paneled car,

11:56.000 --> 11:58.000
a car that has solar panels on the roof?

11:58.000 --> 12:00.000
And would that be enough to power the car?

12:00.000 --> 12:02.000
And he said no.

12:02.000 --> 12:05.000
He said it's just not really there yet.

12:05.000 --> 12:07.000
Right. It's not there yet.

12:07.000 --> 12:09.000
But it will be there in 10 years.

12:09.000 --> 12:10.000
You think so?

12:10.000 --> 12:12.000
Yeah, he seemed to doubt that.

12:12.000 --> 12:14.000
He thought that there's a certain,

12:14.000 --> 12:17.000
there's limitation of the amount of energy you can get from the sun period,

12:17.000 --> 12:20.000
how much it gives out and how much those solar panels can absorb.

12:20.000 --> 12:24.000
Well, you're not going to be able to get it all from the solar panel that fits in a car.

12:24.000 --> 12:26.000
You're going to have to store some of that energy.

12:26.000 --> 12:31.000
Right. So you wouldn't just be able to drive indefinitely on solar power.

12:31.000 --> 12:33.000
Yeah, that was what he was saying.

12:33.000 --> 12:36.000
So, but you can obviously power a house.

12:36.000 --> 12:41.000
And especially if you have a roof, the Tesla has those solar-powered roofs now.

12:41.000 --> 12:44.000
But you can also store the energy for a car.

12:45.000 --> 12:49.000
I mean, we're going to go to all renewable energy,

12:49.000 --> 12:53.000
wind and sun within 10 years,

12:53.000 --> 12:56.000
including our ability to store the energy.

12:56.000 --> 12:58.000
All renewable in 10 years?

12:58.000 --> 13:02.000
So what are they going to do with all these nuclear plants and coal-powered plants and all these things?

13:02.000 --> 13:04.000
That's completely unnecessary.

13:04.000 --> 13:09.000
People say we need nuclear power, which we don't.

13:09.000 --> 13:13.000
You can get it all from the sun and wind within 10 years.

13:14.000 --> 13:19.000
So in 10 years, you'll be able to power Los Angeles with sun and wind?

13:19.000 --> 13:20.000
Yes.

13:20.000 --> 13:21.000
Really?

13:21.000 --> 13:22.000
Yeah.

13:22.000 --> 13:25.000
I was not aware that we were anywhere near that kind of timeline.

13:25.000 --> 13:30.000
Well, that's because people are not taking into account exponential growth.

13:30.000 --> 13:33.000
So the exponential growth also of the grid?

13:33.000 --> 13:40.000
Because just to pull the amount of power that you would need to charge, you know, X amount of million.

13:40.000 --> 13:45.000
Someone has an electric vehicle by 2035, let's say then.

13:45.000 --> 13:50.000
Just the amount of change you would need on the grid would be pretty substantial.

13:50.000 --> 13:52.000
Well, we're making exponential gains on that as well.

13:52.000 --> 13:53.000
Are we?

13:53.000 --> 13:54.000
Yeah.

13:54.000 --> 13:55.000
Yeah.

13:55.000 --> 13:56.000
I wasn't aware.

13:56.000 --> 14:00.000
I had this impression that there was a problem with that,

14:00.000 --> 14:04.000
and especially in Los Angeles, they've actually asked people at certain times

14:04.000 --> 14:06.000
when it's hot out to not charge your car.

14:06.000 --> 14:11.000
Looking at the future, that's true now, but it's growing exponentially.

14:11.000 --> 14:16.000
In every field of technology then, essentially.

14:16.000 --> 14:19.000
Is the bottleneck a battery technology?

14:19.000 --> 14:23.000
And how close are they to solving some of these problems,

14:23.000 --> 14:30.000
like conflict minerals and the things that we need in order to power these batteries?

14:30.000 --> 14:34.000
Our ability to store energy is also growing exponentially.

14:34.000 --> 14:42.000
So putting all that together, we'll be able to power everything we need within 10 years.

14:42.000 --> 14:43.000
Wow.

14:43.000 --> 14:44.000
Most people don't think that.

14:44.000 --> 14:49.000
So you're thinking that based on this idea that people would have a limited idea.

14:49.000 --> 14:51.000
Let me imagine that computation would grow like this.

14:51.000 --> 14:55.000
It's just continuing to do that.

14:55.000 --> 14:59.000
And so we have large language models, for example.

14:59.000 --> 15:02.000
No one expected that to happen like five years ago.

15:02.000 --> 15:03.000
Right.

15:03.000 --> 15:06.000
And we had them two years ago, but they didn't work very well.

15:06.000 --> 15:13.000
So it began a little less than two years ago that we could actually do large language models.

15:13.000 --> 15:17.000
And that was very much a surprise to everybody.

15:17.000 --> 15:22.000
So that's probably the primary example of exponential growth.

15:22.000 --> 15:23.000
We had Sam Altman on.

15:23.000 --> 15:28.000
One of the things that he and I were talking about was that AI figured out a way to lie.

15:28.000 --> 15:32.000
That they used AI to go through a CAPTCHA system.

15:32.000 --> 15:38.000
And the AI told the system that it was vision impaired, which is not technically a lie.

15:38.000 --> 15:41.000
But it used it to bypass, are you a robot?

15:41.000 --> 15:46.000
Well, we don't know now for large language models to say they don't know something.

15:46.000 --> 15:48.000
So you ask it a question.

15:48.000 --> 15:54.000
And if the answer to that question is not in the system, it still comes up with an answer.

15:54.000 --> 15:58.000
So it'll look at everything and give you its best answer.

15:58.000 --> 16:02.000
And if the best answer is not there, it still gives you an answer.

16:02.000 --> 16:06.000
But that's considered a hallucination.

16:06.000 --> 16:07.000
Oh, hallucination.

16:07.000 --> 16:08.000
Yeah.

16:08.000 --> 16:09.000
That's what it's called.

16:09.000 --> 16:10.000
Really?

16:10.000 --> 16:11.000
AI hallucination.

16:11.000 --> 16:13.000
So they cannot be wrong.

16:13.000 --> 16:15.000
They have to be able to answer this.

16:15.000 --> 16:19.000
So far, we're actually working on being able to tell if it doesn't know something.

16:19.000 --> 16:22.000
So if you ask it something and say, oh, I don't know that.

16:22.000 --> 16:24.000
Right now, it can't do that.

16:24.000 --> 16:25.000
Oh, wow.

16:25.000 --> 16:26.000
That's interesting.

16:26.000 --> 16:31.000
So it gives you some answer.

16:31.000 --> 16:35.000
And if the answer is not there, it's just like make something up.

16:35.000 --> 16:40.000
It's the best answer, but the best answer isn't very good because it doesn't know the answer.

16:40.000 --> 16:47.000
And the way to fix hallucinations is to actually give it more capabilities to memorize things

16:47.000 --> 16:51.000
and give it more information so it knows the answer to it.

16:51.000 --> 17:00.000
And if you tell an answer to a question, it will remember that and give you that correct answer.

17:00.000 --> 17:04.000
But these models are not, we don't know everything.

17:04.000 --> 17:15.000
And it has to, we have to be able to scan an answer to every single question, which we can't quite do.

17:15.000 --> 17:17.000
It'd be actually better if it could actually answer.

17:17.000 --> 17:19.000
Well, gee, I don't know that.

17:19.000 --> 17:20.000
Right.

17:20.000 --> 17:25.000
But in particular, like say when it comes to exploration of the universe,

17:25.000 --> 17:29.000
if there's a certain amount of, I mean, vast amount of the universe we have not explored.

17:29.000 --> 17:33.000
So if it has to answer questions about that, it would just come up with an answer.

17:33.000 --> 17:34.000
Right.

17:34.000 --> 17:37.000
It'll just come up with an answer, which will likely be wrong.

17:37.000 --> 17:39.000
That's interesting.

17:39.000 --> 17:46.000
But that would be a real problem if someone was counting on the AI to have a solution for something too soon.

17:46.000 --> 17:47.000
Right?

17:47.000 --> 17:48.000
Right.

17:48.000 --> 17:55.000
If you don't know everything, search engines actually know are pretty well vetted.

17:55.000 --> 18:00.000
And if it actually answers something, it'll, it's usually correct.

18:00.000 --> 18:02.000
Unless it's curated.

18:02.000 --> 18:06.000
But large language models don't have that capability.

18:06.000 --> 18:09.000
So it'd be good actually if they knew that they were wrong.

18:09.000 --> 18:13.000
They'd also tell us what we have to fix.

18:13.000 --> 18:19.000
What about the idea that AI models are influenced by ideology,

18:19.000 --> 18:23.000
that AI models have been programmed with certain ideologies?

18:23.000 --> 18:25.000
I mean, they do learn from people.

18:25.000 --> 18:26.000
Yeah.

18:26.000 --> 18:28.000
And people have ideologies.

18:28.000 --> 18:29.000
Right.

18:29.000 --> 18:32.000
Some of which are, some of which are not correct.

18:32.000 --> 18:42.000
And that's a large way in which it will make things up because it's learning from people.

18:42.000 --> 18:44.000
Right.

18:44.000 --> 18:52.000
So right now, if somebody has access to a good search engine,

18:52.000 --> 18:58.000
they will check before they actually answer something with a search engine to make sure that it's correct.

18:58.000 --> 19:02.000
Because search engines are generally much more accurate.

19:02.000 --> 19:03.000
Generally.

19:03.000 --> 19:04.000
Right.

19:04.000 --> 19:10.000
When it comes to this idea that people enter information into a computer,

19:10.000 --> 19:12.000
and then the computer relies on that ideology,

19:12.000 --> 19:16.000
do you anticipate that with artificial general intelligence,

19:16.000 --> 19:18.000
it'll be agnostic to ideology,

19:18.000 --> 19:25.000
that it'll be able to reach a point where instead of deciding things based on social norms,

19:25.000 --> 19:28.000
or whatever the culture is accepted currently,

19:28.000 --> 19:32.000
that it would look at things more objectively and rationally?

19:32.000 --> 19:33.000
Well, eventually.

19:33.000 --> 19:34.000
Eventually.

19:34.000 --> 19:35.000
Eventually.

19:35.000 --> 19:38.000
But we still call it artificial general intelligence,

19:38.000 --> 19:39.000
even if it didn't do that.

19:39.000 --> 19:55.000
And people certainly are influenced by whatever their people that they respect feel is correct,

19:55.000 --> 20:00.000
and will be as influenced by as people are.

20:00.000 --> 20:05.000
And we'll still call it artificial general intelligence.

20:05.000 --> 20:13.000
We are starting to check what large language models come up with with search engines,

20:13.000 --> 20:16.000
and that's actually making them more correct.

20:16.000 --> 20:19.000
But we have to actually continue on this curve.

20:19.000 --> 20:22.000
We need more data to be able to store everything.

20:22.000 --> 20:27.000
This is not enough data to be able to store everything correctly.

20:27.000 --> 20:36.000
This is a large amount of large language models for which we don't have storage for the data.

20:36.000 --> 20:39.000
So that's what's holding us back is data and storage?

20:39.000 --> 20:43.000
Yeah, we also have to have the correct storage.

20:43.000 --> 20:51.000
So that's really where the effort is going to be able to get rid of these hallucinations.

20:51.000 --> 20:56.000
That's a fun thing to say, hallucinations in terms of artificial intelligence.

20:56.000 --> 20:59.000
Well, we usually come up with the wrong things.

20:59.000 --> 21:04.000
Large language models is not really the correct way to talk about this.

21:04.000 --> 21:09.000
It does know language, but there's a lot of other things it knows.

21:09.000 --> 21:21.000
We're using them now to come up with medicines.

21:21.000 --> 21:24.000
For example, the Moderna vaccine.

21:24.000 --> 21:41.000
We wrote down every possible type of medicine that might work.

21:41.000 --> 21:45.000
It was actually several billion mRNA sequences,

21:45.000 --> 21:51.000
and we then tested them all and did that in two days.

21:51.000 --> 22:00.000
So it actually came up with tested several billion and decided on it in two days.

22:00.000 --> 22:03.000
We then tested it with people.

22:03.000 --> 22:10.000
We'll be able to overcome that as well because we'll be able to test it with machines.

22:10.000 --> 22:14.000
But we actually did test it with people for ten months.

22:14.000 --> 22:16.000
There was still a record.

22:16.000 --> 22:22.000
So for machines, when they start testing medications with machines, how will they audit that?

22:22.000 --> 22:27.000
So the concept will be that you take into account biological variability,

22:27.000 --> 22:32.000
all the different factors that would lead to a person to have an adverse reaction to a certain compound,

22:32.000 --> 22:39.000
and then you program all the known data about how things interact with the body.

22:39.000 --> 22:44.000
You need to be able to simulate all the different possibilities.

22:44.000 --> 22:48.000
And then come up with a number of how many people will be adversely affected by something?

22:48.000 --> 22:51.000
That's one of the things you would look at.

22:51.000 --> 22:54.000
And then efficacy based on age, health?

22:54.000 --> 23:04.000
But that could be done literally in a matter of days rather than years.

23:04.000 --> 23:10.000
The question would be who's in charge of that data and how does that get resolved?

23:10.000 --> 23:15.000
And if artificial intelligence is still prone to hallucinations,

23:15.000 --> 23:20.000
and they start using those hallucinations to justify medications, that could be a bit of an issue.

23:20.000 --> 23:24.000
Especially if it's controlled by a corporation that wants to make a lot of money.

23:24.000 --> 23:26.000
Well, that's the issue.

23:26.000 --> 23:28.000
To be able to do it correctly.

23:28.000 --> 23:32.000
There's going to have to be a point in time where we all decide that artificial intelligence

23:32.000 --> 23:36.000
has reached this place where we can trust it implicitly.

23:36.000 --> 23:44.000
Right. Well, that's why they take now the leading candidate and actually test it with people.

23:44.000 --> 23:54.000
But we'll be able to get rid of the testing with people once we can have reliance on the simulation.

23:54.000 --> 23:58.000
So we've got to make the simulations correct.

23:58.000 --> 24:07.000
But like right now we actually tested with people and that takes, well, took 10 months in this case.

24:07.000 --> 24:15.000
When you look at artificial intelligence and you look at the expansion of it and the ultimate place that it will eventually be,

24:15.000 --> 24:20.000
what do you see happening inside of our lifetime, like inside of 20 years?

24:20.000 --> 24:25.000
What kind of revolutionary changes on society would this have?

24:25.000 --> 24:35.000
Well, one thing I feel will happen in five years by 2029 is we'll reach longevity escape velocity.

24:35.000 --> 24:40.000
So right now you go through a year and you use up a year of your longevity.

24:40.000 --> 24:42.000
You're then a year older.

24:42.000 --> 24:51.000
However, we do have scientific progress and we're making coming up with new cures for diseases and so on.

24:51.000 --> 24:53.000
Right now you're getting back about four months.

24:53.000 --> 25:00.000
So you lose a year, but through scientific progress, you're getting back four months.

25:00.000 --> 25:02.000
So you're only losing eight months.

25:02.000 --> 25:06.000
However, the scientific progress is progressing exponentially.

25:06.000 --> 25:10.000
And by 2029, you'll get back a full year.

25:10.000 --> 25:15.000
So you lose a year, but you get back a year and you pretty much stay in the same place.

25:15.000 --> 25:18.000
So by 2029, you'll be static.

25:18.000 --> 25:22.000
And past 2029, you'll actually get back more than a year.

25:22.000 --> 25:23.000
You'll get back.

25:23.000 --> 25:28.000
Can I be a baby again?

25:28.000 --> 25:33.000
No, but in terms of your longevity, you'll get back more than a year.

25:33.000 --> 25:34.000
Right.

25:34.000 --> 25:38.000
So you'll be able to essentially go back in biological age.

25:38.000 --> 25:42.000
So the telomeres changing the elasticity of the skin.

25:42.000 --> 25:46.000
Eventually you'll be able to do that.

25:46.000 --> 25:49.000
It doesn't guarantee you living forever.

25:49.000 --> 25:57.000
I mean, you could have a 10 year old and you could compute that he's got many decades of longevity and he could die tomorrow.

25:57.000 --> 25:59.000
Sure.

25:59.000 --> 26:04.000
But overall, there'd be an expansion of the age that most people die.

26:04.000 --> 26:06.000
And that's something that we're going to get.

26:06.000 --> 26:12.000
And it's also using the same type of logic as large language models.

26:12.000 --> 26:14.000
But that's not language.

26:14.000 --> 26:16.000
You're actually creating medications.

26:16.000 --> 26:22.000
So we should call the large event models, not large language models, because it's not just dealing with language.

26:22.000 --> 26:25.000
It's dealing with all kinds of things.

26:25.000 --> 26:32.000
When I talked to you 10 years ago, you were telling me about this pretty extensive supplement routine that you're on.

26:32.000 --> 26:39.000
Well, I'm trying to get to the point where we have longevity escape velocity in good shape.

26:39.000 --> 26:40.000
Right.

26:40.000 --> 26:42.000
And yes, I do follow that.

26:42.000 --> 26:50.000
I take maybe 80 pills a day and some injections and so on.

26:50.000 --> 26:51.000
Peptides.

26:51.000 --> 26:53.000
Yes, peptides.

26:53.000 --> 26:57.000
So far it works.

26:57.000 --> 27:01.000
Have you ever gone off of it to see what you feel like normally?

27:01.000 --> 27:02.000
No.

27:02.000 --> 27:03.000
Well, I do that, right?

27:03.000 --> 27:04.000
Yeah.

27:04.000 --> 27:06.000
I mean, it seems to work.

27:06.000 --> 27:08.000
And there's evidence behind it.

27:08.000 --> 27:09.000
How old are you now?

27:09.000 --> 27:12.000
76.

27:12.000 --> 27:13.000
You look good.

27:13.000 --> 27:15.000
You look good for 76, man.

27:15.000 --> 27:16.000
That's great.

27:16.000 --> 27:17.000
So it's doing something.

27:17.000 --> 27:18.000
Yeah.

27:18.000 --> 27:20.000
I think it's working.

27:20.000 --> 27:31.000
And so your goal is to get to that point where they start doing, you live a year, you stay static, and then eventually get back to youthfulness.

27:31.000 --> 27:32.000
Right.

27:32.000 --> 27:33.000
And it's not that far off.

27:33.000 --> 27:38.000
If you're diligent, I think we'll get there by 2029.

27:38.000 --> 27:40.000
Now, not everybody's diligent.

27:40.000 --> 27:41.000
Right.

27:41.000 --> 27:42.000
Of course.

27:42.000 --> 27:46.000
Now, past that, this is for life extension, which is great.

27:46.000 --> 27:51.000
But what about how AI is going to change society?

27:51.000 --> 27:52.000
Yes.

27:52.000 --> 27:54.000
Well, that's a very big issue.

27:54.000 --> 28:01.000
And it's already doing lots of things, makes some people uncomfortable.

28:01.000 --> 28:05.000
What we're actually doing is increasing our intelligence.

28:05.000 --> 28:07.000
I mean, right now you have a brain.

28:07.000 --> 28:12.000
It has different modules in it to deal with different things.

28:12.000 --> 28:17.000
But really, it's able to connect one concept to another concept.

28:17.000 --> 28:20.000
And that's what your brain does.

28:20.000 --> 28:24.000
We can actually increase that by, for example, carrying around a phone.

28:24.000 --> 28:26.000
This has connections in it.

28:26.000 --> 28:29.000
It's a little bit of a hassle to use.

28:29.000 --> 28:33.000
If I ask you to do something, you've got to kind of mess with it.

28:33.000 --> 28:37.000
Actually, it'd be good if this actually listened to your conversation.

28:37.000 --> 28:38.000
Oh, it does.

28:38.000 --> 28:42.000
And without saying anything, you're just talking.

28:42.000 --> 28:45.000
And it says, oh, the name of that actress is so-and-so.

28:45.000 --> 28:47.000
Yeah, but then it's a busy body.

28:47.000 --> 28:50.000
It's like interfering with your life, talking to you all the time.

28:50.000 --> 28:52.000
Well, there's ways of dealing with that, too.

28:52.000 --> 28:53.000
You shut it off.

28:53.000 --> 28:58.000
So we haven't done that yet.

28:58.000 --> 29:04.000
But that's a way of expanding your connections.

29:04.000 --> 29:12.000
What a large language model does, it has connections in it as well.

29:12.000 --> 29:19.000
And the fact that it's getting now to a point that's getting fairly comparable to the human brain,

29:19.000 --> 29:23.000
we have about a trillion connections in our brain.

29:23.000 --> 29:30.000
Things like the top model from Google or GPT-4,

29:30.000 --> 29:38.000
they have about 400 billion connections approximately.

29:38.000 --> 29:41.000
They'll be at a trillion probably within a year.

29:41.000 --> 29:45.000
That's pretty comparable to what the human brain does.

29:45.000 --> 29:50.000
Eventually, it'll go beyond that and will have access to that.

29:50.000 --> 29:53.000
So it's basically making us smarter.

29:53.000 --> 30:05.000
So if you have the ability to be smarter, that's something that's positive, really.

30:05.000 --> 30:15.000
I mean, if we were like mice today and we had the opportunity to become like humans,

30:15.000 --> 30:17.000
we wouldn't object to that.

30:17.000 --> 30:20.000
In fact, we are humans and we don't object to that.

30:20.000 --> 30:24.000
It used to be shrews.

30:24.000 --> 30:28.000
And this is going to basically make us smarter.

30:28.000 --> 30:32.000
Eventually, we'll be much smarter than we are today.

30:32.000 --> 30:34.000
And that's a positive thing.

30:34.000 --> 30:46.000
We'll be able to do things that are today that we find bothersome in a way that's much more palatable.

30:46.000 --> 30:49.000
The idea of us getting smarter sounds great.

30:49.000 --> 30:51.000
It'd be great to be smarter.

30:51.000 --> 30:56.000
Right, but people object to that because it's like competition.

30:56.000 --> 30:58.000
In what way?

30:58.000 --> 31:05.000
Well, I mean, Google has, I don't know, 60,000, 70,000 programmers.

31:05.000 --> 31:09.000
How many programmers exist in the world?

31:09.000 --> 31:13.000
How much longer is that going to be a viable career?

31:13.000 --> 31:18.000
Because language models already can code.

31:18.000 --> 31:25.000
Not quite as good as a real expert coder, but how long is that going to be?

31:25.000 --> 31:27.000
It's not going to be 100 years.

31:27.000 --> 31:30.000
It's going to be a few years.

31:30.000 --> 31:34.000
So people see it as competition.

31:34.000 --> 31:36.000
I have a slightly different view of that.

31:36.000 --> 31:41.000
I see these things as actually adding to our own intelligence.

31:41.000 --> 31:47.000
And we're merging with these kinds of computers and making ourselves smarter

31:47.000 --> 31:49.000
by merging with it.

31:49.000 --> 31:56.000
And eventually it'll go inside our brain and be able to make us smarter instantly

31:56.000 --> 32:01.000
just like we had more connections inside our own brain.

32:01.000 --> 32:05.000
Well, I think people have reservations always when it comes to great change.

32:05.000 --> 32:07.000
And this is probably the greatest change.

32:07.000 --> 32:11.000
The greatest change we've ever experienced in our lifetimes for sure has been the internet.

32:11.000 --> 32:15.000
And this will make that look like nothing.

32:15.000 --> 32:17.000
It'll change everything.

32:17.000 --> 32:20.000
And it seems inevitable.

32:20.000 --> 32:27.000
I understand that people are upset about it, but it just seems like what human beings were sort of designed to do.

32:27.000 --> 32:31.000
Right. We're the only animal that actually creates technology.

32:31.000 --> 32:36.000
It's a combination of our brain and something else, which is our thumb.

32:36.000 --> 32:38.000
So I can imagine something.

32:38.000 --> 32:45.000
Oh, if I take that leaf from a tree, I could create a tool with it.

32:45.000 --> 32:55.000
Other animals have actually a bigger brain like the whale, dolphins, elephants.

32:55.000 --> 33:00.000
They have a larger brain than we do, but they don't have something equivalent to the thumb.

33:00.000 --> 33:06.000
Monkey has a thing that looks like the thumb, but it's actually an inch down and it doesn't actually work very well.

33:06.000 --> 33:13.000
So they can actually create a tool, but they don't create a tool that's powerful enough to create the next tool.

33:13.000 --> 33:22.000
So we're actually able to use our tools and create something that's that much more significant.

33:22.000 --> 33:28.000
So we can create tools, and that's really part of who we are.

33:28.000 --> 33:35.000
It makes us that much more intelligent, and that's a good thing.

33:35.000 --> 33:50.000
I mean, here's...

33:50.000 --> 33:53.000
So here's U.S. personal income per capita.

33:53.000 --> 34:02.000
So this is the average amount that we make per person in constant dollars.

34:02.000 --> 34:05.000
Right here, it's on the screen.

34:05.000 --> 34:09.000
Do we make a lot more money, but things cost a lot more money too, right?

34:09.000 --> 34:11.000
No, this is constant dollars.

34:11.000 --> 34:14.000
Constant dollars in relation to the inflation?

34:14.000 --> 34:17.000
Yeah, so this does not show you inflation.

34:17.000 --> 34:20.000
These are constant dollars.

34:20.000 --> 34:26.000
So we're actually making that much more each year on average.

34:26.000 --> 34:29.000
Right, but it doesn't take into account inflation, correct?

34:29.000 --> 34:32.000
So it's not taking into account the rise of cost of things.

34:32.000 --> 34:34.000
No, it is taking that.

34:34.000 --> 34:36.000
Oh, it is, okay.

34:36.000 --> 34:40.000
So we're making that much more in constant dollars.

34:40.000 --> 34:46.000
If you look over the past hundred years, we've made about ten times as much.

34:46.000 --> 34:52.000
I wonder if there's a similar chart about consumerism, just about material possessions.

34:52.000 --> 34:55.000
I wonder if, like, how much more we're purchasing and creating.

34:55.000 --> 35:01.000
I've always felt like that's one of the things that materialism is one of those instincts

35:01.000 --> 35:09.000
that human beings sort of look down upon and this aimless pursuit of buying things.

35:09.000 --> 35:16.000
But I feel like that motivates technology because the constant need for the newest,

35:16.000 --> 35:22.000
greatest thing is one of the things that fuels the creation and innovation of new things.

35:22.000 --> 35:25.000
But if you were to go back a hundred years, you'd be very unhappy.

35:25.000 --> 35:26.000
Oh, yeah.

35:26.000 --> 35:30.000
Because you wouldn't have, I mean, you wouldn't have a computer, for example.

35:30.000 --> 35:32.000
You wouldn't have anything.

35:32.000 --> 35:34.000
You'd have most things you've grown accustomed to.

35:34.000 --> 35:35.000
Yeah.

35:35.000 --> 35:39.000
I mean, unless that's why you wanted that.

35:39.000 --> 35:42.000
Also, we didn't live very long.

35:42.000 --> 35:44.000
Right, medical advancements.

35:44.000 --> 35:52.000
At average, life was 48 years in 1900.

35:52.000 --> 35:54.000
It's 35 years in 1800.

35:54.000 --> 35:55.000
Right.

35:55.000 --> 35:58.000
Go back a thousand years, it was 20 years.

35:58.000 --> 36:01.000
That takes into account child mortality, too, though, right?

36:01.000 --> 36:04.000
But it's also injuries, death.

36:04.000 --> 36:06.000
Some people did live long.

36:06.000 --> 36:08.000
There was people that lived back then.

36:08.000 --> 36:11.000
If nothing happened to you, you did live to be 80, like a normal person.

36:11.000 --> 36:14.000
But that was actually very rare.

36:14.000 --> 36:16.000
Because most things happen to people.

36:16.000 --> 36:19.000
Most people, by the time you get to 80, you've had at least one hospital visit.

36:19.000 --> 36:20.000
Something's gone wrong.

36:20.000 --> 36:23.000
Broken arm, broken this, broken that.

36:23.000 --> 36:26.000
It was very rare to make it to 80.

36:26.000 --> 36:27.000
Right.

36:27.000 --> 36:28.000
200 years ago.

36:28.000 --> 36:31.000
Right, but the human body was physically capable of doing it.

36:31.000 --> 36:32.000
Right.

36:32.000 --> 36:40.000
Well, our human body can go on forever if you fix things properly.

36:40.000 --> 36:46.000
There's nothing in our body that means that you have to die at 100 or even 120.

36:46.000 --> 36:50.000
We can go on really indefinitely.

36:50.000 --> 36:52.000
Well, that's the groundbreaking work today, right?

36:52.000 --> 36:57.000
They're treating disease or, excuse me, age as if it is a disease,

36:57.000 --> 36:59.000
not just inevitable consequences.

36:59.000 --> 37:00.000
Right.

37:00.000 --> 37:04.000
And our FDA doesn't accept that, but they're actually beginning to accept it now.

37:04.000 --> 37:05.000
Well, as they get older.

37:05.000 --> 37:07.000
Yeah, exactly.

37:07.000 --> 37:09.000
They're forced into it.

37:10.000 --> 37:15.000
The concept of artificial general intelligence scares a lot of people also because of Hollywood, right?

37:15.000 --> 37:18.000
Because of the Terminator films and things along those lines.

37:18.000 --> 37:25.000
Like, how far away are we, do you think, from actual artificial humans or will we ever get there?

37:25.000 --> 37:28.000
Will we integrate before that takes place?

37:28.000 --> 37:36.000
I mean, all of this additional intelligence that we're creating is something that we use.

37:36.000 --> 37:39.000
And it's just like it came with us.

37:39.000 --> 37:45.000
So we're actually making ourselves more intelligent and ultimately that's a good thing.

37:45.000 --> 37:49.000
And if we have it and then we say, well, gee, we don't really like this.

37:49.000 --> 37:51.000
Let's take it away.

37:51.000 --> 37:53.000
People would never accept that.

37:53.000 --> 38:02.000
They may be against the idea of general intelligence, but once they get it, nobody wants to give that up.

38:02.000 --> 38:07.000
And it will be beneficial.

38:11.000 --> 38:22.000
The Blue Knight started 200 years ago because the cotton genie come out and all these people that were making money with the cotton genie were against it.

38:22.000 --> 38:26.000
And they would actually destroy these machines at night.

38:27.000 --> 38:32.000
And they said, gee, if this keeps going, all jobs are going to go away.

38:32.000 --> 38:39.000
And indeed, people using the cotton genie to create more wealth, that did go away.

38:39.000 --> 38:44.000
But we actually made more money because we created things that didn't exist then.

38:44.000 --> 38:47.000
We didn't have anything like electronics, for example.

38:47.000 --> 38:59.000
And as we can actually see, we make 10 times as much in constant dollars as we did 100 years ago.

38:59.000 --> 39:04.000
And if you were to ask, well, what are people going to be doing?

39:04.000 --> 39:11.000
You couldn't answer it because we didn't understand the internet, for example.

39:11.000 --> 39:16.000
And there's probably some technologies down the pipe that are going to have a similar impact.

39:16.000 --> 39:17.000
Exactly.

39:17.000 --> 39:20.000
And they're going to extend life, for example.

39:20.000 --> 39:22.000
But are they going to create life?

39:25.000 --> 39:32.000
Well, we know how to create life.

39:40.000 --> 39:42.000
Well, that's an interesting question.

39:43.000 --> 39:46.000
What do you mean by create life?

39:46.000 --> 39:55.000
What I think is that human beings are some sort of a biological caterpillar that makes a cocoon that gives birth to an electronic butterfly.

39:55.000 --> 40:01.000
I think we are creating a life form and that we're merely conduits for this thing.

40:01.000 --> 40:08.000
And that all of our instincts and ego and emotions and all these things feed into it, materialism feeds into it.

40:08.000 --> 40:14.000
We keep buying and keep innovating and technology keeps increasing exponentially.

40:14.000 --> 40:20.000
And eventually it's going to be artificial intelligence and artificial intelligence is going to create better artificial intelligence

40:20.000 --> 40:29.000
and a form of being that has no limitations in terms of what's capable of doing and capable of traveling anywhere

40:29.000 --> 40:31.000
and not having any biological limitations in terms of...

40:31.000 --> 40:33.000
But that's going to be ourselves.

40:33.000 --> 40:41.000
We're going to be able to create life that is like humans but far greater than we are today.

40:41.000 --> 40:43.000
With an integration of technology.

40:43.000 --> 40:44.000
Yeah.

40:44.000 --> 40:46.000
If we choose to go that route.

40:46.000 --> 40:49.000
But that's the prediction that you have, that we will go that route.

40:49.000 --> 40:52.000
Like a neural link type deal, something along those lines.

40:52.000 --> 40:53.000
Right.

40:53.000 --> 40:56.000
So I don't see this competition like the things are going to...

40:56.000 --> 40:58.000
No, I don't think it's competition.

40:58.000 --> 41:00.000
Well, it will seem like that.

41:00.000 --> 41:06.000
I mean, if you have a job doing coding and suddenly they don't really want you anymore

41:06.000 --> 41:11.000
because they can do coding with a large language model, it's going to feel like it's competition.

41:11.000 --> 41:13.000
Well, there's an issue now with films.

41:13.000 --> 41:21.000
Tyler Perry, who owns and he was building an $800 million television studio and he stopped production.

41:21.000 --> 41:23.000
What is it called, Sora?

41:23.000 --> 41:25.000
Is that what it's called, Jamie?

41:25.000 --> 41:34.000
He stopped production when he saw the capabilities of AI just for creating visuals, scenes, movies.

41:34.000 --> 41:37.000
There's one that's incredibly impressive.

41:37.000 --> 41:38.000
It's Tokyo.

41:38.000 --> 41:41.000
They're walking down the street of Tokyo in the winter.

41:41.000 --> 41:46.000
So it's snowing and they're walking down the street and you look at it, you're like, this is insane.

41:46.000 --> 41:48.000
This looks like a film.

41:48.000 --> 41:51.000
See if you can find that film because it's incredible.

41:51.000 --> 41:53.000
But would you want to get rid of that?

41:53.000 --> 41:54.000
Get rid of what?

41:54.000 --> 41:56.000
The capability.

41:56.000 --> 41:58.000
No, I don't want to get rid of the capability.

41:58.000 --> 42:01.000
But people do want to get rid of it.

42:01.000 --> 42:08.000
People that make movies, people that actually film things with cameras and use actors are going to be very upset.

42:08.000 --> 42:12.000
So this, this is all fake, which is insane.

42:12.000 --> 42:14.000
Beautiful snowy Tokyo city is bustling.

42:14.000 --> 42:19.000
The camera moves through the bustling city street, following several people enjoying the beautiful snowy weather

42:19.000 --> 42:21.000
and shopping at nearby stalls.

42:21.000 --> 42:25.000
Gorgeous Sakura petals are flying through the wind along with snowflakes.

42:25.000 --> 42:27.000
And this is what you get.

42:27.000 --> 42:30.000
I mean, this is insanely good.

42:30.000 --> 42:33.000
The variability, like just the way people are dressed.

42:33.000 --> 42:39.000
If you saw this somewhere else, look at this, a robot's life in a cyberpunk setting.

42:39.000 --> 42:44.000
If you saw this, you would say, oh, they filmed this.

42:44.000 --> 42:48.000
But just look at what they're able to do with animation and kids movies and things along those lines.

42:48.000 --> 42:50.000
Yeah, and it's going to get better.

42:50.000 --> 42:52.000
Yeah, it's just incredible.

42:52.000 --> 42:55.000
I mean, it's a new art form.

42:55.000 --> 42:59.000
So right there, the smoke looks a little uniform, but yeah.

42:59.000 --> 43:01.000
I mean, there's some problems with this, but...

43:01.000 --> 43:02.000
Not much.

43:02.000 --> 43:03.000
Yeah.

43:03.000 --> 43:07.000
And you imagine what it was like five years ago and then imagine what it's going to be like five years from now.

43:07.000 --> 43:09.000
Yeah, absolutely.

43:09.000 --> 43:10.000
And it's insane.

43:10.000 --> 43:16.000
I mean, no one took into consideration the idea that kids are going to be cheating on their school papers using chat GPT.

43:16.000 --> 43:20.000
But my kids tell me that's a real problem in school now.

43:20.000 --> 43:24.000
Yes, definitely.

43:24.000 --> 43:31.000
So no one saw that coming, no one saw this coming, and what we're at now is with chat GPT 4, right?

43:31.000 --> 43:32.000
4.5?

43:32.000 --> 43:33.000
Is that what it is?

43:33.000 --> 43:34.000
Well, 4.5 is coming.

43:34.000 --> 43:35.000
4.5 is coming.

43:35.000 --> 43:40.000
5 is supposed to be the massive leap.

43:40.000 --> 43:41.000
It'll be a leap.

43:41.000 --> 43:44.000
Just like 3 to 4 was a massive leap.

43:44.000 --> 43:45.000
Yeah.

43:45.000 --> 43:47.000
It's going to continue.

43:47.000 --> 43:49.000
It's never going to be finished.

43:49.000 --> 43:50.000
Right.

43:50.000 --> 43:51.000
It'll keep going.

43:51.000 --> 43:54.000
And it will also be able to make better versions of itself, correct?

43:54.000 --> 43:57.000
And yes, well, we do that.

43:57.000 --> 43:59.000
I mean, technology does that already.

43:59.000 --> 44:00.000
Right.

44:00.000 --> 44:04.000
But if you scale that out 100 years from now, what are you looking at?

44:04.000 --> 44:06.000
You're looking at a God.

44:06.000 --> 44:09.000
Well, it'll be less than 100 years.

44:09.000 --> 44:13.000
So you're looking at a God in 50 years?

44:13.000 --> 44:14.000
Less than that.

44:14.000 --> 44:20.000
I mean, once we have an ability to emulate everything that humans can do, and not just

44:20.000 --> 44:24.000
one human, but all humans, and that's only like 2029.

44:24.000 --> 44:27.000
That's only five years from now.

44:27.000 --> 44:30.000
And then it will make better versions of that.

44:30.000 --> 44:35.000
So it will probably solve a lot of the problems that we have in terms of energy storage, data

44:35.000 --> 44:38.000
storage, data speeds, computation speeds.

44:38.000 --> 44:41.000
And also medications.

44:41.000 --> 44:42.000
For us.

44:42.000 --> 44:43.000
For humans, yeah.

44:43.000 --> 44:48.000
Wouldn't it be better just, Ray, just download yourself into this beautiful electronic body?

44:48.000 --> 44:51.000
Why do you want to be biological?

44:51.000 --> 44:56.000
I mean, ultimately, that's what we're going to be able to do.

44:56.000 --> 44:58.000
You think that's going to happen?

44:58.000 --> 44:59.000
Yeah.

44:59.000 --> 45:01.000
So do you think that we'll be able to...

45:01.000 --> 45:04.000
I mean, we'll be able to create...

45:04.000 --> 45:10.000
I mean, the singularity is when we multiply our intelligence a million fold, and that's 2045.

45:10.000 --> 45:12.000
So that's not that long from now.

45:12.000 --> 45:14.000
That's like 20 years from now.

45:14.000 --> 45:16.000
Right.

45:16.000 --> 45:27.000
And therefore, most of your intelligence will be handled by the computer part of ourselves.

45:27.000 --> 45:33.000
The only thing that won't be captured is what comes with our body originally.

45:33.000 --> 45:36.000
We'll ultimately be able to do that as well.

45:36.000 --> 45:47.000
We'll take a little longer, but we'll be able to actually capture what comes with our normal body and be able to recreate that.

45:47.000 --> 45:56.000
So that also has to do with how long we live, because if everything is backed up...

45:56.000 --> 46:02.000
I mean, right now, anytime you put anything into a phone or any kind of electronics, it's backed up.

46:02.000 --> 46:05.000
So, I mean, this has a lot of data.

46:05.000 --> 46:12.000
I could flip it, and it ends up in a river, and we can't capture anymore.

46:12.000 --> 46:15.000
I can recreate it, because it's all backed up.

46:15.000 --> 46:18.000
And you think that's going to be the case with consciousness?

46:18.000 --> 46:23.000
That's going to be the case of our normal biological body as well.

46:23.000 --> 46:29.000
What's to stop someone like Donald Trump from just making 100,000 versions of himself?

46:29.000 --> 46:33.000
Like, if you can back someone up, could you duplicate it?

46:33.000 --> 46:35.000
Couldn't you have three or four of them?

46:35.000 --> 46:36.000
Couldn't you have a bunch of them?

46:36.000 --> 46:38.000
Couldn't you live multiple lives?

46:38.000 --> 46:40.000
Yes.

46:40.000 --> 46:46.000
Would you be interacting with each other while you're living multiple lives, having consultations about what is St. Louis Ray doing?

46:46.000 --> 46:48.000
Well, I don't know. Let's talk to San Francisco Ray.

46:48.000 --> 46:51.000
San Francisco Ray is talking to Florida Ray.

46:52.000 --> 47:02.000
It's basically a matter of increasing our intelligence and being able to multiply Donald Trump, for example, that comes with that.

47:02.000 --> 47:08.000
Do you think there'll be regulations on that to stop people from making 100,000 versions of themselves that operate a city?

47:08.000 --> 47:12.000
There'll be lots of regulations. There's lots of regulations we have already.

47:12.000 --> 47:17.000
You can't just create a medication and sell it to people that cares its disease.

47:17.000 --> 47:18.000
Right.

47:18.000 --> 47:20.000
We have a tremendous amount of regulations.

47:20.000 --> 47:27.000
Sure, but we don't really with phones. With your phone, essentially, if you had the money, you could make as many copies of that as you wanted.

47:27.000 --> 47:36.000
Yes. There are some regulations. We regulate everything, but you're right.

47:36.000 --> 47:43.000
Generally, electronics doesn't have as much regulation as...

47:43.000 --> 47:48.000
Right. And when you get to a certain point, we will be electronics.

47:48.000 --> 48:01.000
Yes. Certainly, if we multiply our intelligence a million fold, everything of that additional million fold of yours is not regulated.

48:01.000 --> 48:14.000
Right. When you think about the concept of integration and technological integration, when do you think that will start taking place and what will be the initial usage of it?

48:14.000 --> 48:20.000
Like, what will be the first versions and what would they provide?

48:20.000 --> 48:23.000
Well, we have it now. Large language models are pretty impressive.

48:23.000 --> 48:25.000
I mean, if you look at what they can do...

48:25.000 --> 48:31.000
I mean, I'm talking about physical integration with the human body, like a Neuralink type thing.

48:31.000 --> 48:42.000
Right. Some people feel that we could actually understand what's going on in your brain and actually put things into your brain without actually going into the brain with something like Neuralink.

48:42.000 --> 48:45.000
So something that sits on the outside of your head?

48:45.000 --> 48:53.000
Yeah. It's not clear to me if that's feasible or not. I've been assuming that you have to actually go in.

48:53.000 --> 49:04.000
Now, Neuralink isn't exactly what we want because it's too slow and it actually will do what it's advertised to do.

49:05.000 --> 49:20.000
I actually know some people like this who were active people and they completely lost the ability to speak and to understand language and so on.

49:20.000 --> 49:31.000
So they can't actually say anything to you and we can use something like Neuralink to actually have them express something.

49:31.000 --> 49:35.000
They can think something and then have it be expressed to you.

49:35.000 --> 49:39.000
Right. And they're doing that, right? They had the first patient. The first patient that was...

49:39.000 --> 49:40.000
Yeah.

49:40.000 --> 49:44.000
Yeah. And apparently that person can move a cursor around on a screen.

49:44.000 --> 49:50.000
Right. And therefore you can do anything. It's fairly slow, though. And Neuralink is slow.

49:50.000 --> 49:55.000
And if you really want to extend your brain, you need to do it at a much faster pace.

49:55.000 --> 49:57.000
But isn't that going to increase exponentially as well?

49:57.000 --> 49:58.000
Yes, absolutely.

49:58.000 --> 50:02.000
So how long do you think it will be before it's implemented?

50:02.000 --> 50:19.000
Well, it's got to be by 2045 because that's when the singularity exists and we can actually multiply our intelligence on the order of a million fold.

50:19.000 --> 50:29.000
And when you say 2045, what is the source of that estimation?

50:29.000 --> 50:46.000
Because we'll be able to, based actually on this chart and also the increase in the ability of software to also expand,

50:46.000 --> 50:56.000
we'll be able to multiply our intelligence a million fold and we'll be able to put that inside of our brain.

50:56.000 --> 50:59.000
It would be just like it's part of our brain.

50:59.000 --> 51:02.000
So this is just following the current graph of progress?

51:02.000 --> 51:03.000
Yeah, exactly.

51:03.000 --> 51:07.000
So if you follow the current graph of progress and if you do understand exponential growth,

51:07.000 --> 51:11.000
then what we're looking at in 2045 is inevitable.

51:11.000 --> 51:13.000
Right.

51:13.000 --> 51:16.000
Does that concern you at all or are you excited about it?

51:16.000 --> 51:22.000
Do you think it's just a thing that is happening and you're a part of it and you're experiencing it?

51:22.000 --> 51:30.000
I think it will be enthusiastic about it.

51:30.000 --> 51:38.000
I mean, imagine if you were to ask a mouse, would you like to actually be as intelligent as a human?

51:38.000 --> 51:41.000
Right.

51:41.000 --> 51:45.000
It's hard to know what people would say, but generally that's a positive thing.

51:45.000 --> 51:46.000
Generally.

51:46.000 --> 51:47.000
Yeah.

51:47.000 --> 51:49.000
And that's what it's going to be like.

51:49.000 --> 51:51.000
We're going to be that much smarter.

51:51.000 --> 51:52.000
And what do you...

51:52.000 --> 51:56.000
And once we're there, is someone going to say, no, I don't really like this.

51:56.000 --> 52:02.000
I want to be stupid like human beings used to be.

52:02.000 --> 52:05.000
Nobody's really going to say that.

52:05.000 --> 52:08.000
Do human beings now say, gee, I'm really too smart.

52:08.000 --> 52:11.000
I'd really like to be like a mouse.

52:11.000 --> 52:18.000
Not necessarily, but what people do say is that technology is too invasive and that it's too much a part of my life.

52:18.000 --> 52:24.000
And I'd like to sort of have a bit of an electronic vacation and separate from it.

52:24.000 --> 52:27.000
And there's a lot of people that I know that have gone to...

52:27.000 --> 52:28.000
But nobody does that.

52:28.000 --> 52:35.000
I mean, nobody becomes stupid like we used to be when we were mice.

52:35.000 --> 52:36.000
Right.

52:36.000 --> 52:37.000
But I'm not saying stupid.

52:37.000 --> 52:41.000
I'm saying some people, just like being a human, the way humans are now.

52:41.000 --> 52:46.000
Because one of the complications that comes with the integration of technology is what we're seeing now with people.

52:46.000 --> 52:51.000
Massive increases in anxiety from social media use, being manipulated by algorithms,

52:51.000 --> 52:56.000
the effect that it has on culture, misinformation and disinformation and propaganda.

52:56.000 --> 53:05.000
There's so many different factors that are at play now that make people more anxious and more depressed statistically than ever.

53:05.000 --> 53:13.000
I'm not sure we had more anxiety today than we used to have.

53:13.000 --> 53:16.000
Well, we certainly had more when the Mongols were invading.

53:16.000 --> 53:20.000
We certainly had more anxiety when we were worried constantly about war.

53:20.000 --> 53:23.000
But I think people have a pretty heightened level of social anxiety.

53:23.000 --> 53:24.000
Well, they take war.

53:24.000 --> 53:32.000
I mean, 80 years ago we had 100 million people die in Europe and Asia from World War II.

53:32.000 --> 53:38.000
We're very concerned about wars today and they're terrible.

53:38.000 --> 53:42.000
But we're not losing millions of people.

53:42.000 --> 53:43.000
Right.

53:43.000 --> 53:44.000
But we could.

53:44.000 --> 53:45.000
We most certainly could.

53:45.000 --> 53:51.000
With what's going on with Israel and Gaza, what's going on with Ukraine and Russia,

53:51.000 --> 53:53.000
it could easily escalate with...

53:53.000 --> 53:54.000
It's thousands of people.

53:54.000 --> 53:56.000
It's not millions of people.

53:56.000 --> 53:57.000
For now.

53:57.000 --> 53:58.000
Yeah.

53:58.000 --> 54:03.000
But if it escalates to a hot war where it's involving the entire world.

54:03.000 --> 54:10.000
What would really cause a tremendous amount of danger is something that's not really artificial intelligence.

54:10.000 --> 54:15.000
It was invented when I was a child, which is atomic weapons.

54:15.000 --> 54:16.000
Right.

54:16.000 --> 54:24.000
I remember when I was five or six, we'd actually go outside, put our hands behind our back

54:24.000 --> 54:27.000
to protect us from a nuclear war.

54:27.000 --> 54:28.000
Yeah, drills.

54:28.000 --> 54:30.000
And it seemed to work.

54:30.000 --> 54:31.000
We're still here.

54:31.000 --> 54:35.000
Do you remember those things where they tell kids to get under the desk?

54:35.000 --> 54:36.000
Yes, that's right.

54:36.000 --> 54:38.000
We went under the desk and put out...

54:38.000 --> 54:42.000
Which is hilarious, as if a desk is going to protect you from a nuclear bomb.

54:42.000 --> 54:43.000
Right.

54:43.000 --> 54:45.000
But that's not AI.

54:45.000 --> 54:46.000
Right.

54:46.000 --> 54:50.000
No, but AI applied to nuclear weapons makes them significantly more dangerous.

54:50.000 --> 54:55.000
And isn't one of the problems with AI is that AI will find a solution to a problem.

54:55.000 --> 55:01.000
So if you have AI running your military and AI says, what do you want me to do?

55:01.000 --> 55:04.000
And you say, well, I'd like to take over Taiwan.

55:04.000 --> 55:06.000
And AI says, well, this is how to do it.

55:06.000 --> 55:09.000
And it just implements it with no morals.

55:09.000 --> 55:18.000
No thought of any sort of diplomacy or just force.

55:18.000 --> 55:19.000
Right.

55:19.000 --> 55:21.000
It hasn't happened yet.

55:21.000 --> 55:26.000
Because we do have people in charge and the people are enhanced with AI.

55:26.000 --> 55:32.000
And AI can actually help us to avoid that kind of problem by thinking through the implications

55:32.000 --> 55:35.000
of different solutions.

55:35.000 --> 55:36.000
Sure.

55:36.000 --> 55:39.000
If it has some sort of autonomy.

55:39.000 --> 55:44.000
But if we get to the point where one superpower has AI, artificial general intelligence,

55:44.000 --> 55:51.000
the other one doesn't, how much of a significant advantage would that be?

55:51.000 --> 55:53.000
I mean, I do think there are problems.

55:53.000 --> 55:56.000
Basically, there's problems with intelligence.

55:56.000 --> 56:04.000
And we'd like to say stupid.

56:04.000 --> 56:08.000
But actually, it's better to be intelligent.

56:08.000 --> 56:11.000
I believe it's better to have intelligence.

56:11.000 --> 56:12.000
Overall.

56:12.000 --> 56:13.000
Sure.

56:13.000 --> 56:14.000
Right.

56:14.000 --> 56:21.000
But my question was, if there's a race to achieve AGI, how close is this race?

56:21.000 --> 56:22.000
Is it neck and neck?

56:22.000 --> 56:24.000
I mean, who's at the lead?

56:24.000 --> 56:28.000
And how much capital is being put into these companies that are at the lead?

56:28.000 --> 56:35.000
And whoever achieves it first, if that is under the control of a government, it's completely

56:35.000 --> 56:38.000
dependent upon what are the morals and ethics of that government?

56:38.000 --> 56:40.000
What was the Constitution?

56:40.000 --> 56:41.000
What if it happens in China?

56:41.000 --> 56:42.000
What if it happens in Russia?

56:42.000 --> 56:45.000
What if it happens somewhere other than the United States?

56:45.000 --> 56:49.000
And even if it does happen in the United States, who's controlling it?

56:49.000 --> 56:54.000
I mean, the knowledge of how to create these things is pretty widespread.

56:54.000 --> 57:02.000
It's not like somebody can just capitalize on a way to do it and nobody else understands

57:02.000 --> 57:03.000
it.

57:03.000 --> 57:14.000
The knowledge of how to create a large language model or how to create the type of chips that

57:14.000 --> 57:19.000
would enable you to create this is actually pretty widespread.

57:19.000 --> 57:24.000
So do you think essentially the competition is pretty even in all the countries currently?

57:24.000 --> 57:25.000
Yeah.

57:25.000 --> 57:29.000
And there's also probably espionage, there's espionage where they're stealing information

57:29.000 --> 57:33.000
and sharing information and selling information.

57:33.000 --> 57:46.000
And in terms of differences, the United States actually has superior AI compared to other

57:46.000 --> 57:47.000
places.

57:47.000 --> 57:48.000
That's pretty good for us.

57:48.000 --> 57:55.000
I mean, we're actually way ahead of China, I would say.

57:55.000 --> 57:59.000
Right, but China has a way of figuring out what we're doing in copying it.

57:59.000 --> 58:01.000
Pretty good at that.

58:01.000 --> 58:03.000
They have been, yeah.

58:03.000 --> 58:04.000
Yeah.

58:04.000 --> 58:10.000
So do you have any concern whatsoever in the idea that AI gets in the hands of the wrong

58:10.000 --> 58:12.000
people?

58:12.000 --> 58:16.000
So when it first gets implemented, that's the big problem.

58:16.000 --> 58:21.000
Because before it exists, before artificial general intelligence really exists, it doesn't.

58:21.000 --> 58:22.000
And then it does.

58:22.000 --> 58:23.000
And who has it?

58:23.000 --> 58:27.000
And then once it does, can that AGI stop other people from getting it?

58:27.000 --> 58:32.000
Can you program it to make sure, can you sabotage grids?

58:32.000 --> 58:36.000
Can you do whatever you can to take down the internet in these opposing places?

58:36.000 --> 58:39.000
Could you inject their computations with viruses?

58:39.000 --> 58:45.000
What could you do to stop other people from getting to where you're at if you have an infinitely

58:45.000 --> 58:46.000
higher intelligence?

58:46.000 --> 58:47.000
First.

58:47.000 --> 58:52.000
If that's what your goal is, then yes, you could do that.

58:52.000 --> 58:54.000
Are you worried about that at all?

58:54.000 --> 58:55.000
Yes, I worry about it.

58:55.000 --> 58:57.000
What is your main worry?

58:57.000 --> 59:00.000
I mean, you worry about the implementation of artificial intelligence.

59:00.000 --> 59:10.000
What's your main worry?

59:10.000 --> 59:23.000
I mean, I'm worried if people who have a destructive idea of how to use these capabilities get into

59:23.000 --> 59:24.000
control.

59:24.000 --> 59:27.000
Right.

59:27.000 --> 59:29.000
And that could happen.

59:29.000 --> 59:36.000
And I've got a chapter in the book about perils that are like what we're talking about.

59:37.000 --> 59:44.000
And what do you think that could look like if the wrong people got to hold this technology?

59:44.000 --> 59:52.000
Well, if you look at actually who controls atomic weapons, which is not AI, some of the

59:52.000 --> 01:00:01.000
worst people in the world, and if you were to ask people right after we used two atomic

01:00:02.000 --> 01:00:08.000
weapons within a week 80 years ago, what's the likelihood that we're going to go another

01:00:08.000 --> 01:00:12.000
80 years and not have that happen again?

01:00:12.000 --> 01:00:14.000
Everybody would say zero.

01:00:14.000 --> 01:00:15.000
Right.

01:00:15.000 --> 01:00:17.000
But it actually has happened.

01:00:17.000 --> 01:00:18.000
Chalking.

01:00:18.000 --> 01:00:19.000
Yeah.

01:00:19.000 --> 01:00:20.000
Yeah.

01:00:20.000 --> 01:00:25.000
And I think there's actually some message there.

01:00:25.000 --> 01:00:27.000
Mutual assured destruction.

01:00:27.000 --> 01:00:30.000
But the thing is, would artificial intelligence...

01:00:30.000 --> 01:00:32.000
But that has not happened.

01:00:32.000 --> 01:00:33.000
Right.

01:00:33.000 --> 01:00:34.000
It has not happened yet.

01:00:34.000 --> 01:00:38.000
But would artificial general intelligence in the control of the wrong people negate

01:00:38.000 --> 01:00:42.000
that mutually assured destruction that keeps people from doing things?

01:00:42.000 --> 01:00:47.000
Obviously, we did drop bombs on Hiroshima and Nagasaki.

01:00:47.000 --> 01:00:48.000
We did.

01:00:48.000 --> 01:00:52.000
We did indiscriminately kill who knows how many hundreds of thousands of people with

01:00:52.000 --> 01:00:53.000
those weapons.

01:00:53.000 --> 01:00:54.000
We did it.

01:00:54.000 --> 01:00:59.000
And if human beings were capable of doing it because no one else had it, if artificial

01:00:59.000 --> 01:01:06.000
intelligence reaches that sentient level and is in control of the wrong people, what's

01:01:06.000 --> 01:01:08.000
to stop them from doing...

01:01:08.000 --> 01:01:11.000
There's no mutually assured destruction if you're the one who's got it.

01:01:11.000 --> 01:01:13.000
You're the only one who's got it.

01:01:13.000 --> 01:01:15.000
And you possibly...

01:01:15.000 --> 01:01:20.000
My concern is that whoever gets it could possibly stop it from being spread everywhere

01:01:20.000 --> 01:01:23.000
else and control it completely.

01:01:23.000 --> 01:01:28.000
And then you're looking at a completely dystopian world.

01:01:28.000 --> 01:01:29.000
Right.

01:01:29.000 --> 01:01:33.000
So if you ask me what I'm concerned about, it's one of those lines.

01:01:33.000 --> 01:01:36.000
Because that's what I always want to get out of you guys.

01:01:36.000 --> 01:01:41.000
Because there's so many people that are rightfully so, so high on this technology and the possibilities

01:01:41.000 --> 01:01:43.000
for enhancing our lives.

01:01:43.000 --> 01:01:48.000
But the concern that a lot of people have is that at what cost and what are we signing

01:01:48.000 --> 01:01:49.000
up for?

01:01:49.000 --> 01:01:50.000
Right.

01:01:50.000 --> 01:01:57.000
But I mean, if we want to, for example, live indefinitely, this is what we need to do.

01:01:57.000 --> 01:01:58.000
We can't do...

01:01:58.000 --> 01:02:00.000
What if you're denying yourself heaven?

01:02:00.000 --> 01:02:02.000
You ever thought of that possibility?

01:02:02.000 --> 01:02:06.000
I know that's a ridiculous abstract concept, but if heaven is real, if the idea of the

01:02:06.000 --> 01:02:11.000
afterlife is real and it's the next level of existence and you're constantly going through

01:02:11.000 --> 01:02:17.000
these cycles of life, what if you're stepping in artificially denying that?

01:02:17.000 --> 01:02:19.000
It's hard to imagine.

01:02:19.000 --> 01:02:21.000
It is hard to imagine, but so is life.

01:02:21.000 --> 01:02:23.000
So is the universe itself.

01:02:23.000 --> 01:02:24.000
So is the big bang.

01:02:24.000 --> 01:02:25.000
My father...

01:02:25.000 --> 01:02:35.000
My father died when I was 22, so it's more than 50, 60 years ago.

01:02:35.000 --> 01:02:39.000
And it's hard...

01:02:39.000 --> 01:02:46.000
And he was actually a great musician and he created fantastic music, but he hasn't done

01:02:46.000 --> 01:02:51.000
that since he died.

01:02:51.000 --> 01:03:00.000
And there's nothing that exists that is at all creative based on him.

01:03:00.000 --> 01:03:03.000
We have his memories.

01:03:03.000 --> 01:03:06.000
Actually created a large language model that represented him.

01:03:06.000 --> 01:03:08.000
I can actually talk to him.

01:03:08.000 --> 01:03:10.000
You do that now?

01:03:10.000 --> 01:03:11.000
Yeah.

01:03:11.000 --> 01:03:13.000
It's in the book.

01:03:13.000 --> 01:03:19.000
When you do that, have you thought about implementing some sort of a Sora-type deal where you're

01:03:19.000 --> 01:03:22.000
talking to him?

01:03:22.000 --> 01:03:24.000
Well you can do that now with language.

01:03:24.000 --> 01:03:29.000
Right, but I mean physically like looking at him like you're on a zoom call with him.

01:03:29.000 --> 01:03:35.000
That's a little bit in the future to be able to actually capture the way he looks, but

01:03:35.000 --> 01:03:37.000
that's also feasible.

01:03:37.000 --> 01:03:39.000
It seems pretty feasible.

01:03:39.000 --> 01:03:43.000
And certainly it could be something representative of what he looks based on photographs that

01:03:43.000 --> 01:03:45.000
you have, right?

01:03:45.000 --> 01:03:52.000
Things like that is a reason to continue so that we can create that and create our own

01:03:52.000 --> 01:03:58.000
ability to continue to exist.

01:03:58.000 --> 01:04:06.000
You talk to people and they say, well I don't really want to live past 90 or whatever, 100.

01:04:06.000 --> 01:04:15.000
But in my mind if you don't exist, there's nothing for you to experience.

01:04:15.000 --> 01:04:17.000
That's true in this dimension.

01:04:17.000 --> 01:04:22.000
My thought on that, people saying that I don't want to live past 90, it's like okay are you

01:04:22.000 --> 01:04:23.000
alive now?

01:04:23.000 --> 01:04:24.000
Do you like being alive now?

01:04:24.000 --> 01:04:26.000
What's the difference between now and 90?

01:04:26.000 --> 01:04:31.000
Is it just a number or is it the deterioration of your physical body and how much effort

01:04:31.000 --> 01:04:36.000
have you put into mitigating the deterioration of your natural body so that you can enjoy

01:04:36.000 --> 01:04:37.000
life now?

01:04:37.000 --> 01:04:38.000
Exactly.

01:04:38.000 --> 01:04:42.000
And we've actually seen who would want to take their lives.

01:04:42.000 --> 01:04:45.000
People do take their lives.

01:04:45.000 --> 01:04:54.000
If they are experiencing something that's miserable, if they're suffering physically, emotionally,

01:04:54.000 --> 01:05:04.000
mentally, spiritually, and they just cannot stand the way life is carrying on, then they

01:05:04.000 --> 01:05:06.000
want to take their lives.

01:05:06.000 --> 01:05:09.000
Otherwise people don't.

01:05:09.000 --> 01:05:12.000
If they're enjoying their lives, they continue.

01:05:12.000 --> 01:05:15.000
And people say, I don't want to live past 100.

01:05:15.000 --> 01:05:26.000
But when they get to be 99.9, they don't want to disappear unless they're suffering.

01:05:26.000 --> 01:05:27.000
Unless they're suffering.

01:05:27.000 --> 01:05:32.000
That's what's interesting about the positive aspects of AI.

01:05:32.000 --> 01:05:37.000
Once we can manipulate human neurochemistry to the point where we figure out what is causing

01:05:37.000 --> 01:05:42.000
great depression, what is causing anxiety, what is causing a lot of these schizophrenic

01:05:42.000 --> 01:05:43.000
people.

01:05:43.000 --> 01:05:44.000
And we definitely had that before.

01:05:44.000 --> 01:05:45.000
We didn't have the terms.

01:05:45.000 --> 01:05:47.000
We didn't understand schizophrenia.

01:05:47.000 --> 01:05:49.000
But people definitely had it.

01:05:49.000 --> 01:05:50.000
For sure.

01:05:50.000 --> 01:05:53.000
But what if we get to a point where we can mitigate that with technology?

01:05:53.000 --> 01:05:56.000
Where we can say, this is what's going on in the human body.

01:05:56.000 --> 01:05:57.000
That's why we're continuing.

01:05:57.000 --> 01:05:58.000
Right.

01:05:58.000 --> 01:05:59.000
I was saying that's a good thing.

01:05:59.000 --> 01:06:01.000
That's a positive aspect of this technology.

01:06:01.000 --> 01:06:04.000
And think about also profoundly.

01:06:04.000 --> 01:06:09.000
Think about how many people do take their lives and with this technology would not just live

01:06:09.000 --> 01:06:15.000
happily but also be productive and also contribute to whatever society is doing.

01:06:15.000 --> 01:06:19.000
That's why we're carrying on with this.

01:06:19.000 --> 01:06:25.000
But in order to do that, we do have to overcome some of the problems that you've articulated.

01:06:25.000 --> 01:06:31.000
I think what a lot of people are terrified of is that these people that are creating this

01:06:31.000 --> 01:06:35.000
technology, there's oversight.

01:06:35.000 --> 01:06:40.000
But it's oversight by people that don't necessarily understand it the way the people that are creating

01:06:40.000 --> 01:06:41.000
it.

01:06:41.000 --> 01:06:43.000
And they don't know what guardrails are in place.

01:06:43.000 --> 01:06:44.000
How safe is this?

01:06:44.000 --> 01:06:50.000
Especially when it's implemented with some sort of weapons technology.

01:06:50.000 --> 01:06:52.000
Or some sort of a military application.

01:06:52.000 --> 01:06:57.000
Especially a military application that can be insanely profitable.

01:06:57.000 --> 01:07:01.000
And the motivations behind utilizing that are that profit.

01:07:01.000 --> 01:07:06.000
And then we do horrible things and somehow justify it.

01:07:06.000 --> 01:07:12.000
I mean I think democracy is actually an important issue here because democratic nations tend

01:07:12.000 --> 01:07:15.000
not to go to war with each other.

01:07:15.000 --> 01:07:27.000
And I mean you look at the way we're handling military technology.

01:07:27.000 --> 01:07:32.000
If everybody was a democracy, I think there'd be much less war.

01:07:32.000 --> 01:07:36.000
As long as it's a legitimate democracy, it's not controlled by money.

01:07:36.000 --> 01:07:41.000
As long as it's a legitimate democracy, it's not controlled by the military industrial complex.

01:07:41.000 --> 01:07:46.000
The pharmaceutical industry or whoever puts the people that are in elected places.

01:07:46.000 --> 01:07:47.000
Who puts them in there?

01:07:47.000 --> 01:07:48.000
How do they get funded?

01:07:48.000 --> 01:07:51.000
And what do they represent once they get in there?

01:07:51.000 --> 01:07:53.000
Are they there for the will of the people?

01:07:53.000 --> 01:07:54.000
They're there for their own career?

01:07:54.000 --> 01:07:58.000
Do they bypass the safety and the future of the people for their own personal gain,

01:07:58.000 --> 01:08:01.000
which we've seen politicians do?

01:08:01.000 --> 01:08:06.000
There's certain problems with every system that involves human beings.

01:08:06.000 --> 01:08:09.000
This is another thing that technology may be able to do.

01:08:09.000 --> 01:08:14.000
One of the things, if you think about the worst attributes of humans.

01:08:14.000 --> 01:08:23.000
Whether it's war, crime, some of the horrible things that human beings are capable of.

01:08:23.000 --> 01:08:31.000
Imagine that technology can find what causes those thoughts and behaviors in human beings and mitigate them.

01:08:31.000 --> 01:08:38.000
I've joked around about this, but if we came up with something that would elevate dopamine just 300% worldwide,

01:08:38.000 --> 01:08:39.000
there would be no more war.

01:08:39.000 --> 01:08:40.000
It'd be over.

01:08:40.000 --> 01:08:42.000
Everybody would be loving everybody.

01:08:42.000 --> 01:08:44.000
We'd be interacting with each other.

01:08:44.000 --> 01:08:46.000
Well, that's the point of doing this.

01:08:46.000 --> 01:08:48.000
But there would also be no sad songs.

01:08:48.000 --> 01:08:49.000
Wow.

01:08:49.000 --> 01:08:51.000
You need some blues in your life.

01:08:51.000 --> 01:08:53.000
You need a little bit of that too.

01:08:53.000 --> 01:08:54.000
Or do we?

01:08:54.000 --> 01:08:55.000
Maybe we don't.

01:08:55.000 --> 01:08:58.000
Maybe that's just a byproduct of our monkey minds.

01:08:58.000 --> 01:09:04.000
And that one day we'll surpass that and get to this point of enlightenment.

01:09:04.000 --> 01:09:12.000
Enlightenment seems possible without technological innovation, but maybe not.

01:09:12.000 --> 01:09:15.000
I've never really met a truly enlightened person.

01:09:15.000 --> 01:09:16.000
I've met some people that are pretty close.

01:09:16.000 --> 01:09:19.000
But if you could get there with technology.

01:09:19.000 --> 01:09:25.000
If technology just completely elevated the human consciousness to the point where all of our conflicts come erased.

01:09:25.000 --> 01:09:33.000
Just for starters, if you could actually live longer, quite aside from the motivations of people,

01:09:33.000 --> 01:09:40.000
most people die not because of people's motivations, but because our bodies just won't last that long.

01:09:40.000 --> 01:09:41.000
Right.

01:09:42.000 --> 01:09:50.000
And a lot of people say, you know, I don't want to live longer, which makes no sense to me.

01:09:50.000 --> 01:09:57.000
Why would you want to disappear and not be able to have any kind of experience?

01:09:57.000 --> 01:10:00.000
Well, I think some people don't think you're disappearing.

01:10:00.000 --> 01:10:09.000
I mean, there is a long held thought in many cultures that this life is but one step.

01:10:09.000 --> 01:10:20.000
And that there is an afterlife and maybe that exists to comfort us because we deal with existential angst and the reality of our own inevitable demise.

01:10:20.000 --> 01:10:26.000
Or maybe it's a function of consciousness being something that we don't truly understand.

01:10:26.000 --> 01:10:38.000
And what you are is a soul contained in a body and that we have a very primitive understanding of the existence of life itself and of the existence of everything.

01:10:38.000 --> 01:10:41.000
Well, I guess that makes sense.

01:10:41.000 --> 01:10:44.000
But I don't really accept it.

01:10:44.000 --> 01:10:46.000
Well, there's no evidence, right?

01:10:46.000 --> 01:10:54.000
But is it there's no evidence because we're not capable of determining it yet and understanding it?

01:10:54.000 --> 01:10:57.000
Or is it just because it doesn't exist?

01:10:57.000 --> 01:10:59.000
That's the real question.

01:10:59.000 --> 01:11:01.000
It's like, is this it?

01:11:01.000 --> 01:11:02.000
Is this everything?

01:11:02.000 --> 01:11:04.000
Or is this merely a stage?

01:11:04.000 --> 01:11:12.000
And are we monkeying with that stage by interfering with the process of life and death?

01:11:12.000 --> 01:11:14.000
Well, it makes sense.

01:11:14.000 --> 01:11:17.000
But I don't really see the evidence for that.

01:11:17.000 --> 01:11:20.000
I can see from your perspective.

01:11:20.000 --> 01:11:24.000
I don't see the evidence of it either, but it's a concept that is not.

01:11:24.000 --> 01:11:36.000
It's just when you start talking to strength theorists and they start talking about things existing and not existing at the same time, particles in superposition, you're talking about magic.

01:11:36.000 --> 01:11:40.000
You're talking about something that's impossible to wrap your head around.

01:11:40.000 --> 01:11:43.000
Even just the structure of an atom.

01:11:43.000 --> 01:11:44.000
What's in there?

01:11:44.000 --> 01:11:45.000
Nothing?

01:11:45.000 --> 01:11:47.000
How much of it is space?

01:11:47.000 --> 01:11:53.000
The entire existence of everything in the universe seems preposterous.

01:11:53.000 --> 01:11:55.000
But it's all real.

01:11:55.000 --> 01:12:03.000
And we only have a limited grasp of understanding of what this is really all about and what processes are really in place.

01:12:03.000 --> 01:12:04.000
Right.

01:12:04.000 --> 01:12:16.000
But if you look at people's perspective, if somebody gets a disease and is kind of known they could only live like another six months, people are not happy with that.

01:12:16.000 --> 01:12:17.000
No.

01:12:17.000 --> 01:12:18.000
Well, they're scared.

01:12:18.000 --> 01:12:19.000
They're scared to die.

01:12:19.000 --> 01:12:24.000
It's a natural human instinct that keeps us alive for all these hundreds of millions of years.

01:12:24.000 --> 01:12:26.000
Yes, but very few people would be happy with that.

01:12:26.000 --> 01:12:34.000
And if you then had something, gee, we have this new device, you could take this and you won't die.

01:12:34.000 --> 01:12:35.000
Right.

01:12:35.000 --> 01:12:37.000
Almost everybody would do that.

01:12:37.000 --> 01:12:38.000
Sure.

01:12:38.000 --> 01:12:42.000
But would they appreciate life if they knew it had no end?

01:12:43.000 --> 01:12:51.000
Or would it be like a lottery winner just goes nuts and spends all their money and loses their marbles because they can't believe they can't die?

01:12:51.000 --> 01:12:55.000
Well, first of all, it's not guaranteed to live forever.

01:12:55.000 --> 01:12:56.000
Sure.

01:12:56.000 --> 01:12:57.000
You can get in an accident.

01:12:57.000 --> 01:12:58.000
Something can happen.

01:12:58.000 --> 01:12:59.000
You get injured.

01:12:59.000 --> 01:13:06.000
But if we get to a point where you have automated cars that significantly reduce the amount of automobile accidents.

01:13:06.000 --> 01:13:11.000
Well, also, we can back up everything, everything in our physical body as well as...

01:13:11.000 --> 01:13:13.000
How far away are we from that?

01:13:13.000 --> 01:13:20.000
That idea of, I mean, we don't really truly understand what consciousness is, correct?

01:13:20.000 --> 01:13:21.000
Right.

01:13:21.000 --> 01:13:30.000
So how would we be able to manipulate it or reduplicate it to the point where you're putting it inside of some kind of a computation device?

01:13:30.000 --> 01:13:44.000
Well, we know to be able to create a computation that matches what our brain does.

01:13:44.000 --> 01:13:47.000
That's what we're doing with these large language models.

01:13:47.000 --> 01:13:48.000
Right.

01:13:48.000 --> 01:13:56.000
And we're actually very close now to what our brain can do with these large language models and it'll be there like within a year.

01:13:56.000 --> 01:14:13.000
And we can back up the electronic version and we'll get to the point where we can back up what our brain normally does.

01:14:13.000 --> 01:14:16.000
So we'll be able to actually back that up as well.

01:14:16.000 --> 01:14:21.000
We'll be able to detect what it is and back that up just like our computers.

01:14:21.000 --> 01:14:27.000
We'll create it in the form of an artificial version of everything that it is to be a human being.

01:14:27.000 --> 01:14:28.000
Right, exactly.

01:14:28.000 --> 01:14:31.000
In terms of emotions, love, excitement.

01:14:31.000 --> 01:14:34.000
And that's going to happen over the next 20 years.

01:14:34.000 --> 01:14:37.000
It's not a thousand years.

01:14:37.000 --> 01:14:39.000
But will that be a person?

01:14:39.000 --> 01:14:42.000
I mean, or will it be some sort of a zombie?

01:14:42.000 --> 01:14:44.000
Like what motivations will it have?

01:14:44.000 --> 01:14:50.000
If you can take human consciousness and duplicate it, much like you could duplicate your phone,

01:14:50.000 --> 01:14:53.000
and you make this new thing, what does that thing feel like?

01:14:53.000 --> 01:14:55.000
Does that thing live in hell?

01:14:55.000 --> 01:14:58.000
What does that experience like for that thing?

01:14:58.000 --> 01:15:00.000
What about large language models?

01:15:00.000 --> 01:15:02.000
Do they really exist?

01:15:02.000 --> 01:15:04.000
I mean, they can talk.

01:15:04.000 --> 01:15:07.000
They certainly do, but would you want to be one?

01:15:07.000 --> 01:15:09.000
Are we different than that?

01:15:09.000 --> 01:15:10.000
Yeah, we're people.

01:15:10.000 --> 01:15:11.000
We shake hands.

01:15:11.000 --> 01:15:12.000
I give you a hug.

01:15:12.000 --> 01:15:13.000
You pet my dog.

01:15:13.000 --> 01:15:15.000
You listen to music.

01:15:15.000 --> 01:15:17.000
We'll be able to do all of that as well.

01:15:17.000 --> 01:15:18.000
Right, but will you want to?

01:15:18.000 --> 01:15:19.000
Will you even care?

01:15:19.000 --> 01:15:23.000
A lot of what gives us joy in life is biological motivations.

01:15:23.000 --> 01:15:26.000
There's human reward systems that are put in place that allow us to...

01:15:26.000 --> 01:15:28.000
Well, it's going to be part of who we are.

01:15:28.000 --> 01:15:29.000
Right.

01:15:29.000 --> 01:15:34.000
And we just like a person, and we'll also have our physical bodies as well.

01:15:34.000 --> 01:15:37.000
And that'll also be able to be backed up.

01:15:37.000 --> 01:15:43.000
And we'll be doing the things that we do now, except we'll be able to have them continue.

01:15:43.000 --> 01:15:47.000
So if you get hit by a car and you die, there's another array that just pops up.

01:15:47.000 --> 01:15:49.000
Oh, we got the backup array.

01:15:49.000 --> 01:15:57.000
And the backup array will have no feelings at all about having it had died and come back to life.

01:15:57.000 --> 01:15:59.000
Well, that's a question.

01:15:59.000 --> 01:16:00.000
Yeah.

01:16:00.000 --> 01:16:05.000
I mean, why wouldn't it be just like Ray is now?

01:16:05.000 --> 01:16:06.000
Why wouldn't it?

01:16:06.000 --> 01:16:12.000
If we figure out that if biological life is essentially a kind of technology that the

01:16:12.000 --> 01:16:17.000
universe has created, and we can manipulate that to the point where we understand it,

01:16:17.000 --> 01:16:23.000
we get it, we've optimized it, and then replicate it.

01:16:23.000 --> 01:16:24.000
Physically replicate it.

01:16:24.000 --> 01:16:30.000
Not just replicate it in the form of a computer, but an actual physical being.

01:16:30.000 --> 01:16:31.000
Right.

01:16:31.000 --> 01:16:32.000
Well, that's where we're headed.

01:16:32.000 --> 01:16:37.000
Do you anticipate that people will be happy with whatever they have?

01:16:37.000 --> 01:16:42.000
Because if you decide, I don't like being five, six, I wish I was six, six.

01:16:42.000 --> 01:16:43.000
I don't like being a woman.

01:16:43.000 --> 01:16:44.000
I like, I want to be a man.

01:16:44.000 --> 01:16:46.000
I don't want to be Asian.

01:16:46.000 --> 01:16:48.000
I want to be, you know, whatever.

01:16:48.000 --> 01:16:49.000
I want to be a black person.

01:16:49.000 --> 01:16:50.000
I want to be...

01:16:50.000 --> 01:16:57.000
We'll actually be able to do all of those things simultaneously and so on.

01:16:57.000 --> 01:17:01.000
We're not going to be limited by those kinds of happenstance.

01:17:01.000 --> 01:17:03.000
Which is going to be very strange.

01:17:03.000 --> 01:17:07.000
What will human beings look like if you give people the ability to manipulate your physical

01:17:07.000 --> 01:17:08.000
form?

01:17:08.000 --> 01:17:11.000
Well, we do things now that were impossible even 10 years ago.

01:17:11.000 --> 01:17:16.000
We certainly do, but we don't change race, size, sex, gender, height.

01:17:16.000 --> 01:17:21.000
We don't do all those...the radical increase in just your intelligence.

01:17:21.000 --> 01:17:23.000
Like, what is that going to look like?

01:17:23.000 --> 01:17:28.000
What kind of an interaction is it going to be between two human beings when you have

01:17:28.000 --> 01:17:30.000
a completely new form?

01:17:30.000 --> 01:17:34.000
You know, you're much different physically than you ever were when you were alive.

01:17:34.000 --> 01:17:35.000
You're taller.

01:17:35.000 --> 01:17:36.000
You're stronger.

01:17:36.000 --> 01:17:37.000
You're smarter.

01:17:37.000 --> 01:17:38.000
You're faster.

01:17:38.000 --> 01:17:40.000
You're basically not really a human anymore.

01:17:40.000 --> 01:17:42.000
You're a new thing.

01:17:42.000 --> 01:17:44.000
I mean, we're expanding who we are.

01:17:44.000 --> 01:17:47.000
We're already expanded who we are from, you know...

01:17:47.000 --> 01:17:48.000
Sure.

01:17:48.000 --> 01:17:49.000
Right.

01:17:49.000 --> 01:17:53.000
Over a course of hundreds of thousands of years, we've gone from being Australopithecus

01:17:53.000 --> 01:17:54.000
to what we are now.

01:17:54.000 --> 01:18:01.000
That has to do with the pace at which we make changes.

01:18:01.000 --> 01:18:02.000
Right.

01:18:02.000 --> 01:18:10.000
And we can make changes now much more quickly than we could, you know, 100,000 years ago.

01:18:10.000 --> 01:18:11.000
Right.

01:18:11.000 --> 01:18:16.000
But if we can manipulate our physical form with no limitations, I mean, what are...we

01:18:16.000 --> 01:18:18.000
have six armed people that can fly?

01:18:18.000 --> 01:18:19.000
Like, what is it going to look like?

01:18:19.000 --> 01:18:21.000
Well, do you have a problem with that?

01:18:21.000 --> 01:18:24.000
Yeah, I would discriminate against six armed people that can fly.

01:18:24.000 --> 01:18:27.000
That's the one area I allow myself to give prejudice to.

01:18:27.000 --> 01:18:28.000
Okay.

01:18:28.000 --> 01:18:32.000
No, I'm just curious as to how much time you've spent...

01:18:32.000 --> 01:18:34.000
Seven armed people would be okay.

01:18:34.000 --> 01:18:39.000
Yeah, seven armed people is cool because it's like, you know, maybe five on one side,

01:18:39.000 --> 01:18:40.000
two on the other.

01:18:40.000 --> 01:18:47.000
No, I'm just curious as to like how much time you've spent thinking about what this could

01:18:47.000 --> 01:18:48.000
look like.

01:18:48.000 --> 01:18:53.000
And I just...I don't think it's going to be as simple as, you know, it's going to be

01:18:53.000 --> 01:18:59.000
Ray Kurzweil, but Ray Kurzweil as like a 30-year-old man, 50 years from now.

01:18:59.000 --> 01:19:03.000
I think it's probably going to be...you're going to be all kinds of different things.

01:19:03.000 --> 01:19:04.000
You could be kind of whatever you want.

01:19:04.000 --> 01:19:05.000
You could be a bird.

01:19:05.000 --> 01:19:11.000
I mean, what's to stop...if we can get to manipulate the physical form and we can take consciousness

01:19:11.000 --> 01:19:12.000
and put it into a physical form.

01:19:12.000 --> 01:19:17.000
But that's a description, I think, of something that's positive rather than negative.

01:19:17.000 --> 01:19:18.000
You could be a giant eagle.

01:19:18.000 --> 01:19:25.000
I mean, negative is people that want to destroy things, getting power.

01:19:25.000 --> 01:19:26.000
Sure.

01:19:26.000 --> 01:19:28.000
And that is a problem.

01:19:28.000 --> 01:19:32.000
Well, it's certainly improvement in terms of the viability.

01:19:32.000 --> 01:19:35.000
Seven arms and being like an eagle and so on.

01:19:35.000 --> 01:19:40.000
I mean, and you can also change that.

01:19:40.000 --> 01:19:41.000
Right.

01:19:41.000 --> 01:19:46.960
So I think that's a positive aspect and we will be able to do that kind of thing.

01:19:46.960 --> 01:19:47.960
Sure.

01:19:47.960 --> 01:19:52.560
If you want to look at it in a binary fashion, positive and negative, but it's also going

01:19:52.560 --> 01:19:58.240
to be insanely strange, like it's not going to be as simple as there'll be people that

01:19:58.240 --> 01:19:59.960
are living in 2069.

01:19:59.960 --> 01:20:04.160
Well, it seems strange once it's first reported.

01:20:04.160 --> 01:20:08.960
If it's been reported now for five years and people are constantly doing it, you won't find

01:20:08.960 --> 01:20:09.960
it that strange.

01:20:09.960 --> 01:20:11.960
It'll just be life.

01:20:11.960 --> 01:20:12.960
Yeah.

01:20:12.960 --> 01:20:13.960
Yeah.

01:20:13.960 --> 01:20:14.960
So that's what I'm asking.

01:20:14.960 --> 01:20:19.560
Like when you think about the implementation of this technology to its fullest, what does

01:20:19.560 --> 01:20:21.440
the world look like?

01:20:21.440 --> 01:20:28.040
What does the world look like in 2069?

01:20:28.040 --> 01:20:34.480
I mean, the kind of things that you can imagine right now will be able to do.

01:20:34.480 --> 01:20:39.560
And it might seem strange when it first happens, but when it happens for the, you know, billions

01:20:39.560 --> 01:20:43.400
of dollars in time, it won't seem that strange.

01:20:43.400 --> 01:20:49.880
And maybe you're like being an eagle for a few minutes.

01:20:49.880 --> 01:20:51.640
It's certainly interesting.

01:20:51.640 --> 01:20:54.000
It's certainly interesting.

01:20:54.000 --> 01:20:59.520
I just wonder how much time you've spent thinking about what this world looks like with the

01:20:59.520 --> 01:21:05.760
full implementation of the kind of exponential growth of technology that would exist if we

01:21:05.760 --> 01:21:07.640
do make it to 2069.

01:21:07.640 --> 01:21:20.200
Well, I did write a book, Danielle, and this young girl has fantastic capabilities and

01:21:20.200 --> 01:21:23.680
no one really can figure out how she does this.

01:21:23.680 --> 01:21:35.840
She actually takes over China at age 15 and she makes it a democracy and then she actually

01:21:36.040 --> 01:21:43.320
becomes president of the United States at 19, she has to, of course, create a constitutional

01:21:43.320 --> 01:21:49.080
amendment that at least she can become president at 19.

01:21:49.080 --> 01:21:51.640
That sounds like what a dictator would do.

01:21:51.640 --> 01:21:53.200
Right.

01:21:53.200 --> 01:22:00.480
But unlike a dictator, she's very popular and she writes very good music.

01:22:00.480 --> 01:22:03.480
And this is one artificial intelligence creature?

01:22:03.480 --> 01:22:04.480
Yes.

01:22:04.480 --> 01:22:05.480
And how was she created?

01:22:05.720 --> 01:22:12.840
It never says that she gets these capabilities through AI.

01:22:12.840 --> 01:22:18.440
I didn't want to spell that out, but that would be the only way that she could do this.

01:22:18.440 --> 01:22:19.440
Right.

01:22:19.440 --> 01:22:22.200
Unless it's some insane freak of genetics.

01:22:22.200 --> 01:22:25.840
And she's like a very positive person.

01:22:25.840 --> 01:22:27.840
She's very popular.

01:22:27.840 --> 01:22:31.120
Yeah, but she's the only one that has that.

01:22:31.120 --> 01:22:32.120
Yeah.

01:22:32.120 --> 01:22:33.120
Right.

01:22:33.120 --> 01:22:35.440
She doesn't give it to everybody, which is where it gets really weird.

01:22:35.480 --> 01:22:37.160
You have a cell phone, I have a cell phone.

01:22:37.160 --> 01:22:38.560
Pretty much everybody has one now.

01:22:38.560 --> 01:22:41.960
What happens when everybody gets the kind of technology we're discussing?

01:22:41.960 --> 01:22:48.960
Well, it shows you the benefit that she has it and if everybody gets it, that would be

01:22:48.960 --> 01:22:50.480
even more positive.

01:22:50.480 --> 01:22:51.480
Right.

01:22:51.480 --> 01:22:52.480
Perhaps, yeah.

01:22:52.480 --> 01:22:56.840
I mean, that's the best way of looking at it, that we become a completely altruistic,

01:22:56.840 --> 01:23:04.240
positive, beneficial to each other, society of integrated minds.

01:23:04.240 --> 01:23:05.240
A benefit.

01:23:05.240 --> 01:23:09.200
If you have more intelligence, you'd be more likely to do this.

01:23:09.200 --> 01:23:10.200
Yes.

01:23:10.200 --> 01:23:12.680
Yeah, for sure.

01:23:12.680 --> 01:23:13.680
That's the benefit.

01:23:13.680 --> 01:23:14.680
Yeah.

01:23:14.680 --> 01:23:15.680
Yeah.

01:23:15.680 --> 01:23:21.600
So we live longer and we're also smarter than making more rational decisions towards each

01:23:21.600 --> 01:23:23.640
other.

01:23:23.640 --> 01:23:28.440
So overall, when you're looking at this, you just don't concentrate really on the negative

01:23:28.440 --> 01:23:29.440
possibilities.

01:23:29.440 --> 01:23:30.440
Well, no.

01:23:30.440 --> 01:23:33.200
I mean, I do focus on that as well.

01:23:33.200 --> 01:23:34.200
I mean.

01:23:34.200 --> 01:23:37.280
But you think overall it's net positive?

01:23:37.280 --> 01:23:38.280
Yes.

01:23:38.280 --> 01:23:45.320
It's called intelligence and if you have more intelligence, we'll be doing things that

01:23:45.320 --> 01:23:49.200
are more beneficial to ourselves and other people.

01:23:49.200 --> 01:23:51.680
Do you think that the experiences that we're having right now?

01:23:51.680 --> 01:23:56.960
Like right now, we have much less crime than we did 50 years ago.

01:23:56.960 --> 01:24:04.120
Now, if you listen to people debating presidential politics, they'll say, crime is worse than

01:24:04.120 --> 01:24:14.880
it ever, but if you look at the actual statistics, it's gone way down and if you actually go

01:24:14.880 --> 01:24:24.440
back like a few hundred years, crime and murder and so on was far, far higher than it is today.

01:24:24.440 --> 01:24:27.000
It's actually pretty rare.

01:24:27.000 --> 01:24:34.360
So the kind of additional intelligence that we've created is actually good for people.

01:24:34.360 --> 01:24:36.560
If you look at the actual data.

01:24:36.560 --> 01:24:37.560
Sure.

01:24:37.560 --> 01:24:43.360
If you look at Stephen Pinker's work, scale it from 100 plus years ago to today, things

01:24:43.360 --> 01:24:47.320
are generally always seem to be moving in a better direction.

01:24:47.320 --> 01:24:48.320
Right.

01:24:48.320 --> 01:24:52.680
Well, Pinker didn't credit this to technology.

01:24:52.680 --> 01:24:58.160
He just looks at the data and says, it's gotten better.

01:24:58.160 --> 01:25:02.120
What I try to do in the current book is to show how it's related to technology and as

01:25:02.120 --> 01:25:05.800
we have more technology, we're actually moving in this direction.

01:25:05.800 --> 01:25:10.400
So you feel it's a function of technology that we're moving in this direction?

01:25:10.400 --> 01:25:11.400
Absolutely.

01:25:11.400 --> 01:25:15.000
I mean, that's why.

01:25:15.000 --> 01:25:16.760
I mean, look at the technology.

01:25:16.760 --> 01:25:24.480
In 80 years, we've multiplied the amount of computation 20 quadrillion times.

01:25:24.480 --> 01:25:29.160
And so we have things that didn't exist two years ago.

01:25:29.160 --> 01:25:30.520
Right.

01:25:30.520 --> 01:25:37.040
When you think about the idea of life on earth and that this is happening and that we are

01:25:37.040 --> 01:25:43.800
on this journey to 2045, to the singularity, do you consider whether or not this is happening

01:25:43.800 --> 01:25:47.360
elsewhere in the universe or whether it's already happened?

01:25:47.360 --> 01:25:56.240
Yeah, we see no evidence that there's any form of life, let alone intelligent life anywhere

01:25:56.240 --> 01:25:57.240
else.

01:25:57.240 --> 01:26:01.760
And I say, well, we're not in touch with these other people.

01:26:01.760 --> 01:26:13.680
It is possible, but it seems, I mean, given the exponential impact of this type of technology

01:26:13.680 --> 01:26:40.040
we would be spaced out based on, over a launch period of time, so some people that might

01:26:40.040 --> 01:26:46.360
be ahead of us, it could be ahead of us, certainly thousands of years, even millions

01:26:46.360 --> 01:26:47.360
of years.

01:26:47.360 --> 01:26:54.880
And so they'd be like way ahead of us and they'd be doing galaxy-wide engineering.

01:26:54.880 --> 01:26:59.000
How is it that we look out there and we don't see anybody doing galaxy-wide engineering?

01:26:59.000 --> 01:27:02.400
And maybe we don't have the capability to actually see it.

01:27:02.400 --> 01:27:09.080
I mean, the universe is, what's the 13.7 billion years old or whatever it is?

01:27:09.080 --> 01:27:18.640
But even just incidental capabilities would affect galaxies, we would see that somehow.

01:27:18.640 --> 01:27:25.440
Would we, if we were at the peak, if there is intelligent life in the universe, some

01:27:25.440 --> 01:27:29.600
form of that intelligent life has to be the most advanced?

01:27:29.600 --> 01:27:35.000
And what if we are underestimating our position in the universe, that we are the most advanced

01:27:35.000 --> 01:27:36.000
people?

01:27:36.000 --> 01:27:39.440
But maybe there's something that's like 10 years, maybe there's an industrial age.

01:27:39.440 --> 01:27:44.880
I think there's a good argument that we are ahead of other people.

01:27:44.880 --> 01:27:50.320
But we don't have the capability of observing the goings on of a planet 5,000 light years

01:27:50.320 --> 01:27:51.320
away.

01:27:51.320 --> 01:27:58.080
We can't see into their atmosphere, we can't look at high-resolution video of activity on

01:27:58.080 --> 01:27:59.080
that planet.

01:27:59.080 --> 01:28:02.240
But if they were doing galaxy-wide engineering, I think we would notice that.

01:28:02.240 --> 01:28:04.400
If they were more advanced than us, maybe we would.

01:28:04.400 --> 01:28:05.400
But what if they're not?

01:28:05.400 --> 01:28:07.200
What if they're at the level that we're at?

01:28:07.200 --> 01:28:08.960
Well, that's what I'm saying.

01:28:08.960 --> 01:28:09.960
What if we're at the peak?

01:28:09.960 --> 01:28:10.960
And this is like...

01:28:10.960 --> 01:28:13.920
I think it's an argument that we are at the peak.

01:28:13.920 --> 01:28:18.320
What if it gets to the point where artificial intelligence gets implemented and then that

01:28:18.320 --> 01:28:23.120
becomes the primary form of life and it doesn't have the desire to do anything in terms of

01:28:23.120 --> 01:28:28.480
like galactic engineering?

01:28:28.480 --> 01:28:34.840
But even just incidental things would affect whole galaxies.

01:28:34.840 --> 01:28:35.840
Like what?

01:28:35.840 --> 01:28:37.640
Things like we're doing, are we affecting the whole galaxy?

01:28:37.640 --> 01:28:38.640
No, not yet.

01:28:38.640 --> 01:28:39.640
Right.

01:28:39.640 --> 01:28:42.960
But what if it's like us, but it gets to the point where it becomes artificial intelligence

01:28:42.960 --> 01:28:46.840
and then it doesn't have emotions, it doesn't have desires, it doesn't have ambitions, so

01:28:46.840 --> 01:28:48.600
why would it decide to expand?

01:28:48.600 --> 01:28:50.000
Why would it not have those things?

01:28:50.000 --> 01:28:53.840
Well, we'd have to program it into it, but it would probably decide that that's foolish

01:28:53.840 --> 01:28:56.200
and that those things have caused all these problems.

01:28:56.200 --> 01:28:58.040
All the problems in human race.

01:28:58.080 --> 01:29:00.080
That's our number one issue, war.

01:29:00.080 --> 01:29:02.480
What is war caused by?

01:29:02.480 --> 01:29:09.200
It's caused by ideologies, it's caused by acquisition of resources, theft of resources, violence.

01:29:09.200 --> 01:29:14.080
War is not the primary thing that we are motivated by.

01:29:14.080 --> 01:29:18.960
It's not the primary thing we're motivated by, but it's existed in every single step

01:29:18.960 --> 01:29:21.880
of the way of human existence.

01:29:21.880 --> 01:29:23.600
But it's actually getting better.

01:29:23.600 --> 01:29:25.720
I mean, just look at the effect of war.

01:29:25.720 --> 01:29:26.720
Sure.

01:29:26.800 --> 01:29:30.640
We have a couple of wars going on, they're not killing millions of people like they used

01:29:30.640 --> 01:29:31.640
to.

01:29:31.640 --> 01:29:32.640
Right.

01:29:32.640 --> 01:29:33.640
Right.

01:29:33.640 --> 01:29:38.280
My point is that if artificial intelligence recognizes that the problem with human beings

01:29:38.280 --> 01:29:45.760
is these emotions and a lot of it is fueled by these desires, like the desire to expand,

01:29:45.760 --> 01:29:50.280
the desire to acquire things, the desire to achieve.

01:29:50.280 --> 01:29:53.720
Well, the emotion is positive, I mean, music and other things.

01:29:53.720 --> 01:29:55.040
To us.

01:29:55.040 --> 01:29:56.040
To us.

01:29:56.160 --> 01:30:03.160
If it gets to the point where artificial intelligence is no longer stimulated by mere human creations,

01:30:03.160 --> 01:30:08.560
creativity, all these different things, why would it even have the ambition to do any sort

01:30:08.560 --> 01:30:10.680
of galaxy-wide engineering?

01:30:10.680 --> 01:30:12.480
Why would it want to?

01:30:12.480 --> 01:30:17.120
Because it's based on us.

01:30:17.120 --> 01:30:19.920
It is based on us until it decides it's not based on us anymore.

01:30:19.920 --> 01:30:20.920
That's my point.

01:30:20.920 --> 01:30:26.480
We realize that if we're based on a very violent chimpanzee, and we say, you know what, there's

01:30:26.480 --> 01:30:31.480
a lot of what we are because of our genetics, that it really are a problem, and this is

01:30:31.480 --> 01:30:36.240
what's causing all of our violence, all of our crime, all of our war.

01:30:36.240 --> 01:30:42.480
If we just step in and put a stop to all that, will we also put a stop to our ambition?

01:30:42.480 --> 01:30:46.280
I would maintain that we're actually moving away from that.

01:30:46.280 --> 01:30:49.280
We are moving away from that, but that's just natural, right?

01:30:49.320 --> 01:30:54.000
That's natural with our understanding and our mitigations of these social problems.

01:30:54.000 --> 01:30:55.000
Right.

01:30:55.000 --> 01:30:58.320
So if we expand that even more, we'll be even more in that direction.

01:30:58.320 --> 01:30:59.960
As long as we're still we.

01:30:59.960 --> 01:31:04.360
But as soon as you become something different, why would it even have the desire to expand?

01:31:04.360 --> 01:31:09.760
If it was infinitely intelligent, why would it even want to physically go anywhere?

01:31:09.760 --> 01:31:11.160
Why would it want to?

01:31:11.160 --> 01:31:14.960
What's the reason for our motivation to expand?

01:31:14.960 --> 01:31:15.960
What is it?

01:31:15.960 --> 01:31:16.960
It's human.

01:31:17.040 --> 01:31:21.840
The same humans that were tribal creatures that roam, the same humans that stole resources

01:31:21.840 --> 01:31:23.120
from neighboring villages.

01:31:23.120 --> 01:31:24.400
This is our genes, right?

01:31:24.400 --> 01:31:27.400
This is what made us that got us to this point.

01:31:27.400 --> 01:31:32.920
If we create a sentient artificial intelligence that's far superior to us, and it can create

01:31:32.920 --> 01:31:36.920
its own version of artificial intelligence, the first thing it's going to engineer out

01:31:36.920 --> 01:31:40.920
is all these stupid emotions that get us in trouble.

01:31:40.920 --> 01:31:49.600
If it just can create happiness and joy from programming, why would it create happiness

01:31:49.600 --> 01:31:55.520
and joy through the acquisition of other people's creativity, art, music, all those things?

01:31:55.520 --> 01:31:59.240
And then why would it have any ambition at all to travel?

01:31:59.240 --> 01:32:01.000
Why would it want to go anywhere?

01:32:01.000 --> 01:32:04.200
Well, I mean, it's an interesting philosophical problem.

01:32:04.200 --> 01:32:05.200
Right.

01:32:05.200 --> 01:32:09.800
It is a problem because a lot of what we are and the things that we create is because of

01:32:09.800 --> 01:32:12.360
all these flaws that you would say.

01:32:12.360 --> 01:32:16.240
If you were programming us, you'd say, well, what is the cause of all these issues that

01:32:16.240 --> 01:32:17.240
plague the human race?

01:32:17.240 --> 01:32:18.880
I wouldn't necessarily say that there are flaws.

01:32:18.880 --> 01:32:19.880
Murder is a flaw.

01:32:19.880 --> 01:32:21.600
Isn't it a flaw?

01:32:21.600 --> 01:32:23.080
But that's way down.

01:32:23.080 --> 01:32:24.080
Right.

01:32:24.080 --> 01:32:26.720
But it's a technology that moves ahead.

01:32:26.720 --> 01:32:28.400
If it happens to you, it's a flaw.

01:32:28.400 --> 01:32:30.120
And crime is a flaw.

01:32:30.120 --> 01:32:32.960
All these theft is a fraud.

01:32:32.960 --> 01:32:34.040
Those are flaws.

01:32:34.040 --> 01:32:38.320
If we could engineer those out, what would be the way that we do it?

01:32:38.320 --> 01:32:41.560
Well, one of the things we do, we get rid of what it is to be a person.

01:32:41.560 --> 01:32:47.080
Because what it is is corrupt people that go down these terrible paths and cause harm

01:32:47.080 --> 01:32:48.080
to other people.

01:32:48.080 --> 01:32:49.080
Right?

01:32:49.080 --> 01:32:54.720
You're taking a step there that our ability to feel emotion and so on is a flaw.

01:32:54.720 --> 01:32:55.720
No, I'm not.

01:32:55.720 --> 01:32:58.560
I'm saying that it's the root of these flaws.

01:32:58.560 --> 01:33:03.000
That greed and envy and lust and anger are the root.

01:33:03.000 --> 01:33:05.520
I'd like to go to the bathroom.

01:33:05.520 --> 01:33:06.520
Yeah.

01:33:06.520 --> 01:33:07.520
Okay.

01:33:07.520 --> 01:33:08.520
We'll come back.

01:33:08.520 --> 01:33:09.520
We'll talk about flaws.

01:33:09.520 --> 01:33:10.520
And we're back.

01:33:10.520 --> 01:33:12.520
Provide an answer to that.

01:33:12.520 --> 01:33:24.800
I mean, as I think about myself now, it's when I have emotions that are positive emotions,

01:33:24.800 --> 01:33:31.880
like really getting off on a song or a picture or some new art form that didn't exist in

01:33:31.880 --> 01:33:34.480
the past.

01:33:34.480 --> 01:33:35.480
That's positive.

01:33:36.040 --> 01:33:47.120
That's what I live for, relating to another person in a way that's intimate.

01:33:47.120 --> 01:33:53.680
So I mean, the idea, if we're actually more intelligent, we'd not to get rid of that,

01:33:53.680 --> 01:33:59.080
but to actually enjoy that to a greater extent.

01:33:59.080 --> 01:34:00.560
Hopefully.

01:34:00.560 --> 01:34:03.120
But what I'm saying is that...

01:34:03.320 --> 01:34:09.520
Yes, there are things that can go wrong, but lead us in the incorrect direction.

01:34:09.520 --> 01:34:12.400
I'm not even saying it's wrong.

01:34:12.400 --> 01:34:15.080
I'm not saying that it's going to go wrong.

01:34:15.080 --> 01:34:22.240
I'm saying that if you wanted to program away some of the issues that human beings have

01:34:22.240 --> 01:34:29.000
in terms of what keeps us from working with each other universally, all over the globe,

01:34:29.000 --> 01:34:30.440
what keeps us from these things?

01:34:30.560 --> 01:34:33.160
We're actually doing that more than we used to do.

01:34:33.160 --> 01:34:35.080
Sure, but also not.

01:34:35.080 --> 01:34:36.760
We're also massive inequality.

01:34:36.760 --> 01:34:41.280
You've got people in the Congo mining cobalt with sticks that powers your cell phones.

01:34:41.280 --> 01:34:43.760
There's a lot of real problems with society today.

01:34:43.760 --> 01:34:45.880
But there used to be even more of that.

01:34:45.880 --> 01:34:46.960
There's a lot of that, though.

01:34:46.960 --> 01:34:48.760
There's a lot of that.

01:34:48.760 --> 01:34:54.600
If you looked at greed and war and crime and all the problems with human beings, a lot

01:34:54.600 --> 01:35:00.240
of it has to do with these biological instincts, these instincts to control things.

01:35:00.800 --> 01:35:06.400
Built-in genetic codes that we have that are from our ancestors.

01:35:06.400 --> 01:35:09.120
That's because we haven't gotten there yet.

01:35:09.120 --> 01:35:09.800
Right.

01:35:09.800 --> 01:35:18.320
But when we get there, you think we will be a better version of a human being and we will

01:35:18.320 --> 01:35:24.800
be able to experience all the good, the positive aspects of being a human being, the art and

01:35:24.800 --> 01:35:26.400
creativity and all these different things.

01:35:26.400 --> 01:35:27.720
Yeah, I hope so.

01:35:27.720 --> 01:35:35.320
And actually, if you look at what human beings have done already, we're moving in that direction.

01:35:35.320 --> 01:35:36.320
Right.

01:35:36.320 --> 01:35:38.880
I may not seem that way.

01:35:38.880 --> 01:35:40.840
No, it does seem that way to me.

01:35:40.840 --> 01:35:42.320
It does overall.

01:35:42.320 --> 01:35:47.160
But it's also like, if you look at a graph of temperatures, it goes up and it goes down

01:35:47.160 --> 01:35:48.160
and it goes up and it goes down.

01:35:48.160 --> 01:35:50.480
But it's moving in a general direction.

01:35:50.480 --> 01:35:53.080
We are moving in a generally positive direction.

01:35:53.080 --> 01:35:57.280
However, we want to continue moving in this same direction.

01:35:57.280 --> 01:35:59.000
Yeah, I don't think the word...

01:35:59.000 --> 01:36:01.200
It's not a guarantee.

01:36:01.200 --> 01:36:07.360
You can describe things that would be horrible and it's feasible.

01:36:07.360 --> 01:36:09.280
Yeah.

01:36:09.280 --> 01:36:13.040
It could be the end of the human race, right?

01:36:13.040 --> 01:36:15.960
Or it could be the beginning of the next race of this new thing.

01:36:16.480 --> 01:36:23.600
Well, I mean, when I was born, we created nuclear weapons and people were concerned...

01:36:23.600 --> 01:36:31.480
Very soon we had hydrogen weapons and we have enough hydrogen weapons to wipe out all humanity.

01:36:31.480 --> 01:36:34.040
We still have that.

01:36:34.040 --> 01:36:42.440
That didn't exist like a hundred years ago, well, it didn't exist 80 years ago.

01:36:42.440 --> 01:36:48.960
So that is something that concerns me.

01:36:48.960 --> 01:36:51.640
And you could do the same thing with the artificial intelligence.

01:36:51.640 --> 01:36:55.080
It could also create something that would be very negative.

01:36:55.080 --> 01:37:01.320
But what I'm getting at is like, what do you think life looks like if it's engineered?

01:37:01.320 --> 01:37:06.960
What do you think human life looks like if it's engineered by a far superior intelligence?

01:37:06.960 --> 01:37:10.400
And what would it change about what it means to be a person?

01:37:10.400 --> 01:37:19.640
I mean, first of all, we would base it on what human beings are already.

01:37:19.640 --> 01:37:24.360
So we'd become better versions of ourselves.

01:37:24.360 --> 01:37:31.440
For example, we'd be able to overcome life-threatening diseases.

01:37:31.440 --> 01:37:32.760
And we're actually working on that.

01:37:32.760 --> 01:37:36.720
And that's going to go into high gear very soon.

01:37:36.720 --> 01:37:42.760
Yes, but that's still being a human being.

01:37:42.760 --> 01:37:52.920
If you're implementing large-scale artificial intelligence, you're essentially a superhuman.

01:37:52.920 --> 01:37:54.040
You're a different thing.

01:37:54.040 --> 01:37:56.880
You're not what we are.

01:37:56.880 --> 01:37:57.880
If you have the computational power...

01:37:57.880 --> 01:38:02.120
Well, if you're superhuman, you have the human being as part of it.

01:38:03.040 --> 01:38:04.040
But this is the thing.

01:38:04.040 --> 01:38:09.080
If you're engineering this artificial intelligence and you're engineering this with essentially

01:38:09.080 --> 01:38:14.480
like a superior life form, it's going to look at it logically.

01:38:14.480 --> 01:38:20.160
It's going to look at the issues that human beings have logically and say, well, we don't

01:38:20.160 --> 01:38:21.160
need this.

01:38:21.160 --> 01:38:22.160
This is a problem.

01:38:22.160 --> 01:38:25.040
This is what we needed when we were primates, and we're not that anymore.

01:38:25.040 --> 01:38:26.040
This new thing.

01:38:26.040 --> 01:38:27.040
We're going to...

01:38:27.040 --> 01:38:28.520
Who cares what the movie's like?

01:38:28.520 --> 01:38:32.800
It's just a thing that's tricking your body into pretending that it's involved in drama,

01:38:32.800 --> 01:38:33.800
but it's not really...

01:38:33.800 --> 01:38:37.760
Well, you're making certain assumptions about what we'll create.

01:38:37.760 --> 01:38:41.720
No, I'm just making an assumption.

01:38:41.720 --> 01:38:50.360
I mean, in my mind, we would want to create better music and better art and better relationships.

01:38:50.360 --> 01:38:56.320
Well, the relationships should be all perfect eventually if we keep going in this general

01:38:56.320 --> 01:38:57.320
direction.

01:38:57.320 --> 01:38:58.320
Yeah, perfect.

01:38:58.320 --> 01:38:59.320
I mean...

01:38:59.320 --> 01:39:02.840
But if you get artificial intelligence, we're all reading each other's minds and everyone's

01:39:02.840 --> 01:39:04.320
working towards the same goal.

01:39:04.320 --> 01:39:06.320
Well, no, you can't read each other's minds.

01:39:06.320 --> 01:39:07.320
Ever?

01:39:07.320 --> 01:39:08.320
I mean, we can create...

01:39:08.320 --> 01:39:14.320
Yes, we can create privacy that's virtually unbreakable, and you could keep the privacy

01:39:14.320 --> 01:39:15.320
to yourselves.

01:39:15.320 --> 01:39:17.600
But can you do that as technology scales upward?

01:39:17.600 --> 01:39:21.720
If it continues to move, I mean, it's difficult like your phone.

01:39:21.720 --> 01:39:23.200
Anyone can listen to you on your phone.

01:39:23.200 --> 01:39:25.400
I mean, anyone who has a significant technology...

01:39:25.400 --> 01:39:28.560
Actually, it has pretty good technology already.

01:39:28.560 --> 01:39:31.240
You can't really read someone else's phone.

01:39:31.240 --> 01:39:32.240
You're definitely good.

01:39:32.240 --> 01:39:37.040
Yeah, if you have Pegasus, you could hack into your phone easily, not hard at all.

01:39:37.040 --> 01:39:39.840
The new software that they have, all they need is your phone number.

01:39:39.840 --> 01:39:43.440
All they need is your phone number, and they can look at every text message you send, every

01:39:43.440 --> 01:39:48.400
email you send, they can look at your camera, they can turn on your microphone, easy.

01:39:48.400 --> 01:39:53.160
We have ways of keeping total privacy, and if it's not built into your phone now, it

01:39:53.160 --> 01:39:54.160
will be.

01:39:54.320 --> 01:39:56.960
But it's definitely not built into your phone now.

01:39:56.960 --> 01:40:02.560
The security people that really understand the capabilities of intelligence agencies,

01:40:02.560 --> 01:40:04.640
they 100% can listen to your phone.

01:40:04.640 --> 01:40:06.360
100% can turn on your camera.

01:40:06.360 --> 01:40:08.760
100% can record your voice.

01:40:08.760 --> 01:40:10.840
Yes and no.

01:40:10.840 --> 01:40:17.560
I mean, we have an ability to keep total privacy in a device.

01:40:17.560 --> 01:40:19.040
But from who?

01:40:19.040 --> 01:40:22.480
You can keep privacy from me, because I don't have access to your device.

01:40:22.480 --> 01:40:26.680
But if I was working for an intelligence agency and I had access to a Pegasus program,

01:40:26.680 --> 01:40:28.600
I am in your device.

01:40:28.600 --> 01:40:31.320
Now, I've talked to people.

01:40:31.320 --> 01:40:32.760
Only because it's not perfect.

01:40:32.760 --> 01:40:37.480
We can actually build much better privacy than exists today.

01:40:37.480 --> 01:40:41.640
But the privacy that we have today is far less than the privacy that we had before we

01:40:41.640 --> 01:40:42.640
had phones.

01:40:42.640 --> 01:40:47.360
I don't really quite agree with that.

01:40:47.360 --> 01:40:49.560
How so?

01:40:49.560 --> 01:40:54.280
If you didn't have a phone and you were at home having a conversation, a sensitive conversation

01:40:54.280 --> 01:40:57.920
about maybe you didn't pay as much taxes as you should, there's no way anybody would

01:40:57.920 --> 01:40:59.200
hear that.

01:40:59.200 --> 01:41:00.520
But now your phone hears that.

01:41:00.520 --> 01:41:04.120
If you have an Alexa in your home, your Alexa hears you say that.

01:41:04.120 --> 01:41:12.640
People have been charged with crimes because Alexa heard them committing murder.

01:41:12.640 --> 01:41:18.480
We actually know how to create perfect privacy in your phone.

01:41:18.480 --> 01:41:23.200
If your phone doesn't have that, that's just an imperfection in the way we're building

01:41:23.200 --> 01:41:24.400
these things now.

01:41:24.400 --> 01:41:25.680
But it's not just an imperfection.

01:41:25.680 --> 01:41:30.040
It's sort of built into the program itself, because that's what fuels the algorithm, is

01:41:30.040 --> 01:41:32.480
that it has access to all of your data.

01:41:32.480 --> 01:41:37.320
It has access to all of what you're interested in, what you like, what you don't like, you

01:41:37.320 --> 01:41:38.320
can opt out of it.

01:41:38.320 --> 01:41:39.320
Especially you.

01:41:39.320 --> 01:41:40.320
You've got a Google phone.

01:41:40.320 --> 01:41:45.520
It's just a net scooping up information.

01:41:45.520 --> 01:41:50.720
We know how to build perfect privacy.

01:41:50.720 --> 01:42:02.920
How do we do it?

01:42:02.920 --> 01:42:08.000
I mean if it's not built into your phone now, it should be.

01:42:08.000 --> 01:42:10.960
Because they don't want it to be built in there, because there's an actual business

01:42:10.960 --> 01:42:13.160
model and it not being built in there.

01:42:13.160 --> 01:42:20.920
Okay, but it can be done, and if people want that, it'll happen.

01:42:20.920 --> 01:42:23.840
But you recognize the financial incentive in not doing that, right?

01:42:23.840 --> 01:42:28.120
Because that's what a company like Google, for instance, that's where they make the majority

01:42:28.120 --> 01:42:34.560
of their money is from data, or a lot of their money, I should say.

01:42:34.560 --> 01:42:45.960
There's actually a lot of effort that goes into keeping what's on your phone private.

01:42:45.960 --> 01:42:46.960
It's not that easy.

01:42:46.960 --> 01:42:51.320
Private from some people, but not really private.

01:42:51.320 --> 01:42:53.800
It's only private until they want to listen.

01:42:53.800 --> 01:42:58.600
And now the capability of listening to your phone is super easy.

01:42:58.600 --> 01:42:59.600
Not really.

01:42:59.600 --> 01:43:00.600
No?

01:43:00.600 --> 01:43:02.160
With the Pegasus program?

01:43:02.160 --> 01:43:04.280
It's very easy.

01:43:04.280 --> 01:43:08.000
Well that has to do with imperfections in the way phones are created.

01:43:08.000 --> 01:43:10.320
Right, but I think it's a feature.

01:43:10.320 --> 01:43:15.520
I think part of the feature is that they want as much data from you, and knowing about what

01:43:15.520 --> 01:43:16.840
you're doing, what you're talking about.

01:43:16.840 --> 01:43:25.120
Have you ever had a conversation with someone and you see an ad for that thing on Google?

01:43:25.120 --> 01:43:26.120
That happens.

01:43:26.120 --> 01:43:27.800
Yes, but...

01:43:27.800 --> 01:43:32.840
So something's going on where it's listening to your conversations.

01:43:33.120 --> 01:43:34.680
Picking up on keywords.

01:43:34.680 --> 01:43:36.840
It's not picking up on everything.

01:43:36.840 --> 01:43:37.840
Not yet.

01:43:37.840 --> 01:43:38.840
Well, it's not unless it wants to.

01:43:38.840 --> 01:43:43.180
Like I said, if they're using a program, an intelligence program, to gather information

01:43:43.180 --> 01:43:44.880
from your phone, it is.

01:43:44.880 --> 01:43:45.880
And you're basically...

01:43:45.880 --> 01:43:50.040
You got a little spy that you carry around with you everywhere you go.

01:43:50.040 --> 01:43:51.040
Unless you're using...

01:43:51.040 --> 01:43:57.280
I mean, if you think that's a major issue, we could build phones that are impossible

01:43:57.280 --> 01:44:01.240
to spy on.

01:44:01.240 --> 01:44:02.760
Maybe.

01:44:03.680 --> 01:44:07.520
Well, there are some phones that like graphing, do you know about that?

01:44:07.520 --> 01:44:12.600
Do you know about people that they take a Google phone and they put a different Linux-based

01:44:12.600 --> 01:44:16.400
operating system on it, makes it much more difficult to track, and there's multi-levels

01:44:16.400 --> 01:44:17.400
of protection.

01:44:17.400 --> 01:44:22.000
There's a bunch of phones that are being made that are security phones.

01:44:22.000 --> 01:44:24.240
But you lose access to apps.

01:44:24.240 --> 01:44:28.720
You lose access to a lot of the features that people rely on when it comes to phones.

01:44:28.720 --> 01:44:32.560
For instance, like if you have GPS on your phone, as soon as you're using GPS, you're

01:44:32.560 --> 01:44:33.560
easy to find.

01:44:33.560 --> 01:44:34.560
Right?

01:44:34.560 --> 01:44:35.560
So you lose that privacy.

01:44:35.560 --> 01:44:39.480
If they want to know where Ray's phone is, they know exactly where Ray's phone is.

01:44:39.480 --> 01:44:43.640
And that's where you are, and you're with your phone, they've got you tracked everywhere

01:44:43.640 --> 01:44:44.640
you go.

01:44:44.640 --> 01:44:45.640
It's complicated.

01:44:45.640 --> 01:44:49.400
If this were a major issue, we could definitely overcome that.

01:44:49.400 --> 01:44:54.920
I think it's a major issue, but I don't think it's a major concern for most people.

01:44:54.920 --> 01:44:57.480
But it's because they reap the benefits of it.

01:44:57.480 --> 01:45:00.440
Like the algorithm is specifically tailored to their interests.

01:45:00.440 --> 01:45:04.200
That's how we find the kinds of things we put on phones.

01:45:04.200 --> 01:45:05.200
Right.

01:45:05.200 --> 01:45:08.960
But you can't opt out of it unless you just decide to get a flip phone.

01:45:08.960 --> 01:45:13.000
But even if you do, they figure out where you are to triangulate you from cell phone

01:45:13.000 --> 01:45:17.200
towers.

01:45:17.200 --> 01:45:23.080
I mean, we give up certain things in order to get the benefits of phones.

01:45:23.080 --> 01:45:27.240
Yeah, we do.

01:45:27.240 --> 01:45:32.320
If what you're giving up is a grave concern, we could overcome that.

01:45:32.320 --> 01:45:35.240
We know how to do that.

01:45:35.240 --> 01:45:37.800
Yeah.

01:45:37.800 --> 01:45:44.520
If people agree that the benefit of overcoming that outweighs the loss in the financial loss

01:45:44.520 --> 01:45:47.800
that you would have with not having access to everybody's data and information.

01:45:47.800 --> 01:45:53.680
Well, I mean, what you're giving up is a certain type of data that you want, a certain

01:45:53.680 --> 01:46:04.200
type of capability that you could buy, so they can advertise that to you and people

01:46:04.200 --> 01:46:10.120
feel that that's okay.

01:46:10.120 --> 01:46:18.000
But for example, keeping your email private is quite feasible.

01:46:18.000 --> 01:46:21.680
It's possible, but it's also easy to hack.

01:46:21.680 --> 01:46:23.360
People could be reading your emails all the time.

01:46:23.360 --> 01:46:28.040
We should probably assume that they do.

01:46:28.040 --> 01:46:40.920
Well, it's a complicated issue, but we keep, for example, your emails private, and generally

01:46:40.920 --> 01:46:43.480
we actually do do that.

01:46:43.480 --> 01:46:45.360
Generally, for most people.

01:46:45.360 --> 01:46:51.320
But my point is, as this technology scales upward, when you have greater and greater

01:46:51.360 --> 01:46:57.880
computational power, and then you're also integrated with this technology, how does that

01:46:57.880 --> 01:47:06.360
keep whatever group is in charge from being able to essentially access the thing that

01:47:06.360 --> 01:47:09.920
is inside your head now?

01:47:09.920 --> 01:47:16.400
If you have a technology that's going to be upgraded and you're going to get new software

01:47:16.480 --> 01:47:22.840
that's going to keep improving as time goes on, what kind of privacy would be involved

01:47:22.840 --> 01:47:26.040
in that if you're literally having something that can get into your brain?

01:47:26.040 --> 01:47:30.280
If most people can't get into your brain, can intelligence agencies get into your brain?

01:47:30.280 --> 01:47:33.840
Can foreign governments get into your brain?

01:47:33.840 --> 01:47:35.680
What does that look like?

01:47:35.680 --> 01:47:37.160
I'm not looking at this as a negative.

01:47:37.160 --> 01:47:42.720
I'm just saying, if you're just looking at this completely objectively, what are the

01:47:42.720 --> 01:47:45.240
possibilities that this could look like?

01:47:45.280 --> 01:47:48.960
I'm trying to paint a weird picture of what this could look like.

01:47:48.960 --> 01:47:52.760
Well, a lot of things you want to share.

01:47:52.760 --> 01:47:59.760
Music and so on, it's desirable to share that, and you'd want that to be shared.

01:47:59.760 --> 01:48:03.160
If you didn't share anything, you'd be pretty lonely.

01:48:03.160 --> 01:48:04.160
Sure.

01:48:04.160 --> 01:48:09.280
What do you think about the potential for a universal language?

01:48:10.200 --> 01:48:17.200
One of the things that holds people back is the Rosetta Stone, the Tower of Battle.

01:48:17.200 --> 01:48:20.640
The idea that we can't really understand what all these other people are saying.

01:48:20.640 --> 01:48:21.640
We don't know how they think.

01:48:21.640 --> 01:48:28.160
If we can develop a universal worldwide language through this, do you think it's feasible?

01:48:28.160 --> 01:48:30.800
All languages that we have were created.

01:48:30.800 --> 01:48:34.560
We have a certain means of changing one language into another.

01:48:34.560 --> 01:48:35.560
Right.

01:48:35.560 --> 01:48:36.560
That's what I'm saying.

01:48:36.720 --> 01:48:40.640
We're doing that now with some, like Google does that with Translate, and the new Samsung

01:48:40.640 --> 01:48:43.040
phones do that in real time.

01:48:43.040 --> 01:48:44.040
Yeah.

01:48:44.040 --> 01:48:52.040
I wrote about that in 1989, that we'd be able to have universal translation between languages.

01:48:52.720 --> 01:48:56.200
But do you think that the adoption of a universal language?

01:48:56.200 --> 01:48:58.000
It's not perfect, but it's actually pretty good.

01:48:58.000 --> 01:48:59.080
It's pretty good.

01:48:59.080 --> 01:49:05.960
But there's also context that's missing, because there's different cultural significance.

01:49:06.000 --> 01:49:08.960
There's different ways that people say things.

01:49:08.960 --> 01:49:14.040
There's gendered language and other nationalities used and other countries used.

01:49:14.040 --> 01:49:17.680
Well, you could try to get that into the language translation as well.

01:49:17.680 --> 01:49:19.400
You can, but it's a little bit imperfect, right?

01:49:19.400 --> 01:49:20.840
This is what I'm saying.

01:49:20.840 --> 01:49:25.520
You might have something that's said very quickly, and you'd have to translate it into

01:49:25.520 --> 01:49:30.000
much longer language in order to capture that.

01:49:30.000 --> 01:49:35.480
But would a universal language be possible?

01:49:35.480 --> 01:49:37.960
If you're creating something...

01:49:37.960 --> 01:49:39.320
Why would you need that?

01:49:39.320 --> 01:49:45.400
Because what we have, all of our language, is pretty flawed, ultimately.

01:49:45.400 --> 01:49:49.200
We use it, but how many versions of your do we have?

01:49:49.200 --> 01:49:53.480
There's a bunch of different weird things about language that's imperfect, because it's

01:49:53.480 --> 01:49:54.480
old.

01:49:54.480 --> 01:49:56.160
It's like old technology.

01:49:56.160 --> 01:50:03.560
If we decided to make a better version of language through artificial technology and

01:50:03.560 --> 01:50:08.240
say, listen, instead of trying to translate everything, now that we're super powerful

01:50:08.240 --> 01:50:13.120
intelligent beings that are enhanced by artificial intelligence, let's create a better, more

01:50:13.120 --> 01:50:16.720
superior, universally adopted language.

01:50:16.720 --> 01:50:17.720
Maybe.

01:50:17.720 --> 01:50:21.920
Do you see that as a major need?

01:50:21.920 --> 01:50:22.920
Yeah, I do.

01:50:22.920 --> 01:50:23.920
Yeah.

01:50:23.920 --> 01:50:24.920
I think that would change a lot.

01:50:24.920 --> 01:50:30.880
I mean, we'd lose all the amazing nuances of cultures, which I don't think is good for

01:50:30.880 --> 01:50:34.280
us as human beings, but we're not going to be human beings.

01:50:34.280 --> 01:50:39.960
So maybe it would be better if we could communicate exactly the way we prefer to.

01:50:39.960 --> 01:50:41.720
Well, we would be human beings.

01:50:41.720 --> 01:50:48.000
And in my mind, the human being is someone who can change both ourselves and means of

01:50:48.000 --> 01:51:01.760
communication to enjoy better means of expressing art and culture and so on.

01:51:01.760 --> 01:51:05.760
No other animal really quite does that, except human beings.

01:51:05.760 --> 01:51:10.840
So that is an essence of what it means to be a human being.

01:51:10.840 --> 01:51:11.840
For now.

01:51:11.840 --> 01:51:16.000
But when you're a mind reading eagle and you're flying around, are you really a human being

01:51:16.000 --> 01:51:17.000
anymore?

01:51:17.000 --> 01:51:20.640
Yes, because we are able to change ourselves.

01:51:20.640 --> 01:51:24.080
So that's just a new definition of what a human being is.

01:51:24.080 --> 01:51:30.800
What are your thoughts on simulation theory?

01:51:30.800 --> 01:51:40.440
If you mean that we're living in a simulation, well, first of all, some people believe that

01:51:40.440 --> 01:52:04.360
we can express physics as formulas and that the universe is actually able to, is capable

01:52:04.360 --> 01:52:12.800
of computation and therefore everything that happens is the result of some computation.

01:52:12.800 --> 01:52:25.680
And therefore, the universe is capable of, we are living in something that is computable.

01:52:25.680 --> 01:52:31.800
And there's some debate about whether that's feasible, but that doesn't necessarily mean

01:52:32.720 --> 01:52:34.520
that we're living in a simulation.

01:52:34.520 --> 01:52:42.760
Generally, if you say we're living in a simulation, you assume that some other place and teenagers

01:52:42.760 --> 01:52:47.880
in that world like to create a simulation.

01:52:47.880 --> 01:52:53.400
So they created a simulation that we live in and you want to make sure that they don't

01:52:53.400 --> 01:52:58.680
turn the simulation off, so we'd have to be interesting to them, and so they keep the

01:52:58.680 --> 01:53:02.960
simulation going.

01:53:02.960 --> 01:53:13.520
But the whole universe could be capable of simulating reality and that's what we live

01:53:13.520 --> 01:53:20.520
in and it's not a game, it's just the way the universe works.

01:53:20.520 --> 01:53:25.400
I mean, what would the difference be if we lived in a simulation?

01:53:26.120 --> 01:53:32.360
This is what I'm saying, if we can and we're on our way to creating something that is indiscernible

01:53:32.360 --> 01:53:37.520
from reality itself, I don't think we're that far away from that, many decades away from

01:53:37.520 --> 01:53:43.440
having some sort of a virtual experience that's indiscernible from regular reality.

01:53:43.440 --> 01:53:46.960
We try to do that with games and so on.

01:53:46.960 --> 01:53:52.720
And those are far superior to what they were, I mean, I'm younger than you, but I can remember

01:53:53.640 --> 01:53:56.000
Pong, it was groundbreaking.

01:53:56.000 --> 01:53:59.680
You could play a video game on your television, this is crazy, it was so nuts.

01:53:59.680 --> 01:54:01.480
And we're way beyond that now.

01:54:01.480 --> 01:54:07.520
Now you look at the Unreal 5 engine, it's insane how beautiful it is and how incredible

01:54:07.520 --> 01:54:08.800
and what the capabilities are.

01:54:08.800 --> 01:54:12.480
So if you live in that, that's kind of a simulation that we live in.

01:54:12.480 --> 01:54:16.400
Right, but as you expand that further and you get to the point where you're actually

01:54:16.400 --> 01:54:23.320
in a simulation and that your life is not this carbon based biological life, feeling

01:54:23.320 --> 01:54:28.160
and texture that you think it is, whether you're really a part of this thing that's

01:54:28.160 --> 01:54:33.040
been created, this is where it gets real weird with like probability theory, right?

01:54:33.040 --> 01:54:40.040
Because they think that if a simulation is possible, it's more likely it's already happened.

01:54:40.960 --> 01:54:47.960
I mean, there's really an unlimited amount of things that we could simulate and experience

01:54:47.960 --> 01:54:54.960
and so it's hard to say we're living in a simulation because a lot of what we're doing

01:54:54.960 --> 01:55:03.960
is it's living in a computational world anyway, so it's basically being simulated in a way.

01:55:04.960 --> 01:55:11.960
And if you were some sort of an alien life form, wouldn't that be the way you go, instead

01:55:12.400 --> 01:55:19.400
of like taking physical metal crafts and shooting them off into space, wouldn't you sort of

01:55:20.760 --> 01:55:27.240
create artificial space, create artificial worlds, create something that exists in the

01:55:27.240 --> 01:55:31.840
sense that you experience it and it's indiscernible to the person experiencing it.

01:55:31.840 --> 01:55:36.320
So if you're intelligent enough, you'll be able to tell what's being simulated and what's

01:55:36.320 --> 01:55:37.320
not.

01:55:37.320 --> 01:55:43.320
Up to a point, until it actually does all the same things that regular reality does,

01:55:43.320 --> 01:55:49.440
it just does it through technology and maybe that's what the universe is.

01:55:49.440 --> 01:55:56.440
But that's okay, we could still experience what's happening and we could also experience

01:55:57.200 --> 01:56:04.200
people doing galaxy-wide engineering, which not all of which would be simulated.

01:56:04.400 --> 01:56:09.360
So the galaxy-wide engineering is the main thing that you look at to the point where

01:56:09.360 --> 01:56:14.000
I don't see any evidence for life outside. Well, there's definitely no real evidence

01:56:14.000 --> 01:56:19.560
that we see other than these people that talk about UFOs, UAPs and pilots and all these

01:56:19.560 --> 01:56:21.000
people that say that there's these things.

01:56:21.080 --> 01:56:30.080
We don't see any evidence that life is simulated outside of our own life. We can simulate things

01:56:30.160 --> 01:56:37.160
and experience it. We don't see any evidence that other beings are doing that elsewhere.

01:56:39.080 --> 01:56:41.480
This is based on such limited data though, right?

01:56:41.480 --> 01:56:47.480
I mean, look at what limited data we just have of Mars, rover rolling around, satellites

01:56:47.480 --> 01:56:53.040
in orbit. It's very limited data with something that's just one planet over. We don't really

01:56:53.040 --> 01:56:56.040
have the data to understand what's going on in Alpha Centauri.

01:56:56.040 --> 01:57:03.040
It's possible that this simulated life elsewhere. I mean, we don't see any evidence for it,

01:57:04.920 --> 01:57:06.600
but it's possible.

01:57:06.600 --> 01:57:09.640
Is it something that intrigues you or do you just look at it like there's no evidence

01:57:09.640 --> 01:57:11.840
so I'm not going to concentrate on that?

01:57:12.800 --> 01:57:18.280
I'm very interested to see what we can achieve because we're actually, I can see that we're

01:57:18.280 --> 01:57:25.280
on that path. So it doesn't take a lot of curiosity in my part to imagine other people

01:57:31.440 --> 01:57:38.440
simulating life and enjoying it. I'm much more interested to see what will be feasible

01:57:39.280 --> 01:57:44.760
for us and we're not that far away from it.

01:57:44.760 --> 01:57:51.260
So over the next four years, five years, you think we're going to be able to far surpass

01:57:51.260 --> 01:57:57.660
the ability of human beings. We're going to be able to stop aging and then eventually

01:57:57.660 --> 01:58:04.660
reverse aging, and then 2045 comes along. What does that look like?

01:58:05.180 --> 01:58:11.780
Well, one of the reasons we call it singularity is because we really don't know. I mean, that's

01:58:11.780 --> 01:58:18.780
why it's called a singularity. Singularity in physics is where you have a black hole,

01:58:20.420 --> 01:58:25.060
no energy can get out of a black hole, and therefore we don't really know what's going

01:58:25.060 --> 01:58:30.940
on in it and we call it a singularity. So this is a historical singularity based on

01:58:30.940 --> 01:58:36.460
the kinds of things we've been talking about. And again, we don't really know what that

01:58:36.460 --> 01:58:41.460
will be like and that's why we call it a singularity.

01:58:41.460 --> 01:58:44.460
Do you have any theories?

01:58:44.460 --> 01:58:51.460
Another way of looking at it, I mean, we have mice and they have experiences. It's a limited

01:58:51.460 --> 01:58:58.460
amount of complexity because that particular species hasn't really evolved very much. And

01:59:00.860 --> 01:59:07.860
we'll be going beyond what human beings can do. So to ask a human being what it's like

01:59:08.060 --> 01:59:15.060
to be a human being, what it's like to be a human being, what it's like to be a human

01:59:21.600 --> 01:59:27.180
being in singularity, it's like asking a mouse, what would it be like if you were to

01:59:27.180 --> 01:59:33.940
evolve to become like a human? Now, if you ask a mouse that, it wouldn't understand

01:59:33.940 --> 01:59:39.380
the question, it wouldn't be able to formulate an answer, it wouldn't even be able to think

01:59:39.380 --> 01:59:46.380
about it. And asking a card human being what it's going to be like to live in a singularity

01:59:46.740 --> 01:59:52.300
is a little bit like that.

01:59:52.300 --> 01:59:57.180
So it's just who knows? It's going to be wild.

01:59:57.180 --> 02:00:00.860
Be able to do things that we can't even imagine today, right?

02:00:00.860 --> 02:00:07.340
Well, I'm very excited about it. Even though it's scary, I know I ask a lot of tough questions

02:00:07.340 --> 02:00:11.020
about this because these are my own questions. This is like what bounces around inside my

02:00:11.020 --> 02:00:12.020
own head.

02:00:12.020 --> 02:00:18.620
Well, that's why I'm excited about it also because it basically means more intelligence

02:00:18.620 --> 02:00:22.980
and we'll be able to think about things that we can't even imagine today.

02:00:22.980 --> 02:00:24.660
And solve problems.

02:00:24.660 --> 02:00:29.660
Yes, including like dying, for example.

02:00:29.660 --> 02:00:35.660
Well, listen, man, I'm glad you're out there. It's very important that people have access

02:00:35.660 --> 02:00:40.020
to this kind of thinking and you've dedicated your whole life to this. In this book, Ray

02:00:40.020 --> 02:00:44.380
Kurzweil, the Singularity is Near When We Merge with AI. It's available now. Did you

02:00:44.380 --> 02:00:47.940
do the audio version of it?

02:00:47.940 --> 02:00:49.500
That's being worked on now.

02:00:49.500 --> 02:00:50.500
Are you doing it?

02:00:50.500 --> 02:00:53.500
It's coming out June.

02:00:53.500 --> 02:00:54.500
No.

02:00:54.500 --> 02:00:59.500
No? I want to hear it in your voice. It's your words.

02:00:59.500 --> 02:01:01.500
Yeah, that's what people say.

02:01:01.500 --> 02:01:05.140
Yeah, why don't you do it? You should do it. You know what you should do? Just get AI to

02:01:05.140 --> 02:01:09.500
do it. Why waste all that time sitting around doing it? Basically, they could do it now.

02:01:09.980 --> 02:01:10.980
They did that yesterday.

02:01:10.980 --> 02:01:16.500
100%. Look, and they could take your voice from this podcast and do this book in an

02:01:16.500 --> 02:01:22.540
audio version. Easy. Do you know what they're doing now? It's Spotify. They're translating

02:01:22.540 --> 02:01:28.580
this podcast. They're going to translate it to German, French, and Spanish. And it's

02:01:28.580 --> 02:01:32.900
going to be like your voice in perfect Spanish, my voice in perfect Spanish.

02:01:32.900 --> 02:01:36.220
This actually came up yesterday. I'll think about that.

02:01:36.220 --> 02:01:37.220
Pretty wild.

02:01:37.220 --> 02:01:38.220
Yeah.

02:01:38.220 --> 02:01:39.220
It's 100%. You should do that.

02:01:39.420 --> 02:01:43.620
My friend Duncan does that all the time. He'll have friends, text friends, or send a voice

02:01:43.620 --> 02:01:47.980
message as a fake voice message. That's ridiculous. You know, talking about how he's marrying his

02:01:47.980 --> 02:01:52.820
cat or something like that. It's just like, just, but he does it with AI and it sounds

02:01:52.820 --> 02:01:55.980
exactly like whoever that person is.

02:01:55.980 --> 02:02:00.940
So that's the, that's the solution. Have AI read your, of course you should have AI read

02:02:00.940 --> 02:02:06.940
your book. I can't believe we even would think of you sitting down for 40 hours or whatever

02:02:07.060 --> 02:02:12.140
it would take. It'd probably take more than that to read this whole book. And then if

02:02:12.140 --> 02:02:14.140
you mess up, you got to go back and start again.

02:02:14.140 --> 02:02:18.540
Well, certainly that's going to be feasible. Whether that's feasible now, they could get

02:02:18.540 --> 02:02:20.540
all the nuances correct.

02:02:20.540 --> 02:02:22.540
I bet it's pretty close.

02:02:22.540 --> 02:02:23.540
Yeah.

02:02:23.540 --> 02:02:24.540
I bet it's pretty close right now.

02:02:24.540 --> 02:02:28.540
But it has to be very close because we're doing it like in the next month or so.

02:02:28.540 --> 02:02:31.540
I bet. Don't you think they could do it, Jamie?

02:02:31.540 --> 02:02:35.660
Yeah. I think they could do it right now. Listen, Ray, I appreciate you very much. Thank

02:02:35.740 --> 02:02:39.540
you very much for being here. Thank you for your time. And thank you for this book. When

02:02:39.540 --> 02:02:41.540
is it available?

02:02:41.540 --> 02:02:42.540
June 24th.

02:02:42.540 --> 02:02:44.540
June, I got an early copy, kids.

02:02:44.540 --> 02:02:45.540
Yeah.

02:02:45.540 --> 02:02:46.540
Thank you, sir. Really appreciate you. Thank you very much.

02:02:46.540 --> 02:02:47.540
My pleasure.

02:02:47.540 --> 02:02:56.540
Bye, everybody.

