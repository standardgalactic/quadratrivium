start	end	text
0	4000	Joe Rogan Podcast, check it out!
4000	6000	The Joe Rogan Experience.
6000	10000	Train by day! Joe Rogan Podcast by night! All day!
12000	14000	Good to see you, sir.
14000	15000	Great to see you.
15000	18000	I was telling you before, I'm admiring your suspenders,
18000	20000	and you told me you have how many pairs of these things?
20000	21000	30 of them, yeah.
21000	22000	How did you?
22000	23000	I wear them every day.
23000	24000	Do you really? Every day?
24000	25000	Yeah.
25000	27000	Why do you like suspenders?
27000	29000	Practicality thing?
29000	36000	No, it expresses my personality.
36000	45000	And different ones have different personalities
45000	48000	that express how I feel that day.
48000	50000	I see, so it's just another style point.
50000	51000	Yeah.
51000	52000	See the reason why I was asking?
52000	55000	But you don't see any hand-painted suspenders.
55000	57000	Have you ever seen one?
57000	58000	I don't know.
58000	60000	I would have not noticed.
60000	62000	I only noticed because you were here.
62000	64000	I'm not really a suspender aficionado.
64000	68000	But the reason why I'm asking is because you're basically a technologist.
68000	70000	I mean, you know a lot about technology,
70000	77000	when you think that suspenders are kind of outdated tech.
77000	79000	Well, people like them.
79000	80000	Clearly.
80000	81000	Yeah.
81000	84000	And I'm surprised I haven't caught on.
84000	87000	But you have to have somebody who can actually paint them.
87000	90000	I mean, these are hand-painted suspenders.
90000	93000	So the ones that you have, right here, these are hand-painted?
93000	94000	Yeah.
94000	95000	Interesting.
95000	96000	Okay, so that's part of it.
96000	97000	So you're wearing art.
97000	98000	Exactly.
98000	99000	Got it.
99000	100000	So...
100000	102000	And art is part of technology.
102000	105000	We're using technology to create art now, so...
105000	106000	That's true.
106000	107000	And it's...
107000	109000	In fact, the very first...
109000	116000	I mean, I've been now in AI for 61 years, which is actually a record.
116000	124000	And the first thing I did was create something that could write music.
124000	129000	Writing music now with AI is a major field today,
129000	133000	but this was actually the first time that it had ever been done.
133000	136000	Yeah, that was one of your many inventions.
136000	138000	That was the first one, yeah.
138000	140000	And why did you go about doing that?
140000	145000	What was your desire to create artificial intelligence music?
145000	148000	Well, my father was a musician,
148000	151000	and I felt this would be a good way to relate to him.
151000	156000	And he actually worked with me on it.
156000	158000	And you could feed in music,
158000	162000	like you could feed in, let's say, Mozart or Chopin,
162000	167000	and I would figure out how they created melodies
167000	170000	and then write melodies in the same style.
170000	174000	So you could actually tell this is Mozart, this is Chopin.
174000	181000	It wasn't as good, but it's the first time that that had been done.
181000	183000	It wasn't as good then.
183000	185000	What are the capabilities now?
185000	188000	Because now they can do some pretty extraordinary things.
188000	192000	Yeah, it's still not up to what humans can do,
192000	194000	but it's getting there,
194000	197000	and it's really, it's pleasant to listen to.
197000	203000	We still have a while to do art, both art, music, so on.
203000	209000	Well, one of the main arguments against AI art comes from actual artists
209000	212000	who are upset that what essentially they're doing is they're,
212000	216000	like you could say, right, draw a paint,
216000	220000	or create a painting in the style of Frank Frasetta, for instance.
220000	225000	And what it would be, they would take all of Frasetta's work that he's ever done,
225000	228000	which is all documented on the internet,
228000	232000	and then you create an image that's representative of that.
232000	235000	So you're essentially, in one way or another,
235000	238000	you're kind of taking from the art.
238000	240000	Right, but it's not quite as good.
240000	242000	It will be as good.
242000	247000	I think we'll match human experience by 2029.
247000	251000	That's been my idea.
251000	254000	It's not as good.
254000	256000	Which is the best image generator right now, Jamie?
256000	258000	Pull one up.
258000	261000	They really change almost from day to day right now,
261000	263000	but mid-journey was the most popular one at first,
263000	268000	and then Dolly, I think, is a really good one too.
268000	270000	Mid-journey is incredibly impressive.
270000	272000	Incredibly impressive graphics.
272000	274000	I've seen some of the mid-journey stuff.
274000	276000	It's mind-blowing.
276000	278000	Still not quite as good.
278000	280000	But boys, it's so much better than it was five years ago.
280000	282000	That's what's scary. It's so quick.
282000	285000	I mean, it's never going to reach its limit.
285000	286000	We're not going to get to a point,
286000	288000	okay, this is how good it's going to be.
288000	291000	It's going to keep getting better.
291000	293000	And what would that look like?
293000	295000	If it can get to a certain point,
295000	299000	it will far exceed what human creativity is capable of.
299000	304000	Yes, I mean, when we reach the ability of humans,
304000	306000	it's not going to just match one human.
306000	308000	It's going to match all humans.
308000	311000	It's going to do everything that any human can do.
311000	314000	If it's playing a game like Go,
314000	316000	it's going to play it better than any human.
316000	318000	Right. Well, that's already been proven, right?
318000	320000	That they invented moves.
320000	324000	AI has invented moves that have now been implemented by humans
324000	326000	in a very complex game that they never thought
326000	328000	that AI was going to be able to be,
328000	330000	because it requires so much creativity.
330000	333000	Right. Arthur, we're not quite there,
333000	335000	but we will be there.
335000	342000	And by 2029, it will match any person.
342000	344000	That's it? 2029?
344000	346000	That's just a few years away.
346000	348000	Yeah, well, I'm actually considered conservative.
348000	351000	People think that will happen like next year or the year after,
351000	356000	but I actually said that in 1999.
356000	362000	I said we would match any person by 2029.
362000	364000	So 30 years.
364000	367000	People thought that was totally crazy.
367000	372000	And in fact, Stanford had a conference.
372000	375000	They invited several hundred people from around the world
375000	378000	to talk about my prediction.
378000	382000	And people came in and people thought that this would happen,
382000	385000	but not by 2029. They thought it would take 100 years.
385000	387000	Yeah, I've heard that.
387000	390000	I've heard that, but I think people are amending those.
390000	393000	Is it because human beings have a very difficult time
393000	396000	grasping the concept of exponential growth?
396000	399000	That's exactly right.
399000	403000	In fact, still, economists have a linear view.
403000	406000	And if you say, well, it's going to grow exponentially.
406000	412000	They say, yeah, but maybe 2% a year.
412000	417000	It actually doubles in 14 years.
417000	426000	And I brought a chart I can show you that really illustrates this.
426000	429000	Is this chart available online so we could show people?
429000	430000	Yeah, it's in the book.
430000	434000	But is it available online, that chart, where Jamie could pull it up
434000	436000	and someone could see it?
436000	439000	Just so the folks watching the podcast could see it too.
439000	441000	But I could just hold it up to the camera.
441000	443000	What's the title of it?
443000	449000	It says price performance of computation 1939 to 2023.
449000	450000	You have it.
450000	451000	OK, great.
451000	452000	Jamie already has it.
452000	454000	Yeah, the climb is insane.
454000	456000	It's like the San Juan Mountains.
456000	461000	What's interesting is that it's an exponential curve
461000	464000	and a straight line represents exponential growth.
464000	470000	And that's an absolute straight line for 80 years.
470000	475000	The very first point, this is the speed of computers.
475000	486000	It was 0.0007 calculations per second per constant dollar.
486000	491000	The last point is 35 billion calculations per second.
491000	497000	So that's a 20 quadrillion fold increase in those 80 years.
497000	503000	But the speed with which it gained is actually the same
503000	505000	throughout the entire 80 years.
505000	508000	Because if it was sometimes better and sometimes worse,
508000	510000	this curve would bend.
510000	512000	It would bend up and down.
512000	516000	It's really very much a straight line.
516000	520000	So the speed with which we increased it was the same
520000	522000	regardless of the technologies.
522000	524000	And the technology was radically different
524000	526000	at the beginning versus the end.
526000	533000	And yet it increased the speed exactly the same for 80 years.
533000	536000	In fact, the first 40 years, nobody even knew this was happening.
536000	538000	So it's not like somebody was in charge
538000	540000	and saying, OK, next year we have to get to here
540000	542000	and people would try to match that.
542000	545000	We didn't even know this was happening for 40 years.
545000	549000	40 years later, I noticed this for various reasons.
549000	551000	I predicted it would stay the same,
551000	556000	the same speed increase each year, which it has.
556000	559000	In fact, we just put the last dot like two weeks ago
559000	562000	and it's exactly where it should be.
562000	569000	So technology and computation, certainly prime form of technology,
569000	573000	increases at the same speed.
573000	575000	And this goes through a worn piece.
575000	577000	You might say, well, maybe it's greater during war.
577000	579000	No, it's exactly the same.
579000	581000	You can't tell when there's war or peace
581000	583000	or anything else on here.
583000	590000	It just matches from one type of technology to the next.
590000	593000	And it's also true of other things,
593000	599000	like, for example, getting energy from the sun.
599000	601000	That's also exponential.
601000	604000	It's also just like this.
604000	610000	It's increased.
610000	619000	We're now getting about a thousand times as much energy
619000	623000	from the sun that we did 20 years ago.
623000	626000	Because the implementation of solar panels and the like.
626000	631000	Has the function of it increased exponentially as well?
631000	636000	What I had understood was that there was a bottleneck in the technology
636000	640000	as far as how much you could extract from the sun from those panels.
640000	642000	No, not at all.
642000	650000	I mean, it's increased 99.7% since we started.
650000	653000	And it does the same every year.
653000	655000	It's an exponential curve.
655000	657000	And if you look at the curve,
657000	661000	you'll be getting 100% of all the energy we need in 10 years.
661000	662000	The person who told me that was Elon.
662000	664000	And Elon was telling me that this is the reason
664000	667000	why you can't have a fully solar-powered electric car.
667000	670000	Because it's not capable of absorbing that much from the sun
670000	672000	with a small panel like that.
672000	675000	He said there's a physical limitation in the panel size.
675000	680000	No, I mean, it's increased 99.7% since we started.
680000	682000	Since what year?
682000	689000	This is about 35 years ago.
689000	699000	In 99% of the ability of it as well as the expansion of use?
699000	701000	I mean, you might have to store it.
701000	704000	We're also making exponential gains in the storage of electricity.
704000	706000	Right. Battery technology.
706000	711000	So you don't have to get it all from a solar panel that fits in a car.
711000	716000	The concept was like, could you make a solar-paneled car,
716000	718000	a car that has solar panels on the roof?
718000	720000	And would that be enough to power the car?
720000	722000	And he said no.
722000	725000	He said it's just not really there yet.
725000	727000	Right. It's not there yet.
727000	729000	But it will be there in 10 years.
729000	730000	You think so?
730000	732000	Yeah, he seemed to doubt that.
732000	734000	He thought that there's a certain,
734000	737000	there's limitation of the amount of energy you can get from the sun period,
737000	740000	how much it gives out and how much those solar panels can absorb.
740000	744000	Well, you're not going to be able to get it all from the solar panel that fits in a car.
744000	746000	You're going to have to store some of that energy.
746000	751000	Right. So you wouldn't just be able to drive indefinitely on solar power.
751000	753000	Yeah, that was what he was saying.
753000	756000	So, but you can obviously power a house.
756000	761000	And especially if you have a roof, the Tesla has those solar-powered roofs now.
761000	764000	But you can also store the energy for a car.
765000	769000	I mean, we're going to go to all renewable energy,
769000	773000	wind and sun within 10 years,
773000	776000	including our ability to store the energy.
776000	778000	All renewable in 10 years?
778000	782000	So what are they going to do with all these nuclear plants and coal-powered plants and all these things?
782000	784000	That's completely unnecessary.
784000	789000	People say we need nuclear power, which we don't.
789000	793000	You can get it all from the sun and wind within 10 years.
794000	799000	So in 10 years, you'll be able to power Los Angeles with sun and wind?
799000	800000	Yes.
800000	801000	Really?
801000	802000	Yeah.
802000	805000	I was not aware that we were anywhere near that kind of timeline.
805000	810000	Well, that's because people are not taking into account exponential growth.
810000	813000	So the exponential growth also of the grid?
813000	820000	Because just to pull the amount of power that you would need to charge, you know, X amount of million.
820000	825000	Someone has an electric vehicle by 2035, let's say then.
825000	830000	Just the amount of change you would need on the grid would be pretty substantial.
830000	832000	Well, we're making exponential gains on that as well.
832000	833000	Are we?
833000	834000	Yeah.
834000	835000	Yeah.
835000	836000	I wasn't aware.
836000	840000	I had this impression that there was a problem with that,
840000	844000	and especially in Los Angeles, they've actually asked people at certain times
844000	846000	when it's hot out to not charge your car.
846000	851000	Looking at the future, that's true now, but it's growing exponentially.
851000	856000	In every field of technology then, essentially.
856000	859000	Is the bottleneck a battery technology?
859000	863000	And how close are they to solving some of these problems,
863000	870000	like conflict minerals and the things that we need in order to power these batteries?
870000	874000	Our ability to store energy is also growing exponentially.
874000	882000	So putting all that together, we'll be able to power everything we need within 10 years.
882000	883000	Wow.
883000	884000	Most people don't think that.
884000	889000	So you're thinking that based on this idea that people would have a limited idea.
889000	891000	Let me imagine that computation would grow like this.
891000	895000	It's just continuing to do that.
895000	899000	And so we have large language models, for example.
899000	902000	No one expected that to happen like five years ago.
902000	903000	Right.
903000	906000	And we had them two years ago, but they didn't work very well.
906000	913000	So it began a little less than two years ago that we could actually do large language models.
913000	917000	And that was very much a surprise to everybody.
917000	922000	So that's probably the primary example of exponential growth.
922000	923000	We had Sam Altman on.
923000	928000	One of the things that he and I were talking about was that AI figured out a way to lie.
928000	932000	That they used AI to go through a CAPTCHA system.
932000	938000	And the AI told the system that it was vision impaired, which is not technically a lie.
938000	941000	But it used it to bypass, are you a robot?
941000	946000	Well, we don't know now for large language models to say they don't know something.
946000	948000	So you ask it a question.
948000	954000	And if the answer to that question is not in the system, it still comes up with an answer.
954000	958000	So it'll look at everything and give you its best answer.
958000	962000	And if the best answer is not there, it still gives you an answer.
962000	966000	But that's considered a hallucination.
966000	967000	Oh, hallucination.
967000	968000	Yeah.
968000	969000	That's what it's called.
969000	970000	Really?
970000	971000	AI hallucination.
971000	973000	So they cannot be wrong.
973000	975000	They have to be able to answer this.
975000	979000	So far, we're actually working on being able to tell if it doesn't know something.
979000	982000	So if you ask it something and say, oh, I don't know that.
982000	984000	Right now, it can't do that.
984000	985000	Oh, wow.
985000	986000	That's interesting.
986000	991000	So it gives you some answer.
991000	995000	And if the answer is not there, it's just like make something up.
995000	1000000	It's the best answer, but the best answer isn't very good because it doesn't know the answer.
1000000	1007000	And the way to fix hallucinations is to actually give it more capabilities to memorize things
1007000	1011000	and give it more information so it knows the answer to it.
1011000	1020000	And if you tell an answer to a question, it will remember that and give you that correct answer.
1020000	1024000	But these models are not, we don't know everything.
1024000	1035000	And it has to, we have to be able to scan an answer to every single question, which we can't quite do.
1035000	1037000	It'd be actually better if it could actually answer.
1037000	1039000	Well, gee, I don't know that.
1039000	1040000	Right.
1040000	1045000	But in particular, like say when it comes to exploration of the universe,
1045000	1049000	if there's a certain amount of, I mean, vast amount of the universe we have not explored.
1049000	1053000	So if it has to answer questions about that, it would just come up with an answer.
1053000	1054000	Right.
1054000	1057000	It'll just come up with an answer, which will likely be wrong.
1057000	1059000	That's interesting.
1059000	1066000	But that would be a real problem if someone was counting on the AI to have a solution for something too soon.
1066000	1067000	Right?
1067000	1068000	Right.
1068000	1075000	If you don't know everything, search engines actually know are pretty well vetted.
1075000	1080000	And if it actually answers something, it'll, it's usually correct.
1080000	1082000	Unless it's curated.
1082000	1086000	But large language models don't have that capability.
1086000	1089000	So it'd be good actually if they knew that they were wrong.
1089000	1093000	They'd also tell us what we have to fix.
1093000	1099000	What about the idea that AI models are influenced by ideology,
1099000	1103000	that AI models have been programmed with certain ideologies?
1103000	1105000	I mean, they do learn from people.
1105000	1106000	Yeah.
1106000	1108000	And people have ideologies.
1108000	1109000	Right.
1109000	1112000	Some of which are, some of which are not correct.
1112000	1122000	And that's a large way in which it will make things up because it's learning from people.
1122000	1124000	Right.
1124000	1132000	So right now, if somebody has access to a good search engine,
1132000	1138000	they will check before they actually answer something with a search engine to make sure that it's correct.
1138000	1142000	Because search engines are generally much more accurate.
1142000	1143000	Generally.
1143000	1144000	Right.
1144000	1150000	When it comes to this idea that people enter information into a computer,
1150000	1152000	and then the computer relies on that ideology,
1152000	1156000	do you anticipate that with artificial general intelligence,
1156000	1158000	it'll be agnostic to ideology,
1158000	1165000	that it'll be able to reach a point where instead of deciding things based on social norms,
1165000	1168000	or whatever the culture is accepted currently,
1168000	1172000	that it would look at things more objectively and rationally?
1172000	1173000	Well, eventually.
1173000	1174000	Eventually.
1174000	1175000	Eventually.
1175000	1178000	But we still call it artificial general intelligence,
1178000	1179000	even if it didn't do that.
1179000	1195000	And people certainly are influenced by whatever their people that they respect feel is correct,
1195000	1200000	and will be as influenced by as people are.
1200000	1205000	And we'll still call it artificial general intelligence.
1205000	1213000	We are starting to check what large language models come up with with search engines,
1213000	1216000	and that's actually making them more correct.
1216000	1219000	But we have to actually continue on this curve.
1219000	1222000	We need more data to be able to store everything.
1222000	1227000	This is not enough data to be able to store everything correctly.
1227000	1236000	This is a large amount of large language models for which we don't have storage for the data.
1236000	1239000	So that's what's holding us back is data and storage?
1239000	1243000	Yeah, we also have to have the correct storage.
1243000	1251000	So that's really where the effort is going to be able to get rid of these hallucinations.
1251000	1256000	That's a fun thing to say, hallucinations in terms of artificial intelligence.
1256000	1259000	Well, we usually come up with the wrong things.
1259000	1264000	Large language models is not really the correct way to talk about this.
1264000	1269000	It does know language, but there's a lot of other things it knows.
1269000	1281000	We're using them now to come up with medicines.
1281000	1284000	For example, the Moderna vaccine.
1284000	1301000	We wrote down every possible type of medicine that might work.
1301000	1305000	It was actually several billion mRNA sequences,
1305000	1311000	and we then tested them all and did that in two days.
1311000	1320000	So it actually came up with tested several billion and decided on it in two days.
1320000	1323000	We then tested it with people.
1323000	1330000	We'll be able to overcome that as well because we'll be able to test it with machines.
1330000	1334000	But we actually did test it with people for ten months.
1334000	1336000	There was still a record.
1336000	1342000	So for machines, when they start testing medications with machines, how will they audit that?
1342000	1347000	So the concept will be that you take into account biological variability,
1347000	1352000	all the different factors that would lead to a person to have an adverse reaction to a certain compound,
1352000	1359000	and then you program all the known data about how things interact with the body.
1359000	1364000	You need to be able to simulate all the different possibilities.
1364000	1368000	And then come up with a number of how many people will be adversely affected by something?
1368000	1371000	That's one of the things you would look at.
1371000	1374000	And then efficacy based on age, health?
1374000	1384000	But that could be done literally in a matter of days rather than years.
1384000	1390000	The question would be who's in charge of that data and how does that get resolved?
1390000	1395000	And if artificial intelligence is still prone to hallucinations,
1395000	1400000	and they start using those hallucinations to justify medications, that could be a bit of an issue.
1400000	1404000	Especially if it's controlled by a corporation that wants to make a lot of money.
1404000	1406000	Well, that's the issue.
1406000	1408000	To be able to do it correctly.
1408000	1412000	There's going to have to be a point in time where we all decide that artificial intelligence
1412000	1416000	has reached this place where we can trust it implicitly.
1416000	1424000	Right. Well, that's why they take now the leading candidate and actually test it with people.
1424000	1434000	But we'll be able to get rid of the testing with people once we can have reliance on the simulation.
1434000	1438000	So we've got to make the simulations correct.
1438000	1447000	But like right now we actually tested with people and that takes, well, took 10 months in this case.
1447000	1455000	When you look at artificial intelligence and you look at the expansion of it and the ultimate place that it will eventually be,
1455000	1460000	what do you see happening inside of our lifetime, like inside of 20 years?
1460000	1465000	What kind of revolutionary changes on society would this have?
1465000	1475000	Well, one thing I feel will happen in five years by 2029 is we'll reach longevity escape velocity.
1475000	1480000	So right now you go through a year and you use up a year of your longevity.
1480000	1482000	You're then a year older.
1482000	1491000	However, we do have scientific progress and we're making coming up with new cures for diseases and so on.
1491000	1493000	Right now you're getting back about four months.
1493000	1500000	So you lose a year, but through scientific progress, you're getting back four months.
1500000	1502000	So you're only losing eight months.
1502000	1506000	However, the scientific progress is progressing exponentially.
1506000	1510000	And by 2029, you'll get back a full year.
1510000	1515000	So you lose a year, but you get back a year and you pretty much stay in the same place.
1515000	1518000	So by 2029, you'll be static.
1518000	1522000	And past 2029, you'll actually get back more than a year.
1522000	1523000	You'll get back.
1523000	1528000	Can I be a baby again?
1528000	1533000	No, but in terms of your longevity, you'll get back more than a year.
1533000	1534000	Right.
1534000	1538000	So you'll be able to essentially go back in biological age.
1538000	1542000	So the telomeres changing the elasticity of the skin.
1542000	1546000	Eventually you'll be able to do that.
1546000	1549000	It doesn't guarantee you living forever.
1549000	1557000	I mean, you could have a 10 year old and you could compute that he's got many decades of longevity and he could die tomorrow.
1557000	1559000	Sure.
1559000	1564000	But overall, there'd be an expansion of the age that most people die.
1564000	1566000	And that's something that we're going to get.
1566000	1572000	And it's also using the same type of logic as large language models.
1572000	1574000	But that's not language.
1574000	1576000	You're actually creating medications.
1576000	1582000	So we should call the large event models, not large language models, because it's not just dealing with language.
1582000	1585000	It's dealing with all kinds of things.
1585000	1592000	When I talked to you 10 years ago, you were telling me about this pretty extensive supplement routine that you're on.
1592000	1599000	Well, I'm trying to get to the point where we have longevity escape velocity in good shape.
1599000	1600000	Right.
1600000	1602000	And yes, I do follow that.
1602000	1610000	I take maybe 80 pills a day and some injections and so on.
1610000	1611000	Peptides.
1611000	1613000	Yes, peptides.
1613000	1617000	So far it works.
1617000	1621000	Have you ever gone off of it to see what you feel like normally?
1621000	1622000	No.
1622000	1623000	Well, I do that, right?
1623000	1624000	Yeah.
1624000	1626000	I mean, it seems to work.
1626000	1628000	And there's evidence behind it.
1628000	1629000	How old are you now?
1629000	1632000	76.
1632000	1633000	You look good.
1633000	1635000	You look good for 76, man.
1635000	1636000	That's great.
1636000	1637000	So it's doing something.
1637000	1638000	Yeah.
1638000	1640000	I think it's working.
1640000	1651000	And so your goal is to get to that point where they start doing, you live a year, you stay static, and then eventually get back to youthfulness.
1651000	1652000	Right.
1652000	1653000	And it's not that far off.
1653000	1658000	If you're diligent, I think we'll get there by 2029.
1658000	1660000	Now, not everybody's diligent.
1660000	1661000	Right.
1661000	1662000	Of course.
1662000	1666000	Now, past that, this is for life extension, which is great.
1666000	1671000	But what about how AI is going to change society?
1671000	1672000	Yes.
1672000	1674000	Well, that's a very big issue.
1674000	1681000	And it's already doing lots of things, makes some people uncomfortable.
1681000	1685000	What we're actually doing is increasing our intelligence.
1685000	1687000	I mean, right now you have a brain.
1687000	1692000	It has different modules in it to deal with different things.
1692000	1697000	But really, it's able to connect one concept to another concept.
1697000	1700000	And that's what your brain does.
1700000	1704000	We can actually increase that by, for example, carrying around a phone.
1704000	1706000	This has connections in it.
1706000	1709000	It's a little bit of a hassle to use.
1709000	1713000	If I ask you to do something, you've got to kind of mess with it.
1713000	1717000	Actually, it'd be good if this actually listened to your conversation.
1717000	1718000	Oh, it does.
1718000	1722000	And without saying anything, you're just talking.
1722000	1725000	And it says, oh, the name of that actress is so-and-so.
1725000	1727000	Yeah, but then it's a busy body.
1727000	1730000	It's like interfering with your life, talking to you all the time.
1730000	1732000	Well, there's ways of dealing with that, too.
1732000	1733000	You shut it off.
1733000	1738000	So we haven't done that yet.
1738000	1744000	But that's a way of expanding your connections.
1744000	1752000	What a large language model does, it has connections in it as well.
1752000	1759000	And the fact that it's getting now to a point that's getting fairly comparable to the human brain,
1759000	1763000	we have about a trillion connections in our brain.
1763000	1770000	Things like the top model from Google or GPT-4,
1770000	1778000	they have about 400 billion connections approximately.
1778000	1781000	They'll be at a trillion probably within a year.
1781000	1785000	That's pretty comparable to what the human brain does.
1785000	1790000	Eventually, it'll go beyond that and will have access to that.
1790000	1793000	So it's basically making us smarter.
1793000	1805000	So if you have the ability to be smarter, that's something that's positive, really.
1805000	1815000	I mean, if we were like mice today and we had the opportunity to become like humans,
1815000	1817000	we wouldn't object to that.
1817000	1820000	In fact, we are humans and we don't object to that.
1820000	1824000	It used to be shrews.
1824000	1828000	And this is going to basically make us smarter.
1828000	1832000	Eventually, we'll be much smarter than we are today.
1832000	1834000	And that's a positive thing.
1834000	1846000	We'll be able to do things that are today that we find bothersome in a way that's much more palatable.
1846000	1849000	The idea of us getting smarter sounds great.
1849000	1851000	It'd be great to be smarter.
1851000	1856000	Right, but people object to that because it's like competition.
1856000	1858000	In what way?
1858000	1865000	Well, I mean, Google has, I don't know, 60,000, 70,000 programmers.
1865000	1869000	How many programmers exist in the world?
1869000	1873000	How much longer is that going to be a viable career?
1873000	1878000	Because language models already can code.
1878000	1885000	Not quite as good as a real expert coder, but how long is that going to be?
1885000	1887000	It's not going to be 100 years.
1887000	1890000	It's going to be a few years.
1890000	1894000	So people see it as competition.
1894000	1896000	I have a slightly different view of that.
1896000	1901000	I see these things as actually adding to our own intelligence.
1901000	1907000	And we're merging with these kinds of computers and making ourselves smarter
1907000	1909000	by merging with it.
1909000	1916000	And eventually it'll go inside our brain and be able to make us smarter instantly
1916000	1921000	just like we had more connections inside our own brain.
1921000	1925000	Well, I think people have reservations always when it comes to great change.
1925000	1927000	And this is probably the greatest change.
1927000	1931000	The greatest change we've ever experienced in our lifetimes for sure has been the internet.
1931000	1935000	And this will make that look like nothing.
1935000	1937000	It'll change everything.
1937000	1940000	And it seems inevitable.
1940000	1947000	I understand that people are upset about it, but it just seems like what human beings were sort of designed to do.
1947000	1951000	Right. We're the only animal that actually creates technology.
1951000	1956000	It's a combination of our brain and something else, which is our thumb.
1956000	1958000	So I can imagine something.
1958000	1965000	Oh, if I take that leaf from a tree, I could create a tool with it.
1965000	1975000	Other animals have actually a bigger brain like the whale, dolphins, elephants.
1975000	1980000	They have a larger brain than we do, but they don't have something equivalent to the thumb.
1980000	1986000	Monkey has a thing that looks like the thumb, but it's actually an inch down and it doesn't actually work very well.
1986000	1993000	So they can actually create a tool, but they don't create a tool that's powerful enough to create the next tool.
1993000	2002000	So we're actually able to use our tools and create something that's that much more significant.
2002000	2008000	So we can create tools, and that's really part of who we are.
2008000	2015000	It makes us that much more intelligent, and that's a good thing.
2015000	2030000	I mean, here's...
2030000	2033000	So here's U.S. personal income per capita.
2033000	2042000	So this is the average amount that we make per person in constant dollars.
2042000	2045000	Right here, it's on the screen.
2045000	2049000	Do we make a lot more money, but things cost a lot more money too, right?
2049000	2051000	No, this is constant dollars.
2051000	2054000	Constant dollars in relation to the inflation?
2054000	2057000	Yeah, so this does not show you inflation.
2057000	2060000	These are constant dollars.
2060000	2066000	So we're actually making that much more each year on average.
2066000	2069000	Right, but it doesn't take into account inflation, correct?
2069000	2072000	So it's not taking into account the rise of cost of things.
2072000	2074000	No, it is taking that.
2074000	2076000	Oh, it is, okay.
2076000	2080000	So we're making that much more in constant dollars.
2080000	2086000	If you look over the past hundred years, we've made about ten times as much.
2086000	2092000	I wonder if there's a similar chart about consumerism, just about material possessions.
2092000	2095000	I wonder if, like, how much more we're purchasing and creating.
2095000	2101000	I've always felt like that's one of the things that materialism is one of those instincts
2101000	2109000	that human beings sort of look down upon and this aimless pursuit of buying things.
2109000	2116000	But I feel like that motivates technology because the constant need for the newest,
2116000	2122000	greatest thing is one of the things that fuels the creation and innovation of new things.
2122000	2125000	But if you were to go back a hundred years, you'd be very unhappy.
2125000	2126000	Oh, yeah.
2126000	2130000	Because you wouldn't have, I mean, you wouldn't have a computer, for example.
2130000	2132000	You wouldn't have anything.
2132000	2134000	You'd have most things you've grown accustomed to.
2134000	2135000	Yeah.
2135000	2139000	I mean, unless that's why you wanted that.
2139000	2142000	Also, we didn't live very long.
2142000	2144000	Right, medical advancements.
2144000	2152000	At average, life was 48 years in 1900.
2152000	2154000	It's 35 years in 1800.
2154000	2155000	Right.
2155000	2158000	Go back a thousand years, it was 20 years.
2158000	2161000	That takes into account child mortality, too, though, right?
2161000	2164000	But it's also injuries, death.
2164000	2166000	Some people did live long.
2166000	2168000	There was people that lived back then.
2168000	2171000	If nothing happened to you, you did live to be 80, like a normal person.
2171000	2174000	But that was actually very rare.
2174000	2176000	Because most things happen to people.
2176000	2179000	Most people, by the time you get to 80, you've had at least one hospital visit.
2179000	2180000	Something's gone wrong.
2180000	2183000	Broken arm, broken this, broken that.
2183000	2186000	It was very rare to make it to 80.
2186000	2187000	Right.
2187000	2188000	200 years ago.
2188000	2191000	Right, but the human body was physically capable of doing it.
2191000	2192000	Right.
2192000	2200000	Well, our human body can go on forever if you fix things properly.
2200000	2206000	There's nothing in our body that means that you have to die at 100 or even 120.
2206000	2210000	We can go on really indefinitely.
2210000	2212000	Well, that's the groundbreaking work today, right?
2212000	2217000	They're treating disease or, excuse me, age as if it is a disease,
2217000	2219000	not just inevitable consequences.
2219000	2220000	Right.
2220000	2224000	And our FDA doesn't accept that, but they're actually beginning to accept it now.
2224000	2225000	Well, as they get older.
2225000	2227000	Yeah, exactly.
2227000	2229000	They're forced into it.
2230000	2235000	The concept of artificial general intelligence scares a lot of people also because of Hollywood, right?
2235000	2238000	Because of the Terminator films and things along those lines.
2238000	2245000	Like, how far away are we, do you think, from actual artificial humans or will we ever get there?
2245000	2248000	Will we integrate before that takes place?
2248000	2256000	I mean, all of this additional intelligence that we're creating is something that we use.
2256000	2259000	And it's just like it came with us.
2259000	2265000	So we're actually making ourselves more intelligent and ultimately that's a good thing.
2265000	2269000	And if we have it and then we say, well, gee, we don't really like this.
2269000	2271000	Let's take it away.
2271000	2273000	People would never accept that.
2273000	2282000	They may be against the idea of general intelligence, but once they get it, nobody wants to give that up.
2282000	2287000	And it will be beneficial.
2291000	2302000	The Blue Knight started 200 years ago because the cotton genie come out and all these people that were making money with the cotton genie were against it.
2302000	2306000	And they would actually destroy these machines at night.
2307000	2312000	And they said, gee, if this keeps going, all jobs are going to go away.
2312000	2319000	And indeed, people using the cotton genie to create more wealth, that did go away.
2319000	2324000	But we actually made more money because we created things that didn't exist then.
2324000	2327000	We didn't have anything like electronics, for example.
2327000	2339000	And as we can actually see, we make 10 times as much in constant dollars as we did 100 years ago.
2339000	2344000	And if you were to ask, well, what are people going to be doing?
2344000	2351000	You couldn't answer it because we didn't understand the internet, for example.
2351000	2356000	And there's probably some technologies down the pipe that are going to have a similar impact.
2356000	2357000	Exactly.
2357000	2360000	And they're going to extend life, for example.
2360000	2362000	But are they going to create life?
2365000	2372000	Well, we know how to create life.
2380000	2382000	Well, that's an interesting question.
2383000	2386000	What do you mean by create life?
2386000	2395000	What I think is that human beings are some sort of a biological caterpillar that makes a cocoon that gives birth to an electronic butterfly.
2395000	2401000	I think we are creating a life form and that we're merely conduits for this thing.
2401000	2408000	And that all of our instincts and ego and emotions and all these things feed into it, materialism feeds into it.
2408000	2414000	We keep buying and keep innovating and technology keeps increasing exponentially.
2414000	2420000	And eventually it's going to be artificial intelligence and artificial intelligence is going to create better artificial intelligence
2420000	2429000	and a form of being that has no limitations in terms of what's capable of doing and capable of traveling anywhere
2429000	2431000	and not having any biological limitations in terms of...
2431000	2433000	But that's going to be ourselves.
2433000	2441000	We're going to be able to create life that is like humans but far greater than we are today.
2441000	2443000	With an integration of technology.
2443000	2444000	Yeah.
2444000	2446000	If we choose to go that route.
2446000	2449000	But that's the prediction that you have, that we will go that route.
2449000	2452000	Like a neural link type deal, something along those lines.
2452000	2453000	Right.
2453000	2456000	So I don't see this competition like the things are going to...
2456000	2458000	No, I don't think it's competition.
2458000	2460000	Well, it will seem like that.
2460000	2466000	I mean, if you have a job doing coding and suddenly they don't really want you anymore
2466000	2471000	because they can do coding with a large language model, it's going to feel like it's competition.
2471000	2473000	Well, there's an issue now with films.
2473000	2481000	Tyler Perry, who owns and he was building an $800 million television studio and he stopped production.
2481000	2483000	What is it called, Sora?
2483000	2485000	Is that what it's called, Jamie?
2485000	2494000	He stopped production when he saw the capabilities of AI just for creating visuals, scenes, movies.
2494000	2497000	There's one that's incredibly impressive.
2497000	2498000	It's Tokyo.
2498000	2501000	They're walking down the street of Tokyo in the winter.
2501000	2506000	So it's snowing and they're walking down the street and you look at it, you're like, this is insane.
2506000	2508000	This looks like a film.
2508000	2511000	See if you can find that film because it's incredible.
2511000	2513000	But would you want to get rid of that?
2513000	2514000	Get rid of what?
2514000	2516000	The capability.
2516000	2518000	No, I don't want to get rid of the capability.
2518000	2521000	But people do want to get rid of it.
2521000	2528000	People that make movies, people that actually film things with cameras and use actors are going to be very upset.
2528000	2532000	So this, this is all fake, which is insane.
2532000	2534000	Beautiful snowy Tokyo city is bustling.
2534000	2539000	The camera moves through the bustling city street, following several people enjoying the beautiful snowy weather
2539000	2541000	and shopping at nearby stalls.
2541000	2545000	Gorgeous Sakura petals are flying through the wind along with snowflakes.
2545000	2547000	And this is what you get.
2547000	2550000	I mean, this is insanely good.
2550000	2553000	The variability, like just the way people are dressed.
2553000	2559000	If you saw this somewhere else, look at this, a robot's life in a cyberpunk setting.
2559000	2564000	If you saw this, you would say, oh, they filmed this.
2564000	2568000	But just look at what they're able to do with animation and kids movies and things along those lines.
2568000	2570000	Yeah, and it's going to get better.
2570000	2572000	Yeah, it's just incredible.
2572000	2575000	I mean, it's a new art form.
2575000	2579000	So right there, the smoke looks a little uniform, but yeah.
2579000	2581000	I mean, there's some problems with this, but...
2581000	2582000	Not much.
2582000	2583000	Yeah.
2583000	2587000	And you imagine what it was like five years ago and then imagine what it's going to be like five years from now.
2587000	2589000	Yeah, absolutely.
2589000	2590000	And it's insane.
2590000	2596000	I mean, no one took into consideration the idea that kids are going to be cheating on their school papers using chat GPT.
2596000	2600000	But my kids tell me that's a real problem in school now.
2600000	2604000	Yes, definitely.
2604000	2611000	So no one saw that coming, no one saw this coming, and what we're at now is with chat GPT 4, right?
2611000	2612000	4.5?
2612000	2613000	Is that what it is?
2613000	2614000	Well, 4.5 is coming.
2614000	2615000	4.5 is coming.
2615000	2620000	5 is supposed to be the massive leap.
2620000	2621000	It'll be a leap.
2621000	2624000	Just like 3 to 4 was a massive leap.
2624000	2625000	Yeah.
2625000	2627000	It's going to continue.
2627000	2629000	It's never going to be finished.
2629000	2630000	Right.
2630000	2631000	It'll keep going.
2631000	2634000	And it will also be able to make better versions of itself, correct?
2634000	2637000	And yes, well, we do that.
2637000	2639000	I mean, technology does that already.
2639000	2640000	Right.
2640000	2644000	But if you scale that out 100 years from now, what are you looking at?
2644000	2646000	You're looking at a God.
2646000	2649000	Well, it'll be less than 100 years.
2649000	2653000	So you're looking at a God in 50 years?
2653000	2654000	Less than that.
2654000	2660000	I mean, once we have an ability to emulate everything that humans can do, and not just
2660000	2664000	one human, but all humans, and that's only like 2029.
2664000	2667000	That's only five years from now.
2667000	2670000	And then it will make better versions of that.
2670000	2675000	So it will probably solve a lot of the problems that we have in terms of energy storage, data
2675000	2678000	storage, data speeds, computation speeds.
2678000	2681000	And also medications.
2681000	2682000	For us.
2682000	2683000	For humans, yeah.
2683000	2688000	Wouldn't it be better just, Ray, just download yourself into this beautiful electronic body?
2688000	2691000	Why do you want to be biological?
2691000	2696000	I mean, ultimately, that's what we're going to be able to do.
2696000	2698000	You think that's going to happen?
2698000	2699000	Yeah.
2699000	2701000	So do you think that we'll be able to...
2701000	2704000	I mean, we'll be able to create...
2704000	2710000	I mean, the singularity is when we multiply our intelligence a million fold, and that's 2045.
2710000	2712000	So that's not that long from now.
2712000	2714000	That's like 20 years from now.
2714000	2716000	Right.
2716000	2727000	And therefore, most of your intelligence will be handled by the computer part of ourselves.
2727000	2733000	The only thing that won't be captured is what comes with our body originally.
2733000	2736000	We'll ultimately be able to do that as well.
2736000	2747000	We'll take a little longer, but we'll be able to actually capture what comes with our normal body and be able to recreate that.
2747000	2756000	So that also has to do with how long we live, because if everything is backed up...
2756000	2762000	I mean, right now, anytime you put anything into a phone or any kind of electronics, it's backed up.
2762000	2765000	So, I mean, this has a lot of data.
2765000	2772000	I could flip it, and it ends up in a river, and we can't capture anymore.
2772000	2775000	I can recreate it, because it's all backed up.
2775000	2778000	And you think that's going to be the case with consciousness?
2778000	2783000	That's going to be the case of our normal biological body as well.
2783000	2789000	What's to stop someone like Donald Trump from just making 100,000 versions of himself?
2789000	2793000	Like, if you can back someone up, could you duplicate it?
2793000	2795000	Couldn't you have three or four of them?
2795000	2796000	Couldn't you have a bunch of them?
2796000	2798000	Couldn't you live multiple lives?
2798000	2800000	Yes.
2800000	2806000	Would you be interacting with each other while you're living multiple lives, having consultations about what is St. Louis Ray doing?
2806000	2808000	Well, I don't know. Let's talk to San Francisco Ray.
2808000	2811000	San Francisco Ray is talking to Florida Ray.
2812000	2822000	It's basically a matter of increasing our intelligence and being able to multiply Donald Trump, for example, that comes with that.
2822000	2828000	Do you think there'll be regulations on that to stop people from making 100,000 versions of themselves that operate a city?
2828000	2832000	There'll be lots of regulations. There's lots of regulations we have already.
2832000	2837000	You can't just create a medication and sell it to people that cares its disease.
2837000	2838000	Right.
2838000	2840000	We have a tremendous amount of regulations.
2840000	2847000	Sure, but we don't really with phones. With your phone, essentially, if you had the money, you could make as many copies of that as you wanted.
2847000	2856000	Yes. There are some regulations. We regulate everything, but you're right.
2856000	2863000	Generally, electronics doesn't have as much regulation as...
2863000	2868000	Right. And when you get to a certain point, we will be electronics.
2868000	2881000	Yes. Certainly, if we multiply our intelligence a million fold, everything of that additional million fold of yours is not regulated.
2881000	2894000	Right. When you think about the concept of integration and technological integration, when do you think that will start taking place and what will be the initial usage of it?
2894000	2900000	Like, what will be the first versions and what would they provide?
2900000	2903000	Well, we have it now. Large language models are pretty impressive.
2903000	2905000	I mean, if you look at what they can do...
2905000	2911000	I mean, I'm talking about physical integration with the human body, like a Neuralink type thing.
2911000	2922000	Right. Some people feel that we could actually understand what's going on in your brain and actually put things into your brain without actually going into the brain with something like Neuralink.
2922000	2925000	So something that sits on the outside of your head?
2925000	2933000	Yeah. It's not clear to me if that's feasible or not. I've been assuming that you have to actually go in.
2933000	2944000	Now, Neuralink isn't exactly what we want because it's too slow and it actually will do what it's advertised to do.
2945000	2960000	I actually know some people like this who were active people and they completely lost the ability to speak and to understand language and so on.
2960000	2971000	So they can't actually say anything to you and we can use something like Neuralink to actually have them express something.
2971000	2975000	They can think something and then have it be expressed to you.
2975000	2979000	Right. And they're doing that, right? They had the first patient. The first patient that was...
2979000	2980000	Yeah.
2980000	2984000	Yeah. And apparently that person can move a cursor around on a screen.
2984000	2990000	Right. And therefore you can do anything. It's fairly slow, though. And Neuralink is slow.
2990000	2995000	And if you really want to extend your brain, you need to do it at a much faster pace.
2995000	2997000	But isn't that going to increase exponentially as well?
2997000	2998000	Yes, absolutely.
2998000	3002000	So how long do you think it will be before it's implemented?
3002000	3019000	Well, it's got to be by 2045 because that's when the singularity exists and we can actually multiply our intelligence on the order of a million fold.
3019000	3029000	And when you say 2045, what is the source of that estimation?
3029000	3046000	Because we'll be able to, based actually on this chart and also the increase in the ability of software to also expand,
3046000	3056000	we'll be able to multiply our intelligence a million fold and we'll be able to put that inside of our brain.
3056000	3059000	It would be just like it's part of our brain.
3059000	3062000	So this is just following the current graph of progress?
3062000	3063000	Yeah, exactly.
3063000	3067000	So if you follow the current graph of progress and if you do understand exponential growth,
3067000	3071000	then what we're looking at in 2045 is inevitable.
3071000	3073000	Right.
3073000	3076000	Does that concern you at all or are you excited about it?
3076000	3082000	Do you think it's just a thing that is happening and you're a part of it and you're experiencing it?
3082000	3090000	I think it will be enthusiastic about it.
3090000	3098000	I mean, imagine if you were to ask a mouse, would you like to actually be as intelligent as a human?
3098000	3101000	Right.
3101000	3105000	It's hard to know what people would say, but generally that's a positive thing.
3105000	3106000	Generally.
3106000	3107000	Yeah.
3107000	3109000	And that's what it's going to be like.
3109000	3111000	We're going to be that much smarter.
3111000	3112000	And what do you...
3112000	3116000	And once we're there, is someone going to say, no, I don't really like this.
3116000	3122000	I want to be stupid like human beings used to be.
3122000	3125000	Nobody's really going to say that.
3125000	3128000	Do human beings now say, gee, I'm really too smart.
3128000	3131000	I'd really like to be like a mouse.
3131000	3138000	Not necessarily, but what people do say is that technology is too invasive and that it's too much a part of my life.
3138000	3144000	And I'd like to sort of have a bit of an electronic vacation and separate from it.
3144000	3147000	And there's a lot of people that I know that have gone to...
3147000	3148000	But nobody does that.
3148000	3155000	I mean, nobody becomes stupid like we used to be when we were mice.
3155000	3156000	Right.
3156000	3157000	But I'm not saying stupid.
3157000	3161000	I'm saying some people, just like being a human, the way humans are now.
3161000	3166000	Because one of the complications that comes with the integration of technology is what we're seeing now with people.
3166000	3171000	Massive increases in anxiety from social media use, being manipulated by algorithms,
3171000	3176000	the effect that it has on culture, misinformation and disinformation and propaganda.
3176000	3185000	There's so many different factors that are at play now that make people more anxious and more depressed statistically than ever.
3185000	3193000	I'm not sure we had more anxiety today than we used to have.
3193000	3196000	Well, we certainly had more when the Mongols were invading.
3196000	3200000	We certainly had more anxiety when we were worried constantly about war.
3200000	3203000	But I think people have a pretty heightened level of social anxiety.
3203000	3204000	Well, they take war.
3204000	3212000	I mean, 80 years ago we had 100 million people die in Europe and Asia from World War II.
3212000	3218000	We're very concerned about wars today and they're terrible.
3218000	3222000	But we're not losing millions of people.
3222000	3223000	Right.
3223000	3224000	But we could.
3224000	3225000	We most certainly could.
3225000	3231000	With what's going on with Israel and Gaza, what's going on with Ukraine and Russia,
3231000	3233000	it could easily escalate with...
3233000	3234000	It's thousands of people.
3234000	3236000	It's not millions of people.
3236000	3237000	For now.
3237000	3238000	Yeah.
3238000	3243000	But if it escalates to a hot war where it's involving the entire world.
3243000	3250000	What would really cause a tremendous amount of danger is something that's not really artificial intelligence.
3250000	3255000	It was invented when I was a child, which is atomic weapons.
3255000	3256000	Right.
3256000	3264000	I remember when I was five or six, we'd actually go outside, put our hands behind our back
3264000	3267000	to protect us from a nuclear war.
3267000	3268000	Yeah, drills.
3268000	3270000	And it seemed to work.
3270000	3271000	We're still here.
3271000	3275000	Do you remember those things where they tell kids to get under the desk?
3275000	3276000	Yes, that's right.
3276000	3278000	We went under the desk and put out...
3278000	3282000	Which is hilarious, as if a desk is going to protect you from a nuclear bomb.
3282000	3283000	Right.
3283000	3285000	But that's not AI.
3285000	3286000	Right.
3286000	3290000	No, but AI applied to nuclear weapons makes them significantly more dangerous.
3290000	3295000	And isn't one of the problems with AI is that AI will find a solution to a problem.
3295000	3301000	So if you have AI running your military and AI says, what do you want me to do?
3301000	3304000	And you say, well, I'd like to take over Taiwan.
3304000	3306000	And AI says, well, this is how to do it.
3306000	3309000	And it just implements it with no morals.
3309000	3318000	No thought of any sort of diplomacy or just force.
3318000	3319000	Right.
3319000	3321000	It hasn't happened yet.
3321000	3326000	Because we do have people in charge and the people are enhanced with AI.
3326000	3332000	And AI can actually help us to avoid that kind of problem by thinking through the implications
3332000	3335000	of different solutions.
3335000	3336000	Sure.
3336000	3339000	If it has some sort of autonomy.
3339000	3344000	But if we get to the point where one superpower has AI, artificial general intelligence,
3344000	3351000	the other one doesn't, how much of a significant advantage would that be?
3351000	3353000	I mean, I do think there are problems.
3353000	3356000	Basically, there's problems with intelligence.
3356000	3364000	And we'd like to say stupid.
3364000	3368000	But actually, it's better to be intelligent.
3368000	3371000	I believe it's better to have intelligence.
3371000	3372000	Overall.
3372000	3373000	Sure.
3373000	3374000	Right.
3374000	3381000	But my question was, if there's a race to achieve AGI, how close is this race?
3381000	3382000	Is it neck and neck?
3382000	3384000	I mean, who's at the lead?
3384000	3388000	And how much capital is being put into these companies that are at the lead?
3388000	3395000	And whoever achieves it first, if that is under the control of a government, it's completely
3395000	3398000	dependent upon what are the morals and ethics of that government?
3398000	3400000	What was the Constitution?
3400000	3401000	What if it happens in China?
3401000	3402000	What if it happens in Russia?
3402000	3405000	What if it happens somewhere other than the United States?
3405000	3409000	And even if it does happen in the United States, who's controlling it?
3409000	3414000	I mean, the knowledge of how to create these things is pretty widespread.
3414000	3422000	It's not like somebody can just capitalize on a way to do it and nobody else understands
3422000	3423000	it.
3423000	3434000	The knowledge of how to create a large language model or how to create the type of chips that
3434000	3439000	would enable you to create this is actually pretty widespread.
3439000	3444000	So do you think essentially the competition is pretty even in all the countries currently?
3444000	3445000	Yeah.
3445000	3449000	And there's also probably espionage, there's espionage where they're stealing information
3449000	3453000	and sharing information and selling information.
3453000	3466000	And in terms of differences, the United States actually has superior AI compared to other
3466000	3467000	places.
3467000	3468000	That's pretty good for us.
3468000	3475000	I mean, we're actually way ahead of China, I would say.
3475000	3479000	Right, but China has a way of figuring out what we're doing in copying it.
3479000	3481000	Pretty good at that.
3481000	3483000	They have been, yeah.
3483000	3484000	Yeah.
3484000	3490000	So do you have any concern whatsoever in the idea that AI gets in the hands of the wrong
3490000	3492000	people?
3492000	3496000	So when it first gets implemented, that's the big problem.
3496000	3501000	Because before it exists, before artificial general intelligence really exists, it doesn't.
3501000	3502000	And then it does.
3502000	3503000	And who has it?
3503000	3507000	And then once it does, can that AGI stop other people from getting it?
3507000	3512000	Can you program it to make sure, can you sabotage grids?
3512000	3516000	Can you do whatever you can to take down the internet in these opposing places?
3516000	3519000	Could you inject their computations with viruses?
3519000	3525000	What could you do to stop other people from getting to where you're at if you have an infinitely
3525000	3526000	higher intelligence?
3526000	3527000	First.
3527000	3532000	If that's what your goal is, then yes, you could do that.
3532000	3534000	Are you worried about that at all?
3534000	3535000	Yes, I worry about it.
3535000	3537000	What is your main worry?
3537000	3540000	I mean, you worry about the implementation of artificial intelligence.
3540000	3550000	What's your main worry?
3550000	3563000	I mean, I'm worried if people who have a destructive idea of how to use these capabilities get into
3563000	3564000	control.
3564000	3567000	Right.
3567000	3569000	And that could happen.
3569000	3576000	And I've got a chapter in the book about perils that are like what we're talking about.
3577000	3584000	And what do you think that could look like if the wrong people got to hold this technology?
3584000	3592000	Well, if you look at actually who controls atomic weapons, which is not AI, some of the
3592000	3601000	worst people in the world, and if you were to ask people right after we used two atomic
3602000	3608000	weapons within a week 80 years ago, what's the likelihood that we're going to go another
3608000	3612000	80 years and not have that happen again?
3612000	3614000	Everybody would say zero.
3614000	3615000	Right.
3615000	3617000	But it actually has happened.
3617000	3618000	Chalking.
3618000	3619000	Yeah.
3619000	3620000	Yeah.
3620000	3625000	And I think there's actually some message there.
3625000	3627000	Mutual assured destruction.
3627000	3630000	But the thing is, would artificial intelligence...
3630000	3632000	But that has not happened.
3632000	3633000	Right.
3633000	3634000	It has not happened yet.
3634000	3638000	But would artificial general intelligence in the control of the wrong people negate
3638000	3642000	that mutually assured destruction that keeps people from doing things?
3642000	3647000	Obviously, we did drop bombs on Hiroshima and Nagasaki.
3647000	3648000	We did.
3648000	3652000	We did indiscriminately kill who knows how many hundreds of thousands of people with
3652000	3653000	those weapons.
3653000	3654000	We did it.
3654000	3659000	And if human beings were capable of doing it because no one else had it, if artificial
3659000	3666000	intelligence reaches that sentient level and is in control of the wrong people, what's
3666000	3668000	to stop them from doing...
3668000	3671000	There's no mutually assured destruction if you're the one who's got it.
3671000	3673000	You're the only one who's got it.
3673000	3675000	And you possibly...
3675000	3680000	My concern is that whoever gets it could possibly stop it from being spread everywhere
3680000	3683000	else and control it completely.
3683000	3688000	And then you're looking at a completely dystopian world.
3688000	3689000	Right.
3689000	3693000	So if you ask me what I'm concerned about, it's one of those lines.
3693000	3696000	Because that's what I always want to get out of you guys.
3696000	3701000	Because there's so many people that are rightfully so, so high on this technology and the possibilities
3701000	3703000	for enhancing our lives.
3703000	3708000	But the concern that a lot of people have is that at what cost and what are we signing
3708000	3709000	up for?
3709000	3710000	Right.
3710000	3717000	But I mean, if we want to, for example, live indefinitely, this is what we need to do.
3717000	3718000	We can't do...
3718000	3720000	What if you're denying yourself heaven?
3720000	3722000	You ever thought of that possibility?
3722000	3726000	I know that's a ridiculous abstract concept, but if heaven is real, if the idea of the
3726000	3731000	afterlife is real and it's the next level of existence and you're constantly going through
3731000	3737000	these cycles of life, what if you're stepping in artificially denying that?
3737000	3739000	It's hard to imagine.
3739000	3741000	It is hard to imagine, but so is life.
3741000	3743000	So is the universe itself.
3743000	3744000	So is the big bang.
3744000	3745000	My father...
3745000	3755000	My father died when I was 22, so it's more than 50, 60 years ago.
3755000	3759000	And it's hard...
3759000	3766000	And he was actually a great musician and he created fantastic music, but he hasn't done
3766000	3771000	that since he died.
3771000	3780000	And there's nothing that exists that is at all creative based on him.
3780000	3783000	We have his memories.
3783000	3786000	Actually created a large language model that represented him.
3786000	3788000	I can actually talk to him.
3788000	3790000	You do that now?
3790000	3791000	Yeah.
3791000	3793000	It's in the book.
3793000	3799000	When you do that, have you thought about implementing some sort of a Sora-type deal where you're
3799000	3802000	talking to him?
3802000	3804000	Well you can do that now with language.
3804000	3809000	Right, but I mean physically like looking at him like you're on a zoom call with him.
3809000	3815000	That's a little bit in the future to be able to actually capture the way he looks, but
3815000	3817000	that's also feasible.
3817000	3819000	It seems pretty feasible.
3819000	3823000	And certainly it could be something representative of what he looks based on photographs that
3823000	3825000	you have, right?
3825000	3832000	Things like that is a reason to continue so that we can create that and create our own
3832000	3838000	ability to continue to exist.
3838000	3846000	You talk to people and they say, well I don't really want to live past 90 or whatever, 100.
3846000	3855000	But in my mind if you don't exist, there's nothing for you to experience.
3855000	3857000	That's true in this dimension.
3857000	3862000	My thought on that, people saying that I don't want to live past 90, it's like okay are you
3862000	3863000	alive now?
3863000	3864000	Do you like being alive now?
3864000	3866000	What's the difference between now and 90?
3866000	3871000	Is it just a number or is it the deterioration of your physical body and how much effort
3871000	3876000	have you put into mitigating the deterioration of your natural body so that you can enjoy
3876000	3877000	life now?
3877000	3878000	Exactly.
3878000	3882000	And we've actually seen who would want to take their lives.
3882000	3885000	People do take their lives.
3885000	3894000	If they are experiencing something that's miserable, if they're suffering physically, emotionally,
3894000	3904000	mentally, spiritually, and they just cannot stand the way life is carrying on, then they
3904000	3906000	want to take their lives.
3906000	3909000	Otherwise people don't.
3909000	3912000	If they're enjoying their lives, they continue.
3912000	3915000	And people say, I don't want to live past 100.
3915000	3926000	But when they get to be 99.9, they don't want to disappear unless they're suffering.
3926000	3927000	Unless they're suffering.
3927000	3932000	That's what's interesting about the positive aspects of AI.
3932000	3937000	Once we can manipulate human neurochemistry to the point where we figure out what is causing
3937000	3942000	great depression, what is causing anxiety, what is causing a lot of these schizophrenic
3942000	3943000	people.
3943000	3944000	And we definitely had that before.
3944000	3945000	We didn't have the terms.
3945000	3947000	We didn't understand schizophrenia.
3947000	3949000	But people definitely had it.
3949000	3950000	For sure.
3950000	3953000	But what if we get to a point where we can mitigate that with technology?
3953000	3956000	Where we can say, this is what's going on in the human body.
3956000	3957000	That's why we're continuing.
3957000	3958000	Right.
3958000	3959000	I was saying that's a good thing.
3959000	3961000	That's a positive aspect of this technology.
3961000	3964000	And think about also profoundly.
3964000	3969000	Think about how many people do take their lives and with this technology would not just live
3969000	3975000	happily but also be productive and also contribute to whatever society is doing.
3975000	3979000	That's why we're carrying on with this.
3979000	3985000	But in order to do that, we do have to overcome some of the problems that you've articulated.
3985000	3991000	I think what a lot of people are terrified of is that these people that are creating this
3991000	3995000	technology, there's oversight.
3995000	4000000	But it's oversight by people that don't necessarily understand it the way the people that are creating
4000000	4001000	it.
4001000	4003000	And they don't know what guardrails are in place.
4003000	4004000	How safe is this?
4004000	4010000	Especially when it's implemented with some sort of weapons technology.
4010000	4012000	Or some sort of a military application.
4012000	4017000	Especially a military application that can be insanely profitable.
4017000	4021000	And the motivations behind utilizing that are that profit.
4021000	4026000	And then we do horrible things and somehow justify it.
4026000	4032000	I mean I think democracy is actually an important issue here because democratic nations tend
4032000	4035000	not to go to war with each other.
4035000	4047000	And I mean you look at the way we're handling military technology.
4047000	4052000	If everybody was a democracy, I think there'd be much less war.
4052000	4056000	As long as it's a legitimate democracy, it's not controlled by money.
4056000	4061000	As long as it's a legitimate democracy, it's not controlled by the military industrial complex.
4061000	4066000	The pharmaceutical industry or whoever puts the people that are in elected places.
4066000	4067000	Who puts them in there?
4067000	4068000	How do they get funded?
4068000	4071000	And what do they represent once they get in there?
4071000	4073000	Are they there for the will of the people?
4073000	4074000	They're there for their own career?
4074000	4078000	Do they bypass the safety and the future of the people for their own personal gain,
4078000	4081000	which we've seen politicians do?
4081000	4086000	There's certain problems with every system that involves human beings.
4086000	4089000	This is another thing that technology may be able to do.
4089000	4094000	One of the things, if you think about the worst attributes of humans.
4094000	4103000	Whether it's war, crime, some of the horrible things that human beings are capable of.
4103000	4111000	Imagine that technology can find what causes those thoughts and behaviors in human beings and mitigate them.
4111000	4118000	I've joked around about this, but if we came up with something that would elevate dopamine just 300% worldwide,
4118000	4119000	there would be no more war.
4119000	4120000	It'd be over.
4120000	4122000	Everybody would be loving everybody.
4122000	4124000	We'd be interacting with each other.
4124000	4126000	Well, that's the point of doing this.
4126000	4128000	But there would also be no sad songs.
4128000	4129000	Wow.
4129000	4131000	You need some blues in your life.
4131000	4133000	You need a little bit of that too.
4133000	4134000	Or do we?
4134000	4135000	Maybe we don't.
4135000	4138000	Maybe that's just a byproduct of our monkey minds.
4138000	4144000	And that one day we'll surpass that and get to this point of enlightenment.
4144000	4152000	Enlightenment seems possible without technological innovation, but maybe not.
4152000	4155000	I've never really met a truly enlightened person.
4155000	4156000	I've met some people that are pretty close.
4156000	4159000	But if you could get there with technology.
4159000	4165000	If technology just completely elevated the human consciousness to the point where all of our conflicts come erased.
4165000	4173000	Just for starters, if you could actually live longer, quite aside from the motivations of people,
4173000	4180000	most people die not because of people's motivations, but because our bodies just won't last that long.
4180000	4181000	Right.
4182000	4190000	And a lot of people say, you know, I don't want to live longer, which makes no sense to me.
4190000	4197000	Why would you want to disappear and not be able to have any kind of experience?
4197000	4200000	Well, I think some people don't think you're disappearing.
4200000	4209000	I mean, there is a long held thought in many cultures that this life is but one step.
4209000	4220000	And that there is an afterlife and maybe that exists to comfort us because we deal with existential angst and the reality of our own inevitable demise.
4220000	4226000	Or maybe it's a function of consciousness being something that we don't truly understand.
4226000	4238000	And what you are is a soul contained in a body and that we have a very primitive understanding of the existence of life itself and of the existence of everything.
4238000	4241000	Well, I guess that makes sense.
4241000	4244000	But I don't really accept it.
4244000	4246000	Well, there's no evidence, right?
4246000	4254000	But is it there's no evidence because we're not capable of determining it yet and understanding it?
4254000	4257000	Or is it just because it doesn't exist?
4257000	4259000	That's the real question.
4259000	4261000	It's like, is this it?
4261000	4262000	Is this everything?
4262000	4264000	Or is this merely a stage?
4264000	4272000	And are we monkeying with that stage by interfering with the process of life and death?
4272000	4274000	Well, it makes sense.
4274000	4277000	But I don't really see the evidence for that.
4277000	4280000	I can see from your perspective.
4280000	4284000	I don't see the evidence of it either, but it's a concept that is not.
4284000	4296000	It's just when you start talking to strength theorists and they start talking about things existing and not existing at the same time, particles in superposition, you're talking about magic.
4296000	4300000	You're talking about something that's impossible to wrap your head around.
4300000	4303000	Even just the structure of an atom.
4303000	4304000	What's in there?
4304000	4305000	Nothing?
4305000	4307000	How much of it is space?
4307000	4313000	The entire existence of everything in the universe seems preposterous.
4313000	4315000	But it's all real.
4315000	4323000	And we only have a limited grasp of understanding of what this is really all about and what processes are really in place.
4323000	4324000	Right.
4324000	4336000	But if you look at people's perspective, if somebody gets a disease and is kind of known they could only live like another six months, people are not happy with that.
4336000	4337000	No.
4337000	4338000	Well, they're scared.
4338000	4339000	They're scared to die.
4339000	4344000	It's a natural human instinct that keeps us alive for all these hundreds of millions of years.
4344000	4346000	Yes, but very few people would be happy with that.
4346000	4354000	And if you then had something, gee, we have this new device, you could take this and you won't die.
4354000	4355000	Right.
4355000	4357000	Almost everybody would do that.
4357000	4358000	Sure.
4358000	4362000	But would they appreciate life if they knew it had no end?
4363000	4371000	Or would it be like a lottery winner just goes nuts and spends all their money and loses their marbles because they can't believe they can't die?
4371000	4375000	Well, first of all, it's not guaranteed to live forever.
4375000	4376000	Sure.
4376000	4377000	You can get in an accident.
4377000	4378000	Something can happen.
4378000	4379000	You get injured.
4379000	4386000	But if we get to a point where you have automated cars that significantly reduce the amount of automobile accidents.
4386000	4391000	Well, also, we can back up everything, everything in our physical body as well as...
4391000	4393000	How far away are we from that?
4393000	4400000	That idea of, I mean, we don't really truly understand what consciousness is, correct?
4400000	4401000	Right.
4401000	4410000	So how would we be able to manipulate it or reduplicate it to the point where you're putting it inside of some kind of a computation device?
4410000	4424000	Well, we know to be able to create a computation that matches what our brain does.
4424000	4427000	That's what we're doing with these large language models.
4427000	4428000	Right.
4428000	4436000	And we're actually very close now to what our brain can do with these large language models and it'll be there like within a year.
4436000	4453000	And we can back up the electronic version and we'll get to the point where we can back up what our brain normally does.
4453000	4456000	So we'll be able to actually back that up as well.
4456000	4461000	We'll be able to detect what it is and back that up just like our computers.
4461000	4467000	We'll create it in the form of an artificial version of everything that it is to be a human being.
4467000	4468000	Right, exactly.
4468000	4471000	In terms of emotions, love, excitement.
4471000	4474000	And that's going to happen over the next 20 years.
4474000	4477000	It's not a thousand years.
4477000	4479000	But will that be a person?
4479000	4482000	I mean, or will it be some sort of a zombie?
4482000	4484000	Like what motivations will it have?
4484000	4490000	If you can take human consciousness and duplicate it, much like you could duplicate your phone,
4490000	4493000	and you make this new thing, what does that thing feel like?
4493000	4495000	Does that thing live in hell?
4495000	4498000	What does that experience like for that thing?
4498000	4500000	What about large language models?
4500000	4502000	Do they really exist?
4502000	4504000	I mean, they can talk.
4504000	4507000	They certainly do, but would you want to be one?
4507000	4509000	Are we different than that?
4509000	4510000	Yeah, we're people.
4510000	4511000	We shake hands.
4511000	4512000	I give you a hug.
4512000	4513000	You pet my dog.
4513000	4515000	You listen to music.
4515000	4517000	We'll be able to do all of that as well.
4517000	4518000	Right, but will you want to?
4518000	4519000	Will you even care?
4519000	4523000	A lot of what gives us joy in life is biological motivations.
4523000	4526000	There's human reward systems that are put in place that allow us to...
4526000	4528000	Well, it's going to be part of who we are.
4528000	4529000	Right.
4529000	4534000	And we just like a person, and we'll also have our physical bodies as well.
4534000	4537000	And that'll also be able to be backed up.
4537000	4543000	And we'll be doing the things that we do now, except we'll be able to have them continue.
4543000	4547000	So if you get hit by a car and you die, there's another array that just pops up.
4547000	4549000	Oh, we got the backup array.
4549000	4557000	And the backup array will have no feelings at all about having it had died and come back to life.
4557000	4559000	Well, that's a question.
4559000	4560000	Yeah.
4560000	4565000	I mean, why wouldn't it be just like Ray is now?
4565000	4566000	Why wouldn't it?
4566000	4572000	If we figure out that if biological life is essentially a kind of technology that the
4572000	4577000	universe has created, and we can manipulate that to the point where we understand it,
4577000	4583000	we get it, we've optimized it, and then replicate it.
4583000	4584000	Physically replicate it.
4584000	4590000	Not just replicate it in the form of a computer, but an actual physical being.
4590000	4591000	Right.
4591000	4592000	Well, that's where we're headed.
4592000	4597000	Do you anticipate that people will be happy with whatever they have?
4597000	4602000	Because if you decide, I don't like being five, six, I wish I was six, six.
4602000	4603000	I don't like being a woman.
4603000	4604000	I like, I want to be a man.
4604000	4606000	I don't want to be Asian.
4606000	4608000	I want to be, you know, whatever.
4608000	4609000	I want to be a black person.
4609000	4610000	I want to be...
4610000	4617000	We'll actually be able to do all of those things simultaneously and so on.
4617000	4621000	We're not going to be limited by those kinds of happenstance.
4621000	4623000	Which is going to be very strange.
4623000	4627000	What will human beings look like if you give people the ability to manipulate your physical
4627000	4628000	form?
4628000	4631000	Well, we do things now that were impossible even 10 years ago.
4631000	4636000	We certainly do, but we don't change race, size, sex, gender, height.
4636000	4641000	We don't do all those...the radical increase in just your intelligence.
4641000	4643000	Like, what is that going to look like?
4643000	4648000	What kind of an interaction is it going to be between two human beings when you have
4648000	4650000	a completely new form?
4650000	4654000	You know, you're much different physically than you ever were when you were alive.
4654000	4655000	You're taller.
4655000	4656000	You're stronger.
4656000	4657000	You're smarter.
4657000	4658000	You're faster.
4658000	4660000	You're basically not really a human anymore.
4660000	4662000	You're a new thing.
4662000	4664000	I mean, we're expanding who we are.
4664000	4667000	We're already expanded who we are from, you know...
4667000	4668000	Sure.
4668000	4669000	Right.
4669000	4673000	Over a course of hundreds of thousands of years, we've gone from being Australopithecus
4673000	4674000	to what we are now.
4674000	4681000	That has to do with the pace at which we make changes.
4681000	4682000	Right.
4682000	4690000	And we can make changes now much more quickly than we could, you know, 100,000 years ago.
4690000	4691000	Right.
4691000	4696000	But if we can manipulate our physical form with no limitations, I mean, what are...we
4696000	4698000	have six armed people that can fly?
4698000	4699000	Like, what is it going to look like?
4699000	4701000	Well, do you have a problem with that?
4701000	4704000	Yeah, I would discriminate against six armed people that can fly.
4704000	4707000	That's the one area I allow myself to give prejudice to.
4707000	4708000	Okay.
4708000	4712000	No, I'm just curious as to how much time you've spent...
4712000	4714000	Seven armed people would be okay.
4714000	4719000	Yeah, seven armed people is cool because it's like, you know, maybe five on one side,
4719000	4720000	two on the other.
4720000	4727000	No, I'm just curious as to like how much time you've spent thinking about what this could
4727000	4728000	look like.
4728000	4733000	And I just...I don't think it's going to be as simple as, you know, it's going to be
4733000	4739000	Ray Kurzweil, but Ray Kurzweil as like a 30-year-old man, 50 years from now.
4739000	4743000	I think it's probably going to be...you're going to be all kinds of different things.
4743000	4744000	You could be kind of whatever you want.
4744000	4745000	You could be a bird.
4745000	4751000	I mean, what's to stop...if we can get to manipulate the physical form and we can take consciousness
4751000	4752000	and put it into a physical form.
4752000	4757000	But that's a description, I think, of something that's positive rather than negative.
4757000	4758000	You could be a giant eagle.
4758000	4765000	I mean, negative is people that want to destroy things, getting power.
4765000	4766000	Sure.
4766000	4768000	And that is a problem.
4768000	4772000	Well, it's certainly improvement in terms of the viability.
4772000	4775000	Seven arms and being like an eagle and so on.
4775000	4780000	I mean, and you can also change that.
4780000	4781000	Right.
4781000	4786960	So I think that's a positive aspect and we will be able to do that kind of thing.
4786960	4787960	Sure.
4787960	4792560	If you want to look at it in a binary fashion, positive and negative, but it's also going
4792560	4798240	to be insanely strange, like it's not going to be as simple as there'll be people that
4798240	4799960	are living in 2069.
4799960	4804160	Well, it seems strange once it's first reported.
4804160	4808960	If it's been reported now for five years and people are constantly doing it, you won't find
4808960	4809960	it that strange.
4809960	4811960	It'll just be life.
4811960	4812960	Yeah.
4812960	4813960	Yeah.
4813960	4814960	So that's what I'm asking.
4814960	4819560	Like when you think about the implementation of this technology to its fullest, what does
4819560	4821440	the world look like?
4821440	4828040	What does the world look like in 2069?
4828040	4834480	I mean, the kind of things that you can imagine right now will be able to do.
4834480	4839560	And it might seem strange when it first happens, but when it happens for the, you know, billions
4839560	4843400	of dollars in time, it won't seem that strange.
4843400	4849880	And maybe you're like being an eagle for a few minutes.
4849880	4851640	It's certainly interesting.
4851640	4854000	It's certainly interesting.
4854000	4859520	I just wonder how much time you've spent thinking about what this world looks like with the
4859520	4865760	full implementation of the kind of exponential growth of technology that would exist if we
4865760	4867640	do make it to 2069.
4867640	4880200	Well, I did write a book, Danielle, and this young girl has fantastic capabilities and
4880200	4883680	no one really can figure out how she does this.
4883680	4895840	She actually takes over China at age 15 and she makes it a democracy and then she actually
4896040	4903320	becomes president of the United States at 19, she has to, of course, create a constitutional
4903320	4909080	amendment that at least she can become president at 19.
4909080	4911640	That sounds like what a dictator would do.
4911640	4913200	Right.
4913200	4920480	But unlike a dictator, she's very popular and she writes very good music.
4920480	4923480	And this is one artificial intelligence creature?
4923480	4924480	Yes.
4924480	4925480	And how was she created?
4925720	4932840	It never says that she gets these capabilities through AI.
4932840	4938440	I didn't want to spell that out, but that would be the only way that she could do this.
4938440	4939440	Right.
4939440	4942200	Unless it's some insane freak of genetics.
4942200	4945840	And she's like a very positive person.
4945840	4947840	She's very popular.
4947840	4951120	Yeah, but she's the only one that has that.
4951120	4952120	Yeah.
4952120	4953120	Right.
4953120	4955440	She doesn't give it to everybody, which is where it gets really weird.
4955480	4957160	You have a cell phone, I have a cell phone.
4957160	4958560	Pretty much everybody has one now.
4958560	4961960	What happens when everybody gets the kind of technology we're discussing?
4961960	4968960	Well, it shows you the benefit that she has it and if everybody gets it, that would be
4968960	4970480	even more positive.
4970480	4971480	Right.
4971480	4972480	Perhaps, yeah.
4972480	4976840	I mean, that's the best way of looking at it, that we become a completely altruistic,
4976840	4984240	positive, beneficial to each other, society of integrated minds.
4984240	4985240	A benefit.
4985240	4989200	If you have more intelligence, you'd be more likely to do this.
4989200	4990200	Yes.
4990200	4992680	Yeah, for sure.
4992680	4993680	That's the benefit.
4993680	4994680	Yeah.
4994680	4995680	Yeah.
4995680	5001600	So we live longer and we're also smarter than making more rational decisions towards each
5001600	5003640	other.
5003640	5008440	So overall, when you're looking at this, you just don't concentrate really on the negative
5008440	5009440	possibilities.
5009440	5010440	Well, no.
5010440	5013200	I mean, I do focus on that as well.
5013200	5014200	I mean.
5014200	5017280	But you think overall it's net positive?
5017280	5018280	Yes.
5018280	5025320	It's called intelligence and if you have more intelligence, we'll be doing things that
5025320	5029200	are more beneficial to ourselves and other people.
5029200	5031680	Do you think that the experiences that we're having right now?
5031680	5036960	Like right now, we have much less crime than we did 50 years ago.
5036960	5044120	Now, if you listen to people debating presidential politics, they'll say, crime is worse than
5044120	5054880	it ever, but if you look at the actual statistics, it's gone way down and if you actually go
5054880	5064440	back like a few hundred years, crime and murder and so on was far, far higher than it is today.
5064440	5067000	It's actually pretty rare.
5067000	5074360	So the kind of additional intelligence that we've created is actually good for people.
5074360	5076560	If you look at the actual data.
5076560	5077560	Sure.
5077560	5083360	If you look at Stephen Pinker's work, scale it from 100 plus years ago to today, things
5083360	5087320	are generally always seem to be moving in a better direction.
5087320	5088320	Right.
5088320	5092680	Well, Pinker didn't credit this to technology.
5092680	5098160	He just looks at the data and says, it's gotten better.
5098160	5102120	What I try to do in the current book is to show how it's related to technology and as
5102120	5105800	we have more technology, we're actually moving in this direction.
5105800	5110400	So you feel it's a function of technology that we're moving in this direction?
5110400	5111400	Absolutely.
5111400	5115000	I mean, that's why.
5115000	5116760	I mean, look at the technology.
5116760	5124480	In 80 years, we've multiplied the amount of computation 20 quadrillion times.
5124480	5129160	And so we have things that didn't exist two years ago.
5129160	5130520	Right.
5130520	5137040	When you think about the idea of life on earth and that this is happening and that we are
5137040	5143800	on this journey to 2045, to the singularity, do you consider whether or not this is happening
5143800	5147360	elsewhere in the universe or whether it's already happened?
5147360	5156240	Yeah, we see no evidence that there's any form of life, let alone intelligent life anywhere
5156240	5157240	else.
5157240	5161760	And I say, well, we're not in touch with these other people.
5161760	5173680	It is possible, but it seems, I mean, given the exponential impact of this type of technology
5173680	5200040	we would be spaced out based on, over a launch period of time, so some people that might
5200040	5206360	be ahead of us, it could be ahead of us, certainly thousands of years, even millions
5206360	5207360	of years.
5207360	5214880	And so they'd be like way ahead of us and they'd be doing galaxy-wide engineering.
5214880	5219000	How is it that we look out there and we don't see anybody doing galaxy-wide engineering?
5219000	5222400	And maybe we don't have the capability to actually see it.
5222400	5229080	I mean, the universe is, what's the 13.7 billion years old or whatever it is?
5229080	5238640	But even just incidental capabilities would affect galaxies, we would see that somehow.
5238640	5245440	Would we, if we were at the peak, if there is intelligent life in the universe, some
5245440	5249600	form of that intelligent life has to be the most advanced?
5249600	5255000	And what if we are underestimating our position in the universe, that we are the most advanced
5255000	5256000	people?
5256000	5259440	But maybe there's something that's like 10 years, maybe there's an industrial age.
5259440	5264880	I think there's a good argument that we are ahead of other people.
5264880	5270320	But we don't have the capability of observing the goings on of a planet 5,000 light years
5270320	5271320	away.
5271320	5278080	We can't see into their atmosphere, we can't look at high-resolution video of activity on
5278080	5279080	that planet.
5279080	5282240	But if they were doing galaxy-wide engineering, I think we would notice that.
5282240	5284400	If they were more advanced than us, maybe we would.
5284400	5285400	But what if they're not?
5285400	5287200	What if they're at the level that we're at?
5287200	5288960	Well, that's what I'm saying.
5288960	5289960	What if we're at the peak?
5289960	5290960	And this is like...
5290960	5293920	I think it's an argument that we are at the peak.
5293920	5298320	What if it gets to the point where artificial intelligence gets implemented and then that
5298320	5303120	becomes the primary form of life and it doesn't have the desire to do anything in terms of
5303120	5308480	like galactic engineering?
5308480	5314840	But even just incidental things would affect whole galaxies.
5314840	5315840	Like what?
5315840	5317640	Things like we're doing, are we affecting the whole galaxy?
5317640	5318640	No, not yet.
5318640	5319640	Right.
5319640	5322960	But what if it's like us, but it gets to the point where it becomes artificial intelligence
5322960	5326840	and then it doesn't have emotions, it doesn't have desires, it doesn't have ambitions, so
5326840	5328600	why would it decide to expand?
5328600	5330000	Why would it not have those things?
5330000	5333840	Well, we'd have to program it into it, but it would probably decide that that's foolish
5333840	5336200	and that those things have caused all these problems.
5336200	5338040	All the problems in human race.
5338080	5340080	That's our number one issue, war.
5340080	5342480	What is war caused by?
5342480	5349200	It's caused by ideologies, it's caused by acquisition of resources, theft of resources, violence.
5349200	5354080	War is not the primary thing that we are motivated by.
5354080	5358960	It's not the primary thing we're motivated by, but it's existed in every single step
5358960	5361880	of the way of human existence.
5361880	5363600	But it's actually getting better.
5363600	5365720	I mean, just look at the effect of war.
5365720	5366720	Sure.
5366800	5370640	We have a couple of wars going on, they're not killing millions of people like they used
5370640	5371640	to.
5371640	5372640	Right.
5372640	5373640	Right.
5373640	5378280	My point is that if artificial intelligence recognizes that the problem with human beings
5378280	5385760	is these emotions and a lot of it is fueled by these desires, like the desire to expand,
5385760	5390280	the desire to acquire things, the desire to achieve.
5390280	5393720	Well, the emotion is positive, I mean, music and other things.
5393720	5395040	To us.
5395040	5396040	To us.
5396160	5403160	If it gets to the point where artificial intelligence is no longer stimulated by mere human creations,
5403160	5408560	creativity, all these different things, why would it even have the ambition to do any sort
5408560	5410680	of galaxy-wide engineering?
5410680	5412480	Why would it want to?
5412480	5417120	Because it's based on us.
5417120	5419920	It is based on us until it decides it's not based on us anymore.
5419920	5420920	That's my point.
5420920	5426480	We realize that if we're based on a very violent chimpanzee, and we say, you know what, there's
5426480	5431480	a lot of what we are because of our genetics, that it really are a problem, and this is
5431480	5436240	what's causing all of our violence, all of our crime, all of our war.
5436240	5442480	If we just step in and put a stop to all that, will we also put a stop to our ambition?
5442480	5446280	I would maintain that we're actually moving away from that.
5446280	5449280	We are moving away from that, but that's just natural, right?
5449320	5454000	That's natural with our understanding and our mitigations of these social problems.
5454000	5455000	Right.
5455000	5458320	So if we expand that even more, we'll be even more in that direction.
5458320	5459960	As long as we're still we.
5459960	5464360	But as soon as you become something different, why would it even have the desire to expand?
5464360	5469760	If it was infinitely intelligent, why would it even want to physically go anywhere?
5469760	5471160	Why would it want to?
5471160	5474960	What's the reason for our motivation to expand?
5474960	5475960	What is it?
5475960	5476960	It's human.
5477040	5481840	The same humans that were tribal creatures that roam, the same humans that stole resources
5481840	5483120	from neighboring villages.
5483120	5484400	This is our genes, right?
5484400	5487400	This is what made us that got us to this point.
5487400	5492920	If we create a sentient artificial intelligence that's far superior to us, and it can create
5492920	5496920	its own version of artificial intelligence, the first thing it's going to engineer out
5496920	5500920	is all these stupid emotions that get us in trouble.
5500920	5509600	If it just can create happiness and joy from programming, why would it create happiness
5509600	5515520	and joy through the acquisition of other people's creativity, art, music, all those things?
5515520	5519240	And then why would it have any ambition at all to travel?
5519240	5521000	Why would it want to go anywhere?
5521000	5524200	Well, I mean, it's an interesting philosophical problem.
5524200	5525200	Right.
5525200	5529800	It is a problem because a lot of what we are and the things that we create is because of
5529800	5532360	all these flaws that you would say.
5532360	5536240	If you were programming us, you'd say, well, what is the cause of all these issues that
5536240	5537240	plague the human race?
5537240	5538880	I wouldn't necessarily say that there are flaws.
5538880	5539880	Murder is a flaw.
5539880	5541600	Isn't it a flaw?
5541600	5543080	But that's way down.
5543080	5544080	Right.
5544080	5546720	But it's a technology that moves ahead.
5546720	5548400	If it happens to you, it's a flaw.
5548400	5550120	And crime is a flaw.
5550120	5552960	All these theft is a fraud.
5552960	5554040	Those are flaws.
5554040	5558320	If we could engineer those out, what would be the way that we do it?
5558320	5561560	Well, one of the things we do, we get rid of what it is to be a person.
5561560	5567080	Because what it is is corrupt people that go down these terrible paths and cause harm
5567080	5568080	to other people.
5568080	5569080	Right?
5569080	5574720	You're taking a step there that our ability to feel emotion and so on is a flaw.
5574720	5575720	No, I'm not.
5575720	5578560	I'm saying that it's the root of these flaws.
5578560	5583000	That greed and envy and lust and anger are the root.
5583000	5585520	I'd like to go to the bathroom.
5585520	5586520	Yeah.
5586520	5587520	Okay.
5587520	5588520	We'll come back.
5588520	5589520	We'll talk about flaws.
5589520	5590520	And we're back.
5590520	5592520	Provide an answer to that.
5592520	5604800	I mean, as I think about myself now, it's when I have emotions that are positive emotions,
5604800	5611880	like really getting off on a song or a picture or some new art form that didn't exist in
5611880	5614480	the past.
5614480	5615480	That's positive.
5616040	5627120	That's what I live for, relating to another person in a way that's intimate.
5627120	5633680	So I mean, the idea, if we're actually more intelligent, we'd not to get rid of that,
5633680	5639080	but to actually enjoy that to a greater extent.
5639080	5640560	Hopefully.
5640560	5643120	But what I'm saying is that...
5643320	5649520	Yes, there are things that can go wrong, but lead us in the incorrect direction.
5649520	5652400	I'm not even saying it's wrong.
5652400	5655080	I'm not saying that it's going to go wrong.
5655080	5662240	I'm saying that if you wanted to program away some of the issues that human beings have
5662240	5669000	in terms of what keeps us from working with each other universally, all over the globe,
5669000	5670440	what keeps us from these things?
5670560	5673160	We're actually doing that more than we used to do.
5673160	5675080	Sure, but also not.
5675080	5676760	We're also massive inequality.
5676760	5681280	You've got people in the Congo mining cobalt with sticks that powers your cell phones.
5681280	5683760	There's a lot of real problems with society today.
5683760	5685880	But there used to be even more of that.
5685880	5686960	There's a lot of that, though.
5686960	5688760	There's a lot of that.
5688760	5694600	If you looked at greed and war and crime and all the problems with human beings, a lot
5694600	5700240	of it has to do with these biological instincts, these instincts to control things.
5700800	5706400	Built-in genetic codes that we have that are from our ancestors.
5706400	5709120	That's because we haven't gotten there yet.
5709120	5709800	Right.
5709800	5718320	But when we get there, you think we will be a better version of a human being and we will
5718320	5724800	be able to experience all the good, the positive aspects of being a human being, the art and
5724800	5726400	creativity and all these different things.
5726400	5727720	Yeah, I hope so.
5727720	5735320	And actually, if you look at what human beings have done already, we're moving in that direction.
5735320	5736320	Right.
5736320	5738880	I may not seem that way.
5738880	5740840	No, it does seem that way to me.
5740840	5742320	It does overall.
5742320	5747160	But it's also like, if you look at a graph of temperatures, it goes up and it goes down
5747160	5748160	and it goes up and it goes down.
5748160	5750480	But it's moving in a general direction.
5750480	5753080	We are moving in a generally positive direction.
5753080	5757280	However, we want to continue moving in this same direction.
5757280	5759000	Yeah, I don't think the word...
5759000	5761200	It's not a guarantee.
5761200	5767360	You can describe things that would be horrible and it's feasible.
5767360	5769280	Yeah.
5769280	5773040	It could be the end of the human race, right?
5773040	5775960	Or it could be the beginning of the next race of this new thing.
5776480	5783600	Well, I mean, when I was born, we created nuclear weapons and people were concerned...
5783600	5791480	Very soon we had hydrogen weapons and we have enough hydrogen weapons to wipe out all humanity.
5791480	5794040	We still have that.
5794040	5802440	That didn't exist like a hundred years ago, well, it didn't exist 80 years ago.
5802440	5808960	So that is something that concerns me.
5808960	5811640	And you could do the same thing with the artificial intelligence.
5811640	5815080	It could also create something that would be very negative.
5815080	5821320	But what I'm getting at is like, what do you think life looks like if it's engineered?
5821320	5826960	What do you think human life looks like if it's engineered by a far superior intelligence?
5826960	5830400	And what would it change about what it means to be a person?
5830400	5839640	I mean, first of all, we would base it on what human beings are already.
5839640	5844360	So we'd become better versions of ourselves.
5844360	5851440	For example, we'd be able to overcome life-threatening diseases.
5851440	5852760	And we're actually working on that.
5852760	5856720	And that's going to go into high gear very soon.
5856720	5862760	Yes, but that's still being a human being.
5862760	5872920	If you're implementing large-scale artificial intelligence, you're essentially a superhuman.
5872920	5874040	You're a different thing.
5874040	5876880	You're not what we are.
5876880	5877880	If you have the computational power...
5877880	5882120	Well, if you're superhuman, you have the human being as part of it.
5883040	5884040	But this is the thing.
5884040	5889080	If you're engineering this artificial intelligence and you're engineering this with essentially
5889080	5894480	like a superior life form, it's going to look at it logically.
5894480	5900160	It's going to look at the issues that human beings have logically and say, well, we don't
5900160	5901160	need this.
5901160	5902160	This is a problem.
5902160	5905040	This is what we needed when we were primates, and we're not that anymore.
5905040	5906040	This new thing.
5906040	5907040	We're going to...
5907040	5908520	Who cares what the movie's like?
5908520	5912800	It's just a thing that's tricking your body into pretending that it's involved in drama,
5912800	5913800	but it's not really...
5913800	5917760	Well, you're making certain assumptions about what we'll create.
5917760	5921720	No, I'm just making an assumption.
5921720	5930360	I mean, in my mind, we would want to create better music and better art and better relationships.
5930360	5936320	Well, the relationships should be all perfect eventually if we keep going in this general
5936320	5937320	direction.
5937320	5938320	Yeah, perfect.
5938320	5939320	I mean...
5939320	5942840	But if you get artificial intelligence, we're all reading each other's minds and everyone's
5942840	5944320	working towards the same goal.
5944320	5946320	Well, no, you can't read each other's minds.
5946320	5947320	Ever?
5947320	5948320	I mean, we can create...
5948320	5954320	Yes, we can create privacy that's virtually unbreakable, and you could keep the privacy
5954320	5955320	to yourselves.
5955320	5957600	But can you do that as technology scales upward?
5957600	5961720	If it continues to move, I mean, it's difficult like your phone.
5961720	5963200	Anyone can listen to you on your phone.
5963200	5965400	I mean, anyone who has a significant technology...
5965400	5968560	Actually, it has pretty good technology already.
5968560	5971240	You can't really read someone else's phone.
5971240	5972240	You're definitely good.
5972240	5977040	Yeah, if you have Pegasus, you could hack into your phone easily, not hard at all.
5977040	5979840	The new software that they have, all they need is your phone number.
5979840	5983440	All they need is your phone number, and they can look at every text message you send, every
5983440	5988400	email you send, they can look at your camera, they can turn on your microphone, easy.
5988400	5993160	We have ways of keeping total privacy, and if it's not built into your phone now, it
5993160	5994160	will be.
5994320	5996960	But it's definitely not built into your phone now.
5996960	6002560	The security people that really understand the capabilities of intelligence agencies,
6002560	6004640	they 100% can listen to your phone.
6004640	6006360	100% can turn on your camera.
6006360	6008760	100% can record your voice.
6008760	6010840	Yes and no.
6010840	6017560	I mean, we have an ability to keep total privacy in a device.
6017560	6019040	But from who?
6019040	6022480	You can keep privacy from me, because I don't have access to your device.
6022480	6026680	But if I was working for an intelligence agency and I had access to a Pegasus program,
6026680	6028600	I am in your device.
6028600	6031320	Now, I've talked to people.
6031320	6032760	Only because it's not perfect.
6032760	6037480	We can actually build much better privacy than exists today.
6037480	6041640	But the privacy that we have today is far less than the privacy that we had before we
6041640	6042640	had phones.
6042640	6047360	I don't really quite agree with that.
6047360	6049560	How so?
6049560	6054280	If you didn't have a phone and you were at home having a conversation, a sensitive conversation
6054280	6057920	about maybe you didn't pay as much taxes as you should, there's no way anybody would
6057920	6059200	hear that.
6059200	6060520	But now your phone hears that.
6060520	6064120	If you have an Alexa in your home, your Alexa hears you say that.
6064120	6072640	People have been charged with crimes because Alexa heard them committing murder.
6072640	6078480	We actually know how to create perfect privacy in your phone.
6078480	6083200	If your phone doesn't have that, that's just an imperfection in the way we're building
6083200	6084400	these things now.
6084400	6085680	But it's not just an imperfection.
6085680	6090040	It's sort of built into the program itself, because that's what fuels the algorithm, is
6090040	6092480	that it has access to all of your data.
6092480	6097320	It has access to all of what you're interested in, what you like, what you don't like, you
6097320	6098320	can opt out of it.
6098320	6099320	Especially you.
6099320	6100320	You've got a Google phone.
6100320	6105520	It's just a net scooping up information.
6105520	6110720	We know how to build perfect privacy.
6110720	6122920	How do we do it?
6122920	6128000	I mean if it's not built into your phone now, it should be.
6128000	6130960	Because they don't want it to be built in there, because there's an actual business
6130960	6133160	model and it not being built in there.
6133160	6140920	Okay, but it can be done, and if people want that, it'll happen.
6140920	6143840	But you recognize the financial incentive in not doing that, right?
6143840	6148120	Because that's what a company like Google, for instance, that's where they make the majority
6148120	6154560	of their money is from data, or a lot of their money, I should say.
6154560	6165960	There's actually a lot of effort that goes into keeping what's on your phone private.
6165960	6166960	It's not that easy.
6166960	6171320	Private from some people, but not really private.
6171320	6173800	It's only private until they want to listen.
6173800	6178600	And now the capability of listening to your phone is super easy.
6178600	6179600	Not really.
6179600	6180600	No?
6180600	6182160	With the Pegasus program?
6182160	6184280	It's very easy.
6184280	6188000	Well that has to do with imperfections in the way phones are created.
6188000	6190320	Right, but I think it's a feature.
6190320	6195520	I think part of the feature is that they want as much data from you, and knowing about what
6195520	6196840	you're doing, what you're talking about.
6196840	6205120	Have you ever had a conversation with someone and you see an ad for that thing on Google?
6205120	6206120	That happens.
6206120	6207800	Yes, but...
6207800	6212840	So something's going on where it's listening to your conversations.
6213120	6214680	Picking up on keywords.
6214680	6216840	It's not picking up on everything.
6216840	6217840	Not yet.
6217840	6218840	Well, it's not unless it wants to.
6218840	6223180	Like I said, if they're using a program, an intelligence program, to gather information
6223180	6224880	from your phone, it is.
6224880	6225880	And you're basically...
6225880	6230040	You got a little spy that you carry around with you everywhere you go.
6230040	6231040	Unless you're using...
6231040	6237280	I mean, if you think that's a major issue, we could build phones that are impossible
6237280	6241240	to spy on.
6241240	6242760	Maybe.
6243680	6247520	Well, there are some phones that like graphing, do you know about that?
6247520	6252600	Do you know about people that they take a Google phone and they put a different Linux-based
6252600	6256400	operating system on it, makes it much more difficult to track, and there's multi-levels
6256400	6257400	of protection.
6257400	6262000	There's a bunch of phones that are being made that are security phones.
6262000	6264240	But you lose access to apps.
6264240	6268720	You lose access to a lot of the features that people rely on when it comes to phones.
6268720	6272560	For instance, like if you have GPS on your phone, as soon as you're using GPS, you're
6272560	6273560	easy to find.
6273560	6274560	Right?
6274560	6275560	So you lose that privacy.
6275560	6279480	If they want to know where Ray's phone is, they know exactly where Ray's phone is.
6279480	6283640	And that's where you are, and you're with your phone, they've got you tracked everywhere
6283640	6284640	you go.
6284640	6285640	It's complicated.
6285640	6289400	If this were a major issue, we could definitely overcome that.
6289400	6294920	I think it's a major issue, but I don't think it's a major concern for most people.
6294920	6297480	But it's because they reap the benefits of it.
6297480	6300440	Like the algorithm is specifically tailored to their interests.
6300440	6304200	That's how we find the kinds of things we put on phones.
6304200	6305200	Right.
6305200	6308960	But you can't opt out of it unless you just decide to get a flip phone.
6308960	6313000	But even if you do, they figure out where you are to triangulate you from cell phone
6313000	6317200	towers.
6317200	6323080	I mean, we give up certain things in order to get the benefits of phones.
6323080	6327240	Yeah, we do.
6327240	6332320	If what you're giving up is a grave concern, we could overcome that.
6332320	6335240	We know how to do that.
6335240	6337800	Yeah.
6337800	6344520	If people agree that the benefit of overcoming that outweighs the loss in the financial loss
6344520	6347800	that you would have with not having access to everybody's data and information.
6347800	6353680	Well, I mean, what you're giving up is a certain type of data that you want, a certain
6353680	6364200	type of capability that you could buy, so they can advertise that to you and people
6364200	6370120	feel that that's okay.
6370120	6378000	But for example, keeping your email private is quite feasible.
6378000	6381680	It's possible, but it's also easy to hack.
6381680	6383360	People could be reading your emails all the time.
6383360	6388040	We should probably assume that they do.
6388040	6400920	Well, it's a complicated issue, but we keep, for example, your emails private, and generally
6400920	6403480	we actually do do that.
6403480	6405360	Generally, for most people.
6405360	6411320	But my point is, as this technology scales upward, when you have greater and greater
6411360	6417880	computational power, and then you're also integrated with this technology, how does that
6417880	6426360	keep whatever group is in charge from being able to essentially access the thing that
6426360	6429920	is inside your head now?
6429920	6436400	If you have a technology that's going to be upgraded and you're going to get new software
6436480	6442840	that's going to keep improving as time goes on, what kind of privacy would be involved
6442840	6446040	in that if you're literally having something that can get into your brain?
6446040	6450280	If most people can't get into your brain, can intelligence agencies get into your brain?
6450280	6453840	Can foreign governments get into your brain?
6453840	6455680	What does that look like?
6455680	6457160	I'm not looking at this as a negative.
6457160	6462720	I'm just saying, if you're just looking at this completely objectively, what are the
6462720	6465240	possibilities that this could look like?
6465280	6468960	I'm trying to paint a weird picture of what this could look like.
6468960	6472760	Well, a lot of things you want to share.
6472760	6479760	Music and so on, it's desirable to share that, and you'd want that to be shared.
6479760	6483160	If you didn't share anything, you'd be pretty lonely.
6483160	6484160	Sure.
6484160	6489280	What do you think about the potential for a universal language?
6490200	6497200	One of the things that holds people back is the Rosetta Stone, the Tower of Battle.
6497200	6500640	The idea that we can't really understand what all these other people are saying.
6500640	6501640	We don't know how they think.
6501640	6508160	If we can develop a universal worldwide language through this, do you think it's feasible?
6508160	6510800	All languages that we have were created.
6510800	6514560	We have a certain means of changing one language into another.
6514560	6515560	Right.
6515560	6516560	That's what I'm saying.
6516720	6520640	We're doing that now with some, like Google does that with Translate, and the new Samsung
6520640	6523040	phones do that in real time.
6523040	6524040	Yeah.
6524040	6532040	I wrote about that in 1989, that we'd be able to have universal translation between languages.
6532720	6536200	But do you think that the adoption of a universal language?
6536200	6538000	It's not perfect, but it's actually pretty good.
6538000	6539080	It's pretty good.
6539080	6545960	But there's also context that's missing, because there's different cultural significance.
6546000	6548960	There's different ways that people say things.
6548960	6554040	There's gendered language and other nationalities used and other countries used.
6554040	6557680	Well, you could try to get that into the language translation as well.
6557680	6559400	You can, but it's a little bit imperfect, right?
6559400	6560840	This is what I'm saying.
6560840	6565520	You might have something that's said very quickly, and you'd have to translate it into
6565520	6570000	much longer language in order to capture that.
6570000	6575480	But would a universal language be possible?
6575480	6577960	If you're creating something...
6577960	6579320	Why would you need that?
6579320	6585400	Because what we have, all of our language, is pretty flawed, ultimately.
6585400	6589200	We use it, but how many versions of your do we have?
6589200	6593480	There's a bunch of different weird things about language that's imperfect, because it's
6593480	6594480	old.
6594480	6596160	It's like old technology.
6596160	6603560	If we decided to make a better version of language through artificial technology and
6603560	6608240	say, listen, instead of trying to translate everything, now that we're super powerful
6608240	6613120	intelligent beings that are enhanced by artificial intelligence, let's create a better, more
6613120	6616720	superior, universally adopted language.
6616720	6617720	Maybe.
6617720	6621920	Do you see that as a major need?
6621920	6622920	Yeah, I do.
6622920	6623920	Yeah.
6623920	6624920	I think that would change a lot.
6624920	6630880	I mean, we'd lose all the amazing nuances of cultures, which I don't think is good for
6630880	6634280	us as human beings, but we're not going to be human beings.
6634280	6639960	So maybe it would be better if we could communicate exactly the way we prefer to.
6639960	6641720	Well, we would be human beings.
6641720	6648000	And in my mind, the human being is someone who can change both ourselves and means of
6648000	6661760	communication to enjoy better means of expressing art and culture and so on.
6661760	6665760	No other animal really quite does that, except human beings.
6665760	6670840	So that is an essence of what it means to be a human being.
6670840	6671840	For now.
6671840	6676000	But when you're a mind reading eagle and you're flying around, are you really a human being
6676000	6677000	anymore?
6677000	6680640	Yes, because we are able to change ourselves.
6680640	6684080	So that's just a new definition of what a human being is.
6684080	6690800	What are your thoughts on simulation theory?
6690800	6700440	If you mean that we're living in a simulation, well, first of all, some people believe that
6700440	6724360	we can express physics as formulas and that the universe is actually able to, is capable
6724360	6732800	of computation and therefore everything that happens is the result of some computation.
6732800	6745680	And therefore, the universe is capable of, we are living in something that is computable.
6745680	6751800	And there's some debate about whether that's feasible, but that doesn't necessarily mean
6752720	6754520	that we're living in a simulation.
6754520	6762760	Generally, if you say we're living in a simulation, you assume that some other place and teenagers
6762760	6767880	in that world like to create a simulation.
6767880	6773400	So they created a simulation that we live in and you want to make sure that they don't
6773400	6778680	turn the simulation off, so we'd have to be interesting to them, and so they keep the
6778680	6782960	simulation going.
6782960	6793520	But the whole universe could be capable of simulating reality and that's what we live
6793520	6800520	in and it's not a game, it's just the way the universe works.
6800520	6805400	I mean, what would the difference be if we lived in a simulation?
6806120	6812360	This is what I'm saying, if we can and we're on our way to creating something that is indiscernible
6812360	6817520	from reality itself, I don't think we're that far away from that, many decades away from
6817520	6823440	having some sort of a virtual experience that's indiscernible from regular reality.
6823440	6826960	We try to do that with games and so on.
6826960	6832720	And those are far superior to what they were, I mean, I'm younger than you, but I can remember
6833640	6836000	Pong, it was groundbreaking.
6836000	6839680	You could play a video game on your television, this is crazy, it was so nuts.
6839680	6841480	And we're way beyond that now.
6841480	6847520	Now you look at the Unreal 5 engine, it's insane how beautiful it is and how incredible
6847520	6848800	and what the capabilities are.
6848800	6852480	So if you live in that, that's kind of a simulation that we live in.
6852480	6856400	Right, but as you expand that further and you get to the point where you're actually
6856400	6863320	in a simulation and that your life is not this carbon based biological life, feeling
6863320	6868160	and texture that you think it is, whether you're really a part of this thing that's
6868160	6873040	been created, this is where it gets real weird with like probability theory, right?
6873040	6880040	Because they think that if a simulation is possible, it's more likely it's already happened.
6880960	6887960	I mean, there's really an unlimited amount of things that we could simulate and experience
6887960	6894960	and so it's hard to say we're living in a simulation because a lot of what we're doing
6894960	6903960	is it's living in a computational world anyway, so it's basically being simulated in a way.
6904960	6911960	And if you were some sort of an alien life form, wouldn't that be the way you go, instead
6912400	6919400	of like taking physical metal crafts and shooting them off into space, wouldn't you sort of
6920760	6927240	create artificial space, create artificial worlds, create something that exists in the
6927240	6931840	sense that you experience it and it's indiscernible to the person experiencing it.
6931840	6936320	So if you're intelligent enough, you'll be able to tell what's being simulated and what's
6936320	6937320	not.
6937320	6943320	Up to a point, until it actually does all the same things that regular reality does,
6943320	6949440	it just does it through technology and maybe that's what the universe is.
6949440	6956440	But that's okay, we could still experience what's happening and we could also experience
6957200	6964200	people doing galaxy-wide engineering, which not all of which would be simulated.
6964400	6969360	So the galaxy-wide engineering is the main thing that you look at to the point where
6969360	6974000	I don't see any evidence for life outside. Well, there's definitely no real evidence
6974000	6979560	that we see other than these people that talk about UFOs, UAPs and pilots and all these
6979560	6981000	people that say that there's these things.
6981080	6990080	We don't see any evidence that life is simulated outside of our own life. We can simulate things
6990160	6997160	and experience it. We don't see any evidence that other beings are doing that elsewhere.
6999080	7001480	This is based on such limited data though, right?
7001480	7007480	I mean, look at what limited data we just have of Mars, rover rolling around, satellites
7007480	7013040	in orbit. It's very limited data with something that's just one planet over. We don't really
7013040	7016040	have the data to understand what's going on in Alpha Centauri.
7016040	7023040	It's possible that this simulated life elsewhere. I mean, we don't see any evidence for it,
7024920	7026600	but it's possible.
7026600	7029640	Is it something that intrigues you or do you just look at it like there's no evidence
7029640	7031840	so I'm not going to concentrate on that?
7032800	7038280	I'm very interested to see what we can achieve because we're actually, I can see that we're
7038280	7045280	on that path. So it doesn't take a lot of curiosity in my part to imagine other people
7051440	7058440	simulating life and enjoying it. I'm much more interested to see what will be feasible
7059280	7064760	for us and we're not that far away from it.
7064760	7071260	So over the next four years, five years, you think we're going to be able to far surpass
7071260	7077660	the ability of human beings. We're going to be able to stop aging and then eventually
7077660	7084660	reverse aging, and then 2045 comes along. What does that look like?
7085180	7091780	Well, one of the reasons we call it singularity is because we really don't know. I mean, that's
7091780	7098780	why it's called a singularity. Singularity in physics is where you have a black hole,
7100420	7105060	no energy can get out of a black hole, and therefore we don't really know what's going
7105060	7110940	on in it and we call it a singularity. So this is a historical singularity based on
7110940	7116460	the kinds of things we've been talking about. And again, we don't really know what that
7116460	7121460	will be like and that's why we call it a singularity.
7121460	7124460	Do you have any theories?
7124460	7131460	Another way of looking at it, I mean, we have mice and they have experiences. It's a limited
7131460	7138460	amount of complexity because that particular species hasn't really evolved very much. And
7140860	7147860	we'll be going beyond what human beings can do. So to ask a human being what it's like
7148060	7155060	to be a human being, what it's like to be a human being, what it's like to be a human
7161600	7167180	being in singularity, it's like asking a mouse, what would it be like if you were to
7167180	7173940	evolve to become like a human? Now, if you ask a mouse that, it wouldn't understand
7173940	7179380	the question, it wouldn't be able to formulate an answer, it wouldn't even be able to think
7179380	7186380	about it. And asking a card human being what it's going to be like to live in a singularity
7186740	7192300	is a little bit like that.
7192300	7197180	So it's just who knows? It's going to be wild.
7197180	7200860	Be able to do things that we can't even imagine today, right?
7200860	7207340	Well, I'm very excited about it. Even though it's scary, I know I ask a lot of tough questions
7207340	7211020	about this because these are my own questions. This is like what bounces around inside my
7211020	7212020	own head.
7212020	7218620	Well, that's why I'm excited about it also because it basically means more intelligence
7218620	7222980	and we'll be able to think about things that we can't even imagine today.
7222980	7224660	And solve problems.
7224660	7229660	Yes, including like dying, for example.
7229660	7235660	Well, listen, man, I'm glad you're out there. It's very important that people have access
7235660	7240020	to this kind of thinking and you've dedicated your whole life to this. In this book, Ray
7240020	7244380	Kurzweil, the Singularity is Near When We Merge with AI. It's available now. Did you
7244380	7247940	do the audio version of it?
7247940	7249500	That's being worked on now.
7249500	7250500	Are you doing it?
7250500	7253500	It's coming out June.
7253500	7254500	No.
7254500	7259500	No? I want to hear it in your voice. It's your words.
7259500	7261500	Yeah, that's what people say.
7261500	7265140	Yeah, why don't you do it? You should do it. You know what you should do? Just get AI to
7265140	7269500	do it. Why waste all that time sitting around doing it? Basically, they could do it now.
7269980	7270980	They did that yesterday.
7270980	7276500	100%. Look, and they could take your voice from this podcast and do this book in an
7276500	7282540	audio version. Easy. Do you know what they're doing now? It's Spotify. They're translating
7282540	7288580	this podcast. They're going to translate it to German, French, and Spanish. And it's
7288580	7292900	going to be like your voice in perfect Spanish, my voice in perfect Spanish.
7292900	7296220	This actually came up yesterday. I'll think about that.
7296220	7297220	Pretty wild.
7297220	7298220	Yeah.
7298220	7299220	It's 100%. You should do that.
7299420	7303620	My friend Duncan does that all the time. He'll have friends, text friends, or send a voice
7303620	7307980	message as a fake voice message. That's ridiculous. You know, talking about how he's marrying his
7307980	7312820	cat or something like that. It's just like, just, but he does it with AI and it sounds
7312820	7315980	exactly like whoever that person is.
7315980	7320940	So that's the, that's the solution. Have AI read your, of course you should have AI read
7320940	7326940	your book. I can't believe we even would think of you sitting down for 40 hours or whatever
7327060	7332140	it would take. It'd probably take more than that to read this whole book. And then if
7332140	7334140	you mess up, you got to go back and start again.
7334140	7338540	Well, certainly that's going to be feasible. Whether that's feasible now, they could get
7338540	7340540	all the nuances correct.
7340540	7342540	I bet it's pretty close.
7342540	7343540	Yeah.
7343540	7344540	I bet it's pretty close right now.
7344540	7348540	But it has to be very close because we're doing it like in the next month or so.
7348540	7351540	I bet. Don't you think they could do it, Jamie?
7351540	7355660	Yeah. I think they could do it right now. Listen, Ray, I appreciate you very much. Thank
7355740	7359540	you very much for being here. Thank you for your time. And thank you for this book. When
7359540	7361540	is it available?
7361540	7362540	June 24th.
7362540	7364540	June, I got an early copy, kids.
7364540	7365540	Yeah.
7365540	7366540	Thank you, sir. Really appreciate you. Thank you very much.
7366540	7367540	My pleasure.
7367540	7376540	Bye, everybody.
