WEBVTT

00:00.000 --> 00:06.000
Let me try for you. Okay.

00:06.000 --> 00:28.000
Thank you.

00:28.000 --> 00:41.000
Thank you.

00:58.000 --> 01:18.000
Thank you.

01:28.000 --> 01:43.000
Hello and welcome.

01:43.000 --> 01:49.000
Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel

01:49.000 --> 01:53.000
Tremblay. We both work for the Meteorological Service of Canada within

01:53.000 --> 01:58.000
Environment and Climate Change Canada and we're happy to be here exploring AI

01:58.000 --> 02:04.000
with you for a second year. Before we begin, a few reminders. 15 minutes are

02:04.000 --> 02:08.000
reserved for each presentation and we encourage our presenters to keep three

02:08.000 --> 02:12.000
minutes for questions at the end. Time permitting participants with questions

02:12.000 --> 02:17.000
will be able to use the chat at the end of the presentation and their questions

02:17.000 --> 02:25.000
can be upvoted by the other attendees. Miguel will provide up at 10 and 12

02:25.000 --> 02:30.000
minutes and will bring the presentation to an end at 15 and will be ruthless

02:30.000 --> 02:35.000
guys because we want everyone to have time to present. Miguel, do you want to do

02:35.000 --> 02:58.000
a quick recap in French?

02:58.000 --> 03:04.000
A few reminders. 15 minutes are reserved for each presentation and we encourage

03:04.000 --> 03:16.000
our presenters to keep time for questions at the end of the presentation and we

03:16.000 --> 03:22.000
will be able to use the chat at the end of the presentation and we will be able to

03:22.000 --> 03:27.000
use the chat at the end of the presentation and we will be able to use the chat at

03:27.000 --> 03:32.720
the end of the presentation and we will be able to use the chat

03:42.000 --> 03:51.000
fpspe conditional 10 minutes for one and a half minutes for 10 minutes for

03:51.000 --> 03:55.400
weather forecasting, and ECCC's research plans.

03:55.400 --> 03:58.600
So without further ado, over to you, Christopher.

03:58.600 --> 03:59.440
Thank you.

03:59.440 --> 04:02.540
Let me get the screen sharing going and share.

04:03.880 --> 04:06.400
Okay, yep, looks like we're good.

04:06.400 --> 04:07.760
Okay, hello everybody.

04:07.760 --> 04:08.600
I'm Christopher Subic

04:08.600 --> 04:10.200
from Environment and Climate Change Canada.

04:10.200 --> 04:12.040
I'm here to speak as said about,

04:14.400 --> 04:16.080
well, click, okay.

04:16.080 --> 04:17.760
This talk is essentially in two parts.

04:17.760 --> 04:20.120
The first part is going to be a brief summary

04:20.120 --> 04:23.520
of the current state of machine learning based forecasting

04:24.640 --> 04:29.400
with a broad focus on medium range weather forecasts.

04:30.200 --> 04:33.800
Second part of this talk will be the forthcoming AI roadmap

04:33.800 --> 04:36.520
out of the atmospheric science

04:36.520 --> 04:38.280
of technology director at NCCMAP,

04:39.280 --> 04:42.640
which effectively broadly sets out

04:42.640 --> 04:46.080
where this part of Environment and Climate Change Canada

04:46.080 --> 04:49.920
will be going in the short of medium term future with AI.

04:50.240 --> 04:51.840
And finally, time permitting,

04:51.840 --> 04:55.040
I'll conclude with a preview of talks to come

04:55.040 --> 05:00.040
and just general thoughts on the state of AI in forecasting.

05:00.240 --> 05:02.400
So in case you haven't noticed,

05:02.400 --> 05:04.040
AI is becoming a big deal.

05:04.040 --> 05:06.040
It's no longer just pictures of cats

05:06.040 --> 05:08.040
or helping kids cheat on their homework,

05:08.040 --> 05:12.880
but it's starting to influence the industries

05:12.880 --> 05:16.120
and feels previously thought unassailable.

05:17.120 --> 05:20.920
Now, this isn't truly a stranger to weather forecasting.

05:20.920 --> 05:23.560
AI adjacent topics have long had roles

05:23.560 --> 05:27.760
in the forecast production system.

05:28.800 --> 05:30.640
We've long had statistical models

05:30.640 --> 05:32.880
for quality control of observational data

05:32.880 --> 05:34.680
and for post-processing,

05:34.680 --> 05:38.640
but a phase change has been that over the past two years,

05:38.640 --> 05:42.360
maybe three models from the academic and private sectors

05:42.360 --> 05:45.800
have gone from things that we should probably

05:45.800 --> 05:47.840
keep our eye on as interesting things

05:47.840 --> 05:50.120
for the long-term future to, well,

05:50.120 --> 05:54.000
these two nearly full-featured forecast systems

05:54.000 --> 05:56.280
that are competitive with the state of the art.

05:58.040 --> 06:01.000
With that in mind, I can just state very plainly

06:01.000 --> 06:02.920
that Environment Climate Change Canada,

06:02.920 --> 06:07.040
in particular my part of it, is taking AI very seriously

06:07.040 --> 06:09.360
and we intend for it to have an increasing role

06:09.360 --> 06:13.480
in numerical weather prediction systems going forward.

06:13.600 --> 06:16.400
We're taking a sort of trust but verify approach

06:16.400 --> 06:19.760
in that we intend to make AI advances operational,

06:19.760 --> 06:21.880
but only after they've been scientifically proven.

06:21.880 --> 06:26.880
We're not in a rush to perform science by press release

06:27.480 --> 06:31.440
and this is also a medium to long-term plan.

06:31.440 --> 06:33.680
Capacity building is going to take years,

06:33.680 --> 06:37.920
both in terms of acquiring sufficient computational power

06:37.920 --> 06:40.080
for this and developing the human expertise

06:40.080 --> 06:41.880
necessary to properly use it.

06:43.600 --> 06:48.600
The modern AI forecast field essentially began

06:49.720 --> 06:50.680
about two years ago.

06:50.680 --> 06:54.000
I dated from the first publication of GraphCast

06:54.000 --> 06:57.320
by Lam in 2023, which is the first time

06:57.320 --> 07:01.680
that an AI model could truly claim to beat

07:01.680 --> 07:04.840
the state of the art from, in this case,

07:04.840 --> 07:07.920
the equivalent forecast from ECMWF.

07:10.320 --> 07:12.720
In addition, the truly shocking claim

07:12.760 --> 07:14.640
was that these forecasts came with orders

07:14.640 --> 07:16.720
of magnitude faster running time.

07:16.720 --> 07:19.360
GraphCast will produce a 10-day quarter-degree forecast

07:19.360 --> 07:22.000
in about 30 seconds on a single GPU.

07:22.920 --> 07:26.840
That put all of the weather centers globally on notice

07:26.840 --> 07:30.080
that if these trends continue, we must adapt

07:30.080 --> 07:31.280
or we'll fall behind.

07:31.280 --> 07:34.080
And I mean, in some sense, research has always been that.

07:34.080 --> 07:37.320
You can't sit on your laurels from 20 years ago

07:37.320 --> 07:38.600
and pretend to be competitive,

07:38.640 --> 07:43.640
but this field is truly advancing at a revolutionary rate.

07:48.800 --> 07:51.320
Models are being constantly released.

07:51.320 --> 07:53.320
Any summary, like the one I'm about to give

07:53.320 --> 07:54.680
was going to be out of date within weeks.

07:54.680 --> 07:57.320
In fact, just this morning, I've had to update my slides

07:57.320 --> 07:59.600
to add two new preprints that have come out

07:59.600 --> 08:03.320
in the past four or five days.

08:03.320 --> 08:05.120
Nonetheless, there are some common features

08:05.120 --> 08:07.880
between medium-range models that are shared

08:07.880 --> 08:09.920
by, if not all of them, then most of them.

08:11.200 --> 08:13.520
Most broadly speaking, these models attempt to learn

08:13.520 --> 08:15.480
from data, which means rather than simulate

08:15.480 --> 08:17.800
the atmosphere from first principles,

08:17.800 --> 08:22.600
they try to look at some form of data

08:22.600 --> 08:24.920
and predict what it will become.

08:24.920 --> 08:26.800
In this case, for the medium-range forecasting,

08:26.800 --> 08:29.360
the data is almost always the error of five reanalysis.

08:29.360 --> 08:31.640
That's our highest quality, longest-term,

08:31.640 --> 08:33.960
and most uniform record of the atmospheric state

08:33.960 --> 08:37.240
that we have, and it's accepted as ground truth

08:37.240 --> 08:39.840
for most of the AI models, but this data set

08:39.840 --> 08:42.960
does have known issues such as relatively poor precipitation.

08:44.880 --> 08:47.880
The other common feature about AI forecast models

08:47.880 --> 08:50.600
is that most of them have sparse limited outputs.

08:50.600 --> 08:53.600
They tend to predict only a few variables.

08:53.600 --> 08:57.080
They tend to predict a small subset of vertical levels

08:57.080 --> 09:00.160
compared to what we're used to from an operational forecast,

09:00.160 --> 09:02.960
and they have a limited selection of lead times.

09:04.320 --> 09:06.440
This creates new downscaling problems.

09:06.440 --> 09:08.720
If you were in the plenary talk,

09:08.720 --> 09:11.600
you heard our friend from Nvidia say that

09:13.280 --> 09:16.600
this might not be a barrier to full prediction,

09:16.600 --> 09:19.000
but at the same time, we're used to having the prediction

09:19.000 --> 09:21.240
plus all of the rich output for free,

09:21.240 --> 09:25.440
and not having that means we'll need to contemplate

09:25.440 --> 09:27.160
new kinds of downscaling problems

09:27.160 --> 09:31.520
to get rich data from a sparse forecast.

09:31.520 --> 09:33.800
And finally, these AI forecast systems

09:33.800 --> 09:37.440
have essentially no proofs of physical consistency.

09:37.440 --> 09:42.440
Even hybrid models and development in that area

09:42.760 --> 09:44.600
has a classical dynamical core,

09:44.600 --> 09:46.920
but leaves a physics system that is

09:48.080 --> 09:49.920
only tangentially concerned with things

09:49.920 --> 09:51.000
like conserving energy.

09:52.560 --> 09:55.040
Now, medium range forecasts tend to break down

09:55.040 --> 09:56.360
into a few different categories.

09:56.360 --> 09:59.080
The first and most conventional of these

09:59.080 --> 10:00.960
are deterministic models.

10:00.960 --> 10:02.440
These are analysis predictors,

10:02.440 --> 10:04.320
and they essentially answer the question of,

10:04.320 --> 10:06.960
if I have the atmospheric analysis at time zero,

10:06.960 --> 10:08.680
what is the minimum error prediction

10:08.680 --> 10:11.560
of what that analysis is going to be six hours from now?

10:12.600 --> 10:14.120
As a general rule, these tend to give

10:14.120 --> 10:17.480
overly smooth forecasts that over long lead times,

10:17.480 --> 10:19.880
erase fine scale structure in the atmosphere.

10:19.880 --> 10:22.880
The widely held belief is that this is a consequence

10:22.880 --> 10:27.880
of training based on mean squared error measures,

10:29.040 --> 10:31.080
because the lowest mean squared error prediction

10:31.080 --> 10:34.000
in the future is your ensemble average.

10:34.000 --> 10:35.520
But the ensemble average is not

10:35.520 --> 10:37.840
a physically plausible forecast.

10:39.520 --> 10:42.640
Now, that being said, these models have shown great success,

10:42.640 --> 10:45.880
and having a really good ensemble mean prediction

10:45.880 --> 10:47.640
of the future is still a really good prediction

10:47.640 --> 10:48.480
of the future.

10:48.480 --> 10:51.120
Each day of predictability translates to billions

10:51.120 --> 10:55.440
or billions of dollars of enabled economic activity.

10:56.440 --> 11:01.280
This category of models is in some sense the oldest,

11:01.280 --> 11:03.080
and I really hesitate to use that word

11:03.080 --> 11:05.200
with a field that's about two or three years old,

11:05.200 --> 11:08.120
but they're models from many different groups.

11:08.120 --> 11:11.160
Graphcast and ForecastNet have been previously mentioned.

11:11.160 --> 11:12.680
Graphcast is a graph neural network.

11:12.680 --> 11:16.600
ForecastNet is now a spectral Fourier neural operator

11:16.600 --> 11:20.840
that operates in a spherical harmonic space.

11:20.840 --> 11:23.040
Pangu weather came out as a vision transformer,

11:23.040 --> 11:26.720
and AIFS is now apparently officially published

11:26.720 --> 11:29.120
with a pre-print, and it's a graph transformer.

11:31.360 --> 11:33.360
The jargon here is not that important,

11:33.360 --> 11:36.760
but the underlying point is that you can reach

11:36.760 --> 11:38.120
a deterministic forecast

11:38.120 --> 11:41.320
with many different AI architectures.

11:41.320 --> 11:45.760
There's, as of yet, no specific Royal Road to a forecast.

11:46.880 --> 11:49.120
The second category that I've somewhat arbitrarily divided

11:49.120 --> 11:51.200
things into are ensemble models.

11:51.200 --> 11:54.160
These try to address the problem of overly smooth forecast

11:54.160 --> 11:56.840
by adding some element of randomness.

11:56.840 --> 11:58.840
An ensemble forecaster will receive

11:58.840 --> 12:00.520
some kind of random data source,

12:00.520 --> 12:03.840
either a random number generator or a field of noise,

12:03.840 --> 12:06.640
and it's asked to generate essentially different forecasts

12:06.640 --> 12:08.520
from the same seven initial conditions.

12:08.520 --> 12:13.480
This more or less solves the problem of smoothing

12:13.480 --> 12:15.480
based on mean squared error loss functions

12:15.480 --> 12:19.360
because each individual forecast no longer has to track

12:19.360 --> 12:24.720
the long-term forecast.

12:24.720 --> 12:26.440
No longer has to track the long-term truth,

12:26.440 --> 12:27.840
but each one can be plausible,

12:27.840 --> 12:31.000
and then you're tracking the future

12:31.000 --> 12:32.840
with a true ensemble mean.

12:32.840 --> 12:34.560
Now, the downside is that these models

12:34.560 --> 12:36.560
are all more expensive to train and run

12:36.560 --> 12:39.320
the deterministic systems of the same size.

12:39.320 --> 12:42.560
In operations, you will need at least one inference

12:42.560 --> 12:44.120
run per ensemble member.

12:44.120 --> 12:49.040
So, graphcast's 30-second, 10-day forecast is great,

12:49.040 --> 12:51.240
but if I want a 100-member ensemble,

12:51.240 --> 12:54.520
now I'm talking an hour or two,

12:54.520 --> 12:58.000
and that can add up very quickly.

12:58.000 --> 12:59.720
In terms of training, if you're training

12:59.720 --> 13:01.080
with an ensemble error measure

13:01.080 --> 13:04.080
like the cumulative rank probabilistic score,

13:04.080 --> 13:06.040
that requires training over an ensemble,

13:06.040 --> 13:09.120
and increasing the size of things during training

13:09.120 --> 13:13.600
tends to increase the scarce GPU memory requirements

13:13.600 --> 13:18.280
and the overall cost of building the system.

13:18.280 --> 13:20.160
This is a newer frontier of AI forecasting.

13:20.160 --> 13:22.960
It's probably going to be more popular in the months to come,

13:22.960 --> 13:25.960
and the examples here are also somewhat newer.

13:25.960 --> 13:28.400
Gencast was previously mentioned in our plenary.

13:28.400 --> 13:31.400
It's a diffusion model based on graph transformer network.

13:31.400 --> 13:35.560
Neural GCM is a hybrid model, also previously mentioned,

13:35.560 --> 13:37.800
and it's a dynamical core

13:37.800 --> 13:39.920
that has learned physics parameterizations,

13:39.920 --> 13:42.000
and in particular, can operate in a non-subtle mode,

13:42.000 --> 13:43.680
which is why I've included it here.

13:43.680 --> 13:46.280
And finally, there are models such as seeds,

13:46.280 --> 13:49.400
also out of Google, that don't try to forecast directly,

13:49.400 --> 13:53.160
but they try to take an ensemble mean

13:54.280 --> 13:55.800
that already exists from some method,

13:55.800 --> 13:57.280
like a traditional forecasting system,

13:57.280 --> 13:59.000
and generate new ensemble members

13:59.000 --> 14:01.520
to fill out the probability distributions.

14:02.600 --> 14:04.760
The final category of forecast models,

14:04.760 --> 14:07.640
and I'm going to divide things into our foundation models,

14:07.640 --> 14:09.200
and these essentially answer the question of,

14:09.200 --> 14:12.320
what if we had GPT-4, but for weather?

14:12.320 --> 14:15.400
The idea here is and shared by foundation models

14:15.400 --> 14:16.680
that exist in our development,

14:16.680 --> 14:19.880
is that you break up a weather state,

14:19.880 --> 14:21.880
like the analysis, into tokens

14:21.880 --> 14:24.920
by regionally subdividing it usually,

14:24.920 --> 14:27.400
and then you ask the foundation model

14:27.400 --> 14:29.520
to predict missing pieces of it.

14:29.520 --> 14:32.120
You say, you can mask out the future,

14:32.120 --> 14:33.480
and say, okay, predict this,

14:33.480 --> 14:36.600
and then you're training it in a forecast context,

14:36.600 --> 14:39.840
or you can mask out a missing regional piece,

14:39.840 --> 14:41.160
and ask it to fill in the blank.

14:41.160 --> 14:43.840
You could even ask it to predict the past, I suppose.

14:45.640 --> 14:48.000
This allows the foundation model

14:48.000 --> 14:51.720
to be trained on qualitatively different data sources.

14:51.720 --> 14:56.120
For example, you might have an encoder suite

14:56.120 --> 14:58.280
that takes satellite observations directly,

14:58.280 --> 15:00.740
and tries to turn it into token space,

15:01.800 --> 15:04.720
or the analysis from era five,

15:04.720 --> 15:09.120
or lower resolution climate forecasts.

15:10.320 --> 15:13.760
And once tokenized, the idea of a foundation model

15:13.760 --> 15:16.480
is that you have a giant middle processor layer

15:16.480 --> 15:18.440
that operates in this latent space,

15:18.440 --> 15:20.840
and from there it is relatively simple

15:20.840 --> 15:23.760
to build out decoder models to give you things you want,

15:23.760 --> 15:25.440
like tomorrow's forecast today,

15:25.440 --> 15:29.320
or what the radar is going to look like in 30 minutes.

15:30.480 --> 15:33.180
Foundation models certainly prove their worth

15:33.180 --> 15:36.080
with text-based processing like GPT,

15:36.080 --> 15:38.560
and they're also very popular and emerging

15:38.560 --> 15:40.840
in Earth observation applications

15:40.840 --> 15:42.800
with interpretation of satellite data.

15:42.800 --> 15:47.400
For example, at an ECMWFESA conference a few weeks ago,

15:47.400 --> 15:48.560
there were some interesting talks

15:48.560 --> 15:51.000
about taking satellite data

15:51.000 --> 15:55.800
and using it to infer tree cover near power lines in Norway,

15:55.800 --> 15:57.040
tasks that would normally take

15:57.040 --> 15:58.840
some very expensive helicopters,

15:58.840 --> 16:00.740
and doing them much, much more cheaply

16:00.740 --> 16:03.720
with readily available Earth observation data.

16:03.720 --> 16:05.440
The downside is that foundation models

16:05.440 --> 16:07.920
tend to be extremely expensive to train,

16:07.920 --> 16:10.020
and these will push the limits

16:10.060 --> 16:12.860
of what the public sector is probably willing to spend

16:12.860 --> 16:15.940
should foundation models prove themselves for forecasting.

16:15.940 --> 16:18.180
Few of these models exist right now for NWP,

16:18.180 --> 16:20.260
but there's plenty of private sector interest.

16:20.260 --> 16:24.060
I mean, our friend from NVIDIA talked at length about that.

16:24.060 --> 16:27.140
The two models that are currently published

16:27.140 --> 16:29.180
are ATMO-REP from Christian Lesig,

16:29.180 --> 16:31.160
which is a transformer model,

16:31.160 --> 16:33.740
tested on both forecast and downscaling applications,

16:33.740 --> 16:37.300
and Microsoft has published, as of a few days ago,

16:37.300 --> 16:38.660
it's Aurora Foundation model,

16:38.660 --> 16:43.660
which most notably uses several different data sources

16:43.700 --> 16:45.560
for training, not just air five,

16:45.560 --> 16:48.820
but also climate simulations and operational forecasts,

16:48.820 --> 16:52.020
and I believe one of Noah's ensembles.

16:53.180 --> 16:56.620
Now, to integrate all of this,

16:56.620 --> 16:59.140
as a researcher, I can make a few broad predictions

16:59.140 --> 17:02.740
on where the trends are in this area.

17:02.740 --> 17:04.680
First is going to be the convergence

17:04.680 --> 17:06.180
of data simulation forecasts,

17:06.180 --> 17:08.140
nowcast and downscaling roles.

17:08.140 --> 17:11.100
Right now, the leading NWP forecast,

17:11.100 --> 17:13.380
take in an analysis and give you a forecast,

17:13.380 --> 17:15.900
but the obvious question is to what extent

17:15.900 --> 17:18.140
can the rest of the chain be included?

17:19.780 --> 17:23.460
For example, forecasts that are ensemble generating

17:23.460 --> 17:25.560
can often be reversed to perform data simulation,

17:25.560 --> 17:28.180
and a couple of references here are one,

17:28.180 --> 17:29.760
using a diffusion-based model

17:29.760 --> 17:32.340
to assimilate sparse observations,

17:32.340 --> 17:36.140
and the second one is to perform data simulation

17:36.140 --> 17:38.580
inside the latent space of an autoencoder,

17:38.580 --> 17:41.660
which effectively replaces the background

17:41.660 --> 17:45.980
error covariance matrices of a DA system

17:45.980 --> 17:49.380
with ones that are nonlinear and flow dependent

17:49.380 --> 17:51.180
from the autoencoder space.

17:51.180 --> 17:54.580
A second avenue here is to combine conventional models

17:54.580 --> 17:56.660
with an AI-based bias correction.

17:56.660 --> 18:00.700
Farshie from ECMWF has published recently

18:00.700 --> 18:03.540
on using a neural network bias correction

18:03.540 --> 18:08.540
to include some element of AI inside the IFS-40 var system.

18:08.860 --> 18:11.260
And Said, my colleague, is going to present

18:11.260 --> 18:15.260
in about half an hour on spectral nudging

18:15.260 --> 18:18.900
to bring a classical NWP system

18:18.900 --> 18:23.460
closer to an AI forecast to preserve rich data

18:23.460 --> 18:26.140
and have the accuracy of AI.

18:27.780 --> 18:29.220
And finally, there's some effort

18:29.220 --> 18:31.180
on all-in-one AI forecast systems

18:31.180 --> 18:34.100
that go directly from observations to forecasts

18:34.100 --> 18:36.420
to post-processing to produce predictions

18:36.420 --> 18:37.700
of future observations.

18:39.140 --> 18:42.300
The Vaughn 2024 is the Aurora mentioned in the plenary,

18:42.300 --> 18:44.780
which is an encoder, decoder architecture

18:44.780 --> 18:47.980
that uses Unets, and it's the obvious long-term future

18:47.980 --> 18:49.660
of foundation models.

18:49.660 --> 18:51.780
There's also rumors that Google is buying up

18:51.780 --> 18:54.380
satellite data for its next generation,

18:54.380 --> 18:56.460
GraphCast version two or something like that,

18:56.460 --> 18:58.820
but I don't have any particular

18:58.820 --> 19:00.380
confidential information to share.

19:01.180 --> 19:02.780
Okay, now the second part of this talk

19:02.780 --> 19:05.180
is the Environment and Climate Change Canada AI roadmap.

19:05.180 --> 19:09.460
This is a joint product of the AASTD and CCMAP.

19:09.460 --> 19:11.380
It's a product of an internal Tiger team

19:11.380 --> 19:14.020
across the research and operational divisions

19:14.020 --> 19:17.340
that was put together last fall after an internal study

19:17.340 --> 19:19.500
on the current state of AI and weather forecasting,

19:19.500 --> 19:20.660
where we had a whole lot of talks

19:20.660 --> 19:23.260
like the first half of what I just gave.

19:23.260 --> 19:26.620
This roadmap is designed to set very broad research priorities.

19:26.620 --> 19:29.420
It's not designed to currently pick and choose

19:29.420 --> 19:30.580
what projects are worth funding,

19:30.580 --> 19:34.180
but to set out how we should think about AI as an institution.

19:35.340 --> 19:37.700
And the largest theme from this roadmap

19:37.700 --> 19:40.100
is the need for capacity building,

19:40.100 --> 19:42.460
both in terms of compute capacity,

19:42.460 --> 19:44.380
in light of the supercomputer update

19:44.380 --> 19:45.620
we're likely to have next year

19:45.620 --> 19:48.620
and whatever gets procured in the years thereafter.

19:50.540 --> 19:52.660
How we should think about the use of cloud computing

19:52.660 --> 19:54.500
and finally what we need to do

19:54.500 --> 19:56.860
from a human resources and training standpoint.

19:56.860 --> 19:58.900
This is a living document.

19:58.900 --> 20:00.180
It should be published soon.

20:00.460 --> 20:02.900
I'd hoped I could begin this talk

20:02.900 --> 20:04.140
with a reference to the live document,

20:04.140 --> 20:05.980
but I think it's still in translation.

20:05.980 --> 20:07.940
But once it is published,

20:07.940 --> 20:11.260
it'll be updated every few months to every year or so

20:11.260 --> 20:13.900
with internal reviews and updates

20:13.900 --> 20:15.860
just as we understand more about AI.

20:16.860 --> 20:18.980
The broad themes of this document

20:18.980 --> 20:21.340
are how we intend to integrate AI

20:21.340 --> 20:23.580
throughout the research, development and operation cycle.

20:23.580 --> 20:27.460
So we're not limiting it to just graphcast style forecasts,

20:27.460 --> 20:30.500
but we're interested in applying AI everywhere

20:30.500 --> 20:34.780
from data gathering to post-processing.

20:35.740 --> 20:38.060
The main evaluation criteria here

20:38.060 --> 20:41.260
are the triumvirate of feasibility,

20:41.260 --> 20:42.740
service and efficiency.

20:42.740 --> 20:45.540
Feasibility answers the question of how capable are we

20:45.540 --> 20:50.220
of developing and running a proposed AI project?

20:50.220 --> 20:53.180
And that includes not just the scientific risk

20:53.180 --> 20:56.420
of well, putting a whole lot of time and money

20:56.420 --> 20:58.180
into a project and having it not work,

20:58.180 --> 21:00.980
but also whether or not we have the compute resources

21:00.980 --> 21:05.020
to do this or the human resources to develop and manage it.

21:06.540 --> 21:08.060
The second criterion is service

21:08.060 --> 21:11.460
of how a project will improve ECCC's

21:11.460 --> 21:13.500
weather-based services to Canadians.

21:13.500 --> 21:16.300
And that is not just in terms of accuracy,

21:16.300 --> 21:18.580
but also whether we can make our forecast products

21:18.580 --> 21:22.380
more timely, whether we can have more numerous projects

21:22.380 --> 21:25.300
or whether we can have qualitatively new products

21:25.340 --> 21:29.940
like hypothetically, rapidly updated now casting,

21:29.940 --> 21:31.700
which we just currently don't have available

21:31.700 --> 21:33.300
for many government source.

21:33.300 --> 21:35.580
And finally, there's a question of efficiency.

21:35.580 --> 21:37.380
How will a project make efficient use

21:37.380 --> 21:39.140
of our computational resources?

21:39.140 --> 21:42.100
That includes the broad themes of energy efficiency,

21:42.100 --> 21:45.100
but also the government-specific theme

21:45.100 --> 21:48.300
of being a good steward of public funds.

21:48.300 --> 21:52.580
Supercomputers are, as one might guess, rather expensive.

21:52.580 --> 21:55.700
And we ought to demonstrate to Canadians

21:55.700 --> 21:59.180
that Canadians are getting their money's worth.

22:00.260 --> 22:03.020
Okay, so more specifically,

22:03.820 --> 22:05.100
how are we looking at AI

22:05.100 --> 22:08.060
from the observation to product pipeline?

22:09.100 --> 22:11.660
First category here are observations and data assimilation.

22:11.660 --> 22:14.940
So in observations, AI is in some sense

22:14.940 --> 22:17.020
an extension of what's already happening

22:17.020 --> 22:19.060
in terms of looking at quality control

22:19.060 --> 22:20.980
and error estimation of our inputs.

22:22.660 --> 22:27.660
AI can perhaps allow us to have some new capabilities

22:29.780 --> 22:31.180
with learned observation operators

22:31.180 --> 22:34.220
to take advantage of parameters that are,

22:36.780 --> 22:38.580
to learn parameters that are not directly observed,

22:38.580 --> 22:41.220
such as complicated satellite measurements

22:41.220 --> 22:45.900
that are non-linear features of the atmospheric state

22:45.900 --> 22:47.620
that are hard to directly measure

22:47.620 --> 22:49.580
but might be possible for an AI to learn.

22:49.580 --> 22:53.500
And finally, we have some ideas on unconventional data sources.

22:53.500 --> 22:55.180
One idea thrown around in discussions

22:55.180 --> 23:00.180
was using ad hoc webcam data like traffic cameras

23:00.220 --> 23:04.060
to evaluate real-time precipitation class information.

23:04.060 --> 23:05.780
In principle, someone can look at the feed

23:05.780 --> 23:07.340
and say, yep, it's raining,

23:07.340 --> 23:09.460
and we can have an AI do the same thing

23:09.460 --> 23:11.620
and potentially have useful data.

23:11.620 --> 23:13.300
On the data assimilation side,

23:14.700 --> 23:16.380
in some senses, this area is most advanced

23:16.380 --> 23:21.380
because DA is already using some heavy computation

23:22.460 --> 23:24.740
for its error matrix operations

23:24.740 --> 23:27.020
and they're investigating GPU use.

23:27.020 --> 23:28.020
As previously mentioned,

23:28.020 --> 23:29.340
there's a data of data assimilation

23:29.340 --> 23:30.780
in the latent space of a model.

23:30.780 --> 23:34.180
If we can vastly increase ensemble sizes through AI,

23:34.180 --> 23:37.620
we can perform better PDF estimation

23:37.620 --> 23:42.620
through particle filters or non-Gaussian-based statistics.

23:43.020 --> 23:44.860
And finally, we can start to estimate,

23:44.860 --> 23:46.540
perhaps, the model parameters themselves

23:46.540 --> 23:48.340
rather than just initial conditions.

23:49.620 --> 23:50.700
The numerical prediction side,

23:50.700 --> 23:53.100
this is closest to what I talked about previously

23:53.100 --> 23:54.900
with AI forecast models.

23:54.900 --> 23:57.860
In the near term, we're looking at implementing

23:57.860 --> 23:59.460
the AI models as they are

23:59.460 --> 24:02.780
to provide second opinions about the weather to forecasters.

24:04.740 --> 24:07.580
In the medium term, we're looking at fine-tuning

24:07.580 --> 24:10.020
large models on our operational data sets

24:10.020 --> 24:13.420
to provide better AI forecasts.

24:13.420 --> 24:14.260
And in the longer term,

24:14.260 --> 24:15.740
we're looking at structural changes

24:15.740 --> 24:18.300
and developing new models in general.

24:18.300 --> 24:20.300
In addition, we'd like to hybridize classical

24:20.300 --> 24:24.420
and WP and AI models to help fix the problem of sparse outputs.

24:25.660 --> 24:27.980
And along the way,

24:27.980 --> 24:29.980
we'd also like to investigate emulation

24:29.980 --> 24:31.460
of the physical parameterizations.

24:31.460 --> 24:35.500
For example, radiation is very expensive

24:35.500 --> 24:37.100
inside the atmospheric model

24:37.100 --> 24:40.100
and 3D radiation is probably better than 1D radiation

24:40.100 --> 24:41.820
but it's too expensive to run operationally.

24:41.820 --> 24:43.380
If we can emulate that,

24:43.380 --> 24:45.940
we can have a better parameterization

24:45.940 --> 24:48.700
that is still within our compute budget.

24:48.700 --> 24:50.420
Also, we'd like to extend this

24:50.420 --> 24:53.020
to ocean ice and land surface prediction,

24:53.020 --> 24:56.660
but that is still fairly preliminary

24:56.660 --> 25:00.020
in part because the data sources aren't quite as complete.

25:00.020 --> 25:03.980
Okay, now in terms of the tail end of the forecast

25:03.980 --> 25:08.420
for post-processing and final products,

25:08.420 --> 25:12.580
this is the realm of downscaling and now casting.

25:14.380 --> 25:17.820
We have a 2.5 kilometer high resolution regional system

25:17.820 --> 25:20.180
but it's just too expensive to run too often.

25:20.180 --> 25:21.260
If we can downscale,

25:21.260 --> 25:26.260
we can potentially achieve that resolution of output

25:26.660 --> 25:31.660
and evaluate extremes and risks at that scale

25:32.500 --> 25:37.060
without being limited by melting the supercomputer.

25:37.060 --> 25:38.740
It would also be really nice if we could have

25:38.740 --> 25:41.260
near real-time assimilation of weather station radar data

25:41.580 --> 25:44.580
to improve the half hour, one hour forecast,

25:44.580 --> 25:47.700
which is not something we can currently do very well.

25:47.700 --> 25:49.220
In terms of post-processing,

25:50.820 --> 25:52.700
the statistical post-processing systems

25:52.700 --> 25:55.140
have all long had systematic error adjustments

25:55.140 --> 25:56.660
and station specific adjustments

25:56.660 --> 25:59.580
for representative this errors and the like

25:59.580 --> 26:04.580
and AI can help turn that into better nonlinear corrections.

26:04.860 --> 26:07.260
And finally, in terms of the expert product sides,

26:07.260 --> 26:09.740
we'd like to have better high impact weather diagnostics

26:09.740 --> 26:13.660
with well-calibrated forecasts for all of the big ones,

26:13.660 --> 26:16.940
tornadoes, hail, blizzards that are just don't show up

26:16.940 --> 26:19.380
in the larger scale forecast

26:19.380 --> 26:22.020
but are extremely important for the people stuck in them.

26:22.020 --> 26:24.860
Excuse me, Christopher, so only two minutes left before.

26:24.860 --> 26:27.740
Yes, okay, I am going to be very quick.

26:27.740 --> 26:31.000
Okay, challenges and opportunities.

26:31.860 --> 26:34.700
As I've hinted at the entire time, we have challenges

26:34.700 --> 26:36.180
and these are opportunities.

26:37.180 --> 26:40.460
On the physical side, we have limited compute capacity.

26:40.460 --> 26:42.700
GPU compute demands are only going to go up.

26:42.700 --> 26:44.220
We don't have very many of them.

26:45.220 --> 26:47.700
We're going to probably get more in the future

26:47.700 --> 26:49.220
but we need to manage them.

26:49.220 --> 26:51.580
We also need to care about data management

26:51.580 --> 26:54.740
in terms of not just having an archive that exists on tape

26:54.740 --> 26:57.380
but one that is living and can answer

26:57.380 --> 26:59.820
training-based questions very quickly.

26:59.820 --> 27:02.100
In terms of HR, we have similar problems

27:02.100 --> 27:04.620
that our researchers are all very good

27:04.780 --> 27:07.620
but they're also not necessarily well-trained on AI.

27:07.620 --> 27:10.140
We need to close that gap.

27:10.140 --> 27:14.380
And in particular, we would love to have increased collaboration

27:14.380 --> 27:17.020
with both the ivory tower and private sector.

27:17.020 --> 27:20.500
Okay, the roadmap sets out targets and milestones.

27:20.500 --> 27:23.260
We would like to have our first operational AI systems

27:23.260 --> 27:25.420
for IC innovation cycle five,

27:25.420 --> 27:27.980
which is targeted early 2026

27:27.980 --> 27:30.740
after the supercomputer update.

27:31.700 --> 27:33.540
The current innovation cycle is just closing

27:33.620 --> 27:35.580
because it's a bit too early for it.

27:35.580 --> 27:37.860
And ultimately we'd like to have AI

27:37.860 --> 27:41.380
as just another forecast tool by 2030 or so.

27:41.380 --> 27:44.420
Okay, I would love to talk more about this

27:44.420 --> 27:45.740
but unfortunately there's no time

27:45.740 --> 27:49.700
but in general, our divisions have all been investigating

27:49.700 --> 27:53.580
how we can integrate AI into research flows.

27:54.500 --> 27:55.660
Some projects have started,

27:55.660 --> 27:58.100
some are waiting for people

27:58.100 --> 28:01.540
and some just simply need more resources

28:01.540 --> 28:02.500
that we don't currently have

28:02.500 --> 28:04.860
and we would love to collaborate on them.

28:04.860 --> 28:06.980
If you have AI skill

28:06.980 --> 28:08.580
and you have a weather-related project,

28:08.580 --> 28:12.140
please email someone at our division.

28:12.140 --> 28:13.980
We would probably love to talk to you.

28:15.340 --> 28:16.820
Unfortunately, I have to cancel this slide

28:16.820 --> 28:19.060
where I was going to talk up

28:19.060 --> 28:20.860
all of my colleagues' presentations to come.

28:20.860 --> 28:23.620
Please stick around, they are going to be great.

28:23.620 --> 28:26.060
And finally, conclusions-wise,

28:26.060 --> 28:28.340
AI is rapidly advancing the state of the art

28:28.340 --> 28:30.740
in numerical weather computation.

28:30.740 --> 28:35.540
This is a phase change of forecasting

28:35.540 --> 28:37.460
and I'm excited to see where it goes.

28:37.460 --> 28:39.740
We will make AI and machine learning technologies

28:39.740 --> 28:40.980
a major part of our systems

28:40.980 --> 28:44.380
as they prove themselves.

28:45.620 --> 28:47.460
But we're a public service organization,

28:47.460 --> 28:49.900
we recognize that we have to be very careful

28:49.900 --> 28:52.220
about what we stand behind operationally.

28:54.100 --> 28:56.500
And finally, I hope these slides are available

28:56.500 --> 28:57.660
afterwards for the references.

28:57.660 --> 29:00.660
There are references to all of the systems I've mentioned.

29:00.660 --> 29:02.100
And I'd also just like to highlight

29:02.100 --> 29:03.540
that of the 14 references here,

29:03.540 --> 29:05.020
about eight are preprints.

29:05.020 --> 29:08.140
This is a really, really rapidly moving field.

29:09.780 --> 29:11.380
Thank you, I am...

29:11.380 --> 29:13.340
Missy Christopher.

29:13.340 --> 29:16.780
We have time maybe for one question.

29:16.780 --> 29:17.780
I know.

29:19.300 --> 29:23.020
I'm going to take the highest ranked one.

29:23.020 --> 29:24.420
It's a long one, are you ready?

29:24.420 --> 29:26.300
Yes, yes.

29:26.300 --> 29:27.180
Perfect.

29:27.180 --> 29:29.940
Currently, AI seems to be quite expensive

29:29.940 --> 29:31.700
and rapidly developing.

29:31.700 --> 29:34.460
While we were waiting for AI-based models

29:34.460 --> 29:36.580
to build better foundations and training

29:36.580 --> 29:39.060
to replace numerical weather prediction

29:39.060 --> 29:40.740
or for climate studies,

29:40.740 --> 29:43.780
can we exploit its computational speed

29:43.780 --> 29:48.780
in the near term for now casting or HRR-like output?

29:49.460 --> 29:52.900
For example, ECCC can incorporate into CAM

29:52.900 --> 29:55.220
for very short thunderstorm prediction

29:55.220 --> 29:59.540
or perhaps weather elements on grid now cast.

30:00.780 --> 30:04.820
Okay, I believe these projects are under investigation.

30:04.820 --> 30:06.900
I'm on the medium range forecast side,

30:06.900 --> 30:11.740
so it's not my particular side,

30:11.740 --> 30:15.580
but there's a presentation in this session,

30:15.580 --> 30:19.700
I believe after lunch that investigates now casting

30:19.700 --> 30:21.220
via an IBM Foundation model.

30:21.220 --> 30:24.340
And I think that'll begin to answer that kind of question.

30:24.340 --> 30:27.140
In general, yes, this would be very good.

30:27.220 --> 30:29.860
In practice, the nearest term limits

30:29.860 --> 30:31.340
are probably compute potential

30:31.340 --> 30:33.140
because we have relatively few

30:33.140 --> 30:35.700
operationally available GPUs,

30:35.700 --> 30:38.100
but hopefully in the next few months to year or so,

30:38.100 --> 30:39.020
that will improve.

30:40.780 --> 30:42.820
I might sneak in another question then,

30:42.820 --> 30:44.540
Christopher, we have a minute.

30:44.540 --> 30:47.300
Have you considered the role that Canadian industry

30:47.300 --> 30:51.260
will have in developing AI capacity at ECCC?

30:52.060 --> 30:54.620
We would love collaborations from industry.

30:54.620 --> 30:59.620
That is my politically correct and also true answer.

31:01.380 --> 31:05.860
We have limits on our resources in part

31:05.860 --> 31:09.500
because until, well, procurement thus far

31:09.500 --> 31:13.380
has been focused on making our operational systems better

31:13.380 --> 31:15.380
for obvious reasons.

31:15.380 --> 31:20.380
And the cycles of this mean that it is practically difficult

31:21.220 --> 31:25.380
to, sorry, I'm speaking as a researcher,

31:25.380 --> 31:26.700
my boss is probably listening,

31:26.700 --> 31:28.900
so there's some things I need to be very circumspect

31:28.900 --> 31:29.740
about saying.

31:31.060 --> 31:33.700
We need to be very careful about how we commit resources

31:33.700 --> 31:35.420
for training large models.

31:35.420 --> 31:38.460
Industry is, I think, a fantastic partner,

31:38.460 --> 31:42.300
both for the potential of having compute resources

31:42.300 --> 31:44.500
we could borrow and also a better focus

31:44.500 --> 31:48.020
on some of the most downstream applications.

31:48.060 --> 31:52.820
For example, our leading talk that opened CMOS

31:52.820 --> 31:56.220
was on the weather impacts on the insurance industry.

31:56.220 --> 31:59.580
And to the extent we can be of value there,

31:59.580 --> 32:02.340
I think there's room for joint products.

32:06.500 --> 32:10.380
Okay, so I will try to answer other questions

32:10.380 --> 32:12.500
in the chat as we continue.

32:12.500 --> 32:14.700
But otherwise, I will stop sharing, meet myself

32:14.700 --> 32:19.180
and thank our hosts.

32:19.180 --> 32:20.260
Well, thank you, Christopher.

32:20.260 --> 32:21.620
That was an amazing feat.

32:21.620 --> 32:24.940
You put in two very large presentations

32:24.940 --> 32:27.260
into one 30-minute presentation.

32:27.260 --> 32:29.860
You have all my admiration and thanks for doing that.

32:30.860 --> 32:32.740
Congratulations, actually, in 30 minutes.

32:32.740 --> 32:34.940
Okay, and now we're on the subject of airvests.

32:34.940 --> 32:37.020
So we're not going to take any longer.

32:37.020 --> 32:38.620
I'm going to introduce you to Airvests GLaPalm,

32:38.620 --> 32:41.180
who comes from Canada as well, a researcher.

32:41.180 --> 32:45.700
He's going to introduce us to the NWPEI-based model,

32:45.700 --> 32:47.940
the verification against the observations

32:47.940 --> 32:50.980
of airvests and airvests.

32:50.980 --> 32:52.660
It's up to you.

32:52.660 --> 32:54.180
Hello.

32:54.180 --> 32:55.140
I don't know.

32:55.140 --> 32:57.660
So you hear me.

32:57.660 --> 32:58.580
Very well.

32:58.580 --> 33:00.820
Do you see my screen?

33:00.820 --> 33:01.700
Also.

33:01.700 --> 33:02.420
Wonderful.

33:02.420 --> 33:04.900
So I'm going to do the presentation in French.

33:04.900 --> 33:07.060
If you have any questions in English, there's no problem.

33:07.060 --> 33:09.900
If you have any questions in English, there's no problem.

33:10.140 --> 33:13.580
It would be easier for me if I do it in French and for you also.

33:13.580 --> 33:19.460
So I'm here to present the work that I did in collaboration

33:19.460 --> 33:25.140
with my colleagues on the evaluation of the models based

33:25.140 --> 33:29.380
on the airCCC in a second.

33:29.380 --> 33:30.340
I don't know.

33:30.340 --> 33:35.500
So the context is that with the emergence of these models,

33:35.500 --> 33:45.780
we realized that we had to check these models

33:45.780 --> 33:49.340
with our traditional verification methods,

33:49.340 --> 33:52.340
which allow us to evaluate the innovations

33:52.340 --> 33:54.740
that are made on traditional models.

33:54.740 --> 34:01.780
So I was asked by my boss to install, turn and check

34:01.780 --> 34:07.340
these models on our HPC installations.

34:07.340 --> 34:11.660
And I started this work in October 2023.

34:11.660 --> 34:15.020
And I worked on it until April 2024.

34:15.020 --> 34:18.020
So what I want to present to you is just that.

34:18.020 --> 34:22.020
So the activities that were completed during this special project

34:22.020 --> 34:26.660
there, it's that we turned, we chose two models,

34:26.660 --> 34:30.100
ForecastNet and Graphcast, which were available for free.

34:30.100 --> 34:31.380
It's easy.

34:31.380 --> 34:35.340
And, well, ForecastNet, Christopher,

34:35.340 --> 34:36.940
we talked about it a little bit earlier.

34:36.940 --> 34:39.220
So it's a model that was developed by Renvidia.

34:39.220 --> 34:42.660
And then we turned two graphs of Graphcast,

34:42.660 --> 34:47.860
one with 13 levels of pressure, with a roof of 50 hectopascals

34:47.860 --> 34:52.420
and a version at 37 levels, a roof of 1 hectopascal.

34:52.420 --> 34:56.220
And we turned each model with three analysis sets.

34:56.260 --> 35:02.620
One, the first is the operational analysis of the OVF,

35:02.620 --> 35:06.420
called IFS, on three levels only.

35:06.420 --> 35:09.940
We also used R5.

35:09.940 --> 35:13.860
So the R5 analyses are the ones that were used

35:13.860 --> 35:17.420
to train these models.

35:17.420 --> 35:21.100
We were able to turn the configurations at 13 and 37 levels.

35:21.100 --> 35:26.140
And of course, we wanted to compare the operational provisions

35:26.180 --> 35:27.300
with the same analysis.

35:27.300 --> 35:30.620
So we started these models, these AI models,

35:30.620 --> 35:35.060
with the operational analysis of CCC.

35:35.060 --> 35:41.260
And we turned in two real-time modes,

35:41.260 --> 35:45.380
where we turned twice a day,

35:45.380 --> 35:47.900
at the same time as the operational model.

35:47.900 --> 35:49.940
And like that, the operational metrologists

35:49.940 --> 35:53.020
can compare the Forecasts based on AI

35:53.020 --> 35:55.140
with the operational provisions.

35:55.140 --> 35:59.940
And also, on my side, I did an evaluation

35:59.940 --> 36:04.500
on a period of one year, which allows us to see

36:04.500 --> 36:07.740
what the models are like.

36:07.740 --> 36:11.420
So here, I put a slide on the description of the models.

36:11.420 --> 36:12.420
It's very precise.

36:12.420 --> 36:13.780
I don't have much time.

36:13.780 --> 36:17.620
Basically, the two Graphcast and Forecast Net models

36:17.620 --> 36:21.860
use about the same information.

36:21.860 --> 36:25.140
But I put a lot of detail there to be complete.

36:25.140 --> 36:29.940
But I don't think I'll be able to save a little time.

36:29.940 --> 36:35.940
So one of the advantages of AI models

36:35.940 --> 36:39.180
is their informatic efficiency.

36:39.180 --> 36:41.860
If we compare the operational model,

36:41.860 --> 36:44.700
presently, it takes a little less than an hour,

36:44.700 --> 36:46.620
more than 6,000 CPUs.

36:46.620 --> 36:50.460
So it's a big deal.

36:50.460 --> 36:53.540
So it generates 500 gigabytes of data

36:53.540 --> 36:55.940
at each provision twice a day.

36:55.940 --> 36:59.740
It makes outings at all ages up to 10 days.

36:59.740 --> 37:02.460
And then it's models at 15 kilometers of resolution

37:02.460 --> 37:04.620
with a lot of vertical resolutions.

37:04.620 --> 37:07.540
AI models have less good resolutions.

37:07.540 --> 37:09.820
They release data at 6 hours.

37:09.820 --> 37:12.420
And a less good vertical resolution too.

37:12.420 --> 37:15.380
So, but Forecast Net is very, very light.

37:15.380 --> 37:16.700
It's impressive.

37:16.700 --> 37:18.820
It takes 20 minutes on a CPU.

37:18.820 --> 37:20.020
I don't speak of a GPU here.

37:20.020 --> 37:22.220
I have a GPU even faster, of course.

37:22.220 --> 37:24.100
But on a CPU, you can turn it on.

37:24.100 --> 37:26.540
You can turn it on on your laptop and it works.

37:26.540 --> 37:29.620
And it's very fast.

37:29.620 --> 37:31.140
It still gives you a good preview.

37:31.140 --> 37:34.700
And Graphcast, it's a little bit more expensive.

37:34.700 --> 37:36.620
The confidence at 13 levels

37:36.620 --> 37:40.540
requires 100 gigabytes of memory.

37:40.540 --> 37:43.060
So it's a little bit more expensive.

37:43.060 --> 37:45.540
But still, comparing the operational model,

37:45.540 --> 37:49.180
it's very, very much, much smaller.

37:49.180 --> 37:50.660
Smaller orders.

37:50.660 --> 37:53.340
And I invite you to the second presentation

37:53.340 --> 37:56.220
of Christopher Subick on exactly comparing

37:56.220 --> 37:59.100
the computer performance between AI models,

37:59.100 --> 38:02.420
Graphcast and GEM, the operational model.

38:02.420 --> 38:04.340
And it makes a very good comparison.

38:04.340 --> 38:08.220
If you're interested, I invite you to have

38:08.220 --> 38:11.380
this presentation on your computer this afternoon.

38:11.380 --> 38:13.500
So what does it look like as a verification

38:13.500 --> 38:16.580
once we've done the average over a year?

38:16.580 --> 38:21.060
So here, I have several curves.

38:21.060 --> 38:24.660
Here, I present the errors, the prediction

38:24.660 --> 38:29.780
of the geopotential at 55 to Pascal.

38:29.780 --> 38:34.900
So these are the errors.

38:34.900 --> 38:42.100
The very thick blue gray here, that's the baseline.

38:42.100 --> 38:45.500
So that's the operational preview.

38:45.500 --> 38:51.260
So we see the errors that are missing from 0 to 240.

38:51.260 --> 38:54.100
And the other curves, it's all the previews,

38:54.100 --> 38:56.660
the verification, the AI models.

38:56.660 --> 38:59.820
So we can look at them and then, as these are errors,

38:59.820 --> 39:02.220
but the closer we are to 0, the better it is.

39:02.220 --> 39:07.220
So we see here a group of previews.

39:07.220 --> 39:08.380
That's the forecast net.

39:08.380 --> 39:10.220
So we see that for the GZ500,

39:10.220 --> 39:15.100
the forecast net is less good than the operational preview.

39:15.100 --> 39:17.900
On the other hand, all the other curves, the five other curves,

39:17.900 --> 39:19.660
it's all the previews made by Gravcast

39:19.660 --> 39:21.660
using different analyses.

39:21.660 --> 39:24.700
So we see that Gravcast, from day five,

39:24.700 --> 39:26.620
is better than the operational model,

39:26.620 --> 39:29.340
no matter the analysis we give him.

39:29.340 --> 39:35.500
So that's when using the 05 and FS analyses, it's better.

39:35.500 --> 39:37.900
I don't know, but what's important here,

39:37.980 --> 39:40.900
what's interesting here is that the two curves here,

39:40.900 --> 39:43.300
orange and green,

39:43.300 --> 39:47.420
the previews are initialized with the same analysis

39:47.420 --> 39:49.060
as the blue curve.

39:49.060 --> 39:52.540
So we see that with equal information,

39:52.540 --> 39:57.140
Gravcast is better from day five on the GZ500.

39:57.140 --> 39:58.940
If we look at another variable,

39:58.940 --> 40:01.820
which is the temperature at 850 tectopascals,

40:01.820 --> 40:04.100
so it's the same colors, the same curves,

40:04.180 --> 40:09.180
in this case, we see that from 72 hours,

40:09.180 --> 40:12.380
all the AI models are the operational model,

40:12.380 --> 40:13.500
even for the GZ500,

40:13.500 --> 40:19.180
but we still see that Gravcast is the best model,

40:19.180 --> 40:20.300
this variable.

40:20.300 --> 40:23.220
So I showed you two variables.

40:23.220 --> 40:26.060
So we can conclude that Gravcast is better than for the GZ500,

40:26.060 --> 40:29.540
we will focus on that from now on.

40:29.540 --> 40:31.660
And then what does it look like for other variables,

40:31.740 --> 40:34.180
like wind, humidity.

40:34.180 --> 40:36.780
So here, I present you graphics,

40:36.780 --> 40:39.220
it's vertical profiles.

40:41.500 --> 40:46.260
Here, in the Y axis of the vertical coordinate curve,

40:46.260 --> 40:47.740
we talk about the surface,

40:47.740 --> 40:51.140
from 1,000 tectopascals to 50 tectopascals.

40:51.140 --> 40:58.500
And so the clean curve,

40:58.500 --> 41:01.900
it's the error's quarter, according to the variable.

41:01.900 --> 41:06.700
And the tight curve is the bias.

41:06.700 --> 41:08.860
And the different variables are the following.

41:08.860 --> 41:12.180
Here, on the top left, we have the meridian wind,

41:12.180 --> 41:16.620
the wind component, the wind module here on the right.

41:16.620 --> 41:24.740
Here, can you do this for us in the next two minutes?

41:24.740 --> 41:26.740
Yes, that's it.

41:26.740 --> 41:28.020
Yes, I'll go faster.

41:28.100 --> 41:30.180
So what I wanted to show you,

41:30.180 --> 41:34.340
here we have the Gravcast configuration,

41:34.340 --> 41:37.940
so at 13 levels, 37 levels.

41:37.940 --> 41:43.580
So what I wanted to show you is that the red curve

41:43.580 --> 41:46.060
is always better than the blue curve,

41:46.060 --> 41:48.300
so the model has all the variables,

41:48.300 --> 41:50.940
all the failures, not all the failures,

41:50.940 --> 41:52.820
but all the variables, all the levels.

41:52.820 --> 41:54.980
We see that Gravcast is better.

41:54.980 --> 42:01.500
So are these two models not different resolutions?

42:01.500 --> 42:04.900
Is the fact that Gravcast is a bigger resolution,

42:04.900 --> 42:07.660
is it the advantage?

42:07.660 --> 42:10.460
Is it the advantage compared to the operational model?

42:10.460 --> 42:14.740
So what we did is that we did the same verification

42:14.740 --> 42:17.860
that I presented to you earlier, but we filtered it.

42:17.860 --> 42:19.860
We filtered, we removed all the scales

42:19.860 --> 42:22.060
that were smaller than 1,000 km,

42:22.060 --> 42:24.700
and we kept all those that were 2,000 km.

42:24.740 --> 42:27.500
So I have two graphics to present here.

42:27.500 --> 42:30.220
So on the left, it's the same graphic

42:30.220 --> 42:32.860
that I presented to you earlier, not filtered,

42:32.860 --> 42:35.500
and on the right, it's the filtered previews.

42:35.500 --> 42:37.660
So that's on the average, on the winter,

42:37.660 --> 42:39.860
it's not on the full year.

42:39.860 --> 42:43.500
So we see that even if filtered, Gravcast is better.

42:43.500 --> 42:45.980
So that's going to bring to the work of Spectreur Nodging

42:45.980 --> 42:49.380
of Syed, who will present in the next presentation.

42:49.380 --> 42:56.340
So and for the summer, it's a little less spectacular,

42:56.340 --> 43:05.420
but we still see that Gravcast has a lot of good information.

43:05.420 --> 43:10.260
So what are the future activities to do more verification?

43:10.260 --> 43:14.340
We even have an internal site that allows us to visualize

43:14.340 --> 43:16.740
these previews day by day.

43:16.820 --> 43:21.220
And I invite you to have the next seminar

43:21.220 --> 43:23.260
of Syed about Spectreur Nodging,

43:23.260 --> 43:26.580
which is a very interesting approach to integrate

43:26.580 --> 43:28.820
physical models and AI models.

43:28.820 --> 43:37.140
And Christopher Subick's work on entering Gravcast

43:37.140 --> 43:42.500
with the operational analysis that we have done internally

43:42.500 --> 43:46.700
so that it can be better adapted to our model.

43:47.980 --> 43:49.460
Thank you.

43:49.460 --> 43:52.220
Hi, thank you, Hervé.

43:52.220 --> 43:54.060
Hi, I have a question for you.

43:54.060 --> 43:55.900
There, you made the internal models run,

43:55.900 --> 43:57.820
you installed them to do the verification,

43:57.820 --> 43:59.460
but there are more and more models,

43:59.460 --> 44:02.060
there are almost two days,

44:02.060 --> 44:04.780
is there perhaps a way to have

44:04.780 --> 44:07.060
verification, counter-observation,

44:07.060 --> 44:08.340
like that, in a standardized way,

44:08.340 --> 44:10.740
or each time, you will have to install the models

44:11.140 --> 44:13.180
and then the scoring themselves?

44:13.180 --> 44:18.180
Well, listen, that's what the software that we use

44:18.180 --> 44:20.620
to do the verification and counter-observation

44:20.620 --> 44:24.060
works very well only locally.

44:24.060 --> 44:28.260
So it's difficult to publish that externally.

44:28.260 --> 44:31.380
On the other hand, running these models,

44:31.380 --> 44:33.660
if it's not done very, very easily,

44:33.660 --> 44:35.780
I worked for three months in the middle of the day

44:35.780 --> 44:37.900
just to start these models.

44:38.220 --> 44:42.420
So I imagine that each model has its own peculiarities.

44:42.420 --> 44:45.900
So it's not obvious, it's not as plug-and-play

44:45.900 --> 44:47.020
that we believe in.

44:47.020 --> 44:49.700
There's still a lot of work to be done.

44:49.700 --> 44:54.460
So if there are all the two, all the two weeks,

44:54.460 --> 44:58.500
it will be difficult to do this same evaluation

44:58.500 --> 45:00.660
to follow the run.

45:02.260 --> 45:04.940
Thank you, I don't know if there are other questions.

45:04.940 --> 45:07.180
Maybe a word for advanced systems.

45:07.180 --> 45:09.900
If you could reset the Q&A on EventMobi

45:09.900 --> 45:13.020
because I still see the questions

45:13.020 --> 45:16.060
that have been asked to Christopher Subic.

45:16.060 --> 45:18.940
So it's hard to see which one.

45:18.940 --> 45:20.700
I have one for you, Hervé.

45:20.700 --> 45:22.060
How do artificial intelligence models

45:22.060 --> 45:24.780
behave in complex mountainous terrain?

45:26.660 --> 45:29.540
I didn't evaluate that.

45:29.540 --> 45:31.420
That's more what my colleague, Marc Verville,

45:31.420 --> 45:33.500
did for verification on the surface.

45:34.500 --> 45:43.700
I don't have any memory of the results.

45:43.700 --> 45:48.100
My focus was really on the verification in addition.

45:48.100 --> 45:50.900
But of course, having a good resolution

45:50.900 --> 45:54.900
will certainly not be a problem.

45:54.900 --> 45:56.220
Perfect.

45:56.220 --> 45:58.620
Thank you very much, Hervé.

45:58.620 --> 46:00.140
Thank you.

46:00.140 --> 46:01.740
Thank you, Hervé.

46:02.020 --> 46:05.900
We, so we're going to continue.

46:05.900 --> 46:09.500
This time we're inviting Syed Zahid Hussein,

46:09.500 --> 46:14.020
who's a research scientist at ECCC.

46:14.020 --> 46:19.460
He will be presenting leveraging data-driven weather

46:19.460 --> 46:22.700
emulators to guide physics-based numerical weather

46:22.700 --> 46:27.940
prediction models, a fusion of forecasting paradigms.

46:27.940 --> 46:34.780
So without further ado, Syed, this is your turn.

46:34.780 --> 46:36.420
Thank you, Rand.

46:36.420 --> 46:37.500
Hello, everyone.

46:37.500 --> 46:41.220
In my presentation today, I will be talking

46:41.220 --> 46:47.140
about how we can leverage the strengths of data-driven weather

46:47.140 --> 46:50.860
models to improve predictions from NWP models.

46:50.860 --> 46:53.380
This is a work that we have been doing recently.

46:53.380 --> 46:56.900
And here is a list of my principal collaborators

46:56.900 --> 46:59.540
and the others who have contributed to this research.

47:03.660 --> 47:07.620
As we know, the current state of the earth

47:07.620 --> 47:09.540
for operational weather forecasting

47:09.540 --> 47:13.660
is based on physics-based NWP models.

47:13.660 --> 47:17.540
However, we have seen these presentations earlier today

47:17.540 --> 47:20.380
from the plenary to the previous two presentations

47:20.380 --> 47:24.500
that we have recently seen the emergence of new data-driven

47:24.500 --> 47:27.740
models for predicting weather.

47:27.740 --> 47:30.580
And most of these models are using some form of deep neural

47:30.580 --> 47:32.900
network to emulate the training data.

47:32.900 --> 47:37.660
And the training data generally is RFI re-analysis.

47:37.660 --> 47:41.900
So we also can call them artificial intelligence-based

47:41.900 --> 47:43.300
weather emulators.

47:43.300 --> 47:46.300
And recently, they have started to gain prominence

47:46.300 --> 47:49.540
and started to also challenge the existing forecasting

47:49.540 --> 47:50.500
paradigm.

47:50.500 --> 47:53.180
Because as you have seen in the previous presentation

47:53.180 --> 47:57.500
from my colleague, Erveg, that these data-driven models

47:57.500 --> 48:00.460
can produce forecast orders of magnitude

48:00.460 --> 48:03.940
faster with minimal computational resources

48:03.940 --> 48:06.860
compared to the traditional NWP models.

48:06.860 --> 48:10.860
And also, they can be highly competitive against state

48:10.860 --> 48:15.100
of the art NWP models in terms of their accuracy.

48:15.100 --> 48:18.540
However, despite their strengths,

48:18.540 --> 48:21.220
strictly when I'm talking in the deterministic sense

48:21.220 --> 48:25.260
for these AI-based models, they have their limitations also.

48:25.260 --> 48:28.340
And one of the most widely known limitation

48:28.340 --> 48:31.340
is considerable smoothing of fine scales,

48:31.340 --> 48:33.180
particularly for longer lead times.

48:33.180 --> 48:37.220
Also, they only offer a limited range of forecast fields

48:37.220 --> 48:40.500
and improving nominal resolution of these AI models

48:40.500 --> 48:41.340
not straightforward.

48:41.340 --> 48:43.060
They can be quite challenging.

48:43.060 --> 48:45.020
So the objective of our research was

48:45.020 --> 48:48.820
to see if we can leverage the strengths of these AI-based

48:48.820 --> 48:54.020
models to improve the predictability of an NWP model.

48:54.020 --> 48:58.780
And for that, we chose or selected

48:58.780 --> 49:01.660
like the GEM model, which is used operationally

49:01.660 --> 49:06.420
as the NWP model, operationally by Environment Canada,

49:06.420 --> 49:10.580
and the GraphCast model from Google DeepMind as the AI-based

49:10.580 --> 49:11.380
model.

49:11.380 --> 49:14.900
And the nominal grid resolutions of GraphCast

49:14.900 --> 49:18.220
and the GEM-based Global Deterministic Prediction System,

49:18.220 --> 49:22.620
or GDPS, are approximately 25 kilometers and 15 kilometers,

49:22.620 --> 49:24.020
respectively.

49:24.020 --> 49:26.140
And in the previous presentation,

49:26.140 --> 49:31.100
AirVig has shown that the GraphCast actually poses

49:31.100 --> 49:35.660
more skilled large scales compared to our GDPS.

49:35.660 --> 49:38.980
But if we want to leverage the information from GraphCast,

49:38.980 --> 49:41.580
we need to know about the effective resolution of GraphCast

49:41.580 --> 49:44.860
so that we can see what are the scales that we can really

49:44.860 --> 49:48.700
utilize for improving NWP model.

49:48.700 --> 49:53.740
And in order to do that, we look at the variance ratio of GDPS

49:53.740 --> 49:58.500
and GraphCast with respect to our own CMC analysis.

49:58.500 --> 50:01.300
And before I talk about anything else,

50:01.300 --> 50:04.900
I must emphasize on the fact that the version of GraphCast

50:04.900 --> 50:10.020
that we are using has not been through any fine tuning.

50:10.020 --> 50:16.100
So it has been trained by Google on emulating error-5 analysis

50:16.100 --> 50:18.340
by training with error-5 data.

50:18.340 --> 50:22.140
And we are using that GraphCast model

50:22.140 --> 50:26.420
but initializing it with our own analysis.

50:26.420 --> 50:29.020
And in these figures in this slide,

50:29.020 --> 50:33.220
I am showing the transient component of variance ratio

50:33.220 --> 50:35.660
for 500 hectopascal kinetic energy.

50:35.660 --> 50:39.180
On the left, I have for lead time 24 hours.

50:39.180 --> 50:42.220
And on the right, I have for 120 hours.

50:42.220 --> 50:44.380
In blue, I have GraphCast.

50:44.380 --> 50:48.460
And in blue, I have GDPS.

50:48.460 --> 50:50.460
And in red, I have GraphCast.

50:50.460 --> 50:54.060
And we can see by looking at the variance ratio of GDPS,

50:54.060 --> 50:59.660
the blue lines in both 24-hour and 120-hour cases,

50:59.660 --> 51:02.100
that the variance ratio is close to 1.

51:02.100 --> 51:05.380
So that means its effective resolution

51:05.380 --> 51:08.860
is not changing with respect to lead times.

51:08.860 --> 51:11.060
However, what we see with GraphCast,

51:11.060 --> 51:13.900
first of all, at 24-hour lead time,

51:13.900 --> 51:16.900
we see the scales as large as 1,500 kilometers

51:16.900 --> 51:20.020
are smoothed out to some extent.

51:20.020 --> 51:22.620
And we see considerable smoothing for scales smaller

51:22.620 --> 51:23.540
than that.

51:23.540 --> 51:26.860
And then we see when we go to 120-hour lead time,

51:26.860 --> 51:31.700
the scales that are getting smoothed out actually increases.

51:31.700 --> 51:35.300
And it affects scales as large as 2,750.

51:35.300 --> 51:38.140
So we know that large scales in GraphCast are better.

51:38.140 --> 51:41.780
But at the same time, we see that scales below 2,750

51:41.780 --> 51:45.380
for longer lead times are problematic because of the reduced

51:45.380 --> 51:47.820
variance ratio.

51:47.820 --> 51:52.500
Now, the question is how we can really use or leverage

51:52.500 --> 51:56.260
this good large-scale information from GraphCast.

51:56.260 --> 52:00.500
And one way to do that would be to use spectral nudging,

52:00.500 --> 52:02.180
large-scale spectral nudging.

52:02.180 --> 52:05.740
And this is a very widely used idea

52:05.740 --> 52:09.260
in the field of regional climate modeling and hindcasting.

52:09.260 --> 52:12.700
Our expectation is if we nudge our gem predictions

52:12.700 --> 52:14.900
towards the large scales of GraphCast,

52:14.900 --> 52:17.740
we can improve the quality of prediction of gem.

52:17.740 --> 52:21.220
At the same time, we should be able to address the fine-scale

52:21.220 --> 52:23.100
smoothing in GraphCast while we will

52:23.100 --> 52:26.740
be able to generate the full set of focus fields

52:26.740 --> 52:32.220
that we are currently having access to through gem.

52:32.220 --> 52:34.940
The concept of nudging is quite simple,

52:34.940 --> 52:38.260
as illustrated by this equation here.

52:38.260 --> 52:44.180
So here, F is the solution of our gem model

52:44.180 --> 52:45.780
after the dynamic substep.

52:45.780 --> 52:48.820
And this term highlighted in purple color

52:48.820 --> 52:51.340
actually corresponds to the nudging increments.

52:51.340 --> 52:55.060
So we add the nudging increment to the model solution

52:55.060 --> 52:58.140
after the dynamic step to get the nudge solution, which

52:58.140 --> 52:59.620
is then fed to the physics.

52:59.620 --> 53:02.900
And then it completes the complete model time step.

53:02.900 --> 53:05.220
And then it fits back to the next dynamic step.

53:05.220 --> 53:06.860
And this is how it continues.

53:06.860 --> 53:12.220
And if we look at this nudging increment term,

53:12.220 --> 53:14.700
we have this subscript Ls, which actually

53:14.700 --> 53:16.220
implies the large scale.

53:16.220 --> 53:18.660
So we are only considering the large scales

53:18.660 --> 53:20.180
when we are applying the nudging.

53:20.180 --> 53:23.060
And this omega is a relaxation parameter.

53:23.060 --> 53:25.260
And if we break down this equation,

53:25.260 --> 53:27.380
we can see basically what we have

53:27.380 --> 53:31.140
is a weighted average of the large scales coming from GraphCast

53:31.140 --> 53:35.980
and our model, whereas the fine scales are remaining intact

53:35.980 --> 53:37.860
that is being credited by the model.

53:37.860 --> 53:40.420
So this is how we can leverage the large scale accuracy

53:40.420 --> 53:43.940
of GraphCast while allowing our model to freely evolve

53:43.940 --> 53:45.580
the small scales.

53:45.580 --> 53:48.340
And the scale separation between large and small scales,

53:48.340 --> 53:50.740
we are doing that by decomposing the nudging

53:50.740 --> 53:52.740
increments in the spectral space.

53:52.740 --> 53:55.860
Although we are applying the nudging increment

53:55.860 --> 53:58.780
in the grid point space, but we are decomposing it

53:58.780 --> 53:59.660
in the spectral space.

53:59.660 --> 54:03.140
And hence, we call it spectral nudging.

54:03.140 --> 54:05.900
How we optimize the spectral nudging configuration

54:05.900 --> 54:09.900
to support our objectives for this study

54:09.900 --> 54:11.460
is a computationally demanding task.

54:11.460 --> 54:13.340
And in a sense, it is still ongoing.

54:13.340 --> 54:16.700
I mean, we are still sort of fine tuning the configuration.

54:16.700 --> 54:19.460
But at present, the most optimal configuration

54:19.460 --> 54:22.820
that we have has these following features.

54:22.820 --> 54:27.580
We are only applying nudging to horizontal wind and temperature.

54:27.620 --> 54:29.980
And we are not nudging the stratosphere and the boundary

54:29.980 --> 54:31.380
layer for different reasons.

54:31.380 --> 54:34.780
And we are nudging scales that are larger than 2750

54:34.780 --> 54:35.900
for obvious reasons.

54:35.900 --> 54:38.780
That should be obvious from the variance ratio comparison.

54:38.780 --> 54:42.700
And we have 12 hours as the nudging realization scale.

54:42.700 --> 54:46.140
And we apply nudging at every time step.

54:46.140 --> 54:48.900
With that, I'll be going to some of the results

54:48.900 --> 54:53.460
of verification that we will try to prove

54:53.460 --> 54:55.260
that this approach actually helps

54:55.260 --> 54:58.500
to improve the predictability of gem.

54:58.500 --> 55:03.700
We ran a series of experiments for winter and summer of 2022.

55:03.700 --> 55:05.780
So we have the control experiments

55:05.780 --> 55:09.420
where there is no nudging, the control GDPS.

55:09.420 --> 55:12.260
And then we have the GDPS with spectral nudging.

55:12.260 --> 55:15.540
So control would be, in the next few slides, all the results.

55:15.540 --> 55:17.700
Control would be shown in blue color.

55:17.700 --> 55:21.060
And the results from spectral nudging with GDPS

55:21.060 --> 55:23.620
would be shown with red.

55:23.620 --> 55:26.540
So the first scores that I want to show

55:26.540 --> 55:28.900
are verification against radios and observations.

55:28.900 --> 55:31.900
It's similar to what Elvig has shown

55:31.900 --> 55:34.260
in the previous presentation.

55:34.260 --> 55:38.380
Just to repeat, we have in these figures,

55:38.380 --> 55:43.060
we have different variables, zonal wind, wind modulus,

55:43.060 --> 55:46.220
geopotential high temperature, and dew point depletion.

55:46.220 --> 55:49.140
In all these figures, we have the dashed lines representing

55:49.140 --> 55:51.700
the bias and solid lines representing standard deviation

55:51.700 --> 55:52.380
of error.

55:52.380 --> 55:55.260
And we have the shades of red and blue

55:55.260 --> 55:59.780
that represent the statistically significant improvements

55:59.780 --> 56:02.700
corresponding to the color of the experiment.

56:02.700 --> 56:04.900
So if we see red color, then it implies

56:04.900 --> 56:07.220
that we have statistically significant improvement

56:07.220 --> 56:09.500
with the spectral nudging configuration.

56:09.500 --> 56:13.420
And if it is blue, then we have deterioration

56:13.420 --> 56:14.900
with spectral nudging.

56:14.900 --> 56:16.380
What we can see from these figures,

56:16.380 --> 56:19.500
this is for winter over the globe.

56:19.500 --> 56:23.660
That beyond five days and up to 10 days,

56:23.660 --> 56:26.540
we can see that there is tremendous improvement

56:26.540 --> 56:29.660
in the scores for the standard deviation of error, which

56:29.660 --> 56:30.380
is more difficult.

56:30.380 --> 56:31.780
Excuse me, Sayed.

56:31.780 --> 56:34.380
You have two minutes left.

56:34.380 --> 56:37.580
OK, I'll try to be faster.

56:37.580 --> 56:42.540
For the bias, the differences are mixed.

56:42.540 --> 56:46.020
But the bias is less difficult to improve.

56:46.540 --> 56:48.900
Standard deviation is more difficult.

56:48.900 --> 56:52.340
So we see tremendous improvement, up to 10% improvement

56:52.340 --> 56:54.420
in the RMSE for longer lead times.

56:54.420 --> 56:56.580
In summer, though, we see modest improvement.

56:56.580 --> 57:00.420
Still, we see statistically significant improvement

57:00.420 --> 57:03.020
for the standard deviation of error.

57:03.020 --> 57:05.420
And when you look at the animal liquid relation

57:05.420 --> 57:09.260
coefficient, this is for the 500 hectropascal geopotential.

57:09.260 --> 57:11.700
We see improvements both in winter and summer.

57:11.700 --> 57:14.940
In winter, in terms of the gain in predictability,

57:14.940 --> 57:16.940
it's around 18 hours.

57:16.940 --> 57:20.340
In summer, it's about eight hours improvement.

57:20.340 --> 57:23.220
And they're both statistically significant.

57:23.220 --> 57:27.700
And last results is about tropical cyclone position error.

57:27.700 --> 57:30.340
This is another thing that is very difficult to improve.

57:30.340 --> 57:34.980
And our model tends to have, for the alone track position,

57:34.980 --> 57:38.100
we tend to lag the model.

57:38.100 --> 57:42.020
And we can improve that lagging with spectral nudging

57:42.020 --> 57:42.940
towards graphcast.

57:42.940 --> 57:44.980
And for the cross-track position error,

57:44.980 --> 57:48.100
our cyclones tend to veer to the right

57:48.100 --> 57:50.020
from the observed trajectory.

57:50.020 --> 57:52.860
And we are able to considerably improve

57:52.860 --> 57:57.740
that aspect of the position of tropical cyclone also.

57:57.740 --> 58:00.380
And finally, just this figure, I'm

58:00.380 --> 58:04.700
showing the temperature anomaly at 850 hectropascal

58:04.700 --> 58:07.340
for a lead time of 240 hours for a single case.

58:07.340 --> 58:10.860
We have the GDPS control on the left,

58:10.860 --> 58:13.620
graphcast in the middle, and GDPS with spectral nudging

58:13.620 --> 58:14.540
on the right.

58:14.540 --> 58:18.420
And we can see that graphcast barely has any fine scale.

58:18.420 --> 58:21.700
And both GDPS control and with spectral nudging,

58:21.700 --> 58:24.820
they both have comparable fine scale information.

58:24.820 --> 58:27.980
And we get this improvement by nudging as well.

58:27.980 --> 58:31.860
So to summarize, we developed and hybrid NWP system

58:31.860 --> 58:36.140
that fuses NWP models with AI models to spectral nudging.

58:36.140 --> 58:38.460
And by leveraging more accurate large-scale predictions

58:38.460 --> 58:42.820
from graphcast, we are able to significantly improve

58:42.820 --> 58:47.580
our prediction scale with GDPS.

58:47.580 --> 58:50.700
And I want to stress that the improvement that we have

58:50.700 --> 58:53.700
is roughly equivalent to one solid innovation cycle, which

58:53.700 --> 58:57.260
is about four years of work involving many scientists

58:57.260 --> 58:59.740
from across the Meteorological Research Division

58:59.740 --> 59:00.980
of Environment Canada.

59:00.980 --> 59:04.540
And we were able to achieve something comparable

59:04.540 --> 59:06.820
in a matter of four to five months.

59:06.820 --> 59:10.220
And also, I want to stress that this

59:10.220 --> 59:13.540
is based on work that uses a graphcast model that

59:13.540 --> 59:15.060
has not been fine-tuned.

59:15.060 --> 59:19.780
And with fine-tuning graphcast to emulate our own CMC

59:19.780 --> 59:24.340
analysis, which is a work in progress by my colleague Christopher,

59:24.340 --> 59:27.620
we hope that we could improve further.

59:27.620 --> 59:29.260
And currently, with this configuration,

59:29.260 --> 59:32.380
we have about 25% increase in the computational cost.

59:32.380 --> 59:34.380
But this is without any optimization.

59:34.380 --> 59:36.260
And with some optimization, we hope

59:36.260 --> 59:38.460
that we will be able to reduce it to something

59:38.460 --> 59:41.700
like less than 15% in the near future.

59:41.700 --> 59:44.700
So with that, I will end my presentation.

59:44.700 --> 59:46.460
Thank you, Sayada.

59:46.460 --> 59:50.860
It's a very impressive conclusion.

59:50.860 --> 59:53.780
I have one question here.

59:53.780 --> 01:00:00.980
Nudging is done as gem integration goes or at posteriori?

01:00:00.980 --> 01:00:03.140
No, it's online.

01:00:03.140 --> 01:00:05.940
So what happens is, as I said, you

01:00:06.820 --> 01:00:09.180
solve the dynamic step.

01:00:09.180 --> 01:00:12.380
Because we have the dynamic sub-step and the physics sub-step.

01:00:12.380 --> 01:00:14.660
And then we do the coupling in the split mode.

01:00:14.660 --> 01:00:16.820
So we solve the dynamic sub-step.

01:00:16.820 --> 01:00:18.660
We have a solution from dynamics,

01:00:18.660 --> 01:00:20.020
which is an intermediate solution.

01:00:20.020 --> 01:00:22.500
Then we update that by nudging.

01:00:22.500 --> 01:00:25.220
And then we feed that updated solution to the physics.

01:00:25.220 --> 01:00:27.540
And then we get the complete solution of the model time

01:00:27.540 --> 01:00:28.040
step.

01:00:28.040 --> 01:00:31.540
And then the next time, dynamics uses that solution

01:00:31.540 --> 01:00:33.500
to predict the next dynamic step.

01:00:33.500 --> 01:00:36.420
So it's not a posteriori.

01:00:36.420 --> 01:00:40.780
It's an online update.

01:00:40.780 --> 01:00:45.820
Last quick one, how does GDPSSN compare with GraphCast?

01:00:48.740 --> 01:00:53.380
That was already shown by Ervig in the previous presentation.

01:00:53.380 --> 01:00:57.780
That GDPSS, I mean, if you talk about control GDPSS

01:00:57.780 --> 01:01:00.380
versus GraphCast, that presentation of Ervig

01:01:00.380 --> 01:01:04.260
should allow you to see like it actually improves.

01:01:04.260 --> 01:01:06.020
What I am missing in my figures, because this

01:01:06.020 --> 01:01:08.940
is still a work in progress, I mean,

01:01:08.940 --> 01:01:12.100
in our paper that we expect to submit soon,

01:01:12.100 --> 01:01:16.140
which we will be adding the GraphCast also,

01:01:16.140 --> 01:01:20.300
focus in this, for example, in these figures to show

01:01:20.300 --> 01:01:25.060
control GDPS with GraphCast and GraphCast itself,

01:01:25.060 --> 01:01:28.260
like how much of the improvement is coming from GraphCast.

01:01:28.260 --> 01:01:30.220
But it is definitely coming from GraphCast,

01:01:30.220 --> 01:01:33.660
as Ervig's presentation showed that the large scales in GraphCast

01:01:33.660 --> 01:01:35.220
are much better.

01:01:35.220 --> 01:01:38.220
Yes, Isayed, looking forward to read the print.

01:01:38.220 --> 01:01:41.300
That will be very popular, I'm sure.

01:01:41.300 --> 01:01:42.620
Thank you.

01:01:42.620 --> 01:01:43.140
Merci.

01:01:43.140 --> 01:01:44.860
Anne, at what?

01:01:44.860 --> 01:01:46.940
Yes, so we're moving.

01:01:46.940 --> 01:01:48.420
You might have noticed in the schedule

01:01:48.420 --> 01:01:50.500
that Christian Isayed was originally

01:01:50.500 --> 01:01:51.500
supposed to present this.

01:01:51.500 --> 01:01:56.540
But I want to thank Madalina Socer to have accepted

01:01:56.540 --> 01:02:00.140
to present and prepare this presentation for us.

01:02:00.140 --> 01:02:03.900
So she's a Climate Extreme Specialist at Environment Canada.

01:02:03.900 --> 01:02:06.460
She's going to be presenting on the development

01:02:06.460 --> 01:02:10.100
of artificial intelligence downscaling applications

01:02:10.100 --> 01:02:15.660
for medium-range forecasts of weather elements at CCMEP.

01:02:15.660 --> 01:02:19.100
So without further ado, over to you, Madalina.

01:02:19.100 --> 01:02:20.220
OK, thank you.

01:02:20.220 --> 01:02:22.060
I hope you're hearing me well.

01:02:22.060 --> 01:02:23.540
We are, yes.

01:02:23.540 --> 01:02:24.340
OK, great.

01:02:24.340 --> 01:02:26.580
So I am here today to present to you

01:02:26.580 --> 01:02:29.380
some development of artificial intelligence downscaling

01:02:29.380 --> 01:02:31.860
techniques that we're doing at CCMEP in collaboration

01:02:31.860 --> 01:02:33.220
with IBM Research.

01:02:33.220 --> 01:02:35.020
And I would like to acknowledge my co-authors

01:02:35.020 --> 01:02:38.900
that are listed here, both from ECCC and IBM Research.

01:02:43.780 --> 01:02:46.500
So first, I would like to say that the vision for this project

01:02:46.500 --> 01:02:49.180
is actually to help us offer seamless day one to day 10

01:02:49.180 --> 01:02:52.500
public forecast products as part of the transformation

01:02:52.500 --> 01:02:54.740
of the meteorological service of Canada.

01:02:54.740 --> 01:02:56.380
So in order to do this, we need to bridge

01:02:56.380 --> 01:02:58.820
the gap between high-resolution, short-term forecasts,

01:02:58.820 --> 01:03:02.780
and medium-range, lower-resolution forecasts.

01:03:02.780 --> 01:03:04.380
And the reason why we want to do this

01:03:04.380 --> 01:03:06.700
is because we know that insufficient horizontal

01:03:06.700 --> 01:03:09.860
resolution causes forecast errors and especially biases.

01:03:09.860 --> 01:03:12.020
I am showing here a graph of bias,

01:03:12.020 --> 01:03:15.900
a comparison between a low-resolution model in red

01:03:15.900 --> 01:03:17.820
and the high-resolution model in blue.

01:03:17.820 --> 01:03:20.620
And what we are saying here, the closest we are to zero,

01:03:20.620 --> 01:03:21.620
the less bias we have.

01:03:21.620 --> 01:03:25.300
And we really see that increasing horizontal resolution

01:03:25.300 --> 01:03:26.860
is improving this bias.

01:03:26.860 --> 01:03:28.340
Usually, the way that we do that is

01:03:28.340 --> 01:03:29.980
by doing dynamical downscaling.

01:03:29.980 --> 01:03:32.540
So running numerical weather prediction models.

01:03:32.540 --> 01:03:34.900
But this is very computationally expensive,

01:03:34.900 --> 01:03:37.140
which makes it limiting.

01:03:37.140 --> 01:03:39.980
A partial solution to this is doing statistical downscaling,

01:03:39.980 --> 01:03:43.180
which means using past data and deriving

01:03:43.180 --> 01:03:45.740
statistical relationships from this past data,

01:03:45.740 --> 01:03:48.300
such that we apply these relationships

01:03:48.300 --> 01:03:52.820
on the course input and we obtain high-resolution output.

01:03:52.820 --> 01:03:54.820
These type of solutions are limited

01:03:54.820 --> 01:03:56.620
by the fact that we have to impose certain

01:03:56.620 --> 01:03:59.020
non-mathematical relationships in the data.

01:03:59.020 --> 01:04:01.740
So now we have a data-driven alternative,

01:04:01.740 --> 01:04:05.300
which is to apply artificial intelligence techniques

01:04:05.300 --> 01:04:06.460
to do downscaling.

01:04:06.460 --> 01:04:09.020
And what this does is that it allows

01:04:09.020 --> 01:04:11.180
to derive complex relationships in the data

01:04:11.180 --> 01:04:13.260
that we provide to these models.

01:04:13.260 --> 01:04:16.500
So our objective is to develop

01:04:16.500 --> 01:04:19.140
artificial intelligence downscaling techniques

01:04:19.140 --> 01:04:22.500
to downscale weather elements from medium range forecast

01:04:22.500 --> 01:04:23.620
to the kilometric scale.

01:04:23.620 --> 01:04:26.180
And we will hope that this will help both deterministic

01:04:26.180 --> 01:04:28.940
and ensemble forecasting.

01:04:28.940 --> 01:04:31.340
So just to briefly present our project.

01:04:31.340 --> 01:04:33.260
So this is a collaboration that started this year

01:04:33.260 --> 01:04:35.980
between CCMAP and IBM Research.

01:04:35.980 --> 01:04:37.620
And here at Environment Canada, we

01:04:37.620 --> 01:04:39.100
have the meteorological expertise.

01:04:39.100 --> 01:04:43.140
And we definitely have subjects, problems

01:04:43.140 --> 01:04:45.700
that could really benefit from these artificial intelligence

01:04:45.700 --> 01:04:47.180
solutions, but we don't necessarily

01:04:47.180 --> 01:04:49.820
have the artificial intelligence expertise, which

01:04:49.820 --> 01:04:52.780
is where IBM Research comes into play.

01:04:52.780 --> 01:04:54.820
And collaborating with them, they are really experts

01:04:54.820 --> 01:04:57.820
in this field, will allow us to advance much faster.

01:04:57.820 --> 01:05:00.140
And the expected outcome from this collaboration

01:05:00.140 --> 01:05:02.060
that for now it's only meant to be on one year,

01:05:02.060 --> 01:05:05.500
is to develop low-cost and efficient alternatives

01:05:05.500 --> 01:05:09.780
to the computationally expensive dynamical models.

01:05:09.780 --> 01:05:13.100
And the other thing that we want to get from this collaboration

01:05:13.100 --> 01:05:14.900
is we want to learn from IBM Research.

01:05:14.900 --> 01:05:16.540
So at the end of this project, we

01:05:16.540 --> 01:05:19.100
want to enhance our capability at Environment Canada

01:05:19.100 --> 01:05:21.780
to be carrying out this type of research and development

01:05:21.780 --> 01:05:23.740
and eventual operational implementation

01:05:23.740 --> 01:05:27.540
of AI downscaling techniques.

01:05:27.540 --> 01:05:31.100
So the specific goals of our projects are as follows.

01:05:31.100 --> 01:05:33.940
So we are interested in downscaling weather elements.

01:05:33.940 --> 01:05:36.060
And by this, I mean surface winds, temperature,

01:05:36.060 --> 01:05:37.900
and precipitation in the first stage,

01:05:37.900 --> 01:05:41.260
a forecast from the GDPS, which is our global deterministic

01:05:41.260 --> 01:05:44.460
prediction system shown here.

01:05:44.500 --> 01:05:47.300
And runs at a resolution of 15 kilometers

01:05:47.300 --> 01:05:49.500
to the resolution of the HRDPS, which

01:05:49.500 --> 01:05:51.420
is our high-resolution deterministic prediction

01:05:51.420 --> 01:05:55.940
system that is run for 48 hours for now at 2.5 kilometer

01:05:55.940 --> 01:05:56.900
resolution.

01:05:56.900 --> 01:05:59.340
And in this project, we are taking a two-step approach.

01:05:59.340 --> 01:06:02.540
So in the first step, we will be looking at the baseline model

01:06:02.540 --> 01:06:05.660
that is based on generative adversarial networks.

01:06:05.660 --> 01:06:08.020
And in the second stage of the project,

01:06:08.020 --> 01:06:12.940
we will be taking advantage of foundation models

01:06:12.940 --> 01:06:14.940
and tuning them for our application.

01:06:14.940 --> 01:06:16.660
The training data for the models will

01:06:16.660 --> 01:06:19.060
be forecasting data from the GDPS

01:06:19.060 --> 01:06:23.700
as the low-resolution data set and from the HRDPS

01:06:23.700 --> 01:06:25.980
as the high-resolution data set.

01:06:25.980 --> 01:06:28.980
And it is very important for our operational needs

01:06:28.980 --> 01:06:32.580
that the downscale products are available on the HRDPS grid.

01:06:35.100 --> 01:06:37.820
For now in this talk, I will only focus on the baseline model

01:06:37.820 --> 01:06:40.220
as this is a collaboration that just started.

01:06:40.220 --> 01:06:42.140
And we haven't gotten yet to the second part.

01:06:43.540 --> 01:06:47.420
So just briefly, what is a generative adversarial network?

01:06:47.420 --> 01:06:50.860
And generative adversarial networks

01:06:50.860 --> 01:06:53.420
consist of two networks, a generator, two neural networks,

01:06:53.420 --> 01:06:54.620
a generator and discriminator.

01:06:54.620 --> 01:06:55.980
And the way that they work is that they

01:06:55.980 --> 01:06:58.180
are trained in a competing process.

01:06:58.180 --> 01:07:00.420
So the generator pretty much generates images

01:07:00.420 --> 01:07:03.660
that look as much as possible as the real data.

01:07:03.660 --> 01:07:05.820
And the goal of the generator is to just

01:07:05.820 --> 01:07:07.260
fold the discriminator.

01:07:07.260 --> 01:07:09.060
On the other hand, we have the discriminator

01:07:09.060 --> 01:07:12.340
that receives both real data, which in our case is HRDPS

01:07:12.340 --> 01:07:15.220
data, and generated data, and has to decide

01:07:15.220 --> 01:07:18.340
whether this generated data is real or fake.

01:07:18.340 --> 01:07:23.580
And in our case, we use the ANAU et al. 2023 implementation

01:07:23.580 --> 01:07:24.740
of a Vassar SteamGAN.

01:07:27.740 --> 01:07:32.220
I will show you some preliminary results for our project.

01:07:32.220 --> 01:07:36.100
So these preliminary results are based on the WGAN from ANAU

01:07:36.100 --> 01:07:36.580
et al.

01:07:36.580 --> 01:07:37.740
Without any covariates.

01:07:37.740 --> 01:07:40.740
So what this means is that the only data that goes for now

01:07:40.740 --> 01:07:45.020
in this model is zonal and meridional 10-meter wing

01:07:45.020 --> 01:07:45.700
components.

01:07:45.700 --> 01:07:48.940
And this is the data that we are trying to obtain.

01:07:48.940 --> 01:07:51.980
So the low-resolution data comes from GDPS.

01:07:51.980 --> 01:07:54.580
High resolution comes from the HRDPS.

01:07:54.580 --> 01:07:57.300
And so far, we are training the model with one year of data

01:07:57.300 --> 01:07:58.980
that is divided as follows.

01:07:58.980 --> 01:08:01.420
75% of the data is used for training.

01:08:01.420 --> 01:08:03.220
It's about 7,000 forecasts.

01:08:03.220 --> 01:08:05.900
12.5% is used for validation.

01:08:05.900 --> 01:08:08.500
And this validation data is used during the training

01:08:08.500 --> 01:08:09.180
of the model.

01:08:09.180 --> 01:08:12.460
So it's used to stop overfitting the model.

01:08:12.460 --> 01:08:15.300
And then 12.5% of the data is used for testing.

01:08:15.300 --> 01:08:17.580
So once we have a tuned model, we

01:08:17.580 --> 01:08:20.380
will use this data set, about 1,200 forecasts,

01:08:20.380 --> 01:08:24.060
to be seeing how well this model functions.

01:08:24.060 --> 01:08:25.700
Because we are using forecasting data,

01:08:25.700 --> 01:08:26.700
it is a bit difficult.

01:08:26.700 --> 01:08:28.420
Because as you know, forecasts, there

01:08:28.420 --> 01:08:31.140
are increases with forecast lead time.

01:08:31.140 --> 01:08:33.180
And so we might have too much divergence

01:08:33.180 --> 01:08:35.700
between the HRDPS, the high-resolution data set,

01:08:35.700 --> 01:08:38.700
and our low-resolution data set.

01:08:38.700 --> 01:08:40.660
On the other hand, the first six hours of the forecasts

01:08:40.660 --> 01:08:42.660
are affected by the spin-up time of the model.

01:08:42.660 --> 01:08:46.900
So we have decided for now to go with forecasts our 6 to 18.

01:08:46.900 --> 01:08:50.300
And we are using them from the 0,0 and the 12 UTC

01:08:50.300 --> 01:08:52.060
initialization of both models.

01:08:52.060 --> 01:08:57.060
So in this way, we are covering the entire day.

01:08:57.060 --> 01:09:01.780
So our challenge is covering the entire HRDPS domain.

01:09:01.780 --> 01:09:03.980
I am showing here the HRDPS domain.

01:09:03.980 --> 01:09:07.580
It is on the order of 2,500 by 1,200 grid points.

01:09:07.580 --> 01:09:09.660
So it's a very large domain that's

01:09:09.660 --> 01:09:11.740
spanning the width of Canada.

01:09:11.740 --> 01:09:15.180
And it isn't really possible, or at least we don't think

01:09:15.180 --> 01:09:17.460
it's possible so far, to be training directly

01:09:17.460 --> 01:09:19.740
on such a large domain.

01:09:19.740 --> 01:09:22.180
We do not have the computational resources to do that.

01:09:22.180 --> 01:09:24.620
And we also aren't sure exactly how much data

01:09:24.620 --> 01:09:27.300
you would need to be able to train on such a model.

01:09:27.300 --> 01:09:31.340
The now and all implementation, in that implementation,

01:09:31.340 --> 01:09:36.140
the training is done on 16 by 16 pixel low resolution

01:09:36.140 --> 01:09:41.380
patches and 128 by 128 pixel high resolution patches.

01:09:41.380 --> 01:09:45.420
So what we will do is we will adjust to that type of training.

01:09:45.420 --> 01:09:48.780
And in order to do that, the strategy that we have adapted

01:09:48.780 --> 01:09:53.420
is to just select random patches from one forecast

01:09:53.420 --> 01:09:55.620
from our HRDPS data set.

01:09:55.620 --> 01:10:00.540
We will re-read the GDPS data set on the HRDPS domain

01:10:00.540 --> 01:10:04.300
and then course-crain it to go back to its resolution.

01:10:04.300 --> 01:10:06.340
And in this way, we are going to end up

01:10:06.340 --> 01:10:07.900
with this type of patches that we'll

01:10:07.900 --> 01:10:10.380
use to do the training.

01:10:10.380 --> 01:10:12.660
So here is the high resolution data patch

01:10:12.660 --> 01:10:16.180
and the corresponding GDPS low resolution data patch.

01:10:16.180 --> 01:10:20.740
So at each epoch, we are using between 300 and 700 random

01:10:20.740 --> 01:10:21.580
patches.

01:10:21.580 --> 01:10:24.020
And the model that we are showing today

01:10:24.020 --> 01:10:27.860
has been trained on 17,370 epochs.

01:10:27.860 --> 01:10:32.260
So this took about 149 hours to train on one GPU.

01:10:32.260 --> 01:10:34.100
And we have done more than two passes

01:10:34.100 --> 01:10:37.300
through the entire data set.

01:10:37.300 --> 01:10:39.220
So once you have a trained model,

01:10:39.220 --> 01:10:41.700
you can perform inference with the GDPS data.

01:10:41.700 --> 01:10:46.180
And that inference will also be done on 16 by 16 pixel patches.

01:10:46.180 --> 01:10:49.060
So here I have a GDPS input.

01:10:49.060 --> 01:10:50.780
We perform inference.

01:10:50.780 --> 01:10:53.100
And like this, we obtain the downscale forecast,

01:10:53.100 --> 01:10:55.260
the downscale U and V fields.

01:10:55.260 --> 01:10:59.500
And just as a comparison, we have here the HRDPS forecast.

01:10:59.500 --> 01:11:02.220
So the first things that we can say

01:11:02.220 --> 01:11:04.500
is that we are definitely downscaling.

01:11:04.500 --> 01:11:08.660
So we are obtaining information at a small scale.

01:11:08.660 --> 01:11:12.060
It isn't as much as the HRDPS is showing.

01:11:12.060 --> 01:11:14.020
But as I was mentioning before, one problem

01:11:14.020 --> 01:11:16.420
with a low resolution is the fact that you have biases.

01:11:16.420 --> 01:11:18.620
And we are definitely achieving some sort

01:11:18.620 --> 01:11:21.580
of a bias correction.

01:11:21.580 --> 01:11:25.060
Now, of course, you have to parse the entire HRDPS domain.

01:11:25.060 --> 01:11:27.500
One way to do that would be to sequentially process

01:11:27.500 --> 01:11:29.820
128 by 128 patches.

01:11:29.820 --> 01:11:32.860
But that would result into artifacts at the borders.

01:11:32.860 --> 01:11:35.260
So the strategy that we have adopted instead

01:11:35.260 --> 01:11:37.300
is to do some overlapping and then

01:11:37.300 --> 01:11:41.500
to take the median of the ensemble of overlaps.

01:11:41.500 --> 01:11:44.260
And in this way, we managed to patch and obtain this figure

01:11:44.260 --> 01:11:46.580
over the entire domain.

01:11:46.580 --> 01:11:49.340
We have performed validation of our model.

01:11:49.340 --> 01:11:54.620
So we have used the test data, the 1,200 forecast from the GDPS.

01:11:54.620 --> 01:11:57.540
And here I am showing the root mean square error

01:11:57.540 --> 01:12:00.940
for the U-wind component and for the V-wind component.

01:12:00.940 --> 01:12:04.020
And on the bottom, I'm showing the mean absolute error.

01:12:04.020 --> 01:12:06.140
And these are the metrics that are computed

01:12:06.140 --> 01:12:12.420
between the downscale GDPS and the HRDPS corresponding

01:12:12.420 --> 01:12:13.660
verification.

01:12:13.660 --> 01:12:15.740
And we are comparing it with some baselines that

01:12:15.740 --> 01:12:17.260
can be used for interpolation.

01:12:17.260 --> 01:12:19.740
So I'm showing in orange you have bilinear interpolation

01:12:19.740 --> 01:12:22.700
and in green you have nearest neighbor interpolation.

01:12:22.700 --> 01:12:24.620
So as we are seeing in all the metrics,

01:12:24.620 --> 01:12:27.340
the downscale is showing better results,

01:12:27.340 --> 01:12:30.020
is performing better than the other types of interpolation

01:12:30.020 --> 01:12:32.820
for our test data set.

01:12:32.820 --> 01:12:34.900
We have also done a power spectrum analysis

01:12:34.900 --> 01:12:38.580
in order to really quantify how much detail we

01:12:38.580 --> 01:12:40.100
are getting at the small scales.

01:12:40.100 --> 01:12:42.020
And here I'm showing the radially average power

01:12:42.020 --> 01:12:45.780
spectral density between HRDPS in blue and the downscale

01:12:45.780 --> 01:12:48.180
forecast in orange.

01:12:48.180 --> 01:12:51.460
And these power spectra are average over the test data

01:12:51.460 --> 01:12:52.580
set as well.

01:12:52.620 --> 01:12:55.100
What we are seeing is that indeed at small scales,

01:12:55.100 --> 01:12:57.700
we are still not getting enough power.

01:12:57.700 --> 01:13:00.020
So we do not get enough detail.

01:13:00.020 --> 01:13:04.900
Madelina, if you could lend this in one or two minutes.

01:13:04.900 --> 01:13:06.020
Sounds good.

01:13:06.020 --> 01:13:09.700
So of course, we are training so far with one year of data.

01:13:09.700 --> 01:13:12.540
And it would be very interesting to see

01:13:12.540 --> 01:13:16.260
how much more detail we can obtain by training further.

01:13:16.260 --> 01:13:18.460
I'm just showing also an integrated measure

01:13:18.460 --> 01:13:20.020
of the difference in the power spectra.

01:13:20.020 --> 01:13:22.220
And what you are seeing here is that compared

01:13:22.220 --> 01:13:24.860
to the bilinear and the nearest neighbor interpolation,

01:13:24.860 --> 01:13:29.660
the downscaled AI downscaling performance much, much better.

01:13:29.660 --> 01:13:31.780
So it's able to recover way more structure

01:13:31.780 --> 01:13:34.660
at the small scales.

01:13:34.660 --> 01:13:38.340
So the next steps with our forecast, with our project,

01:13:38.340 --> 01:13:40.900
the WGAN needs further development and testing.

01:13:40.900 --> 01:13:43.580
So we are planning on testing with more data.

01:13:43.580 --> 01:13:45.820
A very important thing that we are working on right now

01:13:45.820 --> 01:13:47.700
is to add other covariates.

01:13:47.700 --> 01:13:50.340
So as I said, for now, we are only using windfields.

01:13:50.380 --> 01:13:52.700
But we are adding topography, surface pressure,

01:13:52.700 --> 01:13:54.660
and we are thinking about what other covariates

01:13:54.660 --> 01:13:57.100
may be such escape to add to our model.

01:13:57.100 --> 01:14:00.460
And once we have a baseline that we are satisfied with,

01:14:00.460 --> 01:14:01.980
the important part comes.

01:14:01.980 --> 01:14:04.300
And that is doing a thorough meteorological verification

01:14:04.300 --> 01:14:05.300
of the downscale forecast.

01:14:05.300 --> 01:14:07.660
Because like I said, we have operational goals.

01:14:07.660 --> 01:14:10.380
And we really want to see how this forecast

01:14:10.380 --> 01:14:12.340
respond to our needs.

01:14:12.340 --> 01:14:14.820
Finally, once we finish this first step of the project,

01:14:14.820 --> 01:14:17.820
we are planning on moving to the second more ambitious part,

01:14:18.100 --> 01:14:20.940
which is to develop a large GI model that

01:14:20.940 --> 01:14:23.660
is based on a pre-trained foundation model

01:14:23.660 --> 01:14:26.900
that we will be fine-tuning in order

01:14:26.900 --> 01:14:28.980
to obtain downscale forecasts.

01:14:28.980 --> 01:14:32.180
And this is where, again, collaborating with IBM Research

01:14:32.180 --> 01:14:35.900
is extremely important, as they have very much experience

01:14:35.900 --> 01:14:37.140
in these foundation models.

01:14:37.140 --> 01:14:41.020
And we are hoping to advance at least as fast as now.

01:14:41.020 --> 01:14:42.340
So this is it for me.

01:14:42.340 --> 01:14:44.220
Thank you very much.

01:14:44.220 --> 01:14:48.140
Thank you, Medellina.

01:14:48.140 --> 01:14:49.540
I have a question in the chat.

01:14:49.540 --> 01:14:51.540
I have one.

01:14:51.540 --> 01:14:57.860
I saw that you train, especially, the model.

01:14:57.860 --> 01:14:59.940
But I was surprised to see that you only

01:14:59.940 --> 01:15:02.500
used one year of data to do that.

01:15:02.500 --> 01:15:03.740
Is there a reason behind that?

01:15:03.740 --> 01:15:07.460
Because it seems to me that it's not a lot of data.

01:15:07.460 --> 01:15:10.620
Yes, well, like I said, we have.

01:15:10.620 --> 01:15:15.900
So we do end up having a lot of forecast samples.

01:15:15.900 --> 01:15:22.060
We are training on 128 by 128 pixel patches out

01:15:22.060 --> 01:15:23.060
of a very large domain.

01:15:23.060 --> 01:15:24.900
So it ends up being a lot of data.

01:15:24.900 --> 01:15:26.260
But it is not finalized.

01:15:26.260 --> 01:15:28.420
So here we are in a developing mode.

01:15:28.420 --> 01:15:29.620
We are trying to develop the model

01:15:29.620 --> 01:15:31.300
and make sure it's working properly.

01:15:31.300 --> 01:15:32.940
And this is just the first step.

01:15:32.940 --> 01:15:35.220
We are definitely planning on adding more data

01:15:35.220 --> 01:15:37.900
and seeing how we can improve.

01:15:37.900 --> 01:15:40.540
Thank you, Medellina.

01:15:40.540 --> 01:15:44.060
I don't have a question.

01:15:44.060 --> 01:15:47.700
Otherwise, Anne, I'm going to speak.

01:15:47.700 --> 01:15:48.780
Right on time.

01:15:48.780 --> 01:15:50.940
Thank you, Medellina.

01:15:50.940 --> 01:15:57.100
So now we're going over to Reynel Sospedra Alfonso, who

01:15:57.100 --> 01:16:00.060
is also a research scientist at Environment and Climate

01:16:00.060 --> 01:16:01.540
Change Canada.

01:16:01.540 --> 01:16:05.460
He will be presenting on deep learning-based bias

01:16:05.500 --> 01:16:09.140
adjustments of Arctic sea ice forecasts

01:16:09.140 --> 01:16:12.140
from version three of the Canadian seasonal to

01:16:12.140 --> 01:16:15.020
inter-annual prediction systems.

01:16:15.020 --> 01:16:18.220
Also known as CANSTEP version three.

01:16:18.220 --> 01:16:21.260
So, Reynel, the mic is all yours.

01:16:21.260 --> 01:16:22.700
Thank you.

01:16:22.700 --> 01:16:23.580
Thank you, Anne.

01:16:23.580 --> 01:16:26.420
Can you hear me well?

01:16:26.420 --> 01:16:27.380
Yes, we do.

01:16:27.380 --> 01:16:29.700
Your presentation is not full screen, though.

01:16:29.700 --> 01:16:30.500
It's not.

01:16:30.500 --> 01:16:30.980
No, it's not.

01:16:30.980 --> 01:16:33.980
All right.

01:16:33.980 --> 01:16:34.580
How about now?

01:16:35.740 --> 01:16:36.780
Yes, it is.

01:16:36.780 --> 01:16:37.620
Perfect.

01:16:37.620 --> 01:16:39.380
Thank you so much, Medell.

01:16:39.380 --> 01:16:41.700
Thank you, yes, to the organizer for this opportunity.

01:16:41.700 --> 01:16:43.540
My name is Reynel Sospedra Alfonso.

01:16:43.540 --> 01:16:46.780
I'm a research scientist at the Canadian Center for Climate

01:16:46.780 --> 01:16:49.380
Modeling and Analysis, based in Victoria.

01:16:49.380 --> 01:16:53.180
And I want to start by acknowledging the contributions

01:16:53.180 --> 01:16:55.300
or the work of colleagues at CCMA,

01:16:55.300 --> 01:16:59.020
which made this type of project possible.

01:16:59.020 --> 01:17:01.260
I list some of them down here.

01:17:01.260 --> 01:17:03.620
And I also want to acknowledge the co-authors

01:17:03.620 --> 01:17:05.820
of this presentation, or this work,

01:17:05.820 --> 01:17:09.260
Joseph Martin, Michael Simon, and especially

01:17:09.260 --> 01:17:14.140
Parca Guilla, who has been key for this project going forward.

01:17:14.140 --> 01:17:16.980
He has taken the time to do the implementation, training,

01:17:16.980 --> 01:17:20.860
and testing of the models we have been looking at.

01:17:20.860 --> 01:17:23.540
And I want to mention that this is part, what I'm going to talk

01:17:23.540 --> 01:17:25.460
about here is part of a bigger project

01:17:25.460 --> 01:17:28.780
that we are trying to pursue at CCMA, which

01:17:28.780 --> 01:17:32.860
is the use or applications of machine learning methods,

01:17:32.900 --> 01:17:36.300
in particular deep learning, to post-process our seasonal

01:17:36.300 --> 01:17:38.580
to the scale forecast.

01:17:38.580 --> 01:17:40.780
So for this talk, I will talk in particular

01:17:40.780 --> 01:17:44.140
about the post-processing or seasonal forecast of CIS.

01:17:46.980 --> 01:17:50.140
The seasonal forecast, as you may know,

01:17:50.140 --> 01:17:53.340
CANSIPS is the Canadian seasonal and internal prediction

01:17:53.340 --> 01:17:55.900
system, which provides the Environment and Climate Change

01:17:55.900 --> 01:17:58.580
Canada's operational, probabilistic seasonal

01:17:58.580 --> 01:18:01.700
forecast, both national and global.

01:18:01.700 --> 01:18:06.660
And CANSIPS first appeared or was first

01:18:06.660 --> 01:18:11.420
debuted in 2011 as a two-model forecasting system.

01:18:11.420 --> 01:18:14.540
It has evolved since, and now we are in 2024,

01:18:14.540 --> 01:18:17.620
with the new version of CANSIPS B3,

01:18:17.620 --> 01:18:21.660
which actually will be launched next month.

01:18:21.660 --> 01:18:25.300
And so here, what I'm going to talk about

01:18:25.300 --> 01:18:29.900
is the forecast that we produce with CANIAS M5, which

01:18:29.900 --> 01:18:34.300
is a new model that now we'll be using in CANSIPS.

01:18:34.300 --> 01:18:39.900
And CANIAS M5 is an air system model that is produced at CCMA,

01:18:39.900 --> 01:18:45.460
and now will be then, as I said, used for our seasonal forecast.

01:18:45.460 --> 01:18:47.700
CANIAS M5 air system models, so it

01:18:47.700 --> 01:18:52.460
couples the atmosphere, the ocean, CIS component, land,

01:18:52.460 --> 01:18:57.940
and also biochemistry, both on land and the ocean.

01:18:57.980 --> 01:19:00.100
What we do, we take our climate model,

01:19:00.100 --> 01:19:03.460
we initialize the climate model following the indications

01:19:03.460 --> 01:19:05.340
that you see here on the right.

01:19:05.340 --> 01:19:07.540
So when we initialize the forecast,

01:19:07.540 --> 01:19:10.780
we take, we notch the model towards re-analysis,

01:19:10.780 --> 01:19:15.300
and then we launch those forecasts in time.

01:19:15.300 --> 01:19:17.700
The version of CANIAS M5 that I'm going to be talking about

01:19:17.700 --> 01:19:22.980
is actually an optimal bias-corrected version, which

01:19:22.980 --> 01:19:26.220
follows the work by Sino-Kankarin, which

01:19:26.220 --> 01:19:32.100
does an online bias optimization to the model.

01:19:32.100 --> 01:19:37.500
Now, the work that I'm going to be presenting to you

01:19:37.500 --> 01:19:40.740
deals with the post-processing of those forecasts.

01:19:40.740 --> 01:19:43.020
So here, what you see is a representation

01:19:43.020 --> 01:19:46.620
of those forecasts in black.

01:19:46.620 --> 01:19:49.700
It's the observations that we are verifying,

01:19:49.700 --> 01:19:52.020
observations, the monthly values.

01:19:52.020 --> 01:19:54.900
And then what you see is the representation

01:19:54.900 --> 01:19:57.540
of those forecasts, which are initialized

01:19:57.540 --> 01:20:01.100
at the start of every month during the handcast period.

01:20:01.100 --> 01:20:05.020
So we'll be looking at handcasts from 1980 to 2021.

01:20:05.020 --> 01:20:08.140
We have, for each month in that time,

01:20:08.140 --> 01:20:12.260
we launch an ensemble of forecasts of 10 members,

01:20:12.260 --> 01:20:14.860
which run for 12 months.

01:20:14.860 --> 01:20:17.060
The variable of interest for us here

01:20:17.060 --> 01:20:19.580
is CIS concentration, which is simply

01:20:19.580 --> 01:20:24.500
the fraction of CIS, or the fraction of the grid cells

01:20:24.500 --> 01:20:26.660
that is covered by CIS.

01:20:26.660 --> 01:20:29.780
And this is what we want to adjust.

01:20:33.660 --> 01:20:36.860
Well, we do that looking at the ensemble mean forecast.

01:20:36.860 --> 01:20:39.340
So the adjustment is not done to the ensemble itself,

01:20:39.340 --> 01:20:41.780
it's done to the ensemble mean.

01:20:41.780 --> 01:20:44.300
And the question is, why do we have to do that?

01:20:44.300 --> 01:20:45.660
I mean, after all, we are even doing

01:20:45.660 --> 01:20:49.260
an online bias optimization or bias correction

01:20:49.260 --> 01:20:50.540
of my model.

01:20:50.540 --> 01:20:55.140
So still, we do need to adjust those forecasts,

01:20:55.140 --> 01:20:58.180
because as we know, we have several sources of error,

01:20:58.180 --> 01:21:00.940
structural errors, errors due to initialization,

01:21:00.940 --> 01:21:02.260
and so forth.

01:21:02.260 --> 01:21:04.860
And typically, this is done by doing

01:21:04.860 --> 01:21:08.460
some climatological bias correction to those forecasts.

01:21:08.460 --> 01:21:10.900
Now here, just to give you an example,

01:21:10.900 --> 01:21:14.300
I'm showing you the September CIS concentration

01:21:14.300 --> 01:21:17.540
over the time period 2006 to 2020.

01:21:17.580 --> 01:21:20.500
On the left, these are the very fine observations

01:21:20.500 --> 01:21:25.820
that we use, which comes from NOAA data products.

01:21:25.820 --> 01:21:28.820
And here, again, this is the CIS concentration,

01:21:28.820 --> 01:21:31.180
which has value from 0 to 1.

01:21:31.180 --> 01:21:35.220
And we see on the right, the growth forecast

01:21:35.220 --> 01:21:36.820
as it comes out of the model.

01:21:36.820 --> 01:21:40.260
So this is the output directly coming from our forecast.

01:21:40.260 --> 01:21:43.300
And what we see is that there is definitely

01:21:43.300 --> 01:21:45.940
a strong bias that we notice, particularly

01:21:45.940 --> 01:21:49.740
in the center Arctic, where we have a much lower CIS

01:21:49.740 --> 01:21:53.260
concentration relative to observations.

01:21:53.260 --> 01:21:56.340
Now on the right to that, we see the bias adjusted forecast,

01:21:56.340 --> 01:21:59.700
in which we compensate or we subtract the bias,

01:21:59.700 --> 01:22:01.660
compute it on a previous time.

01:22:01.660 --> 01:22:04.540
And then what we see is that we adjust somehow

01:22:04.540 --> 01:22:08.420
that forecast by doing this simple bias correction.

01:22:08.420 --> 01:22:11.060
Now still, even after doing a bias correction,

01:22:11.060 --> 01:22:13.940
we see that there are some differences

01:22:13.940 --> 01:22:16.260
between the observed CIS concentration

01:22:16.260 --> 01:22:18.180
and the one that is bias adjusted.

01:22:18.180 --> 01:22:20.620
So that tells us that we need to do something else in order

01:22:20.620 --> 01:22:23.420
to actually improve our forecast.

01:22:23.420 --> 01:22:25.260
And for that, and this is now the project

01:22:25.260 --> 01:22:27.780
that we are working on, is to use this machine learning

01:22:27.780 --> 01:22:31.940
or deep learning method to improve even further

01:22:31.940 --> 01:22:33.540
those forecasts.

01:22:33.540 --> 01:22:35.180
The method that I'm going to use here,

01:22:35.180 --> 01:22:37.140
and I'm going to talk a little bit more about that

01:22:37.140 --> 01:22:40.420
in the few slides, is the unit.

01:22:40.420 --> 01:22:42.540
Probably most of the people in the audience

01:22:42.540 --> 01:22:44.020
are familiar with units.

01:22:44.020 --> 01:22:46.300
As I said, I will talk a little bit more about that.

01:22:46.300 --> 01:22:51.220
But here, I'm just showing you some results in which you see

01:22:51.220 --> 01:22:57.020
how the unit is able to reproduce the spatial pattern a lot

01:22:57.020 --> 01:23:01.500
better than what we can do with a simple bias correction.

01:23:01.500 --> 01:23:04.220
I should have said that this is a forecast done

01:23:04.220 --> 01:23:05.580
at two monthly time.

01:23:05.580 --> 01:23:07.580
So this is the forecast two months

01:23:07.580 --> 01:23:10.220
after the forecast is initialized,

01:23:10.260 --> 01:23:17.420
where the biases are, you can see, are particularly strong.

01:23:17.420 --> 01:23:21.700
Now, the unit, this is the tool of choice.

01:23:21.700 --> 01:23:25.620
What we have here is a fully connected network,

01:23:25.620 --> 01:23:27.660
or a fully convolutional network,

01:23:27.660 --> 01:23:33.460
that has a downscaling, down sampling encoder,

01:23:33.460 --> 01:23:38.460
followed by up sampling decoder.

01:23:38.460 --> 01:23:39.900
This is a classical unit.

01:23:39.900 --> 01:23:43.860
What we see is that the future maps are reduced in size.

01:23:43.860 --> 01:23:48.740
So we see that the resolution is reduced by its health,

01:23:48.740 --> 01:23:50.580
and the channels are increased by two.

01:23:50.580 --> 01:23:53.900
And this allows us to have a better representation

01:23:53.900 --> 01:23:57.820
of capacity of the network, while preserving some information

01:23:57.820 --> 01:23:59.980
of the image that we input.

01:23:59.980 --> 01:24:03.900
So to be clear, what we input here is our raw forecast,

01:24:03.900 --> 01:24:07.220
which is denoted by the YMN.

01:24:07.300 --> 01:24:10.300
Thanks, and will be the resolution of my forecast,

01:24:10.300 --> 01:24:12.540
which is a function of the initial month

01:24:12.540 --> 01:24:17.220
and the target month, and the time relative to the first month

01:24:17.220 --> 01:24:20.900
in the data that we are inputting.

01:24:20.900 --> 01:24:24.020
So the input is made of six channels,

01:24:24.020 --> 01:24:28.100
one channel that is the actual variable

01:24:28.100 --> 01:24:31.500
that we want to, or the actual map that we want to correct,

01:24:31.500 --> 01:24:33.980
plus five temporal features that takes into account,

01:24:33.980 --> 01:24:36.340
again, the initial month that we're looking at,

01:24:36.380 --> 01:24:38.540
the target month, and this temporal information

01:24:38.540 --> 01:24:40.460
relative to the initial time.

01:24:40.460 --> 01:24:43.420
So in other words, how many years and how many months

01:24:43.420 --> 01:24:45.500
from the starting of your data.

01:24:45.500 --> 01:24:47.340
You input that into the network,

01:24:47.340 --> 01:24:49.660
and then the output is, hopefully,

01:24:49.660 --> 01:24:51.500
is an adjusted forecast,

01:24:51.500 --> 01:24:54.180
which then correct for those biases

01:24:54.180 --> 01:24:55.580
that I mentioned earlier.

01:24:57.700 --> 01:24:59.580
Moving a little bit forward here,

01:24:59.580 --> 01:25:01.340
let me just be a little bit more precise

01:25:01.340 --> 01:25:03.500
on the kind of task that we are doing.

01:25:03.500 --> 01:25:04.940
This is just a specific example,

01:25:04.940 --> 01:25:08.700
which hopefully will clarify how we do this.

01:25:08.700 --> 01:25:11.100
So in this case, let's say that we want to adjust

01:25:11.100 --> 01:25:13.700
the March CIS concentration forecast,

01:25:13.700 --> 01:25:17.060
which is initializing February of a year Y.

01:25:17.060 --> 01:25:20.500
So essentially it will be this red dot denoted here

01:25:20.500 --> 01:25:21.700
on the figure.

01:25:21.700 --> 01:25:25.900
So again, what we want to do is to post-process this forecast.

01:25:25.900 --> 01:25:27.740
We leverage the forecast that are produced

01:25:27.740 --> 01:25:28.900
with our climate model.

01:25:28.900 --> 01:25:30.620
We do not do prediction.

01:25:30.620 --> 01:25:33.060
We just do that kind of bias adjustment

01:25:33.060 --> 01:25:34.980
or post-processing of the predictions

01:25:34.980 --> 01:25:36.980
that we get from our climate model.

01:25:36.980 --> 01:25:41.300
So for this specific task, to correct that March CIS,

01:25:41.300 --> 01:25:46.180
we train on the data that is available for the years

01:25:46.180 --> 01:25:48.900
before the test year that we have.

01:25:48.900 --> 01:25:50.700
So in this case, I'm denoting that here

01:25:50.700 --> 01:25:53.020
for this in this shadow region.

01:25:53.020 --> 01:25:57.260
So we take all the pairs of forecast and observations

01:25:57.260 --> 01:25:59.980
for all lead times and all target month,

01:25:59.980 --> 01:26:02.460
and then we train our network on that data,

01:26:02.460 --> 01:26:05.260
and then we will do that iteratively for every test

01:26:05.260 --> 01:26:10.060
years that we want to make the adjustment.

01:26:10.060 --> 01:26:12.780
So what I'm going to do now is to show you some of the results

01:26:12.780 --> 01:26:13.780
that we have.

01:26:13.780 --> 01:26:18.900
I should say that this is very much a work in progress.

01:26:18.900 --> 01:26:23.580
And that is different avenues that we are working on,

01:26:23.580 --> 01:26:27.140
but we have some results that I want to share with you.

01:26:27.140 --> 01:26:31.060
For instance, what we see here is the CIS concentration bias.

01:26:31.060 --> 01:26:35.940
At the zero-month lead, average over the 2006 and 2020 time

01:26:35.940 --> 01:26:41.180
period, which is the test years that we have for our analysis.

01:26:41.180 --> 01:26:45.740
At the top, we have the bias for the March CIS concentration,

01:26:45.740 --> 01:26:49.900
which is at the time of maximum CIS extent.

01:26:49.900 --> 01:26:52.740
And at the bottom, we have the results or the bias

01:26:52.740 --> 01:26:55.780
for the September CIS concentration,

01:26:55.780 --> 01:26:59.140
which is at the time of a minimum CIS extent.

01:26:59.180 --> 01:27:02.300
On the left, we see here what happened with the raw forecast.

01:27:02.300 --> 01:27:06.700
We see the biases happening in several regions,

01:27:06.700 --> 01:27:08.020
which is actually substantial.

01:27:08.020 --> 01:27:10.460
Here, the bias is given in percent.

01:27:10.460 --> 01:27:12.060
For the bias adjusted case, which

01:27:12.060 --> 01:27:15.100
is the simple bias correction that I'm using here

01:27:15.100 --> 01:27:17.420
as a benchmark, we see that we do improve

01:27:17.420 --> 01:27:19.420
relative to the raw forecast, but there are still

01:27:19.420 --> 01:27:22.180
some biases that are apparent, particularly

01:27:22.180 --> 01:27:24.380
during the CIS minimum.

01:27:24.380 --> 01:27:26.620
And then on the right is the results

01:27:26.620 --> 01:27:27.860
that we get with the unit.

01:27:27.860 --> 01:27:31.660
And we see that the bias are likely removed.

01:27:31.660 --> 01:27:33.620
And now, this is for the zero-month lead.

01:27:33.620 --> 01:27:37.700
So this is soon after we initialize our forecast.

01:27:37.700 --> 01:27:39.820
Now, two months ahead in our forecast,

01:27:39.820 --> 01:27:42.300
of course, the biases are increased.

01:27:42.300 --> 01:27:45.020
Excuse me, we have two minutes.

01:27:45.020 --> 01:27:45.540
Two minutes.

01:27:45.540 --> 01:27:47.460
Thank you.

01:27:47.460 --> 01:27:49.060
All right, so here is the case in which we

01:27:49.060 --> 01:27:50.580
have two-month lead forecasts.

01:27:50.580 --> 01:27:52.460
And we see that the bias, of course, are increased.

01:27:52.460 --> 01:27:55.660
But still, we are able to manage those biases with the unit,

01:27:55.700 --> 01:28:01.180
given a better representation of the CIS edges.

01:28:01.180 --> 01:28:03.060
Another measure that we look at here

01:28:03.060 --> 01:28:07.460
is the integrated ICH error, which is essentially the area,

01:28:07.460 --> 01:28:09.220
the integrated area that we have,

01:28:09.220 --> 01:28:12.460
in which both forecast and observations

01:28:12.460 --> 01:28:16.380
disagree in the concentration with a threshold of 15%.

01:28:16.380 --> 01:28:21.020
So we both have more than 50% concentration

01:28:21.020 --> 01:28:23.140
or less than 50%.

01:28:23.140 --> 01:28:24.860
This binary error will be zero.

01:28:24.900 --> 01:28:27.060
If they are different, then the binary error will be one.

01:28:27.060 --> 01:28:32.140
And those grid cells will contribute to this area error.

01:28:32.140 --> 01:28:35.140
So this is at the top here, we see a hit map

01:28:35.140 --> 01:28:38.540
in which we have our target month in the X axis.

01:28:38.540 --> 01:28:40.940
And on the Y, we have the lead month.

01:28:40.940 --> 01:28:44.860
And bottom message here is that the blue are bad.

01:28:44.860 --> 01:28:48.700
The red are good, meaning there is less error.

01:28:48.700 --> 01:28:52.140
And the unit bids both the row, of course,

01:28:52.140 --> 01:28:54.500
and the bias are used to forecast.

01:28:54.500 --> 01:28:56.740
Down here is just an integration over lead month

01:28:56.740 --> 01:28:58.580
just to give a more clear picture

01:28:58.580 --> 01:29:02.140
of how the unit outperforms the alternative.

01:29:03.180 --> 01:29:04.740
Now, I know that I don't have much time,

01:29:04.740 --> 01:29:09.060
so I will not go into the details of these results,

01:29:09.060 --> 01:29:11.460
but at least to give you an idea of what is happening.

01:29:11.460 --> 01:29:14.620
In this case, we are looking at the CIS area.

01:29:14.620 --> 01:29:15.780
Same time of floods.

01:29:15.780 --> 01:29:20.380
Again, red means that we have a better room mean

01:29:20.380 --> 01:29:21.900
and square error, which is the measure

01:29:21.980 --> 01:29:24.820
that we are using here for the CIS area.

01:29:24.820 --> 01:29:27.100
Blue means that the errors are increased.

01:29:27.100 --> 01:29:29.660
And in this particular case, we see that the unit

01:29:29.660 --> 01:29:33.420
is slightly better than the bias corrected one.

01:29:33.420 --> 01:29:35.900
However, if we look at CIS extent,

01:29:35.900 --> 01:29:40.260
which is now the area in which we count for all those grid cells

01:29:40.260 --> 01:29:42.620
with concentration greater than 50%,

01:29:42.620 --> 01:29:45.140
we see that unit largely outperform

01:29:45.140 --> 01:29:46.580
this bias-adjusted method.

01:29:47.580 --> 01:29:52.740
Finally, I want just to mention that here,

01:29:52.740 --> 01:29:55.700
we have seen different measures of the skill

01:29:55.700 --> 01:29:58.980
in which we outperform both the benchmark

01:29:58.980 --> 01:30:01.740
and the raw forecast, but there is still some issues

01:30:01.740 --> 01:30:04.980
in terms of the representation of the temporal dependence

01:30:04.980 --> 01:30:06.500
of our adjusted forecast.

01:30:06.500 --> 01:30:10.180
And so, but unlike for this slide here,

01:30:10.180 --> 01:30:12.860
just to mention that we still have some work to do

01:30:12.860 --> 01:30:17.380
to be able to better represent the temporal dependence

01:30:17.380 --> 01:30:19.220
of those forecasts.

01:30:19.220 --> 01:30:21.060
And because I don't have much more time,

01:30:21.060 --> 01:30:24.300
I will finish with this slide, which is some final remarks.

01:30:24.300 --> 01:30:25.900
We'll leave it there for you.

01:30:25.900 --> 01:30:28.940
And I will be happy to take any questions.

01:30:28.940 --> 01:30:29.780
Thank you.

01:30:31.620 --> 01:30:32.940
Thank you, Renel.

01:30:32.940 --> 01:30:37.940
I saw that there seems to be to have a seasonal pattern

01:30:38.260 --> 01:30:39.780
in your verification.

01:30:39.780 --> 01:30:42.980
That leads me to the questions.

01:30:42.980 --> 01:30:47.180
Are there any biases that are harder to adjust

01:30:47.180 --> 01:30:48.420
with this method?

01:30:50.420 --> 01:30:52.340
The answer for that, yes.

01:30:52.340 --> 01:30:56.220
And this is anality that we see has to do in part

01:30:56.220 --> 01:30:58.020
with how the eyes behave.

01:30:58.020 --> 01:31:01.100
We have what is known as this predictability barrier

01:31:01.100 --> 01:31:03.180
after spring, which makes it difficult

01:31:03.180 --> 01:31:05.740
to do a good prediction of the sea eyes.

01:31:05.740 --> 01:31:07.580
This is something that perhaps we can see

01:31:07.700 --> 01:31:11.380
all those metrics, actually perhaps we'll go to this one.

01:31:11.380 --> 01:31:15.260
So this barrier, which reduces the skill

01:31:15.260 --> 01:31:16.620
that we have in our predictions,

01:31:16.620 --> 01:31:20.740
can be seen here around the month of June to October,

01:31:20.740 --> 01:31:22.540
looking here at target month.

01:31:22.540 --> 01:31:26.260
And so that is something within the raw forecast

01:31:26.260 --> 01:31:29.740
in person, the bias corrected one, but also in the unit.

01:31:29.740 --> 01:31:32.100
So we do improve on those months,

01:31:32.100 --> 01:31:35.060
but still there is some skill that is missing there,

01:31:35.060 --> 01:31:37.140
which is part of the natural process

01:31:37.140 --> 01:31:39.180
in the sea eyes formation and melting,

01:31:39.180 --> 01:31:40.700
which then translate into the skill

01:31:40.700 --> 01:31:42.540
that we see in our just forecast.

01:31:43.700 --> 01:31:45.020
Thank you.

01:31:45.020 --> 01:31:49.620
One last question is coming from the online Q&A.

01:31:50.580 --> 01:31:53.300
Would training the model on all months of the year

01:31:53.300 --> 01:31:55.820
versus only certain months affect its ability

01:31:55.820 --> 01:31:56.900
to improve the forecast?

01:31:56.900 --> 01:32:00.060
For example, forecasts of sea eyes in spring

01:32:00.060 --> 01:32:01.620
are known to perform poorly.

01:32:01.620 --> 01:32:03.700
So if you were to exclude these months,

01:32:04.220 --> 01:32:07.340
would this improve the bias adjustment?

01:32:07.340 --> 01:32:09.260
Yeah, that's a good question.

01:32:09.260 --> 01:32:11.100
Actually, when I was, I mean, it's too bad

01:32:11.100 --> 01:32:12.660
that I'm rushing through all these slides,

01:32:12.660 --> 01:32:14.500
but one of the things that I wanted to mention

01:32:14.500 --> 01:32:18.140
at this particular slide is that we do the training.

01:32:18.140 --> 01:32:21.140
You're looking at the previous years in my forecast,

01:32:21.140 --> 01:32:22.820
but we are missing some of the information

01:32:22.820 --> 01:32:26.460
of more recent months that can also contribute

01:32:26.460 --> 01:32:27.300
to the forecast.

01:32:27.300 --> 01:32:29.060
Like looking here, for instance,

01:32:29.060 --> 01:32:31.740
we do not use information from January

01:32:31.740 --> 01:32:33.500
of this specific year.

01:32:33.500 --> 01:32:35.100
Now, one thing that we are exploring

01:32:35.100 --> 01:32:38.060
is to look at training based on lead times

01:32:38.060 --> 01:32:39.060
or a specific month.

01:32:39.060 --> 01:32:41.620
I think that that's the idea of the question,

01:32:41.620 --> 01:32:45.820
which we try to train our model specific to the lead times

01:32:45.820 --> 01:32:48.540
in which we have the biases that we want to correct

01:32:48.540 --> 01:32:53.020
and the kind of information specific to the seasonality

01:32:53.020 --> 01:32:54.700
that we want also to correct.

01:32:54.700 --> 01:32:56.740
So those are things that we are exploring,

01:32:56.740 --> 01:32:59.580
but so far this is what we have.

01:33:00.580 --> 01:33:02.580
Thank you, Rénel.

01:33:02.580 --> 01:33:07.580
This concludes our first block for the first part

01:33:07.580 --> 01:33:10.580
of the presentation for artificial intelligence.

01:33:10.580 --> 01:33:13.580
A big thank you to Rénel, Madlena, Saïd, Hervé,

01:33:13.580 --> 01:33:17.580
and Christopher for having made these presentations.

01:33:17.580 --> 01:33:19.580
Anne, I'll leave you here.

01:33:19.580 --> 01:33:23.580
Yes, a big thank you to all the presenters,

01:33:23.580 --> 01:33:25.580
all the attendees as well.

01:33:25.580 --> 01:33:28.580
And as several mentioned before,

01:33:28.580 --> 01:33:31.580
there is a second session coming up in about 20 minutes,

01:33:31.580 --> 01:33:32.580
not even.

01:33:32.580 --> 01:33:35.580
So we'd love to see you back here,

01:33:35.580 --> 01:33:40.580
a lot more to see and to hear from people outside ECCC

01:33:40.580 --> 01:33:41.580
as well.

01:33:41.580 --> 01:33:42.580
So please do join us.

01:33:42.580 --> 01:33:43.580
Thank you.

01:33:43.580 --> 01:33:44.580
There's not a sign.

01:33:44.580 --> 01:33:47.580
There's not a sign, that's it.

01:33:47.580 --> 01:33:48.580
Thanks.

01:33:48.580 --> 01:33:49.580
You're better.

01:33:49.580 --> 01:33:50.580
Bye.

01:33:50.580 --> 01:33:51.580
Bye.

01:33:55.580 --> 01:33:56.580
Thank you.

