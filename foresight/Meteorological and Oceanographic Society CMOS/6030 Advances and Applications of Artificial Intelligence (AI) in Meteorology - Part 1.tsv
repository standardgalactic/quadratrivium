start	end	text
0	6000	Let me try for you. Okay.
6000	28000	Thank you.
28000	41000	Thank you.
58000	78000	Thank you.
88000	103000	Hello and welcome.
103000	109000	Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel
109000	113000	Tremblay. We both work for the Meteorological Service of Canada within
113000	118000	Environment and Climate Change Canada and we're happy to be here exploring AI
118000	124000	with you for a second year. Before we begin, a few reminders. 15 minutes are
124000	128000	reserved for each presentation and we encourage our presenters to keep three
128000	132000	minutes for questions at the end. Time permitting participants with questions
132000	137000	will be able to use the chat at the end of the presentation and their questions
137000	145000	can be upvoted by the other attendees. Miguel will provide up at 10 and 12
145000	150000	minutes and will bring the presentation to an end at 15 and will be ruthless
150000	155000	guys because we want everyone to have time to present. Miguel, do you want to do
155000	178000	a quick recap in French?
178000	184000	A few reminders. 15 minutes are reserved for each presentation and we encourage
184000	196000	our presenters to keep time for questions at the end of the presentation and we
196000	202000	will be able to use the chat at the end of the presentation and we will be able to
202000	207000	use the chat at the end of the presentation and we will be able to use the chat at
207000	212720	the end of the presentation and we will be able to use the chat
222000	231000	fpspe conditional 10 minutes for one and a half minutes for 10 minutes for
231000	235400	weather forecasting, and ECCC's research plans.
235400	238600	So without further ado, over to you, Christopher.
238600	239440	Thank you.
239440	242540	Let me get the screen sharing going and share.
243880	246400	Okay, yep, looks like we're good.
246400	247760	Okay, hello everybody.
247760	248600	I'm Christopher Subic
248600	250200	from Environment and Climate Change Canada.
250200	252040	I'm here to speak as said about,
254400	256080	well, click, okay.
256080	257760	This talk is essentially in two parts.
257760	260120	The first part is going to be a brief summary
260120	263520	of the current state of machine learning based forecasting
264640	269400	with a broad focus on medium range weather forecasts.
270200	273800	Second part of this talk will be the forthcoming AI roadmap
273800	276520	out of the atmospheric science
276520	278280	of technology director at NCCMAP,
279280	282640	which effectively broadly sets out
282640	286080	where this part of Environment and Climate Change Canada
286080	289920	will be going in the short of medium term future with AI.
290240	291840	And finally, time permitting,
291840	295040	I'll conclude with a preview of talks to come
295040	300040	and just general thoughts on the state of AI in forecasting.
300240	302400	So in case you haven't noticed,
302400	304040	AI is becoming a big deal.
304040	306040	It's no longer just pictures of cats
306040	308040	or helping kids cheat on their homework,
308040	312880	but it's starting to influence the industries
312880	316120	and feels previously thought unassailable.
317120	320920	Now, this isn't truly a stranger to weather forecasting.
320920	323560	AI adjacent topics have long had roles
323560	327760	in the forecast production system.
328800	330640	We've long had statistical models
330640	332880	for quality control of observational data
332880	334680	and for post-processing,
334680	338640	but a phase change has been that over the past two years,
338640	342360	maybe three models from the academic and private sectors
342360	345800	have gone from things that we should probably
345800	347840	keep our eye on as interesting things
347840	350120	for the long-term future to, well,
350120	354000	these two nearly full-featured forecast systems
354000	356280	that are competitive with the state of the art.
358040	361000	With that in mind, I can just state very plainly
361000	362920	that Environment Climate Change Canada,
362920	367040	in particular my part of it, is taking AI very seriously
367040	369360	and we intend for it to have an increasing role
369360	373480	in numerical weather prediction systems going forward.
373600	376400	We're taking a sort of trust but verify approach
376400	379760	in that we intend to make AI advances operational,
379760	381880	but only after they've been scientifically proven.
381880	386880	We're not in a rush to perform science by press release
387480	391440	and this is also a medium to long-term plan.
391440	393680	Capacity building is going to take years,
393680	397920	both in terms of acquiring sufficient computational power
397920	400080	for this and developing the human expertise
400080	401880	necessary to properly use it.
403600	408600	The modern AI forecast field essentially began
409720	410680	about two years ago.
410680	414000	I dated from the first publication of GraphCast
414000	417320	by Lam in 2023, which is the first time
417320	421680	that an AI model could truly claim to beat
421680	424840	the state of the art from, in this case,
424840	427920	the equivalent forecast from ECMWF.
430320	432720	In addition, the truly shocking claim
432760	434640	was that these forecasts came with orders
434640	436720	of magnitude faster running time.
436720	439360	GraphCast will produce a 10-day quarter-degree forecast
439360	442000	in about 30 seconds on a single GPU.
442920	446840	That put all of the weather centers globally on notice
446840	450080	that if these trends continue, we must adapt
450080	451280	or we'll fall behind.
451280	454080	And I mean, in some sense, research has always been that.
454080	457320	You can't sit on your laurels from 20 years ago
457320	458600	and pretend to be competitive,
458640	463640	but this field is truly advancing at a revolutionary rate.
468800	471320	Models are being constantly released.
471320	473320	Any summary, like the one I'm about to give
473320	474680	was going to be out of date within weeks.
474680	477320	In fact, just this morning, I've had to update my slides
477320	479600	to add two new preprints that have come out
479600	483320	in the past four or five days.
483320	485120	Nonetheless, there are some common features
485120	487880	between medium-range models that are shared
487880	489920	by, if not all of them, then most of them.
491200	493520	Most broadly speaking, these models attempt to learn
493520	495480	from data, which means rather than simulate
495480	497800	the atmosphere from first principles,
497800	502600	they try to look at some form of data
502600	504920	and predict what it will become.
504920	506800	In this case, for the medium-range forecasting,
506800	509360	the data is almost always the error of five reanalysis.
509360	511640	That's our highest quality, longest-term,
511640	513960	and most uniform record of the atmospheric state
513960	517240	that we have, and it's accepted as ground truth
517240	519840	for most of the AI models, but this data set
519840	522960	does have known issues such as relatively poor precipitation.
524880	527880	The other common feature about AI forecast models
527880	530600	is that most of them have sparse limited outputs.
530600	533600	They tend to predict only a few variables.
533600	537080	They tend to predict a small subset of vertical levels
537080	540160	compared to what we're used to from an operational forecast,
540160	542960	and they have a limited selection of lead times.
544320	546440	This creates new downscaling problems.
546440	548720	If you were in the plenary talk,
548720	551600	you heard our friend from Nvidia say that
553280	556600	this might not be a barrier to full prediction,
556600	559000	but at the same time, we're used to having the prediction
559000	561240	plus all of the rich output for free,
561240	565440	and not having that means we'll need to contemplate
565440	567160	new kinds of downscaling problems
567160	571520	to get rich data from a sparse forecast.
571520	573800	And finally, these AI forecast systems
573800	577440	have essentially no proofs of physical consistency.
577440	582440	Even hybrid models and development in that area
582760	584600	has a classical dynamical core,
584600	586920	but leaves a physics system that is
588080	589920	only tangentially concerned with things
589920	591000	like conserving energy.
592560	595040	Now, medium range forecasts tend to break down
595040	596360	into a few different categories.
596360	599080	The first and most conventional of these
599080	600960	are deterministic models.
600960	602440	These are analysis predictors,
602440	604320	and they essentially answer the question of,
604320	606960	if I have the atmospheric analysis at time zero,
606960	608680	what is the minimum error prediction
608680	611560	of what that analysis is going to be six hours from now?
612600	614120	As a general rule, these tend to give
614120	617480	overly smooth forecasts that over long lead times,
617480	619880	erase fine scale structure in the atmosphere.
619880	622880	The widely held belief is that this is a consequence
622880	627880	of training based on mean squared error measures,
629040	631080	because the lowest mean squared error prediction
631080	634000	in the future is your ensemble average.
634000	635520	But the ensemble average is not
635520	637840	a physically plausible forecast.
639520	642640	Now, that being said, these models have shown great success,
642640	645880	and having a really good ensemble mean prediction
645880	647640	of the future is still a really good prediction
647640	648480	of the future.
648480	651120	Each day of predictability translates to billions
651120	655440	or billions of dollars of enabled economic activity.
656440	661280	This category of models is in some sense the oldest,
661280	663080	and I really hesitate to use that word
663080	665200	with a field that's about two or three years old,
665200	668120	but they're models from many different groups.
668120	671160	Graphcast and ForecastNet have been previously mentioned.
671160	672680	Graphcast is a graph neural network.
672680	676600	ForecastNet is now a spectral Fourier neural operator
676600	680840	that operates in a spherical harmonic space.
680840	683040	Pangu weather came out as a vision transformer,
683040	686720	and AIFS is now apparently officially published
686720	689120	with a pre-print, and it's a graph transformer.
691360	693360	The jargon here is not that important,
693360	696760	but the underlying point is that you can reach
696760	698120	a deterministic forecast
698120	701320	with many different AI architectures.
701320	705760	There's, as of yet, no specific Royal Road to a forecast.
706880	709120	The second category that I've somewhat arbitrarily divided
709120	711200	things into are ensemble models.
711200	714160	These try to address the problem of overly smooth forecast
714160	716840	by adding some element of randomness.
716840	718840	An ensemble forecaster will receive
718840	720520	some kind of random data source,
720520	723840	either a random number generator or a field of noise,
723840	726640	and it's asked to generate essentially different forecasts
726640	728520	from the same seven initial conditions.
728520	733480	This more or less solves the problem of smoothing
733480	735480	based on mean squared error loss functions
735480	739360	because each individual forecast no longer has to track
739360	744720	the long-term forecast.
744720	746440	No longer has to track the long-term truth,
746440	747840	but each one can be plausible,
747840	751000	and then you're tracking the future
751000	752840	with a true ensemble mean.
752840	754560	Now, the downside is that these models
754560	756560	are all more expensive to train and run
756560	759320	the deterministic systems of the same size.
759320	762560	In operations, you will need at least one inference
762560	764120	run per ensemble member.
764120	769040	So, graphcast's 30-second, 10-day forecast is great,
769040	771240	but if I want a 100-member ensemble,
771240	774520	now I'm talking an hour or two,
774520	778000	and that can add up very quickly.
778000	779720	In terms of training, if you're training
779720	781080	with an ensemble error measure
781080	784080	like the cumulative rank probabilistic score,
784080	786040	that requires training over an ensemble,
786040	789120	and increasing the size of things during training
789120	793600	tends to increase the scarce GPU memory requirements
793600	798280	and the overall cost of building the system.
798280	800160	This is a newer frontier of AI forecasting.
800160	802960	It's probably going to be more popular in the months to come,
802960	805960	and the examples here are also somewhat newer.
805960	808400	Gencast was previously mentioned in our plenary.
808400	811400	It's a diffusion model based on graph transformer network.
811400	815560	Neural GCM is a hybrid model, also previously mentioned,
815560	817800	and it's a dynamical core
817800	819920	that has learned physics parameterizations,
819920	822000	and in particular, can operate in a non-subtle mode,
822000	823680	which is why I've included it here.
823680	826280	And finally, there are models such as seeds,
826280	829400	also out of Google, that don't try to forecast directly,
829400	833160	but they try to take an ensemble mean
834280	835800	that already exists from some method,
835800	837280	like a traditional forecasting system,
837280	839000	and generate new ensemble members
839000	841520	to fill out the probability distributions.
842600	844760	The final category of forecast models,
844760	847640	and I'm going to divide things into our foundation models,
847640	849200	and these essentially answer the question of,
849200	852320	what if we had GPT-4, but for weather?
852320	855400	The idea here is and shared by foundation models
855400	856680	that exist in our development,
856680	859880	is that you break up a weather state,
859880	861880	like the analysis, into tokens
861880	864920	by regionally subdividing it usually,
864920	867400	and then you ask the foundation model
867400	869520	to predict missing pieces of it.
869520	872120	You say, you can mask out the future,
872120	873480	and say, okay, predict this,
873480	876600	and then you're training it in a forecast context,
876600	879840	or you can mask out a missing regional piece,
879840	881160	and ask it to fill in the blank.
881160	883840	You could even ask it to predict the past, I suppose.
885640	888000	This allows the foundation model
888000	891720	to be trained on qualitatively different data sources.
891720	896120	For example, you might have an encoder suite
896120	898280	that takes satellite observations directly,
898280	900740	and tries to turn it into token space,
901800	904720	or the analysis from era five,
904720	909120	or lower resolution climate forecasts.
910320	913760	And once tokenized, the idea of a foundation model
913760	916480	is that you have a giant middle processor layer
916480	918440	that operates in this latent space,
918440	920840	and from there it is relatively simple
920840	923760	to build out decoder models to give you things you want,
923760	925440	like tomorrow's forecast today,
925440	929320	or what the radar is going to look like in 30 minutes.
930480	933180	Foundation models certainly prove their worth
933180	936080	with text-based processing like GPT,
936080	938560	and they're also very popular and emerging
938560	940840	in Earth observation applications
940840	942800	with interpretation of satellite data.
942800	947400	For example, at an ECMWFESA conference a few weeks ago,
947400	948560	there were some interesting talks
948560	951000	about taking satellite data
951000	955800	and using it to infer tree cover near power lines in Norway,
955800	957040	tasks that would normally take
957040	958840	some very expensive helicopters,
958840	960740	and doing them much, much more cheaply
960740	963720	with readily available Earth observation data.
963720	965440	The downside is that foundation models
965440	967920	tend to be extremely expensive to train,
967920	970020	and these will push the limits
970060	972860	of what the public sector is probably willing to spend
972860	975940	should foundation models prove themselves for forecasting.
975940	978180	Few of these models exist right now for NWP,
978180	980260	but there's plenty of private sector interest.
980260	984060	I mean, our friend from NVIDIA talked at length about that.
984060	987140	The two models that are currently published
987140	989180	are ATMO-REP from Christian Lesig,
989180	991160	which is a transformer model,
991160	993740	tested on both forecast and downscaling applications,
993740	997300	and Microsoft has published, as of a few days ago,
997300	998660	it's Aurora Foundation model,
998660	1003660	which most notably uses several different data sources
1003700	1005560	for training, not just air five,
1005560	1008820	but also climate simulations and operational forecasts,
1008820	1012020	and I believe one of Noah's ensembles.
1013180	1016620	Now, to integrate all of this,
1016620	1019140	as a researcher, I can make a few broad predictions
1019140	1022740	on where the trends are in this area.
1022740	1024680	First is going to be the convergence
1024680	1026180	of data simulation forecasts,
1026180	1028140	nowcast and downscaling roles.
1028140	1031100	Right now, the leading NWP forecast,
1031100	1033380	take in an analysis and give you a forecast,
1033380	1035900	but the obvious question is to what extent
1035900	1038140	can the rest of the chain be included?
1039780	1043460	For example, forecasts that are ensemble generating
1043460	1045560	can often be reversed to perform data simulation,
1045560	1048180	and a couple of references here are one,
1048180	1049760	using a diffusion-based model
1049760	1052340	to assimilate sparse observations,
1052340	1056140	and the second one is to perform data simulation
1056140	1058580	inside the latent space of an autoencoder,
1058580	1061660	which effectively replaces the background
1061660	1065980	error covariance matrices of a DA system
1065980	1069380	with ones that are nonlinear and flow dependent
1069380	1071180	from the autoencoder space.
1071180	1074580	A second avenue here is to combine conventional models
1074580	1076660	with an AI-based bias correction.
1076660	1080700	Farshie from ECMWF has published recently
1080700	1083540	on using a neural network bias correction
1083540	1088540	to include some element of AI inside the IFS-40 var system.
1088860	1091260	And Said, my colleague, is going to present
1091260	1095260	in about half an hour on spectral nudging
1095260	1098900	to bring a classical NWP system
1098900	1103460	closer to an AI forecast to preserve rich data
1103460	1106140	and have the accuracy of AI.
1107780	1109220	And finally, there's some effort
1109220	1111180	on all-in-one AI forecast systems
1111180	1114100	that go directly from observations to forecasts
1114100	1116420	to post-processing to produce predictions
1116420	1117700	of future observations.
1119140	1122300	The Vaughn 2024 is the Aurora mentioned in the plenary,
1122300	1124780	which is an encoder, decoder architecture
1124780	1127980	that uses Unets, and it's the obvious long-term future
1127980	1129660	of foundation models.
1129660	1131780	There's also rumors that Google is buying up
1131780	1134380	satellite data for its next generation,
1134380	1136460	GraphCast version two or something like that,
1136460	1138820	but I don't have any particular
1138820	1140380	confidential information to share.
1141180	1142780	Okay, now the second part of this talk
1142780	1145180	is the Environment and Climate Change Canada AI roadmap.
1145180	1149460	This is a joint product of the AASTD and CCMAP.
1149460	1151380	It's a product of an internal Tiger team
1151380	1154020	across the research and operational divisions
1154020	1157340	that was put together last fall after an internal study
1157340	1159500	on the current state of AI and weather forecasting,
1159500	1160660	where we had a whole lot of talks
1160660	1163260	like the first half of what I just gave.
1163260	1166620	This roadmap is designed to set very broad research priorities.
1166620	1169420	It's not designed to currently pick and choose
1169420	1170580	what projects are worth funding,
1170580	1174180	but to set out how we should think about AI as an institution.
1175340	1177700	And the largest theme from this roadmap
1177700	1180100	is the need for capacity building,
1180100	1182460	both in terms of compute capacity,
1182460	1184380	in light of the supercomputer update
1184380	1185620	we're likely to have next year
1185620	1188620	and whatever gets procured in the years thereafter.
1190540	1192660	How we should think about the use of cloud computing
1192660	1194500	and finally what we need to do
1194500	1196860	from a human resources and training standpoint.
1196860	1198900	This is a living document.
1198900	1200180	It should be published soon.
1200460	1202900	I'd hoped I could begin this talk
1202900	1204140	with a reference to the live document,
1204140	1205980	but I think it's still in translation.
1205980	1207940	But once it is published,
1207940	1211260	it'll be updated every few months to every year or so
1211260	1213900	with internal reviews and updates
1213900	1215860	just as we understand more about AI.
1216860	1218980	The broad themes of this document
1218980	1221340	are how we intend to integrate AI
1221340	1223580	throughout the research, development and operation cycle.
1223580	1227460	So we're not limiting it to just graphcast style forecasts,
1227460	1230500	but we're interested in applying AI everywhere
1230500	1234780	from data gathering to post-processing.
1235740	1238060	The main evaluation criteria here
1238060	1241260	are the triumvirate of feasibility,
1241260	1242740	service and efficiency.
1242740	1245540	Feasibility answers the question of how capable are we
1245540	1250220	of developing and running a proposed AI project?
1250220	1253180	And that includes not just the scientific risk
1253180	1256420	of well, putting a whole lot of time and money
1256420	1258180	into a project and having it not work,
1258180	1260980	but also whether or not we have the compute resources
1260980	1265020	to do this or the human resources to develop and manage it.
1266540	1268060	The second criterion is service
1268060	1271460	of how a project will improve ECCC's
1271460	1273500	weather-based services to Canadians.
1273500	1276300	And that is not just in terms of accuracy,
1276300	1278580	but also whether we can make our forecast products
1278580	1282380	more timely, whether we can have more numerous projects
1282380	1285300	or whether we can have qualitatively new products
1285340	1289940	like hypothetically, rapidly updated now casting,
1289940	1291700	which we just currently don't have available
1291700	1293300	for many government source.
1293300	1295580	And finally, there's a question of efficiency.
1295580	1297380	How will a project make efficient use
1297380	1299140	of our computational resources?
1299140	1302100	That includes the broad themes of energy efficiency,
1302100	1305100	but also the government-specific theme
1305100	1308300	of being a good steward of public funds.
1308300	1312580	Supercomputers are, as one might guess, rather expensive.
1312580	1315700	And we ought to demonstrate to Canadians
1315700	1319180	that Canadians are getting their money's worth.
1320260	1323020	Okay, so more specifically,
1323820	1325100	how are we looking at AI
1325100	1328060	from the observation to product pipeline?
1329100	1331660	First category here are observations and data assimilation.
1331660	1334940	So in observations, AI is in some sense
1334940	1337020	an extension of what's already happening
1337020	1339060	in terms of looking at quality control
1339060	1340980	and error estimation of our inputs.
1342660	1347660	AI can perhaps allow us to have some new capabilities
1349780	1351180	with learned observation operators
1351180	1354220	to take advantage of parameters that are,
1356780	1358580	to learn parameters that are not directly observed,
1358580	1361220	such as complicated satellite measurements
1361220	1365900	that are non-linear features of the atmospheric state
1365900	1367620	that are hard to directly measure
1367620	1369580	but might be possible for an AI to learn.
1369580	1373500	And finally, we have some ideas on unconventional data sources.
1373500	1375180	One idea thrown around in discussions
1375180	1380180	was using ad hoc webcam data like traffic cameras
1380220	1384060	to evaluate real-time precipitation class information.
1384060	1385780	In principle, someone can look at the feed
1385780	1387340	and say, yep, it's raining,
1387340	1389460	and we can have an AI do the same thing
1389460	1391620	and potentially have useful data.
1391620	1393300	On the data assimilation side,
1394700	1396380	in some senses, this area is most advanced
1396380	1401380	because DA is already using some heavy computation
1402460	1404740	for its error matrix operations
1404740	1407020	and they're investigating GPU use.
1407020	1408020	As previously mentioned,
1408020	1409340	there's a data of data assimilation
1409340	1410780	in the latent space of a model.
1410780	1414180	If we can vastly increase ensemble sizes through AI,
1414180	1417620	we can perform better PDF estimation
1417620	1422620	through particle filters or non-Gaussian-based statistics.
1423020	1424860	And finally, we can start to estimate,
1424860	1426540	perhaps, the model parameters themselves
1426540	1428340	rather than just initial conditions.
1429620	1430700	The numerical prediction side,
1430700	1433100	this is closest to what I talked about previously
1433100	1434900	with AI forecast models.
1434900	1437860	In the near term, we're looking at implementing
1437860	1439460	the AI models as they are
1439460	1442780	to provide second opinions about the weather to forecasters.
1444740	1447580	In the medium term, we're looking at fine-tuning
1447580	1450020	large models on our operational data sets
1450020	1453420	to provide better AI forecasts.
1453420	1454260	And in the longer term,
1454260	1455740	we're looking at structural changes
1455740	1458300	and developing new models in general.
1458300	1460300	In addition, we'd like to hybridize classical
1460300	1464420	and WP and AI models to help fix the problem of sparse outputs.
1465660	1467980	And along the way,
1467980	1469980	we'd also like to investigate emulation
1469980	1471460	of the physical parameterizations.
1471460	1475500	For example, radiation is very expensive
1475500	1477100	inside the atmospheric model
1477100	1480100	and 3D radiation is probably better than 1D radiation
1480100	1481820	but it's too expensive to run operationally.
1481820	1483380	If we can emulate that,
1483380	1485940	we can have a better parameterization
1485940	1488700	that is still within our compute budget.
1488700	1490420	Also, we'd like to extend this
1490420	1493020	to ocean ice and land surface prediction,
1493020	1496660	but that is still fairly preliminary
1496660	1500020	in part because the data sources aren't quite as complete.
1500020	1503980	Okay, now in terms of the tail end of the forecast
1503980	1508420	for post-processing and final products,
1508420	1512580	this is the realm of downscaling and now casting.
1514380	1517820	We have a 2.5 kilometer high resolution regional system
1517820	1520180	but it's just too expensive to run too often.
1520180	1521260	If we can downscale,
1521260	1526260	we can potentially achieve that resolution of output
1526660	1531660	and evaluate extremes and risks at that scale
1532500	1537060	without being limited by melting the supercomputer.
1537060	1538740	It would also be really nice if we could have
1538740	1541260	near real-time assimilation of weather station radar data
1541580	1544580	to improve the half hour, one hour forecast,
1544580	1547700	which is not something we can currently do very well.
1547700	1549220	In terms of post-processing,
1550820	1552700	the statistical post-processing systems
1552700	1555140	have all long had systematic error adjustments
1555140	1556660	and station specific adjustments
1556660	1559580	for representative this errors and the like
1559580	1564580	and AI can help turn that into better nonlinear corrections.
1564860	1567260	And finally, in terms of the expert product sides,
1567260	1569740	we'd like to have better high impact weather diagnostics
1569740	1573660	with well-calibrated forecasts for all of the big ones,
1573660	1576940	tornadoes, hail, blizzards that are just don't show up
1576940	1579380	in the larger scale forecast
1579380	1582020	but are extremely important for the people stuck in them.
1582020	1584860	Excuse me, Christopher, so only two minutes left before.
1584860	1587740	Yes, okay, I am going to be very quick.
1587740	1591000	Okay, challenges and opportunities.
1591860	1594700	As I've hinted at the entire time, we have challenges
1594700	1596180	and these are opportunities.
1597180	1600460	On the physical side, we have limited compute capacity.
1600460	1602700	GPU compute demands are only going to go up.
1602700	1604220	We don't have very many of them.
1605220	1607700	We're going to probably get more in the future
1607700	1609220	but we need to manage them.
1609220	1611580	We also need to care about data management
1611580	1614740	in terms of not just having an archive that exists on tape
1614740	1617380	but one that is living and can answer
1617380	1619820	training-based questions very quickly.
1619820	1622100	In terms of HR, we have similar problems
1622100	1624620	that our researchers are all very good
1624780	1627620	but they're also not necessarily well-trained on AI.
1627620	1630140	We need to close that gap.
1630140	1634380	And in particular, we would love to have increased collaboration
1634380	1637020	with both the ivory tower and private sector.
1637020	1640500	Okay, the roadmap sets out targets and milestones.
1640500	1643260	We would like to have our first operational AI systems
1643260	1645420	for IC innovation cycle five,
1645420	1647980	which is targeted early 2026
1647980	1650740	after the supercomputer update.
1651700	1653540	The current innovation cycle is just closing
1653620	1655580	because it's a bit too early for it.
1655580	1657860	And ultimately we'd like to have AI
1657860	1661380	as just another forecast tool by 2030 or so.
1661380	1664420	Okay, I would love to talk more about this
1664420	1665740	but unfortunately there's no time
1665740	1669700	but in general, our divisions have all been investigating
1669700	1673580	how we can integrate AI into research flows.
1674500	1675660	Some projects have started,
1675660	1678100	some are waiting for people
1678100	1681540	and some just simply need more resources
1681540	1682500	that we don't currently have
1682500	1684860	and we would love to collaborate on them.
1684860	1686980	If you have AI skill
1686980	1688580	and you have a weather-related project,
1688580	1692140	please email someone at our division.
1692140	1693980	We would probably love to talk to you.
1695340	1696820	Unfortunately, I have to cancel this slide
1696820	1699060	where I was going to talk up
1699060	1700860	all of my colleagues' presentations to come.
1700860	1703620	Please stick around, they are going to be great.
1703620	1706060	And finally, conclusions-wise,
1706060	1708340	AI is rapidly advancing the state of the art
1708340	1710740	in numerical weather computation.
1710740	1715540	This is a phase change of forecasting
1715540	1717460	and I'm excited to see where it goes.
1717460	1719740	We will make AI and machine learning technologies
1719740	1720980	a major part of our systems
1720980	1724380	as they prove themselves.
1725620	1727460	But we're a public service organization,
1727460	1729900	we recognize that we have to be very careful
1729900	1732220	about what we stand behind operationally.
1734100	1736500	And finally, I hope these slides are available
1736500	1737660	afterwards for the references.
1737660	1740660	There are references to all of the systems I've mentioned.
1740660	1742100	And I'd also just like to highlight
1742100	1743540	that of the 14 references here,
1743540	1745020	about eight are preprints.
1745020	1748140	This is a really, really rapidly moving field.
1749780	1751380	Thank you, I am...
1751380	1753340	Missy Christopher.
1753340	1756780	We have time maybe for one question.
1756780	1757780	I know.
1759300	1763020	I'm going to take the highest ranked one.
1763020	1764420	It's a long one, are you ready?
1764420	1766300	Yes, yes.
1766300	1767180	Perfect.
1767180	1769940	Currently, AI seems to be quite expensive
1769940	1771700	and rapidly developing.
1771700	1774460	While we were waiting for AI-based models
1774460	1776580	to build better foundations and training
1776580	1779060	to replace numerical weather prediction
1779060	1780740	or for climate studies,
1780740	1783780	can we exploit its computational speed
1783780	1788780	in the near term for now casting or HRR-like output?
1789460	1792900	For example, ECCC can incorporate into CAM
1792900	1795220	for very short thunderstorm prediction
1795220	1799540	or perhaps weather elements on grid now cast.
1800780	1804820	Okay, I believe these projects are under investigation.
1804820	1806900	I'm on the medium range forecast side,
1806900	1811740	so it's not my particular side,
1811740	1815580	but there's a presentation in this session,
1815580	1819700	I believe after lunch that investigates now casting
1819700	1821220	via an IBM Foundation model.
1821220	1824340	And I think that'll begin to answer that kind of question.
1824340	1827140	In general, yes, this would be very good.
1827220	1829860	In practice, the nearest term limits
1829860	1831340	are probably compute potential
1831340	1833140	because we have relatively few
1833140	1835700	operationally available GPUs,
1835700	1838100	but hopefully in the next few months to year or so,
1838100	1839020	that will improve.
1840780	1842820	I might sneak in another question then,
1842820	1844540	Christopher, we have a minute.
1844540	1847300	Have you considered the role that Canadian industry
1847300	1851260	will have in developing AI capacity at ECCC?
1852060	1854620	We would love collaborations from industry.
1854620	1859620	That is my politically correct and also true answer.
1861380	1865860	We have limits on our resources in part
1865860	1869500	because until, well, procurement thus far
1869500	1873380	has been focused on making our operational systems better
1873380	1875380	for obvious reasons.
1875380	1880380	And the cycles of this mean that it is practically difficult
1881220	1885380	to, sorry, I'm speaking as a researcher,
1885380	1886700	my boss is probably listening,
1886700	1888900	so there's some things I need to be very circumspect
1888900	1889740	about saying.
1891060	1893700	We need to be very careful about how we commit resources
1893700	1895420	for training large models.
1895420	1898460	Industry is, I think, a fantastic partner,
1898460	1902300	both for the potential of having compute resources
1902300	1904500	we could borrow and also a better focus
1904500	1908020	on some of the most downstream applications.
1908060	1912820	For example, our leading talk that opened CMOS
1912820	1916220	was on the weather impacts on the insurance industry.
1916220	1919580	And to the extent we can be of value there,
1919580	1922340	I think there's room for joint products.
1926500	1930380	Okay, so I will try to answer other questions
1930380	1932500	in the chat as we continue.
1932500	1934700	But otherwise, I will stop sharing, meet myself
1934700	1939180	and thank our hosts.
1939180	1940260	Well, thank you, Christopher.
1940260	1941620	That was an amazing feat.
1941620	1944940	You put in two very large presentations
1944940	1947260	into one 30-minute presentation.
1947260	1949860	You have all my admiration and thanks for doing that.
1950860	1952740	Congratulations, actually, in 30 minutes.
1952740	1954940	Okay, and now we're on the subject of airvests.
1954940	1957020	So we're not going to take any longer.
1957020	1958620	I'm going to introduce you to Airvests GLaPalm,
1958620	1961180	who comes from Canada as well, a researcher.
1961180	1965700	He's going to introduce us to the NWPEI-based model,
1965700	1967940	the verification against the observations
1967940	1970980	of airvests and airvests.
1970980	1972660	It's up to you.
1972660	1974180	Hello.
1974180	1975140	I don't know.
1975140	1977660	So you hear me.
1977660	1978580	Very well.
1978580	1980820	Do you see my screen?
1980820	1981700	Also.
1981700	1982420	Wonderful.
1982420	1984900	So I'm going to do the presentation in French.
1984900	1987060	If you have any questions in English, there's no problem.
1987060	1989900	If you have any questions in English, there's no problem.
1990140	1993580	It would be easier for me if I do it in French and for you also.
1993580	1999460	So I'm here to present the work that I did in collaboration
1999460	2005140	with my colleagues on the evaluation of the models based
2005140	2009380	on the airCCC in a second.
2009380	2010340	I don't know.
2010340	2015500	So the context is that with the emergence of these models,
2015500	2025780	we realized that we had to check these models
2025780	2029340	with our traditional verification methods,
2029340	2032340	which allow us to evaluate the innovations
2032340	2034740	that are made on traditional models.
2034740	2041780	So I was asked by my boss to install, turn and check
2041780	2047340	these models on our HPC installations.
2047340	2051660	And I started this work in October 2023.
2051660	2055020	And I worked on it until April 2024.
2055020	2058020	So what I want to present to you is just that.
2058020	2062020	So the activities that were completed during this special project
2062020	2066660	there, it's that we turned, we chose two models,
2066660	2070100	ForecastNet and Graphcast, which were available for free.
2070100	2071380	It's easy.
2071380	2075340	And, well, ForecastNet, Christopher,
2075340	2076940	we talked about it a little bit earlier.
2076940	2079220	So it's a model that was developed by Renvidia.
2079220	2082660	And then we turned two graphs of Graphcast,
2082660	2087860	one with 13 levels of pressure, with a roof of 50 hectopascals
2087860	2092420	and a version at 37 levels, a roof of 1 hectopascal.
2092420	2096220	And we turned each model with three analysis sets.
2096260	2102620	One, the first is the operational analysis of the OVF,
2102620	2106420	called IFS, on three levels only.
2106420	2109940	We also used R5.
2109940	2113860	So the R5 analyses are the ones that were used
2113860	2117420	to train these models.
2117420	2121100	We were able to turn the configurations at 13 and 37 levels.
2121100	2126140	And of course, we wanted to compare the operational provisions
2126180	2127300	with the same analysis.
2127300	2130620	So we started these models, these AI models,
2130620	2135060	with the operational analysis of CCC.
2135060	2141260	And we turned in two real-time modes,
2141260	2145380	where we turned twice a day,
2145380	2147900	at the same time as the operational model.
2147900	2149940	And like that, the operational metrologists
2149940	2153020	can compare the Forecasts based on AI
2153020	2155140	with the operational provisions.
2155140	2159940	And also, on my side, I did an evaluation
2159940	2164500	on a period of one year, which allows us to see
2164500	2167740	what the models are like.
2167740	2171420	So here, I put a slide on the description of the models.
2171420	2172420	It's very precise.
2172420	2173780	I don't have much time.
2173780	2177620	Basically, the two Graphcast and Forecast Net models
2177620	2181860	use about the same information.
2181860	2185140	But I put a lot of detail there to be complete.
2185140	2189940	But I don't think I'll be able to save a little time.
2189940	2195940	So one of the advantages of AI models
2195940	2199180	is their informatic efficiency.
2199180	2201860	If we compare the operational model,
2201860	2204700	presently, it takes a little less than an hour,
2204700	2206620	more than 6,000 CPUs.
2206620	2210460	So it's a big deal.
2210460	2213540	So it generates 500 gigabytes of data
2213540	2215940	at each provision twice a day.
2215940	2219740	It makes outings at all ages up to 10 days.
2219740	2222460	And then it's models at 15 kilometers of resolution
2222460	2224620	with a lot of vertical resolutions.
2224620	2227540	AI models have less good resolutions.
2227540	2229820	They release data at 6 hours.
2229820	2232420	And a less good vertical resolution too.
2232420	2235380	So, but Forecast Net is very, very light.
2235380	2236700	It's impressive.
2236700	2238820	It takes 20 minutes on a CPU.
2238820	2240020	I don't speak of a GPU here.
2240020	2242220	I have a GPU even faster, of course.
2242220	2244100	But on a CPU, you can turn it on.
2244100	2246540	You can turn it on on your laptop and it works.
2246540	2249620	And it's very fast.
2249620	2251140	It still gives you a good preview.
2251140	2254700	And Graphcast, it's a little bit more expensive.
2254700	2256620	The confidence at 13 levels
2256620	2260540	requires 100 gigabytes of memory.
2260540	2263060	So it's a little bit more expensive.
2263060	2265540	But still, comparing the operational model,
2265540	2269180	it's very, very much, much smaller.
2269180	2270660	Smaller orders.
2270660	2273340	And I invite you to the second presentation
2273340	2276220	of Christopher Subick on exactly comparing
2276220	2279100	the computer performance between AI models,
2279100	2282420	Graphcast and GEM, the operational model.
2282420	2284340	And it makes a very good comparison.
2284340	2288220	If you're interested, I invite you to have
2288220	2291380	this presentation on your computer this afternoon.
2291380	2293500	So what does it look like as a verification
2293500	2296580	once we've done the average over a year?
2296580	2301060	So here, I have several curves.
2301060	2304660	Here, I present the errors, the prediction
2304660	2309780	of the geopotential at 55 to Pascal.
2309780	2314900	So these are the errors.
2314900	2322100	The very thick blue gray here, that's the baseline.
2322100	2325500	So that's the operational preview.
2325500	2331260	So we see the errors that are missing from 0 to 240.
2331260	2334100	And the other curves, it's all the previews,
2334100	2336660	the verification, the AI models.
2336660	2339820	So we can look at them and then, as these are errors,
2339820	2342220	but the closer we are to 0, the better it is.
2342220	2347220	So we see here a group of previews.
2347220	2348380	That's the forecast net.
2348380	2350220	So we see that for the GZ500,
2350220	2355100	the forecast net is less good than the operational preview.
2355100	2357900	On the other hand, all the other curves, the five other curves,
2357900	2359660	it's all the previews made by Gravcast
2359660	2361660	using different analyses.
2361660	2364700	So we see that Gravcast, from day five,
2364700	2366620	is better than the operational model,
2366620	2369340	no matter the analysis we give him.
2369340	2375500	So that's when using the 05 and FS analyses, it's better.
2375500	2377900	I don't know, but what's important here,
2377980	2380900	what's interesting here is that the two curves here,
2380900	2383300	orange and green,
2383300	2387420	the previews are initialized with the same analysis
2387420	2389060	as the blue curve.
2389060	2392540	So we see that with equal information,
2392540	2397140	Gravcast is better from day five on the GZ500.
2397140	2398940	If we look at another variable,
2398940	2401820	which is the temperature at 850 tectopascals,
2401820	2404100	so it's the same colors, the same curves,
2404180	2409180	in this case, we see that from 72 hours,
2409180	2412380	all the AI models are the operational model,
2412380	2413500	even for the GZ500,
2413500	2419180	but we still see that Gravcast is the best model,
2419180	2420300	this variable.
2420300	2423220	So I showed you two variables.
2423220	2426060	So we can conclude that Gravcast is better than for the GZ500,
2426060	2429540	we will focus on that from now on.
2429540	2431660	And then what does it look like for other variables,
2431740	2434180	like wind, humidity.
2434180	2436780	So here, I present you graphics,
2436780	2439220	it's vertical profiles.
2441500	2446260	Here, in the Y axis of the vertical coordinate curve,
2446260	2447740	we talk about the surface,
2447740	2451140	from 1,000 tectopascals to 50 tectopascals.
2451140	2458500	And so the clean curve,
2458500	2461900	it's the error's quarter, according to the variable.
2461900	2466700	And the tight curve is the bias.
2466700	2468860	And the different variables are the following.
2468860	2472180	Here, on the top left, we have the meridian wind,
2472180	2476620	the wind component, the wind module here on the right.
2476620	2484740	Here, can you do this for us in the next two minutes?
2484740	2486740	Yes, that's it.
2486740	2488020	Yes, I'll go faster.
2488100	2490180	So what I wanted to show you,
2490180	2494340	here we have the Gravcast configuration,
2494340	2497940	so at 13 levels, 37 levels.
2497940	2503580	So what I wanted to show you is that the red curve
2503580	2506060	is always better than the blue curve,
2506060	2508300	so the model has all the variables,
2508300	2510940	all the failures, not all the failures,
2510940	2512820	but all the variables, all the levels.
2512820	2514980	We see that Gravcast is better.
2514980	2521500	So are these two models not different resolutions?
2521500	2524900	Is the fact that Gravcast is a bigger resolution,
2524900	2527660	is it the advantage?
2527660	2530460	Is it the advantage compared to the operational model?
2530460	2534740	So what we did is that we did the same verification
2534740	2537860	that I presented to you earlier, but we filtered it.
2537860	2539860	We filtered, we removed all the scales
2539860	2542060	that were smaller than 1,000 km,
2542060	2544700	and we kept all those that were 2,000 km.
2544740	2547500	So I have two graphics to present here.
2547500	2550220	So on the left, it's the same graphic
2550220	2552860	that I presented to you earlier, not filtered,
2552860	2555500	and on the right, it's the filtered previews.
2555500	2557660	So that's on the average, on the winter,
2557660	2559860	it's not on the full year.
2559860	2563500	So we see that even if filtered, Gravcast is better.
2563500	2565980	So that's going to bring to the work of Spectreur Nodging
2565980	2569380	of Syed, who will present in the next presentation.
2569380	2576340	So and for the summer, it's a little less spectacular,
2576340	2585420	but we still see that Gravcast has a lot of good information.
2585420	2590260	So what are the future activities to do more verification?
2590260	2594340	We even have an internal site that allows us to visualize
2594340	2596740	these previews day by day.
2596820	2601220	And I invite you to have the next seminar
2601220	2603260	of Syed about Spectreur Nodging,
2603260	2606580	which is a very interesting approach to integrate
2606580	2608820	physical models and AI models.
2608820	2617140	And Christopher Subick's work on entering Gravcast
2617140	2622500	with the operational analysis that we have done internally
2622500	2626700	so that it can be better adapted to our model.
2627980	2629460	Thank you.
2629460	2632220	Hi, thank you, Hervé.
2632220	2634060	Hi, I have a question for you.
2634060	2635900	There, you made the internal models run,
2635900	2637820	you installed them to do the verification,
2637820	2639460	but there are more and more models,
2639460	2642060	there are almost two days,
2642060	2644780	is there perhaps a way to have
2644780	2647060	verification, counter-observation,
2647060	2648340	like that, in a standardized way,
2648340	2650740	or each time, you will have to install the models
2651140	2653180	and then the scoring themselves?
2653180	2658180	Well, listen, that's what the software that we use
2658180	2660620	to do the verification and counter-observation
2660620	2664060	works very well only locally.
2664060	2668260	So it's difficult to publish that externally.
2668260	2671380	On the other hand, running these models,
2671380	2673660	if it's not done very, very easily,
2673660	2675780	I worked for three months in the middle of the day
2675780	2677900	just to start these models.
2678220	2682420	So I imagine that each model has its own peculiarities.
2682420	2685900	So it's not obvious, it's not as plug-and-play
2685900	2687020	that we believe in.
2687020	2689700	There's still a lot of work to be done.
2689700	2694460	So if there are all the two, all the two weeks,
2694460	2698500	it will be difficult to do this same evaluation
2698500	2700660	to follow the run.
2702260	2704940	Thank you, I don't know if there are other questions.
2704940	2707180	Maybe a word for advanced systems.
2707180	2709900	If you could reset the Q&A on EventMobi
2709900	2713020	because I still see the questions
2713020	2716060	that have been asked to Christopher Subic.
2716060	2718940	So it's hard to see which one.
2718940	2720700	I have one for you, Hervé.
2720700	2722060	How do artificial intelligence models
2722060	2724780	behave in complex mountainous terrain?
2726660	2729540	I didn't evaluate that.
2729540	2731420	That's more what my colleague, Marc Verville,
2731420	2733500	did for verification on the surface.
2734500	2743700	I don't have any memory of the results.
2743700	2748100	My focus was really on the verification in addition.
2748100	2750900	But of course, having a good resolution
2750900	2754900	will certainly not be a problem.
2754900	2756220	Perfect.
2756220	2758620	Thank you very much, Hervé.
2758620	2760140	Thank you.
2760140	2761740	Thank you, Hervé.
2762020	2765900	We, so we're going to continue.
2765900	2769500	This time we're inviting Syed Zahid Hussein,
2769500	2774020	who's a research scientist at ECCC.
2774020	2779460	He will be presenting leveraging data-driven weather
2779460	2782700	emulators to guide physics-based numerical weather
2782700	2787940	prediction models, a fusion of forecasting paradigms.
2787940	2794780	So without further ado, Syed, this is your turn.
2794780	2796420	Thank you, Rand.
2796420	2797500	Hello, everyone.
2797500	2801220	In my presentation today, I will be talking
2801220	2807140	about how we can leverage the strengths of data-driven weather
2807140	2810860	models to improve predictions from NWP models.
2810860	2813380	This is a work that we have been doing recently.
2813380	2816900	And here is a list of my principal collaborators
2816900	2819540	and the others who have contributed to this research.
2823660	2827620	As we know, the current state of the earth
2827620	2829540	for operational weather forecasting
2829540	2833660	is based on physics-based NWP models.
2833660	2837540	However, we have seen these presentations earlier today
2837540	2840380	from the plenary to the previous two presentations
2840380	2844500	that we have recently seen the emergence of new data-driven
2844500	2847740	models for predicting weather.
2847740	2850580	And most of these models are using some form of deep neural
2850580	2852900	network to emulate the training data.
2852900	2857660	And the training data generally is RFI re-analysis.
2857660	2861900	So we also can call them artificial intelligence-based
2861900	2863300	weather emulators.
2863300	2866300	And recently, they have started to gain prominence
2866300	2869540	and started to also challenge the existing forecasting
2869540	2870500	paradigm.
2870500	2873180	Because as you have seen in the previous presentation
2873180	2877500	from my colleague, Erveg, that these data-driven models
2877500	2880460	can produce forecast orders of magnitude
2880460	2883940	faster with minimal computational resources
2883940	2886860	compared to the traditional NWP models.
2886860	2890860	And also, they can be highly competitive against state
2890860	2895100	of the art NWP models in terms of their accuracy.
2895100	2898540	However, despite their strengths,
2898540	2901220	strictly when I'm talking in the deterministic sense
2901220	2905260	for these AI-based models, they have their limitations also.
2905260	2908340	And one of the most widely known limitation
2908340	2911340	is considerable smoothing of fine scales,
2911340	2913180	particularly for longer lead times.
2913180	2917220	Also, they only offer a limited range of forecast fields
2917220	2920500	and improving nominal resolution of these AI models
2920500	2921340	not straightforward.
2921340	2923060	They can be quite challenging.
2923060	2925020	So the objective of our research was
2925020	2928820	to see if we can leverage the strengths of these AI-based
2928820	2934020	models to improve the predictability of an NWP model.
2934020	2938780	And for that, we chose or selected
2938780	2941660	like the GEM model, which is used operationally
2941660	2946420	as the NWP model, operationally by Environment Canada,
2946420	2950580	and the GraphCast model from Google DeepMind as the AI-based
2950580	2951380	model.
2951380	2954900	And the nominal grid resolutions of GraphCast
2954900	2958220	and the GEM-based Global Deterministic Prediction System,
2958220	2962620	or GDPS, are approximately 25 kilometers and 15 kilometers,
2962620	2964020	respectively.
2964020	2966140	And in the previous presentation,
2966140	2971100	AirVig has shown that the GraphCast actually poses
2971100	2975660	more skilled large scales compared to our GDPS.
2975660	2978980	But if we want to leverage the information from GraphCast,
2978980	2981580	we need to know about the effective resolution of GraphCast
2981580	2984860	so that we can see what are the scales that we can really
2984860	2988700	utilize for improving NWP model.
2988700	2993740	And in order to do that, we look at the variance ratio of GDPS
2993740	2998500	and GraphCast with respect to our own CMC analysis.
2998500	3001300	And before I talk about anything else,
3001300	3004900	I must emphasize on the fact that the version of GraphCast
3004900	3010020	that we are using has not been through any fine tuning.
3010020	3016100	So it has been trained by Google on emulating error-5 analysis
3016100	3018340	by training with error-5 data.
3018340	3022140	And we are using that GraphCast model
3022140	3026420	but initializing it with our own analysis.
3026420	3029020	And in these figures in this slide,
3029020	3033220	I am showing the transient component of variance ratio
3033220	3035660	for 500 hectopascal kinetic energy.
3035660	3039180	On the left, I have for lead time 24 hours.
3039180	3042220	And on the right, I have for 120 hours.
3042220	3044380	In blue, I have GraphCast.
3044380	3048460	And in blue, I have GDPS.
3048460	3050460	And in red, I have GraphCast.
3050460	3054060	And we can see by looking at the variance ratio of GDPS,
3054060	3059660	the blue lines in both 24-hour and 120-hour cases,
3059660	3062100	that the variance ratio is close to 1.
3062100	3065380	So that means its effective resolution
3065380	3068860	is not changing with respect to lead times.
3068860	3071060	However, what we see with GraphCast,
3071060	3073900	first of all, at 24-hour lead time,
3073900	3076900	we see the scales as large as 1,500 kilometers
3076900	3080020	are smoothed out to some extent.
3080020	3082620	And we see considerable smoothing for scales smaller
3082620	3083540	than that.
3083540	3086860	And then we see when we go to 120-hour lead time,
3086860	3091700	the scales that are getting smoothed out actually increases.
3091700	3095300	And it affects scales as large as 2,750.
3095300	3098140	So we know that large scales in GraphCast are better.
3098140	3101780	But at the same time, we see that scales below 2,750
3101780	3105380	for longer lead times are problematic because of the reduced
3105380	3107820	variance ratio.
3107820	3112500	Now, the question is how we can really use or leverage
3112500	3116260	this good large-scale information from GraphCast.
3116260	3120500	And one way to do that would be to use spectral nudging,
3120500	3122180	large-scale spectral nudging.
3122180	3125740	And this is a very widely used idea
3125740	3129260	in the field of regional climate modeling and hindcasting.
3129260	3132700	Our expectation is if we nudge our gem predictions
3132700	3134900	towards the large scales of GraphCast,
3134900	3137740	we can improve the quality of prediction of gem.
3137740	3141220	At the same time, we should be able to address the fine-scale
3141220	3143100	smoothing in GraphCast while we will
3143100	3146740	be able to generate the full set of focus fields
3146740	3152220	that we are currently having access to through gem.
3152220	3154940	The concept of nudging is quite simple,
3154940	3158260	as illustrated by this equation here.
3158260	3164180	So here, F is the solution of our gem model
3164180	3165780	after the dynamic substep.
3165780	3168820	And this term highlighted in purple color
3168820	3171340	actually corresponds to the nudging increments.
3171340	3175060	So we add the nudging increment to the model solution
3175060	3178140	after the dynamic step to get the nudge solution, which
3178140	3179620	is then fed to the physics.
3179620	3182900	And then it completes the complete model time step.
3182900	3185220	And then it fits back to the next dynamic step.
3185220	3186860	And this is how it continues.
3186860	3192220	And if we look at this nudging increment term,
3192220	3194700	we have this subscript Ls, which actually
3194700	3196220	implies the large scale.
3196220	3198660	So we are only considering the large scales
3198660	3200180	when we are applying the nudging.
3200180	3203060	And this omega is a relaxation parameter.
3203060	3205260	And if we break down this equation,
3205260	3207380	we can see basically what we have
3207380	3211140	is a weighted average of the large scales coming from GraphCast
3211140	3215980	and our model, whereas the fine scales are remaining intact
3215980	3217860	that is being credited by the model.
3217860	3220420	So this is how we can leverage the large scale accuracy
3220420	3223940	of GraphCast while allowing our model to freely evolve
3223940	3225580	the small scales.
3225580	3228340	And the scale separation between large and small scales,
3228340	3230740	we are doing that by decomposing the nudging
3230740	3232740	increments in the spectral space.
3232740	3235860	Although we are applying the nudging increment
3235860	3238780	in the grid point space, but we are decomposing it
3238780	3239660	in the spectral space.
3239660	3243140	And hence, we call it spectral nudging.
3243140	3245900	How we optimize the spectral nudging configuration
3245900	3249900	to support our objectives for this study
3249900	3251460	is a computationally demanding task.
3251460	3253340	And in a sense, it is still ongoing.
3253340	3256700	I mean, we are still sort of fine tuning the configuration.
3256700	3259460	But at present, the most optimal configuration
3259460	3262820	that we have has these following features.
3262820	3267580	We are only applying nudging to horizontal wind and temperature.
3267620	3269980	And we are not nudging the stratosphere and the boundary
3269980	3271380	layer for different reasons.
3271380	3274780	And we are nudging scales that are larger than 2750
3274780	3275900	for obvious reasons.
3275900	3278780	That should be obvious from the variance ratio comparison.
3278780	3282700	And we have 12 hours as the nudging realization scale.
3282700	3286140	And we apply nudging at every time step.
3286140	3288900	With that, I'll be going to some of the results
3288900	3293460	of verification that we will try to prove
3293460	3295260	that this approach actually helps
3295260	3298500	to improve the predictability of gem.
3298500	3303700	We ran a series of experiments for winter and summer of 2022.
3303700	3305780	So we have the control experiments
3305780	3309420	where there is no nudging, the control GDPS.
3309420	3312260	And then we have the GDPS with spectral nudging.
3312260	3315540	So control would be, in the next few slides, all the results.
3315540	3317700	Control would be shown in blue color.
3317700	3321060	And the results from spectral nudging with GDPS
3321060	3323620	would be shown with red.
3323620	3326540	So the first scores that I want to show
3326540	3328900	are verification against radios and observations.
3328900	3331900	It's similar to what Elvig has shown
3331900	3334260	in the previous presentation.
3334260	3338380	Just to repeat, we have in these figures,
3338380	3343060	we have different variables, zonal wind, wind modulus,
3343060	3346220	geopotential high temperature, and dew point depletion.
3346220	3349140	In all these figures, we have the dashed lines representing
3349140	3351700	the bias and solid lines representing standard deviation
3351700	3352380	of error.
3352380	3355260	And we have the shades of red and blue
3355260	3359780	that represent the statistically significant improvements
3359780	3362700	corresponding to the color of the experiment.
3362700	3364900	So if we see red color, then it implies
3364900	3367220	that we have statistically significant improvement
3367220	3369500	with the spectral nudging configuration.
3369500	3373420	And if it is blue, then we have deterioration
3373420	3374900	with spectral nudging.
3374900	3376380	What we can see from these figures,
3376380	3379500	this is for winter over the globe.
3379500	3383660	That beyond five days and up to 10 days,
3383660	3386540	we can see that there is tremendous improvement
3386540	3389660	in the scores for the standard deviation of error, which
3389660	3390380	is more difficult.
3390380	3391780	Excuse me, Sayed.
3391780	3394380	You have two minutes left.
3394380	3397580	OK, I'll try to be faster.
3397580	3402540	For the bias, the differences are mixed.
3402540	3406020	But the bias is less difficult to improve.
3406540	3408900	Standard deviation is more difficult.
3408900	3412340	So we see tremendous improvement, up to 10% improvement
3412340	3414420	in the RMSE for longer lead times.
3414420	3416580	In summer, though, we see modest improvement.
3416580	3420420	Still, we see statistically significant improvement
3420420	3423020	for the standard deviation of error.
3423020	3425420	And when you look at the animal liquid relation
3425420	3429260	coefficient, this is for the 500 hectropascal geopotential.
3429260	3431700	We see improvements both in winter and summer.
3431700	3434940	In winter, in terms of the gain in predictability,
3434940	3436940	it's around 18 hours.
3436940	3440340	In summer, it's about eight hours improvement.
3440340	3443220	And they're both statistically significant.
3443220	3447700	And last results is about tropical cyclone position error.
3447700	3450340	This is another thing that is very difficult to improve.
3450340	3454980	And our model tends to have, for the alone track position,
3454980	3458100	we tend to lag the model.
3458100	3462020	And we can improve that lagging with spectral nudging
3462020	3462940	towards graphcast.
3462940	3464980	And for the cross-track position error,
3464980	3468100	our cyclones tend to veer to the right
3468100	3470020	from the observed trajectory.
3470020	3472860	And we are able to considerably improve
3472860	3477740	that aspect of the position of tropical cyclone also.
3477740	3480380	And finally, just this figure, I'm
3480380	3484700	showing the temperature anomaly at 850 hectropascal
3484700	3487340	for a lead time of 240 hours for a single case.
3487340	3490860	We have the GDPS control on the left,
3490860	3493620	graphcast in the middle, and GDPS with spectral nudging
3493620	3494540	on the right.
3494540	3498420	And we can see that graphcast barely has any fine scale.
3498420	3501700	And both GDPS control and with spectral nudging,
3501700	3504820	they both have comparable fine scale information.
3504820	3507980	And we get this improvement by nudging as well.
3507980	3511860	So to summarize, we developed and hybrid NWP system
3511860	3516140	that fuses NWP models with AI models to spectral nudging.
3516140	3518460	And by leveraging more accurate large-scale predictions
3518460	3522820	from graphcast, we are able to significantly improve
3522820	3527580	our prediction scale with GDPS.
3527580	3530700	And I want to stress that the improvement that we have
3530700	3533700	is roughly equivalent to one solid innovation cycle, which
3533700	3537260	is about four years of work involving many scientists
3537260	3539740	from across the Meteorological Research Division
3539740	3540980	of Environment Canada.
3540980	3544540	And we were able to achieve something comparable
3544540	3546820	in a matter of four to five months.
3546820	3550220	And also, I want to stress that this
3550220	3553540	is based on work that uses a graphcast model that
3553540	3555060	has not been fine-tuned.
3555060	3559780	And with fine-tuning graphcast to emulate our own CMC
3559780	3564340	analysis, which is a work in progress by my colleague Christopher,
3564340	3567620	we hope that we could improve further.
3567620	3569260	And currently, with this configuration,
3569260	3572380	we have about 25% increase in the computational cost.
3572380	3574380	But this is without any optimization.
3574380	3576260	And with some optimization, we hope
3576260	3578460	that we will be able to reduce it to something
3578460	3581700	like less than 15% in the near future.
3581700	3584700	So with that, I will end my presentation.
3584700	3586460	Thank you, Sayada.
3586460	3590860	It's a very impressive conclusion.
3590860	3593780	I have one question here.
3593780	3600980	Nudging is done as gem integration goes or at posteriori?
3600980	3603140	No, it's online.
3603140	3605940	So what happens is, as I said, you
3606820	3609180	solve the dynamic step.
3609180	3612380	Because we have the dynamic sub-step and the physics sub-step.
3612380	3614660	And then we do the coupling in the split mode.
3614660	3616820	So we solve the dynamic sub-step.
3616820	3618660	We have a solution from dynamics,
3618660	3620020	which is an intermediate solution.
3620020	3622500	Then we update that by nudging.
3622500	3625220	And then we feed that updated solution to the physics.
3625220	3627540	And then we get the complete solution of the model time
3627540	3628040	step.
3628040	3631540	And then the next time, dynamics uses that solution
3631540	3633500	to predict the next dynamic step.
3633500	3636420	So it's not a posteriori.
3636420	3640780	It's an online update.
3640780	3645820	Last quick one, how does GDPSSN compare with GraphCast?
3648740	3653380	That was already shown by Ervig in the previous presentation.
3653380	3657780	That GDPSS, I mean, if you talk about control GDPSS
3657780	3660380	versus GraphCast, that presentation of Ervig
3660380	3664260	should allow you to see like it actually improves.
3664260	3666020	What I am missing in my figures, because this
3666020	3668940	is still a work in progress, I mean,
3668940	3672100	in our paper that we expect to submit soon,
3672100	3676140	which we will be adding the GraphCast also,
3676140	3680300	focus in this, for example, in these figures to show
3680300	3685060	control GDPS with GraphCast and GraphCast itself,
3685060	3688260	like how much of the improvement is coming from GraphCast.
3688260	3690220	But it is definitely coming from GraphCast,
3690220	3693660	as Ervig's presentation showed that the large scales in GraphCast
3693660	3695220	are much better.
3695220	3698220	Yes, Isayed, looking forward to read the print.
3698220	3701300	That will be very popular, I'm sure.
3701300	3702620	Thank you.
3702620	3703140	Merci.
3703140	3704860	Anne, at what?
3704860	3706940	Yes, so we're moving.
3706940	3708420	You might have noticed in the schedule
3708420	3710500	that Christian Isayed was originally
3710500	3711500	supposed to present this.
3711500	3716540	But I want to thank Madalina Socer to have accepted
3716540	3720140	to present and prepare this presentation for us.
3720140	3723900	So she's a Climate Extreme Specialist at Environment Canada.
3723900	3726460	She's going to be presenting on the development
3726460	3730100	of artificial intelligence downscaling applications
3730100	3735660	for medium-range forecasts of weather elements at CCMEP.
3735660	3739100	So without further ado, over to you, Madalina.
3739100	3740220	OK, thank you.
3740220	3742060	I hope you're hearing me well.
3742060	3743540	We are, yes.
3743540	3744340	OK, great.
3744340	3746580	So I am here today to present to you
3746580	3749380	some development of artificial intelligence downscaling
3749380	3751860	techniques that we're doing at CCMEP in collaboration
3751860	3753220	with IBM Research.
3753220	3755020	And I would like to acknowledge my co-authors
3755020	3758900	that are listed here, both from ECCC and IBM Research.
3763780	3766500	So first, I would like to say that the vision for this project
3766500	3769180	is actually to help us offer seamless day one to day 10
3769180	3772500	public forecast products as part of the transformation
3772500	3774740	of the meteorological service of Canada.
3774740	3776380	So in order to do this, we need to bridge
3776380	3778820	the gap between high-resolution, short-term forecasts,
3778820	3782780	and medium-range, lower-resolution forecasts.
3782780	3784380	And the reason why we want to do this
3784380	3786700	is because we know that insufficient horizontal
3786700	3789860	resolution causes forecast errors and especially biases.
3789860	3792020	I am showing here a graph of bias,
3792020	3795900	a comparison between a low-resolution model in red
3795900	3797820	and the high-resolution model in blue.
3797820	3800620	And what we are saying here, the closest we are to zero,
3800620	3801620	the less bias we have.
3801620	3805300	And we really see that increasing horizontal resolution
3805300	3806860	is improving this bias.
3806860	3808340	Usually, the way that we do that is
3808340	3809980	by doing dynamical downscaling.
3809980	3812540	So running numerical weather prediction models.
3812540	3814900	But this is very computationally expensive,
3814900	3817140	which makes it limiting.
3817140	3819980	A partial solution to this is doing statistical downscaling,
3819980	3823180	which means using past data and deriving
3823180	3825740	statistical relationships from this past data,
3825740	3828300	such that we apply these relationships
3828300	3832820	on the course input and we obtain high-resolution output.
3832820	3834820	These type of solutions are limited
3834820	3836620	by the fact that we have to impose certain
3836620	3839020	non-mathematical relationships in the data.
3839020	3841740	So now we have a data-driven alternative,
3841740	3845300	which is to apply artificial intelligence techniques
3845300	3846460	to do downscaling.
3846460	3849020	And what this does is that it allows
3849020	3851180	to derive complex relationships in the data
3851180	3853260	that we provide to these models.
3853260	3856500	So our objective is to develop
3856500	3859140	artificial intelligence downscaling techniques
3859140	3862500	to downscale weather elements from medium range forecast
3862500	3863620	to the kilometric scale.
3863620	3866180	And we will hope that this will help both deterministic
3866180	3868940	and ensemble forecasting.
3868940	3871340	So just to briefly present our project.
3871340	3873260	So this is a collaboration that started this year
3873260	3875980	between CCMAP and IBM Research.
3875980	3877620	And here at Environment Canada, we
3877620	3879100	have the meteorological expertise.
3879100	3883140	And we definitely have subjects, problems
3883140	3885700	that could really benefit from these artificial intelligence
3885700	3887180	solutions, but we don't necessarily
3887180	3889820	have the artificial intelligence expertise, which
3889820	3892780	is where IBM Research comes into play.
3892780	3894820	And collaborating with them, they are really experts
3894820	3897820	in this field, will allow us to advance much faster.
3897820	3900140	And the expected outcome from this collaboration
3900140	3902060	that for now it's only meant to be on one year,
3902060	3905500	is to develop low-cost and efficient alternatives
3905500	3909780	to the computationally expensive dynamical models.
3909780	3913100	And the other thing that we want to get from this collaboration
3913100	3914900	is we want to learn from IBM Research.
3914900	3916540	So at the end of this project, we
3916540	3919100	want to enhance our capability at Environment Canada
3919100	3921780	to be carrying out this type of research and development
3921780	3923740	and eventual operational implementation
3923740	3927540	of AI downscaling techniques.
3927540	3931100	So the specific goals of our projects are as follows.
3931100	3933940	So we are interested in downscaling weather elements.
3933940	3936060	And by this, I mean surface winds, temperature,
3936060	3937900	and precipitation in the first stage,
3937900	3941260	a forecast from the GDPS, which is our global deterministic
3941260	3944460	prediction system shown here.
3944500	3947300	And runs at a resolution of 15 kilometers
3947300	3949500	to the resolution of the HRDPS, which
3949500	3951420	is our high-resolution deterministic prediction
3951420	3955940	system that is run for 48 hours for now at 2.5 kilometer
3955940	3956900	resolution.
3956900	3959340	And in this project, we are taking a two-step approach.
3959340	3962540	So in the first step, we will be looking at the baseline model
3962540	3965660	that is based on generative adversarial networks.
3965660	3968020	And in the second stage of the project,
3968020	3972940	we will be taking advantage of foundation models
3972940	3974940	and tuning them for our application.
3974940	3976660	The training data for the models will
3976660	3979060	be forecasting data from the GDPS
3979060	3983700	as the low-resolution data set and from the HRDPS
3983700	3985980	as the high-resolution data set.
3985980	3988980	And it is very important for our operational needs
3988980	3992580	that the downscale products are available on the HRDPS grid.
3995100	3997820	For now in this talk, I will only focus on the baseline model
3997820	4000220	as this is a collaboration that just started.
4000220	4002140	And we haven't gotten yet to the second part.
4003540	4007420	So just briefly, what is a generative adversarial network?
4007420	4010860	And generative adversarial networks
4010860	4013420	consist of two networks, a generator, two neural networks,
4013420	4014620	a generator and discriminator.
4014620	4015980	And the way that they work is that they
4015980	4018180	are trained in a competing process.
4018180	4020420	So the generator pretty much generates images
4020420	4023660	that look as much as possible as the real data.
4023660	4025820	And the goal of the generator is to just
4025820	4027260	fold the discriminator.
4027260	4029060	On the other hand, we have the discriminator
4029060	4032340	that receives both real data, which in our case is HRDPS
4032340	4035220	data, and generated data, and has to decide
4035220	4038340	whether this generated data is real or fake.
4038340	4043580	And in our case, we use the ANAU et al. 2023 implementation
4043580	4044740	of a Vassar SteamGAN.
4047740	4052220	I will show you some preliminary results for our project.
4052220	4056100	So these preliminary results are based on the WGAN from ANAU
4056100	4056580	et al.
4056580	4057740	Without any covariates.
4057740	4060740	So what this means is that the only data that goes for now
4060740	4065020	in this model is zonal and meridional 10-meter wing
4065020	4065700	components.
4065700	4068940	And this is the data that we are trying to obtain.
4068940	4071980	So the low-resolution data comes from GDPS.
4071980	4074580	High resolution comes from the HRDPS.
4074580	4077300	And so far, we are training the model with one year of data
4077300	4078980	that is divided as follows.
4078980	4081420	75% of the data is used for training.
4081420	4083220	It's about 7,000 forecasts.
4083220	4085900	12.5% is used for validation.
4085900	4088500	And this validation data is used during the training
4088500	4089180	of the model.
4089180	4092460	So it's used to stop overfitting the model.
4092460	4095300	And then 12.5% of the data is used for testing.
4095300	4097580	So once we have a tuned model, we
4097580	4100380	will use this data set, about 1,200 forecasts,
4100380	4104060	to be seeing how well this model functions.
4104060	4105700	Because we are using forecasting data,
4105700	4106700	it is a bit difficult.
4106700	4108420	Because as you know, forecasts, there
4108420	4111140	are increases with forecast lead time.
4111140	4113180	And so we might have too much divergence
4113180	4115700	between the HRDPS, the high-resolution data set,
4115700	4118700	and our low-resolution data set.
4118700	4120660	On the other hand, the first six hours of the forecasts
4120660	4122660	are affected by the spin-up time of the model.
4122660	4126900	So we have decided for now to go with forecasts our 6 to 18.
4126900	4130300	And we are using them from the 0,0 and the 12 UTC
4130300	4132060	initialization of both models.
4132060	4137060	So in this way, we are covering the entire day.
4137060	4141780	So our challenge is covering the entire HRDPS domain.
4141780	4143980	I am showing here the HRDPS domain.
4143980	4147580	It is on the order of 2,500 by 1,200 grid points.
4147580	4149660	So it's a very large domain that's
4149660	4151740	spanning the width of Canada.
4151740	4155180	And it isn't really possible, or at least we don't think
4155180	4157460	it's possible so far, to be training directly
4157460	4159740	on such a large domain.
4159740	4162180	We do not have the computational resources to do that.
4162180	4164620	And we also aren't sure exactly how much data
4164620	4167300	you would need to be able to train on such a model.
4167300	4171340	The now and all implementation, in that implementation,
4171340	4176140	the training is done on 16 by 16 pixel low resolution
4176140	4181380	patches and 128 by 128 pixel high resolution patches.
4181380	4185420	So what we will do is we will adjust to that type of training.
4185420	4188780	And in order to do that, the strategy that we have adapted
4188780	4193420	is to just select random patches from one forecast
4193420	4195620	from our HRDPS data set.
4195620	4200540	We will re-read the GDPS data set on the HRDPS domain
4200540	4204300	and then course-crain it to go back to its resolution.
4204300	4206340	And in this way, we are going to end up
4206340	4207900	with this type of patches that we'll
4207900	4210380	use to do the training.
4210380	4212660	So here is the high resolution data patch
4212660	4216180	and the corresponding GDPS low resolution data patch.
4216180	4220740	So at each epoch, we are using between 300 and 700 random
4220740	4221580	patches.
4221580	4224020	And the model that we are showing today
4224020	4227860	has been trained on 17,370 epochs.
4227860	4232260	So this took about 149 hours to train on one GPU.
4232260	4234100	And we have done more than two passes
4234100	4237300	through the entire data set.
4237300	4239220	So once you have a trained model,
4239220	4241700	you can perform inference with the GDPS data.
4241700	4246180	And that inference will also be done on 16 by 16 pixel patches.
4246180	4249060	So here I have a GDPS input.
4249060	4250780	We perform inference.
4250780	4253100	And like this, we obtain the downscale forecast,
4253100	4255260	the downscale U and V fields.
4255260	4259500	And just as a comparison, we have here the HRDPS forecast.
4259500	4262220	So the first things that we can say
4262220	4264500	is that we are definitely downscaling.
4264500	4268660	So we are obtaining information at a small scale.
4268660	4272060	It isn't as much as the HRDPS is showing.
4272060	4274020	But as I was mentioning before, one problem
4274020	4276420	with a low resolution is the fact that you have biases.
4276420	4278620	And we are definitely achieving some sort
4278620	4281580	of a bias correction.
4281580	4285060	Now, of course, you have to parse the entire HRDPS domain.
4285060	4287500	One way to do that would be to sequentially process
4287500	4289820	128 by 128 patches.
4289820	4292860	But that would result into artifacts at the borders.
4292860	4295260	So the strategy that we have adopted instead
4295260	4297300	is to do some overlapping and then
4297300	4301500	to take the median of the ensemble of overlaps.
4301500	4304260	And in this way, we managed to patch and obtain this figure
4304260	4306580	over the entire domain.
4306580	4309340	We have performed validation of our model.
4309340	4314620	So we have used the test data, the 1,200 forecast from the GDPS.
4314620	4317540	And here I am showing the root mean square error
4317540	4320940	for the U-wind component and for the V-wind component.
4320940	4324020	And on the bottom, I'm showing the mean absolute error.
4324020	4326140	And these are the metrics that are computed
4326140	4332420	between the downscale GDPS and the HRDPS corresponding
4332420	4333660	verification.
4333660	4335740	And we are comparing it with some baselines that
4335740	4337260	can be used for interpolation.
4337260	4339740	So I'm showing in orange you have bilinear interpolation
4339740	4342700	and in green you have nearest neighbor interpolation.
4342700	4344620	So as we are seeing in all the metrics,
4344620	4347340	the downscale is showing better results,
4347340	4350020	is performing better than the other types of interpolation
4350020	4352820	for our test data set.
4352820	4354900	We have also done a power spectrum analysis
4354900	4358580	in order to really quantify how much detail we
4358580	4360100	are getting at the small scales.
4360100	4362020	And here I'm showing the radially average power
4362020	4365780	spectral density between HRDPS in blue and the downscale
4365780	4368180	forecast in orange.
4368180	4371460	And these power spectra are average over the test data
4371460	4372580	set as well.
4372620	4375100	What we are seeing is that indeed at small scales,
4375100	4377700	we are still not getting enough power.
4377700	4380020	So we do not get enough detail.
4380020	4384900	Madelina, if you could lend this in one or two minutes.
4384900	4386020	Sounds good.
4386020	4389700	So of course, we are training so far with one year of data.
4389700	4392540	And it would be very interesting to see
4392540	4396260	how much more detail we can obtain by training further.
4396260	4398460	I'm just showing also an integrated measure
4398460	4400020	of the difference in the power spectra.
4400020	4402220	And what you are seeing here is that compared
4402220	4404860	to the bilinear and the nearest neighbor interpolation,
4404860	4409660	the downscaled AI downscaling performance much, much better.
4409660	4411780	So it's able to recover way more structure
4411780	4414660	at the small scales.
4414660	4418340	So the next steps with our forecast, with our project,
4418340	4420900	the WGAN needs further development and testing.
4420900	4423580	So we are planning on testing with more data.
4423580	4425820	A very important thing that we are working on right now
4425820	4427700	is to add other covariates.
4427700	4430340	So as I said, for now, we are only using windfields.
4430380	4432700	But we are adding topography, surface pressure,
4432700	4434660	and we are thinking about what other covariates
4434660	4437100	may be such escape to add to our model.
4437100	4440460	And once we have a baseline that we are satisfied with,
4440460	4441980	the important part comes.
4441980	4444300	And that is doing a thorough meteorological verification
4444300	4445300	of the downscale forecast.
4445300	4447660	Because like I said, we have operational goals.
4447660	4450380	And we really want to see how this forecast
4450380	4452340	respond to our needs.
4452340	4454820	Finally, once we finish this first step of the project,
4454820	4457820	we are planning on moving to the second more ambitious part,
4458100	4460940	which is to develop a large GI model that
4460940	4463660	is based on a pre-trained foundation model
4463660	4466900	that we will be fine-tuning in order
4466900	4468980	to obtain downscale forecasts.
4468980	4472180	And this is where, again, collaborating with IBM Research
4472180	4475900	is extremely important, as they have very much experience
4475900	4477140	in these foundation models.
4477140	4481020	And we are hoping to advance at least as fast as now.
4481020	4482340	So this is it for me.
4482340	4484220	Thank you very much.
4484220	4488140	Thank you, Medellina.
4488140	4489540	I have a question in the chat.
4489540	4491540	I have one.
4491540	4497860	I saw that you train, especially, the model.
4497860	4499940	But I was surprised to see that you only
4499940	4502500	used one year of data to do that.
4502500	4503740	Is there a reason behind that?
4503740	4507460	Because it seems to me that it's not a lot of data.
4507460	4510620	Yes, well, like I said, we have.
4510620	4515900	So we do end up having a lot of forecast samples.
4515900	4522060	We are training on 128 by 128 pixel patches out
4522060	4523060	of a very large domain.
4523060	4524900	So it ends up being a lot of data.
4524900	4526260	But it is not finalized.
4526260	4528420	So here we are in a developing mode.
4528420	4529620	We are trying to develop the model
4529620	4531300	and make sure it's working properly.
4531300	4532940	And this is just the first step.
4532940	4535220	We are definitely planning on adding more data
4535220	4537900	and seeing how we can improve.
4537900	4540540	Thank you, Medellina.
4540540	4544060	I don't have a question.
4544060	4547700	Otherwise, Anne, I'm going to speak.
4547700	4548780	Right on time.
4548780	4550940	Thank you, Medellina.
4550940	4557100	So now we're going over to Reynel Sospedra Alfonso, who
4557100	4560060	is also a research scientist at Environment and Climate
4560060	4561540	Change Canada.
4561540	4565460	He will be presenting on deep learning-based bias
4565500	4569140	adjustments of Arctic sea ice forecasts
4569140	4572140	from version three of the Canadian seasonal to
4572140	4575020	inter-annual prediction systems.
4575020	4578220	Also known as CANSTEP version three.
4578220	4581260	So, Reynel, the mic is all yours.
4581260	4582700	Thank you.
4582700	4583580	Thank you, Anne.
4583580	4586420	Can you hear me well?
4586420	4587380	Yes, we do.
4587380	4589700	Your presentation is not full screen, though.
4589700	4590500	It's not.
4590500	4590980	No, it's not.
4590980	4593980	All right.
4593980	4594580	How about now?
4595740	4596780	Yes, it is.
4596780	4597620	Perfect.
4597620	4599380	Thank you so much, Medell.
4599380	4601700	Thank you, yes, to the organizer for this opportunity.
4601700	4603540	My name is Reynel Sospedra Alfonso.
4603540	4606780	I'm a research scientist at the Canadian Center for Climate
4606780	4609380	Modeling and Analysis, based in Victoria.
4609380	4613180	And I want to start by acknowledging the contributions
4613180	4615300	or the work of colleagues at CCMA,
4615300	4619020	which made this type of project possible.
4619020	4621260	I list some of them down here.
4621260	4623620	And I also want to acknowledge the co-authors
4623620	4625820	of this presentation, or this work,
4625820	4629260	Joseph Martin, Michael Simon, and especially
4629260	4634140	Parca Guilla, who has been key for this project going forward.
4634140	4636980	He has taken the time to do the implementation, training,
4636980	4640860	and testing of the models we have been looking at.
4640860	4643540	And I want to mention that this is part, what I'm going to talk
4643540	4645460	about here is part of a bigger project
4645460	4648780	that we are trying to pursue at CCMA, which
4648780	4652860	is the use or applications of machine learning methods,
4652900	4656300	in particular deep learning, to post-process our seasonal
4656300	4658580	to the scale forecast.
4658580	4660780	So for this talk, I will talk in particular
4660780	4664140	about the post-processing or seasonal forecast of CIS.
4666980	4670140	The seasonal forecast, as you may know,
4670140	4673340	CANSIPS is the Canadian seasonal and internal prediction
4673340	4675900	system, which provides the Environment and Climate Change
4675900	4678580	Canada's operational, probabilistic seasonal
4678580	4681700	forecast, both national and global.
4681700	4686660	And CANSIPS first appeared or was first
4686660	4691420	debuted in 2011 as a two-model forecasting system.
4691420	4694540	It has evolved since, and now we are in 2024,
4694540	4697620	with the new version of CANSIPS B3,
4697620	4701660	which actually will be launched next month.
4701660	4705300	And so here, what I'm going to talk about
4705300	4709900	is the forecast that we produce with CANIAS M5, which
4709900	4714300	is a new model that now we'll be using in CANSIPS.
4714300	4719900	And CANIAS M5 is an air system model that is produced at CCMA,
4719900	4725460	and now will be then, as I said, used for our seasonal forecast.
4725460	4727700	CANIAS M5 air system models, so it
4727700	4732460	couples the atmosphere, the ocean, CIS component, land,
4732460	4737940	and also biochemistry, both on land and the ocean.
4737980	4740100	What we do, we take our climate model,
4740100	4743460	we initialize the climate model following the indications
4743460	4745340	that you see here on the right.
4745340	4747540	So when we initialize the forecast,
4747540	4750780	we take, we notch the model towards re-analysis,
4750780	4755300	and then we launch those forecasts in time.
4755300	4757700	The version of CANIAS M5 that I'm going to be talking about
4757700	4762980	is actually an optimal bias-corrected version, which
4762980	4766220	follows the work by Sino-Kankarin, which
4766220	4772100	does an online bias optimization to the model.
4772100	4777500	Now, the work that I'm going to be presenting to you
4777500	4780740	deals with the post-processing of those forecasts.
4780740	4783020	So here, what you see is a representation
4783020	4786620	of those forecasts in black.
4786620	4789700	It's the observations that we are verifying,
4789700	4792020	observations, the monthly values.
4792020	4794900	And then what you see is the representation
4794900	4797540	of those forecasts, which are initialized
4797540	4801100	at the start of every month during the handcast period.
4801100	4805020	So we'll be looking at handcasts from 1980 to 2021.
4805020	4808140	We have, for each month in that time,
4808140	4812260	we launch an ensemble of forecasts of 10 members,
4812260	4814860	which run for 12 months.
4814860	4817060	The variable of interest for us here
4817060	4819580	is CIS concentration, which is simply
4819580	4824500	the fraction of CIS, or the fraction of the grid cells
4824500	4826660	that is covered by CIS.
4826660	4829780	And this is what we want to adjust.
4833660	4836860	Well, we do that looking at the ensemble mean forecast.
4836860	4839340	So the adjustment is not done to the ensemble itself,
4839340	4841780	it's done to the ensemble mean.
4841780	4844300	And the question is, why do we have to do that?
4844300	4845660	I mean, after all, we are even doing
4845660	4849260	an online bias optimization or bias correction
4849260	4850540	of my model.
4850540	4855140	So still, we do need to adjust those forecasts,
4855140	4858180	because as we know, we have several sources of error,
4858180	4860940	structural errors, errors due to initialization,
4860940	4862260	and so forth.
4862260	4864860	And typically, this is done by doing
4864860	4868460	some climatological bias correction to those forecasts.
4868460	4870900	Now here, just to give you an example,
4870900	4874300	I'm showing you the September CIS concentration
4874300	4877540	over the time period 2006 to 2020.
4877580	4880500	On the left, these are the very fine observations
4880500	4885820	that we use, which comes from NOAA data products.
4885820	4888820	And here, again, this is the CIS concentration,
4888820	4891180	which has value from 0 to 1.
4891180	4895220	And we see on the right, the growth forecast
4895220	4896820	as it comes out of the model.
4896820	4900260	So this is the output directly coming from our forecast.
4900260	4903300	And what we see is that there is definitely
4903300	4905940	a strong bias that we notice, particularly
4905940	4909740	in the center Arctic, where we have a much lower CIS
4909740	4913260	concentration relative to observations.
4913260	4916340	Now on the right to that, we see the bias adjusted forecast,
4916340	4919700	in which we compensate or we subtract the bias,
4919700	4921660	compute it on a previous time.
4921660	4924540	And then what we see is that we adjust somehow
4924540	4928420	that forecast by doing this simple bias correction.
4928420	4931060	Now still, even after doing a bias correction,
4931060	4933940	we see that there are some differences
4933940	4936260	between the observed CIS concentration
4936260	4938180	and the one that is bias adjusted.
4938180	4940620	So that tells us that we need to do something else in order
4940620	4943420	to actually improve our forecast.
4943420	4945260	And for that, and this is now the project
4945260	4947780	that we are working on, is to use this machine learning
4947780	4951940	or deep learning method to improve even further
4951940	4953540	those forecasts.
4953540	4955180	The method that I'm going to use here,
4955180	4957140	and I'm going to talk a little bit more about that
4957140	4960420	in the few slides, is the unit.
4960420	4962540	Probably most of the people in the audience
4962540	4964020	are familiar with units.
4964020	4966300	As I said, I will talk a little bit more about that.
4966300	4971220	But here, I'm just showing you some results in which you see
4971220	4977020	how the unit is able to reproduce the spatial pattern a lot
4977020	4981500	better than what we can do with a simple bias correction.
4981500	4984220	I should have said that this is a forecast done
4984220	4985580	at two monthly time.
4985580	4987580	So this is the forecast two months
4987580	4990220	after the forecast is initialized,
4990260	4997420	where the biases are, you can see, are particularly strong.
4997420	5001700	Now, the unit, this is the tool of choice.
5001700	5005620	What we have here is a fully connected network,
5005620	5007660	or a fully convolutional network,
5007660	5013460	that has a downscaling, down sampling encoder,
5013460	5018460	followed by up sampling decoder.
5018460	5019900	This is a classical unit.
5019900	5023860	What we see is that the future maps are reduced in size.
5023860	5028740	So we see that the resolution is reduced by its health,
5028740	5030580	and the channels are increased by two.
5030580	5033900	And this allows us to have a better representation
5033900	5037820	of capacity of the network, while preserving some information
5037820	5039980	of the image that we input.
5039980	5043900	So to be clear, what we input here is our raw forecast,
5043900	5047220	which is denoted by the YMN.
5047300	5050300	Thanks, and will be the resolution of my forecast,
5050300	5052540	which is a function of the initial month
5052540	5057220	and the target month, and the time relative to the first month
5057220	5060900	in the data that we are inputting.
5060900	5064020	So the input is made of six channels,
5064020	5068100	one channel that is the actual variable
5068100	5071500	that we want to, or the actual map that we want to correct,
5071500	5073980	plus five temporal features that takes into account,
5073980	5076340	again, the initial month that we're looking at,
5076380	5078540	the target month, and this temporal information
5078540	5080460	relative to the initial time.
5080460	5083420	So in other words, how many years and how many months
5083420	5085500	from the starting of your data.
5085500	5087340	You input that into the network,
5087340	5089660	and then the output is, hopefully,
5089660	5091500	is an adjusted forecast,
5091500	5094180	which then correct for those biases
5094180	5095580	that I mentioned earlier.
5097700	5099580	Moving a little bit forward here,
5099580	5101340	let me just be a little bit more precise
5101340	5103500	on the kind of task that we are doing.
5103500	5104940	This is just a specific example,
5104940	5108700	which hopefully will clarify how we do this.
5108700	5111100	So in this case, let's say that we want to adjust
5111100	5113700	the March CIS concentration forecast,
5113700	5117060	which is initializing February of a year Y.
5117060	5120500	So essentially it will be this red dot denoted here
5120500	5121700	on the figure.
5121700	5125900	So again, what we want to do is to post-process this forecast.
5125900	5127740	We leverage the forecast that are produced
5127740	5128900	with our climate model.
5128900	5130620	We do not do prediction.
5130620	5133060	We just do that kind of bias adjustment
5133060	5134980	or post-processing of the predictions
5134980	5136980	that we get from our climate model.
5136980	5141300	So for this specific task, to correct that March CIS,
5141300	5146180	we train on the data that is available for the years
5146180	5148900	before the test year that we have.
5148900	5150700	So in this case, I'm denoting that here
5150700	5153020	for this in this shadow region.
5153020	5157260	So we take all the pairs of forecast and observations
5157260	5159980	for all lead times and all target month,
5159980	5162460	and then we train our network on that data,
5162460	5165260	and then we will do that iteratively for every test
5165260	5170060	years that we want to make the adjustment.
5170060	5172780	So what I'm going to do now is to show you some of the results
5172780	5173780	that we have.
5173780	5178900	I should say that this is very much a work in progress.
5178900	5183580	And that is different avenues that we are working on,
5183580	5187140	but we have some results that I want to share with you.
5187140	5191060	For instance, what we see here is the CIS concentration bias.
5191060	5195940	At the zero-month lead, average over the 2006 and 2020 time
5195940	5201180	period, which is the test years that we have for our analysis.
5201180	5205740	At the top, we have the bias for the March CIS concentration,
5205740	5209900	which is at the time of maximum CIS extent.
5209900	5212740	And at the bottom, we have the results or the bias
5212740	5215780	for the September CIS concentration,
5215780	5219140	which is at the time of a minimum CIS extent.
5219180	5222300	On the left, we see here what happened with the raw forecast.
5222300	5226700	We see the biases happening in several regions,
5226700	5228020	which is actually substantial.
5228020	5230460	Here, the bias is given in percent.
5230460	5232060	For the bias adjusted case, which
5232060	5235100	is the simple bias correction that I'm using here
5235100	5237420	as a benchmark, we see that we do improve
5237420	5239420	relative to the raw forecast, but there are still
5239420	5242180	some biases that are apparent, particularly
5242180	5244380	during the CIS minimum.
5244380	5246620	And then on the right is the results
5246620	5247860	that we get with the unit.
5247860	5251660	And we see that the bias are likely removed.
5251660	5253620	And now, this is for the zero-month lead.
5253620	5257700	So this is soon after we initialize our forecast.
5257700	5259820	Now, two months ahead in our forecast,
5259820	5262300	of course, the biases are increased.
5262300	5265020	Excuse me, we have two minutes.
5265020	5265540	Two minutes.
5265540	5267460	Thank you.
5267460	5269060	All right, so here is the case in which we
5269060	5270580	have two-month lead forecasts.
5270580	5272460	And we see that the bias, of course, are increased.
5272460	5275660	But still, we are able to manage those biases with the unit,
5275700	5281180	given a better representation of the CIS edges.
5281180	5283060	Another measure that we look at here
5283060	5287460	is the integrated ICH error, which is essentially the area,
5287460	5289220	the integrated area that we have,
5289220	5292460	in which both forecast and observations
5292460	5296380	disagree in the concentration with a threshold of 15%.
5296380	5301020	So we both have more than 50% concentration
5301020	5303140	or less than 50%.
5303140	5304860	This binary error will be zero.
5304900	5307060	If they are different, then the binary error will be one.
5307060	5312140	And those grid cells will contribute to this area error.
5312140	5315140	So this is at the top here, we see a hit map
5315140	5318540	in which we have our target month in the X axis.
5318540	5320940	And on the Y, we have the lead month.
5320940	5324860	And bottom message here is that the blue are bad.
5324860	5328700	The red are good, meaning there is less error.
5328700	5332140	And the unit bids both the row, of course,
5332140	5334500	and the bias are used to forecast.
5334500	5336740	Down here is just an integration over lead month
5336740	5338580	just to give a more clear picture
5338580	5342140	of how the unit outperforms the alternative.
5343180	5344740	Now, I know that I don't have much time,
5344740	5349060	so I will not go into the details of these results,
5349060	5351460	but at least to give you an idea of what is happening.
5351460	5354620	In this case, we are looking at the CIS area.
5354620	5355780	Same time of floods.
5355780	5360380	Again, red means that we have a better room mean
5360380	5361900	and square error, which is the measure
5361980	5364820	that we are using here for the CIS area.
5364820	5367100	Blue means that the errors are increased.
5367100	5369660	And in this particular case, we see that the unit
5369660	5373420	is slightly better than the bias corrected one.
5373420	5375900	However, if we look at CIS extent,
5375900	5380260	which is now the area in which we count for all those grid cells
5380260	5382620	with concentration greater than 50%,
5382620	5385140	we see that unit largely outperform
5385140	5386580	this bias-adjusted method.
5387580	5392740	Finally, I want just to mention that here,
5392740	5395700	we have seen different measures of the skill
5395700	5398980	in which we outperform both the benchmark
5398980	5401740	and the raw forecast, but there is still some issues
5401740	5404980	in terms of the representation of the temporal dependence
5404980	5406500	of our adjusted forecast.
5406500	5410180	And so, but unlike for this slide here,
5410180	5412860	just to mention that we still have some work to do
5412860	5417380	to be able to better represent the temporal dependence
5417380	5419220	of those forecasts.
5419220	5421060	And because I don't have much more time,
5421060	5424300	I will finish with this slide, which is some final remarks.
5424300	5425900	We'll leave it there for you.
5425900	5428940	And I will be happy to take any questions.
5428940	5429780	Thank you.
5431620	5432940	Thank you, Renel.
5432940	5437940	I saw that there seems to be to have a seasonal pattern
5438260	5439780	in your verification.
5439780	5442980	That leads me to the questions.
5442980	5447180	Are there any biases that are harder to adjust
5447180	5448420	with this method?
5450420	5452340	The answer for that, yes.
5452340	5456220	And this is anality that we see has to do in part
5456220	5458020	with how the eyes behave.
5458020	5461100	We have what is known as this predictability barrier
5461100	5463180	after spring, which makes it difficult
5463180	5465740	to do a good prediction of the sea eyes.
5465740	5467580	This is something that perhaps we can see
5467700	5471380	all those metrics, actually perhaps we'll go to this one.
5471380	5475260	So this barrier, which reduces the skill
5475260	5476620	that we have in our predictions,
5476620	5480740	can be seen here around the month of June to October,
5480740	5482540	looking here at target month.
5482540	5486260	And so that is something within the raw forecast
5486260	5489740	in person, the bias corrected one, but also in the unit.
5489740	5492100	So we do improve on those months,
5492100	5495060	but still there is some skill that is missing there,
5495060	5497140	which is part of the natural process
5497140	5499180	in the sea eyes formation and melting,
5499180	5500700	which then translate into the skill
5500700	5502540	that we see in our just forecast.
5503700	5505020	Thank you.
5505020	5509620	One last question is coming from the online Q&A.
5510580	5513300	Would training the model on all months of the year
5513300	5515820	versus only certain months affect its ability
5515820	5516900	to improve the forecast?
5516900	5520060	For example, forecasts of sea eyes in spring
5520060	5521620	are known to perform poorly.
5521620	5523700	So if you were to exclude these months,
5524220	5527340	would this improve the bias adjustment?
5527340	5529260	Yeah, that's a good question.
5529260	5531100	Actually, when I was, I mean, it's too bad
5531100	5532660	that I'm rushing through all these slides,
5532660	5534500	but one of the things that I wanted to mention
5534500	5538140	at this particular slide is that we do the training.
5538140	5541140	You're looking at the previous years in my forecast,
5541140	5542820	but we are missing some of the information
5542820	5546460	of more recent months that can also contribute
5546460	5547300	to the forecast.
5547300	5549060	Like looking here, for instance,
5549060	5551740	we do not use information from January
5551740	5553500	of this specific year.
5553500	5555100	Now, one thing that we are exploring
5555100	5558060	is to look at training based on lead times
5558060	5559060	or a specific month.
5559060	5561620	I think that that's the idea of the question,
5561620	5565820	which we try to train our model specific to the lead times
5565820	5568540	in which we have the biases that we want to correct
5568540	5573020	and the kind of information specific to the seasonality
5573020	5574700	that we want also to correct.
5574700	5576740	So those are things that we are exploring,
5576740	5579580	but so far this is what we have.
5580580	5582580	Thank you, Rénel.
5582580	5587580	This concludes our first block for the first part
5587580	5590580	of the presentation for artificial intelligence.
5590580	5593580	A big thank you to Rénel, Madlena, Saïd, Hervé,
5593580	5597580	and Christopher for having made these presentations.
5597580	5599580	Anne, I'll leave you here.
5599580	5603580	Yes, a big thank you to all the presenters,
5603580	5605580	all the attendees as well.
5605580	5608580	And as several mentioned before,
5608580	5611580	there is a second session coming up in about 20 minutes,
5611580	5612580	not even.
5612580	5615580	So we'd love to see you back here,
5615580	5620580	a lot more to see and to hear from people outside ECCC
5620580	5621580	as well.
5621580	5622580	So please do join us.
5622580	5623580	Thank you.
5623580	5624580	There's not a sign.
5624580	5627580	There's not a sign, that's it.
5627580	5628580	Thanks.
5628580	5629580	You're better.
5629580	5630580	Bye.
5630580	5631580	Bye.
5635580	5636580	Thank you.
