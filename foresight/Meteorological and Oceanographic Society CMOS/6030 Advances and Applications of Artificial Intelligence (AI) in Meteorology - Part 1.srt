1
00:00:00,000 --> 00:00:06,000
Let me try for you. Okay.

2
00:00:06,000 --> 00:00:28,000
Thank you.

3
00:00:28,000 --> 00:00:41,000
Thank you.

4
00:00:58,000 --> 00:01:18,000
Thank you.

5
00:01:28,000 --> 00:01:43,000
Hello and welcome.

6
00:01:43,000 --> 00:01:49,000
Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel

7
00:01:49,000 --> 00:01:53,000
Tremblay. We both work for the Meteorological Service of Canada within

8
00:01:53,000 --> 00:01:58,000
Environment and Climate Change Canada and we're happy to be here exploring AI

9
00:01:58,000 --> 00:02:04,000
with you for a second year. Before we begin, a few reminders. 15 minutes are

10
00:02:04,000 --> 00:02:08,000
reserved for each presentation and we encourage our presenters to keep three

11
00:02:08,000 --> 00:02:12,000
minutes for questions at the end. Time permitting participants with questions

12
00:02:12,000 --> 00:02:17,000
will be able to use the chat at the end of the presentation and their questions

13
00:02:17,000 --> 00:02:25,000
can be upvoted by the other attendees. Miguel will provide up at 10 and 12

14
00:02:25,000 --> 00:02:30,000
minutes and will bring the presentation to an end at 15 and will be ruthless

15
00:02:30,000 --> 00:02:35,000
guys because we want everyone to have time to present. Miguel, do you want to do

16
00:02:35,000 --> 00:02:58,000
a quick recap in French?

17
00:02:58,000 --> 00:03:04,000
A few reminders. 15 minutes are reserved for each presentation and we encourage

18
00:03:04,000 --> 00:03:16,000
our presenters to keep time for questions at the end of the presentation and we

19
00:03:16,000 --> 00:03:22,000
will be able to use the chat at the end of the presentation and we will be able to

20
00:03:22,000 --> 00:03:27,000
use the chat at the end of the presentation and we will be able to use the chat at

21
00:03:27,000 --> 00:03:32,720
the end of the presentation and we will be able to use the chat

22
00:03:42,000 --> 00:03:51,000
fpspe conditional 10 minutes for one and a half minutes for 10 minutes for

23
00:03:51,000 --> 00:03:55,400
weather forecasting, and ECCC's research plans.

24
00:03:55,400 --> 00:03:58,600
So without further ado, over to you, Christopher.

25
00:03:58,600 --> 00:03:59,440
Thank you.

26
00:03:59,440 --> 00:04:02,540
Let me get the screen sharing going and share.

27
00:04:03,880 --> 00:04:06,400
Okay, yep, looks like we're good.

28
00:04:06,400 --> 00:04:07,760
Okay, hello everybody.

29
00:04:07,760 --> 00:04:08,600
I'm Christopher Subic

30
00:04:08,600 --> 00:04:10,200
from Environment and Climate Change Canada.

31
00:04:10,200 --> 00:04:12,040
I'm here to speak as said about,

32
00:04:14,400 --> 00:04:16,080
well, click, okay.

33
00:04:16,080 --> 00:04:17,760
This talk is essentially in two parts.

34
00:04:17,760 --> 00:04:20,120
The first part is going to be a brief summary

35
00:04:20,120 --> 00:04:23,520
of the current state of machine learning based forecasting

36
00:04:24,640 --> 00:04:29,400
with a broad focus on medium range weather forecasts.

37
00:04:30,200 --> 00:04:33,800
Second part of this talk will be the forthcoming AI roadmap

38
00:04:33,800 --> 00:04:36,520
out of the atmospheric science

39
00:04:36,520 --> 00:04:38,280
of technology director at NCCMAP,

40
00:04:39,280 --> 00:04:42,640
which effectively broadly sets out

41
00:04:42,640 --> 00:04:46,080
where this part of Environment and Climate Change Canada

42
00:04:46,080 --> 00:04:49,920
will be going in the short of medium term future with AI.

43
00:04:50,240 --> 00:04:51,840
And finally, time permitting,

44
00:04:51,840 --> 00:04:55,040
I'll conclude with a preview of talks to come

45
00:04:55,040 --> 00:05:00,040
and just general thoughts on the state of AI in forecasting.

46
00:05:00,240 --> 00:05:02,400
So in case you haven't noticed,

47
00:05:02,400 --> 00:05:04,040
AI is becoming a big deal.

48
00:05:04,040 --> 00:05:06,040
It's no longer just pictures of cats

49
00:05:06,040 --> 00:05:08,040
or helping kids cheat on their homework,

50
00:05:08,040 --> 00:05:12,880
but it's starting to influence the industries

51
00:05:12,880 --> 00:05:16,120
and feels previously thought unassailable.

52
00:05:17,120 --> 00:05:20,920
Now, this isn't truly a stranger to weather forecasting.

53
00:05:20,920 --> 00:05:23,560
AI adjacent topics have long had roles

54
00:05:23,560 --> 00:05:27,760
in the forecast production system.

55
00:05:28,800 --> 00:05:30,640
We've long had statistical models

56
00:05:30,640 --> 00:05:32,880
for quality control of observational data

57
00:05:32,880 --> 00:05:34,680
and for post-processing,

58
00:05:34,680 --> 00:05:38,640
but a phase change has been that over the past two years,

59
00:05:38,640 --> 00:05:42,360
maybe three models from the academic and private sectors

60
00:05:42,360 --> 00:05:45,800
have gone from things that we should probably

61
00:05:45,800 --> 00:05:47,840
keep our eye on as interesting things

62
00:05:47,840 --> 00:05:50,120
for the long-term future to, well,

63
00:05:50,120 --> 00:05:54,000
these two nearly full-featured forecast systems

64
00:05:54,000 --> 00:05:56,280
that are competitive with the state of the art.

65
00:05:58,040 --> 00:06:01,000
With that in mind, I can just state very plainly

66
00:06:01,000 --> 00:06:02,920
that Environment Climate Change Canada,

67
00:06:02,920 --> 00:06:07,040
in particular my part of it, is taking AI very seriously

68
00:06:07,040 --> 00:06:09,360
and we intend for it to have an increasing role

69
00:06:09,360 --> 00:06:13,480
in numerical weather prediction systems going forward.

70
00:06:13,600 --> 00:06:16,400
We're taking a sort of trust but verify approach

71
00:06:16,400 --> 00:06:19,760
in that we intend to make AI advances operational,

72
00:06:19,760 --> 00:06:21,880
but only after they've been scientifically proven.

73
00:06:21,880 --> 00:06:26,880
We're not in a rush to perform science by press release

74
00:06:27,480 --> 00:06:31,440
and this is also a medium to long-term plan.

75
00:06:31,440 --> 00:06:33,680
Capacity building is going to take years,

76
00:06:33,680 --> 00:06:37,920
both in terms of acquiring sufficient computational power

77
00:06:37,920 --> 00:06:40,080
for this and developing the human expertise

78
00:06:40,080 --> 00:06:41,880
necessary to properly use it.

79
00:06:43,600 --> 00:06:48,600
The modern AI forecast field essentially began

80
00:06:49,720 --> 00:06:50,680
about two years ago.

81
00:06:50,680 --> 00:06:54,000
I dated from the first publication of GraphCast

82
00:06:54,000 --> 00:06:57,320
by Lam in 2023, which is the first time

83
00:06:57,320 --> 00:07:01,680
that an AI model could truly claim to beat

84
00:07:01,680 --> 00:07:04,840
the state of the art from, in this case,

85
00:07:04,840 --> 00:07:07,920
the equivalent forecast from ECMWF.

86
00:07:10,320 --> 00:07:12,720
In addition, the truly shocking claim

87
00:07:12,760 --> 00:07:14,640
was that these forecasts came with orders

88
00:07:14,640 --> 00:07:16,720
of magnitude faster running time.

89
00:07:16,720 --> 00:07:19,360
GraphCast will produce a 10-day quarter-degree forecast

90
00:07:19,360 --> 00:07:22,000
in about 30 seconds on a single GPU.

91
00:07:22,920 --> 00:07:26,840
That put all of the weather centers globally on notice

92
00:07:26,840 --> 00:07:30,080
that if these trends continue, we must adapt

93
00:07:30,080 --> 00:07:31,280
or we'll fall behind.

94
00:07:31,280 --> 00:07:34,080
And I mean, in some sense, research has always been that.

95
00:07:34,080 --> 00:07:37,320
You can't sit on your laurels from 20 years ago

96
00:07:37,320 --> 00:07:38,600
and pretend to be competitive,

97
00:07:38,640 --> 00:07:43,640
but this field is truly advancing at a revolutionary rate.

98
00:07:48,800 --> 00:07:51,320
Models are being constantly released.

99
00:07:51,320 --> 00:07:53,320
Any summary, like the one I'm about to give

100
00:07:53,320 --> 00:07:54,680
was going to be out of date within weeks.

101
00:07:54,680 --> 00:07:57,320
In fact, just this morning, I've had to update my slides

102
00:07:57,320 --> 00:07:59,600
to add two new preprints that have come out

103
00:07:59,600 --> 00:08:03,320
in the past four or five days.

104
00:08:03,320 --> 00:08:05,120
Nonetheless, there are some common features

105
00:08:05,120 --> 00:08:07,880
between medium-range models that are shared

106
00:08:07,880 --> 00:08:09,920
by, if not all of them, then most of them.

107
00:08:11,200 --> 00:08:13,520
Most broadly speaking, these models attempt to learn

108
00:08:13,520 --> 00:08:15,480
from data, which means rather than simulate

109
00:08:15,480 --> 00:08:17,800
the atmosphere from first principles,

110
00:08:17,800 --> 00:08:22,600
they try to look at some form of data

111
00:08:22,600 --> 00:08:24,920
and predict what it will become.

112
00:08:24,920 --> 00:08:26,800
In this case, for the medium-range forecasting,

113
00:08:26,800 --> 00:08:29,360
the data is almost always the error of five reanalysis.

114
00:08:29,360 --> 00:08:31,640
That's our highest quality, longest-term,

115
00:08:31,640 --> 00:08:33,960
and most uniform record of the atmospheric state

116
00:08:33,960 --> 00:08:37,240
that we have, and it's accepted as ground truth

117
00:08:37,240 --> 00:08:39,840
for most of the AI models, but this data set

118
00:08:39,840 --> 00:08:42,960
does have known issues such as relatively poor precipitation.

119
00:08:44,880 --> 00:08:47,880
The other common feature about AI forecast models

120
00:08:47,880 --> 00:08:50,600
is that most of them have sparse limited outputs.

121
00:08:50,600 --> 00:08:53,600
They tend to predict only a few variables.

122
00:08:53,600 --> 00:08:57,080
They tend to predict a small subset of vertical levels

123
00:08:57,080 --> 00:09:00,160
compared to what we're used to from an operational forecast,

124
00:09:00,160 --> 00:09:02,960
and they have a limited selection of lead times.

125
00:09:04,320 --> 00:09:06,440
This creates new downscaling problems.

126
00:09:06,440 --> 00:09:08,720
If you were in the plenary talk,

127
00:09:08,720 --> 00:09:11,600
you heard our friend from Nvidia say that

128
00:09:13,280 --> 00:09:16,600
this might not be a barrier to full prediction,

129
00:09:16,600 --> 00:09:19,000
but at the same time, we're used to having the prediction

130
00:09:19,000 --> 00:09:21,240
plus all of the rich output for free,

131
00:09:21,240 --> 00:09:25,440
and not having that means we'll need to contemplate

132
00:09:25,440 --> 00:09:27,160
new kinds of downscaling problems

133
00:09:27,160 --> 00:09:31,520
to get rich data from a sparse forecast.

134
00:09:31,520 --> 00:09:33,800
And finally, these AI forecast systems

135
00:09:33,800 --> 00:09:37,440
have essentially no proofs of physical consistency.

136
00:09:37,440 --> 00:09:42,440
Even hybrid models and development in that area

137
00:09:42,760 --> 00:09:44,600
has a classical dynamical core,

138
00:09:44,600 --> 00:09:46,920
but leaves a physics system that is

139
00:09:48,080 --> 00:09:49,920
only tangentially concerned with things

140
00:09:49,920 --> 00:09:51,000
like conserving energy.

141
00:09:52,560 --> 00:09:55,040
Now, medium range forecasts tend to break down

142
00:09:55,040 --> 00:09:56,360
into a few different categories.

143
00:09:56,360 --> 00:09:59,080
The first and most conventional of these

144
00:09:59,080 --> 00:10:00,960
are deterministic models.

145
00:10:00,960 --> 00:10:02,440
These are analysis predictors,

146
00:10:02,440 --> 00:10:04,320
and they essentially answer the question of,

147
00:10:04,320 --> 00:10:06,960
if I have the atmospheric analysis at time zero,

148
00:10:06,960 --> 00:10:08,680
what is the minimum error prediction

149
00:10:08,680 --> 00:10:11,560
of what that analysis is going to be six hours from now?

150
00:10:12,600 --> 00:10:14,120
As a general rule, these tend to give

151
00:10:14,120 --> 00:10:17,480
overly smooth forecasts that over long lead times,

152
00:10:17,480 --> 00:10:19,880
erase fine scale structure in the atmosphere.

153
00:10:19,880 --> 00:10:22,880
The widely held belief is that this is a consequence

154
00:10:22,880 --> 00:10:27,880
of training based on mean squared error measures,

155
00:10:29,040 --> 00:10:31,080
because the lowest mean squared error prediction

156
00:10:31,080 --> 00:10:34,000
in the future is your ensemble average.

157
00:10:34,000 --> 00:10:35,520
But the ensemble average is not

158
00:10:35,520 --> 00:10:37,840
a physically plausible forecast.

159
00:10:39,520 --> 00:10:42,640
Now, that being said, these models have shown great success,

160
00:10:42,640 --> 00:10:45,880
and having a really good ensemble mean prediction

161
00:10:45,880 --> 00:10:47,640
of the future is still a really good prediction

162
00:10:47,640 --> 00:10:48,480
of the future.

163
00:10:48,480 --> 00:10:51,120
Each day of predictability translates to billions

164
00:10:51,120 --> 00:10:55,440
or billions of dollars of enabled economic activity.

165
00:10:56,440 --> 00:11:01,280
This category of models is in some sense the oldest,

166
00:11:01,280 --> 00:11:03,080
and I really hesitate to use that word

167
00:11:03,080 --> 00:11:05,200
with a field that's about two or three years old,

168
00:11:05,200 --> 00:11:08,120
but they're models from many different groups.

169
00:11:08,120 --> 00:11:11,160
Graphcast and ForecastNet have been previously mentioned.

170
00:11:11,160 --> 00:11:12,680
Graphcast is a graph neural network.

171
00:11:12,680 --> 00:11:16,600
ForecastNet is now a spectral Fourier neural operator

172
00:11:16,600 --> 00:11:20,840
that operates in a spherical harmonic space.

173
00:11:20,840 --> 00:11:23,040
Pangu weather came out as a vision transformer,

174
00:11:23,040 --> 00:11:26,720
and AIFS is now apparently officially published

175
00:11:26,720 --> 00:11:29,120
with a pre-print, and it's a graph transformer.

176
00:11:31,360 --> 00:11:33,360
The jargon here is not that important,

177
00:11:33,360 --> 00:11:36,760
but the underlying point is that you can reach

178
00:11:36,760 --> 00:11:38,120
a deterministic forecast

179
00:11:38,120 --> 00:11:41,320
with many different AI architectures.

180
00:11:41,320 --> 00:11:45,760
There's, as of yet, no specific Royal Road to a forecast.

181
00:11:46,880 --> 00:11:49,120
The second category that I've somewhat arbitrarily divided

182
00:11:49,120 --> 00:11:51,200
things into are ensemble models.

183
00:11:51,200 --> 00:11:54,160
These try to address the problem of overly smooth forecast

184
00:11:54,160 --> 00:11:56,840
by adding some element of randomness.

185
00:11:56,840 --> 00:11:58,840
An ensemble forecaster will receive

186
00:11:58,840 --> 00:12:00,520
some kind of random data source,

187
00:12:00,520 --> 00:12:03,840
either a random number generator or a field of noise,

188
00:12:03,840 --> 00:12:06,640
and it's asked to generate essentially different forecasts

189
00:12:06,640 --> 00:12:08,520
from the same seven initial conditions.

190
00:12:08,520 --> 00:12:13,480
This more or less solves the problem of smoothing

191
00:12:13,480 --> 00:12:15,480
based on mean squared error loss functions

192
00:12:15,480 --> 00:12:19,360
because each individual forecast no longer has to track

193
00:12:19,360 --> 00:12:24,720
the long-term forecast.

194
00:12:24,720 --> 00:12:26,440
No longer has to track the long-term truth,

195
00:12:26,440 --> 00:12:27,840
but each one can be plausible,

196
00:12:27,840 --> 00:12:31,000
and then you're tracking the future

197
00:12:31,000 --> 00:12:32,840
with a true ensemble mean.

198
00:12:32,840 --> 00:12:34,560
Now, the downside is that these models

199
00:12:34,560 --> 00:12:36,560
are all more expensive to train and run

200
00:12:36,560 --> 00:12:39,320
the deterministic systems of the same size.

201
00:12:39,320 --> 00:12:42,560
In operations, you will need at least one inference

202
00:12:42,560 --> 00:12:44,120
run per ensemble member.

203
00:12:44,120 --> 00:12:49,040
So, graphcast's 30-second, 10-day forecast is great,

204
00:12:49,040 --> 00:12:51,240
but if I want a 100-member ensemble,

205
00:12:51,240 --> 00:12:54,520
now I'm talking an hour or two,

206
00:12:54,520 --> 00:12:58,000
and that can add up very quickly.

207
00:12:58,000 --> 00:12:59,720
In terms of training, if you're training

208
00:12:59,720 --> 00:13:01,080
with an ensemble error measure

209
00:13:01,080 --> 00:13:04,080
like the cumulative rank probabilistic score,

210
00:13:04,080 --> 00:13:06,040
that requires training over an ensemble,

211
00:13:06,040 --> 00:13:09,120
and increasing the size of things during training

212
00:13:09,120 --> 00:13:13,600
tends to increase the scarce GPU memory requirements

213
00:13:13,600 --> 00:13:18,280
and the overall cost of building the system.

214
00:13:18,280 --> 00:13:20,160
This is a newer frontier of AI forecasting.

215
00:13:20,160 --> 00:13:22,960
It's probably going to be more popular in the months to come,

216
00:13:22,960 --> 00:13:25,960
and the examples here are also somewhat newer.

217
00:13:25,960 --> 00:13:28,400
Gencast was previously mentioned in our plenary.

218
00:13:28,400 --> 00:13:31,400
It's a diffusion model based on graph transformer network.

219
00:13:31,400 --> 00:13:35,560
Neural GCM is a hybrid model, also previously mentioned,

220
00:13:35,560 --> 00:13:37,800
and it's a dynamical core

221
00:13:37,800 --> 00:13:39,920
that has learned physics parameterizations,

222
00:13:39,920 --> 00:13:42,000
and in particular, can operate in a non-subtle mode,

223
00:13:42,000 --> 00:13:43,680
which is why I've included it here.

224
00:13:43,680 --> 00:13:46,280
And finally, there are models such as seeds,

225
00:13:46,280 --> 00:13:49,400
also out of Google, that don't try to forecast directly,

226
00:13:49,400 --> 00:13:53,160
but they try to take an ensemble mean

227
00:13:54,280 --> 00:13:55,800
that already exists from some method,

228
00:13:55,800 --> 00:13:57,280
like a traditional forecasting system,

229
00:13:57,280 --> 00:13:59,000
and generate new ensemble members

230
00:13:59,000 --> 00:14:01,520
to fill out the probability distributions.

231
00:14:02,600 --> 00:14:04,760
The final category of forecast models,

232
00:14:04,760 --> 00:14:07,640
and I'm going to divide things into our foundation models,

233
00:14:07,640 --> 00:14:09,200
and these essentially answer the question of,

234
00:14:09,200 --> 00:14:12,320
what if we had GPT-4, but for weather?

235
00:14:12,320 --> 00:14:15,400
The idea here is and shared by foundation models

236
00:14:15,400 --> 00:14:16,680
that exist in our development,

237
00:14:16,680 --> 00:14:19,880
is that you break up a weather state,

238
00:14:19,880 --> 00:14:21,880
like the analysis, into tokens

239
00:14:21,880 --> 00:14:24,920
by regionally subdividing it usually,

240
00:14:24,920 --> 00:14:27,400
and then you ask the foundation model

241
00:14:27,400 --> 00:14:29,520
to predict missing pieces of it.

242
00:14:29,520 --> 00:14:32,120
You say, you can mask out the future,

243
00:14:32,120 --> 00:14:33,480
and say, okay, predict this,

244
00:14:33,480 --> 00:14:36,600
and then you're training it in a forecast context,

245
00:14:36,600 --> 00:14:39,840
or you can mask out a missing regional piece,

246
00:14:39,840 --> 00:14:41,160
and ask it to fill in the blank.

247
00:14:41,160 --> 00:14:43,840
You could even ask it to predict the past, I suppose.

248
00:14:45,640 --> 00:14:48,000
This allows the foundation model

249
00:14:48,000 --> 00:14:51,720
to be trained on qualitatively different data sources.

250
00:14:51,720 --> 00:14:56,120
For example, you might have an encoder suite

251
00:14:56,120 --> 00:14:58,280
that takes satellite observations directly,

252
00:14:58,280 --> 00:15:00,740
and tries to turn it into token space,

253
00:15:01,800 --> 00:15:04,720
or the analysis from era five,

254
00:15:04,720 --> 00:15:09,120
or lower resolution climate forecasts.

255
00:15:10,320 --> 00:15:13,760
And once tokenized, the idea of a foundation model

256
00:15:13,760 --> 00:15:16,480
is that you have a giant middle processor layer

257
00:15:16,480 --> 00:15:18,440
that operates in this latent space,

258
00:15:18,440 --> 00:15:20,840
and from there it is relatively simple

259
00:15:20,840 --> 00:15:23,760
to build out decoder models to give you things you want,

260
00:15:23,760 --> 00:15:25,440
like tomorrow's forecast today,

261
00:15:25,440 --> 00:15:29,320
or what the radar is going to look like in 30 minutes.

262
00:15:30,480 --> 00:15:33,180
Foundation models certainly prove their worth

263
00:15:33,180 --> 00:15:36,080
with text-based processing like GPT,

264
00:15:36,080 --> 00:15:38,560
and they're also very popular and emerging

265
00:15:38,560 --> 00:15:40,840
in Earth observation applications

266
00:15:40,840 --> 00:15:42,800
with interpretation of satellite data.

267
00:15:42,800 --> 00:15:47,400
For example, at an ECMWFESA conference a few weeks ago,

268
00:15:47,400 --> 00:15:48,560
there were some interesting talks

269
00:15:48,560 --> 00:15:51,000
about taking satellite data

270
00:15:51,000 --> 00:15:55,800
and using it to infer tree cover near power lines in Norway,

271
00:15:55,800 --> 00:15:57,040
tasks that would normally take

272
00:15:57,040 --> 00:15:58,840
some very expensive helicopters,

273
00:15:58,840 --> 00:16:00,740
and doing them much, much more cheaply

274
00:16:00,740 --> 00:16:03,720
with readily available Earth observation data.

275
00:16:03,720 --> 00:16:05,440
The downside is that foundation models

276
00:16:05,440 --> 00:16:07,920
tend to be extremely expensive to train,

277
00:16:07,920 --> 00:16:10,020
and these will push the limits

278
00:16:10,060 --> 00:16:12,860
of what the public sector is probably willing to spend

279
00:16:12,860 --> 00:16:15,940
should foundation models prove themselves for forecasting.

280
00:16:15,940 --> 00:16:18,180
Few of these models exist right now for NWP,

281
00:16:18,180 --> 00:16:20,260
but there's plenty of private sector interest.

282
00:16:20,260 --> 00:16:24,060
I mean, our friend from NVIDIA talked at length about that.

283
00:16:24,060 --> 00:16:27,140
The two models that are currently published

284
00:16:27,140 --> 00:16:29,180
are ATMO-REP from Christian Lesig,

285
00:16:29,180 --> 00:16:31,160
which is a transformer model,

286
00:16:31,160 --> 00:16:33,740
tested on both forecast and downscaling applications,

287
00:16:33,740 --> 00:16:37,300
and Microsoft has published, as of a few days ago,

288
00:16:37,300 --> 00:16:38,660
it's Aurora Foundation model,

289
00:16:38,660 --> 00:16:43,660
which most notably uses several different data sources

290
00:16:43,700 --> 00:16:45,560
for training, not just air five,

291
00:16:45,560 --> 00:16:48,820
but also climate simulations and operational forecasts,

292
00:16:48,820 --> 00:16:52,020
and I believe one of Noah's ensembles.

293
00:16:53,180 --> 00:16:56,620
Now, to integrate all of this,

294
00:16:56,620 --> 00:16:59,140
as a researcher, I can make a few broad predictions

295
00:16:59,140 --> 00:17:02,740
on where the trends are in this area.

296
00:17:02,740 --> 00:17:04,680
First is going to be the convergence

297
00:17:04,680 --> 00:17:06,180
of data simulation forecasts,

298
00:17:06,180 --> 00:17:08,140
nowcast and downscaling roles.

299
00:17:08,140 --> 00:17:11,100
Right now, the leading NWP forecast,

300
00:17:11,100 --> 00:17:13,380
take in an analysis and give you a forecast,

301
00:17:13,380 --> 00:17:15,900
but the obvious question is to what extent

302
00:17:15,900 --> 00:17:18,140
can the rest of the chain be included?

303
00:17:19,780 --> 00:17:23,460
For example, forecasts that are ensemble generating

304
00:17:23,460 --> 00:17:25,560
can often be reversed to perform data simulation,

305
00:17:25,560 --> 00:17:28,180
and a couple of references here are one,

306
00:17:28,180 --> 00:17:29,760
using a diffusion-based model

307
00:17:29,760 --> 00:17:32,340
to assimilate sparse observations,

308
00:17:32,340 --> 00:17:36,140
and the second one is to perform data simulation

309
00:17:36,140 --> 00:17:38,580
inside the latent space of an autoencoder,

310
00:17:38,580 --> 00:17:41,660
which effectively replaces the background

311
00:17:41,660 --> 00:17:45,980
error covariance matrices of a DA system

312
00:17:45,980 --> 00:17:49,380
with ones that are nonlinear and flow dependent

313
00:17:49,380 --> 00:17:51,180
from the autoencoder space.

314
00:17:51,180 --> 00:17:54,580
A second avenue here is to combine conventional models

315
00:17:54,580 --> 00:17:56,660
with an AI-based bias correction.

316
00:17:56,660 --> 00:18:00,700
Farshie from ECMWF has published recently

317
00:18:00,700 --> 00:18:03,540
on using a neural network bias correction

318
00:18:03,540 --> 00:18:08,540
to include some element of AI inside the IFS-40 var system.

319
00:18:08,860 --> 00:18:11,260
And Said, my colleague, is going to present

320
00:18:11,260 --> 00:18:15,260
in about half an hour on spectral nudging

321
00:18:15,260 --> 00:18:18,900
to bring a classical NWP system

322
00:18:18,900 --> 00:18:23,460
closer to an AI forecast to preserve rich data

323
00:18:23,460 --> 00:18:26,140
and have the accuracy of AI.

324
00:18:27,780 --> 00:18:29,220
And finally, there's some effort

325
00:18:29,220 --> 00:18:31,180
on all-in-one AI forecast systems

326
00:18:31,180 --> 00:18:34,100
that go directly from observations to forecasts

327
00:18:34,100 --> 00:18:36,420
to post-processing to produce predictions

328
00:18:36,420 --> 00:18:37,700
of future observations.

329
00:18:39,140 --> 00:18:42,300
The Vaughn 2024 is the Aurora mentioned in the plenary,

330
00:18:42,300 --> 00:18:44,780
which is an encoder, decoder architecture

331
00:18:44,780 --> 00:18:47,980
that uses Unets, and it's the obvious long-term future

332
00:18:47,980 --> 00:18:49,660
of foundation models.

333
00:18:49,660 --> 00:18:51,780
There's also rumors that Google is buying up

334
00:18:51,780 --> 00:18:54,380
satellite data for its next generation,

335
00:18:54,380 --> 00:18:56,460
GraphCast version two or something like that,

336
00:18:56,460 --> 00:18:58,820
but I don't have any particular

337
00:18:58,820 --> 00:19:00,380
confidential information to share.

338
00:19:01,180 --> 00:19:02,780
Okay, now the second part of this talk

339
00:19:02,780 --> 00:19:05,180
is the Environment and Climate Change Canada AI roadmap.

340
00:19:05,180 --> 00:19:09,460
This is a joint product of the AASTD and CCMAP.

341
00:19:09,460 --> 00:19:11,380
It's a product of an internal Tiger team

342
00:19:11,380 --> 00:19:14,020
across the research and operational divisions

343
00:19:14,020 --> 00:19:17,340
that was put together last fall after an internal study

344
00:19:17,340 --> 00:19:19,500
on the current state of AI and weather forecasting,

345
00:19:19,500 --> 00:19:20,660
where we had a whole lot of talks

346
00:19:20,660 --> 00:19:23,260
like the first half of what I just gave.

347
00:19:23,260 --> 00:19:26,620
This roadmap is designed to set very broad research priorities.

348
00:19:26,620 --> 00:19:29,420
It's not designed to currently pick and choose

349
00:19:29,420 --> 00:19:30,580
what projects are worth funding,

350
00:19:30,580 --> 00:19:34,180
but to set out how we should think about AI as an institution.

351
00:19:35,340 --> 00:19:37,700
And the largest theme from this roadmap

352
00:19:37,700 --> 00:19:40,100
is the need for capacity building,

353
00:19:40,100 --> 00:19:42,460
both in terms of compute capacity,

354
00:19:42,460 --> 00:19:44,380
in light of the supercomputer update

355
00:19:44,380 --> 00:19:45,620
we're likely to have next year

356
00:19:45,620 --> 00:19:48,620
and whatever gets procured in the years thereafter.

357
00:19:50,540 --> 00:19:52,660
How we should think about the use of cloud computing

358
00:19:52,660 --> 00:19:54,500
and finally what we need to do

359
00:19:54,500 --> 00:19:56,860
from a human resources and training standpoint.

360
00:19:56,860 --> 00:19:58,900
This is a living document.

361
00:19:58,900 --> 00:20:00,180
It should be published soon.

362
00:20:00,460 --> 00:20:02,900
I'd hoped I could begin this talk

363
00:20:02,900 --> 00:20:04,140
with a reference to the live document,

364
00:20:04,140 --> 00:20:05,980
but I think it's still in translation.

365
00:20:05,980 --> 00:20:07,940
But once it is published,

366
00:20:07,940 --> 00:20:11,260
it'll be updated every few months to every year or so

367
00:20:11,260 --> 00:20:13,900
with internal reviews and updates

368
00:20:13,900 --> 00:20:15,860
just as we understand more about AI.

369
00:20:16,860 --> 00:20:18,980
The broad themes of this document

370
00:20:18,980 --> 00:20:21,340
are how we intend to integrate AI

371
00:20:21,340 --> 00:20:23,580
throughout the research, development and operation cycle.

372
00:20:23,580 --> 00:20:27,460
So we're not limiting it to just graphcast style forecasts,

373
00:20:27,460 --> 00:20:30,500
but we're interested in applying AI everywhere

374
00:20:30,500 --> 00:20:34,780
from data gathering to post-processing.

375
00:20:35,740 --> 00:20:38,060
The main evaluation criteria here

376
00:20:38,060 --> 00:20:41,260
are the triumvirate of feasibility,

377
00:20:41,260 --> 00:20:42,740
service and efficiency.

378
00:20:42,740 --> 00:20:45,540
Feasibility answers the question of how capable are we

379
00:20:45,540 --> 00:20:50,220
of developing and running a proposed AI project?

380
00:20:50,220 --> 00:20:53,180
And that includes not just the scientific risk

381
00:20:53,180 --> 00:20:56,420
of well, putting a whole lot of time and money

382
00:20:56,420 --> 00:20:58,180
into a project and having it not work,

383
00:20:58,180 --> 00:21:00,980
but also whether or not we have the compute resources

384
00:21:00,980 --> 00:21:05,020
to do this or the human resources to develop and manage it.

385
00:21:06,540 --> 00:21:08,060
The second criterion is service

386
00:21:08,060 --> 00:21:11,460
of how a project will improve ECCC's

387
00:21:11,460 --> 00:21:13,500
weather-based services to Canadians.

388
00:21:13,500 --> 00:21:16,300
And that is not just in terms of accuracy,

389
00:21:16,300 --> 00:21:18,580
but also whether we can make our forecast products

390
00:21:18,580 --> 00:21:22,380
more timely, whether we can have more numerous projects

391
00:21:22,380 --> 00:21:25,300
or whether we can have qualitatively new products

392
00:21:25,340 --> 00:21:29,940
like hypothetically, rapidly updated now casting,

393
00:21:29,940 --> 00:21:31,700
which we just currently don't have available

394
00:21:31,700 --> 00:21:33,300
for many government source.

395
00:21:33,300 --> 00:21:35,580
And finally, there's a question of efficiency.

396
00:21:35,580 --> 00:21:37,380
How will a project make efficient use

397
00:21:37,380 --> 00:21:39,140
of our computational resources?

398
00:21:39,140 --> 00:21:42,100
That includes the broad themes of energy efficiency,

399
00:21:42,100 --> 00:21:45,100
but also the government-specific theme

400
00:21:45,100 --> 00:21:48,300
of being a good steward of public funds.

401
00:21:48,300 --> 00:21:52,580
Supercomputers are, as one might guess, rather expensive.

402
00:21:52,580 --> 00:21:55,700
And we ought to demonstrate to Canadians

403
00:21:55,700 --> 00:21:59,180
that Canadians are getting their money's worth.

404
00:22:00,260 --> 00:22:03,020
Okay, so more specifically,

405
00:22:03,820 --> 00:22:05,100
how are we looking at AI

406
00:22:05,100 --> 00:22:08,060
from the observation to product pipeline?

407
00:22:09,100 --> 00:22:11,660
First category here are observations and data assimilation.

408
00:22:11,660 --> 00:22:14,940
So in observations, AI is in some sense

409
00:22:14,940 --> 00:22:17,020
an extension of what's already happening

410
00:22:17,020 --> 00:22:19,060
in terms of looking at quality control

411
00:22:19,060 --> 00:22:20,980
and error estimation of our inputs.

412
00:22:22,660 --> 00:22:27,660
AI can perhaps allow us to have some new capabilities

413
00:22:29,780 --> 00:22:31,180
with learned observation operators

414
00:22:31,180 --> 00:22:34,220
to take advantage of parameters that are,

415
00:22:36,780 --> 00:22:38,580
to learn parameters that are not directly observed,

416
00:22:38,580 --> 00:22:41,220
such as complicated satellite measurements

417
00:22:41,220 --> 00:22:45,900
that are non-linear features of the atmospheric state

418
00:22:45,900 --> 00:22:47,620
that are hard to directly measure

419
00:22:47,620 --> 00:22:49,580
but might be possible for an AI to learn.

420
00:22:49,580 --> 00:22:53,500
And finally, we have some ideas on unconventional data sources.

421
00:22:53,500 --> 00:22:55,180
One idea thrown around in discussions

422
00:22:55,180 --> 00:23:00,180
was using ad hoc webcam data like traffic cameras

423
00:23:00,220 --> 00:23:04,060
to evaluate real-time precipitation class information.

424
00:23:04,060 --> 00:23:05,780
In principle, someone can look at the feed

425
00:23:05,780 --> 00:23:07,340
and say, yep, it's raining,

426
00:23:07,340 --> 00:23:09,460
and we can have an AI do the same thing

427
00:23:09,460 --> 00:23:11,620
and potentially have useful data.

428
00:23:11,620 --> 00:23:13,300
On the data assimilation side,

429
00:23:14,700 --> 00:23:16,380
in some senses, this area is most advanced

430
00:23:16,380 --> 00:23:21,380
because DA is already using some heavy computation

431
00:23:22,460 --> 00:23:24,740
for its error matrix operations

432
00:23:24,740 --> 00:23:27,020
and they're investigating GPU use.

433
00:23:27,020 --> 00:23:28,020
As previously mentioned,

434
00:23:28,020 --> 00:23:29,340
there's a data of data assimilation

435
00:23:29,340 --> 00:23:30,780
in the latent space of a model.

436
00:23:30,780 --> 00:23:34,180
If we can vastly increase ensemble sizes through AI,

437
00:23:34,180 --> 00:23:37,620
we can perform better PDF estimation

438
00:23:37,620 --> 00:23:42,620
through particle filters or non-Gaussian-based statistics.

439
00:23:43,020 --> 00:23:44,860
And finally, we can start to estimate,

440
00:23:44,860 --> 00:23:46,540
perhaps, the model parameters themselves

441
00:23:46,540 --> 00:23:48,340
rather than just initial conditions.

442
00:23:49,620 --> 00:23:50,700
The numerical prediction side,

443
00:23:50,700 --> 00:23:53,100
this is closest to what I talked about previously

444
00:23:53,100 --> 00:23:54,900
with AI forecast models.

445
00:23:54,900 --> 00:23:57,860
In the near term, we're looking at implementing

446
00:23:57,860 --> 00:23:59,460
the AI models as they are

447
00:23:59,460 --> 00:24:02,780
to provide second opinions about the weather to forecasters.

448
00:24:04,740 --> 00:24:07,580
In the medium term, we're looking at fine-tuning

449
00:24:07,580 --> 00:24:10,020
large models on our operational data sets

450
00:24:10,020 --> 00:24:13,420
to provide better AI forecasts.

451
00:24:13,420 --> 00:24:14,260
And in the longer term,

452
00:24:14,260 --> 00:24:15,740
we're looking at structural changes

453
00:24:15,740 --> 00:24:18,300
and developing new models in general.

454
00:24:18,300 --> 00:24:20,300
In addition, we'd like to hybridize classical

455
00:24:20,300 --> 00:24:24,420
and WP and AI models to help fix the problem of sparse outputs.

456
00:24:25,660 --> 00:24:27,980
And along the way,

457
00:24:27,980 --> 00:24:29,980
we'd also like to investigate emulation

458
00:24:29,980 --> 00:24:31,460
of the physical parameterizations.

459
00:24:31,460 --> 00:24:35,500
For example, radiation is very expensive

460
00:24:35,500 --> 00:24:37,100
inside the atmospheric model

461
00:24:37,100 --> 00:24:40,100
and 3D radiation is probably better than 1D radiation

462
00:24:40,100 --> 00:24:41,820
but it's too expensive to run operationally.

463
00:24:41,820 --> 00:24:43,380
If we can emulate that,

464
00:24:43,380 --> 00:24:45,940
we can have a better parameterization

465
00:24:45,940 --> 00:24:48,700
that is still within our compute budget.

466
00:24:48,700 --> 00:24:50,420
Also, we'd like to extend this

467
00:24:50,420 --> 00:24:53,020
to ocean ice and land surface prediction,

468
00:24:53,020 --> 00:24:56,660
but that is still fairly preliminary

469
00:24:56,660 --> 00:25:00,020
in part because the data sources aren't quite as complete.

470
00:25:00,020 --> 00:25:03,980
Okay, now in terms of the tail end of the forecast

471
00:25:03,980 --> 00:25:08,420
for post-processing and final products,

472
00:25:08,420 --> 00:25:12,580
this is the realm of downscaling and now casting.

473
00:25:14,380 --> 00:25:17,820
We have a 2.5 kilometer high resolution regional system

474
00:25:17,820 --> 00:25:20,180
but it's just too expensive to run too often.

475
00:25:20,180 --> 00:25:21,260
If we can downscale,

476
00:25:21,260 --> 00:25:26,260
we can potentially achieve that resolution of output

477
00:25:26,660 --> 00:25:31,660
and evaluate extremes and risks at that scale

478
00:25:32,500 --> 00:25:37,060
without being limited by melting the supercomputer.

479
00:25:37,060 --> 00:25:38,740
It would also be really nice if we could have

480
00:25:38,740 --> 00:25:41,260
near real-time assimilation of weather station radar data

481
00:25:41,580 --> 00:25:44,580
to improve the half hour, one hour forecast,

482
00:25:44,580 --> 00:25:47,700
which is not something we can currently do very well.

483
00:25:47,700 --> 00:25:49,220
In terms of post-processing,

484
00:25:50,820 --> 00:25:52,700
the statistical post-processing systems

485
00:25:52,700 --> 00:25:55,140
have all long had systematic error adjustments

486
00:25:55,140 --> 00:25:56,660
and station specific adjustments

487
00:25:56,660 --> 00:25:59,580
for representative this errors and the like

488
00:25:59,580 --> 00:26:04,580
and AI can help turn that into better nonlinear corrections.

489
00:26:04,860 --> 00:26:07,260
And finally, in terms of the expert product sides,

490
00:26:07,260 --> 00:26:09,740
we'd like to have better high impact weather diagnostics

491
00:26:09,740 --> 00:26:13,660
with well-calibrated forecasts for all of the big ones,

492
00:26:13,660 --> 00:26:16,940
tornadoes, hail, blizzards that are just don't show up

493
00:26:16,940 --> 00:26:19,380
in the larger scale forecast

494
00:26:19,380 --> 00:26:22,020
but are extremely important for the people stuck in them.

495
00:26:22,020 --> 00:26:24,860
Excuse me, Christopher, so only two minutes left before.

496
00:26:24,860 --> 00:26:27,740
Yes, okay, I am going to be very quick.

497
00:26:27,740 --> 00:26:31,000
Okay, challenges and opportunities.

498
00:26:31,860 --> 00:26:34,700
As I've hinted at the entire time, we have challenges

499
00:26:34,700 --> 00:26:36,180
and these are opportunities.

500
00:26:37,180 --> 00:26:40,460
On the physical side, we have limited compute capacity.

501
00:26:40,460 --> 00:26:42,700
GPU compute demands are only going to go up.

502
00:26:42,700 --> 00:26:44,220
We don't have very many of them.

503
00:26:45,220 --> 00:26:47,700
We're going to probably get more in the future

504
00:26:47,700 --> 00:26:49,220
but we need to manage them.

505
00:26:49,220 --> 00:26:51,580
We also need to care about data management

506
00:26:51,580 --> 00:26:54,740
in terms of not just having an archive that exists on tape

507
00:26:54,740 --> 00:26:57,380
but one that is living and can answer

508
00:26:57,380 --> 00:26:59,820
training-based questions very quickly.

509
00:26:59,820 --> 00:27:02,100
In terms of HR, we have similar problems

510
00:27:02,100 --> 00:27:04,620
that our researchers are all very good

511
00:27:04,780 --> 00:27:07,620
but they're also not necessarily well-trained on AI.

512
00:27:07,620 --> 00:27:10,140
We need to close that gap.

513
00:27:10,140 --> 00:27:14,380
And in particular, we would love to have increased collaboration

514
00:27:14,380 --> 00:27:17,020
with both the ivory tower and private sector.

515
00:27:17,020 --> 00:27:20,500
Okay, the roadmap sets out targets and milestones.

516
00:27:20,500 --> 00:27:23,260
We would like to have our first operational AI systems

517
00:27:23,260 --> 00:27:25,420
for IC innovation cycle five,

518
00:27:25,420 --> 00:27:27,980
which is targeted early 2026

519
00:27:27,980 --> 00:27:30,740
after the supercomputer update.

520
00:27:31,700 --> 00:27:33,540
The current innovation cycle is just closing

521
00:27:33,620 --> 00:27:35,580
because it's a bit too early for it.

522
00:27:35,580 --> 00:27:37,860
And ultimately we'd like to have AI

523
00:27:37,860 --> 00:27:41,380
as just another forecast tool by 2030 or so.

524
00:27:41,380 --> 00:27:44,420
Okay, I would love to talk more about this

525
00:27:44,420 --> 00:27:45,740
but unfortunately there's no time

526
00:27:45,740 --> 00:27:49,700
but in general, our divisions have all been investigating

527
00:27:49,700 --> 00:27:53,580
how we can integrate AI into research flows.

528
00:27:54,500 --> 00:27:55,660
Some projects have started,

529
00:27:55,660 --> 00:27:58,100
some are waiting for people

530
00:27:58,100 --> 00:28:01,540
and some just simply need more resources

531
00:28:01,540 --> 00:28:02,500
that we don't currently have

532
00:28:02,500 --> 00:28:04,860
and we would love to collaborate on them.

533
00:28:04,860 --> 00:28:06,980
If you have AI skill

534
00:28:06,980 --> 00:28:08,580
and you have a weather-related project,

535
00:28:08,580 --> 00:28:12,140
please email someone at our division.

536
00:28:12,140 --> 00:28:13,980
We would probably love to talk to you.

537
00:28:15,340 --> 00:28:16,820
Unfortunately, I have to cancel this slide

538
00:28:16,820 --> 00:28:19,060
where I was going to talk up

539
00:28:19,060 --> 00:28:20,860
all of my colleagues' presentations to come.

540
00:28:20,860 --> 00:28:23,620
Please stick around, they are going to be great.

541
00:28:23,620 --> 00:28:26,060
And finally, conclusions-wise,

542
00:28:26,060 --> 00:28:28,340
AI is rapidly advancing the state of the art

543
00:28:28,340 --> 00:28:30,740
in numerical weather computation.

544
00:28:30,740 --> 00:28:35,540
This is a phase change of forecasting

545
00:28:35,540 --> 00:28:37,460
and I'm excited to see where it goes.

546
00:28:37,460 --> 00:28:39,740
We will make AI and machine learning technologies

547
00:28:39,740 --> 00:28:40,980
a major part of our systems

548
00:28:40,980 --> 00:28:44,380
as they prove themselves.

549
00:28:45,620 --> 00:28:47,460
But we're a public service organization,

550
00:28:47,460 --> 00:28:49,900
we recognize that we have to be very careful

551
00:28:49,900 --> 00:28:52,220
about what we stand behind operationally.

552
00:28:54,100 --> 00:28:56,500
And finally, I hope these slides are available

553
00:28:56,500 --> 00:28:57,660
afterwards for the references.

554
00:28:57,660 --> 00:29:00,660
There are references to all of the systems I've mentioned.

555
00:29:00,660 --> 00:29:02,100
And I'd also just like to highlight

556
00:29:02,100 --> 00:29:03,540
that of the 14 references here,

557
00:29:03,540 --> 00:29:05,020
about eight are preprints.

558
00:29:05,020 --> 00:29:08,140
This is a really, really rapidly moving field.

559
00:29:09,780 --> 00:29:11,380
Thank you, I am...

560
00:29:11,380 --> 00:29:13,340
Missy Christopher.

561
00:29:13,340 --> 00:29:16,780
We have time maybe for one question.

562
00:29:16,780 --> 00:29:17,780
I know.

563
00:29:19,300 --> 00:29:23,020
I'm going to take the highest ranked one.

564
00:29:23,020 --> 00:29:24,420
It's a long one, are you ready?

565
00:29:24,420 --> 00:29:26,300
Yes, yes.

566
00:29:26,300 --> 00:29:27,180
Perfect.

567
00:29:27,180 --> 00:29:29,940
Currently, AI seems to be quite expensive

568
00:29:29,940 --> 00:29:31,700
and rapidly developing.

569
00:29:31,700 --> 00:29:34,460
While we were waiting for AI-based models

570
00:29:34,460 --> 00:29:36,580
to build better foundations and training

571
00:29:36,580 --> 00:29:39,060
to replace numerical weather prediction

572
00:29:39,060 --> 00:29:40,740
or for climate studies,

573
00:29:40,740 --> 00:29:43,780
can we exploit its computational speed

574
00:29:43,780 --> 00:29:48,780
in the near term for now casting or HRR-like output?

575
00:29:49,460 --> 00:29:52,900
For example, ECCC can incorporate into CAM

576
00:29:52,900 --> 00:29:55,220
for very short thunderstorm prediction

577
00:29:55,220 --> 00:29:59,540
or perhaps weather elements on grid now cast.

578
00:30:00,780 --> 00:30:04,820
Okay, I believe these projects are under investigation.

579
00:30:04,820 --> 00:30:06,900
I'm on the medium range forecast side,

580
00:30:06,900 --> 00:30:11,740
so it's not my particular side,

581
00:30:11,740 --> 00:30:15,580
but there's a presentation in this session,

582
00:30:15,580 --> 00:30:19,700
I believe after lunch that investigates now casting

583
00:30:19,700 --> 00:30:21,220
via an IBM Foundation model.

584
00:30:21,220 --> 00:30:24,340
And I think that'll begin to answer that kind of question.

585
00:30:24,340 --> 00:30:27,140
In general, yes, this would be very good.

586
00:30:27,220 --> 00:30:29,860
In practice, the nearest term limits

587
00:30:29,860 --> 00:30:31,340
are probably compute potential

588
00:30:31,340 --> 00:30:33,140
because we have relatively few

589
00:30:33,140 --> 00:30:35,700
operationally available GPUs,

590
00:30:35,700 --> 00:30:38,100
but hopefully in the next few months to year or so,

591
00:30:38,100 --> 00:30:39,020
that will improve.

592
00:30:40,780 --> 00:30:42,820
I might sneak in another question then,

593
00:30:42,820 --> 00:30:44,540
Christopher, we have a minute.

594
00:30:44,540 --> 00:30:47,300
Have you considered the role that Canadian industry

595
00:30:47,300 --> 00:30:51,260
will have in developing AI capacity at ECCC?

596
00:30:52,060 --> 00:30:54,620
We would love collaborations from industry.

597
00:30:54,620 --> 00:30:59,620
That is my politically correct and also true answer.

598
00:31:01,380 --> 00:31:05,860
We have limits on our resources in part

599
00:31:05,860 --> 00:31:09,500
because until, well, procurement thus far

600
00:31:09,500 --> 00:31:13,380
has been focused on making our operational systems better

601
00:31:13,380 --> 00:31:15,380
for obvious reasons.

602
00:31:15,380 --> 00:31:20,380
And the cycles of this mean that it is practically difficult

603
00:31:21,220 --> 00:31:25,380
to, sorry, I'm speaking as a researcher,

604
00:31:25,380 --> 00:31:26,700
my boss is probably listening,

605
00:31:26,700 --> 00:31:28,900
so there's some things I need to be very circumspect

606
00:31:28,900 --> 00:31:29,740
about saying.

607
00:31:31,060 --> 00:31:33,700
We need to be very careful about how we commit resources

608
00:31:33,700 --> 00:31:35,420
for training large models.

609
00:31:35,420 --> 00:31:38,460
Industry is, I think, a fantastic partner,

610
00:31:38,460 --> 00:31:42,300
both for the potential of having compute resources

611
00:31:42,300 --> 00:31:44,500
we could borrow and also a better focus

612
00:31:44,500 --> 00:31:48,020
on some of the most downstream applications.

613
00:31:48,060 --> 00:31:52,820
For example, our leading talk that opened CMOS

614
00:31:52,820 --> 00:31:56,220
was on the weather impacts on the insurance industry.

615
00:31:56,220 --> 00:31:59,580
And to the extent we can be of value there,

616
00:31:59,580 --> 00:32:02,340
I think there's room for joint products.

617
00:32:06,500 --> 00:32:10,380
Okay, so I will try to answer other questions

618
00:32:10,380 --> 00:32:12,500
in the chat as we continue.

619
00:32:12,500 --> 00:32:14,700
But otherwise, I will stop sharing, meet myself

620
00:32:14,700 --> 00:32:19,180
and thank our hosts.

621
00:32:19,180 --> 00:32:20,260
Well, thank you, Christopher.

622
00:32:20,260 --> 00:32:21,620
That was an amazing feat.

623
00:32:21,620 --> 00:32:24,940
You put in two very large presentations

624
00:32:24,940 --> 00:32:27,260
into one 30-minute presentation.

625
00:32:27,260 --> 00:32:29,860
You have all my admiration and thanks for doing that.

626
00:32:30,860 --> 00:32:32,740
Congratulations, actually, in 30 minutes.

627
00:32:32,740 --> 00:32:34,940
Okay, and now we're on the subject of airvests.

628
00:32:34,940 --> 00:32:37,020
So we're not going to take any longer.

629
00:32:37,020 --> 00:32:38,620
I'm going to introduce you to Airvests GLaPalm,

630
00:32:38,620 --> 00:32:41,180
who comes from Canada as well, a researcher.

631
00:32:41,180 --> 00:32:45,700
He's going to introduce us to the NWPEI-based model,

632
00:32:45,700 --> 00:32:47,940
the verification against the observations

633
00:32:47,940 --> 00:32:50,980
of airvests and airvests.

634
00:32:50,980 --> 00:32:52,660
It's up to you.

635
00:32:52,660 --> 00:32:54,180
Hello.

636
00:32:54,180 --> 00:32:55,140
I don't know.

637
00:32:55,140 --> 00:32:57,660
So you hear me.

638
00:32:57,660 --> 00:32:58,580
Very well.

639
00:32:58,580 --> 00:33:00,820
Do you see my screen?

640
00:33:00,820 --> 00:33:01,700
Also.

641
00:33:01,700 --> 00:33:02,420
Wonderful.

642
00:33:02,420 --> 00:33:04,900
So I'm going to do the presentation in French.

643
00:33:04,900 --> 00:33:07,060
If you have any questions in English, there's no problem.

644
00:33:07,060 --> 00:33:09,900
If you have any questions in English, there's no problem.

645
00:33:10,140 --> 00:33:13,580
It would be easier for me if I do it in French and for you also.

646
00:33:13,580 --> 00:33:19,460
So I'm here to present the work that I did in collaboration

647
00:33:19,460 --> 00:33:25,140
with my colleagues on the evaluation of the models based

648
00:33:25,140 --> 00:33:29,380
on the airCCC in a second.

649
00:33:29,380 --> 00:33:30,340
I don't know.

650
00:33:30,340 --> 00:33:35,500
So the context is that with the emergence of these models,

651
00:33:35,500 --> 00:33:45,780
we realized that we had to check these models

652
00:33:45,780 --> 00:33:49,340
with our traditional verification methods,

653
00:33:49,340 --> 00:33:52,340
which allow us to evaluate the innovations

654
00:33:52,340 --> 00:33:54,740
that are made on traditional models.

655
00:33:54,740 --> 00:34:01,780
So I was asked by my boss to install, turn and check

656
00:34:01,780 --> 00:34:07,340
these models on our HPC installations.

657
00:34:07,340 --> 00:34:11,660
And I started this work in October 2023.

658
00:34:11,660 --> 00:34:15,020
And I worked on it until April 2024.

659
00:34:15,020 --> 00:34:18,020
So what I want to present to you is just that.

660
00:34:18,020 --> 00:34:22,020
So the activities that were completed during this special project

661
00:34:22,020 --> 00:34:26,660
there, it's that we turned, we chose two models,

662
00:34:26,660 --> 00:34:30,100
ForecastNet and Graphcast, which were available for free.

663
00:34:30,100 --> 00:34:31,380
It's easy.

664
00:34:31,380 --> 00:34:35,340
And, well, ForecastNet, Christopher,

665
00:34:35,340 --> 00:34:36,940
we talked about it a little bit earlier.

666
00:34:36,940 --> 00:34:39,220
So it's a model that was developed by Renvidia.

667
00:34:39,220 --> 00:34:42,660
And then we turned two graphs of Graphcast,

668
00:34:42,660 --> 00:34:47,860
one with 13 levels of pressure, with a roof of 50 hectopascals

669
00:34:47,860 --> 00:34:52,420
and a version at 37 levels, a roof of 1 hectopascal.

670
00:34:52,420 --> 00:34:56,220
And we turned each model with three analysis sets.

671
00:34:56,260 --> 00:35:02,620
One, the first is the operational analysis of the OVF,

672
00:35:02,620 --> 00:35:06,420
called IFS, on three levels only.

673
00:35:06,420 --> 00:35:09,940
We also used R5.

674
00:35:09,940 --> 00:35:13,860
So the R5 analyses are the ones that were used

675
00:35:13,860 --> 00:35:17,420
to train these models.

676
00:35:17,420 --> 00:35:21,100
We were able to turn the configurations at 13 and 37 levels.

677
00:35:21,100 --> 00:35:26,140
And of course, we wanted to compare the operational provisions

678
00:35:26,180 --> 00:35:27,300
with the same analysis.

679
00:35:27,300 --> 00:35:30,620
So we started these models, these AI models,

680
00:35:30,620 --> 00:35:35,060
with the operational analysis of CCC.

681
00:35:35,060 --> 00:35:41,260
And we turned in two real-time modes,

682
00:35:41,260 --> 00:35:45,380
where we turned twice a day,

683
00:35:45,380 --> 00:35:47,900
at the same time as the operational model.

684
00:35:47,900 --> 00:35:49,940
And like that, the operational metrologists

685
00:35:49,940 --> 00:35:53,020
can compare the Forecasts based on AI

686
00:35:53,020 --> 00:35:55,140
with the operational provisions.

687
00:35:55,140 --> 00:35:59,940
And also, on my side, I did an evaluation

688
00:35:59,940 --> 00:36:04,500
on a period of one year, which allows us to see

689
00:36:04,500 --> 00:36:07,740
what the models are like.

690
00:36:07,740 --> 00:36:11,420
So here, I put a slide on the description of the models.

691
00:36:11,420 --> 00:36:12,420
It's very precise.

692
00:36:12,420 --> 00:36:13,780
I don't have much time.

693
00:36:13,780 --> 00:36:17,620
Basically, the two Graphcast and Forecast Net models

694
00:36:17,620 --> 00:36:21,860
use about the same information.

695
00:36:21,860 --> 00:36:25,140
But I put a lot of detail there to be complete.

696
00:36:25,140 --> 00:36:29,940
But I don't think I'll be able to save a little time.

697
00:36:29,940 --> 00:36:35,940
So one of the advantages of AI models

698
00:36:35,940 --> 00:36:39,180
is their informatic efficiency.

699
00:36:39,180 --> 00:36:41,860
If we compare the operational model,

700
00:36:41,860 --> 00:36:44,700
presently, it takes a little less than an hour,

701
00:36:44,700 --> 00:36:46,620
more than 6,000 CPUs.

702
00:36:46,620 --> 00:36:50,460
So it's a big deal.

703
00:36:50,460 --> 00:36:53,540
So it generates 500 gigabytes of data

704
00:36:53,540 --> 00:36:55,940
at each provision twice a day.

705
00:36:55,940 --> 00:36:59,740
It makes outings at all ages up to 10 days.

706
00:36:59,740 --> 00:37:02,460
And then it's models at 15 kilometers of resolution

707
00:37:02,460 --> 00:37:04,620
with a lot of vertical resolutions.

708
00:37:04,620 --> 00:37:07,540
AI models have less good resolutions.

709
00:37:07,540 --> 00:37:09,820
They release data at 6 hours.

710
00:37:09,820 --> 00:37:12,420
And a less good vertical resolution too.

711
00:37:12,420 --> 00:37:15,380
So, but Forecast Net is very, very light.

712
00:37:15,380 --> 00:37:16,700
It's impressive.

713
00:37:16,700 --> 00:37:18,820
It takes 20 minutes on a CPU.

714
00:37:18,820 --> 00:37:20,020
I don't speak of a GPU here.

715
00:37:20,020 --> 00:37:22,220
I have a GPU even faster, of course.

716
00:37:22,220 --> 00:37:24,100
But on a CPU, you can turn it on.

717
00:37:24,100 --> 00:37:26,540
You can turn it on on your laptop and it works.

718
00:37:26,540 --> 00:37:29,620
And it's very fast.

719
00:37:29,620 --> 00:37:31,140
It still gives you a good preview.

720
00:37:31,140 --> 00:37:34,700
And Graphcast, it's a little bit more expensive.

721
00:37:34,700 --> 00:37:36,620
The confidence at 13 levels

722
00:37:36,620 --> 00:37:40,540
requires 100 gigabytes of memory.

723
00:37:40,540 --> 00:37:43,060
So it's a little bit more expensive.

724
00:37:43,060 --> 00:37:45,540
But still, comparing the operational model,

725
00:37:45,540 --> 00:37:49,180
it's very, very much, much smaller.

726
00:37:49,180 --> 00:37:50,660
Smaller orders.

727
00:37:50,660 --> 00:37:53,340
And I invite you to the second presentation

728
00:37:53,340 --> 00:37:56,220
of Christopher Subick on exactly comparing

729
00:37:56,220 --> 00:37:59,100
the computer performance between AI models,

730
00:37:59,100 --> 00:38:02,420
Graphcast and GEM, the operational model.

731
00:38:02,420 --> 00:38:04,340
And it makes a very good comparison.

732
00:38:04,340 --> 00:38:08,220
If you're interested, I invite you to have

733
00:38:08,220 --> 00:38:11,380
this presentation on your computer this afternoon.

734
00:38:11,380 --> 00:38:13,500
So what does it look like as a verification

735
00:38:13,500 --> 00:38:16,580
once we've done the average over a year?

736
00:38:16,580 --> 00:38:21,060
So here, I have several curves.

737
00:38:21,060 --> 00:38:24,660
Here, I present the errors, the prediction

738
00:38:24,660 --> 00:38:29,780
of the geopotential at 55 to Pascal.

739
00:38:29,780 --> 00:38:34,900
So these are the errors.

740
00:38:34,900 --> 00:38:42,100
The very thick blue gray here, that's the baseline.

741
00:38:42,100 --> 00:38:45,500
So that's the operational preview.

742
00:38:45,500 --> 00:38:51,260
So we see the errors that are missing from 0 to 240.

743
00:38:51,260 --> 00:38:54,100
And the other curves, it's all the previews,

744
00:38:54,100 --> 00:38:56,660
the verification, the AI models.

745
00:38:56,660 --> 00:38:59,820
So we can look at them and then, as these are errors,

746
00:38:59,820 --> 00:39:02,220
but the closer we are to 0, the better it is.

747
00:39:02,220 --> 00:39:07,220
So we see here a group of previews.

748
00:39:07,220 --> 00:39:08,380
That's the forecast net.

749
00:39:08,380 --> 00:39:10,220
So we see that for the GZ500,

750
00:39:10,220 --> 00:39:15,100
the forecast net is less good than the operational preview.

751
00:39:15,100 --> 00:39:17,900
On the other hand, all the other curves, the five other curves,

752
00:39:17,900 --> 00:39:19,660
it's all the previews made by Gravcast

753
00:39:19,660 --> 00:39:21,660
using different analyses.

754
00:39:21,660 --> 00:39:24,700
So we see that Gravcast, from day five,

755
00:39:24,700 --> 00:39:26,620
is better than the operational model,

756
00:39:26,620 --> 00:39:29,340
no matter the analysis we give him.

757
00:39:29,340 --> 00:39:35,500
So that's when using the 05 and FS analyses, it's better.

758
00:39:35,500 --> 00:39:37,900
I don't know, but what's important here,

759
00:39:37,980 --> 00:39:40,900
what's interesting here is that the two curves here,

760
00:39:40,900 --> 00:39:43,300
orange and green,

761
00:39:43,300 --> 00:39:47,420
the previews are initialized with the same analysis

762
00:39:47,420 --> 00:39:49,060
as the blue curve.

763
00:39:49,060 --> 00:39:52,540
So we see that with equal information,

764
00:39:52,540 --> 00:39:57,140
Gravcast is better from day five on the GZ500.

765
00:39:57,140 --> 00:39:58,940
If we look at another variable,

766
00:39:58,940 --> 00:40:01,820
which is the temperature at 850 tectopascals,

767
00:40:01,820 --> 00:40:04,100
so it's the same colors, the same curves,

768
00:40:04,180 --> 00:40:09,180
in this case, we see that from 72 hours,

769
00:40:09,180 --> 00:40:12,380
all the AI models are the operational model,

770
00:40:12,380 --> 00:40:13,500
even for the GZ500,

771
00:40:13,500 --> 00:40:19,180
but we still see that Gravcast is the best model,

772
00:40:19,180 --> 00:40:20,300
this variable.

773
00:40:20,300 --> 00:40:23,220
So I showed you two variables.

774
00:40:23,220 --> 00:40:26,060
So we can conclude that Gravcast is better than for the GZ500,

775
00:40:26,060 --> 00:40:29,540
we will focus on that from now on.

776
00:40:29,540 --> 00:40:31,660
And then what does it look like for other variables,

777
00:40:31,740 --> 00:40:34,180
like wind, humidity.

778
00:40:34,180 --> 00:40:36,780
So here, I present you graphics,

779
00:40:36,780 --> 00:40:39,220
it's vertical profiles.

780
00:40:41,500 --> 00:40:46,260
Here, in the Y axis of the vertical coordinate curve,

781
00:40:46,260 --> 00:40:47,740
we talk about the surface,

782
00:40:47,740 --> 00:40:51,140
from 1,000 tectopascals to 50 tectopascals.

783
00:40:51,140 --> 00:40:58,500
And so the clean curve,

784
00:40:58,500 --> 00:41:01,900
it's the error's quarter, according to the variable.

785
00:41:01,900 --> 00:41:06,700
And the tight curve is the bias.

786
00:41:06,700 --> 00:41:08,860
And the different variables are the following.

787
00:41:08,860 --> 00:41:12,180
Here, on the top left, we have the meridian wind,

788
00:41:12,180 --> 00:41:16,620
the wind component, the wind module here on the right.

789
00:41:16,620 --> 00:41:24,740
Here, can you do this for us in the next two minutes?

790
00:41:24,740 --> 00:41:26,740
Yes, that's it.

791
00:41:26,740 --> 00:41:28,020
Yes, I'll go faster.

792
00:41:28,100 --> 00:41:30,180
So what I wanted to show you,

793
00:41:30,180 --> 00:41:34,340
here we have the Gravcast configuration,

794
00:41:34,340 --> 00:41:37,940
so at 13 levels, 37 levels.

795
00:41:37,940 --> 00:41:43,580
So what I wanted to show you is that the red curve

796
00:41:43,580 --> 00:41:46,060
is always better than the blue curve,

797
00:41:46,060 --> 00:41:48,300
so the model has all the variables,

798
00:41:48,300 --> 00:41:50,940
all the failures, not all the failures,

799
00:41:50,940 --> 00:41:52,820
but all the variables, all the levels.

800
00:41:52,820 --> 00:41:54,980
We see that Gravcast is better.

801
00:41:54,980 --> 00:42:01,500
So are these two models not different resolutions?

802
00:42:01,500 --> 00:42:04,900
Is the fact that Gravcast is a bigger resolution,

803
00:42:04,900 --> 00:42:07,660
is it the advantage?

804
00:42:07,660 --> 00:42:10,460
Is it the advantage compared to the operational model?

805
00:42:10,460 --> 00:42:14,740
So what we did is that we did the same verification

806
00:42:14,740 --> 00:42:17,860
that I presented to you earlier, but we filtered it.

807
00:42:17,860 --> 00:42:19,860
We filtered, we removed all the scales

808
00:42:19,860 --> 00:42:22,060
that were smaller than 1,000 km,

809
00:42:22,060 --> 00:42:24,700
and we kept all those that were 2,000 km.

810
00:42:24,740 --> 00:42:27,500
So I have two graphics to present here.

811
00:42:27,500 --> 00:42:30,220
So on the left, it's the same graphic

812
00:42:30,220 --> 00:42:32,860
that I presented to you earlier, not filtered,

813
00:42:32,860 --> 00:42:35,500
and on the right, it's the filtered previews.

814
00:42:35,500 --> 00:42:37,660
So that's on the average, on the winter,

815
00:42:37,660 --> 00:42:39,860
it's not on the full year.

816
00:42:39,860 --> 00:42:43,500
So we see that even if filtered, Gravcast is better.

817
00:42:43,500 --> 00:42:45,980
So that's going to bring to the work of Spectreur Nodging

818
00:42:45,980 --> 00:42:49,380
of Syed, who will present in the next presentation.

819
00:42:49,380 --> 00:42:56,340
So and for the summer, it's a little less spectacular,

820
00:42:56,340 --> 00:43:05,420
but we still see that Gravcast has a lot of good information.

821
00:43:05,420 --> 00:43:10,260
So what are the future activities to do more verification?

822
00:43:10,260 --> 00:43:14,340
We even have an internal site that allows us to visualize

823
00:43:14,340 --> 00:43:16,740
these previews day by day.

824
00:43:16,820 --> 00:43:21,220
And I invite you to have the next seminar

825
00:43:21,220 --> 00:43:23,260
of Syed about Spectreur Nodging,

826
00:43:23,260 --> 00:43:26,580
which is a very interesting approach to integrate

827
00:43:26,580 --> 00:43:28,820
physical models and AI models.

828
00:43:28,820 --> 00:43:37,140
And Christopher Subick's work on entering Gravcast

829
00:43:37,140 --> 00:43:42,500
with the operational analysis that we have done internally

830
00:43:42,500 --> 00:43:46,700
so that it can be better adapted to our model.

831
00:43:47,980 --> 00:43:49,460
Thank you.

832
00:43:49,460 --> 00:43:52,220
Hi, thank you, Hervé.

833
00:43:52,220 --> 00:43:54,060
Hi, I have a question for you.

834
00:43:54,060 --> 00:43:55,900
There, you made the internal models run,

835
00:43:55,900 --> 00:43:57,820
you installed them to do the verification,

836
00:43:57,820 --> 00:43:59,460
but there are more and more models,

837
00:43:59,460 --> 00:44:02,060
there are almost two days,

838
00:44:02,060 --> 00:44:04,780
is there perhaps a way to have

839
00:44:04,780 --> 00:44:07,060
verification, counter-observation,

840
00:44:07,060 --> 00:44:08,340
like that, in a standardized way,

841
00:44:08,340 --> 00:44:10,740
or each time, you will have to install the models

842
00:44:11,140 --> 00:44:13,180
and then the scoring themselves?

843
00:44:13,180 --> 00:44:18,180
Well, listen, that's what the software that we use

844
00:44:18,180 --> 00:44:20,620
to do the verification and counter-observation

845
00:44:20,620 --> 00:44:24,060
works very well only locally.

846
00:44:24,060 --> 00:44:28,260
So it's difficult to publish that externally.

847
00:44:28,260 --> 00:44:31,380
On the other hand, running these models,

848
00:44:31,380 --> 00:44:33,660
if it's not done very, very easily,

849
00:44:33,660 --> 00:44:35,780
I worked for three months in the middle of the day

850
00:44:35,780 --> 00:44:37,900
just to start these models.

851
00:44:38,220 --> 00:44:42,420
So I imagine that each model has its own peculiarities.

852
00:44:42,420 --> 00:44:45,900
So it's not obvious, it's not as plug-and-play

853
00:44:45,900 --> 00:44:47,020
that we believe in.

854
00:44:47,020 --> 00:44:49,700
There's still a lot of work to be done.

855
00:44:49,700 --> 00:44:54,460
So if there are all the two, all the two weeks,

856
00:44:54,460 --> 00:44:58,500
it will be difficult to do this same evaluation

857
00:44:58,500 --> 00:45:00,660
to follow the run.

858
00:45:02,260 --> 00:45:04,940
Thank you, I don't know if there are other questions.

859
00:45:04,940 --> 00:45:07,180
Maybe a word for advanced systems.

860
00:45:07,180 --> 00:45:09,900
If you could reset the Q&A on EventMobi

861
00:45:09,900 --> 00:45:13,020
because I still see the questions

862
00:45:13,020 --> 00:45:16,060
that have been asked to Christopher Subic.

863
00:45:16,060 --> 00:45:18,940
So it's hard to see which one.

864
00:45:18,940 --> 00:45:20,700
I have one for you, Hervé.

865
00:45:20,700 --> 00:45:22,060
How do artificial intelligence models

866
00:45:22,060 --> 00:45:24,780
behave in complex mountainous terrain?

867
00:45:26,660 --> 00:45:29,540
I didn't evaluate that.

868
00:45:29,540 --> 00:45:31,420
That's more what my colleague, Marc Verville,

869
00:45:31,420 --> 00:45:33,500
did for verification on the surface.

870
00:45:34,500 --> 00:45:43,700
I don't have any memory of the results.

871
00:45:43,700 --> 00:45:48,100
My focus was really on the verification in addition.

872
00:45:48,100 --> 00:45:50,900
But of course, having a good resolution

873
00:45:50,900 --> 00:45:54,900
will certainly not be a problem.

874
00:45:54,900 --> 00:45:56,220
Perfect.

875
00:45:56,220 --> 00:45:58,620
Thank you very much, Hervé.

876
00:45:58,620 --> 00:46:00,140
Thank you.

877
00:46:00,140 --> 00:46:01,740
Thank you, Hervé.

878
00:46:02,020 --> 00:46:05,900
We, so we're going to continue.

879
00:46:05,900 --> 00:46:09,500
This time we're inviting Syed Zahid Hussein,

880
00:46:09,500 --> 00:46:14,020
who's a research scientist at ECCC.

881
00:46:14,020 --> 00:46:19,460
He will be presenting leveraging data-driven weather

882
00:46:19,460 --> 00:46:22,700
emulators to guide physics-based numerical weather

883
00:46:22,700 --> 00:46:27,940
prediction models, a fusion of forecasting paradigms.

884
00:46:27,940 --> 00:46:34,780
So without further ado, Syed, this is your turn.

885
00:46:34,780 --> 00:46:36,420
Thank you, Rand.

886
00:46:36,420 --> 00:46:37,500
Hello, everyone.

887
00:46:37,500 --> 00:46:41,220
In my presentation today, I will be talking

888
00:46:41,220 --> 00:46:47,140
about how we can leverage the strengths of data-driven weather

889
00:46:47,140 --> 00:46:50,860
models to improve predictions from NWP models.

890
00:46:50,860 --> 00:46:53,380
This is a work that we have been doing recently.

891
00:46:53,380 --> 00:46:56,900
And here is a list of my principal collaborators

892
00:46:56,900 --> 00:46:59,540
and the others who have contributed to this research.

893
00:47:03,660 --> 00:47:07,620
As we know, the current state of the earth

894
00:47:07,620 --> 00:47:09,540
for operational weather forecasting

895
00:47:09,540 --> 00:47:13,660
is based on physics-based NWP models.

896
00:47:13,660 --> 00:47:17,540
However, we have seen these presentations earlier today

897
00:47:17,540 --> 00:47:20,380
from the plenary to the previous two presentations

898
00:47:20,380 --> 00:47:24,500
that we have recently seen the emergence of new data-driven

899
00:47:24,500 --> 00:47:27,740
models for predicting weather.

900
00:47:27,740 --> 00:47:30,580
And most of these models are using some form of deep neural

901
00:47:30,580 --> 00:47:32,900
network to emulate the training data.

902
00:47:32,900 --> 00:47:37,660
And the training data generally is RFI re-analysis.

903
00:47:37,660 --> 00:47:41,900
So we also can call them artificial intelligence-based

904
00:47:41,900 --> 00:47:43,300
weather emulators.

905
00:47:43,300 --> 00:47:46,300
And recently, they have started to gain prominence

906
00:47:46,300 --> 00:47:49,540
and started to also challenge the existing forecasting

907
00:47:49,540 --> 00:47:50,500
paradigm.

908
00:47:50,500 --> 00:47:53,180
Because as you have seen in the previous presentation

909
00:47:53,180 --> 00:47:57,500
from my colleague, Erveg, that these data-driven models

910
00:47:57,500 --> 00:48:00,460
can produce forecast orders of magnitude

911
00:48:00,460 --> 00:48:03,940
faster with minimal computational resources

912
00:48:03,940 --> 00:48:06,860
compared to the traditional NWP models.

913
00:48:06,860 --> 00:48:10,860
And also, they can be highly competitive against state

914
00:48:10,860 --> 00:48:15,100
of the art NWP models in terms of their accuracy.

915
00:48:15,100 --> 00:48:18,540
However, despite their strengths,

916
00:48:18,540 --> 00:48:21,220
strictly when I'm talking in the deterministic sense

917
00:48:21,220 --> 00:48:25,260
for these AI-based models, they have their limitations also.

918
00:48:25,260 --> 00:48:28,340
And one of the most widely known limitation

919
00:48:28,340 --> 00:48:31,340
is considerable smoothing of fine scales,

920
00:48:31,340 --> 00:48:33,180
particularly for longer lead times.

921
00:48:33,180 --> 00:48:37,220
Also, they only offer a limited range of forecast fields

922
00:48:37,220 --> 00:48:40,500
and improving nominal resolution of these AI models

923
00:48:40,500 --> 00:48:41,340
not straightforward.

924
00:48:41,340 --> 00:48:43,060
They can be quite challenging.

925
00:48:43,060 --> 00:48:45,020
So the objective of our research was

926
00:48:45,020 --> 00:48:48,820
to see if we can leverage the strengths of these AI-based

927
00:48:48,820 --> 00:48:54,020
models to improve the predictability of an NWP model.

928
00:48:54,020 --> 00:48:58,780
And for that, we chose or selected

929
00:48:58,780 --> 00:49:01,660
like the GEM model, which is used operationally

930
00:49:01,660 --> 00:49:06,420
as the NWP model, operationally by Environment Canada,

931
00:49:06,420 --> 00:49:10,580
and the GraphCast model from Google DeepMind as the AI-based

932
00:49:10,580 --> 00:49:11,380
model.

933
00:49:11,380 --> 00:49:14,900
And the nominal grid resolutions of GraphCast

934
00:49:14,900 --> 00:49:18,220
and the GEM-based Global Deterministic Prediction System,

935
00:49:18,220 --> 00:49:22,620
or GDPS, are approximately 25 kilometers and 15 kilometers,

936
00:49:22,620 --> 00:49:24,020
respectively.

937
00:49:24,020 --> 00:49:26,140
And in the previous presentation,

938
00:49:26,140 --> 00:49:31,100
AirVig has shown that the GraphCast actually poses

939
00:49:31,100 --> 00:49:35,660
more skilled large scales compared to our GDPS.

940
00:49:35,660 --> 00:49:38,980
But if we want to leverage the information from GraphCast,

941
00:49:38,980 --> 00:49:41,580
we need to know about the effective resolution of GraphCast

942
00:49:41,580 --> 00:49:44,860
so that we can see what are the scales that we can really

943
00:49:44,860 --> 00:49:48,700
utilize for improving NWP model.

944
00:49:48,700 --> 00:49:53,740
And in order to do that, we look at the variance ratio of GDPS

945
00:49:53,740 --> 00:49:58,500
and GraphCast with respect to our own CMC analysis.

946
00:49:58,500 --> 00:50:01,300
And before I talk about anything else,

947
00:50:01,300 --> 00:50:04,900
I must emphasize on the fact that the version of GraphCast

948
00:50:04,900 --> 00:50:10,020
that we are using has not been through any fine tuning.

949
00:50:10,020 --> 00:50:16,100
So it has been trained by Google on emulating error-5 analysis

950
00:50:16,100 --> 00:50:18,340
by training with error-5 data.

951
00:50:18,340 --> 00:50:22,140
And we are using that GraphCast model

952
00:50:22,140 --> 00:50:26,420
but initializing it with our own analysis.

953
00:50:26,420 --> 00:50:29,020
And in these figures in this slide,

954
00:50:29,020 --> 00:50:33,220
I am showing the transient component of variance ratio

955
00:50:33,220 --> 00:50:35,660
for 500 hectopascal kinetic energy.

956
00:50:35,660 --> 00:50:39,180
On the left, I have for lead time 24 hours.

957
00:50:39,180 --> 00:50:42,220
And on the right, I have for 120 hours.

958
00:50:42,220 --> 00:50:44,380
In blue, I have GraphCast.

959
00:50:44,380 --> 00:50:48,460
And in blue, I have GDPS.

960
00:50:48,460 --> 00:50:50,460
And in red, I have GraphCast.

961
00:50:50,460 --> 00:50:54,060
And we can see by looking at the variance ratio of GDPS,

962
00:50:54,060 --> 00:50:59,660
the blue lines in both 24-hour and 120-hour cases,

963
00:50:59,660 --> 00:51:02,100
that the variance ratio is close to 1.

964
00:51:02,100 --> 00:51:05,380
So that means its effective resolution

965
00:51:05,380 --> 00:51:08,860
is not changing with respect to lead times.

966
00:51:08,860 --> 00:51:11,060
However, what we see with GraphCast,

967
00:51:11,060 --> 00:51:13,900
first of all, at 24-hour lead time,

968
00:51:13,900 --> 00:51:16,900
we see the scales as large as 1,500 kilometers

969
00:51:16,900 --> 00:51:20,020
are smoothed out to some extent.

970
00:51:20,020 --> 00:51:22,620
And we see considerable smoothing for scales smaller

971
00:51:22,620 --> 00:51:23,540
than that.

972
00:51:23,540 --> 00:51:26,860
And then we see when we go to 120-hour lead time,

973
00:51:26,860 --> 00:51:31,700
the scales that are getting smoothed out actually increases.

974
00:51:31,700 --> 00:51:35,300
And it affects scales as large as 2,750.

975
00:51:35,300 --> 00:51:38,140
So we know that large scales in GraphCast are better.

976
00:51:38,140 --> 00:51:41,780
But at the same time, we see that scales below 2,750

977
00:51:41,780 --> 00:51:45,380
for longer lead times are problematic because of the reduced

978
00:51:45,380 --> 00:51:47,820
variance ratio.

979
00:51:47,820 --> 00:51:52,500
Now, the question is how we can really use or leverage

980
00:51:52,500 --> 00:51:56,260
this good large-scale information from GraphCast.

981
00:51:56,260 --> 00:52:00,500
And one way to do that would be to use spectral nudging,

982
00:52:00,500 --> 00:52:02,180
large-scale spectral nudging.

983
00:52:02,180 --> 00:52:05,740
And this is a very widely used idea

984
00:52:05,740 --> 00:52:09,260
in the field of regional climate modeling and hindcasting.

985
00:52:09,260 --> 00:52:12,700
Our expectation is if we nudge our gem predictions

986
00:52:12,700 --> 00:52:14,900
towards the large scales of GraphCast,

987
00:52:14,900 --> 00:52:17,740
we can improve the quality of prediction of gem.

988
00:52:17,740 --> 00:52:21,220
At the same time, we should be able to address the fine-scale

989
00:52:21,220 --> 00:52:23,100
smoothing in GraphCast while we will

990
00:52:23,100 --> 00:52:26,740
be able to generate the full set of focus fields

991
00:52:26,740 --> 00:52:32,220
that we are currently having access to through gem.

992
00:52:32,220 --> 00:52:34,940
The concept of nudging is quite simple,

993
00:52:34,940 --> 00:52:38,260
as illustrated by this equation here.

994
00:52:38,260 --> 00:52:44,180
So here, F is the solution of our gem model

995
00:52:44,180 --> 00:52:45,780
after the dynamic substep.

996
00:52:45,780 --> 00:52:48,820
And this term highlighted in purple color

997
00:52:48,820 --> 00:52:51,340
actually corresponds to the nudging increments.

998
00:52:51,340 --> 00:52:55,060
So we add the nudging increment to the model solution

999
00:52:55,060 --> 00:52:58,140
after the dynamic step to get the nudge solution, which

1000
00:52:58,140 --> 00:52:59,620
is then fed to the physics.

1001
00:52:59,620 --> 00:53:02,900
And then it completes the complete model time step.

1002
00:53:02,900 --> 00:53:05,220
And then it fits back to the next dynamic step.

1003
00:53:05,220 --> 00:53:06,860
And this is how it continues.

1004
00:53:06,860 --> 00:53:12,220
And if we look at this nudging increment term,

1005
00:53:12,220 --> 00:53:14,700
we have this subscript Ls, which actually

1006
00:53:14,700 --> 00:53:16,220
implies the large scale.

1007
00:53:16,220 --> 00:53:18,660
So we are only considering the large scales

1008
00:53:18,660 --> 00:53:20,180
when we are applying the nudging.

1009
00:53:20,180 --> 00:53:23,060
And this omega is a relaxation parameter.

1010
00:53:23,060 --> 00:53:25,260
And if we break down this equation,

1011
00:53:25,260 --> 00:53:27,380
we can see basically what we have

1012
00:53:27,380 --> 00:53:31,140
is a weighted average of the large scales coming from GraphCast

1013
00:53:31,140 --> 00:53:35,980
and our model, whereas the fine scales are remaining intact

1014
00:53:35,980 --> 00:53:37,860
that is being credited by the model.

1015
00:53:37,860 --> 00:53:40,420
So this is how we can leverage the large scale accuracy

1016
00:53:40,420 --> 00:53:43,940
of GraphCast while allowing our model to freely evolve

1017
00:53:43,940 --> 00:53:45,580
the small scales.

1018
00:53:45,580 --> 00:53:48,340
And the scale separation between large and small scales,

1019
00:53:48,340 --> 00:53:50,740
we are doing that by decomposing the nudging

1020
00:53:50,740 --> 00:53:52,740
increments in the spectral space.

1021
00:53:52,740 --> 00:53:55,860
Although we are applying the nudging increment

1022
00:53:55,860 --> 00:53:58,780
in the grid point space, but we are decomposing it

1023
00:53:58,780 --> 00:53:59,660
in the spectral space.

1024
00:53:59,660 --> 00:54:03,140
And hence, we call it spectral nudging.

1025
00:54:03,140 --> 00:54:05,900
How we optimize the spectral nudging configuration

1026
00:54:05,900 --> 00:54:09,900
to support our objectives for this study

1027
00:54:09,900 --> 00:54:11,460
is a computationally demanding task.

1028
00:54:11,460 --> 00:54:13,340
And in a sense, it is still ongoing.

1029
00:54:13,340 --> 00:54:16,700
I mean, we are still sort of fine tuning the configuration.

1030
00:54:16,700 --> 00:54:19,460
But at present, the most optimal configuration

1031
00:54:19,460 --> 00:54:22,820
that we have has these following features.

1032
00:54:22,820 --> 00:54:27,580
We are only applying nudging to horizontal wind and temperature.

1033
00:54:27,620 --> 00:54:29,980
And we are not nudging the stratosphere and the boundary

1034
00:54:29,980 --> 00:54:31,380
layer for different reasons.

1035
00:54:31,380 --> 00:54:34,780
And we are nudging scales that are larger than 2750

1036
00:54:34,780 --> 00:54:35,900
for obvious reasons.

1037
00:54:35,900 --> 00:54:38,780
That should be obvious from the variance ratio comparison.

1038
00:54:38,780 --> 00:54:42,700
And we have 12 hours as the nudging realization scale.

1039
00:54:42,700 --> 00:54:46,140
And we apply nudging at every time step.

1040
00:54:46,140 --> 00:54:48,900
With that, I'll be going to some of the results

1041
00:54:48,900 --> 00:54:53,460
of verification that we will try to prove

1042
00:54:53,460 --> 00:54:55,260
that this approach actually helps

1043
00:54:55,260 --> 00:54:58,500
to improve the predictability of gem.

1044
00:54:58,500 --> 00:55:03,700
We ran a series of experiments for winter and summer of 2022.

1045
00:55:03,700 --> 00:55:05,780
So we have the control experiments

1046
00:55:05,780 --> 00:55:09,420
where there is no nudging, the control GDPS.

1047
00:55:09,420 --> 00:55:12,260
And then we have the GDPS with spectral nudging.

1048
00:55:12,260 --> 00:55:15,540
So control would be, in the next few slides, all the results.

1049
00:55:15,540 --> 00:55:17,700
Control would be shown in blue color.

1050
00:55:17,700 --> 00:55:21,060
And the results from spectral nudging with GDPS

1051
00:55:21,060 --> 00:55:23,620
would be shown with red.

1052
00:55:23,620 --> 00:55:26,540
So the first scores that I want to show

1053
00:55:26,540 --> 00:55:28,900
are verification against radios and observations.

1054
00:55:28,900 --> 00:55:31,900
It's similar to what Elvig has shown

1055
00:55:31,900 --> 00:55:34,260
in the previous presentation.

1056
00:55:34,260 --> 00:55:38,380
Just to repeat, we have in these figures,

1057
00:55:38,380 --> 00:55:43,060
we have different variables, zonal wind, wind modulus,

1058
00:55:43,060 --> 00:55:46,220
geopotential high temperature, and dew point depletion.

1059
00:55:46,220 --> 00:55:49,140
In all these figures, we have the dashed lines representing

1060
00:55:49,140 --> 00:55:51,700
the bias and solid lines representing standard deviation

1061
00:55:51,700 --> 00:55:52,380
of error.

1062
00:55:52,380 --> 00:55:55,260
And we have the shades of red and blue

1063
00:55:55,260 --> 00:55:59,780
that represent the statistically significant improvements

1064
00:55:59,780 --> 00:56:02,700
corresponding to the color of the experiment.

1065
00:56:02,700 --> 00:56:04,900
So if we see red color, then it implies

1066
00:56:04,900 --> 00:56:07,220
that we have statistically significant improvement

1067
00:56:07,220 --> 00:56:09,500
with the spectral nudging configuration.

1068
00:56:09,500 --> 00:56:13,420
And if it is blue, then we have deterioration

1069
00:56:13,420 --> 00:56:14,900
with spectral nudging.

1070
00:56:14,900 --> 00:56:16,380
What we can see from these figures,

1071
00:56:16,380 --> 00:56:19,500
this is for winter over the globe.

1072
00:56:19,500 --> 00:56:23,660
That beyond five days and up to 10 days,

1073
00:56:23,660 --> 00:56:26,540
we can see that there is tremendous improvement

1074
00:56:26,540 --> 00:56:29,660
in the scores for the standard deviation of error, which

1075
00:56:29,660 --> 00:56:30,380
is more difficult.

1076
00:56:30,380 --> 00:56:31,780
Excuse me, Sayed.

1077
00:56:31,780 --> 00:56:34,380
You have two minutes left.

1078
00:56:34,380 --> 00:56:37,580
OK, I'll try to be faster.

1079
00:56:37,580 --> 00:56:42,540
For the bias, the differences are mixed.

1080
00:56:42,540 --> 00:56:46,020
But the bias is less difficult to improve.

1081
00:56:46,540 --> 00:56:48,900
Standard deviation is more difficult.

1082
00:56:48,900 --> 00:56:52,340
So we see tremendous improvement, up to 10% improvement

1083
00:56:52,340 --> 00:56:54,420
in the RMSE for longer lead times.

1084
00:56:54,420 --> 00:56:56,580
In summer, though, we see modest improvement.

1085
00:56:56,580 --> 00:57:00,420
Still, we see statistically significant improvement

1086
00:57:00,420 --> 00:57:03,020
for the standard deviation of error.

1087
00:57:03,020 --> 00:57:05,420
And when you look at the animal liquid relation

1088
00:57:05,420 --> 00:57:09,260
coefficient, this is for the 500 hectropascal geopotential.

1089
00:57:09,260 --> 00:57:11,700
We see improvements both in winter and summer.

1090
00:57:11,700 --> 00:57:14,940
In winter, in terms of the gain in predictability,

1091
00:57:14,940 --> 00:57:16,940
it's around 18 hours.

1092
00:57:16,940 --> 00:57:20,340
In summer, it's about eight hours improvement.

1093
00:57:20,340 --> 00:57:23,220
And they're both statistically significant.

1094
00:57:23,220 --> 00:57:27,700
And last results is about tropical cyclone position error.

1095
00:57:27,700 --> 00:57:30,340
This is another thing that is very difficult to improve.

1096
00:57:30,340 --> 00:57:34,980
And our model tends to have, for the alone track position,

1097
00:57:34,980 --> 00:57:38,100
we tend to lag the model.

1098
00:57:38,100 --> 00:57:42,020
And we can improve that lagging with spectral nudging

1099
00:57:42,020 --> 00:57:42,940
towards graphcast.

1100
00:57:42,940 --> 00:57:44,980
And for the cross-track position error,

1101
00:57:44,980 --> 00:57:48,100
our cyclones tend to veer to the right

1102
00:57:48,100 --> 00:57:50,020
from the observed trajectory.

1103
00:57:50,020 --> 00:57:52,860
And we are able to considerably improve

1104
00:57:52,860 --> 00:57:57,740
that aspect of the position of tropical cyclone also.

1105
00:57:57,740 --> 00:58:00,380
And finally, just this figure, I'm

1106
00:58:00,380 --> 00:58:04,700
showing the temperature anomaly at 850 hectropascal

1107
00:58:04,700 --> 00:58:07,340
for a lead time of 240 hours for a single case.

1108
00:58:07,340 --> 00:58:10,860
We have the GDPS control on the left,

1109
00:58:10,860 --> 00:58:13,620
graphcast in the middle, and GDPS with spectral nudging

1110
00:58:13,620 --> 00:58:14,540
on the right.

1111
00:58:14,540 --> 00:58:18,420
And we can see that graphcast barely has any fine scale.

1112
00:58:18,420 --> 00:58:21,700
And both GDPS control and with spectral nudging,

1113
00:58:21,700 --> 00:58:24,820
they both have comparable fine scale information.

1114
00:58:24,820 --> 00:58:27,980
And we get this improvement by nudging as well.

1115
00:58:27,980 --> 00:58:31,860
So to summarize, we developed and hybrid NWP system

1116
00:58:31,860 --> 00:58:36,140
that fuses NWP models with AI models to spectral nudging.

1117
00:58:36,140 --> 00:58:38,460
And by leveraging more accurate large-scale predictions

1118
00:58:38,460 --> 00:58:42,820
from graphcast, we are able to significantly improve

1119
00:58:42,820 --> 00:58:47,580
our prediction scale with GDPS.

1120
00:58:47,580 --> 00:58:50,700
And I want to stress that the improvement that we have

1121
00:58:50,700 --> 00:58:53,700
is roughly equivalent to one solid innovation cycle, which

1122
00:58:53,700 --> 00:58:57,260
is about four years of work involving many scientists

1123
00:58:57,260 --> 00:58:59,740
from across the Meteorological Research Division

1124
00:58:59,740 --> 00:59:00,980
of Environment Canada.

1125
00:59:00,980 --> 00:59:04,540
And we were able to achieve something comparable

1126
00:59:04,540 --> 00:59:06,820
in a matter of four to five months.

1127
00:59:06,820 --> 00:59:10,220
And also, I want to stress that this

1128
00:59:10,220 --> 00:59:13,540
is based on work that uses a graphcast model that

1129
00:59:13,540 --> 00:59:15,060
has not been fine-tuned.

1130
00:59:15,060 --> 00:59:19,780
And with fine-tuning graphcast to emulate our own CMC

1131
00:59:19,780 --> 00:59:24,340
analysis, which is a work in progress by my colleague Christopher,

1132
00:59:24,340 --> 00:59:27,620
we hope that we could improve further.

1133
00:59:27,620 --> 00:59:29,260
And currently, with this configuration,

1134
00:59:29,260 --> 00:59:32,380
we have about 25% increase in the computational cost.

1135
00:59:32,380 --> 00:59:34,380
But this is without any optimization.

1136
00:59:34,380 --> 00:59:36,260
And with some optimization, we hope

1137
00:59:36,260 --> 00:59:38,460
that we will be able to reduce it to something

1138
00:59:38,460 --> 00:59:41,700
like less than 15% in the near future.

1139
00:59:41,700 --> 00:59:44,700
So with that, I will end my presentation.

1140
00:59:44,700 --> 00:59:46,460
Thank you, Sayada.

1141
00:59:46,460 --> 00:59:50,860
It's a very impressive conclusion.

1142
00:59:50,860 --> 00:59:53,780
I have one question here.

1143
00:59:53,780 --> 01:00:00,980
Nudging is done as gem integration goes or at posteriori?

1144
01:00:00,980 --> 01:00:03,140
No, it's online.

1145
01:00:03,140 --> 01:00:05,940
So what happens is, as I said, you

1146
01:00:06,820 --> 01:00:09,180
solve the dynamic step.

1147
01:00:09,180 --> 01:00:12,380
Because we have the dynamic sub-step and the physics sub-step.

1148
01:00:12,380 --> 01:00:14,660
And then we do the coupling in the split mode.

1149
01:00:14,660 --> 01:00:16,820
So we solve the dynamic sub-step.

1150
01:00:16,820 --> 01:00:18,660
We have a solution from dynamics,

1151
01:00:18,660 --> 01:00:20,020
which is an intermediate solution.

1152
01:00:20,020 --> 01:00:22,500
Then we update that by nudging.

1153
01:00:22,500 --> 01:00:25,220
And then we feed that updated solution to the physics.

1154
01:00:25,220 --> 01:00:27,540
And then we get the complete solution of the model time

1155
01:00:27,540 --> 01:00:28,040
step.

1156
01:00:28,040 --> 01:00:31,540
And then the next time, dynamics uses that solution

1157
01:00:31,540 --> 01:00:33,500
to predict the next dynamic step.

1158
01:00:33,500 --> 01:00:36,420
So it's not a posteriori.

1159
01:00:36,420 --> 01:00:40,780
It's an online update.

1160
01:00:40,780 --> 01:00:45,820
Last quick one, how does GDPSSN compare with GraphCast?

1161
01:00:48,740 --> 01:00:53,380
That was already shown by Ervig in the previous presentation.

1162
01:00:53,380 --> 01:00:57,780
That GDPSS, I mean, if you talk about control GDPSS

1163
01:00:57,780 --> 01:01:00,380
versus GraphCast, that presentation of Ervig

1164
01:01:00,380 --> 01:01:04,260
should allow you to see like it actually improves.

1165
01:01:04,260 --> 01:01:06,020
What I am missing in my figures, because this

1166
01:01:06,020 --> 01:01:08,940
is still a work in progress, I mean,

1167
01:01:08,940 --> 01:01:12,100
in our paper that we expect to submit soon,

1168
01:01:12,100 --> 01:01:16,140
which we will be adding the GraphCast also,

1169
01:01:16,140 --> 01:01:20,300
focus in this, for example, in these figures to show

1170
01:01:20,300 --> 01:01:25,060
control GDPS with GraphCast and GraphCast itself,

1171
01:01:25,060 --> 01:01:28,260
like how much of the improvement is coming from GraphCast.

1172
01:01:28,260 --> 01:01:30,220
But it is definitely coming from GraphCast,

1173
01:01:30,220 --> 01:01:33,660
as Ervig's presentation showed that the large scales in GraphCast

1174
01:01:33,660 --> 01:01:35,220
are much better.

1175
01:01:35,220 --> 01:01:38,220
Yes, Isayed, looking forward to read the print.

1176
01:01:38,220 --> 01:01:41,300
That will be very popular, I'm sure.

1177
01:01:41,300 --> 01:01:42,620
Thank you.

1178
01:01:42,620 --> 01:01:43,140
Merci.

1179
01:01:43,140 --> 01:01:44,860
Anne, at what?

1180
01:01:44,860 --> 01:01:46,940
Yes, so we're moving.

1181
01:01:46,940 --> 01:01:48,420
You might have noticed in the schedule

1182
01:01:48,420 --> 01:01:50,500
that Christian Isayed was originally

1183
01:01:50,500 --> 01:01:51,500
supposed to present this.

1184
01:01:51,500 --> 01:01:56,540
But I want to thank Madalina Socer to have accepted

1185
01:01:56,540 --> 01:02:00,140
to present and prepare this presentation for us.

1186
01:02:00,140 --> 01:02:03,900
So she's a Climate Extreme Specialist at Environment Canada.

1187
01:02:03,900 --> 01:02:06,460
She's going to be presenting on the development

1188
01:02:06,460 --> 01:02:10,100
of artificial intelligence downscaling applications

1189
01:02:10,100 --> 01:02:15,660
for medium-range forecasts of weather elements at CCMEP.

1190
01:02:15,660 --> 01:02:19,100
So without further ado, over to you, Madalina.

1191
01:02:19,100 --> 01:02:20,220
OK, thank you.

1192
01:02:20,220 --> 01:02:22,060
I hope you're hearing me well.

1193
01:02:22,060 --> 01:02:23,540
We are, yes.

1194
01:02:23,540 --> 01:02:24,340
OK, great.

1195
01:02:24,340 --> 01:02:26,580
So I am here today to present to you

1196
01:02:26,580 --> 01:02:29,380
some development of artificial intelligence downscaling

1197
01:02:29,380 --> 01:02:31,860
techniques that we're doing at CCMEP in collaboration

1198
01:02:31,860 --> 01:02:33,220
with IBM Research.

1199
01:02:33,220 --> 01:02:35,020
And I would like to acknowledge my co-authors

1200
01:02:35,020 --> 01:02:38,900
that are listed here, both from ECCC and IBM Research.

1201
01:02:43,780 --> 01:02:46,500
So first, I would like to say that the vision for this project

1202
01:02:46,500 --> 01:02:49,180
is actually to help us offer seamless day one to day 10

1203
01:02:49,180 --> 01:02:52,500
public forecast products as part of the transformation

1204
01:02:52,500 --> 01:02:54,740
of the meteorological service of Canada.

1205
01:02:54,740 --> 01:02:56,380
So in order to do this, we need to bridge

1206
01:02:56,380 --> 01:02:58,820
the gap between high-resolution, short-term forecasts,

1207
01:02:58,820 --> 01:03:02,780
and medium-range, lower-resolution forecasts.

1208
01:03:02,780 --> 01:03:04,380
And the reason why we want to do this

1209
01:03:04,380 --> 01:03:06,700
is because we know that insufficient horizontal

1210
01:03:06,700 --> 01:03:09,860
resolution causes forecast errors and especially biases.

1211
01:03:09,860 --> 01:03:12,020
I am showing here a graph of bias,

1212
01:03:12,020 --> 01:03:15,900
a comparison between a low-resolution model in red

1213
01:03:15,900 --> 01:03:17,820
and the high-resolution model in blue.

1214
01:03:17,820 --> 01:03:20,620
And what we are saying here, the closest we are to zero,

1215
01:03:20,620 --> 01:03:21,620
the less bias we have.

1216
01:03:21,620 --> 01:03:25,300
And we really see that increasing horizontal resolution

1217
01:03:25,300 --> 01:03:26,860
is improving this bias.

1218
01:03:26,860 --> 01:03:28,340
Usually, the way that we do that is

1219
01:03:28,340 --> 01:03:29,980
by doing dynamical downscaling.

1220
01:03:29,980 --> 01:03:32,540
So running numerical weather prediction models.

1221
01:03:32,540 --> 01:03:34,900
But this is very computationally expensive,

1222
01:03:34,900 --> 01:03:37,140
which makes it limiting.

1223
01:03:37,140 --> 01:03:39,980
A partial solution to this is doing statistical downscaling,

1224
01:03:39,980 --> 01:03:43,180
which means using past data and deriving

1225
01:03:43,180 --> 01:03:45,740
statistical relationships from this past data,

1226
01:03:45,740 --> 01:03:48,300
such that we apply these relationships

1227
01:03:48,300 --> 01:03:52,820
on the course input and we obtain high-resolution output.

1228
01:03:52,820 --> 01:03:54,820
These type of solutions are limited

1229
01:03:54,820 --> 01:03:56,620
by the fact that we have to impose certain

1230
01:03:56,620 --> 01:03:59,020
non-mathematical relationships in the data.

1231
01:03:59,020 --> 01:04:01,740
So now we have a data-driven alternative,

1232
01:04:01,740 --> 01:04:05,300
which is to apply artificial intelligence techniques

1233
01:04:05,300 --> 01:04:06,460
to do downscaling.

1234
01:04:06,460 --> 01:04:09,020
And what this does is that it allows

1235
01:04:09,020 --> 01:04:11,180
to derive complex relationships in the data

1236
01:04:11,180 --> 01:04:13,260
that we provide to these models.

1237
01:04:13,260 --> 01:04:16,500
So our objective is to develop

1238
01:04:16,500 --> 01:04:19,140
artificial intelligence downscaling techniques

1239
01:04:19,140 --> 01:04:22,500
to downscale weather elements from medium range forecast

1240
01:04:22,500 --> 01:04:23,620
to the kilometric scale.

1241
01:04:23,620 --> 01:04:26,180
And we will hope that this will help both deterministic

1242
01:04:26,180 --> 01:04:28,940
and ensemble forecasting.

1243
01:04:28,940 --> 01:04:31,340
So just to briefly present our project.

1244
01:04:31,340 --> 01:04:33,260
So this is a collaboration that started this year

1245
01:04:33,260 --> 01:04:35,980
between CCMAP and IBM Research.

1246
01:04:35,980 --> 01:04:37,620
And here at Environment Canada, we

1247
01:04:37,620 --> 01:04:39,100
have the meteorological expertise.

1248
01:04:39,100 --> 01:04:43,140
And we definitely have subjects, problems

1249
01:04:43,140 --> 01:04:45,700
that could really benefit from these artificial intelligence

1250
01:04:45,700 --> 01:04:47,180
solutions, but we don't necessarily

1251
01:04:47,180 --> 01:04:49,820
have the artificial intelligence expertise, which

1252
01:04:49,820 --> 01:04:52,780
is where IBM Research comes into play.

1253
01:04:52,780 --> 01:04:54,820
And collaborating with them, they are really experts

1254
01:04:54,820 --> 01:04:57,820
in this field, will allow us to advance much faster.

1255
01:04:57,820 --> 01:05:00,140
And the expected outcome from this collaboration

1256
01:05:00,140 --> 01:05:02,060
that for now it's only meant to be on one year,

1257
01:05:02,060 --> 01:05:05,500
is to develop low-cost and efficient alternatives

1258
01:05:05,500 --> 01:05:09,780
to the computationally expensive dynamical models.

1259
01:05:09,780 --> 01:05:13,100
And the other thing that we want to get from this collaboration

1260
01:05:13,100 --> 01:05:14,900
is we want to learn from IBM Research.

1261
01:05:14,900 --> 01:05:16,540
So at the end of this project, we

1262
01:05:16,540 --> 01:05:19,100
want to enhance our capability at Environment Canada

1263
01:05:19,100 --> 01:05:21,780
to be carrying out this type of research and development

1264
01:05:21,780 --> 01:05:23,740
and eventual operational implementation

1265
01:05:23,740 --> 01:05:27,540
of AI downscaling techniques.

1266
01:05:27,540 --> 01:05:31,100
So the specific goals of our projects are as follows.

1267
01:05:31,100 --> 01:05:33,940
So we are interested in downscaling weather elements.

1268
01:05:33,940 --> 01:05:36,060
And by this, I mean surface winds, temperature,

1269
01:05:36,060 --> 01:05:37,900
and precipitation in the first stage,

1270
01:05:37,900 --> 01:05:41,260
a forecast from the GDPS, which is our global deterministic

1271
01:05:41,260 --> 01:05:44,460
prediction system shown here.

1272
01:05:44,500 --> 01:05:47,300
And runs at a resolution of 15 kilometers

1273
01:05:47,300 --> 01:05:49,500
to the resolution of the HRDPS, which

1274
01:05:49,500 --> 01:05:51,420
is our high-resolution deterministic prediction

1275
01:05:51,420 --> 01:05:55,940
system that is run for 48 hours for now at 2.5 kilometer

1276
01:05:55,940 --> 01:05:56,900
resolution.

1277
01:05:56,900 --> 01:05:59,340
And in this project, we are taking a two-step approach.

1278
01:05:59,340 --> 01:06:02,540
So in the first step, we will be looking at the baseline model

1279
01:06:02,540 --> 01:06:05,660
that is based on generative adversarial networks.

1280
01:06:05,660 --> 01:06:08,020
And in the second stage of the project,

1281
01:06:08,020 --> 01:06:12,940
we will be taking advantage of foundation models

1282
01:06:12,940 --> 01:06:14,940
and tuning them for our application.

1283
01:06:14,940 --> 01:06:16,660
The training data for the models will

1284
01:06:16,660 --> 01:06:19,060
be forecasting data from the GDPS

1285
01:06:19,060 --> 01:06:23,700
as the low-resolution data set and from the HRDPS

1286
01:06:23,700 --> 01:06:25,980
as the high-resolution data set.

1287
01:06:25,980 --> 01:06:28,980
And it is very important for our operational needs

1288
01:06:28,980 --> 01:06:32,580
that the downscale products are available on the HRDPS grid.

1289
01:06:35,100 --> 01:06:37,820
For now in this talk, I will only focus on the baseline model

1290
01:06:37,820 --> 01:06:40,220
as this is a collaboration that just started.

1291
01:06:40,220 --> 01:06:42,140
And we haven't gotten yet to the second part.

1292
01:06:43,540 --> 01:06:47,420
So just briefly, what is a generative adversarial network?

1293
01:06:47,420 --> 01:06:50,860
And generative adversarial networks

1294
01:06:50,860 --> 01:06:53,420
consist of two networks, a generator, two neural networks,

1295
01:06:53,420 --> 01:06:54,620
a generator and discriminator.

1296
01:06:54,620 --> 01:06:55,980
And the way that they work is that they

1297
01:06:55,980 --> 01:06:58,180
are trained in a competing process.

1298
01:06:58,180 --> 01:07:00,420
So the generator pretty much generates images

1299
01:07:00,420 --> 01:07:03,660
that look as much as possible as the real data.

1300
01:07:03,660 --> 01:07:05,820
And the goal of the generator is to just

1301
01:07:05,820 --> 01:07:07,260
fold the discriminator.

1302
01:07:07,260 --> 01:07:09,060
On the other hand, we have the discriminator

1303
01:07:09,060 --> 01:07:12,340
that receives both real data, which in our case is HRDPS

1304
01:07:12,340 --> 01:07:15,220
data, and generated data, and has to decide

1305
01:07:15,220 --> 01:07:18,340
whether this generated data is real or fake.

1306
01:07:18,340 --> 01:07:23,580
And in our case, we use the ANAU et al. 2023 implementation

1307
01:07:23,580 --> 01:07:24,740
of a Vassar SteamGAN.

1308
01:07:27,740 --> 01:07:32,220
I will show you some preliminary results for our project.

1309
01:07:32,220 --> 01:07:36,100
So these preliminary results are based on the WGAN from ANAU

1310
01:07:36,100 --> 01:07:36,580
et al.

1311
01:07:36,580 --> 01:07:37,740
Without any covariates.

1312
01:07:37,740 --> 01:07:40,740
So what this means is that the only data that goes for now

1313
01:07:40,740 --> 01:07:45,020
in this model is zonal and meridional 10-meter wing

1314
01:07:45,020 --> 01:07:45,700
components.

1315
01:07:45,700 --> 01:07:48,940
And this is the data that we are trying to obtain.

1316
01:07:48,940 --> 01:07:51,980
So the low-resolution data comes from GDPS.

1317
01:07:51,980 --> 01:07:54,580
High resolution comes from the HRDPS.

1318
01:07:54,580 --> 01:07:57,300
And so far, we are training the model with one year of data

1319
01:07:57,300 --> 01:07:58,980
that is divided as follows.

1320
01:07:58,980 --> 01:08:01,420
75% of the data is used for training.

1321
01:08:01,420 --> 01:08:03,220
It's about 7,000 forecasts.

1322
01:08:03,220 --> 01:08:05,900
12.5% is used for validation.

1323
01:08:05,900 --> 01:08:08,500
And this validation data is used during the training

1324
01:08:08,500 --> 01:08:09,180
of the model.

1325
01:08:09,180 --> 01:08:12,460
So it's used to stop overfitting the model.

1326
01:08:12,460 --> 01:08:15,300
And then 12.5% of the data is used for testing.

1327
01:08:15,300 --> 01:08:17,580
So once we have a tuned model, we

1328
01:08:17,580 --> 01:08:20,380
will use this data set, about 1,200 forecasts,

1329
01:08:20,380 --> 01:08:24,060
to be seeing how well this model functions.

1330
01:08:24,060 --> 01:08:25,700
Because we are using forecasting data,

1331
01:08:25,700 --> 01:08:26,700
it is a bit difficult.

1332
01:08:26,700 --> 01:08:28,420
Because as you know, forecasts, there

1333
01:08:28,420 --> 01:08:31,140
are increases with forecast lead time.

1334
01:08:31,140 --> 01:08:33,180
And so we might have too much divergence

1335
01:08:33,180 --> 01:08:35,700
between the HRDPS, the high-resolution data set,

1336
01:08:35,700 --> 01:08:38,700
and our low-resolution data set.

1337
01:08:38,700 --> 01:08:40,660
On the other hand, the first six hours of the forecasts

1338
01:08:40,660 --> 01:08:42,660
are affected by the spin-up time of the model.

1339
01:08:42,660 --> 01:08:46,900
So we have decided for now to go with forecasts our 6 to 18.

1340
01:08:46,900 --> 01:08:50,300
And we are using them from the 0,0 and the 12 UTC

1341
01:08:50,300 --> 01:08:52,060
initialization of both models.

1342
01:08:52,060 --> 01:08:57,060
So in this way, we are covering the entire day.

1343
01:08:57,060 --> 01:09:01,780
So our challenge is covering the entire HRDPS domain.

1344
01:09:01,780 --> 01:09:03,980
I am showing here the HRDPS domain.

1345
01:09:03,980 --> 01:09:07,580
It is on the order of 2,500 by 1,200 grid points.

1346
01:09:07,580 --> 01:09:09,660
So it's a very large domain that's

1347
01:09:09,660 --> 01:09:11,740
spanning the width of Canada.

1348
01:09:11,740 --> 01:09:15,180
And it isn't really possible, or at least we don't think

1349
01:09:15,180 --> 01:09:17,460
it's possible so far, to be training directly

1350
01:09:17,460 --> 01:09:19,740
on such a large domain.

1351
01:09:19,740 --> 01:09:22,180
We do not have the computational resources to do that.

1352
01:09:22,180 --> 01:09:24,620
And we also aren't sure exactly how much data

1353
01:09:24,620 --> 01:09:27,300
you would need to be able to train on such a model.

1354
01:09:27,300 --> 01:09:31,340
The now and all implementation, in that implementation,

1355
01:09:31,340 --> 01:09:36,140
the training is done on 16 by 16 pixel low resolution

1356
01:09:36,140 --> 01:09:41,380
patches and 128 by 128 pixel high resolution patches.

1357
01:09:41,380 --> 01:09:45,420
So what we will do is we will adjust to that type of training.

1358
01:09:45,420 --> 01:09:48,780
And in order to do that, the strategy that we have adapted

1359
01:09:48,780 --> 01:09:53,420
is to just select random patches from one forecast

1360
01:09:53,420 --> 01:09:55,620
from our HRDPS data set.

1361
01:09:55,620 --> 01:10:00,540
We will re-read the GDPS data set on the HRDPS domain

1362
01:10:00,540 --> 01:10:04,300
and then course-crain it to go back to its resolution.

1363
01:10:04,300 --> 01:10:06,340
And in this way, we are going to end up

1364
01:10:06,340 --> 01:10:07,900
with this type of patches that we'll

1365
01:10:07,900 --> 01:10:10,380
use to do the training.

1366
01:10:10,380 --> 01:10:12,660
So here is the high resolution data patch

1367
01:10:12,660 --> 01:10:16,180
and the corresponding GDPS low resolution data patch.

1368
01:10:16,180 --> 01:10:20,740
So at each epoch, we are using between 300 and 700 random

1369
01:10:20,740 --> 01:10:21,580
patches.

1370
01:10:21,580 --> 01:10:24,020
And the model that we are showing today

1371
01:10:24,020 --> 01:10:27,860
has been trained on 17,370 epochs.

1372
01:10:27,860 --> 01:10:32,260
So this took about 149 hours to train on one GPU.

1373
01:10:32,260 --> 01:10:34,100
And we have done more than two passes

1374
01:10:34,100 --> 01:10:37,300
through the entire data set.

1375
01:10:37,300 --> 01:10:39,220
So once you have a trained model,

1376
01:10:39,220 --> 01:10:41,700
you can perform inference with the GDPS data.

1377
01:10:41,700 --> 01:10:46,180
And that inference will also be done on 16 by 16 pixel patches.

1378
01:10:46,180 --> 01:10:49,060
So here I have a GDPS input.

1379
01:10:49,060 --> 01:10:50,780
We perform inference.

1380
01:10:50,780 --> 01:10:53,100
And like this, we obtain the downscale forecast,

1381
01:10:53,100 --> 01:10:55,260
the downscale U and V fields.

1382
01:10:55,260 --> 01:10:59,500
And just as a comparison, we have here the HRDPS forecast.

1383
01:10:59,500 --> 01:11:02,220
So the first things that we can say

1384
01:11:02,220 --> 01:11:04,500
is that we are definitely downscaling.

1385
01:11:04,500 --> 01:11:08,660
So we are obtaining information at a small scale.

1386
01:11:08,660 --> 01:11:12,060
It isn't as much as the HRDPS is showing.

1387
01:11:12,060 --> 01:11:14,020
But as I was mentioning before, one problem

1388
01:11:14,020 --> 01:11:16,420
with a low resolution is the fact that you have biases.

1389
01:11:16,420 --> 01:11:18,620
And we are definitely achieving some sort

1390
01:11:18,620 --> 01:11:21,580
of a bias correction.

1391
01:11:21,580 --> 01:11:25,060
Now, of course, you have to parse the entire HRDPS domain.

1392
01:11:25,060 --> 01:11:27,500
One way to do that would be to sequentially process

1393
01:11:27,500 --> 01:11:29,820
128 by 128 patches.

1394
01:11:29,820 --> 01:11:32,860
But that would result into artifacts at the borders.

1395
01:11:32,860 --> 01:11:35,260
So the strategy that we have adopted instead

1396
01:11:35,260 --> 01:11:37,300
is to do some overlapping and then

1397
01:11:37,300 --> 01:11:41,500
to take the median of the ensemble of overlaps.

1398
01:11:41,500 --> 01:11:44,260
And in this way, we managed to patch and obtain this figure

1399
01:11:44,260 --> 01:11:46,580
over the entire domain.

1400
01:11:46,580 --> 01:11:49,340
We have performed validation of our model.

1401
01:11:49,340 --> 01:11:54,620
So we have used the test data, the 1,200 forecast from the GDPS.

1402
01:11:54,620 --> 01:11:57,540
And here I am showing the root mean square error

1403
01:11:57,540 --> 01:12:00,940
for the U-wind component and for the V-wind component.

1404
01:12:00,940 --> 01:12:04,020
And on the bottom, I'm showing the mean absolute error.

1405
01:12:04,020 --> 01:12:06,140
And these are the metrics that are computed

1406
01:12:06,140 --> 01:12:12,420
between the downscale GDPS and the HRDPS corresponding

1407
01:12:12,420 --> 01:12:13,660
verification.

1408
01:12:13,660 --> 01:12:15,740
And we are comparing it with some baselines that

1409
01:12:15,740 --> 01:12:17,260
can be used for interpolation.

1410
01:12:17,260 --> 01:12:19,740
So I'm showing in orange you have bilinear interpolation

1411
01:12:19,740 --> 01:12:22,700
and in green you have nearest neighbor interpolation.

1412
01:12:22,700 --> 01:12:24,620
So as we are seeing in all the metrics,

1413
01:12:24,620 --> 01:12:27,340
the downscale is showing better results,

1414
01:12:27,340 --> 01:12:30,020
is performing better than the other types of interpolation

1415
01:12:30,020 --> 01:12:32,820
for our test data set.

1416
01:12:32,820 --> 01:12:34,900
We have also done a power spectrum analysis

1417
01:12:34,900 --> 01:12:38,580
in order to really quantify how much detail we

1418
01:12:38,580 --> 01:12:40,100
are getting at the small scales.

1419
01:12:40,100 --> 01:12:42,020
And here I'm showing the radially average power

1420
01:12:42,020 --> 01:12:45,780
spectral density between HRDPS in blue and the downscale

1421
01:12:45,780 --> 01:12:48,180
forecast in orange.

1422
01:12:48,180 --> 01:12:51,460
And these power spectra are average over the test data

1423
01:12:51,460 --> 01:12:52,580
set as well.

1424
01:12:52,620 --> 01:12:55,100
What we are seeing is that indeed at small scales,

1425
01:12:55,100 --> 01:12:57,700
we are still not getting enough power.

1426
01:12:57,700 --> 01:13:00,020
So we do not get enough detail.

1427
01:13:00,020 --> 01:13:04,900
Madelina, if you could lend this in one or two minutes.

1428
01:13:04,900 --> 01:13:06,020
Sounds good.

1429
01:13:06,020 --> 01:13:09,700
So of course, we are training so far with one year of data.

1430
01:13:09,700 --> 01:13:12,540
And it would be very interesting to see

1431
01:13:12,540 --> 01:13:16,260
how much more detail we can obtain by training further.

1432
01:13:16,260 --> 01:13:18,460
I'm just showing also an integrated measure

1433
01:13:18,460 --> 01:13:20,020
of the difference in the power spectra.

1434
01:13:20,020 --> 01:13:22,220
And what you are seeing here is that compared

1435
01:13:22,220 --> 01:13:24,860
to the bilinear and the nearest neighbor interpolation,

1436
01:13:24,860 --> 01:13:29,660
the downscaled AI downscaling performance much, much better.

1437
01:13:29,660 --> 01:13:31,780
So it's able to recover way more structure

1438
01:13:31,780 --> 01:13:34,660
at the small scales.

1439
01:13:34,660 --> 01:13:38,340
So the next steps with our forecast, with our project,

1440
01:13:38,340 --> 01:13:40,900
the WGAN needs further development and testing.

1441
01:13:40,900 --> 01:13:43,580
So we are planning on testing with more data.

1442
01:13:43,580 --> 01:13:45,820
A very important thing that we are working on right now

1443
01:13:45,820 --> 01:13:47,700
is to add other covariates.

1444
01:13:47,700 --> 01:13:50,340
So as I said, for now, we are only using windfields.

1445
01:13:50,380 --> 01:13:52,700
But we are adding topography, surface pressure,

1446
01:13:52,700 --> 01:13:54,660
and we are thinking about what other covariates

1447
01:13:54,660 --> 01:13:57,100
may be such escape to add to our model.

1448
01:13:57,100 --> 01:14:00,460
And once we have a baseline that we are satisfied with,

1449
01:14:00,460 --> 01:14:01,980
the important part comes.

1450
01:14:01,980 --> 01:14:04,300
And that is doing a thorough meteorological verification

1451
01:14:04,300 --> 01:14:05,300
of the downscale forecast.

1452
01:14:05,300 --> 01:14:07,660
Because like I said, we have operational goals.

1453
01:14:07,660 --> 01:14:10,380
And we really want to see how this forecast

1454
01:14:10,380 --> 01:14:12,340
respond to our needs.

1455
01:14:12,340 --> 01:14:14,820
Finally, once we finish this first step of the project,

1456
01:14:14,820 --> 01:14:17,820
we are planning on moving to the second more ambitious part,

1457
01:14:18,100 --> 01:14:20,940
which is to develop a large GI model that

1458
01:14:20,940 --> 01:14:23,660
is based on a pre-trained foundation model

1459
01:14:23,660 --> 01:14:26,900
that we will be fine-tuning in order

1460
01:14:26,900 --> 01:14:28,980
to obtain downscale forecasts.

1461
01:14:28,980 --> 01:14:32,180
And this is where, again, collaborating with IBM Research

1462
01:14:32,180 --> 01:14:35,900
is extremely important, as they have very much experience

1463
01:14:35,900 --> 01:14:37,140
in these foundation models.

1464
01:14:37,140 --> 01:14:41,020
And we are hoping to advance at least as fast as now.

1465
01:14:41,020 --> 01:14:42,340
So this is it for me.

1466
01:14:42,340 --> 01:14:44,220
Thank you very much.

1467
01:14:44,220 --> 01:14:48,140
Thank you, Medellina.

1468
01:14:48,140 --> 01:14:49,540
I have a question in the chat.

1469
01:14:49,540 --> 01:14:51,540
I have one.

1470
01:14:51,540 --> 01:14:57,860
I saw that you train, especially, the model.

1471
01:14:57,860 --> 01:14:59,940
But I was surprised to see that you only

1472
01:14:59,940 --> 01:15:02,500
used one year of data to do that.

1473
01:15:02,500 --> 01:15:03,740
Is there a reason behind that?

1474
01:15:03,740 --> 01:15:07,460
Because it seems to me that it's not a lot of data.

1475
01:15:07,460 --> 01:15:10,620
Yes, well, like I said, we have.

1476
01:15:10,620 --> 01:15:15,900
So we do end up having a lot of forecast samples.

1477
01:15:15,900 --> 01:15:22,060
We are training on 128 by 128 pixel patches out

1478
01:15:22,060 --> 01:15:23,060
of a very large domain.

1479
01:15:23,060 --> 01:15:24,900
So it ends up being a lot of data.

1480
01:15:24,900 --> 01:15:26,260
But it is not finalized.

1481
01:15:26,260 --> 01:15:28,420
So here we are in a developing mode.

1482
01:15:28,420 --> 01:15:29,620
We are trying to develop the model

1483
01:15:29,620 --> 01:15:31,300
and make sure it's working properly.

1484
01:15:31,300 --> 01:15:32,940
And this is just the first step.

1485
01:15:32,940 --> 01:15:35,220
We are definitely planning on adding more data

1486
01:15:35,220 --> 01:15:37,900
and seeing how we can improve.

1487
01:15:37,900 --> 01:15:40,540
Thank you, Medellina.

1488
01:15:40,540 --> 01:15:44,060
I don't have a question.

1489
01:15:44,060 --> 01:15:47,700
Otherwise, Anne, I'm going to speak.

1490
01:15:47,700 --> 01:15:48,780
Right on time.

1491
01:15:48,780 --> 01:15:50,940
Thank you, Medellina.

1492
01:15:50,940 --> 01:15:57,100
So now we're going over to Reynel Sospedra Alfonso, who

1493
01:15:57,100 --> 01:16:00,060
is also a research scientist at Environment and Climate

1494
01:16:00,060 --> 01:16:01,540
Change Canada.

1495
01:16:01,540 --> 01:16:05,460
He will be presenting on deep learning-based bias

1496
01:16:05,500 --> 01:16:09,140
adjustments of Arctic sea ice forecasts

1497
01:16:09,140 --> 01:16:12,140
from version three of the Canadian seasonal to

1498
01:16:12,140 --> 01:16:15,020
inter-annual prediction systems.

1499
01:16:15,020 --> 01:16:18,220
Also known as CANSTEP version three.

1500
01:16:18,220 --> 01:16:21,260
So, Reynel, the mic is all yours.

1501
01:16:21,260 --> 01:16:22,700
Thank you.

1502
01:16:22,700 --> 01:16:23,580
Thank you, Anne.

1503
01:16:23,580 --> 01:16:26,420
Can you hear me well?

1504
01:16:26,420 --> 01:16:27,380
Yes, we do.

1505
01:16:27,380 --> 01:16:29,700
Your presentation is not full screen, though.

1506
01:16:29,700 --> 01:16:30,500
It's not.

1507
01:16:30,500 --> 01:16:30,980
No, it's not.

1508
01:16:30,980 --> 01:16:33,980
All right.

1509
01:16:33,980 --> 01:16:34,580
How about now?

1510
01:16:35,740 --> 01:16:36,780
Yes, it is.

1511
01:16:36,780 --> 01:16:37,620
Perfect.

1512
01:16:37,620 --> 01:16:39,380
Thank you so much, Medell.

1513
01:16:39,380 --> 01:16:41,700
Thank you, yes, to the organizer for this opportunity.

1514
01:16:41,700 --> 01:16:43,540
My name is Reynel Sospedra Alfonso.

1515
01:16:43,540 --> 01:16:46,780
I'm a research scientist at the Canadian Center for Climate

1516
01:16:46,780 --> 01:16:49,380
Modeling and Analysis, based in Victoria.

1517
01:16:49,380 --> 01:16:53,180
And I want to start by acknowledging the contributions

1518
01:16:53,180 --> 01:16:55,300
or the work of colleagues at CCMA,

1519
01:16:55,300 --> 01:16:59,020
which made this type of project possible.

1520
01:16:59,020 --> 01:17:01,260
I list some of them down here.

1521
01:17:01,260 --> 01:17:03,620
And I also want to acknowledge the co-authors

1522
01:17:03,620 --> 01:17:05,820
of this presentation, or this work,

1523
01:17:05,820 --> 01:17:09,260
Joseph Martin, Michael Simon, and especially

1524
01:17:09,260 --> 01:17:14,140
Parca Guilla, who has been key for this project going forward.

1525
01:17:14,140 --> 01:17:16,980
He has taken the time to do the implementation, training,

1526
01:17:16,980 --> 01:17:20,860
and testing of the models we have been looking at.

1527
01:17:20,860 --> 01:17:23,540
And I want to mention that this is part, what I'm going to talk

1528
01:17:23,540 --> 01:17:25,460
about here is part of a bigger project

1529
01:17:25,460 --> 01:17:28,780
that we are trying to pursue at CCMA, which

1530
01:17:28,780 --> 01:17:32,860
is the use or applications of machine learning methods,

1531
01:17:32,900 --> 01:17:36,300
in particular deep learning, to post-process our seasonal

1532
01:17:36,300 --> 01:17:38,580
to the scale forecast.

1533
01:17:38,580 --> 01:17:40,780
So for this talk, I will talk in particular

1534
01:17:40,780 --> 01:17:44,140
about the post-processing or seasonal forecast of CIS.

1535
01:17:46,980 --> 01:17:50,140
The seasonal forecast, as you may know,

1536
01:17:50,140 --> 01:17:53,340
CANSIPS is the Canadian seasonal and internal prediction

1537
01:17:53,340 --> 01:17:55,900
system, which provides the Environment and Climate Change

1538
01:17:55,900 --> 01:17:58,580
Canada's operational, probabilistic seasonal

1539
01:17:58,580 --> 01:18:01,700
forecast, both national and global.

1540
01:18:01,700 --> 01:18:06,660
And CANSIPS first appeared or was first

1541
01:18:06,660 --> 01:18:11,420
debuted in 2011 as a two-model forecasting system.

1542
01:18:11,420 --> 01:18:14,540
It has evolved since, and now we are in 2024,

1543
01:18:14,540 --> 01:18:17,620
with the new version of CANSIPS B3,

1544
01:18:17,620 --> 01:18:21,660
which actually will be launched next month.

1545
01:18:21,660 --> 01:18:25,300
And so here, what I'm going to talk about

1546
01:18:25,300 --> 01:18:29,900
is the forecast that we produce with CANIAS M5, which

1547
01:18:29,900 --> 01:18:34,300
is a new model that now we'll be using in CANSIPS.

1548
01:18:34,300 --> 01:18:39,900
And CANIAS M5 is an air system model that is produced at CCMA,

1549
01:18:39,900 --> 01:18:45,460
and now will be then, as I said, used for our seasonal forecast.

1550
01:18:45,460 --> 01:18:47,700
CANIAS M5 air system models, so it

1551
01:18:47,700 --> 01:18:52,460
couples the atmosphere, the ocean, CIS component, land,

1552
01:18:52,460 --> 01:18:57,940
and also biochemistry, both on land and the ocean.

1553
01:18:57,980 --> 01:19:00,100
What we do, we take our climate model,

1554
01:19:00,100 --> 01:19:03,460
we initialize the climate model following the indications

1555
01:19:03,460 --> 01:19:05,340
that you see here on the right.

1556
01:19:05,340 --> 01:19:07,540
So when we initialize the forecast,

1557
01:19:07,540 --> 01:19:10,780
we take, we notch the model towards re-analysis,

1558
01:19:10,780 --> 01:19:15,300
and then we launch those forecasts in time.

1559
01:19:15,300 --> 01:19:17,700
The version of CANIAS M5 that I'm going to be talking about

1560
01:19:17,700 --> 01:19:22,980
is actually an optimal bias-corrected version, which

1561
01:19:22,980 --> 01:19:26,220
follows the work by Sino-Kankarin, which

1562
01:19:26,220 --> 01:19:32,100
does an online bias optimization to the model.

1563
01:19:32,100 --> 01:19:37,500
Now, the work that I'm going to be presenting to you

1564
01:19:37,500 --> 01:19:40,740
deals with the post-processing of those forecasts.

1565
01:19:40,740 --> 01:19:43,020
So here, what you see is a representation

1566
01:19:43,020 --> 01:19:46,620
of those forecasts in black.

1567
01:19:46,620 --> 01:19:49,700
It's the observations that we are verifying,

1568
01:19:49,700 --> 01:19:52,020
observations, the monthly values.

1569
01:19:52,020 --> 01:19:54,900
And then what you see is the representation

1570
01:19:54,900 --> 01:19:57,540
of those forecasts, which are initialized

1571
01:19:57,540 --> 01:20:01,100
at the start of every month during the handcast period.

1572
01:20:01,100 --> 01:20:05,020
So we'll be looking at handcasts from 1980 to 2021.

1573
01:20:05,020 --> 01:20:08,140
We have, for each month in that time,

1574
01:20:08,140 --> 01:20:12,260
we launch an ensemble of forecasts of 10 members,

1575
01:20:12,260 --> 01:20:14,860
which run for 12 months.

1576
01:20:14,860 --> 01:20:17,060
The variable of interest for us here

1577
01:20:17,060 --> 01:20:19,580
is CIS concentration, which is simply

1578
01:20:19,580 --> 01:20:24,500
the fraction of CIS, or the fraction of the grid cells

1579
01:20:24,500 --> 01:20:26,660
that is covered by CIS.

1580
01:20:26,660 --> 01:20:29,780
And this is what we want to adjust.

1581
01:20:33,660 --> 01:20:36,860
Well, we do that looking at the ensemble mean forecast.

1582
01:20:36,860 --> 01:20:39,340
So the adjustment is not done to the ensemble itself,

1583
01:20:39,340 --> 01:20:41,780
it's done to the ensemble mean.

1584
01:20:41,780 --> 01:20:44,300
And the question is, why do we have to do that?

1585
01:20:44,300 --> 01:20:45,660
I mean, after all, we are even doing

1586
01:20:45,660 --> 01:20:49,260
an online bias optimization or bias correction

1587
01:20:49,260 --> 01:20:50,540
of my model.

1588
01:20:50,540 --> 01:20:55,140
So still, we do need to adjust those forecasts,

1589
01:20:55,140 --> 01:20:58,180
because as we know, we have several sources of error,

1590
01:20:58,180 --> 01:21:00,940
structural errors, errors due to initialization,

1591
01:21:00,940 --> 01:21:02,260
and so forth.

1592
01:21:02,260 --> 01:21:04,860
And typically, this is done by doing

1593
01:21:04,860 --> 01:21:08,460
some climatological bias correction to those forecasts.

1594
01:21:08,460 --> 01:21:10,900
Now here, just to give you an example,

1595
01:21:10,900 --> 01:21:14,300
I'm showing you the September CIS concentration

1596
01:21:14,300 --> 01:21:17,540
over the time period 2006 to 2020.

1597
01:21:17,580 --> 01:21:20,500
On the left, these are the very fine observations

1598
01:21:20,500 --> 01:21:25,820
that we use, which comes from NOAA data products.

1599
01:21:25,820 --> 01:21:28,820
And here, again, this is the CIS concentration,

1600
01:21:28,820 --> 01:21:31,180
which has value from 0 to 1.

1601
01:21:31,180 --> 01:21:35,220
And we see on the right, the growth forecast

1602
01:21:35,220 --> 01:21:36,820
as it comes out of the model.

1603
01:21:36,820 --> 01:21:40,260
So this is the output directly coming from our forecast.

1604
01:21:40,260 --> 01:21:43,300
And what we see is that there is definitely

1605
01:21:43,300 --> 01:21:45,940
a strong bias that we notice, particularly

1606
01:21:45,940 --> 01:21:49,740
in the center Arctic, where we have a much lower CIS

1607
01:21:49,740 --> 01:21:53,260
concentration relative to observations.

1608
01:21:53,260 --> 01:21:56,340
Now on the right to that, we see the bias adjusted forecast,

1609
01:21:56,340 --> 01:21:59,700
in which we compensate or we subtract the bias,

1610
01:21:59,700 --> 01:22:01,660
compute it on a previous time.

1611
01:22:01,660 --> 01:22:04,540
And then what we see is that we adjust somehow

1612
01:22:04,540 --> 01:22:08,420
that forecast by doing this simple bias correction.

1613
01:22:08,420 --> 01:22:11,060
Now still, even after doing a bias correction,

1614
01:22:11,060 --> 01:22:13,940
we see that there are some differences

1615
01:22:13,940 --> 01:22:16,260
between the observed CIS concentration

1616
01:22:16,260 --> 01:22:18,180
and the one that is bias adjusted.

1617
01:22:18,180 --> 01:22:20,620
So that tells us that we need to do something else in order

1618
01:22:20,620 --> 01:22:23,420
to actually improve our forecast.

1619
01:22:23,420 --> 01:22:25,260
And for that, and this is now the project

1620
01:22:25,260 --> 01:22:27,780
that we are working on, is to use this machine learning

1621
01:22:27,780 --> 01:22:31,940
or deep learning method to improve even further

1622
01:22:31,940 --> 01:22:33,540
those forecasts.

1623
01:22:33,540 --> 01:22:35,180
The method that I'm going to use here,

1624
01:22:35,180 --> 01:22:37,140
and I'm going to talk a little bit more about that

1625
01:22:37,140 --> 01:22:40,420
in the few slides, is the unit.

1626
01:22:40,420 --> 01:22:42,540
Probably most of the people in the audience

1627
01:22:42,540 --> 01:22:44,020
are familiar with units.

1628
01:22:44,020 --> 01:22:46,300
As I said, I will talk a little bit more about that.

1629
01:22:46,300 --> 01:22:51,220
But here, I'm just showing you some results in which you see

1630
01:22:51,220 --> 01:22:57,020
how the unit is able to reproduce the spatial pattern a lot

1631
01:22:57,020 --> 01:23:01,500
better than what we can do with a simple bias correction.

1632
01:23:01,500 --> 01:23:04,220
I should have said that this is a forecast done

1633
01:23:04,220 --> 01:23:05,580
at two monthly time.

1634
01:23:05,580 --> 01:23:07,580
So this is the forecast two months

1635
01:23:07,580 --> 01:23:10,220
after the forecast is initialized,

1636
01:23:10,260 --> 01:23:17,420
where the biases are, you can see, are particularly strong.

1637
01:23:17,420 --> 01:23:21,700
Now, the unit, this is the tool of choice.

1638
01:23:21,700 --> 01:23:25,620
What we have here is a fully connected network,

1639
01:23:25,620 --> 01:23:27,660
or a fully convolutional network,

1640
01:23:27,660 --> 01:23:33,460
that has a downscaling, down sampling encoder,

1641
01:23:33,460 --> 01:23:38,460
followed by up sampling decoder.

1642
01:23:38,460 --> 01:23:39,900
This is a classical unit.

1643
01:23:39,900 --> 01:23:43,860
What we see is that the future maps are reduced in size.

1644
01:23:43,860 --> 01:23:48,740
So we see that the resolution is reduced by its health,

1645
01:23:48,740 --> 01:23:50,580
and the channels are increased by two.

1646
01:23:50,580 --> 01:23:53,900
And this allows us to have a better representation

1647
01:23:53,900 --> 01:23:57,820
of capacity of the network, while preserving some information

1648
01:23:57,820 --> 01:23:59,980
of the image that we input.

1649
01:23:59,980 --> 01:24:03,900
So to be clear, what we input here is our raw forecast,

1650
01:24:03,900 --> 01:24:07,220
which is denoted by the YMN.

1651
01:24:07,300 --> 01:24:10,300
Thanks, and will be the resolution of my forecast,

1652
01:24:10,300 --> 01:24:12,540
which is a function of the initial month

1653
01:24:12,540 --> 01:24:17,220
and the target month, and the time relative to the first month

1654
01:24:17,220 --> 01:24:20,900
in the data that we are inputting.

1655
01:24:20,900 --> 01:24:24,020
So the input is made of six channels,

1656
01:24:24,020 --> 01:24:28,100
one channel that is the actual variable

1657
01:24:28,100 --> 01:24:31,500
that we want to, or the actual map that we want to correct,

1658
01:24:31,500 --> 01:24:33,980
plus five temporal features that takes into account,

1659
01:24:33,980 --> 01:24:36,340
again, the initial month that we're looking at,

1660
01:24:36,380 --> 01:24:38,540
the target month, and this temporal information

1661
01:24:38,540 --> 01:24:40,460
relative to the initial time.

1662
01:24:40,460 --> 01:24:43,420
So in other words, how many years and how many months

1663
01:24:43,420 --> 01:24:45,500
from the starting of your data.

1664
01:24:45,500 --> 01:24:47,340
You input that into the network,

1665
01:24:47,340 --> 01:24:49,660
and then the output is, hopefully,

1666
01:24:49,660 --> 01:24:51,500
is an adjusted forecast,

1667
01:24:51,500 --> 01:24:54,180
which then correct for those biases

1668
01:24:54,180 --> 01:24:55,580
that I mentioned earlier.

1669
01:24:57,700 --> 01:24:59,580
Moving a little bit forward here,

1670
01:24:59,580 --> 01:25:01,340
let me just be a little bit more precise

1671
01:25:01,340 --> 01:25:03,500
on the kind of task that we are doing.

1672
01:25:03,500 --> 01:25:04,940
This is just a specific example,

1673
01:25:04,940 --> 01:25:08,700
which hopefully will clarify how we do this.

1674
01:25:08,700 --> 01:25:11,100
So in this case, let's say that we want to adjust

1675
01:25:11,100 --> 01:25:13,700
the March CIS concentration forecast,

1676
01:25:13,700 --> 01:25:17,060
which is initializing February of a year Y.

1677
01:25:17,060 --> 01:25:20,500
So essentially it will be this red dot denoted here

1678
01:25:20,500 --> 01:25:21,700
on the figure.

1679
01:25:21,700 --> 01:25:25,900
So again, what we want to do is to post-process this forecast.

1680
01:25:25,900 --> 01:25:27,740
We leverage the forecast that are produced

1681
01:25:27,740 --> 01:25:28,900
with our climate model.

1682
01:25:28,900 --> 01:25:30,620
We do not do prediction.

1683
01:25:30,620 --> 01:25:33,060
We just do that kind of bias adjustment

1684
01:25:33,060 --> 01:25:34,980
or post-processing of the predictions

1685
01:25:34,980 --> 01:25:36,980
that we get from our climate model.

1686
01:25:36,980 --> 01:25:41,300
So for this specific task, to correct that March CIS,

1687
01:25:41,300 --> 01:25:46,180
we train on the data that is available for the years

1688
01:25:46,180 --> 01:25:48,900
before the test year that we have.

1689
01:25:48,900 --> 01:25:50,700
So in this case, I'm denoting that here

1690
01:25:50,700 --> 01:25:53,020
for this in this shadow region.

1691
01:25:53,020 --> 01:25:57,260
So we take all the pairs of forecast and observations

1692
01:25:57,260 --> 01:25:59,980
for all lead times and all target month,

1693
01:25:59,980 --> 01:26:02,460
and then we train our network on that data,

1694
01:26:02,460 --> 01:26:05,260
and then we will do that iteratively for every test

1695
01:26:05,260 --> 01:26:10,060
years that we want to make the adjustment.

1696
01:26:10,060 --> 01:26:12,780
So what I'm going to do now is to show you some of the results

1697
01:26:12,780 --> 01:26:13,780
that we have.

1698
01:26:13,780 --> 01:26:18,900
I should say that this is very much a work in progress.

1699
01:26:18,900 --> 01:26:23,580
And that is different avenues that we are working on,

1700
01:26:23,580 --> 01:26:27,140
but we have some results that I want to share with you.

1701
01:26:27,140 --> 01:26:31,060
For instance, what we see here is the CIS concentration bias.

1702
01:26:31,060 --> 01:26:35,940
At the zero-month lead, average over the 2006 and 2020 time

1703
01:26:35,940 --> 01:26:41,180
period, which is the test years that we have for our analysis.

1704
01:26:41,180 --> 01:26:45,740
At the top, we have the bias for the March CIS concentration,

1705
01:26:45,740 --> 01:26:49,900
which is at the time of maximum CIS extent.

1706
01:26:49,900 --> 01:26:52,740
And at the bottom, we have the results or the bias

1707
01:26:52,740 --> 01:26:55,780
for the September CIS concentration,

1708
01:26:55,780 --> 01:26:59,140
which is at the time of a minimum CIS extent.

1709
01:26:59,180 --> 01:27:02,300
On the left, we see here what happened with the raw forecast.

1710
01:27:02,300 --> 01:27:06,700
We see the biases happening in several regions,

1711
01:27:06,700 --> 01:27:08,020
which is actually substantial.

1712
01:27:08,020 --> 01:27:10,460
Here, the bias is given in percent.

1713
01:27:10,460 --> 01:27:12,060
For the bias adjusted case, which

1714
01:27:12,060 --> 01:27:15,100
is the simple bias correction that I'm using here

1715
01:27:15,100 --> 01:27:17,420
as a benchmark, we see that we do improve

1716
01:27:17,420 --> 01:27:19,420
relative to the raw forecast, but there are still

1717
01:27:19,420 --> 01:27:22,180
some biases that are apparent, particularly

1718
01:27:22,180 --> 01:27:24,380
during the CIS minimum.

1719
01:27:24,380 --> 01:27:26,620
And then on the right is the results

1720
01:27:26,620 --> 01:27:27,860
that we get with the unit.

1721
01:27:27,860 --> 01:27:31,660
And we see that the bias are likely removed.

1722
01:27:31,660 --> 01:27:33,620
And now, this is for the zero-month lead.

1723
01:27:33,620 --> 01:27:37,700
So this is soon after we initialize our forecast.

1724
01:27:37,700 --> 01:27:39,820
Now, two months ahead in our forecast,

1725
01:27:39,820 --> 01:27:42,300
of course, the biases are increased.

1726
01:27:42,300 --> 01:27:45,020
Excuse me, we have two minutes.

1727
01:27:45,020 --> 01:27:45,540
Two minutes.

1728
01:27:45,540 --> 01:27:47,460
Thank you.

1729
01:27:47,460 --> 01:27:49,060
All right, so here is the case in which we

1730
01:27:49,060 --> 01:27:50,580
have two-month lead forecasts.

1731
01:27:50,580 --> 01:27:52,460
And we see that the bias, of course, are increased.

1732
01:27:52,460 --> 01:27:55,660
But still, we are able to manage those biases with the unit,

1733
01:27:55,700 --> 01:28:01,180
given a better representation of the CIS edges.

1734
01:28:01,180 --> 01:28:03,060
Another measure that we look at here

1735
01:28:03,060 --> 01:28:07,460
is the integrated ICH error, which is essentially the area,

1736
01:28:07,460 --> 01:28:09,220
the integrated area that we have,

1737
01:28:09,220 --> 01:28:12,460
in which both forecast and observations

1738
01:28:12,460 --> 01:28:16,380
disagree in the concentration with a threshold of 15%.

1739
01:28:16,380 --> 01:28:21,020
So we both have more than 50% concentration

1740
01:28:21,020 --> 01:28:23,140
or less than 50%.

1741
01:28:23,140 --> 01:28:24,860
This binary error will be zero.

1742
01:28:24,900 --> 01:28:27,060
If they are different, then the binary error will be one.

1743
01:28:27,060 --> 01:28:32,140
And those grid cells will contribute to this area error.

1744
01:28:32,140 --> 01:28:35,140
So this is at the top here, we see a hit map

1745
01:28:35,140 --> 01:28:38,540
in which we have our target month in the X axis.

1746
01:28:38,540 --> 01:28:40,940
And on the Y, we have the lead month.

1747
01:28:40,940 --> 01:28:44,860
And bottom message here is that the blue are bad.

1748
01:28:44,860 --> 01:28:48,700
The red are good, meaning there is less error.

1749
01:28:48,700 --> 01:28:52,140
And the unit bids both the row, of course,

1750
01:28:52,140 --> 01:28:54,500
and the bias are used to forecast.

1751
01:28:54,500 --> 01:28:56,740
Down here is just an integration over lead month

1752
01:28:56,740 --> 01:28:58,580
just to give a more clear picture

1753
01:28:58,580 --> 01:29:02,140
of how the unit outperforms the alternative.

1754
01:29:03,180 --> 01:29:04,740
Now, I know that I don't have much time,

1755
01:29:04,740 --> 01:29:09,060
so I will not go into the details of these results,

1756
01:29:09,060 --> 01:29:11,460
but at least to give you an idea of what is happening.

1757
01:29:11,460 --> 01:29:14,620
In this case, we are looking at the CIS area.

1758
01:29:14,620 --> 01:29:15,780
Same time of floods.

1759
01:29:15,780 --> 01:29:20,380
Again, red means that we have a better room mean

1760
01:29:20,380 --> 01:29:21,900
and square error, which is the measure

1761
01:29:21,980 --> 01:29:24,820
that we are using here for the CIS area.

1762
01:29:24,820 --> 01:29:27,100
Blue means that the errors are increased.

1763
01:29:27,100 --> 01:29:29,660
And in this particular case, we see that the unit

1764
01:29:29,660 --> 01:29:33,420
is slightly better than the bias corrected one.

1765
01:29:33,420 --> 01:29:35,900
However, if we look at CIS extent,

1766
01:29:35,900 --> 01:29:40,260
which is now the area in which we count for all those grid cells

1767
01:29:40,260 --> 01:29:42,620
with concentration greater than 50%,

1768
01:29:42,620 --> 01:29:45,140
we see that unit largely outperform

1769
01:29:45,140 --> 01:29:46,580
this bias-adjusted method.

1770
01:29:47,580 --> 01:29:52,740
Finally, I want just to mention that here,

1771
01:29:52,740 --> 01:29:55,700
we have seen different measures of the skill

1772
01:29:55,700 --> 01:29:58,980
in which we outperform both the benchmark

1773
01:29:58,980 --> 01:30:01,740
and the raw forecast, but there is still some issues

1774
01:30:01,740 --> 01:30:04,980
in terms of the representation of the temporal dependence

1775
01:30:04,980 --> 01:30:06,500
of our adjusted forecast.

1776
01:30:06,500 --> 01:30:10,180
And so, but unlike for this slide here,

1777
01:30:10,180 --> 01:30:12,860
just to mention that we still have some work to do

1778
01:30:12,860 --> 01:30:17,380
to be able to better represent the temporal dependence

1779
01:30:17,380 --> 01:30:19,220
of those forecasts.

1780
01:30:19,220 --> 01:30:21,060
And because I don't have much more time,

1781
01:30:21,060 --> 01:30:24,300
I will finish with this slide, which is some final remarks.

1782
01:30:24,300 --> 01:30:25,900
We'll leave it there for you.

1783
01:30:25,900 --> 01:30:28,940
And I will be happy to take any questions.

1784
01:30:28,940 --> 01:30:29,780
Thank you.

1785
01:30:31,620 --> 01:30:32,940
Thank you, Renel.

1786
01:30:32,940 --> 01:30:37,940
I saw that there seems to be to have a seasonal pattern

1787
01:30:38,260 --> 01:30:39,780
in your verification.

1788
01:30:39,780 --> 01:30:42,980
That leads me to the questions.

1789
01:30:42,980 --> 01:30:47,180
Are there any biases that are harder to adjust

1790
01:30:47,180 --> 01:30:48,420
with this method?

1791
01:30:50,420 --> 01:30:52,340
The answer for that, yes.

1792
01:30:52,340 --> 01:30:56,220
And this is anality that we see has to do in part

1793
01:30:56,220 --> 01:30:58,020
with how the eyes behave.

1794
01:30:58,020 --> 01:31:01,100
We have what is known as this predictability barrier

1795
01:31:01,100 --> 01:31:03,180
after spring, which makes it difficult

1796
01:31:03,180 --> 01:31:05,740
to do a good prediction of the sea eyes.

1797
01:31:05,740 --> 01:31:07,580
This is something that perhaps we can see

1798
01:31:07,700 --> 01:31:11,380
all those metrics, actually perhaps we'll go to this one.

1799
01:31:11,380 --> 01:31:15,260
So this barrier, which reduces the skill

1800
01:31:15,260 --> 01:31:16,620
that we have in our predictions,

1801
01:31:16,620 --> 01:31:20,740
can be seen here around the month of June to October,

1802
01:31:20,740 --> 01:31:22,540
looking here at target month.

1803
01:31:22,540 --> 01:31:26,260
And so that is something within the raw forecast

1804
01:31:26,260 --> 01:31:29,740
in person, the bias corrected one, but also in the unit.

1805
01:31:29,740 --> 01:31:32,100
So we do improve on those months,

1806
01:31:32,100 --> 01:31:35,060
but still there is some skill that is missing there,

1807
01:31:35,060 --> 01:31:37,140
which is part of the natural process

1808
01:31:37,140 --> 01:31:39,180
in the sea eyes formation and melting,

1809
01:31:39,180 --> 01:31:40,700
which then translate into the skill

1810
01:31:40,700 --> 01:31:42,540
that we see in our just forecast.

1811
01:31:43,700 --> 01:31:45,020
Thank you.

1812
01:31:45,020 --> 01:31:49,620
One last question is coming from the online Q&A.

1813
01:31:50,580 --> 01:31:53,300
Would training the model on all months of the year

1814
01:31:53,300 --> 01:31:55,820
versus only certain months affect its ability

1815
01:31:55,820 --> 01:31:56,900
to improve the forecast?

1816
01:31:56,900 --> 01:32:00,060
For example, forecasts of sea eyes in spring

1817
01:32:00,060 --> 01:32:01,620
are known to perform poorly.

1818
01:32:01,620 --> 01:32:03,700
So if you were to exclude these months,

1819
01:32:04,220 --> 01:32:07,340
would this improve the bias adjustment?

1820
01:32:07,340 --> 01:32:09,260
Yeah, that's a good question.

1821
01:32:09,260 --> 01:32:11,100
Actually, when I was, I mean, it's too bad

1822
01:32:11,100 --> 01:32:12,660
that I'm rushing through all these slides,

1823
01:32:12,660 --> 01:32:14,500
but one of the things that I wanted to mention

1824
01:32:14,500 --> 01:32:18,140
at this particular slide is that we do the training.

1825
01:32:18,140 --> 01:32:21,140
You're looking at the previous years in my forecast,

1826
01:32:21,140 --> 01:32:22,820
but we are missing some of the information

1827
01:32:22,820 --> 01:32:26,460
of more recent months that can also contribute

1828
01:32:26,460 --> 01:32:27,300
to the forecast.

1829
01:32:27,300 --> 01:32:29,060
Like looking here, for instance,

1830
01:32:29,060 --> 01:32:31,740
we do not use information from January

1831
01:32:31,740 --> 01:32:33,500
of this specific year.

1832
01:32:33,500 --> 01:32:35,100
Now, one thing that we are exploring

1833
01:32:35,100 --> 01:32:38,060
is to look at training based on lead times

1834
01:32:38,060 --> 01:32:39,060
or a specific month.

1835
01:32:39,060 --> 01:32:41,620
I think that that's the idea of the question,

1836
01:32:41,620 --> 01:32:45,820
which we try to train our model specific to the lead times

1837
01:32:45,820 --> 01:32:48,540
in which we have the biases that we want to correct

1838
01:32:48,540 --> 01:32:53,020
and the kind of information specific to the seasonality

1839
01:32:53,020 --> 01:32:54,700
that we want also to correct.

1840
01:32:54,700 --> 01:32:56,740
So those are things that we are exploring,

1841
01:32:56,740 --> 01:32:59,580
but so far this is what we have.

1842
01:33:00,580 --> 01:33:02,580
Thank you, Rénel.

1843
01:33:02,580 --> 01:33:07,580
This concludes our first block for the first part

1844
01:33:07,580 --> 01:33:10,580
of the presentation for artificial intelligence.

1845
01:33:10,580 --> 01:33:13,580
A big thank you to Rénel, Madlena, Saïd, Hervé,

1846
01:33:13,580 --> 01:33:17,580
and Christopher for having made these presentations.

1847
01:33:17,580 --> 01:33:19,580
Anne, I'll leave you here.

1848
01:33:19,580 --> 01:33:23,580
Yes, a big thank you to all the presenters,

1849
01:33:23,580 --> 01:33:25,580
all the attendees as well.

1850
01:33:25,580 --> 01:33:28,580
And as several mentioned before,

1851
01:33:28,580 --> 01:33:31,580
there is a second session coming up in about 20 minutes,

1852
01:33:31,580 --> 01:33:32,580
not even.

1853
01:33:32,580 --> 01:33:35,580
So we'd love to see you back here,

1854
01:33:35,580 --> 01:33:40,580
a lot more to see and to hear from people outside ECCC

1855
01:33:40,580 --> 01:33:41,580
as well.

1856
01:33:41,580 --> 01:33:42,580
So please do join us.

1857
01:33:42,580 --> 01:33:43,580
Thank you.

1858
01:33:43,580 --> 01:33:44,580
There's not a sign.

1859
01:33:44,580 --> 01:33:47,580
There's not a sign, that's it.

1860
01:33:47,580 --> 01:33:48,580
Thanks.

1861
01:33:48,580 --> 01:33:49,580
You're better.

1862
01:33:49,580 --> 01:33:50,580
Bye.

1863
01:33:50,580 --> 01:33:51,580
Bye.

1864
01:33:55,580 --> 01:33:56,580
Thank you.

