{"text": " Let me try for you. Okay. Thank you. Thank you. Thank you. Hello and welcome. Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel Tremblay. We both work for the Meteorological Service of Canada within Environment and Climate Change Canada and we're happy to be here exploring AI with you for a second year. Before we begin, a few reminders. 15 minutes are reserved for each presentation and we encourage our presenters to keep three minutes for questions at the end. Time permitting participants with questions will be able to use the chat at the end of the presentation and their questions can be upvoted by the other attendees. Miguel will provide up at 10 and 12 minutes and will bring the presentation to an end at 15 and will be ruthless guys because we want everyone to have time to present. Miguel, do you want to do a quick recap in French? A few reminders. 15 minutes are reserved for each presentation and we encourage our presenters to keep time for questions at the end of the presentation and we will be able to use the chat at the end of the presentation and we will be able to use the chat at the end of the presentation and we will be able to use the chat at the end of the presentation and we will be able to use the chatfpspe conditional 10 minutes for one and a half minutes for 10 minutes for weather forecasting, and ECCC's research plans. So without further ado, over to you, Christopher. Thank you. Let me get the screen sharing going and share. Okay, yep, looks like we're good. Okay, hello everybody. I'm Christopher Subic from Environment and Climate Change Canada. I'm here to speak as said about, well, click, okay. This talk is essentially in two parts. The first part is going to be a brief summary of the current state of machine learning based forecasting with a broad focus on medium range weather forecasts. Second part of this talk will be the forthcoming AI roadmap out of the atmospheric science of technology director at NCCMAP, which effectively broadly sets out where this part of Environment and Climate Change Canada will be going in the short of medium term future with AI. And finally, time permitting, I'll conclude with a preview of talks to come and just general thoughts on the state of AI in forecasting. So in case you haven't noticed, AI is becoming a big deal. It's no longer just pictures of cats or helping kids cheat on their homework, but it's starting to influence the industries and feels previously thought unassailable. Now, this isn't truly a stranger to weather forecasting. AI adjacent topics have long had roles in the forecast production system. We've long had statistical models for quality control of observational data and for post-processing, but a phase change has been that over the past two years, maybe three models from the academic and private sectors have gone from things that we should probably keep our eye on as interesting things for the long-term future to, well, these two nearly full-featured forecast systems that are competitive with the state of the art. With that in mind, I can just state very plainly that Environment Climate Change Canada, in particular my part of it, is taking AI very seriously and we intend for it to have an increasing role in numerical weather prediction systems going forward. We're taking a sort of trust but verify approach in that we intend to make AI advances operational, but only after they've been scientifically proven. We're not in a rush to perform science by press release and this is also a medium to long-term plan. Capacity building is going to take years, both in terms of acquiring sufficient computational power for this and developing the human expertise necessary to properly use it. The modern AI forecast field essentially began about two years ago. I dated from the first publication of GraphCast by Lam in 2023, which is the first time that an AI model could truly claim to beat the state of the art from, in this case, the equivalent forecast from ECMWF. In addition, the truly shocking claim was that these forecasts came with orders of magnitude faster running time. GraphCast will produce a 10-day quarter-degree forecast in about 30 seconds on a single GPU. That put all of the weather centers globally on notice that if these trends continue, we must adapt or we'll fall behind. And I mean, in some sense, research has always been that. You can't sit on your laurels from 20 years ago and pretend to be competitive, but this field is truly advancing at a revolutionary rate. Models are being constantly released. Any summary, like the one I'm about to give was going to be out of date within weeks. In fact, just this morning, I've had to update my slides to add two new preprints that have come out in the past four or five days. Nonetheless, there are some common features between medium-range models that are shared by, if not all of them, then most of them. Most broadly speaking, these models attempt to learn from data, which means rather than simulate the atmosphere from first principles, they try to look at some form of data and predict what it will become. In this case, for the medium-range forecasting, the data is almost always the error of five reanalysis. That's our highest quality, longest-term, and most uniform record of the atmospheric state that we have, and it's accepted as ground truth for most of the AI models, but this data set does have known issues such as relatively poor precipitation. The other common feature about AI forecast models is that most of them have sparse limited outputs. They tend to predict only a few variables. They tend to predict a small subset of vertical levels compared to what we're used to from an operational forecast, and they have a limited selection of lead times. This creates new downscaling problems. If you were in the plenary talk, you heard our friend from Nvidia say that this might not be a barrier to full prediction, but at the same time, we're used to having the prediction plus all of the rich output for free, and not having that means we'll need to contemplate new kinds of downscaling problems to get rich data from a sparse forecast. And finally, these AI forecast systems have essentially no proofs of physical consistency. Even hybrid models and development in that area has a classical dynamical core, but leaves a physics system that is only tangentially concerned with things like conserving energy. Now, medium range forecasts tend to break down into a few different categories. The first and most conventional of these are deterministic models. These are analysis predictors, and they essentially answer the question of, if I have the atmospheric analysis at time zero, what is the minimum error prediction of what that analysis is going to be six hours from now? As a general rule, these tend to give overly smooth forecasts that over long lead times, erase fine scale structure in the atmosphere. The widely held belief is that this is a consequence of training based on mean squared error measures, because the lowest mean squared error prediction in the future is your ensemble average. But the ensemble average is not a physically plausible forecast. Now, that being said, these models have shown great success, and having a really good ensemble mean prediction of the future is still a really good prediction of the future. Each day of predictability translates to billions or billions of dollars of enabled economic activity. This category of models is in some sense the oldest, and I really hesitate to use that word with a field that's about two or three years old, but they're models from many different groups. Graphcast and ForecastNet have been previously mentioned. Graphcast is a graph neural network. ForecastNet is now a spectral Fourier neural operator that operates in a spherical harmonic space. Pangu weather came out as a vision transformer, and AIFS is now apparently officially published with a pre-print, and it's a graph transformer. The jargon here is not that important, but the underlying point is that you can reach a deterministic forecast with many different AI architectures. There's, as of yet, no specific Royal Road to a forecast. The second category that I've somewhat arbitrarily divided things into are ensemble models. These try to address the problem of overly smooth forecast by adding some element of randomness. An ensemble forecaster will receive some kind of random data source, either a random number generator or a field of noise, and it's asked to generate essentially different forecasts from the same seven initial conditions. This more or less solves the problem of smoothing based on mean squared error loss functions because each individual forecast no longer has to track the long-term forecast. No longer has to track the long-term truth, but each one can be plausible, and then you're tracking the future with a true ensemble mean. Now, the downside is that these models are all more expensive to train and run the deterministic systems of the same size. In operations, you will need at least one inference run per ensemble member. So, graphcast's 30-second, 10-day forecast is great, but if I want a 100-member ensemble, now I'm talking an hour or two, and that can add up very quickly. In terms of training, if you're training with an ensemble error measure like the cumulative rank probabilistic score, that requires training over an ensemble, and increasing the size of things during training tends to increase the scarce GPU memory requirements and the overall cost of building the system. This is a newer frontier of AI forecasting. It's probably going to be more popular in the months to come, and the examples here are also somewhat newer. Gencast was previously mentioned in our plenary. It's a diffusion model based on graph transformer network. Neural GCM is a hybrid model, also previously mentioned, and it's a dynamical core that has learned physics parameterizations, and in particular, can operate in a non-subtle mode, which is why I've included it here. And finally, there are models such as seeds, also out of Google, that don't try to forecast directly, but they try to take an ensemble mean that already exists from some method, like a traditional forecasting system, and generate new ensemble members to fill out the probability distributions. The final category of forecast models, and I'm going to divide things into our foundation models, and these essentially answer the question of, what if we had GPT-4, but for weather? The idea here is and shared by foundation models that exist in our development, is that you break up a weather state, like the analysis, into tokens by regionally subdividing it usually, and then you ask the foundation model to predict missing pieces of it. You say, you can mask out the future, and say, okay, predict this, and then you're training it in a forecast context, or you can mask out a missing regional piece, and ask it to fill in the blank. You could even ask it to predict the past, I suppose. This allows the foundation model to be trained on qualitatively different data sources. For example, you might have an encoder suite that takes satellite observations directly, and tries to turn it into token space, or the analysis from era five, or lower resolution climate forecasts. And once tokenized, the idea of a foundation model is that you have a giant middle processor layer that operates in this latent space, and from there it is relatively simple to build out decoder models to give you things you want, like tomorrow's forecast today, or what the radar is going to look like in 30 minutes. Foundation models certainly prove their worth with text-based processing like GPT, and they're also very popular and emerging in Earth observation applications with interpretation of satellite data. For example, at an ECMWFESA conference a few weeks ago, there were some interesting talks about taking satellite data and using it to infer tree cover near power lines in Norway, tasks that would normally take some very expensive helicopters, and doing them much, much more cheaply with readily available Earth observation data. The downside is that foundation models tend to be extremely expensive to train, and these will push the limits of what the public sector is probably willing to spend should foundation models prove themselves for forecasting. Few of these models exist right now for NWP, but there's plenty of private sector interest. I mean, our friend from NVIDIA talked at length about that. The two models that are currently published are ATMO-REP from Christian Lesig, which is a transformer model, tested on both forecast and downscaling applications, and Microsoft has published, as of a few days ago, it's Aurora Foundation model, which most notably uses several different data sources for training, not just air five, but also climate simulations and operational forecasts, and I believe one of Noah's ensembles. Now, to integrate all of this, as a researcher, I can make a few broad predictions on where the trends are in this area. First is going to be the convergence of data simulation forecasts, nowcast and downscaling roles. Right now, the leading NWP forecast, take in an analysis and give you a forecast, but the obvious question is to what extent can the rest of the chain be included? For example, forecasts that are ensemble generating can often be reversed to perform data simulation, and a couple of references here are one, using a diffusion-based model to assimilate sparse observations, and the second one is to perform data simulation inside the latent space of an autoencoder, which effectively replaces the background error covariance matrices of a DA system with ones that are nonlinear and flow dependent from the autoencoder space. A second avenue here is to combine conventional models with an AI-based bias correction. Farshie from ECMWF has published recently on using a neural network bias correction to include some element of AI inside the IFS-40 var system. And Said, my colleague, is going to present in about half an hour on spectral nudging to bring a classical NWP system closer to an AI forecast to preserve rich data and have the accuracy of AI. And finally, there's some effort on all-in-one AI forecast systems that go directly from observations to forecasts to post-processing to produce predictions of future observations. The Vaughn 2024 is the Aurora mentioned in the plenary, which is an encoder, decoder architecture that uses Unets, and it's the obvious long-term future of foundation models. There's also rumors that Google is buying up satellite data for its next generation, GraphCast version two or something like that, but I don't have any particular confidential information to share. Okay, now the second part of this talk is the Environment and Climate Change Canada AI roadmap. This is a joint product of the AASTD and CCMAP. It's a product of an internal Tiger team across the research and operational divisions that was put together last fall after an internal study on the current state of AI and weather forecasting, where we had a whole lot of talks like the first half of what I just gave. This roadmap is designed to set very broad research priorities. It's not designed to currently pick and choose what projects are worth funding, but to set out how we should think about AI as an institution. And the largest theme from this roadmap is the need for capacity building, both in terms of compute capacity, in light of the supercomputer update we're likely to have next year and whatever gets procured in the years thereafter. How we should think about the use of cloud computing and finally what we need to do from a human resources and training standpoint. This is a living document. It should be published soon. I'd hoped I could begin this talk with a reference to the live document, but I think it's still in translation. But once it is published, it'll be updated every few months to every year or so with internal reviews and updates just as we understand more about AI. The broad themes of this document are how we intend to integrate AI throughout the research, development and operation cycle. So we're not limiting it to just graphcast style forecasts, but we're interested in applying AI everywhere from data gathering to post-processing. The main evaluation criteria here are the triumvirate of feasibility, service and efficiency. Feasibility answers the question of how capable are we of developing and running a proposed AI project? And that includes not just the scientific risk of well, putting a whole lot of time and money into a project and having it not work, but also whether or not we have the compute resources to do this or the human resources to develop and manage it. The second criterion is service of how a project will improve ECCC's weather-based services to Canadians. And that is not just in terms of accuracy, but also whether we can make our forecast products more timely, whether we can have more numerous projects or whether we can have qualitatively new products like hypothetically, rapidly updated now casting, which we just currently don't have available for many government source. And finally, there's a question of efficiency. How will a project make efficient use of our computational resources? That includes the broad themes of energy efficiency, but also the government-specific theme of being a good steward of public funds. Supercomputers are, as one might guess, rather expensive. And we ought to demonstrate to Canadians that Canadians are getting their money's worth. Okay, so more specifically, how are we looking at AI from the observation to product pipeline? First category here are observations and data assimilation. So in observations, AI is in some sense an extension of what's already happening in terms of looking at quality control and error estimation of our inputs. AI can perhaps allow us to have some new capabilities with learned observation operators to take advantage of parameters that are, to learn parameters that are not directly observed, such as complicated satellite measurements that are non-linear features of the atmospheric state that are hard to directly measure but might be possible for an AI to learn. And finally, we have some ideas on unconventional data sources. One idea thrown around in discussions was using ad hoc webcam data like traffic cameras to evaluate real-time precipitation class information. In principle, someone can look at the feed and say, yep, it's raining, and we can have an AI do the same thing and potentially have useful data. On the data assimilation side, in some senses, this area is most advanced because DA is already using some heavy computation for its error matrix operations and they're investigating GPU use. As previously mentioned, there's a data of data assimilation in the latent space of a model. If we can vastly increase ensemble sizes through AI, we can perform better PDF estimation through particle filters or non-Gaussian-based statistics. And finally, we can start to estimate, perhaps, the model parameters themselves rather than just initial conditions. The numerical prediction side, this is closest to what I talked about previously with AI forecast models. In the near term, we're looking at implementing the AI models as they are to provide second opinions about the weather to forecasters. In the medium term, we're looking at fine-tuning large models on our operational data sets to provide better AI forecasts. And in the longer term, we're looking at structural changes and developing new models in general. In addition, we'd like to hybridize classical and WP and AI models to help fix the problem of sparse outputs. And along the way, we'd also like to investigate emulation of the physical parameterizations. For example, radiation is very expensive inside the atmospheric model and 3D radiation is probably better than 1D radiation but it's too expensive to run operationally. If we can emulate that, we can have a better parameterization that is still within our compute budget. Also, we'd like to extend this to ocean ice and land surface prediction, but that is still fairly preliminary in part because the data sources aren't quite as complete. Okay, now in terms of the tail end of the forecast for post-processing and final products, this is the realm of downscaling and now casting. We have a 2.5 kilometer high resolution regional system but it's just too expensive to run too often. If we can downscale, we can potentially achieve that resolution of output and evaluate extremes and risks at that scale without being limited by melting the supercomputer. It would also be really nice if we could have near real-time assimilation of weather station radar data to improve the half hour, one hour forecast, which is not something we can currently do very well. In terms of post-processing, the statistical post-processing systems have all long had systematic error adjustments and station specific adjustments for representative this errors and the like and AI can help turn that into better nonlinear corrections. And finally, in terms of the expert product sides, we'd like to have better high impact weather diagnostics with well-calibrated forecasts for all of the big ones, tornadoes, hail, blizzards that are just don't show up in the larger scale forecast but are extremely important for the people stuck in them. Excuse me, Christopher, so only two minutes left before. Yes, okay, I am going to be very quick. Okay, challenges and opportunities. As I've hinted at the entire time, we have challenges and these are opportunities. On the physical side, we have limited compute capacity. GPU compute demands are only going to go up. We don't have very many of them. We're going to probably get more in the future but we need to manage them. We also need to care about data management in terms of not just having an archive that exists on tape but one that is living and can answer training-based questions very quickly. In terms of HR, we have similar problems that our researchers are all very good but they're also not necessarily well-trained on AI. We need to close that gap. And in particular, we would love to have increased collaboration with both the ivory tower and private sector. Okay, the roadmap sets out targets and milestones. We would like to have our first operational AI systems for IC innovation cycle five, which is targeted early 2026 after the supercomputer update. The current innovation cycle is just closing because it's a bit too early for it. And ultimately we'd like to have AI as just another forecast tool by 2030 or so. Okay, I would love to talk more about this but unfortunately there's no time but in general, our divisions have all been investigating how we can integrate AI into research flows. Some projects have started, some are waiting for people and some just simply need more resources that we don't currently have and we would love to collaborate on them. If you have AI skill and you have a weather-related project, please email someone at our division. We would probably love to talk to you. Unfortunately, I have to cancel this slide where I was going to talk up all of my colleagues' presentations to come. Please stick around, they are going to be great. And finally, conclusions-wise, AI is rapidly advancing the state of the art in numerical weather computation. This is a phase change of forecasting and I'm excited to see where it goes. We will make AI and machine learning technologies a major part of our systems as they prove themselves. But we're a public service organization, we recognize that we have to be very careful about what we stand behind operationally. And finally, I hope these slides are available afterwards for the references. There are references to all of the systems I've mentioned. And I'd also just like to highlight that of the 14 references here, about eight are preprints. This is a really, really rapidly moving field. Thank you, I am... Missy Christopher. We have time maybe for one question. I know. I'm going to take the highest ranked one. It's a long one, are you ready? Yes, yes. Perfect. Currently, AI seems to be quite expensive and rapidly developing. While we were waiting for AI-based models to build better foundations and training to replace numerical weather prediction or for climate studies, can we exploit its computational speed in the near term for now casting or HRR-like output? For example, ECCC can incorporate into CAM for very short thunderstorm prediction or perhaps weather elements on grid now cast. Okay, I believe these projects are under investigation. I'm on the medium range forecast side, so it's not my particular side, but there's a presentation in this session, I believe after lunch that investigates now casting via an IBM Foundation model. And I think that'll begin to answer that kind of question. In general, yes, this would be very good. In practice, the nearest term limits are probably compute potential because we have relatively few operationally available GPUs, but hopefully in the next few months to year or so, that will improve. I might sneak in another question then, Christopher, we have a minute. Have you considered the role that Canadian industry will have in developing AI capacity at ECCC? We would love collaborations from industry. That is my politically correct and also true answer. We have limits on our resources in part because until, well, procurement thus far has been focused on making our operational systems better for obvious reasons. And the cycles of this mean that it is practically difficult to, sorry, I'm speaking as a researcher, my boss is probably listening, so there's some things I need to be very circumspect about saying. We need to be very careful about how we commit resources for training large models. Industry is, I think, a fantastic partner, both for the potential of having compute resources we could borrow and also a better focus on some of the most downstream applications. For example, our leading talk that opened CMOS was on the weather impacts on the insurance industry. And to the extent we can be of value there, I think there's room for joint products. Okay, so I will try to answer other questions in the chat as we continue. But otherwise, I will stop sharing, meet myself and thank our hosts. Well, thank you, Christopher. That was an amazing feat. You put in two very large presentations into one 30-minute presentation. You have all my admiration and thanks for doing that. Congratulations, actually, in 30 minutes. Okay, and now we're on the subject of airvests. So we're not going to take any longer. I'm going to introduce you to Airvests GLaPalm, who comes from Canada as well, a researcher. He's going to introduce us to the NWPEI-based model, the verification against the observations of airvests and airvests. It's up to you. Hello. I don't know. So you hear me. Very well. Do you see my screen? Also. Wonderful. So I'm going to do the presentation in French. If you have any questions in English, there's no problem. If you have any questions in English, there's no problem. It would be easier for me if I do it in French and for you also. So I'm here to present the work that I did in collaboration with my colleagues on the evaluation of the models based on the airCCC in a second. I don't know. So the context is that with the emergence of these models, we realized that we had to check these models with our traditional verification methods, which allow us to evaluate the innovations that are made on traditional models. So I was asked by my boss to install, turn and check these models on our HPC installations. And I started this work in October 2023. And I worked on it until April 2024. So what I want to present to you is just that. So the activities that were completed during this special project there, it's that we turned, we chose two models, ForecastNet and Graphcast, which were available for free. It's easy. And, well, ForecastNet, Christopher, we talked about it a little bit earlier. So it's a model that was developed by Renvidia. And then we turned two graphs of Graphcast, one with 13 levels of pressure, with a roof of 50 hectopascals and a version at 37 levels, a roof of 1 hectopascal. And we turned each model with three analysis sets. One, the first is the operational analysis of the OVF, called IFS, on three levels only. We also used R5. So the R5 analyses are the ones that were used to train these models. We were able to turn the configurations at 13 and 37 levels. And of course, we wanted to compare the operational provisions with the same analysis. So we started these models, these AI models, with the operational analysis of CCC. And we turned in two real-time modes, where we turned twice a day, at the same time as the operational model. And like that, the operational metrologists can compare the Forecasts based on AI with the operational provisions. And also, on my side, I did an evaluation on a period of one year, which allows us to see what the models are like. So here, I put a slide on the description of the models. It's very precise. I don't have much time. Basically, the two Graphcast and Forecast Net models use about the same information. But I put a lot of detail there to be complete. But I don't think I'll be able to save a little time. So one of the advantages of AI models is their informatic efficiency. If we compare the operational model, presently, it takes a little less than an hour, more than 6,000 CPUs. So it's a big deal. So it generates 500 gigabytes of data at each provision twice a day. It makes outings at all ages up to 10 days. And then it's models at 15 kilometers of resolution with a lot of vertical resolutions. AI models have less good resolutions. They release data at 6 hours. And a less good vertical resolution too. So, but Forecast Net is very, very light. It's impressive. It takes 20 minutes on a CPU. I don't speak of a GPU here. I have a GPU even faster, of course. But on a CPU, you can turn it on. You can turn it on on your laptop and it works. And it's very fast. It still gives you a good preview. And Graphcast, it's a little bit more expensive. The confidence at 13 levels requires 100 gigabytes of memory. So it's a little bit more expensive. But still, comparing the operational model, it's very, very much, much smaller. Smaller orders. And I invite you to the second presentation of Christopher Subick on exactly comparing the computer performance between AI models, Graphcast and GEM, the operational model. And it makes a very good comparison. If you're interested, I invite you to have this presentation on your computer this afternoon. So what does it look like as a verification once we've done the average over a year? So here, I have several curves. Here, I present the errors, the prediction of the geopotential at 55 to Pascal. So these are the errors. The very thick blue gray here, that's the baseline. So that's the operational preview. So we see the errors that are missing from 0 to 240. And the other curves, it's all the previews, the verification, the AI models. So we can look at them and then, as these are errors, but the closer we are to 0, the better it is. So we see here a group of previews. That's the forecast net. So we see that for the GZ500, the forecast net is less good than the operational preview. On the other hand, all the other curves, the five other curves, it's all the previews made by Gravcast using different analyses. So we see that Gravcast, from day five, is better than the operational model, no matter the analysis we give him. So that's when using the 05 and FS analyses, it's better. I don't know, but what's important here, what's interesting here is that the two curves here, orange and green, the previews are initialized with the same analysis as the blue curve. So we see that with equal information, Gravcast is better from day five on the GZ500. If we look at another variable, which is the temperature at 850 tectopascals, so it's the same colors, the same curves, in this case, we see that from 72 hours, all the AI models are the operational model, even for the GZ500, but we still see that Gravcast is the best model, this variable. So I showed you two variables. So we can conclude that Gravcast is better than for the GZ500, we will focus on that from now on. And then what does it look like for other variables, like wind, humidity. So here, I present you graphics, it's vertical profiles. Here, in the Y axis of the vertical coordinate curve, we talk about the surface, from 1,000 tectopascals to 50 tectopascals. And so the clean curve, it's the error's quarter, according to the variable. And the tight curve is the bias. And the different variables are the following. Here, on the top left, we have the meridian wind, the wind component, the wind module here on the right. Here, can you do this for us in the next two minutes? Yes, that's it. Yes, I'll go faster. So what I wanted to show you, here we have the Gravcast configuration, so at 13 levels, 37 levels. So what I wanted to show you is that the red curve is always better than the blue curve, so the model has all the variables, all the failures, not all the failures, but all the variables, all the levels. We see that Gravcast is better. So are these two models not different resolutions? Is the fact that Gravcast is a bigger resolution, is it the advantage? Is it the advantage compared to the operational model? So what we did is that we did the same verification that I presented to you earlier, but we filtered it. We filtered, we removed all the scales that were smaller than 1,000 km, and we kept all those that were 2,000 km. So I have two graphics to present here. So on the left, it's the same graphic that I presented to you earlier, not filtered, and on the right, it's the filtered previews. So that's on the average, on the winter, it's not on the full year. So we see that even if filtered, Gravcast is better. So that's going to bring to the work of Spectreur Nodging of Syed, who will present in the next presentation. So and for the summer, it's a little less spectacular, but we still see that Gravcast has a lot of good information. So what are the future activities to do more verification? We even have an internal site that allows us to visualize these previews day by day. And I invite you to have the next seminar of Syed about Spectreur Nodging, which is a very interesting approach to integrate physical models and AI models. And Christopher Subick's work on entering Gravcast with the operational analysis that we have done internally so that it can be better adapted to our model. Thank you. Hi, thank you, Herv\u00e9. Hi, I have a question for you. There, you made the internal models run, you installed them to do the verification, but there are more and more models, there are almost two days, is there perhaps a way to have verification, counter-observation, like that, in a standardized way, or each time, you will have to install the models and then the scoring themselves? Well, listen, that's what the software that we use to do the verification and counter-observation works very well only locally. So it's difficult to publish that externally. On the other hand, running these models, if it's not done very, very easily, I worked for three months in the middle of the day just to start these models. So I imagine that each model has its own peculiarities. So it's not obvious, it's not as plug-and-play that we believe in. There's still a lot of work to be done. So if there are all the two, all the two weeks, it will be difficult to do this same evaluation to follow the run. Thank you, I don't know if there are other questions. Maybe a word for advanced systems. If you could reset the Q&A on EventMobi because I still see the questions that have been asked to Christopher Subic. So it's hard to see which one. I have one for you, Herv\u00e9. How do artificial intelligence models behave in complex mountainous terrain? I didn't evaluate that. That's more what my colleague, Marc Verville, did for verification on the surface. I don't have any memory of the results. My focus was really on the verification in addition. But of course, having a good resolution will certainly not be a problem. Perfect. Thank you very much, Herv\u00e9. Thank you. Thank you, Herv\u00e9. We, so we're going to continue. This time we're inviting Syed Zahid Hussein, who's a research scientist at ECCC. He will be presenting leveraging data-driven weather emulators to guide physics-based numerical weather prediction models, a fusion of forecasting paradigms. So without further ado, Syed, this is your turn. Thank you, Rand. Hello, everyone. In my presentation today, I will be talking about how we can leverage the strengths of data-driven weather models to improve predictions from NWP models. This is a work that we have been doing recently. And here is a list of my principal collaborators and the others who have contributed to this research. As we know, the current state of the earth for operational weather forecasting is based on physics-based NWP models. However, we have seen these presentations earlier today from the plenary to the previous two presentations that we have recently seen the emergence of new data-driven models for predicting weather. And most of these models are using some form of deep neural network to emulate the training data. And the training data generally is RFI re-analysis. So we also can call them artificial intelligence-based weather emulators. And recently, they have started to gain prominence and started to also challenge the existing forecasting paradigm. Because as you have seen in the previous presentation from my colleague, Erveg, that these data-driven models can produce forecast orders of magnitude faster with minimal computational resources compared to the traditional NWP models. And also, they can be highly competitive against state of the art NWP models in terms of their accuracy. However, despite their strengths, strictly when I'm talking in the deterministic sense for these AI-based models, they have their limitations also. And one of the most widely known limitation is considerable smoothing of fine scales, particularly for longer lead times. Also, they only offer a limited range of forecast fields and improving nominal resolution of these AI models not straightforward. They can be quite challenging. So the objective of our research was to see if we can leverage the strengths of these AI-based models to improve the predictability of an NWP model. And for that, we chose or selected like the GEM model, which is used operationally as the NWP model, operationally by Environment Canada, and the GraphCast model from Google DeepMind as the AI-based model. And the nominal grid resolutions of GraphCast and the GEM-based Global Deterministic Prediction System, or GDPS, are approximately 25 kilometers and 15 kilometers, respectively. And in the previous presentation, AirVig has shown that the GraphCast actually poses more skilled large scales compared to our GDPS. But if we want to leverage the information from GraphCast, we need to know about the effective resolution of GraphCast so that we can see what are the scales that we can really utilize for improving NWP model. And in order to do that, we look at the variance ratio of GDPS and GraphCast with respect to our own CMC analysis. And before I talk about anything else, I must emphasize on the fact that the version of GraphCast that we are using has not been through any fine tuning. So it has been trained by Google on emulating error-5 analysis by training with error-5 data. And we are using that GraphCast model but initializing it with our own analysis. And in these figures in this slide, I am showing the transient component of variance ratio for 500 hectopascal kinetic energy. On the left, I have for lead time 24 hours. And on the right, I have for 120 hours. In blue, I have GraphCast. And in blue, I have GDPS. And in red, I have GraphCast. And we can see by looking at the variance ratio of GDPS, the blue lines in both 24-hour and 120-hour cases, that the variance ratio is close to 1. So that means its effective resolution is not changing with respect to lead times. However, what we see with GraphCast, first of all, at 24-hour lead time, we see the scales as large as 1,500 kilometers are smoothed out to some extent. And we see considerable smoothing for scales smaller than that. And then we see when we go to 120-hour lead time, the scales that are getting smoothed out actually increases. And it affects scales as large as 2,750. So we know that large scales in GraphCast are better. But at the same time, we see that scales below 2,750 for longer lead times are problematic because of the reduced variance ratio. Now, the question is how we can really use or leverage this good large-scale information from GraphCast. And one way to do that would be to use spectral nudging, large-scale spectral nudging. And this is a very widely used idea in the field of regional climate modeling and hindcasting. Our expectation is if we nudge our gem predictions towards the large scales of GraphCast, we can improve the quality of prediction of gem. At the same time, we should be able to address the fine-scale smoothing in GraphCast while we will be able to generate the full set of focus fields that we are currently having access to through gem. The concept of nudging is quite simple, as illustrated by this equation here. So here, F is the solution of our gem model after the dynamic substep. And this term highlighted in purple color actually corresponds to the nudging increments. So we add the nudging increment to the model solution after the dynamic step to get the nudge solution, which is then fed to the physics. And then it completes the complete model time step. And then it fits back to the next dynamic step. And this is how it continues. And if we look at this nudging increment term, we have this subscript Ls, which actually implies the large scale. So we are only considering the large scales when we are applying the nudging. And this omega is a relaxation parameter. And if we break down this equation, we can see basically what we have is a weighted average of the large scales coming from GraphCast and our model, whereas the fine scales are remaining intact that is being credited by the model. So this is how we can leverage the large scale accuracy of GraphCast while allowing our model to freely evolve the small scales. And the scale separation between large and small scales, we are doing that by decomposing the nudging increments in the spectral space. Although we are applying the nudging increment in the grid point space, but we are decomposing it in the spectral space. And hence, we call it spectral nudging. How we optimize the spectral nudging configuration to support our objectives for this study is a computationally demanding task. And in a sense, it is still ongoing. I mean, we are still sort of fine tuning the configuration. But at present, the most optimal configuration that we have has these following features. We are only applying nudging to horizontal wind and temperature. And we are not nudging the stratosphere and the boundary layer for different reasons. And we are nudging scales that are larger than 2750 for obvious reasons. That should be obvious from the variance ratio comparison. And we have 12 hours as the nudging realization scale. And we apply nudging at every time step. With that, I'll be going to some of the results of verification that we will try to prove that this approach actually helps to improve the predictability of gem. We ran a series of experiments for winter and summer of 2022. So we have the control experiments where there is no nudging, the control GDPS. And then we have the GDPS with spectral nudging. So control would be, in the next few slides, all the results. Control would be shown in blue color. And the results from spectral nudging with GDPS would be shown with red. So the first scores that I want to show are verification against radios and observations. It's similar to what Elvig has shown in the previous presentation. Just to repeat, we have in these figures, we have different variables, zonal wind, wind modulus, geopotential high temperature, and dew point depletion. In all these figures, we have the dashed lines representing the bias and solid lines representing standard deviation of error. And we have the shades of red and blue that represent the statistically significant improvements corresponding to the color of the experiment. So if we see red color, then it implies that we have statistically significant improvement with the spectral nudging configuration. And if it is blue, then we have deterioration with spectral nudging. What we can see from these figures, this is for winter over the globe. That beyond five days and up to 10 days, we can see that there is tremendous improvement in the scores for the standard deviation of error, which is more difficult. Excuse me, Sayed. You have two minutes left. OK, I'll try to be faster. For the bias, the differences are mixed. But the bias is less difficult to improve. Standard deviation is more difficult. So we see tremendous improvement, up to 10% improvement in the RMSE for longer lead times. In summer, though, we see modest improvement. Still, we see statistically significant improvement for the standard deviation of error. And when you look at the animal liquid relation coefficient, this is for the 500 hectropascal geopotential. We see improvements both in winter and summer. In winter, in terms of the gain in predictability, it's around 18 hours. In summer, it's about eight hours improvement. And they're both statistically significant. And last results is about tropical cyclone position error. This is another thing that is very difficult to improve. And our model tends to have, for the alone track position, we tend to lag the model. And we can improve that lagging with spectral nudging towards graphcast. And for the cross-track position error, our cyclones tend to veer to the right from the observed trajectory. And we are able to considerably improve that aspect of the position of tropical cyclone also. And finally, just this figure, I'm showing the temperature anomaly at 850 hectropascal for a lead time of 240 hours for a single case. We have the GDPS control on the left, graphcast in the middle, and GDPS with spectral nudging on the right. And we can see that graphcast barely has any fine scale. And both GDPS control and with spectral nudging, they both have comparable fine scale information. And we get this improvement by nudging as well. So to summarize, we developed and hybrid NWP system that fuses NWP models with AI models to spectral nudging. And by leveraging more accurate large-scale predictions from graphcast, we are able to significantly improve our prediction scale with GDPS. And I want to stress that the improvement that we have is roughly equivalent to one solid innovation cycle, which is about four years of work involving many scientists from across the Meteorological Research Division of Environment Canada. And we were able to achieve something comparable in a matter of four to five months. And also, I want to stress that this is based on work that uses a graphcast model that has not been fine-tuned. And with fine-tuning graphcast to emulate our own CMC analysis, which is a work in progress by my colleague Christopher, we hope that we could improve further. And currently, with this configuration, we have about 25% increase in the computational cost. But this is without any optimization. And with some optimization, we hope that we will be able to reduce it to something like less than 15% in the near future. So with that, I will end my presentation. Thank you, Sayada. It's a very impressive conclusion. I have one question here. Nudging is done as gem integration goes or at posteriori? No, it's online. So what happens is, as I said, you solve the dynamic step. Because we have the dynamic sub-step and the physics sub-step. And then we do the coupling in the split mode. So we solve the dynamic sub-step. We have a solution from dynamics, which is an intermediate solution. Then we update that by nudging. And then we feed that updated solution to the physics. And then we get the complete solution of the model time step. And then the next time, dynamics uses that solution to predict the next dynamic step. So it's not a posteriori. It's an online update. Last quick one, how does GDPSSN compare with GraphCast? That was already shown by Ervig in the previous presentation. That GDPSS, I mean, if you talk about control GDPSS versus GraphCast, that presentation of Ervig should allow you to see like it actually improves. What I am missing in my figures, because this is still a work in progress, I mean, in our paper that we expect to submit soon, which we will be adding the GraphCast also, focus in this, for example, in these figures to show control GDPS with GraphCast and GraphCast itself, like how much of the improvement is coming from GraphCast. But it is definitely coming from GraphCast, as Ervig's presentation showed that the large scales in GraphCast are much better. Yes, Isayed, looking forward to read the print. That will be very popular, I'm sure. Thank you. Merci. Anne, at what? Yes, so we're moving. You might have noticed in the schedule that Christian Isayed was originally supposed to present this. But I want to thank Madalina Socer to have accepted to present and prepare this presentation for us. So she's a Climate Extreme Specialist at Environment Canada. She's going to be presenting on the development of artificial intelligence downscaling applications for medium-range forecasts of weather elements at CCMEP. So without further ado, over to you, Madalina. OK, thank you. I hope you're hearing me well. We are, yes. OK, great. So I am here today to present to you some development of artificial intelligence downscaling techniques that we're doing at CCMEP in collaboration with IBM Research. And I would like to acknowledge my co-authors that are listed here, both from ECCC and IBM Research. So first, I would like to say that the vision for this project is actually to help us offer seamless day one to day 10 public forecast products as part of the transformation of the meteorological service of Canada. So in order to do this, we need to bridge the gap between high-resolution, short-term forecasts, and medium-range, lower-resolution forecasts. And the reason why we want to do this is because we know that insufficient horizontal resolution causes forecast errors and especially biases. I am showing here a graph of bias, a comparison between a low-resolution model in red and the high-resolution model in blue. And what we are saying here, the closest we are to zero, the less bias we have. And we really see that increasing horizontal resolution is improving this bias. Usually, the way that we do that is by doing dynamical downscaling. So running numerical weather prediction models. But this is very computationally expensive, which makes it limiting. A partial solution to this is doing statistical downscaling, which means using past data and deriving statistical relationships from this past data, such that we apply these relationships on the course input and we obtain high-resolution output. These type of solutions are limited by the fact that we have to impose certain non-mathematical relationships in the data. So now we have a data-driven alternative, which is to apply artificial intelligence techniques to do downscaling. And what this does is that it allows to derive complex relationships in the data that we provide to these models. So our objective is to develop artificial intelligence downscaling techniques to downscale weather elements from medium range forecast to the kilometric scale. And we will hope that this will help both deterministic and ensemble forecasting. So just to briefly present our project. So this is a collaboration that started this year between CCMAP and IBM Research. And here at Environment Canada, we have the meteorological expertise. And we definitely have subjects, problems that could really benefit from these artificial intelligence solutions, but we don't necessarily have the artificial intelligence expertise, which is where IBM Research comes into play. And collaborating with them, they are really experts in this field, will allow us to advance much faster. And the expected outcome from this collaboration that for now it's only meant to be on one year, is to develop low-cost and efficient alternatives to the computationally expensive dynamical models. And the other thing that we want to get from this collaboration is we want to learn from IBM Research. So at the end of this project, we want to enhance our capability at Environment Canada to be carrying out this type of research and development and eventual operational implementation of AI downscaling techniques. So the specific goals of our projects are as follows. So we are interested in downscaling weather elements. And by this, I mean surface winds, temperature, and precipitation in the first stage, a forecast from the GDPS, which is our global deterministic prediction system shown here. And runs at a resolution of 15 kilometers to the resolution of the HRDPS, which is our high-resolution deterministic prediction system that is run for 48 hours for now at 2.5 kilometer resolution. And in this project, we are taking a two-step approach. So in the first step, we will be looking at the baseline model that is based on generative adversarial networks. And in the second stage of the project, we will be taking advantage of foundation models and tuning them for our application. The training data for the models will be forecasting data from the GDPS as the low-resolution data set and from the HRDPS as the high-resolution data set. And it is very important for our operational needs that the downscale products are available on the HRDPS grid. For now in this talk, I will only focus on the baseline model as this is a collaboration that just started. And we haven't gotten yet to the second part. So just briefly, what is a generative adversarial network? And generative adversarial networks consist of two networks, a generator, two neural networks, a generator and discriminator. And the way that they work is that they are trained in a competing process. So the generator pretty much generates images that look as much as possible as the real data. And the goal of the generator is to just fold the discriminator. On the other hand, we have the discriminator that receives both real data, which in our case is HRDPS data, and generated data, and has to decide whether this generated data is real or fake. And in our case, we use the ANAU et al. 2023 implementation of a Vassar SteamGAN. I will show you some preliminary results for our project. So these preliminary results are based on the WGAN from ANAU et al. Without any covariates. So what this means is that the only data that goes for now in this model is zonal and meridional 10-meter wing components. And this is the data that we are trying to obtain. So the low-resolution data comes from GDPS. High resolution comes from the HRDPS. And so far, we are training the model with one year of data that is divided as follows. 75% of the data is used for training. It's about 7,000 forecasts. 12.5% is used for validation. And this validation data is used during the training of the model. So it's used to stop overfitting the model. And then 12.5% of the data is used for testing. So once we have a tuned model, we will use this data set, about 1,200 forecasts, to be seeing how well this model functions. Because we are using forecasting data, it is a bit difficult. Because as you know, forecasts, there are increases with forecast lead time. And so we might have too much divergence between the HRDPS, the high-resolution data set, and our low-resolution data set. On the other hand, the first six hours of the forecasts are affected by the spin-up time of the model. So we have decided for now to go with forecasts our 6 to 18. And we are using them from the 0,0 and the 12 UTC initialization of both models. So in this way, we are covering the entire day. So our challenge is covering the entire HRDPS domain. I am showing here the HRDPS domain. It is on the order of 2,500 by 1,200 grid points. So it's a very large domain that's spanning the width of Canada. And it isn't really possible, or at least we don't think it's possible so far, to be training directly on such a large domain. We do not have the computational resources to do that. And we also aren't sure exactly how much data you would need to be able to train on such a model. The now and all implementation, in that implementation, the training is done on 16 by 16 pixel low resolution patches and 128 by 128 pixel high resolution patches. So what we will do is we will adjust to that type of training. And in order to do that, the strategy that we have adapted is to just select random patches from one forecast from our HRDPS data set. We will re-read the GDPS data set on the HRDPS domain and then course-crain it to go back to its resolution. And in this way, we are going to end up with this type of patches that we'll use to do the training. So here is the high resolution data patch and the corresponding GDPS low resolution data patch. So at each epoch, we are using between 300 and 700 random patches. And the model that we are showing today has been trained on 17,370 epochs. So this took about 149 hours to train on one GPU. And we have done more than two passes through the entire data set. So once you have a trained model, you can perform inference with the GDPS data. And that inference will also be done on 16 by 16 pixel patches. So here I have a GDPS input. We perform inference. And like this, we obtain the downscale forecast, the downscale U and V fields. And just as a comparison, we have here the HRDPS forecast. So the first things that we can say is that we are definitely downscaling. So we are obtaining information at a small scale. It isn't as much as the HRDPS is showing. But as I was mentioning before, one problem with a low resolution is the fact that you have biases. And we are definitely achieving some sort of a bias correction. Now, of course, you have to parse the entire HRDPS domain. One way to do that would be to sequentially process 128 by 128 patches. But that would result into artifacts at the borders. So the strategy that we have adopted instead is to do some overlapping and then to take the median of the ensemble of overlaps. And in this way, we managed to patch and obtain this figure over the entire domain. We have performed validation of our model. So we have used the test data, the 1,200 forecast from the GDPS. And here I am showing the root mean square error for the U-wind component and for the V-wind component. And on the bottom, I'm showing the mean absolute error. And these are the metrics that are computed between the downscale GDPS and the HRDPS corresponding verification. And we are comparing it with some baselines that can be used for interpolation. So I'm showing in orange you have bilinear interpolation and in green you have nearest neighbor interpolation. So as we are seeing in all the metrics, the downscale is showing better results, is performing better than the other types of interpolation for our test data set. We have also done a power spectrum analysis in order to really quantify how much detail we are getting at the small scales. And here I'm showing the radially average power spectral density between HRDPS in blue and the downscale forecast in orange. And these power spectra are average over the test data set as well. What we are seeing is that indeed at small scales, we are still not getting enough power. So we do not get enough detail. Madelina, if you could lend this in one or two minutes. Sounds good. So of course, we are training so far with one year of data. And it would be very interesting to see how much more detail we can obtain by training further. I'm just showing also an integrated measure of the difference in the power spectra. And what you are seeing here is that compared to the bilinear and the nearest neighbor interpolation, the downscaled AI downscaling performance much, much better. So it's able to recover way more structure at the small scales. So the next steps with our forecast, with our project, the WGAN needs further development and testing. So we are planning on testing with more data. A very important thing that we are working on right now is to add other covariates. So as I said, for now, we are only using windfields. But we are adding topography, surface pressure, and we are thinking about what other covariates may be such escape to add to our model. And once we have a baseline that we are satisfied with, the important part comes. And that is doing a thorough meteorological verification of the downscale forecast. Because like I said, we have operational goals. And we really want to see how this forecast respond to our needs. Finally, once we finish this first step of the project, we are planning on moving to the second more ambitious part, which is to develop a large GI model that is based on a pre-trained foundation model that we will be fine-tuning in order to obtain downscale forecasts. And this is where, again, collaborating with IBM Research is extremely important, as they have very much experience in these foundation models. And we are hoping to advance at least as fast as now. So this is it for me. Thank you very much. Thank you, Medellina. I have a question in the chat. I have one. I saw that you train, especially, the model. But I was surprised to see that you only used one year of data to do that. Is there a reason behind that? Because it seems to me that it's not a lot of data. Yes, well, like I said, we have. So we do end up having a lot of forecast samples. We are training on 128 by 128 pixel patches out of a very large domain. So it ends up being a lot of data. But it is not finalized. So here we are in a developing mode. We are trying to develop the model and make sure it's working properly. And this is just the first step. We are definitely planning on adding more data and seeing how we can improve. Thank you, Medellina. I don't have a question. Otherwise, Anne, I'm going to speak. Right on time. Thank you, Medellina. So now we're going over to Reynel Sospedra Alfonso, who is also a research scientist at Environment and Climate Change Canada. He will be presenting on deep learning-based bias adjustments of Arctic sea ice forecasts from version three of the Canadian seasonal to inter-annual prediction systems. Also known as CANSTEP version three. So, Reynel, the mic is all yours. Thank you. Thank you, Anne. Can you hear me well? Yes, we do. Your presentation is not full screen, though. It's not. No, it's not. All right. How about now? Yes, it is. Perfect. Thank you so much, Medell. Thank you, yes, to the organizer for this opportunity. My name is Reynel Sospedra Alfonso. I'm a research scientist at the Canadian Center for Climate Modeling and Analysis, based in Victoria. And I want to start by acknowledging the contributions or the work of colleagues at CCMA, which made this type of project possible. I list some of them down here. And I also want to acknowledge the co-authors of this presentation, or this work, Joseph Martin, Michael Simon, and especially Parca Guilla, who has been key for this project going forward. He has taken the time to do the implementation, training, and testing of the models we have been looking at. And I want to mention that this is part, what I'm going to talk about here is part of a bigger project that we are trying to pursue at CCMA, which is the use or applications of machine learning methods, in particular deep learning, to post-process our seasonal to the scale forecast. So for this talk, I will talk in particular about the post-processing or seasonal forecast of CIS. The seasonal forecast, as you may know, CANSIPS is the Canadian seasonal and internal prediction system, which provides the Environment and Climate Change Canada's operational, probabilistic seasonal forecast, both national and global. And CANSIPS first appeared or was first debuted in 2011 as a two-model forecasting system. It has evolved since, and now we are in 2024, with the new version of CANSIPS B3, which actually will be launched next month. And so here, what I'm going to talk about is the forecast that we produce with CANIAS M5, which is a new model that now we'll be using in CANSIPS. And CANIAS M5 is an air system model that is produced at CCMA, and now will be then, as I said, used for our seasonal forecast. CANIAS M5 air system models, so it couples the atmosphere, the ocean, CIS component, land, and also biochemistry, both on land and the ocean. What we do, we take our climate model, we initialize the climate model following the indications that you see here on the right. So when we initialize the forecast, we take, we notch the model towards re-analysis, and then we launch those forecasts in time. The version of CANIAS M5 that I'm going to be talking about is actually an optimal bias-corrected version, which follows the work by Sino-Kankarin, which does an online bias optimization to the model. Now, the work that I'm going to be presenting to you deals with the post-processing of those forecasts. So here, what you see is a representation of those forecasts in black. It's the observations that we are verifying, observations, the monthly values. And then what you see is the representation of those forecasts, which are initialized at the start of every month during the handcast period. So we'll be looking at handcasts from 1980 to 2021. We have, for each month in that time, we launch an ensemble of forecasts of 10 members, which run for 12 months. The variable of interest for us here is CIS concentration, which is simply the fraction of CIS, or the fraction of the grid cells that is covered by CIS. And this is what we want to adjust. Well, we do that looking at the ensemble mean forecast. So the adjustment is not done to the ensemble itself, it's done to the ensemble mean. And the question is, why do we have to do that? I mean, after all, we are even doing an online bias optimization or bias correction of my model. So still, we do need to adjust those forecasts, because as we know, we have several sources of error, structural errors, errors due to initialization, and so forth. And typically, this is done by doing some climatological bias correction to those forecasts. Now here, just to give you an example, I'm showing you the September CIS concentration over the time period 2006 to 2020. On the left, these are the very fine observations that we use, which comes from NOAA data products. And here, again, this is the CIS concentration, which has value from 0 to 1. And we see on the right, the growth forecast as it comes out of the model. So this is the output directly coming from our forecast. And what we see is that there is definitely a strong bias that we notice, particularly in the center Arctic, where we have a much lower CIS concentration relative to observations. Now on the right to that, we see the bias adjusted forecast, in which we compensate or we subtract the bias, compute it on a previous time. And then what we see is that we adjust somehow that forecast by doing this simple bias correction. Now still, even after doing a bias correction, we see that there are some differences between the observed CIS concentration and the one that is bias adjusted. So that tells us that we need to do something else in order to actually improve our forecast. And for that, and this is now the project that we are working on, is to use this machine learning or deep learning method to improve even further those forecasts. The method that I'm going to use here, and I'm going to talk a little bit more about that in the few slides, is the unit. Probably most of the people in the audience are familiar with units. As I said, I will talk a little bit more about that. But here, I'm just showing you some results in which you see how the unit is able to reproduce the spatial pattern a lot better than what we can do with a simple bias correction. I should have said that this is a forecast done at two monthly time. So this is the forecast two months after the forecast is initialized, where the biases are, you can see, are particularly strong. Now, the unit, this is the tool of choice. What we have here is a fully connected network, or a fully convolutional network, that has a downscaling, down sampling encoder, followed by up sampling decoder. This is a classical unit. What we see is that the future maps are reduced in size. So we see that the resolution is reduced by its health, and the channels are increased by two. And this allows us to have a better representation of capacity of the network, while preserving some information of the image that we input. So to be clear, what we input here is our raw forecast, which is denoted by the YMN. Thanks, and will be the resolution of my forecast, which is a function of the initial month and the target month, and the time relative to the first month in the data that we are inputting. So the input is made of six channels, one channel that is the actual variable that we want to, or the actual map that we want to correct, plus five temporal features that takes into account, again, the initial month that we're looking at, the target month, and this temporal information relative to the initial time. So in other words, how many years and how many months from the starting of your data. You input that into the network, and then the output is, hopefully, is an adjusted forecast, which then correct for those biases that I mentioned earlier. Moving a little bit forward here, let me just be a little bit more precise on the kind of task that we are doing. This is just a specific example, which hopefully will clarify how we do this. So in this case, let's say that we want to adjust the March CIS concentration forecast, which is initializing February of a year Y. So essentially it will be this red dot denoted here on the figure. So again, what we want to do is to post-process this forecast. We leverage the forecast that are produced with our climate model. We do not do prediction. We just do that kind of bias adjustment or post-processing of the predictions that we get from our climate model. So for this specific task, to correct that March CIS, we train on the data that is available for the years before the test year that we have. So in this case, I'm denoting that here for this in this shadow region. So we take all the pairs of forecast and observations for all lead times and all target month, and then we train our network on that data, and then we will do that iteratively for every test years that we want to make the adjustment. So what I'm going to do now is to show you some of the results that we have. I should say that this is very much a work in progress. And that is different avenues that we are working on, but we have some results that I want to share with you. For instance, what we see here is the CIS concentration bias. At the zero-month lead, average over the 2006 and 2020 time period, which is the test years that we have for our analysis. At the top, we have the bias for the March CIS concentration, which is at the time of maximum CIS extent. And at the bottom, we have the results or the bias for the September CIS concentration, which is at the time of a minimum CIS extent. On the left, we see here what happened with the raw forecast. We see the biases happening in several regions, which is actually substantial. Here, the bias is given in percent. For the bias adjusted case, which is the simple bias correction that I'm using here as a benchmark, we see that we do improve relative to the raw forecast, but there are still some biases that are apparent, particularly during the CIS minimum. And then on the right is the results that we get with the unit. And we see that the bias are likely removed. And now, this is for the zero-month lead. So this is soon after we initialize our forecast. Now, two months ahead in our forecast, of course, the biases are increased. Excuse me, we have two minutes. Two minutes. Thank you. All right, so here is the case in which we have two-month lead forecasts. And we see that the bias, of course, are increased. But still, we are able to manage those biases with the unit, given a better representation of the CIS edges. Another measure that we look at here is the integrated ICH error, which is essentially the area, the integrated area that we have, in which both forecast and observations disagree in the concentration with a threshold of 15%. So we both have more than 50% concentration or less than 50%. This binary error will be zero. If they are different, then the binary error will be one. And those grid cells will contribute to this area error. So this is at the top here, we see a hit map in which we have our target month in the X axis. And on the Y, we have the lead month. And bottom message here is that the blue are bad. The red are good, meaning there is less error. And the unit bids both the row, of course, and the bias are used to forecast. Down here is just an integration over lead month just to give a more clear picture of how the unit outperforms the alternative. Now, I know that I don't have much time, so I will not go into the details of these results, but at least to give you an idea of what is happening. In this case, we are looking at the CIS area. Same time of floods. Again, red means that we have a better room mean and square error, which is the measure that we are using here for the CIS area. Blue means that the errors are increased. And in this particular case, we see that the unit is slightly better than the bias corrected one. However, if we look at CIS extent, which is now the area in which we count for all those grid cells with concentration greater than 50%, we see that unit largely outperform this bias-adjusted method. Finally, I want just to mention that here, we have seen different measures of the skill in which we outperform both the benchmark and the raw forecast, but there is still some issues in terms of the representation of the temporal dependence of our adjusted forecast. And so, but unlike for this slide here, just to mention that we still have some work to do to be able to better represent the temporal dependence of those forecasts. And because I don't have much more time, I will finish with this slide, which is some final remarks. We'll leave it there for you. And I will be happy to take any questions. Thank you. Thank you, Renel. I saw that there seems to be to have a seasonal pattern in your verification. That leads me to the questions. Are there any biases that are harder to adjust with this method? The answer for that, yes. And this is anality that we see has to do in part with how the eyes behave. We have what is known as this predictability barrier after spring, which makes it difficult to do a good prediction of the sea eyes. This is something that perhaps we can see all those metrics, actually perhaps we'll go to this one. So this barrier, which reduces the skill that we have in our predictions, can be seen here around the month of June to October, looking here at target month. And so that is something within the raw forecast in person, the bias corrected one, but also in the unit. So we do improve on those months, but still there is some skill that is missing there, which is part of the natural process in the sea eyes formation and melting, which then translate into the skill that we see in our just forecast. Thank you. One last question is coming from the online Q&A. Would training the model on all months of the year versus only certain months affect its ability to improve the forecast? For example, forecasts of sea eyes in spring are known to perform poorly. So if you were to exclude these months, would this improve the bias adjustment? Yeah, that's a good question. Actually, when I was, I mean, it's too bad that I'm rushing through all these slides, but one of the things that I wanted to mention at this particular slide is that we do the training. You're looking at the previous years in my forecast, but we are missing some of the information of more recent months that can also contribute to the forecast. Like looking here, for instance, we do not use information from January of this specific year. Now, one thing that we are exploring is to look at training based on lead times or a specific month. I think that that's the idea of the question, which we try to train our model specific to the lead times in which we have the biases that we want to correct and the kind of information specific to the seasonality that we want also to correct. So those are things that we are exploring, but so far this is what we have. Thank you, R\u00e9nel. This concludes our first block for the first part of the presentation for artificial intelligence. A big thank you to R\u00e9nel, Madlena, Sa\u00efd, Herv\u00e9, and Christopher for having made these presentations. Anne, I'll leave you here. Yes, a big thank you to all the presenters, all the attendees as well. And as several mentioned before, there is a second session coming up in about 20 minutes, not even. So we'd love to see you back here, a lot more to see and to hear from people outside ECCC as well. So please do join us. Thank you. There's not a sign. There's not a sign, that's it. Thanks. You're better. Bye. Bye. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.0, "text": " Let me try for you. Okay.", "tokens": [50364, 961, 385, 853, 337, 291, 13, 1033, 13, 50664], "temperature": 0.0, "avg_logprob": -0.8233691851298014, "compression_ratio": 0.7575757575757576, "no_speech_prob": 0.04256459325551987}, {"id": 1, "seek": 600, "start": 6.0, "end": 28.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.7781201090131488, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.06338080763816833}, {"id": 2, "seek": 2800, "start": 28.0, "end": 41.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.6853872140248617, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.8823546171188354}, {"id": 3, "seek": 5800, "start": 58.0, "end": 78.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.6717267831166586, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9719316363334656}, {"id": 4, "seek": 8800, "start": 88.0, "end": 103.0, "text": " Hello and welcome.", "tokens": [50364, 2425, 293, 2928, 13, 51114], "temperature": 0.0, "avg_logprob": -0.5488680005073547, "compression_ratio": 0.6923076923076923, "no_speech_prob": 0.06151125952601433}, {"id": 5, "seek": 10300, "start": 103.0, "end": 109.0, "text": " Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel", "tokens": [50364, 2425, 293, 2928, 13, 1222, 1315, 307, 8860, 413, 19552, 293, 43648, 385, 965, 307, 452, 598, 12, 6037, 29150, 50664], "temperature": 0.0, "avg_logprob": -0.1878809706072941, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.22878794372081757}, {"id": 6, "seek": 10300, "start": 109.0, "end": 113.0, "text": " Tremblay. We both work for the Meteorological Service of Canada within", "tokens": [50664, 8648, 2504, 8376, 13, 492, 1293, 589, 337, 264, 43328, 284, 4383, 9561, 295, 6309, 1951, 50864], "temperature": 0.0, "avg_logprob": -0.1878809706072941, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.22878794372081757}, {"id": 7, "seek": 10300, "start": 113.0, "end": 118.0, "text": " Environment and Climate Change Canada and we're happy to be here exploring AI", "tokens": [50864, 35354, 293, 27025, 15060, 6309, 293, 321, 434, 2055, 281, 312, 510, 12736, 7318, 51114], "temperature": 0.0, "avg_logprob": -0.1878809706072941, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.22878794372081757}, {"id": 8, "seek": 10300, "start": 118.0, "end": 124.0, "text": " with you for a second year. Before we begin, a few reminders. 15 minutes are", "tokens": [51114, 365, 291, 337, 257, 1150, 1064, 13, 4546, 321, 1841, 11, 257, 1326, 43458, 13, 2119, 2077, 366, 51414], "temperature": 0.0, "avg_logprob": -0.1878809706072941, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.22878794372081757}, {"id": 9, "seek": 10300, "start": 124.0, "end": 128.0, "text": " reserved for each presentation and we encourage our presenters to keep three", "tokens": [51414, 24819, 337, 1184, 5860, 293, 321, 5373, 527, 36987, 281, 1066, 1045, 51614], "temperature": 0.0, "avg_logprob": -0.1878809706072941, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.22878794372081757}, {"id": 10, "seek": 10300, "start": 128.0, "end": 132.0, "text": " minutes for questions at the end. Time permitting participants with questions", "tokens": [51614, 2077, 337, 1651, 412, 264, 917, 13, 6161, 4784, 2414, 10503, 365, 1651, 51814], "temperature": 0.0, "avg_logprob": -0.1878809706072941, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.22878794372081757}, {"id": 11, "seek": 13200, "start": 132.0, "end": 137.0, "text": " will be able to use the chat at the end of the presentation and their questions", "tokens": [50364, 486, 312, 1075, 281, 764, 264, 5081, 412, 264, 917, 295, 264, 5860, 293, 641, 1651, 50614], "temperature": 0.0, "avg_logprob": -0.1026507414780654, "compression_ratio": 1.6864864864864866, "no_speech_prob": 0.05365802347660065}, {"id": 12, "seek": 13200, "start": 137.0, "end": 145.0, "text": " can be upvoted by the other attendees. Miguel will provide up at 10 and 12", "tokens": [50614, 393, 312, 493, 85, 23325, 538, 264, 661, 34826, 13, 29150, 486, 2893, 493, 412, 1266, 293, 2272, 51014], "temperature": 0.0, "avg_logprob": -0.1026507414780654, "compression_ratio": 1.6864864864864866, "no_speech_prob": 0.05365802347660065}, {"id": 13, "seek": 13200, "start": 145.0, "end": 150.0, "text": " minutes and will bring the presentation to an end at 15 and will be ruthless", "tokens": [51014, 2077, 293, 486, 1565, 264, 5860, 281, 364, 917, 412, 2119, 293, 486, 312, 47096, 51264], "temperature": 0.0, "avg_logprob": -0.1026507414780654, "compression_ratio": 1.6864864864864866, "no_speech_prob": 0.05365802347660065}, {"id": 14, "seek": 13200, "start": 150.0, "end": 155.0, "text": " guys because we want everyone to have time to present. Miguel, do you want to do", "tokens": [51264, 1074, 570, 321, 528, 1518, 281, 362, 565, 281, 1974, 13, 29150, 11, 360, 291, 528, 281, 360, 51514], "temperature": 0.0, "avg_logprob": -0.1026507414780654, "compression_ratio": 1.6864864864864866, "no_speech_prob": 0.05365802347660065}, {"id": 15, "seek": 15500, "start": 155.0, "end": 178.0, "text": " a quick recap in French?", "tokens": [50364, 257, 1702, 20928, 294, 5522, 30, 51514], "temperature": 0.0, "avg_logprob": -0.3136736392974854, "compression_ratio": 0.75, "no_speech_prob": 0.11493062973022461}, {"id": 16, "seek": 17800, "start": 178.0, "end": 184.0, "text": " A few reminders. 15 minutes are reserved for each presentation and we encourage", "tokens": [50364, 316, 1326, 43458, 13, 2119, 2077, 366, 24819, 337, 1184, 5860, 293, 321, 5373, 50664], "temperature": 0.0, "avg_logprob": -0.5179406915392194, "compression_ratio": 1.890625, "no_speech_prob": 0.7816728353500366}, {"id": 17, "seek": 17800, "start": 184.0, "end": 196.0, "text": " our presenters to keep time for questions at the end of the presentation and we", "tokens": [50664, 527, 36987, 281, 1066, 565, 337, 1651, 412, 264, 917, 295, 264, 5860, 293, 321, 51264], "temperature": 0.0, "avg_logprob": -0.5179406915392194, "compression_ratio": 1.890625, "no_speech_prob": 0.7816728353500366}, {"id": 18, "seek": 17800, "start": 196.0, "end": 202.0, "text": " will be able to use the chat at the end of the presentation and we will be able to", "tokens": [51264, 486, 312, 1075, 281, 764, 264, 5081, 412, 264, 917, 295, 264, 5860, 293, 321, 486, 312, 1075, 281, 51564], "temperature": 0.0, "avg_logprob": -0.5179406915392194, "compression_ratio": 1.890625, "no_speech_prob": 0.7816728353500366}, {"id": 19, "seek": 20200, "start": 202.0, "end": 207.0, "text": " use the chat at the end of the presentation and we will be able to use the chat at", "tokens": [50364, 764, 264, 5081, 412, 264, 917, 295, 264, 5860, 293, 321, 486, 312, 1075, 281, 764, 264, 5081, 412, 50614], "temperature": 1.0, "avg_logprob": -1.5367084230695451, "compression_ratio": 2.2448979591836733, "no_speech_prob": 0.4841078817844391}, {"id": 20, "seek": 20200, "start": 207.0, "end": 212.72, "text": " the end of the presentation and we will be able to use the chat", "tokens": [50614, 264, 917, 295, 264, 5860, 293, 321, 486, 312, 1075, 281, 764, 264, 5081, 50900], "temperature": 1.0, "avg_logprob": -1.5367084230695451, "compression_ratio": 2.2448979591836733, "no_speech_prob": 0.4841078817844391}, {"id": 21, "seek": 20200, "start": 222.0, "end": 231.0, "text": "fpspe conditional 10 minutes for one and a half minutes for 10 minutes for", "tokens": [51364, 50084, 494, 27708, 1266, 2077, 337, 472, 293, 257, 1922, 2077, 337, 1266, 2077, 337, 51814], "temperature": 1.0, "avg_logprob": -1.5367084230695451, "compression_ratio": 2.2448979591836733, "no_speech_prob": 0.4841078817844391}, {"id": 22, "seek": 23100, "start": 231.0, "end": 235.4, "text": " weather forecasting, and ECCC's research plans.", "tokens": [50364, 5503, 44331, 11, 293, 462, 11717, 34, 311, 2132, 5482, 13, 50584], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 23, "seek": 23100, "start": 235.4, "end": 238.6, "text": " So without further ado, over to you, Christopher.", "tokens": [50584, 407, 1553, 3052, 22450, 11, 670, 281, 291, 11, 20649, 13, 50744], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 24, "seek": 23100, "start": 238.6, "end": 239.44, "text": " Thank you.", "tokens": [50744, 1044, 291, 13, 50786], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 25, "seek": 23100, "start": 239.44, "end": 242.54, "text": " Let me get the screen sharing going and share.", "tokens": [50786, 961, 385, 483, 264, 2568, 5414, 516, 293, 2073, 13, 50941], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 26, "seek": 23100, "start": 243.88, "end": 246.4, "text": " Okay, yep, looks like we're good.", "tokens": [51008, 1033, 11, 18633, 11, 1542, 411, 321, 434, 665, 13, 51134], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 27, "seek": 23100, "start": 246.4, "end": 247.76, "text": " Okay, hello everybody.", "tokens": [51134, 1033, 11, 7751, 2201, 13, 51202], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 28, "seek": 23100, "start": 247.76, "end": 248.6, "text": " I'm Christopher Subic", "tokens": [51202, 286, 478, 20649, 8511, 299, 51244], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 29, "seek": 23100, "start": 248.6, "end": 250.2, "text": " from Environment and Climate Change Canada.", "tokens": [51244, 490, 35354, 293, 27025, 15060, 6309, 13, 51324], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 30, "seek": 23100, "start": 250.2, "end": 252.04, "text": " I'm here to speak as said about,", "tokens": [51324, 286, 478, 510, 281, 1710, 382, 848, 466, 11, 51416], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 31, "seek": 23100, "start": 254.4, "end": 256.08, "text": " well, click, okay.", "tokens": [51534, 731, 11, 2052, 11, 1392, 13, 51618], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 32, "seek": 23100, "start": 256.08, "end": 257.76, "text": " This talk is essentially in two parts.", "tokens": [51618, 639, 751, 307, 4476, 294, 732, 3166, 13, 51702], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 33, "seek": 23100, "start": 257.76, "end": 260.12, "text": " The first part is going to be a brief summary", "tokens": [51702, 440, 700, 644, 307, 516, 281, 312, 257, 5353, 12691, 51820], "temperature": 0.0, "avg_logprob": -0.24089080243071248, "compression_ratio": 1.554307116104869, "no_speech_prob": 0.3237176835536957}, {"id": 34, "seek": 26012, "start": 260.12, "end": 263.52, "text": " of the current state of machine learning based forecasting", "tokens": [50364, 295, 264, 2190, 1785, 295, 3479, 2539, 2361, 44331, 50534], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 35, "seek": 26012, "start": 264.64, "end": 269.4, "text": " with a broad focus on medium range weather forecasts.", "tokens": [50590, 365, 257, 4152, 1879, 322, 6399, 3613, 5503, 49421, 13, 50828], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 36, "seek": 26012, "start": 270.2, "end": 273.8, "text": " Second part of this talk will be the forthcoming AI roadmap", "tokens": [50868, 5736, 644, 295, 341, 751, 486, 312, 264, 5220, 6590, 7318, 35738, 51048], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 37, "seek": 26012, "start": 273.8, "end": 276.52, "text": " out of the atmospheric science", "tokens": [51048, 484, 295, 264, 28854, 3497, 51184], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 38, "seek": 26012, "start": 276.52, "end": 278.28000000000003, "text": " of technology director at NCCMAP,", "tokens": [51184, 295, 2899, 5391, 412, 426, 11717, 44, 4715, 11, 51272], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 39, "seek": 26012, "start": 279.28000000000003, "end": 282.64, "text": " which effectively broadly sets out", "tokens": [51322, 597, 8659, 19511, 6352, 484, 51490], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 40, "seek": 26012, "start": 282.64, "end": 286.08, "text": " where this part of Environment and Climate Change Canada", "tokens": [51490, 689, 341, 644, 295, 35354, 293, 27025, 15060, 6309, 51662], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 41, "seek": 26012, "start": 286.08, "end": 289.92, "text": " will be going in the short of medium term future with AI.", "tokens": [51662, 486, 312, 516, 294, 264, 2099, 295, 6399, 1433, 2027, 365, 7318, 13, 51854], "temperature": 0.0, "avg_logprob": -0.2281145460150215, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.001808159169740975}, {"id": 42, "seek": 29012, "start": 290.24, "end": 291.84000000000003, "text": " And finally, time permitting,", "tokens": [50370, 400, 2721, 11, 565, 4784, 2414, 11, 50450], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 43, "seek": 29012, "start": 291.84000000000003, "end": 295.04, "text": " I'll conclude with a preview of talks to come", "tokens": [50450, 286, 603, 16886, 365, 257, 14281, 295, 6686, 281, 808, 50610], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 44, "seek": 29012, "start": 295.04, "end": 300.04, "text": " and just general thoughts on the state of AI in forecasting.", "tokens": [50610, 293, 445, 2674, 4598, 322, 264, 1785, 295, 7318, 294, 44331, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 45, "seek": 29012, "start": 300.24, "end": 302.4, "text": " So in case you haven't noticed,", "tokens": [50870, 407, 294, 1389, 291, 2378, 380, 5694, 11, 50978], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 46, "seek": 29012, "start": 302.4, "end": 304.04, "text": " AI is becoming a big deal.", "tokens": [50978, 7318, 307, 5617, 257, 955, 2028, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 47, "seek": 29012, "start": 304.04, "end": 306.04, "text": " It's no longer just pictures of cats", "tokens": [51060, 467, 311, 572, 2854, 445, 5242, 295, 11111, 51160], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 48, "seek": 29012, "start": 306.04, "end": 308.04, "text": " or helping kids cheat on their homework,", "tokens": [51160, 420, 4315, 2301, 17470, 322, 641, 14578, 11, 51260], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 49, "seek": 29012, "start": 308.04, "end": 312.88, "text": " but it's starting to influence the industries", "tokens": [51260, 457, 309, 311, 2891, 281, 6503, 264, 13284, 51502], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 50, "seek": 29012, "start": 312.88, "end": 316.12, "text": " and feels previously thought unassailable.", "tokens": [51502, 293, 3417, 8046, 1194, 517, 640, 32699, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1130335529645284, "compression_ratio": 1.5404255319148936, "no_speech_prob": 0.0002866088761948049}, {"id": 51, "seek": 31612, "start": 317.12, "end": 320.92, "text": " Now, this isn't truly a stranger to weather forecasting.", "tokens": [50414, 823, 11, 341, 1943, 380, 4908, 257, 18834, 281, 5503, 44331, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 52, "seek": 31612, "start": 320.92, "end": 323.56, "text": " AI adjacent topics have long had roles", "tokens": [50604, 7318, 24441, 8378, 362, 938, 632, 9604, 50736], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 53, "seek": 31612, "start": 323.56, "end": 327.76, "text": " in the forecast production system.", "tokens": [50736, 294, 264, 14330, 4265, 1185, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 54, "seek": 31612, "start": 328.8, "end": 330.64, "text": " We've long had statistical models", "tokens": [50998, 492, 600, 938, 632, 22820, 5245, 51090], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 55, "seek": 31612, "start": 330.64, "end": 332.88, "text": " for quality control of observational data", "tokens": [51090, 337, 3125, 1969, 295, 9951, 1478, 1412, 51202], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 56, "seek": 31612, "start": 332.88, "end": 334.68, "text": " and for post-processing,", "tokens": [51202, 293, 337, 2183, 12, 41075, 278, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 57, "seek": 31612, "start": 334.68, "end": 338.64, "text": " but a phase change has been that over the past two years,", "tokens": [51292, 457, 257, 5574, 1319, 575, 668, 300, 670, 264, 1791, 732, 924, 11, 51490], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 58, "seek": 31612, "start": 338.64, "end": 342.36, "text": " maybe three models from the academic and private sectors", "tokens": [51490, 1310, 1045, 5245, 490, 264, 7778, 293, 4551, 18373, 51676], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 59, "seek": 31612, "start": 342.36, "end": 345.8, "text": " have gone from things that we should probably", "tokens": [51676, 362, 2780, 490, 721, 300, 321, 820, 1391, 51848], "temperature": 0.0, "avg_logprob": -0.1509612434788754, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.0003798607212956995}, {"id": 60, "seek": 34580, "start": 345.8, "end": 347.84000000000003, "text": " keep our eye on as interesting things", "tokens": [50364, 1066, 527, 3313, 322, 382, 1880, 721, 50466], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 61, "seek": 34580, "start": 347.84000000000003, "end": 350.12, "text": " for the long-term future to, well,", "tokens": [50466, 337, 264, 938, 12, 7039, 2027, 281, 11, 731, 11, 50580], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 62, "seek": 34580, "start": 350.12, "end": 354.0, "text": " these two nearly full-featured forecast systems", "tokens": [50580, 613, 732, 6217, 1577, 12, 2106, 1503, 67, 14330, 3652, 50774], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 63, "seek": 34580, "start": 354.0, "end": 356.28000000000003, "text": " that are competitive with the state of the art.", "tokens": [50774, 300, 366, 10043, 365, 264, 1785, 295, 264, 1523, 13, 50888], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 64, "seek": 34580, "start": 358.04, "end": 361.0, "text": " With that in mind, I can just state very plainly", "tokens": [50976, 2022, 300, 294, 1575, 11, 286, 393, 445, 1785, 588, 11121, 356, 51124], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 65, "seek": 34580, "start": 361.0, "end": 362.92, "text": " that Environment Climate Change Canada,", "tokens": [51124, 300, 35354, 27025, 15060, 6309, 11, 51220], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 66, "seek": 34580, "start": 362.92, "end": 367.04, "text": " in particular my part of it, is taking AI very seriously", "tokens": [51220, 294, 1729, 452, 644, 295, 309, 11, 307, 1940, 7318, 588, 6638, 51426], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 67, "seek": 34580, "start": 367.04, "end": 369.36, "text": " and we intend for it to have an increasing role", "tokens": [51426, 293, 321, 19759, 337, 309, 281, 362, 364, 5662, 3090, 51542], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 68, "seek": 34580, "start": 369.36, "end": 373.48, "text": " in numerical weather prediction systems going forward.", "tokens": [51542, 294, 29054, 5503, 17630, 3652, 516, 2128, 13, 51748], "temperature": 0.0, "avg_logprob": -0.15684799920944942, "compression_ratio": 1.61003861003861, "no_speech_prob": 0.001570745138451457}, {"id": 69, "seek": 37348, "start": 373.6, "end": 376.40000000000003, "text": " We're taking a sort of trust but verify approach", "tokens": [50370, 492, 434, 1940, 257, 1333, 295, 3361, 457, 16888, 3109, 50510], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 70, "seek": 37348, "start": 376.40000000000003, "end": 379.76, "text": " in that we intend to make AI advances operational,", "tokens": [50510, 294, 300, 321, 19759, 281, 652, 7318, 25297, 16607, 11, 50678], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 71, "seek": 37348, "start": 379.76, "end": 381.88, "text": " but only after they've been scientifically proven.", "tokens": [50678, 457, 787, 934, 436, 600, 668, 39719, 12785, 13, 50784], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 72, "seek": 37348, "start": 381.88, "end": 386.88, "text": " We're not in a rush to perform science by press release", "tokens": [50784, 492, 434, 406, 294, 257, 9300, 281, 2042, 3497, 538, 1886, 4374, 51034], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 73, "seek": 37348, "start": 387.48, "end": 391.44, "text": " and this is also a medium to long-term plan.", "tokens": [51064, 293, 341, 307, 611, 257, 6399, 281, 938, 12, 7039, 1393, 13, 51262], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 74, "seek": 37348, "start": 391.44, "end": 393.68, "text": " Capacity building is going to take years,", "tokens": [51262, 8363, 19008, 2390, 307, 516, 281, 747, 924, 11, 51374], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 75, "seek": 37348, "start": 393.68, "end": 397.92, "text": " both in terms of acquiring sufficient computational power", "tokens": [51374, 1293, 294, 2115, 295, 37374, 11563, 28270, 1347, 51586], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 76, "seek": 37348, "start": 397.92, "end": 400.08000000000004, "text": " for this and developing the human expertise", "tokens": [51586, 337, 341, 293, 6416, 264, 1952, 11769, 51694], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 77, "seek": 37348, "start": 400.08000000000004, "end": 401.88, "text": " necessary to properly use it.", "tokens": [51694, 4818, 281, 6108, 764, 309, 13, 51784], "temperature": 0.0, "avg_logprob": -0.10883359348072726, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.0011505649890750647}, {"id": 78, "seek": 40348, "start": 403.6, "end": 408.6, "text": " The modern AI forecast field essentially began", "tokens": [50370, 440, 4363, 7318, 14330, 2519, 4476, 4283, 50620], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 79, "seek": 40348, "start": 409.72, "end": 410.68, "text": " about two years ago.", "tokens": [50676, 466, 732, 924, 2057, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 80, "seek": 40348, "start": 410.68, "end": 414.0, "text": " I dated from the first publication of GraphCast", "tokens": [50724, 286, 23804, 490, 264, 700, 19953, 295, 21884, 34, 525, 50890], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 81, "seek": 40348, "start": 414.0, "end": 417.32, "text": " by Lam in 2023, which is the first time", "tokens": [50890, 538, 18825, 294, 44377, 11, 597, 307, 264, 700, 565, 51056], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 82, "seek": 40348, "start": 417.32, "end": 421.68, "text": " that an AI model could truly claim to beat", "tokens": [51056, 300, 364, 7318, 2316, 727, 4908, 3932, 281, 4224, 51274], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 83, "seek": 40348, "start": 421.68, "end": 424.84000000000003, "text": " the state of the art from, in this case,", "tokens": [51274, 264, 1785, 295, 264, 1523, 490, 11, 294, 341, 1389, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 84, "seek": 40348, "start": 424.84000000000003, "end": 427.92, "text": " the equivalent forecast from ECMWF.", "tokens": [51432, 264, 10344, 14330, 490, 19081, 44, 54, 37, 13, 51586], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 85, "seek": 40348, "start": 430.32, "end": 432.72, "text": " In addition, the truly shocking claim", "tokens": [51706, 682, 4500, 11, 264, 4908, 18776, 3932, 51826], "temperature": 0.0, "avg_logprob": -0.1496997877608898, "compression_ratio": 1.4904761904761905, "no_speech_prob": 0.00024143535119947046}, {"id": 86, "seek": 43272, "start": 432.76000000000005, "end": 434.64000000000004, "text": " was that these forecasts came with orders", "tokens": [50366, 390, 300, 613, 49421, 1361, 365, 9470, 50460], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 87, "seek": 43272, "start": 434.64000000000004, "end": 436.72, "text": " of magnitude faster running time.", "tokens": [50460, 295, 15668, 4663, 2614, 565, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 88, "seek": 43272, "start": 436.72, "end": 439.36, "text": " GraphCast will produce a 10-day quarter-degree forecast", "tokens": [50564, 21884, 34, 525, 486, 5258, 257, 1266, 12, 810, 6555, 12, 34368, 14330, 50696], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 89, "seek": 43272, "start": 439.36, "end": 442.0, "text": " in about 30 seconds on a single GPU.", "tokens": [50696, 294, 466, 2217, 3949, 322, 257, 2167, 18407, 13, 50828], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 90, "seek": 43272, "start": 442.92, "end": 446.84000000000003, "text": " That put all of the weather centers globally on notice", "tokens": [50874, 663, 829, 439, 295, 264, 5503, 10898, 18958, 322, 3449, 51070], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 91, "seek": 43272, "start": 446.84000000000003, "end": 450.08000000000004, "text": " that if these trends continue, we must adapt", "tokens": [51070, 300, 498, 613, 13892, 2354, 11, 321, 1633, 6231, 51232], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 92, "seek": 43272, "start": 450.08000000000004, "end": 451.28000000000003, "text": " or we'll fall behind.", "tokens": [51232, 420, 321, 603, 2100, 2261, 13, 51292], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 93, "seek": 43272, "start": 451.28000000000003, "end": 454.08000000000004, "text": " And I mean, in some sense, research has always been that.", "tokens": [51292, 400, 286, 914, 11, 294, 512, 2020, 11, 2132, 575, 1009, 668, 300, 13, 51432], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 94, "seek": 43272, "start": 454.08000000000004, "end": 457.32000000000005, "text": " You can't sit on your laurels from 20 years ago", "tokens": [51432, 509, 393, 380, 1394, 322, 428, 49469, 11784, 490, 945, 924, 2057, 51594], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 95, "seek": 43272, "start": 457.32000000000005, "end": 458.6, "text": " and pretend to be competitive,", "tokens": [51594, 293, 11865, 281, 312, 10043, 11, 51658], "temperature": 0.0, "avg_logprob": -0.13114485824317262, "compression_ratio": 1.5141843971631206, "no_speech_prob": 0.0017534279031679034}, {"id": 96, "seek": 45860, "start": 458.64000000000004, "end": 463.64000000000004, "text": " but this field is truly advancing at a revolutionary rate.", "tokens": [50366, 457, 341, 2519, 307, 4908, 27267, 412, 257, 22687, 3314, 13, 50616], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 97, "seek": 45860, "start": 468.8, "end": 471.32000000000005, "text": " Models are being constantly released.", "tokens": [50874, 6583, 1625, 366, 885, 6460, 4736, 13, 51000], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 98, "seek": 45860, "start": 471.32000000000005, "end": 473.32000000000005, "text": " Any summary, like the one I'm about to give", "tokens": [51000, 2639, 12691, 11, 411, 264, 472, 286, 478, 466, 281, 976, 51100], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 99, "seek": 45860, "start": 473.32000000000005, "end": 474.68, "text": " was going to be out of date within weeks.", "tokens": [51100, 390, 516, 281, 312, 484, 295, 4002, 1951, 3259, 13, 51168], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 100, "seek": 45860, "start": 474.68, "end": 477.32000000000005, "text": " In fact, just this morning, I've had to update my slides", "tokens": [51168, 682, 1186, 11, 445, 341, 2446, 11, 286, 600, 632, 281, 5623, 452, 9788, 51300], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 101, "seek": 45860, "start": 477.32000000000005, "end": 479.6, "text": " to add two new preprints that have come out", "tokens": [51300, 281, 909, 732, 777, 659, 25788, 300, 362, 808, 484, 51414], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 102, "seek": 45860, "start": 479.6, "end": 483.32000000000005, "text": " in the past four or five days.", "tokens": [51414, 294, 264, 1791, 1451, 420, 1732, 1708, 13, 51600], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 103, "seek": 45860, "start": 483.32000000000005, "end": 485.12, "text": " Nonetheless, there are some common features", "tokens": [51600, 45437, 11, 456, 366, 512, 2689, 4122, 51690], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 104, "seek": 45860, "start": 485.12, "end": 487.88, "text": " between medium-range models that are shared", "tokens": [51690, 1296, 6399, 12, 14521, 5245, 300, 366, 5507, 51828], "temperature": 0.0, "avg_logprob": -0.10785022771583413, "compression_ratio": 1.5703125, "no_speech_prob": 0.0004582173132803291}, {"id": 105, "seek": 48788, "start": 487.88, "end": 489.92, "text": " by, if not all of them, then most of them.", "tokens": [50364, 538, 11, 498, 406, 439, 295, 552, 11, 550, 881, 295, 552, 13, 50466], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 106, "seek": 48788, "start": 491.2, "end": 493.52, "text": " Most broadly speaking, these models attempt to learn", "tokens": [50530, 4534, 19511, 4124, 11, 613, 5245, 5217, 281, 1466, 50646], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 107, "seek": 48788, "start": 493.52, "end": 495.48, "text": " from data, which means rather than simulate", "tokens": [50646, 490, 1412, 11, 597, 1355, 2831, 813, 27817, 50744], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 108, "seek": 48788, "start": 495.48, "end": 497.8, "text": " the atmosphere from first principles,", "tokens": [50744, 264, 8018, 490, 700, 9156, 11, 50860], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 109, "seek": 48788, "start": 497.8, "end": 502.6, "text": " they try to look at some form of data", "tokens": [50860, 436, 853, 281, 574, 412, 512, 1254, 295, 1412, 51100], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 110, "seek": 48788, "start": 502.6, "end": 504.92, "text": " and predict what it will become.", "tokens": [51100, 293, 6069, 437, 309, 486, 1813, 13, 51216], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 111, "seek": 48788, "start": 504.92, "end": 506.8, "text": " In this case, for the medium-range forecasting,", "tokens": [51216, 682, 341, 1389, 11, 337, 264, 6399, 12, 14521, 44331, 11, 51310], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 112, "seek": 48788, "start": 506.8, "end": 509.36, "text": " the data is almost always the error of five reanalysis.", "tokens": [51310, 264, 1412, 307, 1920, 1009, 264, 6713, 295, 1732, 319, 29702, 4642, 13, 51438], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 113, "seek": 48788, "start": 509.36, "end": 511.64, "text": " That's our highest quality, longest-term,", "tokens": [51438, 663, 311, 527, 6343, 3125, 11, 15438, 12, 7039, 11, 51552], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 114, "seek": 48788, "start": 511.64, "end": 513.96, "text": " and most uniform record of the atmospheric state", "tokens": [51552, 293, 881, 9452, 2136, 295, 264, 28854, 1785, 51668], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 115, "seek": 48788, "start": 513.96, "end": 517.24, "text": " that we have, and it's accepted as ground truth", "tokens": [51668, 300, 321, 362, 11, 293, 309, 311, 9035, 382, 2727, 3494, 51832], "temperature": 0.0, "avg_logprob": -0.14287073298018108, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.0008824285469017923}, {"id": 116, "seek": 51724, "start": 517.24, "end": 519.84, "text": " for most of the AI models, but this data set", "tokens": [50364, 337, 881, 295, 264, 7318, 5245, 11, 457, 341, 1412, 992, 50494], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 117, "seek": 51724, "start": 519.84, "end": 522.96, "text": " does have known issues such as relatively poor precipitation.", "tokens": [50494, 775, 362, 2570, 2663, 1270, 382, 7226, 4716, 37662, 13, 50650], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 118, "seek": 51724, "start": 524.88, "end": 527.88, "text": " The other common feature about AI forecast models", "tokens": [50746, 440, 661, 2689, 4111, 466, 7318, 14330, 5245, 50896], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 119, "seek": 51724, "start": 527.88, "end": 530.6, "text": " is that most of them have sparse limited outputs.", "tokens": [50896, 307, 300, 881, 295, 552, 362, 637, 11668, 5567, 23930, 13, 51032], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 120, "seek": 51724, "start": 530.6, "end": 533.6, "text": " They tend to predict only a few variables.", "tokens": [51032, 814, 3928, 281, 6069, 787, 257, 1326, 9102, 13, 51182], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 121, "seek": 51724, "start": 533.6, "end": 537.08, "text": " They tend to predict a small subset of vertical levels", "tokens": [51182, 814, 3928, 281, 6069, 257, 1359, 25993, 295, 9429, 4358, 51356], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 122, "seek": 51724, "start": 537.08, "end": 540.16, "text": " compared to what we're used to from an operational forecast,", "tokens": [51356, 5347, 281, 437, 321, 434, 1143, 281, 490, 364, 16607, 14330, 11, 51510], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 123, "seek": 51724, "start": 540.16, "end": 542.96, "text": " and they have a limited selection of lead times.", "tokens": [51510, 293, 436, 362, 257, 5567, 9450, 295, 1477, 1413, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 124, "seek": 51724, "start": 544.32, "end": 546.44, "text": " This creates new downscaling problems.", "tokens": [51718, 639, 7829, 777, 760, 4417, 4270, 2740, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1248032543637337, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0004876999300904572}, {"id": 125, "seek": 54644, "start": 546.44, "end": 548.72, "text": " If you were in the plenary talk,", "tokens": [50364, 759, 291, 645, 294, 264, 499, 42245, 751, 11, 50478], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 126, "seek": 54644, "start": 548.72, "end": 551.6, "text": " you heard our friend from Nvidia say that", "tokens": [50478, 291, 2198, 527, 1277, 490, 46284, 584, 300, 50622], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 127, "seek": 54644, "start": 553.2800000000001, "end": 556.6, "text": " this might not be a barrier to full prediction,", "tokens": [50706, 341, 1062, 406, 312, 257, 13357, 281, 1577, 17630, 11, 50872], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 128, "seek": 54644, "start": 556.6, "end": 559.0, "text": " but at the same time, we're used to having the prediction", "tokens": [50872, 457, 412, 264, 912, 565, 11, 321, 434, 1143, 281, 1419, 264, 17630, 50992], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 129, "seek": 54644, "start": 559.0, "end": 561.24, "text": " plus all of the rich output for free,", "tokens": [50992, 1804, 439, 295, 264, 4593, 5598, 337, 1737, 11, 51104], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 130, "seek": 54644, "start": 561.24, "end": 565.44, "text": " and not having that means we'll need to contemplate", "tokens": [51104, 293, 406, 1419, 300, 1355, 321, 603, 643, 281, 19935, 473, 51314], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 131, "seek": 54644, "start": 565.44, "end": 567.1600000000001, "text": " new kinds of downscaling problems", "tokens": [51314, 777, 3685, 295, 760, 4417, 4270, 2740, 51400], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 132, "seek": 54644, "start": 567.1600000000001, "end": 571.5200000000001, "text": " to get rich data from a sparse forecast.", "tokens": [51400, 281, 483, 4593, 1412, 490, 257, 637, 11668, 14330, 13, 51618], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 133, "seek": 54644, "start": 571.5200000000001, "end": 573.8000000000001, "text": " And finally, these AI forecast systems", "tokens": [51618, 400, 2721, 11, 613, 7318, 14330, 3652, 51732], "temperature": 0.0, "avg_logprob": -0.09360796671647292, "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.0003457882849033922}, {"id": 134, "seek": 57380, "start": 573.8, "end": 577.4399999999999, "text": " have essentially no proofs of physical consistency.", "tokens": [50364, 362, 4476, 572, 8177, 82, 295, 4001, 14416, 13, 50546], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 135, "seek": 57380, "start": 577.4399999999999, "end": 582.4399999999999, "text": " Even hybrid models and development in that area", "tokens": [50546, 2754, 13051, 5245, 293, 3250, 294, 300, 1859, 50796], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 136, "seek": 57380, "start": 582.76, "end": 584.5999999999999, "text": " has a classical dynamical core,", "tokens": [50812, 575, 257, 13735, 5999, 804, 4965, 11, 50904], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 137, "seek": 57380, "start": 584.5999999999999, "end": 586.92, "text": " but leaves a physics system that is", "tokens": [50904, 457, 5510, 257, 10649, 1185, 300, 307, 51020], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 138, "seek": 57380, "start": 588.0799999999999, "end": 589.92, "text": " only tangentially concerned with things", "tokens": [51078, 787, 10266, 3137, 5922, 365, 721, 51170], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 139, "seek": 57380, "start": 589.92, "end": 591.0, "text": " like conserving energy.", "tokens": [51170, 411, 1014, 20186, 2281, 13, 51224], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 140, "seek": 57380, "start": 592.56, "end": 595.04, "text": " Now, medium range forecasts tend to break down", "tokens": [51302, 823, 11, 6399, 3613, 49421, 3928, 281, 1821, 760, 51426], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 141, "seek": 57380, "start": 595.04, "end": 596.3599999999999, "text": " into a few different categories.", "tokens": [51426, 666, 257, 1326, 819, 10479, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 142, "seek": 57380, "start": 596.3599999999999, "end": 599.0799999999999, "text": " The first and most conventional of these", "tokens": [51492, 440, 700, 293, 881, 16011, 295, 613, 51628], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 143, "seek": 57380, "start": 599.0799999999999, "end": 600.9599999999999, "text": " are deterministic models.", "tokens": [51628, 366, 15957, 3142, 5245, 13, 51722], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 144, "seek": 57380, "start": 600.9599999999999, "end": 602.4399999999999, "text": " These are analysis predictors,", "tokens": [51722, 1981, 366, 5215, 6069, 830, 11, 51796], "temperature": 0.0, "avg_logprob": -0.12004763670641967, "compression_ratio": 1.616600790513834, "no_speech_prob": 0.000588298193179071}, {"id": 145, "seek": 60244, "start": 602.44, "end": 604.32, "text": " and they essentially answer the question of,", "tokens": [50364, 293, 436, 4476, 1867, 264, 1168, 295, 11, 50458], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 146, "seek": 60244, "start": 604.32, "end": 606.96, "text": " if I have the atmospheric analysis at time zero,", "tokens": [50458, 498, 286, 362, 264, 28854, 5215, 412, 565, 4018, 11, 50590], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 147, "seek": 60244, "start": 606.96, "end": 608.6800000000001, "text": " what is the minimum error prediction", "tokens": [50590, 437, 307, 264, 7285, 6713, 17630, 50676], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 148, "seek": 60244, "start": 608.6800000000001, "end": 611.5600000000001, "text": " of what that analysis is going to be six hours from now?", "tokens": [50676, 295, 437, 300, 5215, 307, 516, 281, 312, 2309, 2496, 490, 586, 30, 50820], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 149, "seek": 60244, "start": 612.6, "end": 614.12, "text": " As a general rule, these tend to give", "tokens": [50872, 1018, 257, 2674, 4978, 11, 613, 3928, 281, 976, 50948], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 150, "seek": 60244, "start": 614.12, "end": 617.48, "text": " overly smooth forecasts that over long lead times,", "tokens": [50948, 24324, 5508, 49421, 300, 670, 938, 1477, 1413, 11, 51116], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 151, "seek": 60244, "start": 617.48, "end": 619.8800000000001, "text": " erase fine scale structure in the atmosphere.", "tokens": [51116, 23525, 2489, 4373, 3877, 294, 264, 8018, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 152, "seek": 60244, "start": 619.8800000000001, "end": 622.8800000000001, "text": " The widely held belief is that this is a consequence", "tokens": [51236, 440, 13371, 5167, 7107, 307, 300, 341, 307, 257, 18326, 51386], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 153, "seek": 60244, "start": 622.8800000000001, "end": 627.8800000000001, "text": " of training based on mean squared error measures,", "tokens": [51386, 295, 3097, 2361, 322, 914, 8889, 6713, 8000, 11, 51636], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 154, "seek": 60244, "start": 629.0400000000001, "end": 631.08, "text": " because the lowest mean squared error prediction", "tokens": [51694, 570, 264, 12437, 914, 8889, 6713, 17630, 51796], "temperature": 0.0, "avg_logprob": -0.1253638997808233, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.0003458767314441502}, {"id": 155, "seek": 63108, "start": 631.08, "end": 634.0, "text": " in the future is your ensemble average.", "tokens": [50364, 294, 264, 2027, 307, 428, 19492, 4274, 13, 50510], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 156, "seek": 63108, "start": 634.0, "end": 635.5200000000001, "text": " But the ensemble average is not", "tokens": [50510, 583, 264, 19492, 4274, 307, 406, 50586], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 157, "seek": 63108, "start": 635.5200000000001, "end": 637.84, "text": " a physically plausible forecast.", "tokens": [50586, 257, 9762, 39925, 14330, 13, 50702], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 158, "seek": 63108, "start": 639.5200000000001, "end": 642.64, "text": " Now, that being said, these models have shown great success,", "tokens": [50786, 823, 11, 300, 885, 848, 11, 613, 5245, 362, 4898, 869, 2245, 11, 50942], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 159, "seek": 63108, "start": 642.64, "end": 645.88, "text": " and having a really good ensemble mean prediction", "tokens": [50942, 293, 1419, 257, 534, 665, 19492, 914, 17630, 51104], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 160, "seek": 63108, "start": 645.88, "end": 647.64, "text": " of the future is still a really good prediction", "tokens": [51104, 295, 264, 2027, 307, 920, 257, 534, 665, 17630, 51192], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 161, "seek": 63108, "start": 647.64, "end": 648.48, "text": " of the future.", "tokens": [51192, 295, 264, 2027, 13, 51234], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 162, "seek": 63108, "start": 648.48, "end": 651.12, "text": " Each day of predictability translates to billions", "tokens": [51234, 6947, 786, 295, 6069, 2310, 28468, 281, 17375, 51366], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 163, "seek": 63108, "start": 651.12, "end": 655.44, "text": " or billions of dollars of enabled economic activity.", "tokens": [51366, 420, 17375, 295, 3808, 295, 15172, 4836, 5191, 13, 51582], "temperature": 0.0, "avg_logprob": -0.12221869362725152, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0009396954555995762}, {"id": 164, "seek": 65544, "start": 656.44, "end": 661.2800000000001, "text": " This category of models is in some sense the oldest,", "tokens": [50414, 639, 7719, 295, 5245, 307, 294, 512, 2020, 264, 14026, 11, 50656], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 165, "seek": 65544, "start": 661.2800000000001, "end": 663.08, "text": " and I really hesitate to use that word", "tokens": [50656, 293, 286, 534, 20842, 281, 764, 300, 1349, 50746], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 166, "seek": 65544, "start": 663.08, "end": 665.2, "text": " with a field that's about two or three years old,", "tokens": [50746, 365, 257, 2519, 300, 311, 466, 732, 420, 1045, 924, 1331, 11, 50852], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 167, "seek": 65544, "start": 665.2, "end": 668.12, "text": " but they're models from many different groups.", "tokens": [50852, 457, 436, 434, 5245, 490, 867, 819, 3935, 13, 50998], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 168, "seek": 65544, "start": 668.12, "end": 671.1600000000001, "text": " Graphcast and ForecastNet have been previously mentioned.", "tokens": [50998, 21884, 3734, 293, 9018, 3734, 31890, 362, 668, 8046, 2835, 13, 51150], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 169, "seek": 65544, "start": 671.1600000000001, "end": 672.6800000000001, "text": " Graphcast is a graph neural network.", "tokens": [51150, 21884, 3734, 307, 257, 4295, 18161, 3209, 13, 51226], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 170, "seek": 65544, "start": 672.6800000000001, "end": 676.6, "text": " ForecastNet is now a spectral Fourier neural operator", "tokens": [51226, 9018, 3734, 31890, 307, 586, 257, 42761, 36810, 18161, 12973, 51422], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 171, "seek": 65544, "start": 676.6, "end": 680.84, "text": " that operates in a spherical harmonic space.", "tokens": [51422, 300, 22577, 294, 257, 37300, 32270, 1901, 13, 51634], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 172, "seek": 65544, "start": 680.84, "end": 683.0400000000001, "text": " Pangu weather came out as a vision transformer,", "tokens": [51634, 49499, 84, 5503, 1361, 484, 382, 257, 5201, 31782, 11, 51744], "temperature": 0.0, "avg_logprob": -0.14624011850802698, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.002114708535373211}, {"id": 173, "seek": 68304, "start": 683.04, "end": 686.7199999999999, "text": " and AIFS is now apparently officially published", "tokens": [50364, 293, 7318, 29318, 307, 586, 7970, 12053, 6572, 50548], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 174, "seek": 68304, "start": 686.7199999999999, "end": 689.12, "text": " with a pre-print, and it's a graph transformer.", "tokens": [50548, 365, 257, 659, 12, 14030, 11, 293, 309, 311, 257, 4295, 31782, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 175, "seek": 68304, "start": 691.36, "end": 693.36, "text": " The jargon here is not that important,", "tokens": [50780, 440, 15181, 10660, 510, 307, 406, 300, 1021, 11, 50880], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 176, "seek": 68304, "start": 693.36, "end": 696.76, "text": " but the underlying point is that you can reach", "tokens": [50880, 457, 264, 14217, 935, 307, 300, 291, 393, 2524, 51050], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 177, "seek": 68304, "start": 696.76, "end": 698.12, "text": " a deterministic forecast", "tokens": [51050, 257, 15957, 3142, 14330, 51118], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 178, "seek": 68304, "start": 698.12, "end": 701.3199999999999, "text": " with many different AI architectures.", "tokens": [51118, 365, 867, 819, 7318, 6331, 1303, 13, 51278], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 179, "seek": 68304, "start": 701.3199999999999, "end": 705.76, "text": " There's, as of yet, no specific Royal Road to a forecast.", "tokens": [51278, 821, 311, 11, 382, 295, 1939, 11, 572, 2685, 12717, 11507, 281, 257, 14330, 13, 51500], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 180, "seek": 68304, "start": 706.88, "end": 709.12, "text": " The second category that I've somewhat arbitrarily divided", "tokens": [51556, 440, 1150, 7719, 300, 286, 600, 8344, 19071, 3289, 6666, 51668], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 181, "seek": 68304, "start": 709.12, "end": 711.1999999999999, "text": " things into are ensemble models.", "tokens": [51668, 721, 666, 366, 19492, 5245, 13, 51772], "temperature": 0.0, "avg_logprob": -0.14683491168635907, "compression_ratio": 1.5390625, "no_speech_prob": 0.00015591458941344172}, {"id": 182, "seek": 71120, "start": 711.2, "end": 714.1600000000001, "text": " These try to address the problem of overly smooth forecast", "tokens": [50364, 1981, 853, 281, 2985, 264, 1154, 295, 24324, 5508, 14330, 50512], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 183, "seek": 71120, "start": 714.1600000000001, "end": 716.84, "text": " by adding some element of randomness.", "tokens": [50512, 538, 5127, 512, 4478, 295, 4974, 1287, 13, 50646], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 184, "seek": 71120, "start": 716.84, "end": 718.84, "text": " An ensemble forecaster will receive", "tokens": [50646, 1107, 19492, 2091, 42640, 486, 4774, 50746], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 185, "seek": 71120, "start": 718.84, "end": 720.5200000000001, "text": " some kind of random data source,", "tokens": [50746, 512, 733, 295, 4974, 1412, 4009, 11, 50830], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 186, "seek": 71120, "start": 720.5200000000001, "end": 723.84, "text": " either a random number generator or a field of noise,", "tokens": [50830, 2139, 257, 4974, 1230, 19265, 420, 257, 2519, 295, 5658, 11, 50996], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 187, "seek": 71120, "start": 723.84, "end": 726.6400000000001, "text": " and it's asked to generate essentially different forecasts", "tokens": [50996, 293, 309, 311, 2351, 281, 8460, 4476, 819, 49421, 51136], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 188, "seek": 71120, "start": 726.6400000000001, "end": 728.5200000000001, "text": " from the same seven initial conditions.", "tokens": [51136, 490, 264, 912, 3407, 5883, 4487, 13, 51230], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 189, "seek": 71120, "start": 728.5200000000001, "end": 733.48, "text": " This more or less solves the problem of smoothing", "tokens": [51230, 639, 544, 420, 1570, 39890, 264, 1154, 295, 899, 6259, 571, 51478], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 190, "seek": 71120, "start": 733.48, "end": 735.48, "text": " based on mean squared error loss functions", "tokens": [51478, 2361, 322, 914, 8889, 6713, 4470, 6828, 51578], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 191, "seek": 71120, "start": 735.48, "end": 739.36, "text": " because each individual forecast no longer has to track", "tokens": [51578, 570, 1184, 2609, 14330, 572, 2854, 575, 281, 2837, 51772], "temperature": 0.0, "avg_logprob": -0.12841258539217654, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.000855589285492897}, {"id": 192, "seek": 73936, "start": 739.36, "end": 744.72, "text": " the long-term forecast.", "tokens": [50364, 264, 938, 12, 7039, 14330, 13, 50632], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 193, "seek": 73936, "start": 744.72, "end": 746.44, "text": " No longer has to track the long-term truth,", "tokens": [50632, 883, 2854, 575, 281, 2837, 264, 938, 12, 7039, 3494, 11, 50718], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 194, "seek": 73936, "start": 746.44, "end": 747.84, "text": " but each one can be plausible,", "tokens": [50718, 457, 1184, 472, 393, 312, 39925, 11, 50788], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 195, "seek": 73936, "start": 747.84, "end": 751.0, "text": " and then you're tracking the future", "tokens": [50788, 293, 550, 291, 434, 11603, 264, 2027, 50946], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 196, "seek": 73936, "start": 751.0, "end": 752.84, "text": " with a true ensemble mean.", "tokens": [50946, 365, 257, 2074, 19492, 914, 13, 51038], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 197, "seek": 73936, "start": 752.84, "end": 754.5600000000001, "text": " Now, the downside is that these models", "tokens": [51038, 823, 11, 264, 25060, 307, 300, 613, 5245, 51124], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 198, "seek": 73936, "start": 754.5600000000001, "end": 756.5600000000001, "text": " are all more expensive to train and run", "tokens": [51124, 366, 439, 544, 5124, 281, 3847, 293, 1190, 51224], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 199, "seek": 73936, "start": 756.5600000000001, "end": 759.32, "text": " the deterministic systems of the same size.", "tokens": [51224, 264, 15957, 3142, 3652, 295, 264, 912, 2744, 13, 51362], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 200, "seek": 73936, "start": 759.32, "end": 762.5600000000001, "text": " In operations, you will need at least one inference", "tokens": [51362, 682, 7705, 11, 291, 486, 643, 412, 1935, 472, 38253, 51524], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 201, "seek": 73936, "start": 762.5600000000001, "end": 764.12, "text": " run per ensemble member.", "tokens": [51524, 1190, 680, 19492, 4006, 13, 51602], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 202, "seek": 73936, "start": 764.12, "end": 769.04, "text": " So, graphcast's 30-second, 10-day forecast is great,", "tokens": [51602, 407, 11, 4295, 3734, 311, 2217, 12, 27375, 11, 1266, 12, 810, 14330, 307, 869, 11, 51848], "temperature": 0.0, "avg_logprob": -0.16227469485030216, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0010317496489733458}, {"id": 203, "seek": 76904, "start": 769.04, "end": 771.24, "text": " but if I want a 100-member ensemble,", "tokens": [50364, 457, 498, 286, 528, 257, 2319, 12, 38249, 19492, 11, 50474], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 204, "seek": 76904, "start": 771.24, "end": 774.52, "text": " now I'm talking an hour or two,", "tokens": [50474, 586, 286, 478, 1417, 364, 1773, 420, 732, 11, 50638], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 205, "seek": 76904, "start": 774.52, "end": 778.0, "text": " and that can add up very quickly.", "tokens": [50638, 293, 300, 393, 909, 493, 588, 2661, 13, 50812], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 206, "seek": 76904, "start": 778.0, "end": 779.7199999999999, "text": " In terms of training, if you're training", "tokens": [50812, 682, 2115, 295, 3097, 11, 498, 291, 434, 3097, 50898], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 207, "seek": 76904, "start": 779.7199999999999, "end": 781.0799999999999, "text": " with an ensemble error measure", "tokens": [50898, 365, 364, 19492, 6713, 3481, 50966], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 208, "seek": 76904, "start": 781.0799999999999, "end": 784.0799999999999, "text": " like the cumulative rank probabilistic score,", "tokens": [50966, 411, 264, 38379, 6181, 31959, 3142, 6175, 11, 51116], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 209, "seek": 76904, "start": 784.0799999999999, "end": 786.04, "text": " that requires training over an ensemble,", "tokens": [51116, 300, 7029, 3097, 670, 364, 19492, 11, 51214], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 210, "seek": 76904, "start": 786.04, "end": 789.12, "text": " and increasing the size of things during training", "tokens": [51214, 293, 5662, 264, 2744, 295, 721, 1830, 3097, 51368], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 211, "seek": 76904, "start": 789.12, "end": 793.5999999999999, "text": " tends to increase the scarce GPU memory requirements", "tokens": [51368, 12258, 281, 3488, 264, 41340, 18407, 4675, 7728, 51592], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 212, "seek": 76904, "start": 793.5999999999999, "end": 798.28, "text": " and the overall cost of building the system.", "tokens": [51592, 293, 264, 4787, 2063, 295, 2390, 264, 1185, 13, 51826], "temperature": 0.0, "avg_logprob": -0.12030920010168576, "compression_ratio": 1.6626016260162602, "no_speech_prob": 0.004750493448227644}, {"id": 213, "seek": 79828, "start": 798.28, "end": 800.16, "text": " This is a newer frontier of AI forecasting.", "tokens": [50364, 639, 307, 257, 17628, 35853, 295, 7318, 44331, 13, 50458], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 214, "seek": 79828, "start": 800.16, "end": 802.9599999999999, "text": " It's probably going to be more popular in the months to come,", "tokens": [50458, 467, 311, 1391, 516, 281, 312, 544, 3743, 294, 264, 2493, 281, 808, 11, 50598], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 215, "seek": 79828, "start": 802.9599999999999, "end": 805.9599999999999, "text": " and the examples here are also somewhat newer.", "tokens": [50598, 293, 264, 5110, 510, 366, 611, 8344, 17628, 13, 50748], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 216, "seek": 79828, "start": 805.9599999999999, "end": 808.4, "text": " Gencast was previously mentioned in our plenary.", "tokens": [50748, 3632, 3734, 390, 8046, 2835, 294, 527, 499, 42245, 13, 50870], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 217, "seek": 79828, "start": 808.4, "end": 811.4, "text": " It's a diffusion model based on graph transformer network.", "tokens": [50870, 467, 311, 257, 25242, 2316, 2361, 322, 4295, 31782, 3209, 13, 51020], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 218, "seek": 79828, "start": 811.4, "end": 815.56, "text": " Neural GCM is a hybrid model, also previously mentioned,", "tokens": [51020, 1734, 1807, 29435, 44, 307, 257, 13051, 2316, 11, 611, 8046, 2835, 11, 51228], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 219, "seek": 79828, "start": 815.56, "end": 817.8, "text": " and it's a dynamical core", "tokens": [51228, 293, 309, 311, 257, 5999, 804, 4965, 51340], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 220, "seek": 79828, "start": 817.8, "end": 819.92, "text": " that has learned physics parameterizations,", "tokens": [51340, 300, 575, 3264, 10649, 13075, 14455, 11, 51446], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 221, "seek": 79828, "start": 819.92, "end": 822.0, "text": " and in particular, can operate in a non-subtle mode,", "tokens": [51446, 293, 294, 1729, 11, 393, 9651, 294, 257, 2107, 12, 30131, 10972, 4391, 11, 51550], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 222, "seek": 79828, "start": 822.0, "end": 823.68, "text": " which is why I've included it here.", "tokens": [51550, 597, 307, 983, 286, 600, 5556, 309, 510, 13, 51634], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 223, "seek": 79828, "start": 823.68, "end": 826.28, "text": " And finally, there are models such as seeds,", "tokens": [51634, 400, 2721, 11, 456, 366, 5245, 1270, 382, 9203, 11, 51764], "temperature": 0.0, "avg_logprob": -0.15825984425788378, "compression_ratio": 1.6592356687898089, "no_speech_prob": 0.0950840637087822}, {"id": 224, "seek": 82628, "start": 826.28, "end": 829.4, "text": " also out of Google, that don't try to forecast directly,", "tokens": [50364, 611, 484, 295, 3329, 11, 300, 500, 380, 853, 281, 14330, 3838, 11, 50520], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 225, "seek": 82628, "start": 829.4, "end": 833.16, "text": " but they try to take an ensemble mean", "tokens": [50520, 457, 436, 853, 281, 747, 364, 19492, 914, 50708], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 226, "seek": 82628, "start": 834.28, "end": 835.8, "text": " that already exists from some method,", "tokens": [50764, 300, 1217, 8198, 490, 512, 3170, 11, 50840], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 227, "seek": 82628, "start": 835.8, "end": 837.28, "text": " like a traditional forecasting system,", "tokens": [50840, 411, 257, 5164, 44331, 1185, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 228, "seek": 82628, "start": 837.28, "end": 839.0, "text": " and generate new ensemble members", "tokens": [50914, 293, 8460, 777, 19492, 2679, 51000], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 229, "seek": 82628, "start": 839.0, "end": 841.52, "text": " to fill out the probability distributions.", "tokens": [51000, 281, 2836, 484, 264, 8482, 37870, 13, 51126], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 230, "seek": 82628, "start": 842.6, "end": 844.76, "text": " The final category of forecast models,", "tokens": [51180, 440, 2572, 7719, 295, 14330, 5245, 11, 51288], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 231, "seek": 82628, "start": 844.76, "end": 847.64, "text": " and I'm going to divide things into our foundation models,", "tokens": [51288, 293, 286, 478, 516, 281, 9845, 721, 666, 527, 7030, 5245, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 232, "seek": 82628, "start": 847.64, "end": 849.1999999999999, "text": " and these essentially answer the question of,", "tokens": [51432, 293, 613, 4476, 1867, 264, 1168, 295, 11, 51510], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 233, "seek": 82628, "start": 849.1999999999999, "end": 852.3199999999999, "text": " what if we had GPT-4, but for weather?", "tokens": [51510, 437, 498, 321, 632, 26039, 51, 12, 19, 11, 457, 337, 5503, 30, 51666], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 234, "seek": 82628, "start": 852.3199999999999, "end": 855.4, "text": " The idea here is and shared by foundation models", "tokens": [51666, 440, 1558, 510, 307, 293, 5507, 538, 7030, 5245, 51820], "temperature": 0.0, "avg_logprob": -0.1358506178655544, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.0015971665270626545}, {"id": 235, "seek": 85540, "start": 855.4, "end": 856.68, "text": " that exist in our development,", "tokens": [50364, 300, 2514, 294, 527, 3250, 11, 50428], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 236, "seek": 85540, "start": 856.68, "end": 859.88, "text": " is that you break up a weather state,", "tokens": [50428, 307, 300, 291, 1821, 493, 257, 5503, 1785, 11, 50588], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 237, "seek": 85540, "start": 859.88, "end": 861.88, "text": " like the analysis, into tokens", "tokens": [50588, 411, 264, 5215, 11, 666, 22667, 50688], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 238, "seek": 85540, "start": 861.88, "end": 864.92, "text": " by regionally subdividing it usually,", "tokens": [50688, 538, 4458, 379, 31662, 1843, 278, 309, 2673, 11, 50840], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 239, "seek": 85540, "start": 864.92, "end": 867.4, "text": " and then you ask the foundation model", "tokens": [50840, 293, 550, 291, 1029, 264, 7030, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 240, "seek": 85540, "start": 867.4, "end": 869.52, "text": " to predict missing pieces of it.", "tokens": [50964, 281, 6069, 5361, 3755, 295, 309, 13, 51070], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 241, "seek": 85540, "start": 869.52, "end": 872.12, "text": " You say, you can mask out the future,", "tokens": [51070, 509, 584, 11, 291, 393, 6094, 484, 264, 2027, 11, 51200], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 242, "seek": 85540, "start": 872.12, "end": 873.48, "text": " and say, okay, predict this,", "tokens": [51200, 293, 584, 11, 1392, 11, 6069, 341, 11, 51268], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 243, "seek": 85540, "start": 873.48, "end": 876.6, "text": " and then you're training it in a forecast context,", "tokens": [51268, 293, 550, 291, 434, 3097, 309, 294, 257, 14330, 4319, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 244, "seek": 85540, "start": 876.6, "end": 879.84, "text": " or you can mask out a missing regional piece,", "tokens": [51424, 420, 291, 393, 6094, 484, 257, 5361, 10964, 2522, 11, 51586], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 245, "seek": 85540, "start": 879.84, "end": 881.16, "text": " and ask it to fill in the blank.", "tokens": [51586, 293, 1029, 309, 281, 2836, 294, 264, 8247, 13, 51652], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 246, "seek": 85540, "start": 881.16, "end": 883.84, "text": " You could even ask it to predict the past, I suppose.", "tokens": [51652, 509, 727, 754, 1029, 309, 281, 6069, 264, 1791, 11, 286, 7297, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10918684739332933, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.0019859205931425095}, {"id": 247, "seek": 88540, "start": 885.64, "end": 888.0, "text": " This allows the foundation model", "tokens": [50376, 639, 4045, 264, 7030, 2316, 50494], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 248, "seek": 88540, "start": 888.0, "end": 891.72, "text": " to be trained on qualitatively different data sources.", "tokens": [50494, 281, 312, 8895, 322, 31312, 356, 819, 1412, 7139, 13, 50680], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 249, "seek": 88540, "start": 891.72, "end": 896.12, "text": " For example, you might have an encoder suite", "tokens": [50680, 1171, 1365, 11, 291, 1062, 362, 364, 2058, 19866, 14205, 50900], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 250, "seek": 88540, "start": 896.12, "end": 898.28, "text": " that takes satellite observations directly,", "tokens": [50900, 300, 2516, 16016, 18163, 3838, 11, 51008], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 251, "seek": 88540, "start": 898.28, "end": 900.74, "text": " and tries to turn it into token space,", "tokens": [51008, 293, 9898, 281, 1261, 309, 666, 14862, 1901, 11, 51131], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 252, "seek": 88540, "start": 901.8, "end": 904.72, "text": " or the analysis from era five,", "tokens": [51184, 420, 264, 5215, 490, 4249, 1732, 11, 51330], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 253, "seek": 88540, "start": 904.72, "end": 909.12, "text": " or lower resolution climate forecasts.", "tokens": [51330, 420, 3126, 8669, 5659, 49421, 13, 51550], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 254, "seek": 88540, "start": 910.3199999999999, "end": 913.76, "text": " And once tokenized, the idea of a foundation model", "tokens": [51610, 400, 1564, 14862, 1602, 11, 264, 1558, 295, 257, 7030, 2316, 51782], "temperature": 0.0, "avg_logprob": -0.13158442334430973, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0004876668972428888}, {"id": 255, "seek": 91376, "start": 913.76, "end": 916.48, "text": " is that you have a giant middle processor layer", "tokens": [50364, 307, 300, 291, 362, 257, 7410, 2808, 15321, 4583, 50500], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 256, "seek": 91376, "start": 916.48, "end": 918.4399999999999, "text": " that operates in this latent space,", "tokens": [50500, 300, 22577, 294, 341, 48994, 1901, 11, 50598], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 257, "seek": 91376, "start": 918.4399999999999, "end": 920.84, "text": " and from there it is relatively simple", "tokens": [50598, 293, 490, 456, 309, 307, 7226, 2199, 50718], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 258, "seek": 91376, "start": 920.84, "end": 923.76, "text": " to build out decoder models to give you things you want,", "tokens": [50718, 281, 1322, 484, 979, 19866, 5245, 281, 976, 291, 721, 291, 528, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 259, "seek": 91376, "start": 923.76, "end": 925.4399999999999, "text": " like tomorrow's forecast today,", "tokens": [50864, 411, 4153, 311, 14330, 965, 11, 50948], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 260, "seek": 91376, "start": 925.4399999999999, "end": 929.3199999999999, "text": " or what the radar is going to look like in 30 minutes.", "tokens": [50948, 420, 437, 264, 16544, 307, 516, 281, 574, 411, 294, 2217, 2077, 13, 51142], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 261, "seek": 91376, "start": 930.48, "end": 933.18, "text": " Foundation models certainly prove their worth", "tokens": [51200, 10335, 5245, 3297, 7081, 641, 3163, 51335], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 262, "seek": 91376, "start": 933.18, "end": 936.08, "text": " with text-based processing like GPT,", "tokens": [51335, 365, 2487, 12, 6032, 9007, 411, 26039, 51, 11, 51480], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 263, "seek": 91376, "start": 936.08, "end": 938.56, "text": " and they're also very popular and emerging", "tokens": [51480, 293, 436, 434, 611, 588, 3743, 293, 14989, 51604], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 264, "seek": 91376, "start": 938.56, "end": 940.84, "text": " in Earth observation applications", "tokens": [51604, 294, 4755, 14816, 5821, 51718], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 265, "seek": 91376, "start": 940.84, "end": 942.8, "text": " with interpretation of satellite data.", "tokens": [51718, 365, 14174, 295, 16016, 1412, 13, 51816], "temperature": 0.0, "avg_logprob": -0.10690070050103324, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.0002867449657060206}, {"id": 266, "seek": 94280, "start": 942.8, "end": 947.4, "text": " For example, at an ECMWFESA conference a few weeks ago,", "tokens": [50364, 1171, 1365, 11, 412, 364, 19081, 44, 54, 37, 2358, 32, 7586, 257, 1326, 3259, 2057, 11, 50594], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 267, "seek": 94280, "start": 947.4, "end": 948.56, "text": " there were some interesting talks", "tokens": [50594, 456, 645, 512, 1880, 6686, 50652], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 268, "seek": 94280, "start": 948.56, "end": 951.0, "text": " about taking satellite data", "tokens": [50652, 466, 1940, 16016, 1412, 50774], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 269, "seek": 94280, "start": 951.0, "end": 955.8, "text": " and using it to infer tree cover near power lines in Norway,", "tokens": [50774, 293, 1228, 309, 281, 13596, 4230, 2060, 2651, 1347, 3876, 294, 24354, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 270, "seek": 94280, "start": 955.8, "end": 957.04, "text": " tasks that would normally take", "tokens": [51014, 9608, 300, 576, 5646, 747, 51076], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 271, "seek": 94280, "start": 957.04, "end": 958.8399999999999, "text": " some very expensive helicopters,", "tokens": [51076, 512, 588, 5124, 39016, 11, 51166], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 272, "seek": 94280, "start": 958.8399999999999, "end": 960.74, "text": " and doing them much, much more cheaply", "tokens": [51166, 293, 884, 552, 709, 11, 709, 544, 7084, 356, 51261], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 273, "seek": 94280, "start": 960.74, "end": 963.7199999999999, "text": " with readily available Earth observation data.", "tokens": [51261, 365, 26336, 2435, 4755, 14816, 1412, 13, 51410], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 274, "seek": 94280, "start": 963.7199999999999, "end": 965.4399999999999, "text": " The downside is that foundation models", "tokens": [51410, 440, 25060, 307, 300, 7030, 5245, 51496], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 275, "seek": 94280, "start": 965.4399999999999, "end": 967.92, "text": " tend to be extremely expensive to train,", "tokens": [51496, 3928, 281, 312, 4664, 5124, 281, 3847, 11, 51620], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 276, "seek": 94280, "start": 967.92, "end": 970.02, "text": " and these will push the limits", "tokens": [51620, 293, 613, 486, 2944, 264, 10406, 51725], "temperature": 0.0, "avg_logprob": -0.11312485616141503, "compression_ratio": 1.5905797101449275, "no_speech_prob": 0.00016087695257738233}, {"id": 277, "seek": 97002, "start": 970.06, "end": 972.86, "text": " of what the public sector is probably willing to spend", "tokens": [50366, 295, 437, 264, 1908, 6977, 307, 1391, 4950, 281, 3496, 50506], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 278, "seek": 97002, "start": 972.86, "end": 975.9399999999999, "text": " should foundation models prove themselves for forecasting.", "tokens": [50506, 820, 7030, 5245, 7081, 2969, 337, 44331, 13, 50660], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 279, "seek": 97002, "start": 975.9399999999999, "end": 978.18, "text": " Few of these models exist right now for NWP,", "tokens": [50660, 33468, 295, 613, 5245, 2514, 558, 586, 337, 426, 54, 47, 11, 50772], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 280, "seek": 97002, "start": 978.18, "end": 980.26, "text": " but there's plenty of private sector interest.", "tokens": [50772, 457, 456, 311, 7140, 295, 4551, 6977, 1179, 13, 50876], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 281, "seek": 97002, "start": 980.26, "end": 984.06, "text": " I mean, our friend from NVIDIA talked at length about that.", "tokens": [50876, 286, 914, 11, 527, 1277, 490, 426, 3958, 6914, 2825, 412, 4641, 466, 300, 13, 51066], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 282, "seek": 97002, "start": 984.06, "end": 987.14, "text": " The two models that are currently published", "tokens": [51066, 440, 732, 5245, 300, 366, 4362, 6572, 51220], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 283, "seek": 97002, "start": 987.14, "end": 989.18, "text": " are ATMO-REP from Christian Lesig,", "tokens": [51220, 366, 8872, 18976, 12, 3850, 47, 490, 5778, 6965, 328, 11, 51322], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 284, "seek": 97002, "start": 989.18, "end": 991.16, "text": " which is a transformer model,", "tokens": [51322, 597, 307, 257, 31782, 2316, 11, 51421], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 285, "seek": 97002, "start": 991.16, "end": 993.74, "text": " tested on both forecast and downscaling applications,", "tokens": [51421, 8246, 322, 1293, 14330, 293, 760, 4417, 4270, 5821, 11, 51550], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 286, "seek": 97002, "start": 993.74, "end": 997.3, "text": " and Microsoft has published, as of a few days ago,", "tokens": [51550, 293, 8116, 575, 6572, 11, 382, 295, 257, 1326, 1708, 2057, 11, 51728], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 287, "seek": 97002, "start": 997.3, "end": 998.66, "text": " it's Aurora Foundation model,", "tokens": [51728, 309, 311, 40663, 10335, 2316, 11, 51796], "temperature": 0.0, "avg_logprob": -0.13131712399996243, "compression_ratio": 1.6056782334384858, "no_speech_prob": 0.003374626627191901}, {"id": 288, "seek": 99866, "start": 998.66, "end": 1003.66, "text": " which most notably uses several different data sources", "tokens": [50364, 597, 881, 31357, 4960, 2940, 819, 1412, 7139, 50614], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 289, "seek": 99866, "start": 1003.6999999999999, "end": 1005.56, "text": " for training, not just air five,", "tokens": [50616, 337, 3097, 11, 406, 445, 1988, 1732, 11, 50709], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 290, "seek": 99866, "start": 1005.56, "end": 1008.8199999999999, "text": " but also climate simulations and operational forecasts,", "tokens": [50709, 457, 611, 5659, 35138, 293, 16607, 49421, 11, 50872], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 291, "seek": 99866, "start": 1008.8199999999999, "end": 1012.02, "text": " and I believe one of Noah's ensembles.", "tokens": [50872, 293, 286, 1697, 472, 295, 20895, 311, 12567, 2504, 904, 13, 51032], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 292, "seek": 99866, "start": 1013.18, "end": 1016.62, "text": " Now, to integrate all of this,", "tokens": [51090, 823, 11, 281, 13365, 439, 295, 341, 11, 51262], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 293, "seek": 99866, "start": 1016.62, "end": 1019.14, "text": " as a researcher, I can make a few broad predictions", "tokens": [51262, 382, 257, 21751, 11, 286, 393, 652, 257, 1326, 4152, 21264, 51388], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 294, "seek": 99866, "start": 1019.14, "end": 1022.74, "text": " on where the trends are in this area.", "tokens": [51388, 322, 689, 264, 13892, 366, 294, 341, 1859, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 295, "seek": 99866, "start": 1022.74, "end": 1024.68, "text": " First is going to be the convergence", "tokens": [51568, 2386, 307, 516, 281, 312, 264, 32181, 51665], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 296, "seek": 99866, "start": 1024.68, "end": 1026.18, "text": " of data simulation forecasts,", "tokens": [51665, 295, 1412, 16575, 49421, 11, 51740], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 297, "seek": 99866, "start": 1026.18, "end": 1028.1399999999999, "text": " nowcast and downscaling roles.", "tokens": [51740, 586, 3734, 293, 760, 4417, 4270, 9604, 13, 51838], "temperature": 0.0, "avg_logprob": -0.1814913068498884, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.00016088497068267316}, {"id": 298, "seek": 102814, "start": 1028.14, "end": 1031.1000000000001, "text": " Right now, the leading NWP forecast,", "tokens": [50364, 1779, 586, 11, 264, 5775, 426, 54, 47, 14330, 11, 50512], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 299, "seek": 102814, "start": 1031.1000000000001, "end": 1033.38, "text": " take in an analysis and give you a forecast,", "tokens": [50512, 747, 294, 364, 5215, 293, 976, 291, 257, 14330, 11, 50626], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 300, "seek": 102814, "start": 1033.38, "end": 1035.9, "text": " but the obvious question is to what extent", "tokens": [50626, 457, 264, 6322, 1168, 307, 281, 437, 8396, 50752], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 301, "seek": 102814, "start": 1035.9, "end": 1038.14, "text": " can the rest of the chain be included?", "tokens": [50752, 393, 264, 1472, 295, 264, 5021, 312, 5556, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 302, "seek": 102814, "start": 1039.7800000000002, "end": 1043.46, "text": " For example, forecasts that are ensemble generating", "tokens": [50946, 1171, 1365, 11, 49421, 300, 366, 19492, 17746, 51130], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 303, "seek": 102814, "start": 1043.46, "end": 1045.5600000000002, "text": " can often be reversed to perform data simulation,", "tokens": [51130, 393, 2049, 312, 30563, 281, 2042, 1412, 16575, 11, 51235], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 304, "seek": 102814, "start": 1045.5600000000002, "end": 1048.18, "text": " and a couple of references here are one,", "tokens": [51235, 293, 257, 1916, 295, 15400, 510, 366, 472, 11, 51366], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 305, "seek": 102814, "start": 1048.18, "end": 1049.76, "text": " using a diffusion-based model", "tokens": [51366, 1228, 257, 25242, 12, 6032, 2316, 51445], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 306, "seek": 102814, "start": 1049.76, "end": 1052.3400000000001, "text": " to assimilate sparse observations,", "tokens": [51445, 281, 8249, 48104, 637, 11668, 18163, 11, 51574], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 307, "seek": 102814, "start": 1052.3400000000001, "end": 1056.14, "text": " and the second one is to perform data simulation", "tokens": [51574, 293, 264, 1150, 472, 307, 281, 2042, 1412, 16575, 51764], "temperature": 0.0, "avg_logprob": -0.1570514785909207, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.00010550497245276347}, {"id": 308, "seek": 105614, "start": 1056.14, "end": 1058.5800000000002, "text": " inside the latent space of an autoencoder,", "tokens": [50364, 1854, 264, 48994, 1901, 295, 364, 8399, 22660, 19866, 11, 50486], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 309, "seek": 105614, "start": 1058.5800000000002, "end": 1061.66, "text": " which effectively replaces the background", "tokens": [50486, 597, 8659, 46734, 264, 3678, 50640], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 310, "seek": 105614, "start": 1061.66, "end": 1065.98, "text": " error covariance matrices of a DA system", "tokens": [50640, 6713, 49851, 719, 32284, 295, 257, 9578, 1185, 50856], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 311, "seek": 105614, "start": 1065.98, "end": 1069.38, "text": " with ones that are nonlinear and flow dependent", "tokens": [50856, 365, 2306, 300, 366, 2107, 28263, 293, 3095, 12334, 51026], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 312, "seek": 105614, "start": 1069.38, "end": 1071.18, "text": " from the autoencoder space.", "tokens": [51026, 490, 264, 8399, 22660, 19866, 1901, 13, 51116], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 313, "seek": 105614, "start": 1071.18, "end": 1074.5800000000002, "text": " A second avenue here is to combine conventional models", "tokens": [51116, 316, 1150, 39230, 510, 307, 281, 10432, 16011, 5245, 51286], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 314, "seek": 105614, "start": 1074.5800000000002, "end": 1076.66, "text": " with an AI-based bias correction.", "tokens": [51286, 365, 364, 7318, 12, 6032, 12577, 19984, 13, 51390], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 315, "seek": 105614, "start": 1076.66, "end": 1080.7, "text": " Farshie from ECMWF has published recently", "tokens": [51390, 479, 7064, 414, 490, 19081, 44, 54, 37, 575, 6572, 3938, 51592], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 316, "seek": 105614, "start": 1080.7, "end": 1083.5400000000002, "text": " on using a neural network bias correction", "tokens": [51592, 322, 1228, 257, 18161, 3209, 12577, 19984, 51734], "temperature": 0.0, "avg_logprob": -0.14214889039384557, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0016471006674692035}, {"id": 317, "seek": 108354, "start": 1083.54, "end": 1088.54, "text": " to include some element of AI inside the IFS-40 var system.", "tokens": [50364, 281, 4090, 512, 4478, 295, 7318, 1854, 264, 26080, 50, 12, 5254, 1374, 1185, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 318, "seek": 108354, "start": 1088.86, "end": 1091.26, "text": " And Said, my colleague, is going to present", "tokens": [50630, 400, 26490, 11, 452, 13532, 11, 307, 516, 281, 1974, 50750], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 319, "seek": 108354, "start": 1091.26, "end": 1095.26, "text": " in about half an hour on spectral nudging", "tokens": [50750, 294, 466, 1922, 364, 1773, 322, 42761, 40045, 3249, 50950], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 320, "seek": 108354, "start": 1095.26, "end": 1098.8999999999999, "text": " to bring a classical NWP system", "tokens": [50950, 281, 1565, 257, 13735, 426, 54, 47, 1185, 51132], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 321, "seek": 108354, "start": 1098.8999999999999, "end": 1103.46, "text": " closer to an AI forecast to preserve rich data", "tokens": [51132, 4966, 281, 364, 7318, 14330, 281, 15665, 4593, 1412, 51360], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 322, "seek": 108354, "start": 1103.46, "end": 1106.1399999999999, "text": " and have the accuracy of AI.", "tokens": [51360, 293, 362, 264, 14170, 295, 7318, 13, 51494], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 323, "seek": 108354, "start": 1107.78, "end": 1109.22, "text": " And finally, there's some effort", "tokens": [51576, 400, 2721, 11, 456, 311, 512, 4630, 51648], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 324, "seek": 108354, "start": 1109.22, "end": 1111.18, "text": " on all-in-one AI forecast systems", "tokens": [51648, 322, 439, 12, 259, 12, 546, 7318, 14330, 3652, 51746], "temperature": 0.0, "avg_logprob": -0.14233621307041333, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.0007205363363027573}, {"id": 325, "seek": 111118, "start": 1111.18, "end": 1114.1000000000001, "text": " that go directly from observations to forecasts", "tokens": [50364, 300, 352, 3838, 490, 18163, 281, 49421, 50510], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 326, "seek": 111118, "start": 1114.1000000000001, "end": 1116.42, "text": " to post-processing to produce predictions", "tokens": [50510, 281, 2183, 12, 41075, 278, 281, 5258, 21264, 50626], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 327, "seek": 111118, "start": 1116.42, "end": 1117.7, "text": " of future observations.", "tokens": [50626, 295, 2027, 18163, 13, 50690], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 328, "seek": 111118, "start": 1119.14, "end": 1122.3, "text": " The Vaughn 2024 is the Aurora mentioned in the plenary,", "tokens": [50762, 440, 16822, 1984, 77, 45237, 307, 264, 40663, 2835, 294, 264, 499, 42245, 11, 50920], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 329, "seek": 111118, "start": 1122.3, "end": 1124.78, "text": " which is an encoder, decoder architecture", "tokens": [50920, 597, 307, 364, 2058, 19866, 11, 979, 19866, 9482, 51044], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 330, "seek": 111118, "start": 1124.78, "end": 1127.98, "text": " that uses Unets, and it's the obvious long-term future", "tokens": [51044, 300, 4960, 1156, 1385, 11, 293, 309, 311, 264, 6322, 938, 12, 7039, 2027, 51204], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 331, "seek": 111118, "start": 1127.98, "end": 1129.66, "text": " of foundation models.", "tokens": [51204, 295, 7030, 5245, 13, 51288], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 332, "seek": 111118, "start": 1129.66, "end": 1131.78, "text": " There's also rumors that Google is buying up", "tokens": [51288, 821, 311, 611, 21201, 300, 3329, 307, 6382, 493, 51394], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 333, "seek": 111118, "start": 1131.78, "end": 1134.38, "text": " satellite data for its next generation,", "tokens": [51394, 16016, 1412, 337, 1080, 958, 5125, 11, 51524], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 334, "seek": 111118, "start": 1134.38, "end": 1136.46, "text": " GraphCast version two or something like that,", "tokens": [51524, 21884, 34, 525, 3037, 732, 420, 746, 411, 300, 11, 51628], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 335, "seek": 111118, "start": 1136.46, "end": 1138.8200000000002, "text": " but I don't have any particular", "tokens": [51628, 457, 286, 500, 380, 362, 604, 1729, 51746], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 336, "seek": 111118, "start": 1138.8200000000002, "end": 1140.38, "text": " confidential information to share.", "tokens": [51746, 27054, 1589, 281, 2073, 13, 51824], "temperature": 0.0, "avg_logprob": -0.17736658235875571, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0021150035317987204}, {"id": 337, "seek": 114118, "start": 1141.18, "end": 1142.78, "text": " Okay, now the second part of this talk", "tokens": [50364, 1033, 11, 586, 264, 1150, 644, 295, 341, 751, 50444], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 338, "seek": 114118, "start": 1142.78, "end": 1145.18, "text": " is the Environment and Climate Change Canada AI roadmap.", "tokens": [50444, 307, 264, 35354, 293, 27025, 15060, 6309, 7318, 35738, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 339, "seek": 114118, "start": 1145.18, "end": 1149.46, "text": " This is a joint product of the AASTD and CCMAP.", "tokens": [50564, 639, 307, 257, 7225, 1674, 295, 264, 316, 20398, 35, 293, 12630, 44, 4715, 13, 50778], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 340, "seek": 114118, "start": 1149.46, "end": 1151.38, "text": " It's a product of an internal Tiger team", "tokens": [50778, 467, 311, 257, 1674, 295, 364, 6920, 22025, 1469, 50874], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 341, "seek": 114118, "start": 1151.38, "end": 1154.02, "text": " across the research and operational divisions", "tokens": [50874, 2108, 264, 2132, 293, 16607, 24328, 51006], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 342, "seek": 114118, "start": 1154.02, "end": 1157.3400000000001, "text": " that was put together last fall after an internal study", "tokens": [51006, 300, 390, 829, 1214, 1036, 2100, 934, 364, 6920, 2979, 51172], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 343, "seek": 114118, "start": 1157.3400000000001, "end": 1159.5, "text": " on the current state of AI and weather forecasting,", "tokens": [51172, 322, 264, 2190, 1785, 295, 7318, 293, 5503, 44331, 11, 51280], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 344, "seek": 114118, "start": 1159.5, "end": 1160.66, "text": " where we had a whole lot of talks", "tokens": [51280, 689, 321, 632, 257, 1379, 688, 295, 6686, 51338], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 345, "seek": 114118, "start": 1160.66, "end": 1163.26, "text": " like the first half of what I just gave.", "tokens": [51338, 411, 264, 700, 1922, 295, 437, 286, 445, 2729, 13, 51468], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 346, "seek": 114118, "start": 1163.26, "end": 1166.6200000000001, "text": " This roadmap is designed to set very broad research priorities.", "tokens": [51468, 639, 35738, 307, 4761, 281, 992, 588, 4152, 2132, 15503, 13, 51636], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 347, "seek": 114118, "start": 1166.6200000000001, "end": 1169.42, "text": " It's not designed to currently pick and choose", "tokens": [51636, 467, 311, 406, 4761, 281, 4362, 1888, 293, 2826, 51776], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 348, "seek": 114118, "start": 1169.42, "end": 1170.5800000000002, "text": " what projects are worth funding,", "tokens": [51776, 437, 4455, 366, 3163, 6137, 11, 51834], "temperature": 0.0, "avg_logprob": -0.09559519513905478, "compression_ratio": 1.7033639143730888, "no_speech_prob": 0.00023773493012413383}, {"id": 349, "seek": 117058, "start": 1170.58, "end": 1174.1799999999998, "text": " but to set out how we should think about AI as an institution.", "tokens": [50364, 457, 281, 992, 484, 577, 321, 820, 519, 466, 7318, 382, 364, 7818, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 350, "seek": 117058, "start": 1175.34, "end": 1177.6999999999998, "text": " And the largest theme from this roadmap", "tokens": [50602, 400, 264, 6443, 6314, 490, 341, 35738, 50720], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 351, "seek": 117058, "start": 1177.6999999999998, "end": 1180.1, "text": " is the need for capacity building,", "tokens": [50720, 307, 264, 643, 337, 6042, 2390, 11, 50840], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 352, "seek": 117058, "start": 1180.1, "end": 1182.46, "text": " both in terms of compute capacity,", "tokens": [50840, 1293, 294, 2115, 295, 14722, 6042, 11, 50958], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 353, "seek": 117058, "start": 1182.46, "end": 1184.3799999999999, "text": " in light of the supercomputer update", "tokens": [50958, 294, 1442, 295, 264, 36708, 5623, 51054], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 354, "seek": 117058, "start": 1184.3799999999999, "end": 1185.62, "text": " we're likely to have next year", "tokens": [51054, 321, 434, 3700, 281, 362, 958, 1064, 51116], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 355, "seek": 117058, "start": 1185.62, "end": 1188.62, "text": " and whatever gets procured in the years thereafter.", "tokens": [51116, 293, 2035, 2170, 9510, 3831, 294, 264, 924, 38729, 13, 51266], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 356, "seek": 117058, "start": 1190.54, "end": 1192.6599999999999, "text": " How we should think about the use of cloud computing", "tokens": [51362, 1012, 321, 820, 519, 466, 264, 764, 295, 4588, 15866, 51468], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 357, "seek": 117058, "start": 1192.6599999999999, "end": 1194.5, "text": " and finally what we need to do", "tokens": [51468, 293, 2721, 437, 321, 643, 281, 360, 51560], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 358, "seek": 117058, "start": 1194.5, "end": 1196.86, "text": " from a human resources and training standpoint.", "tokens": [51560, 490, 257, 1952, 3593, 293, 3097, 15827, 13, 51678], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 359, "seek": 117058, "start": 1196.86, "end": 1198.8999999999999, "text": " This is a living document.", "tokens": [51678, 639, 307, 257, 2647, 4166, 13, 51780], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 360, "seek": 117058, "start": 1198.8999999999999, "end": 1200.1799999999998, "text": " It should be published soon.", "tokens": [51780, 467, 820, 312, 6572, 2321, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11107206738684788, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00017395535542163998}, {"id": 361, "seek": 120018, "start": 1200.46, "end": 1202.9, "text": " I'd hoped I could begin this talk", "tokens": [50378, 286, 1116, 19737, 286, 727, 1841, 341, 751, 50500], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 362, "seek": 120018, "start": 1202.9, "end": 1204.14, "text": " with a reference to the live document,", "tokens": [50500, 365, 257, 6408, 281, 264, 1621, 4166, 11, 50562], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 363, "seek": 120018, "start": 1204.14, "end": 1205.98, "text": " but I think it's still in translation.", "tokens": [50562, 457, 286, 519, 309, 311, 920, 294, 12853, 13, 50654], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 364, "seek": 120018, "start": 1205.98, "end": 1207.94, "text": " But once it is published,", "tokens": [50654, 583, 1564, 309, 307, 6572, 11, 50752], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 365, "seek": 120018, "start": 1207.94, "end": 1211.26, "text": " it'll be updated every few months to every year or so", "tokens": [50752, 309, 603, 312, 10588, 633, 1326, 2493, 281, 633, 1064, 420, 370, 50918], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 366, "seek": 120018, "start": 1211.26, "end": 1213.9, "text": " with internal reviews and updates", "tokens": [50918, 365, 6920, 10229, 293, 9205, 51050], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 367, "seek": 120018, "start": 1213.9, "end": 1215.8600000000001, "text": " just as we understand more about AI.", "tokens": [51050, 445, 382, 321, 1223, 544, 466, 7318, 13, 51148], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 368, "seek": 120018, "start": 1216.8600000000001, "end": 1218.98, "text": " The broad themes of this document", "tokens": [51198, 440, 4152, 13544, 295, 341, 4166, 51304], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 369, "seek": 120018, "start": 1218.98, "end": 1221.3400000000001, "text": " are how we intend to integrate AI", "tokens": [51304, 366, 577, 321, 19759, 281, 13365, 7318, 51422], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 370, "seek": 120018, "start": 1221.3400000000001, "end": 1223.5800000000002, "text": " throughout the research, development and operation cycle.", "tokens": [51422, 3710, 264, 2132, 11, 3250, 293, 6916, 6586, 13, 51534], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 371, "seek": 120018, "start": 1223.5800000000002, "end": 1227.46, "text": " So we're not limiting it to just graphcast style forecasts,", "tokens": [51534, 407, 321, 434, 406, 22083, 309, 281, 445, 4295, 3734, 3758, 49421, 11, 51728], "temperature": 0.0, "avg_logprob": -0.14744561236837636, "compression_ratio": 1.6057347670250897, "no_speech_prob": 0.00033510057255625725}, {"id": 372, "seek": 122746, "start": 1227.46, "end": 1230.5, "text": " but we're interested in applying AI everywhere", "tokens": [50364, 457, 321, 434, 3102, 294, 9275, 7318, 5315, 50516], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 373, "seek": 122746, "start": 1230.5, "end": 1234.78, "text": " from data gathering to post-processing.", "tokens": [50516, 490, 1412, 13519, 281, 2183, 12, 41075, 278, 13, 50730], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 374, "seek": 122746, "start": 1235.74, "end": 1238.06, "text": " The main evaluation criteria here", "tokens": [50778, 440, 2135, 13344, 11101, 510, 50894], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 375, "seek": 122746, "start": 1238.06, "end": 1241.26, "text": " are the triumvirate of feasibility,", "tokens": [50894, 366, 264, 1376, 449, 46551, 473, 295, 21781, 2841, 11, 51054], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 376, "seek": 122746, "start": 1241.26, "end": 1242.74, "text": " service and efficiency.", "tokens": [51054, 2643, 293, 10493, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 377, "seek": 122746, "start": 1242.74, "end": 1245.54, "text": " Feasibility answers the question of how capable are we", "tokens": [51128, 3697, 296, 2841, 6338, 264, 1168, 295, 577, 8189, 366, 321, 51268], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 378, "seek": 122746, "start": 1245.54, "end": 1250.22, "text": " of developing and running a proposed AI project?", "tokens": [51268, 295, 6416, 293, 2614, 257, 10348, 7318, 1716, 30, 51502], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 379, "seek": 122746, "start": 1250.22, "end": 1253.18, "text": " And that includes not just the scientific risk", "tokens": [51502, 400, 300, 5974, 406, 445, 264, 8134, 3148, 51650], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 380, "seek": 122746, "start": 1253.18, "end": 1256.42, "text": " of well, putting a whole lot of time and money", "tokens": [51650, 295, 731, 11, 3372, 257, 1379, 688, 295, 565, 293, 1460, 51812], "temperature": 0.0, "avg_logprob": -0.12478578467118112, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.0001851340348366648}, {"id": 381, "seek": 125642, "start": 1256.42, "end": 1258.18, "text": " into a project and having it not work,", "tokens": [50364, 666, 257, 1716, 293, 1419, 309, 406, 589, 11, 50452], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 382, "seek": 125642, "start": 1258.18, "end": 1260.98, "text": " but also whether or not we have the compute resources", "tokens": [50452, 457, 611, 1968, 420, 406, 321, 362, 264, 14722, 3593, 50592], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 383, "seek": 125642, "start": 1260.98, "end": 1265.02, "text": " to do this or the human resources to develop and manage it.", "tokens": [50592, 281, 360, 341, 420, 264, 1952, 3593, 281, 1499, 293, 3067, 309, 13, 50794], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 384, "seek": 125642, "start": 1266.54, "end": 1268.0600000000002, "text": " The second criterion is service", "tokens": [50870, 440, 1150, 46691, 307, 2643, 50946], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 385, "seek": 125642, "start": 1268.0600000000002, "end": 1271.46, "text": " of how a project will improve ECCC's", "tokens": [50946, 295, 577, 257, 1716, 486, 3470, 462, 11717, 34, 311, 51116], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 386, "seek": 125642, "start": 1271.46, "end": 1273.5, "text": " weather-based services to Canadians.", "tokens": [51116, 5503, 12, 6032, 3328, 281, 30053, 13, 51218], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 387, "seek": 125642, "start": 1273.5, "end": 1276.3000000000002, "text": " And that is not just in terms of accuracy,", "tokens": [51218, 400, 300, 307, 406, 445, 294, 2115, 295, 14170, 11, 51358], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 388, "seek": 125642, "start": 1276.3000000000002, "end": 1278.5800000000002, "text": " but also whether we can make our forecast products", "tokens": [51358, 457, 611, 1968, 321, 393, 652, 527, 14330, 3383, 51472], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 389, "seek": 125642, "start": 1278.5800000000002, "end": 1282.38, "text": " more timely, whether we can have more numerous projects", "tokens": [51472, 544, 25150, 11, 1968, 321, 393, 362, 544, 12546, 4455, 51662], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 390, "seek": 125642, "start": 1282.38, "end": 1285.3000000000002, "text": " or whether we can have qualitatively new products", "tokens": [51662, 420, 1968, 321, 393, 362, 31312, 356, 777, 3383, 51808], "temperature": 0.0, "avg_logprob": -0.09177011356019137, "compression_ratio": 1.8102766798418972, "no_speech_prob": 0.003119470551609993}, {"id": 391, "seek": 128530, "start": 1285.34, "end": 1289.94, "text": " like hypothetically, rapidly updated now casting,", "tokens": [50366, 411, 24371, 22652, 11, 12910, 10588, 586, 17301, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 392, "seek": 128530, "start": 1289.94, "end": 1291.7, "text": " which we just currently don't have available", "tokens": [50596, 597, 321, 445, 4362, 500, 380, 362, 2435, 50684], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 393, "seek": 128530, "start": 1291.7, "end": 1293.3, "text": " for many government source.", "tokens": [50684, 337, 867, 2463, 4009, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 394, "seek": 128530, "start": 1293.3, "end": 1295.58, "text": " And finally, there's a question of efficiency.", "tokens": [50764, 400, 2721, 11, 456, 311, 257, 1168, 295, 10493, 13, 50878], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 395, "seek": 128530, "start": 1295.58, "end": 1297.3799999999999, "text": " How will a project make efficient use", "tokens": [50878, 1012, 486, 257, 1716, 652, 7148, 764, 50968], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 396, "seek": 128530, "start": 1297.3799999999999, "end": 1299.1399999999999, "text": " of our computational resources?", "tokens": [50968, 295, 527, 28270, 3593, 30, 51056], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 397, "seek": 128530, "start": 1299.1399999999999, "end": 1302.1, "text": " That includes the broad themes of energy efficiency,", "tokens": [51056, 663, 5974, 264, 4152, 13544, 295, 2281, 10493, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 398, "seek": 128530, "start": 1302.1, "end": 1305.1, "text": " but also the government-specific theme", "tokens": [51204, 457, 611, 264, 2463, 12, 29258, 6314, 51354], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 399, "seek": 128530, "start": 1305.1, "end": 1308.3, "text": " of being a good steward of public funds.", "tokens": [51354, 295, 885, 257, 665, 2126, 1007, 295, 1908, 8271, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 400, "seek": 128530, "start": 1308.3, "end": 1312.58, "text": " Supercomputers are, as one might guess, rather expensive.", "tokens": [51514, 4548, 1112, 2582, 433, 366, 11, 382, 472, 1062, 2041, 11, 2831, 5124, 13, 51728], "temperature": 0.0, "avg_logprob": -0.10015949213279868, "compression_ratio": 1.5808823529411764, "no_speech_prob": 0.00035122610279358923}, {"id": 401, "seek": 131258, "start": 1312.58, "end": 1315.6999999999998, "text": " And we ought to demonstrate to Canadians", "tokens": [50364, 400, 321, 13416, 281, 11698, 281, 30053, 50520], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 402, "seek": 131258, "start": 1315.6999999999998, "end": 1319.1799999999998, "text": " that Canadians are getting their money's worth.", "tokens": [50520, 300, 30053, 366, 1242, 641, 1460, 311, 3163, 13, 50694], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 403, "seek": 131258, "start": 1320.26, "end": 1323.02, "text": " Okay, so more specifically,", "tokens": [50748, 1033, 11, 370, 544, 4682, 11, 50886], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 404, "seek": 131258, "start": 1323.82, "end": 1325.1, "text": " how are we looking at AI", "tokens": [50926, 577, 366, 321, 1237, 412, 7318, 50990], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 405, "seek": 131258, "start": 1325.1, "end": 1328.06, "text": " from the observation to product pipeline?", "tokens": [50990, 490, 264, 14816, 281, 1674, 15517, 30, 51138], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 406, "seek": 131258, "start": 1329.1, "end": 1331.6599999999999, "text": " First category here are observations and data assimilation.", "tokens": [51190, 2386, 7719, 510, 366, 18163, 293, 1412, 8249, 16067, 13, 51318], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 407, "seek": 131258, "start": 1331.6599999999999, "end": 1334.9399999999998, "text": " So in observations, AI is in some sense", "tokens": [51318, 407, 294, 18163, 11, 7318, 307, 294, 512, 2020, 51482], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 408, "seek": 131258, "start": 1334.9399999999998, "end": 1337.02, "text": " an extension of what's already happening", "tokens": [51482, 364, 10320, 295, 437, 311, 1217, 2737, 51586], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 409, "seek": 131258, "start": 1337.02, "end": 1339.06, "text": " in terms of looking at quality control", "tokens": [51586, 294, 2115, 295, 1237, 412, 3125, 1969, 51688], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 410, "seek": 131258, "start": 1339.06, "end": 1340.98, "text": " and error estimation of our inputs.", "tokens": [51688, 293, 6713, 35701, 295, 527, 15743, 13, 51784], "temperature": 0.0, "avg_logprob": -0.14598231514294943, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.00014420656953006983}, {"id": 411, "seek": 134258, "start": 1342.6599999999999, "end": 1347.6599999999999, "text": " AI can perhaps allow us to have some new capabilities", "tokens": [50368, 7318, 393, 4317, 2089, 505, 281, 362, 512, 777, 10862, 50618], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 412, "seek": 134258, "start": 1349.78, "end": 1351.1799999999998, "text": " with learned observation operators", "tokens": [50724, 365, 3264, 14816, 19077, 50794], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 413, "seek": 134258, "start": 1351.1799999999998, "end": 1354.22, "text": " to take advantage of parameters that are,", "tokens": [50794, 281, 747, 5002, 295, 9834, 300, 366, 11, 50946], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 414, "seek": 134258, "start": 1356.78, "end": 1358.58, "text": " to learn parameters that are not directly observed,", "tokens": [51074, 281, 1466, 9834, 300, 366, 406, 3838, 13095, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 415, "seek": 134258, "start": 1358.58, "end": 1361.22, "text": " such as complicated satellite measurements", "tokens": [51164, 1270, 382, 6179, 16016, 15383, 51296], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 416, "seek": 134258, "start": 1361.22, "end": 1365.8999999999999, "text": " that are non-linear features of the atmospheric state", "tokens": [51296, 300, 366, 2107, 12, 28263, 4122, 295, 264, 28854, 1785, 51530], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 417, "seek": 134258, "start": 1365.8999999999999, "end": 1367.62, "text": " that are hard to directly measure", "tokens": [51530, 300, 366, 1152, 281, 3838, 3481, 51616], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 418, "seek": 134258, "start": 1367.62, "end": 1369.58, "text": " but might be possible for an AI to learn.", "tokens": [51616, 457, 1062, 312, 1944, 337, 364, 7318, 281, 1466, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12814297676086425, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0002001809043576941}, {"id": 419, "seek": 136958, "start": 1369.58, "end": 1373.5, "text": " And finally, we have some ideas on unconventional data sources.", "tokens": [50364, 400, 2721, 11, 321, 362, 512, 3487, 322, 35847, 46105, 1412, 7139, 13, 50560], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 420, "seek": 136958, "start": 1373.5, "end": 1375.1799999999998, "text": " One idea thrown around in discussions", "tokens": [50560, 1485, 1558, 11732, 926, 294, 11088, 50644], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 421, "seek": 136958, "start": 1375.1799999999998, "end": 1380.1799999999998, "text": " was using ad hoc webcam data like traffic cameras", "tokens": [50644, 390, 1228, 614, 16708, 39490, 1412, 411, 6419, 8622, 50894], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 422, "seek": 136958, "start": 1380.22, "end": 1384.06, "text": " to evaluate real-time precipitation class information.", "tokens": [50896, 281, 13059, 957, 12, 3766, 37662, 1508, 1589, 13, 51088], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 423, "seek": 136958, "start": 1384.06, "end": 1385.78, "text": " In principle, someone can look at the feed", "tokens": [51088, 682, 8665, 11, 1580, 393, 574, 412, 264, 3154, 51174], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 424, "seek": 136958, "start": 1385.78, "end": 1387.34, "text": " and say, yep, it's raining,", "tokens": [51174, 293, 584, 11, 18633, 11, 309, 311, 18441, 11, 51252], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 425, "seek": 136958, "start": 1387.34, "end": 1389.46, "text": " and we can have an AI do the same thing", "tokens": [51252, 293, 321, 393, 362, 364, 7318, 360, 264, 912, 551, 51358], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 426, "seek": 136958, "start": 1389.46, "end": 1391.62, "text": " and potentially have useful data.", "tokens": [51358, 293, 7263, 362, 4420, 1412, 13, 51466], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 427, "seek": 136958, "start": 1391.62, "end": 1393.3, "text": " On the data assimilation side,", "tokens": [51466, 1282, 264, 1412, 8249, 16067, 1252, 11, 51550], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 428, "seek": 136958, "start": 1394.6999999999998, "end": 1396.3799999999999, "text": " in some senses, this area is most advanced", "tokens": [51620, 294, 512, 17057, 11, 341, 1859, 307, 881, 7339, 51704], "temperature": 0.0, "avg_logprob": -0.11180050876162467, "compression_ratio": 1.6472868217054264, "no_speech_prob": 0.00042360491352155805}, {"id": 429, "seek": 139638, "start": 1396.38, "end": 1401.38, "text": " because DA is already using some heavy computation", "tokens": [50364, 570, 9578, 307, 1217, 1228, 512, 4676, 24903, 50614], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 430, "seek": 139638, "start": 1402.46, "end": 1404.74, "text": " for its error matrix operations", "tokens": [50668, 337, 1080, 6713, 8141, 7705, 50782], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 431, "seek": 139638, "start": 1404.74, "end": 1407.0200000000002, "text": " and they're investigating GPU use.", "tokens": [50782, 293, 436, 434, 22858, 18407, 764, 13, 50896], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 432, "seek": 139638, "start": 1407.0200000000002, "end": 1408.0200000000002, "text": " As previously mentioned,", "tokens": [50896, 1018, 8046, 2835, 11, 50946], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 433, "seek": 139638, "start": 1408.0200000000002, "end": 1409.3400000000001, "text": " there's a data of data assimilation", "tokens": [50946, 456, 311, 257, 1412, 295, 1412, 8249, 16067, 51012], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 434, "seek": 139638, "start": 1409.3400000000001, "end": 1410.7800000000002, "text": " in the latent space of a model.", "tokens": [51012, 294, 264, 48994, 1901, 295, 257, 2316, 13, 51084], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 435, "seek": 139638, "start": 1410.7800000000002, "end": 1414.18, "text": " If we can vastly increase ensemble sizes through AI,", "tokens": [51084, 759, 321, 393, 41426, 3488, 19492, 11602, 807, 7318, 11, 51254], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 436, "seek": 139638, "start": 1414.18, "end": 1417.6200000000001, "text": " we can perform better PDF estimation", "tokens": [51254, 321, 393, 2042, 1101, 17752, 35701, 51426], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 437, "seek": 139638, "start": 1417.6200000000001, "end": 1422.6200000000001, "text": " through particle filters or non-Gaussian-based statistics.", "tokens": [51426, 807, 12359, 15995, 420, 2107, 12, 38, 64, 21948, 12, 6032, 12523, 13, 51676], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 438, "seek": 139638, "start": 1423.0200000000002, "end": 1424.8600000000001, "text": " And finally, we can start to estimate,", "tokens": [51696, 400, 2721, 11, 321, 393, 722, 281, 12539, 11, 51788], "temperature": 0.0, "avg_logprob": -0.16397598266601562, "compression_ratio": 1.5607843137254902, "no_speech_prob": 0.0007432871498167515}, {"id": 439, "seek": 142486, "start": 1424.86, "end": 1426.54, "text": " perhaps, the model parameters themselves", "tokens": [50364, 4317, 11, 264, 2316, 9834, 2969, 50448], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 440, "seek": 142486, "start": 1426.54, "end": 1428.34, "text": " rather than just initial conditions.", "tokens": [50448, 2831, 813, 445, 5883, 4487, 13, 50538], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 441, "seek": 142486, "start": 1429.62, "end": 1430.6999999999998, "text": " The numerical prediction side,", "tokens": [50602, 440, 29054, 17630, 1252, 11, 50656], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 442, "seek": 142486, "start": 1430.6999999999998, "end": 1433.1, "text": " this is closest to what I talked about previously", "tokens": [50656, 341, 307, 13699, 281, 437, 286, 2825, 466, 8046, 50776], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 443, "seek": 142486, "start": 1433.1, "end": 1434.8999999999999, "text": " with AI forecast models.", "tokens": [50776, 365, 7318, 14330, 5245, 13, 50866], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 444, "seek": 142486, "start": 1434.8999999999999, "end": 1437.86, "text": " In the near term, we're looking at implementing", "tokens": [50866, 682, 264, 2651, 1433, 11, 321, 434, 1237, 412, 18114, 51014], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 445, "seek": 142486, "start": 1437.86, "end": 1439.4599999999998, "text": " the AI models as they are", "tokens": [51014, 264, 7318, 5245, 382, 436, 366, 51094], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 446, "seek": 142486, "start": 1439.4599999999998, "end": 1442.78, "text": " to provide second opinions about the weather to forecasters.", "tokens": [51094, 281, 2893, 1150, 11819, 466, 264, 5503, 281, 2091, 16369, 1559, 13, 51260], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 447, "seek": 142486, "start": 1444.74, "end": 1447.58, "text": " In the medium term, we're looking at fine-tuning", "tokens": [51358, 682, 264, 6399, 1433, 11, 321, 434, 1237, 412, 2489, 12, 83, 37726, 51500], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 448, "seek": 142486, "start": 1447.58, "end": 1450.02, "text": " large models on our operational data sets", "tokens": [51500, 2416, 5245, 322, 527, 16607, 1412, 6352, 51622], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 449, "seek": 142486, "start": 1450.02, "end": 1453.4199999999998, "text": " to provide better AI forecasts.", "tokens": [51622, 281, 2893, 1101, 7318, 49421, 13, 51792], "temperature": 0.0, "avg_logprob": -0.12132557160263761, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.0007091563893482089}, {"id": 450, "seek": 145342, "start": 1453.42, "end": 1454.26, "text": " And in the longer term,", "tokens": [50364, 400, 294, 264, 2854, 1433, 11, 50406], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 451, "seek": 145342, "start": 1454.26, "end": 1455.74, "text": " we're looking at structural changes", "tokens": [50406, 321, 434, 1237, 412, 15067, 2962, 50480], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 452, "seek": 145342, "start": 1455.74, "end": 1458.3000000000002, "text": " and developing new models in general.", "tokens": [50480, 293, 6416, 777, 5245, 294, 2674, 13, 50608], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 453, "seek": 145342, "start": 1458.3000000000002, "end": 1460.3000000000002, "text": " In addition, we'd like to hybridize classical", "tokens": [50608, 682, 4500, 11, 321, 1116, 411, 281, 13051, 1125, 13735, 50708], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 454, "seek": 145342, "start": 1460.3000000000002, "end": 1464.42, "text": " and WP and AI models to help fix the problem of sparse outputs.", "tokens": [50708, 293, 343, 47, 293, 7318, 5245, 281, 854, 3191, 264, 1154, 295, 637, 11668, 23930, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 455, "seek": 145342, "start": 1465.66, "end": 1467.98, "text": " And along the way,", "tokens": [50976, 400, 2051, 264, 636, 11, 51092], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 456, "seek": 145342, "start": 1467.98, "end": 1469.98, "text": " we'd also like to investigate emulation", "tokens": [51092, 321, 1116, 611, 411, 281, 15013, 846, 2776, 51192], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 457, "seek": 145342, "start": 1469.98, "end": 1471.46, "text": " of the physical parameterizations.", "tokens": [51192, 295, 264, 4001, 13075, 14455, 13, 51266], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 458, "seek": 145342, "start": 1471.46, "end": 1475.5, "text": " For example, radiation is very expensive", "tokens": [51266, 1171, 1365, 11, 12420, 307, 588, 5124, 51468], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 459, "seek": 145342, "start": 1475.5, "end": 1477.1000000000001, "text": " inside the atmospheric model", "tokens": [51468, 1854, 264, 28854, 2316, 51548], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 460, "seek": 145342, "start": 1477.1000000000001, "end": 1480.1000000000001, "text": " and 3D radiation is probably better than 1D radiation", "tokens": [51548, 293, 805, 35, 12420, 307, 1391, 1101, 813, 502, 35, 12420, 51698], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 461, "seek": 145342, "start": 1480.1000000000001, "end": 1481.8200000000002, "text": " but it's too expensive to run operationally.", "tokens": [51698, 457, 309, 311, 886, 5124, 281, 1190, 6916, 379, 13, 51784], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 462, "seek": 145342, "start": 1481.8200000000002, "end": 1483.38, "text": " If we can emulate that,", "tokens": [51784, 759, 321, 393, 45497, 300, 11, 51862], "temperature": 0.0, "avg_logprob": -0.13369348966158354, "compression_ratio": 1.6802721088435375, "no_speech_prob": 0.00018804005230776966}, {"id": 463, "seek": 148338, "start": 1483.38, "end": 1485.94, "text": " we can have a better parameterization", "tokens": [50364, 321, 393, 362, 257, 1101, 13075, 2144, 50492], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 464, "seek": 148338, "start": 1485.94, "end": 1488.7, "text": " that is still within our compute budget.", "tokens": [50492, 300, 307, 920, 1951, 527, 14722, 4706, 13, 50630], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 465, "seek": 148338, "start": 1488.7, "end": 1490.42, "text": " Also, we'd like to extend this", "tokens": [50630, 2743, 11, 321, 1116, 411, 281, 10101, 341, 50716], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 466, "seek": 148338, "start": 1490.42, "end": 1493.0200000000002, "text": " to ocean ice and land surface prediction,", "tokens": [50716, 281, 7810, 4435, 293, 2117, 3753, 17630, 11, 50846], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 467, "seek": 148338, "start": 1493.0200000000002, "end": 1496.66, "text": " but that is still fairly preliminary", "tokens": [50846, 457, 300, 307, 920, 6457, 28817, 51028], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 468, "seek": 148338, "start": 1496.66, "end": 1500.0200000000002, "text": " in part because the data sources aren't quite as complete.", "tokens": [51028, 294, 644, 570, 264, 1412, 7139, 3212, 380, 1596, 382, 3566, 13, 51196], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 469, "seek": 148338, "start": 1500.0200000000002, "end": 1503.98, "text": " Okay, now in terms of the tail end of the forecast", "tokens": [51196, 1033, 11, 586, 294, 2115, 295, 264, 6838, 917, 295, 264, 14330, 51394], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 470, "seek": 148338, "start": 1503.98, "end": 1508.42, "text": " for post-processing and final products,", "tokens": [51394, 337, 2183, 12, 41075, 278, 293, 2572, 3383, 11, 51616], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 471, "seek": 148338, "start": 1508.42, "end": 1512.5800000000002, "text": " this is the realm of downscaling and now casting.", "tokens": [51616, 341, 307, 264, 15355, 295, 760, 4417, 4270, 293, 586, 17301, 13, 51824], "temperature": 0.0, "avg_logprob": -0.12217216680545619, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00016341549053322524}, {"id": 472, "seek": 151338, "start": 1514.38, "end": 1517.8200000000002, "text": " We have a 2.5 kilometer high resolution regional system", "tokens": [50414, 492, 362, 257, 568, 13, 20, 33795, 1090, 8669, 10964, 1185, 50586], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 473, "seek": 151338, "start": 1517.8200000000002, "end": 1520.18, "text": " but it's just too expensive to run too often.", "tokens": [50586, 457, 309, 311, 445, 886, 5124, 281, 1190, 886, 2049, 13, 50704], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 474, "seek": 151338, "start": 1520.18, "end": 1521.2600000000002, "text": " If we can downscale,", "tokens": [50704, 759, 321, 393, 760, 20033, 11, 50758], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 475, "seek": 151338, "start": 1521.2600000000002, "end": 1526.2600000000002, "text": " we can potentially achieve that resolution of output", "tokens": [50758, 321, 393, 7263, 4584, 300, 8669, 295, 5598, 51008], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 476, "seek": 151338, "start": 1526.66, "end": 1531.66, "text": " and evaluate extremes and risks at that scale", "tokens": [51028, 293, 13059, 41119, 293, 10888, 412, 300, 4373, 51278], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 477, "seek": 151338, "start": 1532.5, "end": 1537.0600000000002, "text": " without being limited by melting the supercomputer.", "tokens": [51320, 1553, 885, 5567, 538, 20493, 264, 36708, 13, 51548], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 478, "seek": 151338, "start": 1537.0600000000002, "end": 1538.74, "text": " It would also be really nice if we could have", "tokens": [51548, 467, 576, 611, 312, 534, 1481, 498, 321, 727, 362, 51632], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 479, "seek": 151338, "start": 1538.74, "end": 1541.2600000000002, "text": " near real-time assimilation of weather station radar data", "tokens": [51632, 2651, 957, 12, 3766, 8249, 16067, 295, 5503, 5214, 16544, 1412, 51758], "temperature": 0.0, "avg_logprob": -0.14889637454525456, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.00017668557120487094}, {"id": 480, "seek": 154126, "start": 1541.58, "end": 1544.58, "text": " to improve the half hour, one hour forecast,", "tokens": [50380, 281, 3470, 264, 1922, 1773, 11, 472, 1773, 14330, 11, 50530], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 481, "seek": 154126, "start": 1544.58, "end": 1547.7, "text": " which is not something we can currently do very well.", "tokens": [50530, 597, 307, 406, 746, 321, 393, 4362, 360, 588, 731, 13, 50686], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 482, "seek": 154126, "start": 1547.7, "end": 1549.22, "text": " In terms of post-processing,", "tokens": [50686, 682, 2115, 295, 2183, 12, 41075, 278, 11, 50762], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 483, "seek": 154126, "start": 1550.82, "end": 1552.7, "text": " the statistical post-processing systems", "tokens": [50842, 264, 22820, 2183, 12, 41075, 278, 3652, 50936], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 484, "seek": 154126, "start": 1552.7, "end": 1555.14, "text": " have all long had systematic error adjustments", "tokens": [50936, 362, 439, 938, 632, 27249, 6713, 18624, 51058], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 485, "seek": 154126, "start": 1555.14, "end": 1556.66, "text": " and station specific adjustments", "tokens": [51058, 293, 5214, 2685, 18624, 51134], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 486, "seek": 154126, "start": 1556.66, "end": 1559.58, "text": " for representative this errors and the like", "tokens": [51134, 337, 12424, 341, 13603, 293, 264, 411, 51280], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 487, "seek": 154126, "start": 1559.58, "end": 1564.58, "text": " and AI can help turn that into better nonlinear corrections.", "tokens": [51280, 293, 7318, 393, 854, 1261, 300, 666, 1101, 2107, 28263, 36406, 13, 51530], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 488, "seek": 154126, "start": 1564.86, "end": 1567.26, "text": " And finally, in terms of the expert product sides,", "tokens": [51544, 400, 2721, 11, 294, 2115, 295, 264, 5844, 1674, 4881, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 489, "seek": 154126, "start": 1567.26, "end": 1569.74, "text": " we'd like to have better high impact weather diagnostics", "tokens": [51664, 321, 1116, 411, 281, 362, 1101, 1090, 2712, 5503, 43215, 1167, 51788], "temperature": 0.0, "avg_logprob": -0.12273668809370561, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.0012443114537745714}, {"id": 490, "seek": 156974, "start": 1569.74, "end": 1573.66, "text": " with well-calibrated forecasts for all of the big ones,", "tokens": [50364, 365, 731, 12, 9895, 897, 5468, 49421, 337, 439, 295, 264, 955, 2306, 11, 50560], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 491, "seek": 156974, "start": 1573.66, "end": 1576.94, "text": " tornadoes, hail, blizzards that are just don't show up", "tokens": [50560, 27935, 279, 11, 38157, 11, 888, 8072, 2287, 300, 366, 445, 500, 380, 855, 493, 50724], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 492, "seek": 156974, "start": 1576.94, "end": 1579.38, "text": " in the larger scale forecast", "tokens": [50724, 294, 264, 4833, 4373, 14330, 50846], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 493, "seek": 156974, "start": 1579.38, "end": 1582.02, "text": " but are extremely important for the people stuck in them.", "tokens": [50846, 457, 366, 4664, 1021, 337, 264, 561, 5541, 294, 552, 13, 50978], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 494, "seek": 156974, "start": 1582.02, "end": 1584.86, "text": " Excuse me, Christopher, so only two minutes left before.", "tokens": [50978, 11359, 385, 11, 20649, 11, 370, 787, 732, 2077, 1411, 949, 13, 51120], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 495, "seek": 156974, "start": 1584.86, "end": 1587.74, "text": " Yes, okay, I am going to be very quick.", "tokens": [51120, 1079, 11, 1392, 11, 286, 669, 516, 281, 312, 588, 1702, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 496, "seek": 156974, "start": 1587.74, "end": 1591.0, "text": " Okay, challenges and opportunities.", "tokens": [51264, 1033, 11, 4759, 293, 4786, 13, 51427], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 497, "seek": 156974, "start": 1591.86, "end": 1594.7, "text": " As I've hinted at the entire time, we have challenges", "tokens": [51470, 1018, 286, 600, 12075, 292, 412, 264, 2302, 565, 11, 321, 362, 4759, 51612], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 498, "seek": 156974, "start": 1594.7, "end": 1596.18, "text": " and these are opportunities.", "tokens": [51612, 293, 613, 366, 4786, 13, 51686], "temperature": 0.0, "avg_logprob": -0.19300362916119332, "compression_ratio": 1.6007751937984496, "no_speech_prob": 0.0009693076717667282}, {"id": 499, "seek": 159618, "start": 1597.18, "end": 1600.46, "text": " On the physical side, we have limited compute capacity.", "tokens": [50414, 1282, 264, 4001, 1252, 11, 321, 362, 5567, 14722, 6042, 13, 50578], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 500, "seek": 159618, "start": 1600.46, "end": 1602.7, "text": " GPU compute demands are only going to go up.", "tokens": [50578, 18407, 14722, 15107, 366, 787, 516, 281, 352, 493, 13, 50690], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 501, "seek": 159618, "start": 1602.7, "end": 1604.22, "text": " We don't have very many of them.", "tokens": [50690, 492, 500, 380, 362, 588, 867, 295, 552, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 502, "seek": 159618, "start": 1605.22, "end": 1607.7, "text": " We're going to probably get more in the future", "tokens": [50816, 492, 434, 516, 281, 1391, 483, 544, 294, 264, 2027, 50940], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 503, "seek": 159618, "start": 1607.7, "end": 1609.22, "text": " but we need to manage them.", "tokens": [50940, 457, 321, 643, 281, 3067, 552, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 504, "seek": 159618, "start": 1609.22, "end": 1611.5800000000002, "text": " We also need to care about data management", "tokens": [51016, 492, 611, 643, 281, 1127, 466, 1412, 4592, 51134], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 505, "seek": 159618, "start": 1611.5800000000002, "end": 1614.74, "text": " in terms of not just having an archive that exists on tape", "tokens": [51134, 294, 2115, 295, 406, 445, 1419, 364, 23507, 300, 8198, 322, 7314, 51292], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 506, "seek": 159618, "start": 1614.74, "end": 1617.38, "text": " but one that is living and can answer", "tokens": [51292, 457, 472, 300, 307, 2647, 293, 393, 1867, 51424], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 507, "seek": 159618, "start": 1617.38, "end": 1619.8200000000002, "text": " training-based questions very quickly.", "tokens": [51424, 3097, 12, 6032, 1651, 588, 2661, 13, 51546], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 508, "seek": 159618, "start": 1619.8200000000002, "end": 1622.1000000000001, "text": " In terms of HR, we have similar problems", "tokens": [51546, 682, 2115, 295, 19460, 11, 321, 362, 2531, 2740, 51660], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 509, "seek": 159618, "start": 1622.1000000000001, "end": 1624.6200000000001, "text": " that our researchers are all very good", "tokens": [51660, 300, 527, 10309, 366, 439, 588, 665, 51786], "temperature": 0.0, "avg_logprob": -0.11310570357275791, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.0004726915212813765}, {"id": 510, "seek": 162462, "start": 1624.78, "end": 1627.62, "text": " but they're also not necessarily well-trained on AI.", "tokens": [50372, 457, 436, 434, 611, 406, 4725, 731, 12, 17227, 2001, 322, 7318, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 511, "seek": 162462, "start": 1627.62, "end": 1630.1399999999999, "text": " We need to close that gap.", "tokens": [50514, 492, 643, 281, 1998, 300, 7417, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 512, "seek": 162462, "start": 1630.1399999999999, "end": 1634.3799999999999, "text": " And in particular, we would love to have increased collaboration", "tokens": [50640, 400, 294, 1729, 11, 321, 576, 959, 281, 362, 6505, 9363, 50852], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 513, "seek": 162462, "start": 1634.3799999999999, "end": 1637.02, "text": " with both the ivory tower and private sector.", "tokens": [50852, 365, 1293, 264, 49218, 10567, 293, 4551, 6977, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 514, "seek": 162462, "start": 1637.02, "end": 1640.5, "text": " Okay, the roadmap sets out targets and milestones.", "tokens": [50984, 1033, 11, 264, 35738, 6352, 484, 12911, 293, 42038, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 515, "seek": 162462, "start": 1640.5, "end": 1643.26, "text": " We would like to have our first operational AI systems", "tokens": [51158, 492, 576, 411, 281, 362, 527, 700, 16607, 7318, 3652, 51296], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 516, "seek": 162462, "start": 1643.26, "end": 1645.4199999999998, "text": " for IC innovation cycle five,", "tokens": [51296, 337, 14360, 8504, 6586, 1732, 11, 51404], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 517, "seek": 162462, "start": 1645.4199999999998, "end": 1647.9799999999998, "text": " which is targeted early 2026", "tokens": [51404, 597, 307, 15045, 2440, 945, 10880, 51532], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 518, "seek": 162462, "start": 1647.9799999999998, "end": 1650.7399999999998, "text": " after the supercomputer update.", "tokens": [51532, 934, 264, 36708, 5623, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 519, "seek": 162462, "start": 1651.6999999999998, "end": 1653.54, "text": " The current innovation cycle is just closing", "tokens": [51718, 440, 2190, 8504, 6586, 307, 445, 10377, 51810], "temperature": 0.0, "avg_logprob": -0.1303897353838075, "compression_ratio": 1.5766423357664234, "no_speech_prob": 0.0022502902429550886}, {"id": 520, "seek": 165354, "start": 1653.62, "end": 1655.58, "text": " because it's a bit too early for it.", "tokens": [50368, 570, 309, 311, 257, 857, 886, 2440, 337, 309, 13, 50466], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 521, "seek": 165354, "start": 1655.58, "end": 1657.86, "text": " And ultimately we'd like to have AI", "tokens": [50466, 400, 6284, 321, 1116, 411, 281, 362, 7318, 50580], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 522, "seek": 165354, "start": 1657.86, "end": 1661.3799999999999, "text": " as just another forecast tool by 2030 or so.", "tokens": [50580, 382, 445, 1071, 14330, 2290, 538, 28638, 420, 370, 13, 50756], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 523, "seek": 165354, "start": 1661.3799999999999, "end": 1664.42, "text": " Okay, I would love to talk more about this", "tokens": [50756, 1033, 11, 286, 576, 959, 281, 751, 544, 466, 341, 50908], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 524, "seek": 165354, "start": 1664.42, "end": 1665.74, "text": " but unfortunately there's no time", "tokens": [50908, 457, 7015, 456, 311, 572, 565, 50974], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 525, "seek": 165354, "start": 1665.74, "end": 1669.7, "text": " but in general, our divisions have all been investigating", "tokens": [50974, 457, 294, 2674, 11, 527, 24328, 362, 439, 668, 22858, 51172], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 526, "seek": 165354, "start": 1669.7, "end": 1673.58, "text": " how we can integrate AI into research flows.", "tokens": [51172, 577, 321, 393, 13365, 7318, 666, 2132, 12867, 13, 51366], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 527, "seek": 165354, "start": 1674.5, "end": 1675.6599999999999, "text": " Some projects have started,", "tokens": [51412, 2188, 4455, 362, 1409, 11, 51470], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 528, "seek": 165354, "start": 1675.6599999999999, "end": 1678.1, "text": " some are waiting for people", "tokens": [51470, 512, 366, 3806, 337, 561, 51592], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 529, "seek": 165354, "start": 1678.1, "end": 1681.54, "text": " and some just simply need more resources", "tokens": [51592, 293, 512, 445, 2935, 643, 544, 3593, 51764], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 530, "seek": 165354, "start": 1681.54, "end": 1682.5, "text": " that we don't currently have", "tokens": [51764, 300, 321, 500, 380, 4362, 362, 51812], "temperature": 0.0, "avg_logprob": -0.10404867692427201, "compression_ratio": 1.5608856088560885, "no_speech_prob": 0.0012439771089702845}, {"id": 531, "seek": 168250, "start": 1682.5, "end": 1684.86, "text": " and we would love to collaborate on them.", "tokens": [50364, 293, 321, 576, 959, 281, 18338, 322, 552, 13, 50482], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 532, "seek": 168250, "start": 1684.86, "end": 1686.98, "text": " If you have AI skill", "tokens": [50482, 759, 291, 362, 7318, 5389, 50588], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 533, "seek": 168250, "start": 1686.98, "end": 1688.58, "text": " and you have a weather-related project,", "tokens": [50588, 293, 291, 362, 257, 5503, 12, 12004, 1716, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 534, "seek": 168250, "start": 1688.58, "end": 1692.14, "text": " please email someone at our division.", "tokens": [50668, 1767, 3796, 1580, 412, 527, 10044, 13, 50846], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 535, "seek": 168250, "start": 1692.14, "end": 1693.98, "text": " We would probably love to talk to you.", "tokens": [50846, 492, 576, 1391, 959, 281, 751, 281, 291, 13, 50938], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 536, "seek": 168250, "start": 1695.34, "end": 1696.82, "text": " Unfortunately, I have to cancel this slide", "tokens": [51006, 8590, 11, 286, 362, 281, 10373, 341, 4137, 51080], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 537, "seek": 168250, "start": 1696.82, "end": 1699.06, "text": " where I was going to talk up", "tokens": [51080, 689, 286, 390, 516, 281, 751, 493, 51192], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 538, "seek": 168250, "start": 1699.06, "end": 1700.86, "text": " all of my colleagues' presentations to come.", "tokens": [51192, 439, 295, 452, 7734, 6, 18964, 281, 808, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 539, "seek": 168250, "start": 1700.86, "end": 1703.62, "text": " Please stick around, they are going to be great.", "tokens": [51282, 2555, 2897, 926, 11, 436, 366, 516, 281, 312, 869, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 540, "seek": 168250, "start": 1703.62, "end": 1706.06, "text": " And finally, conclusions-wise,", "tokens": [51420, 400, 2721, 11, 22865, 12, 3711, 11, 51542], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 541, "seek": 168250, "start": 1706.06, "end": 1708.34, "text": " AI is rapidly advancing the state of the art", "tokens": [51542, 7318, 307, 12910, 27267, 264, 1785, 295, 264, 1523, 51656], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 542, "seek": 168250, "start": 1708.34, "end": 1710.74, "text": " in numerical weather computation.", "tokens": [51656, 294, 29054, 5503, 24903, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1184920712936023, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.0007203949498943985}, {"id": 543, "seek": 171074, "start": 1710.74, "end": 1715.54, "text": " This is a phase change of forecasting", "tokens": [50364, 639, 307, 257, 5574, 1319, 295, 44331, 50604], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 544, "seek": 171074, "start": 1715.54, "end": 1717.46, "text": " and I'm excited to see where it goes.", "tokens": [50604, 293, 286, 478, 2919, 281, 536, 689, 309, 1709, 13, 50700], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 545, "seek": 171074, "start": 1717.46, "end": 1719.74, "text": " We will make AI and machine learning technologies", "tokens": [50700, 492, 486, 652, 7318, 293, 3479, 2539, 7943, 50814], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 546, "seek": 171074, "start": 1719.74, "end": 1720.98, "text": " a major part of our systems", "tokens": [50814, 257, 2563, 644, 295, 527, 3652, 50876], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 547, "seek": 171074, "start": 1720.98, "end": 1724.38, "text": " as they prove themselves.", "tokens": [50876, 382, 436, 7081, 2969, 13, 51046], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 548, "seek": 171074, "start": 1725.6200000000001, "end": 1727.46, "text": " But we're a public service organization,", "tokens": [51108, 583, 321, 434, 257, 1908, 2643, 4475, 11, 51200], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 549, "seek": 171074, "start": 1727.46, "end": 1729.9, "text": " we recognize that we have to be very careful", "tokens": [51200, 321, 5521, 300, 321, 362, 281, 312, 588, 5026, 51322], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 550, "seek": 171074, "start": 1729.9, "end": 1732.22, "text": " about what we stand behind operationally.", "tokens": [51322, 466, 437, 321, 1463, 2261, 6916, 379, 13, 51438], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 551, "seek": 171074, "start": 1734.1, "end": 1736.5, "text": " And finally, I hope these slides are available", "tokens": [51532, 400, 2721, 11, 286, 1454, 613, 9788, 366, 2435, 51652], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 552, "seek": 171074, "start": 1736.5, "end": 1737.66, "text": " afterwards for the references.", "tokens": [51652, 10543, 337, 264, 15400, 13, 51710], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 553, "seek": 171074, "start": 1737.66, "end": 1740.66, "text": " There are references to all of the systems I've mentioned.", "tokens": [51710, 821, 366, 15400, 281, 439, 295, 264, 3652, 286, 600, 2835, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12552043983528205, "compression_ratio": 1.6028880866425992, "no_speech_prob": 0.0005028637242503464}, {"id": 554, "seek": 174066, "start": 1740.66, "end": 1742.1000000000001, "text": " And I'd also just like to highlight", "tokens": [50364, 400, 286, 1116, 611, 445, 411, 281, 5078, 50436], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 555, "seek": 174066, "start": 1742.1000000000001, "end": 1743.5400000000002, "text": " that of the 14 references here,", "tokens": [50436, 300, 295, 264, 3499, 15400, 510, 11, 50508], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 556, "seek": 174066, "start": 1743.5400000000002, "end": 1745.02, "text": " about eight are preprints.", "tokens": [50508, 466, 3180, 366, 659, 25788, 13, 50582], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 557, "seek": 174066, "start": 1745.02, "end": 1748.14, "text": " This is a really, really rapidly moving field.", "tokens": [50582, 639, 307, 257, 534, 11, 534, 12910, 2684, 2519, 13, 50738], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 558, "seek": 174066, "start": 1749.78, "end": 1751.38, "text": " Thank you, I am...", "tokens": [50820, 1044, 291, 11, 286, 669, 485, 50900], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 559, "seek": 174066, "start": 1751.38, "end": 1753.3400000000001, "text": " Missy Christopher.", "tokens": [50900, 5275, 88, 20649, 13, 50998], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 560, "seek": 174066, "start": 1753.3400000000001, "end": 1756.78, "text": " We have time maybe for one question.", "tokens": [50998, 492, 362, 565, 1310, 337, 472, 1168, 13, 51170], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 561, "seek": 174066, "start": 1756.78, "end": 1757.78, "text": " I know.", "tokens": [51170, 286, 458, 13, 51220], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 562, "seek": 174066, "start": 1759.3000000000002, "end": 1763.02, "text": " I'm going to take the highest ranked one.", "tokens": [51296, 286, 478, 516, 281, 747, 264, 6343, 20197, 472, 13, 51482], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 563, "seek": 174066, "start": 1763.02, "end": 1764.42, "text": " It's a long one, are you ready?", "tokens": [51482, 467, 311, 257, 938, 472, 11, 366, 291, 1919, 30, 51552], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 564, "seek": 174066, "start": 1764.42, "end": 1766.3000000000002, "text": " Yes, yes.", "tokens": [51552, 1079, 11, 2086, 13, 51646], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 565, "seek": 174066, "start": 1766.3000000000002, "end": 1767.18, "text": " Perfect.", "tokens": [51646, 10246, 13, 51690], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 566, "seek": 174066, "start": 1767.18, "end": 1769.94, "text": " Currently, AI seems to be quite expensive", "tokens": [51690, 19964, 11, 7318, 2544, 281, 312, 1596, 5124, 51828], "temperature": 0.0, "avg_logprob": -0.21943247945685135, "compression_ratio": 1.4475806451612903, "no_speech_prob": 0.0009208741830661893}, {"id": 567, "seek": 176994, "start": 1769.94, "end": 1771.7, "text": " and rapidly developing.", "tokens": [50364, 293, 12910, 6416, 13, 50452], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 568, "seek": 176994, "start": 1771.7, "end": 1774.46, "text": " While we were waiting for AI-based models", "tokens": [50452, 3987, 321, 645, 3806, 337, 7318, 12, 6032, 5245, 50590], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 569, "seek": 176994, "start": 1774.46, "end": 1776.5800000000002, "text": " to build better foundations and training", "tokens": [50590, 281, 1322, 1101, 22467, 293, 3097, 50696], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 570, "seek": 176994, "start": 1776.5800000000002, "end": 1779.06, "text": " to replace numerical weather prediction", "tokens": [50696, 281, 7406, 29054, 5503, 17630, 50820], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 571, "seek": 176994, "start": 1779.06, "end": 1780.74, "text": " or for climate studies,", "tokens": [50820, 420, 337, 5659, 5313, 11, 50904], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 572, "seek": 176994, "start": 1780.74, "end": 1783.78, "text": " can we exploit its computational speed", "tokens": [50904, 393, 321, 25924, 1080, 28270, 3073, 51056], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 573, "seek": 176994, "start": 1783.78, "end": 1788.78, "text": " in the near term for now casting or HRR-like output?", "tokens": [51056, 294, 264, 2651, 1433, 337, 586, 17301, 420, 19460, 49, 12, 4092, 5598, 30, 51306], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 574, "seek": 176994, "start": 1789.46, "end": 1792.9, "text": " For example, ECCC can incorporate into CAM", "tokens": [51340, 1171, 1365, 11, 462, 11717, 34, 393, 16091, 666, 27040, 51512], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 575, "seek": 176994, "start": 1792.9, "end": 1795.22, "text": " for very short thunderstorm prediction", "tokens": [51512, 337, 588, 2099, 39618, 17630, 51628], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 576, "seek": 176994, "start": 1795.22, "end": 1799.54, "text": " or perhaps weather elements on grid now cast.", "tokens": [51628, 420, 4317, 5503, 4959, 322, 10748, 586, 4193, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1170150675672166, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.010592807084321976}, {"id": 577, "seek": 179994, "start": 1800.78, "end": 1804.8200000000002, "text": " Okay, I believe these projects are under investigation.", "tokens": [50406, 1033, 11, 286, 1697, 613, 4455, 366, 833, 9627, 13, 50608], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 578, "seek": 179994, "start": 1804.8200000000002, "end": 1806.9, "text": " I'm on the medium range forecast side,", "tokens": [50608, 286, 478, 322, 264, 6399, 3613, 14330, 1252, 11, 50712], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 579, "seek": 179994, "start": 1806.9, "end": 1811.74, "text": " so it's not my particular side,", "tokens": [50712, 370, 309, 311, 406, 452, 1729, 1252, 11, 50954], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 580, "seek": 179994, "start": 1811.74, "end": 1815.5800000000002, "text": " but there's a presentation in this session,", "tokens": [50954, 457, 456, 311, 257, 5860, 294, 341, 5481, 11, 51146], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 581, "seek": 179994, "start": 1815.5800000000002, "end": 1819.7, "text": " I believe after lunch that investigates now casting", "tokens": [51146, 286, 1697, 934, 6349, 300, 4557, 1024, 586, 17301, 51352], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 582, "seek": 179994, "start": 1819.7, "end": 1821.22, "text": " via an IBM Foundation model.", "tokens": [51352, 5766, 364, 23487, 10335, 2316, 13, 51428], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 583, "seek": 179994, "start": 1821.22, "end": 1824.3400000000001, "text": " And I think that'll begin to answer that kind of question.", "tokens": [51428, 400, 286, 519, 300, 603, 1841, 281, 1867, 300, 733, 295, 1168, 13, 51584], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 584, "seek": 179994, "start": 1824.3400000000001, "end": 1827.14, "text": " In general, yes, this would be very good.", "tokens": [51584, 682, 2674, 11, 2086, 11, 341, 576, 312, 588, 665, 13, 51724], "temperature": 0.0, "avg_logprob": -0.15842222398327244, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.0001739413128234446}, {"id": 585, "seek": 182714, "start": 1827.22, "end": 1829.8600000000001, "text": " In practice, the nearest term limits", "tokens": [50368, 682, 3124, 11, 264, 23831, 1433, 10406, 50500], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 586, "seek": 182714, "start": 1829.8600000000001, "end": 1831.3400000000001, "text": " are probably compute potential", "tokens": [50500, 366, 1391, 14722, 3995, 50574], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 587, "seek": 182714, "start": 1831.3400000000001, "end": 1833.14, "text": " because we have relatively few", "tokens": [50574, 570, 321, 362, 7226, 1326, 50664], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 588, "seek": 182714, "start": 1833.14, "end": 1835.7, "text": " operationally available GPUs,", "tokens": [50664, 6916, 379, 2435, 18407, 82, 11, 50792], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 589, "seek": 182714, "start": 1835.7, "end": 1838.1000000000001, "text": " but hopefully in the next few months to year or so,", "tokens": [50792, 457, 4696, 294, 264, 958, 1326, 2493, 281, 1064, 420, 370, 11, 50912], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 590, "seek": 182714, "start": 1838.1000000000001, "end": 1839.0200000000002, "text": " that will improve.", "tokens": [50912, 300, 486, 3470, 13, 50958], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 591, "seek": 182714, "start": 1840.7800000000002, "end": 1842.8200000000002, "text": " I might sneak in another question then,", "tokens": [51046, 286, 1062, 13164, 294, 1071, 1168, 550, 11, 51148], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 592, "seek": 182714, "start": 1842.8200000000002, "end": 1844.5400000000002, "text": " Christopher, we have a minute.", "tokens": [51148, 20649, 11, 321, 362, 257, 3456, 13, 51234], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 593, "seek": 182714, "start": 1844.5400000000002, "end": 1847.3000000000002, "text": " Have you considered the role that Canadian industry", "tokens": [51234, 3560, 291, 4888, 264, 3090, 300, 12641, 3518, 51372], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 594, "seek": 182714, "start": 1847.3000000000002, "end": 1851.26, "text": " will have in developing AI capacity at ECCC?", "tokens": [51372, 486, 362, 294, 6416, 7318, 6042, 412, 462, 11717, 34, 30, 51570], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 595, "seek": 182714, "start": 1852.0600000000002, "end": 1854.6200000000001, "text": " We would love collaborations from industry.", "tokens": [51610, 492, 576, 959, 36908, 490, 3518, 13, 51738], "temperature": 0.0, "avg_logprob": -0.13149552206391268, "compression_ratio": 1.5509433962264152, "no_speech_prob": 0.00019706494640558958}, {"id": 596, "seek": 185462, "start": 1854.62, "end": 1859.62, "text": " That is my politically correct and also true answer.", "tokens": [50364, 663, 307, 452, 21154, 3006, 293, 611, 2074, 1867, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13406816124916077, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0006767773884348571}, {"id": 597, "seek": 185462, "start": 1861.3799999999999, "end": 1865.86, "text": " We have limits on our resources in part", "tokens": [50702, 492, 362, 10406, 322, 527, 3593, 294, 644, 50926], "temperature": 0.0, "avg_logprob": -0.13406816124916077, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0006767773884348571}, {"id": 598, "seek": 185462, "start": 1865.86, "end": 1869.5, "text": " because until, well, procurement thus far", "tokens": [50926, 570, 1826, 11, 731, 11, 35183, 8807, 1400, 51108], "temperature": 0.0, "avg_logprob": -0.13406816124916077, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0006767773884348571}, {"id": 599, "seek": 185462, "start": 1869.5, "end": 1873.3799999999999, "text": " has been focused on making our operational systems better", "tokens": [51108, 575, 668, 5178, 322, 1455, 527, 16607, 3652, 1101, 51302], "temperature": 0.0, "avg_logprob": -0.13406816124916077, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0006767773884348571}, {"id": 600, "seek": 185462, "start": 1873.3799999999999, "end": 1875.3799999999999, "text": " for obvious reasons.", "tokens": [51302, 337, 6322, 4112, 13, 51402], "temperature": 0.0, "avg_logprob": -0.13406816124916077, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0006767773884348571}, {"id": 601, "seek": 185462, "start": 1875.3799999999999, "end": 1880.3799999999999, "text": " And the cycles of this mean that it is practically difficult", "tokens": [51402, 400, 264, 17796, 295, 341, 914, 300, 309, 307, 15667, 2252, 51652], "temperature": 0.0, "avg_logprob": -0.13406816124916077, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0006767773884348571}, {"id": 602, "seek": 188038, "start": 1881.22, "end": 1885.38, "text": " to, sorry, I'm speaking as a researcher,", "tokens": [50406, 281, 11, 2597, 11, 286, 478, 4124, 382, 257, 21751, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 603, "seek": 188038, "start": 1885.38, "end": 1886.7, "text": " my boss is probably listening,", "tokens": [50614, 452, 5741, 307, 1391, 4764, 11, 50680], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 604, "seek": 188038, "start": 1886.7, "end": 1888.9, "text": " so there's some things I need to be very circumspect", "tokens": [50680, 370, 456, 311, 512, 721, 286, 643, 281, 312, 588, 7125, 82, 1043, 50790], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 605, "seek": 188038, "start": 1888.9, "end": 1889.74, "text": " about saying.", "tokens": [50790, 466, 1566, 13, 50832], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 606, "seek": 188038, "start": 1891.0600000000002, "end": 1893.7, "text": " We need to be very careful about how we commit resources", "tokens": [50898, 492, 643, 281, 312, 588, 5026, 466, 577, 321, 5599, 3593, 51030], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 607, "seek": 188038, "start": 1893.7, "end": 1895.42, "text": " for training large models.", "tokens": [51030, 337, 3097, 2416, 5245, 13, 51116], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 608, "seek": 188038, "start": 1895.42, "end": 1898.46, "text": " Industry is, I think, a fantastic partner,", "tokens": [51116, 38178, 307, 11, 286, 519, 11, 257, 5456, 4975, 11, 51268], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 609, "seek": 188038, "start": 1898.46, "end": 1902.3000000000002, "text": " both for the potential of having compute resources", "tokens": [51268, 1293, 337, 264, 3995, 295, 1419, 14722, 3593, 51460], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 610, "seek": 188038, "start": 1902.3000000000002, "end": 1904.5, "text": " we could borrow and also a better focus", "tokens": [51460, 321, 727, 11172, 293, 611, 257, 1101, 1879, 51570], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 611, "seek": 188038, "start": 1904.5, "end": 1908.0200000000002, "text": " on some of the most downstream applications.", "tokens": [51570, 322, 512, 295, 264, 881, 30621, 5821, 13, 51746], "temperature": 0.0, "avg_logprob": -0.13013356526692707, "compression_ratio": 1.6300813008130082, "no_speech_prob": 0.000882389023900032}, {"id": 612, "seek": 190802, "start": 1908.06, "end": 1912.82, "text": " For example, our leading talk that opened CMOS", "tokens": [50366, 1171, 1365, 11, 527, 5775, 751, 300, 5625, 20424, 4367, 50604], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 613, "seek": 190802, "start": 1912.82, "end": 1916.22, "text": " was on the weather impacts on the insurance industry.", "tokens": [50604, 390, 322, 264, 5503, 11606, 322, 264, 7214, 3518, 13, 50774], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 614, "seek": 190802, "start": 1916.22, "end": 1919.58, "text": " And to the extent we can be of value there,", "tokens": [50774, 400, 281, 264, 8396, 321, 393, 312, 295, 2158, 456, 11, 50942], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 615, "seek": 190802, "start": 1919.58, "end": 1922.34, "text": " I think there's room for joint products.", "tokens": [50942, 286, 519, 456, 311, 1808, 337, 7225, 3383, 13, 51080], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 616, "seek": 190802, "start": 1926.5, "end": 1930.3799999999999, "text": " Okay, so I will try to answer other questions", "tokens": [51288, 1033, 11, 370, 286, 486, 853, 281, 1867, 661, 1651, 51482], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 617, "seek": 190802, "start": 1930.3799999999999, "end": 1932.5, "text": " in the chat as we continue.", "tokens": [51482, 294, 264, 5081, 382, 321, 2354, 13, 51588], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 618, "seek": 190802, "start": 1932.5, "end": 1934.7, "text": " But otherwise, I will stop sharing, meet myself", "tokens": [51588, 583, 5911, 11, 286, 486, 1590, 5414, 11, 1677, 2059, 51698], "temperature": 0.0, "avg_logprob": -0.14596258875835372, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.00037964689545333385}, {"id": 619, "seek": 193470, "start": 1934.7, "end": 1939.18, "text": " and thank our hosts.", "tokens": [50364, 293, 1309, 527, 21573, 13, 50588], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 620, "seek": 193470, "start": 1939.18, "end": 1940.26, "text": " Well, thank you, Christopher.", "tokens": [50588, 1042, 11, 1309, 291, 11, 20649, 13, 50642], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 621, "seek": 193470, "start": 1940.26, "end": 1941.6200000000001, "text": " That was an amazing feat.", "tokens": [50642, 663, 390, 364, 2243, 15425, 13, 50710], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 622, "seek": 193470, "start": 1941.6200000000001, "end": 1944.94, "text": " You put in two very large presentations", "tokens": [50710, 509, 829, 294, 732, 588, 2416, 18964, 50876], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 623, "seek": 193470, "start": 1944.94, "end": 1947.26, "text": " into one 30-minute presentation.", "tokens": [50876, 666, 472, 2217, 12, 18256, 5860, 13, 50992], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 624, "seek": 193470, "start": 1947.26, "end": 1949.8600000000001, "text": " You have all my admiration and thanks for doing that.", "tokens": [50992, 509, 362, 439, 452, 44597, 293, 3231, 337, 884, 300, 13, 51122], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 625, "seek": 193470, "start": 1950.8600000000001, "end": 1952.74, "text": " Congratulations, actually, in 30 minutes.", "tokens": [51172, 9694, 11, 767, 11, 294, 2217, 2077, 13, 51266], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 626, "seek": 193470, "start": 1952.74, "end": 1954.94, "text": " Okay, and now we're on the subject of airvests.", "tokens": [51266, 1033, 11, 293, 586, 321, 434, 322, 264, 3983, 295, 1988, 85, 4409, 13, 51376], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 627, "seek": 193470, "start": 1954.94, "end": 1957.02, "text": " So we're not going to take any longer.", "tokens": [51376, 407, 321, 434, 406, 516, 281, 747, 604, 2854, 13, 51480], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 628, "seek": 193470, "start": 1957.02, "end": 1958.6200000000001, "text": " I'm going to introduce you to Airvests GLaPalm,", "tokens": [51480, 286, 478, 516, 281, 5366, 291, 281, 5774, 85, 4409, 460, 5478, 47, 304, 76, 11, 51560], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 629, "seek": 193470, "start": 1958.6200000000001, "end": 1961.18, "text": " who comes from Canada as well, a researcher.", "tokens": [51560, 567, 1487, 490, 6309, 382, 731, 11, 257, 21751, 13, 51688], "temperature": 0.0, "avg_logprob": -0.46316903686523436, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.0065187131986021996}, {"id": 630, "seek": 196118, "start": 1961.18, "end": 1965.7, "text": " He's going to introduce us to the NWPEI-based model,", "tokens": [50364, 634, 311, 516, 281, 5366, 505, 281, 264, 426, 54, 5208, 40, 12, 6032, 2316, 11, 50590], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 631, "seek": 196118, "start": 1965.7, "end": 1967.94, "text": " the verification against the observations", "tokens": [50590, 264, 30206, 1970, 264, 18163, 50702], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 632, "seek": 196118, "start": 1967.94, "end": 1970.98, "text": " of airvests and airvests.", "tokens": [50702, 295, 1988, 85, 4409, 293, 1988, 85, 4409, 13, 50854], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 633, "seek": 196118, "start": 1970.98, "end": 1972.66, "text": " It's up to you.", "tokens": [50854, 467, 311, 493, 281, 291, 13, 50938], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 634, "seek": 196118, "start": 1972.66, "end": 1974.18, "text": " Hello.", "tokens": [50938, 2425, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 635, "seek": 196118, "start": 1974.18, "end": 1975.14, "text": " I don't know.", "tokens": [51014, 286, 500, 380, 458, 13, 51062], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 636, "seek": 196118, "start": 1975.14, "end": 1977.66, "text": " So you hear me.", "tokens": [51062, 407, 291, 1568, 385, 13, 51188], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 637, "seek": 196118, "start": 1977.66, "end": 1978.5800000000002, "text": " Very well.", "tokens": [51188, 4372, 731, 13, 51234], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 638, "seek": 196118, "start": 1978.5800000000002, "end": 1980.8200000000002, "text": " Do you see my screen?", "tokens": [51234, 1144, 291, 536, 452, 2568, 30, 51346], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 639, "seek": 196118, "start": 1980.8200000000002, "end": 1981.7, "text": " Also.", "tokens": [51346, 2743, 13, 51390], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 640, "seek": 196118, "start": 1981.7, "end": 1982.42, "text": " Wonderful.", "tokens": [51390, 22768, 13, 51426], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 641, "seek": 196118, "start": 1982.42, "end": 1984.9, "text": " So I'm going to do the presentation in French.", "tokens": [51426, 407, 286, 478, 516, 281, 360, 264, 5860, 294, 5522, 13, 51550], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 642, "seek": 196118, "start": 1984.9, "end": 1987.0600000000002, "text": " If you have any questions in English, there's no problem.", "tokens": [51550, 759, 291, 362, 604, 1651, 294, 3669, 11, 456, 311, 572, 1154, 13, 51658], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 643, "seek": 196118, "start": 1987.0600000000002, "end": 1989.9, "text": " If you have any questions in English, there's no problem.", "tokens": [51658, 759, 291, 362, 604, 1651, 294, 3669, 11, 456, 311, 572, 1154, 13, 51800], "temperature": 0.0, "avg_logprob": -0.3913954198360443, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.04033397510647774}, {"id": 644, "seek": 198990, "start": 1990.14, "end": 1993.5800000000002, "text": " It would be easier for me if I do it in French and for you also.", "tokens": [50376, 467, 576, 312, 3571, 337, 385, 498, 286, 360, 309, 294, 5522, 293, 337, 291, 611, 13, 50548], "temperature": 0.0, "avg_logprob": -0.4013068675994873, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.004712145775556564}, {"id": 645, "seek": 198990, "start": 1993.5800000000002, "end": 1999.46, "text": " So I'm here to present the work that I did in collaboration", "tokens": [50548, 407, 286, 478, 510, 281, 1974, 264, 589, 300, 286, 630, 294, 9363, 50842], "temperature": 0.0, "avg_logprob": -0.4013068675994873, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.004712145775556564}, {"id": 646, "seek": 198990, "start": 1999.46, "end": 2005.14, "text": " with my colleagues on the evaluation of the models based", "tokens": [50842, 365, 452, 7734, 322, 264, 13344, 295, 264, 5245, 2361, 51126], "temperature": 0.0, "avg_logprob": -0.4013068675994873, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.004712145775556564}, {"id": 647, "seek": 198990, "start": 2005.14, "end": 2009.38, "text": " on the airCCC in a second.", "tokens": [51126, 322, 264, 1988, 11717, 34, 294, 257, 1150, 13, 51338], "temperature": 0.0, "avg_logprob": -0.4013068675994873, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.004712145775556564}, {"id": 648, "seek": 198990, "start": 2009.38, "end": 2010.3400000000001, "text": " I don't know.", "tokens": [51338, 286, 500, 380, 458, 13, 51386], "temperature": 0.0, "avg_logprob": -0.4013068675994873, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.004712145775556564}, {"id": 649, "seek": 198990, "start": 2010.3400000000001, "end": 2015.5, "text": " So the context is that with the emergence of these models,", "tokens": [51386, 407, 264, 4319, 307, 300, 365, 264, 36211, 295, 613, 5245, 11, 51644], "temperature": 0.0, "avg_logprob": -0.4013068675994873, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.004712145775556564}, {"id": 650, "seek": 201550, "start": 2015.5, "end": 2025.78, "text": " we realized that we had to check these models", "tokens": [50364, 321, 5334, 300, 321, 632, 281, 1520, 613, 5245, 50878], "temperature": 0.0, "avg_logprob": -0.44695377349853516, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.025327634066343307}, {"id": 651, "seek": 201550, "start": 2025.78, "end": 2029.34, "text": " with our traditional verification methods,", "tokens": [50878, 365, 527, 5164, 30206, 7150, 11, 51056], "temperature": 0.0, "avg_logprob": -0.44695377349853516, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.025327634066343307}, {"id": 652, "seek": 201550, "start": 2029.34, "end": 2032.34, "text": " which allow us to evaluate the innovations", "tokens": [51056, 597, 2089, 505, 281, 13059, 264, 24283, 51206], "temperature": 0.0, "avg_logprob": -0.44695377349853516, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.025327634066343307}, {"id": 653, "seek": 201550, "start": 2032.34, "end": 2034.74, "text": " that are made on traditional models.", "tokens": [51206, 300, 366, 1027, 322, 5164, 5245, 13, 51326], "temperature": 0.0, "avg_logprob": -0.44695377349853516, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.025327634066343307}, {"id": 654, "seek": 201550, "start": 2034.74, "end": 2041.78, "text": " So I was asked by my boss to install, turn and check", "tokens": [51326, 407, 286, 390, 2351, 538, 452, 5741, 281, 3625, 11, 1261, 293, 1520, 51678], "temperature": 0.0, "avg_logprob": -0.44695377349853516, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.025327634066343307}, {"id": 655, "seek": 204178, "start": 2041.78, "end": 2047.34, "text": " these models on our HPC installations.", "tokens": [50364, 613, 5245, 322, 527, 12557, 34, 41932, 13, 50642], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 656, "seek": 204178, "start": 2047.34, "end": 2051.66, "text": " And I started this work in October 2023.", "tokens": [50642, 400, 286, 1409, 341, 589, 294, 7617, 44377, 13, 50858], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 657, "seek": 204178, "start": 2051.66, "end": 2055.02, "text": " And I worked on it until April 2024.", "tokens": [50858, 400, 286, 2732, 322, 309, 1826, 6929, 45237, 13, 51026], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 658, "seek": 204178, "start": 2055.02, "end": 2058.02, "text": " So what I want to present to you is just that.", "tokens": [51026, 407, 437, 286, 528, 281, 1974, 281, 291, 307, 445, 300, 13, 51176], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 659, "seek": 204178, "start": 2058.02, "end": 2062.02, "text": " So the activities that were completed during this special project", "tokens": [51176, 407, 264, 5354, 300, 645, 7365, 1830, 341, 2121, 1716, 51376], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 660, "seek": 204178, "start": 2062.02, "end": 2066.66, "text": " there, it's that we turned, we chose two models,", "tokens": [51376, 456, 11, 309, 311, 300, 321, 3574, 11, 321, 5111, 732, 5245, 11, 51608], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 661, "seek": 204178, "start": 2066.66, "end": 2070.1, "text": " ForecastNet and Graphcast, which were available for free.", "tokens": [51608, 9018, 3734, 31890, 293, 21884, 3734, 11, 597, 645, 2435, 337, 1737, 13, 51780], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 662, "seek": 204178, "start": 2070.1, "end": 2071.38, "text": " It's easy.", "tokens": [51780, 467, 311, 1858, 13, 51844], "temperature": 0.0, "avg_logprob": -0.3467487891515096, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.050565410405397415}, {"id": 663, "seek": 207138, "start": 2071.38, "end": 2075.34, "text": " And, well, ForecastNet, Christopher,", "tokens": [50364, 400, 11, 731, 11, 9018, 3734, 31890, 11, 20649, 11, 50562], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 664, "seek": 207138, "start": 2075.34, "end": 2076.94, "text": " we talked about it a little bit earlier.", "tokens": [50562, 321, 2825, 466, 309, 257, 707, 857, 3071, 13, 50642], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 665, "seek": 207138, "start": 2076.94, "end": 2079.2200000000003, "text": " So it's a model that was developed by Renvidia.", "tokens": [50642, 407, 309, 311, 257, 2316, 300, 390, 4743, 538, 12883, 6833, 654, 13, 50756], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 666, "seek": 207138, "start": 2079.2200000000003, "end": 2082.6600000000003, "text": " And then we turned two graphs of Graphcast,", "tokens": [50756, 400, 550, 321, 3574, 732, 24877, 295, 21884, 3734, 11, 50928], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 667, "seek": 207138, "start": 2082.6600000000003, "end": 2087.86, "text": " one with 13 levels of pressure, with a roof of 50 hectopascals", "tokens": [50928, 472, 365, 3705, 4358, 295, 3321, 11, 365, 257, 8418, 295, 2625, 37358, 404, 4806, 1124, 51188], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 668, "seek": 207138, "start": 2087.86, "end": 2092.42, "text": " and a version at 37 levels, a roof of 1 hectopascal.", "tokens": [51188, 293, 257, 3037, 412, 13435, 4358, 11, 257, 8418, 295, 502, 37358, 404, 27303, 13, 51416], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 669, "seek": 207138, "start": 2092.42, "end": 2096.2200000000003, "text": " And we turned each model with three analysis sets.", "tokens": [51416, 400, 321, 3574, 1184, 2316, 365, 1045, 5215, 6352, 13, 51606], "temperature": 0.0, "avg_logprob": -0.359934951319839, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.002498542657122016}, {"id": 670, "seek": 209622, "start": 2096.2599999999998, "end": 2102.62, "text": " One, the first is the operational analysis of the OVF,", "tokens": [50366, 1485, 11, 264, 700, 307, 264, 16607, 5215, 295, 264, 422, 53, 37, 11, 50684], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 671, "seek": 209622, "start": 2102.62, "end": 2106.4199999999996, "text": " called IFS, on three levels only.", "tokens": [50684, 1219, 26080, 50, 11, 322, 1045, 4358, 787, 13, 50874], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 672, "seek": 209622, "start": 2106.4199999999996, "end": 2109.9399999999996, "text": " We also used R5.", "tokens": [50874, 492, 611, 1143, 497, 20, 13, 51050], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 673, "seek": 209622, "start": 2109.9399999999996, "end": 2113.8599999999997, "text": " So the R5 analyses are the ones that were used", "tokens": [51050, 407, 264, 497, 20, 37560, 366, 264, 2306, 300, 645, 1143, 51246], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 674, "seek": 209622, "start": 2113.8599999999997, "end": 2117.4199999999996, "text": " to train these models.", "tokens": [51246, 281, 3847, 613, 5245, 13, 51424], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 675, "seek": 209622, "start": 2117.4199999999996, "end": 2121.1, "text": " We were able to turn the configurations at 13 and 37 levels.", "tokens": [51424, 492, 645, 1075, 281, 1261, 264, 31493, 412, 3705, 293, 13435, 4358, 13, 51608], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 676, "seek": 209622, "start": 2121.1, "end": 2126.14, "text": " And of course, we wanted to compare the operational provisions", "tokens": [51608, 400, 295, 1164, 11, 321, 1415, 281, 6794, 264, 16607, 25034, 51860], "temperature": 0.0, "avg_logprob": -0.45788807588465075, "compression_ratio": 1.549222797927461, "no_speech_prob": 0.016032909974455833}, {"id": 677, "seek": 212614, "start": 2126.18, "end": 2127.2999999999997, "text": " with the same analysis.", "tokens": [50366, 365, 264, 912, 5215, 13, 50422], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 678, "seek": 212614, "start": 2127.2999999999997, "end": 2130.62, "text": " So we started these models, these AI models,", "tokens": [50422, 407, 321, 1409, 613, 5245, 11, 613, 7318, 5245, 11, 50588], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 679, "seek": 212614, "start": 2130.62, "end": 2135.06, "text": " with the operational analysis of CCC.", "tokens": [50588, 365, 264, 16607, 5215, 295, 383, 11717, 13, 50810], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 680, "seek": 212614, "start": 2135.06, "end": 2141.2599999999998, "text": " And we turned in two real-time modes,", "tokens": [50810, 400, 321, 3574, 294, 732, 957, 12, 3766, 14068, 11, 51120], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 681, "seek": 212614, "start": 2141.2599999999998, "end": 2145.3799999999997, "text": " where we turned twice a day,", "tokens": [51120, 689, 321, 3574, 6091, 257, 786, 11, 51326], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 682, "seek": 212614, "start": 2145.3799999999997, "end": 2147.9, "text": " at the same time as the operational model.", "tokens": [51326, 412, 264, 912, 565, 382, 264, 16607, 2316, 13, 51452], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 683, "seek": 212614, "start": 2147.9, "end": 2149.94, "text": " And like that, the operational metrologists", "tokens": [51452, 400, 411, 300, 11, 264, 16607, 1131, 20978, 1751, 51554], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 684, "seek": 212614, "start": 2149.94, "end": 2153.02, "text": " can compare the Forecasts based on AI", "tokens": [51554, 393, 6794, 264, 9018, 3734, 82, 2361, 322, 7318, 51708], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 685, "seek": 212614, "start": 2153.02, "end": 2155.14, "text": " with the operational provisions.", "tokens": [51708, 365, 264, 16607, 25034, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3492088732512101, "compression_ratio": 1.8491620111731844, "no_speech_prob": 0.002435653004795313}, {"id": 686, "seek": 215514, "start": 2155.14, "end": 2159.94, "text": " And also, on my side, I did an evaluation", "tokens": [50364, 400, 611, 11, 322, 452, 1252, 11, 286, 630, 364, 13344, 50604], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 687, "seek": 215514, "start": 2159.94, "end": 2164.5, "text": " on a period of one year, which allows us to see", "tokens": [50604, 322, 257, 2896, 295, 472, 1064, 11, 597, 4045, 505, 281, 536, 50832], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 688, "seek": 215514, "start": 2164.5, "end": 2167.74, "text": " what the models are like.", "tokens": [50832, 437, 264, 5245, 366, 411, 13, 50994], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 689, "seek": 215514, "start": 2167.74, "end": 2171.42, "text": " So here, I put a slide on the description of the models.", "tokens": [50994, 407, 510, 11, 286, 829, 257, 4137, 322, 264, 3855, 295, 264, 5245, 13, 51178], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 690, "seek": 215514, "start": 2171.42, "end": 2172.42, "text": " It's very precise.", "tokens": [51178, 467, 311, 588, 13600, 13, 51228], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 691, "seek": 215514, "start": 2172.42, "end": 2173.7799999999997, "text": " I don't have much time.", "tokens": [51228, 286, 500, 380, 362, 709, 565, 13, 51296], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 692, "seek": 215514, "start": 2173.7799999999997, "end": 2177.62, "text": " Basically, the two Graphcast and Forecast Net models", "tokens": [51296, 8537, 11, 264, 732, 21884, 3734, 293, 9018, 3734, 6188, 5245, 51488], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 693, "seek": 215514, "start": 2177.62, "end": 2181.8599999999997, "text": " use about the same information.", "tokens": [51488, 764, 466, 264, 912, 1589, 13, 51700], "temperature": 0.0, "avg_logprob": -0.3668384552001953, "compression_ratio": 1.477832512315271, "no_speech_prob": 0.00273012719117105}, {"id": 694, "seek": 218186, "start": 2181.86, "end": 2185.1400000000003, "text": " But I put a lot of detail there to be complete.", "tokens": [50364, 583, 286, 829, 257, 688, 295, 2607, 456, 281, 312, 3566, 13, 50528], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 695, "seek": 218186, "start": 2185.1400000000003, "end": 2189.94, "text": " But I don't think I'll be able to save a little time.", "tokens": [50528, 583, 286, 500, 380, 519, 286, 603, 312, 1075, 281, 3155, 257, 707, 565, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 696, "seek": 218186, "start": 2189.94, "end": 2195.94, "text": " So one of the advantages of AI models", "tokens": [50768, 407, 472, 295, 264, 14906, 295, 7318, 5245, 51068], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 697, "seek": 218186, "start": 2195.94, "end": 2199.1800000000003, "text": " is their informatic efficiency.", "tokens": [51068, 307, 641, 1356, 2399, 10493, 13, 51230], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 698, "seek": 218186, "start": 2199.1800000000003, "end": 2201.86, "text": " If we compare the operational model,", "tokens": [51230, 759, 321, 6794, 264, 16607, 2316, 11, 51364], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 699, "seek": 218186, "start": 2201.86, "end": 2204.7000000000003, "text": " presently, it takes a little less than an hour,", "tokens": [51364, 1974, 356, 11, 309, 2516, 257, 707, 1570, 813, 364, 1773, 11, 51506], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 700, "seek": 218186, "start": 2204.7000000000003, "end": 2206.6200000000003, "text": " more than 6,000 CPUs.", "tokens": [51506, 544, 813, 1386, 11, 1360, 13199, 82, 13, 51602], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 701, "seek": 218186, "start": 2206.6200000000003, "end": 2210.46, "text": " So it's a big deal.", "tokens": [51602, 407, 309, 311, 257, 955, 2028, 13, 51794], "temperature": 0.0, "avg_logprob": -0.3309058527792654, "compression_ratio": 1.4607843137254901, "no_speech_prob": 0.0032544590067118406}, {"id": 702, "seek": 221046, "start": 2210.46, "end": 2213.54, "text": " So it generates 500 gigabytes of data", "tokens": [50364, 407, 309, 23815, 5923, 42741, 295, 1412, 50518], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 703, "seek": 221046, "start": 2213.54, "end": 2215.94, "text": " at each provision twice a day.", "tokens": [50518, 412, 1184, 17225, 6091, 257, 786, 13, 50638], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 704, "seek": 221046, "start": 2215.94, "end": 2219.7400000000002, "text": " It makes outings at all ages up to 10 days.", "tokens": [50638, 467, 1669, 484, 1109, 412, 439, 12357, 493, 281, 1266, 1708, 13, 50828], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 705, "seek": 221046, "start": 2219.7400000000002, "end": 2222.46, "text": " And then it's models at 15 kilometers of resolution", "tokens": [50828, 400, 550, 309, 311, 5245, 412, 2119, 13904, 295, 8669, 50964], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 706, "seek": 221046, "start": 2222.46, "end": 2224.62, "text": " with a lot of vertical resolutions.", "tokens": [50964, 365, 257, 688, 295, 9429, 32179, 13, 51072], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 707, "seek": 221046, "start": 2224.62, "end": 2227.54, "text": " AI models have less good resolutions.", "tokens": [51072, 7318, 5245, 362, 1570, 665, 32179, 13, 51218], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 708, "seek": 221046, "start": 2227.54, "end": 2229.82, "text": " They release data at 6 hours.", "tokens": [51218, 814, 4374, 1412, 412, 1386, 2496, 13, 51332], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 709, "seek": 221046, "start": 2229.82, "end": 2232.42, "text": " And a less good vertical resolution too.", "tokens": [51332, 400, 257, 1570, 665, 9429, 8669, 886, 13, 51462], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 710, "seek": 221046, "start": 2232.42, "end": 2235.38, "text": " So, but Forecast Net is very, very light.", "tokens": [51462, 407, 11, 457, 9018, 3734, 6188, 307, 588, 11, 588, 1442, 13, 51610], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 711, "seek": 221046, "start": 2235.38, "end": 2236.7, "text": " It's impressive.", "tokens": [51610, 467, 311, 8992, 13, 51676], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 712, "seek": 221046, "start": 2236.7, "end": 2238.82, "text": " It takes 20 minutes on a CPU.", "tokens": [51676, 467, 2516, 945, 2077, 322, 257, 13199, 13, 51782], "temperature": 0.0, "avg_logprob": -0.3981524104565646, "compression_ratio": 1.6311475409836065, "no_speech_prob": 0.0018765854183584452}, {"id": 713, "seek": 223882, "start": 2238.82, "end": 2240.02, "text": " I don't speak of a GPU here.", "tokens": [50364, 286, 500, 380, 1710, 295, 257, 18407, 510, 13, 50424], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 714, "seek": 223882, "start": 2240.02, "end": 2242.2200000000003, "text": " I have a GPU even faster, of course.", "tokens": [50424, 286, 362, 257, 18407, 754, 4663, 11, 295, 1164, 13, 50534], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 715, "seek": 223882, "start": 2242.2200000000003, "end": 2244.1000000000004, "text": " But on a CPU, you can turn it on.", "tokens": [50534, 583, 322, 257, 13199, 11, 291, 393, 1261, 309, 322, 13, 50628], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 716, "seek": 223882, "start": 2244.1000000000004, "end": 2246.54, "text": " You can turn it on on your laptop and it works.", "tokens": [50628, 509, 393, 1261, 309, 322, 322, 428, 10732, 293, 309, 1985, 13, 50750], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 717, "seek": 223882, "start": 2246.54, "end": 2249.6200000000003, "text": " And it's very fast.", "tokens": [50750, 400, 309, 311, 588, 2370, 13, 50904], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 718, "seek": 223882, "start": 2249.6200000000003, "end": 2251.1400000000003, "text": " It still gives you a good preview.", "tokens": [50904, 467, 920, 2709, 291, 257, 665, 14281, 13, 50980], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 719, "seek": 223882, "start": 2251.1400000000003, "end": 2254.7000000000003, "text": " And Graphcast, it's a little bit more expensive.", "tokens": [50980, 400, 21884, 3734, 11, 309, 311, 257, 707, 857, 544, 5124, 13, 51158], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 720, "seek": 223882, "start": 2254.7000000000003, "end": 2256.6200000000003, "text": " The confidence at 13 levels", "tokens": [51158, 440, 6687, 412, 3705, 4358, 51254], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 721, "seek": 223882, "start": 2256.6200000000003, "end": 2260.54, "text": " requires 100 gigabytes of memory.", "tokens": [51254, 7029, 2319, 42741, 295, 4675, 13, 51450], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 722, "seek": 223882, "start": 2260.54, "end": 2263.06, "text": " So it's a little bit more expensive.", "tokens": [51450, 407, 309, 311, 257, 707, 857, 544, 5124, 13, 51576], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 723, "seek": 223882, "start": 2263.06, "end": 2265.54, "text": " But still, comparing the operational model,", "tokens": [51576, 583, 920, 11, 15763, 264, 16607, 2316, 11, 51700], "temperature": 0.0, "avg_logprob": -0.3954554239908854, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.00555878272280097}, {"id": 724, "seek": 226554, "start": 2265.54, "end": 2269.18, "text": " it's very, very much, much smaller.", "tokens": [50364, 309, 311, 588, 11, 588, 709, 11, 709, 4356, 13, 50546], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 725, "seek": 226554, "start": 2269.18, "end": 2270.66, "text": " Smaller orders.", "tokens": [50546, 15287, 260, 9470, 13, 50620], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 726, "seek": 226554, "start": 2270.66, "end": 2273.34, "text": " And I invite you to the second presentation", "tokens": [50620, 400, 286, 7980, 291, 281, 264, 1150, 5860, 50754], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 727, "seek": 226554, "start": 2273.34, "end": 2276.22, "text": " of Christopher Subick on exactly comparing", "tokens": [50754, 295, 20649, 8511, 618, 322, 2293, 15763, 50898], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 728, "seek": 226554, "start": 2276.22, "end": 2279.1, "text": " the computer performance between AI models,", "tokens": [50898, 264, 3820, 3389, 1296, 7318, 5245, 11, 51042], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 729, "seek": 226554, "start": 2279.1, "end": 2282.42, "text": " Graphcast and GEM, the operational model.", "tokens": [51042, 21884, 3734, 293, 460, 6683, 11, 264, 16607, 2316, 13, 51208], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 730, "seek": 226554, "start": 2282.42, "end": 2284.34, "text": " And it makes a very good comparison.", "tokens": [51208, 400, 309, 1669, 257, 588, 665, 9660, 13, 51304], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 731, "seek": 226554, "start": 2284.34, "end": 2288.22, "text": " If you're interested, I invite you to have", "tokens": [51304, 759, 291, 434, 3102, 11, 286, 7980, 291, 281, 362, 51498], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 732, "seek": 226554, "start": 2288.22, "end": 2291.38, "text": " this presentation on your computer this afternoon.", "tokens": [51498, 341, 5860, 322, 428, 3820, 341, 6499, 13, 51656], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 733, "seek": 226554, "start": 2291.38, "end": 2293.5, "text": " So what does it look like as a verification", "tokens": [51656, 407, 437, 775, 309, 574, 411, 382, 257, 30206, 51762], "temperature": 0.0, "avg_logprob": -0.4269535472092119, "compression_ratio": 1.6556016597510372, "no_speech_prob": 0.0030049094930291176}, {"id": 734, "seek": 229350, "start": 2293.5, "end": 2296.58, "text": " once we've done the average over a year?", "tokens": [50364, 1564, 321, 600, 1096, 264, 4274, 670, 257, 1064, 30, 50518], "temperature": 0.0, "avg_logprob": -0.5447636195591518, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.1302405446767807}, {"id": 735, "seek": 229350, "start": 2296.58, "end": 2301.06, "text": " So here, I have several curves.", "tokens": [50518, 407, 510, 11, 286, 362, 2940, 19490, 13, 50742], "temperature": 0.0, "avg_logprob": -0.5447636195591518, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.1302405446767807}, {"id": 736, "seek": 229350, "start": 2301.06, "end": 2304.66, "text": " Here, I present the errors, the prediction", "tokens": [50742, 1692, 11, 286, 1974, 264, 13603, 11, 264, 17630, 50922], "temperature": 0.0, "avg_logprob": -0.5447636195591518, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.1302405446767807}, {"id": 737, "seek": 229350, "start": 2304.66, "end": 2309.78, "text": " of the geopotential at 55 to Pascal.", "tokens": [50922, 295, 264, 1519, 404, 310, 2549, 412, 12330, 281, 41723, 13, 51178], "temperature": 0.0, "avg_logprob": -0.5447636195591518, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.1302405446767807}, {"id": 738, "seek": 229350, "start": 2309.78, "end": 2314.9, "text": " So these are the errors.", "tokens": [51178, 407, 613, 366, 264, 13603, 13, 51434], "temperature": 0.0, "avg_logprob": -0.5447636195591518, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.1302405446767807}, {"id": 739, "seek": 229350, "start": 2314.9, "end": 2322.1, "text": " The very thick blue gray here, that's the baseline.", "tokens": [51434, 440, 588, 5060, 3344, 10855, 510, 11, 300, 311, 264, 20518, 13, 51794], "temperature": 0.0, "avg_logprob": -0.5447636195591518, "compression_ratio": 1.4493670886075949, "no_speech_prob": 0.1302405446767807}, {"id": 740, "seek": 232210, "start": 2322.1, "end": 2325.5, "text": " So that's the operational preview.", "tokens": [50364, 407, 300, 311, 264, 16607, 14281, 13, 50534], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 741, "seek": 232210, "start": 2325.5, "end": 2331.2599999999998, "text": " So we see the errors that are missing from 0 to 240.", "tokens": [50534, 407, 321, 536, 264, 13603, 300, 366, 5361, 490, 1958, 281, 26837, 13, 50822], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 742, "seek": 232210, "start": 2331.2599999999998, "end": 2334.1, "text": " And the other curves, it's all the previews,", "tokens": [50822, 400, 264, 661, 19490, 11, 309, 311, 439, 264, 14281, 82, 11, 50964], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 743, "seek": 232210, "start": 2334.1, "end": 2336.66, "text": " the verification, the AI models.", "tokens": [50964, 264, 30206, 11, 264, 7318, 5245, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 744, "seek": 232210, "start": 2336.66, "end": 2339.8199999999997, "text": " So we can look at them and then, as these are errors,", "tokens": [51092, 407, 321, 393, 574, 412, 552, 293, 550, 11, 382, 613, 366, 13603, 11, 51250], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 745, "seek": 232210, "start": 2339.8199999999997, "end": 2342.22, "text": " but the closer we are to 0, the better it is.", "tokens": [51250, 457, 264, 4966, 321, 366, 281, 1958, 11, 264, 1101, 309, 307, 13, 51370], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 746, "seek": 232210, "start": 2342.22, "end": 2347.22, "text": " So we see here a group of previews.", "tokens": [51370, 407, 321, 536, 510, 257, 1594, 295, 14281, 82, 13, 51620], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 747, "seek": 232210, "start": 2347.22, "end": 2348.38, "text": " That's the forecast net.", "tokens": [51620, 663, 311, 264, 14330, 2533, 13, 51678], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 748, "seek": 232210, "start": 2348.38, "end": 2350.22, "text": " So we see that for the GZ500,", "tokens": [51678, 407, 321, 536, 300, 337, 264, 460, 57, 7526, 11, 51770], "temperature": 0.0, "avg_logprob": -0.3888348511287144, "compression_ratio": 1.6872037914691944, "no_speech_prob": 0.004727554507553577}, {"id": 749, "seek": 235022, "start": 2350.22, "end": 2355.1, "text": " the forecast net is less good than the operational preview.", "tokens": [50364, 264, 14330, 2533, 307, 1570, 665, 813, 264, 16607, 14281, 13, 50608], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 750, "seek": 235022, "start": 2355.1, "end": 2357.8999999999996, "text": " On the other hand, all the other curves, the five other curves,", "tokens": [50608, 1282, 264, 661, 1011, 11, 439, 264, 661, 19490, 11, 264, 1732, 661, 19490, 11, 50748], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 751, "seek": 235022, "start": 2357.8999999999996, "end": 2359.66, "text": " it's all the previews made by Gravcast", "tokens": [50748, 309, 311, 439, 264, 14281, 82, 1027, 538, 8985, 85, 3734, 50836], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 752, "seek": 235022, "start": 2359.66, "end": 2361.66, "text": " using different analyses.", "tokens": [50836, 1228, 819, 37560, 13, 50936], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 753, "seek": 235022, "start": 2361.66, "end": 2364.7, "text": " So we see that Gravcast, from day five,", "tokens": [50936, 407, 321, 536, 300, 8985, 85, 3734, 11, 490, 786, 1732, 11, 51088], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 754, "seek": 235022, "start": 2364.7, "end": 2366.62, "text": " is better than the operational model,", "tokens": [51088, 307, 1101, 813, 264, 16607, 2316, 11, 51184], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 755, "seek": 235022, "start": 2366.62, "end": 2369.3399999999997, "text": " no matter the analysis we give him.", "tokens": [51184, 572, 1871, 264, 5215, 321, 976, 796, 13, 51320], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 756, "seek": 235022, "start": 2369.3399999999997, "end": 2375.5, "text": " So that's when using the 05 and FS analyses, it's better.", "tokens": [51320, 407, 300, 311, 562, 1228, 264, 1958, 20, 293, 41138, 37560, 11, 309, 311, 1101, 13, 51628], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 757, "seek": 235022, "start": 2375.5, "end": 2377.8999999999996, "text": " I don't know, but what's important here,", "tokens": [51628, 286, 500, 380, 458, 11, 457, 437, 311, 1021, 510, 11, 51748], "temperature": 0.0, "avg_logprob": -0.3622032828952955, "compression_ratio": 1.7665198237885462, "no_speech_prob": 0.0019712781067937613}, {"id": 758, "seek": 237790, "start": 2377.98, "end": 2380.9, "text": " what's interesting here is that the two curves here,", "tokens": [50368, 437, 311, 1880, 510, 307, 300, 264, 732, 19490, 510, 11, 50514], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 759, "seek": 237790, "start": 2380.9, "end": 2383.3, "text": " orange and green,", "tokens": [50514, 7671, 293, 3092, 11, 50634], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 760, "seek": 237790, "start": 2383.3, "end": 2387.42, "text": " the previews are initialized with the same analysis", "tokens": [50634, 264, 14281, 82, 366, 5883, 1602, 365, 264, 912, 5215, 50840], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 761, "seek": 237790, "start": 2387.42, "end": 2389.06, "text": " as the blue curve.", "tokens": [50840, 382, 264, 3344, 7605, 13, 50922], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 762, "seek": 237790, "start": 2389.06, "end": 2392.54, "text": " So we see that with equal information,", "tokens": [50922, 407, 321, 536, 300, 365, 2681, 1589, 11, 51096], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 763, "seek": 237790, "start": 2392.54, "end": 2397.14, "text": " Gravcast is better from day five on the GZ500.", "tokens": [51096, 8985, 85, 3734, 307, 1101, 490, 786, 1732, 322, 264, 460, 57, 7526, 13, 51326], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 764, "seek": 237790, "start": 2397.14, "end": 2398.94, "text": " If we look at another variable,", "tokens": [51326, 759, 321, 574, 412, 1071, 7006, 11, 51416], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 765, "seek": 237790, "start": 2398.94, "end": 2401.82, "text": " which is the temperature at 850 tectopascals,", "tokens": [51416, 597, 307, 264, 4292, 412, 1649, 2803, 256, 557, 404, 4806, 1124, 11, 51560], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 766, "seek": 237790, "start": 2401.82, "end": 2404.1, "text": " so it's the same colors, the same curves,", "tokens": [51560, 370, 309, 311, 264, 912, 4577, 11, 264, 912, 19490, 11, 51674], "temperature": 0.0, "avg_logprob": -0.3861528785483351, "compression_ratio": 1.5422222222222222, "no_speech_prob": 0.002349911257624626}, {"id": 767, "seek": 240410, "start": 2404.18, "end": 2409.18, "text": " in this case, we see that from 72 hours,", "tokens": [50368, 294, 341, 1389, 11, 321, 536, 300, 490, 18731, 2496, 11, 50618], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 768, "seek": 240410, "start": 2409.18, "end": 2412.38, "text": " all the AI models are the operational model,", "tokens": [50618, 439, 264, 7318, 5245, 366, 264, 16607, 2316, 11, 50778], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 769, "seek": 240410, "start": 2412.38, "end": 2413.5, "text": " even for the GZ500,", "tokens": [50778, 754, 337, 264, 460, 57, 7526, 11, 50834], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 770, "seek": 240410, "start": 2413.5, "end": 2419.18, "text": " but we still see that Gravcast is the best model,", "tokens": [50834, 457, 321, 920, 536, 300, 8985, 85, 3734, 307, 264, 1151, 2316, 11, 51118], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 771, "seek": 240410, "start": 2419.18, "end": 2420.2999999999997, "text": " this variable.", "tokens": [51118, 341, 7006, 13, 51174], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 772, "seek": 240410, "start": 2420.2999999999997, "end": 2423.22, "text": " So I showed you two variables.", "tokens": [51174, 407, 286, 4712, 291, 732, 9102, 13, 51320], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 773, "seek": 240410, "start": 2423.22, "end": 2426.06, "text": " So we can conclude that Gravcast is better than for the GZ500,", "tokens": [51320, 407, 321, 393, 16886, 300, 8985, 85, 3734, 307, 1101, 813, 337, 264, 460, 57, 7526, 11, 51462], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 774, "seek": 240410, "start": 2426.06, "end": 2429.54, "text": " we will focus on that from now on.", "tokens": [51462, 321, 486, 1879, 322, 300, 490, 586, 322, 13, 51636], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 775, "seek": 240410, "start": 2429.54, "end": 2431.66, "text": " And then what does it look like for other variables,", "tokens": [51636, 400, 550, 437, 775, 309, 574, 411, 337, 661, 9102, 11, 51742], "temperature": 0.0, "avg_logprob": -0.35163091499114707, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.00035294456756673753}, {"id": 776, "seek": 243166, "start": 2431.74, "end": 2434.18, "text": " like wind, humidity.", "tokens": [50368, 411, 2468, 11, 24751, 13, 50490], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 777, "seek": 243166, "start": 2434.18, "end": 2436.7799999999997, "text": " So here, I present you graphics,", "tokens": [50490, 407, 510, 11, 286, 1974, 291, 11837, 11, 50620], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 778, "seek": 243166, "start": 2436.7799999999997, "end": 2439.22, "text": " it's vertical profiles.", "tokens": [50620, 309, 311, 9429, 23693, 13, 50742], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 779, "seek": 243166, "start": 2441.5, "end": 2446.2599999999998, "text": " Here, in the Y axis of the vertical coordinate curve,", "tokens": [50856, 1692, 11, 294, 264, 398, 10298, 295, 264, 9429, 15670, 7605, 11, 51094], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 780, "seek": 243166, "start": 2446.2599999999998, "end": 2447.74, "text": " we talk about the surface,", "tokens": [51094, 321, 751, 466, 264, 3753, 11, 51168], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 781, "seek": 243166, "start": 2447.74, "end": 2451.14, "text": " from 1,000 tectopascals to 50 tectopascals.", "tokens": [51168, 490, 502, 11, 1360, 256, 557, 404, 4806, 1124, 281, 2625, 256, 557, 404, 4806, 1124, 13, 51338], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 782, "seek": 243166, "start": 2451.14, "end": 2458.5, "text": " And so the clean curve,", "tokens": [51338, 400, 370, 264, 2541, 7605, 11, 51706], "temperature": 0.0, "avg_logprob": -0.5057269287109375, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.004086331930011511}, {"id": 783, "seek": 245850, "start": 2458.5, "end": 2461.9, "text": " it's the error's quarter, according to the variable.", "tokens": [50364, 309, 311, 264, 6713, 311, 6555, 11, 4650, 281, 264, 7006, 13, 50534], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 784, "seek": 245850, "start": 2461.9, "end": 2466.7, "text": " And the tight curve is the bias.", "tokens": [50534, 400, 264, 4524, 7605, 307, 264, 12577, 13, 50774], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 785, "seek": 245850, "start": 2466.7, "end": 2468.86, "text": " And the different variables are the following.", "tokens": [50774, 400, 264, 819, 9102, 366, 264, 3480, 13, 50882], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 786, "seek": 245850, "start": 2468.86, "end": 2472.18, "text": " Here, on the top left, we have the meridian wind,", "tokens": [50882, 1692, 11, 322, 264, 1192, 1411, 11, 321, 362, 264, 3551, 34681, 2468, 11, 51048], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 787, "seek": 245850, "start": 2472.18, "end": 2476.62, "text": " the wind component, the wind module here on the right.", "tokens": [51048, 264, 2468, 6542, 11, 264, 2468, 10088, 510, 322, 264, 558, 13, 51270], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 788, "seek": 245850, "start": 2476.62, "end": 2484.74, "text": " Here, can you do this for us in the next two minutes?", "tokens": [51270, 1692, 11, 393, 291, 360, 341, 337, 505, 294, 264, 958, 732, 2077, 30, 51676], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 789, "seek": 245850, "start": 2484.74, "end": 2486.74, "text": " Yes, that's it.", "tokens": [51676, 1079, 11, 300, 311, 309, 13, 51776], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 790, "seek": 245850, "start": 2486.74, "end": 2488.02, "text": " Yes, I'll go faster.", "tokens": [51776, 1079, 11, 286, 603, 352, 4663, 13, 51840], "temperature": 0.0, "avg_logprob": -0.6440971066253354, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.040925003588199615}, {"id": 791, "seek": 248802, "start": 2488.1, "end": 2490.18, "text": " So what I wanted to show you,", "tokens": [50368, 407, 437, 286, 1415, 281, 855, 291, 11, 50472], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 792, "seek": 248802, "start": 2490.18, "end": 2494.34, "text": " here we have the Gravcast configuration,", "tokens": [50472, 510, 321, 362, 264, 8985, 85, 3734, 11694, 11, 50680], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 793, "seek": 248802, "start": 2494.34, "end": 2497.94, "text": " so at 13 levels, 37 levels.", "tokens": [50680, 370, 412, 3705, 4358, 11, 13435, 4358, 13, 50860], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 794, "seek": 248802, "start": 2497.94, "end": 2503.58, "text": " So what I wanted to show you is that the red curve", "tokens": [50860, 407, 437, 286, 1415, 281, 855, 291, 307, 300, 264, 2182, 7605, 51142], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 795, "seek": 248802, "start": 2503.58, "end": 2506.06, "text": " is always better than the blue curve,", "tokens": [51142, 307, 1009, 1101, 813, 264, 3344, 7605, 11, 51266], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 796, "seek": 248802, "start": 2506.06, "end": 2508.3, "text": " so the model has all the variables,", "tokens": [51266, 370, 264, 2316, 575, 439, 264, 9102, 11, 51378], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 797, "seek": 248802, "start": 2508.3, "end": 2510.94, "text": " all the failures, not all the failures,", "tokens": [51378, 439, 264, 20774, 11, 406, 439, 264, 20774, 11, 51510], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 798, "seek": 248802, "start": 2510.94, "end": 2512.82, "text": " but all the variables, all the levels.", "tokens": [51510, 457, 439, 264, 9102, 11, 439, 264, 4358, 13, 51604], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 799, "seek": 248802, "start": 2512.82, "end": 2514.98, "text": " We see that Gravcast is better.", "tokens": [51604, 492, 536, 300, 8985, 85, 3734, 307, 1101, 13, 51712], "temperature": 0.0, "avg_logprob": -0.3000128936767578, "compression_ratio": 1.8659217877094971, "no_speech_prob": 0.0022176974453032017}, {"id": 800, "seek": 251498, "start": 2514.98, "end": 2521.5, "text": " So are these two models not different resolutions?", "tokens": [50364, 407, 366, 613, 732, 5245, 406, 819, 32179, 30, 50690], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 801, "seek": 251498, "start": 2521.5, "end": 2524.9, "text": " Is the fact that Gravcast is a bigger resolution,", "tokens": [50690, 1119, 264, 1186, 300, 8985, 85, 3734, 307, 257, 3801, 8669, 11, 50860], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 802, "seek": 251498, "start": 2524.9, "end": 2527.66, "text": " is it the advantage?", "tokens": [50860, 307, 309, 264, 5002, 30, 50998], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 803, "seek": 251498, "start": 2527.66, "end": 2530.46, "text": " Is it the advantage compared to the operational model?", "tokens": [50998, 1119, 309, 264, 5002, 5347, 281, 264, 16607, 2316, 30, 51138], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 804, "seek": 251498, "start": 2530.46, "end": 2534.7400000000002, "text": " So what we did is that we did the same verification", "tokens": [51138, 407, 437, 321, 630, 307, 300, 321, 630, 264, 912, 30206, 51352], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 805, "seek": 251498, "start": 2534.7400000000002, "end": 2537.86, "text": " that I presented to you earlier, but we filtered it.", "tokens": [51352, 300, 286, 8212, 281, 291, 3071, 11, 457, 321, 37111, 309, 13, 51508], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 806, "seek": 251498, "start": 2537.86, "end": 2539.86, "text": " We filtered, we removed all the scales", "tokens": [51508, 492, 37111, 11, 321, 7261, 439, 264, 17408, 51608], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 807, "seek": 251498, "start": 2539.86, "end": 2542.06, "text": " that were smaller than 1,000 km,", "tokens": [51608, 300, 645, 4356, 813, 502, 11, 1360, 10698, 11, 51718], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 808, "seek": 251498, "start": 2542.06, "end": 2544.7, "text": " and we kept all those that were 2,000 km.", "tokens": [51718, 293, 321, 4305, 439, 729, 300, 645, 568, 11, 1360, 10698, 13, 51850], "temperature": 0.0, "avg_logprob": -0.29861344231499565, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009176127496175468}, {"id": 809, "seek": 254470, "start": 2544.74, "end": 2547.5, "text": " So I have two graphics to present here.", "tokens": [50366, 407, 286, 362, 732, 11837, 281, 1974, 510, 13, 50504], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 810, "seek": 254470, "start": 2547.5, "end": 2550.22, "text": " So on the left, it's the same graphic", "tokens": [50504, 407, 322, 264, 1411, 11, 309, 311, 264, 912, 14089, 50640], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 811, "seek": 254470, "start": 2550.22, "end": 2552.8599999999997, "text": " that I presented to you earlier, not filtered,", "tokens": [50640, 300, 286, 8212, 281, 291, 3071, 11, 406, 37111, 11, 50772], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 812, "seek": 254470, "start": 2552.8599999999997, "end": 2555.5, "text": " and on the right, it's the filtered previews.", "tokens": [50772, 293, 322, 264, 558, 11, 309, 311, 264, 37111, 14281, 82, 13, 50904], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 813, "seek": 254470, "start": 2555.5, "end": 2557.66, "text": " So that's on the average, on the winter,", "tokens": [50904, 407, 300, 311, 322, 264, 4274, 11, 322, 264, 6355, 11, 51012], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 814, "seek": 254470, "start": 2557.66, "end": 2559.8599999999997, "text": " it's not on the full year.", "tokens": [51012, 309, 311, 406, 322, 264, 1577, 1064, 13, 51122], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 815, "seek": 254470, "start": 2559.8599999999997, "end": 2563.5, "text": " So we see that even if filtered, Gravcast is better.", "tokens": [51122, 407, 321, 536, 300, 754, 498, 37111, 11, 8985, 85, 3734, 307, 1101, 13, 51304], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 816, "seek": 254470, "start": 2563.5, "end": 2565.98, "text": " So that's going to bring to the work of Spectreur Nodging", "tokens": [51304, 407, 300, 311, 516, 281, 1565, 281, 264, 589, 295, 27078, 265, 374, 426, 378, 3249, 51428], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 817, "seek": 254470, "start": 2565.98, "end": 2569.3799999999997, "text": " of Syed, who will present in the next presentation.", "tokens": [51428, 295, 3902, 292, 11, 567, 486, 1974, 294, 264, 958, 5860, 13, 51598], "temperature": 0.0, "avg_logprob": -0.41977716664798925, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0011000067461282015}, {"id": 818, "seek": 256938, "start": 2569.38, "end": 2576.34, "text": " So and for the summer, it's a little less spectacular,", "tokens": [50364, 407, 293, 337, 264, 4266, 11, 309, 311, 257, 707, 1570, 18149, 11, 50712], "temperature": 0.0, "avg_logprob": -0.40720124175583106, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0050163231790065765}, {"id": 819, "seek": 256938, "start": 2576.34, "end": 2585.42, "text": " but we still see that Gravcast has a lot of good information.", "tokens": [50712, 457, 321, 920, 536, 300, 8985, 85, 3734, 575, 257, 688, 295, 665, 1589, 13, 51166], "temperature": 0.0, "avg_logprob": -0.40720124175583106, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0050163231790065765}, {"id": 820, "seek": 256938, "start": 2585.42, "end": 2590.26, "text": " So what are the future activities to do more verification?", "tokens": [51166, 407, 437, 366, 264, 2027, 5354, 281, 360, 544, 30206, 30, 51408], "temperature": 0.0, "avg_logprob": -0.40720124175583106, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0050163231790065765}, {"id": 821, "seek": 256938, "start": 2590.26, "end": 2594.34, "text": " We even have an internal site that allows us to visualize", "tokens": [51408, 492, 754, 362, 364, 6920, 3621, 300, 4045, 505, 281, 23273, 51612], "temperature": 0.0, "avg_logprob": -0.40720124175583106, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0050163231790065765}, {"id": 822, "seek": 256938, "start": 2594.34, "end": 2596.7400000000002, "text": " these previews day by day.", "tokens": [51612, 613, 14281, 82, 786, 538, 786, 13, 51732], "temperature": 0.0, "avg_logprob": -0.40720124175583106, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0050163231790065765}, {"id": 823, "seek": 259674, "start": 2596.8199999999997, "end": 2601.22, "text": " And I invite you to have the next seminar", "tokens": [50368, 400, 286, 7980, 291, 281, 362, 264, 958, 29235, 50588], "temperature": 0.0, "avg_logprob": -0.4819347718182732, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.004356773570179939}, {"id": 824, "seek": 259674, "start": 2601.22, "end": 2603.2599999999998, "text": " of Syed about Spectreur Nodging,", "tokens": [50588, 295, 3902, 292, 466, 27078, 265, 374, 426, 378, 3249, 11, 50690], "temperature": 0.0, "avg_logprob": -0.4819347718182732, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.004356773570179939}, {"id": 825, "seek": 259674, "start": 2603.2599999999998, "end": 2606.58, "text": " which is a very interesting approach to integrate", "tokens": [50690, 597, 307, 257, 588, 1880, 3109, 281, 13365, 50856], "temperature": 0.0, "avg_logprob": -0.4819347718182732, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.004356773570179939}, {"id": 826, "seek": 259674, "start": 2606.58, "end": 2608.8199999999997, "text": " physical models and AI models.", "tokens": [50856, 4001, 5245, 293, 7318, 5245, 13, 50968], "temperature": 0.0, "avg_logprob": -0.4819347718182732, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.004356773570179939}, {"id": 827, "seek": 259674, "start": 2608.8199999999997, "end": 2617.14, "text": " And Christopher Subick's work on entering Gravcast", "tokens": [50968, 400, 20649, 8511, 618, 311, 589, 322, 11104, 8985, 85, 3734, 51384], "temperature": 0.0, "avg_logprob": -0.4819347718182732, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.004356773570179939}, {"id": 828, "seek": 259674, "start": 2617.14, "end": 2622.5, "text": " with the operational analysis that we have done internally", "tokens": [51384, 365, 264, 16607, 5215, 300, 321, 362, 1096, 19501, 51652], "temperature": 0.0, "avg_logprob": -0.4819347718182732, "compression_ratio": 1.4095744680851063, "no_speech_prob": 0.004356773570179939}, {"id": 829, "seek": 262250, "start": 2622.5, "end": 2626.7, "text": " so that it can be better adapted to our model.", "tokens": [50364, 370, 300, 309, 393, 312, 1101, 20871, 281, 527, 2316, 13, 50574], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 830, "seek": 262250, "start": 2627.98, "end": 2629.46, "text": " Thank you.", "tokens": [50638, 1044, 291, 13, 50712], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 831, "seek": 262250, "start": 2629.46, "end": 2632.22, "text": " Hi, thank you, Herv\u00e9.", "tokens": [50712, 2421, 11, 1309, 291, 11, 389, 1978, 526, 13, 50850], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 832, "seek": 262250, "start": 2632.22, "end": 2634.06, "text": " Hi, I have a question for you.", "tokens": [50850, 2421, 11, 286, 362, 257, 1168, 337, 291, 13, 50942], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 833, "seek": 262250, "start": 2634.06, "end": 2635.9, "text": " There, you made the internal models run,", "tokens": [50942, 821, 11, 291, 1027, 264, 6920, 5245, 1190, 11, 51034], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 834, "seek": 262250, "start": 2635.9, "end": 2637.82, "text": " you installed them to do the verification,", "tokens": [51034, 291, 8899, 552, 281, 360, 264, 30206, 11, 51130], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 835, "seek": 262250, "start": 2637.82, "end": 2639.46, "text": " but there are more and more models,", "tokens": [51130, 457, 456, 366, 544, 293, 544, 5245, 11, 51212], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 836, "seek": 262250, "start": 2639.46, "end": 2642.06, "text": " there are almost two days,", "tokens": [51212, 456, 366, 1920, 732, 1708, 11, 51342], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 837, "seek": 262250, "start": 2642.06, "end": 2644.78, "text": " is there perhaps a way to have", "tokens": [51342, 307, 456, 4317, 257, 636, 281, 362, 51478], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 838, "seek": 262250, "start": 2644.78, "end": 2647.06, "text": " verification, counter-observation,", "tokens": [51478, 30206, 11, 5682, 12, 16537, 6864, 11, 51592], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 839, "seek": 262250, "start": 2647.06, "end": 2648.34, "text": " like that, in a standardized way,", "tokens": [51592, 411, 300, 11, 294, 257, 31677, 636, 11, 51656], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 840, "seek": 262250, "start": 2648.34, "end": 2650.74, "text": " or each time, you will have to install the models", "tokens": [51656, 420, 1184, 565, 11, 291, 486, 362, 281, 3625, 264, 5245, 51776], "temperature": 0.0, "avg_logprob": -0.5526852842237129, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.01630750298500061}, {"id": 841, "seek": 265074, "start": 2651.14, "end": 2653.18, "text": " and then the scoring themselves?", "tokens": [50384, 293, 550, 264, 22358, 2969, 30, 50486], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 842, "seek": 265074, "start": 2653.18, "end": 2658.18, "text": " Well, listen, that's what the software that we use", "tokens": [50486, 1042, 11, 2140, 11, 300, 311, 437, 264, 4722, 300, 321, 764, 50736], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 843, "seek": 265074, "start": 2658.18, "end": 2660.62, "text": " to do the verification and counter-observation", "tokens": [50736, 281, 360, 264, 30206, 293, 5682, 12, 16537, 6864, 50858], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 844, "seek": 265074, "start": 2660.62, "end": 2664.06, "text": " works very well only locally.", "tokens": [50858, 1985, 588, 731, 787, 16143, 13, 51030], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 845, "seek": 265074, "start": 2664.06, "end": 2668.2599999999998, "text": " So it's difficult to publish that externally.", "tokens": [51030, 407, 309, 311, 2252, 281, 11374, 300, 40899, 13, 51240], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 846, "seek": 265074, "start": 2668.2599999999998, "end": 2671.3799999999997, "text": " On the other hand, running these models,", "tokens": [51240, 1282, 264, 661, 1011, 11, 2614, 613, 5245, 11, 51396], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 847, "seek": 265074, "start": 2671.3799999999997, "end": 2673.66, "text": " if it's not done very, very easily,", "tokens": [51396, 498, 309, 311, 406, 1096, 588, 11, 588, 3612, 11, 51510], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 848, "seek": 265074, "start": 2673.66, "end": 2675.7799999999997, "text": " I worked for three months in the middle of the day", "tokens": [51510, 286, 2732, 337, 1045, 2493, 294, 264, 2808, 295, 264, 786, 51616], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 849, "seek": 265074, "start": 2675.7799999999997, "end": 2677.8999999999996, "text": " just to start these models.", "tokens": [51616, 445, 281, 722, 613, 5245, 13, 51722], "temperature": 0.0, "avg_logprob": -0.6304379677285954, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.023981202393770218}, {"id": 850, "seek": 267790, "start": 2678.2200000000003, "end": 2682.42, "text": " So I imagine that each model has its own peculiarities.", "tokens": [50380, 407, 286, 3811, 300, 1184, 2316, 575, 1080, 1065, 27149, 1088, 13, 50590], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 851, "seek": 267790, "start": 2682.42, "end": 2685.9, "text": " So it's not obvious, it's not as plug-and-play", "tokens": [50590, 407, 309, 311, 406, 6322, 11, 309, 311, 406, 382, 5452, 12, 474, 12, 2858, 50764], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 852, "seek": 267790, "start": 2685.9, "end": 2687.02, "text": " that we believe in.", "tokens": [50764, 300, 321, 1697, 294, 13, 50820], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 853, "seek": 267790, "start": 2687.02, "end": 2689.7000000000003, "text": " There's still a lot of work to be done.", "tokens": [50820, 821, 311, 920, 257, 688, 295, 589, 281, 312, 1096, 13, 50954], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 854, "seek": 267790, "start": 2689.7000000000003, "end": 2694.46, "text": " So if there are all the two, all the two weeks,", "tokens": [50954, 407, 498, 456, 366, 439, 264, 732, 11, 439, 264, 732, 3259, 11, 51192], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 855, "seek": 267790, "start": 2694.46, "end": 2698.5, "text": " it will be difficult to do this same evaluation", "tokens": [51192, 309, 486, 312, 2252, 281, 360, 341, 912, 13344, 51394], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 856, "seek": 267790, "start": 2698.5, "end": 2700.6600000000003, "text": " to follow the run.", "tokens": [51394, 281, 1524, 264, 1190, 13, 51502], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 857, "seek": 267790, "start": 2702.26, "end": 2704.94, "text": " Thank you, I don't know if there are other questions.", "tokens": [51582, 1044, 291, 11, 286, 500, 380, 458, 498, 456, 366, 661, 1651, 13, 51716], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 858, "seek": 267790, "start": 2704.94, "end": 2707.1800000000003, "text": " Maybe a word for advanced systems.", "tokens": [51716, 2704, 257, 1349, 337, 7339, 3652, 13, 51828], "temperature": 0.0, "avg_logprob": -0.41167449951171875, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0053690895438194275}, {"id": 859, "seek": 270718, "start": 2707.18, "end": 2709.8999999999996, "text": " If you could reset the Q&A on EventMobi", "tokens": [50364, 759, 291, 727, 14322, 264, 1249, 5, 32, 322, 13222, 44, 19293, 50500], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 860, "seek": 270718, "start": 2709.8999999999996, "end": 2713.02, "text": " because I still see the questions", "tokens": [50500, 570, 286, 920, 536, 264, 1651, 50656], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 861, "seek": 270718, "start": 2713.02, "end": 2716.06, "text": " that have been asked to Christopher Subic.", "tokens": [50656, 300, 362, 668, 2351, 281, 20649, 8511, 299, 13, 50808], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 862, "seek": 270718, "start": 2716.06, "end": 2718.94, "text": " So it's hard to see which one.", "tokens": [50808, 407, 309, 311, 1152, 281, 536, 597, 472, 13, 50952], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 863, "seek": 270718, "start": 2718.94, "end": 2720.7, "text": " I have one for you, Herv\u00e9.", "tokens": [50952, 286, 362, 472, 337, 291, 11, 389, 1978, 526, 13, 51040], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 864, "seek": 270718, "start": 2720.7, "end": 2722.06, "text": " How do artificial intelligence models", "tokens": [51040, 1012, 360, 11677, 7599, 5245, 51108], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 865, "seek": 270718, "start": 2722.06, "end": 2724.7799999999997, "text": " behave in complex mountainous terrain?", "tokens": [51108, 15158, 294, 3997, 6937, 563, 17674, 30, 51244], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 866, "seek": 270718, "start": 2726.66, "end": 2729.54, "text": " I didn't evaluate that.", "tokens": [51338, 286, 994, 380, 13059, 300, 13, 51482], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 867, "seek": 270718, "start": 2729.54, "end": 2731.4199999999996, "text": " That's more what my colleague, Marc Verville,", "tokens": [51482, 663, 311, 544, 437, 452, 13532, 11, 18460, 4281, 8386, 11, 51576], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 868, "seek": 270718, "start": 2731.4199999999996, "end": 2733.5, "text": " did for verification on the surface.", "tokens": [51576, 630, 337, 30206, 322, 264, 3753, 13, 51680], "temperature": 0.0, "avg_logprob": -0.4162320357102614, "compression_ratio": 1.4302788844621515, "no_speech_prob": 0.007219749037176371}, {"id": 869, "seek": 273350, "start": 2734.5, "end": 2743.7, "text": " I don't have any memory of the results.", "tokens": [50414, 286, 500, 380, 362, 604, 4675, 295, 264, 3542, 13, 50874], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 870, "seek": 273350, "start": 2743.7, "end": 2748.1, "text": " My focus was really on the verification in addition.", "tokens": [50874, 1222, 1879, 390, 534, 322, 264, 30206, 294, 4500, 13, 51094], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 871, "seek": 273350, "start": 2748.1, "end": 2750.9, "text": " But of course, having a good resolution", "tokens": [51094, 583, 295, 1164, 11, 1419, 257, 665, 8669, 51234], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 872, "seek": 273350, "start": 2750.9, "end": 2754.9, "text": " will certainly not be a problem.", "tokens": [51234, 486, 3297, 406, 312, 257, 1154, 13, 51434], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 873, "seek": 273350, "start": 2754.9, "end": 2756.22, "text": " Perfect.", "tokens": [51434, 10246, 13, 51500], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 874, "seek": 273350, "start": 2756.22, "end": 2758.62, "text": " Thank you very much, Herv\u00e9.", "tokens": [51500, 1044, 291, 588, 709, 11, 389, 1978, 526, 13, 51620], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 875, "seek": 273350, "start": 2758.62, "end": 2760.14, "text": " Thank you.", "tokens": [51620, 1044, 291, 13, 51696], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 876, "seek": 273350, "start": 2760.14, "end": 2761.74, "text": " Thank you, Herv\u00e9.", "tokens": [51696, 1044, 291, 11, 389, 1978, 526, 13, 51776], "temperature": 0.0, "avg_logprob": -0.6863692515605205, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.020569752901792526}, {"id": 877, "seek": 276174, "start": 2762.02, "end": 2765.8999999999996, "text": " We, so we're going to continue.", "tokens": [50378, 492, 11, 370, 321, 434, 516, 281, 2354, 13, 50572], "temperature": 0.0, "avg_logprob": -0.22588483174641927, "compression_ratio": 1.40625, "no_speech_prob": 0.0322159007191658}, {"id": 878, "seek": 276174, "start": 2765.8999999999996, "end": 2769.5, "text": " This time we're inviting Syed Zahid Hussein,", "tokens": [50572, 639, 565, 321, 434, 18202, 3902, 292, 1176, 545, 327, 21282, 33042, 11, 50752], "temperature": 0.0, "avg_logprob": -0.22588483174641927, "compression_ratio": 1.40625, "no_speech_prob": 0.0322159007191658}, {"id": 879, "seek": 276174, "start": 2769.5, "end": 2774.02, "text": " who's a research scientist at ECCC.", "tokens": [50752, 567, 311, 257, 2132, 12662, 412, 462, 11717, 34, 13, 50978], "temperature": 0.0, "avg_logprob": -0.22588483174641927, "compression_ratio": 1.40625, "no_speech_prob": 0.0322159007191658}, {"id": 880, "seek": 276174, "start": 2774.02, "end": 2779.4599999999996, "text": " He will be presenting leveraging data-driven weather", "tokens": [50978, 634, 486, 312, 15578, 32666, 1412, 12, 25456, 5503, 51250], "temperature": 0.0, "avg_logprob": -0.22588483174641927, "compression_ratio": 1.40625, "no_speech_prob": 0.0322159007191658}, {"id": 881, "seek": 276174, "start": 2779.4599999999996, "end": 2782.7, "text": " emulators to guide physics-based numerical weather", "tokens": [51250, 846, 39265, 281, 5934, 10649, 12, 6032, 29054, 5503, 51412], "temperature": 0.0, "avg_logprob": -0.22588483174641927, "compression_ratio": 1.40625, "no_speech_prob": 0.0322159007191658}, {"id": 882, "seek": 276174, "start": 2782.7, "end": 2787.9399999999996, "text": " prediction models, a fusion of forecasting paradigms.", "tokens": [51412, 17630, 5245, 11, 257, 23100, 295, 44331, 13480, 328, 2592, 13, 51674], "temperature": 0.0, "avg_logprob": -0.22588483174641927, "compression_ratio": 1.40625, "no_speech_prob": 0.0322159007191658}, {"id": 883, "seek": 278794, "start": 2787.94, "end": 2794.78, "text": " So without further ado, Syed, this is your turn.", "tokens": [50364, 407, 1553, 3052, 22450, 11, 3902, 292, 11, 341, 307, 428, 1261, 13, 50706], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 884, "seek": 278794, "start": 2794.78, "end": 2796.42, "text": " Thank you, Rand.", "tokens": [50706, 1044, 291, 11, 23614, 13, 50788], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 885, "seek": 278794, "start": 2796.42, "end": 2797.5, "text": " Hello, everyone.", "tokens": [50788, 2425, 11, 1518, 13, 50842], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 886, "seek": 278794, "start": 2797.5, "end": 2801.2200000000003, "text": " In my presentation today, I will be talking", "tokens": [50842, 682, 452, 5860, 965, 11, 286, 486, 312, 1417, 51028], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 887, "seek": 278794, "start": 2801.2200000000003, "end": 2807.14, "text": " about how we can leverage the strengths of data-driven weather", "tokens": [51028, 466, 577, 321, 393, 13982, 264, 16986, 295, 1412, 12, 25456, 5503, 51324], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 888, "seek": 278794, "start": 2807.14, "end": 2810.86, "text": " models to improve predictions from NWP models.", "tokens": [51324, 5245, 281, 3470, 21264, 490, 426, 54, 47, 5245, 13, 51510], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 889, "seek": 278794, "start": 2810.86, "end": 2813.38, "text": " This is a work that we have been doing recently.", "tokens": [51510, 639, 307, 257, 589, 300, 321, 362, 668, 884, 3938, 13, 51636], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 890, "seek": 278794, "start": 2813.38, "end": 2816.9, "text": " And here is a list of my principal collaborators", "tokens": [51636, 400, 510, 307, 257, 1329, 295, 452, 9716, 39789, 51812], "temperature": 0.0, "avg_logprob": -0.13687775161240126, "compression_ratio": 1.4778761061946903, "no_speech_prob": 0.0007643265998922288}, {"id": 891, "seek": 281690, "start": 2816.9, "end": 2819.54, "text": " and the others who have contributed to this research.", "tokens": [50364, 293, 264, 2357, 567, 362, 18434, 281, 341, 2132, 13, 50496], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 892, "seek": 281690, "start": 2823.6600000000003, "end": 2827.62, "text": " As we know, the current state of the earth", "tokens": [50702, 1018, 321, 458, 11, 264, 2190, 1785, 295, 264, 4120, 50900], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 893, "seek": 281690, "start": 2827.62, "end": 2829.54, "text": " for operational weather forecasting", "tokens": [50900, 337, 16607, 5503, 44331, 50996], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 894, "seek": 281690, "start": 2829.54, "end": 2833.6600000000003, "text": " is based on physics-based NWP models.", "tokens": [50996, 307, 2361, 322, 10649, 12, 6032, 426, 54, 47, 5245, 13, 51202], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 895, "seek": 281690, "start": 2833.6600000000003, "end": 2837.54, "text": " However, we have seen these presentations earlier today", "tokens": [51202, 2908, 11, 321, 362, 1612, 613, 18964, 3071, 965, 51396], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 896, "seek": 281690, "start": 2837.54, "end": 2840.38, "text": " from the plenary to the previous two presentations", "tokens": [51396, 490, 264, 499, 42245, 281, 264, 3894, 732, 18964, 51538], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 897, "seek": 281690, "start": 2840.38, "end": 2844.5, "text": " that we have recently seen the emergence of new data-driven", "tokens": [51538, 300, 321, 362, 3938, 1612, 264, 36211, 295, 777, 1412, 12, 25456, 51744], "temperature": 0.0, "avg_logprob": -0.11766626805434992, "compression_ratio": 1.61244019138756, "no_speech_prob": 0.000415193586377427}, {"id": 898, "seek": 284450, "start": 2844.5, "end": 2847.74, "text": " models for predicting weather.", "tokens": [50364, 5245, 337, 32884, 5503, 13, 50526], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 899, "seek": 284450, "start": 2847.74, "end": 2850.58, "text": " And most of these models are using some form of deep neural", "tokens": [50526, 400, 881, 295, 613, 5245, 366, 1228, 512, 1254, 295, 2452, 18161, 50668], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 900, "seek": 284450, "start": 2850.58, "end": 2852.9, "text": " network to emulate the training data.", "tokens": [50668, 3209, 281, 45497, 264, 3097, 1412, 13, 50784], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 901, "seek": 284450, "start": 2852.9, "end": 2857.66, "text": " And the training data generally is RFI re-analysis.", "tokens": [50784, 400, 264, 3097, 1412, 5101, 307, 26204, 40, 319, 12, 29702, 4642, 13, 51022], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 902, "seek": 284450, "start": 2857.66, "end": 2861.9, "text": " So we also can call them artificial intelligence-based", "tokens": [51022, 407, 321, 611, 393, 818, 552, 11677, 7599, 12, 6032, 51234], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 903, "seek": 284450, "start": 2861.9, "end": 2863.3, "text": " weather emulators.", "tokens": [51234, 5503, 846, 39265, 13, 51304], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 904, "seek": 284450, "start": 2863.3, "end": 2866.3, "text": " And recently, they have started to gain prominence", "tokens": [51304, 400, 3938, 11, 436, 362, 1409, 281, 6052, 39225, 655, 51454], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 905, "seek": 284450, "start": 2866.3, "end": 2869.54, "text": " and started to also challenge the existing forecasting", "tokens": [51454, 293, 1409, 281, 611, 3430, 264, 6741, 44331, 51616], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 906, "seek": 284450, "start": 2869.54, "end": 2870.5, "text": " paradigm.", "tokens": [51616, 24709, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 907, "seek": 284450, "start": 2870.5, "end": 2873.18, "text": " Because as you have seen in the previous presentation", "tokens": [51664, 1436, 382, 291, 362, 1612, 294, 264, 3894, 5860, 51798], "temperature": 0.0, "avg_logprob": -0.14007803038054822, "compression_ratio": 1.669291338582677, "no_speech_prob": 0.0009520931635051966}, {"id": 908, "seek": 287318, "start": 2873.18, "end": 2877.5, "text": " from my colleague, Erveg, that these data-driven models", "tokens": [50364, 490, 452, 13532, 11, 3300, 303, 70, 11, 300, 613, 1412, 12, 25456, 5245, 50580], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 909, "seek": 287318, "start": 2877.5, "end": 2880.46, "text": " can produce forecast orders of magnitude", "tokens": [50580, 393, 5258, 14330, 9470, 295, 15668, 50728], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 910, "seek": 287318, "start": 2880.46, "end": 2883.94, "text": " faster with minimal computational resources", "tokens": [50728, 4663, 365, 13206, 28270, 3593, 50902], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 911, "seek": 287318, "start": 2883.94, "end": 2886.8599999999997, "text": " compared to the traditional NWP models.", "tokens": [50902, 5347, 281, 264, 5164, 426, 54, 47, 5245, 13, 51048], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 912, "seek": 287318, "start": 2886.8599999999997, "end": 2890.8599999999997, "text": " And also, they can be highly competitive against state", "tokens": [51048, 400, 611, 11, 436, 393, 312, 5405, 10043, 1970, 1785, 51248], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 913, "seek": 287318, "start": 2890.8599999999997, "end": 2895.1, "text": " of the art NWP models in terms of their accuracy.", "tokens": [51248, 295, 264, 1523, 426, 54, 47, 5245, 294, 2115, 295, 641, 14170, 13, 51460], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 914, "seek": 287318, "start": 2895.1, "end": 2898.54, "text": " However, despite their strengths,", "tokens": [51460, 2908, 11, 7228, 641, 16986, 11, 51632], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 915, "seek": 287318, "start": 2898.54, "end": 2901.22, "text": " strictly when I'm talking in the deterministic sense", "tokens": [51632, 20792, 562, 286, 478, 1417, 294, 264, 15957, 3142, 2020, 51766], "temperature": 0.0, "avg_logprob": -0.14823494376717034, "compression_ratio": 1.55, "no_speech_prob": 0.0003348325553815812}, {"id": 916, "seek": 290122, "start": 2901.22, "end": 2905.2599999999998, "text": " for these AI-based models, they have their limitations also.", "tokens": [50364, 337, 613, 7318, 12, 6032, 5245, 11, 436, 362, 641, 15705, 611, 13, 50566], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 917, "seek": 290122, "start": 2905.2599999999998, "end": 2908.3399999999997, "text": " And one of the most widely known limitation", "tokens": [50566, 400, 472, 295, 264, 881, 13371, 2570, 27432, 50720], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 918, "seek": 290122, "start": 2908.3399999999997, "end": 2911.3399999999997, "text": " is considerable smoothing of fine scales,", "tokens": [50720, 307, 24167, 899, 6259, 571, 295, 2489, 17408, 11, 50870], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 919, "seek": 290122, "start": 2911.3399999999997, "end": 2913.18, "text": " particularly for longer lead times.", "tokens": [50870, 4098, 337, 2854, 1477, 1413, 13, 50962], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 920, "seek": 290122, "start": 2913.18, "end": 2917.22, "text": " Also, they only offer a limited range of forecast fields", "tokens": [50962, 2743, 11, 436, 787, 2626, 257, 5567, 3613, 295, 14330, 7909, 51164], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 921, "seek": 290122, "start": 2917.22, "end": 2920.5, "text": " and improving nominal resolution of these AI models", "tokens": [51164, 293, 11470, 41641, 8669, 295, 613, 7318, 5245, 51328], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 922, "seek": 290122, "start": 2920.5, "end": 2921.3399999999997, "text": " not straightforward.", "tokens": [51328, 406, 15325, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 923, "seek": 290122, "start": 2921.3399999999997, "end": 2923.06, "text": " They can be quite challenging.", "tokens": [51370, 814, 393, 312, 1596, 7595, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 924, "seek": 290122, "start": 2923.06, "end": 2925.02, "text": " So the objective of our research was", "tokens": [51456, 407, 264, 10024, 295, 527, 2132, 390, 51554], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 925, "seek": 290122, "start": 2925.02, "end": 2928.8199999999997, "text": " to see if we can leverage the strengths of these AI-based", "tokens": [51554, 281, 536, 498, 321, 393, 13982, 264, 16986, 295, 613, 7318, 12, 6032, 51744], "temperature": 0.0, "avg_logprob": -0.1355982726474978, "compression_ratio": 1.6653992395437263, "no_speech_prob": 0.0010619978420436382}, {"id": 926, "seek": 292882, "start": 2928.82, "end": 2934.02, "text": " models to improve the predictability of an NWP model.", "tokens": [50364, 5245, 281, 3470, 264, 6069, 2310, 295, 364, 426, 54, 47, 2316, 13, 50624], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 927, "seek": 292882, "start": 2934.02, "end": 2938.78, "text": " And for that, we chose or selected", "tokens": [50624, 400, 337, 300, 11, 321, 5111, 420, 8209, 50862], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 928, "seek": 292882, "start": 2938.78, "end": 2941.6600000000003, "text": " like the GEM model, which is used operationally", "tokens": [50862, 411, 264, 460, 6683, 2316, 11, 597, 307, 1143, 6916, 379, 51006], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 929, "seek": 292882, "start": 2941.6600000000003, "end": 2946.42, "text": " as the NWP model, operationally by Environment Canada,", "tokens": [51006, 382, 264, 426, 54, 47, 2316, 11, 6916, 379, 538, 35354, 6309, 11, 51244], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 930, "seek": 292882, "start": 2946.42, "end": 2950.5800000000004, "text": " and the GraphCast model from Google DeepMind as the AI-based", "tokens": [51244, 293, 264, 21884, 34, 525, 2316, 490, 3329, 14895, 44, 471, 382, 264, 7318, 12, 6032, 51452], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 931, "seek": 292882, "start": 2950.5800000000004, "end": 2951.38, "text": " model.", "tokens": [51452, 2316, 13, 51492], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 932, "seek": 292882, "start": 2951.38, "end": 2954.9, "text": " And the nominal grid resolutions of GraphCast", "tokens": [51492, 400, 264, 41641, 10748, 32179, 295, 21884, 34, 525, 51668], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 933, "seek": 292882, "start": 2954.9, "end": 2958.2200000000003, "text": " and the GEM-based Global Deterministic Prediction System,", "tokens": [51668, 293, 264, 460, 6683, 12, 6032, 14465, 4237, 966, 259, 3142, 32969, 4105, 8910, 11, 51834], "temperature": 0.0, "avg_logprob": -0.14066738855271113, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.0007418568711727858}, {"id": 934, "seek": 295822, "start": 2958.22, "end": 2962.62, "text": " or GDPS, are approximately 25 kilometers and 15 kilometers,", "tokens": [50364, 420, 460, 35, 6273, 11, 366, 10447, 3552, 13904, 293, 2119, 13904, 11, 50584], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 935, "seek": 295822, "start": 2962.62, "end": 2964.02, "text": " respectively.", "tokens": [50584, 25009, 13, 50654], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 936, "seek": 295822, "start": 2964.02, "end": 2966.14, "text": " And in the previous presentation,", "tokens": [50654, 400, 294, 264, 3894, 5860, 11, 50760], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 937, "seek": 295822, "start": 2966.14, "end": 2971.1, "text": " AirVig has shown that the GraphCast actually poses", "tokens": [50760, 5774, 53, 328, 575, 4898, 300, 264, 21884, 34, 525, 767, 26059, 51008], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 938, "seek": 295822, "start": 2971.1, "end": 2975.66, "text": " more skilled large scales compared to our GDPS.", "tokens": [51008, 544, 19690, 2416, 17408, 5347, 281, 527, 460, 35, 6273, 13, 51236], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 939, "seek": 295822, "start": 2975.66, "end": 2978.98, "text": " But if we want to leverage the information from GraphCast,", "tokens": [51236, 583, 498, 321, 528, 281, 13982, 264, 1589, 490, 21884, 34, 525, 11, 51402], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 940, "seek": 295822, "start": 2978.98, "end": 2981.58, "text": " we need to know about the effective resolution of GraphCast", "tokens": [51402, 321, 643, 281, 458, 466, 264, 4942, 8669, 295, 21884, 34, 525, 51532], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 941, "seek": 295822, "start": 2981.58, "end": 2984.8599999999997, "text": " so that we can see what are the scales that we can really", "tokens": [51532, 370, 300, 321, 393, 536, 437, 366, 264, 17408, 300, 321, 393, 534, 51696], "temperature": 0.0, "avg_logprob": -0.18758798599243165, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0009217195911332965}, {"id": 942, "seek": 298486, "start": 2984.86, "end": 2988.7000000000003, "text": " utilize for improving NWP model.", "tokens": [50364, 16117, 337, 11470, 426, 54, 47, 2316, 13, 50556], "temperature": 0.0, "avg_logprob": -0.13221014808205997, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0018319765804335475}, {"id": 943, "seek": 298486, "start": 2988.7000000000003, "end": 2993.7400000000002, "text": " And in order to do that, we look at the variance ratio of GDPS", "tokens": [50556, 400, 294, 1668, 281, 360, 300, 11, 321, 574, 412, 264, 21977, 8509, 295, 460, 35, 6273, 50808], "temperature": 0.0, "avg_logprob": -0.13221014808205997, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0018319765804335475}, {"id": 944, "seek": 298486, "start": 2993.7400000000002, "end": 2998.5, "text": " and GraphCast with respect to our own CMC analysis.", "tokens": [50808, 293, 21884, 34, 525, 365, 3104, 281, 527, 1065, 20424, 34, 5215, 13, 51046], "temperature": 0.0, "avg_logprob": -0.13221014808205997, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0018319765804335475}, {"id": 945, "seek": 298486, "start": 2998.5, "end": 3001.3, "text": " And before I talk about anything else,", "tokens": [51046, 400, 949, 286, 751, 466, 1340, 1646, 11, 51186], "temperature": 0.0, "avg_logprob": -0.13221014808205997, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0018319765804335475}, {"id": 946, "seek": 298486, "start": 3001.3, "end": 3004.9, "text": " I must emphasize on the fact that the version of GraphCast", "tokens": [51186, 286, 1633, 16078, 322, 264, 1186, 300, 264, 3037, 295, 21884, 34, 525, 51366], "temperature": 0.0, "avg_logprob": -0.13221014808205997, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0018319765804335475}, {"id": 947, "seek": 298486, "start": 3004.9, "end": 3010.02, "text": " that we are using has not been through any fine tuning.", "tokens": [51366, 300, 321, 366, 1228, 575, 406, 668, 807, 604, 2489, 15164, 13, 51622], "temperature": 0.0, "avg_logprob": -0.13221014808205997, "compression_ratio": 1.4682926829268292, "no_speech_prob": 0.0018319765804335475}, {"id": 948, "seek": 301002, "start": 3010.02, "end": 3016.1, "text": " So it has been trained by Google on emulating error-5 analysis", "tokens": [50364, 407, 309, 575, 668, 8895, 538, 3329, 322, 846, 12162, 6713, 12, 20, 5215, 50668], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 949, "seek": 301002, "start": 3016.1, "end": 3018.34, "text": " by training with error-5 data.", "tokens": [50668, 538, 3097, 365, 6713, 12, 20, 1412, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 950, "seek": 301002, "start": 3018.34, "end": 3022.14, "text": " And we are using that GraphCast model", "tokens": [50780, 400, 321, 366, 1228, 300, 21884, 34, 525, 2316, 50970], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 951, "seek": 301002, "start": 3022.14, "end": 3026.42, "text": " but initializing it with our own analysis.", "tokens": [50970, 457, 5883, 3319, 309, 365, 527, 1065, 5215, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 952, "seek": 301002, "start": 3026.42, "end": 3029.02, "text": " And in these figures in this slide,", "tokens": [51184, 400, 294, 613, 9624, 294, 341, 4137, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 953, "seek": 301002, "start": 3029.02, "end": 3033.22, "text": " I am showing the transient component of variance ratio", "tokens": [51314, 286, 669, 4099, 264, 41998, 6542, 295, 21977, 8509, 51524], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 954, "seek": 301002, "start": 3033.22, "end": 3035.66, "text": " for 500 hectopascal kinetic energy.", "tokens": [51524, 337, 5923, 37358, 404, 27303, 27135, 2281, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 955, "seek": 301002, "start": 3035.66, "end": 3039.18, "text": " On the left, I have for lead time 24 hours.", "tokens": [51646, 1282, 264, 1411, 11, 286, 362, 337, 1477, 565, 4022, 2496, 13, 51822], "temperature": 0.0, "avg_logprob": -0.1688314538252981, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0009090945241041481}, {"id": 956, "seek": 303918, "start": 3039.18, "end": 3042.22, "text": " And on the right, I have for 120 hours.", "tokens": [50364, 400, 322, 264, 558, 11, 286, 362, 337, 10411, 2496, 13, 50516], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 957, "seek": 303918, "start": 3042.22, "end": 3044.3799999999997, "text": " In blue, I have GraphCast.", "tokens": [50516, 682, 3344, 11, 286, 362, 21884, 34, 525, 13, 50624], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 958, "seek": 303918, "start": 3044.3799999999997, "end": 3048.46, "text": " And in blue, I have GDPS.", "tokens": [50624, 400, 294, 3344, 11, 286, 362, 460, 35, 6273, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 959, "seek": 303918, "start": 3048.46, "end": 3050.46, "text": " And in red, I have GraphCast.", "tokens": [50828, 400, 294, 2182, 11, 286, 362, 21884, 34, 525, 13, 50928], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 960, "seek": 303918, "start": 3050.46, "end": 3054.06, "text": " And we can see by looking at the variance ratio of GDPS,", "tokens": [50928, 400, 321, 393, 536, 538, 1237, 412, 264, 21977, 8509, 295, 460, 35, 6273, 11, 51108], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 961, "seek": 303918, "start": 3054.06, "end": 3059.66, "text": " the blue lines in both 24-hour and 120-hour cases,", "tokens": [51108, 264, 3344, 3876, 294, 1293, 4022, 12, 18048, 293, 10411, 12, 18048, 3331, 11, 51388], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 962, "seek": 303918, "start": 3059.66, "end": 3062.1, "text": " that the variance ratio is close to 1.", "tokens": [51388, 300, 264, 21977, 8509, 307, 1998, 281, 502, 13, 51510], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 963, "seek": 303918, "start": 3062.1, "end": 3065.3799999999997, "text": " So that means its effective resolution", "tokens": [51510, 407, 300, 1355, 1080, 4942, 8669, 51674], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 964, "seek": 303918, "start": 3065.3799999999997, "end": 3068.8599999999997, "text": " is not changing with respect to lead times.", "tokens": [51674, 307, 406, 4473, 365, 3104, 281, 1477, 1413, 13, 51848], "temperature": 0.0, "avg_logprob": -0.10399396440624135, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.0005268183303996921}, {"id": 965, "seek": 306886, "start": 3068.86, "end": 3071.06, "text": " However, what we see with GraphCast,", "tokens": [50364, 2908, 11, 437, 321, 536, 365, 21884, 34, 525, 11, 50474], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 966, "seek": 306886, "start": 3071.06, "end": 3073.9, "text": " first of all, at 24-hour lead time,", "tokens": [50474, 700, 295, 439, 11, 412, 4022, 12, 18048, 1477, 565, 11, 50616], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 967, "seek": 306886, "start": 3073.9, "end": 3076.9, "text": " we see the scales as large as 1,500 kilometers", "tokens": [50616, 321, 536, 264, 17408, 382, 2416, 382, 502, 11, 7526, 13904, 50766], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 968, "seek": 306886, "start": 3076.9, "end": 3080.02, "text": " are smoothed out to some extent.", "tokens": [50766, 366, 5508, 292, 484, 281, 512, 8396, 13, 50922], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 969, "seek": 306886, "start": 3080.02, "end": 3082.6200000000003, "text": " And we see considerable smoothing for scales smaller", "tokens": [50922, 400, 321, 536, 24167, 899, 6259, 571, 337, 17408, 4356, 51052], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 970, "seek": 306886, "start": 3082.6200000000003, "end": 3083.54, "text": " than that.", "tokens": [51052, 813, 300, 13, 51098], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 971, "seek": 306886, "start": 3083.54, "end": 3086.86, "text": " And then we see when we go to 120-hour lead time,", "tokens": [51098, 400, 550, 321, 536, 562, 321, 352, 281, 10411, 12, 18048, 1477, 565, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 972, "seek": 306886, "start": 3086.86, "end": 3091.7000000000003, "text": " the scales that are getting smoothed out actually increases.", "tokens": [51264, 264, 17408, 300, 366, 1242, 5508, 292, 484, 767, 8637, 13, 51506], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 973, "seek": 306886, "start": 3091.7000000000003, "end": 3095.3, "text": " And it affects scales as large as 2,750.", "tokens": [51506, 400, 309, 11807, 17408, 382, 2416, 382, 568, 11, 45396, 13, 51686], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 974, "seek": 306886, "start": 3095.3, "end": 3098.1400000000003, "text": " So we know that large scales in GraphCast are better.", "tokens": [51686, 407, 321, 458, 300, 2416, 17408, 294, 21884, 34, 525, 366, 1101, 13, 51828], "temperature": 0.0, "avg_logprob": -0.13827939187326738, "compression_ratio": 1.7881355932203389, "no_speech_prob": 0.0005773044540546834}, {"id": 975, "seek": 309814, "start": 3098.14, "end": 3101.7799999999997, "text": " But at the same time, we see that scales below 2,750", "tokens": [50364, 583, 412, 264, 912, 565, 11, 321, 536, 300, 17408, 2507, 568, 11, 45396, 50546], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 976, "seek": 309814, "start": 3101.7799999999997, "end": 3105.3799999999997, "text": " for longer lead times are problematic because of the reduced", "tokens": [50546, 337, 2854, 1477, 1413, 366, 19011, 570, 295, 264, 9212, 50726], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 977, "seek": 309814, "start": 3105.3799999999997, "end": 3107.8199999999997, "text": " variance ratio.", "tokens": [50726, 21977, 8509, 13, 50848], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 978, "seek": 309814, "start": 3107.8199999999997, "end": 3112.5, "text": " Now, the question is how we can really use or leverage", "tokens": [50848, 823, 11, 264, 1168, 307, 577, 321, 393, 534, 764, 420, 13982, 51082], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 979, "seek": 309814, "start": 3112.5, "end": 3116.2599999999998, "text": " this good large-scale information from GraphCast.", "tokens": [51082, 341, 665, 2416, 12, 20033, 1589, 490, 21884, 34, 525, 13, 51270], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 980, "seek": 309814, "start": 3116.2599999999998, "end": 3120.5, "text": " And one way to do that would be to use spectral nudging,", "tokens": [51270, 400, 472, 636, 281, 360, 300, 576, 312, 281, 764, 42761, 40045, 3249, 11, 51482], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 981, "seek": 309814, "start": 3120.5, "end": 3122.18, "text": " large-scale spectral nudging.", "tokens": [51482, 2416, 12, 20033, 42761, 40045, 3249, 13, 51566], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 982, "seek": 309814, "start": 3122.18, "end": 3125.74, "text": " And this is a very widely used idea", "tokens": [51566, 400, 341, 307, 257, 588, 13371, 1143, 1558, 51744], "temperature": 0.0, "avg_logprob": -0.12802481897098503, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0001736529084155336}, {"id": 983, "seek": 312574, "start": 3125.74, "end": 3129.2599999999998, "text": " in the field of regional climate modeling and hindcasting.", "tokens": [50364, 294, 264, 2519, 295, 10964, 5659, 15983, 293, 20138, 48860, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 984, "seek": 312574, "start": 3129.2599999999998, "end": 3132.7, "text": " Our expectation is if we nudge our gem predictions", "tokens": [50540, 2621, 14334, 307, 498, 321, 297, 16032, 527, 7173, 21264, 50712], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 985, "seek": 312574, "start": 3132.7, "end": 3134.8999999999996, "text": " towards the large scales of GraphCast,", "tokens": [50712, 3030, 264, 2416, 17408, 295, 21884, 34, 525, 11, 50822], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 986, "seek": 312574, "start": 3134.8999999999996, "end": 3137.74, "text": " we can improve the quality of prediction of gem.", "tokens": [50822, 321, 393, 3470, 264, 3125, 295, 17630, 295, 7173, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 987, "seek": 312574, "start": 3137.74, "end": 3141.22, "text": " At the same time, we should be able to address the fine-scale", "tokens": [50964, 1711, 264, 912, 565, 11, 321, 820, 312, 1075, 281, 2985, 264, 2489, 12, 20033, 51138], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 988, "seek": 312574, "start": 3141.22, "end": 3143.1, "text": " smoothing in GraphCast while we will", "tokens": [51138, 899, 6259, 571, 294, 21884, 34, 525, 1339, 321, 486, 51232], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 989, "seek": 312574, "start": 3143.1, "end": 3146.74, "text": " be able to generate the full set of focus fields", "tokens": [51232, 312, 1075, 281, 8460, 264, 1577, 992, 295, 1879, 7909, 51414], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 990, "seek": 312574, "start": 3146.74, "end": 3152.22, "text": " that we are currently having access to through gem.", "tokens": [51414, 300, 321, 366, 4362, 1419, 2105, 281, 807, 7173, 13, 51688], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 991, "seek": 312574, "start": 3152.22, "end": 3154.9399999999996, "text": " The concept of nudging is quite simple,", "tokens": [51688, 440, 3410, 295, 40045, 3249, 307, 1596, 2199, 11, 51824], "temperature": 0.0, "avg_logprob": -0.13142093859220805, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.00046480094897560775}, {"id": 992, "seek": 315494, "start": 3154.94, "end": 3158.26, "text": " as illustrated by this equation here.", "tokens": [50364, 382, 33875, 538, 341, 5367, 510, 13, 50530], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 993, "seek": 315494, "start": 3158.26, "end": 3164.18, "text": " So here, F is the solution of our gem model", "tokens": [50530, 407, 510, 11, 479, 307, 264, 3827, 295, 527, 7173, 2316, 50826], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 994, "seek": 315494, "start": 3164.18, "end": 3165.78, "text": " after the dynamic substep.", "tokens": [50826, 934, 264, 8546, 1422, 16792, 13, 50906], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 995, "seek": 315494, "start": 3165.78, "end": 3168.82, "text": " And this term highlighted in purple color", "tokens": [50906, 400, 341, 1433, 17173, 294, 9656, 2017, 51058], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 996, "seek": 315494, "start": 3168.82, "end": 3171.34, "text": " actually corresponds to the nudging increments.", "tokens": [51058, 767, 23249, 281, 264, 40045, 3249, 1946, 1117, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 997, "seek": 315494, "start": 3171.34, "end": 3175.06, "text": " So we add the nudging increment to the model solution", "tokens": [51184, 407, 321, 909, 264, 40045, 3249, 26200, 281, 264, 2316, 3827, 51370], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 998, "seek": 315494, "start": 3175.06, "end": 3178.14, "text": " after the dynamic step to get the nudge solution, which", "tokens": [51370, 934, 264, 8546, 1823, 281, 483, 264, 297, 16032, 3827, 11, 597, 51524], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 999, "seek": 315494, "start": 3178.14, "end": 3179.62, "text": " is then fed to the physics.", "tokens": [51524, 307, 550, 4636, 281, 264, 10649, 13, 51598], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 1000, "seek": 315494, "start": 3179.62, "end": 3182.9, "text": " And then it completes the complete model time step.", "tokens": [51598, 400, 550, 309, 36362, 264, 3566, 2316, 565, 1823, 13, 51762], "temperature": 0.0, "avg_logprob": -0.15358879089355468, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.0008010620367713273}, {"id": 1001, "seek": 318290, "start": 3182.9, "end": 3185.2200000000003, "text": " And then it fits back to the next dynamic step.", "tokens": [50364, 400, 550, 309, 9001, 646, 281, 264, 958, 8546, 1823, 13, 50480], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1002, "seek": 318290, "start": 3185.2200000000003, "end": 3186.86, "text": " And this is how it continues.", "tokens": [50480, 400, 341, 307, 577, 309, 6515, 13, 50562], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1003, "seek": 318290, "start": 3186.86, "end": 3192.2200000000003, "text": " And if we look at this nudging increment term,", "tokens": [50562, 400, 498, 321, 574, 412, 341, 40045, 3249, 26200, 1433, 11, 50830], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1004, "seek": 318290, "start": 3192.2200000000003, "end": 3194.7000000000003, "text": " we have this subscript Ls, which actually", "tokens": [50830, 321, 362, 341, 2325, 662, 441, 82, 11, 597, 767, 50954], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1005, "seek": 318290, "start": 3194.7000000000003, "end": 3196.2200000000003, "text": " implies the large scale.", "tokens": [50954, 18779, 264, 2416, 4373, 13, 51030], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1006, "seek": 318290, "start": 3196.2200000000003, "end": 3198.6600000000003, "text": " So we are only considering the large scales", "tokens": [51030, 407, 321, 366, 787, 8079, 264, 2416, 17408, 51152], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1007, "seek": 318290, "start": 3198.6600000000003, "end": 3200.1800000000003, "text": " when we are applying the nudging.", "tokens": [51152, 562, 321, 366, 9275, 264, 40045, 3249, 13, 51228], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1008, "seek": 318290, "start": 3200.1800000000003, "end": 3203.06, "text": " And this omega is a relaxation parameter.", "tokens": [51228, 400, 341, 10498, 307, 257, 30315, 13075, 13, 51372], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1009, "seek": 318290, "start": 3203.06, "end": 3205.26, "text": " And if we break down this equation,", "tokens": [51372, 400, 498, 321, 1821, 760, 341, 5367, 11, 51482], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1010, "seek": 318290, "start": 3205.26, "end": 3207.38, "text": " we can see basically what we have", "tokens": [51482, 321, 393, 536, 1936, 437, 321, 362, 51588], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1011, "seek": 318290, "start": 3207.38, "end": 3211.14, "text": " is a weighted average of the large scales coming from GraphCast", "tokens": [51588, 307, 257, 32807, 4274, 295, 264, 2416, 17408, 1348, 490, 21884, 34, 525, 51776], "temperature": 0.0, "avg_logprob": -0.13829806645711262, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0005964856827631593}, {"id": 1012, "seek": 321114, "start": 3211.14, "end": 3215.98, "text": " and our model, whereas the fine scales are remaining intact", "tokens": [50364, 293, 527, 2316, 11, 9735, 264, 2489, 17408, 366, 8877, 23493, 50606], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1013, "seek": 321114, "start": 3215.98, "end": 3217.8599999999997, "text": " that is being credited by the model.", "tokens": [50606, 300, 307, 885, 41155, 538, 264, 2316, 13, 50700], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1014, "seek": 321114, "start": 3217.8599999999997, "end": 3220.42, "text": " So this is how we can leverage the large scale accuracy", "tokens": [50700, 407, 341, 307, 577, 321, 393, 13982, 264, 2416, 4373, 14170, 50828], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1015, "seek": 321114, "start": 3220.42, "end": 3223.94, "text": " of GraphCast while allowing our model to freely evolve", "tokens": [50828, 295, 21884, 34, 525, 1339, 8293, 527, 2316, 281, 16433, 16693, 51004], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1016, "seek": 321114, "start": 3223.94, "end": 3225.58, "text": " the small scales.", "tokens": [51004, 264, 1359, 17408, 13, 51086], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1017, "seek": 321114, "start": 3225.58, "end": 3228.3399999999997, "text": " And the scale separation between large and small scales,", "tokens": [51086, 400, 264, 4373, 14634, 1296, 2416, 293, 1359, 17408, 11, 51224], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1018, "seek": 321114, "start": 3228.3399999999997, "end": 3230.74, "text": " we are doing that by decomposing the nudging", "tokens": [51224, 321, 366, 884, 300, 538, 22867, 6110, 264, 40045, 3249, 51344], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1019, "seek": 321114, "start": 3230.74, "end": 3232.74, "text": " increments in the spectral space.", "tokens": [51344, 1946, 1117, 294, 264, 42761, 1901, 13, 51444], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1020, "seek": 321114, "start": 3232.74, "end": 3235.8599999999997, "text": " Although we are applying the nudging increment", "tokens": [51444, 5780, 321, 366, 9275, 264, 40045, 3249, 26200, 51600], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1021, "seek": 321114, "start": 3235.8599999999997, "end": 3238.7799999999997, "text": " in the grid point space, but we are decomposing it", "tokens": [51600, 294, 264, 10748, 935, 1901, 11, 457, 321, 366, 22867, 6110, 309, 51746], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1022, "seek": 321114, "start": 3238.7799999999997, "end": 3239.66, "text": " in the spectral space.", "tokens": [51746, 294, 264, 42761, 1901, 13, 51790], "temperature": 0.0, "avg_logprob": -0.165322169784672, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.0002373701863689348}, {"id": 1023, "seek": 323966, "start": 3239.66, "end": 3243.14, "text": " And hence, we call it spectral nudging.", "tokens": [50364, 400, 16678, 11, 321, 818, 309, 42761, 40045, 3249, 13, 50538], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1024, "seek": 323966, "start": 3243.14, "end": 3245.8999999999996, "text": " How we optimize the spectral nudging configuration", "tokens": [50538, 1012, 321, 19719, 264, 42761, 40045, 3249, 11694, 50676], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1025, "seek": 323966, "start": 3245.8999999999996, "end": 3249.8999999999996, "text": " to support our objectives for this study", "tokens": [50676, 281, 1406, 527, 15961, 337, 341, 2979, 50876], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1026, "seek": 323966, "start": 3249.8999999999996, "end": 3251.46, "text": " is a computationally demanding task.", "tokens": [50876, 307, 257, 24903, 379, 19960, 5633, 13, 50954], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1027, "seek": 323966, "start": 3251.46, "end": 3253.3399999999997, "text": " And in a sense, it is still ongoing.", "tokens": [50954, 400, 294, 257, 2020, 11, 309, 307, 920, 10452, 13, 51048], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1028, "seek": 323966, "start": 3253.3399999999997, "end": 3256.7, "text": " I mean, we are still sort of fine tuning the configuration.", "tokens": [51048, 286, 914, 11, 321, 366, 920, 1333, 295, 2489, 15164, 264, 11694, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1029, "seek": 323966, "start": 3256.7, "end": 3259.46, "text": " But at present, the most optimal configuration", "tokens": [51216, 583, 412, 1974, 11, 264, 881, 16252, 11694, 51354], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1030, "seek": 323966, "start": 3259.46, "end": 3262.8199999999997, "text": " that we have has these following features.", "tokens": [51354, 300, 321, 362, 575, 613, 3480, 4122, 13, 51522], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1031, "seek": 323966, "start": 3262.8199999999997, "end": 3267.58, "text": " We are only applying nudging to horizontal wind and temperature.", "tokens": [51522, 492, 366, 787, 9275, 40045, 3249, 281, 12750, 2468, 293, 4292, 13, 51760], "temperature": 0.0, "avg_logprob": -0.13617428529609754, "compression_ratio": 1.7004048582995952, "no_speech_prob": 0.0002195308916270733}, {"id": 1032, "seek": 326758, "start": 3267.62, "end": 3269.98, "text": " And we are not nudging the stratosphere and the boundary", "tokens": [50366, 400, 321, 366, 406, 40045, 3249, 264, 23674, 44877, 293, 264, 12866, 50484], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1033, "seek": 326758, "start": 3269.98, "end": 3271.38, "text": " layer for different reasons.", "tokens": [50484, 4583, 337, 819, 4112, 13, 50554], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1034, "seek": 326758, "start": 3271.38, "end": 3274.7799999999997, "text": " And we are nudging scales that are larger than 2750", "tokens": [50554, 400, 321, 366, 40045, 3249, 17408, 300, 366, 4833, 813, 7634, 2803, 50724], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1035, "seek": 326758, "start": 3274.7799999999997, "end": 3275.9, "text": " for obvious reasons.", "tokens": [50724, 337, 6322, 4112, 13, 50780], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1036, "seek": 326758, "start": 3275.9, "end": 3278.7799999999997, "text": " That should be obvious from the variance ratio comparison.", "tokens": [50780, 663, 820, 312, 6322, 490, 264, 21977, 8509, 9660, 13, 50924], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1037, "seek": 326758, "start": 3278.7799999999997, "end": 3282.7, "text": " And we have 12 hours as the nudging realization scale.", "tokens": [50924, 400, 321, 362, 2272, 2496, 382, 264, 40045, 3249, 25138, 4373, 13, 51120], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1038, "seek": 326758, "start": 3282.7, "end": 3286.14, "text": " And we apply nudging at every time step.", "tokens": [51120, 400, 321, 3079, 40045, 3249, 412, 633, 565, 1823, 13, 51292], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1039, "seek": 326758, "start": 3286.14, "end": 3288.9, "text": " With that, I'll be going to some of the results", "tokens": [51292, 2022, 300, 11, 286, 603, 312, 516, 281, 512, 295, 264, 3542, 51430], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1040, "seek": 326758, "start": 3288.9, "end": 3293.46, "text": " of verification that we will try to prove", "tokens": [51430, 295, 30206, 300, 321, 486, 853, 281, 7081, 51658], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1041, "seek": 326758, "start": 3293.46, "end": 3295.2599999999998, "text": " that this approach actually helps", "tokens": [51658, 300, 341, 3109, 767, 3665, 51748], "temperature": 0.0, "avg_logprob": -0.142329284123012, "compression_ratio": 1.7137254901960783, "no_speech_prob": 0.00016314239474013448}, {"id": 1042, "seek": 329526, "start": 3295.26, "end": 3298.5, "text": " to improve the predictability of gem.", "tokens": [50364, 281, 3470, 264, 6069, 2310, 295, 7173, 13, 50526], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1043, "seek": 329526, "start": 3298.5, "end": 3303.7000000000003, "text": " We ran a series of experiments for winter and summer of 2022.", "tokens": [50526, 492, 5872, 257, 2638, 295, 12050, 337, 6355, 293, 4266, 295, 20229, 13, 50786], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1044, "seek": 329526, "start": 3303.7000000000003, "end": 3305.78, "text": " So we have the control experiments", "tokens": [50786, 407, 321, 362, 264, 1969, 12050, 50890], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1045, "seek": 329526, "start": 3305.78, "end": 3309.42, "text": " where there is no nudging, the control GDPS.", "tokens": [50890, 689, 456, 307, 572, 40045, 3249, 11, 264, 1969, 460, 35, 6273, 13, 51072], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1046, "seek": 329526, "start": 3309.42, "end": 3312.26, "text": " And then we have the GDPS with spectral nudging.", "tokens": [51072, 400, 550, 321, 362, 264, 460, 35, 6273, 365, 42761, 40045, 3249, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1047, "seek": 329526, "start": 3312.26, "end": 3315.5400000000004, "text": " So control would be, in the next few slides, all the results.", "tokens": [51214, 407, 1969, 576, 312, 11, 294, 264, 958, 1326, 9788, 11, 439, 264, 3542, 13, 51378], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1048, "seek": 329526, "start": 3315.5400000000004, "end": 3317.7000000000003, "text": " Control would be shown in blue color.", "tokens": [51378, 12912, 576, 312, 4898, 294, 3344, 2017, 13, 51486], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1049, "seek": 329526, "start": 3317.7000000000003, "end": 3321.0600000000004, "text": " And the results from spectral nudging with GDPS", "tokens": [51486, 400, 264, 3542, 490, 42761, 40045, 3249, 365, 460, 35, 6273, 51654], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1050, "seek": 329526, "start": 3321.0600000000004, "end": 3323.6200000000003, "text": " would be shown with red.", "tokens": [51654, 576, 312, 4898, 365, 2182, 13, 51782], "temperature": 0.0, "avg_logprob": -0.12531801240634075, "compression_ratio": 1.8227272727272728, "no_speech_prob": 0.00029931674362160265}, {"id": 1051, "seek": 332362, "start": 3323.62, "end": 3326.54, "text": " So the first scores that I want to show", "tokens": [50364, 407, 264, 700, 13444, 300, 286, 528, 281, 855, 50510], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1052, "seek": 332362, "start": 3326.54, "end": 3328.9, "text": " are verification against radios and observations.", "tokens": [50510, 366, 30206, 1970, 2843, 2717, 293, 18163, 13, 50628], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1053, "seek": 332362, "start": 3328.9, "end": 3331.9, "text": " It's similar to what Elvig has shown", "tokens": [50628, 467, 311, 2531, 281, 437, 2699, 85, 328, 575, 4898, 50778], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1054, "seek": 332362, "start": 3331.9, "end": 3334.2599999999998, "text": " in the previous presentation.", "tokens": [50778, 294, 264, 3894, 5860, 13, 50896], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1055, "seek": 332362, "start": 3334.2599999999998, "end": 3338.38, "text": " Just to repeat, we have in these figures,", "tokens": [50896, 1449, 281, 7149, 11, 321, 362, 294, 613, 9624, 11, 51102], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1056, "seek": 332362, "start": 3338.38, "end": 3343.06, "text": " we have different variables, zonal wind, wind modulus,", "tokens": [51102, 321, 362, 819, 9102, 11, 710, 21523, 2468, 11, 2468, 42287, 11, 51336], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1057, "seek": 332362, "start": 3343.06, "end": 3346.22, "text": " geopotential high temperature, and dew point depletion.", "tokens": [51336, 1519, 45225, 2549, 1090, 4292, 11, 293, 48745, 935, 368, 14657, 313, 13, 51494], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1058, "seek": 332362, "start": 3346.22, "end": 3349.14, "text": " In all these figures, we have the dashed lines representing", "tokens": [51494, 682, 439, 613, 9624, 11, 321, 362, 264, 8240, 292, 3876, 13460, 51640], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1059, "seek": 332362, "start": 3349.14, "end": 3351.7, "text": " the bias and solid lines representing standard deviation", "tokens": [51640, 264, 12577, 293, 5100, 3876, 13460, 3832, 25163, 51768], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1060, "seek": 332362, "start": 3351.7, "end": 3352.38, "text": " of error.", "tokens": [51768, 295, 6713, 13, 51802], "temperature": 0.0, "avg_logprob": -0.19132295676640101, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0005512629868462682}, {"id": 1061, "seek": 335238, "start": 3352.38, "end": 3355.26, "text": " And we have the shades of red and blue", "tokens": [50364, 400, 321, 362, 264, 20639, 295, 2182, 293, 3344, 50508], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1062, "seek": 335238, "start": 3355.26, "end": 3359.78, "text": " that represent the statistically significant improvements", "tokens": [50508, 300, 2906, 264, 36478, 4776, 13797, 50734], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1063, "seek": 335238, "start": 3359.78, "end": 3362.7000000000003, "text": " corresponding to the color of the experiment.", "tokens": [50734, 11760, 281, 264, 2017, 295, 264, 5120, 13, 50880], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1064, "seek": 335238, "start": 3362.7000000000003, "end": 3364.9, "text": " So if we see red color, then it implies", "tokens": [50880, 407, 498, 321, 536, 2182, 2017, 11, 550, 309, 18779, 50990], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1065, "seek": 335238, "start": 3364.9, "end": 3367.2200000000003, "text": " that we have statistically significant improvement", "tokens": [50990, 300, 321, 362, 36478, 4776, 10444, 51106], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1066, "seek": 335238, "start": 3367.2200000000003, "end": 3369.5, "text": " with the spectral nudging configuration.", "tokens": [51106, 365, 264, 42761, 40045, 3249, 11694, 13, 51220], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1067, "seek": 335238, "start": 3369.5, "end": 3373.42, "text": " And if it is blue, then we have deterioration", "tokens": [51220, 400, 498, 309, 307, 3344, 11, 550, 321, 362, 26431, 399, 51416], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1068, "seek": 335238, "start": 3373.42, "end": 3374.9, "text": " with spectral nudging.", "tokens": [51416, 365, 42761, 40045, 3249, 13, 51490], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1069, "seek": 335238, "start": 3374.9, "end": 3376.38, "text": " What we can see from these figures,", "tokens": [51490, 708, 321, 393, 536, 490, 613, 9624, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13158458073933918, "compression_ratio": 1.8669950738916257, "no_speech_prob": 0.00034387613413855433}, {"id": 1070, "seek": 337638, "start": 3376.38, "end": 3379.5, "text": " this is for winter over the globe.", "tokens": [50364, 341, 307, 337, 6355, 670, 264, 15371, 13, 50520], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1071, "seek": 337638, "start": 3379.5, "end": 3383.6600000000003, "text": " That beyond five days and up to 10 days,", "tokens": [50520, 663, 4399, 1732, 1708, 293, 493, 281, 1266, 1708, 11, 50728], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1072, "seek": 337638, "start": 3383.6600000000003, "end": 3386.54, "text": " we can see that there is tremendous improvement", "tokens": [50728, 321, 393, 536, 300, 456, 307, 10048, 10444, 50872], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1073, "seek": 337638, "start": 3386.54, "end": 3389.6600000000003, "text": " in the scores for the standard deviation of error, which", "tokens": [50872, 294, 264, 13444, 337, 264, 3832, 25163, 295, 6713, 11, 597, 51028], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1074, "seek": 337638, "start": 3389.6600000000003, "end": 3390.38, "text": " is more difficult.", "tokens": [51028, 307, 544, 2252, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1075, "seek": 337638, "start": 3390.38, "end": 3391.78, "text": " Excuse me, Sayed.", "tokens": [51064, 11359, 385, 11, 6463, 292, 13, 51134], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1076, "seek": 337638, "start": 3391.78, "end": 3394.38, "text": " You have two minutes left.", "tokens": [51134, 509, 362, 732, 2077, 1411, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1077, "seek": 337638, "start": 3394.38, "end": 3397.58, "text": " OK, I'll try to be faster.", "tokens": [51264, 2264, 11, 286, 603, 853, 281, 312, 4663, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1078, "seek": 337638, "start": 3397.58, "end": 3402.54, "text": " For the bias, the differences are mixed.", "tokens": [51424, 1171, 264, 12577, 11, 264, 7300, 366, 7467, 13, 51672], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1079, "seek": 337638, "start": 3402.54, "end": 3406.02, "text": " But the bias is less difficult to improve.", "tokens": [51672, 583, 264, 12577, 307, 1570, 2252, 281, 3470, 13, 51846], "temperature": 0.0, "avg_logprob": -0.2237864101634306, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.005162043962627649}, {"id": 1080, "seek": 340602, "start": 3406.54, "end": 3408.9, "text": " Standard deviation is more difficult.", "tokens": [50390, 21298, 25163, 307, 544, 2252, 13, 50508], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1081, "seek": 340602, "start": 3408.9, "end": 3412.34, "text": " So we see tremendous improvement, up to 10% improvement", "tokens": [50508, 407, 321, 536, 10048, 10444, 11, 493, 281, 1266, 4, 10444, 50680], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1082, "seek": 340602, "start": 3412.34, "end": 3414.42, "text": " in the RMSE for longer lead times.", "tokens": [50680, 294, 264, 23790, 5879, 337, 2854, 1477, 1413, 13, 50784], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1083, "seek": 340602, "start": 3414.42, "end": 3416.58, "text": " In summer, though, we see modest improvement.", "tokens": [50784, 682, 4266, 11, 1673, 11, 321, 536, 25403, 10444, 13, 50892], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1084, "seek": 340602, "start": 3416.58, "end": 3420.42, "text": " Still, we see statistically significant improvement", "tokens": [50892, 8291, 11, 321, 536, 36478, 4776, 10444, 51084], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1085, "seek": 340602, "start": 3420.42, "end": 3423.02, "text": " for the standard deviation of error.", "tokens": [51084, 337, 264, 3832, 25163, 295, 6713, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1086, "seek": 340602, "start": 3423.02, "end": 3425.42, "text": " And when you look at the animal liquid relation", "tokens": [51214, 400, 562, 291, 574, 412, 264, 5496, 6553, 9721, 51334], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1087, "seek": 340602, "start": 3425.42, "end": 3429.2599999999998, "text": " coefficient, this is for the 500 hectropascal geopotential.", "tokens": [51334, 17619, 11, 341, 307, 337, 264, 5923, 37358, 1513, 27303, 1519, 404, 310, 2549, 13, 51526], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1088, "seek": 340602, "start": 3429.2599999999998, "end": 3431.7, "text": " We see improvements both in winter and summer.", "tokens": [51526, 492, 536, 13797, 1293, 294, 6355, 293, 4266, 13, 51648], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1089, "seek": 340602, "start": 3431.7, "end": 3434.94, "text": " In winter, in terms of the gain in predictability,", "tokens": [51648, 682, 6355, 11, 294, 2115, 295, 264, 6052, 294, 6069, 2310, 11, 51810], "temperature": 0.0, "avg_logprob": -0.18752054067758414, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.0004153022600803524}, {"id": 1090, "seek": 343494, "start": 3434.94, "end": 3436.94, "text": " it's around 18 hours.", "tokens": [50364, 309, 311, 926, 2443, 2496, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1091, "seek": 343494, "start": 3436.94, "end": 3440.34, "text": " In summer, it's about eight hours improvement.", "tokens": [50464, 682, 4266, 11, 309, 311, 466, 3180, 2496, 10444, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1092, "seek": 343494, "start": 3440.34, "end": 3443.2200000000003, "text": " And they're both statistically significant.", "tokens": [50634, 400, 436, 434, 1293, 36478, 4776, 13, 50778], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1093, "seek": 343494, "start": 3443.2200000000003, "end": 3447.7000000000003, "text": " And last results is about tropical cyclone position error.", "tokens": [50778, 400, 1036, 3542, 307, 466, 22857, 19474, 546, 2535, 6713, 13, 51002], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1094, "seek": 343494, "start": 3447.7000000000003, "end": 3450.34, "text": " This is another thing that is very difficult to improve.", "tokens": [51002, 639, 307, 1071, 551, 300, 307, 588, 2252, 281, 3470, 13, 51134], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1095, "seek": 343494, "start": 3450.34, "end": 3454.98, "text": " And our model tends to have, for the alone track position,", "tokens": [51134, 400, 527, 2316, 12258, 281, 362, 11, 337, 264, 3312, 2837, 2535, 11, 51366], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1096, "seek": 343494, "start": 3454.98, "end": 3458.1, "text": " we tend to lag the model.", "tokens": [51366, 321, 3928, 281, 8953, 264, 2316, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1097, "seek": 343494, "start": 3458.1, "end": 3462.02, "text": " And we can improve that lagging with spectral nudging", "tokens": [51522, 400, 321, 393, 3470, 300, 8953, 3249, 365, 42761, 40045, 3249, 51718], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1098, "seek": 343494, "start": 3462.02, "end": 3462.94, "text": " towards graphcast.", "tokens": [51718, 3030, 4295, 3734, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1677731132507324, "compression_ratio": 1.6637931034482758, "no_speech_prob": 0.00017624291649553925}, {"id": 1099, "seek": 346294, "start": 3462.94, "end": 3464.98, "text": " And for the cross-track position error,", "tokens": [50364, 400, 337, 264, 3278, 12, 19466, 2535, 6713, 11, 50466], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1100, "seek": 346294, "start": 3464.98, "end": 3468.1, "text": " our cyclones tend to veer to the right", "tokens": [50466, 527, 19474, 2213, 3928, 281, 1241, 260, 281, 264, 558, 50622], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1101, "seek": 346294, "start": 3468.1, "end": 3470.02, "text": " from the observed trajectory.", "tokens": [50622, 490, 264, 13095, 21512, 13, 50718], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1102, "seek": 346294, "start": 3470.02, "end": 3472.86, "text": " And we are able to considerably improve", "tokens": [50718, 400, 321, 366, 1075, 281, 31308, 3470, 50860], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1103, "seek": 346294, "start": 3472.86, "end": 3477.7400000000002, "text": " that aspect of the position of tropical cyclone also.", "tokens": [50860, 300, 4171, 295, 264, 2535, 295, 22857, 19474, 546, 611, 13, 51104], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1104, "seek": 346294, "start": 3477.7400000000002, "end": 3480.38, "text": " And finally, just this figure, I'm", "tokens": [51104, 400, 2721, 11, 445, 341, 2573, 11, 286, 478, 51236], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1105, "seek": 346294, "start": 3480.38, "end": 3484.7000000000003, "text": " showing the temperature anomaly at 850 hectropascal", "tokens": [51236, 4099, 264, 4292, 42737, 412, 1649, 2803, 37358, 1513, 27303, 51452], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1106, "seek": 346294, "start": 3484.7000000000003, "end": 3487.34, "text": " for a lead time of 240 hours for a single case.", "tokens": [51452, 337, 257, 1477, 565, 295, 26837, 2496, 337, 257, 2167, 1389, 13, 51584], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1107, "seek": 346294, "start": 3487.34, "end": 3490.86, "text": " We have the GDPS control on the left,", "tokens": [51584, 492, 362, 264, 19599, 50, 1969, 322, 264, 1411, 11, 51760], "temperature": 0.0, "avg_logprob": -0.17173648574977246, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00020582435536198318}, {"id": 1108, "seek": 349086, "start": 3490.86, "end": 3493.6200000000003, "text": " graphcast in the middle, and GDPS with spectral nudging", "tokens": [50364, 4295, 3734, 294, 264, 2808, 11, 293, 19599, 50, 365, 42761, 40045, 3249, 50502], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1109, "seek": 349086, "start": 3493.6200000000003, "end": 3494.54, "text": " on the right.", "tokens": [50502, 322, 264, 558, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1110, "seek": 349086, "start": 3494.54, "end": 3498.42, "text": " And we can see that graphcast barely has any fine scale.", "tokens": [50548, 400, 321, 393, 536, 300, 4295, 3734, 10268, 575, 604, 2489, 4373, 13, 50742], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1111, "seek": 349086, "start": 3498.42, "end": 3501.7000000000003, "text": " And both GDPS control and with spectral nudging,", "tokens": [50742, 400, 1293, 19599, 50, 1969, 293, 365, 42761, 40045, 3249, 11, 50906], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1112, "seek": 349086, "start": 3501.7000000000003, "end": 3504.82, "text": " they both have comparable fine scale information.", "tokens": [50906, 436, 1293, 362, 25323, 2489, 4373, 1589, 13, 51062], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1113, "seek": 349086, "start": 3504.82, "end": 3507.98, "text": " And we get this improvement by nudging as well.", "tokens": [51062, 400, 321, 483, 341, 10444, 538, 40045, 3249, 382, 731, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1114, "seek": 349086, "start": 3507.98, "end": 3511.86, "text": " So to summarize, we developed and hybrid NWP system", "tokens": [51220, 407, 281, 20858, 11, 321, 4743, 293, 13051, 426, 54, 47, 1185, 51414], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1115, "seek": 349086, "start": 3511.86, "end": 3516.1400000000003, "text": " that fuses NWP models with AI models to spectral nudging.", "tokens": [51414, 300, 283, 8355, 426, 54, 47, 5245, 365, 7318, 5245, 281, 42761, 40045, 3249, 13, 51628], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1116, "seek": 349086, "start": 3516.1400000000003, "end": 3518.46, "text": " And by leveraging more accurate large-scale predictions", "tokens": [51628, 400, 538, 32666, 544, 8559, 2416, 12, 20033, 21264, 51744], "temperature": 0.0, "avg_logprob": -0.1331870884730898, "compression_ratio": 1.7283464566929134, "no_speech_prob": 0.00034552731085568666}, {"id": 1117, "seek": 351846, "start": 3518.46, "end": 3522.82, "text": " from graphcast, we are able to significantly improve", "tokens": [50364, 490, 4295, 3734, 11, 321, 366, 1075, 281, 10591, 3470, 50582], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1118, "seek": 351846, "start": 3522.82, "end": 3527.58, "text": " our prediction scale with GDPS.", "tokens": [50582, 527, 17630, 4373, 365, 19599, 50, 13, 50820], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1119, "seek": 351846, "start": 3527.58, "end": 3530.7, "text": " And I want to stress that the improvement that we have", "tokens": [50820, 400, 286, 528, 281, 4244, 300, 264, 10444, 300, 321, 362, 50976], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1120, "seek": 351846, "start": 3530.7, "end": 3533.7, "text": " is roughly equivalent to one solid innovation cycle, which", "tokens": [50976, 307, 9810, 10344, 281, 472, 5100, 8504, 6586, 11, 597, 51126], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1121, "seek": 351846, "start": 3533.7, "end": 3537.26, "text": " is about four years of work involving many scientists", "tokens": [51126, 307, 466, 1451, 924, 295, 589, 17030, 867, 7708, 51304], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1122, "seek": 351846, "start": 3537.26, "end": 3539.7400000000002, "text": " from across the Meteorological Research Division", "tokens": [51304, 490, 2108, 264, 43328, 284, 4383, 10303, 17183, 51428], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1123, "seek": 351846, "start": 3539.7400000000002, "end": 3540.98, "text": " of Environment Canada.", "tokens": [51428, 295, 35354, 6309, 13, 51490], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1124, "seek": 351846, "start": 3540.98, "end": 3544.54, "text": " And we were able to achieve something comparable", "tokens": [51490, 400, 321, 645, 1075, 281, 4584, 746, 25323, 51668], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1125, "seek": 351846, "start": 3544.54, "end": 3546.82, "text": " in a matter of four to five months.", "tokens": [51668, 294, 257, 1871, 295, 1451, 281, 1732, 2493, 13, 51782], "temperature": 0.0, "avg_logprob": -0.11769246061642964, "compression_ratio": 1.5670498084291187, "no_speech_prob": 0.0006440583965741098}, {"id": 1126, "seek": 354682, "start": 3546.82, "end": 3550.2200000000003, "text": " And also, I want to stress that this", "tokens": [50364, 400, 611, 11, 286, 528, 281, 4244, 300, 341, 50534], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1127, "seek": 354682, "start": 3550.2200000000003, "end": 3553.54, "text": " is based on work that uses a graphcast model that", "tokens": [50534, 307, 2361, 322, 589, 300, 4960, 257, 4295, 3734, 2316, 300, 50700], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1128, "seek": 354682, "start": 3553.54, "end": 3555.06, "text": " has not been fine-tuned.", "tokens": [50700, 575, 406, 668, 2489, 12, 83, 43703, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1129, "seek": 354682, "start": 3555.06, "end": 3559.78, "text": " And with fine-tuning graphcast to emulate our own CMC", "tokens": [50776, 400, 365, 2489, 12, 83, 37726, 4295, 3734, 281, 45497, 527, 1065, 20424, 34, 51012], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1130, "seek": 354682, "start": 3559.78, "end": 3564.34, "text": " analysis, which is a work in progress by my colleague Christopher,", "tokens": [51012, 5215, 11, 597, 307, 257, 589, 294, 4205, 538, 452, 13532, 20649, 11, 51240], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1131, "seek": 354682, "start": 3564.34, "end": 3567.6200000000003, "text": " we hope that we could improve further.", "tokens": [51240, 321, 1454, 300, 321, 727, 3470, 3052, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1132, "seek": 354682, "start": 3567.6200000000003, "end": 3569.26, "text": " And currently, with this configuration,", "tokens": [51404, 400, 4362, 11, 365, 341, 11694, 11, 51486], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1133, "seek": 354682, "start": 3569.26, "end": 3572.38, "text": " we have about 25% increase in the computational cost.", "tokens": [51486, 321, 362, 466, 3552, 4, 3488, 294, 264, 28270, 2063, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1134, "seek": 354682, "start": 3572.38, "end": 3574.38, "text": " But this is without any optimization.", "tokens": [51642, 583, 341, 307, 1553, 604, 19618, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1135, "seek": 354682, "start": 3574.38, "end": 3576.26, "text": " And with some optimization, we hope", "tokens": [51742, 400, 365, 512, 19618, 11, 321, 1454, 51836], "temperature": 0.0, "avg_logprob": -0.1222390884008163, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.0002992915397044271}, {"id": 1136, "seek": 357626, "start": 3576.26, "end": 3578.46, "text": " that we will be able to reduce it to something", "tokens": [50364, 300, 321, 486, 312, 1075, 281, 5407, 309, 281, 746, 50474], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1137, "seek": 357626, "start": 3578.46, "end": 3581.7000000000003, "text": " like less than 15% in the near future.", "tokens": [50474, 411, 1570, 813, 2119, 4, 294, 264, 2651, 2027, 13, 50636], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1138, "seek": 357626, "start": 3581.7000000000003, "end": 3584.7000000000003, "text": " So with that, I will end my presentation.", "tokens": [50636, 407, 365, 300, 11, 286, 486, 917, 452, 5860, 13, 50786], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1139, "seek": 357626, "start": 3584.7000000000003, "end": 3586.46, "text": " Thank you, Sayada.", "tokens": [50786, 1044, 291, 11, 6463, 1538, 13, 50874], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1140, "seek": 357626, "start": 3586.46, "end": 3590.86, "text": " It's a very impressive conclusion.", "tokens": [50874, 467, 311, 257, 588, 8992, 10063, 13, 51094], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1141, "seek": 357626, "start": 3590.86, "end": 3593.78, "text": " I have one question here.", "tokens": [51094, 286, 362, 472, 1168, 510, 13, 51240], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1142, "seek": 357626, "start": 3593.78, "end": 3600.98, "text": " Nudging is done as gem integration goes or at posteriori?", "tokens": [51240, 426, 532, 3249, 307, 1096, 382, 7173, 10980, 1709, 420, 412, 33529, 72, 30, 51600], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1143, "seek": 357626, "start": 3600.98, "end": 3603.1400000000003, "text": " No, it's online.", "tokens": [51600, 883, 11, 309, 311, 2950, 13, 51708], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1144, "seek": 357626, "start": 3603.1400000000003, "end": 3605.94, "text": " So what happens is, as I said, you", "tokens": [51708, 407, 437, 2314, 307, 11, 382, 286, 848, 11, 291, 51848], "temperature": 0.0, "avg_logprob": -0.26842533458362927, "compression_ratio": 1.4608294930875576, "no_speech_prob": 0.003159273648634553}, {"id": 1145, "seek": 360594, "start": 3606.82, "end": 3609.18, "text": " solve the dynamic step.", "tokens": [50408, 5039, 264, 8546, 1823, 13, 50526], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1146, "seek": 360594, "start": 3609.18, "end": 3612.38, "text": " Because we have the dynamic sub-step and the physics sub-step.", "tokens": [50526, 1436, 321, 362, 264, 8546, 1422, 12, 16792, 293, 264, 10649, 1422, 12, 16792, 13, 50686], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1147, "seek": 360594, "start": 3612.38, "end": 3614.66, "text": " And then we do the coupling in the split mode.", "tokens": [50686, 400, 550, 321, 360, 264, 37447, 294, 264, 7472, 4391, 13, 50800], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1148, "seek": 360594, "start": 3614.66, "end": 3616.82, "text": " So we solve the dynamic sub-step.", "tokens": [50800, 407, 321, 5039, 264, 8546, 1422, 12, 16792, 13, 50908], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1149, "seek": 360594, "start": 3616.82, "end": 3618.66, "text": " We have a solution from dynamics,", "tokens": [50908, 492, 362, 257, 3827, 490, 15679, 11, 51000], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1150, "seek": 360594, "start": 3618.66, "end": 3620.02, "text": " which is an intermediate solution.", "tokens": [51000, 597, 307, 364, 19376, 3827, 13, 51068], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1151, "seek": 360594, "start": 3620.02, "end": 3622.5, "text": " Then we update that by nudging.", "tokens": [51068, 1396, 321, 5623, 300, 538, 40045, 3249, 13, 51192], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1152, "seek": 360594, "start": 3622.5, "end": 3625.2200000000003, "text": " And then we feed that updated solution to the physics.", "tokens": [51192, 400, 550, 321, 3154, 300, 10588, 3827, 281, 264, 10649, 13, 51328], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1153, "seek": 360594, "start": 3625.2200000000003, "end": 3627.54, "text": " And then we get the complete solution of the model time", "tokens": [51328, 400, 550, 321, 483, 264, 3566, 3827, 295, 264, 2316, 565, 51444], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1154, "seek": 360594, "start": 3627.54, "end": 3628.04, "text": " step.", "tokens": [51444, 1823, 13, 51469], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1155, "seek": 360594, "start": 3628.04, "end": 3631.54, "text": " And then the next time, dynamics uses that solution", "tokens": [51469, 400, 550, 264, 958, 565, 11, 15679, 4960, 300, 3827, 51644], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1156, "seek": 360594, "start": 3631.54, "end": 3633.5, "text": " to predict the next dynamic step.", "tokens": [51644, 281, 6069, 264, 958, 8546, 1823, 13, 51742], "temperature": 0.0, "avg_logprob": -0.19357669353485107, "compression_ratio": 2.1705069124423964, "no_speech_prob": 0.007529754191637039}, {"id": 1157, "seek": 363350, "start": 3633.5, "end": 3636.42, "text": " So it's not a posteriori.", "tokens": [50364, 407, 309, 311, 406, 257, 33529, 72, 13, 50510], "temperature": 0.0, "avg_logprob": -0.30007610764614373, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.0010454053990542889}, {"id": 1158, "seek": 363350, "start": 3636.42, "end": 3640.78, "text": " It's an online update.", "tokens": [50510, 467, 311, 364, 2950, 5623, 13, 50728], "temperature": 0.0, "avg_logprob": -0.30007610764614373, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.0010454053990542889}, {"id": 1159, "seek": 363350, "start": 3640.78, "end": 3645.82, "text": " Last quick one, how does GDPSSN compare with GraphCast?", "tokens": [50728, 5264, 1702, 472, 11, 577, 775, 460, 35, 6273, 32481, 6794, 365, 21884, 34, 525, 30, 50980], "temperature": 0.0, "avg_logprob": -0.30007610764614373, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.0010454053990542889}, {"id": 1160, "seek": 363350, "start": 3648.74, "end": 3653.38, "text": " That was already shown by Ervig in the previous presentation.", "tokens": [51126, 663, 390, 1217, 4898, 538, 3300, 85, 328, 294, 264, 3894, 5860, 13, 51358], "temperature": 0.0, "avg_logprob": -0.30007610764614373, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.0010454053990542889}, {"id": 1161, "seek": 363350, "start": 3653.38, "end": 3657.78, "text": " That GDPSS, I mean, if you talk about control GDPSS", "tokens": [51358, 663, 460, 35, 6273, 50, 11, 286, 914, 11, 498, 291, 751, 466, 1969, 460, 35, 6273, 50, 51578], "temperature": 0.0, "avg_logprob": -0.30007610764614373, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.0010454053990542889}, {"id": 1162, "seek": 363350, "start": 3657.78, "end": 3660.38, "text": " versus GraphCast, that presentation of Ervig", "tokens": [51578, 5717, 21884, 34, 525, 11, 300, 5860, 295, 3300, 85, 328, 51708], "temperature": 0.0, "avg_logprob": -0.30007610764614373, "compression_ratio": 1.413978494623656, "no_speech_prob": 0.0010454053990542889}, {"id": 1163, "seek": 366038, "start": 3660.38, "end": 3664.26, "text": " should allow you to see like it actually improves.", "tokens": [50364, 820, 2089, 291, 281, 536, 411, 309, 767, 24771, 13, 50558], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1164, "seek": 366038, "start": 3664.26, "end": 3666.02, "text": " What I am missing in my figures, because this", "tokens": [50558, 708, 286, 669, 5361, 294, 452, 9624, 11, 570, 341, 50646], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1165, "seek": 366038, "start": 3666.02, "end": 3668.94, "text": " is still a work in progress, I mean,", "tokens": [50646, 307, 920, 257, 589, 294, 4205, 11, 286, 914, 11, 50792], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1166, "seek": 366038, "start": 3668.94, "end": 3672.1, "text": " in our paper that we expect to submit soon,", "tokens": [50792, 294, 527, 3035, 300, 321, 2066, 281, 10315, 2321, 11, 50950], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1167, "seek": 366038, "start": 3672.1, "end": 3676.1400000000003, "text": " which we will be adding the GraphCast also,", "tokens": [50950, 597, 321, 486, 312, 5127, 264, 21884, 34, 525, 611, 11, 51152], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1168, "seek": 366038, "start": 3676.1400000000003, "end": 3680.3, "text": " focus in this, for example, in these figures to show", "tokens": [51152, 1879, 294, 341, 11, 337, 1365, 11, 294, 613, 9624, 281, 855, 51360], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1169, "seek": 366038, "start": 3680.3, "end": 3685.06, "text": " control GDPS with GraphCast and GraphCast itself,", "tokens": [51360, 1969, 460, 35, 6273, 365, 21884, 34, 525, 293, 21884, 34, 525, 2564, 11, 51598], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1170, "seek": 366038, "start": 3685.06, "end": 3688.26, "text": " like how much of the improvement is coming from GraphCast.", "tokens": [51598, 411, 577, 709, 295, 264, 10444, 307, 1348, 490, 21884, 34, 525, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2008872561984592, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.00957020279020071}, {"id": 1171, "seek": 368826, "start": 3688.26, "end": 3690.2200000000003, "text": " But it is definitely coming from GraphCast,", "tokens": [50364, 583, 309, 307, 2138, 1348, 490, 21884, 34, 525, 11, 50462], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1172, "seek": 368826, "start": 3690.2200000000003, "end": 3693.6600000000003, "text": " as Ervig's presentation showed that the large scales in GraphCast", "tokens": [50462, 382, 3300, 85, 328, 311, 5860, 4712, 300, 264, 2416, 17408, 294, 21884, 34, 525, 50634], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1173, "seek": 368826, "start": 3693.6600000000003, "end": 3695.2200000000003, "text": " are much better.", "tokens": [50634, 366, 709, 1101, 13, 50712], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1174, "seek": 368826, "start": 3695.2200000000003, "end": 3698.2200000000003, "text": " Yes, Isayed, looking forward to read the print.", "tokens": [50712, 1079, 11, 1119, 320, 292, 11, 1237, 2128, 281, 1401, 264, 4482, 13, 50862], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1175, "seek": 368826, "start": 3698.2200000000003, "end": 3701.3, "text": " That will be very popular, I'm sure.", "tokens": [50862, 663, 486, 312, 588, 3743, 11, 286, 478, 988, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1176, "seek": 368826, "start": 3701.3, "end": 3702.6200000000003, "text": " Thank you.", "tokens": [51016, 1044, 291, 13, 51082], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1177, "seek": 368826, "start": 3702.6200000000003, "end": 3703.1400000000003, "text": " Merci.", "tokens": [51082, 19856, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1178, "seek": 368826, "start": 3703.1400000000003, "end": 3704.86, "text": " Anne, at what?", "tokens": [51108, 13706, 11, 412, 437, 30, 51194], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1179, "seek": 368826, "start": 3704.86, "end": 3706.94, "text": " Yes, so we're moving.", "tokens": [51194, 1079, 11, 370, 321, 434, 2684, 13, 51298], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1180, "seek": 368826, "start": 3706.94, "end": 3708.42, "text": " You might have noticed in the schedule", "tokens": [51298, 509, 1062, 362, 5694, 294, 264, 7567, 51372], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1181, "seek": 368826, "start": 3708.42, "end": 3710.5, "text": " that Christian Isayed was originally", "tokens": [51372, 300, 5778, 1119, 320, 292, 390, 7993, 51476], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1182, "seek": 368826, "start": 3710.5, "end": 3711.5, "text": " supposed to present this.", "tokens": [51476, 3442, 281, 1974, 341, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1183, "seek": 368826, "start": 3711.5, "end": 3716.5400000000004, "text": " But I want to thank Madalina Socer to have accepted", "tokens": [51526, 583, 286, 528, 281, 1309, 5326, 304, 1426, 407, 1776, 281, 362, 9035, 51778], "temperature": 0.0, "avg_logprob": -0.2948310911193375, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.0037344812881201506}, {"id": 1184, "seek": 371654, "start": 3716.54, "end": 3720.14, "text": " to present and prepare this presentation for us.", "tokens": [50364, 281, 1974, 293, 5940, 341, 5860, 337, 505, 13, 50544], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1185, "seek": 371654, "start": 3720.14, "end": 3723.9, "text": " So she's a Climate Extreme Specialist at Environment Canada.", "tokens": [50544, 407, 750, 311, 257, 27025, 39525, 11863, 468, 412, 35354, 6309, 13, 50732], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1186, "seek": 371654, "start": 3723.9, "end": 3726.46, "text": " She's going to be presenting on the development", "tokens": [50732, 1240, 311, 516, 281, 312, 15578, 322, 264, 3250, 50860], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1187, "seek": 371654, "start": 3726.46, "end": 3730.1, "text": " of artificial intelligence downscaling applications", "tokens": [50860, 295, 11677, 7599, 760, 4417, 4270, 5821, 51042], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1188, "seek": 371654, "start": 3730.1, "end": 3735.66, "text": " for medium-range forecasts of weather elements at CCMEP.", "tokens": [51042, 337, 6399, 12, 14521, 49421, 295, 5503, 4959, 412, 12630, 44, 8929, 13, 51320], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1189, "seek": 371654, "start": 3735.66, "end": 3739.1, "text": " So without further ado, over to you, Madalina.", "tokens": [51320, 407, 1553, 3052, 22450, 11, 670, 281, 291, 11, 5326, 304, 1426, 13, 51492], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1190, "seek": 371654, "start": 3739.1, "end": 3740.22, "text": " OK, thank you.", "tokens": [51492, 2264, 11, 1309, 291, 13, 51548], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1191, "seek": 371654, "start": 3740.22, "end": 3742.06, "text": " I hope you're hearing me well.", "tokens": [51548, 286, 1454, 291, 434, 4763, 385, 731, 13, 51640], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1192, "seek": 371654, "start": 3742.06, "end": 3743.54, "text": " We are, yes.", "tokens": [51640, 492, 366, 11, 2086, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1193, "seek": 371654, "start": 3743.54, "end": 3744.34, "text": " OK, great.", "tokens": [51714, 2264, 11, 869, 13, 51754], "temperature": 0.0, "avg_logprob": -0.18260185740818488, "compression_ratio": 1.5258964143426295, "no_speech_prob": 0.04130881279706955}, {"id": 1194, "seek": 374434, "start": 3744.34, "end": 3746.58, "text": " So I am here today to present to you", "tokens": [50364, 407, 286, 669, 510, 965, 281, 1974, 281, 291, 50476], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1195, "seek": 374434, "start": 3746.58, "end": 3749.38, "text": " some development of artificial intelligence downscaling", "tokens": [50476, 512, 3250, 295, 11677, 7599, 760, 4417, 4270, 50616], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1196, "seek": 374434, "start": 3749.38, "end": 3751.86, "text": " techniques that we're doing at CCMEP in collaboration", "tokens": [50616, 7512, 300, 321, 434, 884, 412, 12630, 44, 8929, 294, 9363, 50740], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1197, "seek": 374434, "start": 3751.86, "end": 3753.2200000000003, "text": " with IBM Research.", "tokens": [50740, 365, 23487, 10303, 13, 50808], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1198, "seek": 374434, "start": 3753.2200000000003, "end": 3755.02, "text": " And I would like to acknowledge my co-authors", "tokens": [50808, 400, 286, 576, 411, 281, 10692, 452, 598, 12, 40198, 830, 50898], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1199, "seek": 374434, "start": 3755.02, "end": 3758.9, "text": " that are listed here, both from ECCC and IBM Research.", "tokens": [50898, 300, 366, 10052, 510, 11, 1293, 490, 462, 11717, 34, 293, 23487, 10303, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1200, "seek": 374434, "start": 3763.78, "end": 3766.5, "text": " So first, I would like to say that the vision for this project", "tokens": [51336, 407, 700, 11, 286, 576, 411, 281, 584, 300, 264, 5201, 337, 341, 1716, 51472], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1201, "seek": 374434, "start": 3766.5, "end": 3769.1800000000003, "text": " is actually to help us offer seamless day one to day 10", "tokens": [51472, 307, 767, 281, 854, 505, 2626, 28677, 786, 472, 281, 786, 1266, 51606], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1202, "seek": 374434, "start": 3769.1800000000003, "end": 3772.5, "text": " public forecast products as part of the transformation", "tokens": [51606, 1908, 14330, 3383, 382, 644, 295, 264, 9887, 51772], "temperature": 0.0, "avg_logprob": -0.13891178852803, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.004488181788474321}, {"id": 1203, "seek": 377250, "start": 3772.5, "end": 3774.74, "text": " of the meteorological service of Canada.", "tokens": [50364, 295, 264, 25313, 4383, 2643, 295, 6309, 13, 50476], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1204, "seek": 377250, "start": 3774.74, "end": 3776.38, "text": " So in order to do this, we need to bridge", "tokens": [50476, 407, 294, 1668, 281, 360, 341, 11, 321, 643, 281, 7283, 50558], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1205, "seek": 377250, "start": 3776.38, "end": 3778.82, "text": " the gap between high-resolution, short-term forecasts,", "tokens": [50558, 264, 7417, 1296, 1090, 12, 495, 3386, 11, 2099, 12, 7039, 49421, 11, 50680], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1206, "seek": 377250, "start": 3778.82, "end": 3782.78, "text": " and medium-range, lower-resolution forecasts.", "tokens": [50680, 293, 6399, 12, 14521, 11, 3126, 12, 495, 3386, 49421, 13, 50878], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1207, "seek": 377250, "start": 3782.78, "end": 3784.38, "text": " And the reason why we want to do this", "tokens": [50878, 400, 264, 1778, 983, 321, 528, 281, 360, 341, 50958], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1208, "seek": 377250, "start": 3784.38, "end": 3786.7, "text": " is because we know that insufficient horizontal", "tokens": [50958, 307, 570, 321, 458, 300, 41709, 12750, 51074], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1209, "seek": 377250, "start": 3786.7, "end": 3789.86, "text": " resolution causes forecast errors and especially biases.", "tokens": [51074, 8669, 7700, 14330, 13603, 293, 2318, 32152, 13, 51232], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1210, "seek": 377250, "start": 3789.86, "end": 3792.02, "text": " I am showing here a graph of bias,", "tokens": [51232, 286, 669, 4099, 510, 257, 4295, 295, 12577, 11, 51340], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1211, "seek": 377250, "start": 3792.02, "end": 3795.9, "text": " a comparison between a low-resolution model in red", "tokens": [51340, 257, 9660, 1296, 257, 2295, 12, 495, 3386, 2316, 294, 2182, 51534], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1212, "seek": 377250, "start": 3795.9, "end": 3797.82, "text": " and the high-resolution model in blue.", "tokens": [51534, 293, 264, 1090, 12, 495, 3386, 2316, 294, 3344, 13, 51630], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1213, "seek": 377250, "start": 3797.82, "end": 3800.62, "text": " And what we are saying here, the closest we are to zero,", "tokens": [51630, 400, 437, 321, 366, 1566, 510, 11, 264, 13699, 321, 366, 281, 4018, 11, 51770], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1214, "seek": 377250, "start": 3800.62, "end": 3801.62, "text": " the less bias we have.", "tokens": [51770, 264, 1570, 12577, 321, 362, 13, 51820], "temperature": 0.0, "avg_logprob": -0.12780294885168542, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.017559103667736053}, {"id": 1215, "seek": 380162, "start": 3801.62, "end": 3805.2999999999997, "text": " And we really see that increasing horizontal resolution", "tokens": [50364, 400, 321, 534, 536, 300, 5662, 12750, 8669, 50548], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1216, "seek": 380162, "start": 3805.2999999999997, "end": 3806.8599999999997, "text": " is improving this bias.", "tokens": [50548, 307, 11470, 341, 12577, 13, 50626], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1217, "seek": 380162, "start": 3806.8599999999997, "end": 3808.3399999999997, "text": " Usually, the way that we do that is", "tokens": [50626, 11419, 11, 264, 636, 300, 321, 360, 300, 307, 50700], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1218, "seek": 380162, "start": 3808.3399999999997, "end": 3809.98, "text": " by doing dynamical downscaling.", "tokens": [50700, 538, 884, 5999, 804, 760, 4417, 4270, 13, 50782], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1219, "seek": 380162, "start": 3809.98, "end": 3812.54, "text": " So running numerical weather prediction models.", "tokens": [50782, 407, 2614, 29054, 5503, 17630, 5245, 13, 50910], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1220, "seek": 380162, "start": 3812.54, "end": 3814.9, "text": " But this is very computationally expensive,", "tokens": [50910, 583, 341, 307, 588, 24903, 379, 5124, 11, 51028], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1221, "seek": 380162, "start": 3814.9, "end": 3817.14, "text": " which makes it limiting.", "tokens": [51028, 597, 1669, 309, 22083, 13, 51140], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1222, "seek": 380162, "start": 3817.14, "end": 3819.98, "text": " A partial solution to this is doing statistical downscaling,", "tokens": [51140, 316, 14641, 3827, 281, 341, 307, 884, 22820, 760, 4417, 4270, 11, 51282], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1223, "seek": 380162, "start": 3819.98, "end": 3823.18, "text": " which means using past data and deriving", "tokens": [51282, 597, 1355, 1228, 1791, 1412, 293, 1163, 2123, 51442], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1224, "seek": 380162, "start": 3823.18, "end": 3825.74, "text": " statistical relationships from this past data,", "tokens": [51442, 22820, 6159, 490, 341, 1791, 1412, 11, 51570], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1225, "seek": 380162, "start": 3825.74, "end": 3828.2999999999997, "text": " such that we apply these relationships", "tokens": [51570, 1270, 300, 321, 3079, 613, 6159, 51698], "temperature": 0.0, "avg_logprob": -0.15194137074122918, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0016761713195592165}, {"id": 1226, "seek": 382830, "start": 3828.3, "end": 3832.82, "text": " on the course input and we obtain high-resolution output.", "tokens": [50364, 322, 264, 1164, 4846, 293, 321, 12701, 1090, 12, 495, 3386, 5598, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1227, "seek": 382830, "start": 3832.82, "end": 3834.82, "text": " These type of solutions are limited", "tokens": [50590, 1981, 2010, 295, 6547, 366, 5567, 50690], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1228, "seek": 382830, "start": 3834.82, "end": 3836.6200000000003, "text": " by the fact that we have to impose certain", "tokens": [50690, 538, 264, 1186, 300, 321, 362, 281, 26952, 1629, 50780], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1229, "seek": 382830, "start": 3836.6200000000003, "end": 3839.02, "text": " non-mathematical relationships in the data.", "tokens": [50780, 2107, 12, 24761, 8615, 804, 6159, 294, 264, 1412, 13, 50900], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1230, "seek": 382830, "start": 3839.02, "end": 3841.7400000000002, "text": " So now we have a data-driven alternative,", "tokens": [50900, 407, 586, 321, 362, 257, 1412, 12, 25456, 8535, 11, 51036], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1231, "seek": 382830, "start": 3841.7400000000002, "end": 3845.3, "text": " which is to apply artificial intelligence techniques", "tokens": [51036, 597, 307, 281, 3079, 11677, 7599, 7512, 51214], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1232, "seek": 382830, "start": 3845.3, "end": 3846.46, "text": " to do downscaling.", "tokens": [51214, 281, 360, 760, 4417, 4270, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1233, "seek": 382830, "start": 3846.46, "end": 3849.02, "text": " And what this does is that it allows", "tokens": [51272, 400, 437, 341, 775, 307, 300, 309, 4045, 51400], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1234, "seek": 382830, "start": 3849.02, "end": 3851.1800000000003, "text": " to derive complex relationships in the data", "tokens": [51400, 281, 28446, 3997, 6159, 294, 264, 1412, 51508], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1235, "seek": 382830, "start": 3851.1800000000003, "end": 3853.26, "text": " that we provide to these models.", "tokens": [51508, 300, 321, 2893, 281, 613, 5245, 13, 51612], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1236, "seek": 382830, "start": 3853.26, "end": 3856.5, "text": " So our objective is to develop", "tokens": [51612, 407, 527, 10024, 307, 281, 1499, 51774], "temperature": 0.0, "avg_logprob": -0.12156847725927303, "compression_ratio": 1.71484375, "no_speech_prob": 0.0011651620734483004}, {"id": 1237, "seek": 385650, "start": 3856.5, "end": 3859.14, "text": " artificial intelligence downscaling techniques", "tokens": [50364, 11677, 7599, 760, 4417, 4270, 7512, 50496], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1238, "seek": 385650, "start": 3859.14, "end": 3862.5, "text": " to downscale weather elements from medium range forecast", "tokens": [50496, 281, 760, 20033, 5503, 4959, 490, 6399, 3613, 14330, 50664], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1239, "seek": 385650, "start": 3862.5, "end": 3863.62, "text": " to the kilometric scale.", "tokens": [50664, 281, 264, 5128, 29470, 4373, 13, 50720], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1240, "seek": 385650, "start": 3863.62, "end": 3866.18, "text": " And we will hope that this will help both deterministic", "tokens": [50720, 400, 321, 486, 1454, 300, 341, 486, 854, 1293, 15957, 3142, 50848], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1241, "seek": 385650, "start": 3866.18, "end": 3868.94, "text": " and ensemble forecasting.", "tokens": [50848, 293, 19492, 44331, 13, 50986], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1242, "seek": 385650, "start": 3868.94, "end": 3871.34, "text": " So just to briefly present our project.", "tokens": [50986, 407, 445, 281, 10515, 1974, 527, 1716, 13, 51106], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1243, "seek": 385650, "start": 3871.34, "end": 3873.26, "text": " So this is a collaboration that started this year", "tokens": [51106, 407, 341, 307, 257, 9363, 300, 1409, 341, 1064, 51202], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1244, "seek": 385650, "start": 3873.26, "end": 3875.98, "text": " between CCMAP and IBM Research.", "tokens": [51202, 1296, 12630, 44, 4715, 293, 23487, 10303, 13, 51338], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1245, "seek": 385650, "start": 3875.98, "end": 3877.62, "text": " And here at Environment Canada, we", "tokens": [51338, 400, 510, 412, 35354, 6309, 11, 321, 51420], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1246, "seek": 385650, "start": 3877.62, "end": 3879.1, "text": " have the meteorological expertise.", "tokens": [51420, 362, 264, 25313, 4383, 11769, 13, 51494], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1247, "seek": 385650, "start": 3879.1, "end": 3883.14, "text": " And we definitely have subjects, problems", "tokens": [51494, 400, 321, 2138, 362, 13066, 11, 2740, 51696], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1248, "seek": 385650, "start": 3883.14, "end": 3885.7, "text": " that could really benefit from these artificial intelligence", "tokens": [51696, 300, 727, 534, 5121, 490, 613, 11677, 7599, 51824], "temperature": 0.0, "avg_logprob": -0.18468546660050103, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004981144797056913}, {"id": 1249, "seek": 388570, "start": 3885.7, "end": 3887.18, "text": " solutions, but we don't necessarily", "tokens": [50364, 6547, 11, 457, 321, 500, 380, 4725, 50438], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1250, "seek": 388570, "start": 3887.18, "end": 3889.8199999999997, "text": " have the artificial intelligence expertise, which", "tokens": [50438, 362, 264, 11677, 7599, 11769, 11, 597, 50570], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1251, "seek": 388570, "start": 3889.8199999999997, "end": 3892.7799999999997, "text": " is where IBM Research comes into play.", "tokens": [50570, 307, 689, 23487, 10303, 1487, 666, 862, 13, 50718], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1252, "seek": 388570, "start": 3892.7799999999997, "end": 3894.8199999999997, "text": " And collaborating with them, they are really experts", "tokens": [50718, 400, 30188, 365, 552, 11, 436, 366, 534, 8572, 50820], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1253, "seek": 388570, "start": 3894.8199999999997, "end": 3897.8199999999997, "text": " in this field, will allow us to advance much faster.", "tokens": [50820, 294, 341, 2519, 11, 486, 2089, 505, 281, 7295, 709, 4663, 13, 50970], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1254, "seek": 388570, "start": 3897.8199999999997, "end": 3900.14, "text": " And the expected outcome from this collaboration", "tokens": [50970, 400, 264, 5176, 9700, 490, 341, 9363, 51086], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1255, "seek": 388570, "start": 3900.14, "end": 3902.06, "text": " that for now it's only meant to be on one year,", "tokens": [51086, 300, 337, 586, 309, 311, 787, 4140, 281, 312, 322, 472, 1064, 11, 51182], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1256, "seek": 388570, "start": 3902.06, "end": 3905.5, "text": " is to develop low-cost and efficient alternatives", "tokens": [51182, 307, 281, 1499, 2295, 12, 27718, 293, 7148, 20478, 51354], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1257, "seek": 388570, "start": 3905.5, "end": 3909.7799999999997, "text": " to the computationally expensive dynamical models.", "tokens": [51354, 281, 264, 24903, 379, 5124, 5999, 804, 5245, 13, 51568], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1258, "seek": 388570, "start": 3909.7799999999997, "end": 3913.1, "text": " And the other thing that we want to get from this collaboration", "tokens": [51568, 400, 264, 661, 551, 300, 321, 528, 281, 483, 490, 341, 9363, 51734], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1259, "seek": 388570, "start": 3913.1, "end": 3914.8999999999996, "text": " is we want to learn from IBM Research.", "tokens": [51734, 307, 321, 528, 281, 1466, 490, 23487, 10303, 13, 51824], "temperature": 0.0, "avg_logprob": -0.12215602965581984, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0023976434022188187}, {"id": 1260, "seek": 391490, "start": 3914.9, "end": 3916.54, "text": " So at the end of this project, we", "tokens": [50364, 407, 412, 264, 917, 295, 341, 1716, 11, 321, 50446], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1261, "seek": 391490, "start": 3916.54, "end": 3919.1, "text": " want to enhance our capability at Environment Canada", "tokens": [50446, 528, 281, 11985, 527, 13759, 412, 35354, 6309, 50574], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1262, "seek": 391490, "start": 3919.1, "end": 3921.78, "text": " to be carrying out this type of research and development", "tokens": [50574, 281, 312, 9792, 484, 341, 2010, 295, 2132, 293, 3250, 50708], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1263, "seek": 391490, "start": 3921.78, "end": 3923.7400000000002, "text": " and eventual operational implementation", "tokens": [50708, 293, 33160, 16607, 11420, 50806], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1264, "seek": 391490, "start": 3923.7400000000002, "end": 3927.54, "text": " of AI downscaling techniques.", "tokens": [50806, 295, 7318, 760, 4417, 4270, 7512, 13, 50996], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1265, "seek": 391490, "start": 3927.54, "end": 3931.1, "text": " So the specific goals of our projects are as follows.", "tokens": [50996, 407, 264, 2685, 5493, 295, 527, 4455, 366, 382, 10002, 13, 51174], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1266, "seek": 391490, "start": 3931.1, "end": 3933.94, "text": " So we are interested in downscaling weather elements.", "tokens": [51174, 407, 321, 366, 3102, 294, 760, 4417, 4270, 5503, 4959, 13, 51316], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1267, "seek": 391490, "start": 3933.94, "end": 3936.06, "text": " And by this, I mean surface winds, temperature,", "tokens": [51316, 400, 538, 341, 11, 286, 914, 3753, 17765, 11, 4292, 11, 51422], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1268, "seek": 391490, "start": 3936.06, "end": 3937.9, "text": " and precipitation in the first stage,", "tokens": [51422, 293, 37662, 294, 264, 700, 3233, 11, 51514], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1269, "seek": 391490, "start": 3937.9, "end": 3941.26, "text": " a forecast from the GDPS, which is our global deterministic", "tokens": [51514, 257, 14330, 490, 264, 19599, 50, 11, 597, 307, 527, 4338, 15957, 3142, 51682], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1270, "seek": 391490, "start": 3941.26, "end": 3944.46, "text": " prediction system shown here.", "tokens": [51682, 17630, 1185, 4898, 510, 13, 51842], "temperature": 0.0, "avg_logprob": -0.14679505030314127, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0010270762722939253}, {"id": 1271, "seek": 394446, "start": 3944.5, "end": 3947.3, "text": " And runs at a resolution of 15 kilometers", "tokens": [50366, 400, 6676, 412, 257, 8669, 295, 2119, 13904, 50506], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1272, "seek": 394446, "start": 3947.3, "end": 3949.5, "text": " to the resolution of the HRDPS, which", "tokens": [50506, 281, 264, 8669, 295, 264, 19460, 35, 6273, 11, 597, 50616], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1273, "seek": 394446, "start": 3949.5, "end": 3951.42, "text": " is our high-resolution deterministic prediction", "tokens": [50616, 307, 527, 1090, 12, 495, 3386, 15957, 3142, 17630, 50712], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1274, "seek": 394446, "start": 3951.42, "end": 3955.94, "text": " system that is run for 48 hours for now at 2.5 kilometer", "tokens": [50712, 1185, 300, 307, 1190, 337, 11174, 2496, 337, 586, 412, 568, 13, 20, 33795, 50938], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1275, "seek": 394446, "start": 3955.94, "end": 3956.9, "text": " resolution.", "tokens": [50938, 8669, 13, 50986], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1276, "seek": 394446, "start": 3956.9, "end": 3959.34, "text": " And in this project, we are taking a two-step approach.", "tokens": [50986, 400, 294, 341, 1716, 11, 321, 366, 1940, 257, 732, 12, 16792, 3109, 13, 51108], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1277, "seek": 394446, "start": 3959.34, "end": 3962.54, "text": " So in the first step, we will be looking at the baseline model", "tokens": [51108, 407, 294, 264, 700, 1823, 11, 321, 486, 312, 1237, 412, 264, 20518, 2316, 51268], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1278, "seek": 394446, "start": 3962.54, "end": 3965.66, "text": " that is based on generative adversarial networks.", "tokens": [51268, 300, 307, 2361, 322, 1337, 1166, 17641, 44745, 9590, 13, 51424], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1279, "seek": 394446, "start": 3965.66, "end": 3968.02, "text": " And in the second stage of the project,", "tokens": [51424, 400, 294, 264, 1150, 3233, 295, 264, 1716, 11, 51542], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1280, "seek": 394446, "start": 3968.02, "end": 3972.94, "text": " we will be taking advantage of foundation models", "tokens": [51542, 321, 486, 312, 1940, 5002, 295, 7030, 5245, 51788], "temperature": 0.0, "avg_logprob": -0.14625500043233236, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.0023262174800038338}, {"id": 1281, "seek": 397294, "start": 3972.94, "end": 3974.94, "text": " and tuning them for our application.", "tokens": [50364, 293, 15164, 552, 337, 527, 3861, 13, 50464], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1282, "seek": 397294, "start": 3974.94, "end": 3976.66, "text": " The training data for the models will", "tokens": [50464, 440, 3097, 1412, 337, 264, 5245, 486, 50550], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1283, "seek": 397294, "start": 3976.66, "end": 3979.06, "text": " be forecasting data from the GDPS", "tokens": [50550, 312, 44331, 1412, 490, 264, 19599, 50, 50670], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1284, "seek": 397294, "start": 3979.06, "end": 3983.7000000000003, "text": " as the low-resolution data set and from the HRDPS", "tokens": [50670, 382, 264, 2295, 12, 495, 3386, 1412, 992, 293, 490, 264, 19460, 35, 6273, 50902], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1285, "seek": 397294, "start": 3983.7000000000003, "end": 3985.98, "text": " as the high-resolution data set.", "tokens": [50902, 382, 264, 1090, 12, 495, 3386, 1412, 992, 13, 51016], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1286, "seek": 397294, "start": 3985.98, "end": 3988.98, "text": " And it is very important for our operational needs", "tokens": [51016, 400, 309, 307, 588, 1021, 337, 527, 16607, 2203, 51166], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1287, "seek": 397294, "start": 3988.98, "end": 3992.58, "text": " that the downscale products are available on the HRDPS grid.", "tokens": [51166, 300, 264, 760, 20033, 3383, 366, 2435, 322, 264, 19460, 35, 6273, 10748, 13, 51346], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1288, "seek": 397294, "start": 3995.1, "end": 3997.82, "text": " For now in this talk, I will only focus on the baseline model", "tokens": [51472, 1171, 586, 294, 341, 751, 11, 286, 486, 787, 1879, 322, 264, 20518, 2316, 51608], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1289, "seek": 397294, "start": 3997.82, "end": 4000.2200000000003, "text": " as this is a collaboration that just started.", "tokens": [51608, 382, 341, 307, 257, 9363, 300, 445, 1409, 13, 51728], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1290, "seek": 397294, "start": 4000.2200000000003, "end": 4002.14, "text": " And we haven't gotten yet to the second part.", "tokens": [51728, 400, 321, 2378, 380, 5768, 1939, 281, 264, 1150, 644, 13, 51824], "temperature": 0.0, "avg_logprob": -0.13224187444468014, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.0026375947054475546}, {"id": 1291, "seek": 400294, "start": 4003.54, "end": 4007.42, "text": " So just briefly, what is a generative adversarial network?", "tokens": [50394, 407, 445, 10515, 11, 437, 307, 257, 1337, 1166, 17641, 44745, 3209, 30, 50588], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1292, "seek": 400294, "start": 4007.42, "end": 4010.86, "text": " And generative adversarial networks", "tokens": [50588, 400, 1337, 1166, 17641, 44745, 9590, 50760], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1293, "seek": 400294, "start": 4010.86, "end": 4013.42, "text": " consist of two networks, a generator, two neural networks,", "tokens": [50760, 4603, 295, 732, 9590, 11, 257, 19265, 11, 732, 18161, 9590, 11, 50888], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1294, "seek": 400294, "start": 4013.42, "end": 4014.62, "text": " a generator and discriminator.", "tokens": [50888, 257, 19265, 293, 20828, 1639, 13, 50948], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1295, "seek": 400294, "start": 4014.62, "end": 4015.98, "text": " And the way that they work is that they", "tokens": [50948, 400, 264, 636, 300, 436, 589, 307, 300, 436, 51016], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1296, "seek": 400294, "start": 4015.98, "end": 4018.18, "text": " are trained in a competing process.", "tokens": [51016, 366, 8895, 294, 257, 15439, 1399, 13, 51126], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1297, "seek": 400294, "start": 4018.18, "end": 4020.42, "text": " So the generator pretty much generates images", "tokens": [51126, 407, 264, 19265, 1238, 709, 23815, 5267, 51238], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1298, "seek": 400294, "start": 4020.42, "end": 4023.66, "text": " that look as much as possible as the real data.", "tokens": [51238, 300, 574, 382, 709, 382, 1944, 382, 264, 957, 1412, 13, 51400], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1299, "seek": 400294, "start": 4023.66, "end": 4025.82, "text": " And the goal of the generator is to just", "tokens": [51400, 400, 264, 3387, 295, 264, 19265, 307, 281, 445, 51508], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1300, "seek": 400294, "start": 4025.82, "end": 4027.26, "text": " fold the discriminator.", "tokens": [51508, 4860, 264, 20828, 1639, 13, 51580], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1301, "seek": 400294, "start": 4027.26, "end": 4029.06, "text": " On the other hand, we have the discriminator", "tokens": [51580, 1282, 264, 661, 1011, 11, 321, 362, 264, 20828, 1639, 51670], "temperature": 0.0, "avg_logprob": -0.16257827822901622, "compression_ratio": 2.0262008733624453, "no_speech_prob": 0.0013507392723113298}, {"id": 1302, "seek": 402906, "start": 4029.06, "end": 4032.34, "text": " that receives both real data, which in our case is HRDPS", "tokens": [50364, 300, 20717, 1293, 957, 1412, 11, 597, 294, 527, 1389, 307, 19460, 35, 6273, 50528], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1303, "seek": 402906, "start": 4032.34, "end": 4035.22, "text": " data, and generated data, and has to decide", "tokens": [50528, 1412, 11, 293, 10833, 1412, 11, 293, 575, 281, 4536, 50672], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1304, "seek": 402906, "start": 4035.22, "end": 4038.34, "text": " whether this generated data is real or fake.", "tokens": [50672, 1968, 341, 10833, 1412, 307, 957, 420, 7592, 13, 50828], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1305, "seek": 402906, "start": 4038.34, "end": 4043.58, "text": " And in our case, we use the ANAU et al. 2023 implementation", "tokens": [50828, 400, 294, 527, 1389, 11, 321, 764, 264, 5252, 2340, 1030, 419, 13, 44377, 11420, 51090], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1306, "seek": 402906, "start": 4043.58, "end": 4044.74, "text": " of a Vassar SteamGAN.", "tokens": [51090, 295, 257, 691, 640, 289, 22517, 27699, 13, 51148], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1307, "seek": 402906, "start": 4047.74, "end": 4052.22, "text": " I will show you some preliminary results for our project.", "tokens": [51298, 286, 486, 855, 291, 512, 28817, 3542, 337, 527, 1716, 13, 51522], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1308, "seek": 402906, "start": 4052.22, "end": 4056.1, "text": " So these preliminary results are based on the WGAN from ANAU", "tokens": [51522, 407, 613, 28817, 3542, 366, 2361, 322, 264, 343, 27699, 490, 5252, 2340, 51716], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1309, "seek": 402906, "start": 4056.1, "end": 4056.58, "text": " et al.", "tokens": [51716, 1030, 419, 13, 51740], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1310, "seek": 402906, "start": 4056.58, "end": 4057.74, "text": " Without any covariates.", "tokens": [51740, 9129, 604, 49851, 1024, 13, 51798], "temperature": 0.0, "avg_logprob": -0.20829364988538954, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0024188929237425327}, {"id": 1311, "seek": 405774, "start": 4057.74, "end": 4060.74, "text": " So what this means is that the only data that goes for now", "tokens": [50364, 407, 437, 341, 1355, 307, 300, 264, 787, 1412, 300, 1709, 337, 586, 50514], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1312, "seek": 405774, "start": 4060.74, "end": 4065.02, "text": " in this model is zonal and meridional 10-meter wing", "tokens": [50514, 294, 341, 2316, 307, 710, 21523, 293, 3551, 327, 1966, 1266, 12, 33058, 11162, 50728], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1313, "seek": 405774, "start": 4065.02, "end": 4065.7, "text": " components.", "tokens": [50728, 6677, 13, 50762], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1314, "seek": 405774, "start": 4065.7, "end": 4068.9399999999996, "text": " And this is the data that we are trying to obtain.", "tokens": [50762, 400, 341, 307, 264, 1412, 300, 321, 366, 1382, 281, 12701, 13, 50924], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1315, "seek": 405774, "start": 4068.9399999999996, "end": 4071.9799999999996, "text": " So the low-resolution data comes from GDPS.", "tokens": [50924, 407, 264, 2295, 12, 495, 3386, 1412, 1487, 490, 19599, 50, 13, 51076], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1316, "seek": 405774, "start": 4071.9799999999996, "end": 4074.58, "text": " High resolution comes from the HRDPS.", "tokens": [51076, 5229, 8669, 1487, 490, 264, 19460, 35, 6273, 13, 51206], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1317, "seek": 405774, "start": 4074.58, "end": 4077.2999999999997, "text": " And so far, we are training the model with one year of data", "tokens": [51206, 400, 370, 1400, 11, 321, 366, 3097, 264, 2316, 365, 472, 1064, 295, 1412, 51342], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1318, "seek": 405774, "start": 4077.2999999999997, "end": 4078.9799999999996, "text": " that is divided as follows.", "tokens": [51342, 300, 307, 6666, 382, 10002, 13, 51426], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1319, "seek": 405774, "start": 4078.9799999999996, "end": 4081.4199999999996, "text": " 75% of the data is used for training.", "tokens": [51426, 9562, 4, 295, 264, 1412, 307, 1143, 337, 3097, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1320, "seek": 405774, "start": 4081.4199999999996, "end": 4083.22, "text": " It's about 7,000 forecasts.", "tokens": [51548, 467, 311, 466, 1614, 11, 1360, 49421, 13, 51638], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1321, "seek": 405774, "start": 4083.22, "end": 4085.8999999999996, "text": " 12.5% is used for validation.", "tokens": [51638, 2272, 13, 20, 4, 307, 1143, 337, 24071, 13, 51772], "temperature": 0.0, "avg_logprob": -0.13739686263234993, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.00290921819396317}, {"id": 1322, "seek": 408590, "start": 4085.9, "end": 4088.5, "text": " And this validation data is used during the training", "tokens": [50364, 400, 341, 24071, 1412, 307, 1143, 1830, 264, 3097, 50494], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1323, "seek": 408590, "start": 4088.5, "end": 4089.1800000000003, "text": " of the model.", "tokens": [50494, 295, 264, 2316, 13, 50528], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1324, "seek": 408590, "start": 4089.1800000000003, "end": 4092.46, "text": " So it's used to stop overfitting the model.", "tokens": [50528, 407, 309, 311, 1143, 281, 1590, 670, 69, 2414, 264, 2316, 13, 50692], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1325, "seek": 408590, "start": 4092.46, "end": 4095.3, "text": " And then 12.5% of the data is used for testing.", "tokens": [50692, 400, 550, 2272, 13, 20, 4, 295, 264, 1412, 307, 1143, 337, 4997, 13, 50834], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1326, "seek": 408590, "start": 4095.3, "end": 4097.58, "text": " So once we have a tuned model, we", "tokens": [50834, 407, 1564, 321, 362, 257, 10870, 2316, 11, 321, 50948], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1327, "seek": 408590, "start": 4097.58, "end": 4100.38, "text": " will use this data set, about 1,200 forecasts,", "tokens": [50948, 486, 764, 341, 1412, 992, 11, 466, 502, 11, 7629, 49421, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1328, "seek": 408590, "start": 4100.38, "end": 4104.06, "text": " to be seeing how well this model functions.", "tokens": [51088, 281, 312, 2577, 577, 731, 341, 2316, 6828, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1329, "seek": 408590, "start": 4104.06, "end": 4105.7, "text": " Because we are using forecasting data,", "tokens": [51272, 1436, 321, 366, 1228, 44331, 1412, 11, 51354], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1330, "seek": 408590, "start": 4105.7, "end": 4106.7, "text": " it is a bit difficult.", "tokens": [51354, 309, 307, 257, 857, 2252, 13, 51404], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1331, "seek": 408590, "start": 4106.7, "end": 4108.42, "text": " Because as you know, forecasts, there", "tokens": [51404, 1436, 382, 291, 458, 11, 49421, 11, 456, 51490], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1332, "seek": 408590, "start": 4108.42, "end": 4111.14, "text": " are increases with forecast lead time.", "tokens": [51490, 366, 8637, 365, 14330, 1477, 565, 13, 51626], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1333, "seek": 408590, "start": 4111.14, "end": 4113.18, "text": " And so we might have too much divergence", "tokens": [51626, 400, 370, 321, 1062, 362, 886, 709, 47387, 51728], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1334, "seek": 408590, "start": 4113.18, "end": 4115.7, "text": " between the HRDPS, the high-resolution data set,", "tokens": [51728, 1296, 264, 19460, 35, 6273, 11, 264, 1090, 12, 495, 3386, 1412, 992, 11, 51854], "temperature": 0.0, "avg_logprob": -0.12951883329015199, "compression_ratio": 1.7594501718213058, "no_speech_prob": 0.0003039721050299704}, {"id": 1335, "seek": 411570, "start": 4115.7, "end": 4118.7, "text": " and our low-resolution data set.", "tokens": [50364, 293, 527, 2295, 12, 495, 3386, 1412, 992, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1336, "seek": 411570, "start": 4118.7, "end": 4120.66, "text": " On the other hand, the first six hours of the forecasts", "tokens": [50514, 1282, 264, 661, 1011, 11, 264, 700, 2309, 2496, 295, 264, 49421, 50612], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1337, "seek": 411570, "start": 4120.66, "end": 4122.66, "text": " are affected by the spin-up time of the model.", "tokens": [50612, 366, 8028, 538, 264, 6060, 12, 1010, 565, 295, 264, 2316, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1338, "seek": 411570, "start": 4122.66, "end": 4126.9, "text": " So we have decided for now to go with forecasts our 6 to 18.", "tokens": [50712, 407, 321, 362, 3047, 337, 586, 281, 352, 365, 49421, 527, 1386, 281, 2443, 13, 50924], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1339, "seek": 411570, "start": 4126.9, "end": 4130.3, "text": " And we are using them from the 0,0 and the 12 UTC", "tokens": [50924, 400, 321, 366, 1228, 552, 490, 264, 1958, 11, 15, 293, 264, 2272, 624, 18238, 51094], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1340, "seek": 411570, "start": 4130.3, "end": 4132.0599999999995, "text": " initialization of both models.", "tokens": [51094, 5883, 2144, 295, 1293, 5245, 13, 51182], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1341, "seek": 411570, "start": 4132.0599999999995, "end": 4137.0599999999995, "text": " So in this way, we are covering the entire day.", "tokens": [51182, 407, 294, 341, 636, 11, 321, 366, 10322, 264, 2302, 786, 13, 51432], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1342, "seek": 411570, "start": 4137.0599999999995, "end": 4141.78, "text": " So our challenge is covering the entire HRDPS domain.", "tokens": [51432, 407, 527, 3430, 307, 10322, 264, 2302, 19460, 35, 6273, 9274, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1343, "seek": 411570, "start": 4141.78, "end": 4143.98, "text": " I am showing here the HRDPS domain.", "tokens": [51668, 286, 669, 4099, 510, 264, 19460, 35, 6273, 9274, 13, 51778], "temperature": 0.0, "avg_logprob": -0.1478293969379208, "compression_ratio": 1.640316205533597, "no_speech_prob": 0.0009559139725752175}, {"id": 1344, "seek": 414398, "start": 4143.98, "end": 4147.58, "text": " It is on the order of 2,500 by 1,200 grid points.", "tokens": [50364, 467, 307, 322, 264, 1668, 295, 568, 11, 7526, 538, 502, 11, 7629, 10748, 2793, 13, 50544], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1345, "seek": 414398, "start": 4147.58, "end": 4149.66, "text": " So it's a very large domain that's", "tokens": [50544, 407, 309, 311, 257, 588, 2416, 9274, 300, 311, 50648], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1346, "seek": 414398, "start": 4149.66, "end": 4151.74, "text": " spanning the width of Canada.", "tokens": [50648, 47626, 264, 11402, 295, 6309, 13, 50752], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1347, "seek": 414398, "start": 4151.74, "end": 4155.179999999999, "text": " And it isn't really possible, or at least we don't think", "tokens": [50752, 400, 309, 1943, 380, 534, 1944, 11, 420, 412, 1935, 321, 500, 380, 519, 50924], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1348, "seek": 414398, "start": 4155.179999999999, "end": 4157.459999999999, "text": " it's possible so far, to be training directly", "tokens": [50924, 309, 311, 1944, 370, 1400, 11, 281, 312, 3097, 3838, 51038], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1349, "seek": 414398, "start": 4157.459999999999, "end": 4159.74, "text": " on such a large domain.", "tokens": [51038, 322, 1270, 257, 2416, 9274, 13, 51152], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1350, "seek": 414398, "start": 4159.74, "end": 4162.179999999999, "text": " We do not have the computational resources to do that.", "tokens": [51152, 492, 360, 406, 362, 264, 28270, 3593, 281, 360, 300, 13, 51274], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1351, "seek": 414398, "start": 4162.179999999999, "end": 4164.62, "text": " And we also aren't sure exactly how much data", "tokens": [51274, 400, 321, 611, 3212, 380, 988, 2293, 577, 709, 1412, 51396], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1352, "seek": 414398, "start": 4164.62, "end": 4167.299999999999, "text": " you would need to be able to train on such a model.", "tokens": [51396, 291, 576, 643, 281, 312, 1075, 281, 3847, 322, 1270, 257, 2316, 13, 51530], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1353, "seek": 414398, "start": 4167.299999999999, "end": 4171.339999999999, "text": " The now and all implementation, in that implementation,", "tokens": [51530, 440, 586, 293, 439, 11420, 11, 294, 300, 11420, 11, 51732], "temperature": 0.0, "avg_logprob": -0.13870261785552257, "compression_ratio": 1.6791044776119404, "no_speech_prob": 0.0026887001004070044}, {"id": 1354, "seek": 417134, "start": 4171.34, "end": 4176.14, "text": " the training is done on 16 by 16 pixel low resolution", "tokens": [50364, 264, 3097, 307, 1096, 322, 3165, 538, 3165, 19261, 2295, 8669, 50604], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1355, "seek": 417134, "start": 4176.14, "end": 4181.38, "text": " patches and 128 by 128 pixel high resolution patches.", "tokens": [50604, 26531, 293, 29810, 538, 29810, 19261, 1090, 8669, 26531, 13, 50866], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1356, "seek": 417134, "start": 4181.38, "end": 4185.42, "text": " So what we will do is we will adjust to that type of training.", "tokens": [50866, 407, 437, 321, 486, 360, 307, 321, 486, 4369, 281, 300, 2010, 295, 3097, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1357, "seek": 417134, "start": 4185.42, "end": 4188.78, "text": " And in order to do that, the strategy that we have adapted", "tokens": [51068, 400, 294, 1668, 281, 360, 300, 11, 264, 5206, 300, 321, 362, 20871, 51236], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1358, "seek": 417134, "start": 4188.78, "end": 4193.42, "text": " is to just select random patches from one forecast", "tokens": [51236, 307, 281, 445, 3048, 4974, 26531, 490, 472, 14330, 51468], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1359, "seek": 417134, "start": 4193.42, "end": 4195.62, "text": " from our HRDPS data set.", "tokens": [51468, 490, 527, 19460, 35, 6273, 1412, 992, 13, 51578], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1360, "seek": 417134, "start": 4195.62, "end": 4200.54, "text": " We will re-read the GDPS data set on the HRDPS domain", "tokens": [51578, 492, 486, 319, 12, 2538, 264, 19599, 50, 1412, 992, 322, 264, 19460, 35, 6273, 9274, 51824], "temperature": 0.0, "avg_logprob": -0.1357840129307338, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.01496870443224907}, {"id": 1361, "seek": 420054, "start": 4200.54, "end": 4204.3, "text": " and then course-crain it to go back to its resolution.", "tokens": [50364, 293, 550, 1164, 12, 66, 7146, 309, 281, 352, 646, 281, 1080, 8669, 13, 50552], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1362, "seek": 420054, "start": 4204.3, "end": 4206.34, "text": " And in this way, we are going to end up", "tokens": [50552, 400, 294, 341, 636, 11, 321, 366, 516, 281, 917, 493, 50654], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1363, "seek": 420054, "start": 4206.34, "end": 4207.9, "text": " with this type of patches that we'll", "tokens": [50654, 365, 341, 2010, 295, 26531, 300, 321, 603, 50732], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1364, "seek": 420054, "start": 4207.9, "end": 4210.38, "text": " use to do the training.", "tokens": [50732, 764, 281, 360, 264, 3097, 13, 50856], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1365, "seek": 420054, "start": 4210.38, "end": 4212.66, "text": " So here is the high resolution data patch", "tokens": [50856, 407, 510, 307, 264, 1090, 8669, 1412, 9972, 50970], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1366, "seek": 420054, "start": 4212.66, "end": 4216.18, "text": " and the corresponding GDPS low resolution data patch.", "tokens": [50970, 293, 264, 11760, 19599, 50, 2295, 8669, 1412, 9972, 13, 51146], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1367, "seek": 420054, "start": 4216.18, "end": 4220.74, "text": " So at each epoch, we are using between 300 and 700 random", "tokens": [51146, 407, 412, 1184, 30992, 339, 11, 321, 366, 1228, 1296, 6641, 293, 15204, 4974, 51374], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1368, "seek": 420054, "start": 4220.74, "end": 4221.58, "text": " patches.", "tokens": [51374, 26531, 13, 51416], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1369, "seek": 420054, "start": 4221.58, "end": 4224.0199999999995, "text": " And the model that we are showing today", "tokens": [51416, 400, 264, 2316, 300, 321, 366, 4099, 965, 51538], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1370, "seek": 420054, "start": 4224.0199999999995, "end": 4227.86, "text": " has been trained on 17,370 epochs.", "tokens": [51538, 575, 668, 8895, 322, 3282, 11, 18, 5867, 30992, 28346, 13, 51730], "temperature": 0.0, "avg_logprob": -0.14735424309446102, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001288986997678876}, {"id": 1371, "seek": 422786, "start": 4227.86, "end": 4232.259999999999, "text": " So this took about 149 hours to train on one GPU.", "tokens": [50364, 407, 341, 1890, 466, 3499, 24, 2496, 281, 3847, 322, 472, 18407, 13, 50584], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1372, "seek": 422786, "start": 4232.259999999999, "end": 4234.099999999999, "text": " And we have done more than two passes", "tokens": [50584, 400, 321, 362, 1096, 544, 813, 732, 11335, 50676], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1373, "seek": 422786, "start": 4234.099999999999, "end": 4237.299999999999, "text": " through the entire data set.", "tokens": [50676, 807, 264, 2302, 1412, 992, 13, 50836], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1374, "seek": 422786, "start": 4237.299999999999, "end": 4239.219999999999, "text": " So once you have a trained model,", "tokens": [50836, 407, 1564, 291, 362, 257, 8895, 2316, 11, 50932], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1375, "seek": 422786, "start": 4239.219999999999, "end": 4241.7, "text": " you can perform inference with the GDPS data.", "tokens": [50932, 291, 393, 2042, 38253, 365, 264, 19599, 50, 1412, 13, 51056], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1376, "seek": 422786, "start": 4241.7, "end": 4246.179999999999, "text": " And that inference will also be done on 16 by 16 pixel patches.", "tokens": [51056, 400, 300, 38253, 486, 611, 312, 1096, 322, 3165, 538, 3165, 19261, 26531, 13, 51280], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1377, "seek": 422786, "start": 4246.179999999999, "end": 4249.0599999999995, "text": " So here I have a GDPS input.", "tokens": [51280, 407, 510, 286, 362, 257, 19599, 50, 4846, 13, 51424], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1378, "seek": 422786, "start": 4249.0599999999995, "end": 4250.78, "text": " We perform inference.", "tokens": [51424, 492, 2042, 38253, 13, 51510], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1379, "seek": 422786, "start": 4250.78, "end": 4253.099999999999, "text": " And like this, we obtain the downscale forecast,", "tokens": [51510, 400, 411, 341, 11, 321, 12701, 264, 760, 20033, 14330, 11, 51626], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1380, "seek": 422786, "start": 4253.099999999999, "end": 4255.259999999999, "text": " the downscale U and V fields.", "tokens": [51626, 264, 760, 20033, 624, 293, 691, 7909, 13, 51734], "temperature": 0.0, "avg_logprob": -0.13000175172248774, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0005487472517415881}, {"id": 1381, "seek": 425526, "start": 4255.26, "end": 4259.5, "text": " And just as a comparison, we have here the HRDPS forecast.", "tokens": [50364, 400, 445, 382, 257, 9660, 11, 321, 362, 510, 264, 19460, 35, 6273, 14330, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1382, "seek": 425526, "start": 4259.5, "end": 4262.22, "text": " So the first things that we can say", "tokens": [50576, 407, 264, 700, 721, 300, 321, 393, 584, 50712], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1383, "seek": 425526, "start": 4262.22, "end": 4264.5, "text": " is that we are definitely downscaling.", "tokens": [50712, 307, 300, 321, 366, 2138, 760, 4417, 4270, 13, 50826], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1384, "seek": 425526, "start": 4264.5, "end": 4268.66, "text": " So we are obtaining information at a small scale.", "tokens": [50826, 407, 321, 366, 36749, 1589, 412, 257, 1359, 4373, 13, 51034], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1385, "seek": 425526, "start": 4268.66, "end": 4272.06, "text": " It isn't as much as the HRDPS is showing.", "tokens": [51034, 467, 1943, 380, 382, 709, 382, 264, 19460, 35, 6273, 307, 4099, 13, 51204], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1386, "seek": 425526, "start": 4272.06, "end": 4274.02, "text": " But as I was mentioning before, one problem", "tokens": [51204, 583, 382, 286, 390, 18315, 949, 11, 472, 1154, 51302], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1387, "seek": 425526, "start": 4274.02, "end": 4276.42, "text": " with a low resolution is the fact that you have biases.", "tokens": [51302, 365, 257, 2295, 8669, 307, 264, 1186, 300, 291, 362, 32152, 13, 51422], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1388, "seek": 425526, "start": 4276.42, "end": 4278.62, "text": " And we are definitely achieving some sort", "tokens": [51422, 400, 321, 366, 2138, 19626, 512, 1333, 51532], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1389, "seek": 425526, "start": 4278.62, "end": 4281.58, "text": " of a bias correction.", "tokens": [51532, 295, 257, 12577, 19984, 13, 51680], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1390, "seek": 425526, "start": 4281.58, "end": 4285.06, "text": " Now, of course, you have to parse the entire HRDPS domain.", "tokens": [51680, 823, 11, 295, 1164, 11, 291, 362, 281, 48377, 264, 2302, 19460, 35, 6273, 9274, 13, 51854], "temperature": 0.0, "avg_logprob": -0.11531508158123682, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.000898301659617573}, {"id": 1391, "seek": 428506, "start": 4285.06, "end": 4287.5, "text": " One way to do that would be to sequentially process", "tokens": [50364, 1485, 636, 281, 360, 300, 576, 312, 281, 5123, 3137, 1399, 50486], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1392, "seek": 428506, "start": 4287.5, "end": 4289.820000000001, "text": " 128 by 128 patches.", "tokens": [50486, 29810, 538, 29810, 26531, 13, 50602], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1393, "seek": 428506, "start": 4289.820000000001, "end": 4292.860000000001, "text": " But that would result into artifacts at the borders.", "tokens": [50602, 583, 300, 576, 1874, 666, 24617, 412, 264, 16287, 13, 50754], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1394, "seek": 428506, "start": 4292.860000000001, "end": 4295.26, "text": " So the strategy that we have adopted instead", "tokens": [50754, 407, 264, 5206, 300, 321, 362, 12175, 2602, 50874], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1395, "seek": 428506, "start": 4295.26, "end": 4297.3, "text": " is to do some overlapping and then", "tokens": [50874, 307, 281, 360, 512, 33535, 293, 550, 50976], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1396, "seek": 428506, "start": 4297.3, "end": 4301.5, "text": " to take the median of the ensemble of overlaps.", "tokens": [50976, 281, 747, 264, 26779, 295, 264, 19492, 295, 15986, 2382, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1397, "seek": 428506, "start": 4301.5, "end": 4304.26, "text": " And in this way, we managed to patch and obtain this figure", "tokens": [51186, 400, 294, 341, 636, 11, 321, 6453, 281, 9972, 293, 12701, 341, 2573, 51324], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1398, "seek": 428506, "start": 4304.26, "end": 4306.580000000001, "text": " over the entire domain.", "tokens": [51324, 670, 264, 2302, 9274, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1399, "seek": 428506, "start": 4306.580000000001, "end": 4309.34, "text": " We have performed validation of our model.", "tokens": [51440, 492, 362, 10332, 24071, 295, 527, 2316, 13, 51578], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1400, "seek": 428506, "start": 4309.34, "end": 4314.620000000001, "text": " So we have used the test data, the 1,200 forecast from the GDPS.", "tokens": [51578, 407, 321, 362, 1143, 264, 1500, 1412, 11, 264, 502, 11, 7629, 14330, 490, 264, 19599, 50, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12222466226351464, "compression_ratio": 1.6629213483146068, "no_speech_prob": 0.00040710109169594944}, {"id": 1401, "seek": 431462, "start": 4314.62, "end": 4317.54, "text": " And here I am showing the root mean square error", "tokens": [50364, 400, 510, 286, 669, 4099, 264, 5593, 914, 3732, 6713, 50510], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1402, "seek": 431462, "start": 4317.54, "end": 4320.94, "text": " for the U-wind component and for the V-wind component.", "tokens": [50510, 337, 264, 624, 12, 12199, 6542, 293, 337, 264, 691, 12, 12199, 6542, 13, 50680], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1403, "seek": 431462, "start": 4320.94, "end": 4324.0199999999995, "text": " And on the bottom, I'm showing the mean absolute error.", "tokens": [50680, 400, 322, 264, 2767, 11, 286, 478, 4099, 264, 914, 8236, 6713, 13, 50834], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1404, "seek": 431462, "start": 4324.0199999999995, "end": 4326.14, "text": " And these are the metrics that are computed", "tokens": [50834, 400, 613, 366, 264, 16367, 300, 366, 40610, 50940], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1405, "seek": 431462, "start": 4326.14, "end": 4332.42, "text": " between the downscale GDPS and the HRDPS corresponding", "tokens": [50940, 1296, 264, 760, 20033, 19599, 50, 293, 264, 19460, 35, 6273, 11760, 51254], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1406, "seek": 431462, "start": 4332.42, "end": 4333.66, "text": " verification.", "tokens": [51254, 30206, 13, 51316], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1407, "seek": 431462, "start": 4333.66, "end": 4335.74, "text": " And we are comparing it with some baselines that", "tokens": [51316, 400, 321, 366, 15763, 309, 365, 512, 987, 9173, 300, 51420], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1408, "seek": 431462, "start": 4335.74, "end": 4337.26, "text": " can be used for interpolation.", "tokens": [51420, 393, 312, 1143, 337, 44902, 399, 13, 51496], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1409, "seek": 431462, "start": 4337.26, "end": 4339.74, "text": " So I'm showing in orange you have bilinear interpolation", "tokens": [51496, 407, 286, 478, 4099, 294, 7671, 291, 362, 8588, 533, 289, 44902, 399, 51620], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1410, "seek": 431462, "start": 4339.74, "end": 4342.7, "text": " and in green you have nearest neighbor interpolation.", "tokens": [51620, 293, 294, 3092, 291, 362, 23831, 5987, 44902, 399, 13, 51768], "temperature": 0.0, "avg_logprob": -0.15252223684768046, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.00132645177654922}, {"id": 1411, "seek": 434270, "start": 4342.7, "end": 4344.62, "text": " So as we are seeing in all the metrics,", "tokens": [50364, 407, 382, 321, 366, 2577, 294, 439, 264, 16367, 11, 50460], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1412, "seek": 434270, "start": 4344.62, "end": 4347.34, "text": " the downscale is showing better results,", "tokens": [50460, 264, 760, 20033, 307, 4099, 1101, 3542, 11, 50596], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1413, "seek": 434270, "start": 4347.34, "end": 4350.0199999999995, "text": " is performing better than the other types of interpolation", "tokens": [50596, 307, 10205, 1101, 813, 264, 661, 3467, 295, 44902, 399, 50730], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1414, "seek": 434270, "start": 4350.0199999999995, "end": 4352.82, "text": " for our test data set.", "tokens": [50730, 337, 527, 1500, 1412, 992, 13, 50870], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1415, "seek": 434270, "start": 4352.82, "end": 4354.9, "text": " We have also done a power spectrum analysis", "tokens": [50870, 492, 362, 611, 1096, 257, 1347, 11143, 5215, 50974], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1416, "seek": 434270, "start": 4354.9, "end": 4358.58, "text": " in order to really quantify how much detail we", "tokens": [50974, 294, 1668, 281, 534, 40421, 577, 709, 2607, 321, 51158], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1417, "seek": 434270, "start": 4358.58, "end": 4360.099999999999, "text": " are getting at the small scales.", "tokens": [51158, 366, 1242, 412, 264, 1359, 17408, 13, 51234], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1418, "seek": 434270, "start": 4360.099999999999, "end": 4362.0199999999995, "text": " And here I'm showing the radially average power", "tokens": [51234, 400, 510, 286, 478, 4099, 264, 2843, 2270, 4274, 1347, 51330], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1419, "seek": 434270, "start": 4362.0199999999995, "end": 4365.78, "text": " spectral density between HRDPS in blue and the downscale", "tokens": [51330, 42761, 10305, 1296, 19460, 35, 6273, 294, 3344, 293, 264, 760, 20033, 51518], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1420, "seek": 434270, "start": 4365.78, "end": 4368.179999999999, "text": " forecast in orange.", "tokens": [51518, 14330, 294, 7671, 13, 51638], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1421, "seek": 434270, "start": 4368.179999999999, "end": 4371.46, "text": " And these power spectra are average over the test data", "tokens": [51638, 400, 613, 1347, 6177, 424, 366, 4274, 670, 264, 1500, 1412, 51802], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1422, "seek": 434270, "start": 4371.46, "end": 4372.58, "text": " set as well.", "tokens": [51802, 992, 382, 731, 13, 51858], "temperature": 0.0, "avg_logprob": -0.14085910034179688, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00029041129164397717}, {"id": 1423, "seek": 437258, "start": 4372.62, "end": 4375.1, "text": " What we are seeing is that indeed at small scales,", "tokens": [50366, 708, 321, 366, 2577, 307, 300, 6451, 412, 1359, 17408, 11, 50490], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1424, "seek": 437258, "start": 4375.1, "end": 4377.7, "text": " we are still not getting enough power.", "tokens": [50490, 321, 366, 920, 406, 1242, 1547, 1347, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1425, "seek": 437258, "start": 4377.7, "end": 4380.0199999999995, "text": " So we do not get enough detail.", "tokens": [50620, 407, 321, 360, 406, 483, 1547, 2607, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1426, "seek": 437258, "start": 4380.0199999999995, "end": 4384.9, "text": " Madelina, if you could lend this in one or two minutes.", "tokens": [50736, 5326, 338, 1426, 11, 498, 291, 727, 21774, 341, 294, 472, 420, 732, 2077, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1427, "seek": 437258, "start": 4384.9, "end": 4386.0199999999995, "text": " Sounds good.", "tokens": [50980, 14576, 665, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1428, "seek": 437258, "start": 4386.0199999999995, "end": 4389.7, "text": " So of course, we are training so far with one year of data.", "tokens": [51036, 407, 295, 1164, 11, 321, 366, 3097, 370, 1400, 365, 472, 1064, 295, 1412, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1429, "seek": 437258, "start": 4389.7, "end": 4392.54, "text": " And it would be very interesting to see", "tokens": [51220, 400, 309, 576, 312, 588, 1880, 281, 536, 51362], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1430, "seek": 437258, "start": 4392.54, "end": 4396.26, "text": " how much more detail we can obtain by training further.", "tokens": [51362, 577, 709, 544, 2607, 321, 393, 12701, 538, 3097, 3052, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1431, "seek": 437258, "start": 4396.26, "end": 4398.46, "text": " I'm just showing also an integrated measure", "tokens": [51548, 286, 478, 445, 4099, 611, 364, 10919, 3481, 51658], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1432, "seek": 437258, "start": 4398.46, "end": 4400.0199999999995, "text": " of the difference in the power spectra.", "tokens": [51658, 295, 264, 2649, 294, 264, 1347, 6177, 424, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1433, "seek": 437258, "start": 4400.0199999999995, "end": 4402.22, "text": " And what you are seeing here is that compared", "tokens": [51736, 400, 437, 291, 366, 2577, 510, 307, 300, 5347, 51846], "temperature": 0.0, "avg_logprob": -0.1633809998978016, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.006562232039868832}, {"id": 1434, "seek": 440222, "start": 4402.22, "end": 4404.860000000001, "text": " to the bilinear and the nearest neighbor interpolation,", "tokens": [50364, 281, 264, 8588, 533, 289, 293, 264, 23831, 5987, 44902, 399, 11, 50496], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1435, "seek": 440222, "start": 4404.860000000001, "end": 4409.66, "text": " the downscaled AI downscaling performance much, much better.", "tokens": [50496, 264, 760, 4417, 5573, 7318, 760, 4417, 4270, 3389, 709, 11, 709, 1101, 13, 50736], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1436, "seek": 440222, "start": 4409.66, "end": 4411.780000000001, "text": " So it's able to recover way more structure", "tokens": [50736, 407, 309, 311, 1075, 281, 8114, 636, 544, 3877, 50842], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1437, "seek": 440222, "start": 4411.780000000001, "end": 4414.66, "text": " at the small scales.", "tokens": [50842, 412, 264, 1359, 17408, 13, 50986], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1438, "seek": 440222, "start": 4414.66, "end": 4418.34, "text": " So the next steps with our forecast, with our project,", "tokens": [50986, 407, 264, 958, 4439, 365, 527, 14330, 11, 365, 527, 1716, 11, 51170], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1439, "seek": 440222, "start": 4418.34, "end": 4420.900000000001, "text": " the WGAN needs further development and testing.", "tokens": [51170, 264, 343, 27699, 2203, 3052, 3250, 293, 4997, 13, 51298], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1440, "seek": 440222, "start": 4420.900000000001, "end": 4423.58, "text": " So we are planning on testing with more data.", "tokens": [51298, 407, 321, 366, 5038, 322, 4997, 365, 544, 1412, 13, 51432], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1441, "seek": 440222, "start": 4423.58, "end": 4425.820000000001, "text": " A very important thing that we are working on right now", "tokens": [51432, 316, 588, 1021, 551, 300, 321, 366, 1364, 322, 558, 586, 51544], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1442, "seek": 440222, "start": 4425.820000000001, "end": 4427.7, "text": " is to add other covariates.", "tokens": [51544, 307, 281, 909, 661, 49851, 1024, 13, 51638], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1443, "seek": 440222, "start": 4427.7, "end": 4430.34, "text": " So as I said, for now, we are only using windfields.", "tokens": [51638, 407, 382, 286, 848, 11, 337, 586, 11, 321, 366, 787, 1228, 2468, 7610, 82, 13, 51770], "temperature": 0.0, "avg_logprob": -0.15162268210583785, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0007878885953687131}, {"id": 1444, "seek": 443034, "start": 4430.38, "end": 4432.7, "text": " But we are adding topography, surface pressure,", "tokens": [50366, 583, 321, 366, 5127, 1192, 5820, 11, 3753, 3321, 11, 50482], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1445, "seek": 443034, "start": 4432.7, "end": 4434.66, "text": " and we are thinking about what other covariates", "tokens": [50482, 293, 321, 366, 1953, 466, 437, 661, 49851, 1024, 50580], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1446, "seek": 443034, "start": 4434.66, "end": 4437.1, "text": " may be such escape to add to our model.", "tokens": [50580, 815, 312, 1270, 7615, 281, 909, 281, 527, 2316, 13, 50702], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1447, "seek": 443034, "start": 4437.1, "end": 4440.46, "text": " And once we have a baseline that we are satisfied with,", "tokens": [50702, 400, 1564, 321, 362, 257, 20518, 300, 321, 366, 11239, 365, 11, 50870], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1448, "seek": 443034, "start": 4440.46, "end": 4441.9800000000005, "text": " the important part comes.", "tokens": [50870, 264, 1021, 644, 1487, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1449, "seek": 443034, "start": 4441.9800000000005, "end": 4444.3, "text": " And that is doing a thorough meteorological verification", "tokens": [50946, 400, 300, 307, 884, 257, 12934, 25313, 4383, 30206, 51062], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1450, "seek": 443034, "start": 4444.3, "end": 4445.3, "text": " of the downscale forecast.", "tokens": [51062, 295, 264, 760, 20033, 14330, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1451, "seek": 443034, "start": 4445.3, "end": 4447.66, "text": " Because like I said, we have operational goals.", "tokens": [51112, 1436, 411, 286, 848, 11, 321, 362, 16607, 5493, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1452, "seek": 443034, "start": 4447.66, "end": 4450.38, "text": " And we really want to see how this forecast", "tokens": [51230, 400, 321, 534, 528, 281, 536, 577, 341, 14330, 51366], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1453, "seek": 443034, "start": 4450.38, "end": 4452.34, "text": " respond to our needs.", "tokens": [51366, 4196, 281, 527, 2203, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1454, "seek": 443034, "start": 4452.34, "end": 4454.82, "text": " Finally, once we finish this first step of the project,", "tokens": [51464, 6288, 11, 1564, 321, 2413, 341, 700, 1823, 295, 264, 1716, 11, 51588], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1455, "seek": 443034, "start": 4454.82, "end": 4457.82, "text": " we are planning on moving to the second more ambitious part,", "tokens": [51588, 321, 366, 5038, 322, 2684, 281, 264, 1150, 544, 20239, 644, 11, 51738], "temperature": 0.0, "avg_logprob": -0.1546317912914135, "compression_ratio": 1.75, "no_speech_prob": 0.000664145452901721}, {"id": 1456, "seek": 445782, "start": 4458.099999999999, "end": 4460.94, "text": " which is to develop a large GI model that", "tokens": [50378, 597, 307, 281, 1499, 257, 2416, 26634, 2316, 300, 50520], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1457, "seek": 445782, "start": 4460.94, "end": 4463.66, "text": " is based on a pre-trained foundation model", "tokens": [50520, 307, 2361, 322, 257, 659, 12, 17227, 2001, 7030, 2316, 50656], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1458, "seek": 445782, "start": 4463.66, "end": 4466.9, "text": " that we will be fine-tuning in order", "tokens": [50656, 300, 321, 486, 312, 2489, 12, 83, 37726, 294, 1668, 50818], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1459, "seek": 445782, "start": 4466.9, "end": 4468.98, "text": " to obtain downscale forecasts.", "tokens": [50818, 281, 12701, 760, 20033, 49421, 13, 50922], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1460, "seek": 445782, "start": 4468.98, "end": 4472.179999999999, "text": " And this is where, again, collaborating with IBM Research", "tokens": [50922, 400, 341, 307, 689, 11, 797, 11, 30188, 365, 23487, 10303, 51082], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1461, "seek": 445782, "start": 4472.179999999999, "end": 4475.9, "text": " is extremely important, as they have very much experience", "tokens": [51082, 307, 4664, 1021, 11, 382, 436, 362, 588, 709, 1752, 51268], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1462, "seek": 445782, "start": 4475.9, "end": 4477.139999999999, "text": " in these foundation models.", "tokens": [51268, 294, 613, 7030, 5245, 13, 51330], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1463, "seek": 445782, "start": 4477.139999999999, "end": 4481.0199999999995, "text": " And we are hoping to advance at least as fast as now.", "tokens": [51330, 400, 321, 366, 7159, 281, 7295, 412, 1935, 382, 2370, 382, 586, 13, 51524], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1464, "seek": 445782, "start": 4481.0199999999995, "end": 4482.34, "text": " So this is it for me.", "tokens": [51524, 407, 341, 307, 309, 337, 385, 13, 51590], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1465, "seek": 445782, "start": 4482.34, "end": 4484.219999999999, "text": " Thank you very much.", "tokens": [51590, 1044, 291, 588, 709, 13, 51684], "temperature": 0.0, "avg_logprob": -0.15385848504525643, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.018435323610901833}, {"id": 1466, "seek": 448422, "start": 4484.22, "end": 4488.14, "text": " Thank you, Medellina.", "tokens": [50364, 1044, 291, 11, 3982, 898, 1426, 13, 50560], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1467, "seek": 448422, "start": 4488.14, "end": 4489.54, "text": " I have a question in the chat.", "tokens": [50560, 286, 362, 257, 1168, 294, 264, 5081, 13, 50630], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1468, "seek": 448422, "start": 4489.54, "end": 4491.54, "text": " I have one.", "tokens": [50630, 286, 362, 472, 13, 50730], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1469, "seek": 448422, "start": 4491.54, "end": 4497.860000000001, "text": " I saw that you train, especially, the model.", "tokens": [50730, 286, 1866, 300, 291, 3847, 11, 2318, 11, 264, 2316, 13, 51046], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1470, "seek": 448422, "start": 4497.860000000001, "end": 4499.9400000000005, "text": " But I was surprised to see that you only", "tokens": [51046, 583, 286, 390, 6100, 281, 536, 300, 291, 787, 51150], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1471, "seek": 448422, "start": 4499.9400000000005, "end": 4502.5, "text": " used one year of data to do that.", "tokens": [51150, 1143, 472, 1064, 295, 1412, 281, 360, 300, 13, 51278], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1472, "seek": 448422, "start": 4502.5, "end": 4503.740000000001, "text": " Is there a reason behind that?", "tokens": [51278, 1119, 456, 257, 1778, 2261, 300, 30, 51340], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1473, "seek": 448422, "start": 4503.740000000001, "end": 4507.46, "text": " Because it seems to me that it's not a lot of data.", "tokens": [51340, 1436, 309, 2544, 281, 385, 300, 309, 311, 406, 257, 688, 295, 1412, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1474, "seek": 448422, "start": 4507.46, "end": 4510.62, "text": " Yes, well, like I said, we have.", "tokens": [51526, 1079, 11, 731, 11, 411, 286, 848, 11, 321, 362, 13, 51684], "temperature": 0.0, "avg_logprob": -0.2339550018310547, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.058208320289850235}, {"id": 1475, "seek": 451062, "start": 4510.62, "end": 4515.9, "text": " So we do end up having a lot of forecast samples.", "tokens": [50364, 407, 321, 360, 917, 493, 1419, 257, 688, 295, 14330, 10938, 13, 50628], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1476, "seek": 451062, "start": 4515.9, "end": 4522.0599999999995, "text": " We are training on 128 by 128 pixel patches out", "tokens": [50628, 492, 366, 3097, 322, 29810, 538, 29810, 19261, 26531, 484, 50936], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1477, "seek": 451062, "start": 4522.0599999999995, "end": 4523.0599999999995, "text": " of a very large domain.", "tokens": [50936, 295, 257, 588, 2416, 9274, 13, 50986], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1478, "seek": 451062, "start": 4523.0599999999995, "end": 4524.9, "text": " So it ends up being a lot of data.", "tokens": [50986, 407, 309, 5314, 493, 885, 257, 688, 295, 1412, 13, 51078], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1479, "seek": 451062, "start": 4524.9, "end": 4526.26, "text": " But it is not finalized.", "tokens": [51078, 583, 309, 307, 406, 2572, 1602, 13, 51146], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1480, "seek": 451062, "start": 4526.26, "end": 4528.42, "text": " So here we are in a developing mode.", "tokens": [51146, 407, 510, 321, 366, 294, 257, 6416, 4391, 13, 51254], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1481, "seek": 451062, "start": 4528.42, "end": 4529.62, "text": " We are trying to develop the model", "tokens": [51254, 492, 366, 1382, 281, 1499, 264, 2316, 51314], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1482, "seek": 451062, "start": 4529.62, "end": 4531.3, "text": " and make sure it's working properly.", "tokens": [51314, 293, 652, 988, 309, 311, 1364, 6108, 13, 51398], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1483, "seek": 451062, "start": 4531.3, "end": 4532.94, "text": " And this is just the first step.", "tokens": [51398, 400, 341, 307, 445, 264, 700, 1823, 13, 51480], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1484, "seek": 451062, "start": 4532.94, "end": 4535.22, "text": " We are definitely planning on adding more data", "tokens": [51480, 492, 366, 2138, 5038, 322, 5127, 544, 1412, 51594], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1485, "seek": 451062, "start": 4535.22, "end": 4537.9, "text": " and seeing how we can improve.", "tokens": [51594, 293, 2577, 577, 321, 393, 3470, 13, 51728], "temperature": 0.0, "avg_logprob": -0.18517536952577787, "compression_ratio": 1.6434426229508197, "no_speech_prob": 0.012166409753262997}, {"id": 1486, "seek": 453790, "start": 4537.9, "end": 4540.54, "text": " Thank you, Medellina.", "tokens": [50364, 1044, 291, 11, 3982, 898, 1426, 13, 50496], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1487, "seek": 453790, "start": 4540.54, "end": 4544.0599999999995, "text": " I don't have a question.", "tokens": [50496, 286, 500, 380, 362, 257, 1168, 13, 50672], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1488, "seek": 453790, "start": 4544.0599999999995, "end": 4547.7, "text": " Otherwise, Anne, I'm going to speak.", "tokens": [50672, 10328, 11, 13706, 11, 286, 478, 516, 281, 1710, 13, 50854], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1489, "seek": 453790, "start": 4547.7, "end": 4548.78, "text": " Right on time.", "tokens": [50854, 1779, 322, 565, 13, 50908], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1490, "seek": 453790, "start": 4548.78, "end": 4550.94, "text": " Thank you, Medellina.", "tokens": [50908, 1044, 291, 11, 3982, 898, 1426, 13, 51016], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1491, "seek": 453790, "start": 4550.94, "end": 4557.099999999999, "text": " So now we're going over to Reynel Sospedra Alfonso, who", "tokens": [51016, 407, 586, 321, 434, 516, 670, 281, 1300, 2534, 338, 318, 329, 3452, 424, 967, 14338, 539, 11, 567, 51324], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1492, "seek": 453790, "start": 4557.099999999999, "end": 4560.0599999999995, "text": " is also a research scientist at Environment and Climate", "tokens": [51324, 307, 611, 257, 2132, 12662, 412, 35354, 293, 27025, 51472], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1493, "seek": 453790, "start": 4560.0599999999995, "end": 4561.54, "text": " Change Canada.", "tokens": [51472, 15060, 6309, 13, 51546], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1494, "seek": 453790, "start": 4561.54, "end": 4565.46, "text": " He will be presenting on deep learning-based bias", "tokens": [51546, 634, 486, 312, 15578, 322, 2452, 2539, 12, 6032, 12577, 51742], "temperature": 0.0, "avg_logprob": -0.30622732639312744, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0052010733634233475}, {"id": 1495, "seek": 456546, "start": 4565.5, "end": 4569.14, "text": " adjustments of Arctic sea ice forecasts", "tokens": [50366, 18624, 295, 27241, 4158, 4435, 49421, 50548], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1496, "seek": 456546, "start": 4569.14, "end": 4572.14, "text": " from version three of the Canadian seasonal to", "tokens": [50548, 490, 3037, 1045, 295, 264, 12641, 27421, 281, 50698], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1497, "seek": 456546, "start": 4572.14, "end": 4575.02, "text": " inter-annual prediction systems.", "tokens": [50698, 728, 12, 969, 901, 17630, 3652, 13, 50842], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1498, "seek": 456546, "start": 4575.02, "end": 4578.22, "text": " Also known as CANSTEP version three.", "tokens": [50842, 2743, 2570, 382, 22931, 6840, 8929, 3037, 1045, 13, 51002], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1499, "seek": 456546, "start": 4578.22, "end": 4581.26, "text": " So, Reynel, the mic is all yours.", "tokens": [51002, 407, 11, 1300, 2534, 338, 11, 264, 3123, 307, 439, 6342, 13, 51154], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1500, "seek": 456546, "start": 4581.26, "end": 4582.7, "text": " Thank you.", "tokens": [51154, 1044, 291, 13, 51226], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1501, "seek": 456546, "start": 4582.7, "end": 4583.58, "text": " Thank you, Anne.", "tokens": [51226, 1044, 291, 11, 13706, 13, 51270], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1502, "seek": 456546, "start": 4583.58, "end": 4586.42, "text": " Can you hear me well?", "tokens": [51270, 1664, 291, 1568, 385, 731, 30, 51412], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1503, "seek": 456546, "start": 4586.42, "end": 4587.38, "text": " Yes, we do.", "tokens": [51412, 1079, 11, 321, 360, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1504, "seek": 456546, "start": 4587.38, "end": 4589.7, "text": " Your presentation is not full screen, though.", "tokens": [51460, 2260, 5860, 307, 406, 1577, 2568, 11, 1673, 13, 51576], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1505, "seek": 456546, "start": 4589.7, "end": 4590.5, "text": " It's not.", "tokens": [51576, 467, 311, 406, 13, 51616], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1506, "seek": 456546, "start": 4590.5, "end": 4590.9800000000005, "text": " No, it's not.", "tokens": [51616, 883, 11, 309, 311, 406, 13, 51640], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1507, "seek": 456546, "start": 4590.9800000000005, "end": 4593.9800000000005, "text": " All right.", "tokens": [51640, 1057, 558, 13, 51790], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1508, "seek": 456546, "start": 4593.9800000000005, "end": 4594.58, "text": " How about now?", "tokens": [51790, 1012, 466, 586, 30, 51820], "temperature": 0.0, "avg_logprob": -0.3293098910101529, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.01107187382876873}, {"id": 1509, "seek": 459546, "start": 4595.74, "end": 4596.78, "text": " Yes, it is.", "tokens": [50378, 1079, 11, 309, 307, 13, 50430], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1510, "seek": 459546, "start": 4596.78, "end": 4597.62, "text": " Perfect.", "tokens": [50430, 10246, 13, 50472], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1511, "seek": 459546, "start": 4597.62, "end": 4599.38, "text": " Thank you so much, Medell.", "tokens": [50472, 1044, 291, 370, 709, 11, 3982, 898, 13, 50560], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1512, "seek": 459546, "start": 4599.38, "end": 4601.7, "text": " Thank you, yes, to the organizer for this opportunity.", "tokens": [50560, 1044, 291, 11, 2086, 11, 281, 264, 41363, 337, 341, 2650, 13, 50676], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1513, "seek": 459546, "start": 4601.7, "end": 4603.54, "text": " My name is Reynel Sospedra Alfonso.", "tokens": [50676, 1222, 1315, 307, 1300, 2534, 338, 318, 329, 3452, 424, 967, 14338, 539, 13, 50768], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1514, "seek": 459546, "start": 4603.54, "end": 4606.78, "text": " I'm a research scientist at the Canadian Center for Climate", "tokens": [50768, 286, 478, 257, 2132, 12662, 412, 264, 12641, 5169, 337, 27025, 50930], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1515, "seek": 459546, "start": 4606.78, "end": 4609.38, "text": " Modeling and Analysis, based in Victoria.", "tokens": [50930, 6583, 11031, 293, 38172, 11, 2361, 294, 16656, 13, 51060], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1516, "seek": 459546, "start": 4609.38, "end": 4613.18, "text": " And I want to start by acknowledging the contributions", "tokens": [51060, 400, 286, 528, 281, 722, 538, 30904, 264, 15725, 51250], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1517, "seek": 459546, "start": 4613.18, "end": 4615.3, "text": " or the work of colleagues at CCMA,", "tokens": [51250, 420, 264, 589, 295, 7734, 412, 12630, 9998, 11, 51356], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1518, "seek": 459546, "start": 4615.3, "end": 4619.02, "text": " which made this type of project possible.", "tokens": [51356, 597, 1027, 341, 2010, 295, 1716, 1944, 13, 51542], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1519, "seek": 459546, "start": 4619.02, "end": 4621.26, "text": " I list some of them down here.", "tokens": [51542, 286, 1329, 512, 295, 552, 760, 510, 13, 51654], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1520, "seek": 459546, "start": 4621.26, "end": 4623.62, "text": " And I also want to acknowledge the co-authors", "tokens": [51654, 400, 286, 611, 528, 281, 10692, 264, 598, 12, 40198, 830, 51772], "temperature": 0.0, "avg_logprob": -0.15364318905454694, "compression_ratio": 1.56993006993007, "no_speech_prob": 0.00507763959467411}, {"id": 1521, "seek": 462362, "start": 4623.62, "end": 4625.82, "text": " of this presentation, or this work,", "tokens": [50364, 295, 341, 5860, 11, 420, 341, 589, 11, 50474], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1522, "seek": 462362, "start": 4625.82, "end": 4629.26, "text": " Joseph Martin, Michael Simon, and especially", "tokens": [50474, 11170, 9184, 11, 5116, 13193, 11, 293, 2318, 50646], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1523, "seek": 462362, "start": 4629.26, "end": 4634.14, "text": " Parca Guilla, who has been key for this project going forward.", "tokens": [50646, 3457, 496, 2694, 5291, 11, 567, 575, 668, 2141, 337, 341, 1716, 516, 2128, 13, 50890], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1524, "seek": 462362, "start": 4634.14, "end": 4636.98, "text": " He has taken the time to do the implementation, training,", "tokens": [50890, 634, 575, 2726, 264, 565, 281, 360, 264, 11420, 11, 3097, 11, 51032], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1525, "seek": 462362, "start": 4636.98, "end": 4640.86, "text": " and testing of the models we have been looking at.", "tokens": [51032, 293, 4997, 295, 264, 5245, 321, 362, 668, 1237, 412, 13, 51226], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1526, "seek": 462362, "start": 4640.86, "end": 4643.54, "text": " And I want to mention that this is part, what I'm going to talk", "tokens": [51226, 400, 286, 528, 281, 2152, 300, 341, 307, 644, 11, 437, 286, 478, 516, 281, 751, 51360], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1527, "seek": 462362, "start": 4643.54, "end": 4645.46, "text": " about here is part of a bigger project", "tokens": [51360, 466, 510, 307, 644, 295, 257, 3801, 1716, 51456], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1528, "seek": 462362, "start": 4645.46, "end": 4648.78, "text": " that we are trying to pursue at CCMA, which", "tokens": [51456, 300, 321, 366, 1382, 281, 12392, 412, 12630, 9998, 11, 597, 51622], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1529, "seek": 462362, "start": 4648.78, "end": 4652.86, "text": " is the use or applications of machine learning methods,", "tokens": [51622, 307, 264, 764, 420, 5821, 295, 3479, 2539, 7150, 11, 51826], "temperature": 0.0, "avg_logprob": -0.2021777689957819, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.005574519280344248}, {"id": 1530, "seek": 465286, "start": 4652.9, "end": 4656.299999999999, "text": " in particular deep learning, to post-process our seasonal", "tokens": [50366, 294, 1729, 2452, 2539, 11, 281, 2183, 12, 41075, 527, 27421, 50536], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1531, "seek": 465286, "start": 4656.299999999999, "end": 4658.58, "text": " to the scale forecast.", "tokens": [50536, 281, 264, 4373, 14330, 13, 50650], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1532, "seek": 465286, "start": 4658.58, "end": 4660.78, "text": " So for this talk, I will talk in particular", "tokens": [50650, 407, 337, 341, 751, 11, 286, 486, 751, 294, 1729, 50760], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1533, "seek": 465286, "start": 4660.78, "end": 4664.139999999999, "text": " about the post-processing or seasonal forecast of CIS.", "tokens": [50760, 466, 264, 2183, 12, 41075, 278, 420, 27421, 14330, 295, 383, 2343, 13, 50928], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1534, "seek": 465286, "start": 4666.98, "end": 4670.139999999999, "text": " The seasonal forecast, as you may know,", "tokens": [51070, 440, 27421, 14330, 11, 382, 291, 815, 458, 11, 51228], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1535, "seek": 465286, "start": 4670.139999999999, "end": 4673.339999999999, "text": " CANSIPS is the Canadian seasonal and internal prediction", "tokens": [51228, 22931, 20262, 6273, 307, 264, 12641, 27421, 293, 6920, 17630, 51388], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1536, "seek": 465286, "start": 4673.339999999999, "end": 4675.9, "text": " system, which provides the Environment and Climate Change", "tokens": [51388, 1185, 11, 597, 6417, 264, 35354, 293, 27025, 15060, 51516], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1537, "seek": 465286, "start": 4675.9, "end": 4678.58, "text": " Canada's operational, probabilistic seasonal", "tokens": [51516, 6309, 311, 16607, 11, 31959, 3142, 27421, 51650], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1538, "seek": 465286, "start": 4678.58, "end": 4681.7, "text": " forecast, both national and global.", "tokens": [51650, 14330, 11, 1293, 4048, 293, 4338, 13, 51806], "temperature": 0.0, "avg_logprob": -0.2598851647707495, "compression_ratio": 1.7364016736401673, "no_speech_prob": 0.0009490590891800821}, {"id": 1539, "seek": 468170, "start": 4681.7, "end": 4686.66, "text": " And CANSIPS first appeared or was first", "tokens": [50364, 400, 22931, 20262, 6273, 700, 8516, 420, 390, 700, 50612], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1540, "seek": 468170, "start": 4686.66, "end": 4691.42, "text": " debuted in 2011 as a two-model forecasting system.", "tokens": [50612, 33392, 294, 10154, 382, 257, 732, 12, 8014, 338, 44331, 1185, 13, 50850], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1541, "seek": 468170, "start": 4691.42, "end": 4694.54, "text": " It has evolved since, and now we are in 2024,", "tokens": [50850, 467, 575, 14178, 1670, 11, 293, 586, 321, 366, 294, 45237, 11, 51006], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1542, "seek": 468170, "start": 4694.54, "end": 4697.62, "text": " with the new version of CANSIPS B3,", "tokens": [51006, 365, 264, 777, 3037, 295, 22931, 20262, 6273, 363, 18, 11, 51160], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1543, "seek": 468170, "start": 4697.62, "end": 4701.66, "text": " which actually will be launched next month.", "tokens": [51160, 597, 767, 486, 312, 8730, 958, 1618, 13, 51362], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1544, "seek": 468170, "start": 4701.66, "end": 4705.3, "text": " And so here, what I'm going to talk about", "tokens": [51362, 400, 370, 510, 11, 437, 286, 478, 516, 281, 751, 466, 51544], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1545, "seek": 468170, "start": 4705.3, "end": 4709.9, "text": " is the forecast that we produce with CANIAS M5, which", "tokens": [51544, 307, 264, 14330, 300, 321, 5258, 365, 22931, 40, 3160, 376, 20, 11, 597, 51774], "temperature": 0.0, "avg_logprob": -0.23910846505113828, "compression_ratio": 1.4377880184331797, "no_speech_prob": 0.00032712097163312137}, {"id": 1546, "seek": 470990, "start": 4709.9, "end": 4714.299999999999, "text": " is a new model that now we'll be using in CANSIPS.", "tokens": [50364, 307, 257, 777, 2316, 300, 586, 321, 603, 312, 1228, 294, 22931, 20262, 6273, 13, 50584], "temperature": 0.0, "avg_logprob": -0.20796758478338068, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0009947750950232148}, {"id": 1547, "seek": 470990, "start": 4714.299999999999, "end": 4719.9, "text": " And CANIAS M5 is an air system model that is produced at CCMA,", "tokens": [50584, 400, 22931, 40, 3160, 376, 20, 307, 364, 1988, 1185, 2316, 300, 307, 7126, 412, 12630, 9998, 11, 50864], "temperature": 0.0, "avg_logprob": -0.20796758478338068, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0009947750950232148}, {"id": 1548, "seek": 470990, "start": 4719.9, "end": 4725.46, "text": " and now will be then, as I said, used for our seasonal forecast.", "tokens": [50864, 293, 586, 486, 312, 550, 11, 382, 286, 848, 11, 1143, 337, 527, 27421, 14330, 13, 51142], "temperature": 0.0, "avg_logprob": -0.20796758478338068, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0009947750950232148}, {"id": 1549, "seek": 470990, "start": 4725.46, "end": 4727.7, "text": " CANIAS M5 air system models, so it", "tokens": [51142, 22931, 40, 3160, 376, 20, 1988, 1185, 5245, 11, 370, 309, 51254], "temperature": 0.0, "avg_logprob": -0.20796758478338068, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0009947750950232148}, {"id": 1550, "seek": 470990, "start": 4727.7, "end": 4732.46, "text": " couples the atmosphere, the ocean, CIS component, land,", "tokens": [51254, 20368, 264, 8018, 11, 264, 7810, 11, 383, 2343, 6542, 11, 2117, 11, 51492], "temperature": 0.0, "avg_logprob": -0.20796758478338068, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0009947750950232148}, {"id": 1551, "seek": 470990, "start": 4732.46, "end": 4737.94, "text": " and also biochemistry, both on land and the ocean.", "tokens": [51492, 293, 611, 12198, 48353, 11, 1293, 322, 2117, 293, 264, 7810, 13, 51766], "temperature": 0.0, "avg_logprob": -0.20796758478338068, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0009947750950232148}, {"id": 1552, "seek": 473794, "start": 4737.98, "end": 4740.099999999999, "text": " What we do, we take our climate model,", "tokens": [50366, 708, 321, 360, 11, 321, 747, 527, 5659, 2316, 11, 50472], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1553, "seek": 473794, "start": 4740.099999999999, "end": 4743.46, "text": " we initialize the climate model following the indications", "tokens": [50472, 321, 5883, 1125, 264, 5659, 2316, 3480, 264, 44450, 50640], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1554, "seek": 473794, "start": 4743.46, "end": 4745.339999999999, "text": " that you see here on the right.", "tokens": [50640, 300, 291, 536, 510, 322, 264, 558, 13, 50734], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1555, "seek": 473794, "start": 4745.339999999999, "end": 4747.54, "text": " So when we initialize the forecast,", "tokens": [50734, 407, 562, 321, 5883, 1125, 264, 14330, 11, 50844], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1556, "seek": 473794, "start": 4747.54, "end": 4750.78, "text": " we take, we notch the model towards re-analysis,", "tokens": [50844, 321, 747, 11, 321, 26109, 264, 2316, 3030, 319, 12, 29702, 4642, 11, 51006], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1557, "seek": 473794, "start": 4750.78, "end": 4755.299999999999, "text": " and then we launch those forecasts in time.", "tokens": [51006, 293, 550, 321, 4025, 729, 49421, 294, 565, 13, 51232], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1558, "seek": 473794, "start": 4755.299999999999, "end": 4757.7, "text": " The version of CANIAS M5 that I'm going to be talking about", "tokens": [51232, 440, 3037, 295, 22931, 40, 3160, 376, 20, 300, 286, 478, 516, 281, 312, 1417, 466, 51352], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1559, "seek": 473794, "start": 4757.7, "end": 4762.98, "text": " is actually an optimal bias-corrected version, which", "tokens": [51352, 307, 767, 364, 16252, 12577, 12, 19558, 2554, 292, 3037, 11, 597, 51616], "temperature": 0.0, "avg_logprob": -0.16461535333429725, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.04225824773311615}, {"id": 1560, "seek": 476298, "start": 4762.98, "end": 4766.219999999999, "text": " follows the work by Sino-Kankarin, which", "tokens": [50364, 10002, 264, 589, 538, 318, 2982, 12, 42, 657, 19829, 11, 597, 50526], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1561, "seek": 476298, "start": 4766.219999999999, "end": 4772.099999999999, "text": " does an online bias optimization to the model.", "tokens": [50526, 775, 364, 2950, 12577, 19618, 281, 264, 2316, 13, 50820], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1562, "seek": 476298, "start": 4772.099999999999, "end": 4777.5, "text": " Now, the work that I'm going to be presenting to you", "tokens": [50820, 823, 11, 264, 589, 300, 286, 478, 516, 281, 312, 15578, 281, 291, 51090], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1563, "seek": 476298, "start": 4777.5, "end": 4780.74, "text": " deals with the post-processing of those forecasts.", "tokens": [51090, 11215, 365, 264, 2183, 12, 41075, 278, 295, 729, 49421, 13, 51252], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1564, "seek": 476298, "start": 4780.74, "end": 4783.0199999999995, "text": " So here, what you see is a representation", "tokens": [51252, 407, 510, 11, 437, 291, 536, 307, 257, 10290, 51366], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1565, "seek": 476298, "start": 4783.0199999999995, "end": 4786.62, "text": " of those forecasts in black.", "tokens": [51366, 295, 729, 49421, 294, 2211, 13, 51546], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1566, "seek": 476298, "start": 4786.62, "end": 4789.7, "text": " It's the observations that we are verifying,", "tokens": [51546, 467, 311, 264, 18163, 300, 321, 366, 1306, 5489, 11, 51700], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1567, "seek": 476298, "start": 4789.7, "end": 4792.0199999999995, "text": " observations, the monthly values.", "tokens": [51700, 18163, 11, 264, 12878, 4190, 13, 51816], "temperature": 0.0, "avg_logprob": -0.22529977433224943, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.004175450187176466}, {"id": 1568, "seek": 479202, "start": 4792.02, "end": 4794.900000000001, "text": " And then what you see is the representation", "tokens": [50364, 400, 550, 437, 291, 536, 307, 264, 10290, 50508], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1569, "seek": 479202, "start": 4794.900000000001, "end": 4797.540000000001, "text": " of those forecasts, which are initialized", "tokens": [50508, 295, 729, 49421, 11, 597, 366, 5883, 1602, 50640], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1570, "seek": 479202, "start": 4797.540000000001, "end": 4801.1, "text": " at the start of every month during the handcast period.", "tokens": [50640, 412, 264, 722, 295, 633, 1618, 1830, 264, 1011, 3734, 2896, 13, 50818], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1571, "seek": 479202, "start": 4801.1, "end": 4805.02, "text": " So we'll be looking at handcasts from 1980 to 2021.", "tokens": [50818, 407, 321, 603, 312, 1237, 412, 1011, 3734, 82, 490, 13626, 281, 7201, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1572, "seek": 479202, "start": 4805.02, "end": 4808.14, "text": " We have, for each month in that time,", "tokens": [51014, 492, 362, 11, 337, 1184, 1618, 294, 300, 565, 11, 51170], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1573, "seek": 479202, "start": 4808.14, "end": 4812.26, "text": " we launch an ensemble of forecasts of 10 members,", "tokens": [51170, 321, 4025, 364, 19492, 295, 49421, 295, 1266, 2679, 11, 51376], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1574, "seek": 479202, "start": 4812.26, "end": 4814.860000000001, "text": " which run for 12 months.", "tokens": [51376, 597, 1190, 337, 2272, 2493, 13, 51506], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1575, "seek": 479202, "start": 4814.860000000001, "end": 4817.06, "text": " The variable of interest for us here", "tokens": [51506, 440, 7006, 295, 1179, 337, 505, 510, 51616], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1576, "seek": 479202, "start": 4817.06, "end": 4819.580000000001, "text": " is CIS concentration, which is simply", "tokens": [51616, 307, 383, 2343, 9856, 11, 597, 307, 2935, 51742], "temperature": 0.0, "avg_logprob": -0.14039942361776112, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.0004642759158741683}, {"id": 1577, "seek": 481958, "start": 4819.58, "end": 4824.5, "text": " the fraction of CIS, or the fraction of the grid cells", "tokens": [50364, 264, 14135, 295, 383, 2343, 11, 420, 264, 14135, 295, 264, 10748, 5438, 50610], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1578, "seek": 481958, "start": 4824.5, "end": 4826.66, "text": " that is covered by CIS.", "tokens": [50610, 300, 307, 5343, 538, 383, 2343, 13, 50718], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1579, "seek": 481958, "start": 4826.66, "end": 4829.78, "text": " And this is what we want to adjust.", "tokens": [50718, 400, 341, 307, 437, 321, 528, 281, 4369, 13, 50874], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1580, "seek": 481958, "start": 4833.66, "end": 4836.86, "text": " Well, we do that looking at the ensemble mean forecast.", "tokens": [51068, 1042, 11, 321, 360, 300, 1237, 412, 264, 19492, 914, 14330, 13, 51228], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1581, "seek": 481958, "start": 4836.86, "end": 4839.34, "text": " So the adjustment is not done to the ensemble itself,", "tokens": [51228, 407, 264, 17132, 307, 406, 1096, 281, 264, 19492, 2564, 11, 51352], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1582, "seek": 481958, "start": 4839.34, "end": 4841.78, "text": " it's done to the ensemble mean.", "tokens": [51352, 309, 311, 1096, 281, 264, 19492, 914, 13, 51474], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1583, "seek": 481958, "start": 4841.78, "end": 4844.3, "text": " And the question is, why do we have to do that?", "tokens": [51474, 400, 264, 1168, 307, 11, 983, 360, 321, 362, 281, 360, 300, 30, 51600], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1584, "seek": 481958, "start": 4844.3, "end": 4845.66, "text": " I mean, after all, we are even doing", "tokens": [51600, 286, 914, 11, 934, 439, 11, 321, 366, 754, 884, 51668], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1585, "seek": 481958, "start": 4845.66, "end": 4849.26, "text": " an online bias optimization or bias correction", "tokens": [51668, 364, 2950, 12577, 19618, 420, 12577, 19984, 51848], "temperature": 0.0, "avg_logprob": -0.170893686467951, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.00030011613853275776}, {"id": 1586, "seek": 484926, "start": 4849.26, "end": 4850.54, "text": " of my model.", "tokens": [50364, 295, 452, 2316, 13, 50428], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1587, "seek": 484926, "start": 4850.54, "end": 4855.14, "text": " So still, we do need to adjust those forecasts,", "tokens": [50428, 407, 920, 11, 321, 360, 643, 281, 4369, 729, 49421, 11, 50658], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1588, "seek": 484926, "start": 4855.14, "end": 4858.18, "text": " because as we know, we have several sources of error,", "tokens": [50658, 570, 382, 321, 458, 11, 321, 362, 2940, 7139, 295, 6713, 11, 50810], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1589, "seek": 484926, "start": 4858.18, "end": 4860.9400000000005, "text": " structural errors, errors due to initialization,", "tokens": [50810, 15067, 13603, 11, 13603, 3462, 281, 5883, 2144, 11, 50948], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1590, "seek": 484926, "start": 4860.9400000000005, "end": 4862.26, "text": " and so forth.", "tokens": [50948, 293, 370, 5220, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1591, "seek": 484926, "start": 4862.26, "end": 4864.860000000001, "text": " And typically, this is done by doing", "tokens": [51014, 400, 5850, 11, 341, 307, 1096, 538, 884, 51144], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1592, "seek": 484926, "start": 4864.860000000001, "end": 4868.46, "text": " some climatological bias correction to those forecasts.", "tokens": [51144, 512, 5644, 267, 4383, 12577, 19984, 281, 729, 49421, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1593, "seek": 484926, "start": 4868.46, "end": 4870.900000000001, "text": " Now here, just to give you an example,", "tokens": [51324, 823, 510, 11, 445, 281, 976, 291, 364, 1365, 11, 51446], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1594, "seek": 484926, "start": 4870.900000000001, "end": 4874.3, "text": " I'm showing you the September CIS concentration", "tokens": [51446, 286, 478, 4099, 291, 264, 7216, 383, 2343, 9856, 51616], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1595, "seek": 484926, "start": 4874.3, "end": 4877.54, "text": " over the time period 2006 to 2020.", "tokens": [51616, 670, 264, 565, 2896, 14062, 281, 4808, 13, 51778], "temperature": 0.0, "avg_logprob": -0.12765352302622573, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.001090776757337153}, {"id": 1596, "seek": 487754, "start": 4877.58, "end": 4880.5, "text": " On the left, these are the very fine observations", "tokens": [50366, 1282, 264, 1411, 11, 613, 366, 264, 588, 2489, 18163, 50512], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1597, "seek": 487754, "start": 4880.5, "end": 4885.82, "text": " that we use, which comes from NOAA data products.", "tokens": [50512, 300, 321, 764, 11, 597, 1487, 490, 9146, 5265, 1412, 3383, 13, 50778], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1598, "seek": 487754, "start": 4885.82, "end": 4888.82, "text": " And here, again, this is the CIS concentration,", "tokens": [50778, 400, 510, 11, 797, 11, 341, 307, 264, 383, 2343, 9856, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1599, "seek": 487754, "start": 4888.82, "end": 4891.18, "text": " which has value from 0 to 1.", "tokens": [50928, 597, 575, 2158, 490, 1958, 281, 502, 13, 51046], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1600, "seek": 487754, "start": 4891.18, "end": 4895.22, "text": " And we see on the right, the growth forecast", "tokens": [51046, 400, 321, 536, 322, 264, 558, 11, 264, 4599, 14330, 51248], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1601, "seek": 487754, "start": 4895.22, "end": 4896.82, "text": " as it comes out of the model.", "tokens": [51248, 382, 309, 1487, 484, 295, 264, 2316, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1602, "seek": 487754, "start": 4896.82, "end": 4900.26, "text": " So this is the output directly coming from our forecast.", "tokens": [51328, 407, 341, 307, 264, 5598, 3838, 1348, 490, 527, 14330, 13, 51500], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1603, "seek": 487754, "start": 4900.26, "end": 4903.3, "text": " And what we see is that there is definitely", "tokens": [51500, 400, 437, 321, 536, 307, 300, 456, 307, 2138, 51652], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1604, "seek": 487754, "start": 4903.3, "end": 4905.94, "text": " a strong bias that we notice, particularly", "tokens": [51652, 257, 2068, 12577, 300, 321, 3449, 11, 4098, 51784], "temperature": 0.0, "avg_logprob": -0.1548675607751917, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.006848061922937632}, {"id": 1605, "seek": 490594, "start": 4905.94, "end": 4909.74, "text": " in the center Arctic, where we have a much lower CIS", "tokens": [50364, 294, 264, 3056, 27241, 11, 689, 321, 362, 257, 709, 3126, 383, 2343, 50554], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1606, "seek": 490594, "start": 4909.74, "end": 4913.259999999999, "text": " concentration relative to observations.", "tokens": [50554, 9856, 4972, 281, 18163, 13, 50730], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1607, "seek": 490594, "start": 4913.259999999999, "end": 4916.339999999999, "text": " Now on the right to that, we see the bias adjusted forecast,", "tokens": [50730, 823, 322, 264, 558, 281, 300, 11, 321, 536, 264, 12577, 19871, 14330, 11, 50884], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1608, "seek": 490594, "start": 4916.339999999999, "end": 4919.7, "text": " in which we compensate or we subtract the bias,", "tokens": [50884, 294, 597, 321, 29458, 420, 321, 16390, 264, 12577, 11, 51052], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1609, "seek": 490594, "start": 4919.7, "end": 4921.66, "text": " compute it on a previous time.", "tokens": [51052, 14722, 309, 322, 257, 3894, 565, 13, 51150], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1610, "seek": 490594, "start": 4921.66, "end": 4924.54, "text": " And then what we see is that we adjust somehow", "tokens": [51150, 400, 550, 437, 321, 536, 307, 300, 321, 4369, 6063, 51294], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1611, "seek": 490594, "start": 4924.54, "end": 4928.419999999999, "text": " that forecast by doing this simple bias correction.", "tokens": [51294, 300, 14330, 538, 884, 341, 2199, 12577, 19984, 13, 51488], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1612, "seek": 490594, "start": 4928.419999999999, "end": 4931.0599999999995, "text": " Now still, even after doing a bias correction,", "tokens": [51488, 823, 920, 11, 754, 934, 884, 257, 12577, 19984, 11, 51620], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1613, "seek": 490594, "start": 4931.0599999999995, "end": 4933.94, "text": " we see that there are some differences", "tokens": [51620, 321, 536, 300, 456, 366, 512, 7300, 51764], "temperature": 0.0, "avg_logprob": -0.13268104734874908, "compression_ratio": 1.7231404958677685, "no_speech_prob": 0.0005602855235338211}, {"id": 1614, "seek": 493394, "start": 4933.94, "end": 4936.259999999999, "text": " between the observed CIS concentration", "tokens": [50364, 1296, 264, 13095, 383, 2343, 9856, 50480], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1615, "seek": 493394, "start": 4936.259999999999, "end": 4938.179999999999, "text": " and the one that is bias adjusted.", "tokens": [50480, 293, 264, 472, 300, 307, 12577, 19871, 13, 50576], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1616, "seek": 493394, "start": 4938.179999999999, "end": 4940.62, "text": " So that tells us that we need to do something else in order", "tokens": [50576, 407, 300, 5112, 505, 300, 321, 643, 281, 360, 746, 1646, 294, 1668, 50698], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1617, "seek": 493394, "start": 4940.62, "end": 4943.419999999999, "text": " to actually improve our forecast.", "tokens": [50698, 281, 767, 3470, 527, 14330, 13, 50838], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1618, "seek": 493394, "start": 4943.419999999999, "end": 4945.259999999999, "text": " And for that, and this is now the project", "tokens": [50838, 400, 337, 300, 11, 293, 341, 307, 586, 264, 1716, 50930], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1619, "seek": 493394, "start": 4945.259999999999, "end": 4947.78, "text": " that we are working on, is to use this machine learning", "tokens": [50930, 300, 321, 366, 1364, 322, 11, 307, 281, 764, 341, 3479, 2539, 51056], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1620, "seek": 493394, "start": 4947.78, "end": 4951.94, "text": " or deep learning method to improve even further", "tokens": [51056, 420, 2452, 2539, 3170, 281, 3470, 754, 3052, 51264], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1621, "seek": 493394, "start": 4951.94, "end": 4953.54, "text": " those forecasts.", "tokens": [51264, 729, 49421, 13, 51344], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1622, "seek": 493394, "start": 4953.54, "end": 4955.179999999999, "text": " The method that I'm going to use here,", "tokens": [51344, 440, 3170, 300, 286, 478, 516, 281, 764, 510, 11, 51426], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1623, "seek": 493394, "start": 4955.179999999999, "end": 4957.139999999999, "text": " and I'm going to talk a little bit more about that", "tokens": [51426, 293, 286, 478, 516, 281, 751, 257, 707, 857, 544, 466, 300, 51524], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1624, "seek": 493394, "start": 4957.139999999999, "end": 4960.419999999999, "text": " in the few slides, is the unit.", "tokens": [51524, 294, 264, 1326, 9788, 11, 307, 264, 4985, 13, 51688], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1625, "seek": 493394, "start": 4960.419999999999, "end": 4962.54, "text": " Probably most of the people in the audience", "tokens": [51688, 9210, 881, 295, 264, 561, 294, 264, 4034, 51794], "temperature": 0.0, "avg_logprob": -0.14147079991930314, "compression_ratio": 1.7651245551601424, "no_speech_prob": 0.0024971896782517433}, {"id": 1626, "seek": 496254, "start": 4962.54, "end": 4964.0199999999995, "text": " are familiar with units.", "tokens": [50364, 366, 4963, 365, 6815, 13, 50438], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1627, "seek": 496254, "start": 4964.0199999999995, "end": 4966.3, "text": " As I said, I will talk a little bit more about that.", "tokens": [50438, 1018, 286, 848, 11, 286, 486, 751, 257, 707, 857, 544, 466, 300, 13, 50552], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1628, "seek": 496254, "start": 4966.3, "end": 4971.22, "text": " But here, I'm just showing you some results in which you see", "tokens": [50552, 583, 510, 11, 286, 478, 445, 4099, 291, 512, 3542, 294, 597, 291, 536, 50798], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1629, "seek": 496254, "start": 4971.22, "end": 4977.0199999999995, "text": " how the unit is able to reproduce the spatial pattern a lot", "tokens": [50798, 577, 264, 4985, 307, 1075, 281, 29501, 264, 23598, 5102, 257, 688, 51088], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1630, "seek": 496254, "start": 4977.0199999999995, "end": 4981.5, "text": " better than what we can do with a simple bias correction.", "tokens": [51088, 1101, 813, 437, 321, 393, 360, 365, 257, 2199, 12577, 19984, 13, 51312], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1631, "seek": 496254, "start": 4981.5, "end": 4984.22, "text": " I should have said that this is a forecast done", "tokens": [51312, 286, 820, 362, 848, 300, 341, 307, 257, 14330, 1096, 51448], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1632, "seek": 496254, "start": 4984.22, "end": 4985.58, "text": " at two monthly time.", "tokens": [51448, 412, 732, 12878, 565, 13, 51516], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1633, "seek": 496254, "start": 4985.58, "end": 4987.58, "text": " So this is the forecast two months", "tokens": [51516, 407, 341, 307, 264, 14330, 732, 2493, 51616], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1634, "seek": 496254, "start": 4987.58, "end": 4990.22, "text": " after the forecast is initialized,", "tokens": [51616, 934, 264, 14330, 307, 5883, 1602, 11, 51748], "temperature": 0.0, "avg_logprob": -0.21619911913601858, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0011301321210339665}, {"id": 1635, "seek": 499022, "start": 4990.26, "end": 4997.42, "text": " where the biases are, you can see, are particularly strong.", "tokens": [50366, 689, 264, 32152, 366, 11, 291, 393, 536, 11, 366, 4098, 2068, 13, 50724], "temperature": 0.0, "avg_logprob": -0.23253270467122397, "compression_ratio": 1.5808383233532934, "no_speech_prob": 0.0001235369563801214}, {"id": 1636, "seek": 499022, "start": 4997.42, "end": 5001.7, "text": " Now, the unit, this is the tool of choice.", "tokens": [50724, 823, 11, 264, 4985, 11, 341, 307, 264, 2290, 295, 3922, 13, 50938], "temperature": 0.0, "avg_logprob": -0.23253270467122397, "compression_ratio": 1.5808383233532934, "no_speech_prob": 0.0001235369563801214}, {"id": 1637, "seek": 499022, "start": 5001.7, "end": 5005.62, "text": " What we have here is a fully connected network,", "tokens": [50938, 708, 321, 362, 510, 307, 257, 4498, 4582, 3209, 11, 51134], "temperature": 0.0, "avg_logprob": -0.23253270467122397, "compression_ratio": 1.5808383233532934, "no_speech_prob": 0.0001235369563801214}, {"id": 1638, "seek": 499022, "start": 5005.62, "end": 5007.66, "text": " or a fully convolutional network,", "tokens": [51134, 420, 257, 4498, 45216, 304, 3209, 11, 51236], "temperature": 0.0, "avg_logprob": -0.23253270467122397, "compression_ratio": 1.5808383233532934, "no_speech_prob": 0.0001235369563801214}, {"id": 1639, "seek": 499022, "start": 5007.66, "end": 5013.46, "text": " that has a downscaling, down sampling encoder,", "tokens": [51236, 300, 575, 257, 760, 4417, 4270, 11, 760, 21179, 2058, 19866, 11, 51526], "temperature": 0.0, "avg_logprob": -0.23253270467122397, "compression_ratio": 1.5808383233532934, "no_speech_prob": 0.0001235369563801214}, {"id": 1640, "seek": 499022, "start": 5013.46, "end": 5018.46, "text": " followed by up sampling decoder.", "tokens": [51526, 6263, 538, 493, 21179, 979, 19866, 13, 51776], "temperature": 0.0, "avg_logprob": -0.23253270467122397, "compression_ratio": 1.5808383233532934, "no_speech_prob": 0.0001235369563801214}, {"id": 1641, "seek": 501846, "start": 5018.46, "end": 5019.9, "text": " This is a classical unit.", "tokens": [50364, 639, 307, 257, 13735, 4985, 13, 50436], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1642, "seek": 501846, "start": 5019.9, "end": 5023.86, "text": " What we see is that the future maps are reduced in size.", "tokens": [50436, 708, 321, 536, 307, 300, 264, 2027, 11317, 366, 9212, 294, 2744, 13, 50634], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1643, "seek": 501846, "start": 5023.86, "end": 5028.74, "text": " So we see that the resolution is reduced by its health,", "tokens": [50634, 407, 321, 536, 300, 264, 8669, 307, 9212, 538, 1080, 1585, 11, 50878], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1644, "seek": 501846, "start": 5028.74, "end": 5030.58, "text": " and the channels are increased by two.", "tokens": [50878, 293, 264, 9235, 366, 6505, 538, 732, 13, 50970], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1645, "seek": 501846, "start": 5030.58, "end": 5033.9, "text": " And this allows us to have a better representation", "tokens": [50970, 400, 341, 4045, 505, 281, 362, 257, 1101, 10290, 51136], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1646, "seek": 501846, "start": 5033.9, "end": 5037.82, "text": " of capacity of the network, while preserving some information", "tokens": [51136, 295, 6042, 295, 264, 3209, 11, 1339, 33173, 512, 1589, 51332], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1647, "seek": 501846, "start": 5037.82, "end": 5039.9800000000005, "text": " of the image that we input.", "tokens": [51332, 295, 264, 3256, 300, 321, 4846, 13, 51440], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1648, "seek": 501846, "start": 5039.9800000000005, "end": 5043.9, "text": " So to be clear, what we input here is our raw forecast,", "tokens": [51440, 407, 281, 312, 1850, 11, 437, 321, 4846, 510, 307, 527, 8936, 14330, 11, 51636], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1649, "seek": 501846, "start": 5043.9, "end": 5047.22, "text": " which is denoted by the YMN.", "tokens": [51636, 597, 307, 1441, 23325, 538, 264, 398, 44, 45, 13, 51802], "temperature": 0.0, "avg_logprob": -0.21231541064901088, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0042354934848845005}, {"id": 1650, "seek": 504722, "start": 5047.3, "end": 5050.3, "text": " Thanks, and will be the resolution of my forecast,", "tokens": [50368, 2561, 11, 293, 486, 312, 264, 8669, 295, 452, 14330, 11, 50518], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1651, "seek": 504722, "start": 5050.3, "end": 5052.54, "text": " which is a function of the initial month", "tokens": [50518, 597, 307, 257, 2445, 295, 264, 5883, 1618, 50630], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1652, "seek": 504722, "start": 5052.54, "end": 5057.22, "text": " and the target month, and the time relative to the first month", "tokens": [50630, 293, 264, 3779, 1618, 11, 293, 264, 565, 4972, 281, 264, 700, 1618, 50864], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1653, "seek": 504722, "start": 5057.22, "end": 5060.900000000001, "text": " in the data that we are inputting.", "tokens": [50864, 294, 264, 1412, 300, 321, 366, 4846, 783, 13, 51048], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1654, "seek": 504722, "start": 5060.900000000001, "end": 5064.02, "text": " So the input is made of six channels,", "tokens": [51048, 407, 264, 4846, 307, 1027, 295, 2309, 9235, 11, 51204], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1655, "seek": 504722, "start": 5064.02, "end": 5068.1, "text": " one channel that is the actual variable", "tokens": [51204, 472, 2269, 300, 307, 264, 3539, 7006, 51408], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1656, "seek": 504722, "start": 5068.1, "end": 5071.5, "text": " that we want to, or the actual map that we want to correct,", "tokens": [51408, 300, 321, 528, 281, 11, 420, 264, 3539, 4471, 300, 321, 528, 281, 3006, 11, 51578], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1657, "seek": 504722, "start": 5071.5, "end": 5073.9800000000005, "text": " plus five temporal features that takes into account,", "tokens": [51578, 1804, 1732, 30881, 4122, 300, 2516, 666, 2696, 11, 51702], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1658, "seek": 504722, "start": 5073.9800000000005, "end": 5076.34, "text": " again, the initial month that we're looking at,", "tokens": [51702, 797, 11, 264, 5883, 1618, 300, 321, 434, 1237, 412, 11, 51820], "temperature": 0.0, "avg_logprob": -0.17947772571018764, "compression_ratio": 1.8369098712446352, "no_speech_prob": 0.00032408739207312465}, {"id": 1659, "seek": 507634, "start": 5076.38, "end": 5078.54, "text": " the target month, and this temporal information", "tokens": [50366, 264, 3779, 1618, 11, 293, 341, 30881, 1589, 50474], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1660, "seek": 507634, "start": 5078.54, "end": 5080.46, "text": " relative to the initial time.", "tokens": [50474, 4972, 281, 264, 5883, 565, 13, 50570], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1661, "seek": 507634, "start": 5080.46, "end": 5083.42, "text": " So in other words, how many years and how many months", "tokens": [50570, 407, 294, 661, 2283, 11, 577, 867, 924, 293, 577, 867, 2493, 50718], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1662, "seek": 507634, "start": 5083.42, "end": 5085.5, "text": " from the starting of your data.", "tokens": [50718, 490, 264, 2891, 295, 428, 1412, 13, 50822], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1663, "seek": 507634, "start": 5085.5, "end": 5087.34, "text": " You input that into the network,", "tokens": [50822, 509, 4846, 300, 666, 264, 3209, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1664, "seek": 507634, "start": 5087.34, "end": 5089.66, "text": " and then the output is, hopefully,", "tokens": [50914, 293, 550, 264, 5598, 307, 11, 4696, 11, 51030], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1665, "seek": 507634, "start": 5089.66, "end": 5091.5, "text": " is an adjusted forecast,", "tokens": [51030, 307, 364, 19871, 14330, 11, 51122], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1666, "seek": 507634, "start": 5091.5, "end": 5094.18, "text": " which then correct for those biases", "tokens": [51122, 597, 550, 3006, 337, 729, 32152, 51256], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1667, "seek": 507634, "start": 5094.18, "end": 5095.58, "text": " that I mentioned earlier.", "tokens": [51256, 300, 286, 2835, 3071, 13, 51326], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1668, "seek": 507634, "start": 5097.7, "end": 5099.58, "text": " Moving a little bit forward here,", "tokens": [51432, 14242, 257, 707, 857, 2128, 510, 11, 51526], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1669, "seek": 507634, "start": 5099.58, "end": 5101.34, "text": " let me just be a little bit more precise", "tokens": [51526, 718, 385, 445, 312, 257, 707, 857, 544, 13600, 51614], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1670, "seek": 507634, "start": 5101.34, "end": 5103.5, "text": " on the kind of task that we are doing.", "tokens": [51614, 322, 264, 733, 295, 5633, 300, 321, 366, 884, 13, 51722], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1671, "seek": 507634, "start": 5103.5, "end": 5104.9400000000005, "text": " This is just a specific example,", "tokens": [51722, 639, 307, 445, 257, 2685, 1365, 11, 51794], "temperature": 0.0, "avg_logprob": -0.13571478271484375, "compression_ratio": 1.697080291970803, "no_speech_prob": 0.00028079727781005204}, {"id": 1672, "seek": 510494, "start": 5104.94, "end": 5108.7, "text": " which hopefully will clarify how we do this.", "tokens": [50364, 597, 4696, 486, 17594, 577, 321, 360, 341, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1673, "seek": 510494, "start": 5108.7, "end": 5111.099999999999, "text": " So in this case, let's say that we want to adjust", "tokens": [50552, 407, 294, 341, 1389, 11, 718, 311, 584, 300, 321, 528, 281, 4369, 50672], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1674, "seek": 510494, "start": 5111.099999999999, "end": 5113.7, "text": " the March CIS concentration forecast,", "tokens": [50672, 264, 6129, 383, 2343, 9856, 14330, 11, 50802], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1675, "seek": 510494, "start": 5113.7, "end": 5117.0599999999995, "text": " which is initializing February of a year Y.", "tokens": [50802, 597, 307, 5883, 3319, 8711, 295, 257, 1064, 398, 13, 50970], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1676, "seek": 510494, "start": 5117.0599999999995, "end": 5120.5, "text": " So essentially it will be this red dot denoted here", "tokens": [50970, 407, 4476, 309, 486, 312, 341, 2182, 5893, 1441, 23325, 510, 51142], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1677, "seek": 510494, "start": 5120.5, "end": 5121.7, "text": " on the figure.", "tokens": [51142, 322, 264, 2573, 13, 51202], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1678, "seek": 510494, "start": 5121.7, "end": 5125.9, "text": " So again, what we want to do is to post-process this forecast.", "tokens": [51202, 407, 797, 11, 437, 321, 528, 281, 360, 307, 281, 2183, 12, 41075, 341, 14330, 13, 51412], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1679, "seek": 510494, "start": 5125.9, "end": 5127.74, "text": " We leverage the forecast that are produced", "tokens": [51412, 492, 13982, 264, 14330, 300, 366, 7126, 51504], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1680, "seek": 510494, "start": 5127.74, "end": 5128.9, "text": " with our climate model.", "tokens": [51504, 365, 527, 5659, 2316, 13, 51562], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1681, "seek": 510494, "start": 5128.9, "end": 5130.62, "text": " We do not do prediction.", "tokens": [51562, 492, 360, 406, 360, 17630, 13, 51648], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1682, "seek": 510494, "start": 5130.62, "end": 5133.0599999999995, "text": " We just do that kind of bias adjustment", "tokens": [51648, 492, 445, 360, 300, 733, 295, 12577, 17132, 51770], "temperature": 0.0, "avg_logprob": -0.11786611080169677, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.0002522662398405373}, {"id": 1683, "seek": 513306, "start": 5133.06, "end": 5134.9800000000005, "text": " or post-processing of the predictions", "tokens": [50364, 420, 2183, 12, 41075, 278, 295, 264, 21264, 50460], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1684, "seek": 513306, "start": 5134.9800000000005, "end": 5136.9800000000005, "text": " that we get from our climate model.", "tokens": [50460, 300, 321, 483, 490, 527, 5659, 2316, 13, 50560], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1685, "seek": 513306, "start": 5136.9800000000005, "end": 5141.3, "text": " So for this specific task, to correct that March CIS,", "tokens": [50560, 407, 337, 341, 2685, 5633, 11, 281, 3006, 300, 6129, 383, 2343, 11, 50776], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1686, "seek": 513306, "start": 5141.3, "end": 5146.18, "text": " we train on the data that is available for the years", "tokens": [50776, 321, 3847, 322, 264, 1412, 300, 307, 2435, 337, 264, 924, 51020], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1687, "seek": 513306, "start": 5146.18, "end": 5148.900000000001, "text": " before the test year that we have.", "tokens": [51020, 949, 264, 1500, 1064, 300, 321, 362, 13, 51156], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1688, "seek": 513306, "start": 5148.900000000001, "end": 5150.700000000001, "text": " So in this case, I'm denoting that here", "tokens": [51156, 407, 294, 341, 1389, 11, 286, 478, 1441, 17001, 300, 510, 51246], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1689, "seek": 513306, "start": 5150.700000000001, "end": 5153.02, "text": " for this in this shadow region.", "tokens": [51246, 337, 341, 294, 341, 8576, 4458, 13, 51362], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1690, "seek": 513306, "start": 5153.02, "end": 5157.26, "text": " So we take all the pairs of forecast and observations", "tokens": [51362, 407, 321, 747, 439, 264, 15494, 295, 14330, 293, 18163, 51574], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1691, "seek": 513306, "start": 5157.26, "end": 5159.9800000000005, "text": " for all lead times and all target month,", "tokens": [51574, 337, 439, 1477, 1413, 293, 439, 3779, 1618, 11, 51710], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1692, "seek": 513306, "start": 5159.9800000000005, "end": 5162.46, "text": " and then we train our network on that data,", "tokens": [51710, 293, 550, 321, 3847, 527, 3209, 322, 300, 1412, 11, 51834], "temperature": 0.0, "avg_logprob": -0.10891582619430673, "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.00037281197728589177}, {"id": 1693, "seek": 516246, "start": 5162.46, "end": 5165.26, "text": " and then we will do that iteratively for every test", "tokens": [50364, 293, 550, 321, 486, 360, 300, 17138, 19020, 337, 633, 1500, 50504], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1694, "seek": 516246, "start": 5165.26, "end": 5170.06, "text": " years that we want to make the adjustment.", "tokens": [50504, 924, 300, 321, 528, 281, 652, 264, 17132, 13, 50744], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1695, "seek": 516246, "start": 5170.06, "end": 5172.78, "text": " So what I'm going to do now is to show you some of the results", "tokens": [50744, 407, 437, 286, 478, 516, 281, 360, 586, 307, 281, 855, 291, 512, 295, 264, 3542, 50880], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1696, "seek": 516246, "start": 5172.78, "end": 5173.78, "text": " that we have.", "tokens": [50880, 300, 321, 362, 13, 50930], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1697, "seek": 516246, "start": 5173.78, "end": 5178.9, "text": " I should say that this is very much a work in progress.", "tokens": [50930, 286, 820, 584, 300, 341, 307, 588, 709, 257, 589, 294, 4205, 13, 51186], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1698, "seek": 516246, "start": 5178.9, "end": 5183.58, "text": " And that is different avenues that we are working on,", "tokens": [51186, 400, 300, 307, 819, 43039, 300, 321, 366, 1364, 322, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1699, "seek": 516246, "start": 5183.58, "end": 5187.14, "text": " but we have some results that I want to share with you.", "tokens": [51420, 457, 321, 362, 512, 3542, 300, 286, 528, 281, 2073, 365, 291, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1700, "seek": 516246, "start": 5187.14, "end": 5191.06, "text": " For instance, what we see here is the CIS concentration bias.", "tokens": [51598, 1171, 5197, 11, 437, 321, 536, 510, 307, 264, 383, 2343, 9856, 12577, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1339414141593723, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.00017023304826579988}, {"id": 1701, "seek": 519106, "start": 5191.06, "end": 5195.9400000000005, "text": " At the zero-month lead, average over the 2006 and 2020 time", "tokens": [50364, 1711, 264, 4018, 12, 23534, 1477, 11, 4274, 670, 264, 14062, 293, 4808, 565, 50608], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1702, "seek": 519106, "start": 5195.9400000000005, "end": 5201.18, "text": " period, which is the test years that we have for our analysis.", "tokens": [50608, 2896, 11, 597, 307, 264, 1500, 924, 300, 321, 362, 337, 527, 5215, 13, 50870], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1703, "seek": 519106, "start": 5201.18, "end": 5205.740000000001, "text": " At the top, we have the bias for the March CIS concentration,", "tokens": [50870, 1711, 264, 1192, 11, 321, 362, 264, 12577, 337, 264, 6129, 383, 2343, 9856, 11, 51098], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1704, "seek": 519106, "start": 5205.740000000001, "end": 5209.900000000001, "text": " which is at the time of maximum CIS extent.", "tokens": [51098, 597, 307, 412, 264, 565, 295, 6674, 383, 2343, 8396, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1705, "seek": 519106, "start": 5209.900000000001, "end": 5212.740000000001, "text": " And at the bottom, we have the results or the bias", "tokens": [51306, 400, 412, 264, 2767, 11, 321, 362, 264, 3542, 420, 264, 12577, 51448], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1706, "seek": 519106, "start": 5212.740000000001, "end": 5215.780000000001, "text": " for the September CIS concentration,", "tokens": [51448, 337, 264, 7216, 383, 2343, 9856, 11, 51600], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1707, "seek": 519106, "start": 5215.780000000001, "end": 5219.14, "text": " which is at the time of a minimum CIS extent.", "tokens": [51600, 597, 307, 412, 264, 565, 295, 257, 7285, 383, 2343, 8396, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1474759319041035, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0010946617694571614}, {"id": 1708, "seek": 521914, "start": 5219.18, "end": 5222.3, "text": " On the left, we see here what happened with the raw forecast.", "tokens": [50366, 1282, 264, 1411, 11, 321, 536, 510, 437, 2011, 365, 264, 8936, 14330, 13, 50522], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1709, "seek": 521914, "start": 5222.3, "end": 5226.700000000001, "text": " We see the biases happening in several regions,", "tokens": [50522, 492, 536, 264, 32152, 2737, 294, 2940, 10682, 11, 50742], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1710, "seek": 521914, "start": 5226.700000000001, "end": 5228.02, "text": " which is actually substantial.", "tokens": [50742, 597, 307, 767, 16726, 13, 50808], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1711, "seek": 521914, "start": 5228.02, "end": 5230.46, "text": " Here, the bias is given in percent.", "tokens": [50808, 1692, 11, 264, 12577, 307, 2212, 294, 3043, 13, 50930], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1712, "seek": 521914, "start": 5230.46, "end": 5232.06, "text": " For the bias adjusted case, which", "tokens": [50930, 1171, 264, 12577, 19871, 1389, 11, 597, 51010], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1713, "seek": 521914, "start": 5232.06, "end": 5235.1, "text": " is the simple bias correction that I'm using here", "tokens": [51010, 307, 264, 2199, 12577, 19984, 300, 286, 478, 1228, 510, 51162], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1714, "seek": 521914, "start": 5235.1, "end": 5237.42, "text": " as a benchmark, we see that we do improve", "tokens": [51162, 382, 257, 18927, 11, 321, 536, 300, 321, 360, 3470, 51278], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1715, "seek": 521914, "start": 5237.42, "end": 5239.42, "text": " relative to the raw forecast, but there are still", "tokens": [51278, 4972, 281, 264, 8936, 14330, 11, 457, 456, 366, 920, 51378], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1716, "seek": 521914, "start": 5239.42, "end": 5242.18, "text": " some biases that are apparent, particularly", "tokens": [51378, 512, 32152, 300, 366, 18335, 11, 4098, 51516], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1717, "seek": 521914, "start": 5242.18, "end": 5244.38, "text": " during the CIS minimum.", "tokens": [51516, 1830, 264, 383, 2343, 7285, 13, 51626], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1718, "seek": 521914, "start": 5244.38, "end": 5246.62, "text": " And then on the right is the results", "tokens": [51626, 400, 550, 322, 264, 558, 307, 264, 3542, 51738], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1719, "seek": 521914, "start": 5246.62, "end": 5247.860000000001, "text": " that we get with the unit.", "tokens": [51738, 300, 321, 483, 365, 264, 4985, 13, 51800], "temperature": 0.0, "avg_logprob": -0.12718649208545685, "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0004938932252116501}, {"id": 1720, "seek": 524786, "start": 5247.86, "end": 5251.66, "text": " And we see that the bias are likely removed.", "tokens": [50364, 400, 321, 536, 300, 264, 12577, 366, 3700, 7261, 13, 50554], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1721, "seek": 524786, "start": 5251.66, "end": 5253.62, "text": " And now, this is for the zero-month lead.", "tokens": [50554, 400, 586, 11, 341, 307, 337, 264, 4018, 12, 23534, 1477, 13, 50652], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1722, "seek": 524786, "start": 5253.62, "end": 5257.7, "text": " So this is soon after we initialize our forecast.", "tokens": [50652, 407, 341, 307, 2321, 934, 321, 5883, 1125, 527, 14330, 13, 50856], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1723, "seek": 524786, "start": 5257.7, "end": 5259.82, "text": " Now, two months ahead in our forecast,", "tokens": [50856, 823, 11, 732, 2493, 2286, 294, 527, 14330, 11, 50962], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1724, "seek": 524786, "start": 5259.82, "end": 5262.299999999999, "text": " of course, the biases are increased.", "tokens": [50962, 295, 1164, 11, 264, 32152, 366, 6505, 13, 51086], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1725, "seek": 524786, "start": 5262.299999999999, "end": 5265.0199999999995, "text": " Excuse me, we have two minutes.", "tokens": [51086, 11359, 385, 11, 321, 362, 732, 2077, 13, 51222], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1726, "seek": 524786, "start": 5265.0199999999995, "end": 5265.54, "text": " Two minutes.", "tokens": [51222, 4453, 2077, 13, 51248], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1727, "seek": 524786, "start": 5265.54, "end": 5267.46, "text": " Thank you.", "tokens": [51248, 1044, 291, 13, 51344], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1728, "seek": 524786, "start": 5267.46, "end": 5269.0599999999995, "text": " All right, so here is the case in which we", "tokens": [51344, 1057, 558, 11, 370, 510, 307, 264, 1389, 294, 597, 321, 51424], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1729, "seek": 524786, "start": 5269.0599999999995, "end": 5270.58, "text": " have two-month lead forecasts.", "tokens": [51424, 362, 732, 12, 23534, 1477, 49421, 13, 51500], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1730, "seek": 524786, "start": 5270.58, "end": 5272.46, "text": " And we see that the bias, of course, are increased.", "tokens": [51500, 400, 321, 536, 300, 264, 12577, 11, 295, 1164, 11, 366, 6505, 13, 51594], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1731, "seek": 524786, "start": 5272.46, "end": 5275.66, "text": " But still, we are able to manage those biases with the unit,", "tokens": [51594, 583, 920, 11, 321, 366, 1075, 281, 3067, 729, 32152, 365, 264, 4985, 11, 51754], "temperature": 0.0, "avg_logprob": -0.20595176131637008, "compression_ratio": 1.8958333333333333, "no_speech_prob": 0.0019782038871198893}, {"id": 1732, "seek": 527566, "start": 5275.7, "end": 5281.18, "text": " given a better representation of the CIS edges.", "tokens": [50366, 2212, 257, 1101, 10290, 295, 264, 383, 2343, 8819, 13, 50640], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1733, "seek": 527566, "start": 5281.18, "end": 5283.0599999999995, "text": " Another measure that we look at here", "tokens": [50640, 3996, 3481, 300, 321, 574, 412, 510, 50734], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1734, "seek": 527566, "start": 5283.0599999999995, "end": 5287.46, "text": " is the integrated ICH error, which is essentially the area,", "tokens": [50734, 307, 264, 10919, 286, 5462, 6713, 11, 597, 307, 4476, 264, 1859, 11, 50954], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1735, "seek": 527566, "start": 5287.46, "end": 5289.22, "text": " the integrated area that we have,", "tokens": [50954, 264, 10919, 1859, 300, 321, 362, 11, 51042], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1736, "seek": 527566, "start": 5289.22, "end": 5292.46, "text": " in which both forecast and observations", "tokens": [51042, 294, 597, 1293, 14330, 293, 18163, 51204], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1737, "seek": 527566, "start": 5292.46, "end": 5296.38, "text": " disagree in the concentration with a threshold of 15%.", "tokens": [51204, 14091, 294, 264, 9856, 365, 257, 14678, 295, 2119, 6856, 51400], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1738, "seek": 527566, "start": 5296.38, "end": 5301.0199999999995, "text": " So we both have more than 50% concentration", "tokens": [51400, 407, 321, 1293, 362, 544, 813, 2625, 4, 9856, 51632], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1739, "seek": 527566, "start": 5301.0199999999995, "end": 5303.139999999999, "text": " or less than 50%.", "tokens": [51632, 420, 1570, 813, 2625, 6856, 51738], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1740, "seek": 527566, "start": 5303.139999999999, "end": 5304.86, "text": " This binary error will be zero.", "tokens": [51738, 639, 17434, 6713, 486, 312, 4018, 13, 51824], "temperature": 0.0, "avg_logprob": -0.18525464484032164, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.0005010798922739923}, {"id": 1741, "seek": 530486, "start": 5304.9, "end": 5307.0599999999995, "text": " If they are different, then the binary error will be one.", "tokens": [50366, 759, 436, 366, 819, 11, 550, 264, 17434, 6713, 486, 312, 472, 13, 50474], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1742, "seek": 530486, "start": 5307.0599999999995, "end": 5312.139999999999, "text": " And those grid cells will contribute to this area error.", "tokens": [50474, 400, 729, 10748, 5438, 486, 10586, 281, 341, 1859, 6713, 13, 50728], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1743, "seek": 530486, "start": 5312.139999999999, "end": 5315.139999999999, "text": " So this is at the top here, we see a hit map", "tokens": [50728, 407, 341, 307, 412, 264, 1192, 510, 11, 321, 536, 257, 2045, 4471, 50878], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1744, "seek": 530486, "start": 5315.139999999999, "end": 5318.54, "text": " in which we have our target month in the X axis.", "tokens": [50878, 294, 597, 321, 362, 527, 3779, 1618, 294, 264, 1783, 10298, 13, 51048], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1745, "seek": 530486, "start": 5318.54, "end": 5320.94, "text": " And on the Y, we have the lead month.", "tokens": [51048, 400, 322, 264, 398, 11, 321, 362, 264, 1477, 1618, 13, 51168], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1746, "seek": 530486, "start": 5320.94, "end": 5324.86, "text": " And bottom message here is that the blue are bad.", "tokens": [51168, 400, 2767, 3636, 510, 307, 300, 264, 3344, 366, 1578, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1747, "seek": 530486, "start": 5324.86, "end": 5328.7, "text": " The red are good, meaning there is less error.", "tokens": [51364, 440, 2182, 366, 665, 11, 3620, 456, 307, 1570, 6713, 13, 51556], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1748, "seek": 530486, "start": 5328.7, "end": 5332.139999999999, "text": " And the unit bids both the row, of course,", "tokens": [51556, 400, 264, 4985, 272, 3742, 1293, 264, 5386, 11, 295, 1164, 11, 51728], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1749, "seek": 530486, "start": 5332.139999999999, "end": 5334.5, "text": " and the bias are used to forecast.", "tokens": [51728, 293, 264, 12577, 366, 1143, 281, 14330, 13, 51846], "temperature": 0.0, "avg_logprob": -0.18817504507596375, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.00018774092313833535}, {"id": 1750, "seek": 533450, "start": 5334.5, "end": 5336.74, "text": " Down here is just an integration over lead month", "tokens": [50364, 9506, 510, 307, 445, 364, 10980, 670, 1477, 1618, 50476], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1751, "seek": 533450, "start": 5336.74, "end": 5338.58, "text": " just to give a more clear picture", "tokens": [50476, 445, 281, 976, 257, 544, 1850, 3036, 50568], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1752, "seek": 533450, "start": 5338.58, "end": 5342.14, "text": " of how the unit outperforms the alternative.", "tokens": [50568, 295, 577, 264, 4985, 484, 26765, 82, 264, 8535, 13, 50746], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1753, "seek": 533450, "start": 5343.18, "end": 5344.74, "text": " Now, I know that I don't have much time,", "tokens": [50798, 823, 11, 286, 458, 300, 286, 500, 380, 362, 709, 565, 11, 50876], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1754, "seek": 533450, "start": 5344.74, "end": 5349.06, "text": " so I will not go into the details of these results,", "tokens": [50876, 370, 286, 486, 406, 352, 666, 264, 4365, 295, 613, 3542, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1755, "seek": 533450, "start": 5349.06, "end": 5351.46, "text": " but at least to give you an idea of what is happening.", "tokens": [51092, 457, 412, 1935, 281, 976, 291, 364, 1558, 295, 437, 307, 2737, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1756, "seek": 533450, "start": 5351.46, "end": 5354.62, "text": " In this case, we are looking at the CIS area.", "tokens": [51212, 682, 341, 1389, 11, 321, 366, 1237, 412, 264, 383, 2343, 1859, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1757, "seek": 533450, "start": 5354.62, "end": 5355.78, "text": " Same time of floods.", "tokens": [51370, 10635, 565, 295, 35536, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1758, "seek": 533450, "start": 5355.78, "end": 5360.38, "text": " Again, red means that we have a better room mean", "tokens": [51428, 3764, 11, 2182, 1355, 300, 321, 362, 257, 1101, 1808, 914, 51658], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1759, "seek": 533450, "start": 5360.38, "end": 5361.9, "text": " and square error, which is the measure", "tokens": [51658, 293, 3732, 6713, 11, 597, 307, 264, 3481, 51734], "temperature": 0.0, "avg_logprob": -0.1549613983904729, "compression_ratio": 1.6104868913857677, "no_speech_prob": 0.00022978262859396636}, {"id": 1760, "seek": 536190, "start": 5361.98, "end": 5364.82, "text": " that we are using here for the CIS area.", "tokens": [50368, 300, 321, 366, 1228, 510, 337, 264, 383, 2343, 1859, 13, 50510], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1761, "seek": 536190, "start": 5364.82, "end": 5367.099999999999, "text": " Blue means that the errors are increased.", "tokens": [50510, 8510, 1355, 300, 264, 13603, 366, 6505, 13, 50624], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1762, "seek": 536190, "start": 5367.099999999999, "end": 5369.66, "text": " And in this particular case, we see that the unit", "tokens": [50624, 400, 294, 341, 1729, 1389, 11, 321, 536, 300, 264, 4985, 50752], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1763, "seek": 536190, "start": 5369.66, "end": 5373.42, "text": " is slightly better than the bias corrected one.", "tokens": [50752, 307, 4748, 1101, 813, 264, 12577, 31687, 472, 13, 50940], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1764, "seek": 536190, "start": 5373.42, "end": 5375.9, "text": " However, if we look at CIS extent,", "tokens": [50940, 2908, 11, 498, 321, 574, 412, 383, 2343, 8396, 11, 51064], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1765, "seek": 536190, "start": 5375.9, "end": 5380.259999999999, "text": " which is now the area in which we count for all those grid cells", "tokens": [51064, 597, 307, 586, 264, 1859, 294, 597, 321, 1207, 337, 439, 729, 10748, 5438, 51282], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1766, "seek": 536190, "start": 5380.259999999999, "end": 5382.62, "text": " with concentration greater than 50%,", "tokens": [51282, 365, 9856, 5044, 813, 2625, 8923, 51400], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1767, "seek": 536190, "start": 5382.62, "end": 5385.139999999999, "text": " we see that unit largely outperform", "tokens": [51400, 321, 536, 300, 4985, 11611, 484, 26765, 51526], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1768, "seek": 536190, "start": 5385.139999999999, "end": 5386.58, "text": " this bias-adjusted method.", "tokens": [51526, 341, 12577, 12, 345, 3424, 292, 3170, 13, 51598], "temperature": 0.0, "avg_logprob": -0.16387712038480318, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00019387992506381124}, {"id": 1769, "seek": 538658, "start": 5387.58, "end": 5392.74, "text": " Finally, I want just to mention that here,", "tokens": [50414, 6288, 11, 286, 528, 445, 281, 2152, 300, 510, 11, 50672], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1770, "seek": 538658, "start": 5392.74, "end": 5395.7, "text": " we have seen different measures of the skill", "tokens": [50672, 321, 362, 1612, 819, 8000, 295, 264, 5389, 50820], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1771, "seek": 538658, "start": 5395.7, "end": 5398.98, "text": " in which we outperform both the benchmark", "tokens": [50820, 294, 597, 321, 484, 26765, 1293, 264, 18927, 50984], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1772, "seek": 538658, "start": 5398.98, "end": 5401.74, "text": " and the raw forecast, but there is still some issues", "tokens": [50984, 293, 264, 8936, 14330, 11, 457, 456, 307, 920, 512, 2663, 51122], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1773, "seek": 538658, "start": 5401.74, "end": 5404.98, "text": " in terms of the representation of the temporal dependence", "tokens": [51122, 294, 2115, 295, 264, 10290, 295, 264, 30881, 31704, 51284], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1774, "seek": 538658, "start": 5404.98, "end": 5406.5, "text": " of our adjusted forecast.", "tokens": [51284, 295, 527, 19871, 14330, 13, 51360], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1775, "seek": 538658, "start": 5406.5, "end": 5410.18, "text": " And so, but unlike for this slide here,", "tokens": [51360, 400, 370, 11, 457, 8343, 337, 341, 4137, 510, 11, 51544], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1776, "seek": 538658, "start": 5410.18, "end": 5412.86, "text": " just to mention that we still have some work to do", "tokens": [51544, 445, 281, 2152, 300, 321, 920, 362, 512, 589, 281, 360, 51678], "temperature": 0.0, "avg_logprob": -0.20816235012478299, "compression_ratio": 1.6839622641509433, "no_speech_prob": 0.0002606541384011507}, {"id": 1777, "seek": 541286, "start": 5412.86, "end": 5417.38, "text": " to be able to better represent the temporal dependence", "tokens": [50364, 281, 312, 1075, 281, 1101, 2906, 264, 30881, 31704, 50590], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1778, "seek": 541286, "start": 5417.38, "end": 5419.219999999999, "text": " of those forecasts.", "tokens": [50590, 295, 729, 49421, 13, 50682], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1779, "seek": 541286, "start": 5419.219999999999, "end": 5421.0599999999995, "text": " And because I don't have much more time,", "tokens": [50682, 400, 570, 286, 500, 380, 362, 709, 544, 565, 11, 50774], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1780, "seek": 541286, "start": 5421.0599999999995, "end": 5424.299999999999, "text": " I will finish with this slide, which is some final remarks.", "tokens": [50774, 286, 486, 2413, 365, 341, 4137, 11, 597, 307, 512, 2572, 19151, 13, 50936], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1781, "seek": 541286, "start": 5424.299999999999, "end": 5425.9, "text": " We'll leave it there for you.", "tokens": [50936, 492, 603, 1856, 309, 456, 337, 291, 13, 51016], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1782, "seek": 541286, "start": 5425.9, "end": 5428.94, "text": " And I will be happy to take any questions.", "tokens": [51016, 400, 286, 486, 312, 2055, 281, 747, 604, 1651, 13, 51168], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1783, "seek": 541286, "start": 5428.94, "end": 5429.78, "text": " Thank you.", "tokens": [51168, 1044, 291, 13, 51210], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1784, "seek": 541286, "start": 5431.62, "end": 5432.94, "text": " Thank you, Renel.", "tokens": [51302, 1044, 291, 11, 12883, 338, 13, 51368], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1785, "seek": 541286, "start": 5432.94, "end": 5437.94, "text": " I saw that there seems to be to have a seasonal pattern", "tokens": [51368, 286, 1866, 300, 456, 2544, 281, 312, 281, 362, 257, 27421, 5102, 51618], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1786, "seek": 541286, "start": 5438.259999999999, "end": 5439.78, "text": " in your verification.", "tokens": [51634, 294, 428, 30206, 13, 51710], "temperature": 0.0, "avg_logprob": -0.17114607178338684, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0029128415044397116}, {"id": 1787, "seek": 543978, "start": 5439.78, "end": 5442.98, "text": " That leads me to the questions.", "tokens": [50364, 663, 6689, 385, 281, 264, 1651, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1788, "seek": 543978, "start": 5442.98, "end": 5447.179999999999, "text": " Are there any biases that are harder to adjust", "tokens": [50524, 2014, 456, 604, 32152, 300, 366, 6081, 281, 4369, 50734], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1789, "seek": 543978, "start": 5447.179999999999, "end": 5448.42, "text": " with this method?", "tokens": [50734, 365, 341, 3170, 30, 50796], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1790, "seek": 543978, "start": 5450.42, "end": 5452.34, "text": " The answer for that, yes.", "tokens": [50896, 440, 1867, 337, 300, 11, 2086, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1791, "seek": 543978, "start": 5452.34, "end": 5456.219999999999, "text": " And this is anality that we see has to do in part", "tokens": [50992, 400, 341, 307, 364, 1860, 300, 321, 536, 575, 281, 360, 294, 644, 51186], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1792, "seek": 543978, "start": 5456.219999999999, "end": 5458.0199999999995, "text": " with how the eyes behave.", "tokens": [51186, 365, 577, 264, 2575, 15158, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1793, "seek": 543978, "start": 5458.0199999999995, "end": 5461.099999999999, "text": " We have what is known as this predictability barrier", "tokens": [51276, 492, 362, 437, 307, 2570, 382, 341, 6069, 2310, 13357, 51430], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1794, "seek": 543978, "start": 5461.099999999999, "end": 5463.179999999999, "text": " after spring, which makes it difficult", "tokens": [51430, 934, 5587, 11, 597, 1669, 309, 2252, 51534], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1795, "seek": 543978, "start": 5463.179999999999, "end": 5465.74, "text": " to do a good prediction of the sea eyes.", "tokens": [51534, 281, 360, 257, 665, 17630, 295, 264, 4158, 2575, 13, 51662], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1796, "seek": 543978, "start": 5465.74, "end": 5467.58, "text": " This is something that perhaps we can see", "tokens": [51662, 639, 307, 746, 300, 4317, 321, 393, 536, 51754], "temperature": 0.0, "avg_logprob": -0.1698397479011017, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.0023529506288468838}, {"id": 1797, "seek": 546758, "start": 5467.7, "end": 5471.38, "text": " all those metrics, actually perhaps we'll go to this one.", "tokens": [50370, 439, 729, 16367, 11, 767, 4317, 321, 603, 352, 281, 341, 472, 13, 50554], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1798, "seek": 546758, "start": 5471.38, "end": 5475.26, "text": " So this barrier, which reduces the skill", "tokens": [50554, 407, 341, 13357, 11, 597, 18081, 264, 5389, 50748], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1799, "seek": 546758, "start": 5475.26, "end": 5476.62, "text": " that we have in our predictions,", "tokens": [50748, 300, 321, 362, 294, 527, 21264, 11, 50816], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1800, "seek": 546758, "start": 5476.62, "end": 5480.74, "text": " can be seen here around the month of June to October,", "tokens": [50816, 393, 312, 1612, 510, 926, 264, 1618, 295, 6928, 281, 7617, 11, 51022], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1801, "seek": 546758, "start": 5480.74, "end": 5482.54, "text": " looking here at target month.", "tokens": [51022, 1237, 510, 412, 3779, 1618, 13, 51112], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1802, "seek": 546758, "start": 5482.54, "end": 5486.26, "text": " And so that is something within the raw forecast", "tokens": [51112, 400, 370, 300, 307, 746, 1951, 264, 8936, 14330, 51298], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1803, "seek": 546758, "start": 5486.26, "end": 5489.74, "text": " in person, the bias corrected one, but also in the unit.", "tokens": [51298, 294, 954, 11, 264, 12577, 31687, 472, 11, 457, 611, 294, 264, 4985, 13, 51472], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1804, "seek": 546758, "start": 5489.74, "end": 5492.1, "text": " So we do improve on those months,", "tokens": [51472, 407, 321, 360, 3470, 322, 729, 2493, 11, 51590], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1805, "seek": 546758, "start": 5492.1, "end": 5495.0599999999995, "text": " but still there is some skill that is missing there,", "tokens": [51590, 457, 920, 456, 307, 512, 5389, 300, 307, 5361, 456, 11, 51738], "temperature": 0.0, "avg_logprob": -0.16645987828572592, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.004596473183482885}, {"id": 1806, "seek": 549506, "start": 5495.06, "end": 5497.14, "text": " which is part of the natural process", "tokens": [50364, 597, 307, 644, 295, 264, 3303, 1399, 50468], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1807, "seek": 549506, "start": 5497.14, "end": 5499.18, "text": " in the sea eyes formation and melting,", "tokens": [50468, 294, 264, 4158, 2575, 11723, 293, 20493, 11, 50570], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1808, "seek": 549506, "start": 5499.18, "end": 5500.700000000001, "text": " which then translate into the skill", "tokens": [50570, 597, 550, 13799, 666, 264, 5389, 50646], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1809, "seek": 549506, "start": 5500.700000000001, "end": 5502.54, "text": " that we see in our just forecast.", "tokens": [50646, 300, 321, 536, 294, 527, 445, 14330, 13, 50738], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1810, "seek": 549506, "start": 5503.700000000001, "end": 5505.02, "text": " Thank you.", "tokens": [50796, 1044, 291, 13, 50862], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1811, "seek": 549506, "start": 5505.02, "end": 5509.620000000001, "text": " One last question is coming from the online Q&A.", "tokens": [50862, 1485, 1036, 1168, 307, 1348, 490, 264, 2950, 1249, 5, 32, 13, 51092], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1812, "seek": 549506, "start": 5510.580000000001, "end": 5513.3, "text": " Would training the model on all months of the year", "tokens": [51140, 6068, 3097, 264, 2316, 322, 439, 2493, 295, 264, 1064, 51276], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1813, "seek": 549506, "start": 5513.3, "end": 5515.820000000001, "text": " versus only certain months affect its ability", "tokens": [51276, 5717, 787, 1629, 2493, 3345, 1080, 3485, 51402], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1814, "seek": 549506, "start": 5515.820000000001, "end": 5516.900000000001, "text": " to improve the forecast?", "tokens": [51402, 281, 3470, 264, 14330, 30, 51456], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1815, "seek": 549506, "start": 5516.900000000001, "end": 5520.06, "text": " For example, forecasts of sea eyes in spring", "tokens": [51456, 1171, 1365, 11, 49421, 295, 4158, 2575, 294, 5587, 51614], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1816, "seek": 549506, "start": 5520.06, "end": 5521.620000000001, "text": " are known to perform poorly.", "tokens": [51614, 366, 2570, 281, 2042, 22271, 13, 51692], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1817, "seek": 549506, "start": 5521.620000000001, "end": 5523.700000000001, "text": " So if you were to exclude these months,", "tokens": [51692, 407, 498, 291, 645, 281, 33536, 613, 2493, 11, 51796], "temperature": 0.0, "avg_logprob": -0.18146115335924873, "compression_ratio": 1.6455223880597014, "no_speech_prob": 0.006231479812413454}, {"id": 1818, "seek": 552370, "start": 5524.22, "end": 5527.34, "text": " would this improve the bias adjustment?", "tokens": [50390, 576, 341, 3470, 264, 12577, 17132, 30, 50546], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1819, "seek": 552370, "start": 5527.34, "end": 5529.26, "text": " Yeah, that's a good question.", "tokens": [50546, 865, 11, 300, 311, 257, 665, 1168, 13, 50642], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1820, "seek": 552370, "start": 5529.26, "end": 5531.099999999999, "text": " Actually, when I was, I mean, it's too bad", "tokens": [50642, 5135, 11, 562, 286, 390, 11, 286, 914, 11, 309, 311, 886, 1578, 50734], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1821, "seek": 552370, "start": 5531.099999999999, "end": 5532.66, "text": " that I'm rushing through all these slides,", "tokens": [50734, 300, 286, 478, 25876, 807, 439, 613, 9788, 11, 50812], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1822, "seek": 552370, "start": 5532.66, "end": 5534.5, "text": " but one of the things that I wanted to mention", "tokens": [50812, 457, 472, 295, 264, 721, 300, 286, 1415, 281, 2152, 50904], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1823, "seek": 552370, "start": 5534.5, "end": 5538.139999999999, "text": " at this particular slide is that we do the training.", "tokens": [50904, 412, 341, 1729, 4137, 307, 300, 321, 360, 264, 3097, 13, 51086], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1824, "seek": 552370, "start": 5538.139999999999, "end": 5541.139999999999, "text": " You're looking at the previous years in my forecast,", "tokens": [51086, 509, 434, 1237, 412, 264, 3894, 924, 294, 452, 14330, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1825, "seek": 552370, "start": 5541.139999999999, "end": 5542.82, "text": " but we are missing some of the information", "tokens": [51236, 457, 321, 366, 5361, 512, 295, 264, 1589, 51320], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1826, "seek": 552370, "start": 5542.82, "end": 5546.46, "text": " of more recent months that can also contribute", "tokens": [51320, 295, 544, 5162, 2493, 300, 393, 611, 10586, 51502], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1827, "seek": 552370, "start": 5546.46, "end": 5547.3, "text": " to the forecast.", "tokens": [51502, 281, 264, 14330, 13, 51544], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1828, "seek": 552370, "start": 5547.3, "end": 5549.0599999999995, "text": " Like looking here, for instance,", "tokens": [51544, 1743, 1237, 510, 11, 337, 5197, 11, 51632], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1829, "seek": 552370, "start": 5549.0599999999995, "end": 5551.74, "text": " we do not use information from January", "tokens": [51632, 321, 360, 406, 764, 1589, 490, 7061, 51766], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1830, "seek": 552370, "start": 5551.74, "end": 5553.5, "text": " of this specific year.", "tokens": [51766, 295, 341, 2685, 1064, 13, 51854], "temperature": 0.0, "avg_logprob": -0.1380438383887796, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.002136741066351533}, {"id": 1831, "seek": 555350, "start": 5553.5, "end": 5555.1, "text": " Now, one thing that we are exploring", "tokens": [50364, 823, 11, 472, 551, 300, 321, 366, 12736, 50444], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1832, "seek": 555350, "start": 5555.1, "end": 5558.06, "text": " is to look at training based on lead times", "tokens": [50444, 307, 281, 574, 412, 3097, 2361, 322, 1477, 1413, 50592], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1833, "seek": 555350, "start": 5558.06, "end": 5559.06, "text": " or a specific month.", "tokens": [50592, 420, 257, 2685, 1618, 13, 50642], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1834, "seek": 555350, "start": 5559.06, "end": 5561.62, "text": " I think that that's the idea of the question,", "tokens": [50642, 286, 519, 300, 300, 311, 264, 1558, 295, 264, 1168, 11, 50770], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1835, "seek": 555350, "start": 5561.62, "end": 5565.82, "text": " which we try to train our model specific to the lead times", "tokens": [50770, 597, 321, 853, 281, 3847, 527, 2316, 2685, 281, 264, 1477, 1413, 50980], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1836, "seek": 555350, "start": 5565.82, "end": 5568.54, "text": " in which we have the biases that we want to correct", "tokens": [50980, 294, 597, 321, 362, 264, 32152, 300, 321, 528, 281, 3006, 51116], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1837, "seek": 555350, "start": 5568.54, "end": 5573.02, "text": " and the kind of information specific to the seasonality", "tokens": [51116, 293, 264, 733, 295, 1589, 2685, 281, 264, 3196, 1860, 51340], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1838, "seek": 555350, "start": 5573.02, "end": 5574.7, "text": " that we want also to correct.", "tokens": [51340, 300, 321, 528, 611, 281, 3006, 13, 51424], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1839, "seek": 555350, "start": 5574.7, "end": 5576.74, "text": " So those are things that we are exploring,", "tokens": [51424, 407, 729, 366, 721, 300, 321, 366, 12736, 11, 51526], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1840, "seek": 555350, "start": 5576.74, "end": 5579.58, "text": " but so far this is what we have.", "tokens": [51526, 457, 370, 1400, 341, 307, 437, 321, 362, 13, 51668], "temperature": 0.0, "avg_logprob": -0.14214345206201603, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0014348881086334586}, {"id": 1841, "seek": 557958, "start": 5580.58, "end": 5582.58, "text": " Thank you, R\u00e9nel.", "tokens": [50414, 1044, 291, 11, 497, 526, 6396, 13, 50514], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1842, "seek": 557958, "start": 5582.58, "end": 5587.58, "text": " This concludes our first block for the first part", "tokens": [50514, 639, 24643, 527, 700, 3461, 337, 264, 700, 644, 50764], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1843, "seek": 557958, "start": 5587.58, "end": 5590.58, "text": " of the presentation for artificial intelligence.", "tokens": [50764, 295, 264, 5860, 337, 11677, 7599, 13, 50914], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1844, "seek": 557958, "start": 5590.58, "end": 5593.58, "text": " A big thank you to R\u00e9nel, Madlena, Sa\u00efd, Herv\u00e9,", "tokens": [50914, 316, 955, 1309, 291, 281, 497, 526, 6396, 11, 5326, 75, 4118, 11, 6299, 15487, 67, 11, 389, 1978, 526, 11, 51064], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1845, "seek": 557958, "start": 5593.58, "end": 5597.58, "text": " and Christopher for having made these presentations.", "tokens": [51064, 293, 20649, 337, 1419, 1027, 613, 18964, 13, 51264], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1846, "seek": 557958, "start": 5597.58, "end": 5599.58, "text": " Anne, I'll leave you here.", "tokens": [51264, 13706, 11, 286, 603, 1856, 291, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1847, "seek": 557958, "start": 5599.58, "end": 5603.58, "text": " Yes, a big thank you to all the presenters,", "tokens": [51364, 1079, 11, 257, 955, 1309, 291, 281, 439, 264, 36987, 11, 51564], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1848, "seek": 557958, "start": 5603.58, "end": 5605.58, "text": " all the attendees as well.", "tokens": [51564, 439, 264, 34826, 382, 731, 13, 51664], "temperature": 0.0, "avg_logprob": -0.4182031330309416, "compression_ratio": 1.5560975609756098, "no_speech_prob": 0.014516562223434448}, {"id": 1849, "seek": 560558, "start": 5605.58, "end": 5608.58, "text": " And as several mentioned before,", "tokens": [50364, 400, 382, 2940, 2835, 949, 11, 50514], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1850, "seek": 560558, "start": 5608.58, "end": 5611.58, "text": " there is a second session coming up in about 20 minutes,", "tokens": [50514, 456, 307, 257, 1150, 5481, 1348, 493, 294, 466, 945, 2077, 11, 50664], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1851, "seek": 560558, "start": 5611.58, "end": 5612.58, "text": " not even.", "tokens": [50664, 406, 754, 13, 50714], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1852, "seek": 560558, "start": 5612.58, "end": 5615.58, "text": " So we'd love to see you back here,", "tokens": [50714, 407, 321, 1116, 959, 281, 536, 291, 646, 510, 11, 50864], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1853, "seek": 560558, "start": 5615.58, "end": 5620.58, "text": " a lot more to see and to hear from people outside ECCC", "tokens": [50864, 257, 688, 544, 281, 536, 293, 281, 1568, 490, 561, 2380, 462, 11717, 34, 51114], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1854, "seek": 560558, "start": 5620.58, "end": 5621.58, "text": " as well.", "tokens": [51114, 382, 731, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1855, "seek": 560558, "start": 5621.58, "end": 5622.58, "text": " So please do join us.", "tokens": [51164, 407, 1767, 360, 3917, 505, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1856, "seek": 560558, "start": 5622.58, "end": 5623.58, "text": " Thank you.", "tokens": [51214, 1044, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1857, "seek": 560558, "start": 5623.58, "end": 5624.58, "text": " There's not a sign.", "tokens": [51264, 821, 311, 406, 257, 1465, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1858, "seek": 560558, "start": 5624.58, "end": 5627.58, "text": " There's not a sign, that's it.", "tokens": [51314, 821, 311, 406, 257, 1465, 11, 300, 311, 309, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1859, "seek": 560558, "start": 5627.58, "end": 5628.58, "text": " Thanks.", "tokens": [51464, 2561, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1860, "seek": 560558, "start": 5628.58, "end": 5629.58, "text": " You're better.", "tokens": [51514, 509, 434, 1101, 13, 51564], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1861, "seek": 560558, "start": 5629.58, "end": 5630.58, "text": " Bye.", "tokens": [51564, 4621, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1862, "seek": 560558, "start": 5630.58, "end": 5631.58, "text": " Bye.", "tokens": [51614, 4621, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19150393349783762, "compression_ratio": 1.529126213592233, "no_speech_prob": 0.050782568752765656}, {"id": 1863, "seek": 563558, "start": 5635.58, "end": 5636.58, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.5255079666773478, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9605826735496521}], "language": "en"}