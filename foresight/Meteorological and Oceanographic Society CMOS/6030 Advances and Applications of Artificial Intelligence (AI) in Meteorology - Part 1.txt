Let me try for you. Okay.
Thank you.
Thank you.
Thank you.
Hello and welcome.
Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel
Tremblay. We both work for the Meteorological Service of Canada within
Environment and Climate Change Canada and we're happy to be here exploring AI
with you for a second year. Before we begin, a few reminders. 15 minutes are
reserved for each presentation and we encourage our presenters to keep three
minutes for questions at the end. Time permitting participants with questions
will be able to use the chat at the end of the presentation and their questions
can be upvoted by the other attendees. Miguel will provide up at 10 and 12
minutes and will bring the presentation to an end at 15 and will be ruthless
guys because we want everyone to have time to present. Miguel, do you want to do
a quick recap in French?
A few reminders. 15 minutes are reserved for each presentation and we encourage
our presenters to keep time for questions at the end of the presentation and we
will be able to use the chat at the end of the presentation and we will be able to
use the chat at the end of the presentation and we will be able to use the chat at
the end of the presentation and we will be able to use the chat
fpspe conditional 10 minutes for one and a half minutes for 10 minutes for
weather forecasting, and ECCC's research plans.
So without further ado, over to you, Christopher.
Thank you.
Let me get the screen sharing going and share.
Okay, yep, looks like we're good.
Okay, hello everybody.
I'm Christopher Subic
from Environment and Climate Change Canada.
I'm here to speak as said about,
well, click, okay.
This talk is essentially in two parts.
The first part is going to be a brief summary
of the current state of machine learning based forecasting
with a broad focus on medium range weather forecasts.
Second part of this talk will be the forthcoming AI roadmap
out of the atmospheric science
of technology director at NCCMAP,
which effectively broadly sets out
where this part of Environment and Climate Change Canada
will be going in the short of medium term future with AI.
And finally, time permitting,
I'll conclude with a preview of talks to come
and just general thoughts on the state of AI in forecasting.
So in case you haven't noticed,
AI is becoming a big deal.
It's no longer just pictures of cats
or helping kids cheat on their homework,
but it's starting to influence the industries
and feels previously thought unassailable.
Now, this isn't truly a stranger to weather forecasting.
AI adjacent topics have long had roles
in the forecast production system.
We've long had statistical models
for quality control of observational data
and for post-processing,
but a phase change has been that over the past two years,
maybe three models from the academic and private sectors
have gone from things that we should probably
keep our eye on as interesting things
for the long-term future to, well,
these two nearly full-featured forecast systems
that are competitive with the state of the art.
With that in mind, I can just state very plainly
that Environment Climate Change Canada,
in particular my part of it, is taking AI very seriously
and we intend for it to have an increasing role
in numerical weather prediction systems going forward.
We're taking a sort of trust but verify approach
in that we intend to make AI advances operational,
but only after they've been scientifically proven.
We're not in a rush to perform science by press release
and this is also a medium to long-term plan.
Capacity building is going to take years,
both in terms of acquiring sufficient computational power
for this and developing the human expertise
necessary to properly use it.
The modern AI forecast field essentially began
about two years ago.
I dated from the first publication of GraphCast
by Lam in 2023, which is the first time
that an AI model could truly claim to beat
the state of the art from, in this case,
the equivalent forecast from ECMWF.
In addition, the truly shocking claim
was that these forecasts came with orders
of magnitude faster running time.
GraphCast will produce a 10-day quarter-degree forecast
in about 30 seconds on a single GPU.
That put all of the weather centers globally on notice
that if these trends continue, we must adapt
or we'll fall behind.
And I mean, in some sense, research has always been that.
You can't sit on your laurels from 20 years ago
and pretend to be competitive,
but this field is truly advancing at a revolutionary rate.
Models are being constantly released.
Any summary, like the one I'm about to give
was going to be out of date within weeks.
In fact, just this morning, I've had to update my slides
to add two new preprints that have come out
in the past four or five days.
Nonetheless, there are some common features
between medium-range models that are shared
by, if not all of them, then most of them.
Most broadly speaking, these models attempt to learn
from data, which means rather than simulate
the atmosphere from first principles,
they try to look at some form of data
and predict what it will become.
In this case, for the medium-range forecasting,
the data is almost always the error of five reanalysis.
That's our highest quality, longest-term,
and most uniform record of the atmospheric state
that we have, and it's accepted as ground truth
for most of the AI models, but this data set
does have known issues such as relatively poor precipitation.
The other common feature about AI forecast models
is that most of them have sparse limited outputs.
They tend to predict only a few variables.
They tend to predict a small subset of vertical levels
compared to what we're used to from an operational forecast,
and they have a limited selection of lead times.
This creates new downscaling problems.
If you were in the plenary talk,
you heard our friend from Nvidia say that
this might not be a barrier to full prediction,
but at the same time, we're used to having the prediction
plus all of the rich output for free,
and not having that means we'll need to contemplate
new kinds of downscaling problems
to get rich data from a sparse forecast.
And finally, these AI forecast systems
have essentially no proofs of physical consistency.
Even hybrid models and development in that area
has a classical dynamical core,
but leaves a physics system that is
only tangentially concerned with things
like conserving energy.
Now, medium range forecasts tend to break down
into a few different categories.
The first and most conventional of these
are deterministic models.
These are analysis predictors,
and they essentially answer the question of,
if I have the atmospheric analysis at time zero,
what is the minimum error prediction
of what that analysis is going to be six hours from now?
As a general rule, these tend to give
overly smooth forecasts that over long lead times,
erase fine scale structure in the atmosphere.
The widely held belief is that this is a consequence
of training based on mean squared error measures,
because the lowest mean squared error prediction
in the future is your ensemble average.
But the ensemble average is not
a physically plausible forecast.
Now, that being said, these models have shown great success,
and having a really good ensemble mean prediction
of the future is still a really good prediction
of the future.
Each day of predictability translates to billions
or billions of dollars of enabled economic activity.
This category of models is in some sense the oldest,
and I really hesitate to use that word
with a field that's about two or three years old,
but they're models from many different groups.
Graphcast and ForecastNet have been previously mentioned.
Graphcast is a graph neural network.
ForecastNet is now a spectral Fourier neural operator
that operates in a spherical harmonic space.
Pangu weather came out as a vision transformer,
and AIFS is now apparently officially published
with a pre-print, and it's a graph transformer.
The jargon here is not that important,
but the underlying point is that you can reach
a deterministic forecast
with many different AI architectures.
There's, as of yet, no specific Royal Road to a forecast.
The second category that I've somewhat arbitrarily divided
things into are ensemble models.
These try to address the problem of overly smooth forecast
by adding some element of randomness.
An ensemble forecaster will receive
some kind of random data source,
either a random number generator or a field of noise,
and it's asked to generate essentially different forecasts
from the same seven initial conditions.
This more or less solves the problem of smoothing
based on mean squared error loss functions
because each individual forecast no longer has to track
the long-term forecast.
No longer has to track the long-term truth,
but each one can be plausible,
and then you're tracking the future
with a true ensemble mean.
Now, the downside is that these models
are all more expensive to train and run
the deterministic systems of the same size.
In operations, you will need at least one inference
run per ensemble member.
So, graphcast's 30-second, 10-day forecast is great,
but if I want a 100-member ensemble,
now I'm talking an hour or two,
and that can add up very quickly.
In terms of training, if you're training
with an ensemble error measure
like the cumulative rank probabilistic score,
that requires training over an ensemble,
and increasing the size of things during training
tends to increase the scarce GPU memory requirements
and the overall cost of building the system.
This is a newer frontier of AI forecasting.
It's probably going to be more popular in the months to come,
and the examples here are also somewhat newer.
Gencast was previously mentioned in our plenary.
It's a diffusion model based on graph transformer network.
Neural GCM is a hybrid model, also previously mentioned,
and it's a dynamical core
that has learned physics parameterizations,
and in particular, can operate in a non-subtle mode,
which is why I've included it here.
And finally, there are models such as seeds,
also out of Google, that don't try to forecast directly,
but they try to take an ensemble mean
that already exists from some method,
like a traditional forecasting system,
and generate new ensemble members
to fill out the probability distributions.
The final category of forecast models,
and I'm going to divide things into our foundation models,
and these essentially answer the question of,
what if we had GPT-4, but for weather?
The idea here is and shared by foundation models
that exist in our development,
is that you break up a weather state,
like the analysis, into tokens
by regionally subdividing it usually,
and then you ask the foundation model
to predict missing pieces of it.
You say, you can mask out the future,
and say, okay, predict this,
and then you're training it in a forecast context,
or you can mask out a missing regional piece,
and ask it to fill in the blank.
You could even ask it to predict the past, I suppose.
This allows the foundation model
to be trained on qualitatively different data sources.
For example, you might have an encoder suite
that takes satellite observations directly,
and tries to turn it into token space,
or the analysis from era five,
or lower resolution climate forecasts.
And once tokenized, the idea of a foundation model
is that you have a giant middle processor layer
that operates in this latent space,
and from there it is relatively simple
to build out decoder models to give you things you want,
like tomorrow's forecast today,
or what the radar is going to look like in 30 minutes.
Foundation models certainly prove their worth
with text-based processing like GPT,
and they're also very popular and emerging
in Earth observation applications
with interpretation of satellite data.
For example, at an ECMWFESA conference a few weeks ago,
there were some interesting talks
about taking satellite data
and using it to infer tree cover near power lines in Norway,
tasks that would normally take
some very expensive helicopters,
and doing them much, much more cheaply
with readily available Earth observation data.
The downside is that foundation models
tend to be extremely expensive to train,
and these will push the limits
of what the public sector is probably willing to spend
should foundation models prove themselves for forecasting.
Few of these models exist right now for NWP,
but there's plenty of private sector interest.
I mean, our friend from NVIDIA talked at length about that.
The two models that are currently published
are ATMO-REP from Christian Lesig,
which is a transformer model,
tested on both forecast and downscaling applications,
and Microsoft has published, as of a few days ago,
it's Aurora Foundation model,
which most notably uses several different data sources
for training, not just air five,
but also climate simulations and operational forecasts,
and I believe one of Noah's ensembles.
Now, to integrate all of this,
as a researcher, I can make a few broad predictions
on where the trends are in this area.
First is going to be the convergence
of data simulation forecasts,
nowcast and downscaling roles.
Right now, the leading NWP forecast,
take in an analysis and give you a forecast,
but the obvious question is to what extent
can the rest of the chain be included?
For example, forecasts that are ensemble generating
can often be reversed to perform data simulation,
and a couple of references here are one,
using a diffusion-based model
to assimilate sparse observations,
and the second one is to perform data simulation
inside the latent space of an autoencoder,
which effectively replaces the background
error covariance matrices of a DA system
with ones that are nonlinear and flow dependent
from the autoencoder space.
A second avenue here is to combine conventional models
with an AI-based bias correction.
Farshie from ECMWF has published recently
on using a neural network bias correction
to include some element of AI inside the IFS-40 var system.
And Said, my colleague, is going to present
in about half an hour on spectral nudging
to bring a classical NWP system
closer to an AI forecast to preserve rich data
and have the accuracy of AI.
And finally, there's some effort
on all-in-one AI forecast systems
that go directly from observations to forecasts
to post-processing to produce predictions
of future observations.
The Vaughn 2024 is the Aurora mentioned in the plenary,
which is an encoder, decoder architecture
that uses Unets, and it's the obvious long-term future
of foundation models.
There's also rumors that Google is buying up
satellite data for its next generation,
GraphCast version two or something like that,
but I don't have any particular
confidential information to share.
Okay, now the second part of this talk
is the Environment and Climate Change Canada AI roadmap.
This is a joint product of the AASTD and CCMAP.
It's a product of an internal Tiger team
across the research and operational divisions
that was put together last fall after an internal study
on the current state of AI and weather forecasting,
where we had a whole lot of talks
like the first half of what I just gave.
This roadmap is designed to set very broad research priorities.
It's not designed to currently pick and choose
what projects are worth funding,
but to set out how we should think about AI as an institution.
And the largest theme from this roadmap
is the need for capacity building,
both in terms of compute capacity,
in light of the supercomputer update
we're likely to have next year
and whatever gets procured in the years thereafter.
How we should think about the use of cloud computing
and finally what we need to do
from a human resources and training standpoint.
This is a living document.
It should be published soon.
I'd hoped I could begin this talk
with a reference to the live document,
but I think it's still in translation.
But once it is published,
it'll be updated every few months to every year or so
with internal reviews and updates
just as we understand more about AI.
The broad themes of this document
are how we intend to integrate AI
throughout the research, development and operation cycle.
So we're not limiting it to just graphcast style forecasts,
but we're interested in applying AI everywhere
from data gathering to post-processing.
The main evaluation criteria here
are the triumvirate of feasibility,
service and efficiency.
Feasibility answers the question of how capable are we
of developing and running a proposed AI project?
And that includes not just the scientific risk
of well, putting a whole lot of time and money
into a project and having it not work,
but also whether or not we have the compute resources
to do this or the human resources to develop and manage it.
The second criterion is service
of how a project will improve ECCC's
weather-based services to Canadians.
And that is not just in terms of accuracy,
but also whether we can make our forecast products
more timely, whether we can have more numerous projects
or whether we can have qualitatively new products
like hypothetically, rapidly updated now casting,
which we just currently don't have available
for many government source.
And finally, there's a question of efficiency.
How will a project make efficient use
of our computational resources?
That includes the broad themes of energy efficiency,
but also the government-specific theme
of being a good steward of public funds.
Supercomputers are, as one might guess, rather expensive.
And we ought to demonstrate to Canadians
that Canadians are getting their money's worth.
Okay, so more specifically,
how are we looking at AI
from the observation to product pipeline?
First category here are observations and data assimilation.
So in observations, AI is in some sense
an extension of what's already happening
in terms of looking at quality control
and error estimation of our inputs.
AI can perhaps allow us to have some new capabilities
with learned observation operators
to take advantage of parameters that are,
to learn parameters that are not directly observed,
such as complicated satellite measurements
that are non-linear features of the atmospheric state
that are hard to directly measure
but might be possible for an AI to learn.
And finally, we have some ideas on unconventional data sources.
One idea thrown around in discussions
was using ad hoc webcam data like traffic cameras
to evaluate real-time precipitation class information.
In principle, someone can look at the feed
and say, yep, it's raining,
and we can have an AI do the same thing
and potentially have useful data.
On the data assimilation side,
in some senses, this area is most advanced
because DA is already using some heavy computation
for its error matrix operations
and they're investigating GPU use.
As previously mentioned,
there's a data of data assimilation
in the latent space of a model.
If we can vastly increase ensemble sizes through AI,
we can perform better PDF estimation
through particle filters or non-Gaussian-based statistics.
And finally, we can start to estimate,
perhaps, the model parameters themselves
rather than just initial conditions.
The numerical prediction side,
this is closest to what I talked about previously
with AI forecast models.
In the near term, we're looking at implementing
the AI models as they are
to provide second opinions about the weather to forecasters.
In the medium term, we're looking at fine-tuning
large models on our operational data sets
to provide better AI forecasts.
And in the longer term,
we're looking at structural changes
and developing new models in general.
In addition, we'd like to hybridize classical
and WP and AI models to help fix the problem of sparse outputs.
And along the way,
we'd also like to investigate emulation
of the physical parameterizations.
For example, radiation is very expensive
inside the atmospheric model
and 3D radiation is probably better than 1D radiation
but it's too expensive to run operationally.
If we can emulate that,
we can have a better parameterization
that is still within our compute budget.
Also, we'd like to extend this
to ocean ice and land surface prediction,
but that is still fairly preliminary
in part because the data sources aren't quite as complete.
Okay, now in terms of the tail end of the forecast
for post-processing and final products,
this is the realm of downscaling and now casting.
We have a 2.5 kilometer high resolution regional system
but it's just too expensive to run too often.
If we can downscale,
we can potentially achieve that resolution of output
and evaluate extremes and risks at that scale
without being limited by melting the supercomputer.
It would also be really nice if we could have
near real-time assimilation of weather station radar data
to improve the half hour, one hour forecast,
which is not something we can currently do very well.
In terms of post-processing,
the statistical post-processing systems
have all long had systematic error adjustments
and station specific adjustments
for representative this errors and the like
and AI can help turn that into better nonlinear corrections.
And finally, in terms of the expert product sides,
we'd like to have better high impact weather diagnostics
with well-calibrated forecasts for all of the big ones,
tornadoes, hail, blizzards that are just don't show up
in the larger scale forecast
but are extremely important for the people stuck in them.
Excuse me, Christopher, so only two minutes left before.
Yes, okay, I am going to be very quick.
Okay, challenges and opportunities.
As I've hinted at the entire time, we have challenges
and these are opportunities.
On the physical side, we have limited compute capacity.
GPU compute demands are only going to go up.
We don't have very many of them.
We're going to probably get more in the future
but we need to manage them.
We also need to care about data management
in terms of not just having an archive that exists on tape
but one that is living and can answer
training-based questions very quickly.
In terms of HR, we have similar problems
that our researchers are all very good
but they're also not necessarily well-trained on AI.
We need to close that gap.
And in particular, we would love to have increased collaboration
with both the ivory tower and private sector.
Okay, the roadmap sets out targets and milestones.
We would like to have our first operational AI systems
for IC innovation cycle five,
which is targeted early 2026
after the supercomputer update.
The current innovation cycle is just closing
because it's a bit too early for it.
And ultimately we'd like to have AI
as just another forecast tool by 2030 or so.
Okay, I would love to talk more about this
but unfortunately there's no time
but in general, our divisions have all been investigating
how we can integrate AI into research flows.
Some projects have started,
some are waiting for people
and some just simply need more resources
that we don't currently have
and we would love to collaborate on them.
If you have AI skill
and you have a weather-related project,
please email someone at our division.
We would probably love to talk to you.
Unfortunately, I have to cancel this slide
where I was going to talk up
all of my colleagues' presentations to come.
Please stick around, they are going to be great.
And finally, conclusions-wise,
AI is rapidly advancing the state of the art
in numerical weather computation.
This is a phase change of forecasting
and I'm excited to see where it goes.
We will make AI and machine learning technologies
a major part of our systems
as they prove themselves.
But we're a public service organization,
we recognize that we have to be very careful
about what we stand behind operationally.
And finally, I hope these slides are available
afterwards for the references.
There are references to all of the systems I've mentioned.
And I'd also just like to highlight
that of the 14 references here,
about eight are preprints.
This is a really, really rapidly moving field.
Thank you, I am...
Missy Christopher.
We have time maybe for one question.
I know.
I'm going to take the highest ranked one.
It's a long one, are you ready?
Yes, yes.
Perfect.
Currently, AI seems to be quite expensive
and rapidly developing.
While we were waiting for AI-based models
to build better foundations and training
to replace numerical weather prediction
or for climate studies,
can we exploit its computational speed
in the near term for now casting or HRR-like output?
For example, ECCC can incorporate into CAM
for very short thunderstorm prediction
or perhaps weather elements on grid now cast.
Okay, I believe these projects are under investigation.
I'm on the medium range forecast side,
so it's not my particular side,
but there's a presentation in this session,
I believe after lunch that investigates now casting
via an IBM Foundation model.
And I think that'll begin to answer that kind of question.
In general, yes, this would be very good.
In practice, the nearest term limits
are probably compute potential
because we have relatively few
operationally available GPUs,
but hopefully in the next few months to year or so,
that will improve.
I might sneak in another question then,
Christopher, we have a minute.
Have you considered the role that Canadian industry
will have in developing AI capacity at ECCC?
We would love collaborations from industry.
That is my politically correct and also true answer.
We have limits on our resources in part
because until, well, procurement thus far
has been focused on making our operational systems better
for obvious reasons.
And the cycles of this mean that it is practically difficult
to, sorry, I'm speaking as a researcher,
my boss is probably listening,
so there's some things I need to be very circumspect
about saying.
We need to be very careful about how we commit resources
for training large models.
Industry is, I think, a fantastic partner,
both for the potential of having compute resources
we could borrow and also a better focus
on some of the most downstream applications.
For example, our leading talk that opened CMOS
was on the weather impacts on the insurance industry.
And to the extent we can be of value there,
I think there's room for joint products.
Okay, so I will try to answer other questions
in the chat as we continue.
But otherwise, I will stop sharing, meet myself
and thank our hosts.
Well, thank you, Christopher.
That was an amazing feat.
You put in two very large presentations
into one 30-minute presentation.
You have all my admiration and thanks for doing that.
Congratulations, actually, in 30 minutes.
Okay, and now we're on the subject of airvests.
So we're not going to take any longer.
I'm going to introduce you to Airvests GLaPalm,
who comes from Canada as well, a researcher.
He's going to introduce us to the NWPEI-based model,
the verification against the observations
of airvests and airvests.
It's up to you.
Hello.
I don't know.
So you hear me.
Very well.
Do you see my screen?
Also.
Wonderful.
So I'm going to do the presentation in French.
If you have any questions in English, there's no problem.
If you have any questions in English, there's no problem.
It would be easier for me if I do it in French and for you also.
So I'm here to present the work that I did in collaboration
with my colleagues on the evaluation of the models based
on the airCCC in a second.
I don't know.
So the context is that with the emergence of these models,
we realized that we had to check these models
with our traditional verification methods,
which allow us to evaluate the innovations
that are made on traditional models.
So I was asked by my boss to install, turn and check
these models on our HPC installations.
And I started this work in October 2023.
And I worked on it until April 2024.
So what I want to present to you is just that.
So the activities that were completed during this special project
there, it's that we turned, we chose two models,
ForecastNet and Graphcast, which were available for free.
It's easy.
And, well, ForecastNet, Christopher,
we talked about it a little bit earlier.
So it's a model that was developed by Renvidia.
And then we turned two graphs of Graphcast,
one with 13 levels of pressure, with a roof of 50 hectopascals
and a version at 37 levels, a roof of 1 hectopascal.
And we turned each model with three analysis sets.
One, the first is the operational analysis of the OVF,
called IFS, on three levels only.
We also used R5.
So the R5 analyses are the ones that were used
to train these models.
We were able to turn the configurations at 13 and 37 levels.
And of course, we wanted to compare the operational provisions
with the same analysis.
So we started these models, these AI models,
with the operational analysis of CCC.
And we turned in two real-time modes,
where we turned twice a day,
at the same time as the operational model.
And like that, the operational metrologists
can compare the Forecasts based on AI
with the operational provisions.
And also, on my side, I did an evaluation
on a period of one year, which allows us to see
what the models are like.
So here, I put a slide on the description of the models.
It's very precise.
I don't have much time.
Basically, the two Graphcast and Forecast Net models
use about the same information.
But I put a lot of detail there to be complete.
But I don't think I'll be able to save a little time.
So one of the advantages of AI models
is their informatic efficiency.
If we compare the operational model,
presently, it takes a little less than an hour,
more than 6,000 CPUs.
So it's a big deal.
So it generates 500 gigabytes of data
at each provision twice a day.
It makes outings at all ages up to 10 days.
And then it's models at 15 kilometers of resolution
with a lot of vertical resolutions.
AI models have less good resolutions.
They release data at 6 hours.
And a less good vertical resolution too.
So, but Forecast Net is very, very light.
It's impressive.
It takes 20 minutes on a CPU.
I don't speak of a GPU here.
I have a GPU even faster, of course.
But on a CPU, you can turn it on.
You can turn it on on your laptop and it works.
And it's very fast.
It still gives you a good preview.
And Graphcast, it's a little bit more expensive.
The confidence at 13 levels
requires 100 gigabytes of memory.
So it's a little bit more expensive.
But still, comparing the operational model,
it's very, very much, much smaller.
Smaller orders.
And I invite you to the second presentation
of Christopher Subick on exactly comparing
the computer performance between AI models,
Graphcast and GEM, the operational model.
And it makes a very good comparison.
If you're interested, I invite you to have
this presentation on your computer this afternoon.
So what does it look like as a verification
once we've done the average over a year?
So here, I have several curves.
Here, I present the errors, the prediction
of the geopotential at 55 to Pascal.
So these are the errors.
The very thick blue gray here, that's the baseline.
So that's the operational preview.
So we see the errors that are missing from 0 to 240.
And the other curves, it's all the previews,
the verification, the AI models.
So we can look at them and then, as these are errors,
but the closer we are to 0, the better it is.
So we see here a group of previews.
That's the forecast net.
So we see that for the GZ500,
the forecast net is less good than the operational preview.
On the other hand, all the other curves, the five other curves,
it's all the previews made by Gravcast
using different analyses.
So we see that Gravcast, from day five,
is better than the operational model,
no matter the analysis we give him.
So that's when using the 05 and FS analyses, it's better.
I don't know, but what's important here,
what's interesting here is that the two curves here,
orange and green,
the previews are initialized with the same analysis
as the blue curve.
So we see that with equal information,
Gravcast is better from day five on the GZ500.
If we look at another variable,
which is the temperature at 850 tectopascals,
so it's the same colors, the same curves,
in this case, we see that from 72 hours,
all the AI models are the operational model,
even for the GZ500,
but we still see that Gravcast is the best model,
this variable.
So I showed you two variables.
So we can conclude that Gravcast is better than for the GZ500,
we will focus on that from now on.
And then what does it look like for other variables,
like wind, humidity.
So here, I present you graphics,
it's vertical profiles.
Here, in the Y axis of the vertical coordinate curve,
we talk about the surface,
from 1,000 tectopascals to 50 tectopascals.
And so the clean curve,
it's the error's quarter, according to the variable.
And the tight curve is the bias.
And the different variables are the following.
Here, on the top left, we have the meridian wind,
the wind component, the wind module here on the right.
Here, can you do this for us in the next two minutes?
Yes, that's it.
Yes, I'll go faster.
So what I wanted to show you,
here we have the Gravcast configuration,
so at 13 levels, 37 levels.
So what I wanted to show you is that the red curve
is always better than the blue curve,
so the model has all the variables,
all the failures, not all the failures,
but all the variables, all the levels.
We see that Gravcast is better.
So are these two models not different resolutions?
Is the fact that Gravcast is a bigger resolution,
is it the advantage?
Is it the advantage compared to the operational model?
So what we did is that we did the same verification
that I presented to you earlier, but we filtered it.
We filtered, we removed all the scales
that were smaller than 1,000 km,
and we kept all those that were 2,000 km.
So I have two graphics to present here.
So on the left, it's the same graphic
that I presented to you earlier, not filtered,
and on the right, it's the filtered previews.
So that's on the average, on the winter,
it's not on the full year.
So we see that even if filtered, Gravcast is better.
So that's going to bring to the work of Spectreur Nodging
of Syed, who will present in the next presentation.
So and for the summer, it's a little less spectacular,
but we still see that Gravcast has a lot of good information.
So what are the future activities to do more verification?
We even have an internal site that allows us to visualize
these previews day by day.
And I invite you to have the next seminar
of Syed about Spectreur Nodging,
which is a very interesting approach to integrate
physical models and AI models.
And Christopher Subick's work on entering Gravcast
with the operational analysis that we have done internally
so that it can be better adapted to our model.
Thank you.
Hi, thank you, Hervé.
Hi, I have a question for you.
There, you made the internal models run,
you installed them to do the verification,
but there are more and more models,
there are almost two days,
is there perhaps a way to have
verification, counter-observation,
like that, in a standardized way,
or each time, you will have to install the models
and then the scoring themselves?
Well, listen, that's what the software that we use
to do the verification and counter-observation
works very well only locally.
So it's difficult to publish that externally.
On the other hand, running these models,
if it's not done very, very easily,
I worked for three months in the middle of the day
just to start these models.
So I imagine that each model has its own peculiarities.
So it's not obvious, it's not as plug-and-play
that we believe in.
There's still a lot of work to be done.
So if there are all the two, all the two weeks,
it will be difficult to do this same evaluation
to follow the run.
Thank you, I don't know if there are other questions.
Maybe a word for advanced systems.
If you could reset the Q&A on EventMobi
because I still see the questions
that have been asked to Christopher Subic.
So it's hard to see which one.
I have one for you, Hervé.
How do artificial intelligence models
behave in complex mountainous terrain?
I didn't evaluate that.
That's more what my colleague, Marc Verville,
did for verification on the surface.
I don't have any memory of the results.
My focus was really on the verification in addition.
But of course, having a good resolution
will certainly not be a problem.
Perfect.
Thank you very much, Hervé.
Thank you.
Thank you, Hervé.
We, so we're going to continue.
This time we're inviting Syed Zahid Hussein,
who's a research scientist at ECCC.
He will be presenting leveraging data-driven weather
emulators to guide physics-based numerical weather
prediction models, a fusion of forecasting paradigms.
So without further ado, Syed, this is your turn.
Thank you, Rand.
Hello, everyone.
In my presentation today, I will be talking
about how we can leverage the strengths of data-driven weather
models to improve predictions from NWP models.
This is a work that we have been doing recently.
And here is a list of my principal collaborators
and the others who have contributed to this research.
As we know, the current state of the earth
for operational weather forecasting
is based on physics-based NWP models.
However, we have seen these presentations earlier today
from the plenary to the previous two presentations
that we have recently seen the emergence of new data-driven
models for predicting weather.
And most of these models are using some form of deep neural
network to emulate the training data.
And the training data generally is RFI re-analysis.
So we also can call them artificial intelligence-based
weather emulators.
And recently, they have started to gain prominence
and started to also challenge the existing forecasting
paradigm.
Because as you have seen in the previous presentation
from my colleague, Erveg, that these data-driven models
can produce forecast orders of magnitude
faster with minimal computational resources
compared to the traditional NWP models.
And also, they can be highly competitive against state
of the art NWP models in terms of their accuracy.
However, despite their strengths,
strictly when I'm talking in the deterministic sense
for these AI-based models, they have their limitations also.
And one of the most widely known limitation
is considerable smoothing of fine scales,
particularly for longer lead times.
Also, they only offer a limited range of forecast fields
and improving nominal resolution of these AI models
not straightforward.
They can be quite challenging.
So the objective of our research was
to see if we can leverage the strengths of these AI-based
models to improve the predictability of an NWP model.
And for that, we chose or selected
like the GEM model, which is used operationally
as the NWP model, operationally by Environment Canada,
and the GraphCast model from Google DeepMind as the AI-based
model.
And the nominal grid resolutions of GraphCast
and the GEM-based Global Deterministic Prediction System,
or GDPS, are approximately 25 kilometers and 15 kilometers,
respectively.
And in the previous presentation,
AirVig has shown that the GraphCast actually poses
more skilled large scales compared to our GDPS.
But if we want to leverage the information from GraphCast,
we need to know about the effective resolution of GraphCast
so that we can see what are the scales that we can really
utilize for improving NWP model.
And in order to do that, we look at the variance ratio of GDPS
and GraphCast with respect to our own CMC analysis.
And before I talk about anything else,
I must emphasize on the fact that the version of GraphCast
that we are using has not been through any fine tuning.
So it has been trained by Google on emulating error-5 analysis
by training with error-5 data.
And we are using that GraphCast model
but initializing it with our own analysis.
And in these figures in this slide,
I am showing the transient component of variance ratio
for 500 hectopascal kinetic energy.
On the left, I have for lead time 24 hours.
And on the right, I have for 120 hours.
In blue, I have GraphCast.
And in blue, I have GDPS.
And in red, I have GraphCast.
And we can see by looking at the variance ratio of GDPS,
the blue lines in both 24-hour and 120-hour cases,
that the variance ratio is close to 1.
So that means its effective resolution
is not changing with respect to lead times.
However, what we see with GraphCast,
first of all, at 24-hour lead time,
we see the scales as large as 1,500 kilometers
are smoothed out to some extent.
And we see considerable smoothing for scales smaller
than that.
And then we see when we go to 120-hour lead time,
the scales that are getting smoothed out actually increases.
And it affects scales as large as 2,750.
So we know that large scales in GraphCast are better.
But at the same time, we see that scales below 2,750
for longer lead times are problematic because of the reduced
variance ratio.
Now, the question is how we can really use or leverage
this good large-scale information from GraphCast.
And one way to do that would be to use spectral nudging,
large-scale spectral nudging.
And this is a very widely used idea
in the field of regional climate modeling and hindcasting.
Our expectation is if we nudge our gem predictions
towards the large scales of GraphCast,
we can improve the quality of prediction of gem.
At the same time, we should be able to address the fine-scale
smoothing in GraphCast while we will
be able to generate the full set of focus fields
that we are currently having access to through gem.
The concept of nudging is quite simple,
as illustrated by this equation here.
So here, F is the solution of our gem model
after the dynamic substep.
And this term highlighted in purple color
actually corresponds to the nudging increments.
So we add the nudging increment to the model solution
after the dynamic step to get the nudge solution, which
is then fed to the physics.
And then it completes the complete model time step.
And then it fits back to the next dynamic step.
And this is how it continues.
And if we look at this nudging increment term,
we have this subscript Ls, which actually
implies the large scale.
So we are only considering the large scales
when we are applying the nudging.
And this omega is a relaxation parameter.
And if we break down this equation,
we can see basically what we have
is a weighted average of the large scales coming from GraphCast
and our model, whereas the fine scales are remaining intact
that is being credited by the model.
So this is how we can leverage the large scale accuracy
of GraphCast while allowing our model to freely evolve
the small scales.
And the scale separation between large and small scales,
we are doing that by decomposing the nudging
increments in the spectral space.
Although we are applying the nudging increment
in the grid point space, but we are decomposing it
in the spectral space.
And hence, we call it spectral nudging.
How we optimize the spectral nudging configuration
to support our objectives for this study
is a computationally demanding task.
And in a sense, it is still ongoing.
I mean, we are still sort of fine tuning the configuration.
But at present, the most optimal configuration
that we have has these following features.
We are only applying nudging to horizontal wind and temperature.
And we are not nudging the stratosphere and the boundary
layer for different reasons.
And we are nudging scales that are larger than 2750
for obvious reasons.
That should be obvious from the variance ratio comparison.
And we have 12 hours as the nudging realization scale.
And we apply nudging at every time step.
With that, I'll be going to some of the results
of verification that we will try to prove
that this approach actually helps
to improve the predictability of gem.
We ran a series of experiments for winter and summer of 2022.
So we have the control experiments
where there is no nudging, the control GDPS.
And then we have the GDPS with spectral nudging.
So control would be, in the next few slides, all the results.
Control would be shown in blue color.
And the results from spectral nudging with GDPS
would be shown with red.
So the first scores that I want to show
are verification against radios and observations.
It's similar to what Elvig has shown
in the previous presentation.
Just to repeat, we have in these figures,
we have different variables, zonal wind, wind modulus,
geopotential high temperature, and dew point depletion.
In all these figures, we have the dashed lines representing
the bias and solid lines representing standard deviation
of error.
And we have the shades of red and blue
that represent the statistically significant improvements
corresponding to the color of the experiment.
So if we see red color, then it implies
that we have statistically significant improvement
with the spectral nudging configuration.
And if it is blue, then we have deterioration
with spectral nudging.
What we can see from these figures,
this is for winter over the globe.
That beyond five days and up to 10 days,
we can see that there is tremendous improvement
in the scores for the standard deviation of error, which
is more difficult.
Excuse me, Sayed.
You have two minutes left.
OK, I'll try to be faster.
For the bias, the differences are mixed.
But the bias is less difficult to improve.
Standard deviation is more difficult.
So we see tremendous improvement, up to 10% improvement
in the RMSE for longer lead times.
In summer, though, we see modest improvement.
Still, we see statistically significant improvement
for the standard deviation of error.
And when you look at the animal liquid relation
coefficient, this is for the 500 hectropascal geopotential.
We see improvements both in winter and summer.
In winter, in terms of the gain in predictability,
it's around 18 hours.
In summer, it's about eight hours improvement.
And they're both statistically significant.
And last results is about tropical cyclone position error.
This is another thing that is very difficult to improve.
And our model tends to have, for the alone track position,
we tend to lag the model.
And we can improve that lagging with spectral nudging
towards graphcast.
And for the cross-track position error,
our cyclones tend to veer to the right
from the observed trajectory.
And we are able to considerably improve
that aspect of the position of tropical cyclone also.
And finally, just this figure, I'm
showing the temperature anomaly at 850 hectropascal
for a lead time of 240 hours for a single case.
We have the GDPS control on the left,
graphcast in the middle, and GDPS with spectral nudging
on the right.
And we can see that graphcast barely has any fine scale.
And both GDPS control and with spectral nudging,
they both have comparable fine scale information.
And we get this improvement by nudging as well.
So to summarize, we developed and hybrid NWP system
that fuses NWP models with AI models to spectral nudging.
And by leveraging more accurate large-scale predictions
from graphcast, we are able to significantly improve
our prediction scale with GDPS.
And I want to stress that the improvement that we have
is roughly equivalent to one solid innovation cycle, which
is about four years of work involving many scientists
from across the Meteorological Research Division
of Environment Canada.
And we were able to achieve something comparable
in a matter of four to five months.
And also, I want to stress that this
is based on work that uses a graphcast model that
has not been fine-tuned.
And with fine-tuning graphcast to emulate our own CMC
analysis, which is a work in progress by my colleague Christopher,
we hope that we could improve further.
And currently, with this configuration,
we have about 25% increase in the computational cost.
But this is without any optimization.
And with some optimization, we hope
that we will be able to reduce it to something
like less than 15% in the near future.
So with that, I will end my presentation.
Thank you, Sayada.
It's a very impressive conclusion.
I have one question here.
Nudging is done as gem integration goes or at posteriori?
No, it's online.
So what happens is, as I said, you
solve the dynamic step.
Because we have the dynamic sub-step and the physics sub-step.
And then we do the coupling in the split mode.
So we solve the dynamic sub-step.
We have a solution from dynamics,
which is an intermediate solution.
Then we update that by nudging.
And then we feed that updated solution to the physics.
And then we get the complete solution of the model time
step.
And then the next time, dynamics uses that solution
to predict the next dynamic step.
So it's not a posteriori.
It's an online update.
Last quick one, how does GDPSSN compare with GraphCast?
That was already shown by Ervig in the previous presentation.
That GDPSS, I mean, if you talk about control GDPSS
versus GraphCast, that presentation of Ervig
should allow you to see like it actually improves.
What I am missing in my figures, because this
is still a work in progress, I mean,
in our paper that we expect to submit soon,
which we will be adding the GraphCast also,
focus in this, for example, in these figures to show
control GDPS with GraphCast and GraphCast itself,
like how much of the improvement is coming from GraphCast.
But it is definitely coming from GraphCast,
as Ervig's presentation showed that the large scales in GraphCast
are much better.
Yes, Isayed, looking forward to read the print.
That will be very popular, I'm sure.
Thank you.
Merci.
Anne, at what?
Yes, so we're moving.
You might have noticed in the schedule
that Christian Isayed was originally
supposed to present this.
But I want to thank Madalina Socer to have accepted
to present and prepare this presentation for us.
So she's a Climate Extreme Specialist at Environment Canada.
She's going to be presenting on the development
of artificial intelligence downscaling applications
for medium-range forecasts of weather elements at CCMEP.
So without further ado, over to you, Madalina.
OK, thank you.
I hope you're hearing me well.
We are, yes.
OK, great.
So I am here today to present to you
some development of artificial intelligence downscaling
techniques that we're doing at CCMEP in collaboration
with IBM Research.
And I would like to acknowledge my co-authors
that are listed here, both from ECCC and IBM Research.
So first, I would like to say that the vision for this project
is actually to help us offer seamless day one to day 10
public forecast products as part of the transformation
of the meteorological service of Canada.
So in order to do this, we need to bridge
the gap between high-resolution, short-term forecasts,
and medium-range, lower-resolution forecasts.
And the reason why we want to do this
is because we know that insufficient horizontal
resolution causes forecast errors and especially biases.
I am showing here a graph of bias,
a comparison between a low-resolution model in red
and the high-resolution model in blue.
And what we are saying here, the closest we are to zero,
the less bias we have.
And we really see that increasing horizontal resolution
is improving this bias.
Usually, the way that we do that is
by doing dynamical downscaling.
So running numerical weather prediction models.
But this is very computationally expensive,
which makes it limiting.
A partial solution to this is doing statistical downscaling,
which means using past data and deriving
statistical relationships from this past data,
such that we apply these relationships
on the course input and we obtain high-resolution output.
These type of solutions are limited
by the fact that we have to impose certain
non-mathematical relationships in the data.
So now we have a data-driven alternative,
which is to apply artificial intelligence techniques
to do downscaling.
And what this does is that it allows
to derive complex relationships in the data
that we provide to these models.
So our objective is to develop
artificial intelligence downscaling techniques
to downscale weather elements from medium range forecast
to the kilometric scale.
And we will hope that this will help both deterministic
and ensemble forecasting.
So just to briefly present our project.
So this is a collaboration that started this year
between CCMAP and IBM Research.
And here at Environment Canada, we
have the meteorological expertise.
And we definitely have subjects, problems
that could really benefit from these artificial intelligence
solutions, but we don't necessarily
have the artificial intelligence expertise, which
is where IBM Research comes into play.
And collaborating with them, they are really experts
in this field, will allow us to advance much faster.
And the expected outcome from this collaboration
that for now it's only meant to be on one year,
is to develop low-cost and efficient alternatives
to the computationally expensive dynamical models.
And the other thing that we want to get from this collaboration
is we want to learn from IBM Research.
So at the end of this project, we
want to enhance our capability at Environment Canada
to be carrying out this type of research and development
and eventual operational implementation
of AI downscaling techniques.
So the specific goals of our projects are as follows.
So we are interested in downscaling weather elements.
And by this, I mean surface winds, temperature,
and precipitation in the first stage,
a forecast from the GDPS, which is our global deterministic
prediction system shown here.
And runs at a resolution of 15 kilometers
to the resolution of the HRDPS, which
is our high-resolution deterministic prediction
system that is run for 48 hours for now at 2.5 kilometer
resolution.
And in this project, we are taking a two-step approach.
So in the first step, we will be looking at the baseline model
that is based on generative adversarial networks.
And in the second stage of the project,
we will be taking advantage of foundation models
and tuning them for our application.
The training data for the models will
be forecasting data from the GDPS
as the low-resolution data set and from the HRDPS
as the high-resolution data set.
And it is very important for our operational needs
that the downscale products are available on the HRDPS grid.
For now in this talk, I will only focus on the baseline model
as this is a collaboration that just started.
And we haven't gotten yet to the second part.
So just briefly, what is a generative adversarial network?
And generative adversarial networks
consist of two networks, a generator, two neural networks,
a generator and discriminator.
And the way that they work is that they
are trained in a competing process.
So the generator pretty much generates images
that look as much as possible as the real data.
And the goal of the generator is to just
fold the discriminator.
On the other hand, we have the discriminator
that receives both real data, which in our case is HRDPS
data, and generated data, and has to decide
whether this generated data is real or fake.
And in our case, we use the ANAU et al. 2023 implementation
of a Vassar SteamGAN.
I will show you some preliminary results for our project.
So these preliminary results are based on the WGAN from ANAU
et al.
Without any covariates.
So what this means is that the only data that goes for now
in this model is zonal and meridional 10-meter wing
components.
And this is the data that we are trying to obtain.
So the low-resolution data comes from GDPS.
High resolution comes from the HRDPS.
And so far, we are training the model with one year of data
that is divided as follows.
75% of the data is used for training.
It's about 7,000 forecasts.
12.5% is used for validation.
And this validation data is used during the training
of the model.
So it's used to stop overfitting the model.
And then 12.5% of the data is used for testing.
So once we have a tuned model, we
will use this data set, about 1,200 forecasts,
to be seeing how well this model functions.
Because we are using forecasting data,
it is a bit difficult.
Because as you know, forecasts, there
are increases with forecast lead time.
And so we might have too much divergence
between the HRDPS, the high-resolution data set,
and our low-resolution data set.
On the other hand, the first six hours of the forecasts
are affected by the spin-up time of the model.
So we have decided for now to go with forecasts our 6 to 18.
And we are using them from the 0,0 and the 12 UTC
initialization of both models.
So in this way, we are covering the entire day.
So our challenge is covering the entire HRDPS domain.
I am showing here the HRDPS domain.
It is on the order of 2,500 by 1,200 grid points.
So it's a very large domain that's
spanning the width of Canada.
And it isn't really possible, or at least we don't think
it's possible so far, to be training directly
on such a large domain.
We do not have the computational resources to do that.
And we also aren't sure exactly how much data
you would need to be able to train on such a model.
The now and all implementation, in that implementation,
the training is done on 16 by 16 pixel low resolution
patches and 128 by 128 pixel high resolution patches.
So what we will do is we will adjust to that type of training.
And in order to do that, the strategy that we have adapted
is to just select random patches from one forecast
from our HRDPS data set.
We will re-read the GDPS data set on the HRDPS domain
and then course-crain it to go back to its resolution.
And in this way, we are going to end up
with this type of patches that we'll
use to do the training.
So here is the high resolution data patch
and the corresponding GDPS low resolution data patch.
So at each epoch, we are using between 300 and 700 random
patches.
And the model that we are showing today
has been trained on 17,370 epochs.
So this took about 149 hours to train on one GPU.
And we have done more than two passes
through the entire data set.
So once you have a trained model,
you can perform inference with the GDPS data.
And that inference will also be done on 16 by 16 pixel patches.
So here I have a GDPS input.
We perform inference.
And like this, we obtain the downscale forecast,
the downscale U and V fields.
And just as a comparison, we have here the HRDPS forecast.
So the first things that we can say
is that we are definitely downscaling.
So we are obtaining information at a small scale.
It isn't as much as the HRDPS is showing.
But as I was mentioning before, one problem
with a low resolution is the fact that you have biases.
And we are definitely achieving some sort
of a bias correction.
Now, of course, you have to parse the entire HRDPS domain.
One way to do that would be to sequentially process
128 by 128 patches.
But that would result into artifacts at the borders.
So the strategy that we have adopted instead
is to do some overlapping and then
to take the median of the ensemble of overlaps.
And in this way, we managed to patch and obtain this figure
over the entire domain.
We have performed validation of our model.
So we have used the test data, the 1,200 forecast from the GDPS.
And here I am showing the root mean square error
for the U-wind component and for the V-wind component.
And on the bottom, I'm showing the mean absolute error.
And these are the metrics that are computed
between the downscale GDPS and the HRDPS corresponding
verification.
And we are comparing it with some baselines that
can be used for interpolation.
So I'm showing in orange you have bilinear interpolation
and in green you have nearest neighbor interpolation.
So as we are seeing in all the metrics,
the downscale is showing better results,
is performing better than the other types of interpolation
for our test data set.
We have also done a power spectrum analysis
in order to really quantify how much detail we
are getting at the small scales.
And here I'm showing the radially average power
spectral density between HRDPS in blue and the downscale
forecast in orange.
And these power spectra are average over the test data
set as well.
What we are seeing is that indeed at small scales,
we are still not getting enough power.
So we do not get enough detail.
Madelina, if you could lend this in one or two minutes.
Sounds good.
So of course, we are training so far with one year of data.
And it would be very interesting to see
how much more detail we can obtain by training further.
I'm just showing also an integrated measure
of the difference in the power spectra.
And what you are seeing here is that compared
to the bilinear and the nearest neighbor interpolation,
the downscaled AI downscaling performance much, much better.
So it's able to recover way more structure
at the small scales.
So the next steps with our forecast, with our project,
the WGAN needs further development and testing.
So we are planning on testing with more data.
A very important thing that we are working on right now
is to add other covariates.
So as I said, for now, we are only using windfields.
But we are adding topography, surface pressure,
and we are thinking about what other covariates
may be such escape to add to our model.
And once we have a baseline that we are satisfied with,
the important part comes.
And that is doing a thorough meteorological verification
of the downscale forecast.
Because like I said, we have operational goals.
And we really want to see how this forecast
respond to our needs.
Finally, once we finish this first step of the project,
we are planning on moving to the second more ambitious part,
which is to develop a large GI model that
is based on a pre-trained foundation model
that we will be fine-tuning in order
to obtain downscale forecasts.
And this is where, again, collaborating with IBM Research
is extremely important, as they have very much experience
in these foundation models.
And we are hoping to advance at least as fast as now.
So this is it for me.
Thank you very much.
Thank you, Medellina.
I have a question in the chat.
I have one.
I saw that you train, especially, the model.
But I was surprised to see that you only
used one year of data to do that.
Is there a reason behind that?
Because it seems to me that it's not a lot of data.
Yes, well, like I said, we have.
So we do end up having a lot of forecast samples.
We are training on 128 by 128 pixel patches out
of a very large domain.
So it ends up being a lot of data.
But it is not finalized.
So here we are in a developing mode.
We are trying to develop the model
and make sure it's working properly.
And this is just the first step.
We are definitely planning on adding more data
and seeing how we can improve.
Thank you, Medellina.
I don't have a question.
Otherwise, Anne, I'm going to speak.
Right on time.
Thank you, Medellina.
So now we're going over to Reynel Sospedra Alfonso, who
is also a research scientist at Environment and Climate
Change Canada.
He will be presenting on deep learning-based bias
adjustments of Arctic sea ice forecasts
from version three of the Canadian seasonal to
inter-annual prediction systems.
Also known as CANSTEP version three.
So, Reynel, the mic is all yours.
Thank you.
Thank you, Anne.
Can you hear me well?
Yes, we do.
Your presentation is not full screen, though.
It's not.
No, it's not.
All right.
How about now?
Yes, it is.
Perfect.
Thank you so much, Medell.
Thank you, yes, to the organizer for this opportunity.
My name is Reynel Sospedra Alfonso.
I'm a research scientist at the Canadian Center for Climate
Modeling and Analysis, based in Victoria.
And I want to start by acknowledging the contributions
or the work of colleagues at CCMA,
which made this type of project possible.
I list some of them down here.
And I also want to acknowledge the co-authors
of this presentation, or this work,
Joseph Martin, Michael Simon, and especially
Parca Guilla, who has been key for this project going forward.
He has taken the time to do the implementation, training,
and testing of the models we have been looking at.
And I want to mention that this is part, what I'm going to talk
about here is part of a bigger project
that we are trying to pursue at CCMA, which
is the use or applications of machine learning methods,
in particular deep learning, to post-process our seasonal
to the scale forecast.
So for this talk, I will talk in particular
about the post-processing or seasonal forecast of CIS.
The seasonal forecast, as you may know,
CANSIPS is the Canadian seasonal and internal prediction
system, which provides the Environment and Climate Change
Canada's operational, probabilistic seasonal
forecast, both national and global.
And CANSIPS first appeared or was first
debuted in 2011 as a two-model forecasting system.
It has evolved since, and now we are in 2024,
with the new version of CANSIPS B3,
which actually will be launched next month.
And so here, what I'm going to talk about
is the forecast that we produce with CANIAS M5, which
is a new model that now we'll be using in CANSIPS.
And CANIAS M5 is an air system model that is produced at CCMA,
and now will be then, as I said, used for our seasonal forecast.
CANIAS M5 air system models, so it
couples the atmosphere, the ocean, CIS component, land,
and also biochemistry, both on land and the ocean.
What we do, we take our climate model,
we initialize the climate model following the indications
that you see here on the right.
So when we initialize the forecast,
we take, we notch the model towards re-analysis,
and then we launch those forecasts in time.
The version of CANIAS M5 that I'm going to be talking about
is actually an optimal bias-corrected version, which
follows the work by Sino-Kankarin, which
does an online bias optimization to the model.
Now, the work that I'm going to be presenting to you
deals with the post-processing of those forecasts.
So here, what you see is a representation
of those forecasts in black.
It's the observations that we are verifying,
observations, the monthly values.
And then what you see is the representation
of those forecasts, which are initialized
at the start of every month during the handcast period.
So we'll be looking at handcasts from 1980 to 2021.
We have, for each month in that time,
we launch an ensemble of forecasts of 10 members,
which run for 12 months.
The variable of interest for us here
is CIS concentration, which is simply
the fraction of CIS, or the fraction of the grid cells
that is covered by CIS.
And this is what we want to adjust.
Well, we do that looking at the ensemble mean forecast.
So the adjustment is not done to the ensemble itself,
it's done to the ensemble mean.
And the question is, why do we have to do that?
I mean, after all, we are even doing
an online bias optimization or bias correction
of my model.
So still, we do need to adjust those forecasts,
because as we know, we have several sources of error,
structural errors, errors due to initialization,
and so forth.
And typically, this is done by doing
some climatological bias correction to those forecasts.
Now here, just to give you an example,
I'm showing you the September CIS concentration
over the time period 2006 to 2020.
On the left, these are the very fine observations
that we use, which comes from NOAA data products.
And here, again, this is the CIS concentration,
which has value from 0 to 1.
And we see on the right, the growth forecast
as it comes out of the model.
So this is the output directly coming from our forecast.
And what we see is that there is definitely
a strong bias that we notice, particularly
in the center Arctic, where we have a much lower CIS
concentration relative to observations.
Now on the right to that, we see the bias adjusted forecast,
in which we compensate or we subtract the bias,
compute it on a previous time.
And then what we see is that we adjust somehow
that forecast by doing this simple bias correction.
Now still, even after doing a bias correction,
we see that there are some differences
between the observed CIS concentration
and the one that is bias adjusted.
So that tells us that we need to do something else in order
to actually improve our forecast.
And for that, and this is now the project
that we are working on, is to use this machine learning
or deep learning method to improve even further
those forecasts.
The method that I'm going to use here,
and I'm going to talk a little bit more about that
in the few slides, is the unit.
Probably most of the people in the audience
are familiar with units.
As I said, I will talk a little bit more about that.
But here, I'm just showing you some results in which you see
how the unit is able to reproduce the spatial pattern a lot
better than what we can do with a simple bias correction.
I should have said that this is a forecast done
at two monthly time.
So this is the forecast two months
after the forecast is initialized,
where the biases are, you can see, are particularly strong.
Now, the unit, this is the tool of choice.
What we have here is a fully connected network,
or a fully convolutional network,
that has a downscaling, down sampling encoder,
followed by up sampling decoder.
This is a classical unit.
What we see is that the future maps are reduced in size.
So we see that the resolution is reduced by its health,
and the channels are increased by two.
And this allows us to have a better representation
of capacity of the network, while preserving some information
of the image that we input.
So to be clear, what we input here is our raw forecast,
which is denoted by the YMN.
Thanks, and will be the resolution of my forecast,
which is a function of the initial month
and the target month, and the time relative to the first month
in the data that we are inputting.
So the input is made of six channels,
one channel that is the actual variable
that we want to, or the actual map that we want to correct,
plus five temporal features that takes into account,
again, the initial month that we're looking at,
the target month, and this temporal information
relative to the initial time.
So in other words, how many years and how many months
from the starting of your data.
You input that into the network,
and then the output is, hopefully,
is an adjusted forecast,
which then correct for those biases
that I mentioned earlier.
Moving a little bit forward here,
let me just be a little bit more precise
on the kind of task that we are doing.
This is just a specific example,
which hopefully will clarify how we do this.
So in this case, let's say that we want to adjust
the March CIS concentration forecast,
which is initializing February of a year Y.
So essentially it will be this red dot denoted here
on the figure.
So again, what we want to do is to post-process this forecast.
We leverage the forecast that are produced
with our climate model.
We do not do prediction.
We just do that kind of bias adjustment
or post-processing of the predictions
that we get from our climate model.
So for this specific task, to correct that March CIS,
we train on the data that is available for the years
before the test year that we have.
So in this case, I'm denoting that here
for this in this shadow region.
So we take all the pairs of forecast and observations
for all lead times and all target month,
and then we train our network on that data,
and then we will do that iteratively for every test
years that we want to make the adjustment.
So what I'm going to do now is to show you some of the results
that we have.
I should say that this is very much a work in progress.
And that is different avenues that we are working on,
but we have some results that I want to share with you.
For instance, what we see here is the CIS concentration bias.
At the zero-month lead, average over the 2006 and 2020 time
period, which is the test years that we have for our analysis.
At the top, we have the bias for the March CIS concentration,
which is at the time of maximum CIS extent.
And at the bottom, we have the results or the bias
for the September CIS concentration,
which is at the time of a minimum CIS extent.
On the left, we see here what happened with the raw forecast.
We see the biases happening in several regions,
which is actually substantial.
Here, the bias is given in percent.
For the bias adjusted case, which
is the simple bias correction that I'm using here
as a benchmark, we see that we do improve
relative to the raw forecast, but there are still
some biases that are apparent, particularly
during the CIS minimum.
And then on the right is the results
that we get with the unit.
And we see that the bias are likely removed.
And now, this is for the zero-month lead.
So this is soon after we initialize our forecast.
Now, two months ahead in our forecast,
of course, the biases are increased.
Excuse me, we have two minutes.
Two minutes.
Thank you.
All right, so here is the case in which we
have two-month lead forecasts.
And we see that the bias, of course, are increased.
But still, we are able to manage those biases with the unit,
given a better representation of the CIS edges.
Another measure that we look at here
is the integrated ICH error, which is essentially the area,
the integrated area that we have,
in which both forecast and observations
disagree in the concentration with a threshold of 15%.
So we both have more than 50% concentration
or less than 50%.
This binary error will be zero.
If they are different, then the binary error will be one.
And those grid cells will contribute to this area error.
So this is at the top here, we see a hit map
in which we have our target month in the X axis.
And on the Y, we have the lead month.
And bottom message here is that the blue are bad.
The red are good, meaning there is less error.
And the unit bids both the row, of course,
and the bias are used to forecast.
Down here is just an integration over lead month
just to give a more clear picture
of how the unit outperforms the alternative.
Now, I know that I don't have much time,
so I will not go into the details of these results,
but at least to give you an idea of what is happening.
In this case, we are looking at the CIS area.
Same time of floods.
Again, red means that we have a better room mean
and square error, which is the measure
that we are using here for the CIS area.
Blue means that the errors are increased.
And in this particular case, we see that the unit
is slightly better than the bias corrected one.
However, if we look at CIS extent,
which is now the area in which we count for all those grid cells
with concentration greater than 50%,
we see that unit largely outperform
this bias-adjusted method.
Finally, I want just to mention that here,
we have seen different measures of the skill
in which we outperform both the benchmark
and the raw forecast, but there is still some issues
in terms of the representation of the temporal dependence
of our adjusted forecast.
And so, but unlike for this slide here,
just to mention that we still have some work to do
to be able to better represent the temporal dependence
of those forecasts.
And because I don't have much more time,
I will finish with this slide, which is some final remarks.
We'll leave it there for you.
And I will be happy to take any questions.
Thank you.
Thank you, Renel.
I saw that there seems to be to have a seasonal pattern
in your verification.
That leads me to the questions.
Are there any biases that are harder to adjust
with this method?
The answer for that, yes.
And this is anality that we see has to do in part
with how the eyes behave.
We have what is known as this predictability barrier
after spring, which makes it difficult
to do a good prediction of the sea eyes.
This is something that perhaps we can see
all those metrics, actually perhaps we'll go to this one.
So this barrier, which reduces the skill
that we have in our predictions,
can be seen here around the month of June to October,
looking here at target month.
And so that is something within the raw forecast
in person, the bias corrected one, but also in the unit.
So we do improve on those months,
but still there is some skill that is missing there,
which is part of the natural process
in the sea eyes formation and melting,
which then translate into the skill
that we see in our just forecast.
Thank you.
One last question is coming from the online Q&A.
Would training the model on all months of the year
versus only certain months affect its ability
to improve the forecast?
For example, forecasts of sea eyes in spring
are known to perform poorly.
So if you were to exclude these months,
would this improve the bias adjustment?
Yeah, that's a good question.
Actually, when I was, I mean, it's too bad
that I'm rushing through all these slides,
but one of the things that I wanted to mention
at this particular slide is that we do the training.
You're looking at the previous years in my forecast,
but we are missing some of the information
of more recent months that can also contribute
to the forecast.
Like looking here, for instance,
we do not use information from January
of this specific year.
Now, one thing that we are exploring
is to look at training based on lead times
or a specific month.
I think that that's the idea of the question,
which we try to train our model specific to the lead times
in which we have the biases that we want to correct
and the kind of information specific to the seasonality
that we want also to correct.
So those are things that we are exploring,
but so far this is what we have.
Thank you, Rénel.
This concludes our first block for the first part
of the presentation for artificial intelligence.
A big thank you to Rénel, Madlena, Saïd, Hervé,
and Christopher for having made these presentations.
Anne, I'll leave you here.
Yes, a big thank you to all the presenters,
all the attendees as well.
And as several mentioned before,
there is a second session coming up in about 20 minutes,
not even.
So we'd love to see you back here,
a lot more to see and to hear from people outside ECCC
as well.
So please do join us.
Thank you.
There's not a sign.
There's not a sign, that's it.
Thanks.
You're better.
Bye.
Bye.
Thank you.
