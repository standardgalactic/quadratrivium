WEBVTT

00:00.000 --> 00:05.000
And we are going to see connections of language models

00:05.180 --> 00:08.080
that maybe you did not quite expect to anticipate,

00:08.080 --> 00:09.080
but they are very real.

00:09.080 --> 00:11.920
And to start with, we are very lucky to have

00:11.920 --> 00:15.880
Steven Pantadosi, he is a professor of psychology

00:15.880 --> 00:17.520
and neuroscience at UC Berkeley.

00:17.520 --> 00:22.520
And a friend of AI, we actually have a grant together.

00:23.480 --> 00:28.480
And a lot of us in AI are excited and interested

00:29.480 --> 00:32.080
in learning from the psychologists

00:32.080 --> 00:35.200
and seeing how they can inspire our work.

00:35.200 --> 00:37.320
Well, actually, this is a two-way street

00:37.320 --> 00:40.320
and because the psychologists are also getting excited

00:40.320 --> 00:43.280
about the language models to understand

00:43.280 --> 00:44.280
something about humans.

00:44.280 --> 00:47.840
And Steven will tell us about some of the work

00:47.840 --> 00:49.800
he has been doing, which I think is very exciting.

00:49.800 --> 00:50.640
So, Steven.

00:52.720 --> 00:55.840
So thank you for the invitation to speak here.

00:55.840 --> 00:57.560
I'm going to be talking about the meaning

00:57.560 --> 00:59.040
in the age of large language models

00:59.040 --> 01:00.640
and maybe finding meaning in the age

01:00.640 --> 01:01.480
of large language models.

01:01.480 --> 01:04.880
And this talk isn't a kind of technical talk

01:04.880 --> 01:08.640
about language models or evaluation or anything.

01:08.640 --> 01:12.040
It's almost closer to philosophy

01:12.040 --> 01:14.360
or closer to kind of high level theories

01:14.360 --> 01:17.200
in cognitive science and psychology,

01:17.200 --> 01:18.880
which are about meaning.

01:18.880 --> 01:20.680
And some of the kind of history of work

01:20.680 --> 01:24.280
of people's ideas about where meaning comes from

01:24.280 --> 01:26.720
in language or in semantics

01:26.720 --> 01:27.800
and how we can think about those

01:27.800 --> 01:29.480
in the context of large language models

01:29.480 --> 01:31.640
and in particular in the context of debates

01:31.640 --> 01:33.000
about large language models

01:33.000 --> 01:36.240
and whether they're just stochastic parrots

01:36.240 --> 01:38.920
that don't have any form of understanding

01:38.920 --> 01:41.360
or whether there's some sense in which they have understanding

01:41.360 --> 01:44.200
and if they do have some form of understanding,

01:44.200 --> 01:46.800
if that form of understanding is at all related

01:46.800 --> 01:50.800
to the types of understanding that people have.

01:50.800 --> 01:54.680
So my plan for the talk is first to talk about meaning

01:54.680 --> 01:56.120
and reference kind of generally.

01:56.120 --> 02:00.080
I'll talk about some claims in large language models

02:00.080 --> 02:02.840
and why people often think that there's no sense

02:02.840 --> 02:06.640
of meaning or kind of real semantics in them.

02:06.640 --> 02:10.840
Then I'll talk about some psychological theories

02:10.840 --> 02:12.760
about meaning and how meaning arises

02:12.760 --> 02:15.120
from what are called conceptual roles.

02:16.000 --> 02:17.400
I'll come back to large language models

02:17.400 --> 02:21.200
and talk a little bit about learning conceptual roles

02:21.200 --> 02:22.360
in large language models

02:22.360 --> 02:24.840
or in kind of general machine learning systems.

02:25.080 --> 02:27.560
And then some very kind of brief overview

02:27.560 --> 02:31.920
of a learning conceptual role experiment in people.

02:31.920 --> 02:36.800
So let me start with kind of how I got interested in this

02:36.800 --> 02:40.320
which was this paper by Bender and Kohler in 2020.

02:40.320 --> 02:43.680
Emily Bender is a computer scientist

02:43.680 --> 02:47.560
trained as a linguist also at the University of Washington

02:47.560 --> 02:49.760
who people may know as a very vocal critic

02:49.760 --> 02:53.480
of many aspects of large language models.

02:53.520 --> 02:56.120
The one that initially I think interested me

02:56.120 --> 02:59.720
was claims about the meaningfulness of large language models

02:59.720 --> 03:03.600
and essentially arguments that there's nothing meaningful

03:03.600 --> 03:06.760
at all in what statistical models

03:06.760 --> 03:09.000
that are trained on text can do.

03:10.080 --> 03:12.600
And Bender and Kohler came up with a very nice way

03:12.600 --> 03:17.600
of making this point, what they call the octopus test.

03:17.600 --> 03:19.560
The octopus test goes as follows.

03:19.560 --> 03:22.440
So kind of starting point is for them,

03:22.440 --> 03:26.000
meaning is an association between a word, say,

03:26.000 --> 03:28.000
and something external to language.

03:28.000 --> 03:30.120
Okay, so the meaning of the book

03:30.120 --> 03:34.400
is some physical object, a book that's out in the world.

03:34.400 --> 03:35.400
You can see that that's right

03:35.400 --> 03:37.920
because that's how we label physical objects

03:37.920 --> 03:39.320
and when we're learning words, right?

03:39.320 --> 03:41.400
We hear the word book when there's books around

03:41.400 --> 03:42.960
and we pick up on that association.

03:42.960 --> 03:44.560
And so that's kind of fundamentally

03:44.560 --> 03:47.920
what the, that kind of external reference, right?

03:47.920 --> 03:50.680
To something in the real world is fundamentally

03:51.560 --> 03:52.800
what the word means.

03:54.120 --> 03:55.640
And Bender and Kohler say, okay,

03:55.640 --> 03:58.520
let's take that as our definition of meaning

03:58.520 --> 04:00.160
and let's imagine an octopus.

04:00.160 --> 04:03.080
So an octopus who lives under the ocean

04:03.080 --> 04:06.240
and has tapped into a communication channel,

04:06.240 --> 04:10.520
say a telephone line between two islands, okay?

04:10.520 --> 04:11.360
So the octopus is there,

04:11.360 --> 04:13.240
it's eavesdropping on all of the communication

04:13.240 --> 04:15.160
that happens between those two islands.

04:15.160 --> 04:17.960
You can get a huge amount of linguistic input

04:18.960 --> 04:22.160
and you could imagine very smart octopus

04:22.160 --> 04:25.160
might be able to learn all of the statistical properties

04:25.160 --> 04:28.440
of what's happening across that communication channel, right?

04:28.440 --> 04:32.040
Might be able to learn, become very good at predicting text.

04:32.040 --> 04:34.720
Maybe it could predict it optimally, whatever.

04:35.720 --> 04:38.080
Their argument is that the octopus

04:38.080 --> 04:40.000
could never actually learn the meanings, right?

04:40.000 --> 04:41.320
Because it would never have access

04:41.320 --> 04:43.520
to the physical reference, right?

04:43.520 --> 04:44.360
So yeah.

04:44.360 --> 04:46.520
Is it really different from the Chinese room?

04:46.680 --> 04:50.040
Yeah, so it's interestingly a little different

04:50.040 --> 04:54.160
from the Chinese room and maybe can I defer that question

04:54.160 --> 04:55.600
to the end if that's okay?

04:56.560 --> 04:58.200
Because I think what's going on with the Chinese room

04:58.200 --> 05:01.160
might make more sense with what I say later, okay?

05:01.160 --> 05:03.480
But very similar in spirit, I would say.

05:04.920 --> 05:08.720
So this octopus has no access to the physical reference

05:08.720 --> 05:10.080
and therefore couldn't solve tasks

05:10.080 --> 05:11.800
involving the physical objects, right?

05:11.800 --> 05:14.880
If you ask them to visually recognize what a coconut was

05:14.880 --> 05:17.440
even if they knew all of the statistical properties

05:17.440 --> 05:19.360
of where the word coconut would be used, right?

05:19.360 --> 05:22.960
They wouldn't be able to solve the physical stuff, okay?

05:22.960 --> 05:24.320
And of course this is the situation

05:24.320 --> 05:26.400
that a large language model is in,

05:26.400 --> 05:29.080
at least one that's only trained on text, right?

05:29.080 --> 05:30.920
It doesn't get access to stuff from the world,

05:30.920 --> 05:35.120
it only has kind of text, okay?

05:36.200 --> 05:39.600
Therefore this octopus doesn't have meanings for the words.

05:40.440 --> 05:42.120
Such an octopus is like a large language model

05:42.120 --> 05:43.960
because they only have text.

05:43.960 --> 05:45.440
And predictive ability therefore

05:45.440 --> 05:47.320
can't give you the meanings, right?

05:47.320 --> 05:49.400
Meaning is something just fundamentally different

05:49.400 --> 05:51.760
than predictive ability.

05:53.440 --> 05:55.480
Let me give you one other example of this.

05:55.480 --> 05:57.880
Well, I'll just say, I think that on the surface

05:57.880 --> 06:00.560
this is a somewhat convincing argument, right?

06:00.560 --> 06:05.560
It's kind of compelling to think of meaning in this way.

06:05.840 --> 06:08.960
And certainly if you do, it seems pretty convincing, yeah.

06:09.800 --> 06:12.040
It may not be right in terms of your mathematical history

06:12.040 --> 06:14.520
but it's given me a couple of axioms.

06:14.520 --> 06:17.640
And you know, you're not supposed to have,

06:17.640 --> 06:20.800
because this is really referring to

06:20.800 --> 06:23.160
this particular interpretation of that one.

06:23.160 --> 06:25.240
This would make that up in your mind.

06:25.240 --> 06:27.840
Yeah, great, so hold on to that thought too

06:27.840 --> 06:29.640
because a mathematician thinking about axioms

06:29.640 --> 06:32.680
is very much related to another version of meaning

06:32.680 --> 06:36.040
that I'll talk about in a few minutes, okay?

06:36.040 --> 06:39.200
Even as Hilbert wanted it to be, right?

06:39.200 --> 06:42.520
Hilbert's desire was to convert Euclidean geometry

06:42.520 --> 06:45.360
to a set of axioms such that every symbol

06:45.360 --> 06:48.120
could be replaced by some arbitrary squiggle

06:48.120 --> 06:50.320
and the system should still work.

06:50.320 --> 06:54.400
That is what Hilbert did in the axiomatization of geometry.

06:54.400 --> 06:59.400
Yeah, so I think most mathematicians

06:59.480 --> 07:01.680
would have a slightly different sense of meaning

07:01.680 --> 07:03.760
but one which matches what I'll say

07:03.960 --> 07:05.760
in a few minutes, so yeah.

07:06.760 --> 07:09.720
Let me give you just another kind of gloss

07:09.720 --> 07:12.060
on meaning and language models.

07:12.060 --> 07:15.200
Here's Gary Marcus talking about lambda.

07:15.200 --> 07:17.760
In truth, literally everything the system says is bullshit.

07:17.760 --> 07:20.200
The sooner we realize that lambda's utterances are bullshit,

07:20.200 --> 07:22.000
just games with predictive word tools

07:22.000 --> 07:24.360
and no real meaning, the better off we'll be.

07:25.480 --> 07:27.320
Software like lambda doesn't even try to connect

07:27.320 --> 07:28.560
to the world at large, right?

07:28.560 --> 07:30.880
That's what he thinks makes it bullshit.

07:30.880 --> 07:32.840
Just tries to be the best version of autocomplete

07:32.920 --> 07:33.760
that it can be.

07:33.760 --> 07:34.760
They don't understand language

07:34.760 --> 07:37.720
in the sense of relating sentences to the world

07:37.720 --> 07:41.100
but just sequences of words to one another, okay?

07:42.600 --> 07:46.600
So I got interested in this in part

07:46.600 --> 07:49.440
because I find this view kind of compelling.

07:49.440 --> 07:51.840
On the other hand, I also think it's kind of deeply wrong

07:51.840 --> 07:53.640
and the way in which it's deeply wrong

07:53.640 --> 07:56.320
is really interesting for what it has to say

07:56.320 --> 08:00.040
about conceptual representations and meanings

08:00.040 --> 08:04.160
in human minds as well as in machine learning models.

08:04.160 --> 08:09.160
So a few years ago, I teamed up with Felix Hill,

08:10.000 --> 08:12.840
who's a researcher at DeepMind

08:12.840 --> 08:17.840
and wrote an article basically going through arguments

08:18.200 --> 08:20.640
that meaning is not this form of reference, right?

08:20.640 --> 08:23.560
So in fact, this idea that meaning should be equated

08:23.560 --> 08:26.000
with some mapping to things in the world

08:26.000 --> 08:28.280
has often been rejected by people in linguistics

08:28.280 --> 08:31.040
and philosophy and cognitive science.

08:31.040 --> 08:33.240
And I think for good reason,

08:33.240 --> 08:34.840
so just to give you a kind of flavor

08:34.840 --> 08:38.720
of why people often reject this,

08:38.720 --> 08:41.120
there's many concepts, many words, for example,

08:41.120 --> 08:44.320
that have no reference to the external world, right?

08:44.320 --> 08:46.120
Function words are a good example of this.

08:46.120 --> 08:49.040
Words like to or is or many, right?

08:49.040 --> 08:51.680
There's no to, to, to out in the world

08:51.680 --> 08:53.960
that that word refers to.

08:53.960 --> 08:57.360
There's also no is out in the world or no many.

08:57.400 --> 08:59.240
Those are function words in language

08:59.240 --> 09:02.600
and what they do is actually much more like

09:02.600 --> 09:06.720
kind of what an operator in mathematics or something does,

09:06.720 --> 09:07.560
right?

09:07.560 --> 09:09.840
These words have an internal meaning.

09:09.840 --> 09:13.640
They control the kind of compositional meaning of a sentence,

09:13.640 --> 09:15.880
meaning that they have to be composed

09:15.880 --> 09:18.320
internally in linguistic representations

09:18.320 --> 09:19.960
in order to express their meaning, right?

09:19.960 --> 09:23.240
They're not pointing to something out in the world.

09:23.240 --> 09:25.240
Even if you don't go to function words like this,

09:25.280 --> 09:28.960
there's other words which are very hard to make sense of

09:28.960 --> 09:31.680
in a kind of view that meaning is stuff in the world.

09:31.680 --> 09:35.080
So you can think of very abstract words like justice, right?

09:35.080 --> 09:36.800
There's probably not a justice out there.

09:36.800 --> 09:40.120
That's some kind of construct that we have or wit,

09:40.120 --> 09:42.560
or you can think of things that don't exist like dragons,

09:42.560 --> 09:43.400
right?

09:43.400 --> 09:47.240
There's no external thing in the world, which is a dragon.

09:47.240 --> 09:49.360
There's even words and concepts we have

09:49.360 --> 09:52.080
that have no possible reference to the world, right?

09:52.080 --> 09:54.280
So if I think about an imaginary bicycle,

09:54.320 --> 09:56.800
that's something which is by definition imaginary, right?

09:56.800 --> 10:00.440
It's not out there, or a perpetual motion machine, right?

10:00.440 --> 10:03.040
We can have a concept of a perpetual motion machine

10:03.040 --> 10:04.920
and think about it and reason about it,

10:04.920 --> 10:07.920
but there's certainly not one that exists out in the world.

10:09.760 --> 10:11.840
Even for ordinary concepts,

10:11.840 --> 10:14.880
we likely haven't even considered all of the possible things

10:14.880 --> 10:17.360
which could be reference of those concepts, right?

10:17.360 --> 10:21.680
So I could walk in wearing a shoe made out of eggplants

10:21.680 --> 10:24.000
and you could look at them and everybody might agree

10:24.040 --> 10:26.520
that they're shoes, right?

10:26.520 --> 10:27.720
But you would agree that they're shoes

10:27.720 --> 10:30.960
without ever having seen shoes made of eggplants before, right?

10:30.960 --> 10:32.720
So there's some object in the world

10:32.720 --> 10:35.160
which everybody would agree is a shoe.

10:35.160 --> 10:37.140
Even though you've never encountered that thing before,

10:37.140 --> 10:38.560
that means that it couldn't have been the stuff

10:38.560 --> 10:41.560
in the world which determined whether it was a shoe, right?

10:41.560 --> 10:43.720
Had to be some kind of more abstract conception

10:43.720 --> 10:46.280
of what makes something a shoe, right?

10:46.280 --> 10:48.040
You can think about things like the function

10:48.040 --> 10:51.000
or the origin, how they're used.

10:51.000 --> 10:53.200
These other kinds of properties of objects

10:53.200 --> 10:55.760
seem to be much more important for the categorization

10:55.760 --> 10:58.000
of the concept, yeah.

10:58.000 --> 11:01.320
Well, here this is a little bit of a survivorship bias

11:01.320 --> 11:05.400
because eggplants shoes might still be considered shoes,

11:05.400 --> 11:08.120
but then ice cream shoes are probably,

11:08.120 --> 11:10.360
nobody will recognize them as shoes, right?

11:10.360 --> 11:11.880
But then you don't think about ice cream shoes.

11:11.880 --> 11:15.440
So it's like the things that you can think of as shoes

11:15.440 --> 11:17.440
are in your little bowl

11:17.440 --> 11:18.800
and then you don't think about things

11:18.800 --> 11:19.840
that are already too far.

11:19.840 --> 11:21.920
So it seems like there is still kind of some distance

11:21.920 --> 11:24.520
to the closest real object.

11:24.520 --> 11:25.360
Yeah, yeah.

11:25.360 --> 11:30.360
So all of this is not to say that the real objects

11:31.240 --> 11:32.640
are irrelevant, right?

11:32.640 --> 11:35.280
Like I agree that eggplants are much more plausible issues

11:35.280 --> 11:38.840
than ice cream and that has to do with the kind of real

11:38.840 --> 11:42.080
physical properties of those substances.

11:42.080 --> 11:43.880
My point is just that the physical thing

11:43.880 --> 11:45.600
is not the defining thing, right?

11:45.600 --> 11:47.400
It's not something in the object you look to

11:47.400 --> 11:49.080
to decide whether it's a shoe or not, right?

11:49.080 --> 11:51.900
It's something more abstract about how it's used

11:51.900 --> 11:53.300
or made or something like that.

11:55.380 --> 11:57.380
I'll actually talk a bit in this paper

11:57.380 --> 12:01.780
about the concept of a postage stamp, right?

12:01.780 --> 12:05.060
Which is just an example of one that people probably

12:05.060 --> 12:08.980
have some intuitions about where you could easily think

12:08.980 --> 12:11.580
of postage stamps which are fundamentally different

12:11.580 --> 12:12.940
than anyone's you've seen before.

12:12.940 --> 12:15.180
You could think of one made of glass, for example,

12:15.180 --> 12:17.900
or you could think of one that was an RFID tag,

12:17.900 --> 12:20.500
which is probably physical incarnations

12:20.500 --> 12:22.940
of postage stamps like that that everybody would agree

12:22.940 --> 12:25.420
should be called a postage stamp.

12:25.420 --> 12:27.940
And if you try to get people to define it, right?

12:27.940 --> 12:29.180
You might say something like, well,

12:29.180 --> 12:30.620
a postage stamp is something you pay for

12:30.620 --> 12:32.260
and you put on a letter so that the letter

12:32.260 --> 12:33.540
will be delivered by the government

12:33.540 --> 12:34.620
or something like that, right?

12:34.620 --> 12:39.620
So what the term means is intrinsically connected

12:39.620 --> 12:42.900
to a bunch of these other terms like payment and letters

12:42.900 --> 12:45.060
and being delivered and those things.

12:45.060 --> 12:47.940
And in fact, if those terms change meaning, right?

12:47.940 --> 12:50.980
So if, for example, people develop a new way

12:50.980 --> 12:53.540
of paying for things, paying on the blockchain

12:53.540 --> 12:54.380
or something, right?

12:54.380 --> 12:56.280
Then you kind of know automatically

12:56.280 --> 12:59.060
that a postage stamp can be paid for in that way,

12:59.060 --> 13:00.500
at least in principle, right?

13:00.500 --> 13:03.380
So it's not just that the word is associated

13:03.380 --> 13:05.140
with those other things, but that its meaning

13:05.140 --> 13:07.940
is inherently connected to those other things.

13:09.940 --> 13:14.940
So that's one kind of take on why reference to stuff

13:18.020 --> 13:19.060
out in the world, right?

13:19.060 --> 13:22.860
Is not a good way of thinking about meaning.

13:22.860 --> 13:25.500
Let me tell you what one alternative is.

13:25.500 --> 13:28.460
Actually, before I do that, let me just show

13:28.460 --> 13:29.620
a couple of other alternatives,

13:29.620 --> 13:31.620
which I think are also not plausible,

13:31.620 --> 13:34.020
but might be familiar to people, okay?

13:34.020 --> 13:36.460
So what I just talked about is this kind of

13:36.460 --> 13:37.620
world mapping view, right?

13:37.620 --> 13:39.500
That there's some word and its meaning

13:39.500 --> 13:42.400
is some physical object or some thing.

13:44.100 --> 13:45.500
You can think about other kinds of views

13:45.740 --> 13:49.140
of concepts and meaning might have a kind of

13:49.140 --> 13:50.500
feature spacey kind of views,

13:50.500 --> 13:52.020
port vector machines or something, right?

13:52.020 --> 13:54.060
There's some abstract feature space

13:54.060 --> 13:56.640
and a concept is some dividing line

13:56.640 --> 13:59.100
or some region or something in this space.

14:01.340 --> 14:06.180
That I think is maybe fine in some narrow applications,

14:06.180 --> 14:09.980
but what I'll talk about next are cases of, say,

14:09.980 --> 14:12.860
human cognition, which really don't fit well

14:12.860 --> 14:14.820
into that picture, in the sense that things

14:14.820 --> 14:16.660
are much more complicated for how people think

14:16.660 --> 14:19.740
about concepts and their relationships.

14:19.740 --> 14:22.060
People might also have this sort of hierarchy

14:22.060 --> 14:23.380
or network view, right?

14:23.380 --> 14:25.580
So sometimes people think, oh, sorry, you can't see this.

14:25.580 --> 14:27.880
There's supposed to be lines connecting

14:27.880 --> 14:30.100
one concept book in the middle to a bunch

14:30.100 --> 14:31.460
of other concepts, right?

14:31.460 --> 14:34.380
And you might, you know, there's old theories

14:34.380 --> 14:36.980
of, say, semantic organization or very old,

14:36.980 --> 14:39.100
old AI kind of approaches, right?

14:39.100 --> 14:43.020
That think about building hierarchies of concepts

14:43.940 --> 14:46.620
or sometimes networks of concepts

14:46.620 --> 14:49.500
and trying to define meaning in terms of those relationships.

14:49.500 --> 14:52.060
I actually think that both this and the feature

14:52.060 --> 14:55.380
and the world mapping view have some of the,

14:55.380 --> 14:58.620
some kind of useful properties or useful insights

14:58.620 --> 15:01.340
about concepts, but just aren't quite the whole thing

15:01.340 --> 15:04.020
for reasons that I'll talk about next.

15:04.020 --> 15:08.500
So let me just start with, start trying to introduce

15:08.500 --> 15:11.060
this kind of other view of concepts

15:11.060 --> 15:13.580
by trying to get people's intuitions

15:13.580 --> 15:16.140
on a recent news story, okay?

15:16.140 --> 15:20.460
So here's a little recent news from the US versus Trump.

15:20.460 --> 15:22.140
I think this is not the most recent indictment,

15:22.140 --> 15:23.900
but one or two indictments ago.

15:25.180 --> 15:28.140
If you look through it, you can read all about

15:29.020 --> 15:31.620
Pence and Trump and efforts to manipulate

15:31.620 --> 15:33.300
the election and things.

15:33.300 --> 15:36.500
Here's a little paraphrase of one of the paragraphs, 90 C.

15:36.500 --> 15:38.340
So Pence, the vice president, right,

15:38.340 --> 15:40.740
opposed a Trump team lawsuit arguing

15:40.740 --> 15:43.500
that the vice president could reject electoral votes.

15:43.500 --> 15:45.660
So Pence didn't want them to argue

15:45.660 --> 15:48.220
that he could reject electoral votes.

15:48.220 --> 15:50.100
He said to Trump that he didn't have a constitutional

15:50.100 --> 15:52.660
authority and that the action would be improper.

15:52.660 --> 15:55.700
So it's according to Pence's notes at the time.

15:55.700 --> 15:58.940
And Trump responded, you're too honest, okay?

15:58.940 --> 16:00.660
According to Pence's notes.

16:00.660 --> 16:02.460
So think about that situation and everything

16:02.460 --> 16:04.780
you know about this context, right?

16:04.780 --> 16:07.340
And think about an answer to a question like,

16:07.340 --> 16:09.500
why did Trump say this?

16:09.500 --> 16:10.340
Right.

16:11.780 --> 16:15.940
You think about that, probably what's going on

16:15.940 --> 16:17.260
as you think about it, right?

16:17.260 --> 16:20.060
As you're thinking about lots of other things

16:20.060 --> 16:21.980
and how they're related to this situation,

16:21.980 --> 16:23.980
like what Trump was trying to achieve,

16:23.980 --> 16:26.900
maybe what kind of personality Trump had,

16:26.900 --> 16:29.060
what Trump was trying to do to Pence.

16:29.060 --> 16:30.740
Is he trying to manipulate him

16:30.740 --> 16:33.020
into taking some kinds of actions?

16:33.020 --> 16:35.900
What exactly that action would have, right?

16:35.900 --> 16:38.020
In terms of the election.

16:38.020 --> 16:40.820
Everybody is perfectly capable of reasoning through these

16:40.820 --> 16:45.540
and coming up with kind of plausible causal story

16:45.540 --> 16:46.580
about what's happening, right?

16:46.580 --> 16:48.460
It feels like we can come up with our own

16:48.460 --> 16:52.740
kind of internal explanations about events like these.

16:52.740 --> 16:55.100
And in fact, that process of coming up

16:55.100 --> 16:58.980
with internal explanations, interrelated kind of concepts

16:58.980 --> 17:03.020
and meanings is one that people in developmental psychology

17:03.020 --> 17:05.420
have been very interested in and excited about

17:05.700 --> 17:08.820
as a theory of kind of human cognition.

17:08.820 --> 17:13.180
So basic observation is that people form these

17:13.180 --> 17:16.660
very richly interconnected systems of concepts, right?

17:16.660 --> 17:20.060
All of the kind of interconnected stuff you would need to draw

17:20.060 --> 17:22.620
in order to answer a question like that,

17:22.620 --> 17:25.140
which feels totally, totally normal.

17:25.140 --> 17:27.300
People sometimes call these intuitive theories, right?

17:27.300 --> 17:29.820
You have some intuitive theory of how Trump is acting

17:29.820 --> 17:31.540
or how the political system would work

17:31.540 --> 17:36.140
or some intuitive theory of what Pence might be doing

17:36.140 --> 17:38.140
or might be trying to achieve.

17:38.140 --> 17:39.860
And these things are often compared

17:39.860 --> 17:41.820
to theories in science, right?

17:41.820 --> 17:46.820
So you can think of your theory of why Trump might do this

17:47.780 --> 17:50.780
as kind of analogous to a little scientific theory, right?

17:50.780 --> 17:53.300
It has some pieces, it has some relationships

17:53.300 --> 17:56.100
between the pieces, it has some dynamics.

17:56.100 --> 17:58.540
And maybe you can look at all of that

17:58.540 --> 18:00.340
and kind of reason about it causally

18:00.380 --> 18:02.660
as you might reason about any other kind of system

18:02.660 --> 18:04.380
that you've encountered.

18:05.220 --> 18:07.580
So the idea that people,

18:07.580 --> 18:10.620
and maybe most notably kids do this is one

18:10.620 --> 18:14.140
which has really been very popular

18:14.140 --> 18:18.700
in cognitive development, championed maybe most prominently

18:18.700 --> 18:21.460
by Alison Gopnik, who's a developmental psychologist

18:21.460 --> 18:23.060
here at Berkeley.

18:23.060 --> 18:26.180
Let me just give you a quick example of how kids,

18:26.180 --> 18:30.220
how experiments with kids like kids sometimes go

18:30.420 --> 18:31.260
in this domain.

18:31.260 --> 18:36.260
So here's an experiment from LaZot and Gelman.

18:37.060 --> 18:39.380
So kids are shown these two foxes, right?

18:39.380 --> 18:43.220
Which you might notice are identical pictures, okay?

18:44.060 --> 18:46.740
And then they're told things about these foxes

18:46.740 --> 18:50.740
and asked what they could do in order to answer a question,

18:50.740 --> 18:51.580
right?

18:51.580 --> 18:53.940
So this is like a simple version of why did Trump say that?

18:53.940 --> 18:57.220
You might be told that one is an animal and one is a toy,

18:57.220 --> 18:58.060
okay?

18:58.060 --> 18:59.820
So what could you do in order to determine

18:59.820 --> 19:03.340
which one is an animal and which one is a toy, right?

19:06.820 --> 19:10.340
In this experiment, kids will say that you should do things

19:10.340 --> 19:12.020
like check the insides, right?

19:12.020 --> 19:13.860
Like check their guts or whatever, right?

19:13.860 --> 19:16.140
Open them up and see.

19:16.140 --> 19:19.220
Or look at their behavior, right?

19:19.220 --> 19:21.820
If it acts like an animal then you could use that

19:21.820 --> 19:24.660
to figure out which one is the animal, which one's the toy.

19:24.660 --> 19:26.180
Or look at their parents, right?

19:26.220 --> 19:30.260
Like, you know, the animal will have animal parents

19:30.260 --> 19:31.820
and the toy won't, right?

19:31.820 --> 19:33.740
And importantly, they don't just say, yeah,

19:33.740 --> 19:35.620
you can check everything about these.

19:35.620 --> 19:38.180
They know, for example, that age is not relevant, right?

19:38.180 --> 19:41.540
So they won't tell you that age would tell you

19:41.540 --> 19:44.140
which one is an animal and which one is a toy.

19:44.140 --> 19:45.900
It's worth pausing and just thinking about this

19:45.900 --> 19:49.460
and what this means in terms of conceptual representations,

19:49.460 --> 19:50.660
right?

19:50.660 --> 19:52.820
Because you can think about your concept of what makes

19:52.820 --> 19:55.460
something an animal or what makes something a toy

19:55.460 --> 19:57.460
and kind of like the postage stamp example, right?

19:57.460 --> 19:59.540
It's intrinsically connected to these other things,

19:59.540 --> 20:02.300
like what parents are or what's going on

20:02.300 --> 20:04.340
with your guts inside, right?

20:04.340 --> 20:06.020
Or what your behavior is, right?

20:06.020 --> 20:07.900
That concept is just intrinsically linked there

20:07.900 --> 20:11.500
and kids, I think these are preschoolers know that

20:11.500 --> 20:13.260
from a pretty young age.

20:14.660 --> 20:16.340
Gals asked them a question like one is a dog,

20:16.340 --> 20:19.660
one is a wolf, what would you do, okay?

20:19.660 --> 20:21.180
Kids basically say the same things there.

20:21.180 --> 20:23.460
You could check the insides, you could look at behavior,

20:23.460 --> 20:24.700
you could look at their parents,

20:24.700 --> 20:28.380
see if they had a dog parent or a wolf parent.

20:28.380 --> 20:30.140
Some of these are actually kind of interesting, right?

20:30.140 --> 20:32.620
Because I don't think anybody knows,

20:32.620 --> 20:35.980
at least I don't, what you would look for on the insides

20:35.980 --> 20:38.020
to distinguish a dog versus a wolf, right?

20:38.020 --> 20:41.140
Like maybe you could go down to DNA

20:41.140 --> 20:45.060
or I'm sure you could go down to DNA to tell that.

20:45.060 --> 20:46.660
But people have the intuition that like, okay,

20:46.660 --> 20:48.380
there's something about being in this category

20:48.380 --> 20:50.660
which depends on these other aspects

20:50.660 --> 20:52.420
of being in the concept.

20:53.300 --> 20:54.740
To me, I read this much simpler.

20:54.740 --> 20:57.340
It's like basically what they're saying is look,

20:57.340 --> 20:59.500
like all of this are visual things.

20:59.500 --> 21:01.420
It's just that you're trying to project it into language

21:01.420 --> 21:04.500
but actually what the kids are probably meaning is,

21:04.500 --> 21:06.060
you will know it when you see it

21:06.060 --> 21:07.980
than when you play with it, right?

21:07.980 --> 21:10.300
It's vision and interaction.

21:11.180 --> 21:12.020
Yeah, yeah.

21:12.020 --> 21:13.580
And age is neither.

21:13.580 --> 21:14.420
Yeah, yeah.

21:14.420 --> 21:19.100
So I think it's true that, yes, all of these are visual cues.

21:19.100 --> 21:22.140
I don't know of experiments that look at non-visual cues

21:23.380 --> 21:25.140
but I agree, yeah, that's interesting.

21:25.140 --> 21:26.420
I'll give you one other example

21:26.420 --> 21:30.580
where they know that there's no cue, right?

21:30.580 --> 21:32.140
So if you tell them that one is named Amanda

21:32.140 --> 21:34.500
and one is named Melissa,

21:34.500 --> 21:37.220
then they'll reject all of these as tests, right?

21:37.220 --> 21:39.020
They'll say, okay, the insides are not gonna tell you

21:39.020 --> 21:42.020
which one is Amanda, the behavior and the parents

21:42.020 --> 21:44.820
and the age and these things are not going to, okay?

21:44.820 --> 21:46.340
So all of this is just to say

21:46.340 --> 21:49.780
that people have a, even kids, right,

21:50.060 --> 21:52.060
have pretty sophisticated theories

21:52.060 --> 21:55.260
of how concepts relate to other concepts, right?

21:55.260 --> 21:57.380
And in fact, in a situation like this, right,

21:57.380 --> 22:00.980
there's nothing visual apparently that could tell you, right?

22:00.980 --> 22:02.620
So it's not a visual discrimination task.

22:02.620 --> 22:05.340
It's really a kind of conceptual one

22:05.340 --> 22:07.660
that's asking you to look at other kinds

22:07.660 --> 22:09.300
of conceptual features and things.

22:11.060 --> 22:14.580
So people have these intuitive theories

22:14.580 --> 22:16.900
then one kind of proposal,

22:16.900 --> 22:19.380
quite a few people have argued for is that meaning arises

22:19.380 --> 22:21.940
from essentially the role that a word or a symbol

22:21.940 --> 22:24.660
or a concept plays in this theory, right?

22:24.660 --> 22:28.580
Like the meaning of animals really just intrinsically related

22:28.580 --> 22:31.500
to these ways of testing it and these kinds of features

22:31.500 --> 22:33.540
and all of the other things

22:33.540 --> 22:36.460
that are not kind of simple semantic associates with animal

22:36.460 --> 22:39.620
but are kind of deeply connected

22:39.620 --> 22:42.620
in the sense of an intuitive theory.

22:43.460 --> 22:44.860
I was trying to come up with examples

22:44.860 --> 22:48.460
where this, you know,

22:48.540 --> 22:50.500
we'll give people this intuition, right,

22:50.500 --> 22:54.020
that, you know, if you try to define these words,

22:54.020 --> 22:57.060
if you try to define what an indictment is, for example,

22:57.060 --> 22:58.340
it's very hard to do it in a way

22:58.340 --> 23:00.660
that doesn't reference other legal terms

23:00.660 --> 23:02.420
and other kind of social constructs

23:02.420 --> 23:07.100
and concepts that you already have, right?

23:07.100 --> 23:08.540
It's kind of intrinsically related

23:08.540 --> 23:11.460
to the system of other concepts and terms, right?

23:11.460 --> 23:13.580
Chord change is kind of like this too in music, right?

23:13.580 --> 23:15.380
You have to talk about chords and notes

23:15.860 --> 23:18.940
and circle of fifths or whatever, right?

23:18.940 --> 23:20.860
Like these things are just intrinsically related.

23:20.860 --> 23:23.580
I think force in physics is like this.

23:23.580 --> 23:25.820
It's very hard to talk about it in isolation

23:25.820 --> 23:28.300
independent of, you know, experiments

23:28.300 --> 23:29.740
or other concepts or things.

23:29.740 --> 23:31.060
Or if I said, like, what does a bobbin do

23:31.060 --> 23:32.700
in a sewing machine, okay, right?

23:32.700 --> 23:34.340
Like you have to talk about thread

23:34.340 --> 23:36.060
and you have to talk about the processes of sewing.

23:36.060 --> 23:37.220
Just the meaning of these things

23:37.220 --> 23:39.980
are just all intrinsically linked together.

23:41.260 --> 23:44.500
So this idea that meaning,

23:44.540 --> 23:46.020
not about reference, it's about the role

23:46.020 --> 23:48.740
that something plays is called conceptual role theory.

23:49.660 --> 23:51.220
Meaning of a word or concept is determined

23:51.220 --> 23:53.020
by the role it plays.

23:54.380 --> 23:59.380
And this has been argued for,

23:59.820 --> 24:01.940
I think maybe most prominently by Ned Block

24:01.940 --> 24:04.220
who's a philosopher of mind,

24:04.220 --> 24:07.100
who wrote one of my favorite paper titles,

24:07.100 --> 24:10.260
Advertisement for a Semantics of Psychology,

24:10.260 --> 24:11.900
which is basically all about, you know,

24:11.900 --> 24:14.740
how psychology needs a theory of meaning

24:14.740 --> 24:16.620
and a theory of semantics.

24:16.620 --> 24:18.180
And this idea of conceptual role

24:18.180 --> 24:20.020
is something that could do that.

24:20.020 --> 24:22.380
So it can explain kind of where meaning comes from.

24:22.380 --> 24:25.660
It can address questions of how meaning depends

24:25.660 --> 24:27.340
on things like your representations

24:27.340 --> 24:29.300
or categories that you know,

24:29.300 --> 24:31.380
can play nicely with compositionality

24:31.380 --> 24:34.620
or other aspects of language.

24:34.620 --> 24:36.540
And I think maybe most compellingly

24:36.540 --> 24:38.980
can explain how you could find meaning in brains, right?

24:38.980 --> 24:40.260
So if you open up a brain

24:40.260 --> 24:42.460
and you start recording from neurons,

24:43.740 --> 24:45.100
you know, it's really unclear what it means

24:45.100 --> 24:46.780
for there to be reference in there,

24:46.780 --> 24:49.260
reference to the external world in there.

24:49.260 --> 24:50.740
But maybe you could kind of make sense

24:50.740 --> 24:53.340
of patterns of activity in a way

24:53.340 --> 24:57.900
that lets you kind of interpret systems

24:57.900 --> 25:01.100
of signals and representations.

25:02.420 --> 25:05.020
Let me give you just one other example of this

25:05.020 --> 25:08.820
that maybe might make things more clear.

25:08.820 --> 25:10.940
This idea of conceptual role semantics,

25:10.940 --> 25:15.820
I think is also how meaning works in, say, a computer, okay?

25:15.820 --> 25:17.300
Also, I think in mathematics,

25:17.300 --> 25:19.500
which is why I was deferring the questions

25:19.500 --> 25:21.180
about mathematics,

25:21.180 --> 25:22.540
but you could look at something like this, right?

25:22.540 --> 25:25.660
This is a floating point representation

25:25.660 --> 25:28.260
and ask what makes the bits in this representation

25:28.260 --> 25:29.540
mean what they do, right?

25:29.540 --> 25:32.540
In particular, what makes the first bit mean the sign bit,

25:32.540 --> 25:33.380
right?

25:33.380 --> 25:34.780
It's nothing about being the first one

25:34.780 --> 25:36.900
because there've been dozens of different conventions

25:36.900 --> 25:37.940
for floating point numbers

25:38.060 --> 25:41.660
which put the sign bit in all kinds of different places, right?

25:42.740 --> 25:45.180
What makes it mean the sign bit

25:45.180 --> 25:47.780
is how it interacts with all of the other operations

25:47.780 --> 25:51.020
that you can do with floating point numbers, right?

25:51.020 --> 25:51.980
So meaning, in some sense,

25:51.980 --> 25:54.260
comes from the interaction between symbols,

25:54.260 --> 25:55.900
or in other words, their conceptual role.

25:55.900 --> 25:58.660
So in particular, like what does negation do, right?

25:58.660 --> 26:01.660
If I have a negation operator, okay, it flips the sign bit.

26:01.660 --> 26:06.420
Great, okay, that's where it gets its meaning from, right?

26:06.420 --> 26:07.340
Or what does addition do?

26:07.900 --> 26:10.580
The right thing with respect to the sign bit

26:10.580 --> 26:14.180
or multiplication or rounding or whatever, right?

26:14.180 --> 26:17.140
So what makes this the floating point representation

26:17.140 --> 26:20.380
or what makes that first bit represent sign

26:20.380 --> 26:22.820
is nothing intrinsic in the representation itself.

26:22.820 --> 26:25.100
It's how it interacts with all of the other components

26:25.100 --> 26:27.900
of the system, okay, yeah?

26:27.900 --> 26:30.420
You explain the difference between this way of thinking

26:30.420 --> 26:33.300
about the sense of here's a hierarchical approach

26:33.300 --> 26:35.980
where there's concepts and there's sort of numbers.

26:36.980 --> 26:41.980
Yeah, so I think that there's certainly concepts people have

26:43.300 --> 26:44.700
that are hierarchical, right?

26:44.700 --> 26:46.860
So we know that dogs are kind of animal

26:46.860 --> 26:49.340
and animals are a kind of living thing.

26:49.340 --> 26:52.020
I think what that kind of picture is missing

26:52.020 --> 26:54.540
is that our representations are actually,

26:54.540 --> 26:57.140
like computational objects, like they do something, right?

26:57.140 --> 27:01.140
They interact with each other and they allow us to solve

27:01.140 --> 27:02.700
certain kinds of inference problems

27:02.700 --> 27:05.580
and all of the stuff you could do with your concepts

27:05.660 --> 27:07.820
like the Trump example, right?

27:09.260 --> 27:12.220
Yeah, so the claim is not that they're not hierarchical, right?

27:12.220 --> 27:15.620
It's that the interesting important things they do

27:15.620 --> 27:18.340
come from interactions kind of internally between concepts

27:18.340 --> 27:21.020
much like the way that the sign bit is interesting

27:21.020 --> 27:23.620
or important here comes from its interactions

27:23.620 --> 27:26.620
with things like negation and multiplication, yeah, yeah.

27:28.220 --> 27:30.420
What you're saying here is perfectly good,

27:30.420 --> 27:33.420
but what I have trouble with is buying this

27:33.460 --> 27:36.420
as an exclusive theory of semantics.

27:36.420 --> 27:38.980
Just like you gave good arguments

27:38.980 --> 27:43.260
against that meaning is just reference, okay?

27:43.260 --> 27:45.540
I think you demolished that theory,

27:45.540 --> 27:49.740
but now you put up another theory which also I find

27:49.740 --> 27:51.340
that it has some good aspects,

27:51.340 --> 27:55.140
but to make that an exclusive theory is problematic.

27:55.140 --> 27:57.340
So if we look at children growing up,

27:57.340 --> 28:00.540
there are these studies on sort of concreteness judgments.

28:00.540 --> 28:03.140
So the vocabulary of a child at two,

28:03.140 --> 28:06.660
there are a lot of words in there like milk and bottle

28:06.660 --> 28:10.100
and jump and sit and so forth, which are very concrete,

28:10.100 --> 28:12.020
concrete in a visual sense,

28:12.020 --> 28:14.180
concrete in a motor program sense.

28:15.260 --> 28:19.020
At the age of 10, they have words like justice and fairness

28:19.020 --> 28:22.380
and so forth, which are very much,

28:22.380 --> 28:25.300
which fit much better into this conceptual road story

28:25.300 --> 28:28.380
where is the vocabulary of a child at two,

28:28.380 --> 28:31.460
maybe one where this kind of groundedness

28:31.460 --> 28:34.940
to sensory motor experience is a much better account.

28:34.940 --> 28:37.460
And this is not problematic for me.

28:37.460 --> 28:42.060
Why do we need to have one exclusive theory for meaning?

28:42.060 --> 28:45.860
Both of these are aspects of meaning.

28:45.860 --> 28:47.300
Yeah, I agree.

28:47.300 --> 28:52.300
So I think that you can think of the physical reference,

28:52.820 --> 28:54.700
as in some sense one of the conceptual roles

28:54.700 --> 28:55.940
that something can have.

28:57.300 --> 28:58.220
It is important.

28:58.220 --> 29:01.740
I'm not sure we know kind of how abstract kids early meanings

29:01.740 --> 29:03.980
are for those kinds of words,

29:03.980 --> 29:06.340
because it has to be a little bit abstract

29:06.340 --> 29:08.940
because you'll call a new bottle that you see a bottle still.

29:08.940 --> 29:11.940
So you have some abstraction away from the examples

29:11.940 --> 29:13.380
of bottles that you've seen,

29:13.380 --> 29:17.100
but I agree it feels early on very concrete

29:17.100 --> 29:20.380
and much less abstract than things we come later.

29:23.260 --> 29:25.420
Just trying to make sure I understand

29:25.420 --> 29:26.700
what this theory is saying.

29:26.700 --> 29:29.700
So is there a character to think of this

29:29.700 --> 29:32.340
that you're saying that meaning is basically

29:32.340 --> 29:36.540
like some homomorphism onto some either intuitive

29:36.540 --> 29:38.080
or formal theory?

29:39.340 --> 29:40.260
Ah, sure.

29:41.180 --> 29:44.460
So then maybe a follow up question is like,

29:44.460 --> 29:46.780
how do we know which homomorphisms are valid?

29:46.780 --> 29:48.220
Because I could always,

29:48.220 --> 29:51.300
if I can have some arbitrary correspondence mapping,

29:51.300 --> 29:53.780
I could make anything correspond to anything else.

29:54.780 --> 29:56.540
You know what's so loud here?

29:56.540 --> 29:59.420
Yeah, so I don't think anybody has been that formal.

29:59.420 --> 30:02.020
People like Putnam have made this kind of argument

30:02.020 --> 30:04.420
about understanding computation in physical systems,

30:04.420 --> 30:06.700
basically saying like physical systems,

30:06.700 --> 30:09.300
like a brain or in his example, a wall, right?

30:09.300 --> 30:12.980
Are so complicated that I could come up with

30:12.980 --> 30:15.260
kind of any mapping back and forth between the states of it

30:15.260 --> 30:19.100
and the states of the kind of arbitrary computational system.

30:19.100 --> 30:22.300
And that's probably a much longer thing to get into.

30:22.300 --> 30:24.540
I'll just say that I don't think I have a very easy answer

30:24.540 --> 30:25.740
about that, right?

30:26.740 --> 30:28.980
I think of this as not kind of,

30:30.180 --> 30:32.340
certainly not formalized in that sense,

30:32.340 --> 30:35.160
but in sort of a higher level in terms of like

30:35.160 --> 30:37.740
what kinds of theories we should be looking for, right?

30:37.740 --> 30:38.940
And then there's lots of work to do

30:38.940 --> 30:41.780
in terms of making that precise.

30:41.780 --> 30:42.820
So yeah, yeah.

30:42.820 --> 30:44.660
Yeah, Quine actually uses that example

30:44.660 --> 30:49.220
to motivate this kind of theory in like 1950s philosophy.

30:49.220 --> 30:50.380
Sorry, which example?

30:50.380 --> 30:52.100
Quine uses this example.

30:52.100 --> 30:53.580
He uses an example of Gavagai.

30:53.580 --> 30:56.100
You see something popping out and you're like,

30:56.100 --> 30:58.620
how do you know Gavagai means rabbit, not running,

30:58.620 --> 31:00.660
and not hole, and not something else

31:00.660 --> 31:05.100
because the real world doesn't determine what a meaning is.

31:05.100 --> 31:06.260
Yeah, yeah.

31:06.260 --> 31:07.100
Yeah.

31:07.100 --> 31:09.700
The hierarchical concepts and semantics

31:09.700 --> 31:11.860
have been extensively terminated,

31:11.860 --> 31:13.180
scripting orders and so on.

31:13.180 --> 31:15.660
Has any of this been operationalized at all?

31:15.660 --> 31:17.580
Can you comment on that?

31:17.580 --> 31:18.780
I don't think so, yeah.

31:18.780 --> 31:20.620
So I mean, I can talk,

31:21.620 --> 31:24.580
I have a couple of examples of kind of learning

31:24.580 --> 31:29.380
intuitive theories, which essentially have this kind

31:29.380 --> 31:33.740
of character, so of taking data and then trying to come up

31:33.740 --> 31:37.340
with some structures that obey the right relations, right?

31:37.340 --> 31:41.180
And yeah, I'll talk a little bit about that,

31:41.180 --> 31:45.860
but there hasn't been a ton of work on that, so yeah.

31:45.860 --> 31:48.660
Could you talk about how this theory deals with

31:48.660 --> 31:51.900
when the same symbols or words are in different kind

31:51.900 --> 31:53.900
of theories or settings?

31:53.900 --> 31:56.900
Is it kind of mean that the symbols themselves

31:56.900 --> 31:59.260
don't have meaning or how are the kind of the meanings

31:59.260 --> 32:01.140
shared across different contexts?

32:01.140 --> 32:05.620
Yeah, so that's an interesting question

32:05.620 --> 32:08.140
that I think people have not resolved very well.

32:08.140 --> 32:13.140
So your symbol for your father might play

32:14.420 --> 32:16.140
a bunch of different roles, right?

32:16.140 --> 32:18.700
Because you know what job your father has

32:18.700 --> 32:21.300
and you know what family relations and you know

32:21.300 --> 32:25.940
what hobbies and I don't think that there's good

32:25.940 --> 32:28.940
kind of formalized accounts of how to make sense

32:28.940 --> 32:31.540
of all of that, so there's not great theories

32:31.540 --> 32:33.580
of kind of formalizing conceptual roles.

32:34.820 --> 32:37.420
I'll give some arguments why I think it's possible

32:37.420 --> 32:39.260
that language models are doing this at least

32:39.260 --> 32:42.580
in a tiny version, but in terms of like rich

32:42.580 --> 32:44.740
and kind of human like conceptual roles,

32:44.740 --> 32:47.060
I think that's one of the key problems that's hard to solve,

32:47.060 --> 32:49.740
so, is there another one?

32:49.740 --> 32:50.580
Yeah.

32:50.580 --> 32:55.260
I think that the oncology of the kind of physical world

32:55.260 --> 32:58.140
people think that we're using right now, for example,

32:58.140 --> 33:03.140
is a subset of the oncology of a human's mind

33:05.300 --> 33:08.380
and probably also a subset of all possible

33:08.380 --> 33:12.500
future invented concepts and so on.

33:13.340 --> 33:15.660
Sorry, what was the, I missed the very first part,

33:15.660 --> 33:16.660
what was the question part?

33:16.660 --> 33:19.020
So the question is whether you think

33:20.300 --> 33:23.940
that the existing ontology that you are using now

33:23.940 --> 33:27.420
is a subset of the ontology of human's mind

33:27.420 --> 33:30.540
that we haven't fully explored

33:30.540 --> 33:33.540
and probably that is also a subset of what kind of

33:33.540 --> 33:38.340
can be inventive or creative produced concept

33:38.340 --> 33:39.620
that you mentioned in the beginning.

33:39.620 --> 33:42.300
By ontology, do you mean these theories?

33:42.300 --> 33:45.140
I mean terms, for example, yeah, concepts.

33:45.140 --> 33:46.140
Yeah, concepts.

33:46.140 --> 33:49.140
I mean, I don't think any of these

33:49.140 --> 33:50.940
is quite the right answer, right?

33:50.940 --> 33:54.260
Like these things are actually very difficult to figure out,

33:55.700 --> 33:57.180
but I think they're kind of pointing

33:57.180 --> 33:59.420
in some useful directions or something.

33:59.420 --> 34:03.220
So I don't know if that answers your question, but yeah.

34:03.220 --> 34:05.740
And if you know the symbols that you're using,

34:05.740 --> 34:06.580
why does it matter?

34:06.580 --> 34:09.700
Because that's not to define certain meaning,

34:09.700 --> 34:14.700
whether you use words to represent or you find to represent,

34:15.660 --> 34:16.980
it doesn't matter, right?

34:18.100 --> 34:22.380
In terms of which symbols, like mental representations or?

34:22.380 --> 34:25.860
Yeah, without a concept, whether you use the words

34:25.860 --> 34:28.860
to represent that or you think that's fine,

34:28.860 --> 34:31.300
you think that's all that comes out.

34:31.300 --> 34:33.740
Or like, I'm sure that this kind of concept

34:33.740 --> 34:36.380
has a lot to do with the form,

34:37.340 --> 34:39.700
but I think it's just a little bit of a question.

34:39.700 --> 34:41.220
Yeah, so yeah.

34:41.220 --> 34:44.620
Just to make sure that you, how far do you still have to,

34:44.620 --> 34:45.940
like do you have any?

34:45.940 --> 34:47.300
I have a little ways to go.

34:47.300 --> 34:50.660
Okay, so maybe we should push this the word

34:50.660 --> 34:51.900
after, after at the end,

34:51.900 --> 34:54.780
because it seems like a deeper discussion.

34:54.780 --> 34:56.420
Yeah, yeah, okay, great.

34:58.620 --> 35:03.620
Okay, so I talked about these kinds of accounts of meaning,

35:04.340 --> 35:08.780
particular meaning as conceptual role.

35:08.780 --> 35:11.460
And let me talk a little bit about learning conceptual roles

35:11.460 --> 35:15.820
and why we might think that's plausible or useful.

35:17.740 --> 35:20.540
Seems to me at least that large language models

35:20.540 --> 35:22.620
almost certainly need to learn some of these pieces

35:22.620 --> 35:24.340
of conceptual role,

35:24.340 --> 35:26.140
that these kinds of things seem really necessary

35:26.140 --> 35:29.020
for the stuff large language models are good at, right?

35:29.020 --> 35:31.860
Writing coherent texts or doing translations

35:32.220 --> 35:34.580
or providing definitions or providing elaborations

35:34.580 --> 35:36.700
or explanations, all of those things require you

35:36.700 --> 35:38.740
to put symbols in the right relationships

35:38.740 --> 35:40.740
with other symbols, right?

35:40.740 --> 35:44.660
And that means that to do those things well,

35:44.660 --> 35:46.700
you essentially have to have some little components

35:46.700 --> 35:48.540
of conceptual roles, right?

35:51.620 --> 35:53.780
One way to think about this is that human meanings

35:53.780 --> 35:56.660
or human conceptual roles generated the text, right?

35:56.660 --> 35:59.900
So maybe a smart inferential model could invert that

35:59.900 --> 36:02.660
and figure out what were the likely conceptual roles

36:02.660 --> 36:05.980
that generated the thing that I saw.

36:05.980 --> 36:07.500
I like this, the Stringer quote, right?

36:07.500 --> 36:09.220
The structure of sentences serves as an image

36:09.220 --> 36:10.540
of the structure of thoughts, right?

36:10.540 --> 36:13.820
Some projection of our thoughts or our meanings,

36:13.820 --> 36:16.740
our conceptual roles that gets realized into sentences.

36:16.740 --> 36:17.580
Yeah.

36:17.580 --> 36:18.980
Yeah, so are you gonna follow up on something

36:18.980 --> 36:21.460
that was a great, a recontextuality kind of example?

36:21.460 --> 36:24.700
Is that a recontextuality kind of thing?

36:24.700 --> 36:25.540
Which examples are you talking about?

36:25.540 --> 36:26.660
Are you gonna recontextualize,

36:26.660 --> 36:28.500
let's say a gentleman's two boxes example,

36:28.500 --> 36:30.420
or is that a hyper-projection?

36:31.860 --> 36:33.940
No, I wasn't gonna go back to that.

36:33.940 --> 36:36.380
Yeah, but I'll talk about a study

36:36.380 --> 36:40.020
in Large-Range Models in a minute, okay.

36:41.260 --> 36:42.980
Okay, a lot of people have the intuition

36:42.980 --> 36:43.820
this is not possible.

36:43.820 --> 36:46.740
I think this is kind of the Bender and Marcus intuition

36:46.740 --> 36:48.380
that our thoughts really get projected

36:48.380 --> 36:50.780
into this kind of impoverished sequence of sounds, right?

36:50.780 --> 36:52.180
How could you discover something

36:52.180 --> 36:55.740
like rich conceptual roles there, right?

36:55.740 --> 36:58.060
If you just have this projection of language,

36:58.060 --> 37:00.340
how could that ever support rich and interesting

37:00.340 --> 37:02.220
kinds of conceptual roles?

37:04.580 --> 37:07.260
One kind of way that I think is a helpful analogy,

37:07.260 --> 37:10.180
although not kind of a mathematically precise

37:10.180 --> 37:11.860
implementation or something,

37:11.860 --> 37:13.660
people may know these embedding theorems

37:13.660 --> 37:17.260
from dynamical systems, which I think are very cool.

37:17.260 --> 37:20.140
There's this paper called Geometry from a Time Series,

37:21.140 --> 37:23.140
which essentially shows that in some cases

37:23.140 --> 37:25.900
you can take projections of dynamical systems

37:25.940 --> 37:28.700
and recover things which capture the structure

37:28.700 --> 37:31.420
of the dynamics from that projection.

37:31.420 --> 37:33.300
So in particular, in this paper, they go through this,

37:33.300 --> 37:35.660
which is the Rossler Attractor.

37:35.660 --> 37:38.940
This is a three-dimensional system

37:38.940 --> 37:41.340
of differential equations.

37:41.340 --> 37:44.140
And what you can do is take a one-dimensional projection

37:44.140 --> 37:45.220
of those dynamics.

37:45.220 --> 37:47.060
So you can look at just the X location

37:47.060 --> 37:49.580
of what's happening there.

37:49.580 --> 37:53.180
And through a clever trick essentially translating

37:53.180 --> 37:56.060
the one-dimensions into three-dimensions

37:56.060 --> 38:01.060
using, by going backwards in time, some number of steps,

38:01.260 --> 38:03.060
you can actually recover the structure of this

38:03.060 --> 38:04.900
from the one-dimensional projection.

38:06.780 --> 38:09.420
And there's other kind of general theorems

38:09.420 --> 38:10.820
about when this is possible,

38:10.820 --> 38:13.340
Parkinson's embedding theorem and things like that.

38:13.340 --> 38:17.580
The point here is that we shouldn't really have

38:17.580 --> 38:19.300
strong intuitions about what's possible

38:19.300 --> 38:21.620
from some projection of thoughts,

38:21.620 --> 38:24.980
because oftentimes there might be possible

38:24.980 --> 38:28.100
for people to, or for learning models

38:28.100 --> 38:31.420
to reconstruct kind of interesting parts

38:31.420 --> 38:33.820
of the structure of some system

38:33.820 --> 38:37.220
just from simple kind of measurements of that system.

38:37.220 --> 38:41.100
Actually Shaw here, the senior author wrote an entire book

38:41.100 --> 38:44.300
on recovering the kind of dynamical properties

38:44.300 --> 38:45.620
of a dripping water faucet,

38:46.840 --> 38:48.940
where you can measure the time between drips

38:48.940 --> 38:51.340
and figure out things about the kind of latent variables

38:51.340 --> 38:52.940
and latent structures they're using techniques

38:52.940 --> 38:54.460
that are a lot like these.

38:55.820 --> 38:58.220
In psychology, actually people have also been interested

38:58.220 --> 39:01.340
in kind of closely related types of models.

39:01.340 --> 39:06.000
So there's this work by Roger Shepard in the 80s,

39:06.000 --> 39:09.020
which essentially would take behavioral judgments

39:09.020 --> 39:12.780
and try to infer the underlying structures behind them.

39:12.780 --> 39:17.260
So for example, this matrix here is different colors

39:17.260 --> 39:18.700
or different wavelengths of light

39:18.700 --> 39:22.260
and then confusability between them on judgment tasks.

39:22.260 --> 39:24.100
So just how similar are these things

39:24.100 --> 39:26.300
or how confusable is one color with another.

39:27.380 --> 39:29.960
And Shepard was using multi-dimensional scaling

39:29.960 --> 39:35.020
to go from data like this up to representation like this,

39:35.020 --> 39:37.180
which you might recognize as a color wheel.

39:37.180 --> 39:40.660
Basically you can arrange points

39:40.660 --> 39:42.380
so that their distances correspond

39:42.380 --> 39:45.700
to the distances in the confusion matrix

39:45.700 --> 39:48.220
and therefore recover something

39:48.220 --> 39:49.500
about the kind of underlying,

39:49.500 --> 39:51.540
in this case, psychological structure

39:51.540 --> 39:53.180
that generated that data.

39:55.860 --> 39:59.460
People have also done similar kinds of things

39:59.460 --> 40:02.980
in learning kind of real formalized versions

40:02.980 --> 40:06.620
of theories or of intuitive theories.

40:06.620 --> 40:09.300
I really like this paper by Tomer Ullmann

40:09.300 --> 40:11.940
and Noah Goodman and Josh who's speaking next

40:11.940 --> 40:14.100
on learning a theory of magnetism.

40:14.100 --> 40:15.780
So basically you take observations

40:15.780 --> 40:20.780
of which objects interact with other objects

40:20.860 --> 40:25.860
and do some learning to acquire a kind of high level theory

40:26.340 --> 40:30.120
of the fact that there are two different kinds

40:30.120 --> 40:33.360
of magnetic objects and those two different kinds

40:33.360 --> 40:34.600
of things will interact with each other,

40:34.600 --> 40:35.740
but they won't interact with things

40:35.740 --> 40:37.380
that are non-magnetic.

40:37.380 --> 40:40.020
So this is like a little tiny mini intuitive theory

40:40.020 --> 40:43.060
that you can acquire just from very simple,

40:43.060 --> 40:44.580
you might think kind of impoverished data

40:44.580 --> 40:46.240
about interactions.

40:47.740 --> 40:51.640
So when people talk about LOMs just being based on text,

40:51.640 --> 40:52.980
I think that isn't really enough

40:52.980 --> 40:55.620
to conclude anything about what theories they might induce,

40:55.620 --> 40:57.140
or what kinds of internal structures

40:57.140 --> 41:01.020
and conceptual roles they might induce from that text.

41:01.020 --> 41:02.460
And in fact, there's some evidence I think

41:02.460 --> 41:07.460
that what they are inducing looks pretty plausible

41:07.780 --> 41:09.860
at least in kind of simple domains.

41:09.860 --> 41:14.700
So there's this paper by Grandin and colleagues

41:14.700 --> 41:18.020
which essentially looked at word embedding vectors

41:18.020 --> 41:21.980
and projected them onto say intuitive dimensions.

41:21.980 --> 41:24.580
So here you have a bunch of words,

41:24.580 --> 41:26.540
you project them onto this line,

41:26.540 --> 41:29.020
which is the line connecting small and large.

41:29.020 --> 41:31.900
Okay, so all of our high dimensional word vectors

41:31.900 --> 41:34.780
get projected onto the small versus large line.

41:34.780 --> 41:36.780
And we take that as a way of measuring

41:36.780 --> 41:39.660
how large versus small different objects are.

41:40.580 --> 41:42.020
And then the question is,

41:42.020 --> 41:46.020
is in a model trained only on text prediction,

41:46.020 --> 41:51.020
is that, does that projection recover anything human like

41:51.120 --> 41:54.180
about the underlying conceptual spaces?

41:54.180 --> 41:55.300
And they show yes it does.

41:55.300 --> 41:58.780
So here's six plots where the x-axis

41:58.780 --> 42:01.180
is the semantic projection, right?

42:01.180 --> 42:03.860
So how far on that small to large line something is.

42:03.860 --> 42:05.800
And then the y-axis is human ratings

42:05.800 --> 42:09.280
of how small versus large an object is, right?

42:09.280 --> 42:11.080
You can see that the correlations here are not perfect

42:11.080 --> 42:13.080
but they're also not garbage, right?

42:13.080 --> 42:17.960
They're actually quite strong I think for a model like this

42:17.960 --> 42:21.020
that things which the model calls wet versus dry

42:21.020 --> 42:24.400
or big versus small or dangerous versus safe,

42:25.440 --> 42:27.900
people also agree with, right?

42:27.900 --> 42:29.740
So just in predicting text,

42:29.740 --> 42:32.080
this thing has recovered these kinds of aspects

42:32.080 --> 42:34.720
of semantic structure latent

42:34.720 --> 42:37.680
in the word vector representations, yeah.

42:37.680 --> 42:40.360
But this is probably sitting there in n-grams

42:40.360 --> 42:42.760
and in bi-grams in fact, that information, right?

42:42.760 --> 42:45.360
It doesn't have to do anything with the real world.

42:46.360 --> 42:48.440
Well, it does have something to do with the real world.

42:48.440 --> 42:52.760
It's like a small and large

42:52.760 --> 42:55.080
could just be linguistic constructs

42:55.080 --> 42:57.320
and you're testing it on language.

42:57.320 --> 43:00.200
Oh, I see, you think it's that you say small tiger

43:00.200 --> 43:01.880
versus large tiger or something.

43:02.320 --> 43:04.160
Small puppy, right?

43:04.160 --> 43:06.680
Puppy is always small, tiger is always large, yeah.

43:06.680 --> 43:09.960
So it might be true in n-grams, I'm not sure.

43:09.960 --> 43:12.160
I don't think that they looked at that.

43:13.800 --> 43:18.640
I don't think that defeats the argument though, right?

43:18.640 --> 43:20.960
Because I think it is the case that

43:22.080 --> 43:24.200
even n-gram statistics are statistics

43:24.200 --> 43:26.400
about word relations, right?

43:26.400 --> 43:28.240
So it might be that you don't need fancy language models

43:28.240 --> 43:29.720
or something to do this.

43:30.200 --> 43:31.360
But you're not actually like,

43:31.360 --> 43:33.600
you don't need real world for this to work.

43:35.120 --> 43:37.920
Well, the real world generated

43:37.920 --> 43:40.840
how often you hear small puppy versus large puppy, right?

43:40.840 --> 43:44.920
So the real world is mirrored in those statistics

43:44.920 --> 43:47.400
and then the configuration that system comes up with

43:47.400 --> 43:50.520
is also one that mirrors those properties of the real world.

43:50.520 --> 43:51.360
Yeah.

43:52.640 --> 43:53.960
Can I put back on that as well?

43:53.960 --> 43:56.320
I do kind of feel like what others are saying is right,

43:56.320 --> 43:58.720
but this is much closer to n-grams

43:58.720 --> 44:00.680
than it is to large language models.

44:00.680 --> 44:03.440
And the properties we're seeing are just so much wilder

44:03.440 --> 44:07.240
than any of these embedding tricks in practice.

44:07.240 --> 44:08.520
Oh, you mean that large language model

44:08.520 --> 44:10.160
is much smarter than this?

44:10.160 --> 44:12.880
I don't even see how this is comparable in a way, right?

44:12.880 --> 44:15.320
Like this pops out of PCA,

44:15.320 --> 44:18.040
whereas we're seeing these wild emergent behaviors

44:18.040 --> 44:18.920
come out of large language models.

44:18.920 --> 44:20.400
Yeah, yeah, so I mean,

44:20.400 --> 44:22.680
I don't think this explains wild emergent behaviors.

44:22.680 --> 44:25.120
I think that this was just trying to say

44:25.120 --> 44:27.840
that when you train on text prediction,

44:27.840 --> 44:29.480
you configure yourself to align

44:29.480 --> 44:31.840
with some of the true properties of the world,

44:31.840 --> 44:33.960
which are reflected in the text analysis.

44:33.960 --> 44:34.960
That's all, yeah.

44:37.320 --> 44:38.320
Okay, so I'm short on time.

44:38.320 --> 44:39.160
I'll skip this.

44:39.160 --> 44:40.840
I'll just say that there's other papers

44:40.840 --> 44:45.600
looking at transformers and kind of how they relate

44:45.600 --> 44:48.800
to classic studies on concepts in cognitive science,

44:48.800 --> 44:51.360
classic kinds of effects, but I'll skip that.

44:52.880 --> 44:54.080
Maybe I'll go very briefly

44:54.080 --> 44:56.200
just through this kind of fun experiment.

44:56.200 --> 44:59.080
This is Mark Gorenstein in my lab

44:59.080 --> 45:02.720
has been interested in learning concepts

45:02.720 --> 45:05.880
just from linguistic experience,

45:05.880 --> 45:07.760
maybe linguistic prediction.

45:07.760 --> 45:10.360
He's been doing these kind of cool experiments

45:10.360 --> 45:14.400
where we give people passages of natural language

45:14.400 --> 45:16.400
where there's some blanks.

45:16.400 --> 45:18.120
So here's a passage.

45:18.120 --> 45:19.640
The myth of blank is so powerful

45:19.640 --> 45:21.160
that the very words conjure up blank,

45:21.160 --> 45:25.040
of strudel and blank in a cozy Vietnese cafe, blah, blah, blah.

45:25.040 --> 45:27.720
And the job of participants in this

45:27.720 --> 45:31.000
is to learn where to put the word DAX.

45:31.000 --> 45:32.240
Okay, so DAX is a novel word

45:32.240 --> 45:34.160
they've never encountered before.

45:35.360 --> 45:38.440
You have to read this and understand the context and stuff

45:38.440 --> 45:42.760
in order to figure that out and see where DAX should go.

45:42.760 --> 45:47.440
Secretly behind the scenes, this example has been chosen

45:47.440 --> 45:51.600
as just from a big corpus of text of a really rare word

45:51.600 --> 45:53.400
that people probably don't know.

45:53.800 --> 45:56.600
So the rare word here is soccer tort.

45:57.520 --> 46:00.240
And that means that this language,

46:00.240 --> 46:02.840
like where soccer tort actually occurred here,

46:03.840 --> 46:05.960
was generated from real people

46:05.960 --> 46:07.960
and presumably reflects the underlying meaning

46:07.960 --> 46:09.680
and things of soccer tort.

46:10.680 --> 46:13.200
Maybe I don't know if I'm saying that correctly.

46:13.200 --> 46:15.560
But with enough examples of these people

46:15.560 --> 46:16.880
will learn where DAX is,

46:16.880 --> 46:19.080
they get feedback on whether or not they were correct,

46:19.080 --> 46:20.960
according to whether they chose the place

46:21.000 --> 46:23.640
where soccer tort actually appeared.

46:23.640 --> 46:27.080
Okay, so we're having them do kind of a version

46:27.080 --> 46:28.880
of a prediction task,

46:30.000 --> 46:33.560
trying to figure out where this word goes,

46:33.560 --> 46:34.920
but they don't actually see the word,

46:34.920 --> 46:36.760
they see it as DAX.

46:38.520 --> 46:40.440
People get pretty decent at this,

46:40.440 --> 46:44.760
so up to 80% or so, depending on the word.

46:46.080 --> 46:48.960
These are just, sorry, these are the examples of the words

46:49.000 --> 46:51.600
which generated the unseen context.

46:52.760 --> 46:54.400
And after that, we asked them a bunch of questions.

46:54.400 --> 46:56.400
So we asked them some reading comprehension,

46:56.400 --> 46:58.840
we asked them feature questions about DAX,

46:58.840 --> 47:00.320
is DAX a man-made object?

47:00.320 --> 47:02.240
Do biologists typically study DAX?

47:02.240 --> 47:04.200
Do people use DAX in painting?

47:04.200 --> 47:06.960
Just a whole collection of basic feature,

47:06.960 --> 47:08.800
kind of concept-y questions.

47:08.800 --> 47:10.680
We give them an image recognition task,

47:10.680 --> 47:13.160
right, picking out soccer tort,

47:13.160 --> 47:17.040
the real thing versus alternatives.

47:17.280 --> 47:19.720
And we also asked them for explicit definitions, right?

47:19.720 --> 47:21.440
These contexts are not definitions,

47:21.440 --> 47:23.640
they're not saying here's what a soccer tort is,

47:23.640 --> 47:27.560
it's naturalistic usages of the object.

47:27.560 --> 47:30.080
And what we find actually is within,

47:33.360 --> 47:36.600
within about 20 trials or so,

47:36.600 --> 47:39.240
sorry, everything is after 20 trials,

47:39.240 --> 47:40.760
people are actually very, very good

47:40.760 --> 47:45.240
at judging conceptual features for these concepts,

47:45.280 --> 47:48.040
almost at ceiling in most of the kinds

47:48.040 --> 47:50.560
of feature questions we asked them.

47:50.560 --> 47:54.200
Here's each word on a row,

47:54.200 --> 47:57.640
and then features here on the x-axis,

47:57.640 --> 48:00.080
almost everything is read, meaning they're good at this.

48:00.080 --> 48:05.080
They're also good at picking which picture is the object,

48:05.080 --> 48:06.760
so they've never encountered any pictures at all,

48:06.760 --> 48:10.920
but they're 80% or so good at picking these things out.

48:10.920 --> 48:12.760
And they're even good at giving definitions

48:12.760 --> 48:14.240
for these terms, okay?

48:14.240 --> 48:16.680
So here's a dictionary or dictionary

48:16.680 --> 48:18.440
or something definition of soccer tort,

48:18.440 --> 48:20.800
it's a chocolate cake or tort of Austrian origin

48:20.800 --> 48:24.040
invented by Franz soccer supposedly in 1832

48:24.040 --> 48:26.600
for some prince in Vienna.

48:26.600 --> 48:28.120
And people just from these contexts,

48:28.120 --> 48:30.320
20 of them will learn things like it's a chocolate dessert

48:30.320 --> 48:31.760
similar to a cake that was originally

48:31.760 --> 48:33.600
and most commonly made in Vienna.

48:33.600 --> 48:34.800
I think it's a type of chocolate cake

48:34.800 --> 48:37.280
that can be ordered for dessert in Austria,

48:37.280 --> 48:39.760
kind of rich chocolate cake from Vienna and so on, okay?

48:39.760 --> 48:44.760
So people are pretty good at taking

48:46.520 --> 48:48.200
kind of in-context language use

48:48.200 --> 48:50.320
and figuring out underlying aspects

48:50.320 --> 48:52.680
of conceptual representation from that.

48:56.120 --> 48:57.760
But they will never be able to taste it.

48:57.760 --> 48:58.920
I mean, it tastes so good.

48:58.920 --> 49:00.920
Have you had it?

49:00.920 --> 49:03.800
I've never heard of it, so okay.

49:03.800 --> 49:05.080
It's great.

49:05.080 --> 49:06.800
Okay, so let me just wrap up here.

49:06.800 --> 49:11.800
So I think of these kind of conceptual roles or theories

49:12.280 --> 49:14.080
as really both a strength and a weakness

49:14.080 --> 49:17.200
of these current large language models.

49:18.040 --> 49:19.520
One is that large language models,

49:19.520 --> 49:21.200
I think, seem very good at learning

49:21.200 --> 49:23.640
kind of shallow but broad theories, right?

49:23.640 --> 49:26.120
So things like could shoes be made out of eggplants

49:26.120 --> 49:28.760
or what would happen if shoes were made out of eggplants, right?

49:28.760 --> 49:33.760
They would know what some bad downsides

49:33.760 --> 49:36.000
of that kind of thing might be, right?

49:36.000 --> 49:40.080
Or answer basic kinds of questions

49:40.080 --> 49:43.880
that might rely on kind of reasoning

49:43.880 --> 49:47.440
through one or two kind of links about the relationships

49:47.440 --> 49:50.420
between the objects involved in a situation like that.

49:51.440 --> 49:52.960
I think it's been very surprising to people

49:52.960 --> 49:55.520
that this works so well, right?

49:55.520 --> 49:58.480
And part of, I think, what makes it surprising

49:58.480 --> 50:03.480
is that these models are able to be trained

50:03.680 --> 50:05.880
on a huge number of words, right?

50:05.880 --> 50:07.960
And so sort of superficially knowing a little bit

50:07.960 --> 50:10.120
about conceptual roles of a huge number of words

50:10.120 --> 50:11.920
seems to get you pretty far

50:12.920 --> 50:14.440
in terms of seeming convincing

50:14.440 --> 50:16.660
and in terms of language production.

50:17.600 --> 50:19.680
But it's also these conceptual roles and theories

50:19.680 --> 50:21.720
are also weakness and they don't seem very good

50:21.720 --> 50:23.520
at robust and precise theories, right?

50:23.520 --> 50:25.480
So if you think about conceptual roles

50:25.480 --> 50:26.780
like in mathematics, right?

50:26.780 --> 50:28.960
How you define say a natural number

50:28.960 --> 50:30.800
or how you define an integral or something, right?

50:30.800 --> 50:34.200
Like all of those are symbolic kinds of theories

50:34.200 --> 50:37.080
which are precise and which support

50:37.080 --> 50:39.720
chains of reasoning of arbitrary length, right?

50:39.720 --> 50:41.440
And that's what these systems really seem

50:41.440 --> 50:43.960
not to be very good at.

50:43.960 --> 50:46.000
Likely that's because there's some important things

50:46.000 --> 50:47.040
which are missing, right?

50:47.040 --> 50:50.880
Things like grounding, things like reasoning

50:50.880 --> 50:53.440
or even richer kinds of theories.

50:53.440 --> 50:57.100
I think Josh will talk about this some next.

50:58.100 --> 51:01.080
I think that there's a kind of broader view

51:01.080 --> 51:03.900
of concepts and meanings, which is really,

51:03.900 --> 51:05.260
I think the most exciting for people

51:05.260 --> 51:09.420
that work on concepts and concept representations

51:09.420 --> 51:12.940
in cognitive psychology, which is that large language models

51:12.940 --> 51:14.540
have really shown how vectors can do things

51:14.540 --> 51:16.460
that were long thought to be impossible

51:16.460 --> 51:18.180
for non-symbolic models, right?

51:18.180 --> 51:19.760
In particular, these kinds of arguments

51:19.760 --> 51:22.060
from people like Fodor and Polition

51:22.060 --> 51:25.540
about compositionality and systematicity and productivity,

51:25.540 --> 51:26.380
right?

51:26.380 --> 51:29.020
All of these kinds of things that people have pointed to

51:29.020 --> 51:32.140
as characteristic features of thinking

51:32.660 --> 51:34.700
have argued were characteristic features

51:34.700 --> 51:37.820
of symbolic thinking just turn out not to be right, right?

51:37.820 --> 51:41.540
It turns out you can get vectors to do those things.

51:41.540 --> 51:43.120
And I would argue that the solution

51:43.120 --> 51:44.980
to why vectors could do those things

51:44.980 --> 51:47.900
is probably that what these models are doing

51:47.900 --> 51:51.500
is training vectors that encode conceptual roles, right?

51:51.500 --> 51:54.580
Like what they're learning is representations of meaning

51:54.580 --> 51:58.140
which capture the important parts of conceptual roles.

51:58.140 --> 52:01.980
This is actually something which has been long sought after

52:01.980 --> 52:04.140
in say computational neuroscience.

52:04.140 --> 52:06.260
There's things like vector symbolic architectures

52:06.260 --> 52:09.940
that are very exciting ways of encoding

52:09.940 --> 52:12.220
say arbitrary symbolic systems

52:12.220 --> 52:16.700
or arbitrary mathematical systems into vectors.

52:16.700 --> 52:19.700
And I think that some marriage of those two things

52:19.700 --> 52:22.900
is going to be very exciting.

52:24.500 --> 52:26.380
So large language models point to a theory of meaning

52:26.380 --> 52:30.660
that's based on essentially vector based conceptual roles,

52:31.020 --> 52:34.060
and perhaps can capture a lot of the different features

52:34.060 --> 52:38.820
of meaning that people in say cognitive science

52:38.820 --> 52:42.900
or cognitive development have tried to kind of bring out

52:42.900 --> 52:44.540
in human conceptual systems, right?

52:44.540 --> 52:45.980
Like that our meanings are gradient

52:45.980 --> 52:47.980
or that they have hierarchies,

52:47.980 --> 52:49.620
that we know things like definitions

52:49.620 --> 52:51.700
and we can make inferences about relationships

52:51.700 --> 52:55.500
and similarities and all of those things seem like things

52:55.500 --> 52:58.540
that you can encode at least in principle in vectors

52:59.540 --> 53:00.980
which is great.

53:01.940 --> 53:06.020
So let's get that, let me just end there.

53:06.020 --> 53:07.940
I'll thank you again for the invitation

53:07.940 --> 53:11.540
and thanks also to all of my co-authors on the work here.

53:11.540 --> 53:16.540
All right.

53:16.700 --> 53:17.540
Yes.

53:19.860 --> 53:21.740
Maybe I'm reading too much into it

53:21.740 --> 53:24.380
but it seemed to me that you hinted at

53:26.020 --> 53:27.940
what these vector representations

53:28.700 --> 53:31.260
large language models tell us about

53:31.260 --> 53:35.700
both human cognition as well as about language,

53:35.700 --> 53:37.020
the nature of language.

53:38.020 --> 53:40.900
Could I ask you to do a projective measurement

53:40.900 --> 53:43.700
and come out and say something about that?

53:45.940 --> 53:50.940
I think that they tell us that vectors are really plausible.

53:51.940 --> 53:55.300
They kind of show us how vectors are plausible for meanings,

53:55.820 --> 53:58.940
and the way in which I think that they're plausible

53:58.940 --> 54:01.660
for meanings is that they encode conceptual roles.

54:02.820 --> 54:03.660
That's what I would say.

54:03.660 --> 54:05.860
And I think until them,

54:05.860 --> 54:07.380
until kind of recent deep learning,

54:07.380 --> 54:08.980
I think it was really unclear.

54:08.980 --> 54:12.380
So people had argued for decades about whether

54:12.380 --> 54:14.500
the foundation of concepts was definitions

54:14.500 --> 54:16.300
or is it like somehow similarities

54:16.300 --> 54:17.660
or is it that you just know a word

54:17.660 --> 54:21.140
and you know a bunch of associated features or whatever.

54:22.100 --> 54:26.060
And I think one of the main insights, for example,

54:26.060 --> 54:29.860
is that you can extract a definition

54:29.860 --> 54:31.180
from a large language model.

54:31.180 --> 54:32.340
We've even given it some of these

54:32.340 --> 54:33.820
kind of human experiments we've done

54:33.820 --> 54:35.740
and they're pretty good at coming up

54:35.740 --> 54:39.220
with the chocolate torch kind of definitions from those.

54:39.220 --> 54:40.780
And that tells you that the definitions

54:40.780 --> 54:43.740
can be encoded into vectors, right?

54:43.740 --> 54:44.740
And that's great, right?

54:44.740 --> 54:49.100
That means that you don't need to think about definitions

54:49.100 --> 54:51.860
as the defining part of concepts, right?

54:51.860 --> 54:54.540
There's some other kind of more abstract,

54:54.540 --> 54:57.100
you know, high dimensional space or whatever

54:57.100 --> 54:58.460
that defines the meanings.

54:58.460 --> 55:00.180
And the sense in which it defines the meanings

55:00.180 --> 55:02.140
is in terms of the relationships between vectors

55:02.140 --> 55:04.340
on the tasks that you use the concepts for.

55:05.340 --> 55:08.500
Is it just natural since the brain encodes information

55:08.500 --> 55:11.420
with lots of neurons firing through your brain?

55:11.420 --> 55:13.100
This should not be surprising.

55:13.100 --> 55:17.060
So it's not surprising, it was always unclear

55:17.060 --> 55:18.540
how that was even possible.

55:19.100 --> 55:23.900
Yeah, exactly, yeah, yeah, yeah.

55:23.900 --> 55:25.540
It's like everybody always kind of knew

55:25.540 --> 55:27.580
that there had to be a continuous system

55:27.580 --> 55:28.820
which could support these things.

55:28.820 --> 55:30.900
But when you look at, you know, the discreteness

55:30.900 --> 55:32.860
in language or this discreteness in mathematics,

55:32.860 --> 55:35.340
it was always kind of unclear where that could come from.

55:35.340 --> 55:36.900
So that's why I think things like vector symbolic

55:36.900 --> 55:39.660
architectures are very exciting too.

55:39.660 --> 55:42.900
So when a human fills in the meaning,

55:42.900 --> 55:44.300
they're using a lot of context

55:44.300 --> 55:46.140
that they've gotten from the real world.

55:46.140 --> 55:49.180
When an octopus tries to fill in the meaning,

55:49.180 --> 55:52.180
they have much less context to work with.

55:52.180 --> 55:54.300
And when a LLM fills in the meaning,

55:54.300 --> 55:57.500
they have no kind of context to work with.

55:57.500 --> 55:58.620
So I was wondering if you had thoughts

55:58.620 --> 56:01.980
about the differences and that implies.

56:01.980 --> 56:04.620
It's really interesting to think of what's exactly

56:04.620 --> 56:06.060
happening in that human experiment

56:06.060 --> 56:08.260
because I agree it's transfer of stuff you know,

56:08.260 --> 56:09.420
like you've encountered cakes

56:09.420 --> 56:12.660
and you've encountered fancy pastries or whatever.

56:12.700 --> 56:16.140
And part of what you know about those meanings

56:16.140 --> 56:17.980
are the grounded parts, right?

56:17.980 --> 56:18.980
You know what a cake looks like,

56:18.980 --> 56:23.620
which is why you can recognize the pictures and things.

56:23.620 --> 56:25.380
I always have a little bit of trouble thinking about it

56:25.380 --> 56:27.700
because it's never quite clear to me exactly

56:27.700 --> 56:29.780
what it means to transfer something grounded.

56:29.780 --> 56:31.780
Like it feels a little bit like in order for it

56:31.780 --> 56:34.500
to transfer at all, it has to be a little bit abstract.

56:36.140 --> 56:38.060
But I agree that that's the right question to ask

56:38.060 --> 56:40.700
and we don't have any theories or certainly no evidence

56:40.700 --> 56:43.780
about how exactly people solve that problem

56:43.780 --> 56:46.700
or the way in which it relies on conceptual roles

56:46.700 --> 56:49.460
versus grounded experience or something.

56:49.460 --> 56:51.060
Okay, so we'll have one more question.

56:51.060 --> 56:53.580
Meanwhile, maybe we can have the next speakers

56:53.580 --> 56:54.660
start setting up.

56:58.340 --> 57:00.980
Yeah, thanks Steve for the great talk.

57:00.980 --> 57:03.340
I wanted to understand better what the argument was

57:03.340 --> 57:07.660
in this kind of conceptual embedding experiment

57:07.660 --> 57:09.980
because it seems like you could just ask the LLM

57:10.460 --> 57:11.620
whether it's tall or not.

57:11.620 --> 57:14.260
Like you didn't really need to do this projection

57:14.260 --> 57:16.420
to know that it can do this task.

57:16.420 --> 57:19.020
So is it somehow, is there something special

57:19.020 --> 57:20.780
about the fact that you're looking at embeddings

57:20.780 --> 57:23.860
rather than the outputs or what's kind of going on there?

57:24.900 --> 57:26.180
That's a good question.

57:26.180 --> 57:28.340
So I think you probably could do that.

57:28.340 --> 57:32.100
I don't know how the results would compare

57:32.100 --> 57:34.020
if you just asked versus not.

57:36.260 --> 57:37.740
Yeah, I'm not sure.

57:37.740 --> 57:41.420
I mean, I think it's like if it doesn't succeed

57:41.420 --> 57:43.300
on just asking, then it's interesting to know

57:43.300 --> 57:45.900
whether it's kind of latent representation

57:45.900 --> 57:47.580
still has that information or not.

57:47.580 --> 57:52.580
So, but I don't know the whole space of kind of how you,

57:52.900 --> 57:54.580
how you can interrogate these models

57:54.580 --> 57:55.900
for those questions, so.

57:57.420 --> 58:00.900
All right, let's thank Steve again.

58:03.780 --> 58:06.980
We'll have plenty of time to talk to him more

58:07.940 --> 58:11.140
at the refreshments after this talk.

58:11.140 --> 58:14.220
And who knows, maybe there will be Zafar Turkish in there.

58:14.220 --> 58:15.780
Which is, by the way, amazing.

58:15.780 --> 58:18.660
If you're in Vienna, you should absolutely try it.

58:18.660 --> 58:23.660
So unfortunately, the next speaker could not be here.

58:23.660 --> 58:28.660
Josh Tenenbaum is a latent variable in this session

58:29.980 --> 58:34.660
because both Stephen was a student of Josh's

58:34.660 --> 58:35.620
and...

