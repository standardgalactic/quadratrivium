1
00:00:00,000 --> 00:00:08,000
Second talk today has a title that has evolved since last time I looked at it.

2
00:00:08,000 --> 00:00:17,280
Okay, but it has logic and algebras and we are very excited to hear what this has to do with cloud computing,

3
00:00:17,280 --> 00:00:22,080
which we are all subjected to in our daily lives.

4
00:00:22,080 --> 00:00:31,440
Thank you. So this will be a bit unusual, both Joe and Connor will talk and I let them take

5
00:00:31,440 --> 00:00:38,640
care of the logistics of that. Again, remember that for questions that might be more appropriate

6
00:00:38,640 --> 00:00:43,360
for a longer discussion, we will have that discussion right after their talk.

7
00:00:45,840 --> 00:00:51,680
Okay, thanks. So this is work that obviously we're doing here at Berkeley and I'm also funded by

8
00:00:51,680 --> 00:00:55,840
Sutter Hill Ventures. So it's kind of cool. We've got venture capitalists funding basic research.

9
00:00:55,840 --> 00:01:00,240
I have promised them that there's no applicable. I'm not really sure what this could turn into

10
00:01:00,240 --> 00:01:04,400
as a company, but they're cool and they've given us developers to work on the project,

11
00:01:04,400 --> 00:01:08,000
which has just been great and they're funding me as well. So thanks to them and to Berkeley.

12
00:01:08,800 --> 00:01:12,480
There's this story people like to tell in computing. This is my standard opening slides.

13
00:01:13,920 --> 00:01:16,960
Operating systems people really like this story because it's sort of the Thompson and

14
00:01:16,960 --> 00:01:21,200
Richie Turing Award story. For every platform that comes out, there's a programming environment

15
00:01:21,440 --> 00:01:25,920
that's somehow suited to that platform that emerges and as a result, people write things you

16
00:01:25,920 --> 00:01:30,720
never would have expected on that platform and it succeeds. So the PDP 11 with Unix and C

17
00:01:30,720 --> 00:01:34,800
is the canonical example of this, but one can argue that in every generation of new kind of

18
00:01:34,800 --> 00:01:40,560
computing platforms, programming environments have arisen to allow people to build an app for that.

19
00:01:41,840 --> 00:01:45,760
And so nobody expected all the apps we have on our phones. It's wonderful. Developers were

20
00:01:46,400 --> 00:01:50,560
freed to write all sorts of things. Strangely, there's a platform that's as old as the iPhone

21
00:01:50,560 --> 00:01:56,000
called the cloud. So AWS is approximately the same age as the iPhone, but it doesn't have a

22
00:01:56,000 --> 00:02:01,680
canonical programming model. And there's many reasons why that might be, partly because it's

23
00:02:01,680 --> 00:02:05,680
a really hard programming environment, yes. So it has to deal with all the problems of parallel

24
00:02:05,680 --> 00:02:11,600
computing, as well as things like distributed consistency, what happens when you have partial

25
00:02:11,600 --> 00:02:15,600
failures in your system, but it keeps running. So componentry is down, but the system's still

26
00:02:15,600 --> 00:02:20,400
running. And then in the modern cloud, we want things to auto scale. So you allocate more machines

27
00:02:20,400 --> 00:02:24,160
and then you free up some machines, but the program's still running. So the platform is

28
00:02:24,160 --> 00:02:29,360
changing underneath you as you're executing. So this is all hard and programmers right now are

29
00:02:29,360 --> 00:02:34,640
trying to do this essentially in Java. That's sort of the state of the art. And the annoying thing

30
00:02:34,640 --> 00:02:40,000
is these compilers for these languages don't answer any of these questions that are hard. So I think,

31
00:02:40,000 --> 00:02:44,640
honestly, this is like this hole in computer science that nobody's filled and it seems like

32
00:02:44,640 --> 00:02:48,720
one of our grand challenges from my perspective. So I've been working on it for a long time,

33
00:02:49,360 --> 00:02:53,120
and I think there's still a lot of work to do. I take inspiration, of course, from this gentleman

34
00:02:53,120 --> 00:02:58,160
as maybe we all do. What was cool about Ted Codd was he said, look, you should write things in a

35
00:02:58,160 --> 00:03:02,000
formal language. You should have formal specifications. And then there should be machinery that

36
00:03:02,000 --> 00:03:06,960
automates the implementation. And if we do that, then the implementation can change

37
00:03:06,960 --> 00:03:11,520
while the specification remains the same. This is very nice for things like databases, right?

38
00:03:11,520 --> 00:03:17,520
So the thing is that Codd was trapped in this database prison for all these years. And I think

39
00:03:17,520 --> 00:03:22,560
there's a much broader applicability of the design principle. So we worked on things in our

40
00:03:22,560 --> 00:03:26,800
community like declarative networking. So we brought Codd out of the database and into the network.

41
00:03:27,760 --> 00:03:31,840
So I've done some work on that. Many of us I think in this room have done something around

42
00:03:31,840 --> 00:03:36,560
declarative data science and machine learning. This is a growing area, right? In the program

43
00:03:36,560 --> 00:03:41,120
analysis community, the use of declarative languages has been pretty powerful. So that's

44
00:03:41,120 --> 00:03:45,360
really cool. And then, of course, the hot thing, which is why we're all here, is that we're going

45
00:03:45,360 --> 00:03:49,840
to start to try to look at this stuff through the lenses of algebras instead of logic or in addition

46
00:03:49,840 --> 00:03:54,240
to logic, which is pretty neat. And we're going to, you know, we've heard or are hearing about a

47
00:03:54,240 --> 00:04:00,000
variety of different algebras that people are playing with in this domain. So what I'm interested in

48
00:04:00,000 --> 00:04:04,480
is taking Codd into the cloud, yeah? And here's sort of the analogy, the way to think about it.

49
00:04:04,480 --> 00:04:09,280
The relational database was invented to hide how data is laid out and how queries are executed,

50
00:04:09,280 --> 00:04:14,000
right? And all that should be decided sort of lazily based on the current environment.

51
00:04:14,000 --> 00:04:19,440
Well, the cloud is just a generalization. It was invented to hide all your computing resources

52
00:04:19,440 --> 00:04:23,200
and how they're laid out. So not just your blocks on your desk, but really everything.

53
00:04:23,760 --> 00:04:29,520
And it's for general purpose computations. So the cloud, in a lot of ways, is this abstraction.

54
00:04:29,520 --> 00:04:34,960
It's this physical layer abstraction. The physics of the deployment of your code is going to change,

55
00:04:34,960 --> 00:04:38,160
but you want your spec to remain the same. That's how you'd really like to program in an

56
00:04:38,160 --> 00:04:43,920
environment that is this heterogeneous and elastic. So I believe that it's extremely natural for

57
00:04:43,920 --> 00:04:48,560
techniques that we've been working on in our community to try to be applied to cloud computing.

58
00:04:48,560 --> 00:04:52,560
And we have a project in my group called Hydro, which you can read more about,

59
00:04:52,560 --> 00:04:58,480
which I will tell you a bit about today. Okay. So what are my goals? Well, I kind of want to

60
00:04:58,480 --> 00:05:04,080
build something like LLVM for the cloud. So LLVM, as you may know, is a very successful sort of

61
00:05:04,160 --> 00:05:10,320
language stack. It supports many languages, including C++ and Rust and Swift and others.

62
00:05:11,040 --> 00:05:14,720
It has an internal language called its internal representation, and then it compiles down to

63
00:05:14,720 --> 00:05:19,920
a variety of machine code for different platforms. So it's been extremely successful, but it doesn't

64
00:05:19,920 --> 00:05:24,480
answer any distributed questions. So if you're writing a distributed program, you might ask a

65
00:05:24,480 --> 00:05:29,040
question like, is my program consistent in some sense? Or if I talk to different machines in the

66
00:05:29,040 --> 00:05:33,440
network, will they give me different answers and be confused? That's a question that distributed

67
00:05:33,440 --> 00:05:38,080
programmers need to deal with. Here's another one. My state no longer fits on one computer. How do

68
00:05:38,080 --> 00:05:41,760
I partition it across multiple computers while getting the same answer out of my program?

69
00:05:42,320 --> 00:05:47,920
I want you, you compiler should figure that out for me. What failures can my system tolerate and

70
00:05:47,920 --> 00:05:51,360
how many of them before it stops working the way that the spec declares it should?

71
00:05:52,560 --> 00:05:57,680
What data is going where around the world and who can see it? These are all questions distributed

72
00:05:57,680 --> 00:06:02,800
systems always have to answer. And then I have different objective functions. So I'd like to

73
00:06:02,800 --> 00:06:08,240
optimize some days for maybe my dollar spend in the cloud, but I don't care about latency or maybe

74
00:06:08,240 --> 00:06:15,360
vice versa. Maybe I care about particular latency distribution. So I want the 99th percentile

75
00:06:15,360 --> 00:06:20,800
of my workload to achieve a certain latency versus the 95th or what have you. These will all

76
00:06:20,800 --> 00:06:26,640
lead to different decisions about resource allocation and program structure and so on, right?

77
00:06:26,640 --> 00:06:31,440
And if you ask these questions of LLVM, that's the answer you get. It doesn't deal with any of

78
00:06:31,440 --> 00:06:36,960
these issues. And that's kind of where we'd like to come in. We've written a vision paper a couple

79
00:06:36,960 --> 00:06:42,560
years ago that I can point you to and I won't go through all of it today. But the idea is to use

80
00:06:42,560 --> 00:06:48,640
database techniques and ideas to optimize in concert with LLVM. So LLVM is responsible for the

81
00:06:48,640 --> 00:06:52,960
single node, but database techniques perhaps responsible for the messaging, the data movement

82
00:06:52,960 --> 00:06:57,920
that happens in a distributed program. So here's how we envision the hydro stack. Many programming

83
00:06:57,920 --> 00:07:04,160
languages up top. Some techniques to translate them into an internal representation. We've

84
00:07:04,160 --> 00:07:08,640
got a little bit of initial work here. And then the internal representation should be some

85
00:07:09,920 --> 00:07:15,440
formal spec that is global in some sense. So it doesn't worry yet about how many machines I have

86
00:07:15,440 --> 00:07:21,360
or what the machines can do. It's just a formalized specification of what you wrote in a perhaps

87
00:07:21,360 --> 00:07:25,360
imperative language. Okay, so it's kind of machine oblivious. Maybe it's a logic. Maybe it's an

88
00:07:25,360 --> 00:07:29,840
algebra. Things that are in red are work in progress. Things that are in green kind of work

89
00:07:29,840 --> 00:07:36,080
at this point. And then from there we want to build a compiler and we're working with Max

90
00:07:36,080 --> 00:07:42,080
on using eGraphs for that compiler to translate down into a per node physical algebra. So every

91
00:07:42,080 --> 00:07:46,720
machine would run its own little program. Those programs communicate with each other very much

92
00:07:46,720 --> 00:07:51,280
like a parallel query plan is a bunch of individual query plans running on individual machines talking

93
00:07:51,280 --> 00:07:56,640
to each other over a network. Okay, so this is a sort of per node physical algebra because it's

94
00:07:56,640 --> 00:08:01,840
actually doing stuff. We've implemented this in Rust. It's very fast and I'll show you some of

95
00:08:01,840 --> 00:08:06,640
this today. So that's kind of what we envision as how this is all going to work. At the bottom

96
00:08:06,640 --> 00:08:10,560
there's something that's deploying this on machines and deciding how many machines and how few over

97
00:08:12,400 --> 00:08:16,000
time. Okay, so some of the topics I want to talk about today we're going to focus on this piece of

98
00:08:16,000 --> 00:08:22,640
the stack. Things in red are work in progress. Very much more questions than answers for sure.

99
00:08:22,640 --> 00:08:27,280
So how do we take code and automatically replicate it along with its associated state or data

100
00:08:27,920 --> 00:08:31,840
while ensuring that the program continues to produce the same outcomes as it would have on a

101
00:08:31,840 --> 00:08:36,560
single machine? So I'm particularly interested in doing this in cases where the replication comes

102
00:08:36,560 --> 00:08:41,840
for free and the individual machines don't have to coordinate with each other in a technical sense

103
00:08:41,840 --> 00:08:46,640
I'll talk about. But we'd like to avoid replication when we can. We'll call that free replication and

104
00:08:46,640 --> 00:08:51,840
this is the domain of the calm theorem which you may have heard of and I will review. Unfortunately,

105
00:08:51,840 --> 00:08:59,680
the calm theorem was done in a very particular framework for the proofs. It's not at all clear

106
00:08:59,680 --> 00:09:04,000
how it applies outside that framework and so what we'd really like is a more algebraic notion of the

107
00:09:04,000 --> 00:09:08,400
calm theorem which is something that Connor's working on and after the talk if you're interested

108
00:09:08,400 --> 00:09:13,440
come find Connor and or me to talk about that. Another topic that Connor's going to talk about

109
00:09:13,440 --> 00:09:18,320
today is termination detection and again ideally termination detection where I can decide it locally

110
00:09:18,320 --> 00:09:23,200
for free without asking anyone else. So how do I know in a streaming program that it's done

111
00:09:23,760 --> 00:09:29,200
when there's other agents in the world? So we're going to talk about how to do that with threshold

112
00:09:29,200 --> 00:09:33,920
morphisms but Connor's got ideas about more general notions of equivalence classes that may

113
00:09:33,920 --> 00:09:38,000
allow termination in more settings. So he'll give you a flavor of this work in progress.

114
00:09:39,120 --> 00:09:42,880
The third piece we may or may not have time for today but my student David Chu has been working

115
00:09:42,880 --> 00:09:47,600
on this how do you take a program and partition the state of the program across machines if the

116
00:09:47,600 --> 00:09:52,240
state doesn't fit on one machine. So you really need to partition the code and the data. This is

117
00:09:52,240 --> 00:09:57,440
very much like traditional shared nothing parallel databases if you like but we want to do this to

118
00:09:57,440 --> 00:10:02,880
full and rather complex data log programs and so we've got an implementation of Paxos which is

119
00:10:03,360 --> 00:10:07,520
number of lines of data log where David's able to do some of this automatic partitioning.

120
00:10:08,320 --> 00:10:12,400
Functional dependencies have a role to play here and it would be nice to integrate those into an

121
00:10:12,400 --> 00:10:18,480
algebraic frame as well. And of course all this has to go fast and ideally as fast as

122
00:10:18,480 --> 00:10:23,520
handwritten C++. I'm really previous iterations of my work we settled for interpreters and just

123
00:10:23,520 --> 00:10:28,320
showing things were possible. Now we'd like to convince the systems people that things will be

124
00:10:28,320 --> 00:10:34,560
as fast as they want them to be. So is this business just pie in the sky?

125
00:10:36,080 --> 00:10:42,720
Let's see. All right so we're building on an obsession of some 10 to 12 years that I've had

126
00:10:42,720 --> 00:10:47,600
now in my third group of grad students working in this area. So the initial batch of grad students

127
00:10:47,600 --> 00:10:53,040
was doing formalisms. So we have this very nice logic called Daedalus which is a subset of data

128
00:10:53,040 --> 00:10:58,000
log neg actually that allows you to talk about time and space in a particular way and it's got

129
00:10:58,000 --> 00:11:03,680
a nice model theory. And so that works all there and we can use that as a basis for you know

130
00:11:03,680 --> 00:11:10,000
our semantics to start which is nice. And then there was this lovely work that Tom Amalut did at

131
00:11:10,000 --> 00:11:15,280
Hasselt the column theorem which was a conjecture I had and he and colleagues went ahead and proved

132
00:11:15,920 --> 00:11:20,720
that talks about how things are monotone in some certain sense. Then you can do this free

133
00:11:21,840 --> 00:11:26,640
replication. So you don't need to do coordination to get replica consistency. So I will talk about

134
00:11:26,640 --> 00:11:31,200
the column theorem today to give a review. And then we actually built out a language. It was

135
00:11:31,200 --> 00:11:36,240
slow. It was interpreted. It was written in Ruby but it integrated lattices into a data log like

136
00:11:36,240 --> 00:11:40,560
language and we were able to show that you can get stratified negation and aggregation. You can

137
00:11:40,560 --> 00:11:45,520
get morphisms on your lattices to allow you to do semi-naive evaluation even with the lattices.

138
00:11:45,520 --> 00:11:49,920
So it's really actually rather a nice mixture of algebra and logic. None of this was formally

139
00:11:49,920 --> 00:11:54,000
proved but it was I think one of the earlier systems to observe that you could build this.

140
00:11:54,080 --> 00:11:58,480
That was pretty cool. And that's Neil Conway's thesis work. So we have this as a basis. This kind

141
00:11:58,480 --> 00:12:03,600
of ground zero for my team. And then what happened is I had a batch of students that didn't want to

142
00:12:03,600 --> 00:12:08,640
do languages or theory. So they just built stuff in the spirit of these ideas. And I'm not going

143
00:12:08,640 --> 00:12:13,920
to go through all this but it's things like functions as a service and protocols and testing

144
00:12:13,920 --> 00:12:19,440
and provenance. It's cool stuff. What I do want to focus on is one of those projects which was a

145
00:12:19,520 --> 00:12:24,800
key value store. Key value store is just like a hash table. It's a database where you look

146
00:12:24,800 --> 00:12:29,040
things up by key and you get a value. So you can think of it as a distributed hash table.

147
00:12:29,680 --> 00:12:33,520
These are like the Petri dishes of distributed systems. You're basically saying I have a memory.

148
00:12:33,520 --> 00:12:37,840
It's distributed across many computers. It may be replicated. It may be partitioned. But that's

149
00:12:37,840 --> 00:12:43,840
what I have. It's just registers with values, keys with values. So there's no algorithms per se.

150
00:12:43,840 --> 00:12:48,080
It's all just kind of like focusing on these semantic issues about replication and partitioning.

151
00:12:49,040 --> 00:12:53,280
But the idea that was behind the Anna key value store that we built was everything's a

152
00:12:53,280 --> 00:12:58,720
semi-ladys. And because everything's a semi-ladys and therefore associative, commutative, and item

153
00:12:58,720 --> 00:13:03,280
potent, messages in the network can be replicated. They can be interleaved. They can be reordered and

154
00:13:03,280 --> 00:13:08,080
everything will be fine. So if you design with semi-ladys from the bottom up, you can build a

155
00:13:08,080 --> 00:13:12,720
system that does no coordination. So it's fully monotonic, which means everything can be replicated

156
00:13:12,720 --> 00:13:17,600
as much or as little as you like. Across the globe, between disks and memory, you can replicate

157
00:13:17,600 --> 00:13:22,720
lots of ways. You can do updates anywhere. So multiple updates to the same key can be happening

158
00:13:22,720 --> 00:13:28,800
concurrently in different places at the same time. And they are merged by the lattice lazily via gossip.

159
00:13:28,800 --> 00:13:32,880
And so you build this thing with no concurrency control whatsoever. So there's no locks in the

160
00:13:32,880 --> 00:13:37,600
system. There's no calls to atomic instructions on processors. There's certainly no Paxos protocols

161
00:13:37,600 --> 00:13:42,560
or anything like that. It's just put, get, and gossip. It's really a simple piece of software.

162
00:13:43,520 --> 00:13:47,440
And so Chenggang Wu, the student who led this, won the dissertation award. I think it was very

163
00:13:47,440 --> 00:13:52,320
well deserved because this system was really elegant and really fast. So to give you a sense

164
00:13:52,320 --> 00:13:56,720
of kind of the lattices that he uses, he took from the literature a variety of these kind of

165
00:13:56,720 --> 00:14:00,800
consistency models that people talk about in distributed systems. And he showed how you can

166
00:14:00,800 --> 00:14:05,840
get them by building little composite lattices that wrap up the data. So this is what's called

167
00:14:05,840 --> 00:14:10,800
last-rater wins in the distributed systems, which is just whenever you see a value that's

168
00:14:10,800 --> 00:14:15,440
from later than your value, you update, otherwise you don't. And this you just, you know, take your

169
00:14:15,440 --> 00:14:21,600
map, which is from keys to things, and you wrap the things in a lexical pair of a clock or a version

170
00:14:22,160 --> 00:14:28,160
and the data, right? And you can only have one value per version. So this, this works out as a

171
00:14:28,160 --> 00:14:34,240
lattice. Here's a fancier one, though. This is actually one of the strongest forms of consistency

172
00:14:34,240 --> 00:14:38,560
you can get without coordination. It's called causal consistency. And here what you have is for

173
00:14:38,560 --> 00:14:44,240
every key, you have a vector clock and the data. And the vector clock itself is a map lattice with

174
00:14:45,120 --> 00:14:47,440
node IDs and counters. Yes.

175
00:14:47,440 --> 00:14:50,480
Just want to say a little bit sort of operational transform.

176
00:14:50,480 --> 00:14:52,080
A little bit. Yes.

177
00:14:52,080 --> 00:14:56,880
Can you ever get stuck? Like, do repairs always exist or do you set it up such that they do?

178
00:14:56,880 --> 00:15:03,200
So these, these particular well trodden forms of consistency work fine. And these lattices

179
00:15:03,200 --> 00:15:06,800
are capturing that. They're saying, look, it's just merge. All right. And it's always going to work

180
00:15:06,800 --> 00:15:11,600
because you've defined an associative commutative item potent merge function. OTs are like really

181
00:15:11,600 --> 00:15:16,640
weird and full of all sorts of reasoning I don't understand. And they would never be able to have

182
00:15:16,640 --> 00:15:20,640
such a simple assertion of correctness. All I'm saying here is it's associative commutative

183
00:15:20,640 --> 00:15:24,880
of an item potent. I got nothing more to say. It takes a little bit of convincing to say that

184
00:15:24,880 --> 00:15:29,040
gives you causal consistency, but it's not much convincing because this helps you make causal

185
00:15:29,040 --> 00:15:33,840
consistency with vector clocks. So the observation that clocks and vector clocks are lattices is

186
00:15:33,840 --> 00:15:37,360
just a nice thing about distributed programming. Yeah.

187
00:15:38,080 --> 00:15:40,960
So what do you mean by everything is a lattice? So you mentioned that

188
00:15:41,920 --> 00:15:43,840
Kira's story is a hash map. Right.

189
00:15:44,720 --> 00:15:51,760
Every key has to be a lattice thing. So the nice thing about the map lattice is the keys

190
00:15:51,760 --> 00:15:57,280
are not lattice values. The keys are just keys. The whole table is a lattice. But the object is

191
00:15:57,280 --> 00:16:01,680
a lattice because what happens is the merge function is for a particular key. If there's

192
00:16:01,680 --> 00:16:05,920
nothing, it gets the value you gave. Or for that key, if there's something there, you apply the merge

193
00:16:05,920 --> 00:16:12,000
function of this lattice. So that is itself a lattice. And these lattice constructors are very

194
00:16:12,000 --> 00:16:16,400
nice. We use map lattice. You see lexical pair over there. And these allow you to take simple

195
00:16:16,400 --> 00:16:20,560
lattices like sets and counters and build up richer lattices out of them, which is a trick

196
00:16:20,560 --> 00:16:25,600
our group likes to play a lot. Other groups sort of are doing research on inventing custom lattices

197
00:16:25,600 --> 00:16:29,520
for custom problems. We've been very much in this kind of know let's just build it up from very

198
00:16:29,520 --> 00:16:39,440
simple building blocks. So the quick version is it's monotone. And if it's monotone, the column

199
00:16:39,440 --> 00:16:43,360
theorem says it's going to be free replication. So you don't have to do coordination to get

200
00:16:43,360 --> 00:16:47,920
replicated consistency. Connor's going to give you a longer talk about this when we get to

201
00:16:48,960 --> 00:16:52,880
conversation about semi-lattice. It's a semi-lattice. I should be clear. It's a semi-lattice.

202
00:16:53,760 --> 00:17:00,240
Do you know whether the people working on coordination free replicated data structures

203
00:17:00,240 --> 00:17:05,280
are well-pure? Yes, they are aware. And Connor will talk about it soon. Yeah, that will come up for

204
00:17:05,280 --> 00:17:11,840
sure. Good. So just to kind of close out this anecdote with Anna, the system is ridiculously

205
00:17:11,840 --> 00:17:16,720
fast. And it's especially ridiculously fast under contention relative to other systems.

206
00:17:16,720 --> 00:17:20,560
So we compared against things like mass tree, which is from Harvard. It's 80 colors,

207
00:17:20,560 --> 00:17:24,320
very fast key value store. We also compared against the multi-threaded

208
00:17:24,320 --> 00:17:28,400
hash table that comes from Intel in their thread building blocks library. That's TBB.

209
00:17:28,400 --> 00:17:33,360
And under contention, those systems, if you look down here, spend most of their time trying and

210
00:17:33,360 --> 00:17:38,160
failing to get atomic instructions. So they'll say test and set on a particular memory address,

211
00:17:38,160 --> 00:17:42,800
and they'll be told no, you have to try again. And they'll spend 95% of their time under contention

212
00:17:42,800 --> 00:17:47,680
doing that, not doing useful work. So they're at 5% good put, if you're familiar with that term.

213
00:17:48,400 --> 00:17:52,000
Whereas Anna, because it does no concurrency control, is just doing puts and gets and puts

214
00:17:52,000 --> 00:17:56,800
and gets and puts and gets and spending most of its time doing good put. And that's why Anna can be

215
00:17:57,360 --> 00:18:01,200
700x better throughput under high contention than these other systems.

216
00:18:02,160 --> 00:18:06,880
But also because it does no coordination scales almost perfectly linearly across threads,

217
00:18:06,880 --> 00:18:12,400
and then across machines, and eventually across the globe. There's really nothing to keep it from

218
00:18:12,400 --> 00:18:16,800
scaling linearly, because the only extra work it has to do is some gossip. And that can be done

219
00:18:16,800 --> 00:18:20,800
in the background, and can be done as lazily as you like without breaking the semantics.

220
00:18:21,440 --> 00:18:24,720
So there's maybe a little fudging here on how stale your data is, but it's correct.

221
00:18:26,640 --> 00:18:32,160
So this was a crazy fast system. And the thing about this, oh, and if you try to run it in the

222
00:18:32,160 --> 00:18:35,840
cloud, it's also incredibly cheap to run relative to systems that are wasting all their time doing

223
00:18:35,840 --> 00:18:40,560
this business. They're charging you for this. They're trying to get locks, they're waiting on

224
00:18:40,560 --> 00:18:45,920
locks, and they're charging you money. So you'd like to avoid that if you can. Okay, that's all

225
00:18:45,920 --> 00:18:51,040
very nice. But it was written in C++ by Chenggang, who's an excellent coder. His implementation is

226
00:18:51,040 --> 00:18:56,320
correct by assertion. It would be really nice to be able to kind of do what CAD wants us to do,

227
00:18:56,320 --> 00:19:00,000
formalize a spec that is correct, and then synthesize an implementation from it through

228
00:19:00,000 --> 00:19:04,320
rules that are correct transformations. So we'd really like to do that, and we'd like to maintain

229
00:19:04,320 --> 00:19:10,320
the speed. What kind of formalisms? Well, we're using lattices mostly. So maybe we could have a

230
00:19:10,320 --> 00:19:15,520
type system that starts with some basic semilattices, like sets and counters, some composition

231
00:19:15,520 --> 00:19:20,560
lattices, like key value pairs, products, lexical products, which aren't always lattices. So you

232
00:19:20,560 --> 00:19:25,040
have to, there's some constraints on whether a lexical product is a lattice. And then we want

233
00:19:25,040 --> 00:19:30,640
like a data flow, like a query plan algebra. So you can imagine a semi-ring kind of algebra, but

234
00:19:30,640 --> 00:19:34,480
you know, there's going to be maps and folds, and then there's going to be physical stuff,

235
00:19:34,480 --> 00:19:38,800
like scan a collection, or get stuff over a network. You know, networks do weird things,

236
00:19:38,800 --> 00:19:43,840
like they permit things, and they form batches of things. They parenthesize streams, if you will.

237
00:19:44,800 --> 00:19:48,720
They multiplex and de-multiplex messages. So there's some physical stuff that we want here too,

238
00:19:48,720 --> 00:19:52,960
and I'd like to really be able to prove all that stuff is correct in a meaningful way.

239
00:19:54,320 --> 00:19:58,960
So just for fun, I don't expect you to read this. This is the ANA implementation. You just saw

240
00:19:58,960 --> 00:20:03,520
written in our low-level hydroflow language. This is the whole thing. It's a very simple program.

241
00:20:04,720 --> 00:20:09,040
And you can see this is kind of a data flow language. It's basically specifying graphs of

242
00:20:09,040 --> 00:20:13,920
data flow. The edges are directed edges in a graph. The words are nodes in the graph,

243
00:20:13,920 --> 00:20:21,360
and you'll see familiar operators like map and join, and cross-join, and so on.

244
00:20:21,360 --> 00:20:26,160
All right, and you can give views names. So this is a little name of a subgraph,

245
00:20:26,800 --> 00:20:31,040
and we use it, and so on. So it's just a little language for specifying graphs.

246
00:20:31,040 --> 00:20:37,680
This is a picture of that program that's output by the system. Okay, so it's a one piece of paper

247
00:20:38,240 --> 00:20:46,080
program. So in this particular program, their union is actually joined, semi-latest join,

248
00:20:47,440 --> 00:20:55,360
and that might be the only one. Yeah. Okay, and so just to convince ourselves this is fast,

249
00:20:55,360 --> 00:21:01,760
that's Chengang's original numbers. We have it now running through hydro, that implementation you saw

250
00:21:01,760 --> 00:21:07,120
on very similar machines, and we get very similar performance to the handwritten code. So we're

251
00:21:07,120 --> 00:21:14,000
feeling pretty good that we're hitting our goals for performance. And because this graph is green,

252
00:21:14,000 --> 00:21:17,920
it's telling us that this thing is all monotone, and therefore consistently replicable.

253
00:21:18,720 --> 00:21:23,280
And at a glance, we can see this is a safe program. And I'm sort of cheating at this point,

254
00:21:23,280 --> 00:21:27,920
and I'm going to confess to that. There's, I think, more work we need to do to make this robust. I

255
00:21:27,920 --> 00:21:33,120
think these green edges are kind of a, they're slightly bi-assertion at this point. So I would

256
00:21:33,120 --> 00:21:37,600
like to make them more fundamentally correct, and hopefully we'll have time to talk about that later.

257
00:21:38,720 --> 00:21:42,880
Okay. With that, I'm going to hand off to Connor. He's going to take us through the next chapter.

258
00:21:46,880 --> 00:21:54,160
Hello. People hear me? Yeah. I'm Connor. I'm a PhD student here working on hydro. I like systems

259
00:21:54,160 --> 00:21:57,440
and theories. So I thought I'd show you guys some of the theory stuff we've been thinking about,

260
00:21:57,440 --> 00:21:59,840
see if anyone has any thoughts, wants to collaborate on anything.

261
00:22:04,960 --> 00:22:10,240
Okay. So in the classical database lens, we have, you know, these three layers,

262
00:22:10,240 --> 00:22:15,680
the relational calculus at the top, relational algebra in the middle, and then a physical

263
00:22:15,680 --> 00:22:21,200
algebra at the bottom, concerned with things like hashing and sorting and so on. And we can think

264
00:22:21,200 --> 00:22:26,000
about how this changes when we move to the cloud setting. And there's good news and bad news on

265
00:22:26,000 --> 00:22:30,080
the current state of affairs when we move to the cloud setting. The good news is that,

266
00:22:30,080 --> 00:22:33,760
like Joe said at the top, we have this Daedalus language from the Bloom project

267
00:22:33,760 --> 00:22:40,160
that is a data log like dialect for distributed systems. The bad news is that developers are

268
00:22:40,160 --> 00:22:45,120
not asking for a data log dialect to build distributed systems in. The developers we've

269
00:22:45,120 --> 00:22:48,640
talked to are a lot more interested in a functional algebraic looking interface and

270
00:22:48,640 --> 00:22:55,520
especially something Pythonic looking like pandas. On the algebra side, the good news is that there

271
00:22:55,520 --> 00:22:59,840
is an algebraic model for distributed systems today. It's the semi-ladis model that Joe has

272
00:22:59,840 --> 00:23:04,560
mentioned that is referred to as CRDTs in a lot of places, especially in the programming languages

273
00:23:04,560 --> 00:23:10,000
community. The bad news is that this is a model for coordination-free updates of state and it

274
00:23:10,000 --> 00:23:14,880
doesn't actually have a query language or give guarantees about coordination-free-ness of queries

275
00:23:14,880 --> 00:23:21,680
today. And then at the physical layer, when we add a network to the situation, asynchronous

276
00:23:21,680 --> 00:23:25,680
computer network, a lot of non-determinism emerges that we need to be able to handle,

277
00:23:25,680 --> 00:23:29,600
in particular reordering, batching, and duplication of messages.

278
00:23:33,040 --> 00:23:39,200
So what we'd like to get to is unifying formalism across logic and algebra and this

279
00:23:39,200 --> 00:23:45,920
physical algebra and have correctness at the physical layer that we can prove for

280
00:23:45,920 --> 00:23:52,560
safe against this non-determinism from the network. And we're able to capture things like

281
00:23:52,560 --> 00:23:57,920
replication, partitioning, batching, incrementalization, and termination analysis. We'll talk about

282
00:23:57,920 --> 00:24:08,080
more later. All right, so let's talk about semi-ladis' CRDTs. So this is a model for

283
00:24:08,080 --> 00:24:12,000
distributed systems that in databases we usually call the semi-ladis model. It's what is called an

284
00:24:12,000 --> 00:24:17,040
Anna in Blum L. It came out of the programming languages community and there it's usually

285
00:24:17,040 --> 00:24:23,680
referred to as CRDTs. It stands for conflict-free replicated data types. It's introduced in this

286
00:24:23,680 --> 00:24:31,520
paper here and there's over 179 papers about CRDTs out there. It's also started to get popular

287
00:24:31,520 --> 00:24:36,400
amongst software engineers and you see people talk about this CRDT model on places like Hacker News,

288
00:24:36,400 --> 00:24:42,880
people starting startups with it. So what does it do? It tries to handle the sources of

289
00:24:42,880 --> 00:24:48,640
non-determinism that come from an asynchronous computer network. These are the arbitrary batching

290
00:24:48,640 --> 00:24:56,320
of messages, arbitrary reordering of messages, and arbitrary duplication of messages. And so it

291
00:24:56,320 --> 00:25:00,240
turns out if you want to be robust to these three things, these actually correspond to algebraic

292
00:25:00,240 --> 00:25:05,120
properties that you need to give you that robustness. So associativity gives you robustness to batching.

293
00:25:05,120 --> 00:25:09,680
You're indifferent to the parenthesization of messages. Communicativity gives you robustness

294
00:25:09,680 --> 00:25:15,600
to reordering and idempotence gives you robustness to duplication. And if you have a set with an

295
00:25:15,600 --> 00:25:19,520
operator that satisfies these three properties, that gives you a semi-ladis. So that's why we're

296
00:25:19,520 --> 00:25:27,280
talking about semi-ladises for distributed systems. So the conflict-free replicated data type is one

297
00:25:27,280 --> 00:25:32,160
specific model of a semi-ladis interface, but since it's the most popular one today,

298
00:25:32,240 --> 00:25:38,560
I'm going to talk about it. So the idea is that it's an object-oriented view of a distributed

299
00:25:38,560 --> 00:25:42,080
system where you define some object, and you're going to define three methods on that object.

300
00:25:42,720 --> 00:25:45,760
And then you can replicate this object across the distributed system,

301
00:25:45,760 --> 00:25:51,840
and the replicas will converge, regardless of network non-determinism. So you have your merge

302
00:25:51,840 --> 00:25:58,560
operator, which is your associative, commutative, and idempotent ACI semi-ladis operator, which

303
00:25:58,560 --> 00:26:04,960
combines the state of two replicas. You have a way to update state, which the requirement is just

304
00:26:04,960 --> 00:26:09,760
that that's monotone with respect to the partial order induced by this merge operation. And then

305
00:26:09,760 --> 00:26:14,960
you have a query, which is just a method on this object, but today there's not a specific query

306
00:26:14,960 --> 00:26:19,440
language. You don't have any sort of guarantees on what that query does. It just reads this semi-ladis

307
00:26:19,440 --> 00:26:27,200
state. So we're looking at an example of a CRDT. This comes from the Amazon Dynamo paper for how

308
00:26:27,200 --> 00:26:32,720
they're implementing shopping carts. And the idea is that you have updates that are going to add or

309
00:26:32,720 --> 00:26:36,560
remove elements to your shopping cart. In this case, you add a Ferrari to your shopping cart, add a

310
00:26:36,560 --> 00:26:41,920
potato, and you can also remove the Ferrari. And the state is going to be represented as two sets,

311
00:26:41,920 --> 00:26:48,080
a set of items that you've inserted and a set of items that you've removed. And then to merge,

312
00:26:48,080 --> 00:26:52,720
we do a pairwise union of these two sets. And the query, what's actually in my shopping cart I want

313
00:26:52,720 --> 00:26:59,840
to check out, is set difference. Subtract the removes from the inserts. And there's a few

314
00:26:59,840 --> 00:27:05,200
interesting things going on with this example. One is we're guaranteeing the coordination-free

315
00:27:07,200 --> 00:27:13,520
rights that these two states are going to converge. But our query is a non-monotone query,

316
00:27:13,520 --> 00:27:18,160
which the column theorem tells us is not a coordination-free query. So CRDTs are not giving

317
00:27:18,160 --> 00:27:23,120
us the invariant that the column theorem requires on queries, which is that if we output a tuple

318
00:27:23,120 --> 00:27:28,160
at a certain point in time, we're never going to retract that tuple in the future. Here, over time,

319
00:27:28,160 --> 00:27:36,080
we will retract tuples as the remove set grows. We had a vision paper in BLDB this year about

320
00:27:36,080 --> 00:27:41,280
this gap between what CRDTs guarantee and what the column theorem guarantees and ideas for how to

321
00:27:41,280 --> 00:27:47,760
resolve it. Another thing you might have noticed that's kind of odd about that representation

322
00:27:47,760 --> 00:27:54,320
of data is that if you think about how we might represent updates like this to a shopping cart

323
00:27:54,320 --> 00:27:59,200
in a database, you might have imagined that we would have a count on each item and we would

324
00:27:59,200 --> 00:28:02,720
increment that count when we add an item and we decrement that count when we remove an item.

325
00:28:02,720 --> 00:28:06,720
This is what you'd see, something like incremental view maintenance where your update

326
00:28:06,720 --> 00:28:12,400
operation forms an abelian group, not a semi-ladis. So why not do something like that?

327
00:28:13,280 --> 00:28:19,520
Well, for one, it doesn't form a semi-ladis and it's not immediately obvious how to convert it

328
00:28:19,520 --> 00:28:25,760
into one. So this representation is two sets, what you call a two-phase set. It's more obviously

329
00:28:25,760 --> 00:28:32,560
monotonously growing update operation, but it turns out it actually is possible to convert

330
00:28:32,560 --> 00:28:38,400
this abelian group representation into a valid semi-ladis in terms of being robust to network

331
00:28:38,400 --> 00:28:43,760
non-determinism. I won't go into all the details on that, but it's based on what Joe is saying with

332
00:28:43,760 --> 00:28:52,000
these vector clocks where you wrap the states basically in a vector clock which forms a semi-ladis.

333
00:28:53,200 --> 00:28:57,680
The downside of doing this is that vector clocks require linear memory and the number of replicas

334
00:28:57,680 --> 00:29:02,720
in the system, so that's the reason why people wouldn't use this representation today. But we

335
00:29:02,720 --> 00:29:10,480
have some work on a protocol for enforcing this kind of conversion into a semi-ladis in

336
00:29:10,480 --> 00:29:14,640
constant rather than linear space. So if anyone's interested in that idea, definitely come find me

337
00:29:14,640 --> 00:29:25,600
and talk about it. Okay, so like I said, the semi-ladis model today does not have a query

338
00:29:25,600 --> 00:29:30,800
language on top of it. So what might we want out of a query language for the semi-ladis model?

339
00:29:30,800 --> 00:29:36,640
Well, we want expressivity. Like we saw in the shopping cart example, we need set difference,

340
00:29:36,640 --> 00:29:44,080
so we need negation. Also recursion, something like datalog. We also want obviously classical

341
00:29:44,080 --> 00:29:48,560
query optimization options. We want identities that we can use to transform our query,

342
00:29:48,560 --> 00:29:53,520
get better performance. And we want to be able to do monotonicity analysis as well as functional

343
00:29:53,520 --> 00:30:02,080
and dependency analysis for partitioning. And so something like datalog for semi-ladises

344
00:30:02,960 --> 00:30:06,960
might be a good fit here, but there's a lot to explore. And so now Joe is going to talk about

345
00:30:07,520 --> 00:30:09,840
this monotonicity analysis and functional dependency analysis.

346
00:30:16,560 --> 00:30:21,280
I should say from the previous slide that some of this is things we took a crack at with

347
00:30:21,280 --> 00:30:25,840
BlueMal, so it's not that we've done nothing here. There's some answers to these questions,

348
00:30:25,840 --> 00:30:31,520
but there's also work to be done. All right, so I wanted to step back and review for you folks,

349
00:30:31,520 --> 00:30:37,360
the COM Theorem, which I know is sort of in a sub-corner of the pods community and not everyone's

350
00:30:37,360 --> 00:30:41,360
going to be familiar with it, but I think it's useful to go over. This will be high level,

351
00:30:41,360 --> 00:30:47,200
but hopefully helpful enough for you to get into the game. So the challenge is that we're going to

352
00:30:47,200 --> 00:30:52,800
have computers spread across the globe and we want our replicas to be consistent. So we have

353
00:30:52,800 --> 00:30:57,920
this nice couple here, they're in different places, and the classic example of replica consistency

354
00:30:57,920 --> 00:31:01,760
is data replication. So forget about programs, we're just going to have data, kind of like the

355
00:31:01,760 --> 00:31:07,120
CRDT model. And I want everybody to have common beliefs about the data, at least eventually.

356
00:31:07,840 --> 00:31:12,160
So these two folks currently both believe that X is love, which is lovely, but if it's a beautiful

357
00:31:12,160 --> 00:31:17,600
variable, things could change, right? And that's very sad. And once they disagree on the value,

358
00:31:17,600 --> 00:31:21,600
they might make decisions based on their disagreement that will lead to further

359
00:31:21,600 --> 00:31:25,920
divergence. This is sometimes called the split brain problem, because you can't put it back

360
00:31:25,920 --> 00:31:32,880
together later on, it's too messy. And so we want to generalize the idea of consistency of data

361
00:31:32,880 --> 00:31:38,160
replication to consistency of program outcomes. So I'm not just interested in the data, I'm interested

362
00:31:38,160 --> 00:31:43,520
in the queries, if you will, right? Much more powerful, and it will allow us to cheat sometimes,

363
00:31:43,520 --> 00:31:48,320
the data could be inconsistent if the query outcomes are not, right? So it'll give us more

364
00:31:48,320 --> 00:31:54,320
ability to relax our coordination. So we'd like to generalize this to program outcomes independent

365
00:31:54,320 --> 00:32:00,400
of data races when we can. The classical solution to this stuff is coordination. This is what things

366
00:32:00,400 --> 00:32:05,440
like Paxos and Two-Phase commit were invented to solve. And the way they solve it is by saying,

367
00:32:05,520 --> 00:32:09,840
what if we were just on one computer with one processor? Maybe we could implement that in a

368
00:32:09,840 --> 00:32:14,640
distributed fashion, which is a very heavy handed solution, right? You say that our solution to

369
00:32:14,640 --> 00:32:18,640
parallelism is to remove it. And how can we remove it in the distributed context? Well,

370
00:32:18,640 --> 00:32:24,320
it's expensive. But here's how it goes, right? On a single node, you use atomic instructions,

371
00:32:24,320 --> 00:32:28,320
right? So if you have shared memory, you can use atomic instructions, or maybe you use a locking

372
00:32:28,320 --> 00:32:32,240
system. In the distributed environment, you use something like Paxos or Two-Phase commit. And

373
00:32:32,320 --> 00:32:37,920
at every scale, as you saw in that ANA work, you'd like to not do these things. So even on a single

374
00:32:37,920 --> 00:32:41,520
machine, you really don't want to be doing coordination. And certainly in the distributed

375
00:32:41,520 --> 00:32:45,920
setting, this is very heavy weight. And there's people who will tell you at great length why

376
00:32:45,920 --> 00:32:49,680
they don't let the developers in Amazon call these libraries unless they have, you know,

377
00:32:49,680 --> 00:32:54,480
16 gold stars. Because it will slow down the whole environment and create queue backups and all

378
00:32:54,480 --> 00:32:59,440
kinds of horrible things. So when can we avoid coordination? This was a question that I asked

379
00:32:59,440 --> 00:33:03,600
as a lazy professor, because I was thinking maybe I should learn and teach Paxos, and I kind of

380
00:33:03,600 --> 00:33:07,680
didn't want to. So I was like, maybe, you know, maybe we don't need this stuff. Maybe Lamport's

381
00:33:07,680 --> 00:33:12,720
just a bunch of bunk. So that's kind of where this started, sheer laziness, intellectual laziness,

382
00:33:12,720 --> 00:33:17,600
which I will cop to. But what it led to, sometimes when you ask a question about how can I be lazy,

383
00:33:17,600 --> 00:33:21,600
you end up asking a question that turns out to be quite interesting. I think that's what arose here.

384
00:33:21,600 --> 00:33:26,240
And I'm seeing this not only in my work, but in other places. Back in the 20th century, if you

385
00:33:26,240 --> 00:33:31,200
will, the Lamport Gray era, we were trying to emulate sequential computation. We were doing

386
00:33:31,200 --> 00:33:35,440
everything we could to give the programmer the illusion of a sequential computer. And it was all

387
00:33:35,440 --> 00:33:40,240
about, you know, very low level stuff, reads and writes, accesses and stores, right? And then

388
00:33:41,120 --> 00:33:45,920
guarantees of order, total order, linearizability and serializability. And this was all based on

389
00:33:45,920 --> 00:33:50,480
the idea that programmers are hopeless. They'll write all kinds of crazy code. And the only thing

390
00:33:50,480 --> 00:33:54,080
they understand is sequential computers. So we'll make the worst case assumption that their stuff

391
00:33:54,080 --> 00:33:59,440
wouldn't work in parallel, right? And we'll give them mechanisms for avoiding parallelism.

392
00:33:59,440 --> 00:34:04,400
Seems like a terrible thing to do in a parallel environment. Yeah. So what's happening in the

393
00:34:04,400 --> 00:34:09,600
21st century is if we lift our, so this is all great. And sometimes you need it. I don't mean

394
00:34:09,600 --> 00:34:13,120
to denigrate the work. This is obviously foundational, touring awards. I use this stuff. I teach this

395
00:34:13,120 --> 00:34:17,440
stuff. It's all good. But when we don't need to use it, even better, right? So people have tried

396
00:34:17,440 --> 00:34:20,720
doing things like, what if all our states are mutable? That's a very functional programming game.

397
00:34:21,440 --> 00:34:25,600
It was sort of in my world, it's more about, well, you can mutate things as long as it's

398
00:34:25,600 --> 00:34:29,920
monotone. So if they're mutable, but they're monotone, maybe that'll work. And then using

399
00:34:29,920 --> 00:34:35,680
things like dependencies and provenance, all our ways of using application knowledge to avoid using

400
00:34:35,680 --> 00:34:41,120
the expensive stuff on the left. But the really big query is when do I need coordination and why

401
00:34:41,120 --> 00:34:47,360
do I need coordination? So if you ask, you know, a typical undergraduate or frankly, most people in

402
00:34:47,360 --> 00:34:52,560
computer science, including professors, when do you need coordination? What's a lock for, right?

403
00:34:52,560 --> 00:34:57,920
They'll say, well, it's to avoid conflicts on shared resources, right? This intersection needs

404
00:34:57,920 --> 00:35:02,880
coordination. If you would just put up some damn stop lights, right, then, you know, north, south

405
00:35:02,880 --> 00:35:07,040
could go for a while and west, east, west would wait. And then east, west would go for a while,

406
00:35:07,040 --> 00:35:12,960
north, south would wait, problem solved, right? But like, do I really need coordination? That's

407
00:35:13,040 --> 00:35:17,760
a solution. Is it the only solution? No, it's not the only solution, right? Here's a coordination

408
00:35:17,760 --> 00:35:23,120
free solution to that intersection problem, right? So I'd like to be able to think out of the box,

409
00:35:23,120 --> 00:35:28,400
right, and say, really, what is coordination for? Why am I required to use it?

410
00:35:30,720 --> 00:35:37,760
Okay. So that's a theory problem. So, you know, which programs have a coordination free implementation?

411
00:35:37,760 --> 00:35:42,000
We call those the green programs. These are specifications for which a clever programmer

412
00:35:42,000 --> 00:35:46,080
can find a coordination free solution. And then, of course, there's the rest of the programs,

413
00:35:47,440 --> 00:35:51,440
right? And I want to know this green line. Will someone please tell me, you know,

414
00:35:51,440 --> 00:35:55,760
Mr. Lamport, I think I only need you out here. So will you please tell me when I need you? And

415
00:35:55,760 --> 00:35:59,360
there's no answer from Mr. Lamport. At least he didn't, you know, pick up the phone when I call.

416
00:36:00,400 --> 00:36:05,600
But I'm happy to say that people at Hussalt did. And this is what led to the column theorem. So

417
00:36:05,600 --> 00:36:09,680
this is really a computability question. What's the expressive power of languages without coordination?

418
00:36:10,240 --> 00:36:16,720
Yeah. That's the green circle. Okay. So give you some intuition. Easy and hard questions.

419
00:36:16,720 --> 00:36:18,880
Here's an easy question. Is anyone in the room over 18?

420
00:36:21,360 --> 00:36:27,040
Excellent. Not only were you all happy to answer that coordination free, but you engaged in a

421
00:36:27,040 --> 00:36:30,560
little protocol, right? You made up a protocol where you raise a hand if you think it's true. So

422
00:36:30,560 --> 00:36:34,480
that was cool. So that was the monotone hand raising protocol or something. Great. All right.

423
00:36:34,480 --> 00:36:40,480
Who's the youngest person in the room? Oh, we have some brave assertions. But clearly,

424
00:36:40,480 --> 00:36:45,440
you don't know that. You could look at everyone, but that's cheating and also not necessarily

425
00:36:45,440 --> 00:36:52,880
right. Maybe, maybe. I don't know. I don't know. But the point here is, right, that somehow this

426
00:36:52,880 --> 00:36:57,360
requires you to communicate with people. And the first one maybe doesn't. Okay. More to the point.

427
00:36:57,360 --> 00:37:01,520
Let's look at the logic here, right? This is an existential question. And this is a question with

428
00:37:01,520 --> 00:37:07,680
the universal quantifier in it. Or for people like me who just want to do total pattern matching

429
00:37:07,680 --> 00:37:12,720
and look for not symbols, that one appears to be positive. So I'll say that it's monotone.

430
00:37:12,720 --> 00:37:16,640
And that one appears to be negative. So I'll say it's not monotone. So it gives you some intuition

431
00:37:16,640 --> 00:37:22,400
that universal quantification or negation requires coordination. It is coordination. That's what

432
00:37:22,400 --> 00:37:28,080
coordination is. It's universal quantification. So what is Lamport for? It's for universal quantifiers.

433
00:37:28,960 --> 00:37:33,040
So let's just prove this, right? I was like, well, somebody prove it. I'm not going to prove it.

434
00:37:33,040 --> 00:37:38,160
So nice guy named Tom Omelette wrote a thesis on this stuff. My conjecture was called the

435
00:37:38,160 --> 00:37:44,400
calm conjecture consistency is logical monotonicity. It was in a Paz keynote that I was gave some years

436
00:37:44,400 --> 00:37:48,720
ago. And then just a year later, there was a conference paper from the good folks at Haselt,

437
00:37:48,720 --> 00:37:54,400
which then they extended and then was further extended with weaker definitions for the monotonicity

438
00:37:54,400 --> 00:37:59,920
to really expand the results. If you want a quick, you know, kind of a version of what I'm saying now,

439
00:37:59,920 --> 00:38:03,600
you can read this CACOM overview, but it's really for systems people. I think you guys should just

440
00:38:03,600 --> 00:38:09,440
read Tom's papers. All right. To give you a flavor of what Tom did, definitions are half the battle.

441
00:38:09,440 --> 00:38:12,800
It seems, you know, when I read Paz papers, that's all the hard parts are the definitions, right?

442
00:38:13,920 --> 00:38:18,400
So, you know what monotonicity is in logic? That's fine. What is consistency here? Well,

443
00:38:18,400 --> 00:38:22,800
we want the program to produce the same output regardless of where the initial data is placed.

444
00:38:22,800 --> 00:38:26,720
So, I should be able to start the program with the data replicated pops possibly and

445
00:38:26,720 --> 00:38:32,080
partitioned in any way and get the same answer. And if that's true, then it would be the same

446
00:38:32,080 --> 00:38:35,360
answer across replicas. It would be the same answer across different runs. It would be the

447
00:38:35,360 --> 00:38:39,600
same answer if we start gossiping the data between each other. And this is what we want,

448
00:38:39,600 --> 00:38:44,000
right? So, that's our definition of consistency, where I think what's really clever and was the

449
00:38:44,000 --> 00:38:48,960
most beautiful part of the work is defining what coordination really means. So, we're sending

450
00:38:48,960 --> 00:38:53,840
messages around, right? That's data. But which data is really data and which data is kind of

451
00:38:53,840 --> 00:39:00,240
control messages? And how do you differentiate those in a formal way? And so, what they define

452
00:39:00,240 --> 00:39:05,760
in this paper is program is coordination free if there's some partitioning of the data when

453
00:39:05,760 --> 00:39:09,840
you first start. So, there's some way out of the data where you first start, such that the query

454
00:39:09,840 --> 00:39:14,560
can be answered without communication. So, for a particular query, for a particular data set,

455
00:39:14,640 --> 00:39:18,640
there is some world of where you lay it out where no communication is required.

456
00:39:19,920 --> 00:39:25,440
That's the definition of coordination freeness. And a program that you can't do that on is doing

457
00:39:25,440 --> 00:39:30,960
coordination messages. So, it's not really saying which messages are coordination and which messages

458
00:39:30,960 --> 00:39:39,360
are data, but it's telling you which programs can be run coordination. Yes. So, the trivial example

459
00:39:39,360 --> 00:39:43,840
of this is you put all the data in one node. And again, you know, this question of is there anybody

460
00:39:43,840 --> 00:39:50,400
who is older than me? What you don't know is whether anyone else has data. So, I may have all

461
00:39:50,400 --> 00:39:54,080
the data, but I don't know that. So, I still have to ask everybody, anybody got any data?

462
00:39:54,080 --> 00:39:58,560
And I have to wait for everybody to respond, right? So, it's a very nice intuition to just think

463
00:39:58,560 --> 00:40:06,560
about having all the data. All right. There's another thing in the paper that I hadn't even

464
00:40:06,560 --> 00:40:11,040
anticipated, which is really beautiful and speaks to stuff that the distributed systems

465
00:40:11,040 --> 00:40:15,280
community kind of knows, which is there's a third equivalent, which is that you can

466
00:40:15,280 --> 00:40:21,200
distributively compute this program on an oblivious transducer. And I haven't even talked

467
00:40:21,200 --> 00:40:25,440
about transducers yet, but just a minute. But what does it mean by oblivious? It means that

468
00:40:25,440 --> 00:40:30,240
the agent in the system doesn't know its own identity. It cannot distinguish messages from

469
00:40:30,240 --> 00:40:36,880
itself from messages from anyone else. So, it doesn't actually know who itself is. And it

470
00:40:36,880 --> 00:40:42,480
doesn't know the set of participants. We call this an oblivious agent, right? Oblivious programs

471
00:40:42,480 --> 00:40:46,320
that can be computed by oblivious agents are exactly the monotone programs and exactly the

472
00:40:46,320 --> 00:40:50,240
coordination free programs. So, that was very cool. And it speaks to questions of like

473
00:40:50,960 --> 00:40:55,120
membership protocols in distributed systems, which is about establishing common knowledge

474
00:40:55,120 --> 00:40:58,800
of the all relation. That's like one of the things that Paxos does is it has a membership

475
00:40:58,800 --> 00:41:03,680
protocol built in. So, it's one of the reasons it's coordination full is to establish all.

476
00:41:03,680 --> 00:41:08,960
So, this was really, really nice. So, this is all in this JACM paper. It's actually in the

477
00:41:08,960 --> 00:41:14,640
conference, the Paz paper as well. That's just a flavor of the calm stuff. And I'm going to stop

478
00:41:14,640 --> 00:41:18,480
with that. But happy to answer questions as best they can afterwards. And with that,

479
00:41:18,480 --> 00:41:19,600
I'm going to give it back to Connor.

480
00:41:20,160 --> 00:41:34,800
You guys can still hear me? All right. So, we have this calm theorem view of the world,

481
00:41:34,800 --> 00:41:41,040
relational transducers, logic, operating over sets. And then we have this semi lattice

482
00:41:41,040 --> 00:41:46,960
algebra view of the world. And they both are dealing with coordination and freeness in different

483
00:41:46,960 --> 00:41:52,240
lenses. But currently, they guarantee different things. The algebra view, like we said, is

484
00:41:52,240 --> 00:41:55,680
concerned with the coordination of freeness of rights and does not guarantee coordination

485
00:41:55,680 --> 00:41:59,760
of freeness of queries. Whereas the calm theorem view is only concerned with the

486
00:41:59,760 --> 00:42:02,400
coordination of freeness of queries. It actually doesn't have to worry about the

487
00:42:02,400 --> 00:42:06,000
coordination of freeness of rights because it assumes operating over sets and gets

488
00:42:06,000 --> 00:42:11,920
coordination of rights for free that way. And so, we're interested in the question of

489
00:42:11,920 --> 00:42:17,360
how can we combine these two lenses? Can we do an algebraic view of the calm theorem?

490
00:42:18,560 --> 00:42:25,280
And some intuition for how that might work is, you know, the semi lattice operator induces a

491
00:42:25,280 --> 00:42:31,120
partial order. And so, instead of having monotone logical queries without negation, you have

492
00:42:31,120 --> 00:42:37,600
monotone functions between lattices, between partial orders. So, that's something we've

493
00:42:37,600 --> 00:42:41,840
been exploring. We'd love to chat more about it with folks. I'm actually going to talk about

494
00:42:42,640 --> 00:42:48,640
a problem specific to the question Remy asked. It comes up in this setting.

495
00:42:49,360 --> 00:42:54,640
So, the calm theorem is all about basically not knowing when you have the entire input. What can

496
00:42:54,640 --> 00:43:01,360
I output and tell downstream consumers with certainty, even though I might have more updates

497
00:43:01,360 --> 00:43:06,000
in the future, there might be more messages arriving. And so, we call the ability to do this free

498
00:43:06,000 --> 00:43:15,680
termination without coordination. What can we be sure that we can output? And we're exploring

499
00:43:15,680 --> 00:43:21,280
this in a very generic setting of just we have two functions, an operation that's going to change

500
00:43:21,280 --> 00:43:29,520
our state over time in a query that's going to return some output. So, looking at an example

501
00:43:29,520 --> 00:43:34,480
of when we might be able to do this, we can look at this lattice. This is the set over these four

502
00:43:34,480 --> 00:43:42,400
socks and say that this is our state of a local node and we're at top in this lattice. And our

503
00:43:42,400 --> 00:43:51,120
update operation in the CRDT sense is union. So, we're going to union in more socks. We know that

504
00:43:51,120 --> 00:43:57,040
if we're at top, as updated as monotone, we'll never change our state. We're stuck at top.

505
00:43:57,920 --> 00:44:01,840
And so, whatever query we might be asking when we're in this state, we'd be able to locally

506
00:44:01,840 --> 00:44:06,000
detect that our query result is not going to change in the future and we'd be able to return

507
00:44:06,000 --> 00:44:12,000
our result with certainty. This might sound like it would not happen particularly often,

508
00:44:12,000 --> 00:44:16,240
but let's try and look at more examples where we would be able to figure out that with certainty,

509
00:44:16,240 --> 00:44:22,960
we can return an answer right now. So, what if we consider also the query that we're asking

510
00:44:22,960 --> 00:44:29,520
and say that this query is going to map us from this set socks lattice to the boolean lattice,

511
00:44:29,520 --> 00:44:37,600
true false, where top is true. Now, if this query is monotone, meaning as we go up in the partial

512
00:44:37,600 --> 00:44:44,640
order of socks, we also go up in the partial order of false and true, then we don't need to be at

513
00:44:44,640 --> 00:44:52,080
top on the left. We can actually be at top in the true false lattice and guarantee that our result

514
00:44:52,080 --> 00:44:57,040
won't change as future updates arrive. Any update that arrives is going to cause us to increase

515
00:44:57,040 --> 00:45:02,000
monotonically in the domain, which has to increase monotonically in the range and therefore

516
00:45:02,000 --> 00:45:09,600
our result will always stay true. For example, a query, is there a pair of socks?

517
00:45:11,120 --> 00:45:15,520
So, we call a threshold query. It effectively draws a cut through this partial order and says

518
00:45:15,520 --> 00:45:19,760
everything above this line in the partial order is true, everything below this line is false.

519
00:45:21,360 --> 00:45:25,120
So, these boolean threshold queries are a class of queries that we can freely terminate

520
00:45:25,120 --> 00:45:33,200
on if we know that our update is monotone. What about a totally different setting?

521
00:45:33,200 --> 00:45:38,240
What if we throw away monotonicity? So, now imagine that we have a deterministic finite

522
00:45:38,240 --> 00:45:45,040
automata and our query is mapping to true and false from accept and reject states in the automata

523
00:45:45,760 --> 00:45:49,200
and our update is appending a character. So, we think of a streaming

524
00:45:50,000 --> 00:45:54,960
character as appending. So, each update is going to transition us one step in this automata.

525
00:45:58,480 --> 00:46:05,040
And in this automata, any state that we're in, we can't conclude what the final result will be

526
00:46:05,040 --> 00:46:10,080
because from every state, there's a state that's accepting, that's reachable, and there's a state

527
00:46:10,080 --> 00:46:13,520
that's rejecting, that's reachable. So, some sequence of future updates might take us to

528
00:46:13,520 --> 00:46:15,680
false, some sequence of superstructures might take us to true.

529
00:46:18,400 --> 00:46:25,040
In contrast, if state three were also true, now from state one, we actually don't know if we're

530
00:46:25,040 --> 00:46:29,200
going to end up accepting or rejecting if we don't have the whole input yet. But if we're in states

531
00:46:29,200 --> 00:46:33,760
two or three, we know that every state that's reachable via future updates is going to keep us

532
00:46:33,760 --> 00:46:40,160
in our current result, which is true. And so, we can be certain that we can terminate here and return

533
00:46:40,160 --> 00:46:46,080
true. This is kind of like a reachability sort of visual view of how we're thinking about whether

534
00:46:46,080 --> 00:46:50,640
or not you can freely terminate given some arbitrary update operation on a domain and query operation

535
00:46:50,640 --> 00:46:56,160
that maps you to a range. This is a question raised in exploring a lot of different domains. If

536
00:46:56,160 --> 00:47:01,760
anyone has any ideas for what might connect to this, definitely come find us. Now, Joe is going to

537
00:47:01,760 --> 00:47:08,560
talk about partitioning. All right, this is a bit of a survey time. Boris, did you want to ask a

538
00:47:08,560 --> 00:47:12,960
question, Connor? I have a question to the last slide, right? Because in a sense, this now still

539
00:47:12,960 --> 00:47:18,640
has some monotone ordering, right? This kind of like, in a sense, it's kind of like if I can reach a

540
00:47:18,640 --> 00:47:26,240
node and it's smaller and now basically the nodes, if the terminal nodes are the top nodes and they

541
00:47:26,240 --> 00:47:31,520
don't have larger nodes, then so it's still monotone in the sense. Yeah, you can find some sort of

542
00:47:31,600 --> 00:47:37,760
it that, yeah. Yeah, I don't know if that's true for every freely terminating function.

543
00:47:38,480 --> 00:47:52,240
Psycho, do you think? Yeah, maybe. There's something about quotient lattice is, too, going at it in

544
00:47:52,240 --> 00:47:59,040
partial orders. Yeah, the graph looks like one. So this is love to have this conversation afterwards.

545
00:47:59,040 --> 00:48:04,240
That's why we're touching on a few things so we can have many conversations. So I think given time,

546
00:48:04,240 --> 00:48:08,400
I'm not going to go through this in any detail. I'm going to basically skip this chapter of the

547
00:48:08,400 --> 00:48:13,600
talk, except to quickly give some assertions. So first of all, we don't have HydroFlow, so we

548
00:48:13,600 --> 00:48:18,960
use Daedalus and we do have a full Daedalus to HydroFlow compiler. So we're able to write global

549
00:48:18,960 --> 00:48:24,160
programs in Daedalus and then auto partition and auto replicate them. And that's work being led by

550
00:48:24,160 --> 00:48:29,200
David Chu, who's in the room over here. David, three years ago, promised to do this and they

551
00:48:29,200 --> 00:48:34,560
gave him a certificate saying that's cool. So he won the SSP student research award. And three years

552
00:48:34,560 --> 00:48:39,200
later, he's got his first results. So it took a while. This was not an easy problem, but he's able

553
00:48:39,200 --> 00:48:47,440
to take arbitrary Daedalus programs, which are Daedalug, and partition them to run on multiple

554
00:48:47,440 --> 00:48:51,760
machines. And I'm really not going to spend a lot of time on this. What I'll say is that

555
00:48:52,720 --> 00:48:58,320
earlier student Michael Whitaker, who again did this by assertion, he found all sorts of opportunities

556
00:48:58,320 --> 00:49:03,520
to optimize Paxos because inside of Paxos, it was bottlenecking on things that were trivially

557
00:49:03,520 --> 00:49:08,720
parallelizable, like network message handling. So he's like, if I can just bust apart some of the

558
00:49:08,720 --> 00:49:14,160
roles in Paxos into sub roles, some of those sub roles can be replicated. And he got state-of-the-art

559
00:49:14,160 --> 00:49:19,280
performance in terms of throughput on Paxos by doing this. And what I observed after he did it

560
00:49:19,280 --> 00:49:23,920
was, oh my gosh, most of the things that you've split out are monotone subcomponents. And I should

561
00:49:23,920 --> 00:49:28,480
have known that we could pull those out and replicate those. In fact, I wish Bloom could do

562
00:49:28,480 --> 00:49:32,720
that automatically, but it couldn't. So three years later, David can now automatically pull out

563
00:49:32,720 --> 00:49:39,360
these things that Michael was observing and transform the program to do it. And the ideas are

564
00:49:39,360 --> 00:49:47,520
basically just two tricks. One trick is to take a pipeline on a single machine and split it across

565
00:49:47,520 --> 00:49:52,720
two machines. He calls that decoupling. Now in a general purpose program, this is taking,

566
00:49:52,720 --> 00:49:56,400
I don't know, 10,000 lines of C++ and figuring out which ones to run on this machine and which

567
00:49:56,400 --> 00:50:00,400
ones to run on that machine. That would be horrible, right? But in a data flow language or

568
00:50:00,400 --> 00:50:04,320
logic language, it's quite nice. And so he has conditions for when this is safe and when it's

569
00:50:04,320 --> 00:50:08,480
not. So that's decoupling. So you can think of this as refactoring a program into components

570
00:50:08,480 --> 00:50:13,040
that can be run on different machines with asynchronous communication. The other thing

571
00:50:13,040 --> 00:50:18,720
he does is what's sometimes called sharding in the practitioner community, but it's partitioning,

572
00:50:18,720 --> 00:50:24,240
shared nothing partitioning of subplants, right? So instead of having BC take all of the data from A,

573
00:50:24,240 --> 00:50:29,040
you have a hash partitioning here and certain values go to each machine. And how do you know

574
00:50:29,040 --> 00:50:32,560
that each one of these computations can be done independently? That's done through things like

575
00:50:34,160 --> 00:50:39,120
functional dependency analysis so that you can show that certain values have no dependency on

576
00:50:39,120 --> 00:50:43,840
other values because they're partitioned by, say, NFD. So I'm not going to go into any of this,

577
00:50:43,840 --> 00:50:47,680
but basically what David was able to do was take many of the optimizations here that were

578
00:50:47,680 --> 00:50:53,280
handwritten in Scala and automate them and formalize their correctness. And without getting

579
00:50:53,280 --> 00:50:58,160
into too much detail, although it is kind of fun, oh, and we borrowed some work from Paris. So

580
00:50:58,160 --> 00:51:08,960
shout out to Paris for parallel disjoint correctness and colleagues. It is really fast. So he was

581
00:51:09,200 --> 00:51:14,080
able automatically. This is Michael's results that we re-ran. This is Scala. This is throughput

582
00:51:14,080 --> 00:51:18,080
against latency. So what you want to do is you get as much throughput as you can until it starts

583
00:51:18,080 --> 00:51:22,560
to hurt you at latency and it curls back. So this is kind of where things start to

584
00:51:25,040 --> 00:51:30,160
top out, if you will. So that's Whitaker's implementation. This is the same logic as

585
00:51:30,160 --> 00:51:34,720
Whitaker's implementation in Hydro. So this is just Hydro is faster than Scala written by hand.

586
00:51:34,800 --> 00:51:40,080
So this is just a testament to the folks who wrote the Hydro flow engine. But the blue line is

587
00:51:40,080 --> 00:51:46,000
what David achieved through systematic correctly proven rewrites. So he was able to get performance

588
00:51:46,000 --> 00:51:51,280
that actually, because Hydro is fast, is better than Whitaker's Paxos implementation. And this gap

589
00:51:51,280 --> 00:51:54,880
is kind of what he's given up. These are the tricks that Michael had that we didn't cover in our

590
00:51:54,880 --> 00:52:02,960
rewrites. But we're doing 90% with the easy tricks. So it gives me confidence that simple

591
00:52:03,040 --> 00:52:07,920
query optimizations known in parallel databases can have impacts on places that we're really very

592
00:52:07,920 --> 00:52:12,480
fine tuned performance issues that people write PhDs to get this kind of performance and we're

593
00:52:12,480 --> 00:52:18,240
getting it through systematic rewrites. Very promising. David's only halfway there though,

594
00:52:18,240 --> 00:52:22,080
because he has to have a proper cost-based optimizer. Right now what he has is correct

595
00:52:22,080 --> 00:52:27,040
transformation rules. He needs a cost model with an objective function. And then he needs a search

596
00:52:27,040 --> 00:52:33,680
strategy. And we're hopefully going to be using egg log or some implementation of egg log in Hydro

597
00:52:33,680 --> 00:52:39,040
flow to achieve this. So we're one of the things with Maxis stuff that overlaps is if there's a

598
00:52:39,040 --> 00:52:46,080
lovely semi lattice based data flow implementation of Maxis stuff, maybe we can clean up some of the

599
00:52:46,080 --> 00:52:54,080
things where he's doing updates in place. This work? This work. Well, so this was written in

600
00:52:54,080 --> 00:52:59,680
Datalog and translated down into that Hydro flow data flow language you saw at the top. This stuff

601
00:52:59,680 --> 00:53:06,240
is also written in Datalog currently in a runtime that plays some ad-hoc tricks. That's not traditional

602
00:53:06,240 --> 00:53:13,040
Datalog execution here as Maxis. But I think, you know, Union Find is a nice target for an

603
00:53:13,040 --> 00:53:18,960
algebraic treatment and I think we have opportunity. Okay, what I'd like to do in the last few minutes

604
00:53:18,960 --> 00:53:23,440
is berate you with questions because these are the things that I don't know how to answer yet and I

605
00:53:23,440 --> 00:53:27,840
would love to get help with. So the first is, and this is an outline, so this section goes on for

606
00:53:27,840 --> 00:53:31,760
many slides, but there's the four questions. Can we please have one theory for all this nonsense

607
00:53:31,760 --> 00:53:37,280
instead of the list I'm about to show you? What would be a good type system for the physical

608
00:53:37,280 --> 00:53:43,040
layer where we could prove correctness of transformations? I have a follow on to Sudipa

609
00:53:43,040 --> 00:53:49,440
about the role of our kinds of work in the era of generative AI. And then I have this ongoing

610
00:53:49,440 --> 00:53:55,280
question of what time is for, which I probably don't have time to explain. But quickly, you know,

611
00:53:55,280 --> 00:54:00,400
the unifying theory thing. So CRD teaser semi-ladyses, Datalog, Daedalus was all done with model

612
00:54:00,400 --> 00:54:06,640
theory and it's fancy actually. It uses like stable models and stuff. It's actually ended up

613
00:54:06,640 --> 00:54:10,960
being kind of fancy. The column theorem, Amalut, proves have these relational transducers, which

614
00:54:10,960 --> 00:54:16,240
are this halfway world between operational and declarative semantics. You have these state

615
00:54:16,240 --> 00:54:22,000
machines on each node. They run declarative languages on each step, but then they output stuff and

616
00:54:26,720 --> 00:54:28,960
I think you can write non-terminating programs if you want to.

617
00:54:30,560 --> 00:54:36,880
So you can write Toggle, for example, and Daedalus and the transducers.

618
00:54:39,440 --> 00:54:43,920
Now they don't have to be terminated. In any sense, I don't think. But the point is,

619
00:54:43,920 --> 00:54:47,600
I really wish he'd have done this work with this, because he also was on this work, but he didn't.

620
00:54:47,600 --> 00:54:52,240
He did it with transducers, which is a bummer. If you talk to distributed systems people, they

621
00:54:52,240 --> 00:54:55,920
talk about essentially order theory. They talk about partial orders all the time, which is related

622
00:54:55,920 --> 00:55:01,600
to lattices, but you know, it's annoying. Programmers want to write these sort of algebraic functional

623
00:55:01,600 --> 00:55:06,880
expressions, which I think is a good thing for all of us. And then yeah, I give all these talks

624
00:55:06,880 --> 00:55:11,680
and then some joker raises their hand and says, well, what about transactions? And in fact, Peter

625
00:55:11,680 --> 00:55:15,680
Bayless, when he was a student, basically did an end run around my entire group and just wrote

626
00:55:15,680 --> 00:55:20,880
papers about transactions and coordination, and they don't align with the rest of this stuff.

627
00:55:20,880 --> 00:55:25,600
So it's an open challenge to reintegrate that work. And then, you know, I didn't actually say the S

628
00:55:25,600 --> 00:55:31,760
word yet, because I apparently didn't do joins as of yet. But we do do joins, so we probably need.

629
00:55:32,640 --> 00:55:38,080
So it would be really great to get all of it here. I would like to bring all of this work

630
00:55:38,080 --> 00:55:44,640
into this domain. That would be really nice. Okay. Here's a flavor of what I'm dealing with,

631
00:55:44,640 --> 00:55:51,200
though. So just finished reading the DBSP paper, which was very nice and related to our work, but

632
00:55:51,200 --> 00:55:56,640
we have some other things we need to keep track of that are relating to the network messing with

633
00:55:56,640 --> 00:56:02,480
stuff. So when we look at a stream, it's got some values. It's got some happenstance ordering,

634
00:56:02,480 --> 00:56:07,600
that's a mapping of the naturals to those values. It's got some happenstance batching. It came in

635
00:56:07,600 --> 00:56:12,560
over the network in chunks. So there's like singly nested parentheses in this stream that are

636
00:56:12,560 --> 00:56:18,160
randomly placed. Randomly, you know, arbitrarily ordered, arbitrarily placed. Maybe this is a

637
00:56:18,160 --> 00:56:22,960
stream of lattice points, but maybe it's not. I don't know. But if it is, you could say things

638
00:56:22,960 --> 00:56:27,280
like there's a monotonicity relationship between the type's natural order and the total order of

639
00:56:27,280 --> 00:56:34,560
arrival or not, right? And then what sort does is it enforces something like this, right? It's

640
00:56:34,560 --> 00:56:39,120
nice when these are atomistic, like data flow is basically a set lattice that you flow individual

641
00:56:39,120 --> 00:56:43,680
atoms. That's the traditional kind of database iterator thing, one tuple at a time, right?

642
00:56:43,680 --> 00:56:48,720
One tuple at a time is an atomistic lattice of the sets of tuples. And it's nice when you know

643
00:56:48,720 --> 00:56:52,880
you're dealing with atoms, because you can say things like item potents, right? I gave you

644
00:56:52,880 --> 00:56:56,640
Bob's tuple once, I gave you Bob's tuple twice, sorry, but you should only have it once. So delete

645
00:56:56,640 --> 00:57:01,360
one. But if I give you subsets, now you have to find how they overlap and you have to make sure

646
00:57:01,360 --> 00:57:05,600
that when you union them, you remove the overlapping bits. And so when you have non-atomistic things,

647
00:57:05,600 --> 00:57:11,680
it's just a little uglier. And you end up talking about like, does their meat, is there meat bot?

648
00:57:11,680 --> 00:57:15,040
kinds of things. Do they have no intersection, right? So these are the kinds of properties

649
00:57:15,040 --> 00:57:20,160
that I think I need to track in my rewrite rules. And then, you know, the operators are the invariants

650
00:57:20,160 --> 00:57:25,040
to these properties, like lattice, lattice operations are invariants of order and parenthesization.

651
00:57:25,600 --> 00:57:29,200
Do they preserve the properties? Do they enforce different values for the properties?

652
00:57:29,200 --> 00:57:35,280
The network non-deterministically enforces orders and parenthesizations. So like this is the stuff

653
00:57:35,280 --> 00:57:39,600
that I worry about in my operators. And this is kind of the soup I'm swimming in with this

654
00:57:40,400 --> 00:57:45,200
physical algebra. So I would like help with this. All right, I'm going to do one more quick slide.

655
00:57:45,920 --> 00:57:49,360
This is very much in the realm of what Sudipa was talking about. You know, we're in the era where

656
00:57:49,360 --> 00:57:54,880
people will be programming with green goo, right? It's just this is large language models, they're

657
00:57:54,880 --> 00:57:59,600
magical, they produce stuff. But what we really want is building blocks that we can count on,

658
00:57:59,600 --> 00:58:03,360
right? We're a database people, our industry is all about foundational building blocks.

659
00:58:03,360 --> 00:58:07,760
And I really do think declarative specification is a lovely narrow waste here between these two,

660
00:58:07,760 --> 00:58:13,520
where we can take a formal spec as Cod asked us to, we can render it in some sense so that it's

661
00:58:13,520 --> 00:58:18,480
readable, right? And this relates to things like Wolfgang's work on visualizing queries,

662
00:58:18,480 --> 00:58:22,320
and what Sudipa was talking about in terms of giving natural language things, but helping

663
00:58:22,320 --> 00:58:26,320
people look at this and say, is this what you meant? Not is it correct, but is this what you

664
00:58:26,320 --> 00:58:31,760
meant since a spec after all, right? Did you mean this query? And then of course, if it's in a nice

665
00:58:31,760 --> 00:58:37,920
formal language, we can check it for other things, right? And so that would be, I think, a role that

666
00:58:37,920 --> 00:58:43,040
we can really play in the world. And I suspect things like this will happen. These programs are

667
00:58:43,040 --> 00:58:46,720
going to be a selection of programs. You're constantly going to be given a menu of which of

668
00:58:46,720 --> 00:58:51,680
these things did you mean. And the answer to which is either some invariant checks or something,

669
00:58:51,680 --> 00:58:58,160
or some human judgment. So I think we're in a good spot in terms of intermediate languages.

670
00:58:58,160 --> 00:59:03,040
And I'll just close with one more slide, maybe just a handful of slides.

671
00:59:04,080 --> 00:59:07,600
What are clocks in time for in distributed systems? So there's this famous saying, which

672
00:59:07,600 --> 00:59:12,080
is correctly attributed to a sci-fi short story. Time is what keeps everything from happening

673
00:59:12,080 --> 00:59:17,360
all at once. So when should we use time and computing? What are clocks for? Well,

674
00:59:18,320 --> 00:59:23,520
they're not for monotone queries. I can run this embarrassingly parallel. It can all happen at the

675
00:59:23,520 --> 00:59:31,200
same time, and it's fine. And Buntalu was doing this long before this discussion. But I can't run

676
00:59:31,200 --> 00:59:36,240
this at the same time. You can't have P and not P at the same time. So what's the deadliest answer

677
00:59:36,240 --> 00:59:43,280
to that is, well, that's what time is for. This is the toggle program, right? And time is there to

678
00:59:43,280 --> 00:59:48,160
separate two things that can't coexist. That should be, I think, the only reason for time.

679
00:59:49,760 --> 00:59:55,120
Except it's not. Distributed systems, people use time for maintaining partial orders and knowing

680
00:59:55,120 --> 01:00:00,320
when things were concurrent. Sometimes you don't need that. Sometimes you do. But this is my question,

681
01:00:00,320 --> 01:00:04,960
especially because Val's in the room and worked on this DSP stuff. Daedalus has one clock per

682
01:00:04,960 --> 01:00:10,960
node and we update it only when there's a network event or we have to cycle through negation.

683
01:00:11,920 --> 01:00:16,560
Timely and differential data flow have clocks all over the damn place. And I'm not sure when you

684
01:00:16,560 --> 01:00:22,960
use them and when you don't. So, for example, tracking iterations of a monotonic recursive

685
01:00:22,960 --> 01:00:26,480
program. Why do I need a clock for that? I don't think I need a clock for that. Maybe if it's a

686
01:00:26,480 --> 01:00:32,160
while and we use the index in our computation, I need to know what index I'm at. So the general

687
01:00:32,160 --> 01:00:38,720
question is, when do we use this structure called a clock? And when don't we need? And can a compiler

688
01:00:38,720 --> 01:00:44,960
decide? All right. We are a little over time. I hope we have given you lots of things to ask us

689
01:00:44,960 --> 01:00:51,680
later. We need lots of help. So we'd love it. And we are big fans of working with folks like you.

690
01:00:51,680 --> 01:00:57,680
Can you leave these last four slides there so we can come up with some more folk questions?

