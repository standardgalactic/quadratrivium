{"text": " Second talk today has a title that has evolved since last time I looked at it. Okay, but it has logic and algebras and we are very excited to hear what this has to do with cloud computing, which we are all subjected to in our daily lives. Thank you. So this will be a bit unusual, both Joe and Connor will talk and I let them take care of the logistics of that. Again, remember that for questions that might be more appropriate for a longer discussion, we will have that discussion right after their talk. Okay, thanks. So this is work that obviously we're doing here at Berkeley and I'm also funded by Sutter Hill Ventures. So it's kind of cool. We've got venture capitalists funding basic research. I have promised them that there's no applicable. I'm not really sure what this could turn into as a company, but they're cool and they've given us developers to work on the project, which has just been great and they're funding me as well. So thanks to them and to Berkeley. There's this story people like to tell in computing. This is my standard opening slides. Operating systems people really like this story because it's sort of the Thompson and Richie Turing Award story. For every platform that comes out, there's a programming environment that's somehow suited to that platform that emerges and as a result, people write things you never would have expected on that platform and it succeeds. So the PDP 11 with Unix and C is the canonical example of this, but one can argue that in every generation of new kind of computing platforms, programming environments have arisen to allow people to build an app for that. And so nobody expected all the apps we have on our phones. It's wonderful. Developers were freed to write all sorts of things. Strangely, there's a platform that's as old as the iPhone called the cloud. So AWS is approximately the same age as the iPhone, but it doesn't have a canonical programming model. And there's many reasons why that might be, partly because it's a really hard programming environment, yes. So it has to deal with all the problems of parallel computing, as well as things like distributed consistency, what happens when you have partial failures in your system, but it keeps running. So componentry is down, but the system's still running. And then in the modern cloud, we want things to auto scale. So you allocate more machines and then you free up some machines, but the program's still running. So the platform is changing underneath you as you're executing. So this is all hard and programmers right now are trying to do this essentially in Java. That's sort of the state of the art. And the annoying thing is these compilers for these languages don't answer any of these questions that are hard. So I think, honestly, this is like this hole in computer science that nobody's filled and it seems like one of our grand challenges from my perspective. So I've been working on it for a long time, and I think there's still a lot of work to do. I take inspiration, of course, from this gentleman as maybe we all do. What was cool about Ted Codd was he said, look, you should write things in a formal language. You should have formal specifications. And then there should be machinery that automates the implementation. And if we do that, then the implementation can change while the specification remains the same. This is very nice for things like databases, right? So the thing is that Codd was trapped in this database prison for all these years. And I think there's a much broader applicability of the design principle. So we worked on things in our community like declarative networking. So we brought Codd out of the database and into the network. So I've done some work on that. Many of us I think in this room have done something around declarative data science and machine learning. This is a growing area, right? In the program analysis community, the use of declarative languages has been pretty powerful. So that's really cool. And then, of course, the hot thing, which is why we're all here, is that we're going to start to try to look at this stuff through the lenses of algebras instead of logic or in addition to logic, which is pretty neat. And we're going to, you know, we've heard or are hearing about a variety of different algebras that people are playing with in this domain. So what I'm interested in is taking Codd into the cloud, yeah? And here's sort of the analogy, the way to think about it. The relational database was invented to hide how data is laid out and how queries are executed, right? And all that should be decided sort of lazily based on the current environment. Well, the cloud is just a generalization. It was invented to hide all your computing resources and how they're laid out. So not just your blocks on your desk, but really everything. And it's for general purpose computations. So the cloud, in a lot of ways, is this abstraction. It's this physical layer abstraction. The physics of the deployment of your code is going to change, but you want your spec to remain the same. That's how you'd really like to program in an environment that is this heterogeneous and elastic. So I believe that it's extremely natural for techniques that we've been working on in our community to try to be applied to cloud computing. And we have a project in my group called Hydro, which you can read more about, which I will tell you a bit about today. Okay. So what are my goals? Well, I kind of want to build something like LLVM for the cloud. So LLVM, as you may know, is a very successful sort of language stack. It supports many languages, including C++ and Rust and Swift and others. It has an internal language called its internal representation, and then it compiles down to a variety of machine code for different platforms. So it's been extremely successful, but it doesn't answer any distributed questions. So if you're writing a distributed program, you might ask a question like, is my program consistent in some sense? Or if I talk to different machines in the network, will they give me different answers and be confused? That's a question that distributed programmers need to deal with. Here's another one. My state no longer fits on one computer. How do I partition it across multiple computers while getting the same answer out of my program? I want you, you compiler should figure that out for me. What failures can my system tolerate and how many of them before it stops working the way that the spec declares it should? What data is going where around the world and who can see it? These are all questions distributed systems always have to answer. And then I have different objective functions. So I'd like to optimize some days for maybe my dollar spend in the cloud, but I don't care about latency or maybe vice versa. Maybe I care about particular latency distribution. So I want the 99th percentile of my workload to achieve a certain latency versus the 95th or what have you. These will all lead to different decisions about resource allocation and program structure and so on, right? And if you ask these questions of LLVM, that's the answer you get. It doesn't deal with any of these issues. And that's kind of where we'd like to come in. We've written a vision paper a couple years ago that I can point you to and I won't go through all of it today. But the idea is to use database techniques and ideas to optimize in concert with LLVM. So LLVM is responsible for the single node, but database techniques perhaps responsible for the messaging, the data movement that happens in a distributed program. So here's how we envision the hydro stack. Many programming languages up top. Some techniques to translate them into an internal representation. We've got a little bit of initial work here. And then the internal representation should be some formal spec that is global in some sense. So it doesn't worry yet about how many machines I have or what the machines can do. It's just a formalized specification of what you wrote in a perhaps imperative language. Okay, so it's kind of machine oblivious. Maybe it's a logic. Maybe it's an algebra. Things that are in red are work in progress. Things that are in green kind of work at this point. And then from there we want to build a compiler and we're working with Max on using eGraphs for that compiler to translate down into a per node physical algebra. So every machine would run its own little program. Those programs communicate with each other very much like a parallel query plan is a bunch of individual query plans running on individual machines talking to each other over a network. Okay, so this is a sort of per node physical algebra because it's actually doing stuff. We've implemented this in Rust. It's very fast and I'll show you some of this today. So that's kind of what we envision as how this is all going to work. At the bottom there's something that's deploying this on machines and deciding how many machines and how few over time. Okay, so some of the topics I want to talk about today we're going to focus on this piece of the stack. Things in red are work in progress. Very much more questions than answers for sure. So how do we take code and automatically replicate it along with its associated state or data while ensuring that the program continues to produce the same outcomes as it would have on a single machine? So I'm particularly interested in doing this in cases where the replication comes for free and the individual machines don't have to coordinate with each other in a technical sense I'll talk about. But we'd like to avoid replication when we can. We'll call that free replication and this is the domain of the calm theorem which you may have heard of and I will review. Unfortunately, the calm theorem was done in a very particular framework for the proofs. It's not at all clear how it applies outside that framework and so what we'd really like is a more algebraic notion of the calm theorem which is something that Connor's working on and after the talk if you're interested come find Connor and or me to talk about that. Another topic that Connor's going to talk about today is termination detection and again ideally termination detection where I can decide it locally for free without asking anyone else. So how do I know in a streaming program that it's done when there's other agents in the world? So we're going to talk about how to do that with threshold morphisms but Connor's got ideas about more general notions of equivalence classes that may allow termination in more settings. So he'll give you a flavor of this work in progress. The third piece we may or may not have time for today but my student David Chu has been working on this how do you take a program and partition the state of the program across machines if the state doesn't fit on one machine. So you really need to partition the code and the data. This is very much like traditional shared nothing parallel databases if you like but we want to do this to full and rather complex data log programs and so we've got an implementation of Paxos which is number of lines of data log where David's able to do some of this automatic partitioning. Functional dependencies have a role to play here and it would be nice to integrate those into an algebraic frame as well. And of course all this has to go fast and ideally as fast as handwritten C++. I'm really previous iterations of my work we settled for interpreters and just showing things were possible. Now we'd like to convince the systems people that things will be as fast as they want them to be. So is this business just pie in the sky? Let's see. All right so we're building on an obsession of some 10 to 12 years that I've had now in my third group of grad students working in this area. So the initial batch of grad students was doing formalisms. So we have this very nice logic called Daedalus which is a subset of data log neg actually that allows you to talk about time and space in a particular way and it's got a nice model theory. And so that works all there and we can use that as a basis for you know our semantics to start which is nice. And then there was this lovely work that Tom Amalut did at Hasselt the column theorem which was a conjecture I had and he and colleagues went ahead and proved that talks about how things are monotone in some certain sense. Then you can do this free replication. So you don't need to do coordination to get replica consistency. So I will talk about the column theorem today to give a review. And then we actually built out a language. It was slow. It was interpreted. It was written in Ruby but it integrated lattices into a data log like language and we were able to show that you can get stratified negation and aggregation. You can get morphisms on your lattices to allow you to do semi-naive evaluation even with the lattices. So it's really actually rather a nice mixture of algebra and logic. None of this was formally proved but it was I think one of the earlier systems to observe that you could build this. That was pretty cool. And that's Neil Conway's thesis work. So we have this as a basis. This kind of ground zero for my team. And then what happened is I had a batch of students that didn't want to do languages or theory. So they just built stuff in the spirit of these ideas. And I'm not going to go through all this but it's things like functions as a service and protocols and testing and provenance. It's cool stuff. What I do want to focus on is one of those projects which was a key value store. Key value store is just like a hash table. It's a database where you look things up by key and you get a value. So you can think of it as a distributed hash table. These are like the Petri dishes of distributed systems. You're basically saying I have a memory. It's distributed across many computers. It may be replicated. It may be partitioned. But that's what I have. It's just registers with values, keys with values. So there's no algorithms per se. It's all just kind of like focusing on these semantic issues about replication and partitioning. But the idea that was behind the Anna key value store that we built was everything's a semi-ladys. And because everything's a semi-ladys and therefore associative, commutative, and item potent, messages in the network can be replicated. They can be interleaved. They can be reordered and everything will be fine. So if you design with semi-ladys from the bottom up, you can build a system that does no coordination. So it's fully monotonic, which means everything can be replicated as much or as little as you like. Across the globe, between disks and memory, you can replicate lots of ways. You can do updates anywhere. So multiple updates to the same key can be happening concurrently in different places at the same time. And they are merged by the lattice lazily via gossip. And so you build this thing with no concurrency control whatsoever. So there's no locks in the system. There's no calls to atomic instructions on processors. There's certainly no Paxos protocols or anything like that. It's just put, get, and gossip. It's really a simple piece of software. And so Chenggang Wu, the student who led this, won the dissertation award. I think it was very well deserved because this system was really elegant and really fast. So to give you a sense of kind of the lattices that he uses, he took from the literature a variety of these kind of consistency models that people talk about in distributed systems. And he showed how you can get them by building little composite lattices that wrap up the data. So this is what's called last-rater wins in the distributed systems, which is just whenever you see a value that's from later than your value, you update, otherwise you don't. And this you just, you know, take your map, which is from keys to things, and you wrap the things in a lexical pair of a clock or a version and the data, right? And you can only have one value per version. So this, this works out as a lattice. Here's a fancier one, though. This is actually one of the strongest forms of consistency you can get without coordination. It's called causal consistency. And here what you have is for every key, you have a vector clock and the data. And the vector clock itself is a map lattice with node IDs and counters. Yes. Just want to say a little bit sort of operational transform. A little bit. Yes. Can you ever get stuck? Like, do repairs always exist or do you set it up such that they do? So these, these particular well trodden forms of consistency work fine. And these lattices are capturing that. They're saying, look, it's just merge. All right. And it's always going to work because you've defined an associative commutative item potent merge function. OTs are like really weird and full of all sorts of reasoning I don't understand. And they would never be able to have such a simple assertion of correctness. All I'm saying here is it's associative commutative of an item potent. I got nothing more to say. It takes a little bit of convincing to say that gives you causal consistency, but it's not much convincing because this helps you make causal consistency with vector clocks. So the observation that clocks and vector clocks are lattices is just a nice thing about distributed programming. Yeah. So what do you mean by everything is a lattice? So you mentioned that Kira's story is a hash map. Right. Every key has to be a lattice thing. So the nice thing about the map lattice is the keys are not lattice values. The keys are just keys. The whole table is a lattice. But the object is a lattice because what happens is the merge function is for a particular key. If there's nothing, it gets the value you gave. Or for that key, if there's something there, you apply the merge function of this lattice. So that is itself a lattice. And these lattice constructors are very nice. We use map lattice. You see lexical pair over there. And these allow you to take simple lattices like sets and counters and build up richer lattices out of them, which is a trick our group likes to play a lot. Other groups sort of are doing research on inventing custom lattices for custom problems. We've been very much in this kind of know let's just build it up from very simple building blocks. So the quick version is it's monotone. And if it's monotone, the column theorem says it's going to be free replication. So you don't have to do coordination to get replicated consistency. Connor's going to give you a longer talk about this when we get to conversation about semi-lattice. It's a semi-lattice. I should be clear. It's a semi-lattice. Do you know whether the people working on coordination free replicated data structures are well-pure? Yes, they are aware. And Connor will talk about it soon. Yeah, that will come up for sure. Good. So just to kind of close out this anecdote with Anna, the system is ridiculously fast. And it's especially ridiculously fast under contention relative to other systems. So we compared against things like mass tree, which is from Harvard. It's 80 colors, very fast key value store. We also compared against the multi-threaded hash table that comes from Intel in their thread building blocks library. That's TBB. And under contention, those systems, if you look down here, spend most of their time trying and failing to get atomic instructions. So they'll say test and set on a particular memory address, and they'll be told no, you have to try again. And they'll spend 95% of their time under contention doing that, not doing useful work. So they're at 5% good put, if you're familiar with that term. Whereas Anna, because it does no concurrency control, is just doing puts and gets and puts and gets and puts and gets and spending most of its time doing good put. And that's why Anna can be 700x better throughput under high contention than these other systems. But also because it does no coordination scales almost perfectly linearly across threads, and then across machines, and eventually across the globe. There's really nothing to keep it from scaling linearly, because the only extra work it has to do is some gossip. And that can be done in the background, and can be done as lazily as you like without breaking the semantics. So there's maybe a little fudging here on how stale your data is, but it's correct. So this was a crazy fast system. And the thing about this, oh, and if you try to run it in the cloud, it's also incredibly cheap to run relative to systems that are wasting all their time doing this business. They're charging you for this. They're trying to get locks, they're waiting on locks, and they're charging you money. So you'd like to avoid that if you can. Okay, that's all very nice. But it was written in C++ by Chenggang, who's an excellent coder. His implementation is correct by assertion. It would be really nice to be able to kind of do what CAD wants us to do, formalize a spec that is correct, and then synthesize an implementation from it through rules that are correct transformations. So we'd really like to do that, and we'd like to maintain the speed. What kind of formalisms? Well, we're using lattices mostly. So maybe we could have a type system that starts with some basic semilattices, like sets and counters, some composition lattices, like key value pairs, products, lexical products, which aren't always lattices. So you have to, there's some constraints on whether a lexical product is a lattice. And then we want like a data flow, like a query plan algebra. So you can imagine a semi-ring kind of algebra, but you know, there's going to be maps and folds, and then there's going to be physical stuff, like scan a collection, or get stuff over a network. You know, networks do weird things, like they permit things, and they form batches of things. They parenthesize streams, if you will. They multiplex and de-multiplex messages. So there's some physical stuff that we want here too, and I'd like to really be able to prove all that stuff is correct in a meaningful way. So just for fun, I don't expect you to read this. This is the ANA implementation. You just saw written in our low-level hydroflow language. This is the whole thing. It's a very simple program. And you can see this is kind of a data flow language. It's basically specifying graphs of data flow. The edges are directed edges in a graph. The words are nodes in the graph, and you'll see familiar operators like map and join, and cross-join, and so on. All right, and you can give views names. So this is a little name of a subgraph, and we use it, and so on. So it's just a little language for specifying graphs. This is a picture of that program that's output by the system. Okay, so it's a one piece of paper program. So in this particular program, their union is actually joined, semi-latest join, and that might be the only one. Yeah. Okay, and so just to convince ourselves this is fast, that's Chengang's original numbers. We have it now running through hydro, that implementation you saw on very similar machines, and we get very similar performance to the handwritten code. So we're feeling pretty good that we're hitting our goals for performance. And because this graph is green, it's telling us that this thing is all monotone, and therefore consistently replicable. And at a glance, we can see this is a safe program. And I'm sort of cheating at this point, and I'm going to confess to that. There's, I think, more work we need to do to make this robust. I think these green edges are kind of a, they're slightly bi-assertion at this point. So I would like to make them more fundamentally correct, and hopefully we'll have time to talk about that later. Okay. With that, I'm going to hand off to Connor. He's going to take us through the next chapter. Hello. People hear me? Yeah. I'm Connor. I'm a PhD student here working on hydro. I like systems and theories. So I thought I'd show you guys some of the theory stuff we've been thinking about, see if anyone has any thoughts, wants to collaborate on anything. Okay. So in the classical database lens, we have, you know, these three layers, the relational calculus at the top, relational algebra in the middle, and then a physical algebra at the bottom, concerned with things like hashing and sorting and so on. And we can think about how this changes when we move to the cloud setting. And there's good news and bad news on the current state of affairs when we move to the cloud setting. The good news is that, like Joe said at the top, we have this Daedalus language from the Bloom project that is a data log like dialect for distributed systems. The bad news is that developers are not asking for a data log dialect to build distributed systems in. The developers we've talked to are a lot more interested in a functional algebraic looking interface and especially something Pythonic looking like pandas. On the algebra side, the good news is that there is an algebraic model for distributed systems today. It's the semi-ladis model that Joe has mentioned that is referred to as CRDTs in a lot of places, especially in the programming languages community. The bad news is that this is a model for coordination-free updates of state and it doesn't actually have a query language or give guarantees about coordination-free-ness of queries today. And then at the physical layer, when we add a network to the situation, asynchronous computer network, a lot of non-determinism emerges that we need to be able to handle, in particular reordering, batching, and duplication of messages. So what we'd like to get to is unifying formalism across logic and algebra and this physical algebra and have correctness at the physical layer that we can prove for safe against this non-determinism from the network. And we're able to capture things like replication, partitioning, batching, incrementalization, and termination analysis. We'll talk about more later. All right, so let's talk about semi-ladis' CRDTs. So this is a model for distributed systems that in databases we usually call the semi-ladis model. It's what is called an Anna in Blum L. It came out of the programming languages community and there it's usually referred to as CRDTs. It stands for conflict-free replicated data types. It's introduced in this paper here and there's over 179 papers about CRDTs out there. It's also started to get popular amongst software engineers and you see people talk about this CRDT model on places like Hacker News, people starting startups with it. So what does it do? It tries to handle the sources of non-determinism that come from an asynchronous computer network. These are the arbitrary batching of messages, arbitrary reordering of messages, and arbitrary duplication of messages. And so it turns out if you want to be robust to these three things, these actually correspond to algebraic properties that you need to give you that robustness. So associativity gives you robustness to batching. You're indifferent to the parenthesization of messages. Communicativity gives you robustness to reordering and idempotence gives you robustness to duplication. And if you have a set with an operator that satisfies these three properties, that gives you a semi-ladis. So that's why we're talking about semi-ladises for distributed systems. So the conflict-free replicated data type is one specific model of a semi-ladis interface, but since it's the most popular one today, I'm going to talk about it. So the idea is that it's an object-oriented view of a distributed system where you define some object, and you're going to define three methods on that object. And then you can replicate this object across the distributed system, and the replicas will converge, regardless of network non-determinism. So you have your merge operator, which is your associative, commutative, and idempotent ACI semi-ladis operator, which combines the state of two replicas. You have a way to update state, which the requirement is just that that's monotone with respect to the partial order induced by this merge operation. And then you have a query, which is just a method on this object, but today there's not a specific query language. You don't have any sort of guarantees on what that query does. It just reads this semi-ladis state. So we're looking at an example of a CRDT. This comes from the Amazon Dynamo paper for how they're implementing shopping carts. And the idea is that you have updates that are going to add or remove elements to your shopping cart. In this case, you add a Ferrari to your shopping cart, add a potato, and you can also remove the Ferrari. And the state is going to be represented as two sets, a set of items that you've inserted and a set of items that you've removed. And then to merge, we do a pairwise union of these two sets. And the query, what's actually in my shopping cart I want to check out, is set difference. Subtract the removes from the inserts. And there's a few interesting things going on with this example. One is we're guaranteeing the coordination-free rights that these two states are going to converge. But our query is a non-monotone query, which the column theorem tells us is not a coordination-free query. So CRDTs are not giving us the invariant that the column theorem requires on queries, which is that if we output a tuple at a certain point in time, we're never going to retract that tuple in the future. Here, over time, we will retract tuples as the remove set grows. We had a vision paper in BLDB this year about this gap between what CRDTs guarantee and what the column theorem guarantees and ideas for how to resolve it. Another thing you might have noticed that's kind of odd about that representation of data is that if you think about how we might represent updates like this to a shopping cart in a database, you might have imagined that we would have a count on each item and we would increment that count when we add an item and we decrement that count when we remove an item. This is what you'd see, something like incremental view maintenance where your update operation forms an abelian group, not a semi-ladis. So why not do something like that? Well, for one, it doesn't form a semi-ladis and it's not immediately obvious how to convert it into one. So this representation is two sets, what you call a two-phase set. It's more obviously monotonously growing update operation, but it turns out it actually is possible to convert this abelian group representation into a valid semi-ladis in terms of being robust to network non-determinism. I won't go into all the details on that, but it's based on what Joe is saying with these vector clocks where you wrap the states basically in a vector clock which forms a semi-ladis. The downside of doing this is that vector clocks require linear memory and the number of replicas in the system, so that's the reason why people wouldn't use this representation today. But we have some work on a protocol for enforcing this kind of conversion into a semi-ladis in constant rather than linear space. So if anyone's interested in that idea, definitely come find me and talk about it. Okay, so like I said, the semi-ladis model today does not have a query language on top of it. So what might we want out of a query language for the semi-ladis model? Well, we want expressivity. Like we saw in the shopping cart example, we need set difference, so we need negation. Also recursion, something like datalog. We also want obviously classical query optimization options. We want identities that we can use to transform our query, get better performance. And we want to be able to do monotonicity analysis as well as functional and dependency analysis for partitioning. And so something like datalog for semi-ladises might be a good fit here, but there's a lot to explore. And so now Joe is going to talk about this monotonicity analysis and functional dependency analysis. I should say from the previous slide that some of this is things we took a crack at with BlueMal, so it's not that we've done nothing here. There's some answers to these questions, but there's also work to be done. All right, so I wanted to step back and review for you folks, the COM Theorem, which I know is sort of in a sub-corner of the pods community and not everyone's going to be familiar with it, but I think it's useful to go over. This will be high level, but hopefully helpful enough for you to get into the game. So the challenge is that we're going to have computers spread across the globe and we want our replicas to be consistent. So we have this nice couple here, they're in different places, and the classic example of replica consistency is data replication. So forget about programs, we're just going to have data, kind of like the CRDT model. And I want everybody to have common beliefs about the data, at least eventually. So these two folks currently both believe that X is love, which is lovely, but if it's a beautiful variable, things could change, right? And that's very sad. And once they disagree on the value, they might make decisions based on their disagreement that will lead to further divergence. This is sometimes called the split brain problem, because you can't put it back together later on, it's too messy. And so we want to generalize the idea of consistency of data replication to consistency of program outcomes. So I'm not just interested in the data, I'm interested in the queries, if you will, right? Much more powerful, and it will allow us to cheat sometimes, the data could be inconsistent if the query outcomes are not, right? So it'll give us more ability to relax our coordination. So we'd like to generalize this to program outcomes independent of data races when we can. The classical solution to this stuff is coordination. This is what things like Paxos and Two-Phase commit were invented to solve. And the way they solve it is by saying, what if we were just on one computer with one processor? Maybe we could implement that in a distributed fashion, which is a very heavy handed solution, right? You say that our solution to parallelism is to remove it. And how can we remove it in the distributed context? Well, it's expensive. But here's how it goes, right? On a single node, you use atomic instructions, right? So if you have shared memory, you can use atomic instructions, or maybe you use a locking system. In the distributed environment, you use something like Paxos or Two-Phase commit. And at every scale, as you saw in that ANA work, you'd like to not do these things. So even on a single machine, you really don't want to be doing coordination. And certainly in the distributed setting, this is very heavy weight. And there's people who will tell you at great length why they don't let the developers in Amazon call these libraries unless they have, you know, 16 gold stars. Because it will slow down the whole environment and create queue backups and all kinds of horrible things. So when can we avoid coordination? This was a question that I asked as a lazy professor, because I was thinking maybe I should learn and teach Paxos, and I kind of didn't want to. So I was like, maybe, you know, maybe we don't need this stuff. Maybe Lamport's just a bunch of bunk. So that's kind of where this started, sheer laziness, intellectual laziness, which I will cop to. But what it led to, sometimes when you ask a question about how can I be lazy, you end up asking a question that turns out to be quite interesting. I think that's what arose here. And I'm seeing this not only in my work, but in other places. Back in the 20th century, if you will, the Lamport Gray era, we were trying to emulate sequential computation. We were doing everything we could to give the programmer the illusion of a sequential computer. And it was all about, you know, very low level stuff, reads and writes, accesses and stores, right? And then guarantees of order, total order, linearizability and serializability. And this was all based on the idea that programmers are hopeless. They'll write all kinds of crazy code. And the only thing they understand is sequential computers. So we'll make the worst case assumption that their stuff wouldn't work in parallel, right? And we'll give them mechanisms for avoiding parallelism. Seems like a terrible thing to do in a parallel environment. Yeah. So what's happening in the 21st century is if we lift our, so this is all great. And sometimes you need it. I don't mean to denigrate the work. This is obviously foundational, touring awards. I use this stuff. I teach this stuff. It's all good. But when we don't need to use it, even better, right? So people have tried doing things like, what if all our states are mutable? That's a very functional programming game. It was sort of in my world, it's more about, well, you can mutate things as long as it's monotone. So if they're mutable, but they're monotone, maybe that'll work. And then using things like dependencies and provenance, all our ways of using application knowledge to avoid using the expensive stuff on the left. But the really big query is when do I need coordination and why do I need coordination? So if you ask, you know, a typical undergraduate or frankly, most people in computer science, including professors, when do you need coordination? What's a lock for, right? They'll say, well, it's to avoid conflicts on shared resources, right? This intersection needs coordination. If you would just put up some damn stop lights, right, then, you know, north, south could go for a while and west, east, west would wait. And then east, west would go for a while, north, south would wait, problem solved, right? But like, do I really need coordination? That's a solution. Is it the only solution? No, it's not the only solution, right? Here's a coordination free solution to that intersection problem, right? So I'd like to be able to think out of the box, right, and say, really, what is coordination for? Why am I required to use it? Okay. So that's a theory problem. So, you know, which programs have a coordination free implementation? We call those the green programs. These are specifications for which a clever programmer can find a coordination free solution. And then, of course, there's the rest of the programs, right? And I want to know this green line. Will someone please tell me, you know, Mr. Lamport, I think I only need you out here. So will you please tell me when I need you? And there's no answer from Mr. Lamport. At least he didn't, you know, pick up the phone when I call. But I'm happy to say that people at Hussalt did. And this is what led to the column theorem. So this is really a computability question. What's the expressive power of languages without coordination? Yeah. That's the green circle. Okay. So give you some intuition. Easy and hard questions. Here's an easy question. Is anyone in the room over 18? Excellent. Not only were you all happy to answer that coordination free, but you engaged in a little protocol, right? You made up a protocol where you raise a hand if you think it's true. So that was cool. So that was the monotone hand raising protocol or something. Great. All right. Who's the youngest person in the room? Oh, we have some brave assertions. But clearly, you don't know that. You could look at everyone, but that's cheating and also not necessarily right. Maybe, maybe. I don't know. I don't know. But the point here is, right, that somehow this requires you to communicate with people. And the first one maybe doesn't. Okay. More to the point. Let's look at the logic here, right? This is an existential question. And this is a question with the universal quantifier in it. Or for people like me who just want to do total pattern matching and look for not symbols, that one appears to be positive. So I'll say that it's monotone. And that one appears to be negative. So I'll say it's not monotone. So it gives you some intuition that universal quantification or negation requires coordination. It is coordination. That's what coordination is. It's universal quantification. So what is Lamport for? It's for universal quantifiers. So let's just prove this, right? I was like, well, somebody prove it. I'm not going to prove it. So nice guy named Tom Omelette wrote a thesis on this stuff. My conjecture was called the calm conjecture consistency is logical monotonicity. It was in a Paz keynote that I was gave some years ago. And then just a year later, there was a conference paper from the good folks at Haselt, which then they extended and then was further extended with weaker definitions for the monotonicity to really expand the results. If you want a quick, you know, kind of a version of what I'm saying now, you can read this CACOM overview, but it's really for systems people. I think you guys should just read Tom's papers. All right. To give you a flavor of what Tom did, definitions are half the battle. It seems, you know, when I read Paz papers, that's all the hard parts are the definitions, right? So, you know what monotonicity is in logic? That's fine. What is consistency here? Well, we want the program to produce the same output regardless of where the initial data is placed. So, I should be able to start the program with the data replicated pops possibly and partitioned in any way and get the same answer. And if that's true, then it would be the same answer across replicas. It would be the same answer across different runs. It would be the same answer if we start gossiping the data between each other. And this is what we want, right? So, that's our definition of consistency, where I think what's really clever and was the most beautiful part of the work is defining what coordination really means. So, we're sending messages around, right? That's data. But which data is really data and which data is kind of control messages? And how do you differentiate those in a formal way? And so, what they define in this paper is program is coordination free if there's some partitioning of the data when you first start. So, there's some way out of the data where you first start, such that the query can be answered without communication. So, for a particular query, for a particular data set, there is some world of where you lay it out where no communication is required. That's the definition of coordination freeness. And a program that you can't do that on is doing coordination messages. So, it's not really saying which messages are coordination and which messages are data, but it's telling you which programs can be run coordination. Yes. So, the trivial example of this is you put all the data in one node. And again, you know, this question of is there anybody who is older than me? What you don't know is whether anyone else has data. So, I may have all the data, but I don't know that. So, I still have to ask everybody, anybody got any data? And I have to wait for everybody to respond, right? So, it's a very nice intuition to just think about having all the data. All right. There's another thing in the paper that I hadn't even anticipated, which is really beautiful and speaks to stuff that the distributed systems community kind of knows, which is there's a third equivalent, which is that you can distributively compute this program on an oblivious transducer. And I haven't even talked about transducers yet, but just a minute. But what does it mean by oblivious? It means that the agent in the system doesn't know its own identity. It cannot distinguish messages from itself from messages from anyone else. So, it doesn't actually know who itself is. And it doesn't know the set of participants. We call this an oblivious agent, right? Oblivious programs that can be computed by oblivious agents are exactly the monotone programs and exactly the coordination free programs. So, that was very cool. And it speaks to questions of like membership protocols in distributed systems, which is about establishing common knowledge of the all relation. That's like one of the things that Paxos does is it has a membership protocol built in. So, it's one of the reasons it's coordination full is to establish all. So, this was really, really nice. So, this is all in this JACM paper. It's actually in the conference, the Paz paper as well. That's just a flavor of the calm stuff. And I'm going to stop with that. But happy to answer questions as best they can afterwards. And with that, I'm going to give it back to Connor. You guys can still hear me? All right. So, we have this calm theorem view of the world, relational transducers, logic, operating over sets. And then we have this semi lattice algebra view of the world. And they both are dealing with coordination and freeness in different lenses. But currently, they guarantee different things. The algebra view, like we said, is concerned with the coordination of freeness of rights and does not guarantee coordination of freeness of queries. Whereas the calm theorem view is only concerned with the coordination of freeness of queries. It actually doesn't have to worry about the coordination of freeness of rights because it assumes operating over sets and gets coordination of rights for free that way. And so, we're interested in the question of how can we combine these two lenses? Can we do an algebraic view of the calm theorem? And some intuition for how that might work is, you know, the semi lattice operator induces a partial order. And so, instead of having monotone logical queries without negation, you have monotone functions between lattices, between partial orders. So, that's something we've been exploring. We'd love to chat more about it with folks. I'm actually going to talk about a problem specific to the question Remy asked. It comes up in this setting. So, the calm theorem is all about basically not knowing when you have the entire input. What can I output and tell downstream consumers with certainty, even though I might have more updates in the future, there might be more messages arriving. And so, we call the ability to do this free termination without coordination. What can we be sure that we can output? And we're exploring this in a very generic setting of just we have two functions, an operation that's going to change our state over time in a query that's going to return some output. So, looking at an example of when we might be able to do this, we can look at this lattice. This is the set over these four socks and say that this is our state of a local node and we're at top in this lattice. And our update operation in the CRDT sense is union. So, we're going to union in more socks. We know that if we're at top, as updated as monotone, we'll never change our state. We're stuck at top. And so, whatever query we might be asking when we're in this state, we'd be able to locally detect that our query result is not going to change in the future and we'd be able to return our result with certainty. This might sound like it would not happen particularly often, but let's try and look at more examples where we would be able to figure out that with certainty, we can return an answer right now. So, what if we consider also the query that we're asking and say that this query is going to map us from this set socks lattice to the boolean lattice, true false, where top is true. Now, if this query is monotone, meaning as we go up in the partial order of socks, we also go up in the partial order of false and true, then we don't need to be at top on the left. We can actually be at top in the true false lattice and guarantee that our result won't change as future updates arrive. Any update that arrives is going to cause us to increase monotonically in the domain, which has to increase monotonically in the range and therefore our result will always stay true. For example, a query, is there a pair of socks? So, we call a threshold query. It effectively draws a cut through this partial order and says everything above this line in the partial order is true, everything below this line is false. So, these boolean threshold queries are a class of queries that we can freely terminate on if we know that our update is monotone. What about a totally different setting? What if we throw away monotonicity? So, now imagine that we have a deterministic finite automata and our query is mapping to true and false from accept and reject states in the automata and our update is appending a character. So, we think of a streaming character as appending. So, each update is going to transition us one step in this automata. And in this automata, any state that we're in, we can't conclude what the final result will be because from every state, there's a state that's accepting, that's reachable, and there's a state that's rejecting, that's reachable. So, some sequence of future updates might take us to false, some sequence of superstructures might take us to true. In contrast, if state three were also true, now from state one, we actually don't know if we're going to end up accepting or rejecting if we don't have the whole input yet. But if we're in states two or three, we know that every state that's reachable via future updates is going to keep us in our current result, which is true. And so, we can be certain that we can terminate here and return true. This is kind of like a reachability sort of visual view of how we're thinking about whether or not you can freely terminate given some arbitrary update operation on a domain and query operation that maps you to a range. This is a question raised in exploring a lot of different domains. If anyone has any ideas for what might connect to this, definitely come find us. Now, Joe is going to talk about partitioning. All right, this is a bit of a survey time. Boris, did you want to ask a question, Connor? I have a question to the last slide, right? Because in a sense, this now still has some monotone ordering, right? This kind of like, in a sense, it's kind of like if I can reach a node and it's smaller and now basically the nodes, if the terminal nodes are the top nodes and they don't have larger nodes, then so it's still monotone in the sense. Yeah, you can find some sort of it that, yeah. Yeah, I don't know if that's true for every freely terminating function. Psycho, do you think? Yeah, maybe. There's something about quotient lattice is, too, going at it in partial orders. Yeah, the graph looks like one. So this is love to have this conversation afterwards. That's why we're touching on a few things so we can have many conversations. So I think given time, I'm not going to go through this in any detail. I'm going to basically skip this chapter of the talk, except to quickly give some assertions. So first of all, we don't have HydroFlow, so we use Daedalus and we do have a full Daedalus to HydroFlow compiler. So we're able to write global programs in Daedalus and then auto partition and auto replicate them. And that's work being led by David Chu, who's in the room over here. David, three years ago, promised to do this and they gave him a certificate saying that's cool. So he won the SSP student research award. And three years later, he's got his first results. So it took a while. This was not an easy problem, but he's able to take arbitrary Daedalus programs, which are Daedalug, and partition them to run on multiple machines. And I'm really not going to spend a lot of time on this. What I'll say is that earlier student Michael Whitaker, who again did this by assertion, he found all sorts of opportunities to optimize Paxos because inside of Paxos, it was bottlenecking on things that were trivially parallelizable, like network message handling. So he's like, if I can just bust apart some of the roles in Paxos into sub roles, some of those sub roles can be replicated. And he got state-of-the-art performance in terms of throughput on Paxos by doing this. And what I observed after he did it was, oh my gosh, most of the things that you've split out are monotone subcomponents. And I should have known that we could pull those out and replicate those. In fact, I wish Bloom could do that automatically, but it couldn't. So three years later, David can now automatically pull out these things that Michael was observing and transform the program to do it. And the ideas are basically just two tricks. One trick is to take a pipeline on a single machine and split it across two machines. He calls that decoupling. Now in a general purpose program, this is taking, I don't know, 10,000 lines of C++ and figuring out which ones to run on this machine and which ones to run on that machine. That would be horrible, right? But in a data flow language or logic language, it's quite nice. And so he has conditions for when this is safe and when it's not. So that's decoupling. So you can think of this as refactoring a program into components that can be run on different machines with asynchronous communication. The other thing he does is what's sometimes called sharding in the practitioner community, but it's partitioning, shared nothing partitioning of subplants, right? So instead of having BC take all of the data from A, you have a hash partitioning here and certain values go to each machine. And how do you know that each one of these computations can be done independently? That's done through things like functional dependency analysis so that you can show that certain values have no dependency on other values because they're partitioned by, say, NFD. So I'm not going to go into any of this, but basically what David was able to do was take many of the optimizations here that were handwritten in Scala and automate them and formalize their correctness. And without getting into too much detail, although it is kind of fun, oh, and we borrowed some work from Paris. So shout out to Paris for parallel disjoint correctness and colleagues. It is really fast. So he was able automatically. This is Michael's results that we re-ran. This is Scala. This is throughput against latency. So what you want to do is you get as much throughput as you can until it starts to hurt you at latency and it curls back. So this is kind of where things start to top out, if you will. So that's Whitaker's implementation. This is the same logic as Whitaker's implementation in Hydro. So this is just Hydro is faster than Scala written by hand. So this is just a testament to the folks who wrote the Hydro flow engine. But the blue line is what David achieved through systematic correctly proven rewrites. So he was able to get performance that actually, because Hydro is fast, is better than Whitaker's Paxos implementation. And this gap is kind of what he's given up. These are the tricks that Michael had that we didn't cover in our rewrites. But we're doing 90% with the easy tricks. So it gives me confidence that simple query optimizations known in parallel databases can have impacts on places that we're really very fine tuned performance issues that people write PhDs to get this kind of performance and we're getting it through systematic rewrites. Very promising. David's only halfway there though, because he has to have a proper cost-based optimizer. Right now what he has is correct transformation rules. He needs a cost model with an objective function. And then he needs a search strategy. And we're hopefully going to be using egg log or some implementation of egg log in Hydro flow to achieve this. So we're one of the things with Maxis stuff that overlaps is if there's a lovely semi lattice based data flow implementation of Maxis stuff, maybe we can clean up some of the things where he's doing updates in place. This work? This work. Well, so this was written in Datalog and translated down into that Hydro flow data flow language you saw at the top. This stuff is also written in Datalog currently in a runtime that plays some ad-hoc tricks. That's not traditional Datalog execution here as Maxis. But I think, you know, Union Find is a nice target for an algebraic treatment and I think we have opportunity. Okay, what I'd like to do in the last few minutes is berate you with questions because these are the things that I don't know how to answer yet and I would love to get help with. So the first is, and this is an outline, so this section goes on for many slides, but there's the four questions. Can we please have one theory for all this nonsense instead of the list I'm about to show you? What would be a good type system for the physical layer where we could prove correctness of transformations? I have a follow on to Sudipa about the role of our kinds of work in the era of generative AI. And then I have this ongoing question of what time is for, which I probably don't have time to explain. But quickly, you know, the unifying theory thing. So CRD teaser semi-ladyses, Datalog, Daedalus was all done with model theory and it's fancy actually. It uses like stable models and stuff. It's actually ended up being kind of fancy. The column theorem, Amalut, proves have these relational transducers, which are this halfway world between operational and declarative semantics. You have these state machines on each node. They run declarative languages on each step, but then they output stuff and I think you can write non-terminating programs if you want to. So you can write Toggle, for example, and Daedalus and the transducers. Now they don't have to be terminated. In any sense, I don't think. But the point is, I really wish he'd have done this work with this, because he also was on this work, but he didn't. He did it with transducers, which is a bummer. If you talk to distributed systems people, they talk about essentially order theory. They talk about partial orders all the time, which is related to lattices, but you know, it's annoying. Programmers want to write these sort of algebraic functional expressions, which I think is a good thing for all of us. And then yeah, I give all these talks and then some joker raises their hand and says, well, what about transactions? And in fact, Peter Bayless, when he was a student, basically did an end run around my entire group and just wrote papers about transactions and coordination, and they don't align with the rest of this stuff. So it's an open challenge to reintegrate that work. And then, you know, I didn't actually say the S word yet, because I apparently didn't do joins as of yet. But we do do joins, so we probably need. So it would be really great to get all of it here. I would like to bring all of this work into this domain. That would be really nice. Okay. Here's a flavor of what I'm dealing with, though. So just finished reading the DBSP paper, which was very nice and related to our work, but we have some other things we need to keep track of that are relating to the network messing with stuff. So when we look at a stream, it's got some values. It's got some happenstance ordering, that's a mapping of the naturals to those values. It's got some happenstance batching. It came in over the network in chunks. So there's like singly nested parentheses in this stream that are randomly placed. Randomly, you know, arbitrarily ordered, arbitrarily placed. Maybe this is a stream of lattice points, but maybe it's not. I don't know. But if it is, you could say things like there's a monotonicity relationship between the type's natural order and the total order of arrival or not, right? And then what sort does is it enforces something like this, right? It's nice when these are atomistic, like data flow is basically a set lattice that you flow individual atoms. That's the traditional kind of database iterator thing, one tuple at a time, right? One tuple at a time is an atomistic lattice of the sets of tuples. And it's nice when you know you're dealing with atoms, because you can say things like item potents, right? I gave you Bob's tuple once, I gave you Bob's tuple twice, sorry, but you should only have it once. So delete one. But if I give you subsets, now you have to find how they overlap and you have to make sure that when you union them, you remove the overlapping bits. And so when you have non-atomistic things, it's just a little uglier. And you end up talking about like, does their meat, is there meat bot? kinds of things. Do they have no intersection, right? So these are the kinds of properties that I think I need to track in my rewrite rules. And then, you know, the operators are the invariants to these properties, like lattice, lattice operations are invariants of order and parenthesization. Do they preserve the properties? Do they enforce different values for the properties? The network non-deterministically enforces orders and parenthesizations. So like this is the stuff that I worry about in my operators. And this is kind of the soup I'm swimming in with this physical algebra. So I would like help with this. All right, I'm going to do one more quick slide. This is very much in the realm of what Sudipa was talking about. You know, we're in the era where people will be programming with green goo, right? It's just this is large language models, they're magical, they produce stuff. But what we really want is building blocks that we can count on, right? We're a database people, our industry is all about foundational building blocks. And I really do think declarative specification is a lovely narrow waste here between these two, where we can take a formal spec as Cod asked us to, we can render it in some sense so that it's readable, right? And this relates to things like Wolfgang's work on visualizing queries, and what Sudipa was talking about in terms of giving natural language things, but helping people look at this and say, is this what you meant? Not is it correct, but is this what you meant since a spec after all, right? Did you mean this query? And then of course, if it's in a nice formal language, we can check it for other things, right? And so that would be, I think, a role that we can really play in the world. And I suspect things like this will happen. These programs are going to be a selection of programs. You're constantly going to be given a menu of which of these things did you mean. And the answer to which is either some invariant checks or something, or some human judgment. So I think we're in a good spot in terms of intermediate languages. And I'll just close with one more slide, maybe just a handful of slides. What are clocks in time for in distributed systems? So there's this famous saying, which is correctly attributed to a sci-fi short story. Time is what keeps everything from happening all at once. So when should we use time and computing? What are clocks for? Well, they're not for monotone queries. I can run this embarrassingly parallel. It can all happen at the same time, and it's fine. And Buntalu was doing this long before this discussion. But I can't run this at the same time. You can't have P and not P at the same time. So what's the deadliest answer to that is, well, that's what time is for. This is the toggle program, right? And time is there to separate two things that can't coexist. That should be, I think, the only reason for time. Except it's not. Distributed systems, people use time for maintaining partial orders and knowing when things were concurrent. Sometimes you don't need that. Sometimes you do. But this is my question, especially because Val's in the room and worked on this DSP stuff. Daedalus has one clock per node and we update it only when there's a network event or we have to cycle through negation. Timely and differential data flow have clocks all over the damn place. And I'm not sure when you use them and when you don't. So, for example, tracking iterations of a monotonic recursive program. Why do I need a clock for that? I don't think I need a clock for that. Maybe if it's a while and we use the index in our computation, I need to know what index I'm at. So the general question is, when do we use this structure called a clock? And when don't we need? And can a compiler decide? All right. We are a little over time. I hope we have given you lots of things to ask us later. We need lots of help. So we'd love it. And we are big fans of working with folks like you. Can you leave these last four slides there so we can come up with some more folk questions?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.0, "text": " Second talk today has a title that has evolved since last time I looked at it.", "tokens": [50364, 5736, 751, 965, 575, 257, 4876, 300, 575, 14178, 1670, 1036, 565, 286, 2956, 412, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3398360898417811, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.05695608630776405}, {"id": 1, "seek": 0, "start": 8.0, "end": 17.28, "text": " Okay, but it has logic and algebras and we are very excited to hear what this has to do with cloud computing,", "tokens": [50764, 1033, 11, 457, 309, 575, 9952, 293, 419, 432, 38182, 293, 321, 366, 588, 2919, 281, 1568, 437, 341, 575, 281, 360, 365, 4588, 15866, 11, 51228], "temperature": 0.0, "avg_logprob": -0.3398360898417811, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.05695608630776405}, {"id": 2, "seek": 0, "start": 17.28, "end": 22.080000000000002, "text": " which we are all subjected to in our daily lives.", "tokens": [51228, 597, 321, 366, 439, 32153, 281, 294, 527, 5212, 2909, 13, 51468], "temperature": 0.0, "avg_logprob": -0.3398360898417811, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.05695608630776405}, {"id": 3, "seek": 2208, "start": 22.08, "end": 31.439999999999998, "text": " Thank you. So this will be a bit unusual, both Joe and Connor will talk and I let them take", "tokens": [50364, 1044, 291, 13, 407, 341, 486, 312, 257, 857, 10901, 11, 1293, 6807, 293, 33133, 486, 751, 293, 286, 718, 552, 747, 50832], "temperature": 0.0, "avg_logprob": -0.14410101134201575, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.12665686011314392}, {"id": 4, "seek": 2208, "start": 31.439999999999998, "end": 38.64, "text": " care of the logistics of that. Again, remember that for questions that might be more appropriate", "tokens": [50832, 1127, 295, 264, 27420, 295, 300, 13, 3764, 11, 1604, 300, 337, 1651, 300, 1062, 312, 544, 6854, 51192], "temperature": 0.0, "avg_logprob": -0.14410101134201575, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.12665686011314392}, {"id": 5, "seek": 2208, "start": 38.64, "end": 43.36, "text": " for a longer discussion, we will have that discussion right after their talk.", "tokens": [51192, 337, 257, 2854, 5017, 11, 321, 486, 362, 300, 5017, 558, 934, 641, 751, 13, 51428], "temperature": 0.0, "avg_logprob": -0.14410101134201575, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.12665686011314392}, {"id": 6, "seek": 2208, "start": 45.84, "end": 51.68, "text": " Okay, thanks. So this is work that obviously we're doing here at Berkeley and I'm also funded by", "tokens": [51552, 1033, 11, 3231, 13, 407, 341, 307, 589, 300, 2745, 321, 434, 884, 510, 412, 23684, 293, 286, 478, 611, 14385, 538, 51844], "temperature": 0.0, "avg_logprob": -0.14410101134201575, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.12665686011314392}, {"id": 7, "seek": 5168, "start": 51.68, "end": 55.84, "text": " Sutter Hill Ventures. So it's kind of cool. We've got venture capitalists funding basic research.", "tokens": [50364, 318, 9947, 9109, 28290, 1303, 13, 407, 309, 311, 733, 295, 1627, 13, 492, 600, 658, 18474, 4238, 1751, 6137, 3875, 2132, 13, 50572], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 8, "seek": 5168, "start": 55.84, "end": 60.24, "text": " I have promised them that there's no applicable. I'm not really sure what this could turn into", "tokens": [50572, 286, 362, 10768, 552, 300, 456, 311, 572, 21142, 13, 286, 478, 406, 534, 988, 437, 341, 727, 1261, 666, 50792], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 9, "seek": 5168, "start": 60.24, "end": 64.4, "text": " as a company, but they're cool and they've given us developers to work on the project,", "tokens": [50792, 382, 257, 2237, 11, 457, 436, 434, 1627, 293, 436, 600, 2212, 505, 8849, 281, 589, 322, 264, 1716, 11, 51000], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 10, "seek": 5168, "start": 64.4, "end": 68.0, "text": " which has just been great and they're funding me as well. So thanks to them and to Berkeley.", "tokens": [51000, 597, 575, 445, 668, 869, 293, 436, 434, 6137, 385, 382, 731, 13, 407, 3231, 281, 552, 293, 281, 23684, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 11, "seek": 5168, "start": 68.8, "end": 72.48, "text": " There's this story people like to tell in computing. This is my standard opening slides.", "tokens": [51220, 821, 311, 341, 1657, 561, 411, 281, 980, 294, 15866, 13, 639, 307, 452, 3832, 5193, 9788, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 12, "seek": 5168, "start": 73.92, "end": 76.96000000000001, "text": " Operating systems people really like this story because it's sort of the Thompson and", "tokens": [51476, 12480, 990, 3652, 561, 534, 411, 341, 1657, 570, 309, 311, 1333, 295, 264, 23460, 293, 51628], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 13, "seek": 5168, "start": 76.96000000000001, "end": 81.2, "text": " Richie Turing Award story. For every platform that comes out, there's a programming environment", "tokens": [51628, 6781, 414, 314, 1345, 13894, 1657, 13, 1171, 633, 3663, 300, 1487, 484, 11, 456, 311, 257, 9410, 2823, 51840], "temperature": 0.0, "avg_logprob": -0.1327904470605788, "compression_ratio": 1.7238605898123325, "no_speech_prob": 0.005216733552515507}, {"id": 14, "seek": 8120, "start": 81.44, "end": 85.92, "text": " that's somehow suited to that platform that emerges and as a result, people write things you", "tokens": [50376, 300, 311, 6063, 24736, 281, 300, 3663, 300, 38965, 293, 382, 257, 1874, 11, 561, 2464, 721, 291, 50600], "temperature": 0.0, "avg_logprob": -0.09041893751101386, "compression_ratio": 1.72, "no_speech_prob": 0.0006069376831874251}, {"id": 15, "seek": 8120, "start": 85.92, "end": 90.72, "text": " never would have expected on that platform and it succeeds. So the PDP 11 with Unix and C", "tokens": [50600, 1128, 576, 362, 5176, 322, 300, 3663, 293, 309, 49263, 13, 407, 264, 10464, 47, 2975, 365, 1156, 970, 293, 383, 50840], "temperature": 0.0, "avg_logprob": -0.09041893751101386, "compression_ratio": 1.72, "no_speech_prob": 0.0006069376831874251}, {"id": 16, "seek": 8120, "start": 90.72, "end": 94.8, "text": " is the canonical example of this, but one can argue that in every generation of new kind of", "tokens": [50840, 307, 264, 46491, 1365, 295, 341, 11, 457, 472, 393, 9695, 300, 294, 633, 5125, 295, 777, 733, 295, 51044], "temperature": 0.0, "avg_logprob": -0.09041893751101386, "compression_ratio": 1.72, "no_speech_prob": 0.0006069376831874251}, {"id": 17, "seek": 8120, "start": 94.8, "end": 100.56, "text": " computing platforms, programming environments have arisen to allow people to build an app for that.", "tokens": [51044, 15866, 9473, 11, 9410, 12388, 362, 594, 11106, 281, 2089, 561, 281, 1322, 364, 724, 337, 300, 13, 51332], "temperature": 0.0, "avg_logprob": -0.09041893751101386, "compression_ratio": 1.72, "no_speech_prob": 0.0006069376831874251}, {"id": 18, "seek": 8120, "start": 101.84, "end": 105.76, "text": " And so nobody expected all the apps we have on our phones. It's wonderful. Developers were", "tokens": [51396, 400, 370, 5079, 5176, 439, 264, 7733, 321, 362, 322, 527, 10216, 13, 467, 311, 3715, 13, 11442, 433, 645, 51592], "temperature": 0.0, "avg_logprob": -0.09041893751101386, "compression_ratio": 1.72, "no_speech_prob": 0.0006069376831874251}, {"id": 19, "seek": 8120, "start": 106.4, "end": 110.56, "text": " freed to write all sorts of things. Strangely, there's a platform that's as old as the iPhone", "tokens": [51624, 21796, 281, 2464, 439, 7527, 295, 721, 13, 8251, 656, 736, 11, 456, 311, 257, 3663, 300, 311, 382, 1331, 382, 264, 7252, 51832], "temperature": 0.0, "avg_logprob": -0.09041893751101386, "compression_ratio": 1.72, "no_speech_prob": 0.0006069376831874251}, {"id": 20, "seek": 11056, "start": 110.56, "end": 116.0, "text": " called the cloud. So AWS is approximately the same age as the iPhone, but it doesn't have a", "tokens": [50364, 1219, 264, 4588, 13, 407, 17650, 307, 10447, 264, 912, 3205, 382, 264, 7252, 11, 457, 309, 1177, 380, 362, 257, 50636], "temperature": 0.0, "avg_logprob": -0.10699028835118374, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0005192417302168906}, {"id": 21, "seek": 11056, "start": 116.0, "end": 121.68, "text": " canonical programming model. And there's many reasons why that might be, partly because it's", "tokens": [50636, 46491, 9410, 2316, 13, 400, 456, 311, 867, 4112, 983, 300, 1062, 312, 11, 17031, 570, 309, 311, 50920], "temperature": 0.0, "avg_logprob": -0.10699028835118374, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0005192417302168906}, {"id": 22, "seek": 11056, "start": 121.68, "end": 125.68, "text": " a really hard programming environment, yes. So it has to deal with all the problems of parallel", "tokens": [50920, 257, 534, 1152, 9410, 2823, 11, 2086, 13, 407, 309, 575, 281, 2028, 365, 439, 264, 2740, 295, 8952, 51120], "temperature": 0.0, "avg_logprob": -0.10699028835118374, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0005192417302168906}, {"id": 23, "seek": 11056, "start": 125.68, "end": 131.6, "text": " computing, as well as things like distributed consistency, what happens when you have partial", "tokens": [51120, 15866, 11, 382, 731, 382, 721, 411, 12631, 14416, 11, 437, 2314, 562, 291, 362, 14641, 51416], "temperature": 0.0, "avg_logprob": -0.10699028835118374, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0005192417302168906}, {"id": 24, "seek": 11056, "start": 131.6, "end": 135.6, "text": " failures in your system, but it keeps running. So componentry is down, but the system's still", "tokens": [51416, 20774, 294, 428, 1185, 11, 457, 309, 5965, 2614, 13, 407, 6542, 627, 307, 760, 11, 457, 264, 1185, 311, 920, 51616], "temperature": 0.0, "avg_logprob": -0.10699028835118374, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0005192417302168906}, {"id": 25, "seek": 13560, "start": 135.6, "end": 140.4, "text": " running. And then in the modern cloud, we want things to auto scale. So you allocate more machines", "tokens": [50364, 2614, 13, 400, 550, 294, 264, 4363, 4588, 11, 321, 528, 721, 281, 8399, 4373, 13, 407, 291, 35713, 544, 8379, 50604], "temperature": 0.0, "avg_logprob": -0.09128485079164858, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.006691615562886}, {"id": 26, "seek": 13560, "start": 140.4, "end": 144.16, "text": " and then you free up some machines, but the program's still running. So the platform is", "tokens": [50604, 293, 550, 291, 1737, 493, 512, 8379, 11, 457, 264, 1461, 311, 920, 2614, 13, 407, 264, 3663, 307, 50792], "temperature": 0.0, "avg_logprob": -0.09128485079164858, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.006691615562886}, {"id": 27, "seek": 13560, "start": 144.16, "end": 149.35999999999999, "text": " changing underneath you as you're executing. So this is all hard and programmers right now are", "tokens": [50792, 4473, 7223, 291, 382, 291, 434, 32368, 13, 407, 341, 307, 439, 1152, 293, 41504, 558, 586, 366, 51052], "temperature": 0.0, "avg_logprob": -0.09128485079164858, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.006691615562886}, {"id": 28, "seek": 13560, "start": 149.35999999999999, "end": 154.64, "text": " trying to do this essentially in Java. That's sort of the state of the art. And the annoying thing", "tokens": [51052, 1382, 281, 360, 341, 4476, 294, 10745, 13, 663, 311, 1333, 295, 264, 1785, 295, 264, 1523, 13, 400, 264, 11304, 551, 51316], "temperature": 0.0, "avg_logprob": -0.09128485079164858, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.006691615562886}, {"id": 29, "seek": 13560, "start": 154.64, "end": 160.0, "text": " is these compilers for these languages don't answer any of these questions that are hard. So I think,", "tokens": [51316, 307, 613, 715, 388, 433, 337, 613, 8650, 500, 380, 1867, 604, 295, 613, 1651, 300, 366, 1152, 13, 407, 286, 519, 11, 51584], "temperature": 0.0, "avg_logprob": -0.09128485079164858, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.006691615562886}, {"id": 30, "seek": 13560, "start": 160.0, "end": 164.64, "text": " honestly, this is like this hole in computer science that nobody's filled and it seems like", "tokens": [51584, 6095, 11, 341, 307, 411, 341, 5458, 294, 3820, 3497, 300, 5079, 311, 6412, 293, 309, 2544, 411, 51816], "temperature": 0.0, "avg_logprob": -0.09128485079164858, "compression_ratio": 1.8050314465408805, "no_speech_prob": 0.006691615562886}, {"id": 31, "seek": 16464, "start": 164.64, "end": 168.72, "text": " one of our grand challenges from my perspective. So I've been working on it for a long time,", "tokens": [50364, 472, 295, 527, 2697, 4759, 490, 452, 4585, 13, 407, 286, 600, 668, 1364, 322, 309, 337, 257, 938, 565, 11, 50568], "temperature": 0.0, "avg_logprob": -0.09276987901374475, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.0005191899836063385}, {"id": 32, "seek": 16464, "start": 169.35999999999999, "end": 173.11999999999998, "text": " and I think there's still a lot of work to do. I take inspiration, of course, from this gentleman", "tokens": [50600, 293, 286, 519, 456, 311, 920, 257, 688, 295, 589, 281, 360, 13, 286, 747, 10249, 11, 295, 1164, 11, 490, 341, 15761, 50788], "temperature": 0.0, "avg_logprob": -0.09276987901374475, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.0005191899836063385}, {"id": 33, "seek": 16464, "start": 173.11999999999998, "end": 178.16, "text": " as maybe we all do. What was cool about Ted Codd was he said, look, you should write things in a", "tokens": [50788, 382, 1310, 321, 439, 360, 13, 708, 390, 1627, 466, 14985, 383, 378, 67, 390, 415, 848, 11, 574, 11, 291, 820, 2464, 721, 294, 257, 51040], "temperature": 0.0, "avg_logprob": -0.09276987901374475, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.0005191899836063385}, {"id": 34, "seek": 16464, "start": 178.16, "end": 182.0, "text": " formal language. You should have formal specifications. And then there should be machinery that", "tokens": [51040, 9860, 2856, 13, 509, 820, 362, 9860, 29448, 13, 400, 550, 456, 820, 312, 27302, 300, 51232], "temperature": 0.0, "avg_logprob": -0.09276987901374475, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.0005191899836063385}, {"id": 35, "seek": 16464, "start": 182.0, "end": 186.95999999999998, "text": " automates the implementation. And if we do that, then the implementation can change", "tokens": [51232, 3553, 1024, 264, 11420, 13, 400, 498, 321, 360, 300, 11, 550, 264, 11420, 393, 1319, 51480], "temperature": 0.0, "avg_logprob": -0.09276987901374475, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.0005191899836063385}, {"id": 36, "seek": 16464, "start": 186.95999999999998, "end": 191.51999999999998, "text": " while the specification remains the same. This is very nice for things like databases, right?", "tokens": [51480, 1339, 264, 31256, 7023, 264, 912, 13, 639, 307, 588, 1481, 337, 721, 411, 22380, 11, 558, 30, 51708], "temperature": 0.0, "avg_logprob": -0.09276987901374475, "compression_ratio": 1.7208588957055215, "no_speech_prob": 0.0005191899836063385}, {"id": 37, "seek": 19152, "start": 191.52, "end": 197.52, "text": " So the thing is that Codd was trapped in this database prison for all these years. And I think", "tokens": [50364, 407, 264, 551, 307, 300, 383, 378, 67, 390, 14994, 294, 341, 8149, 6168, 337, 439, 613, 924, 13, 400, 286, 519, 50664], "temperature": 0.0, "avg_logprob": -0.07505363091490322, "compression_ratio": 1.809061488673139, "no_speech_prob": 0.00016862612392287701}, {"id": 38, "seek": 19152, "start": 197.52, "end": 202.56, "text": " there's a much broader applicability of the design principle. So we worked on things in our", "tokens": [50664, 456, 311, 257, 709, 13227, 2580, 2310, 295, 264, 1715, 8665, 13, 407, 321, 2732, 322, 721, 294, 527, 50916], "temperature": 0.0, "avg_logprob": -0.07505363091490322, "compression_ratio": 1.809061488673139, "no_speech_prob": 0.00016862612392287701}, {"id": 39, "seek": 19152, "start": 202.56, "end": 206.8, "text": " community like declarative networking. So we brought Codd out of the database and into the network.", "tokens": [50916, 1768, 411, 16694, 1166, 17985, 13, 407, 321, 3038, 383, 378, 67, 484, 295, 264, 8149, 293, 666, 264, 3209, 13, 51128], "temperature": 0.0, "avg_logprob": -0.07505363091490322, "compression_ratio": 1.809061488673139, "no_speech_prob": 0.00016862612392287701}, {"id": 40, "seek": 19152, "start": 207.76000000000002, "end": 211.84, "text": " So I've done some work on that. Many of us I think in this room have done something around", "tokens": [51176, 407, 286, 600, 1096, 512, 589, 322, 300, 13, 5126, 295, 505, 286, 519, 294, 341, 1808, 362, 1096, 746, 926, 51380], "temperature": 0.0, "avg_logprob": -0.07505363091490322, "compression_ratio": 1.809061488673139, "no_speech_prob": 0.00016862612392287701}, {"id": 41, "seek": 19152, "start": 211.84, "end": 216.56, "text": " declarative data science and machine learning. This is a growing area, right? In the program", "tokens": [51380, 16694, 1166, 1412, 3497, 293, 3479, 2539, 13, 639, 307, 257, 4194, 1859, 11, 558, 30, 682, 264, 1461, 51616], "temperature": 0.0, "avg_logprob": -0.07505363091490322, "compression_ratio": 1.809061488673139, "no_speech_prob": 0.00016862612392287701}, {"id": 42, "seek": 19152, "start": 216.56, "end": 221.12, "text": " analysis community, the use of declarative languages has been pretty powerful. So that's", "tokens": [51616, 5215, 1768, 11, 264, 764, 295, 16694, 1166, 8650, 575, 668, 1238, 4005, 13, 407, 300, 311, 51844], "temperature": 0.0, "avg_logprob": -0.07505363091490322, "compression_ratio": 1.809061488673139, "no_speech_prob": 0.00016862612392287701}, {"id": 43, "seek": 22112, "start": 221.12, "end": 225.36, "text": " really cool. And then, of course, the hot thing, which is why we're all here, is that we're going", "tokens": [50364, 534, 1627, 13, 400, 550, 11, 295, 1164, 11, 264, 2368, 551, 11, 597, 307, 983, 321, 434, 439, 510, 11, 307, 300, 321, 434, 516, 50576], "temperature": 0.0, "avg_logprob": -0.07305987257706492, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0001233840739587322}, {"id": 44, "seek": 22112, "start": 225.36, "end": 229.84, "text": " to start to try to look at this stuff through the lenses of algebras instead of logic or in addition", "tokens": [50576, 281, 722, 281, 853, 281, 574, 412, 341, 1507, 807, 264, 18059, 295, 419, 432, 38182, 2602, 295, 9952, 420, 294, 4500, 50800], "temperature": 0.0, "avg_logprob": -0.07305987257706492, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0001233840739587322}, {"id": 45, "seek": 22112, "start": 229.84, "end": 234.24, "text": " to logic, which is pretty neat. And we're going to, you know, we've heard or are hearing about a", "tokens": [50800, 281, 9952, 11, 597, 307, 1238, 10654, 13, 400, 321, 434, 516, 281, 11, 291, 458, 11, 321, 600, 2198, 420, 366, 4763, 466, 257, 51020], "temperature": 0.0, "avg_logprob": -0.07305987257706492, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0001233840739587322}, {"id": 46, "seek": 22112, "start": 234.24, "end": 240.0, "text": " variety of different algebras that people are playing with in this domain. So what I'm interested in", "tokens": [51020, 5673, 295, 819, 419, 432, 38182, 300, 561, 366, 2433, 365, 294, 341, 9274, 13, 407, 437, 286, 478, 3102, 294, 51308], "temperature": 0.0, "avg_logprob": -0.07305987257706492, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0001233840739587322}, {"id": 47, "seek": 22112, "start": 240.0, "end": 244.48000000000002, "text": " is taking Codd into the cloud, yeah? And here's sort of the analogy, the way to think about it.", "tokens": [51308, 307, 1940, 383, 378, 67, 666, 264, 4588, 11, 1338, 30, 400, 510, 311, 1333, 295, 264, 21663, 11, 264, 636, 281, 519, 466, 309, 13, 51532], "temperature": 0.0, "avg_logprob": -0.07305987257706492, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0001233840739587322}, {"id": 48, "seek": 22112, "start": 244.48000000000002, "end": 249.28, "text": " The relational database was invented to hide how data is laid out and how queries are executed,", "tokens": [51532, 440, 38444, 8149, 390, 14479, 281, 6479, 577, 1412, 307, 9897, 484, 293, 577, 24109, 366, 17577, 11, 51772], "temperature": 0.0, "avg_logprob": -0.07305987257706492, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.0001233840739587322}, {"id": 49, "seek": 24928, "start": 249.28, "end": 254.0, "text": " right? And all that should be decided sort of lazily based on the current environment.", "tokens": [50364, 558, 30, 400, 439, 300, 820, 312, 3047, 1333, 295, 19320, 953, 2361, 322, 264, 2190, 2823, 13, 50600], "temperature": 0.0, "avg_logprob": -0.08071467788131148, "compression_ratio": 1.73125, "no_speech_prob": 0.00021653286239597946}, {"id": 50, "seek": 24928, "start": 254.0, "end": 259.44, "text": " Well, the cloud is just a generalization. It was invented to hide all your computing resources", "tokens": [50600, 1042, 11, 264, 4588, 307, 445, 257, 2674, 2144, 13, 467, 390, 14479, 281, 6479, 439, 428, 15866, 3593, 50872], "temperature": 0.0, "avg_logprob": -0.08071467788131148, "compression_ratio": 1.73125, "no_speech_prob": 0.00021653286239597946}, {"id": 51, "seek": 24928, "start": 259.44, "end": 263.2, "text": " and how they're laid out. So not just your blocks on your desk, but really everything.", "tokens": [50872, 293, 577, 436, 434, 9897, 484, 13, 407, 406, 445, 428, 8474, 322, 428, 10026, 11, 457, 534, 1203, 13, 51060], "temperature": 0.0, "avg_logprob": -0.08071467788131148, "compression_ratio": 1.73125, "no_speech_prob": 0.00021653286239597946}, {"id": 52, "seek": 24928, "start": 263.76, "end": 269.52, "text": " And it's for general purpose computations. So the cloud, in a lot of ways, is this abstraction.", "tokens": [51088, 400, 309, 311, 337, 2674, 4334, 2807, 763, 13, 407, 264, 4588, 11, 294, 257, 688, 295, 2098, 11, 307, 341, 37765, 13, 51376], "temperature": 0.0, "avg_logprob": -0.08071467788131148, "compression_ratio": 1.73125, "no_speech_prob": 0.00021653286239597946}, {"id": 53, "seek": 24928, "start": 269.52, "end": 274.96, "text": " It's this physical layer abstraction. The physics of the deployment of your code is going to change,", "tokens": [51376, 467, 311, 341, 4001, 4583, 37765, 13, 440, 10649, 295, 264, 19317, 295, 428, 3089, 307, 516, 281, 1319, 11, 51648], "temperature": 0.0, "avg_logprob": -0.08071467788131148, "compression_ratio": 1.73125, "no_speech_prob": 0.00021653286239597946}, {"id": 54, "seek": 24928, "start": 274.96, "end": 278.16, "text": " but you want your spec to remain the same. That's how you'd really like to program in an", "tokens": [51648, 457, 291, 528, 428, 1608, 281, 6222, 264, 912, 13, 663, 311, 577, 291, 1116, 534, 411, 281, 1461, 294, 364, 51808], "temperature": 0.0, "avg_logprob": -0.08071467788131148, "compression_ratio": 1.73125, "no_speech_prob": 0.00021653286239597946}, {"id": 55, "seek": 27816, "start": 278.16, "end": 283.92, "text": " environment that is this heterogeneous and elastic. So I believe that it's extremely natural for", "tokens": [50364, 2823, 300, 307, 341, 20789, 31112, 293, 17115, 13, 407, 286, 1697, 300, 309, 311, 4664, 3303, 337, 50652], "temperature": 0.0, "avg_logprob": -0.051719925620339134, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00021653276053257287}, {"id": 56, "seek": 27816, "start": 283.92, "end": 288.56, "text": " techniques that we've been working on in our community to try to be applied to cloud computing.", "tokens": [50652, 7512, 300, 321, 600, 668, 1364, 322, 294, 527, 1768, 281, 853, 281, 312, 6456, 281, 4588, 15866, 13, 50884], "temperature": 0.0, "avg_logprob": -0.051719925620339134, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00021653276053257287}, {"id": 57, "seek": 27816, "start": 288.56, "end": 292.56, "text": " And we have a project in my group called Hydro, which you can read more about,", "tokens": [50884, 400, 321, 362, 257, 1716, 294, 452, 1594, 1219, 24231, 340, 11, 597, 291, 393, 1401, 544, 466, 11, 51084], "temperature": 0.0, "avg_logprob": -0.051719925620339134, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00021653276053257287}, {"id": 58, "seek": 27816, "start": 292.56, "end": 298.48, "text": " which I will tell you a bit about today. Okay. So what are my goals? Well, I kind of want to", "tokens": [51084, 597, 286, 486, 980, 291, 257, 857, 466, 965, 13, 1033, 13, 407, 437, 366, 452, 5493, 30, 1042, 11, 286, 733, 295, 528, 281, 51380], "temperature": 0.0, "avg_logprob": -0.051719925620339134, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00021653276053257287}, {"id": 59, "seek": 27816, "start": 298.48, "end": 304.08000000000004, "text": " build something like LLVM for the cloud. So LLVM, as you may know, is a very successful sort of", "tokens": [51380, 1322, 746, 411, 441, 43, 53, 44, 337, 264, 4588, 13, 407, 441, 43, 53, 44, 11, 382, 291, 815, 458, 11, 307, 257, 588, 4406, 1333, 295, 51660], "temperature": 0.0, "avg_logprob": -0.051719925620339134, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.00021653276053257287}, {"id": 60, "seek": 30408, "start": 304.15999999999997, "end": 310.32, "text": " language stack. It supports many languages, including C++ and Rust and Swift and others.", "tokens": [50368, 2856, 8630, 13, 467, 9346, 867, 8650, 11, 3009, 383, 25472, 293, 34952, 293, 25539, 293, 2357, 13, 50676], "temperature": 0.0, "avg_logprob": -0.07732796478271485, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.0026314102578908205}, {"id": 61, "seek": 30408, "start": 311.03999999999996, "end": 314.71999999999997, "text": " It has an internal language called its internal representation, and then it compiles down to", "tokens": [50712, 467, 575, 364, 6920, 2856, 1219, 1080, 6920, 10290, 11, 293, 550, 309, 715, 4680, 760, 281, 50896], "temperature": 0.0, "avg_logprob": -0.07732796478271485, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.0026314102578908205}, {"id": 62, "seek": 30408, "start": 314.71999999999997, "end": 319.91999999999996, "text": " a variety of machine code for different platforms. So it's been extremely successful, but it doesn't", "tokens": [50896, 257, 5673, 295, 3479, 3089, 337, 819, 9473, 13, 407, 309, 311, 668, 4664, 4406, 11, 457, 309, 1177, 380, 51156], "temperature": 0.0, "avg_logprob": -0.07732796478271485, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.0026314102578908205}, {"id": 63, "seek": 30408, "start": 319.91999999999996, "end": 324.47999999999996, "text": " answer any distributed questions. So if you're writing a distributed program, you might ask a", "tokens": [51156, 1867, 604, 12631, 1651, 13, 407, 498, 291, 434, 3579, 257, 12631, 1461, 11, 291, 1062, 1029, 257, 51384], "temperature": 0.0, "avg_logprob": -0.07732796478271485, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.0026314102578908205}, {"id": 64, "seek": 30408, "start": 324.47999999999996, "end": 329.03999999999996, "text": " question like, is my program consistent in some sense? Or if I talk to different machines in the", "tokens": [51384, 1168, 411, 11, 307, 452, 1461, 8398, 294, 512, 2020, 30, 1610, 498, 286, 751, 281, 819, 8379, 294, 264, 51612], "temperature": 0.0, "avg_logprob": -0.07732796478271485, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.0026314102578908205}, {"id": 65, "seek": 30408, "start": 329.03999999999996, "end": 333.44, "text": " network, will they give me different answers and be confused? That's a question that distributed", "tokens": [51612, 3209, 11, 486, 436, 976, 385, 819, 6338, 293, 312, 9019, 30, 663, 311, 257, 1168, 300, 12631, 51832], "temperature": 0.0, "avg_logprob": -0.07732796478271485, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.0026314102578908205}, {"id": 66, "seek": 33344, "start": 333.44, "end": 338.08, "text": " programmers need to deal with. Here's another one. My state no longer fits on one computer. How do", "tokens": [50364, 41504, 643, 281, 2028, 365, 13, 1692, 311, 1071, 472, 13, 1222, 1785, 572, 2854, 9001, 322, 472, 3820, 13, 1012, 360, 50596], "temperature": 0.0, "avg_logprob": -0.09372138231992722, "compression_ratio": 1.6686567164179105, "no_speech_prob": 0.0012064233887940645}, {"id": 67, "seek": 33344, "start": 338.08, "end": 341.76, "text": " I partition it across multiple computers while getting the same answer out of my program?", "tokens": [50596, 286, 24808, 309, 2108, 3866, 10807, 1339, 1242, 264, 912, 1867, 484, 295, 452, 1461, 30, 50780], "temperature": 0.0, "avg_logprob": -0.09372138231992722, "compression_ratio": 1.6686567164179105, "no_speech_prob": 0.0012064233887940645}, {"id": 68, "seek": 33344, "start": 342.32, "end": 347.92, "text": " I want you, you compiler should figure that out for me. What failures can my system tolerate and", "tokens": [50808, 286, 528, 291, 11, 291, 31958, 820, 2573, 300, 484, 337, 385, 13, 708, 20774, 393, 452, 1185, 25773, 293, 51088], "temperature": 0.0, "avg_logprob": -0.09372138231992722, "compression_ratio": 1.6686567164179105, "no_speech_prob": 0.0012064233887940645}, {"id": 69, "seek": 33344, "start": 347.92, "end": 351.36, "text": " how many of them before it stops working the way that the spec declares it should?", "tokens": [51088, 577, 867, 295, 552, 949, 309, 10094, 1364, 264, 636, 300, 264, 1608, 979, 19415, 309, 820, 30, 51260], "temperature": 0.0, "avg_logprob": -0.09372138231992722, "compression_ratio": 1.6686567164179105, "no_speech_prob": 0.0012064233887940645}, {"id": 70, "seek": 33344, "start": 352.56, "end": 357.68, "text": " What data is going where around the world and who can see it? These are all questions distributed", "tokens": [51320, 708, 1412, 307, 516, 689, 926, 264, 1002, 293, 567, 393, 536, 309, 30, 1981, 366, 439, 1651, 12631, 51576], "temperature": 0.0, "avg_logprob": -0.09372138231992722, "compression_ratio": 1.6686567164179105, "no_speech_prob": 0.0012064233887940645}, {"id": 71, "seek": 33344, "start": 357.68, "end": 362.8, "text": " systems always have to answer. And then I have different objective functions. So I'd like to", "tokens": [51576, 3652, 1009, 362, 281, 1867, 13, 400, 550, 286, 362, 819, 10024, 6828, 13, 407, 286, 1116, 411, 281, 51832], "temperature": 0.0, "avg_logprob": -0.09372138231992722, "compression_ratio": 1.6686567164179105, "no_speech_prob": 0.0012064233887940645}, {"id": 72, "seek": 36280, "start": 362.8, "end": 368.24, "text": " optimize some days for maybe my dollar spend in the cloud, but I don't care about latency or maybe", "tokens": [50364, 19719, 512, 1708, 337, 1310, 452, 7241, 3496, 294, 264, 4588, 11, 457, 286, 500, 380, 1127, 466, 27043, 420, 1310, 50636], "temperature": 0.0, "avg_logprob": -0.10122432206806384, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.00047281524166464806}, {"id": 73, "seek": 36280, "start": 368.24, "end": 375.36, "text": " vice versa. Maybe I care about particular latency distribution. So I want the 99th percentile", "tokens": [50636, 11964, 25650, 13, 2704, 286, 1127, 466, 1729, 27043, 7316, 13, 407, 286, 528, 264, 11803, 392, 3043, 794, 50992], "temperature": 0.0, "avg_logprob": -0.10122432206806384, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.00047281524166464806}, {"id": 74, "seek": 36280, "start": 375.36, "end": 380.8, "text": " of my workload to achieve a certain latency versus the 95th or what have you. These will all", "tokens": [50992, 295, 452, 20139, 281, 4584, 257, 1629, 27043, 5717, 264, 13420, 392, 420, 437, 362, 291, 13, 1981, 486, 439, 51264], "temperature": 0.0, "avg_logprob": -0.10122432206806384, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.00047281524166464806}, {"id": 75, "seek": 36280, "start": 380.8, "end": 386.64, "text": " lead to different decisions about resource allocation and program structure and so on, right?", "tokens": [51264, 1477, 281, 819, 5327, 466, 7684, 27599, 293, 1461, 3877, 293, 370, 322, 11, 558, 30, 51556], "temperature": 0.0, "avg_logprob": -0.10122432206806384, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.00047281524166464806}, {"id": 76, "seek": 36280, "start": 386.64, "end": 391.44, "text": " And if you ask these questions of LLVM, that's the answer you get. It doesn't deal with any of", "tokens": [51556, 400, 498, 291, 1029, 613, 1651, 295, 441, 43, 53, 44, 11, 300, 311, 264, 1867, 291, 483, 13, 467, 1177, 380, 2028, 365, 604, 295, 51796], "temperature": 0.0, "avg_logprob": -0.10122432206806384, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.00047281524166464806}, {"id": 77, "seek": 39144, "start": 391.44, "end": 396.96, "text": " these issues. And that's kind of where we'd like to come in. We've written a vision paper a couple", "tokens": [50364, 613, 2663, 13, 400, 300, 311, 733, 295, 689, 321, 1116, 411, 281, 808, 294, 13, 492, 600, 3720, 257, 5201, 3035, 257, 1916, 50640], "temperature": 0.0, "avg_logprob": -0.073045075949976, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0007553163450211287}, {"id": 78, "seek": 39144, "start": 396.96, "end": 402.56, "text": " years ago that I can point you to and I won't go through all of it today. But the idea is to use", "tokens": [50640, 924, 2057, 300, 286, 393, 935, 291, 281, 293, 286, 1582, 380, 352, 807, 439, 295, 309, 965, 13, 583, 264, 1558, 307, 281, 764, 50920], "temperature": 0.0, "avg_logprob": -0.073045075949976, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0007553163450211287}, {"id": 79, "seek": 39144, "start": 402.56, "end": 408.64, "text": " database techniques and ideas to optimize in concert with LLVM. So LLVM is responsible for the", "tokens": [50920, 8149, 7512, 293, 3487, 281, 19719, 294, 8543, 365, 441, 43, 53, 44, 13, 407, 441, 43, 53, 44, 307, 6250, 337, 264, 51224], "temperature": 0.0, "avg_logprob": -0.073045075949976, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0007553163450211287}, {"id": 80, "seek": 39144, "start": 408.64, "end": 412.96, "text": " single node, but database techniques perhaps responsible for the messaging, the data movement", "tokens": [51224, 2167, 9984, 11, 457, 8149, 7512, 4317, 6250, 337, 264, 21812, 11, 264, 1412, 3963, 51440], "temperature": 0.0, "avg_logprob": -0.073045075949976, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0007553163450211287}, {"id": 81, "seek": 39144, "start": 412.96, "end": 417.92, "text": " that happens in a distributed program. So here's how we envision the hydro stack. Many programming", "tokens": [51440, 300, 2314, 294, 257, 12631, 1461, 13, 407, 510, 311, 577, 321, 24739, 264, 15435, 8630, 13, 5126, 9410, 51688], "temperature": 0.0, "avg_logprob": -0.073045075949976, "compression_ratio": 1.694736842105263, "no_speech_prob": 0.0007553163450211287}, {"id": 82, "seek": 41792, "start": 417.92, "end": 424.16, "text": " languages up top. Some techniques to translate them into an internal representation. We've", "tokens": [50364, 8650, 493, 1192, 13, 2188, 7512, 281, 13799, 552, 666, 364, 6920, 10290, 13, 492, 600, 50676], "temperature": 0.0, "avg_logprob": -0.06972611057865727, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.00348273734562099}, {"id": 83, "seek": 41792, "start": 424.16, "end": 428.64000000000004, "text": " got a little bit of initial work here. And then the internal representation should be some", "tokens": [50676, 658, 257, 707, 857, 295, 5883, 589, 510, 13, 400, 550, 264, 6920, 10290, 820, 312, 512, 50900], "temperature": 0.0, "avg_logprob": -0.06972611057865727, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.00348273734562099}, {"id": 84, "seek": 41792, "start": 429.92, "end": 435.44, "text": " formal spec that is global in some sense. So it doesn't worry yet about how many machines I have", "tokens": [50964, 9860, 1608, 300, 307, 4338, 294, 512, 2020, 13, 407, 309, 1177, 380, 3292, 1939, 466, 577, 867, 8379, 286, 362, 51240], "temperature": 0.0, "avg_logprob": -0.06972611057865727, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.00348273734562099}, {"id": 85, "seek": 41792, "start": 435.44, "end": 441.36, "text": " or what the machines can do. It's just a formalized specification of what you wrote in a perhaps", "tokens": [51240, 420, 437, 264, 8379, 393, 360, 13, 467, 311, 445, 257, 9860, 1602, 31256, 295, 437, 291, 4114, 294, 257, 4317, 51536], "temperature": 0.0, "avg_logprob": -0.06972611057865727, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.00348273734562099}, {"id": 86, "seek": 41792, "start": 441.36, "end": 445.36, "text": " imperative language. Okay, so it's kind of machine oblivious. Maybe it's a logic. Maybe it's an", "tokens": [51536, 32490, 2856, 13, 1033, 11, 370, 309, 311, 733, 295, 3479, 47039, 851, 13, 2704, 309, 311, 257, 9952, 13, 2704, 309, 311, 364, 51736], "temperature": 0.0, "avg_logprob": -0.06972611057865727, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.00348273734562099}, {"id": 87, "seek": 44536, "start": 445.36, "end": 449.84000000000003, "text": " algebra. Things that are in red are work in progress. Things that are in green kind of work", "tokens": [50364, 21989, 13, 9514, 300, 366, 294, 2182, 366, 589, 294, 4205, 13, 9514, 300, 366, 294, 3092, 733, 295, 589, 50588], "temperature": 0.0, "avg_logprob": -0.10179333416920788, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0003799651749432087}, {"id": 88, "seek": 44536, "start": 449.84000000000003, "end": 456.08000000000004, "text": " at this point. And then from there we want to build a compiler and we're working with Max", "tokens": [50588, 412, 341, 935, 13, 400, 550, 490, 456, 321, 528, 281, 1322, 257, 31958, 293, 321, 434, 1364, 365, 7402, 50900], "temperature": 0.0, "avg_logprob": -0.10179333416920788, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0003799651749432087}, {"id": 89, "seek": 44536, "start": 456.08000000000004, "end": 462.08000000000004, "text": " on using eGraphs for that compiler to translate down into a per node physical algebra. So every", "tokens": [50900, 322, 1228, 308, 38, 2662, 82, 337, 300, 31958, 281, 13799, 760, 666, 257, 680, 9984, 4001, 21989, 13, 407, 633, 51200], "temperature": 0.0, "avg_logprob": -0.10179333416920788, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0003799651749432087}, {"id": 90, "seek": 44536, "start": 462.08000000000004, "end": 466.72, "text": " machine would run its own little program. Those programs communicate with each other very much", "tokens": [51200, 3479, 576, 1190, 1080, 1065, 707, 1461, 13, 3950, 4268, 7890, 365, 1184, 661, 588, 709, 51432], "temperature": 0.0, "avg_logprob": -0.10179333416920788, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0003799651749432087}, {"id": 91, "seek": 44536, "start": 466.72, "end": 471.28000000000003, "text": " like a parallel query plan is a bunch of individual query plans running on individual machines talking", "tokens": [51432, 411, 257, 8952, 14581, 1393, 307, 257, 3840, 295, 2609, 14581, 5482, 2614, 322, 2609, 8379, 1417, 51660], "temperature": 0.0, "avg_logprob": -0.10179333416920788, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0003799651749432087}, {"id": 92, "seek": 47128, "start": 471.28, "end": 476.64, "text": " to each other over a network. Okay, so this is a sort of per node physical algebra because it's", "tokens": [50364, 281, 1184, 661, 670, 257, 3209, 13, 1033, 11, 370, 341, 307, 257, 1333, 295, 680, 9984, 4001, 21989, 570, 309, 311, 50632], "temperature": 0.0, "avg_logprob": -0.08534398354774664, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0005357385380193591}, {"id": 93, "seek": 47128, "start": 476.64, "end": 481.84, "text": " actually doing stuff. We've implemented this in Rust. It's very fast and I'll show you some of", "tokens": [50632, 767, 884, 1507, 13, 492, 600, 12270, 341, 294, 34952, 13, 467, 311, 588, 2370, 293, 286, 603, 855, 291, 512, 295, 50892], "temperature": 0.0, "avg_logprob": -0.08534398354774664, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0005357385380193591}, {"id": 94, "seek": 47128, "start": 481.84, "end": 486.64, "text": " this today. So that's kind of what we envision as how this is all going to work. At the bottom", "tokens": [50892, 341, 965, 13, 407, 300, 311, 733, 295, 437, 321, 24739, 382, 577, 341, 307, 439, 516, 281, 589, 13, 1711, 264, 2767, 51132], "temperature": 0.0, "avg_logprob": -0.08534398354774664, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0005357385380193591}, {"id": 95, "seek": 47128, "start": 486.64, "end": 490.55999999999995, "text": " there's something that's deploying this on machines and deciding how many machines and how few over", "tokens": [51132, 456, 311, 746, 300, 311, 34198, 341, 322, 8379, 293, 17990, 577, 867, 8379, 293, 577, 1326, 670, 51328], "temperature": 0.0, "avg_logprob": -0.08534398354774664, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0005357385380193591}, {"id": 96, "seek": 47128, "start": 492.4, "end": 496.0, "text": " time. Okay, so some of the topics I want to talk about today we're going to focus on this piece of", "tokens": [51420, 565, 13, 1033, 11, 370, 512, 295, 264, 8378, 286, 528, 281, 751, 466, 965, 321, 434, 516, 281, 1879, 322, 341, 2522, 295, 51600], "temperature": 0.0, "avg_logprob": -0.08534398354774664, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.0005357385380193591}, {"id": 97, "seek": 49600, "start": 496.0, "end": 502.64, "text": " the stack. Things in red are work in progress. Very much more questions than answers for sure.", "tokens": [50364, 264, 8630, 13, 9514, 294, 2182, 366, 589, 294, 4205, 13, 4372, 709, 544, 1651, 813, 6338, 337, 988, 13, 50696], "temperature": 0.0, "avg_logprob": -0.08192444791888247, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0014102280838415027}, {"id": 98, "seek": 49600, "start": 502.64, "end": 507.28, "text": " So how do we take code and automatically replicate it along with its associated state or data", "tokens": [50696, 407, 577, 360, 321, 747, 3089, 293, 6772, 25356, 309, 2051, 365, 1080, 6615, 1785, 420, 1412, 50928], "temperature": 0.0, "avg_logprob": -0.08192444791888247, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0014102280838415027}, {"id": 99, "seek": 49600, "start": 507.92, "end": 511.84, "text": " while ensuring that the program continues to produce the same outcomes as it would have on a", "tokens": [50960, 1339, 16882, 300, 264, 1461, 6515, 281, 5258, 264, 912, 10070, 382, 309, 576, 362, 322, 257, 51156], "temperature": 0.0, "avg_logprob": -0.08192444791888247, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0014102280838415027}, {"id": 100, "seek": 49600, "start": 511.84, "end": 516.56, "text": " single machine? So I'm particularly interested in doing this in cases where the replication comes", "tokens": [51156, 2167, 3479, 30, 407, 286, 478, 4098, 3102, 294, 884, 341, 294, 3331, 689, 264, 39911, 1487, 51392], "temperature": 0.0, "avg_logprob": -0.08192444791888247, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0014102280838415027}, {"id": 101, "seek": 49600, "start": 516.56, "end": 521.84, "text": " for free and the individual machines don't have to coordinate with each other in a technical sense", "tokens": [51392, 337, 1737, 293, 264, 2609, 8379, 500, 380, 362, 281, 15670, 365, 1184, 661, 294, 257, 6191, 2020, 51656], "temperature": 0.0, "avg_logprob": -0.08192444791888247, "compression_ratio": 1.6655052264808363, "no_speech_prob": 0.0014102280838415027}, {"id": 102, "seek": 52184, "start": 521.84, "end": 526.64, "text": " I'll talk about. But we'd like to avoid replication when we can. We'll call that free replication and", "tokens": [50364, 286, 603, 751, 466, 13, 583, 321, 1116, 411, 281, 5042, 39911, 562, 321, 393, 13, 492, 603, 818, 300, 1737, 39911, 293, 50604], "temperature": 0.0, "avg_logprob": -0.10601223748305748, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.009410831145942211}, {"id": 103, "seek": 52184, "start": 526.64, "end": 531.84, "text": " this is the domain of the calm theorem which you may have heard of and I will review. Unfortunately,", "tokens": [50604, 341, 307, 264, 9274, 295, 264, 7151, 20904, 597, 291, 815, 362, 2198, 295, 293, 286, 486, 3131, 13, 8590, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10601223748305748, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.009410831145942211}, {"id": 104, "seek": 52184, "start": 531.84, "end": 539.6800000000001, "text": " the calm theorem was done in a very particular framework for the proofs. It's not at all clear", "tokens": [50864, 264, 7151, 20904, 390, 1096, 294, 257, 588, 1729, 8388, 337, 264, 8177, 82, 13, 467, 311, 406, 412, 439, 1850, 51256], "temperature": 0.0, "avg_logprob": -0.10601223748305748, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.009410831145942211}, {"id": 105, "seek": 52184, "start": 539.6800000000001, "end": 544.0, "text": " how it applies outside that framework and so what we'd really like is a more algebraic notion of the", "tokens": [51256, 577, 309, 13165, 2380, 300, 8388, 293, 370, 437, 321, 1116, 534, 411, 307, 257, 544, 21989, 299, 10710, 295, 264, 51472], "temperature": 0.0, "avg_logprob": -0.10601223748305748, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.009410831145942211}, {"id": 106, "seek": 52184, "start": 544.0, "end": 548.4000000000001, "text": " calm theorem which is something that Connor's working on and after the talk if you're interested", "tokens": [51472, 7151, 20904, 597, 307, 746, 300, 33133, 311, 1364, 322, 293, 934, 264, 751, 498, 291, 434, 3102, 51692], "temperature": 0.0, "avg_logprob": -0.10601223748305748, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.009410831145942211}, {"id": 107, "seek": 54840, "start": 548.4, "end": 553.4399999999999, "text": " come find Connor and or me to talk about that. Another topic that Connor's going to talk about", "tokens": [50364, 808, 915, 33133, 293, 420, 385, 281, 751, 466, 300, 13, 3996, 4829, 300, 33133, 311, 516, 281, 751, 466, 50616], "temperature": 0.0, "avg_logprob": -0.07532351105301469, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.0014547883765771985}, {"id": 108, "seek": 54840, "start": 553.4399999999999, "end": 558.3199999999999, "text": " today is termination detection and again ideally termination detection where I can decide it locally", "tokens": [50616, 965, 307, 1433, 2486, 17784, 293, 797, 22915, 1433, 2486, 17784, 689, 286, 393, 4536, 309, 16143, 50860], "temperature": 0.0, "avg_logprob": -0.07532351105301469, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.0014547883765771985}, {"id": 109, "seek": 54840, "start": 558.3199999999999, "end": 563.1999999999999, "text": " for free without asking anyone else. So how do I know in a streaming program that it's done", "tokens": [50860, 337, 1737, 1553, 3365, 2878, 1646, 13, 407, 577, 360, 286, 458, 294, 257, 11791, 1461, 300, 309, 311, 1096, 51104], "temperature": 0.0, "avg_logprob": -0.07532351105301469, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.0014547883765771985}, {"id": 110, "seek": 54840, "start": 563.76, "end": 569.1999999999999, "text": " when there's other agents in the world? So we're going to talk about how to do that with threshold", "tokens": [51132, 562, 456, 311, 661, 12554, 294, 264, 1002, 30, 407, 321, 434, 516, 281, 751, 466, 577, 281, 360, 300, 365, 14678, 51404], "temperature": 0.0, "avg_logprob": -0.07532351105301469, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.0014547883765771985}, {"id": 111, "seek": 54840, "start": 569.1999999999999, "end": 573.92, "text": " morphisms but Connor's got ideas about more general notions of equivalence classes that may", "tokens": [51404, 25778, 13539, 457, 33133, 311, 658, 3487, 466, 544, 2674, 35799, 295, 9052, 655, 5359, 300, 815, 51640], "temperature": 0.0, "avg_logprob": -0.07532351105301469, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.0014547883765771985}, {"id": 112, "seek": 57392, "start": 573.92, "end": 578.0, "text": " allow termination in more settings. So he'll give you a flavor of this work in progress.", "tokens": [50364, 2089, 1433, 2486, 294, 544, 6257, 13, 407, 415, 603, 976, 291, 257, 6813, 295, 341, 589, 294, 4205, 13, 50568], "temperature": 0.0, "avg_logprob": -0.09446079569651668, "compression_ratio": 1.746177370030581, "no_speech_prob": 0.003944630268961191}, {"id": 113, "seek": 57392, "start": 579.12, "end": 582.88, "text": " The third piece we may or may not have time for today but my student David Chu has been working", "tokens": [50624, 440, 2636, 2522, 321, 815, 420, 815, 406, 362, 565, 337, 965, 457, 452, 3107, 4389, 25585, 575, 668, 1364, 50812], "temperature": 0.0, "avg_logprob": -0.09446079569651668, "compression_ratio": 1.746177370030581, "no_speech_prob": 0.003944630268961191}, {"id": 114, "seek": 57392, "start": 582.88, "end": 587.5999999999999, "text": " on this how do you take a program and partition the state of the program across machines if the", "tokens": [50812, 322, 341, 577, 360, 291, 747, 257, 1461, 293, 24808, 264, 1785, 295, 264, 1461, 2108, 8379, 498, 264, 51048], "temperature": 0.0, "avg_logprob": -0.09446079569651668, "compression_ratio": 1.746177370030581, "no_speech_prob": 0.003944630268961191}, {"id": 115, "seek": 57392, "start": 587.5999999999999, "end": 592.24, "text": " state doesn't fit on one machine. So you really need to partition the code and the data. This is", "tokens": [51048, 1785, 1177, 380, 3318, 322, 472, 3479, 13, 407, 291, 534, 643, 281, 24808, 264, 3089, 293, 264, 1412, 13, 639, 307, 51280], "temperature": 0.0, "avg_logprob": -0.09446079569651668, "compression_ratio": 1.746177370030581, "no_speech_prob": 0.003944630268961191}, {"id": 116, "seek": 57392, "start": 592.24, "end": 597.4399999999999, "text": " very much like traditional shared nothing parallel databases if you like but we want to do this to", "tokens": [51280, 588, 709, 411, 5164, 5507, 1825, 8952, 22380, 498, 291, 411, 457, 321, 528, 281, 360, 341, 281, 51540], "temperature": 0.0, "avg_logprob": -0.09446079569651668, "compression_ratio": 1.746177370030581, "no_speech_prob": 0.003944630268961191}, {"id": 117, "seek": 57392, "start": 597.4399999999999, "end": 602.88, "text": " full and rather complex data log programs and so we've got an implementation of Paxos which is", "tokens": [51540, 1577, 293, 2831, 3997, 1412, 3565, 4268, 293, 370, 321, 600, 658, 364, 11420, 295, 430, 2797, 329, 597, 307, 51812], "temperature": 0.0, "avg_logprob": -0.09446079569651668, "compression_ratio": 1.746177370030581, "no_speech_prob": 0.003944630268961191}, {"id": 118, "seek": 60288, "start": 603.36, "end": 607.52, "text": " number of lines of data log where David's able to do some of this automatic partitioning.", "tokens": [50388, 1230, 295, 3876, 295, 1412, 3565, 689, 4389, 311, 1075, 281, 360, 512, 295, 341, 12509, 24808, 278, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09120451079474555, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0002453424967825413}, {"id": 119, "seek": 60288, "start": 608.32, "end": 612.4, "text": " Functional dependencies have a role to play here and it would be nice to integrate those into an", "tokens": [50636, 11166, 41048, 36606, 362, 257, 3090, 281, 862, 510, 293, 309, 576, 312, 1481, 281, 13365, 729, 666, 364, 50840], "temperature": 0.0, "avg_logprob": -0.09120451079474555, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0002453424967825413}, {"id": 120, "seek": 60288, "start": 612.4, "end": 618.48, "text": " algebraic frame as well. And of course all this has to go fast and ideally as fast as", "tokens": [50840, 21989, 299, 3920, 382, 731, 13, 400, 295, 1164, 439, 341, 575, 281, 352, 2370, 293, 22915, 382, 2370, 382, 51144], "temperature": 0.0, "avg_logprob": -0.09120451079474555, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0002453424967825413}, {"id": 121, "seek": 60288, "start": 618.48, "end": 623.52, "text": " handwritten C++. I'm really previous iterations of my work we settled for interpreters and just", "tokens": [51144, 1011, 26859, 383, 25472, 13, 286, 478, 534, 3894, 36540, 295, 452, 589, 321, 14819, 337, 17489, 1559, 293, 445, 51396], "temperature": 0.0, "avg_logprob": -0.09120451079474555, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0002453424967825413}, {"id": 122, "seek": 60288, "start": 623.52, "end": 628.32, "text": " showing things were possible. Now we'd like to convince the systems people that things will be", "tokens": [51396, 4099, 721, 645, 1944, 13, 823, 321, 1116, 411, 281, 13447, 264, 3652, 561, 300, 721, 486, 312, 51636], "temperature": 0.0, "avg_logprob": -0.09120451079474555, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0002453424967825413}, {"id": 123, "seek": 62832, "start": 628.32, "end": 634.5600000000001, "text": " as fast as they want them to be. So is this business just pie in the sky?", "tokens": [50364, 382, 2370, 382, 436, 528, 552, 281, 312, 13, 407, 307, 341, 1606, 445, 1730, 294, 264, 5443, 30, 50676], "temperature": 0.0, "avg_logprob": -0.11271424578805254, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0022515400778502226}, {"id": 124, "seek": 62832, "start": 636.08, "end": 642.72, "text": " Let's see. All right so we're building on an obsession of some 10 to 12 years that I've had", "tokens": [50752, 961, 311, 536, 13, 1057, 558, 370, 321, 434, 2390, 322, 364, 30521, 295, 512, 1266, 281, 2272, 924, 300, 286, 600, 632, 51084], "temperature": 0.0, "avg_logprob": -0.11271424578805254, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0022515400778502226}, {"id": 125, "seek": 62832, "start": 642.72, "end": 647.6, "text": " now in my third group of grad students working in this area. So the initial batch of grad students", "tokens": [51084, 586, 294, 452, 2636, 1594, 295, 2771, 1731, 1364, 294, 341, 1859, 13, 407, 264, 5883, 15245, 295, 2771, 1731, 51328], "temperature": 0.0, "avg_logprob": -0.11271424578805254, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0022515400778502226}, {"id": 126, "seek": 62832, "start": 647.6, "end": 653.0400000000001, "text": " was doing formalisms. So we have this very nice logic called Daedalus which is a subset of data", "tokens": [51328, 390, 884, 9860, 13539, 13, 407, 321, 362, 341, 588, 1481, 9952, 1219, 3933, 292, 304, 301, 597, 307, 257, 25993, 295, 1412, 51600], "temperature": 0.0, "avg_logprob": -0.11271424578805254, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0022515400778502226}, {"id": 127, "seek": 62832, "start": 653.0400000000001, "end": 658.0, "text": " log neg actually that allows you to talk about time and space in a particular way and it's got", "tokens": [51600, 3565, 2485, 767, 300, 4045, 291, 281, 751, 466, 565, 293, 1901, 294, 257, 1729, 636, 293, 309, 311, 658, 51848], "temperature": 0.0, "avg_logprob": -0.11271424578805254, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.0022515400778502226}, {"id": 128, "seek": 65800, "start": 658.0, "end": 663.68, "text": " a nice model theory. And so that works all there and we can use that as a basis for you know", "tokens": [50364, 257, 1481, 2316, 5261, 13, 400, 370, 300, 1985, 439, 456, 293, 321, 393, 764, 300, 382, 257, 5143, 337, 291, 458, 50648], "temperature": 0.0, "avg_logprob": -0.1302617768109855, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0004441077180672437}, {"id": 129, "seek": 65800, "start": 663.68, "end": 670.0, "text": " our semantics to start which is nice. And then there was this lovely work that Tom Amalut did at", "tokens": [50648, 527, 4361, 45298, 281, 722, 597, 307, 1481, 13, 400, 550, 456, 390, 341, 7496, 589, 300, 5041, 2012, 304, 325, 630, 412, 50964], "temperature": 0.0, "avg_logprob": -0.1302617768109855, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0004441077180672437}, {"id": 130, "seek": 65800, "start": 670.0, "end": 675.28, "text": " Hasselt the column theorem which was a conjecture I had and he and colleagues went ahead and proved", "tokens": [50964, 32711, 2018, 264, 7738, 20904, 597, 390, 257, 416, 1020, 540, 286, 632, 293, 415, 293, 7734, 1437, 2286, 293, 14617, 51228], "temperature": 0.0, "avg_logprob": -0.1302617768109855, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0004441077180672437}, {"id": 131, "seek": 65800, "start": 675.92, "end": 680.72, "text": " that talks about how things are monotone in some certain sense. Then you can do this free", "tokens": [51260, 300, 6686, 466, 577, 721, 366, 1108, 310, 546, 294, 512, 1629, 2020, 13, 1396, 291, 393, 360, 341, 1737, 51500], "temperature": 0.0, "avg_logprob": -0.1302617768109855, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0004441077180672437}, {"id": 132, "seek": 65800, "start": 681.84, "end": 686.64, "text": " replication. So you don't need to do coordination to get replica consistency. So I will talk about", "tokens": [51556, 39911, 13, 407, 291, 500, 380, 643, 281, 360, 21252, 281, 483, 35456, 14416, 13, 407, 286, 486, 751, 466, 51796], "temperature": 0.0, "avg_logprob": -0.1302617768109855, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.0004441077180672437}, {"id": 133, "seek": 68664, "start": 686.64, "end": 691.1999999999999, "text": " the column theorem today to give a review. And then we actually built out a language. It was", "tokens": [50364, 264, 7738, 20904, 965, 281, 976, 257, 3131, 13, 400, 550, 321, 767, 3094, 484, 257, 2856, 13, 467, 390, 50592], "temperature": 0.0, "avg_logprob": -0.07520530867750627, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.00025313335936516523}, {"id": 134, "seek": 68664, "start": 691.1999999999999, "end": 696.24, "text": " slow. It was interpreted. It was written in Ruby but it integrated lattices into a data log like", "tokens": [50592, 2964, 13, 467, 390, 26749, 13, 467, 390, 3720, 294, 19907, 457, 309, 10919, 29025, 1473, 666, 257, 1412, 3565, 411, 50844], "temperature": 0.0, "avg_logprob": -0.07520530867750627, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.00025313335936516523}, {"id": 135, "seek": 68664, "start": 696.24, "end": 700.56, "text": " language and we were able to show that you can get stratified negation and aggregation. You can", "tokens": [50844, 2856, 293, 321, 645, 1075, 281, 855, 300, 291, 393, 483, 23674, 2587, 2485, 399, 293, 16743, 399, 13, 509, 393, 51060], "temperature": 0.0, "avg_logprob": -0.07520530867750627, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.00025313335936516523}, {"id": 136, "seek": 68664, "start": 700.56, "end": 705.52, "text": " get morphisms on your lattices to allow you to do semi-naive evaluation even with the lattices.", "tokens": [51060, 483, 25778, 13539, 322, 428, 29025, 1473, 281, 2089, 291, 281, 360, 12909, 12, 629, 488, 13344, 754, 365, 264, 29025, 1473, 13, 51308], "temperature": 0.0, "avg_logprob": -0.07520530867750627, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.00025313335936516523}, {"id": 137, "seek": 68664, "start": 705.52, "end": 709.92, "text": " So it's really actually rather a nice mixture of algebra and logic. None of this was formally", "tokens": [51308, 407, 309, 311, 534, 767, 2831, 257, 1481, 9925, 295, 21989, 293, 9952, 13, 14492, 295, 341, 390, 25983, 51528], "temperature": 0.0, "avg_logprob": -0.07520530867750627, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.00025313335936516523}, {"id": 138, "seek": 68664, "start": 709.92, "end": 714.0, "text": " proved but it was I think one of the earlier systems to observe that you could build this.", "tokens": [51528, 14617, 457, 309, 390, 286, 519, 472, 295, 264, 3071, 3652, 281, 11441, 300, 291, 727, 1322, 341, 13, 51732], "temperature": 0.0, "avg_logprob": -0.07520530867750627, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.00025313335936516523}, {"id": 139, "seek": 71400, "start": 714.08, "end": 718.48, "text": " That was pretty cool. And that's Neil Conway's thesis work. So we have this as a basis. This kind", "tokens": [50368, 663, 390, 1238, 1627, 13, 400, 300, 311, 18615, 2656, 676, 311, 22288, 589, 13, 407, 321, 362, 341, 382, 257, 5143, 13, 639, 733, 50588], "temperature": 0.0, "avg_logprob": -0.09440488969126055, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0007095502223819494}, {"id": 140, "seek": 71400, "start": 718.48, "end": 723.6, "text": " of ground zero for my team. And then what happened is I had a batch of students that didn't want to", "tokens": [50588, 295, 2727, 4018, 337, 452, 1469, 13, 400, 550, 437, 2011, 307, 286, 632, 257, 15245, 295, 1731, 300, 994, 380, 528, 281, 50844], "temperature": 0.0, "avg_logprob": -0.09440488969126055, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0007095502223819494}, {"id": 141, "seek": 71400, "start": 723.6, "end": 728.64, "text": " do languages or theory. So they just built stuff in the spirit of these ideas. And I'm not going", "tokens": [50844, 360, 8650, 420, 5261, 13, 407, 436, 445, 3094, 1507, 294, 264, 3797, 295, 613, 3487, 13, 400, 286, 478, 406, 516, 51096], "temperature": 0.0, "avg_logprob": -0.09440488969126055, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0007095502223819494}, {"id": 142, "seek": 71400, "start": 728.64, "end": 733.92, "text": " to go through all this but it's things like functions as a service and protocols and testing", "tokens": [51096, 281, 352, 807, 439, 341, 457, 309, 311, 721, 411, 6828, 382, 257, 2643, 293, 20618, 293, 4997, 51360], "temperature": 0.0, "avg_logprob": -0.09440488969126055, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0007095502223819494}, {"id": 143, "seek": 71400, "start": 733.92, "end": 739.44, "text": " and provenance. It's cool stuff. What I do want to focus on is one of those projects which was a", "tokens": [51360, 293, 12785, 719, 13, 467, 311, 1627, 1507, 13, 708, 286, 360, 528, 281, 1879, 322, 307, 472, 295, 729, 4455, 597, 390, 257, 51636], "temperature": 0.0, "avg_logprob": -0.09440488969126055, "compression_ratio": 1.6462585034013606, "no_speech_prob": 0.0007095502223819494}, {"id": 144, "seek": 73944, "start": 739.5200000000001, "end": 744.8000000000001, "text": " key value store. Key value store is just like a hash table. It's a database where you look", "tokens": [50368, 2141, 2158, 3531, 13, 12759, 2158, 3531, 307, 445, 411, 257, 22019, 3199, 13, 467, 311, 257, 8149, 689, 291, 574, 50632], "temperature": 0.0, "avg_logprob": -0.0910693175501103, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.004198412410914898}, {"id": 145, "seek": 73944, "start": 744.8000000000001, "end": 749.0400000000001, "text": " things up by key and you get a value. So you can think of it as a distributed hash table.", "tokens": [50632, 721, 493, 538, 2141, 293, 291, 483, 257, 2158, 13, 407, 291, 393, 519, 295, 309, 382, 257, 12631, 22019, 3199, 13, 50844], "temperature": 0.0, "avg_logprob": -0.0910693175501103, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.004198412410914898}, {"id": 146, "seek": 73944, "start": 749.6800000000001, "end": 753.5200000000001, "text": " These are like the Petri dishes of distributed systems. You're basically saying I have a memory.", "tokens": [50876, 1981, 366, 411, 264, 10472, 470, 10814, 295, 12631, 3652, 13, 509, 434, 1936, 1566, 286, 362, 257, 4675, 13, 51068], "temperature": 0.0, "avg_logprob": -0.0910693175501103, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.004198412410914898}, {"id": 147, "seek": 73944, "start": 753.5200000000001, "end": 757.84, "text": " It's distributed across many computers. It may be replicated. It may be partitioned. But that's", "tokens": [51068, 467, 311, 12631, 2108, 867, 10807, 13, 467, 815, 312, 46365, 13, 467, 815, 312, 24808, 292, 13, 583, 300, 311, 51284], "temperature": 0.0, "avg_logprob": -0.0910693175501103, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.004198412410914898}, {"id": 148, "seek": 73944, "start": 757.84, "end": 763.84, "text": " what I have. It's just registers with values, keys with values. So there's no algorithms per se.", "tokens": [51284, 437, 286, 362, 13, 467, 311, 445, 38351, 365, 4190, 11, 9317, 365, 4190, 13, 407, 456, 311, 572, 14642, 680, 369, 13, 51584], "temperature": 0.0, "avg_logprob": -0.0910693175501103, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.004198412410914898}, {"id": 149, "seek": 73944, "start": 763.84, "end": 768.08, "text": " It's all just kind of like focusing on these semantic issues about replication and partitioning.", "tokens": [51584, 467, 311, 439, 445, 733, 295, 411, 8416, 322, 613, 47982, 2663, 466, 39911, 293, 24808, 278, 13, 51796], "temperature": 0.0, "avg_logprob": -0.0910693175501103, "compression_ratio": 1.8651315789473684, "no_speech_prob": 0.004198412410914898}, {"id": 150, "seek": 76808, "start": 769.0400000000001, "end": 773.2800000000001, "text": " But the idea that was behind the Anna key value store that we built was everything's a", "tokens": [50412, 583, 264, 1558, 300, 390, 2261, 264, 12899, 2141, 2158, 3531, 300, 321, 3094, 390, 1203, 311, 257, 50624], "temperature": 0.0, "avg_logprob": -0.11386024212015086, "compression_ratio": 1.8856209150326797, "no_speech_prob": 8.219766459660605e-05}, {"id": 151, "seek": 76808, "start": 773.2800000000001, "end": 778.72, "text": " semi-ladys. And because everything's a semi-ladys and therefore associative, commutative, and item", "tokens": [50624, 12909, 12, 9290, 749, 13, 400, 570, 1203, 311, 257, 12909, 12, 9290, 749, 293, 4412, 4180, 1166, 11, 800, 325, 1166, 11, 293, 3174, 50896], "temperature": 0.0, "avg_logprob": -0.11386024212015086, "compression_ratio": 1.8856209150326797, "no_speech_prob": 8.219766459660605e-05}, {"id": 152, "seek": 76808, "start": 778.72, "end": 783.2800000000001, "text": " potent, messages in the network can be replicated. They can be interleaved. They can be reordered and", "tokens": [50896, 27073, 11, 7897, 294, 264, 3209, 393, 312, 46365, 13, 814, 393, 312, 728, 306, 12865, 13, 814, 393, 312, 319, 765, 4073, 293, 51124], "temperature": 0.0, "avg_logprob": -0.11386024212015086, "compression_ratio": 1.8856209150326797, "no_speech_prob": 8.219766459660605e-05}, {"id": 153, "seek": 76808, "start": 783.2800000000001, "end": 788.08, "text": " everything will be fine. So if you design with semi-ladys from the bottom up, you can build a", "tokens": [51124, 1203, 486, 312, 2489, 13, 407, 498, 291, 1715, 365, 12909, 12, 9290, 749, 490, 264, 2767, 493, 11, 291, 393, 1322, 257, 51364], "temperature": 0.0, "avg_logprob": -0.11386024212015086, "compression_ratio": 1.8856209150326797, "no_speech_prob": 8.219766459660605e-05}, {"id": 154, "seek": 76808, "start": 788.08, "end": 792.72, "text": " system that does no coordination. So it's fully monotonic, which means everything can be replicated", "tokens": [51364, 1185, 300, 775, 572, 21252, 13, 407, 309, 311, 4498, 1108, 310, 11630, 11, 597, 1355, 1203, 393, 312, 46365, 51596], "temperature": 0.0, "avg_logprob": -0.11386024212015086, "compression_ratio": 1.8856209150326797, "no_speech_prob": 8.219766459660605e-05}, {"id": 155, "seek": 76808, "start": 792.72, "end": 797.6, "text": " as much or as little as you like. Across the globe, between disks and memory, you can replicate", "tokens": [51596, 382, 709, 420, 382, 707, 382, 291, 411, 13, 34527, 264, 15371, 11, 1296, 41617, 293, 4675, 11, 291, 393, 25356, 51840], "temperature": 0.0, "avg_logprob": -0.11386024212015086, "compression_ratio": 1.8856209150326797, "no_speech_prob": 8.219766459660605e-05}, {"id": 156, "seek": 79760, "start": 797.6, "end": 802.72, "text": " lots of ways. You can do updates anywhere. So multiple updates to the same key can be happening", "tokens": [50364, 3195, 295, 2098, 13, 509, 393, 360, 9205, 4992, 13, 407, 3866, 9205, 281, 264, 912, 2141, 393, 312, 2737, 50620], "temperature": 0.0, "avg_logprob": -0.08579752808910306, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.0005032525514252484}, {"id": 157, "seek": 79760, "start": 802.72, "end": 808.8000000000001, "text": " concurrently in different places at the same time. And they are merged by the lattice lazily via gossip.", "tokens": [50620, 37702, 356, 294, 819, 3190, 412, 264, 912, 565, 13, 400, 436, 366, 36427, 538, 264, 34011, 19320, 953, 5766, 31788, 13, 50924], "temperature": 0.0, "avg_logprob": -0.08579752808910306, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.0005032525514252484}, {"id": 158, "seek": 79760, "start": 808.8000000000001, "end": 812.88, "text": " And so you build this thing with no concurrency control whatsoever. So there's no locks in the", "tokens": [50924, 400, 370, 291, 1322, 341, 551, 365, 572, 23702, 10457, 1969, 17076, 13, 407, 456, 311, 572, 20703, 294, 264, 51128], "temperature": 0.0, "avg_logprob": -0.08579752808910306, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.0005032525514252484}, {"id": 159, "seek": 79760, "start": 812.88, "end": 817.6, "text": " system. There's no calls to atomic instructions on processors. There's certainly no Paxos protocols", "tokens": [51128, 1185, 13, 821, 311, 572, 5498, 281, 22275, 9415, 322, 27751, 13, 821, 311, 3297, 572, 430, 2797, 329, 20618, 51364], "temperature": 0.0, "avg_logprob": -0.08579752808910306, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.0005032525514252484}, {"id": 160, "seek": 79760, "start": 817.6, "end": 822.5600000000001, "text": " or anything like that. It's just put, get, and gossip. It's really a simple piece of software.", "tokens": [51364, 420, 1340, 411, 300, 13, 467, 311, 445, 829, 11, 483, 11, 293, 31788, 13, 467, 311, 534, 257, 2199, 2522, 295, 4722, 13, 51612], "temperature": 0.0, "avg_logprob": -0.08579752808910306, "compression_ratio": 1.6838487972508591, "no_speech_prob": 0.0005032525514252484}, {"id": 161, "seek": 82256, "start": 823.52, "end": 827.4399999999999, "text": " And so Chenggang Wu, the student who led this, won the dissertation award. I think it was very", "tokens": [50412, 400, 370, 24363, 19619, 17287, 11, 264, 3107, 567, 4684, 341, 11, 1582, 264, 39555, 7130, 13, 286, 519, 309, 390, 588, 50608], "temperature": 0.0, "avg_logprob": -0.0783121676845405, "compression_ratio": 1.7244582043343653, "no_speech_prob": 0.0009695274056866765}, {"id": 162, "seek": 82256, "start": 827.4399999999999, "end": 832.3199999999999, "text": " well deserved because this system was really elegant and really fast. So to give you a sense", "tokens": [50608, 731, 27964, 570, 341, 1185, 390, 534, 21117, 293, 534, 2370, 13, 407, 281, 976, 291, 257, 2020, 50852], "temperature": 0.0, "avg_logprob": -0.0783121676845405, "compression_ratio": 1.7244582043343653, "no_speech_prob": 0.0009695274056866765}, {"id": 163, "seek": 82256, "start": 832.3199999999999, "end": 836.7199999999999, "text": " of kind of the lattices that he uses, he took from the literature a variety of these kind of", "tokens": [50852, 295, 733, 295, 264, 29025, 1473, 300, 415, 4960, 11, 415, 1890, 490, 264, 10394, 257, 5673, 295, 613, 733, 295, 51072], "temperature": 0.0, "avg_logprob": -0.0783121676845405, "compression_ratio": 1.7244582043343653, "no_speech_prob": 0.0009695274056866765}, {"id": 164, "seek": 82256, "start": 836.7199999999999, "end": 840.8, "text": " consistency models that people talk about in distributed systems. And he showed how you can", "tokens": [51072, 14416, 5245, 300, 561, 751, 466, 294, 12631, 3652, 13, 400, 415, 4712, 577, 291, 393, 51276], "temperature": 0.0, "avg_logprob": -0.0783121676845405, "compression_ratio": 1.7244582043343653, "no_speech_prob": 0.0009695274056866765}, {"id": 165, "seek": 82256, "start": 840.8, "end": 845.8399999999999, "text": " get them by building little composite lattices that wrap up the data. So this is what's called", "tokens": [51276, 483, 552, 538, 2390, 707, 25557, 29025, 1473, 300, 7019, 493, 264, 1412, 13, 407, 341, 307, 437, 311, 1219, 51528], "temperature": 0.0, "avg_logprob": -0.0783121676845405, "compression_ratio": 1.7244582043343653, "no_speech_prob": 0.0009695274056866765}, {"id": 166, "seek": 82256, "start": 845.8399999999999, "end": 850.8, "text": " last-rater wins in the distributed systems, which is just whenever you see a value that's", "tokens": [51528, 1036, 12, 81, 771, 10641, 294, 264, 12631, 3652, 11, 597, 307, 445, 5699, 291, 536, 257, 2158, 300, 311, 51776], "temperature": 0.0, "avg_logprob": -0.0783121676845405, "compression_ratio": 1.7244582043343653, "no_speech_prob": 0.0009695274056866765}, {"id": 167, "seek": 85080, "start": 850.8, "end": 855.4399999999999, "text": " from later than your value, you update, otherwise you don't. And this you just, you know, take your", "tokens": [50364, 490, 1780, 813, 428, 2158, 11, 291, 5623, 11, 5911, 291, 500, 380, 13, 400, 341, 291, 445, 11, 291, 458, 11, 747, 428, 50596], "temperature": 0.0, "avg_logprob": -0.09781318997579908, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0005033095367252827}, {"id": 168, "seek": 85080, "start": 855.4399999999999, "end": 861.5999999999999, "text": " map, which is from keys to things, and you wrap the things in a lexical pair of a clock or a version", "tokens": [50596, 4471, 11, 597, 307, 490, 9317, 281, 721, 11, 293, 291, 7019, 264, 721, 294, 257, 476, 87, 804, 6119, 295, 257, 7830, 420, 257, 3037, 50904], "temperature": 0.0, "avg_logprob": -0.09781318997579908, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0005033095367252827}, {"id": 169, "seek": 85080, "start": 862.16, "end": 868.16, "text": " and the data, right? And you can only have one value per version. So this, this works out as a", "tokens": [50932, 293, 264, 1412, 11, 558, 30, 400, 291, 393, 787, 362, 472, 2158, 680, 3037, 13, 407, 341, 11, 341, 1985, 484, 382, 257, 51232], "temperature": 0.0, "avg_logprob": -0.09781318997579908, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0005033095367252827}, {"id": 170, "seek": 85080, "start": 868.16, "end": 874.24, "text": " lattice. Here's a fancier one, though. This is actually one of the strongest forms of consistency", "tokens": [51232, 34011, 13, 1692, 311, 257, 3429, 27674, 472, 11, 1673, 13, 639, 307, 767, 472, 295, 264, 16595, 6422, 295, 14416, 51536], "temperature": 0.0, "avg_logprob": -0.09781318997579908, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0005033095367252827}, {"id": 171, "seek": 85080, "start": 874.24, "end": 878.56, "text": " you can get without coordination. It's called causal consistency. And here what you have is for", "tokens": [51536, 291, 393, 483, 1553, 21252, 13, 467, 311, 1219, 38755, 14416, 13, 400, 510, 437, 291, 362, 307, 337, 51752], "temperature": 0.0, "avg_logprob": -0.09781318997579908, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0005033095367252827}, {"id": 172, "seek": 87856, "start": 878.56, "end": 884.2399999999999, "text": " every key, you have a vector clock and the data. And the vector clock itself is a map lattice with", "tokens": [50364, 633, 2141, 11, 291, 362, 257, 8062, 7830, 293, 264, 1412, 13, 400, 264, 8062, 7830, 2564, 307, 257, 4471, 34011, 365, 50648], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 173, "seek": 87856, "start": 885.1199999999999, "end": 887.4399999999999, "text": " node IDs and counters. Yes.", "tokens": [50692, 9984, 48212, 293, 39338, 13, 1079, 13, 50808], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 174, "seek": 87856, "start": 887.4399999999999, "end": 890.4799999999999, "text": " Just want to say a little bit sort of operational transform.", "tokens": [50808, 1449, 528, 281, 584, 257, 707, 857, 1333, 295, 16607, 4088, 13, 50960], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 175, "seek": 87856, "start": 890.4799999999999, "end": 892.0799999999999, "text": " A little bit. Yes.", "tokens": [50960, 316, 707, 857, 13, 1079, 13, 51040], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 176, "seek": 87856, "start": 892.0799999999999, "end": 896.88, "text": " Can you ever get stuck? Like, do repairs always exist or do you set it up such that they do?", "tokens": [51040, 1664, 291, 1562, 483, 5541, 30, 1743, 11, 360, 28823, 1009, 2514, 420, 360, 291, 992, 309, 493, 1270, 300, 436, 360, 30, 51280], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 177, "seek": 87856, "start": 896.88, "end": 903.1999999999999, "text": " So these, these particular well trodden forms of consistency work fine. And these lattices", "tokens": [51280, 407, 613, 11, 613, 1729, 731, 4495, 67, 1556, 6422, 295, 14416, 589, 2489, 13, 400, 613, 29025, 1473, 51596], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 178, "seek": 87856, "start": 903.1999999999999, "end": 906.8, "text": " are capturing that. They're saying, look, it's just merge. All right. And it's always going to work", "tokens": [51596, 366, 23384, 300, 13, 814, 434, 1566, 11, 574, 11, 309, 311, 445, 22183, 13, 1057, 558, 13, 400, 309, 311, 1009, 516, 281, 589, 51776], "temperature": 0.0, "avg_logprob": -0.198260131249061, "compression_ratio": 1.6498316498316499, "no_speech_prob": 0.00019716666429303586}, {"id": 179, "seek": 90680, "start": 906.8, "end": 911.5999999999999, "text": " because you've defined an associative commutative item potent merge function. OTs are like really", "tokens": [50364, 570, 291, 600, 7642, 364, 4180, 1166, 800, 325, 1166, 3174, 27073, 22183, 2445, 13, 422, 33424, 366, 411, 534, 50604], "temperature": 0.0, "avg_logprob": -0.13926146084204652, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.0007096111658029258}, {"id": 180, "seek": 90680, "start": 911.5999999999999, "end": 916.64, "text": " weird and full of all sorts of reasoning I don't understand. And they would never be able to have", "tokens": [50604, 3657, 293, 1577, 295, 439, 7527, 295, 21577, 286, 500, 380, 1223, 13, 400, 436, 576, 1128, 312, 1075, 281, 362, 50856], "temperature": 0.0, "avg_logprob": -0.13926146084204652, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.0007096111658029258}, {"id": 181, "seek": 90680, "start": 916.64, "end": 920.64, "text": " such a simple assertion of correctness. All I'm saying here is it's associative commutative", "tokens": [50856, 1270, 257, 2199, 19810, 313, 295, 3006, 1287, 13, 1057, 286, 478, 1566, 510, 307, 309, 311, 4180, 1166, 800, 325, 1166, 51056], "temperature": 0.0, "avg_logprob": -0.13926146084204652, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.0007096111658029258}, {"id": 182, "seek": 90680, "start": 920.64, "end": 924.88, "text": " of an item potent. I got nothing more to say. It takes a little bit of convincing to say that", "tokens": [51056, 295, 364, 3174, 27073, 13, 286, 658, 1825, 544, 281, 584, 13, 467, 2516, 257, 707, 857, 295, 24823, 281, 584, 300, 51268], "temperature": 0.0, "avg_logprob": -0.13926146084204652, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.0007096111658029258}, {"id": 183, "seek": 90680, "start": 924.88, "end": 929.04, "text": " gives you causal consistency, but it's not much convincing because this helps you make causal", "tokens": [51268, 2709, 291, 38755, 14416, 11, 457, 309, 311, 406, 709, 24823, 570, 341, 3665, 291, 652, 38755, 51476], "temperature": 0.0, "avg_logprob": -0.13926146084204652, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.0007096111658029258}, {"id": 184, "seek": 90680, "start": 929.04, "end": 933.8399999999999, "text": " consistency with vector clocks. So the observation that clocks and vector clocks are lattices is", "tokens": [51476, 14416, 365, 8062, 41528, 13, 407, 264, 14816, 300, 41528, 293, 8062, 41528, 366, 29025, 1473, 307, 51716], "temperature": 0.0, "avg_logprob": -0.13926146084204652, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.0007096111658029258}, {"id": 185, "seek": 93384, "start": 933.84, "end": 937.36, "text": " just a nice thing about distributed programming. Yeah.", "tokens": [50364, 445, 257, 1481, 551, 466, 12631, 9410, 13, 865, 13, 50540], "temperature": 0.0, "avg_logprob": -0.2317724748091264, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0009398911497555673}, {"id": 186, "seek": 93384, "start": 938.08, "end": 940.96, "text": " So what do you mean by everything is a lattice? So you mentioned that", "tokens": [50576, 407, 437, 360, 291, 914, 538, 1203, 307, 257, 34011, 30, 407, 291, 2835, 300, 50720], "temperature": 0.0, "avg_logprob": -0.2317724748091264, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0009398911497555673}, {"id": 187, "seek": 93384, "start": 941.9200000000001, "end": 943.84, "text": " Kira's story is a hash map. Right.", "tokens": [50768, 591, 4271, 311, 1657, 307, 257, 22019, 4471, 13, 1779, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2317724748091264, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0009398911497555673}, {"id": 188, "seek": 93384, "start": 944.72, "end": 951.76, "text": " Every key has to be a lattice thing. So the nice thing about the map lattice is the keys", "tokens": [50908, 2048, 2141, 575, 281, 312, 257, 34011, 551, 13, 407, 264, 1481, 551, 466, 264, 4471, 34011, 307, 264, 9317, 51260], "temperature": 0.0, "avg_logprob": -0.2317724748091264, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0009398911497555673}, {"id": 189, "seek": 93384, "start": 951.76, "end": 957.2800000000001, "text": " are not lattice values. The keys are just keys. The whole table is a lattice. But the object is", "tokens": [51260, 366, 406, 34011, 4190, 13, 440, 9317, 366, 445, 9317, 13, 440, 1379, 3199, 307, 257, 34011, 13, 583, 264, 2657, 307, 51536], "temperature": 0.0, "avg_logprob": -0.2317724748091264, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0009398911497555673}, {"id": 190, "seek": 93384, "start": 957.2800000000001, "end": 961.6800000000001, "text": " a lattice because what happens is the merge function is for a particular key. If there's", "tokens": [51536, 257, 34011, 570, 437, 2314, 307, 264, 22183, 2445, 307, 337, 257, 1729, 2141, 13, 759, 456, 311, 51756], "temperature": 0.0, "avg_logprob": -0.2317724748091264, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.0009398911497555673}, {"id": 191, "seek": 96168, "start": 961.68, "end": 965.92, "text": " nothing, it gets the value you gave. Or for that key, if there's something there, you apply the merge", "tokens": [50364, 1825, 11, 309, 2170, 264, 2158, 291, 2729, 13, 1610, 337, 300, 2141, 11, 498, 456, 311, 746, 456, 11, 291, 3079, 264, 22183, 50576], "temperature": 0.0, "avg_logprob": -0.09106126531854376, "compression_ratio": 1.8201892744479495, "no_speech_prob": 0.0004878273175563663}, {"id": 192, "seek": 96168, "start": 965.92, "end": 972.0, "text": " function of this lattice. So that is itself a lattice. And these lattice constructors are very", "tokens": [50576, 2445, 295, 341, 34011, 13, 407, 300, 307, 2564, 257, 34011, 13, 400, 613, 34011, 7690, 830, 366, 588, 50880], "temperature": 0.0, "avg_logprob": -0.09106126531854376, "compression_ratio": 1.8201892744479495, "no_speech_prob": 0.0004878273175563663}, {"id": 193, "seek": 96168, "start": 972.0, "end": 976.4, "text": " nice. We use map lattice. You see lexical pair over there. And these allow you to take simple", "tokens": [50880, 1481, 13, 492, 764, 4471, 34011, 13, 509, 536, 476, 87, 804, 6119, 670, 456, 13, 400, 613, 2089, 291, 281, 747, 2199, 51100], "temperature": 0.0, "avg_logprob": -0.09106126531854376, "compression_ratio": 1.8201892744479495, "no_speech_prob": 0.0004878273175563663}, {"id": 194, "seek": 96168, "start": 976.4, "end": 980.56, "text": " lattices like sets and counters and build up richer lattices out of them, which is a trick", "tokens": [51100, 29025, 1473, 411, 6352, 293, 39338, 293, 1322, 493, 29021, 29025, 1473, 484, 295, 552, 11, 597, 307, 257, 4282, 51308], "temperature": 0.0, "avg_logprob": -0.09106126531854376, "compression_ratio": 1.8201892744479495, "no_speech_prob": 0.0004878273175563663}, {"id": 195, "seek": 96168, "start": 980.56, "end": 985.5999999999999, "text": " our group likes to play a lot. Other groups sort of are doing research on inventing custom lattices", "tokens": [51308, 527, 1594, 5902, 281, 862, 257, 688, 13, 5358, 3935, 1333, 295, 366, 884, 2132, 322, 7962, 278, 2375, 29025, 1473, 51560], "temperature": 0.0, "avg_logprob": -0.09106126531854376, "compression_ratio": 1.8201892744479495, "no_speech_prob": 0.0004878273175563663}, {"id": 196, "seek": 96168, "start": 985.5999999999999, "end": 989.52, "text": " for custom problems. We've been very much in this kind of know let's just build it up from very", "tokens": [51560, 337, 2375, 2740, 13, 492, 600, 668, 588, 709, 294, 341, 733, 295, 458, 718, 311, 445, 1322, 309, 493, 490, 588, 51756], "temperature": 0.0, "avg_logprob": -0.09106126531854376, "compression_ratio": 1.8201892744479495, "no_speech_prob": 0.0004878273175563663}, {"id": 197, "seek": 98952, "start": 989.52, "end": 999.4399999999999, "text": " simple building blocks. So the quick version is it's monotone. And if it's monotone, the column", "tokens": [50364, 2199, 2390, 8474, 13, 407, 264, 1702, 3037, 307, 309, 311, 1108, 310, 546, 13, 400, 498, 309, 311, 1108, 310, 546, 11, 264, 7738, 50860], "temperature": 0.0, "avg_logprob": -0.18030146452096793, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0013039582408964634}, {"id": 198, "seek": 98952, "start": 999.4399999999999, "end": 1003.36, "text": " theorem says it's going to be free replication. So you don't have to do coordination to get", "tokens": [50860, 20904, 1619, 309, 311, 516, 281, 312, 1737, 39911, 13, 407, 291, 500, 380, 362, 281, 360, 21252, 281, 483, 51056], "temperature": 0.0, "avg_logprob": -0.18030146452096793, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0013039582408964634}, {"id": 199, "seek": 98952, "start": 1003.36, "end": 1007.92, "text": " replicated consistency. Connor's going to give you a longer talk about this when we get to", "tokens": [51056, 46365, 14416, 13, 33133, 311, 516, 281, 976, 291, 257, 2854, 751, 466, 341, 562, 321, 483, 281, 51284], "temperature": 0.0, "avg_logprob": -0.18030146452096793, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0013039582408964634}, {"id": 200, "seek": 98952, "start": 1008.96, "end": 1012.88, "text": " conversation about semi-lattice. It's a semi-lattice. I should be clear. It's a semi-lattice.", "tokens": [51336, 3761, 466, 12909, 12, 75, 1591, 573, 13, 467, 311, 257, 12909, 12, 75, 1591, 573, 13, 286, 820, 312, 1850, 13, 467, 311, 257, 12909, 12, 75, 1591, 573, 13, 51532], "temperature": 0.0, "avg_logprob": -0.18030146452096793, "compression_ratio": 1.7383177570093458, "no_speech_prob": 0.0013039582408964634}, {"id": 201, "seek": 101288, "start": 1013.76, "end": 1020.24, "text": " Do you know whether the people working on coordination free replicated data structures", "tokens": [50408, 1144, 291, 458, 1968, 264, 561, 1364, 322, 21252, 1737, 46365, 1412, 9227, 50732], "temperature": 0.0, "avg_logprob": -0.19006440349828416, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.0015724282711744308}, {"id": 202, "seek": 101288, "start": 1020.24, "end": 1025.28, "text": " are well-pure? Yes, they are aware. And Connor will talk about it soon. Yeah, that will come up for", "tokens": [50732, 366, 731, 12, 79, 540, 30, 1079, 11, 436, 366, 3650, 13, 400, 33133, 486, 751, 466, 309, 2321, 13, 865, 11, 300, 486, 808, 493, 337, 50984], "temperature": 0.0, "avg_logprob": -0.19006440349828416, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.0015724282711744308}, {"id": 203, "seek": 101288, "start": 1025.28, "end": 1031.84, "text": " sure. Good. So just to kind of close out this anecdote with Anna, the system is ridiculously", "tokens": [50984, 988, 13, 2205, 13, 407, 445, 281, 733, 295, 1998, 484, 341, 49845, 365, 12899, 11, 264, 1185, 307, 41358, 51312], "temperature": 0.0, "avg_logprob": -0.19006440349828416, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.0015724282711744308}, {"id": 204, "seek": 101288, "start": 1031.84, "end": 1036.72, "text": " fast. And it's especially ridiculously fast under contention relative to other systems.", "tokens": [51312, 2370, 13, 400, 309, 311, 2318, 41358, 2370, 833, 660, 1251, 4972, 281, 661, 3652, 13, 51556], "temperature": 0.0, "avg_logprob": -0.19006440349828416, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.0015724282711744308}, {"id": 205, "seek": 101288, "start": 1036.72, "end": 1040.56, "text": " So we compared against things like mass tree, which is from Harvard. It's 80 colors,", "tokens": [51556, 407, 321, 5347, 1970, 721, 411, 2758, 4230, 11, 597, 307, 490, 13378, 13, 467, 311, 4688, 4577, 11, 51748], "temperature": 0.0, "avg_logprob": -0.19006440349828416, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.0015724282711744308}, {"id": 206, "seek": 104056, "start": 1040.56, "end": 1044.32, "text": " very fast key value store. We also compared against the multi-threaded", "tokens": [50364, 588, 2370, 2141, 2158, 3531, 13, 492, 611, 5347, 1970, 264, 4825, 12, 392, 2538, 292, 50552], "temperature": 0.0, "avg_logprob": -0.07857738024946573, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.0020504784770309925}, {"id": 207, "seek": 104056, "start": 1044.32, "end": 1048.3999999999999, "text": " hash table that comes from Intel in their thread building blocks library. That's TBB.", "tokens": [50552, 22019, 3199, 300, 1487, 490, 19762, 294, 641, 7207, 2390, 8474, 6405, 13, 663, 311, 29711, 33, 13, 50756], "temperature": 0.0, "avg_logprob": -0.07857738024946573, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.0020504784770309925}, {"id": 208, "seek": 104056, "start": 1048.3999999999999, "end": 1053.36, "text": " And under contention, those systems, if you look down here, spend most of their time trying and", "tokens": [50756, 400, 833, 660, 1251, 11, 729, 3652, 11, 498, 291, 574, 760, 510, 11, 3496, 881, 295, 641, 565, 1382, 293, 51004], "temperature": 0.0, "avg_logprob": -0.07857738024946573, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.0020504784770309925}, {"id": 209, "seek": 104056, "start": 1053.36, "end": 1058.1599999999999, "text": " failing to get atomic instructions. So they'll say test and set on a particular memory address,", "tokens": [51004, 18223, 281, 483, 22275, 9415, 13, 407, 436, 603, 584, 1500, 293, 992, 322, 257, 1729, 4675, 2985, 11, 51244], "temperature": 0.0, "avg_logprob": -0.07857738024946573, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.0020504784770309925}, {"id": 210, "seek": 104056, "start": 1058.1599999999999, "end": 1062.8, "text": " and they'll be told no, you have to try again. And they'll spend 95% of their time under contention", "tokens": [51244, 293, 436, 603, 312, 1907, 572, 11, 291, 362, 281, 853, 797, 13, 400, 436, 603, 3496, 13420, 4, 295, 641, 565, 833, 660, 1251, 51476], "temperature": 0.0, "avg_logprob": -0.07857738024946573, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.0020504784770309925}, {"id": 211, "seek": 104056, "start": 1062.8, "end": 1067.6799999999998, "text": " doing that, not doing useful work. So they're at 5% good put, if you're familiar with that term.", "tokens": [51476, 884, 300, 11, 406, 884, 4420, 589, 13, 407, 436, 434, 412, 1025, 4, 665, 829, 11, 498, 291, 434, 4963, 365, 300, 1433, 13, 51720], "temperature": 0.0, "avg_logprob": -0.07857738024946573, "compression_ratio": 1.7192429022082019, "no_speech_prob": 0.0020504784770309925}, {"id": 212, "seek": 106768, "start": 1068.4, "end": 1072.0, "text": " Whereas Anna, because it does no concurrency control, is just doing puts and gets and puts", "tokens": [50400, 13813, 12899, 11, 570, 309, 775, 572, 23702, 10457, 1969, 11, 307, 445, 884, 8137, 293, 2170, 293, 8137, 50580], "temperature": 0.0, "avg_logprob": -0.10329278310139973, "compression_ratio": 1.8793103448275863, "no_speech_prob": 0.000535758154001087}, {"id": 213, "seek": 106768, "start": 1072.0, "end": 1076.8, "text": " and gets and puts and gets and spending most of its time doing good put. And that's why Anna can be", "tokens": [50580, 293, 2170, 293, 8137, 293, 2170, 293, 6434, 881, 295, 1080, 565, 884, 665, 829, 13, 400, 300, 311, 983, 12899, 393, 312, 50820], "temperature": 0.0, "avg_logprob": -0.10329278310139973, "compression_ratio": 1.8793103448275863, "no_speech_prob": 0.000535758154001087}, {"id": 214, "seek": 106768, "start": 1077.3600000000001, "end": 1081.2, "text": " 700x better throughput under high contention than these other systems.", "tokens": [50848, 15204, 87, 1101, 44629, 833, 1090, 660, 1251, 813, 613, 661, 3652, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10329278310139973, "compression_ratio": 1.8793103448275863, "no_speech_prob": 0.000535758154001087}, {"id": 215, "seek": 106768, "start": 1082.16, "end": 1086.88, "text": " But also because it does no coordination scales almost perfectly linearly across threads,", "tokens": [51088, 583, 611, 570, 309, 775, 572, 21252, 17408, 1920, 6239, 43586, 2108, 19314, 11, 51324], "temperature": 0.0, "avg_logprob": -0.10329278310139973, "compression_ratio": 1.8793103448275863, "no_speech_prob": 0.000535758154001087}, {"id": 216, "seek": 106768, "start": 1086.88, "end": 1092.4, "text": " and then across machines, and eventually across the globe. There's really nothing to keep it from", "tokens": [51324, 293, 550, 2108, 8379, 11, 293, 4728, 2108, 264, 15371, 13, 821, 311, 534, 1825, 281, 1066, 309, 490, 51600], "temperature": 0.0, "avg_logprob": -0.10329278310139973, "compression_ratio": 1.8793103448275863, "no_speech_prob": 0.000535758154001087}, {"id": 217, "seek": 106768, "start": 1092.4, "end": 1096.8, "text": " scaling linearly, because the only extra work it has to do is some gossip. And that can be done", "tokens": [51600, 21589, 43586, 11, 570, 264, 787, 2857, 589, 309, 575, 281, 360, 307, 512, 31788, 13, 400, 300, 393, 312, 1096, 51820], "temperature": 0.0, "avg_logprob": -0.10329278310139973, "compression_ratio": 1.8793103448275863, "no_speech_prob": 0.000535758154001087}, {"id": 218, "seek": 109680, "start": 1096.8, "end": 1100.8, "text": " in the background, and can be done as lazily as you like without breaking the semantics.", "tokens": [50364, 294, 264, 3678, 11, 293, 393, 312, 1096, 382, 19320, 953, 382, 291, 411, 1553, 7697, 264, 4361, 45298, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0815234865461077, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0008295233710668981}, {"id": 219, "seek": 109680, "start": 1101.44, "end": 1104.72, "text": " So there's maybe a little fudging here on how stale your data is, but it's correct.", "tokens": [50596, 407, 456, 311, 1310, 257, 707, 283, 532, 3249, 510, 322, 577, 342, 1220, 428, 1412, 307, 11, 457, 309, 311, 3006, 13, 50760], "temperature": 0.0, "avg_logprob": -0.0815234865461077, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0008295233710668981}, {"id": 220, "seek": 109680, "start": 1106.6399999999999, "end": 1112.1599999999999, "text": " So this was a crazy fast system. And the thing about this, oh, and if you try to run it in the", "tokens": [50856, 407, 341, 390, 257, 3219, 2370, 1185, 13, 400, 264, 551, 466, 341, 11, 1954, 11, 293, 498, 291, 853, 281, 1190, 309, 294, 264, 51132], "temperature": 0.0, "avg_logprob": -0.0815234865461077, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0008295233710668981}, {"id": 221, "seek": 109680, "start": 1112.1599999999999, "end": 1115.84, "text": " cloud, it's also incredibly cheap to run relative to systems that are wasting all their time doing", "tokens": [51132, 4588, 11, 309, 311, 611, 6252, 7084, 281, 1190, 4972, 281, 3652, 300, 366, 20457, 439, 641, 565, 884, 51316], "temperature": 0.0, "avg_logprob": -0.0815234865461077, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0008295233710668981}, {"id": 222, "seek": 109680, "start": 1115.84, "end": 1120.56, "text": " this business. They're charging you for this. They're trying to get locks, they're waiting on", "tokens": [51316, 341, 1606, 13, 814, 434, 11379, 291, 337, 341, 13, 814, 434, 1382, 281, 483, 20703, 11, 436, 434, 3806, 322, 51552], "temperature": 0.0, "avg_logprob": -0.0815234865461077, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0008295233710668981}, {"id": 223, "seek": 109680, "start": 1120.56, "end": 1125.9199999999998, "text": " locks, and they're charging you money. So you'd like to avoid that if you can. Okay, that's all", "tokens": [51552, 20703, 11, 293, 436, 434, 11379, 291, 1460, 13, 407, 291, 1116, 411, 281, 5042, 300, 498, 291, 393, 13, 1033, 11, 300, 311, 439, 51820], "temperature": 0.0, "avg_logprob": -0.0815234865461077, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0008295233710668981}, {"id": 224, "seek": 112592, "start": 1125.92, "end": 1131.04, "text": " very nice. But it was written in C++ by Chenggang, who's an excellent coder. His implementation is", "tokens": [50364, 588, 1481, 13, 583, 309, 390, 3720, 294, 383, 25472, 538, 24363, 19619, 11, 567, 311, 364, 7103, 17656, 260, 13, 2812, 11420, 307, 50620], "temperature": 0.0, "avg_logprob": -0.09804647619074042, "compression_ratio": 1.7408536585365855, "no_speech_prob": 0.001700416556559503}, {"id": 225, "seek": 112592, "start": 1131.04, "end": 1136.3200000000002, "text": " correct by assertion. It would be really nice to be able to kind of do what CAD wants us to do,", "tokens": [50620, 3006, 538, 19810, 313, 13, 467, 576, 312, 534, 1481, 281, 312, 1075, 281, 733, 295, 360, 437, 41143, 2738, 505, 281, 360, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09804647619074042, "compression_ratio": 1.7408536585365855, "no_speech_prob": 0.001700416556559503}, {"id": 226, "seek": 112592, "start": 1136.3200000000002, "end": 1140.0, "text": " formalize a spec that is correct, and then synthesize an implementation from it through", "tokens": [50884, 9860, 1125, 257, 1608, 300, 307, 3006, 11, 293, 550, 26617, 1125, 364, 11420, 490, 309, 807, 51068], "temperature": 0.0, "avg_logprob": -0.09804647619074042, "compression_ratio": 1.7408536585365855, "no_speech_prob": 0.001700416556559503}, {"id": 227, "seek": 112592, "start": 1140.0, "end": 1144.3200000000002, "text": " rules that are correct transformations. So we'd really like to do that, and we'd like to maintain", "tokens": [51068, 4474, 300, 366, 3006, 34852, 13, 407, 321, 1116, 534, 411, 281, 360, 300, 11, 293, 321, 1116, 411, 281, 6909, 51284], "temperature": 0.0, "avg_logprob": -0.09804647619074042, "compression_ratio": 1.7408536585365855, "no_speech_prob": 0.001700416556559503}, {"id": 228, "seek": 112592, "start": 1144.3200000000002, "end": 1150.3200000000002, "text": " the speed. What kind of formalisms? Well, we're using lattices mostly. So maybe we could have a", "tokens": [51284, 264, 3073, 13, 708, 733, 295, 9860, 13539, 30, 1042, 11, 321, 434, 1228, 29025, 1473, 5240, 13, 407, 1310, 321, 727, 362, 257, 51584], "temperature": 0.0, "avg_logprob": -0.09804647619074042, "compression_ratio": 1.7408536585365855, "no_speech_prob": 0.001700416556559503}, {"id": 229, "seek": 112592, "start": 1150.3200000000002, "end": 1155.52, "text": " type system that starts with some basic semilattices, like sets and counters, some composition", "tokens": [51584, 2010, 1185, 300, 3719, 365, 512, 3875, 4361, 388, 1591, 1473, 11, 411, 6352, 293, 39338, 11, 512, 12686, 51844], "temperature": 0.0, "avg_logprob": -0.09804647619074042, "compression_ratio": 1.7408536585365855, "no_speech_prob": 0.001700416556559503}, {"id": 230, "seek": 115552, "start": 1155.52, "end": 1160.56, "text": " lattices, like key value pairs, products, lexical products, which aren't always lattices. So you", "tokens": [50364, 29025, 1473, 11, 411, 2141, 2158, 15494, 11, 3383, 11, 476, 87, 804, 3383, 11, 597, 3212, 380, 1009, 29025, 1473, 13, 407, 291, 50616], "temperature": 0.0, "avg_logprob": -0.12506184161909475, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.000606966728810221}, {"id": 231, "seek": 115552, "start": 1160.56, "end": 1165.04, "text": " have to, there's some constraints on whether a lexical product is a lattice. And then we want", "tokens": [50616, 362, 281, 11, 456, 311, 512, 18491, 322, 1968, 257, 476, 87, 804, 1674, 307, 257, 34011, 13, 400, 550, 321, 528, 50840], "temperature": 0.0, "avg_logprob": -0.12506184161909475, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.000606966728810221}, {"id": 232, "seek": 115552, "start": 1165.04, "end": 1170.6399999999999, "text": " like a data flow, like a query plan algebra. So you can imagine a semi-ring kind of algebra, but", "tokens": [50840, 411, 257, 1412, 3095, 11, 411, 257, 14581, 1393, 21989, 13, 407, 291, 393, 3811, 257, 12909, 12, 2937, 733, 295, 21989, 11, 457, 51120], "temperature": 0.0, "avg_logprob": -0.12506184161909475, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.000606966728810221}, {"id": 233, "seek": 115552, "start": 1170.6399999999999, "end": 1174.48, "text": " you know, there's going to be maps and folds, and then there's going to be physical stuff,", "tokens": [51120, 291, 458, 11, 456, 311, 516, 281, 312, 11317, 293, 31341, 11, 293, 550, 456, 311, 516, 281, 312, 4001, 1507, 11, 51312], "temperature": 0.0, "avg_logprob": -0.12506184161909475, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.000606966728810221}, {"id": 234, "seek": 115552, "start": 1174.48, "end": 1178.8, "text": " like scan a collection, or get stuff over a network. You know, networks do weird things,", "tokens": [51312, 411, 11049, 257, 5765, 11, 420, 483, 1507, 670, 257, 3209, 13, 509, 458, 11, 9590, 360, 3657, 721, 11, 51528], "temperature": 0.0, "avg_logprob": -0.12506184161909475, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.000606966728810221}, {"id": 235, "seek": 115552, "start": 1178.8, "end": 1183.84, "text": " like they permit things, and they form batches of things. They parenthesize streams, if you will.", "tokens": [51528, 411, 436, 13423, 721, 11, 293, 436, 1254, 15245, 279, 295, 721, 13, 814, 23350, 279, 1125, 15842, 11, 498, 291, 486, 13, 51780], "temperature": 0.0, "avg_logprob": -0.12506184161909475, "compression_ratio": 1.8646864686468647, "no_speech_prob": 0.000606966728810221}, {"id": 236, "seek": 118384, "start": 1184.8, "end": 1188.72, "text": " They multiplex and de-multiplex messages. So there's some physical stuff that we want here too,", "tokens": [50412, 814, 3311, 2021, 293, 368, 12, 76, 723, 647, 2021, 7897, 13, 407, 456, 311, 512, 4001, 1507, 300, 321, 528, 510, 886, 11, 50608], "temperature": 0.0, "avg_logprob": -0.09548552443341511, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.00016864364442881197}, {"id": 237, "seek": 118384, "start": 1188.72, "end": 1192.9599999999998, "text": " and I'd like to really be able to prove all that stuff is correct in a meaningful way.", "tokens": [50608, 293, 286, 1116, 411, 281, 534, 312, 1075, 281, 7081, 439, 300, 1507, 307, 3006, 294, 257, 10995, 636, 13, 50820], "temperature": 0.0, "avg_logprob": -0.09548552443341511, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.00016864364442881197}, {"id": 238, "seek": 118384, "start": 1194.32, "end": 1198.9599999999998, "text": " So just for fun, I don't expect you to read this. This is the ANA implementation. You just saw", "tokens": [50888, 407, 445, 337, 1019, 11, 286, 500, 380, 2066, 291, 281, 1401, 341, 13, 639, 307, 264, 5252, 32, 11420, 13, 509, 445, 1866, 51120], "temperature": 0.0, "avg_logprob": -0.09548552443341511, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.00016864364442881197}, {"id": 239, "seek": 118384, "start": 1198.9599999999998, "end": 1203.52, "text": " written in our low-level hydroflow language. This is the whole thing. It's a very simple program.", "tokens": [51120, 3720, 294, 527, 2295, 12, 12418, 15435, 10565, 2856, 13, 639, 307, 264, 1379, 551, 13, 467, 311, 257, 588, 2199, 1461, 13, 51348], "temperature": 0.0, "avg_logprob": -0.09548552443341511, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.00016864364442881197}, {"id": 240, "seek": 118384, "start": 1204.72, "end": 1209.04, "text": " And you can see this is kind of a data flow language. It's basically specifying graphs of", "tokens": [51408, 400, 291, 393, 536, 341, 307, 733, 295, 257, 1412, 3095, 2856, 13, 467, 311, 1936, 1608, 5489, 24877, 295, 51624], "temperature": 0.0, "avg_logprob": -0.09548552443341511, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.00016864364442881197}, {"id": 241, "seek": 120904, "start": 1209.04, "end": 1213.92, "text": " data flow. The edges are directed edges in a graph. The words are nodes in the graph,", "tokens": [50364, 1412, 3095, 13, 440, 8819, 366, 12898, 8819, 294, 257, 4295, 13, 440, 2283, 366, 13891, 294, 264, 4295, 11, 50608], "temperature": 0.0, "avg_logprob": -0.130024805790236, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.013425139710307121}, {"id": 242, "seek": 120904, "start": 1213.92, "end": 1221.36, "text": " and you'll see familiar operators like map and join, and cross-join, and so on.", "tokens": [50608, 293, 291, 603, 536, 4963, 19077, 411, 4471, 293, 3917, 11, 293, 3278, 12, 5134, 259, 11, 293, 370, 322, 13, 50980], "temperature": 0.0, "avg_logprob": -0.130024805790236, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.013425139710307121}, {"id": 243, "seek": 120904, "start": 1221.36, "end": 1226.1599999999999, "text": " All right, and you can give views names. So this is a little name of a subgraph,", "tokens": [50980, 1057, 558, 11, 293, 291, 393, 976, 6809, 5288, 13, 407, 341, 307, 257, 707, 1315, 295, 257, 1422, 34091, 11, 51220], "temperature": 0.0, "avg_logprob": -0.130024805790236, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.013425139710307121}, {"id": 244, "seek": 120904, "start": 1226.8, "end": 1231.04, "text": " and we use it, and so on. So it's just a little language for specifying graphs.", "tokens": [51252, 293, 321, 764, 309, 11, 293, 370, 322, 13, 407, 309, 311, 445, 257, 707, 2856, 337, 1608, 5489, 24877, 13, 51464], "temperature": 0.0, "avg_logprob": -0.130024805790236, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.013425139710307121}, {"id": 245, "seek": 120904, "start": 1231.04, "end": 1237.68, "text": " This is a picture of that program that's output by the system. Okay, so it's a one piece of paper", "tokens": [51464, 639, 307, 257, 3036, 295, 300, 1461, 300, 311, 5598, 538, 264, 1185, 13, 1033, 11, 370, 309, 311, 257, 472, 2522, 295, 3035, 51796], "temperature": 0.0, "avg_logprob": -0.130024805790236, "compression_ratio": 1.7235772357723578, "no_speech_prob": 0.013425139710307121}, {"id": 246, "seek": 123768, "start": 1238.24, "end": 1246.0800000000002, "text": " program. So in this particular program, their union is actually joined, semi-latest join,", "tokens": [50392, 1461, 13, 407, 294, 341, 1729, 1461, 11, 641, 11671, 307, 767, 6869, 11, 12909, 12, 14087, 377, 3917, 11, 50784], "temperature": 0.0, "avg_logprob": -0.20418046868365744, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0005792115698568523}, {"id": 247, "seek": 123768, "start": 1247.44, "end": 1255.3600000000001, "text": " and that might be the only one. Yeah. Okay, and so just to convince ourselves this is fast,", "tokens": [50852, 293, 300, 1062, 312, 264, 787, 472, 13, 865, 13, 1033, 11, 293, 370, 445, 281, 13447, 4175, 341, 307, 2370, 11, 51248], "temperature": 0.0, "avg_logprob": -0.20418046868365744, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0005792115698568523}, {"id": 248, "seek": 123768, "start": 1255.3600000000001, "end": 1261.76, "text": " that's Chengang's original numbers. We have it now running through hydro, that implementation you saw", "tokens": [51248, 300, 311, 24363, 656, 311, 3380, 3547, 13, 492, 362, 309, 586, 2614, 807, 15435, 11, 300, 11420, 291, 1866, 51568], "temperature": 0.0, "avg_logprob": -0.20418046868365744, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0005792115698568523}, {"id": 249, "seek": 123768, "start": 1261.76, "end": 1267.1200000000001, "text": " on very similar machines, and we get very similar performance to the handwritten code. So we're", "tokens": [51568, 322, 588, 2531, 8379, 11, 293, 321, 483, 588, 2531, 3389, 281, 264, 1011, 26859, 3089, 13, 407, 321, 434, 51836], "temperature": 0.0, "avg_logprob": -0.20418046868365744, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0005792115698568523}, {"id": 250, "seek": 126712, "start": 1267.12, "end": 1274.0, "text": " feeling pretty good that we're hitting our goals for performance. And because this graph is green,", "tokens": [50364, 2633, 1238, 665, 300, 321, 434, 8850, 527, 5493, 337, 3389, 13, 400, 570, 341, 4295, 307, 3092, 11, 50708], "temperature": 0.0, "avg_logprob": -0.10192435455322266, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00017398816999047995}, {"id": 251, "seek": 126712, "start": 1274.0, "end": 1277.9199999999998, "text": " it's telling us that this thing is all monotone, and therefore consistently replicable.", "tokens": [50708, 309, 311, 3585, 505, 300, 341, 551, 307, 439, 1108, 310, 546, 11, 293, 4412, 14961, 3248, 43023, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10192435455322266, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00017398816999047995}, {"id": 252, "seek": 126712, "start": 1278.7199999999998, "end": 1283.28, "text": " And at a glance, we can see this is a safe program. And I'm sort of cheating at this point,", "tokens": [50944, 400, 412, 257, 21094, 11, 321, 393, 536, 341, 307, 257, 3273, 1461, 13, 400, 286, 478, 1333, 295, 18309, 412, 341, 935, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10192435455322266, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00017398816999047995}, {"id": 253, "seek": 126712, "start": 1283.28, "end": 1287.9199999999998, "text": " and I'm going to confess to that. There's, I think, more work we need to do to make this robust. I", "tokens": [51172, 293, 286, 478, 516, 281, 19367, 281, 300, 13, 821, 311, 11, 286, 519, 11, 544, 589, 321, 643, 281, 360, 281, 652, 341, 13956, 13, 286, 51404], "temperature": 0.0, "avg_logprob": -0.10192435455322266, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00017398816999047995}, {"id": 254, "seek": 126712, "start": 1287.9199999999998, "end": 1293.12, "text": " think these green edges are kind of a, they're slightly bi-assertion at this point. So I would", "tokens": [51404, 519, 613, 3092, 8819, 366, 733, 295, 257, 11, 436, 434, 4748, 3228, 12, 640, 911, 313, 412, 341, 935, 13, 407, 286, 576, 51664], "temperature": 0.0, "avg_logprob": -0.10192435455322266, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.00017398816999047995}, {"id": 255, "seek": 129312, "start": 1293.12, "end": 1297.6, "text": " like to make them more fundamentally correct, and hopefully we'll have time to talk about that later.", "tokens": [50364, 411, 281, 652, 552, 544, 17879, 3006, 11, 293, 4696, 321, 603, 362, 565, 281, 751, 466, 300, 1780, 13, 50588], "temperature": 0.0, "avg_logprob": -0.11980982387767118, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.00036826878204010427}, {"id": 256, "seek": 129312, "start": 1298.7199999999998, "end": 1302.8799999999999, "text": " Okay. With that, I'm going to hand off to Connor. He's going to take us through the next chapter.", "tokens": [50644, 1033, 13, 2022, 300, 11, 286, 478, 516, 281, 1011, 766, 281, 33133, 13, 634, 311, 516, 281, 747, 505, 807, 264, 958, 7187, 13, 50852], "temperature": 0.0, "avg_logprob": -0.11980982387767118, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.00036826878204010427}, {"id": 257, "seek": 129312, "start": 1306.8799999999999, "end": 1314.1599999999999, "text": " Hello. People hear me? Yeah. I'm Connor. I'm a PhD student here working on hydro. I like systems", "tokens": [51052, 2425, 13, 3432, 1568, 385, 30, 865, 13, 286, 478, 33133, 13, 286, 478, 257, 14476, 3107, 510, 1364, 322, 15435, 13, 286, 411, 3652, 51416], "temperature": 0.0, "avg_logprob": -0.11980982387767118, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.00036826878204010427}, {"id": 258, "seek": 129312, "start": 1314.1599999999999, "end": 1317.4399999999998, "text": " and theories. So I thought I'd show you guys some of the theory stuff we've been thinking about,", "tokens": [51416, 293, 13667, 13, 407, 286, 1194, 286, 1116, 855, 291, 1074, 512, 295, 264, 5261, 1507, 321, 600, 668, 1953, 466, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11980982387767118, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.00036826878204010427}, {"id": 259, "seek": 131744, "start": 1317.44, "end": 1319.8400000000001, "text": " see if anyone has any thoughts, wants to collaborate on anything.", "tokens": [50364, 536, 498, 2878, 575, 604, 4598, 11, 2738, 281, 18338, 322, 1340, 13, 50484], "temperature": 0.0, "avg_logprob": -0.08493017918855242, "compression_ratio": 1.716, "no_speech_prob": 0.010650938376784325}, {"id": 260, "seek": 131744, "start": 1324.96, "end": 1330.24, "text": " Okay. So in the classical database lens, we have, you know, these three layers,", "tokens": [50740, 1033, 13, 407, 294, 264, 13735, 8149, 6765, 11, 321, 362, 11, 291, 458, 11, 613, 1045, 7914, 11, 51004], "temperature": 0.0, "avg_logprob": -0.08493017918855242, "compression_ratio": 1.716, "no_speech_prob": 0.010650938376784325}, {"id": 261, "seek": 131744, "start": 1330.24, "end": 1335.68, "text": " the relational calculus at the top, relational algebra in the middle, and then a physical", "tokens": [51004, 264, 38444, 33400, 412, 264, 1192, 11, 38444, 21989, 294, 264, 2808, 11, 293, 550, 257, 4001, 51276], "temperature": 0.0, "avg_logprob": -0.08493017918855242, "compression_ratio": 1.716, "no_speech_prob": 0.010650938376784325}, {"id": 262, "seek": 131744, "start": 1335.68, "end": 1341.2, "text": " algebra at the bottom, concerned with things like hashing and sorting and so on. And we can think", "tokens": [51276, 21989, 412, 264, 2767, 11, 5922, 365, 721, 411, 575, 571, 293, 32411, 293, 370, 322, 13, 400, 321, 393, 519, 51552], "temperature": 0.0, "avg_logprob": -0.08493017918855242, "compression_ratio": 1.716, "no_speech_prob": 0.010650938376784325}, {"id": 263, "seek": 131744, "start": 1341.2, "end": 1346.0, "text": " about how this changes when we move to the cloud setting. And there's good news and bad news on", "tokens": [51552, 466, 577, 341, 2962, 562, 321, 1286, 281, 264, 4588, 3287, 13, 400, 456, 311, 665, 2583, 293, 1578, 2583, 322, 51792], "temperature": 0.0, "avg_logprob": -0.08493017918855242, "compression_ratio": 1.716, "no_speech_prob": 0.010650938376784325}, {"id": 264, "seek": 134600, "start": 1346.0, "end": 1350.08, "text": " the current state of affairs when we move to the cloud setting. The good news is that,", "tokens": [50364, 264, 2190, 1785, 295, 17478, 562, 321, 1286, 281, 264, 4588, 3287, 13, 440, 665, 2583, 307, 300, 11, 50568], "temperature": 0.0, "avg_logprob": -0.10596932739507957, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0006262928945943713}, {"id": 265, "seek": 134600, "start": 1350.08, "end": 1353.76, "text": " like Joe said at the top, we have this Daedalus language from the Bloom project", "tokens": [50568, 411, 6807, 848, 412, 264, 1192, 11, 321, 362, 341, 3933, 292, 304, 301, 2856, 490, 264, 25927, 1716, 50752], "temperature": 0.0, "avg_logprob": -0.10596932739507957, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0006262928945943713}, {"id": 266, "seek": 134600, "start": 1353.76, "end": 1360.16, "text": " that is a data log like dialect for distributed systems. The bad news is that developers are", "tokens": [50752, 300, 307, 257, 1412, 3565, 411, 24652, 337, 12631, 3652, 13, 440, 1578, 2583, 307, 300, 8849, 366, 51072], "temperature": 0.0, "avg_logprob": -0.10596932739507957, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0006262928945943713}, {"id": 267, "seek": 134600, "start": 1360.16, "end": 1365.12, "text": " not asking for a data log dialect to build distributed systems in. The developers we've", "tokens": [51072, 406, 3365, 337, 257, 1412, 3565, 24652, 281, 1322, 12631, 3652, 294, 13, 440, 8849, 321, 600, 51320], "temperature": 0.0, "avg_logprob": -0.10596932739507957, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0006262928945943713}, {"id": 268, "seek": 134600, "start": 1365.12, "end": 1368.64, "text": " talked to are a lot more interested in a functional algebraic looking interface and", "tokens": [51320, 2825, 281, 366, 257, 688, 544, 3102, 294, 257, 11745, 21989, 299, 1237, 9226, 293, 51496], "temperature": 0.0, "avg_logprob": -0.10596932739507957, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0006262928945943713}, {"id": 269, "seek": 134600, "start": 1368.64, "end": 1375.52, "text": " especially something Pythonic looking like pandas. On the algebra side, the good news is that there", "tokens": [51496, 2318, 746, 15329, 299, 1237, 411, 4565, 296, 13, 1282, 264, 21989, 1252, 11, 264, 665, 2583, 307, 300, 456, 51840], "temperature": 0.0, "avg_logprob": -0.10596932739507957, "compression_ratio": 1.824742268041237, "no_speech_prob": 0.0006262928945943713}, {"id": 270, "seek": 137552, "start": 1375.52, "end": 1379.84, "text": " is an algebraic model for distributed systems today. It's the semi-ladis model that Joe has", "tokens": [50364, 307, 364, 21989, 299, 2316, 337, 12631, 3652, 965, 13, 467, 311, 264, 12909, 12, 9290, 271, 2316, 300, 6807, 575, 50580], "temperature": 0.0, "avg_logprob": -0.11465484445745294, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.00020340825722087175}, {"id": 271, "seek": 137552, "start": 1379.84, "end": 1384.56, "text": " mentioned that is referred to as CRDTs in a lot of places, especially in the programming languages", "tokens": [50580, 2835, 300, 307, 10839, 281, 382, 14123, 35, 33424, 294, 257, 688, 295, 3190, 11, 2318, 294, 264, 9410, 8650, 50816], "temperature": 0.0, "avg_logprob": -0.11465484445745294, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.00020340825722087175}, {"id": 272, "seek": 137552, "start": 1384.56, "end": 1390.0, "text": " community. The bad news is that this is a model for coordination-free updates of state and it", "tokens": [50816, 1768, 13, 440, 1578, 2583, 307, 300, 341, 307, 257, 2316, 337, 21252, 12, 10792, 9205, 295, 1785, 293, 309, 51088], "temperature": 0.0, "avg_logprob": -0.11465484445745294, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.00020340825722087175}, {"id": 273, "seek": 137552, "start": 1390.0, "end": 1394.8799999999999, "text": " doesn't actually have a query language or give guarantees about coordination-free-ness of queries", "tokens": [51088, 1177, 380, 767, 362, 257, 14581, 2856, 420, 976, 32567, 466, 21252, 12, 10792, 12, 1287, 295, 24109, 51332], "temperature": 0.0, "avg_logprob": -0.11465484445745294, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.00020340825722087175}, {"id": 274, "seek": 137552, "start": 1394.8799999999999, "end": 1401.68, "text": " today. And then at the physical layer, when we add a network to the situation, asynchronous", "tokens": [51332, 965, 13, 400, 550, 412, 264, 4001, 4583, 11, 562, 321, 909, 257, 3209, 281, 264, 2590, 11, 49174, 51672], "temperature": 0.0, "avg_logprob": -0.11465484445745294, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.00020340825722087175}, {"id": 275, "seek": 140168, "start": 1401.68, "end": 1405.68, "text": " computer network, a lot of non-determinism emerges that we need to be able to handle,", "tokens": [50364, 3820, 3209, 11, 257, 688, 295, 2107, 12, 49136, 259, 1434, 38965, 300, 321, 643, 281, 312, 1075, 281, 4813, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12967960834503173, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.000503288465552032}, {"id": 276, "seek": 140168, "start": 1405.68, "end": 1409.6000000000001, "text": " in particular reordering, batching, and duplication of messages.", "tokens": [50564, 294, 1729, 319, 765, 1794, 11, 15245, 278, 11, 293, 17154, 399, 295, 7897, 13, 50760], "temperature": 0.0, "avg_logprob": -0.12967960834503173, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.000503288465552032}, {"id": 277, "seek": 140168, "start": 1413.04, "end": 1419.2, "text": " So what we'd like to get to is unifying formalism across logic and algebra and this", "tokens": [50932, 407, 437, 321, 1116, 411, 281, 483, 281, 307, 517, 5489, 9860, 1434, 2108, 9952, 293, 21989, 293, 341, 51240], "temperature": 0.0, "avg_logprob": -0.12967960834503173, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.000503288465552032}, {"id": 278, "seek": 140168, "start": 1419.2, "end": 1425.92, "text": " physical algebra and have correctness at the physical layer that we can prove for", "tokens": [51240, 4001, 21989, 293, 362, 3006, 1287, 412, 264, 4001, 4583, 300, 321, 393, 7081, 337, 51576], "temperature": 0.0, "avg_logprob": -0.12967960834503173, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.000503288465552032}, {"id": 279, "seek": 142592, "start": 1425.92, "end": 1432.5600000000002, "text": " safe against this non-determinism from the network. And we're able to capture things like", "tokens": [50364, 3273, 1970, 341, 2107, 12, 49136, 259, 1434, 490, 264, 3209, 13, 400, 321, 434, 1075, 281, 7983, 721, 411, 50696], "temperature": 0.0, "avg_logprob": -0.15163826696651497, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0010986177949234843}, {"id": 280, "seek": 142592, "start": 1432.5600000000002, "end": 1437.92, "text": " replication, partitioning, batching, incrementalization, and termination analysis. We'll talk about", "tokens": [50696, 39911, 11, 24808, 278, 11, 15245, 278, 11, 35759, 2144, 11, 293, 1433, 2486, 5215, 13, 492, 603, 751, 466, 50964], "temperature": 0.0, "avg_logprob": -0.15163826696651497, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0010986177949234843}, {"id": 281, "seek": 142592, "start": 1437.92, "end": 1448.0800000000002, "text": " more later. All right, so let's talk about semi-ladis' CRDTs. So this is a model for", "tokens": [50964, 544, 1780, 13, 1057, 558, 11, 370, 718, 311, 751, 466, 12909, 12, 9290, 271, 6, 14123, 35, 33424, 13, 407, 341, 307, 257, 2316, 337, 51472], "temperature": 0.0, "avg_logprob": -0.15163826696651497, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0010986177949234843}, {"id": 282, "seek": 142592, "start": 1448.0800000000002, "end": 1452.0, "text": " distributed systems that in databases we usually call the semi-ladis model. It's what is called an", "tokens": [51472, 12631, 3652, 300, 294, 22380, 321, 2673, 818, 264, 12909, 12, 9290, 271, 2316, 13, 467, 311, 437, 307, 1219, 364, 51668], "temperature": 0.0, "avg_logprob": -0.15163826696651497, "compression_ratio": 1.5606694560669456, "no_speech_prob": 0.0010986177949234843}, {"id": 283, "seek": 145200, "start": 1452.0, "end": 1457.04, "text": " Anna in Blum L. It came out of the programming languages community and there it's usually", "tokens": [50364, 12899, 294, 2177, 449, 441, 13, 467, 1361, 484, 295, 264, 9410, 8650, 1768, 293, 456, 309, 311, 2673, 50616], "temperature": 0.0, "avg_logprob": -0.12007122039794922, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0010648351162672043}, {"id": 284, "seek": 145200, "start": 1457.04, "end": 1463.68, "text": " referred to as CRDTs. It stands for conflict-free replicated data types. It's introduced in this", "tokens": [50616, 10839, 281, 382, 14123, 35, 33424, 13, 467, 7382, 337, 6596, 12, 10792, 46365, 1412, 3467, 13, 467, 311, 7268, 294, 341, 50948], "temperature": 0.0, "avg_logprob": -0.12007122039794922, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0010648351162672043}, {"id": 285, "seek": 145200, "start": 1463.68, "end": 1471.52, "text": " paper here and there's over 179 papers about CRDTs out there. It's also started to get popular", "tokens": [50948, 3035, 510, 293, 456, 311, 670, 3282, 24, 10577, 466, 14123, 35, 33424, 484, 456, 13, 467, 311, 611, 1409, 281, 483, 3743, 51340], "temperature": 0.0, "avg_logprob": -0.12007122039794922, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0010648351162672043}, {"id": 286, "seek": 145200, "start": 1471.52, "end": 1476.4, "text": " amongst software engineers and you see people talk about this CRDT model on places like Hacker News,", "tokens": [51340, 12918, 4722, 11955, 293, 291, 536, 561, 751, 466, 341, 14123, 35, 51, 2316, 322, 3190, 411, 389, 23599, 7987, 11, 51584], "temperature": 0.0, "avg_logprob": -0.12007122039794922, "compression_ratio": 1.5465587044534412, "no_speech_prob": 0.0010648351162672043}, {"id": 287, "seek": 147640, "start": 1476.4, "end": 1482.88, "text": " people starting startups with it. So what does it do? It tries to handle the sources of", "tokens": [50364, 561, 2891, 28041, 365, 309, 13, 407, 437, 775, 309, 360, 30, 467, 9898, 281, 4813, 264, 7139, 295, 50688], "temperature": 0.0, "avg_logprob": -0.06867178495939788, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.005554156843572855}, {"id": 288, "seek": 147640, "start": 1482.88, "end": 1488.64, "text": " non-determinism that come from an asynchronous computer network. These are the arbitrary batching", "tokens": [50688, 2107, 12, 49136, 259, 1434, 300, 808, 490, 364, 49174, 3820, 3209, 13, 1981, 366, 264, 23211, 15245, 278, 50976], "temperature": 0.0, "avg_logprob": -0.06867178495939788, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.005554156843572855}, {"id": 289, "seek": 147640, "start": 1488.64, "end": 1496.3200000000002, "text": " of messages, arbitrary reordering of messages, and arbitrary duplication of messages. And so it", "tokens": [50976, 295, 7897, 11, 23211, 319, 765, 1794, 295, 7897, 11, 293, 23211, 17154, 399, 295, 7897, 13, 400, 370, 309, 51360], "temperature": 0.0, "avg_logprob": -0.06867178495939788, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.005554156843572855}, {"id": 290, "seek": 147640, "start": 1496.3200000000002, "end": 1500.24, "text": " turns out if you want to be robust to these three things, these actually correspond to algebraic", "tokens": [51360, 4523, 484, 498, 291, 528, 281, 312, 13956, 281, 613, 1045, 721, 11, 613, 767, 6805, 281, 21989, 299, 51556], "temperature": 0.0, "avg_logprob": -0.06867178495939788, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.005554156843572855}, {"id": 291, "seek": 147640, "start": 1500.24, "end": 1505.1200000000001, "text": " properties that you need to give you that robustness. So associativity gives you robustness to batching.", "tokens": [51556, 7221, 300, 291, 643, 281, 976, 291, 300, 13956, 1287, 13, 407, 4180, 30142, 2709, 291, 13956, 1287, 281, 15245, 278, 13, 51800], "temperature": 0.0, "avg_logprob": -0.06867178495939788, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.005554156843572855}, {"id": 292, "seek": 150512, "start": 1505.12, "end": 1509.6799999999998, "text": " You're indifferent to the parenthesization of messages. Communicativity gives you robustness", "tokens": [50364, 509, 434, 48502, 281, 264, 23350, 279, 2144, 295, 7897, 13, 6800, 299, 30142, 2709, 291, 13956, 1287, 50592], "temperature": 0.0, "avg_logprob": -0.07383190872322799, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0002304903173353523}, {"id": 293, "seek": 150512, "start": 1509.6799999999998, "end": 1515.6, "text": " to reordering and idempotence gives you robustness to duplication. And if you have a set with an", "tokens": [50592, 281, 319, 765, 1794, 293, 4496, 15970, 310, 655, 2709, 291, 13956, 1287, 281, 17154, 399, 13, 400, 498, 291, 362, 257, 992, 365, 364, 50888], "temperature": 0.0, "avg_logprob": -0.07383190872322799, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0002304903173353523}, {"id": 294, "seek": 150512, "start": 1515.6, "end": 1519.52, "text": " operator that satisfies these three properties, that gives you a semi-ladis. So that's why we're", "tokens": [50888, 12973, 300, 44271, 613, 1045, 7221, 11, 300, 2709, 291, 257, 12909, 12, 9290, 271, 13, 407, 300, 311, 983, 321, 434, 51084], "temperature": 0.0, "avg_logprob": -0.07383190872322799, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0002304903173353523}, {"id": 295, "seek": 150512, "start": 1519.52, "end": 1527.28, "text": " talking about semi-ladises for distributed systems. So the conflict-free replicated data type is one", "tokens": [51084, 1417, 466, 12909, 12, 9290, 3598, 337, 12631, 3652, 13, 407, 264, 6596, 12, 10792, 46365, 1412, 2010, 307, 472, 51472], "temperature": 0.0, "avg_logprob": -0.07383190872322799, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0002304903173353523}, {"id": 296, "seek": 150512, "start": 1527.28, "end": 1532.1599999999999, "text": " specific model of a semi-ladis interface, but since it's the most popular one today,", "tokens": [51472, 2685, 2316, 295, 257, 12909, 12, 9290, 271, 9226, 11, 457, 1670, 309, 311, 264, 881, 3743, 472, 965, 11, 51716], "temperature": 0.0, "avg_logprob": -0.07383190872322799, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0002304903173353523}, {"id": 297, "seek": 153216, "start": 1532.24, "end": 1538.5600000000002, "text": " I'm going to talk about it. So the idea is that it's an object-oriented view of a distributed", "tokens": [50368, 286, 478, 516, 281, 751, 466, 309, 13, 407, 264, 1558, 307, 300, 309, 311, 364, 2657, 12, 27414, 1910, 295, 257, 12631, 50684], "temperature": 0.0, "avg_logprob": -0.08443678796818826, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0017006108537316322}, {"id": 298, "seek": 153216, "start": 1538.5600000000002, "end": 1542.0800000000002, "text": " system where you define some object, and you're going to define three methods on that object.", "tokens": [50684, 1185, 689, 291, 6964, 512, 2657, 11, 293, 291, 434, 516, 281, 6964, 1045, 7150, 322, 300, 2657, 13, 50860], "temperature": 0.0, "avg_logprob": -0.08443678796818826, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0017006108537316322}, {"id": 299, "seek": 153216, "start": 1542.72, "end": 1545.76, "text": " And then you can replicate this object across the distributed system,", "tokens": [50892, 400, 550, 291, 393, 25356, 341, 2657, 2108, 264, 12631, 1185, 11, 51044], "temperature": 0.0, "avg_logprob": -0.08443678796818826, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0017006108537316322}, {"id": 300, "seek": 153216, "start": 1545.76, "end": 1551.8400000000001, "text": " and the replicas will converge, regardless of network non-determinism. So you have your merge", "tokens": [51044, 293, 264, 3248, 9150, 486, 41881, 11, 10060, 295, 3209, 2107, 12, 49136, 259, 1434, 13, 407, 291, 362, 428, 22183, 51348], "temperature": 0.0, "avg_logprob": -0.08443678796818826, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0017006108537316322}, {"id": 301, "seek": 153216, "start": 1551.8400000000001, "end": 1558.5600000000002, "text": " operator, which is your associative, commutative, and idempotent ACI semi-ladis operator, which", "tokens": [51348, 12973, 11, 597, 307, 428, 4180, 1166, 11, 800, 325, 1166, 11, 293, 4496, 15970, 310, 317, 8157, 40, 12909, 12, 9290, 271, 12973, 11, 597, 51684], "temperature": 0.0, "avg_logprob": -0.08443678796818826, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0017006108537316322}, {"id": 302, "seek": 155856, "start": 1558.56, "end": 1564.96, "text": " combines the state of two replicas. You have a way to update state, which the requirement is just", "tokens": [50364, 29520, 264, 1785, 295, 732, 3248, 9150, 13, 509, 362, 257, 636, 281, 5623, 1785, 11, 597, 264, 11695, 307, 445, 50684], "temperature": 0.0, "avg_logprob": -0.06405388155291157, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011334925657138228}, {"id": 303, "seek": 155856, "start": 1564.96, "end": 1569.76, "text": " that that's monotone with respect to the partial order induced by this merge operation. And then", "tokens": [50684, 300, 300, 311, 1108, 310, 546, 365, 3104, 281, 264, 14641, 1668, 33991, 538, 341, 22183, 6916, 13, 400, 550, 50924], "temperature": 0.0, "avg_logprob": -0.06405388155291157, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011334925657138228}, {"id": 304, "seek": 155856, "start": 1569.76, "end": 1574.96, "text": " you have a query, which is just a method on this object, but today there's not a specific query", "tokens": [50924, 291, 362, 257, 14581, 11, 597, 307, 445, 257, 3170, 322, 341, 2657, 11, 457, 965, 456, 311, 406, 257, 2685, 14581, 51184], "temperature": 0.0, "avg_logprob": -0.06405388155291157, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011334925657138228}, {"id": 305, "seek": 155856, "start": 1574.96, "end": 1579.44, "text": " language. You don't have any sort of guarantees on what that query does. It just reads this semi-ladis", "tokens": [51184, 2856, 13, 509, 500, 380, 362, 604, 1333, 295, 32567, 322, 437, 300, 14581, 775, 13, 467, 445, 15700, 341, 12909, 12, 9290, 271, 51408], "temperature": 0.0, "avg_logprob": -0.06405388155291157, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011334925657138228}, {"id": 306, "seek": 155856, "start": 1579.44, "end": 1587.2, "text": " state. So we're looking at an example of a CRDT. This comes from the Amazon Dynamo paper for how", "tokens": [51408, 1785, 13, 407, 321, 434, 1237, 412, 364, 1365, 295, 257, 14123, 35, 51, 13, 639, 1487, 490, 264, 6795, 22947, 78, 3035, 337, 577, 51796], "temperature": 0.0, "avg_logprob": -0.06405388155291157, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0011334925657138228}, {"id": 307, "seek": 158720, "start": 1587.2, "end": 1592.72, "text": " they're implementing shopping carts. And the idea is that you have updates that are going to add or", "tokens": [50364, 436, 434, 18114, 8688, 48128, 13, 400, 264, 1558, 307, 300, 291, 362, 9205, 300, 366, 516, 281, 909, 420, 50640], "temperature": 0.0, "avg_logprob": -0.07183482007282536, "compression_ratio": 1.9879032258064515, "no_speech_prob": 0.0005192742682993412}, {"id": 308, "seek": 158720, "start": 1592.72, "end": 1596.56, "text": " remove elements to your shopping cart. In this case, you add a Ferrari to your shopping cart, add a", "tokens": [50640, 4159, 4959, 281, 428, 8688, 5467, 13, 682, 341, 1389, 11, 291, 909, 257, 29828, 281, 428, 8688, 5467, 11, 909, 257, 50832], "temperature": 0.0, "avg_logprob": -0.07183482007282536, "compression_ratio": 1.9879032258064515, "no_speech_prob": 0.0005192742682993412}, {"id": 309, "seek": 158720, "start": 1596.56, "end": 1601.92, "text": " potato, and you can also remove the Ferrari. And the state is going to be represented as two sets,", "tokens": [50832, 7445, 11, 293, 291, 393, 611, 4159, 264, 29828, 13, 400, 264, 1785, 307, 516, 281, 312, 10379, 382, 732, 6352, 11, 51100], "temperature": 0.0, "avg_logprob": -0.07183482007282536, "compression_ratio": 1.9879032258064515, "no_speech_prob": 0.0005192742682993412}, {"id": 310, "seek": 158720, "start": 1601.92, "end": 1608.0800000000002, "text": " a set of items that you've inserted and a set of items that you've removed. And then to merge,", "tokens": [51100, 257, 992, 295, 4754, 300, 291, 600, 27992, 293, 257, 992, 295, 4754, 300, 291, 600, 7261, 13, 400, 550, 281, 22183, 11, 51408], "temperature": 0.0, "avg_logprob": -0.07183482007282536, "compression_ratio": 1.9879032258064515, "no_speech_prob": 0.0005192742682993412}, {"id": 311, "seek": 158720, "start": 1608.0800000000002, "end": 1612.72, "text": " we do a pairwise union of these two sets. And the query, what's actually in my shopping cart I want", "tokens": [51408, 321, 360, 257, 6119, 3711, 11671, 295, 613, 732, 6352, 13, 400, 264, 14581, 11, 437, 311, 767, 294, 452, 8688, 5467, 286, 528, 51640], "temperature": 0.0, "avg_logprob": -0.07183482007282536, "compression_ratio": 1.9879032258064515, "no_speech_prob": 0.0005192742682993412}, {"id": 312, "seek": 161272, "start": 1612.72, "end": 1619.84, "text": " to check out, is set difference. Subtract the removes from the inserts. And there's a few", "tokens": [50364, 281, 1520, 484, 11, 307, 992, 2649, 13, 8511, 83, 1897, 264, 30445, 490, 264, 49163, 13, 400, 456, 311, 257, 1326, 50720], "temperature": 0.0, "avg_logprob": -0.0977083226685883, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00023781516938470304}, {"id": 313, "seek": 161272, "start": 1619.84, "end": 1625.2, "text": " interesting things going on with this example. One is we're guaranteeing the coordination-free", "tokens": [50720, 1880, 721, 516, 322, 365, 341, 1365, 13, 1485, 307, 321, 434, 10815, 278, 264, 21252, 12, 10792, 50988], "temperature": 0.0, "avg_logprob": -0.0977083226685883, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00023781516938470304}, {"id": 314, "seek": 161272, "start": 1627.2, "end": 1633.52, "text": " rights that these two states are going to converge. But our query is a non-monotone query,", "tokens": [51088, 4601, 300, 613, 732, 4368, 366, 516, 281, 41881, 13, 583, 527, 14581, 307, 257, 2107, 12, 3317, 310, 546, 14581, 11, 51404], "temperature": 0.0, "avg_logprob": -0.0977083226685883, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00023781516938470304}, {"id": 315, "seek": 161272, "start": 1633.52, "end": 1638.16, "text": " which the column theorem tells us is not a coordination-free query. So CRDTs are not giving", "tokens": [51404, 597, 264, 7738, 20904, 5112, 505, 307, 406, 257, 21252, 12, 10792, 14581, 13, 407, 14123, 35, 33424, 366, 406, 2902, 51636], "temperature": 0.0, "avg_logprob": -0.0977083226685883, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.00023781516938470304}, {"id": 316, "seek": 163816, "start": 1638.16, "end": 1643.1200000000001, "text": " us the invariant that the column theorem requires on queries, which is that if we output a tuple", "tokens": [50364, 505, 264, 33270, 394, 300, 264, 7738, 20904, 7029, 322, 24109, 11, 597, 307, 300, 498, 321, 5598, 257, 2604, 781, 50612], "temperature": 0.0, "avg_logprob": -0.06590932813184015, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.001700555789284408}, {"id": 317, "seek": 163816, "start": 1643.1200000000001, "end": 1648.16, "text": " at a certain point in time, we're never going to retract that tuple in the future. Here, over time,", "tokens": [50612, 412, 257, 1629, 935, 294, 565, 11, 321, 434, 1128, 516, 281, 41107, 300, 2604, 781, 294, 264, 2027, 13, 1692, 11, 670, 565, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06590932813184015, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.001700555789284408}, {"id": 318, "seek": 163816, "start": 1648.16, "end": 1656.0800000000002, "text": " we will retract tuples as the remove set grows. We had a vision paper in BLDB this year about", "tokens": [50864, 321, 486, 41107, 2604, 2622, 382, 264, 4159, 992, 13156, 13, 492, 632, 257, 5201, 3035, 294, 15132, 27735, 341, 1064, 466, 51260], "temperature": 0.0, "avg_logprob": -0.06590932813184015, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.001700555789284408}, {"id": 319, "seek": 163816, "start": 1656.0800000000002, "end": 1661.28, "text": " this gap between what CRDTs guarantee and what the column theorem guarantees and ideas for how to", "tokens": [51260, 341, 7417, 1296, 437, 14123, 35, 33424, 10815, 293, 437, 264, 7738, 20904, 32567, 293, 3487, 337, 577, 281, 51520], "temperature": 0.0, "avg_logprob": -0.06590932813184015, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.001700555789284408}, {"id": 320, "seek": 163816, "start": 1661.28, "end": 1667.76, "text": " resolve it. Another thing you might have noticed that's kind of odd about that representation", "tokens": [51520, 14151, 309, 13, 3996, 551, 291, 1062, 362, 5694, 300, 311, 733, 295, 7401, 466, 300, 10290, 51844], "temperature": 0.0, "avg_logprob": -0.06590932813184015, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.001700555789284408}, {"id": 321, "seek": 166776, "start": 1667.76, "end": 1674.32, "text": " of data is that if you think about how we might represent updates like this to a shopping cart", "tokens": [50364, 295, 1412, 307, 300, 498, 291, 519, 466, 577, 321, 1062, 2906, 9205, 411, 341, 281, 257, 8688, 5467, 50692], "temperature": 0.0, "avg_logprob": -0.12650429436919886, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.001548625878058374}, {"id": 322, "seek": 166776, "start": 1674.32, "end": 1679.2, "text": " in a database, you might have imagined that we would have a count on each item and we would", "tokens": [50692, 294, 257, 8149, 11, 291, 1062, 362, 16590, 300, 321, 576, 362, 257, 1207, 322, 1184, 3174, 293, 321, 576, 50936], "temperature": 0.0, "avg_logprob": -0.12650429436919886, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.001548625878058374}, {"id": 323, "seek": 166776, "start": 1679.2, "end": 1682.72, "text": " increment that count when we add an item and we decrement that count when we remove an item.", "tokens": [50936, 26200, 300, 1207, 562, 321, 909, 364, 3174, 293, 321, 6853, 518, 300, 1207, 562, 321, 4159, 364, 3174, 13, 51112], "temperature": 0.0, "avg_logprob": -0.12650429436919886, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.001548625878058374}, {"id": 324, "seek": 166776, "start": 1682.72, "end": 1686.72, "text": " This is what you'd see, something like incremental view maintenance where your update", "tokens": [51112, 639, 307, 437, 291, 1116, 536, 11, 746, 411, 35759, 1910, 11258, 689, 428, 5623, 51312], "temperature": 0.0, "avg_logprob": -0.12650429436919886, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.001548625878058374}, {"id": 325, "seek": 166776, "start": 1686.72, "end": 1692.4, "text": " operation forms an abelian group, not a semi-ladis. So why not do something like that?", "tokens": [51312, 6916, 6422, 364, 410, 338, 952, 1594, 11, 406, 257, 12909, 12, 9290, 271, 13, 407, 983, 406, 360, 746, 411, 300, 30, 51596], "temperature": 0.0, "avg_logprob": -0.12650429436919886, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.001548625878058374}, {"id": 326, "seek": 169240, "start": 1693.2800000000002, "end": 1699.52, "text": " Well, for one, it doesn't form a semi-ladis and it's not immediately obvious how to convert it", "tokens": [50408, 1042, 11, 337, 472, 11, 309, 1177, 380, 1254, 257, 12909, 12, 9290, 271, 293, 309, 311, 406, 4258, 6322, 577, 281, 7620, 309, 50720], "temperature": 0.0, "avg_logprob": -0.13594526940203727, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0003459859872236848}, {"id": 327, "seek": 169240, "start": 1699.52, "end": 1705.76, "text": " into one. So this representation is two sets, what you call a two-phase set. It's more obviously", "tokens": [50720, 666, 472, 13, 407, 341, 10290, 307, 732, 6352, 11, 437, 291, 818, 257, 732, 12, 43331, 992, 13, 467, 311, 544, 2745, 51032], "temperature": 0.0, "avg_logprob": -0.13594526940203727, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0003459859872236848}, {"id": 328, "seek": 169240, "start": 1705.76, "end": 1712.5600000000002, "text": " monotonously growing update operation, but it turns out it actually is possible to convert", "tokens": [51032, 1108, 27794, 5098, 4194, 5623, 6916, 11, 457, 309, 4523, 484, 309, 767, 307, 1944, 281, 7620, 51372], "temperature": 0.0, "avg_logprob": -0.13594526940203727, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0003459859872236848}, {"id": 329, "seek": 169240, "start": 1712.5600000000002, "end": 1718.4, "text": " this abelian group representation into a valid semi-ladis in terms of being robust to network", "tokens": [51372, 341, 410, 338, 952, 1594, 10290, 666, 257, 7363, 12909, 12, 9290, 271, 294, 2115, 295, 885, 13956, 281, 3209, 51664], "temperature": 0.0, "avg_logprob": -0.13594526940203727, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.0003459859872236848}, {"id": 330, "seek": 171840, "start": 1718.4, "end": 1723.76, "text": " non-determinism. I won't go into all the details on that, but it's based on what Joe is saying with", "tokens": [50364, 2107, 12, 49136, 259, 1434, 13, 286, 1582, 380, 352, 666, 439, 264, 4365, 322, 300, 11, 457, 309, 311, 2361, 322, 437, 6807, 307, 1566, 365, 50632], "temperature": 0.0, "avg_logprob": -0.1096462110678355, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0038238330744206905}, {"id": 331, "seek": 171840, "start": 1723.76, "end": 1732.0, "text": " these vector clocks where you wrap the states basically in a vector clock which forms a semi-ladis.", "tokens": [50632, 613, 8062, 41528, 689, 291, 7019, 264, 4368, 1936, 294, 257, 8062, 7830, 597, 6422, 257, 12909, 12, 9290, 271, 13, 51044], "temperature": 0.0, "avg_logprob": -0.1096462110678355, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0038238330744206905}, {"id": 332, "seek": 171840, "start": 1733.2, "end": 1737.68, "text": " The downside of doing this is that vector clocks require linear memory and the number of replicas", "tokens": [51104, 440, 25060, 295, 884, 341, 307, 300, 8062, 41528, 3651, 8213, 4675, 293, 264, 1230, 295, 3248, 9150, 51328], "temperature": 0.0, "avg_logprob": -0.1096462110678355, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0038238330744206905}, {"id": 333, "seek": 171840, "start": 1737.68, "end": 1742.72, "text": " in the system, so that's the reason why people wouldn't use this representation today. But we", "tokens": [51328, 294, 264, 1185, 11, 370, 300, 311, 264, 1778, 983, 561, 2759, 380, 764, 341, 10290, 965, 13, 583, 321, 51580], "temperature": 0.0, "avg_logprob": -0.1096462110678355, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.0038238330744206905}, {"id": 334, "seek": 174272, "start": 1742.72, "end": 1750.48, "text": " have some work on a protocol for enforcing this kind of conversion into a semi-ladis in", "tokens": [50364, 362, 512, 589, 322, 257, 10336, 337, 25495, 2175, 341, 733, 295, 14298, 666, 257, 12909, 12, 9290, 271, 294, 50752], "temperature": 0.0, "avg_logprob": -0.08141186802657609, "compression_ratio": 1.6711711711711712, "no_speech_prob": 8.749963308218867e-05}, {"id": 335, "seek": 174272, "start": 1750.48, "end": 1754.64, "text": " constant rather than linear space. So if anyone's interested in that idea, definitely come find me", "tokens": [50752, 5754, 2831, 813, 8213, 1901, 13, 407, 498, 2878, 311, 3102, 294, 300, 1558, 11, 2138, 808, 915, 385, 50960], "temperature": 0.0, "avg_logprob": -0.08141186802657609, "compression_ratio": 1.6711711711711712, "no_speech_prob": 8.749963308218867e-05}, {"id": 336, "seek": 174272, "start": 1754.64, "end": 1765.6000000000001, "text": " and talk about it. Okay, so like I said, the semi-ladis model today does not have a query", "tokens": [50960, 293, 751, 466, 309, 13, 1033, 11, 370, 411, 286, 848, 11, 264, 12909, 12, 9290, 271, 2316, 965, 775, 406, 362, 257, 14581, 51508], "temperature": 0.0, "avg_logprob": -0.08141186802657609, "compression_ratio": 1.6711711711711712, "no_speech_prob": 8.749963308218867e-05}, {"id": 337, "seek": 174272, "start": 1765.6000000000001, "end": 1770.8, "text": " language on top of it. So what might we want out of a query language for the semi-ladis model?", "tokens": [51508, 2856, 322, 1192, 295, 309, 13, 407, 437, 1062, 321, 528, 484, 295, 257, 14581, 2856, 337, 264, 12909, 12, 9290, 271, 2316, 30, 51768], "temperature": 0.0, "avg_logprob": -0.08141186802657609, "compression_ratio": 1.6711711711711712, "no_speech_prob": 8.749963308218867e-05}, {"id": 338, "seek": 177080, "start": 1770.8, "end": 1776.6399999999999, "text": " Well, we want expressivity. Like we saw in the shopping cart example, we need set difference,", "tokens": [50364, 1042, 11, 321, 528, 5109, 4253, 13, 1743, 321, 1866, 294, 264, 8688, 5467, 1365, 11, 321, 643, 992, 2649, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1332538453015414, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.00022339179122354835}, {"id": 339, "seek": 177080, "start": 1776.6399999999999, "end": 1784.08, "text": " so we need negation. Also recursion, something like datalog. We also want obviously classical", "tokens": [50656, 370, 321, 643, 2485, 399, 13, 2743, 20560, 313, 11, 746, 411, 1137, 44434, 13, 492, 611, 528, 2745, 13735, 51028], "temperature": 0.0, "avg_logprob": -0.1332538453015414, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.00022339179122354835}, {"id": 340, "seek": 177080, "start": 1784.08, "end": 1788.56, "text": " query optimization options. We want identities that we can use to transform our query,", "tokens": [51028, 14581, 19618, 3956, 13, 492, 528, 24239, 300, 321, 393, 764, 281, 4088, 527, 14581, 11, 51252], "temperature": 0.0, "avg_logprob": -0.1332538453015414, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.00022339179122354835}, {"id": 341, "seek": 177080, "start": 1788.56, "end": 1793.52, "text": " get better performance. And we want to be able to do monotonicity analysis as well as functional", "tokens": [51252, 483, 1101, 3389, 13, 400, 321, 528, 281, 312, 1075, 281, 360, 1108, 310, 11630, 507, 5215, 382, 731, 382, 11745, 51500], "temperature": 0.0, "avg_logprob": -0.1332538453015414, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.00022339179122354835}, {"id": 342, "seek": 179352, "start": 1793.52, "end": 1802.08, "text": " and dependency analysis for partitioning. And so something like datalog for semi-ladises", "tokens": [50364, 293, 33621, 5215, 337, 24808, 278, 13, 400, 370, 746, 411, 1137, 44434, 337, 12909, 12, 9290, 3598, 50792], "temperature": 0.0, "avg_logprob": -0.14497409215787563, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.00505952350795269}, {"id": 343, "seek": 179352, "start": 1802.96, "end": 1806.96, "text": " might be a good fit here, but there's a lot to explore. And so now Joe is going to talk about", "tokens": [50836, 1062, 312, 257, 665, 3318, 510, 11, 457, 456, 311, 257, 688, 281, 6839, 13, 400, 370, 586, 6807, 307, 516, 281, 751, 466, 51036], "temperature": 0.0, "avg_logprob": -0.14497409215787563, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.00505952350795269}, {"id": 344, "seek": 179352, "start": 1807.52, "end": 1809.84, "text": " this monotonicity analysis and functional dependency analysis.", "tokens": [51064, 341, 1108, 310, 11630, 507, 5215, 293, 11745, 33621, 5215, 13, 51180], "temperature": 0.0, "avg_logprob": -0.14497409215787563, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.00505952350795269}, {"id": 345, "seek": 179352, "start": 1816.56, "end": 1821.28, "text": " I should say from the previous slide that some of this is things we took a crack at with", "tokens": [51516, 286, 820, 584, 490, 264, 3894, 4137, 300, 512, 295, 341, 307, 721, 321, 1890, 257, 6226, 412, 365, 51752], "temperature": 0.0, "avg_logprob": -0.14497409215787563, "compression_ratio": 1.6057692307692308, "no_speech_prob": 0.00505952350795269}, {"id": 346, "seek": 182128, "start": 1821.28, "end": 1825.84, "text": " BlueMal, so it's not that we've done nothing here. There's some answers to these questions,", "tokens": [50364, 8510, 44, 304, 11, 370, 309, 311, 406, 300, 321, 600, 1096, 1825, 510, 13, 821, 311, 512, 6338, 281, 613, 1651, 11, 50592], "temperature": 0.0, "avg_logprob": -0.12732714414596558, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.018825460225343704}, {"id": 347, "seek": 182128, "start": 1825.84, "end": 1831.52, "text": " but there's also work to be done. All right, so I wanted to step back and review for you folks,", "tokens": [50592, 457, 456, 311, 611, 589, 281, 312, 1096, 13, 1057, 558, 11, 370, 286, 1415, 281, 1823, 646, 293, 3131, 337, 291, 4024, 11, 50876], "temperature": 0.0, "avg_logprob": -0.12732714414596558, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.018825460225343704}, {"id": 348, "seek": 182128, "start": 1831.52, "end": 1837.36, "text": " the COM Theorem, which I know is sort of in a sub-corner of the pods community and not everyone's", "tokens": [50876, 264, 35074, 440, 37956, 11, 597, 286, 458, 307, 1333, 295, 294, 257, 1422, 12, 19558, 1193, 295, 264, 31925, 1768, 293, 406, 1518, 311, 51168], "temperature": 0.0, "avg_logprob": -0.12732714414596558, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.018825460225343704}, {"id": 349, "seek": 182128, "start": 1837.36, "end": 1841.36, "text": " going to be familiar with it, but I think it's useful to go over. This will be high level,", "tokens": [51168, 516, 281, 312, 4963, 365, 309, 11, 457, 286, 519, 309, 311, 4420, 281, 352, 670, 13, 639, 486, 312, 1090, 1496, 11, 51368], "temperature": 0.0, "avg_logprob": -0.12732714414596558, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.018825460225343704}, {"id": 350, "seek": 182128, "start": 1841.36, "end": 1847.2, "text": " but hopefully helpful enough for you to get into the game. So the challenge is that we're going to", "tokens": [51368, 457, 4696, 4961, 1547, 337, 291, 281, 483, 666, 264, 1216, 13, 407, 264, 3430, 307, 300, 321, 434, 516, 281, 51660], "temperature": 0.0, "avg_logprob": -0.12732714414596558, "compression_ratio": 1.6493055555555556, "no_speech_prob": 0.018825460225343704}, {"id": 351, "seek": 184720, "start": 1847.2, "end": 1852.8, "text": " have computers spread across the globe and we want our replicas to be consistent. So we have", "tokens": [50364, 362, 10807, 3974, 2108, 264, 15371, 293, 321, 528, 527, 3248, 9150, 281, 312, 8398, 13, 407, 321, 362, 50644], "temperature": 0.0, "avg_logprob": -0.1037322694221429, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0026312076952308416}, {"id": 352, "seek": 184720, "start": 1852.8, "end": 1857.92, "text": " this nice couple here, they're in different places, and the classic example of replica consistency", "tokens": [50644, 341, 1481, 1916, 510, 11, 436, 434, 294, 819, 3190, 11, 293, 264, 7230, 1365, 295, 35456, 14416, 50900], "temperature": 0.0, "avg_logprob": -0.1037322694221429, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0026312076952308416}, {"id": 353, "seek": 184720, "start": 1857.92, "end": 1861.76, "text": " is data replication. So forget about programs, we're just going to have data, kind of like the", "tokens": [50900, 307, 1412, 39911, 13, 407, 2870, 466, 4268, 11, 321, 434, 445, 516, 281, 362, 1412, 11, 733, 295, 411, 264, 51092], "temperature": 0.0, "avg_logprob": -0.1037322694221429, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0026312076952308416}, {"id": 354, "seek": 184720, "start": 1861.76, "end": 1867.1200000000001, "text": " CRDT model. And I want everybody to have common beliefs about the data, at least eventually.", "tokens": [51092, 14123, 35, 51, 2316, 13, 400, 286, 528, 2201, 281, 362, 2689, 13585, 466, 264, 1412, 11, 412, 1935, 4728, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1037322694221429, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0026312076952308416}, {"id": 355, "seek": 184720, "start": 1867.8400000000001, "end": 1872.16, "text": " So these two folks currently both believe that X is love, which is lovely, but if it's a beautiful", "tokens": [51396, 407, 613, 732, 4024, 4362, 1293, 1697, 300, 1783, 307, 959, 11, 597, 307, 7496, 11, 457, 498, 309, 311, 257, 2238, 51612], "temperature": 0.0, "avg_logprob": -0.1037322694221429, "compression_ratio": 1.6713286713286712, "no_speech_prob": 0.0026312076952308416}, {"id": 356, "seek": 187216, "start": 1872.16, "end": 1877.6000000000001, "text": " variable, things could change, right? And that's very sad. And once they disagree on the value,", "tokens": [50364, 7006, 11, 721, 727, 1319, 11, 558, 30, 400, 300, 311, 588, 4227, 13, 400, 1564, 436, 14091, 322, 264, 2158, 11, 50636], "temperature": 0.0, "avg_logprob": -0.08957896722811405, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.005729702301323414}, {"id": 357, "seek": 187216, "start": 1877.6000000000001, "end": 1881.6000000000001, "text": " they might make decisions based on their disagreement that will lead to further", "tokens": [50636, 436, 1062, 652, 5327, 2361, 322, 641, 38947, 300, 486, 1477, 281, 3052, 50836], "temperature": 0.0, "avg_logprob": -0.08957896722811405, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.005729702301323414}, {"id": 358, "seek": 187216, "start": 1881.6000000000001, "end": 1885.92, "text": " divergence. This is sometimes called the split brain problem, because you can't put it back", "tokens": [50836, 47387, 13, 639, 307, 2171, 1219, 264, 7472, 3567, 1154, 11, 570, 291, 393, 380, 829, 309, 646, 51052], "temperature": 0.0, "avg_logprob": -0.08957896722811405, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.005729702301323414}, {"id": 359, "seek": 187216, "start": 1885.92, "end": 1892.88, "text": " together later on, it's too messy. And so we want to generalize the idea of consistency of data", "tokens": [51052, 1214, 1780, 322, 11, 309, 311, 886, 16191, 13, 400, 370, 321, 528, 281, 2674, 1125, 264, 1558, 295, 14416, 295, 1412, 51400], "temperature": 0.0, "avg_logprob": -0.08957896722811405, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.005729702301323414}, {"id": 360, "seek": 187216, "start": 1892.88, "end": 1898.16, "text": " replication to consistency of program outcomes. So I'm not just interested in the data, I'm interested", "tokens": [51400, 39911, 281, 14416, 295, 1461, 10070, 13, 407, 286, 478, 406, 445, 3102, 294, 264, 1412, 11, 286, 478, 3102, 51664], "temperature": 0.0, "avg_logprob": -0.08957896722811405, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.005729702301323414}, {"id": 361, "seek": 189816, "start": 1898.16, "end": 1903.52, "text": " in the queries, if you will, right? Much more powerful, and it will allow us to cheat sometimes,", "tokens": [50364, 294, 264, 24109, 11, 498, 291, 486, 11, 558, 30, 12313, 544, 4005, 11, 293, 309, 486, 2089, 505, 281, 17470, 2171, 11, 50632], "temperature": 0.0, "avg_logprob": -0.09733036817130396, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.0010321208974346519}, {"id": 362, "seek": 189816, "start": 1903.52, "end": 1908.3200000000002, "text": " the data could be inconsistent if the query outcomes are not, right? So it'll give us more", "tokens": [50632, 264, 1412, 727, 312, 36891, 498, 264, 14581, 10070, 366, 406, 11, 558, 30, 407, 309, 603, 976, 505, 544, 50872], "temperature": 0.0, "avg_logprob": -0.09733036817130396, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.0010321208974346519}, {"id": 363, "seek": 189816, "start": 1908.3200000000002, "end": 1914.3200000000002, "text": " ability to relax our coordination. So we'd like to generalize this to program outcomes independent", "tokens": [50872, 3485, 281, 5789, 527, 21252, 13, 407, 321, 1116, 411, 281, 2674, 1125, 341, 281, 1461, 10070, 6695, 51172], "temperature": 0.0, "avg_logprob": -0.09733036817130396, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.0010321208974346519}, {"id": 364, "seek": 189816, "start": 1914.3200000000002, "end": 1920.4, "text": " of data races when we can. The classical solution to this stuff is coordination. This is what things", "tokens": [51172, 295, 1412, 15484, 562, 321, 393, 13, 440, 13735, 3827, 281, 341, 1507, 307, 21252, 13, 639, 307, 437, 721, 51476], "temperature": 0.0, "avg_logprob": -0.09733036817130396, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.0010321208974346519}, {"id": 365, "seek": 189816, "start": 1920.4, "end": 1925.44, "text": " like Paxos and Two-Phase commit were invented to solve. And the way they solve it is by saying,", "tokens": [51476, 411, 430, 2797, 329, 293, 4453, 12, 24382, 651, 5599, 645, 14479, 281, 5039, 13, 400, 264, 636, 436, 5039, 309, 307, 538, 1566, 11, 51728], "temperature": 0.0, "avg_logprob": -0.09733036817130396, "compression_ratio": 1.6484641638225257, "no_speech_prob": 0.0010321208974346519}, {"id": 366, "seek": 192544, "start": 1925.52, "end": 1929.8400000000001, "text": " what if we were just on one computer with one processor? Maybe we could implement that in a", "tokens": [50368, 437, 498, 321, 645, 445, 322, 472, 3820, 365, 472, 15321, 30, 2704, 321, 727, 4445, 300, 294, 257, 50584], "temperature": 0.0, "avg_logprob": -0.077158416526905, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.0022515133023262024}, {"id": 367, "seek": 192544, "start": 1929.8400000000001, "end": 1934.64, "text": " distributed fashion, which is a very heavy handed solution, right? You say that our solution to", "tokens": [50584, 12631, 6700, 11, 597, 307, 257, 588, 4676, 16013, 3827, 11, 558, 30, 509, 584, 300, 527, 3827, 281, 50824], "temperature": 0.0, "avg_logprob": -0.077158416526905, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.0022515133023262024}, {"id": 368, "seek": 192544, "start": 1934.64, "end": 1938.64, "text": " parallelism is to remove it. And how can we remove it in the distributed context? Well,", "tokens": [50824, 8952, 1434, 307, 281, 4159, 309, 13, 400, 577, 393, 321, 4159, 309, 294, 264, 12631, 4319, 30, 1042, 11, 51024], "temperature": 0.0, "avg_logprob": -0.077158416526905, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.0022515133023262024}, {"id": 369, "seek": 192544, "start": 1938.64, "end": 1944.3200000000002, "text": " it's expensive. But here's how it goes, right? On a single node, you use atomic instructions,", "tokens": [51024, 309, 311, 5124, 13, 583, 510, 311, 577, 309, 1709, 11, 558, 30, 1282, 257, 2167, 9984, 11, 291, 764, 22275, 9415, 11, 51308], "temperature": 0.0, "avg_logprob": -0.077158416526905, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.0022515133023262024}, {"id": 370, "seek": 192544, "start": 1944.3200000000002, "end": 1948.3200000000002, "text": " right? So if you have shared memory, you can use atomic instructions, or maybe you use a locking", "tokens": [51308, 558, 30, 407, 498, 291, 362, 5507, 4675, 11, 291, 393, 764, 22275, 9415, 11, 420, 1310, 291, 764, 257, 23954, 51508], "temperature": 0.0, "avg_logprob": -0.077158416526905, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.0022515133023262024}, {"id": 371, "seek": 192544, "start": 1948.3200000000002, "end": 1952.24, "text": " system. In the distributed environment, you use something like Paxos or Two-Phase commit. And", "tokens": [51508, 1185, 13, 682, 264, 12631, 2823, 11, 291, 764, 746, 411, 430, 2797, 329, 420, 4453, 12, 24382, 651, 5599, 13, 400, 51704], "temperature": 0.0, "avg_logprob": -0.077158416526905, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.0022515133023262024}, {"id": 372, "seek": 195224, "start": 1952.32, "end": 1957.92, "text": " at every scale, as you saw in that ANA work, you'd like to not do these things. So even on a single", "tokens": [50368, 412, 633, 4373, 11, 382, 291, 1866, 294, 300, 5252, 32, 589, 11, 291, 1116, 411, 281, 406, 360, 613, 721, 13, 407, 754, 322, 257, 2167, 50648], "temperature": 0.0, "avg_logprob": -0.09507655380363751, "compression_ratio": 1.6646884272997033, "no_speech_prob": 0.0007095987675711513}, {"id": 373, "seek": 195224, "start": 1957.92, "end": 1961.52, "text": " machine, you really don't want to be doing coordination. And certainly in the distributed", "tokens": [50648, 3479, 11, 291, 534, 500, 380, 528, 281, 312, 884, 21252, 13, 400, 3297, 294, 264, 12631, 50828], "temperature": 0.0, "avg_logprob": -0.09507655380363751, "compression_ratio": 1.6646884272997033, "no_speech_prob": 0.0007095987675711513}, {"id": 374, "seek": 195224, "start": 1961.52, "end": 1965.92, "text": " setting, this is very heavy weight. And there's people who will tell you at great length why", "tokens": [50828, 3287, 11, 341, 307, 588, 4676, 3364, 13, 400, 456, 311, 561, 567, 486, 980, 291, 412, 869, 4641, 983, 51048], "temperature": 0.0, "avg_logprob": -0.09507655380363751, "compression_ratio": 1.6646884272997033, "no_speech_prob": 0.0007095987675711513}, {"id": 375, "seek": 195224, "start": 1965.92, "end": 1969.68, "text": " they don't let the developers in Amazon call these libraries unless they have, you know,", "tokens": [51048, 436, 500, 380, 718, 264, 8849, 294, 6795, 818, 613, 15148, 5969, 436, 362, 11, 291, 458, 11, 51236], "temperature": 0.0, "avg_logprob": -0.09507655380363751, "compression_ratio": 1.6646884272997033, "no_speech_prob": 0.0007095987675711513}, {"id": 376, "seek": 195224, "start": 1969.68, "end": 1974.48, "text": " 16 gold stars. Because it will slow down the whole environment and create queue backups and all", "tokens": [51236, 3165, 3821, 6105, 13, 1436, 309, 486, 2964, 760, 264, 1379, 2823, 293, 1884, 18639, 50160, 293, 439, 51476], "temperature": 0.0, "avg_logprob": -0.09507655380363751, "compression_ratio": 1.6646884272997033, "no_speech_prob": 0.0007095987675711513}, {"id": 377, "seek": 195224, "start": 1974.48, "end": 1979.44, "text": " kinds of horrible things. So when can we avoid coordination? This was a question that I asked", "tokens": [51476, 3685, 295, 9263, 721, 13, 407, 562, 393, 321, 5042, 21252, 30, 639, 390, 257, 1168, 300, 286, 2351, 51724], "temperature": 0.0, "avg_logprob": -0.09507655380363751, "compression_ratio": 1.6646884272997033, "no_speech_prob": 0.0007095987675711513}, {"id": 378, "seek": 197944, "start": 1979.44, "end": 1983.6000000000001, "text": " as a lazy professor, because I was thinking maybe I should learn and teach Paxos, and I kind of", "tokens": [50364, 382, 257, 14847, 8304, 11, 570, 286, 390, 1953, 1310, 286, 820, 1466, 293, 2924, 430, 2797, 329, 11, 293, 286, 733, 295, 50572], "temperature": 0.0, "avg_logprob": -0.08796265125274658, "compression_ratio": 1.6985507246376812, "no_speech_prob": 0.0008557792170904577}, {"id": 379, "seek": 197944, "start": 1983.6000000000001, "end": 1987.68, "text": " didn't want to. So I was like, maybe, you know, maybe we don't need this stuff. Maybe Lamport's", "tokens": [50572, 994, 380, 528, 281, 13, 407, 286, 390, 411, 11, 1310, 11, 291, 458, 11, 1310, 321, 500, 380, 643, 341, 1507, 13, 2704, 441, 1215, 477, 311, 50776], "temperature": 0.0, "avg_logprob": -0.08796265125274658, "compression_ratio": 1.6985507246376812, "no_speech_prob": 0.0008557792170904577}, {"id": 380, "seek": 197944, "start": 1987.68, "end": 1992.72, "text": " just a bunch of bunk. So that's kind of where this started, sheer laziness, intellectual laziness,", "tokens": [50776, 445, 257, 3840, 295, 25125, 13, 407, 300, 311, 733, 295, 689, 341, 1409, 11, 23061, 19320, 1324, 11, 12576, 19320, 1324, 11, 51028], "temperature": 0.0, "avg_logprob": -0.08796265125274658, "compression_ratio": 1.6985507246376812, "no_speech_prob": 0.0008557792170904577}, {"id": 381, "seek": 197944, "start": 1992.72, "end": 1997.6000000000001, "text": " which I will cop to. But what it led to, sometimes when you ask a question about how can I be lazy,", "tokens": [51028, 597, 286, 486, 2971, 281, 13, 583, 437, 309, 4684, 281, 11, 2171, 562, 291, 1029, 257, 1168, 466, 577, 393, 286, 312, 14847, 11, 51272], "temperature": 0.0, "avg_logprob": -0.08796265125274658, "compression_ratio": 1.6985507246376812, "no_speech_prob": 0.0008557792170904577}, {"id": 382, "seek": 197944, "start": 1997.6000000000001, "end": 2001.6000000000001, "text": " you end up asking a question that turns out to be quite interesting. I think that's what arose here.", "tokens": [51272, 291, 917, 493, 3365, 257, 1168, 300, 4523, 484, 281, 312, 1596, 1880, 13, 286, 519, 300, 311, 437, 37192, 510, 13, 51472], "temperature": 0.0, "avg_logprob": -0.08796265125274658, "compression_ratio": 1.6985507246376812, "no_speech_prob": 0.0008557792170904577}, {"id": 383, "seek": 197944, "start": 2001.6000000000001, "end": 2006.24, "text": " And I'm seeing this not only in my work, but in other places. Back in the 20th century, if you", "tokens": [51472, 400, 286, 478, 2577, 341, 406, 787, 294, 452, 589, 11, 457, 294, 661, 3190, 13, 5833, 294, 264, 945, 392, 4901, 11, 498, 291, 51704], "temperature": 0.0, "avg_logprob": -0.08796265125274658, "compression_ratio": 1.6985507246376812, "no_speech_prob": 0.0008557792170904577}, {"id": 384, "seek": 200624, "start": 2006.24, "end": 2011.2, "text": " will, the Lamport Gray era, we were trying to emulate sequential computation. We were doing", "tokens": [50364, 486, 11, 264, 441, 1215, 477, 22668, 4249, 11, 321, 645, 1382, 281, 45497, 42881, 24903, 13, 492, 645, 884, 50612], "temperature": 0.0, "avg_logprob": -0.08565722013774671, "compression_ratio": 1.7746913580246915, "no_speech_prob": 0.004330707713961601}, {"id": 385, "seek": 200624, "start": 2011.2, "end": 2015.44, "text": " everything we could to give the programmer the illusion of a sequential computer. And it was all", "tokens": [50612, 1203, 321, 727, 281, 976, 264, 32116, 264, 18854, 295, 257, 42881, 3820, 13, 400, 309, 390, 439, 50824], "temperature": 0.0, "avg_logprob": -0.08565722013774671, "compression_ratio": 1.7746913580246915, "no_speech_prob": 0.004330707713961601}, {"id": 386, "seek": 200624, "start": 2015.44, "end": 2020.24, "text": " about, you know, very low level stuff, reads and writes, accesses and stores, right? And then", "tokens": [50824, 466, 11, 291, 458, 11, 588, 2295, 1496, 1507, 11, 15700, 293, 13657, 11, 2105, 279, 293, 9512, 11, 558, 30, 400, 550, 51064], "temperature": 0.0, "avg_logprob": -0.08565722013774671, "compression_ratio": 1.7746913580246915, "no_speech_prob": 0.004330707713961601}, {"id": 387, "seek": 200624, "start": 2021.1200000000001, "end": 2025.92, "text": " guarantees of order, total order, linearizability and serializability. And this was all based on", "tokens": [51108, 32567, 295, 1668, 11, 3217, 1668, 11, 8213, 590, 2310, 293, 17436, 590, 2310, 13, 400, 341, 390, 439, 2361, 322, 51348], "temperature": 0.0, "avg_logprob": -0.08565722013774671, "compression_ratio": 1.7746913580246915, "no_speech_prob": 0.004330707713961601}, {"id": 388, "seek": 200624, "start": 2025.92, "end": 2030.48, "text": " the idea that programmers are hopeless. They'll write all kinds of crazy code. And the only thing", "tokens": [51348, 264, 1558, 300, 41504, 366, 27317, 13, 814, 603, 2464, 439, 3685, 295, 3219, 3089, 13, 400, 264, 787, 551, 51576], "temperature": 0.0, "avg_logprob": -0.08565722013774671, "compression_ratio": 1.7746913580246915, "no_speech_prob": 0.004330707713961601}, {"id": 389, "seek": 200624, "start": 2030.48, "end": 2034.08, "text": " they understand is sequential computers. So we'll make the worst case assumption that their stuff", "tokens": [51576, 436, 1223, 307, 42881, 10807, 13, 407, 321, 603, 652, 264, 5855, 1389, 15302, 300, 641, 1507, 51756], "temperature": 0.0, "avg_logprob": -0.08565722013774671, "compression_ratio": 1.7746913580246915, "no_speech_prob": 0.004330707713961601}, {"id": 390, "seek": 203408, "start": 2034.08, "end": 2039.4399999999998, "text": " wouldn't work in parallel, right? And we'll give them mechanisms for avoiding parallelism.", "tokens": [50364, 2759, 380, 589, 294, 8952, 11, 558, 30, 400, 321, 603, 976, 552, 15902, 337, 20220, 8952, 1434, 13, 50632], "temperature": 0.0, "avg_logprob": -0.11556734909882417, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.0020504132844507694}, {"id": 391, "seek": 203408, "start": 2039.4399999999998, "end": 2044.3999999999999, "text": " Seems like a terrible thing to do in a parallel environment. Yeah. So what's happening in the", "tokens": [50632, 22524, 411, 257, 6237, 551, 281, 360, 294, 257, 8952, 2823, 13, 865, 13, 407, 437, 311, 2737, 294, 264, 50880], "temperature": 0.0, "avg_logprob": -0.11556734909882417, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.0020504132844507694}, {"id": 392, "seek": 203408, "start": 2044.3999999999999, "end": 2049.6, "text": " 21st century is if we lift our, so this is all great. And sometimes you need it. I don't mean", "tokens": [50880, 5080, 372, 4901, 307, 498, 321, 5533, 527, 11, 370, 341, 307, 439, 869, 13, 400, 2171, 291, 643, 309, 13, 286, 500, 380, 914, 51140], "temperature": 0.0, "avg_logprob": -0.11556734909882417, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.0020504132844507694}, {"id": 393, "seek": 203408, "start": 2049.6, "end": 2053.12, "text": " to denigrate the work. This is obviously foundational, touring awards. I use this stuff. I teach this", "tokens": [51140, 281, 1441, 328, 4404, 264, 589, 13, 639, 307, 2745, 32195, 11, 32487, 15193, 13, 286, 764, 341, 1507, 13, 286, 2924, 341, 51316], "temperature": 0.0, "avg_logprob": -0.11556734909882417, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.0020504132844507694}, {"id": 394, "seek": 203408, "start": 2053.12, "end": 2057.44, "text": " stuff. It's all good. But when we don't need to use it, even better, right? So people have tried", "tokens": [51316, 1507, 13, 467, 311, 439, 665, 13, 583, 562, 321, 500, 380, 643, 281, 764, 309, 11, 754, 1101, 11, 558, 30, 407, 561, 362, 3031, 51532], "temperature": 0.0, "avg_logprob": -0.11556734909882417, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.0020504132844507694}, {"id": 395, "seek": 203408, "start": 2057.44, "end": 2060.72, "text": " doing things like, what if all our states are mutable? That's a very functional programming game.", "tokens": [51532, 884, 721, 411, 11, 437, 498, 439, 527, 4368, 366, 5839, 712, 30, 663, 311, 257, 588, 11745, 9410, 1216, 13, 51696], "temperature": 0.0, "avg_logprob": -0.11556734909882417, "compression_ratio": 1.7011834319526626, "no_speech_prob": 0.0020504132844507694}, {"id": 396, "seek": 206072, "start": 2061.4399999999996, "end": 2065.6, "text": " It was sort of in my world, it's more about, well, you can mutate things as long as it's", "tokens": [50400, 467, 390, 1333, 295, 294, 452, 1002, 11, 309, 311, 544, 466, 11, 731, 11, 291, 393, 5839, 473, 721, 382, 938, 382, 309, 311, 50608], "temperature": 0.0, "avg_logprob": -0.11045579441258165, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008295480511151254}, {"id": 397, "seek": 206072, "start": 2065.6, "end": 2069.9199999999996, "text": " monotone. So if they're mutable, but they're monotone, maybe that'll work. And then using", "tokens": [50608, 1108, 310, 546, 13, 407, 498, 436, 434, 5839, 712, 11, 457, 436, 434, 1108, 310, 546, 11, 1310, 300, 603, 589, 13, 400, 550, 1228, 50824], "temperature": 0.0, "avg_logprob": -0.11045579441258165, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008295480511151254}, {"id": 398, "seek": 206072, "start": 2069.9199999999996, "end": 2075.68, "text": " things like dependencies and provenance, all our ways of using application knowledge to avoid using", "tokens": [50824, 721, 411, 36606, 293, 12785, 719, 11, 439, 527, 2098, 295, 1228, 3861, 3601, 281, 5042, 1228, 51112], "temperature": 0.0, "avg_logprob": -0.11045579441258165, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008295480511151254}, {"id": 399, "seek": 206072, "start": 2075.68, "end": 2081.12, "text": " the expensive stuff on the left. But the really big query is when do I need coordination and why", "tokens": [51112, 264, 5124, 1507, 322, 264, 1411, 13, 583, 264, 534, 955, 14581, 307, 562, 360, 286, 643, 21252, 293, 983, 51384], "temperature": 0.0, "avg_logprob": -0.11045579441258165, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008295480511151254}, {"id": 400, "seek": 206072, "start": 2081.12, "end": 2087.3599999999997, "text": " do I need coordination? So if you ask, you know, a typical undergraduate or frankly, most people in", "tokens": [51384, 360, 286, 643, 21252, 30, 407, 498, 291, 1029, 11, 291, 458, 11, 257, 7476, 19113, 420, 11939, 11, 881, 561, 294, 51696], "temperature": 0.0, "avg_logprob": -0.11045579441258165, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008295480511151254}, {"id": 401, "seek": 208736, "start": 2087.36, "end": 2092.56, "text": " computer science, including professors, when do you need coordination? What's a lock for, right?", "tokens": [50364, 3820, 3497, 11, 3009, 15924, 11, 562, 360, 291, 643, 21252, 30, 708, 311, 257, 4017, 337, 11, 558, 30, 50624], "temperature": 0.0, "avg_logprob": -0.13184054953153015, "compression_ratio": 1.8082706766917294, "no_speech_prob": 0.008576514199376106}, {"id": 402, "seek": 208736, "start": 2092.56, "end": 2097.92, "text": " They'll say, well, it's to avoid conflicts on shared resources, right? This intersection needs", "tokens": [50624, 814, 603, 584, 11, 731, 11, 309, 311, 281, 5042, 19807, 322, 5507, 3593, 11, 558, 30, 639, 15236, 2203, 50892], "temperature": 0.0, "avg_logprob": -0.13184054953153015, "compression_ratio": 1.8082706766917294, "no_speech_prob": 0.008576514199376106}, {"id": 403, "seek": 208736, "start": 2097.92, "end": 2102.88, "text": " coordination. If you would just put up some damn stop lights, right, then, you know, north, south", "tokens": [50892, 21252, 13, 759, 291, 576, 445, 829, 493, 512, 8151, 1590, 5811, 11, 558, 11, 550, 11, 291, 458, 11, 6830, 11, 7377, 51140], "temperature": 0.0, "avg_logprob": -0.13184054953153015, "compression_ratio": 1.8082706766917294, "no_speech_prob": 0.008576514199376106}, {"id": 404, "seek": 208736, "start": 2102.88, "end": 2107.04, "text": " could go for a while and west, east, west would wait. And then east, west would go for a while,", "tokens": [51140, 727, 352, 337, 257, 1339, 293, 7009, 11, 10648, 11, 7009, 576, 1699, 13, 400, 550, 10648, 11, 7009, 576, 352, 337, 257, 1339, 11, 51348], "temperature": 0.0, "avg_logprob": -0.13184054953153015, "compression_ratio": 1.8082706766917294, "no_speech_prob": 0.008576514199376106}, {"id": 405, "seek": 208736, "start": 2107.04, "end": 2112.96, "text": " north, south would wait, problem solved, right? But like, do I really need coordination? That's", "tokens": [51348, 6830, 11, 7377, 576, 1699, 11, 1154, 13041, 11, 558, 30, 583, 411, 11, 360, 286, 534, 643, 21252, 30, 663, 311, 51644], "temperature": 0.0, "avg_logprob": -0.13184054953153015, "compression_ratio": 1.8082706766917294, "no_speech_prob": 0.008576514199376106}, {"id": 406, "seek": 211296, "start": 2113.04, "end": 2117.76, "text": " a solution. Is it the only solution? No, it's not the only solution, right? Here's a coordination", "tokens": [50368, 257, 3827, 13, 1119, 309, 264, 787, 3827, 30, 883, 11, 309, 311, 406, 264, 787, 3827, 11, 558, 30, 1692, 311, 257, 21252, 50604], "temperature": 0.0, "avg_logprob": -0.12881443418305497, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.003593365428969264}, {"id": 407, "seek": 211296, "start": 2117.76, "end": 2123.12, "text": " free solution to that intersection problem, right? So I'd like to be able to think out of the box,", "tokens": [50604, 1737, 3827, 281, 300, 15236, 1154, 11, 558, 30, 407, 286, 1116, 411, 281, 312, 1075, 281, 519, 484, 295, 264, 2424, 11, 50872], "temperature": 0.0, "avg_logprob": -0.12881443418305497, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.003593365428969264}, {"id": 408, "seek": 211296, "start": 2123.12, "end": 2128.4, "text": " right, and say, really, what is coordination for? Why am I required to use it?", "tokens": [50872, 558, 11, 293, 584, 11, 534, 11, 437, 307, 21252, 337, 30, 1545, 669, 286, 4739, 281, 764, 309, 30, 51136], "temperature": 0.0, "avg_logprob": -0.12881443418305497, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.003593365428969264}, {"id": 409, "seek": 211296, "start": 2130.7200000000003, "end": 2137.76, "text": " Okay. So that's a theory problem. So, you know, which programs have a coordination free implementation?", "tokens": [51252, 1033, 13, 407, 300, 311, 257, 5261, 1154, 13, 407, 11, 291, 458, 11, 597, 4268, 362, 257, 21252, 1737, 11420, 30, 51604], "temperature": 0.0, "avg_logprob": -0.12881443418305497, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.003593365428969264}, {"id": 410, "seek": 211296, "start": 2137.76, "end": 2142.0, "text": " We call those the green programs. These are specifications for which a clever programmer", "tokens": [51604, 492, 818, 729, 264, 3092, 4268, 13, 1981, 366, 29448, 337, 597, 257, 13494, 32116, 51816], "temperature": 0.0, "avg_logprob": -0.12881443418305497, "compression_ratio": 1.8425196850393701, "no_speech_prob": 0.003593365428969264}, {"id": 411, "seek": 214200, "start": 2142.0, "end": 2146.08, "text": " can find a coordination free solution. And then, of course, there's the rest of the programs,", "tokens": [50364, 393, 915, 257, 21252, 1737, 3827, 13, 400, 550, 11, 295, 1164, 11, 456, 311, 264, 1472, 295, 264, 4268, 11, 50568], "temperature": 0.0, "avg_logprob": -0.10321005029616014, "compression_ratio": 1.75, "no_speech_prob": 0.0012841330608353019}, {"id": 412, "seek": 214200, "start": 2147.44, "end": 2151.44, "text": " right? And I want to know this green line. Will someone please tell me, you know,", "tokens": [50636, 558, 30, 400, 286, 528, 281, 458, 341, 3092, 1622, 13, 3099, 1580, 1767, 980, 385, 11, 291, 458, 11, 50836], "temperature": 0.0, "avg_logprob": -0.10321005029616014, "compression_ratio": 1.75, "no_speech_prob": 0.0012841330608353019}, {"id": 413, "seek": 214200, "start": 2151.44, "end": 2155.76, "text": " Mr. Lamport, I think I only need you out here. So will you please tell me when I need you? And", "tokens": [50836, 2221, 13, 441, 1215, 477, 11, 286, 519, 286, 787, 643, 291, 484, 510, 13, 407, 486, 291, 1767, 980, 385, 562, 286, 643, 291, 30, 400, 51052], "temperature": 0.0, "avg_logprob": -0.10321005029616014, "compression_ratio": 1.75, "no_speech_prob": 0.0012841330608353019}, {"id": 414, "seek": 214200, "start": 2155.76, "end": 2159.36, "text": " there's no answer from Mr. Lamport. At least he didn't, you know, pick up the phone when I call.", "tokens": [51052, 456, 311, 572, 1867, 490, 2221, 13, 441, 1215, 477, 13, 1711, 1935, 415, 994, 380, 11, 291, 458, 11, 1888, 493, 264, 2593, 562, 286, 818, 13, 51232], "temperature": 0.0, "avg_logprob": -0.10321005029616014, "compression_ratio": 1.75, "no_speech_prob": 0.0012841330608353019}, {"id": 415, "seek": 214200, "start": 2160.4, "end": 2165.6, "text": " But I'm happy to say that people at Hussalt did. And this is what led to the column theorem. So", "tokens": [51284, 583, 286, 478, 2055, 281, 584, 300, 561, 412, 389, 2023, 3198, 630, 13, 400, 341, 307, 437, 4684, 281, 264, 7738, 20904, 13, 407, 51544], "temperature": 0.0, "avg_logprob": -0.10321005029616014, "compression_ratio": 1.75, "no_speech_prob": 0.0012841330608353019}, {"id": 416, "seek": 214200, "start": 2165.6, "end": 2169.68, "text": " this is really a computability question. What's the expressive power of languages without coordination?", "tokens": [51544, 341, 307, 534, 257, 2807, 2310, 1168, 13, 708, 311, 264, 40189, 1347, 295, 8650, 1553, 21252, 30, 51748], "temperature": 0.0, "avg_logprob": -0.10321005029616014, "compression_ratio": 1.75, "no_speech_prob": 0.0012841330608353019}, {"id": 417, "seek": 216968, "start": 2170.24, "end": 2176.72, "text": " Yeah. That's the green circle. Okay. So give you some intuition. Easy and hard questions.", "tokens": [50392, 865, 13, 663, 311, 264, 3092, 6329, 13, 1033, 13, 407, 976, 291, 512, 24002, 13, 16002, 293, 1152, 1651, 13, 50716], "temperature": 0.0, "avg_logprob": -0.13100509815387898, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.0003150176489725709}, {"id": 418, "seek": 216968, "start": 2176.72, "end": 2178.8799999999997, "text": " Here's an easy question. Is anyone in the room over 18?", "tokens": [50716, 1692, 311, 364, 1858, 1168, 13, 1119, 2878, 294, 264, 1808, 670, 2443, 30, 50824], "temperature": 0.0, "avg_logprob": -0.13100509815387898, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.0003150176489725709}, {"id": 419, "seek": 216968, "start": 2181.3599999999997, "end": 2187.04, "text": " Excellent. Not only were you all happy to answer that coordination free, but you engaged in a", "tokens": [50948, 16723, 13, 1726, 787, 645, 291, 439, 2055, 281, 1867, 300, 21252, 1737, 11, 457, 291, 8237, 294, 257, 51232], "temperature": 0.0, "avg_logprob": -0.13100509815387898, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.0003150176489725709}, {"id": 420, "seek": 216968, "start": 2187.04, "end": 2190.56, "text": " little protocol, right? You made up a protocol where you raise a hand if you think it's true. So", "tokens": [51232, 707, 10336, 11, 558, 30, 509, 1027, 493, 257, 10336, 689, 291, 5300, 257, 1011, 498, 291, 519, 309, 311, 2074, 13, 407, 51408], "temperature": 0.0, "avg_logprob": -0.13100509815387898, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.0003150176489725709}, {"id": 421, "seek": 216968, "start": 2190.56, "end": 2194.48, "text": " that was cool. So that was the monotone hand raising protocol or something. Great. All right.", "tokens": [51408, 300, 390, 1627, 13, 407, 300, 390, 264, 1108, 310, 546, 1011, 11225, 10336, 420, 746, 13, 3769, 13, 1057, 558, 13, 51604], "temperature": 0.0, "avg_logprob": -0.13100509815387898, "compression_ratio": 1.6226415094339623, "no_speech_prob": 0.0003150176489725709}, {"id": 422, "seek": 219448, "start": 2194.48, "end": 2200.48, "text": " Who's the youngest person in the room? Oh, we have some brave assertions. But clearly,", "tokens": [50364, 2102, 311, 264, 17747, 954, 294, 264, 1808, 30, 876, 11, 321, 362, 512, 12653, 19810, 626, 13, 583, 4448, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11451882700766286, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0031233320478349924}, {"id": 423, "seek": 219448, "start": 2200.48, "end": 2205.44, "text": " you don't know that. You could look at everyone, but that's cheating and also not necessarily", "tokens": [50664, 291, 500, 380, 458, 300, 13, 509, 727, 574, 412, 1518, 11, 457, 300, 311, 18309, 293, 611, 406, 4725, 50912], "temperature": 0.0, "avg_logprob": -0.11451882700766286, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0031233320478349924}, {"id": 424, "seek": 219448, "start": 2205.44, "end": 2212.88, "text": " right. Maybe, maybe. I don't know. I don't know. But the point here is, right, that somehow this", "tokens": [50912, 558, 13, 2704, 11, 1310, 13, 286, 500, 380, 458, 13, 286, 500, 380, 458, 13, 583, 264, 935, 510, 307, 11, 558, 11, 300, 6063, 341, 51284], "temperature": 0.0, "avg_logprob": -0.11451882700766286, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0031233320478349924}, {"id": 425, "seek": 219448, "start": 2212.88, "end": 2217.36, "text": " requires you to communicate with people. And the first one maybe doesn't. Okay. More to the point.", "tokens": [51284, 7029, 291, 281, 7890, 365, 561, 13, 400, 264, 700, 472, 1310, 1177, 380, 13, 1033, 13, 5048, 281, 264, 935, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11451882700766286, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0031233320478349924}, {"id": 426, "seek": 219448, "start": 2217.36, "end": 2221.52, "text": " Let's look at the logic here, right? This is an existential question. And this is a question with", "tokens": [51508, 961, 311, 574, 412, 264, 9952, 510, 11, 558, 30, 639, 307, 364, 37133, 1168, 13, 400, 341, 307, 257, 1168, 365, 51716], "temperature": 0.0, "avg_logprob": -0.11451882700766286, "compression_ratio": 1.6690140845070423, "no_speech_prob": 0.0031233320478349924}, {"id": 427, "seek": 222152, "start": 2221.52, "end": 2227.68, "text": " the universal quantifier in it. Or for people like me who just want to do total pattern matching", "tokens": [50364, 264, 11455, 4426, 9902, 294, 309, 13, 1610, 337, 561, 411, 385, 567, 445, 528, 281, 360, 3217, 5102, 14324, 50672], "temperature": 0.0, "avg_logprob": -0.08281185587898629, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.002322884276509285}, {"id": 428, "seek": 222152, "start": 2227.68, "end": 2232.72, "text": " and look for not symbols, that one appears to be positive. So I'll say that it's monotone.", "tokens": [50672, 293, 574, 337, 406, 16944, 11, 300, 472, 7038, 281, 312, 3353, 13, 407, 286, 603, 584, 300, 309, 311, 1108, 310, 546, 13, 50924], "temperature": 0.0, "avg_logprob": -0.08281185587898629, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.002322884276509285}, {"id": 429, "seek": 222152, "start": 2232.72, "end": 2236.64, "text": " And that one appears to be negative. So I'll say it's not monotone. So it gives you some intuition", "tokens": [50924, 400, 300, 472, 7038, 281, 312, 3671, 13, 407, 286, 603, 584, 309, 311, 406, 1108, 310, 546, 13, 407, 309, 2709, 291, 512, 24002, 51120], "temperature": 0.0, "avg_logprob": -0.08281185587898629, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.002322884276509285}, {"id": 430, "seek": 222152, "start": 2236.64, "end": 2242.4, "text": " that universal quantification or negation requires coordination. It is coordination. That's what", "tokens": [51120, 300, 11455, 4426, 3774, 420, 2485, 399, 7029, 21252, 13, 467, 307, 21252, 13, 663, 311, 437, 51408], "temperature": 0.0, "avg_logprob": -0.08281185587898629, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.002322884276509285}, {"id": 431, "seek": 222152, "start": 2242.4, "end": 2248.08, "text": " coordination is. It's universal quantification. So what is Lamport for? It's for universal quantifiers.", "tokens": [51408, 21252, 307, 13, 467, 311, 11455, 4426, 3774, 13, 407, 437, 307, 441, 1215, 477, 337, 30, 467, 311, 337, 11455, 4426, 23463, 13, 51692], "temperature": 0.0, "avg_logprob": -0.08281185587898629, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.002322884276509285}, {"id": 432, "seek": 224808, "start": 2248.96, "end": 2253.04, "text": " So let's just prove this, right? I was like, well, somebody prove it. I'm not going to prove it.", "tokens": [50408, 407, 718, 311, 445, 7081, 341, 11, 558, 30, 286, 390, 411, 11, 731, 11, 2618, 7081, 309, 13, 286, 478, 406, 516, 281, 7081, 309, 13, 50612], "temperature": 0.0, "avg_logprob": -0.15087889111231242, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0005192617536522448}, {"id": 433, "seek": 224808, "start": 2253.04, "end": 2258.16, "text": " So nice guy named Tom Omelette wrote a thesis on this stuff. My conjecture was called the", "tokens": [50612, 407, 1481, 2146, 4926, 5041, 9757, 338, 3007, 4114, 257, 22288, 322, 341, 1507, 13, 1222, 416, 1020, 540, 390, 1219, 264, 50868], "temperature": 0.0, "avg_logprob": -0.15087889111231242, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0005192617536522448}, {"id": 434, "seek": 224808, "start": 2258.16, "end": 2264.4, "text": " calm conjecture consistency is logical monotonicity. It was in a Paz keynote that I was gave some years", "tokens": [50868, 7151, 416, 1020, 540, 14416, 307, 14978, 1108, 310, 11630, 507, 13, 467, 390, 294, 257, 430, 921, 33896, 300, 286, 390, 2729, 512, 924, 51180], "temperature": 0.0, "avg_logprob": -0.15087889111231242, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0005192617536522448}, {"id": 435, "seek": 224808, "start": 2264.4, "end": 2268.72, "text": " ago. And then just a year later, there was a conference paper from the good folks at Haselt,", "tokens": [51180, 2057, 13, 400, 550, 445, 257, 1064, 1780, 11, 456, 390, 257, 7586, 3035, 490, 264, 665, 4024, 412, 8646, 2018, 11, 51396], "temperature": 0.0, "avg_logprob": -0.15087889111231242, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0005192617536522448}, {"id": 436, "seek": 224808, "start": 2268.72, "end": 2274.4, "text": " which then they extended and then was further extended with weaker definitions for the monotonicity", "tokens": [51396, 597, 550, 436, 10913, 293, 550, 390, 3052, 10913, 365, 24286, 21988, 337, 264, 1108, 310, 11630, 507, 51680], "temperature": 0.0, "avg_logprob": -0.15087889111231242, "compression_ratio": 1.6770833333333333, "no_speech_prob": 0.0005192617536522448}, {"id": 437, "seek": 227440, "start": 2274.4, "end": 2279.92, "text": " to really expand the results. If you want a quick, you know, kind of a version of what I'm saying now,", "tokens": [50364, 281, 534, 5268, 264, 3542, 13, 759, 291, 528, 257, 1702, 11, 291, 458, 11, 733, 295, 257, 3037, 295, 437, 286, 478, 1566, 586, 11, 50640], "temperature": 0.0, "avg_logprob": -0.11649395583511947, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0021154917776584625}, {"id": 438, "seek": 227440, "start": 2279.92, "end": 2283.6, "text": " you can read this CACOM overview, but it's really for systems people. I think you guys should just", "tokens": [50640, 291, 393, 1401, 341, 383, 4378, 5251, 12492, 11, 457, 309, 311, 534, 337, 3652, 561, 13, 286, 519, 291, 1074, 820, 445, 50824], "temperature": 0.0, "avg_logprob": -0.11649395583511947, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0021154917776584625}, {"id": 439, "seek": 227440, "start": 2283.6, "end": 2289.44, "text": " read Tom's papers. All right. To give you a flavor of what Tom did, definitions are half the battle.", "tokens": [50824, 1401, 5041, 311, 10577, 13, 1057, 558, 13, 1407, 976, 291, 257, 6813, 295, 437, 5041, 630, 11, 21988, 366, 1922, 264, 4635, 13, 51116], "temperature": 0.0, "avg_logprob": -0.11649395583511947, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0021154917776584625}, {"id": 440, "seek": 227440, "start": 2289.44, "end": 2292.8, "text": " It seems, you know, when I read Paz papers, that's all the hard parts are the definitions, right?", "tokens": [51116, 467, 2544, 11, 291, 458, 11, 562, 286, 1401, 430, 921, 10577, 11, 300, 311, 439, 264, 1152, 3166, 366, 264, 21988, 11, 558, 30, 51284], "temperature": 0.0, "avg_logprob": -0.11649395583511947, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0021154917776584625}, {"id": 441, "seek": 227440, "start": 2293.92, "end": 2298.4, "text": " So, you know what monotonicity is in logic? That's fine. What is consistency here? Well,", "tokens": [51340, 407, 11, 291, 458, 437, 1108, 310, 11630, 507, 307, 294, 9952, 30, 663, 311, 2489, 13, 708, 307, 14416, 510, 30, 1042, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11649395583511947, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0021154917776584625}, {"id": 442, "seek": 227440, "start": 2298.4, "end": 2302.8, "text": " we want the program to produce the same output regardless of where the initial data is placed.", "tokens": [51564, 321, 528, 264, 1461, 281, 5258, 264, 912, 5598, 10060, 295, 689, 264, 5883, 1412, 307, 7074, 13, 51784], "temperature": 0.0, "avg_logprob": -0.11649395583511947, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0021154917776584625}, {"id": 443, "seek": 230280, "start": 2302.8, "end": 2306.7200000000003, "text": " So, I should be able to start the program with the data replicated pops possibly and", "tokens": [50364, 407, 11, 286, 820, 312, 1075, 281, 722, 264, 1461, 365, 264, 1412, 46365, 16795, 6264, 293, 50560], "temperature": 0.0, "avg_logprob": -0.07885419238697398, "compression_ratio": 1.9094076655052266, "no_speech_prob": 0.00025314444792456925}, {"id": 444, "seek": 230280, "start": 2306.7200000000003, "end": 2312.0800000000004, "text": " partitioned in any way and get the same answer. And if that's true, then it would be the same", "tokens": [50560, 24808, 292, 294, 604, 636, 293, 483, 264, 912, 1867, 13, 400, 498, 300, 311, 2074, 11, 550, 309, 576, 312, 264, 912, 50828], "temperature": 0.0, "avg_logprob": -0.07885419238697398, "compression_ratio": 1.9094076655052266, "no_speech_prob": 0.00025314444792456925}, {"id": 445, "seek": 230280, "start": 2312.0800000000004, "end": 2315.36, "text": " answer across replicas. It would be the same answer across different runs. It would be the", "tokens": [50828, 1867, 2108, 3248, 9150, 13, 467, 576, 312, 264, 912, 1867, 2108, 819, 6676, 13, 467, 576, 312, 264, 50992], "temperature": 0.0, "avg_logprob": -0.07885419238697398, "compression_ratio": 1.9094076655052266, "no_speech_prob": 0.00025314444792456925}, {"id": 446, "seek": 230280, "start": 2315.36, "end": 2319.6000000000004, "text": " same answer if we start gossiping the data between each other. And this is what we want,", "tokens": [50992, 912, 1867, 498, 321, 722, 31788, 278, 264, 1412, 1296, 1184, 661, 13, 400, 341, 307, 437, 321, 528, 11, 51204], "temperature": 0.0, "avg_logprob": -0.07885419238697398, "compression_ratio": 1.9094076655052266, "no_speech_prob": 0.00025314444792456925}, {"id": 447, "seek": 230280, "start": 2319.6000000000004, "end": 2324.0, "text": " right? So, that's our definition of consistency, where I think what's really clever and was the", "tokens": [51204, 558, 30, 407, 11, 300, 311, 527, 7123, 295, 14416, 11, 689, 286, 519, 437, 311, 534, 13494, 293, 390, 264, 51424], "temperature": 0.0, "avg_logprob": -0.07885419238697398, "compression_ratio": 1.9094076655052266, "no_speech_prob": 0.00025314444792456925}, {"id": 448, "seek": 230280, "start": 2324.0, "end": 2328.96, "text": " most beautiful part of the work is defining what coordination really means. So, we're sending", "tokens": [51424, 881, 2238, 644, 295, 264, 589, 307, 17827, 437, 21252, 534, 1355, 13, 407, 11, 321, 434, 7750, 51672], "temperature": 0.0, "avg_logprob": -0.07885419238697398, "compression_ratio": 1.9094076655052266, "no_speech_prob": 0.00025314444792456925}, {"id": 449, "seek": 232896, "start": 2328.96, "end": 2333.84, "text": " messages around, right? That's data. But which data is really data and which data is kind of", "tokens": [50364, 7897, 926, 11, 558, 30, 663, 311, 1412, 13, 583, 597, 1412, 307, 534, 1412, 293, 597, 1412, 307, 733, 295, 50608], "temperature": 0.0, "avg_logprob": -0.09193154562891057, "compression_ratio": 1.8359375, "no_speech_prob": 0.0012447574408724904}, {"id": 450, "seek": 232896, "start": 2333.84, "end": 2340.2400000000002, "text": " control messages? And how do you differentiate those in a formal way? And so, what they define", "tokens": [50608, 1969, 7897, 30, 400, 577, 360, 291, 23203, 729, 294, 257, 9860, 636, 30, 400, 370, 11, 437, 436, 6964, 50928], "temperature": 0.0, "avg_logprob": -0.09193154562891057, "compression_ratio": 1.8359375, "no_speech_prob": 0.0012447574408724904}, {"id": 451, "seek": 232896, "start": 2340.2400000000002, "end": 2345.76, "text": " in this paper is program is coordination free if there's some partitioning of the data when", "tokens": [50928, 294, 341, 3035, 307, 1461, 307, 21252, 1737, 498, 456, 311, 512, 24808, 278, 295, 264, 1412, 562, 51204], "temperature": 0.0, "avg_logprob": -0.09193154562891057, "compression_ratio": 1.8359375, "no_speech_prob": 0.0012447574408724904}, {"id": 452, "seek": 232896, "start": 2345.76, "end": 2349.84, "text": " you first start. So, there's some way out of the data where you first start, such that the query", "tokens": [51204, 291, 700, 722, 13, 407, 11, 456, 311, 512, 636, 484, 295, 264, 1412, 689, 291, 700, 722, 11, 1270, 300, 264, 14581, 51408], "temperature": 0.0, "avg_logprob": -0.09193154562891057, "compression_ratio": 1.8359375, "no_speech_prob": 0.0012447574408724904}, {"id": 453, "seek": 232896, "start": 2349.84, "end": 2354.56, "text": " can be answered without communication. So, for a particular query, for a particular data set,", "tokens": [51408, 393, 312, 10103, 1553, 6101, 13, 407, 11, 337, 257, 1729, 14581, 11, 337, 257, 1729, 1412, 992, 11, 51644], "temperature": 0.0, "avg_logprob": -0.09193154562891057, "compression_ratio": 1.8359375, "no_speech_prob": 0.0012447574408724904}, {"id": 454, "seek": 235456, "start": 2354.64, "end": 2358.64, "text": " there is some world of where you lay it out where no communication is required.", "tokens": [50368, 456, 307, 512, 1002, 295, 689, 291, 2360, 309, 484, 689, 572, 6101, 307, 4739, 13, 50568], "temperature": 0.0, "avg_logprob": -0.13006653956004552, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002631352748721838}, {"id": 455, "seek": 235456, "start": 2359.92, "end": 2365.44, "text": " That's the definition of coordination freeness. And a program that you can't do that on is doing", "tokens": [50632, 663, 311, 264, 7123, 295, 21252, 1737, 1287, 13, 400, 257, 1461, 300, 291, 393, 380, 360, 300, 322, 307, 884, 50908], "temperature": 0.0, "avg_logprob": -0.13006653956004552, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002631352748721838}, {"id": 456, "seek": 235456, "start": 2365.44, "end": 2370.96, "text": " coordination messages. So, it's not really saying which messages are coordination and which messages", "tokens": [50908, 21252, 7897, 13, 407, 11, 309, 311, 406, 534, 1566, 597, 7897, 366, 21252, 293, 597, 7897, 51184], "temperature": 0.0, "avg_logprob": -0.13006653956004552, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002631352748721838}, {"id": 457, "seek": 235456, "start": 2370.96, "end": 2379.36, "text": " are data, but it's telling you which programs can be run coordination. Yes. So, the trivial example", "tokens": [51184, 366, 1412, 11, 457, 309, 311, 3585, 291, 597, 4268, 393, 312, 1190, 21252, 13, 1079, 13, 407, 11, 264, 26703, 1365, 51604], "temperature": 0.0, "avg_logprob": -0.13006653956004552, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002631352748721838}, {"id": 458, "seek": 235456, "start": 2379.36, "end": 2383.84, "text": " of this is you put all the data in one node. And again, you know, this question of is there anybody", "tokens": [51604, 295, 341, 307, 291, 829, 439, 264, 1412, 294, 472, 9984, 13, 400, 797, 11, 291, 458, 11, 341, 1168, 295, 307, 456, 4472, 51828], "temperature": 0.0, "avg_logprob": -0.13006653956004552, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.002631352748721838}, {"id": 459, "seek": 238384, "start": 2383.84, "end": 2390.4, "text": " who is older than me? What you don't know is whether anyone else has data. So, I may have all", "tokens": [50364, 567, 307, 4906, 813, 385, 30, 708, 291, 500, 380, 458, 307, 1968, 2878, 1646, 575, 1412, 13, 407, 11, 286, 815, 362, 439, 50692], "temperature": 0.0, "avg_logprob": -0.09435134014840853, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00047282129526138306}, {"id": 460, "seek": 238384, "start": 2390.4, "end": 2394.08, "text": " the data, but I don't know that. So, I still have to ask everybody, anybody got any data?", "tokens": [50692, 264, 1412, 11, 457, 286, 500, 380, 458, 300, 13, 407, 11, 286, 920, 362, 281, 1029, 2201, 11, 4472, 658, 604, 1412, 30, 50876], "temperature": 0.0, "avg_logprob": -0.09435134014840853, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00047282129526138306}, {"id": 461, "seek": 238384, "start": 2394.08, "end": 2398.56, "text": " And I have to wait for everybody to respond, right? So, it's a very nice intuition to just think", "tokens": [50876, 400, 286, 362, 281, 1699, 337, 2201, 281, 4196, 11, 558, 30, 407, 11, 309, 311, 257, 588, 1481, 24002, 281, 445, 519, 51100], "temperature": 0.0, "avg_logprob": -0.09435134014840853, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00047282129526138306}, {"id": 462, "seek": 238384, "start": 2398.56, "end": 2406.56, "text": " about having all the data. All right. There's another thing in the paper that I hadn't even", "tokens": [51100, 466, 1419, 439, 264, 1412, 13, 1057, 558, 13, 821, 311, 1071, 551, 294, 264, 3035, 300, 286, 8782, 380, 754, 51500], "temperature": 0.0, "avg_logprob": -0.09435134014840853, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00047282129526138306}, {"id": 463, "seek": 238384, "start": 2406.56, "end": 2411.04, "text": " anticipated, which is really beautiful and speaks to stuff that the distributed systems", "tokens": [51500, 23267, 11, 597, 307, 534, 2238, 293, 10789, 281, 1507, 300, 264, 12631, 3652, 51724], "temperature": 0.0, "avg_logprob": -0.09435134014840853, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.00047282129526138306}, {"id": 464, "seek": 241104, "start": 2411.04, "end": 2415.2799999999997, "text": " community kind of knows, which is there's a third equivalent, which is that you can", "tokens": [50364, 1768, 733, 295, 3255, 11, 597, 307, 456, 311, 257, 2636, 10344, 11, 597, 307, 300, 291, 393, 50576], "temperature": 0.0, "avg_logprob": -0.090236403725364, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.002251500030979514}, {"id": 465, "seek": 241104, "start": 2415.2799999999997, "end": 2421.2, "text": " distributively compute this program on an oblivious transducer. And I haven't even talked", "tokens": [50576, 4400, 325, 3413, 14722, 341, 1461, 322, 364, 47039, 851, 1145, 769, 1776, 13, 400, 286, 2378, 380, 754, 2825, 50872], "temperature": 0.0, "avg_logprob": -0.090236403725364, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.002251500030979514}, {"id": 466, "seek": 241104, "start": 2421.2, "end": 2425.44, "text": " about transducers yet, but just a minute. But what does it mean by oblivious? It means that", "tokens": [50872, 466, 1145, 8117, 433, 1939, 11, 457, 445, 257, 3456, 13, 583, 437, 775, 309, 914, 538, 47039, 851, 30, 467, 1355, 300, 51084], "temperature": 0.0, "avg_logprob": -0.090236403725364, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.002251500030979514}, {"id": 467, "seek": 241104, "start": 2425.44, "end": 2430.24, "text": " the agent in the system doesn't know its own identity. It cannot distinguish messages from", "tokens": [51084, 264, 9461, 294, 264, 1185, 1177, 380, 458, 1080, 1065, 6575, 13, 467, 2644, 20206, 7897, 490, 51324], "temperature": 0.0, "avg_logprob": -0.090236403725364, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.002251500030979514}, {"id": 468, "seek": 241104, "start": 2430.24, "end": 2436.88, "text": " itself from messages from anyone else. So, it doesn't actually know who itself is. And it", "tokens": [51324, 2564, 490, 7897, 490, 2878, 1646, 13, 407, 11, 309, 1177, 380, 767, 458, 567, 2564, 307, 13, 400, 309, 51656], "temperature": 0.0, "avg_logprob": -0.090236403725364, "compression_ratio": 1.702290076335878, "no_speech_prob": 0.002251500030979514}, {"id": 469, "seek": 243688, "start": 2436.88, "end": 2442.48, "text": " doesn't know the set of participants. We call this an oblivious agent, right? Oblivious programs", "tokens": [50364, 1177, 380, 458, 264, 992, 295, 10503, 13, 492, 818, 341, 364, 47039, 851, 9461, 11, 558, 30, 4075, 45997, 851, 4268, 50644], "temperature": 0.0, "avg_logprob": -0.07895920971247154, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.008846012875437737}, {"id": 470, "seek": 243688, "start": 2442.48, "end": 2446.32, "text": " that can be computed by oblivious agents are exactly the monotone programs and exactly the", "tokens": [50644, 300, 393, 312, 40610, 538, 47039, 851, 12554, 366, 2293, 264, 1108, 310, 546, 4268, 293, 2293, 264, 50836], "temperature": 0.0, "avg_logprob": -0.07895920971247154, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.008846012875437737}, {"id": 471, "seek": 243688, "start": 2446.32, "end": 2450.2400000000002, "text": " coordination free programs. So, that was very cool. And it speaks to questions of like", "tokens": [50836, 21252, 1737, 4268, 13, 407, 11, 300, 390, 588, 1627, 13, 400, 309, 10789, 281, 1651, 295, 411, 51032], "temperature": 0.0, "avg_logprob": -0.07895920971247154, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.008846012875437737}, {"id": 472, "seek": 243688, "start": 2450.96, "end": 2455.12, "text": " membership protocols in distributed systems, which is about establishing common knowledge", "tokens": [51068, 16560, 20618, 294, 12631, 3652, 11, 597, 307, 466, 22494, 2689, 3601, 51276], "temperature": 0.0, "avg_logprob": -0.07895920971247154, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.008846012875437737}, {"id": 473, "seek": 243688, "start": 2455.12, "end": 2458.8, "text": " of the all relation. That's like one of the things that Paxos does is it has a membership", "tokens": [51276, 295, 264, 439, 9721, 13, 663, 311, 411, 472, 295, 264, 721, 300, 430, 2797, 329, 775, 307, 309, 575, 257, 16560, 51460], "temperature": 0.0, "avg_logprob": -0.07895920971247154, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.008846012875437737}, {"id": 474, "seek": 243688, "start": 2458.8, "end": 2463.6800000000003, "text": " protocol built in. So, it's one of the reasons it's coordination full is to establish all.", "tokens": [51460, 10336, 3094, 294, 13, 407, 11, 309, 311, 472, 295, 264, 4112, 309, 311, 21252, 1577, 307, 281, 8327, 439, 13, 51704], "temperature": 0.0, "avg_logprob": -0.07895920971247154, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.008846012875437737}, {"id": 475, "seek": 246368, "start": 2463.68, "end": 2468.96, "text": " So, this was really, really nice. So, this is all in this JACM paper. It's actually in the", "tokens": [50364, 407, 11, 341, 390, 534, 11, 534, 1481, 13, 407, 11, 341, 307, 439, 294, 341, 508, 4378, 44, 3035, 13, 467, 311, 767, 294, 264, 50628], "temperature": 0.0, "avg_logprob": -0.19582495528660462, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007321473094634712}, {"id": 476, "seek": 246368, "start": 2468.96, "end": 2474.64, "text": " conference, the Paz paper as well. That's just a flavor of the calm stuff. And I'm going to stop", "tokens": [50628, 7586, 11, 264, 430, 921, 3035, 382, 731, 13, 663, 311, 445, 257, 6813, 295, 264, 7151, 1507, 13, 400, 286, 478, 516, 281, 1590, 50912], "temperature": 0.0, "avg_logprob": -0.19582495528660462, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007321473094634712}, {"id": 477, "seek": 246368, "start": 2474.64, "end": 2478.48, "text": " with that. But happy to answer questions as best they can afterwards. And with that,", "tokens": [50912, 365, 300, 13, 583, 2055, 281, 1867, 1651, 382, 1151, 436, 393, 10543, 13, 400, 365, 300, 11, 51104], "temperature": 0.0, "avg_logprob": -0.19582495528660462, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007321473094634712}, {"id": 478, "seek": 246368, "start": 2478.48, "end": 2479.6, "text": " I'm going to give it back to Connor.", "tokens": [51104, 286, 478, 516, 281, 976, 309, 646, 281, 33133, 13, 51160], "temperature": 0.0, "avg_logprob": -0.19582495528660462, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0007321473094634712}, {"id": 479, "seek": 247960, "start": 2480.16, "end": 2494.7999999999997, "text": " You guys can still hear me? All right. So, we have this calm theorem view of the world,", "tokens": [50392, 509, 1074, 393, 920, 1568, 385, 30, 1057, 558, 13, 407, 11, 321, 362, 341, 7151, 20904, 1910, 295, 264, 1002, 11, 51124], "temperature": 0.0, "avg_logprob": -0.20422327157222864, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.005819506477564573}, {"id": 480, "seek": 247960, "start": 2494.7999999999997, "end": 2501.04, "text": " relational transducers, logic, operating over sets. And then we have this semi lattice", "tokens": [51124, 38444, 1145, 8117, 433, 11, 9952, 11, 7447, 670, 6352, 13, 400, 550, 321, 362, 341, 12909, 34011, 51436], "temperature": 0.0, "avg_logprob": -0.20422327157222864, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.005819506477564573}, {"id": 481, "seek": 247960, "start": 2501.04, "end": 2506.96, "text": " algebra view of the world. And they both are dealing with coordination and freeness in different", "tokens": [51436, 21989, 1910, 295, 264, 1002, 13, 400, 436, 1293, 366, 6260, 365, 21252, 293, 1737, 1287, 294, 819, 51732], "temperature": 0.0, "avg_logprob": -0.20422327157222864, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.005819506477564573}, {"id": 482, "seek": 250696, "start": 2506.96, "end": 2512.2400000000002, "text": " lenses. But currently, they guarantee different things. The algebra view, like we said, is", "tokens": [50364, 18059, 13, 583, 4362, 11, 436, 10815, 819, 721, 13, 440, 21989, 1910, 11, 411, 321, 848, 11, 307, 50628], "temperature": 0.0, "avg_logprob": -0.12991957621531444, "compression_ratio": 2.0358565737051793, "no_speech_prob": 0.002472325460985303}, {"id": 483, "seek": 250696, "start": 2512.2400000000002, "end": 2515.68, "text": " concerned with the coordination of freeness of rights and does not guarantee coordination", "tokens": [50628, 5922, 365, 264, 21252, 295, 1737, 1287, 295, 4601, 293, 775, 406, 10815, 21252, 50800], "temperature": 0.0, "avg_logprob": -0.12991957621531444, "compression_ratio": 2.0358565737051793, "no_speech_prob": 0.002472325460985303}, {"id": 484, "seek": 250696, "start": 2515.68, "end": 2519.76, "text": " of freeness of queries. Whereas the calm theorem view is only concerned with the", "tokens": [50800, 295, 1737, 1287, 295, 24109, 13, 13813, 264, 7151, 20904, 1910, 307, 787, 5922, 365, 264, 51004], "temperature": 0.0, "avg_logprob": -0.12991957621531444, "compression_ratio": 2.0358565737051793, "no_speech_prob": 0.002472325460985303}, {"id": 485, "seek": 250696, "start": 2519.76, "end": 2522.4, "text": " coordination of freeness of queries. It actually doesn't have to worry about the", "tokens": [51004, 21252, 295, 1737, 1287, 295, 24109, 13, 467, 767, 1177, 380, 362, 281, 3292, 466, 264, 51136], "temperature": 0.0, "avg_logprob": -0.12991957621531444, "compression_ratio": 2.0358565737051793, "no_speech_prob": 0.002472325460985303}, {"id": 486, "seek": 250696, "start": 2522.4, "end": 2526.0, "text": " coordination of freeness of rights because it assumes operating over sets and gets", "tokens": [51136, 21252, 295, 1737, 1287, 295, 4601, 570, 309, 37808, 7447, 670, 6352, 293, 2170, 51316], "temperature": 0.0, "avg_logprob": -0.12991957621531444, "compression_ratio": 2.0358565737051793, "no_speech_prob": 0.002472325460985303}, {"id": 487, "seek": 250696, "start": 2526.0, "end": 2531.92, "text": " coordination of rights for free that way. And so, we're interested in the question of", "tokens": [51316, 21252, 295, 4601, 337, 1737, 300, 636, 13, 400, 370, 11, 321, 434, 3102, 294, 264, 1168, 295, 51612], "temperature": 0.0, "avg_logprob": -0.12991957621531444, "compression_ratio": 2.0358565737051793, "no_speech_prob": 0.002472325460985303}, {"id": 488, "seek": 253192, "start": 2531.92, "end": 2537.36, "text": " how can we combine these two lenses? Can we do an algebraic view of the calm theorem?", "tokens": [50364, 577, 393, 321, 10432, 613, 732, 18059, 30, 1664, 321, 360, 364, 21989, 299, 1910, 295, 264, 7151, 20904, 30, 50636], "temperature": 0.0, "avg_logprob": -0.09017291369738879, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.002251641359180212}, {"id": 489, "seek": 253192, "start": 2538.56, "end": 2545.28, "text": " And some intuition for how that might work is, you know, the semi lattice operator induces a", "tokens": [50696, 400, 512, 24002, 337, 577, 300, 1062, 589, 307, 11, 291, 458, 11, 264, 12909, 34011, 12973, 13716, 887, 257, 51032], "temperature": 0.0, "avg_logprob": -0.09017291369738879, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.002251641359180212}, {"id": 490, "seek": 253192, "start": 2545.28, "end": 2551.12, "text": " partial order. And so, instead of having monotone logical queries without negation, you have", "tokens": [51032, 14641, 1668, 13, 400, 370, 11, 2602, 295, 1419, 1108, 310, 546, 14978, 24109, 1553, 2485, 399, 11, 291, 362, 51324], "temperature": 0.0, "avg_logprob": -0.09017291369738879, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.002251641359180212}, {"id": 491, "seek": 253192, "start": 2551.12, "end": 2557.6, "text": " monotone functions between lattices, between partial orders. So, that's something we've", "tokens": [51324, 1108, 310, 546, 6828, 1296, 29025, 1473, 11, 1296, 14641, 9470, 13, 407, 11, 300, 311, 746, 321, 600, 51648], "temperature": 0.0, "avg_logprob": -0.09017291369738879, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.002251641359180212}, {"id": 492, "seek": 253192, "start": 2557.6, "end": 2561.84, "text": " been exploring. We'd love to chat more about it with folks. I'm actually going to talk about", "tokens": [51648, 668, 12736, 13, 492, 1116, 959, 281, 5081, 544, 466, 309, 365, 4024, 13, 286, 478, 767, 516, 281, 751, 466, 51860], "temperature": 0.0, "avg_logprob": -0.09017291369738879, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.002251641359180212}, {"id": 493, "seek": 256192, "start": 2562.64, "end": 2568.64, "text": " a problem specific to the question Remy asked. It comes up in this setting.", "tokens": [50400, 257, 1154, 2685, 281, 264, 1168, 497, 3633, 2351, 13, 467, 1487, 493, 294, 341, 3287, 13, 50700], "temperature": 0.0, "avg_logprob": -0.11741274665383732, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0003569266991689801}, {"id": 494, "seek": 256192, "start": 2569.36, "end": 2574.64, "text": " So, the calm theorem is all about basically not knowing when you have the entire input. What can", "tokens": [50736, 407, 11, 264, 7151, 20904, 307, 439, 466, 1936, 406, 5276, 562, 291, 362, 264, 2302, 4846, 13, 708, 393, 51000], "temperature": 0.0, "avg_logprob": -0.11741274665383732, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0003569266991689801}, {"id": 495, "seek": 256192, "start": 2574.64, "end": 2581.36, "text": " I output and tell downstream consumers with certainty, even though I might have more updates", "tokens": [51000, 286, 5598, 293, 980, 30621, 11883, 365, 27022, 11, 754, 1673, 286, 1062, 362, 544, 9205, 51336], "temperature": 0.0, "avg_logprob": -0.11741274665383732, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0003569266991689801}, {"id": 496, "seek": 256192, "start": 2581.36, "end": 2586.0, "text": " in the future, there might be more messages arriving. And so, we call the ability to do this free", "tokens": [51336, 294, 264, 2027, 11, 456, 1062, 312, 544, 7897, 22436, 13, 400, 370, 11, 321, 818, 264, 3485, 281, 360, 341, 1737, 51568], "temperature": 0.0, "avg_logprob": -0.11741274665383732, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0003569266991689801}, {"id": 497, "seek": 258600, "start": 2586.0, "end": 2595.68, "text": " termination without coordination. What can we be sure that we can output? And we're exploring", "tokens": [50364, 1433, 2486, 1553, 21252, 13, 708, 393, 321, 312, 988, 300, 321, 393, 5598, 30, 400, 321, 434, 12736, 50848], "temperature": 0.0, "avg_logprob": -0.08380118329474266, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.002800688147544861}, {"id": 498, "seek": 258600, "start": 2595.68, "end": 2601.28, "text": " this in a very generic setting of just we have two functions, an operation that's going to change", "tokens": [50848, 341, 294, 257, 588, 19577, 3287, 295, 445, 321, 362, 732, 6828, 11, 364, 6916, 300, 311, 516, 281, 1319, 51128], "temperature": 0.0, "avg_logprob": -0.08380118329474266, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.002800688147544861}, {"id": 499, "seek": 258600, "start": 2601.28, "end": 2609.52, "text": " our state over time in a query that's going to return some output. So, looking at an example", "tokens": [51128, 527, 1785, 670, 565, 294, 257, 14581, 300, 311, 516, 281, 2736, 512, 5598, 13, 407, 11, 1237, 412, 364, 1365, 51540], "temperature": 0.0, "avg_logprob": -0.08380118329474266, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.002800688147544861}, {"id": 500, "seek": 258600, "start": 2609.52, "end": 2614.48, "text": " of when we might be able to do this, we can look at this lattice. This is the set over these four", "tokens": [51540, 295, 562, 321, 1062, 312, 1075, 281, 360, 341, 11, 321, 393, 574, 412, 341, 34011, 13, 639, 307, 264, 992, 670, 613, 1451, 51788], "temperature": 0.0, "avg_logprob": -0.08380118329474266, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.002800688147544861}, {"id": 501, "seek": 261448, "start": 2614.48, "end": 2622.4, "text": " socks and say that this is our state of a local node and we're at top in this lattice. And our", "tokens": [50364, 17564, 293, 584, 300, 341, 307, 527, 1785, 295, 257, 2654, 9984, 293, 321, 434, 412, 1192, 294, 341, 34011, 13, 400, 527, 50760], "temperature": 0.0, "avg_logprob": -0.08990097945591188, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004172979388386011}, {"id": 502, "seek": 261448, "start": 2622.4, "end": 2631.12, "text": " update operation in the CRDT sense is union. So, we're going to union in more socks. We know that", "tokens": [50760, 5623, 6916, 294, 264, 14123, 35, 51, 2020, 307, 11671, 13, 407, 11, 321, 434, 516, 281, 11671, 294, 544, 17564, 13, 492, 458, 300, 51196], "temperature": 0.0, "avg_logprob": -0.08990097945591188, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004172979388386011}, {"id": 503, "seek": 261448, "start": 2631.12, "end": 2637.04, "text": " if we're at top, as updated as monotone, we'll never change our state. We're stuck at top.", "tokens": [51196, 498, 321, 434, 412, 1192, 11, 382, 10588, 382, 1108, 310, 546, 11, 321, 603, 1128, 1319, 527, 1785, 13, 492, 434, 5541, 412, 1192, 13, 51492], "temperature": 0.0, "avg_logprob": -0.08990097945591188, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004172979388386011}, {"id": 504, "seek": 261448, "start": 2637.92, "end": 2641.84, "text": " And so, whatever query we might be asking when we're in this state, we'd be able to locally", "tokens": [51536, 400, 370, 11, 2035, 14581, 321, 1062, 312, 3365, 562, 321, 434, 294, 341, 1785, 11, 321, 1116, 312, 1075, 281, 16143, 51732], "temperature": 0.0, "avg_logprob": -0.08990097945591188, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.0004172979388386011}, {"id": 505, "seek": 264184, "start": 2641.84, "end": 2646.0, "text": " detect that our query result is not going to change in the future and we'd be able to return", "tokens": [50364, 5531, 300, 527, 14581, 1874, 307, 406, 516, 281, 1319, 294, 264, 2027, 293, 321, 1116, 312, 1075, 281, 2736, 50572], "temperature": 0.0, "avg_logprob": -0.07407267604555402, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0009696896304376423}, {"id": 506, "seek": 264184, "start": 2646.0, "end": 2652.0, "text": " our result with certainty. This might sound like it would not happen particularly often,", "tokens": [50572, 527, 1874, 365, 27022, 13, 639, 1062, 1626, 411, 309, 576, 406, 1051, 4098, 2049, 11, 50872], "temperature": 0.0, "avg_logprob": -0.07407267604555402, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0009696896304376423}, {"id": 507, "seek": 264184, "start": 2652.0, "end": 2656.2400000000002, "text": " but let's try and look at more examples where we would be able to figure out that with certainty,", "tokens": [50872, 457, 718, 311, 853, 293, 574, 412, 544, 5110, 689, 321, 576, 312, 1075, 281, 2573, 484, 300, 365, 27022, 11, 51084], "temperature": 0.0, "avg_logprob": -0.07407267604555402, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0009696896304376423}, {"id": 508, "seek": 264184, "start": 2656.2400000000002, "end": 2662.96, "text": " we can return an answer right now. So, what if we consider also the query that we're asking", "tokens": [51084, 321, 393, 2736, 364, 1867, 558, 586, 13, 407, 11, 437, 498, 321, 1949, 611, 264, 14581, 300, 321, 434, 3365, 51420], "temperature": 0.0, "avg_logprob": -0.07407267604555402, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0009696896304376423}, {"id": 509, "seek": 264184, "start": 2662.96, "end": 2669.52, "text": " and say that this query is going to map us from this set socks lattice to the boolean lattice,", "tokens": [51420, 293, 584, 300, 341, 14581, 307, 516, 281, 4471, 505, 490, 341, 992, 17564, 34011, 281, 264, 748, 4812, 282, 34011, 11, 51748], "temperature": 0.0, "avg_logprob": -0.07407267604555402, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.0009696896304376423}, {"id": 510, "seek": 266952, "start": 2669.52, "end": 2677.6, "text": " true false, where top is true. Now, if this query is monotone, meaning as we go up in the partial", "tokens": [50364, 2074, 7908, 11, 689, 1192, 307, 2074, 13, 823, 11, 498, 341, 14581, 307, 1108, 310, 546, 11, 3620, 382, 321, 352, 493, 294, 264, 14641, 50768], "temperature": 0.0, "avg_logprob": -0.09153252781027614, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0002453601045999676}, {"id": 511, "seek": 266952, "start": 2677.6, "end": 2684.64, "text": " order of socks, we also go up in the partial order of false and true, then we don't need to be at", "tokens": [50768, 1668, 295, 17564, 11, 321, 611, 352, 493, 294, 264, 14641, 1668, 295, 7908, 293, 2074, 11, 550, 321, 500, 380, 643, 281, 312, 412, 51120], "temperature": 0.0, "avg_logprob": -0.09153252781027614, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0002453601045999676}, {"id": 512, "seek": 266952, "start": 2684.64, "end": 2692.08, "text": " top on the left. We can actually be at top in the true false lattice and guarantee that our result", "tokens": [51120, 1192, 322, 264, 1411, 13, 492, 393, 767, 312, 412, 1192, 294, 264, 2074, 7908, 34011, 293, 10815, 300, 527, 1874, 51492], "temperature": 0.0, "avg_logprob": -0.09153252781027614, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0002453601045999676}, {"id": 513, "seek": 266952, "start": 2692.08, "end": 2697.04, "text": " won't change as future updates arrive. Any update that arrives is going to cause us to increase", "tokens": [51492, 1582, 380, 1319, 382, 2027, 9205, 8881, 13, 2639, 5623, 300, 20116, 307, 516, 281, 3082, 505, 281, 3488, 51740], "temperature": 0.0, "avg_logprob": -0.09153252781027614, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0002453601045999676}, {"id": 514, "seek": 269704, "start": 2697.04, "end": 2702.0, "text": " monotonically in the domain, which has to increase monotonically in the range and therefore", "tokens": [50364, 1108, 27794, 984, 294, 264, 9274, 11, 597, 575, 281, 3488, 1108, 27794, 984, 294, 264, 3613, 293, 4412, 50612], "temperature": 0.0, "avg_logprob": -0.07436353469563421, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.00012339178647380322}, {"id": 515, "seek": 269704, "start": 2702.0, "end": 2709.6, "text": " our result will always stay true. For example, a query, is there a pair of socks?", "tokens": [50612, 527, 1874, 486, 1009, 1754, 2074, 13, 1171, 1365, 11, 257, 14581, 11, 307, 456, 257, 6119, 295, 17564, 30, 50992], "temperature": 0.0, "avg_logprob": -0.07436353469563421, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.00012339178647380322}, {"id": 516, "seek": 269704, "start": 2711.12, "end": 2715.52, "text": " So, we call a threshold query. It effectively draws a cut through this partial order and says", "tokens": [51068, 407, 11, 321, 818, 257, 14678, 14581, 13, 467, 8659, 20045, 257, 1723, 807, 341, 14641, 1668, 293, 1619, 51288], "temperature": 0.0, "avg_logprob": -0.07436353469563421, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.00012339178647380322}, {"id": 517, "seek": 269704, "start": 2715.52, "end": 2719.7599999999998, "text": " everything above this line in the partial order is true, everything below this line is false.", "tokens": [51288, 1203, 3673, 341, 1622, 294, 264, 14641, 1668, 307, 2074, 11, 1203, 2507, 341, 1622, 307, 7908, 13, 51500], "temperature": 0.0, "avg_logprob": -0.07436353469563421, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.00012339178647380322}, {"id": 518, "seek": 269704, "start": 2721.36, "end": 2725.12, "text": " So, these boolean threshold queries are a class of queries that we can freely terminate", "tokens": [51580, 407, 11, 613, 748, 4812, 282, 14678, 24109, 366, 257, 1508, 295, 24109, 300, 321, 393, 16433, 10761, 473, 51768], "temperature": 0.0, "avg_logprob": -0.07436353469563421, "compression_ratio": 1.774703557312253, "no_speech_prob": 0.00012339178647380322}, {"id": 519, "seek": 272512, "start": 2725.12, "end": 2733.2, "text": " on if we know that our update is monotone. What about a totally different setting?", "tokens": [50364, 322, 498, 321, 458, 300, 527, 5623, 307, 1108, 310, 546, 13, 708, 466, 257, 3879, 819, 3287, 30, 50768], "temperature": 0.0, "avg_logprob": -0.1152890449346498, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.0001910928840516135}, {"id": 520, "seek": 272512, "start": 2733.2, "end": 2738.24, "text": " What if we throw away monotonicity? So, now imagine that we have a deterministic finite", "tokens": [50768, 708, 498, 321, 3507, 1314, 1108, 310, 11630, 507, 30, 407, 11, 586, 3811, 300, 321, 362, 257, 15957, 3142, 19362, 51020], "temperature": 0.0, "avg_logprob": -0.1152890449346498, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.0001910928840516135}, {"id": 521, "seek": 272512, "start": 2738.24, "end": 2745.04, "text": " automata and our query is mapping to true and false from accept and reject states in the automata", "tokens": [51020, 3553, 3274, 293, 527, 14581, 307, 18350, 281, 2074, 293, 7908, 490, 3241, 293, 8248, 4368, 294, 264, 3553, 3274, 51360], "temperature": 0.0, "avg_logprob": -0.1152890449346498, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.0001910928840516135}, {"id": 522, "seek": 272512, "start": 2745.7599999999998, "end": 2749.2, "text": " and our update is appending a character. So, we think of a streaming", "tokens": [51396, 293, 527, 5623, 307, 724, 2029, 257, 2517, 13, 407, 11, 321, 519, 295, 257, 11791, 51568], "temperature": 0.0, "avg_logprob": -0.1152890449346498, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.0001910928840516135}, {"id": 523, "seek": 274920, "start": 2750.0, "end": 2754.96, "text": " character as appending. So, each update is going to transition us one step in this automata.", "tokens": [50404, 2517, 382, 724, 2029, 13, 407, 11, 1184, 5623, 307, 516, 281, 6034, 505, 472, 1823, 294, 341, 3553, 3274, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12905050061412693, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.009411138482391834}, {"id": 524, "seek": 274920, "start": 2758.48, "end": 2765.04, "text": " And in this automata, any state that we're in, we can't conclude what the final result will be", "tokens": [50828, 400, 294, 341, 3553, 3274, 11, 604, 1785, 300, 321, 434, 294, 11, 321, 393, 380, 16886, 437, 264, 2572, 1874, 486, 312, 51156], "temperature": 0.0, "avg_logprob": -0.12905050061412693, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.009411138482391834}, {"id": 525, "seek": 274920, "start": 2765.04, "end": 2770.08, "text": " because from every state, there's a state that's accepting, that's reachable, and there's a state", "tokens": [51156, 570, 490, 633, 1785, 11, 456, 311, 257, 1785, 300, 311, 17391, 11, 300, 311, 2524, 712, 11, 293, 456, 311, 257, 1785, 51408], "temperature": 0.0, "avg_logprob": -0.12905050061412693, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.009411138482391834}, {"id": 526, "seek": 274920, "start": 2770.08, "end": 2773.52, "text": " that's rejecting, that's reachable. So, some sequence of future updates might take us to", "tokens": [51408, 300, 311, 45401, 11, 300, 311, 2524, 712, 13, 407, 11, 512, 8310, 295, 2027, 9205, 1062, 747, 505, 281, 51580], "temperature": 0.0, "avg_logprob": -0.12905050061412693, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.009411138482391834}, {"id": 527, "seek": 277352, "start": 2773.52, "end": 2775.68, "text": " false, some sequence of superstructures might take us to true.", "tokens": [50364, 7908, 11, 512, 8310, 295, 29423, 44513, 1062, 747, 505, 281, 2074, 13, 50472], "temperature": 0.0, "avg_logprob": -0.1131525286312761, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018674550810828805}, {"id": 528, "seek": 277352, "start": 2778.4, "end": 2785.04, "text": " In contrast, if state three were also true, now from state one, we actually don't know if we're", "tokens": [50608, 682, 8712, 11, 498, 1785, 1045, 645, 611, 2074, 11, 586, 490, 1785, 472, 11, 321, 767, 500, 380, 458, 498, 321, 434, 50940], "temperature": 0.0, "avg_logprob": -0.1131525286312761, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018674550810828805}, {"id": 529, "seek": 277352, "start": 2785.04, "end": 2789.2, "text": " going to end up accepting or rejecting if we don't have the whole input yet. But if we're in states", "tokens": [50940, 516, 281, 917, 493, 17391, 420, 45401, 498, 321, 500, 380, 362, 264, 1379, 4846, 1939, 13, 583, 498, 321, 434, 294, 4368, 51148], "temperature": 0.0, "avg_logprob": -0.1131525286312761, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018674550810828805}, {"id": 530, "seek": 277352, "start": 2789.2, "end": 2793.7599999999998, "text": " two or three, we know that every state that's reachable via future updates is going to keep us", "tokens": [51148, 732, 420, 1045, 11, 321, 458, 300, 633, 1785, 300, 311, 2524, 712, 5766, 2027, 9205, 307, 516, 281, 1066, 505, 51376], "temperature": 0.0, "avg_logprob": -0.1131525286312761, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018674550810828805}, {"id": 531, "seek": 277352, "start": 2793.7599999999998, "end": 2800.16, "text": " in our current result, which is true. And so, we can be certain that we can terminate here and return", "tokens": [51376, 294, 527, 2190, 1874, 11, 597, 307, 2074, 13, 400, 370, 11, 321, 393, 312, 1629, 300, 321, 393, 10761, 473, 510, 293, 2736, 51696], "temperature": 0.0, "avg_logprob": -0.1131525286312761, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018674550810828805}, {"id": 532, "seek": 280016, "start": 2800.16, "end": 2806.08, "text": " true. This is kind of like a reachability sort of visual view of how we're thinking about whether", "tokens": [50364, 2074, 13, 639, 307, 733, 295, 411, 257, 2524, 2310, 1333, 295, 5056, 1910, 295, 577, 321, 434, 1953, 466, 1968, 50660], "temperature": 0.0, "avg_logprob": -0.12433908296668011, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.003823348321020603}, {"id": 533, "seek": 280016, "start": 2806.08, "end": 2810.64, "text": " or not you can freely terminate given some arbitrary update operation on a domain and query operation", "tokens": [50660, 420, 406, 291, 393, 16433, 10761, 473, 2212, 512, 23211, 5623, 6916, 322, 257, 9274, 293, 14581, 6916, 50888], "temperature": 0.0, "avg_logprob": -0.12433908296668011, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.003823348321020603}, {"id": 534, "seek": 280016, "start": 2810.64, "end": 2816.16, "text": " that maps you to a range. This is a question raised in exploring a lot of different domains. If", "tokens": [50888, 300, 11317, 291, 281, 257, 3613, 13, 639, 307, 257, 1168, 6005, 294, 12736, 257, 688, 295, 819, 25514, 13, 759, 51164], "temperature": 0.0, "avg_logprob": -0.12433908296668011, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.003823348321020603}, {"id": 535, "seek": 280016, "start": 2816.16, "end": 2821.7599999999998, "text": " anyone has any ideas for what might connect to this, definitely come find us. Now, Joe is going to", "tokens": [51164, 2878, 575, 604, 3487, 337, 437, 1062, 1745, 281, 341, 11, 2138, 808, 915, 505, 13, 823, 11, 6807, 307, 516, 281, 51444], "temperature": 0.0, "avg_logprob": -0.12433908296668011, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.003823348321020603}, {"id": 536, "seek": 282176, "start": 2821.76, "end": 2828.5600000000004, "text": " talk about partitioning. All right, this is a bit of a survey time. Boris, did you want to ask a", "tokens": [50364, 751, 466, 24808, 278, 13, 1057, 558, 11, 341, 307, 257, 857, 295, 257, 8984, 565, 13, 27158, 11, 630, 291, 528, 281, 1029, 257, 50704], "temperature": 0.0, "avg_logprob": -0.27777413262261286, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.06651828438043594}, {"id": 537, "seek": 282176, "start": 2828.5600000000004, "end": 2832.96, "text": " question, Connor? I have a question to the last slide, right? Because in a sense, this now still", "tokens": [50704, 1168, 11, 33133, 30, 286, 362, 257, 1168, 281, 264, 1036, 4137, 11, 558, 30, 1436, 294, 257, 2020, 11, 341, 586, 920, 50924], "temperature": 0.0, "avg_logprob": -0.27777413262261286, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.06651828438043594}, {"id": 538, "seek": 282176, "start": 2832.96, "end": 2838.6400000000003, "text": " has some monotone ordering, right? This kind of like, in a sense, it's kind of like if I can reach a", "tokens": [50924, 575, 512, 1108, 310, 546, 21739, 11, 558, 30, 639, 733, 295, 411, 11, 294, 257, 2020, 11, 309, 311, 733, 295, 411, 498, 286, 393, 2524, 257, 51208], "temperature": 0.0, "avg_logprob": -0.27777413262261286, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.06651828438043594}, {"id": 539, "seek": 282176, "start": 2838.6400000000003, "end": 2846.2400000000002, "text": " node and it's smaller and now basically the nodes, if the terminal nodes are the top nodes and they", "tokens": [51208, 9984, 293, 309, 311, 4356, 293, 586, 1936, 264, 13891, 11, 498, 264, 14709, 13891, 366, 264, 1192, 13891, 293, 436, 51588], "temperature": 0.0, "avg_logprob": -0.27777413262261286, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.06651828438043594}, {"id": 540, "seek": 282176, "start": 2846.2400000000002, "end": 2851.5200000000004, "text": " don't have larger nodes, then so it's still monotone in the sense. Yeah, you can find some sort of", "tokens": [51588, 500, 380, 362, 4833, 13891, 11, 550, 370, 309, 311, 920, 1108, 310, 546, 294, 264, 2020, 13, 865, 11, 291, 393, 915, 512, 1333, 295, 51852], "temperature": 0.0, "avg_logprob": -0.27777413262261286, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.06651828438043594}, {"id": 541, "seek": 285152, "start": 2851.6, "end": 2857.7599999999998, "text": " it that, yeah. Yeah, I don't know if that's true for every freely terminating function.", "tokens": [50368, 309, 300, 11, 1338, 13, 865, 11, 286, 500, 380, 458, 498, 300, 311, 2074, 337, 633, 16433, 1433, 8205, 2445, 13, 50676], "temperature": 0.0, "avg_logprob": -0.42469361540559053, "compression_ratio": 1.445, "no_speech_prob": 0.0014547273749485612}, {"id": 542, "seek": 285152, "start": 2858.48, "end": 2872.24, "text": " Psycho, do you think? Yeah, maybe. There's something about quotient lattice is, too, going at it in", "tokens": [50712, 17303, 78, 11, 360, 291, 519, 30, 865, 11, 1310, 13, 821, 311, 746, 466, 9641, 1196, 34011, 307, 11, 886, 11, 516, 412, 309, 294, 51400], "temperature": 0.0, "avg_logprob": -0.42469361540559053, "compression_ratio": 1.445, "no_speech_prob": 0.0014547273749485612}, {"id": 543, "seek": 285152, "start": 2872.24, "end": 2879.04, "text": " partial orders. Yeah, the graph looks like one. So this is love to have this conversation afterwards.", "tokens": [51400, 14641, 9470, 13, 865, 11, 264, 4295, 1542, 411, 472, 13, 407, 341, 307, 959, 281, 362, 341, 3761, 10543, 13, 51740], "temperature": 0.0, "avg_logprob": -0.42469361540559053, "compression_ratio": 1.445, "no_speech_prob": 0.0014547273749485612}, {"id": 544, "seek": 287904, "start": 2879.04, "end": 2884.24, "text": " That's why we're touching on a few things so we can have many conversations. So I think given time,", "tokens": [50364, 663, 311, 983, 321, 434, 11175, 322, 257, 1326, 721, 370, 321, 393, 362, 867, 7315, 13, 407, 286, 519, 2212, 565, 11, 50624], "temperature": 0.0, "avg_logprob": -0.10550004063230572, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.000855807273183018}, {"id": 545, "seek": 287904, "start": 2884.24, "end": 2888.4, "text": " I'm not going to go through this in any detail. I'm going to basically skip this chapter of the", "tokens": [50624, 286, 478, 406, 516, 281, 352, 807, 341, 294, 604, 2607, 13, 286, 478, 516, 281, 1936, 10023, 341, 7187, 295, 264, 50832], "temperature": 0.0, "avg_logprob": -0.10550004063230572, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.000855807273183018}, {"id": 546, "seek": 287904, "start": 2888.4, "end": 2893.6, "text": " talk, except to quickly give some assertions. So first of all, we don't have HydroFlow, so we", "tokens": [50832, 751, 11, 3993, 281, 2661, 976, 512, 19810, 626, 13, 407, 700, 295, 439, 11, 321, 500, 380, 362, 24231, 340, 31091, 11, 370, 321, 51092], "temperature": 0.0, "avg_logprob": -0.10550004063230572, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.000855807273183018}, {"id": 547, "seek": 287904, "start": 2893.6, "end": 2898.96, "text": " use Daedalus and we do have a full Daedalus to HydroFlow compiler. So we're able to write global", "tokens": [51092, 764, 3933, 292, 304, 301, 293, 321, 360, 362, 257, 1577, 3933, 292, 304, 301, 281, 24231, 340, 31091, 31958, 13, 407, 321, 434, 1075, 281, 2464, 4338, 51360], "temperature": 0.0, "avg_logprob": -0.10550004063230572, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.000855807273183018}, {"id": 548, "seek": 287904, "start": 2898.96, "end": 2904.16, "text": " programs in Daedalus and then auto partition and auto replicate them. And that's work being led by", "tokens": [51360, 4268, 294, 3933, 292, 304, 301, 293, 550, 8399, 24808, 293, 8399, 25356, 552, 13, 400, 300, 311, 589, 885, 4684, 538, 51620], "temperature": 0.0, "avg_logprob": -0.10550004063230572, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.000855807273183018}, {"id": 549, "seek": 290416, "start": 2904.16, "end": 2909.2, "text": " David Chu, who's in the room over here. David, three years ago, promised to do this and they", "tokens": [50364, 4389, 25585, 11, 567, 311, 294, 264, 1808, 670, 510, 13, 4389, 11, 1045, 924, 2057, 11, 10768, 281, 360, 341, 293, 436, 50616], "temperature": 0.0, "avg_logprob": -0.09885602403980817, "compression_ratio": 1.6357388316151202, "no_speech_prob": 0.003272735746577382}, {"id": 550, "seek": 290416, "start": 2909.2, "end": 2914.56, "text": " gave him a certificate saying that's cool. So he won the SSP student research award. And three years", "tokens": [50616, 2729, 796, 257, 15953, 1566, 300, 311, 1627, 13, 407, 415, 1582, 264, 12238, 47, 3107, 2132, 7130, 13, 400, 1045, 924, 50884], "temperature": 0.0, "avg_logprob": -0.09885602403980817, "compression_ratio": 1.6357388316151202, "no_speech_prob": 0.003272735746577382}, {"id": 551, "seek": 290416, "start": 2914.56, "end": 2919.2, "text": " later, he's got his first results. So it took a while. This was not an easy problem, but he's able", "tokens": [50884, 1780, 11, 415, 311, 658, 702, 700, 3542, 13, 407, 309, 1890, 257, 1339, 13, 639, 390, 406, 364, 1858, 1154, 11, 457, 415, 311, 1075, 51116], "temperature": 0.0, "avg_logprob": -0.09885602403980817, "compression_ratio": 1.6357388316151202, "no_speech_prob": 0.003272735746577382}, {"id": 552, "seek": 290416, "start": 2919.2, "end": 2927.44, "text": " to take arbitrary Daedalus programs, which are Daedalug, and partition them to run on multiple", "tokens": [51116, 281, 747, 23211, 3933, 292, 304, 301, 4268, 11, 597, 366, 3933, 292, 304, 697, 11, 293, 24808, 552, 281, 1190, 322, 3866, 51528], "temperature": 0.0, "avg_logprob": -0.09885602403980817, "compression_ratio": 1.6357388316151202, "no_speech_prob": 0.003272735746577382}, {"id": 553, "seek": 290416, "start": 2927.44, "end": 2931.7599999999998, "text": " machines. And I'm really not going to spend a lot of time on this. What I'll say is that", "tokens": [51528, 8379, 13, 400, 286, 478, 534, 406, 516, 281, 3496, 257, 688, 295, 565, 322, 341, 13, 708, 286, 603, 584, 307, 300, 51744], "temperature": 0.0, "avg_logprob": -0.09885602403980817, "compression_ratio": 1.6357388316151202, "no_speech_prob": 0.003272735746577382}, {"id": 554, "seek": 293176, "start": 2932.7200000000003, "end": 2938.32, "text": " earlier student Michael Whitaker, who again did this by assertion, he found all sorts of opportunities", "tokens": [50412, 3071, 3107, 5116, 21693, 4003, 11, 567, 797, 630, 341, 538, 19810, 313, 11, 415, 1352, 439, 7527, 295, 4786, 50692], "temperature": 0.0, "avg_logprob": -0.09691909002879309, "compression_ratio": 1.5993485342019544, "no_speech_prob": 0.00048780301585793495}, {"id": 555, "seek": 293176, "start": 2938.32, "end": 2943.5200000000004, "text": " to optimize Paxos because inside of Paxos, it was bottlenecking on things that were trivially", "tokens": [50692, 281, 19719, 430, 2797, 329, 570, 1854, 295, 430, 2797, 329, 11, 309, 390, 44641, 25723, 322, 721, 300, 645, 1376, 85, 2270, 50952], "temperature": 0.0, "avg_logprob": -0.09691909002879309, "compression_ratio": 1.5993485342019544, "no_speech_prob": 0.00048780301585793495}, {"id": 556, "seek": 293176, "start": 2943.5200000000004, "end": 2948.7200000000003, "text": " parallelizable, like network message handling. So he's like, if I can just bust apart some of the", "tokens": [50952, 8952, 22395, 11, 411, 3209, 3636, 13175, 13, 407, 415, 311, 411, 11, 498, 286, 393, 445, 19432, 4936, 512, 295, 264, 51212], "temperature": 0.0, "avg_logprob": -0.09691909002879309, "compression_ratio": 1.5993485342019544, "no_speech_prob": 0.00048780301585793495}, {"id": 557, "seek": 293176, "start": 2948.7200000000003, "end": 2954.1600000000003, "text": " roles in Paxos into sub roles, some of those sub roles can be replicated. And he got state-of-the-art", "tokens": [51212, 9604, 294, 430, 2797, 329, 666, 1422, 9604, 11, 512, 295, 729, 1422, 9604, 393, 312, 46365, 13, 400, 415, 658, 1785, 12, 2670, 12, 3322, 12, 446, 51484], "temperature": 0.0, "avg_logprob": -0.09691909002879309, "compression_ratio": 1.5993485342019544, "no_speech_prob": 0.00048780301585793495}, {"id": 558, "seek": 293176, "start": 2954.1600000000003, "end": 2959.28, "text": " performance in terms of throughput on Paxos by doing this. And what I observed after he did it", "tokens": [51484, 3389, 294, 2115, 295, 44629, 322, 430, 2797, 329, 538, 884, 341, 13, 400, 437, 286, 13095, 934, 415, 630, 309, 51740], "temperature": 0.0, "avg_logprob": -0.09691909002879309, "compression_ratio": 1.5993485342019544, "no_speech_prob": 0.00048780301585793495}, {"id": 559, "seek": 295928, "start": 2959.28, "end": 2963.92, "text": " was, oh my gosh, most of the things that you've split out are monotone subcomponents. And I should", "tokens": [50364, 390, 11, 1954, 452, 6502, 11, 881, 295, 264, 721, 300, 291, 600, 7472, 484, 366, 1108, 310, 546, 1422, 21541, 40496, 13, 400, 286, 820, 50596], "temperature": 0.0, "avg_logprob": -0.046287735648777174, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.003483148757368326}, {"id": 560, "seek": 295928, "start": 2963.92, "end": 2968.48, "text": " have known that we could pull those out and replicate those. In fact, I wish Bloom could do", "tokens": [50596, 362, 2570, 300, 321, 727, 2235, 729, 484, 293, 25356, 729, 13, 682, 1186, 11, 286, 3172, 25927, 727, 360, 50824], "temperature": 0.0, "avg_logprob": -0.046287735648777174, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.003483148757368326}, {"id": 561, "seek": 295928, "start": 2968.48, "end": 2972.7200000000003, "text": " that automatically, but it couldn't. So three years later, David can now automatically pull out", "tokens": [50824, 300, 6772, 11, 457, 309, 2809, 380, 13, 407, 1045, 924, 1780, 11, 4389, 393, 586, 6772, 2235, 484, 51036], "temperature": 0.0, "avg_logprob": -0.046287735648777174, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.003483148757368326}, {"id": 562, "seek": 295928, "start": 2972.7200000000003, "end": 2979.36, "text": " these things that Michael was observing and transform the program to do it. And the ideas are", "tokens": [51036, 613, 721, 300, 5116, 390, 22107, 293, 4088, 264, 1461, 281, 360, 309, 13, 400, 264, 3487, 366, 51368], "temperature": 0.0, "avg_logprob": -0.046287735648777174, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.003483148757368326}, {"id": 563, "seek": 295928, "start": 2979.36, "end": 2987.52, "text": " basically just two tricks. One trick is to take a pipeline on a single machine and split it across", "tokens": [51368, 1936, 445, 732, 11733, 13, 1485, 4282, 307, 281, 747, 257, 15517, 322, 257, 2167, 3479, 293, 7472, 309, 2108, 51776], "temperature": 0.0, "avg_logprob": -0.046287735648777174, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.003483148757368326}, {"id": 564, "seek": 298752, "start": 2987.52, "end": 2992.72, "text": " two machines. He calls that decoupling. Now in a general purpose program, this is taking,", "tokens": [50364, 732, 8379, 13, 634, 5498, 300, 979, 263, 11970, 13, 823, 294, 257, 2674, 4334, 1461, 11, 341, 307, 1940, 11, 50624], "temperature": 0.0, "avg_logprob": -0.06365977088324458, "compression_ratio": 1.7210031347962382, "no_speech_prob": 0.008574895560741425}, {"id": 565, "seek": 298752, "start": 2992.72, "end": 2996.4, "text": " I don't know, 10,000 lines of C++ and figuring out which ones to run on this machine and which", "tokens": [50624, 286, 500, 380, 458, 11, 1266, 11, 1360, 3876, 295, 383, 25472, 293, 15213, 484, 597, 2306, 281, 1190, 322, 341, 3479, 293, 597, 50808], "temperature": 0.0, "avg_logprob": -0.06365977088324458, "compression_ratio": 1.7210031347962382, "no_speech_prob": 0.008574895560741425}, {"id": 566, "seek": 298752, "start": 2996.4, "end": 3000.4, "text": " ones to run on that machine. That would be horrible, right? But in a data flow language or", "tokens": [50808, 2306, 281, 1190, 322, 300, 3479, 13, 663, 576, 312, 9263, 11, 558, 30, 583, 294, 257, 1412, 3095, 2856, 420, 51008], "temperature": 0.0, "avg_logprob": -0.06365977088324458, "compression_ratio": 1.7210031347962382, "no_speech_prob": 0.008574895560741425}, {"id": 567, "seek": 298752, "start": 3000.4, "end": 3004.32, "text": " logic language, it's quite nice. And so he has conditions for when this is safe and when it's", "tokens": [51008, 9952, 2856, 11, 309, 311, 1596, 1481, 13, 400, 370, 415, 575, 4487, 337, 562, 341, 307, 3273, 293, 562, 309, 311, 51204], "temperature": 0.0, "avg_logprob": -0.06365977088324458, "compression_ratio": 1.7210031347962382, "no_speech_prob": 0.008574895560741425}, {"id": 568, "seek": 298752, "start": 3004.32, "end": 3008.48, "text": " not. So that's decoupling. So you can think of this as refactoring a program into components", "tokens": [51204, 406, 13, 407, 300, 311, 979, 263, 11970, 13, 407, 291, 393, 519, 295, 341, 382, 1895, 578, 3662, 257, 1461, 666, 6677, 51412], "temperature": 0.0, "avg_logprob": -0.06365977088324458, "compression_ratio": 1.7210031347962382, "no_speech_prob": 0.008574895560741425}, {"id": 569, "seek": 298752, "start": 3008.48, "end": 3013.04, "text": " that can be run on different machines with asynchronous communication. The other thing", "tokens": [51412, 300, 393, 312, 1190, 322, 819, 8379, 365, 49174, 6101, 13, 440, 661, 551, 51640], "temperature": 0.0, "avg_logprob": -0.06365977088324458, "compression_ratio": 1.7210031347962382, "no_speech_prob": 0.008574895560741425}, {"id": 570, "seek": 301304, "start": 3013.04, "end": 3018.72, "text": " he does is what's sometimes called sharding in the practitioner community, but it's partitioning,", "tokens": [50364, 415, 775, 307, 437, 311, 2171, 1219, 402, 515, 278, 294, 264, 32125, 1768, 11, 457, 309, 311, 24808, 278, 11, 50648], "temperature": 0.0, "avg_logprob": -0.09476306460319309, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.003172104014083743}, {"id": 571, "seek": 301304, "start": 3018.72, "end": 3024.24, "text": " shared nothing partitioning of subplants, right? So instead of having BC take all of the data from A,", "tokens": [50648, 5507, 1825, 24808, 278, 295, 1422, 42433, 11, 558, 30, 407, 2602, 295, 1419, 14359, 747, 439, 295, 264, 1412, 490, 316, 11, 50924], "temperature": 0.0, "avg_logprob": -0.09476306460319309, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.003172104014083743}, {"id": 572, "seek": 301304, "start": 3024.24, "end": 3029.04, "text": " you have a hash partitioning here and certain values go to each machine. And how do you know", "tokens": [50924, 291, 362, 257, 22019, 24808, 278, 510, 293, 1629, 4190, 352, 281, 1184, 3479, 13, 400, 577, 360, 291, 458, 51164], "temperature": 0.0, "avg_logprob": -0.09476306460319309, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.003172104014083743}, {"id": 573, "seek": 301304, "start": 3029.04, "end": 3032.56, "text": " that each one of these computations can be done independently? That's done through things like", "tokens": [51164, 300, 1184, 472, 295, 613, 2807, 763, 393, 312, 1096, 21761, 30, 663, 311, 1096, 807, 721, 411, 51340], "temperature": 0.0, "avg_logprob": -0.09476306460319309, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.003172104014083743}, {"id": 574, "seek": 301304, "start": 3034.16, "end": 3039.12, "text": " functional dependency analysis so that you can show that certain values have no dependency on", "tokens": [51420, 11745, 33621, 5215, 370, 300, 291, 393, 855, 300, 1629, 4190, 362, 572, 33621, 322, 51668], "temperature": 0.0, "avg_logprob": -0.09476306460319309, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.003172104014083743}, {"id": 575, "seek": 303912, "start": 3039.12, "end": 3043.8399999999997, "text": " other values because they're partitioned by, say, NFD. So I'm not going to go into any of this,", "tokens": [50364, 661, 4190, 570, 436, 434, 24808, 292, 538, 11, 584, 11, 13576, 35, 13, 407, 286, 478, 406, 516, 281, 352, 666, 604, 295, 341, 11, 50600], "temperature": 0.0, "avg_logprob": -0.11291294582819535, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.002114642644301057}, {"id": 576, "seek": 303912, "start": 3043.8399999999997, "end": 3047.68, "text": " but basically what David was able to do was take many of the optimizations here that were", "tokens": [50600, 457, 1936, 437, 4389, 390, 1075, 281, 360, 390, 747, 867, 295, 264, 5028, 14455, 510, 300, 645, 50792], "temperature": 0.0, "avg_logprob": -0.11291294582819535, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.002114642644301057}, {"id": 577, "seek": 303912, "start": 3047.68, "end": 3053.2799999999997, "text": " handwritten in Scala and automate them and formalize their correctness. And without getting", "tokens": [50792, 1011, 26859, 294, 2747, 5159, 293, 31605, 552, 293, 9860, 1125, 641, 3006, 1287, 13, 400, 1553, 1242, 51072], "temperature": 0.0, "avg_logprob": -0.11291294582819535, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.002114642644301057}, {"id": 578, "seek": 303912, "start": 3053.2799999999997, "end": 3058.16, "text": " into too much detail, although it is kind of fun, oh, and we borrowed some work from Paris. So", "tokens": [51072, 666, 886, 709, 2607, 11, 4878, 309, 307, 733, 295, 1019, 11, 1954, 11, 293, 321, 26805, 512, 589, 490, 8380, 13, 407, 51316], "temperature": 0.0, "avg_logprob": -0.11291294582819535, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.002114642644301057}, {"id": 579, "seek": 303912, "start": 3058.16, "end": 3068.96, "text": " shout out to Paris for parallel disjoint correctness and colleagues. It is really fast. So he was", "tokens": [51316, 8043, 484, 281, 8380, 337, 8952, 717, 48613, 3006, 1287, 293, 7734, 13, 467, 307, 534, 2370, 13, 407, 415, 390, 51856], "temperature": 0.0, "avg_logprob": -0.11291294582819535, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.002114642644301057}, {"id": 580, "seek": 306896, "start": 3069.2, "end": 3074.08, "text": " able automatically. This is Michael's results that we re-ran. This is Scala. This is throughput", "tokens": [50376, 1075, 6772, 13, 639, 307, 5116, 311, 3542, 300, 321, 319, 12, 4257, 13, 639, 307, 2747, 5159, 13, 639, 307, 44629, 50620], "temperature": 0.0, "avg_logprob": -0.12157743260011834, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0015975767746567726}, {"id": 581, "seek": 306896, "start": 3074.08, "end": 3078.08, "text": " against latency. So what you want to do is you get as much throughput as you can until it starts", "tokens": [50620, 1970, 27043, 13, 407, 437, 291, 528, 281, 360, 307, 291, 483, 382, 709, 44629, 382, 291, 393, 1826, 309, 3719, 50820], "temperature": 0.0, "avg_logprob": -0.12157743260011834, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0015975767746567726}, {"id": 582, "seek": 306896, "start": 3078.08, "end": 3082.56, "text": " to hurt you at latency and it curls back. So this is kind of where things start to", "tokens": [50820, 281, 4607, 291, 412, 27043, 293, 309, 36950, 646, 13, 407, 341, 307, 733, 295, 689, 721, 722, 281, 51044], "temperature": 0.0, "avg_logprob": -0.12157743260011834, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0015975767746567726}, {"id": 583, "seek": 306896, "start": 3085.04, "end": 3090.16, "text": " top out, if you will. So that's Whitaker's implementation. This is the same logic as", "tokens": [51168, 1192, 484, 11, 498, 291, 486, 13, 407, 300, 311, 21693, 4003, 311, 11420, 13, 639, 307, 264, 912, 9952, 382, 51424], "temperature": 0.0, "avg_logprob": -0.12157743260011834, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0015975767746567726}, {"id": 584, "seek": 306896, "start": 3090.16, "end": 3094.7200000000003, "text": " Whitaker's implementation in Hydro. So this is just Hydro is faster than Scala written by hand.", "tokens": [51424, 21693, 4003, 311, 11420, 294, 24231, 340, 13, 407, 341, 307, 445, 24231, 340, 307, 4663, 813, 2747, 5159, 3720, 538, 1011, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12157743260011834, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.0015975767746567726}, {"id": 585, "seek": 309472, "start": 3094.7999999999997, "end": 3100.08, "text": " So this is just a testament to the folks who wrote the Hydro flow engine. But the blue line is", "tokens": [50368, 407, 341, 307, 445, 257, 35499, 281, 264, 4024, 567, 4114, 264, 24231, 340, 3095, 2848, 13, 583, 264, 3344, 1622, 307, 50632], "temperature": 0.0, "avg_logprob": -0.10323071479797363, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.0003053305554203689}, {"id": 586, "seek": 309472, "start": 3100.08, "end": 3106.0, "text": " what David achieved through systematic correctly proven rewrites. So he was able to get performance", "tokens": [50632, 437, 4389, 11042, 807, 27249, 8944, 12785, 319, 86, 30931, 13, 407, 415, 390, 1075, 281, 483, 3389, 50928], "temperature": 0.0, "avg_logprob": -0.10323071479797363, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.0003053305554203689}, {"id": 587, "seek": 309472, "start": 3106.0, "end": 3111.2799999999997, "text": " that actually, because Hydro is fast, is better than Whitaker's Paxos implementation. And this gap", "tokens": [50928, 300, 767, 11, 570, 24231, 340, 307, 2370, 11, 307, 1101, 813, 21693, 4003, 311, 430, 2797, 329, 11420, 13, 400, 341, 7417, 51192], "temperature": 0.0, "avg_logprob": -0.10323071479797363, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.0003053305554203689}, {"id": 588, "seek": 309472, "start": 3111.2799999999997, "end": 3114.8799999999997, "text": " is kind of what he's given up. These are the tricks that Michael had that we didn't cover in our", "tokens": [51192, 307, 733, 295, 437, 415, 311, 2212, 493, 13, 1981, 366, 264, 11733, 300, 5116, 632, 300, 321, 994, 380, 2060, 294, 527, 51372], "temperature": 0.0, "avg_logprob": -0.10323071479797363, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.0003053305554203689}, {"id": 589, "seek": 309472, "start": 3114.8799999999997, "end": 3122.9599999999996, "text": " rewrites. But we're doing 90% with the easy tricks. So it gives me confidence that simple", "tokens": [51372, 319, 86, 30931, 13, 583, 321, 434, 884, 4289, 4, 365, 264, 1858, 11733, 13, 407, 309, 2709, 385, 6687, 300, 2199, 51776], "temperature": 0.0, "avg_logprob": -0.10323071479797363, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.0003053305554203689}, {"id": 590, "seek": 312296, "start": 3123.04, "end": 3127.92, "text": " query optimizations known in parallel databases can have impacts on places that we're really very", "tokens": [50368, 14581, 5028, 14455, 2570, 294, 8952, 22380, 393, 362, 11606, 322, 3190, 300, 321, 434, 534, 588, 50612], "temperature": 0.0, "avg_logprob": -0.10080222900097187, "compression_ratio": 1.6284722222222223, "no_speech_prob": 0.001366867101751268}, {"id": 591, "seek": 312296, "start": 3127.92, "end": 3132.48, "text": " fine tuned performance issues that people write PhDs to get this kind of performance and we're", "tokens": [50612, 2489, 10870, 3389, 2663, 300, 561, 2464, 14476, 82, 281, 483, 341, 733, 295, 3389, 293, 321, 434, 50840], "temperature": 0.0, "avg_logprob": -0.10080222900097187, "compression_ratio": 1.6284722222222223, "no_speech_prob": 0.001366867101751268}, {"id": 592, "seek": 312296, "start": 3132.48, "end": 3138.2400000000002, "text": " getting it through systematic rewrites. Very promising. David's only halfway there though,", "tokens": [50840, 1242, 309, 807, 27249, 319, 86, 30931, 13, 4372, 20257, 13, 4389, 311, 787, 15461, 456, 1673, 11, 51128], "temperature": 0.0, "avg_logprob": -0.10080222900097187, "compression_ratio": 1.6284722222222223, "no_speech_prob": 0.001366867101751268}, {"id": 593, "seek": 312296, "start": 3138.2400000000002, "end": 3142.08, "text": " because he has to have a proper cost-based optimizer. Right now what he has is correct", "tokens": [51128, 570, 415, 575, 281, 362, 257, 2296, 2063, 12, 6032, 5028, 6545, 13, 1779, 586, 437, 415, 575, 307, 3006, 51320], "temperature": 0.0, "avg_logprob": -0.10080222900097187, "compression_ratio": 1.6284722222222223, "no_speech_prob": 0.001366867101751268}, {"id": 594, "seek": 312296, "start": 3142.08, "end": 3147.04, "text": " transformation rules. He needs a cost model with an objective function. And then he needs a search", "tokens": [51320, 9887, 4474, 13, 634, 2203, 257, 2063, 2316, 365, 364, 10024, 2445, 13, 400, 550, 415, 2203, 257, 3164, 51568], "temperature": 0.0, "avg_logprob": -0.10080222900097187, "compression_ratio": 1.6284722222222223, "no_speech_prob": 0.001366867101751268}, {"id": 595, "seek": 314704, "start": 3147.04, "end": 3153.68, "text": " strategy. And we're hopefully going to be using egg log or some implementation of egg log in Hydro", "tokens": [50364, 5206, 13, 400, 321, 434, 4696, 516, 281, 312, 1228, 3777, 3565, 420, 512, 11420, 295, 3777, 3565, 294, 24231, 340, 50696], "temperature": 0.0, "avg_logprob": -0.1270854890961008, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0574738085269928}, {"id": 596, "seek": 314704, "start": 3153.68, "end": 3159.04, "text": " flow to achieve this. So we're one of the things with Maxis stuff that overlaps is if there's a", "tokens": [50696, 3095, 281, 4584, 341, 13, 407, 321, 434, 472, 295, 264, 721, 365, 7402, 271, 1507, 300, 15986, 2382, 307, 498, 456, 311, 257, 50964], "temperature": 0.0, "avg_logprob": -0.1270854890961008, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0574738085269928}, {"id": 597, "seek": 314704, "start": 3159.04, "end": 3166.08, "text": " lovely semi lattice based data flow implementation of Maxis stuff, maybe we can clean up some of the", "tokens": [50964, 7496, 12909, 34011, 2361, 1412, 3095, 11420, 295, 7402, 271, 1507, 11, 1310, 321, 393, 2541, 493, 512, 295, 264, 51316], "temperature": 0.0, "avg_logprob": -0.1270854890961008, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0574738085269928}, {"id": 598, "seek": 314704, "start": 3166.08, "end": 3174.08, "text": " things where he's doing updates in place. This work? This work. Well, so this was written in", "tokens": [51316, 721, 689, 415, 311, 884, 9205, 294, 1081, 13, 639, 589, 30, 639, 589, 13, 1042, 11, 370, 341, 390, 3720, 294, 51716], "temperature": 0.0, "avg_logprob": -0.1270854890961008, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0574738085269928}, {"id": 599, "seek": 317408, "start": 3174.08, "end": 3179.68, "text": " Datalog and translated down into that Hydro flow data flow language you saw at the top. This stuff", "tokens": [50364, 9315, 44434, 293, 16805, 760, 666, 300, 24231, 340, 3095, 1412, 3095, 2856, 291, 1866, 412, 264, 1192, 13, 639, 1507, 50644], "temperature": 0.0, "avg_logprob": -0.16558905570737778, "compression_ratio": 1.6051779935275081, "no_speech_prob": 0.0028893148992210627}, {"id": 600, "seek": 317408, "start": 3179.68, "end": 3186.24, "text": " is also written in Datalog currently in a runtime that plays some ad-hoc tricks. That's not traditional", "tokens": [50644, 307, 611, 3720, 294, 9315, 44434, 4362, 294, 257, 34474, 300, 5749, 512, 614, 12, 71, 905, 11733, 13, 663, 311, 406, 5164, 50972], "temperature": 0.0, "avg_logprob": -0.16558905570737778, "compression_ratio": 1.6051779935275081, "no_speech_prob": 0.0028893148992210627}, {"id": 601, "seek": 317408, "start": 3186.24, "end": 3193.04, "text": " Datalog execution here as Maxis. But I think, you know, Union Find is a nice target for an", "tokens": [50972, 9315, 44434, 15058, 510, 382, 7402, 271, 13, 583, 286, 519, 11, 291, 458, 11, 8133, 11809, 307, 257, 1481, 3779, 337, 364, 51312], "temperature": 0.0, "avg_logprob": -0.16558905570737778, "compression_ratio": 1.6051779935275081, "no_speech_prob": 0.0028893148992210627}, {"id": 602, "seek": 317408, "start": 3193.04, "end": 3198.96, "text": " algebraic treatment and I think we have opportunity. Okay, what I'd like to do in the last few minutes", "tokens": [51312, 21989, 299, 5032, 293, 286, 519, 321, 362, 2650, 13, 1033, 11, 437, 286, 1116, 411, 281, 360, 294, 264, 1036, 1326, 2077, 51608], "temperature": 0.0, "avg_logprob": -0.16558905570737778, "compression_ratio": 1.6051779935275081, "no_speech_prob": 0.0028893148992210627}, {"id": 603, "seek": 317408, "start": 3198.96, "end": 3203.44, "text": " is berate you with questions because these are the things that I don't know how to answer yet and I", "tokens": [51608, 307, 5948, 473, 291, 365, 1651, 570, 613, 366, 264, 721, 300, 286, 500, 380, 458, 577, 281, 1867, 1939, 293, 286, 51832], "temperature": 0.0, "avg_logprob": -0.16558905570737778, "compression_ratio": 1.6051779935275081, "no_speech_prob": 0.0028893148992210627}, {"id": 604, "seek": 320344, "start": 3203.44, "end": 3207.84, "text": " would love to get help with. So the first is, and this is an outline, so this section goes on for", "tokens": [50364, 576, 959, 281, 483, 854, 365, 13, 407, 264, 700, 307, 11, 293, 341, 307, 364, 16387, 11, 370, 341, 3541, 1709, 322, 337, 50584], "temperature": 0.0, "avg_logprob": -0.1001004364530919, "compression_ratio": 1.6456140350877193, "no_speech_prob": 0.0006562071503140032}, {"id": 605, "seek": 320344, "start": 3207.84, "end": 3211.76, "text": " many slides, but there's the four questions. Can we please have one theory for all this nonsense", "tokens": [50584, 867, 9788, 11, 457, 456, 311, 264, 1451, 1651, 13, 1664, 321, 1767, 362, 472, 5261, 337, 439, 341, 14925, 50780], "temperature": 0.0, "avg_logprob": -0.1001004364530919, "compression_ratio": 1.6456140350877193, "no_speech_prob": 0.0006562071503140032}, {"id": 606, "seek": 320344, "start": 3211.76, "end": 3217.28, "text": " instead of the list I'm about to show you? What would be a good type system for the physical", "tokens": [50780, 2602, 295, 264, 1329, 286, 478, 466, 281, 855, 291, 30, 708, 576, 312, 257, 665, 2010, 1185, 337, 264, 4001, 51056], "temperature": 0.0, "avg_logprob": -0.1001004364530919, "compression_ratio": 1.6456140350877193, "no_speech_prob": 0.0006562071503140032}, {"id": 607, "seek": 320344, "start": 3217.28, "end": 3223.04, "text": " layer where we could prove correctness of transformations? I have a follow on to Sudipa", "tokens": [51056, 4583, 689, 321, 727, 7081, 3006, 1287, 295, 34852, 30, 286, 362, 257, 1524, 322, 281, 12323, 647, 64, 51344], "temperature": 0.0, "avg_logprob": -0.1001004364530919, "compression_ratio": 1.6456140350877193, "no_speech_prob": 0.0006562071503140032}, {"id": 608, "seek": 320344, "start": 3223.04, "end": 3229.44, "text": " about the role of our kinds of work in the era of generative AI. And then I have this ongoing", "tokens": [51344, 466, 264, 3090, 295, 527, 3685, 295, 589, 294, 264, 4249, 295, 1337, 1166, 7318, 13, 400, 550, 286, 362, 341, 10452, 51664], "temperature": 0.0, "avg_logprob": -0.1001004364530919, "compression_ratio": 1.6456140350877193, "no_speech_prob": 0.0006562071503140032}, {"id": 609, "seek": 322944, "start": 3229.44, "end": 3235.28, "text": " question of what time is for, which I probably don't have time to explain. But quickly, you know,", "tokens": [50364, 1168, 295, 437, 565, 307, 337, 11, 597, 286, 1391, 500, 380, 362, 565, 281, 2903, 13, 583, 2661, 11, 291, 458, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1894695187403151, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.004608513321727514}, {"id": 610, "seek": 322944, "start": 3235.28, "end": 3240.4, "text": " the unifying theory thing. So CRD teaser semi-ladyses, Datalog, Daedalus was all done with model", "tokens": [50656, 264, 517, 5489, 5261, 551, 13, 407, 14123, 35, 35326, 12909, 12, 9290, 749, 279, 11, 9315, 44434, 11, 3933, 292, 304, 301, 390, 439, 1096, 365, 2316, 50912], "temperature": 0.0, "avg_logprob": -0.1894695187403151, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.004608513321727514}, {"id": 611, "seek": 322944, "start": 3240.4, "end": 3246.64, "text": " theory and it's fancy actually. It uses like stable models and stuff. It's actually ended up", "tokens": [50912, 5261, 293, 309, 311, 10247, 767, 13, 467, 4960, 411, 8351, 5245, 293, 1507, 13, 467, 311, 767, 4590, 493, 51224], "temperature": 0.0, "avg_logprob": -0.1894695187403151, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.004608513321727514}, {"id": 612, "seek": 322944, "start": 3246.64, "end": 3250.96, "text": " being kind of fancy. The column theorem, Amalut, proves have these relational transducers, which", "tokens": [51224, 885, 733, 295, 10247, 13, 440, 7738, 20904, 11, 2012, 304, 325, 11, 25019, 362, 613, 38444, 1145, 8117, 433, 11, 597, 51440], "temperature": 0.0, "avg_logprob": -0.1894695187403151, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.004608513321727514}, {"id": 613, "seek": 322944, "start": 3250.96, "end": 3256.2400000000002, "text": " are this halfway world between operational and declarative semantics. You have these state", "tokens": [51440, 366, 341, 15461, 1002, 1296, 16607, 293, 16694, 1166, 4361, 45298, 13, 509, 362, 613, 1785, 51704], "temperature": 0.0, "avg_logprob": -0.1894695187403151, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.004608513321727514}, {"id": 614, "seek": 325624, "start": 3256.24, "end": 3262.0, "text": " machines on each node. They run declarative languages on each step, but then they output stuff and", "tokens": [50364, 8379, 322, 1184, 9984, 13, 814, 1190, 16694, 1166, 8650, 322, 1184, 1823, 11, 457, 550, 436, 5598, 1507, 293, 50652], "temperature": 0.0, "avg_logprob": -0.2183633322244162, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.01016693003475666}, {"id": 615, "seek": 325624, "start": 3266.72, "end": 3268.9599999999996, "text": " I think you can write non-terminating programs if you want to.", "tokens": [50888, 286, 519, 291, 393, 2464, 2107, 12, 29725, 990, 4268, 498, 291, 528, 281, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2183633322244162, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.01016693003475666}, {"id": 616, "seek": 325624, "start": 3270.56, "end": 3276.8799999999997, "text": " So you can write Toggle, for example, and Daedalus and the transducers.", "tokens": [51080, 407, 291, 393, 2464, 314, 664, 22631, 11, 337, 1365, 11, 293, 3933, 292, 304, 301, 293, 264, 1145, 8117, 433, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2183633322244162, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.01016693003475666}, {"id": 617, "seek": 325624, "start": 3279.4399999999996, "end": 3283.9199999999996, "text": " Now they don't have to be terminated. In any sense, I don't think. But the point is,", "tokens": [51524, 823, 436, 500, 380, 362, 281, 312, 1433, 5410, 13, 682, 604, 2020, 11, 286, 500, 380, 519, 13, 583, 264, 935, 307, 11, 51748], "temperature": 0.0, "avg_logprob": -0.2183633322244162, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.01016693003475666}, {"id": 618, "seek": 328392, "start": 3283.92, "end": 3287.6, "text": " I really wish he'd have done this work with this, because he also was on this work, but he didn't.", "tokens": [50364, 286, 534, 3172, 415, 1116, 362, 1096, 341, 589, 365, 341, 11, 570, 415, 611, 390, 322, 341, 589, 11, 457, 415, 994, 380, 13, 50548], "temperature": 0.0, "avg_logprob": -0.10910531636830922, "compression_ratio": 1.7529761904761905, "no_speech_prob": 0.0009109889506362379}, {"id": 619, "seek": 328392, "start": 3287.6, "end": 3292.2400000000002, "text": " He did it with transducers, which is a bummer. If you talk to distributed systems people, they", "tokens": [50548, 634, 630, 309, 365, 1145, 8117, 433, 11, 597, 307, 257, 13309, 936, 13, 759, 291, 751, 281, 12631, 3652, 561, 11, 436, 50780], "temperature": 0.0, "avg_logprob": -0.10910531636830922, "compression_ratio": 1.7529761904761905, "no_speech_prob": 0.0009109889506362379}, {"id": 620, "seek": 328392, "start": 3292.2400000000002, "end": 3295.92, "text": " talk about essentially order theory. They talk about partial orders all the time, which is related", "tokens": [50780, 751, 466, 4476, 1668, 5261, 13, 814, 751, 466, 14641, 9470, 439, 264, 565, 11, 597, 307, 4077, 50964], "temperature": 0.0, "avg_logprob": -0.10910531636830922, "compression_ratio": 1.7529761904761905, "no_speech_prob": 0.0009109889506362379}, {"id": 621, "seek": 328392, "start": 3295.92, "end": 3301.6, "text": " to lattices, but you know, it's annoying. Programmers want to write these sort of algebraic functional", "tokens": [50964, 281, 29025, 1473, 11, 457, 291, 458, 11, 309, 311, 11304, 13, 8338, 18552, 528, 281, 2464, 613, 1333, 295, 21989, 299, 11745, 51248], "temperature": 0.0, "avg_logprob": -0.10910531636830922, "compression_ratio": 1.7529761904761905, "no_speech_prob": 0.0009109889506362379}, {"id": 622, "seek": 328392, "start": 3301.6, "end": 3306.88, "text": " expressions, which I think is a good thing for all of us. And then yeah, I give all these talks", "tokens": [51248, 15277, 11, 597, 286, 519, 307, 257, 665, 551, 337, 439, 295, 505, 13, 400, 550, 1338, 11, 286, 976, 439, 613, 6686, 51512], "temperature": 0.0, "avg_logprob": -0.10910531636830922, "compression_ratio": 1.7529761904761905, "no_speech_prob": 0.0009109889506362379}, {"id": 623, "seek": 328392, "start": 3306.88, "end": 3311.6800000000003, "text": " and then some joker raises their hand and says, well, what about transactions? And in fact, Peter", "tokens": [51512, 293, 550, 512, 361, 16722, 19658, 641, 1011, 293, 1619, 11, 731, 11, 437, 466, 16856, 30, 400, 294, 1186, 11, 6508, 51752], "temperature": 0.0, "avg_logprob": -0.10910531636830922, "compression_ratio": 1.7529761904761905, "no_speech_prob": 0.0009109889506362379}, {"id": 624, "seek": 331168, "start": 3311.68, "end": 3315.68, "text": " Bayless, when he was a student, basically did an end run around my entire group and just wrote", "tokens": [50364, 7840, 1832, 11, 562, 415, 390, 257, 3107, 11, 1936, 630, 364, 917, 1190, 926, 452, 2302, 1594, 293, 445, 4114, 50564], "temperature": 0.0, "avg_logprob": -0.1419927535518523, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0019265448208898306}, {"id": 625, "seek": 331168, "start": 3315.68, "end": 3320.8799999999997, "text": " papers about transactions and coordination, and they don't align with the rest of this stuff.", "tokens": [50564, 10577, 466, 16856, 293, 21252, 11, 293, 436, 500, 380, 7975, 365, 264, 1472, 295, 341, 1507, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1419927535518523, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0019265448208898306}, {"id": 626, "seek": 331168, "start": 3320.8799999999997, "end": 3325.6, "text": " So it's an open challenge to reintegrate that work. And then, you know, I didn't actually say the S", "tokens": [50824, 407, 309, 311, 364, 1269, 3430, 281, 319, 31131, 473, 300, 589, 13, 400, 550, 11, 291, 458, 11, 286, 994, 380, 767, 584, 264, 318, 51060], "temperature": 0.0, "avg_logprob": -0.1419927535518523, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0019265448208898306}, {"id": 627, "seek": 331168, "start": 3325.6, "end": 3331.7599999999998, "text": " word yet, because I apparently didn't do joins as of yet. But we do do joins, so we probably need.", "tokens": [51060, 1349, 1939, 11, 570, 286, 7970, 994, 380, 360, 24397, 382, 295, 1939, 13, 583, 321, 360, 360, 24397, 11, 370, 321, 1391, 643, 13, 51368], "temperature": 0.0, "avg_logprob": -0.1419927535518523, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0019265448208898306}, {"id": 628, "seek": 331168, "start": 3332.64, "end": 3338.08, "text": " So it would be really great to get all of it here. I would like to bring all of this work", "tokens": [51412, 407, 309, 576, 312, 534, 869, 281, 483, 439, 295, 309, 510, 13, 286, 576, 411, 281, 1565, 439, 295, 341, 589, 51684], "temperature": 0.0, "avg_logprob": -0.1419927535518523, "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.0019265448208898306}, {"id": 629, "seek": 333808, "start": 3338.08, "end": 3344.64, "text": " into this domain. That would be really nice. Okay. Here's a flavor of what I'm dealing with,", "tokens": [50364, 666, 341, 9274, 13, 663, 576, 312, 534, 1481, 13, 1033, 13, 1692, 311, 257, 6813, 295, 437, 286, 478, 6260, 365, 11, 50692], "temperature": 0.0, "avg_logprob": -0.08158318076546736, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.003823835402727127}, {"id": 630, "seek": 333808, "start": 3344.64, "end": 3351.2, "text": " though. So just finished reading the DBSP paper, which was very nice and related to our work, but", "tokens": [50692, 1673, 13, 407, 445, 4335, 3760, 264, 413, 8176, 47, 3035, 11, 597, 390, 588, 1481, 293, 4077, 281, 527, 589, 11, 457, 51020], "temperature": 0.0, "avg_logprob": -0.08158318076546736, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.003823835402727127}, {"id": 631, "seek": 333808, "start": 3351.2, "end": 3356.64, "text": " we have some other things we need to keep track of that are relating to the network messing with", "tokens": [51020, 321, 362, 512, 661, 721, 321, 643, 281, 1066, 2837, 295, 300, 366, 23968, 281, 264, 3209, 23258, 365, 51292], "temperature": 0.0, "avg_logprob": -0.08158318076546736, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.003823835402727127}, {"id": 632, "seek": 333808, "start": 3356.64, "end": 3362.48, "text": " stuff. So when we look at a stream, it's got some values. It's got some happenstance ordering,", "tokens": [51292, 1507, 13, 407, 562, 321, 574, 412, 257, 4309, 11, 309, 311, 658, 512, 4190, 13, 467, 311, 658, 512, 1051, 372, 719, 21739, 11, 51584], "temperature": 0.0, "avg_logprob": -0.08158318076546736, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.003823835402727127}, {"id": 633, "seek": 333808, "start": 3362.48, "end": 3367.6, "text": " that's a mapping of the naturals to those values. It's got some happenstance batching. It came in", "tokens": [51584, 300, 311, 257, 18350, 295, 264, 26389, 1124, 281, 729, 4190, 13, 467, 311, 658, 512, 1051, 372, 719, 15245, 278, 13, 467, 1361, 294, 51840], "temperature": 0.0, "avg_logprob": -0.08158318076546736, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.003823835402727127}, {"id": 634, "seek": 336760, "start": 3367.6, "end": 3372.56, "text": " over the network in chunks. So there's like singly nested parentheses in this stream that are", "tokens": [50364, 670, 264, 3209, 294, 24004, 13, 407, 456, 311, 411, 1522, 356, 15646, 292, 34153, 294, 341, 4309, 300, 366, 50612], "temperature": 0.0, "avg_logprob": -0.10278073618234682, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0008295177249237895}, {"id": 635, "seek": 336760, "start": 3372.56, "end": 3378.16, "text": " randomly placed. Randomly, you know, arbitrarily ordered, arbitrarily placed. Maybe this is a", "tokens": [50612, 16979, 7074, 13, 37603, 356, 11, 291, 458, 11, 19071, 3289, 8866, 11, 19071, 3289, 7074, 13, 2704, 341, 307, 257, 50892], "temperature": 0.0, "avg_logprob": -0.10278073618234682, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0008295177249237895}, {"id": 636, "seek": 336760, "start": 3378.16, "end": 3382.96, "text": " stream of lattice points, but maybe it's not. I don't know. But if it is, you could say things", "tokens": [50892, 4309, 295, 34011, 2793, 11, 457, 1310, 309, 311, 406, 13, 286, 500, 380, 458, 13, 583, 498, 309, 307, 11, 291, 727, 584, 721, 51132], "temperature": 0.0, "avg_logprob": -0.10278073618234682, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0008295177249237895}, {"id": 637, "seek": 336760, "start": 3382.96, "end": 3387.2799999999997, "text": " like there's a monotonicity relationship between the type's natural order and the total order of", "tokens": [51132, 411, 456, 311, 257, 1108, 310, 11630, 507, 2480, 1296, 264, 2010, 311, 3303, 1668, 293, 264, 3217, 1668, 295, 51348], "temperature": 0.0, "avg_logprob": -0.10278073618234682, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0008295177249237895}, {"id": 638, "seek": 336760, "start": 3387.2799999999997, "end": 3394.56, "text": " arrival or not, right? And then what sort does is it enforces something like this, right? It's", "tokens": [51348, 18365, 420, 406, 11, 558, 30, 400, 550, 437, 1333, 775, 307, 309, 25495, 887, 746, 411, 341, 11, 558, 30, 467, 311, 51712], "temperature": 0.0, "avg_logprob": -0.10278073618234682, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0008295177249237895}, {"id": 639, "seek": 339456, "start": 3394.56, "end": 3399.12, "text": " nice when these are atomistic, like data flow is basically a set lattice that you flow individual", "tokens": [50364, 1481, 562, 613, 366, 12018, 3142, 11, 411, 1412, 3095, 307, 1936, 257, 992, 34011, 300, 291, 3095, 2609, 50592], "temperature": 0.0, "avg_logprob": -0.07010845134132787, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.0011694558197632432}, {"id": 640, "seek": 339456, "start": 3399.12, "end": 3403.68, "text": " atoms. That's the traditional kind of database iterator thing, one tuple at a time, right?", "tokens": [50592, 16871, 13, 663, 311, 264, 5164, 733, 295, 8149, 17138, 1639, 551, 11, 472, 2604, 781, 412, 257, 565, 11, 558, 30, 50820], "temperature": 0.0, "avg_logprob": -0.07010845134132787, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.0011694558197632432}, {"id": 641, "seek": 339456, "start": 3403.68, "end": 3408.72, "text": " One tuple at a time is an atomistic lattice of the sets of tuples. And it's nice when you know", "tokens": [50820, 1485, 2604, 781, 412, 257, 565, 307, 364, 12018, 3142, 34011, 295, 264, 6352, 295, 2604, 2622, 13, 400, 309, 311, 1481, 562, 291, 458, 51072], "temperature": 0.0, "avg_logprob": -0.07010845134132787, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.0011694558197632432}, {"id": 642, "seek": 339456, "start": 3408.72, "end": 3412.88, "text": " you're dealing with atoms, because you can say things like item potents, right? I gave you", "tokens": [51072, 291, 434, 6260, 365, 16871, 11, 570, 291, 393, 584, 721, 411, 3174, 1847, 791, 11, 558, 30, 286, 2729, 291, 51280], "temperature": 0.0, "avg_logprob": -0.07010845134132787, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.0011694558197632432}, {"id": 643, "seek": 339456, "start": 3412.88, "end": 3416.64, "text": " Bob's tuple once, I gave you Bob's tuple twice, sorry, but you should only have it once. So delete", "tokens": [51280, 6085, 311, 2604, 781, 1564, 11, 286, 2729, 291, 6085, 311, 2604, 781, 6091, 11, 2597, 11, 457, 291, 820, 787, 362, 309, 1564, 13, 407, 12097, 51468], "temperature": 0.0, "avg_logprob": -0.07010845134132787, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.0011694558197632432}, {"id": 644, "seek": 339456, "start": 3416.64, "end": 3421.36, "text": " one. But if I give you subsets, now you have to find how they overlap and you have to make sure", "tokens": [51468, 472, 13, 583, 498, 286, 976, 291, 2090, 1385, 11, 586, 291, 362, 281, 915, 577, 436, 19959, 293, 291, 362, 281, 652, 988, 51704], "temperature": 0.0, "avg_logprob": -0.07010845134132787, "compression_ratio": 1.8534201954397393, "no_speech_prob": 0.0011694558197632432}, {"id": 645, "seek": 342136, "start": 3421.36, "end": 3425.6, "text": " that when you union them, you remove the overlapping bits. And so when you have non-atomistic things,", "tokens": [50364, 300, 562, 291, 11671, 552, 11, 291, 4159, 264, 33535, 9239, 13, 400, 370, 562, 291, 362, 2107, 12, 267, 298, 3142, 721, 11, 50576], "temperature": 0.0, "avg_logprob": -0.16362662996564592, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.002889256924390793}, {"id": 646, "seek": 342136, "start": 3425.6, "end": 3431.6800000000003, "text": " it's just a little uglier. And you end up talking about like, does their meat, is there meat bot?", "tokens": [50576, 309, 311, 445, 257, 707, 10743, 2753, 13, 400, 291, 917, 493, 1417, 466, 411, 11, 775, 641, 4615, 11, 307, 456, 4615, 10592, 30, 50880], "temperature": 0.0, "avg_logprob": -0.16362662996564592, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.002889256924390793}, {"id": 647, "seek": 342136, "start": 3431.6800000000003, "end": 3435.04, "text": " kinds of things. Do they have no intersection, right? So these are the kinds of properties", "tokens": [50880, 3685, 295, 721, 13, 1144, 436, 362, 572, 15236, 11, 558, 30, 407, 613, 366, 264, 3685, 295, 7221, 51048], "temperature": 0.0, "avg_logprob": -0.16362662996564592, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.002889256924390793}, {"id": 648, "seek": 342136, "start": 3435.04, "end": 3440.1600000000003, "text": " that I think I need to track in my rewrite rules. And then, you know, the operators are the invariants", "tokens": [51048, 300, 286, 519, 286, 643, 281, 2837, 294, 452, 28132, 4474, 13, 400, 550, 11, 291, 458, 11, 264, 19077, 366, 264, 33270, 1719, 51304], "temperature": 0.0, "avg_logprob": -0.16362662996564592, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.002889256924390793}, {"id": 649, "seek": 342136, "start": 3440.1600000000003, "end": 3445.04, "text": " to these properties, like lattice, lattice operations are invariants of order and parenthesization.", "tokens": [51304, 281, 613, 7221, 11, 411, 34011, 11, 34011, 7705, 366, 33270, 1719, 295, 1668, 293, 23350, 279, 2144, 13, 51548], "temperature": 0.0, "avg_logprob": -0.16362662996564592, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.002889256924390793}, {"id": 650, "seek": 342136, "start": 3445.6, "end": 3449.2000000000003, "text": " Do they preserve the properties? Do they enforce different values for the properties?", "tokens": [51576, 1144, 436, 15665, 264, 7221, 30, 1144, 436, 24825, 819, 4190, 337, 264, 7221, 30, 51756], "temperature": 0.0, "avg_logprob": -0.16362662996564592, "compression_ratio": 1.8859934853420195, "no_speech_prob": 0.002889256924390793}, {"id": 651, "seek": 344920, "start": 3449.2, "end": 3455.2799999999997, "text": " The network non-deterministically enforces orders and parenthesizations. So like this is the stuff", "tokens": [50364, 440, 3209, 2107, 12, 49136, 259, 20458, 25495, 887, 9470, 293, 23350, 279, 14455, 13, 407, 411, 341, 307, 264, 1507, 50668], "temperature": 0.0, "avg_logprob": -0.1065822878191548, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0009109266102313995}, {"id": 652, "seek": 344920, "start": 3455.2799999999997, "end": 3459.6, "text": " that I worry about in my operators. And this is kind of the soup I'm swimming in with this", "tokens": [50668, 300, 286, 3292, 466, 294, 452, 19077, 13, 400, 341, 307, 733, 295, 264, 7884, 286, 478, 11989, 294, 365, 341, 50884], "temperature": 0.0, "avg_logprob": -0.1065822878191548, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0009109266102313995}, {"id": 653, "seek": 344920, "start": 3460.3999999999996, "end": 3465.2, "text": " physical algebra. So I would like help with this. All right, I'm going to do one more quick slide.", "tokens": [50924, 4001, 21989, 13, 407, 286, 576, 411, 854, 365, 341, 13, 1057, 558, 11, 286, 478, 516, 281, 360, 472, 544, 1702, 4137, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1065822878191548, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0009109266102313995}, {"id": 654, "seek": 344920, "start": 3465.9199999999996, "end": 3469.3599999999997, "text": " This is very much in the realm of what Sudipa was talking about. You know, we're in the era where", "tokens": [51200, 639, 307, 588, 709, 294, 264, 15355, 295, 437, 12323, 647, 64, 390, 1417, 466, 13, 509, 458, 11, 321, 434, 294, 264, 4249, 689, 51372], "temperature": 0.0, "avg_logprob": -0.1065822878191548, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0009109266102313995}, {"id": 655, "seek": 344920, "start": 3469.3599999999997, "end": 3474.8799999999997, "text": " people will be programming with green goo, right? It's just this is large language models, they're", "tokens": [51372, 561, 486, 312, 9410, 365, 3092, 33192, 11, 558, 30, 467, 311, 445, 341, 307, 2416, 2856, 5245, 11, 436, 434, 51648], "temperature": 0.0, "avg_logprob": -0.1065822878191548, "compression_ratio": 1.632996632996633, "no_speech_prob": 0.0009109266102313995}, {"id": 656, "seek": 347488, "start": 3474.88, "end": 3479.6, "text": " magical, they produce stuff. But what we really want is building blocks that we can count on,", "tokens": [50364, 12066, 11, 436, 5258, 1507, 13, 583, 437, 321, 534, 528, 307, 2390, 8474, 300, 321, 393, 1207, 322, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1180596242424186, "compression_ratio": 1.6606606606606606, "no_speech_prob": 0.014057951048016548}, {"id": 657, "seek": 347488, "start": 3479.6, "end": 3483.36, "text": " right? We're a database people, our industry is all about foundational building blocks.", "tokens": [50600, 558, 30, 492, 434, 257, 8149, 561, 11, 527, 3518, 307, 439, 466, 32195, 2390, 8474, 13, 50788], "temperature": 0.0, "avg_logprob": -0.1180596242424186, "compression_ratio": 1.6606606606606606, "no_speech_prob": 0.014057951048016548}, {"id": 658, "seek": 347488, "start": 3483.36, "end": 3487.76, "text": " And I really do think declarative specification is a lovely narrow waste here between these two,", "tokens": [50788, 400, 286, 534, 360, 519, 16694, 1166, 31256, 307, 257, 7496, 9432, 5964, 510, 1296, 613, 732, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1180596242424186, "compression_ratio": 1.6606606606606606, "no_speech_prob": 0.014057951048016548}, {"id": 659, "seek": 347488, "start": 3487.76, "end": 3493.52, "text": " where we can take a formal spec as Cod asked us to, we can render it in some sense so that it's", "tokens": [51008, 689, 321, 393, 747, 257, 9860, 1608, 382, 383, 378, 2351, 505, 281, 11, 321, 393, 15529, 309, 294, 512, 2020, 370, 300, 309, 311, 51296], "temperature": 0.0, "avg_logprob": -0.1180596242424186, "compression_ratio": 1.6606606606606606, "no_speech_prob": 0.014057951048016548}, {"id": 660, "seek": 347488, "start": 3493.52, "end": 3498.48, "text": " readable, right? And this relates to things like Wolfgang's work on visualizing queries,", "tokens": [51296, 49857, 11, 558, 30, 400, 341, 16155, 281, 721, 411, 16634, 19619, 311, 589, 322, 5056, 3319, 24109, 11, 51544], "temperature": 0.0, "avg_logprob": -0.1180596242424186, "compression_ratio": 1.6606606606606606, "no_speech_prob": 0.014057951048016548}, {"id": 661, "seek": 347488, "start": 3498.48, "end": 3502.32, "text": " and what Sudipa was talking about in terms of giving natural language things, but helping", "tokens": [51544, 293, 437, 12323, 647, 64, 390, 1417, 466, 294, 2115, 295, 2902, 3303, 2856, 721, 11, 457, 4315, 51736], "temperature": 0.0, "avg_logprob": -0.1180596242424186, "compression_ratio": 1.6606606606606606, "no_speech_prob": 0.014057951048016548}, {"id": 662, "seek": 350232, "start": 3502.32, "end": 3506.32, "text": " people look at this and say, is this what you meant? Not is it correct, but is this what you", "tokens": [50364, 561, 574, 412, 341, 293, 584, 11, 307, 341, 437, 291, 4140, 30, 1726, 307, 309, 3006, 11, 457, 307, 341, 437, 291, 50564], "temperature": 0.0, "avg_logprob": -0.10119713963689031, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0028889139648526907}, {"id": 663, "seek": 350232, "start": 3506.32, "end": 3511.76, "text": " meant since a spec after all, right? Did you mean this query? And then of course, if it's in a nice", "tokens": [50564, 4140, 1670, 257, 1608, 934, 439, 11, 558, 30, 2589, 291, 914, 341, 14581, 30, 400, 550, 295, 1164, 11, 498, 309, 311, 294, 257, 1481, 50836], "temperature": 0.0, "avg_logprob": -0.10119713963689031, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0028889139648526907}, {"id": 664, "seek": 350232, "start": 3511.76, "end": 3517.92, "text": " formal language, we can check it for other things, right? And so that would be, I think, a role that", "tokens": [50836, 9860, 2856, 11, 321, 393, 1520, 309, 337, 661, 721, 11, 558, 30, 400, 370, 300, 576, 312, 11, 286, 519, 11, 257, 3090, 300, 51144], "temperature": 0.0, "avg_logprob": -0.10119713963689031, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0028889139648526907}, {"id": 665, "seek": 350232, "start": 3517.92, "end": 3523.04, "text": " we can really play in the world. And I suspect things like this will happen. These programs are", "tokens": [51144, 321, 393, 534, 862, 294, 264, 1002, 13, 400, 286, 9091, 721, 411, 341, 486, 1051, 13, 1981, 4268, 366, 51400], "temperature": 0.0, "avg_logprob": -0.10119713963689031, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0028889139648526907}, {"id": 666, "seek": 350232, "start": 3523.04, "end": 3526.7200000000003, "text": " going to be a selection of programs. You're constantly going to be given a menu of which of", "tokens": [51400, 516, 281, 312, 257, 9450, 295, 4268, 13, 509, 434, 6460, 516, 281, 312, 2212, 257, 6510, 295, 597, 295, 51584], "temperature": 0.0, "avg_logprob": -0.10119713963689031, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0028889139648526907}, {"id": 667, "seek": 350232, "start": 3526.7200000000003, "end": 3531.6800000000003, "text": " these things did you mean. And the answer to which is either some invariant checks or something,", "tokens": [51584, 613, 721, 630, 291, 914, 13, 400, 264, 1867, 281, 597, 307, 2139, 512, 33270, 394, 13834, 420, 746, 11, 51832], "temperature": 0.0, "avg_logprob": -0.10119713963689031, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0028889139648526907}, {"id": 668, "seek": 353168, "start": 3531.68, "end": 3538.16, "text": " or some human judgment. So I think we're in a good spot in terms of intermediate languages.", "tokens": [50364, 420, 512, 1952, 12216, 13, 407, 286, 519, 321, 434, 294, 257, 665, 4008, 294, 2115, 295, 19376, 8650, 13, 50688], "temperature": 0.0, "avg_logprob": -0.11119915190197173, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.0015485299518331885}, {"id": 669, "seek": 353168, "start": 3538.16, "end": 3543.04, "text": " And I'll just close with one more slide, maybe just a handful of slides.", "tokens": [50688, 400, 286, 603, 445, 1998, 365, 472, 544, 4137, 11, 1310, 445, 257, 16458, 295, 9788, 13, 50932], "temperature": 0.0, "avg_logprob": -0.11119915190197173, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.0015485299518331885}, {"id": 670, "seek": 353168, "start": 3544.08, "end": 3547.6, "text": " What are clocks in time for in distributed systems? So there's this famous saying, which", "tokens": [50984, 708, 366, 41528, 294, 565, 337, 294, 12631, 3652, 30, 407, 456, 311, 341, 4618, 1566, 11, 597, 51160], "temperature": 0.0, "avg_logprob": -0.11119915190197173, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.0015485299518331885}, {"id": 671, "seek": 353168, "start": 3547.6, "end": 3552.08, "text": " is correctly attributed to a sci-fi short story. Time is what keeps everything from happening", "tokens": [51160, 307, 8944, 30976, 281, 257, 2180, 12, 13325, 2099, 1657, 13, 6161, 307, 437, 5965, 1203, 490, 2737, 51384], "temperature": 0.0, "avg_logprob": -0.11119915190197173, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.0015485299518331885}, {"id": 672, "seek": 353168, "start": 3552.08, "end": 3557.3599999999997, "text": " all at once. So when should we use time and computing? What are clocks for? Well,", "tokens": [51384, 439, 412, 1564, 13, 407, 562, 820, 321, 764, 565, 293, 15866, 30, 708, 366, 41528, 337, 30, 1042, 11, 51648], "temperature": 0.0, "avg_logprob": -0.11119915190197173, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.0015485299518331885}, {"id": 673, "seek": 355736, "start": 3558.32, "end": 3563.52, "text": " they're not for monotone queries. I can run this embarrassingly parallel. It can all happen at the", "tokens": [50412, 436, 434, 406, 337, 1108, 310, 546, 24109, 13, 286, 393, 1190, 341, 9187, 12163, 8952, 13, 467, 393, 439, 1051, 412, 264, 50672], "temperature": 0.0, "avg_logprob": -0.09057052785700018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0012447121553122997}, {"id": 674, "seek": 355736, "start": 3563.52, "end": 3571.2000000000003, "text": " same time, and it's fine. And Buntalu was doing this long before this discussion. But I can't run", "tokens": [50672, 912, 565, 11, 293, 309, 311, 2489, 13, 400, 363, 2760, 4929, 390, 884, 341, 938, 949, 341, 5017, 13, 583, 286, 393, 380, 1190, 51056], "temperature": 0.0, "avg_logprob": -0.09057052785700018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0012447121553122997}, {"id": 675, "seek": 355736, "start": 3571.2000000000003, "end": 3576.2400000000002, "text": " this at the same time. You can't have P and not P at the same time. So what's the deadliest answer", "tokens": [51056, 341, 412, 264, 912, 565, 13, 509, 393, 380, 362, 430, 293, 406, 430, 412, 264, 912, 565, 13, 407, 437, 311, 264, 3116, 16850, 1867, 51308], "temperature": 0.0, "avg_logprob": -0.09057052785700018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0012447121553122997}, {"id": 676, "seek": 355736, "start": 3576.2400000000002, "end": 3583.28, "text": " to that is, well, that's what time is for. This is the toggle program, right? And time is there to", "tokens": [51308, 281, 300, 307, 11, 731, 11, 300, 311, 437, 565, 307, 337, 13, 639, 307, 264, 31225, 1461, 11, 558, 30, 400, 565, 307, 456, 281, 51660], "temperature": 0.0, "avg_logprob": -0.09057052785700018, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0012447121553122997}, {"id": 677, "seek": 358328, "start": 3583.28, "end": 3588.1600000000003, "text": " separate two things that can't coexist. That should be, I think, the only reason for time.", "tokens": [50364, 4994, 732, 721, 300, 393, 380, 48086, 13, 663, 820, 312, 11, 286, 519, 11, 264, 787, 1778, 337, 565, 13, 50608], "temperature": 0.0, "avg_logprob": -0.12659417168568757, "compression_ratio": 1.6094276094276094, "no_speech_prob": 0.015903545543551445}, {"id": 678, "seek": 358328, "start": 3589.76, "end": 3595.1200000000003, "text": " Except it's not. Distributed systems, people use time for maintaining partial orders and knowing", "tokens": [50688, 16192, 309, 311, 406, 13, 9840, 2024, 4866, 3652, 11, 561, 764, 565, 337, 14916, 14641, 9470, 293, 5276, 50956], "temperature": 0.0, "avg_logprob": -0.12659417168568757, "compression_ratio": 1.6094276094276094, "no_speech_prob": 0.015903545543551445}, {"id": 679, "seek": 358328, "start": 3595.1200000000003, "end": 3600.32, "text": " when things were concurrent. Sometimes you don't need that. Sometimes you do. But this is my question,", "tokens": [50956, 562, 721, 645, 37702, 13, 4803, 291, 500, 380, 643, 300, 13, 4803, 291, 360, 13, 583, 341, 307, 452, 1168, 11, 51216], "temperature": 0.0, "avg_logprob": -0.12659417168568757, "compression_ratio": 1.6094276094276094, "no_speech_prob": 0.015903545543551445}, {"id": 680, "seek": 358328, "start": 3600.32, "end": 3604.96, "text": " especially because Val's in the room and worked on this DSP stuff. Daedalus has one clock per", "tokens": [51216, 2318, 570, 7188, 311, 294, 264, 1808, 293, 2732, 322, 341, 15816, 47, 1507, 13, 3933, 292, 304, 301, 575, 472, 7830, 680, 51448], "temperature": 0.0, "avg_logprob": -0.12659417168568757, "compression_ratio": 1.6094276094276094, "no_speech_prob": 0.015903545543551445}, {"id": 681, "seek": 358328, "start": 3604.96, "end": 3610.96, "text": " node and we update it only when there's a network event or we have to cycle through negation.", "tokens": [51448, 9984, 293, 321, 5623, 309, 787, 562, 456, 311, 257, 3209, 2280, 420, 321, 362, 281, 6586, 807, 2485, 399, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12659417168568757, "compression_ratio": 1.6094276094276094, "no_speech_prob": 0.015903545543551445}, {"id": 682, "seek": 361096, "start": 3611.92, "end": 3616.56, "text": " Timely and differential data flow have clocks all over the damn place. And I'm not sure when you", "tokens": [50412, 7172, 736, 293, 15756, 1412, 3095, 362, 41528, 439, 670, 264, 8151, 1081, 13, 400, 286, 478, 406, 988, 562, 291, 50644], "temperature": 0.0, "avg_logprob": -0.1213607344516488, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.00018521993479225785}, {"id": 683, "seek": 361096, "start": 3616.56, "end": 3622.96, "text": " use them and when you don't. So, for example, tracking iterations of a monotonic recursive", "tokens": [50644, 764, 552, 293, 562, 291, 500, 380, 13, 407, 11, 337, 1365, 11, 11603, 36540, 295, 257, 1108, 310, 11630, 20560, 488, 50964], "temperature": 0.0, "avg_logprob": -0.1213607344516488, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.00018521993479225785}, {"id": 684, "seek": 361096, "start": 3622.96, "end": 3626.48, "text": " program. Why do I need a clock for that? I don't think I need a clock for that. Maybe if it's a", "tokens": [50964, 1461, 13, 1545, 360, 286, 643, 257, 7830, 337, 300, 30, 286, 500, 380, 519, 286, 643, 257, 7830, 337, 300, 13, 2704, 498, 309, 311, 257, 51140], "temperature": 0.0, "avg_logprob": -0.1213607344516488, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.00018521993479225785}, {"id": 685, "seek": 361096, "start": 3626.48, "end": 3632.16, "text": " while and we use the index in our computation, I need to know what index I'm at. So the general", "tokens": [51140, 1339, 293, 321, 764, 264, 8186, 294, 527, 24903, 11, 286, 643, 281, 458, 437, 8186, 286, 478, 412, 13, 407, 264, 2674, 51424], "temperature": 0.0, "avg_logprob": -0.1213607344516488, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.00018521993479225785}, {"id": 686, "seek": 361096, "start": 3632.16, "end": 3638.7200000000003, "text": " question is, when do we use this structure called a clock? And when don't we need? And can a compiler", "tokens": [51424, 1168, 307, 11, 562, 360, 321, 764, 341, 3877, 1219, 257, 7830, 30, 400, 562, 500, 380, 321, 643, 30, 400, 393, 257, 31958, 51752], "temperature": 0.0, "avg_logprob": -0.1213607344516488, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.00018521993479225785}, {"id": 687, "seek": 363872, "start": 3638.72, "end": 3644.9599999999996, "text": " decide? All right. We are a little over time. I hope we have given you lots of things to ask us", "tokens": [50364, 4536, 30, 1057, 558, 13, 492, 366, 257, 707, 670, 565, 13, 286, 1454, 321, 362, 2212, 291, 3195, 295, 721, 281, 1029, 505, 50676], "temperature": 0.0, "avg_logprob": -0.23542820779900803, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.012408290058374405}, {"id": 688, "seek": 363872, "start": 3644.9599999999996, "end": 3651.68, "text": " later. We need lots of help. So we'd love it. And we are big fans of working with folks like you.", "tokens": [50676, 1780, 13, 492, 643, 3195, 295, 854, 13, 407, 321, 1116, 959, 309, 13, 400, 321, 366, 955, 4499, 295, 1364, 365, 4024, 411, 291, 13, 51012], "temperature": 0.0, "avg_logprob": -0.23542820779900803, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.012408290058374405}, {"id": 689, "seek": 363872, "start": 3651.68, "end": 3657.68, "text": " Can you leave these last four slides there so we can come up with some more folk questions?", "tokens": [51012, 1664, 291, 1856, 613, 1036, 1451, 9788, 456, 370, 321, 393, 808, 493, 365, 512, 544, 15748, 1651, 30, 51312], "temperature": 0.0, "avg_logprob": -0.23542820779900803, "compression_ratio": 1.4921465968586387, "no_speech_prob": 0.012408290058374405}], "language": "en"}