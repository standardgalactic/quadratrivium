start	end	text
0	7560	Okay, so I'll just stop here, thanks for coming.
7560	12320	I'm Xiang and today I'll be talking about theoretical and practical insights from linear
12320	13320	transformers.
13320	23320	As you know, large language models work surprisingly well in practice and the basis of large language
23320	26680	models is the transformer neural network.
26680	33080	So an important question is why do transformers work and how can we train them effectively?
33080	39560	But directly this question is difficult to answer because large language models have billions,
39560	44360	maybe even trillions of parameters, they have a lot of moving parts, different choices
44360	49680	of normalization, of embedding and they contain many different kinds of modules.
49680	57240	So it is important to have a mathematical abstraction that captures the essence of transformer
57240	63200	learning and optimization in order to better understand transformers.
63200	69360	And today we are going to look at how the simple linear transformer can shed light on
69360	71240	two important questions.
71240	78560	One is the mechanism behind a phenomenon known as in-context learning and two, I'll tell
78560	83440	you about how linear transformer optimization shares many of the unconventional features
83440	85760	of real transformer optimization.
85760	91880	And these two points are based on two papers which are in joint work with Kwong Joon, Ha-Dee,
91880	101040	Subritz, Ali from MIT as well as Minghak, Charlie from KAIST.
101040	105280	So part one will be understanding in-context learning.
105280	107160	So what is in-context learning?
107160	112680	A very standard task of large language models is that of next-word prediction.
112680	119520	For example, you can give GPT a prompt, Mary has a little blank and it tells you Mary has
119520	120760	a little lamb.
120760	125240	But that's not so surprising because somewhere in this training data I probably saw this
125240	129520	exact sentence, maybe thousands of times.
129520	133760	So then in-context learning refers to the following kind of prompting.
133760	137880	So I'll provide GPT with a few demonstrations.
137880	143480	I'll say apple is red, banana is yellow, then what is a grape?
143480	149800	And then so apple, red and banana yellow serve as contextual examples.
149800	153880	And based off of these contextual examples, GPT infers that I'm looking for the color
153880	157160	of the fruit so it feels in purple.
157160	160880	And this phenomenon even works for arbitrary made-up rules.
160880	166800	So this is a bunch of gibberish but the ad symbol denotes concatenation and it correctly
166800	169800	infers that ad denotes concatenation.
169800	175560	So it probably hasn't seen this exact example anywhere in this training set but it still
175560	177600	does the right thing.
177600	185000	I would say that I tried some very complicated gibberish and it doesn't work there.
185000	194080	So that's a good thing, I guess, because otherwise we are all out of jobs soon.
194080	201600	And so to the best of my knowledge, this in-context learning phenomenon was first reported in
201600	208320	a seminal paper by Brown, Mann, Ryder, Soubia and collaborators.
208320	220640	And this paper was also the one that coined the phrase in-context learning, I think.
220640	225360	I would also say that from a machine learning point of view, if you give your model a few
225360	232440	demonstrations and then it does well on those demonstrations, it's not terribly surprising.
232440	238960	But the thing is here in-context learning works without any updates to the model parameters.
238960	244800	So I'm not doing any fine-tuning, I'm just using the same transformer and then it does
244800	245800	the right thing.
245800	247920	So that's, I guess, the surprising thing.
247920	252440	Some people even go as far as claim that it's one of the main reasons for why large language
252440	254560	models work so well in practice.
254560	261600	But regardless, understanding how large models do in-context learning is a very important
261600	269280	question to us understanding large language models.
269280	277480	So in recent years, there's a few very important papers that try to shed light on this phenomenon.
277480	282640	Our work is based on a few of these papers, so I'll do a quick review of the relevant
282640	284920	literature now.
284920	290360	The first such paper is the one by Garak Sipras-Leon Valiant, 2022.
290360	294160	And the title is, What Can Transformers Learn In-Context, A Case Study of Simple Function
294160	295640	Classes.
295640	300520	So we call the example about fruits and colors and the hidden rule is that y is the color
300520	303560	of x, where x is some fruit and y is some color.
303560	310880	But it's very hard to reason about what does it mean, what kind of function is represented
310880	316240	by the question about color in, say, the embedding space of words.
316240	323360	So what they propose to do is consider a simplified setup where your x are Euclidean
323360	330320	vectors and the y's are linearly related to x by some unobserved data.
330320	332520	So it's a linear regression problem.
332520	339440	Pictorially, you're given a bunch of demonstrations, the black dots, x, y pairs.
339440	344040	And then the question is in the form of the red dot, xn plus 1.
344040	348440	And you're trying to figure out what the label y is.
348440	350440	The revolution is a bit low.
350440	351440	OK.
351440	365840	So a follow-up paper by Akirak, Sherman, Andreas, Ma, and Zoe in 2023.
365840	374480	They further try to characterize what kind of, or they try to characterize how transformers
374480	379560	are able to learn these functions such as linear functions.
379560	387600	And the main takeaway here is that transformers can learn in context because they are able
387600	390200	to implement various learning algorithms.
390800	396400	For example, this plot here shows that experimentally, transformers appear to implement the ordinary
396400	399680	least squares algorithm on noiseless data.
399680	404160	So the green line is like zero throughout.
404160	411760	And so the transform prediction has a very good agreement with ordinary least squares.
411760	418320	And the way they show that transformers are able to implement this learning algorithm
418320	419880	is approved by construction.
419880	425120	So they have this very clever construction, where they define a few algorithmic primitives
425120	431880	like multiplication, division, affine transformation, and they show that the attention unit can
431880	433400	implement this primitive.
433400	437880	So by hooking together various attention units, you're able to implement algorithms
437880	439400	such as iOS.
439400	443320	And I'll mention here that further along this direction, there are also more extreme examples
443320	450440	where people show that the attention architecture can implement some kind of register system
450440	451440	or something.
451440	456920	So you can implement arbitrary algorithms and transformers can be called as programmable
456920	459240	computers.
459240	463760	The catch is that all these constructions are very clever, but the downside is that also
463760	465760	means they are very fragile.
465760	470760	And it's unclear whether these very clever constructions are actually recovered when
470760	475880	you, say, train your transformers with the atom algorithm.
475880	478040	So the next in line are two papers.
478040	482960	Linear transformers are circularly fast-waist programs by Schlag, Erich, Mied, Huber in
482960	483960	2010.
483960	496240	And a closely related paper by Oswald, Nikolson, Luan, Dazzo, Sacramento, Moff, and Blak Mirov
496240	497240	in 2010.
497240	502800	So I'll mainly focus on the second paper, Transformers, Learning Contacts by Gradient
502800	503800	Descent.
503800	507160	And so here, as well, they do approve by construction.
507160	511640	But there is a very important difference from previous papers, which is they consider the
511640	518160	linear transformers, which is, so all previous papers, they may consider a simplified setting
518160	521920	where the problem is linear regression, but the architecture was always full transformers.
521920	525680	But here, it's the first time people look at linear transformers, which I'll define
525680	526680	a bit.
527080	530280	It's simpler than full transformers.
530280	534240	And because they look at linear transformers, they are able to provide a very simple construction
534240	541760	where, under which linear transformers are able to implement gradient descent, which
541760	545480	in turn allows them to learn linear functions in context.
545480	553680	And remarkably, they have some pretty convincing experiments which agree with their construction.
553680	556840	So that's for prior work.
556840	562680	And now I come to our paper, where we try to answer the following question.
562680	566960	So we saw that transformers are expressive enough to implement a whole bunch of algorithms,
566960	571800	but can we show that transformers actually learn to implement any of these algorithms
571800	576160	during training?
576160	579000	So let me set up the problem.
579000	586520	When you have a transformer, the input is of the form of a matrix, d by n, where you
586520	590600	can think of it as a horizontally stacked bunch of tokens.
590600	593080	Each token is kind of like a word in a sentence.
593080	596160	So your sentence will get, you know, embedded in the impedance space and turn into a bunch
596160	598600	of factors.
598600	602360	So if you want to see it.
602360	609840	The standard self-attention module is the following function.
609840	613560	So you have your key value query matrix.
613560	624720	You have a mask for this causality, and then you put a softmax on this thing.
624720	631080	And linear self-attention is basically the same thing except we take out the softmax.
631080	636520	And that's why it's called linear because we took out the non-linearity.
636520	642320	And by the same time, I would say that the phrase linear attention is maybe a bit of
642320	649360	a misnomer because the linear attention module is not linear in either the parameter PQK
649360	650360	or in its input Z.
650360	654840	In fact, it's a third or the polynomial of Z.
654840	660280	Because of this, the representation power, when you stack a bunch of these linear attention
660280	663720	modules on top of each other, it increases.
663720	668840	So this is in contrast to something like a linear fully connected neural network.
668840	673640	So no matter how many of these you stack, you always are a linear function of an input.
673640	677040	But you can actually represent increasingly high-degree polynomials by stacking linear
677040	679520	attention units.
679520	684480	And the linear transformer, which we'll look at, is basically stacking these attention
684480	689240	units by a residual connection.
689240	698000	And to be precise, this is a single-headed transformer.
698000	701480	So that's defining the linear transformer architecture.
701480	705720	And now let me properly set up the learning objective.
705720	711240	So as mentioned earlier, the input to a transformer is a sequence of tokens.
711240	717760	And in the linear regression setting, each token Z consists of an x, y pair, where x
717760	724640	is a d-dimensional Euclidean vector, y is a scalar, and x, y are related by this linear
724640	725640	relationship.
725640	730120	And theta star is unobserved.
730120	734280	And on top of that, each prompt has a different theta star.
734280	741800	The goal, you are also given a xm plus 1, but without the label ym plus 1.
741800	746080	And the goal is to train the transformer to predict the hidden label.
746560	751520	Given the demonstrations, as well as xm plus 1.
751520	758720	I was stressed that this problem is much harder than simply learning a single theta
758720	762520	star, because theta star changes from prompt to prompt.
762520	767360	So you need to learn an algorithm that, given a few demonstrations, infers the right theta
767360	770520	star regardless of what the theta star is.
776520	783360	And at this point, I would also mention that one of the reasons for choosing to focus on
783360	788080	linear attention as opposed to the softmax attention, besides the fact that linear attention
788080	794560	is simpler and easier to understand, is that for this problem of learning a linear function,
794560	802120	linear transformers perform much better than softmax transformers.
802120	806720	And I guess we'll see concretely why that is in a bit.
806720	813360	But even now, just intuitively, if your data are linearly related, then it makes sense
813360	823920	that softmax doesn't really help you with all that much.
823920	827120	So here's the first result I'll talk about.
827120	833160	We study one layer, linear transformer, and we claim that it implements one step of gradient
833160	836240	descent at global minimum.
836240	839440	So what does it mean for a transformer to implement one step of gradient descent?
839440	843240	On the left here, in this box, I show the architecture of a one-layer linear transformer.
843240	844240	It's very simple.
844240	850600	If you have a z, it passes through a single attention layer, and then, you know, we get
850600	859840	some output tfz, where tf subscript lz, denotes the transformer's prediction at layer l,
859840	861560	given input z, and parameter w.
861560	866720	So parameter w is like the correct key value matrices.
866720	870720	And we try to minimize the in-contact loss, which is the expected difference between the
870720	872640	prediction and the true label.
872640	878640	And the expectation is taken over both z, which is the input, as well as theta star,
878680	884640	which is the unobserved linear relationship.
884640	889320	And if you forget about transformers, we're a bit a very reasonable thing to try to do
889320	893960	when given a bunch of demonstrations, and you need to infer the correct label, is to
893960	901000	maintain a theta and then run gradient descent on it, with respect to the empirical least
901000	905720	grass loss, which I highlight in problem.
905720	916080	And so here, I just take one step, a single step, of gradient descent, and if n is sufficiently
916080	920520	large, you know, you'll probably do decently.
920520	922760	So theorem one of our paper states the following.
922760	928440	If you assume that the covariates are sampled from the standard normal distribution and theta
928480	932560	star is also sampled from the standard normal distribution, then the linear transformer
932560	939800	that minimizes the in-contact loss, fw, which is in red, gives the same prediction as the
939800	944760	one step gradient descent on r theta, which I highlight in purple.
944760	955040	So in other words, the output, tf1 of v comma w, is the same as if you ran one step of gradient
955040	965280	descent on theta, and use that to predict the label.
965280	971640	And in fact, you can consider a more general setting when your covariates are sampled from
971640	976720	some distribution with a non-identity covariate.
976720	980360	So when your covariance is sigma here, the linear transformer that globally minimizes
980480	988480	the loss now coincides with running one step of precondition gradient descent, where the
988480	995440	preconditioner a is given by following.
995440	1003320	For when n is very large, which is when you're given a ton of demonstrations of, you know,
1003480	1014000	xy pairs, a in the limit is just inverse of your covariance matrix, the covariance matrix
1014000	1017520	of your covariates.
1017520	1023240	But when n is small, there's this additional regularization.
1023240	1029440	Is it obvious that the global minimum is unique, and if not, is this a statement about any
1029440	1030440	global minimum?
1030560	1033800	Yeah, so this is a statement about any global minimum.
1033800	1042440	In fact, there is some obvious, you know, no spaces in the loss, I guess, because one
1042440	1045840	example is the query times key matrix.
1045840	1051360	You can scale them arbitrarily if their product is the same, then that's the same.
1051360	1056680	Another example is, you know, since this is a linear transformer, scaling the value key
1056760	1062000	current matrices arbitrarily as long as they multiply the same thing also gives the exact
1062000	1065720	same predictor.
1065720	1071080	But then one might wonder, you know, ignoring these inferences, is that unique?
1071080	1072080	I'm not sure.
1072080	1073080	I'm not sure.
1073080	1078160	In perfectly, in all our experiments, this is always recovered.
1078160	1080040	So that's a good sign.
1080040	1086600	And also, as I'm just about to mention, there are two concurrent works, which appear surely
1086600	1089080	after we publish the initial draft.
1089080	1093680	One of them characterizes global optimality for one-layer linear transformer on the similar
1093680	1094680	sign.
1094680	1095680	So similar results at Sowers.
1095680	1100080	And the second paper by Zang Fre and Bartlett in 2023.
1100080	1103760	On top of characterizing global optimality, they show that if you run gradient descent
1103760	1109960	on the linear transformer with some specific initialization or some specific conditions
1109960	1114840	on initialization, you'll always converge to this.
1114840	1122360	So at least, you know, it's a good region of attraction based off of these results.
1122360	1126760	But I'm not sure if it's unique.
1126760	1130440	So that's for one-layer transformer implementing one-stop gradient descent.
1130440	1135480	In practice, you know, transformers work better when there are lots of players and gradient
1135480	1137640	descent works better when there are lots of steps.
1137640	1142880	So then a natural question is, can we extend the similar results to a L-layer linear transformer
1143000	1145800	for some arbitrary integer L?
1145800	1151440	So on the left, again, I show a L-layer linear transformer, same in context learning loss,
1151440	1156520	but this time the predictor is after L-layers.
1156520	1162120	And in the middle, I show L-steps of gradient descent, again, with respect to the same in
1162120	1166680	percolate scores.
1166680	1169680	And here, we establish a weaker guarantee.
1169680	1174880	So instead of saying that gradient descent is a global optimum, we only show that there
1174880	1183080	exists transformers, which are stationary points of the in-contact loss, such that at
1183080	1188880	every layer, the transformer gives the same prediction as L-steps of gradient descent
1188880	1189880	on R.
1189880	1196880	So in other words, TF2 would correspond to the prediction, TF1 corresponds to the prediction
1197080	1202000	for each L.
1202000	1209000	And that's kind of interesting because really the only thing you're training on is TF capital
1209480	1210480	L.
1210480	1217480	And so it's interesting that all these intermediates outputs have a interpretable connection to
1217560	1220560	gradient descent.
1220560	1227560	I have a question trying to parse this.
1227880	1234380	There exist transformers, I'm trying to figure out the quantification.
1234380	1241380	There exist transformers that for a random choice of these parameters, what's the quantification
1243080	1248080	on the X and the theta?
1249080	1254080	Are you saying at the very beginning of the theorem, very beginning of theorem 3?
1254080	1256880	Yes, X and theta are here, expectation.
1256880	1262320	So I define a loss on transformer key value query matrices, which is only a function of
1262320	1269320	W that is expectation over Z, which I guess X, Y, and theta of the prediction minus the
1269960	1271440	true label.
1271440	1277880	And there are stationary points of this loss, F of W, such that the stationary point is
1277880	1283200	some specific choice of key value query matrices, so if it's non-identity, but sigma.
1283200	1289640	And here we assume theta star is from sigma inverse, and there exist stationary points,
1289640	1292640	which coincide with preconditions.
1292640	1297640	Where the preconditioner is sigma inverse.
1297640	1303640	Are these local minima, can you construct one which is also a local minimum?
1303960	1310720	I'm not sure, but we tried, but we couldn't show it, which is why we only show stationary
1310720	1311720	points.
1311720	1317720	Okay, let me show you one more slide, so experimentally, so we only show that there exist stationary
1321440	1327960	point, but experimentally, surprisingly, we always recover the same key value matrices.
1327960	1332960	So specifically, a transformer implements precondition gradient descent by sigma inverse
1332960	1337080	if the product of key query matrices is sigma inverse.
1337080	1342080	And so here I train a three-layer transformer, and I display sigma half, query times key
1342080	1348080	is sigma half, and we see that each of these cases is speculative, pretty much.
1348080	1352160	So in other words, it's always learning to implement precondition.
1352160	1357800	So I think that we don't have a theory for it, but I think there is, you know, we conjecture
1357880	1363880	that maybe these are, in fact, locally or even globally.
1363880	1369120	So it's something worth thinking about.
1369120	1375080	Before I end, I will also mention that I skimmed a bit of detail on characterizing this theorem.
1375080	1379960	We actually need to assume certain sparsities, specifically the last row and column of the
1379960	1385440	key query matrices are at zero, and there are actually two variants of this theorem,
1385440	1389040	depending on what kind of constraints we impose on the value matrix.
1389040	1393280	You could implement precondition gradient descent or something more clever than precondition
1393280	1398040	gradient descent, which interleaves gradient steps with rotation of the gram matrix to
1398040	1399760	make things better conditioned.
1399760	1403520	You can see the details of all this in the arm.
1403520	1408840	So that's all for the first part of the talk, and I'm almost out of time, but maybe I'll
1408840	1414280	quickly talk about the second part, which is how linear transformer loss lens gave first
1414280	1419760	a number of remarkable similarities to the loss landscape of full transformation.
1419760	1424240	So again, transformers are large and complicated, and it's difficult to pinpoint why algorithm
1424240	1428000	works or it doesn't work, and it's difficult to theoretically analyze the behavior of optimization
1428000	1429000	algorithms.
1429000	1431600	Also, it's very expensive to experiment on full transformers.
1431600	1435920	On the other hand, linear transformers may be a useful model to understand transformer
1435920	1436920	optimization.
1436920	1442400	So we surveyed a number of recent papers, which look at the transformers, the optimization
1442400	1448080	landscape of full transformers, and we identify several remarkable features, which are kind
1448080	1452400	of unique to transform optimization, and we observe that shallow linear transformers
1452400	1457200	on the linear regression problem has similar optimization features.
1457200	1462080	So one example is that Adam is significantly faster than stochastic gradient descent for
1462080	1464840	a transformer training.
1464840	1469760	On the left, and this is a phenomenon that is kind of unique to large-language models
1469760	1470760	and transformers.
1470880	1475320	On the left, this plot is taken from Quintetian, I think that means 23.
1475320	1481600	On the left, the two plots, we show training a CNN, or MNIST, and Cpartan, so it's a image
1481600	1482600	test.
1482600	1488600	And there is no obvious gap between SGD and L, but on the right, they show three transformers
1488600	1492480	on different datasets, and there's a clear gap between Adam and SGD.
1492480	1498840	Similar observations were also made in a number of other recent papers, and we show here on
1498880	1504040	the left, you know, the same plot features from the previous slide for the three kinds
1504040	1506160	of language tests.
1506160	1510320	Here on the right, I show a three-layer linear transformer trained on linear regression,
1510320	1516320	and we see that similarly there is a significant gap between Adam and SGD, and the three plots
1516320	1520600	coincide with slightly different settings of the kind.
1520600	1526480	And I'm already over time, so maybe I'll skip over the rest of the features, but the long
1526520	1531160	story short, there's a number of features which are kind of unique to transform optimization,
1531160	1535240	and people conjecture that's maybe why adaptive algorithms are so important when training large
1535240	1541000	language models, and so we went through each of them and we checked if you get the same
1541000	1546960	kind of plots or data that you get from training a simple linear transformer, and each of this
1546960	1551160	case, there is a surprising agreement with what people opt for for four transformers,
1551840	1556840	so with that, I will end the talk.
1570840	1576360	Any more questions for Chim?
1576360	1581160	So I've heard that actual large language model training is like unstable, and if you look
1581160	1585000	at the training pause, they're a place where you get like these spikes.
1585000	1590000	Are those instabilities also replicated in the smaller transformers that you consider?
1590000	1594000	Yeah, they are.
1594000	1601000	In fact, we have a gap for why some of the instability is happening, and it goes back
1601160	1606160	to the fact that the transformer appears to be learning to implement this gradient descent
1606160	1613160	algorithm, and the thing with gradient descent is that the closer the larger your step size
1613160	1617600	is, the better you do until the point where you exceed the lift shift constant, then things
1617600	1624600	blow up very quickly, so we also observed that as your loss gets lower and your learning
1625120	1630920	rate per layer is getting closer to the boundary, it's become more unstable because if you just
1630920	1636840	exceed that a little bit, your transformer, the kind of optimization algorithm that's
1636840	1639600	implemented by a transformer, like hybridism.
1639600	1646600	So that's one example, but yeah, we do offer similar problems.
1653920	1660840	So you showed the linear transform of one layer is equivalent to gradient, empirically,
1660840	1662400	one is the square one.
1662400	1664280	Yes, one step of gradient.
1664280	1670280	For all layers, they actually implement the preconditioned gradient.
1670280	1673280	Yeah, so for both, let me go back.
1678280	1685280	So for one layer, we showed that if the covariance is sigma, it also implements one step of precondition.
1685400	1692400	And then for our layer, if that covariance is identity, it just does L steps of standard
1699760	1706760	gradient descent, but again, if the covariance is sigma, then it does L steps of preconditioned.
1707680	1709600	So it's kind of like two orthogonal.
1709600	1714040	So the linear transform actually is nonlinear in Z, right?
1714040	1721040	So that means this nonlinear minimization actually automatically implement precondition.
1721040	1722440	Yes, yes.
1722440	1729440	Okay, so that's a different way to think about these algorithms and then they adaptively automatically
1729680	1731800	choose what precondition it is.
1731800	1732800	Yes, exactly.
1732800	1733800	That's right.
1733800	1734800	Okay.
1734800	1735800	Interesting.
1736800	1741800	Just one question.
1741800	1747800	You mentioned if you add softmax in this linear regression task, it's actually going to underperform
1747800	1749800	compared to the linear model, right?
1749800	1750800	Yeah.
1750800	1751800	I didn't understand.
1751800	1752800	What was the reason?
1752800	1753800	This is kind of...
1753800	1755800	So did you try like chatGPT2 model or...?
1755800	1760800	No, we code up a softmax with the same number of parameters.
1760800	1765800	We coded up some linear transformer and take a softmax to the place where we just pick
1765800	1767800	it to make a softmax.
1767800	1769800	So it's not like a giant model?
1769800	1770800	I see.
1770800	1773800	So without any residual connections, right?
1773800	1774800	Oh, with residuals.
1774800	1779800	So basically these two are almost exactly the same except, you know, we have a softmax
1779800	1782800	here and we don't.
1782800	1786800	So both look like this.
1786800	1791800	Except different, slight difference in how ATTN is defined.
1791800	1794800	What was the intuition that white softmax is here?
1794800	1795800	Yeah.
1795800	1796800	That's a very good question.
1796800	1798800	So here's an example, right?
1798800	1805800	You know, if I'm trying to predict y and I have some x and in my demonstrations I have
1805800	1807800	minus x, right?
1807800	1811800	Then that should be very informative to predicting x.
1811800	1813800	If you know it's linear.
1813800	1820800	Softmax would compute the, you know, product which is an active number and that becomes
1820800	1826800	that then you can improve our little weight on that sample and so that sample became useful
1826800	1827800	for prediction.
1827800	1828800	And that's just one example.
1828800	1835800	I guess overall it's just that, you know, based on the construction which I didn't show,
1835800	1839800	the linear transformer very easily implements a gradient descent step.
1839800	1842800	The linear transformer not so much because softmax sticks out.
1842800	1849800	It does this weird reweighting of your demonstration samples which doesn't really help in the linear
1849800	1850800	regression setting.
1850800	1857800	So, but probably softmax transformer works more generally if it's not linear.
1857800	1860800	But is there anything like the optimal algorithm for this problem?
1860800	1862800	That's a good question.
1862800	1864800	I don't know.
1864800	1871800	But I guess, you know, whatever algorithm that softmax is doing, it's just not very nice.
1871800	1881800	Thank you.
1881800	1882800	Thank you.
1882800	1883800	I had a clarification question.
1883800	1888800	So you mentioned that there's a different theta star for each input prompt.
1888800	1893800	Does your results also depend on how many prompts or demonstrations are provided as
1893800	1896800	part of in-context learning?
1896800	1902800	Good question.
1902800	1908800	Yes, because, so here n is the length of the prompt.
1908800	1909800	Okay.
1909800	1915800	And how much regularization you put here depends on n.
1915800	1923800	And I guess what this affects is how, I guess, what preconditioner you use exactly.
1923800	1926800	So this is for the non-identity covariance case.
1926800	1931800	And even for the identity covariance case, the exact step size, I think, would be affected
1931800	1936800	depending on, you know, this delta one, which I didn't talk about at all.
1936800	1943800	I think that's going to depend very importantly on how large n is, larger n, probably larger
1943800	1944800	delta.
1944800	1945800	That makes sense, right?
1945800	1949800	Because if you think about it, gradient descent involves this gram matrix.
1949800	1954800	And if the gram matrix is identity, one step of gradient descent will just give you the
1954800	1955800	solution.
1955800	1963800	And when n is very large, the gram matrix does approach identity, whereas when it's small,
1963800	1965800	the gram matrix could be your condition.
1965800	1970800	And how your condition, gram matrix, is related to the condition number of our theta.
1970800	1977800	So it makes sense that for a smaller n, you can take smaller steps of gradient descent
1977800	1980800	and order n.
1980800	1982800	All right.
1982800	1989800	So let's thank Yashiyang again.
