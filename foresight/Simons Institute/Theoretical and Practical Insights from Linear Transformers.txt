Okay, so I'll just stop here, thanks for coming.
I'm Xiang and today I'll be talking about theoretical and practical insights from linear
transformers.
As you know, large language models work surprisingly well in practice and the basis of large language
models is the transformer neural network.
So an important question is why do transformers work and how can we train them effectively?
But directly this question is difficult to answer because large language models have billions,
maybe even trillions of parameters, they have a lot of moving parts, different choices
of normalization, of embedding and they contain many different kinds of modules.
So it is important to have a mathematical abstraction that captures the essence of transformer
learning and optimization in order to better understand transformers.
And today we are going to look at how the simple linear transformer can shed light on
two important questions.
One is the mechanism behind a phenomenon known as in-context learning and two, I'll tell
you about how linear transformer optimization shares many of the unconventional features
of real transformer optimization.
And these two points are based on two papers which are in joint work with Kwong Joon, Ha-Dee,
Subritz, Ali from MIT as well as Minghak, Charlie from KAIST.
So part one will be understanding in-context learning.
So what is in-context learning?
A very standard task of large language models is that of next-word prediction.
For example, you can give GPT a prompt, Mary has a little blank and it tells you Mary has
a little lamb.
But that's not so surprising because somewhere in this training data I probably saw this
exact sentence, maybe thousands of times.
So then in-context learning refers to the following kind of prompting.
So I'll provide GPT with a few demonstrations.
I'll say apple is red, banana is yellow, then what is a grape?
And then so apple, red and banana yellow serve as contextual examples.
And based off of these contextual examples, GPT infers that I'm looking for the color
of the fruit so it feels in purple.
And this phenomenon even works for arbitrary made-up rules.
So this is a bunch of gibberish but the ad symbol denotes concatenation and it correctly
infers that ad denotes concatenation.
So it probably hasn't seen this exact example anywhere in this training set but it still
does the right thing.
I would say that I tried some very complicated gibberish and it doesn't work there.
So that's a good thing, I guess, because otherwise we are all out of jobs soon.
And so to the best of my knowledge, this in-context learning phenomenon was first reported in
a seminal paper by Brown, Mann, Ryder, Soubia and collaborators.
And this paper was also the one that coined the phrase in-context learning, I think.
I would also say that from a machine learning point of view, if you give your model a few
demonstrations and then it does well on those demonstrations, it's not terribly surprising.
But the thing is here in-context learning works without any updates to the model parameters.
So I'm not doing any fine-tuning, I'm just using the same transformer and then it does
the right thing.
So that's, I guess, the surprising thing.
Some people even go as far as claim that it's one of the main reasons for why large language
models work so well in practice.
But regardless, understanding how large models do in-context learning is a very important
question to us understanding large language models.
So in recent years, there's a few very important papers that try to shed light on this phenomenon.
Our work is based on a few of these papers, so I'll do a quick review of the relevant
literature now.
The first such paper is the one by Garak Sipras-Leon Valiant, 2022.
And the title is, What Can Transformers Learn In-Context, A Case Study of Simple Function
Classes.
So we call the example about fruits and colors and the hidden rule is that y is the color
of x, where x is some fruit and y is some color.
But it's very hard to reason about what does it mean, what kind of function is represented
by the question about color in, say, the embedding space of words.
So what they propose to do is consider a simplified setup where your x are Euclidean
vectors and the y's are linearly related to x by some unobserved data.
So it's a linear regression problem.
Pictorially, you're given a bunch of demonstrations, the black dots, x, y pairs.
And then the question is in the form of the red dot, xn plus 1.
And you're trying to figure out what the label y is.
The revolution is a bit low.
OK.
So a follow-up paper by Akirak, Sherman, Andreas, Ma, and Zoe in 2023.
They further try to characterize what kind of, or they try to characterize how transformers
are able to learn these functions such as linear functions.
And the main takeaway here is that transformers can learn in context because they are able
to implement various learning algorithms.
For example, this plot here shows that experimentally, transformers appear to implement the ordinary
least squares algorithm on noiseless data.
So the green line is like zero throughout.
And so the transform prediction has a very good agreement with ordinary least squares.
And the way they show that transformers are able to implement this learning algorithm
is approved by construction.
So they have this very clever construction, where they define a few algorithmic primitives
like multiplication, division, affine transformation, and they show that the attention unit can
implement this primitive.
So by hooking together various attention units, you're able to implement algorithms
such as iOS.
And I'll mention here that further along this direction, there are also more extreme examples
where people show that the attention architecture can implement some kind of register system
or something.
So you can implement arbitrary algorithms and transformers can be called as programmable
computers.
The catch is that all these constructions are very clever, but the downside is that also
means they are very fragile.
And it's unclear whether these very clever constructions are actually recovered when
you, say, train your transformers with the atom algorithm.
So the next in line are two papers.
Linear transformers are circularly fast-waist programs by Schlag, Erich, Mied, Huber in
2010.
And a closely related paper by Oswald, Nikolson, Luan, Dazzo, Sacramento, Moff, and Blak Mirov
in 2010.
So I'll mainly focus on the second paper, Transformers, Learning Contacts by Gradient
Descent.
And so here, as well, they do approve by construction.
But there is a very important difference from previous papers, which is they consider the
linear transformers, which is, so all previous papers, they may consider a simplified setting
where the problem is linear regression, but the architecture was always full transformers.
But here, it's the first time people look at linear transformers, which I'll define
a bit.
It's simpler than full transformers.
And because they look at linear transformers, they are able to provide a very simple construction
where, under which linear transformers are able to implement gradient descent, which
in turn allows them to learn linear functions in context.
And remarkably, they have some pretty convincing experiments which agree with their construction.
So that's for prior work.
And now I come to our paper, where we try to answer the following question.
So we saw that transformers are expressive enough to implement a whole bunch of algorithms,
but can we show that transformers actually learn to implement any of these algorithms
during training?
So let me set up the problem.
When you have a transformer, the input is of the form of a matrix, d by n, where you
can think of it as a horizontally stacked bunch of tokens.
Each token is kind of like a word in a sentence.
So your sentence will get, you know, embedded in the impedance space and turn into a bunch
of factors.
So if you want to see it.
The standard self-attention module is the following function.
So you have your key value query matrix.
You have a mask for this causality, and then you put a softmax on this thing.
And linear self-attention is basically the same thing except we take out the softmax.
And that's why it's called linear because we took out the non-linearity.
And by the same time, I would say that the phrase linear attention is maybe a bit of
a misnomer because the linear attention module is not linear in either the parameter PQK
or in its input Z.
In fact, it's a third or the polynomial of Z.
Because of this, the representation power, when you stack a bunch of these linear attention
modules on top of each other, it increases.
So this is in contrast to something like a linear fully connected neural network.
So no matter how many of these you stack, you always are a linear function of an input.
But you can actually represent increasingly high-degree polynomials by stacking linear
attention units.
And the linear transformer, which we'll look at, is basically stacking these attention
units by a residual connection.
And to be precise, this is a single-headed transformer.
So that's defining the linear transformer architecture.
And now let me properly set up the learning objective.
So as mentioned earlier, the input to a transformer is a sequence of tokens.
And in the linear regression setting, each token Z consists of an x, y pair, where x
is a d-dimensional Euclidean vector, y is a scalar, and x, y are related by this linear
relationship.
And theta star is unobserved.
And on top of that, each prompt has a different theta star.
The goal, you are also given a xm plus 1, but without the label ym plus 1.
And the goal is to train the transformer to predict the hidden label.
Given the demonstrations, as well as xm plus 1.
I was stressed that this problem is much harder than simply learning a single theta
star, because theta star changes from prompt to prompt.
So you need to learn an algorithm that, given a few demonstrations, infers the right theta
star regardless of what the theta star is.
And at this point, I would also mention that one of the reasons for choosing to focus on
linear attention as opposed to the softmax attention, besides the fact that linear attention
is simpler and easier to understand, is that for this problem of learning a linear function,
linear transformers perform much better than softmax transformers.
And I guess we'll see concretely why that is in a bit.
But even now, just intuitively, if your data are linearly related, then it makes sense
that softmax doesn't really help you with all that much.
So here's the first result I'll talk about.
We study one layer, linear transformer, and we claim that it implements one step of gradient
descent at global minimum.
So what does it mean for a transformer to implement one step of gradient descent?
On the left here, in this box, I show the architecture of a one-layer linear transformer.
It's very simple.
If you have a z, it passes through a single attention layer, and then, you know, we get
some output tfz, where tf subscript lz, denotes the transformer's prediction at layer l,
given input z, and parameter w.
So parameter w is like the correct key value matrices.
And we try to minimize the in-contact loss, which is the expected difference between the
prediction and the true label.
And the expectation is taken over both z, which is the input, as well as theta star,
which is the unobserved linear relationship.
And if you forget about transformers, we're a bit a very reasonable thing to try to do
when given a bunch of demonstrations, and you need to infer the correct label, is to
maintain a theta and then run gradient descent on it, with respect to the empirical least
grass loss, which I highlight in problem.
And so here, I just take one step, a single step, of gradient descent, and if n is sufficiently
large, you know, you'll probably do decently.
So theorem one of our paper states the following.
If you assume that the covariates are sampled from the standard normal distribution and theta
star is also sampled from the standard normal distribution, then the linear transformer
that minimizes the in-contact loss, fw, which is in red, gives the same prediction as the
one step gradient descent on r theta, which I highlight in purple.
So in other words, the output, tf1 of v comma w, is the same as if you ran one step of gradient
descent on theta, and use that to predict the label.
And in fact, you can consider a more general setting when your covariates are sampled from
some distribution with a non-identity covariate.
So when your covariance is sigma here, the linear transformer that globally minimizes
the loss now coincides with running one step of precondition gradient descent, where the
preconditioner a is given by following.
For when n is very large, which is when you're given a ton of demonstrations of, you know,
xy pairs, a in the limit is just inverse of your covariance matrix, the covariance matrix
of your covariates.
But when n is small, there's this additional regularization.
Is it obvious that the global minimum is unique, and if not, is this a statement about any
global minimum?
Yeah, so this is a statement about any global minimum.
In fact, there is some obvious, you know, no spaces in the loss, I guess, because one
example is the query times key matrix.
You can scale them arbitrarily if their product is the same, then that's the same.
Another example is, you know, since this is a linear transformer, scaling the value key
current matrices arbitrarily as long as they multiply the same thing also gives the exact
same predictor.
But then one might wonder, you know, ignoring these inferences, is that unique?
I'm not sure.
I'm not sure.
In perfectly, in all our experiments, this is always recovered.
So that's a good sign.
And also, as I'm just about to mention, there are two concurrent works, which appear surely
after we publish the initial draft.
One of them characterizes global optimality for one-layer linear transformer on the similar
sign.
So similar results at Sowers.
And the second paper by Zang Fre and Bartlett in 2023.
On top of characterizing global optimality, they show that if you run gradient descent
on the linear transformer with some specific initialization or some specific conditions
on initialization, you'll always converge to this.
So at least, you know, it's a good region of attraction based off of these results.
But I'm not sure if it's unique.
So that's for one-layer transformer implementing one-stop gradient descent.
In practice, you know, transformers work better when there are lots of players and gradient
descent works better when there are lots of steps.
So then a natural question is, can we extend the similar results to a L-layer linear transformer
for some arbitrary integer L?
So on the left, again, I show a L-layer linear transformer, same in context learning loss,
but this time the predictor is after L-layers.
And in the middle, I show L-steps of gradient descent, again, with respect to the same in
percolate scores.
And here, we establish a weaker guarantee.
So instead of saying that gradient descent is a global optimum, we only show that there
exists transformers, which are stationary points of the in-contact loss, such that at
every layer, the transformer gives the same prediction as L-steps of gradient descent
on R.
So in other words, TF2 would correspond to the prediction, TF1 corresponds to the prediction
for each L.
And that's kind of interesting because really the only thing you're training on is TF capital
L.
And so it's interesting that all these intermediates outputs have a interpretable connection to
gradient descent.
I have a question trying to parse this.
There exist transformers, I'm trying to figure out the quantification.
There exist transformers that for a random choice of these parameters, what's the quantification
on the X and the theta?
Are you saying at the very beginning of the theorem, very beginning of theorem 3?
Yes, X and theta are here, expectation.
So I define a loss on transformer key value query matrices, which is only a function of
W that is expectation over Z, which I guess X, Y, and theta of the prediction minus the
true label.
And there are stationary points of this loss, F of W, such that the stationary point is
some specific choice of key value query matrices, so if it's non-identity, but sigma.
And here we assume theta star is from sigma inverse, and there exist stationary points,
which coincide with preconditions.
Where the preconditioner is sigma inverse.
Are these local minima, can you construct one which is also a local minimum?
I'm not sure, but we tried, but we couldn't show it, which is why we only show stationary
points.
Okay, let me show you one more slide, so experimentally, so we only show that there exist stationary
point, but experimentally, surprisingly, we always recover the same key value matrices.
So specifically, a transformer implements precondition gradient descent by sigma inverse
if the product of key query matrices is sigma inverse.
And so here I train a three-layer transformer, and I display sigma half, query times key
is sigma half, and we see that each of these cases is speculative, pretty much.
So in other words, it's always learning to implement precondition.
So I think that we don't have a theory for it, but I think there is, you know, we conjecture
that maybe these are, in fact, locally or even globally.
So it's something worth thinking about.
Before I end, I will also mention that I skimmed a bit of detail on characterizing this theorem.
We actually need to assume certain sparsities, specifically the last row and column of the
key query matrices are at zero, and there are actually two variants of this theorem,
depending on what kind of constraints we impose on the value matrix.
You could implement precondition gradient descent or something more clever than precondition
gradient descent, which interleaves gradient steps with rotation of the gram matrix to
make things better conditioned.
You can see the details of all this in the arm.
So that's all for the first part of the talk, and I'm almost out of time, but maybe I'll
quickly talk about the second part, which is how linear transformer loss lens gave first
a number of remarkable similarities to the loss landscape of full transformation.
So again, transformers are large and complicated, and it's difficult to pinpoint why algorithm
works or it doesn't work, and it's difficult to theoretically analyze the behavior of optimization
algorithms.
Also, it's very expensive to experiment on full transformers.
On the other hand, linear transformers may be a useful model to understand transformer
optimization.
So we surveyed a number of recent papers, which look at the transformers, the optimization
landscape of full transformers, and we identify several remarkable features, which are kind
of unique to transform optimization, and we observe that shallow linear transformers
on the linear regression problem has similar optimization features.
So one example is that Adam is significantly faster than stochastic gradient descent for
a transformer training.
On the left, and this is a phenomenon that is kind of unique to large-language models
and transformers.
On the left, this plot is taken from Quintetian, I think that means 23.
On the left, the two plots, we show training a CNN, or MNIST, and Cpartan, so it's a image
test.
And there is no obvious gap between SGD and L, but on the right, they show three transformers
on different datasets, and there's a clear gap between Adam and SGD.
Similar observations were also made in a number of other recent papers, and we show here on
the left, you know, the same plot features from the previous slide for the three kinds
of language tests.
Here on the right, I show a three-layer linear transformer trained on linear regression,
and we see that similarly there is a significant gap between Adam and SGD, and the three plots
coincide with slightly different settings of the kind.
And I'm already over time, so maybe I'll skip over the rest of the features, but the long
story short, there's a number of features which are kind of unique to transform optimization,
and people conjecture that's maybe why adaptive algorithms are so important when training large
language models, and so we went through each of them and we checked if you get the same
kind of plots or data that you get from training a simple linear transformer, and each of this
case, there is a surprising agreement with what people opt for for four transformers,
so with that, I will end the talk.
Any more questions for Chim?
So I've heard that actual large language model training is like unstable, and if you look
at the training pause, they're a place where you get like these spikes.
Are those instabilities also replicated in the smaller transformers that you consider?
Yeah, they are.
In fact, we have a gap for why some of the instability is happening, and it goes back
to the fact that the transformer appears to be learning to implement this gradient descent
algorithm, and the thing with gradient descent is that the closer the larger your step size
is, the better you do until the point where you exceed the lift shift constant, then things
blow up very quickly, so we also observed that as your loss gets lower and your learning
rate per layer is getting closer to the boundary, it's become more unstable because if you just
exceed that a little bit, your transformer, the kind of optimization algorithm that's
implemented by a transformer, like hybridism.
So that's one example, but yeah, we do offer similar problems.
So you showed the linear transform of one layer is equivalent to gradient, empirically,
one is the square one.
Yes, one step of gradient.
For all layers, they actually implement the preconditioned gradient.
Yeah, so for both, let me go back.
So for one layer, we showed that if the covariance is sigma, it also implements one step of precondition.
And then for our layer, if that covariance is identity, it just does L steps of standard
gradient descent, but again, if the covariance is sigma, then it does L steps of preconditioned.
So it's kind of like two orthogonal.
So the linear transform actually is nonlinear in Z, right?
So that means this nonlinear minimization actually automatically implement precondition.
Yes, yes.
Okay, so that's a different way to think about these algorithms and then they adaptively automatically
choose what precondition it is.
Yes, exactly.
That's right.
Okay.
Interesting.
Just one question.
You mentioned if you add softmax in this linear regression task, it's actually going to underperform
compared to the linear model, right?
Yeah.
I didn't understand.
What was the reason?
This is kind of...
So did you try like chatGPT2 model or...?
No, we code up a softmax with the same number of parameters.
We coded up some linear transformer and take a softmax to the place where we just pick
it to make a softmax.
So it's not like a giant model?
I see.
So without any residual connections, right?
Oh, with residuals.
So basically these two are almost exactly the same except, you know, we have a softmax
here and we don't.
So both look like this.
Except different, slight difference in how ATTN is defined.
What was the intuition that white softmax is here?
Yeah.
That's a very good question.
So here's an example, right?
You know, if I'm trying to predict y and I have some x and in my demonstrations I have
minus x, right?
Then that should be very informative to predicting x.
If you know it's linear.
Softmax would compute the, you know, product which is an active number and that becomes
that then you can improve our little weight on that sample and so that sample became useful
for prediction.
And that's just one example.
I guess overall it's just that, you know, based on the construction which I didn't show,
the linear transformer very easily implements a gradient descent step.
The linear transformer not so much because softmax sticks out.
It does this weird reweighting of your demonstration samples which doesn't really help in the linear
regression setting.
So, but probably softmax transformer works more generally if it's not linear.
But is there anything like the optimal algorithm for this problem?
That's a good question.
I don't know.
But I guess, you know, whatever algorithm that softmax is doing, it's just not very nice.
Thank you.
Thank you.
I had a clarification question.
So you mentioned that there's a different theta star for each input prompt.
Does your results also depend on how many prompts or demonstrations are provided as
part of in-context learning?
Good question.
Yes, because, so here n is the length of the prompt.
Okay.
And how much regularization you put here depends on n.
And I guess what this affects is how, I guess, what preconditioner you use exactly.
So this is for the non-identity covariance case.
And even for the identity covariance case, the exact step size, I think, would be affected
depending on, you know, this delta one, which I didn't talk about at all.
I think that's going to depend very importantly on how large n is, larger n, probably larger
delta.
That makes sense, right?
Because if you think about it, gradient descent involves this gram matrix.
And if the gram matrix is identity, one step of gradient descent will just give you the
solution.
And when n is very large, the gram matrix does approach identity, whereas when it's small,
the gram matrix could be your condition.
And how your condition, gram matrix, is related to the condition number of our theta.
So it makes sense that for a smaller n, you can take smaller steps of gradient descent
and order n.
All right.
So let's thank Yashiyang again.
