1
00:00:00,000 --> 00:00:03,360
pro six circuits on tiny binary assets.

2
00:00:03,360 --> 00:00:06,880
So what we'll do today is actually train PCs

3
00:00:06,880 --> 00:00:09,120
on language, language models,

4
00:00:09,120 --> 00:00:11,400
and Juan-Wes going to tell us how that works

5
00:00:11,400 --> 00:00:12,800
and what that's useful for.

6
00:00:15,160 --> 00:00:18,800
So today I'll present our paper,

7
00:00:18,800 --> 00:00:22,200
Attractable Control for Autoregressive Language Generation.

8
00:00:23,480 --> 00:00:28,480
So unlike probably the interesting ones,

9
00:00:28,640 --> 00:00:32,040
today I'll focus more on the application side

10
00:00:32,040 --> 00:00:34,080
of probabilistic circuits.

11
00:00:34,080 --> 00:00:36,680
Like more machine learning oriented.

12
00:00:38,480 --> 00:00:43,480
Okay, so let's get started with the basic concept

13
00:00:43,840 --> 00:00:45,560
of large language models.

14
00:00:46,960 --> 00:00:49,520
They have been like really popular recently.

15
00:00:49,520 --> 00:00:52,760
I'm not sure if people here really care or not.

16
00:00:53,760 --> 00:00:58,760
Like I see like this interested face you do.

17
00:01:00,280 --> 00:01:02,000
Okay, that's a good sign.

18
00:01:02,000 --> 00:01:03,680
That's a good sign.

19
00:01:03,680 --> 00:01:07,800
So basically the idea is very simple.

20
00:01:07,800 --> 00:01:11,320
In the sense that you collect large amount of text

21
00:01:11,320 --> 00:01:13,760
consisting of trillions of words,

22
00:01:13,760 --> 00:01:17,400
and you train neural networks with billions of parameters

23
00:01:17,400 --> 00:01:21,400
on these data, then you have these so-called large

24
00:01:21,400 --> 00:01:22,520
language models.

25
00:01:24,880 --> 00:01:29,880
These chat GPT or GPT-4 thing has become like really popular.

26
00:01:32,600 --> 00:01:34,440
People talking to them on the internet,

27
00:01:34,440 --> 00:01:38,200
asking them to polish their papers,

28
00:01:38,200 --> 00:01:40,720
using them to do their homeworks,

29
00:01:40,720 --> 00:01:45,720
and even asking them to play D&D together.

30
00:01:47,520 --> 00:01:50,320
So yeah, that is actually true.

31
00:01:51,320 --> 00:01:56,320
So the idea is, so people have now having this feeling

32
00:01:57,280 --> 00:01:59,840
that we probably have solved AI,

33
00:01:59,840 --> 00:02:03,560
we have solved artificial intelligence,

34
00:02:05,680 --> 00:02:07,640
and chat GPT has passed Turing test,

35
00:02:07,640 --> 00:02:10,840
but is that really the case, okay?

36
00:02:10,840 --> 00:02:15,840
So I tried this a few months ago.

37
00:02:16,840 --> 00:02:21,840
It was an actual conversation between me and chat GPT.

38
00:02:21,920 --> 00:02:25,600
So I asked the model to generate a sentence

39
00:02:25,600 --> 00:02:29,560
using Frisbee, Cod, and Dog following the given order.

40
00:02:29,560 --> 00:02:32,280
Well, I mean, this should be quite simple

41
00:02:32,280 --> 00:02:36,400
for a super intelligent AI model, right?

42
00:02:36,400 --> 00:02:39,760
So it does give me a sentence.

43
00:02:39,760 --> 00:02:42,840
The sentence look quite decent,

44
00:02:43,840 --> 00:02:46,080
and all the key words are in there,

45
00:02:46,080 --> 00:02:50,960
but Dog and Cod are in the wrong order, right?

46
00:02:50,960 --> 00:02:53,240
Well, I mean, humans make mistakes,

47
00:02:53,240 --> 00:02:55,120
so super intelligent AI does.

48
00:02:56,120 --> 00:03:00,120
So I was being patient, I tried again, okay?

49
00:03:00,120 --> 00:03:01,920
Again, it gives me a sentence,

50
00:03:01,920 --> 00:03:04,440
but it's even worse now.

51
00:03:04,440 --> 00:03:06,480
All of the words are in the wrong order.

52
00:03:06,480 --> 00:03:11,480
So you see that chat GPT fails to follow

53
00:03:11,760 --> 00:03:15,040
even this simple logical constraint, okay?

54
00:03:15,040 --> 00:03:17,680
So people have been trying to fix this.

55
00:03:17,680 --> 00:03:20,240
Some of the methods, including like,

56
00:03:20,240 --> 00:03:22,320
okay, we can prompt them in a different way,

57
00:03:22,320 --> 00:03:25,100
like chain of thought.

58
00:03:26,160 --> 00:03:28,200
I'm not comfortable saying this phrase,

59
00:03:28,200 --> 00:03:30,440
but that's one of the methods.

60
00:03:31,680 --> 00:03:34,400
Or people try to use search algorithms

61
00:03:34,400 --> 00:03:36,400
trying to find the correct sentence

62
00:03:36,400 --> 00:03:39,880
in this huge search space, so on and so forth.

63
00:03:39,880 --> 00:03:43,680
But none of them actually guarantees that chat GPT

64
00:03:43,680 --> 00:03:47,720
or whatever other language model give us what we want.

65
00:03:47,720 --> 00:03:52,440
So what we actually want is like 100% guarantee.

66
00:03:52,440 --> 00:03:55,400
When we have like a huge powerful model,

67
00:03:55,400 --> 00:03:57,360
when we instructed to do something,

68
00:03:57,360 --> 00:04:01,480
we wanted to follow our instructions exactly.

69
00:04:01,480 --> 00:04:05,160
So in today's talk, I'll show you how we can do this

70
00:04:05,160 --> 00:04:07,240
with probabilistic circuits.

71
00:04:08,240 --> 00:04:11,320
Before some of the detail,

72
00:04:11,320 --> 00:04:14,840
so let's get started with some basics of language modeling.

73
00:04:14,840 --> 00:04:17,720
So language modeling is really not like a fancy idea.

74
00:04:17,720 --> 00:04:21,280
It's just a joined distribution over some text,

75
00:04:22,400 --> 00:04:25,480
probably less than or equal to n words.

76
00:04:27,400 --> 00:04:32,400
Each random variable here is a word taking value

77
00:04:33,320 --> 00:04:38,320
in a fixed, in a vocabulary of finite size.

78
00:04:40,120 --> 00:04:44,520
The size usually in practice ranges from 10,000 to 50,000

79
00:04:44,520 --> 00:04:46,200
or something, okay?

80
00:04:47,280 --> 00:04:50,000
And here's some examples.

81
00:04:50,000 --> 00:04:53,920
You might notice there's some special word called EOS here,

82
00:04:53,920 --> 00:04:55,120
which means end of sentence,

83
00:04:55,120 --> 00:04:58,000
which is only like a special token or word

84
00:04:58,000 --> 00:05:00,840
we use to pad sentences of different length

85
00:05:00,840 --> 00:05:01,920
to the maximum length.

86
00:05:01,920 --> 00:05:06,120
So we have this like joined distribution well defined, okay?

87
00:05:06,120 --> 00:05:10,000
So we collect a lot of text of this form,

88
00:05:10,000 --> 00:05:13,200
sentence fragments, paragraph fragments,

89
00:05:13,200 --> 00:05:16,000
and we train this joined distribution

90
00:05:16,000 --> 00:05:19,800
by maximizing a lot of likelihood, a very, very simple idea.

91
00:05:20,640 --> 00:05:22,200
Okay, so what about the architecture

92
00:05:22,200 --> 00:05:24,600
of this distribution of our model?

93
00:05:24,600 --> 00:05:29,600
Well, like the most popular ones, like GPT,

94
00:05:30,320 --> 00:05:32,440
they are autoregressive models.

95
00:05:32,440 --> 00:05:36,800
Well, so by the chain rule of probability,

96
00:05:36,800 --> 00:05:40,680
we can decompose our joined distribution this way.

97
00:05:42,360 --> 00:05:45,760
Represented as a graphical model is basically

98
00:05:45,760 --> 00:05:47,760
for each random variable,

99
00:05:47,760 --> 00:05:51,520
we have arrows going from all the previous random variables.

100
00:05:52,600 --> 00:05:55,000
From a generative point of view,

101
00:05:56,000 --> 00:05:58,640
it's basically we start with,

102
00:05:59,800 --> 00:06:01,040
I can't see my cursor.

103
00:06:01,040 --> 00:06:05,360
So we start generating the first word

104
00:06:05,360 --> 00:06:07,480
from this prior distribution.

105
00:06:07,480 --> 00:06:08,920
Once we have the first word,

106
00:06:08,920 --> 00:06:10,920
we move on to the second one,

107
00:06:10,920 --> 00:06:12,320
conditioning on the first one,

108
00:06:12,320 --> 00:06:13,960
and then we generate the third one,

109
00:06:13,960 --> 00:06:17,160
conditioning on the first two words, very simple, okay?

110
00:06:19,560 --> 00:06:22,840
By some classic results,

111
00:06:22,920 --> 00:06:26,680
marginose for Bayesian networks

112
00:06:26,680 --> 00:06:29,400
with these structures is intractable.

113
00:06:31,360 --> 00:06:36,360
Okay, so before when we asked GPT

114
00:06:39,200 --> 00:06:40,600
to generate a sentence for us,

115
00:06:40,600 --> 00:06:44,440
we are like talking to it as if it's human, right?

116
00:06:44,440 --> 00:06:48,080
So the assumption is if our distribution over language

117
00:06:48,080 --> 00:06:51,120
is a perfect distribution,

118
00:06:51,120 --> 00:06:54,960
then we should be able to interact with it as a human.

119
00:06:56,400 --> 00:06:58,640
When we're doing these kind of like prompting,

120
00:06:58,640 --> 00:06:59,840
when we're talking to it,

121
00:06:59,840 --> 00:07:03,560
most of the time we are actually trying to do conditioning.

122
00:07:03,560 --> 00:07:06,560
So let's consider an even simpler example.

123
00:07:07,680 --> 00:07:10,560
Suppose in our autoregressive generation,

124
00:07:10,560 --> 00:07:14,400
we have generated the first three words, the weather is,

125
00:07:14,400 --> 00:07:17,600
and our constraint is that we want the sentence

126
00:07:17,600 --> 00:07:19,280
to contain the keyword winter.

127
00:07:19,280 --> 00:07:21,200
Well, I mean, we wanna write something

128
00:07:23,000 --> 00:07:25,840
with the topic of winter, so that's our constraint.

129
00:07:26,920 --> 00:07:30,160
What we really want is, okay,

130
00:07:30,160 --> 00:07:34,640
so what language model gives us is the next token,

131
00:07:34,640 --> 00:07:36,440
next word probability.

132
00:07:36,440 --> 00:07:39,840
So basically given the weather is,

133
00:07:39,840 --> 00:07:44,040
it might prefer whether it's warm or cold,

134
00:07:45,120 --> 00:07:47,440
while its prior distribution might want the weather

135
00:07:47,440 --> 00:07:48,920
to be warm.

136
00:07:48,920 --> 00:07:51,440
But what we actually want,

137
00:07:52,480 --> 00:07:53,920
but what we'll actually need

138
00:07:53,920 --> 00:07:57,120
is this conditional next word probability, right?

139
00:07:57,120 --> 00:08:00,600
We want our sentence to satisfy the constraint,

140
00:08:00,600 --> 00:08:01,960
once it contain the winter,

141
00:08:03,440 --> 00:08:05,440
and well, intuitively,

142
00:08:05,440 --> 00:08:08,440
if we want the winter to be in the sentence,

143
00:08:08,440 --> 00:08:11,440
weather is more likely to be cold, okay?

144
00:08:11,440 --> 00:08:16,440
But this is intractable for those autoregressive models.

145
00:08:21,240 --> 00:08:23,120
So why is it intractable?

146
00:08:23,120 --> 00:08:25,240
Let's go more into the details.

147
00:08:25,240 --> 00:08:29,440
So by Bayes' rule, this conditional probability

148
00:08:29,440 --> 00:08:32,160
can be decomposed into these two terms.

149
00:08:33,640 --> 00:08:35,440
The next term here is very simple,

150
00:08:35,440 --> 00:08:38,400
it's just a language model next word probability.

151
00:08:38,400 --> 00:08:43,400
But the first term here is actually a marginal probability.

152
00:08:45,000 --> 00:08:47,800
So it's basically the marginal probability

153
00:08:47,800 --> 00:08:51,360
over all possible suffixes that we could generate

154
00:08:51,360 --> 00:08:53,800
that contains winter, okay?

155
00:08:55,480 --> 00:08:58,200
So this is intractable for autoregressive models,

156
00:08:58,200 --> 00:09:00,080
but we have PCs, right?

157
00:09:00,080 --> 00:09:03,200
PCs are tractable, at least,

158
00:09:04,960 --> 00:09:07,880
we know how to compute marginal probabilities with them.

159
00:09:07,880 --> 00:09:12,880
So why don't we just approximate this term here

160
00:09:13,280 --> 00:09:14,840
with probabilistic circuits?

161
00:09:18,120 --> 00:09:22,720
And we refer to our pipeline as gelato,

162
00:09:23,760 --> 00:09:26,120
in generating language with tractable constraints.

163
00:09:28,840 --> 00:09:31,400
Okay, so how do we do this?

164
00:09:31,400 --> 00:09:36,400
So the first step is that we pick our favorite PC

165
00:09:36,400 --> 00:09:40,360
for sequential modeling, which is a hidden markup model,

166
00:09:40,360 --> 00:09:41,880
the simplest we can have.

167
00:09:43,160 --> 00:09:46,480
We have, on the other hand,

168
00:09:46,480 --> 00:09:48,120
we have our pre-trained language model

169
00:09:48,120 --> 00:09:50,360
we want to control or guide.

170
00:09:50,360 --> 00:09:53,360
We sample a lot of data from our language model

171
00:09:53,360 --> 00:09:57,680
unconditionally, and then we train our hidden markup model

172
00:09:57,680 --> 00:10:01,780
on these data with maximum likelihood estimation.

173
00:10:01,780 --> 00:10:04,320
So effectively minimizing the KL divergence

174
00:10:04,320 --> 00:10:08,000
between the two joint distributions, okay?

175
00:10:08,000 --> 00:10:13,000
And our assumption is that if the joint distributions

176
00:10:13,280 --> 00:10:17,920
are close enough, then all of the marginal distributions

177
00:10:17,920 --> 00:10:22,560
and conditional distributions are gonna be similar, okay?

178
00:10:22,560 --> 00:10:23,400
Any questions?

179
00:10:25,040 --> 00:10:27,920
So the intuition is that we have a black box

180
00:10:27,920 --> 00:10:30,280
autoregressive model, and we train

181
00:10:30,280 --> 00:10:32,000
some sort of white box PC,

182
00:10:32,040 --> 00:10:37,040
so we can use as a representative of the black box, okay?

183
00:10:41,400 --> 00:10:45,800
So we don't want to, since this is the workshop

184
00:10:45,800 --> 00:10:50,800
for circuits and logic, let's represent HMMs as PCs.

185
00:10:51,280 --> 00:10:54,760
So here is the graphical model version of PC.

186
00:10:55,600 --> 00:11:00,360
The Zs here are the hidden variables, the latent variables.

187
00:11:01,360 --> 00:11:05,480
And the X here are the observed variables, or the words.

188
00:11:07,040 --> 00:11:11,360
And let me use the whiteboard to show you how to do this.

189
00:11:16,560 --> 00:11:21,560
Okay, so one assumption we have is that our hidden states,

190
00:11:21,720 --> 00:11:25,440
our latent variables, are discrete variables

191
00:11:25,440 --> 00:11:30,200
taking values from one to H, and H is our hidden state.

192
00:11:30,200 --> 00:11:31,040
Any questions?

193
00:11:32,560 --> 00:11:34,840
Okay, so we start from the very beginning.

194
00:11:34,840 --> 00:11:39,200
In an HMM, we start with the initial state, Z1,

195
00:11:39,200 --> 00:11:42,120
and it has H choices.

196
00:11:52,000 --> 00:11:57,000
So we want these eight product nodes to be representing

197
00:11:57,920 --> 00:12:01,440
the probability, can people see it actually?

198
00:12:01,440 --> 00:12:06,440
So representing the probability of X1 to N

199
00:12:06,960 --> 00:12:09,640
conditioning on Z1 equals to I.

200
00:12:11,960 --> 00:12:15,080
So I would be the hidden state, so I will,

201
00:12:15,080 --> 00:12:19,400
this will be one, two, and H, okay?

202
00:12:19,400 --> 00:12:24,400
And the edge will have weights

203
00:12:24,400 --> 00:12:27,320
of the prior distribution on Z1.

204
00:12:33,320 --> 00:12:36,120
Okay, I see nodding, I see confusing faces,

205
00:12:36,120 --> 00:12:37,480
but I'll move on whatever.

206
00:12:39,280 --> 00:12:41,960
Okay, so it'll be clear later.

207
00:12:41,960 --> 00:12:46,960
So given, so by the Markov property, given Z1, X1,

208
00:12:47,840 --> 00:12:51,600
and so basically given Z1, this part

209
00:12:51,600 --> 00:12:53,720
and the remaining part will be independent.

210
00:12:53,720 --> 00:12:56,160
So let's deal with X1 first.

211
00:13:03,880 --> 00:13:06,080
We have some input distributions,

212
00:13:09,720 --> 00:13:14,080
and this input distribution will be representing

213
00:13:14,080 --> 00:13:17,200
the probability, the emission probability,

214
00:13:17,200 --> 00:13:20,760
basically X1 given Z1 equals to I.

215
00:13:21,440 --> 00:13:25,480
Okay, so basically here, we're in the states

216
00:13:25,480 --> 00:13:28,280
of Z1 equals to I, and this will represent

217
00:13:28,280 --> 00:13:31,280
the probability of X1 given at Z1 equals to I.

218
00:13:34,920 --> 00:13:36,560
I see more nodding faces now.

219
00:13:40,480 --> 00:13:43,120
Okay, and we want this part to describe

220
00:13:43,120 --> 00:13:47,680
the remaining distribution, so we have some nodes here,

221
00:13:47,840 --> 00:13:52,840
and they will represent the distribution of P of X2

222
00:14:01,120 --> 00:14:06,120
to N given Z1 equals to I, okay?

223
00:14:08,480 --> 00:14:10,680
Corresponding to the inputs.

224
00:14:10,680 --> 00:14:14,640
And so we proceed to the next layer.

225
00:14:17,680 --> 00:14:22,680
And this part will be the transition probabilities,

226
00:14:29,080 --> 00:14:32,920
will be the transition probability of Z2 equals to J

227
00:14:32,920 --> 00:14:37,920
given Z1 equals to I, and these will recursively,

228
00:14:39,720 --> 00:14:42,320
we do this construction recursively,

229
00:14:42,320 --> 00:14:44,200
will be representing the probability

230
00:14:44,240 --> 00:14:48,240
from X2 to N, conditioning on Z2 equals to I, okay?

231
00:14:50,280 --> 00:14:51,120
Does that make sense?

232
00:14:51,120 --> 00:14:56,120
So we just proceed, so it's like Antonio and Robert

233
00:14:57,400 --> 00:14:59,720
was trying to make the point,

234
00:14:59,720 --> 00:15:03,160
this is a tensorized layer representation of a PC.

235
00:15:06,600 --> 00:15:11,600
Okay, let's move on, okay, so now we have our,

236
00:15:14,320 --> 00:15:15,140
oh, sorry.

237
00:15:18,000 --> 00:15:21,720
So let's move on, so we have our circuit

238
00:15:21,720 --> 00:15:26,440
representing the distribution over text,

239
00:15:26,440 --> 00:15:28,960
then we need to answer the query,

240
00:15:28,960 --> 00:15:31,040
we need to encode the logical constraint

241
00:15:31,040 --> 00:15:32,260
as the logical circuit.

242
00:15:35,600 --> 00:15:40,600
Let's go back to the constraint we have in the very beginning,

243
00:15:41,120 --> 00:15:43,180
we want Frisbee, dog and dog.

244
00:15:44,480 --> 00:15:47,720
Frisbee, caught and dog to appear in the given order.

245
00:15:50,280 --> 00:15:53,760
This is like a naive way to represent this constraint,

246
00:15:54,640 --> 00:15:59,640
basically the IJK here are enumerating all possible

247
00:16:02,120 --> 00:16:06,040
positions of these three words,

248
00:16:06,040 --> 00:16:11,040
and we take a conjunction whenever we know the positions.

249
00:16:11,920 --> 00:16:13,020
Any questions?

250
00:16:15,200 --> 00:16:18,800
Okay, okay, but this is not really ideal,

251
00:16:18,800 --> 00:16:22,800
we can directly convert it into a PC

252
00:16:22,800 --> 00:16:24,280
that represents the constraint,

253
00:16:24,280 --> 00:16:29,280
but what are the problems, do people see that?

254
00:16:37,720 --> 00:16:41,040
I'll give a hint, complexity,

255
00:16:41,040 --> 00:16:42,800
what's the complexity of this,

256
00:16:42,840 --> 00:16:44,480
what's the size of this DNF?

257
00:16:47,880 --> 00:16:50,680
Yes, cubic, cubic, and to be more precise,

258
00:16:50,680 --> 00:16:53,400
it's n choose number of keywords, right?

259
00:16:53,400 --> 00:16:55,480
Well, I mean, we can do it like cubic,

260
00:16:55,480 --> 00:16:57,840
but suppose we have five or 10 keywords,

261
00:16:57,840 --> 00:17:01,160
we can no longer take that complexity, okay?

262
00:17:01,160 --> 00:17:03,880
And the other problem is more subtle,

263
00:17:03,880 --> 00:17:08,880
which is that this DNF is not deterministic, okay?

264
00:17:09,760 --> 00:17:13,600
So we know that we can multiply circuits

265
00:17:13,600 --> 00:17:15,760
when they are compatible,

266
00:17:15,760 --> 00:17:18,280
or when they are structured decomposable

267
00:17:18,280 --> 00:17:20,320
with respect to the same vitri,

268
00:17:20,320 --> 00:17:22,920
but here we want exact conditioning,

269
00:17:22,920 --> 00:17:27,920
so we actually need to make sure that our circuit

270
00:17:29,280 --> 00:17:31,360
represents a uniform distribution

271
00:17:31,360 --> 00:17:35,120
over the support specified by this DNF.

272
00:17:35,560 --> 00:17:40,560
DNF, and in general, it is sharply hard

273
00:17:42,400 --> 00:17:45,240
to do model counting to normalize everything.

274
00:17:45,240 --> 00:17:48,320
Okay, I'll go into the details on the board.

275
00:17:49,280 --> 00:17:53,720
Yes, yes, I'll talk about it on the board.

276
00:17:59,600 --> 00:18:01,440
Okay, I'll move to the other side.

277
00:18:05,120 --> 00:18:10,120
Okay, so for the first question,

278
00:18:10,560 --> 00:18:13,240
why does it need to be deterministic?

279
00:18:13,240 --> 00:18:16,340
It is because so, okay, suppose we have a very,

280
00:18:18,200 --> 00:18:21,280
okay, so suppose we have a very simple distribution,

281
00:18:21,280 --> 00:18:24,320
X1, X2.

282
00:18:35,560 --> 00:18:40,560
So, we have some weights,

283
00:18:48,360 --> 00:18:52,160
so this is a distribution over two random variables,

284
00:18:52,160 --> 00:18:55,360
okay, and this is its probability mass function.

285
00:19:00,520 --> 00:19:02,000
So when we are conditioning,

286
00:19:02,000 --> 00:19:04,840
we are actually selecting all the terms

287
00:19:04,880 --> 00:19:06,880
that satisfy our constraint

288
00:19:06,880 --> 00:19:10,400
and zeroing out everything else, right?

289
00:19:10,400 --> 00:19:14,200
So if we want to do that with circuit multiplication,

290
00:19:15,040 --> 00:19:17,720
suppose our constraint, our support,

291
00:19:17,720 --> 00:19:22,720
would be something like X1, X2, and X1 bar, X2, okay?

292
00:19:25,840 --> 00:19:28,080
So the constraint circuit

293
00:19:28,520 --> 00:19:33,520
should be a uniform distribution over its support,

294
00:19:37,440 --> 00:19:42,440
otherwise it messes up with the original weights, right?

295
00:19:43,360 --> 00:19:44,440
Does that make sense?

296
00:19:45,600 --> 00:19:48,880
Okay, so however, okay, let me,

297
00:19:48,880 --> 00:19:53,880
so this will be 0.5, X1, X2 plus 0.5, X1 bar.

298
00:19:59,040 --> 00:20:01,280
Well, I mean, suppose,

299
00:20:03,480 --> 00:20:04,760
and we have a bunch of zeros,

300
00:20:04,760 --> 00:20:08,240
and we multiply these two circuits point-wise,

301
00:20:09,240 --> 00:20:13,280
and we kind, we keep these two terms

302
00:20:13,280 --> 00:20:15,440
and erase these two terms, right?

303
00:20:15,440 --> 00:20:19,000
But we want W and W3 to be proportional to each other,

304
00:20:19,000 --> 00:20:21,800
like the ratio should stay the same.

305
00:20:21,800 --> 00:20:24,720
So this circuit must be a uniform distribution

306
00:20:24,720 --> 00:20:27,640
over the support of constraint, okay?

307
00:20:27,680 --> 00:20:30,240
So given a logical circuit,

308
00:20:30,240 --> 00:20:32,440
how do we convert it into a PC

309
00:20:32,440 --> 00:20:36,280
that represents a uniform distribution over the support?

310
00:20:37,480 --> 00:20:40,440
To do that, we need to do model counting,

311
00:20:40,440 --> 00:20:43,040
but model counting in general is hard

312
00:20:43,040 --> 00:20:44,720
if determinism is missing.

313
00:20:46,240 --> 00:20:49,760
So this one won't work.

314
00:20:52,200 --> 00:20:53,520
Does that make sense?

315
00:20:55,640 --> 00:20:57,200
Yes, no?

316
00:20:57,200 --> 00:20:58,040
Robert?

317
00:20:59,640 --> 00:21:02,040
Isn't that full of a new mistake?

318
00:21:02,040 --> 00:21:04,640
Oh yeah, so it's a very subtle, subtle thing.

319
00:21:04,640 --> 00:21:09,640
So, well, we only require Frisbee, Caught, and Dog

320
00:21:10,240 --> 00:21:11,920
to appear in some positions,

321
00:21:11,920 --> 00:21:16,760
but we do not limit that they have to appear exactly once.

322
00:21:16,760 --> 00:21:21,760
So you can totally have Frisbee, Frisbee, Caught,

323
00:21:23,120 --> 00:21:26,320
Caught, Dog, Dog, something like this.

324
00:21:27,560 --> 00:21:29,600
So we have two sub-sequences.

325
00:21:30,400 --> 00:21:32,680
We have a lot of sub-sequences, right?

326
00:21:34,320 --> 00:21:39,320
So this is position one, two, three, four, five, six.

327
00:21:41,160 --> 00:21:45,320
So when IJK are one, three, five,

328
00:21:46,240 --> 00:21:49,240
it satisfies the clauses, right?

329
00:21:49,240 --> 00:21:50,760
One of them, right?

330
00:21:57,200 --> 00:21:59,920
Let me choose a different way.

331
00:21:59,920 --> 00:22:04,800
Okay, so basically for this huge disjunction,

332
00:22:04,800 --> 00:22:07,440
for each instantiation to the variable,

333
00:22:07,440 --> 00:22:09,720
we only satisfy one of the clauses, right?

334
00:22:09,720 --> 00:22:11,320
We want, that's determinism.

335
00:22:13,280 --> 00:22:14,480
Yes.

336
00:22:14,480 --> 00:22:19,480
And suppose IJK are one, three, five.

337
00:22:20,240 --> 00:22:22,280
Does this instance satisfy that?

338
00:22:22,280 --> 00:22:23,400
Yes, it does, right?

339
00:22:24,400 --> 00:22:28,040
But when IJK are one, four, five,

340
00:22:28,040 --> 00:22:29,560
it also satisfies that.

341
00:22:29,560 --> 00:22:30,400
So it's not.

342
00:22:31,360 --> 00:22:33,120
Does that make sense?

343
00:22:33,120 --> 00:22:33,960
Okay.

344
00:22:33,960 --> 00:22:35,720
Okay, so now let's construct

345
00:22:35,720 --> 00:22:38,320
a deterministic circuit representing this constraint.

346
00:22:40,520 --> 00:22:45,520
The idea is similar to a deterministic finite automata.

347
00:22:46,640 --> 00:22:51,440
Basically, you track how many words have you included,

348
00:22:51,440 --> 00:22:52,600
how many words have you used,

349
00:22:52,600 --> 00:22:55,480
which state are you in satisfying the constraint?

350
00:22:56,720 --> 00:22:59,520
So we want to construct these sub formulas,

351
00:22:59,520 --> 00:23:03,520
V, T, say cod and dog.

352
00:23:07,720 --> 00:23:10,560
The sub formula representing the constraint

353
00:23:10,560 --> 00:23:15,560
that cod and dog appears

354
00:23:16,560 --> 00:23:21,560
in XT, XT plus one.

355
00:23:27,400 --> 00:23:31,640
So how do we construct this sub circuit or this sub formula?

356
00:23:31,640 --> 00:23:32,680
Oh, it's pretty simple.

357
00:23:32,680 --> 00:23:34,680
It's a sum node.

358
00:23:34,680 --> 00:23:36,640
We consider two different cases.

359
00:23:37,800 --> 00:23:41,480
One of the cases that XT is cod.

360
00:23:42,320 --> 00:23:47,320
And the other case is that XT is not cod.

361
00:23:52,840 --> 00:23:54,640
Does this make sense?

362
00:23:54,640 --> 00:23:55,480
Okay.

363
00:23:55,480 --> 00:24:00,280
So if XT is cod, then we have like step one.

364
00:24:02,680 --> 00:24:05,400
We have made one step towards satisfying the constraint.

365
00:24:07,480 --> 00:24:11,120
So we can reduce it to V, T plus one.

366
00:24:12,080 --> 00:24:12,920
Dog.

367
00:24:14,400 --> 00:24:16,400
And suppose XT is not cod.

368
00:24:16,400 --> 00:24:17,720
We're in the same state.

369
00:24:19,000 --> 00:24:21,040
We reduce it to T plus one.

370
00:24:24,760 --> 00:24:25,600
Dog.

371
00:24:28,560 --> 00:24:29,400
Is that clear?

372
00:24:30,280 --> 00:24:33,720
So suppose we have constructed FI

373
00:24:33,720 --> 00:24:38,720
for all time step greater than T

374
00:24:39,560 --> 00:24:42,800
then we can construct all the fees for T.

375
00:24:45,800 --> 00:24:47,680
So it's a recursive algorithm.

376
00:24:51,400 --> 00:24:52,520
Oh, what's the base case?

377
00:24:52,520 --> 00:24:54,120
Yes, that's very interesting.

378
00:24:58,240 --> 00:24:59,680
Okay, I'm gonna erase this.

379
00:25:03,120 --> 00:25:06,520
So just cod and dog in the part.

380
00:25:06,520 --> 00:25:10,320
So yeah, I mean, we can make it even simpler.

381
00:25:10,320 --> 00:25:12,320
I'm lazy, so I wanna make things simpler.

382
00:25:14,160 --> 00:25:16,440
So suppose in the nth position,

383
00:25:18,160 --> 00:25:20,520
we only have one word, right?

384
00:25:21,760 --> 00:25:23,760
So fee and,

385
00:25:30,240 --> 00:25:32,120
what is this formula?

386
00:25:32,120 --> 00:25:37,120
It means that from Xn to Xn,

387
00:25:38,240 --> 00:25:40,960
we need to have cod and dog.

388
00:25:40,960 --> 00:25:43,520
So this is false, right?

389
00:25:43,520 --> 00:25:44,360
Zero.

390
00:25:47,360 --> 00:25:49,280
What if we only have dog?

391
00:25:49,280 --> 00:25:54,280
So this is true only if Xn equals dog.

392
00:25:57,520 --> 00:25:59,640
So yeah, those are the base cases.

393
00:26:00,480 --> 00:26:05,480
Okay, so what's the size of the circuit?

394
00:26:07,040 --> 00:26:08,760
So at each time step,

395
00:26:10,400 --> 00:26:15,400
we need fee T, maybe we have already satisfied the constraint.

396
00:26:15,400 --> 00:26:18,880
So something empty, right?

397
00:26:18,880 --> 00:26:23,880
We need fee T, we have one word left.

398
00:26:24,880 --> 00:26:29,240
Fee T, we have cod and dog, we have two word left.

399
00:26:30,840 --> 00:26:34,200
And we have generated none of them yet.

400
00:26:38,520 --> 00:26:41,800
Okay, so at each time step,

401
00:26:41,800 --> 00:26:46,800
we have four sum notes or four more notes

402
00:26:47,320 --> 00:26:49,440
representing all these four cases.

403
00:26:50,400 --> 00:26:54,680
And the size, the number of notes in the circuit

404
00:26:54,680 --> 00:26:55,920
would be four times.

405
00:26:57,760 --> 00:26:59,240
Okay, does that make sense?

406
00:27:02,240 --> 00:27:05,440
And you can notice that when we are constructing this circuit,

407
00:27:05,440 --> 00:27:08,280
we are always conditioning on XT,

408
00:27:08,280 --> 00:27:10,680
current variable, which is very similar to HMM.

409
00:27:10,680 --> 00:27:14,240
It generates one variable at a time, one word at a time.

410
00:27:14,240 --> 00:27:16,600
So it is compatible with HMM.

411
00:27:16,600 --> 00:27:18,280
And also it's deterministic.

412
00:27:20,320 --> 00:27:23,160
And we can do the apply operation,

413
00:27:23,160 --> 00:27:25,400
the product operation layer-wise.

414
00:27:25,400 --> 00:27:28,280
So the size of the eventual circuit

415
00:27:29,760 --> 00:27:33,560
would be four, so originally,

416
00:27:33,560 --> 00:27:38,560
so originally, here for each layer, we have H nodes.

417
00:27:39,600 --> 00:27:43,040
Now we, here for each layer, we have four nodes.

418
00:27:43,040 --> 00:27:46,600
So eventually we have four H nodes just for each layer.

419
00:27:47,600 --> 00:27:51,600
Acceptable, okay, does that make sense?

420
00:27:53,280 --> 00:27:55,360
So we can still generate this.

421
00:27:56,360 --> 00:27:58,480
We can generate this even further.

422
00:27:58,480 --> 00:28:02,000
Well, some time, well, one simple generalization is,

423
00:28:02,880 --> 00:28:06,560
you might think it doesn't have to be caught, right?

424
00:28:06,560 --> 00:28:10,520
It can be caught or either catch, right?

425
00:28:10,520 --> 00:28:15,520
Or catches, they all kind of represent the same concept.

426
00:28:17,120 --> 00:28:20,560
So in the construction, we can

427
00:28:23,840 --> 00:28:26,560
modify our original circuit a little bit.

428
00:28:27,520 --> 00:28:31,560
Before it was XT equals caught,

429
00:28:33,160 --> 00:28:36,120
we can replace it with another OR node,

430
00:28:38,560 --> 00:28:41,720
enumerating caught, catch, and catches.

431
00:28:42,320 --> 00:28:46,520
Okay, and the circuit size stays roughly the same.

432
00:28:48,080 --> 00:28:49,920
And there are many other things we can do.

433
00:28:49,920 --> 00:28:52,600
We can also say, oh, we don't need them

434
00:28:52,600 --> 00:28:55,160
to be in the same order, in a fixed order.

435
00:28:55,160 --> 00:28:58,440
We can, we want them to be like in arbitrary order.

436
00:28:58,440 --> 00:28:59,640
We don't care about the order.

437
00:28:59,640 --> 00:29:01,400
How do we do that?

438
00:29:01,400 --> 00:29:04,640
So in this case, we have four states, right?

439
00:29:04,640 --> 00:29:09,640
Basically enumerating all suffixes of these three keywords.

440
00:29:10,640 --> 00:29:15,040
In that case, we would have all subsets of these three keywords.

441
00:29:15,040 --> 00:29:20,040
So in that way, the complexity would be going from four,

442
00:29:21,040 --> 00:29:25,400
we have three keywords, and this is four, two, two to the three.

443
00:29:25,400 --> 00:29:29,400
That's there, eight subsets, two to the three subsets, okay?

444
00:29:29,400 --> 00:29:34,400
So still acceptable, two to the n is not really,

445
00:29:36,040 --> 00:29:38,880
say we have 10 keywords, two to the n, it's just 1024.

446
00:29:39,160 --> 00:29:41,720
It's still doable in practice, yes.

447
00:29:41,720 --> 00:29:45,720
So it seems that you can do any kind of regular expression

448
00:29:45,720 --> 00:29:50,200
constrained, but you're focusing here on language

449
00:29:50,200 --> 00:29:52,080
as a fixed length, right?

450
00:29:52,080 --> 00:29:57,080
Because you have a fixed number of random variables.

451
00:29:59,000 --> 00:30:03,400
So is that the kind of equivalent of what you're saying?

452
00:30:03,400 --> 00:30:05,360
Yes, yes, that's a very, very good point.

453
00:30:05,360 --> 00:30:09,680
So fixed length here is a very essential assumption.

454
00:30:09,680 --> 00:30:14,680
So suppose we have a distribution over language of arbitrary length,

455
00:30:16,560 --> 00:30:21,040
then applying the constraint could be hard to define.

456
00:30:22,160 --> 00:30:24,720
So basically these three words,

457
00:30:24,720 --> 00:30:29,760
they can appear in arbitrarily far positions,

458
00:30:29,760 --> 00:30:32,560
and we need probably you take like an infinite sum

459
00:30:32,560 --> 00:30:34,560
to define this conditional probability.

460
00:30:36,080 --> 00:30:39,880
But this is not terrible in practice,

461
00:30:39,880 --> 00:30:43,120
because in practice, even for models like GPT,

462
00:30:43,120 --> 00:30:46,400
they have a finite sequence length, yeah,

463
00:30:46,400 --> 00:30:51,400
that's a very good point, okay, I'm trying to be fast.

464
00:30:55,080 --> 00:30:57,560
Oh, I mean, there are variations of constraints

465
00:30:57,560 --> 00:31:02,440
we have talked about, and okay,

466
00:31:02,440 --> 00:31:04,800
so now we have our,

467
00:31:07,960 --> 00:31:12,240
sorry, now we have our probabilistic circuit

468
00:31:12,240 --> 00:31:14,200
representing the distribution,

469
00:31:14,200 --> 00:31:16,480
and we have our constraint circuit,

470
00:31:16,480 --> 00:31:18,520
and we'll close them to take the product.

471
00:31:20,120 --> 00:31:21,720
So we have our constraint circuit now,

472
00:31:21,720 --> 00:31:24,080
we can compute the probabilities we need.

473
00:31:24,080 --> 00:31:26,400
So the step two is very simple.

474
00:31:26,400 --> 00:31:30,680
So this is the original like base decomposition

475
00:31:30,680 --> 00:31:33,280
of the conditional probability we have.

476
00:31:33,280 --> 00:31:36,080
So this term here is intractable,

477
00:31:36,920 --> 00:31:40,560
so we secretly replace the subscript of LM with HMM,

478
00:31:40,560 --> 00:31:45,560
the one we notice, and we define this conditional distribution,

479
00:31:47,360 --> 00:31:51,960
the gelato distribution, and we can compute this

480
00:31:51,960 --> 00:31:55,480
with the linear pass of the circuit.

481
00:31:55,840 --> 00:32:01,480
Okay, so what are the advantages?

482
00:32:02,480 --> 00:32:04,800
So number one, by definition,

483
00:32:04,800 --> 00:32:07,240
the constraint alpha is guaranteed to be satisfied,

484
00:32:07,240 --> 00:32:11,160
so finally, other than compared to all the other methods,

485
00:32:11,160 --> 00:32:15,280
we have 100% reliable thing we can trust,

486
00:32:15,280 --> 00:32:18,280
and number two is that the training of this HMM

487
00:32:20,280 --> 00:32:22,120
does not depend on the constraint.

488
00:32:22,120 --> 00:32:25,200
So basically once we have this HMM trained,

489
00:32:25,200 --> 00:32:29,240
we can use it to enforce whatever constraint we want.

490
00:32:29,240 --> 00:32:32,160
So maybe today I want to write something

491
00:32:32,160 --> 00:32:34,920
using some keywords and key concepts,

492
00:32:34,920 --> 00:32:37,080
and tomorrow I feel like this language model

493
00:32:37,080 --> 00:32:41,720
is like using a lot of inappropriate languages,

494
00:32:41,720 --> 00:32:45,440
and we can detoxify it by specifying a list of bad words

495
00:32:45,440 --> 00:32:50,120
that it cannot use, and all the same model,

496
00:32:50,120 --> 00:32:53,600
and all the constraints are only enforced at inference time.

497
00:32:54,600 --> 00:32:55,600
Yes?

498
00:32:55,600 --> 00:32:57,520
What kind of constraints can be represented

499
00:32:57,520 --> 00:33:01,360
as the composable pieces?

500
00:33:01,360 --> 00:33:03,840
Yes, that's a very, very good question.

501
00:33:03,840 --> 00:33:06,000
That's actually one of the main reason

502
00:33:06,000 --> 00:33:08,200
in presenting this work here,

503
00:33:08,200 --> 00:33:11,640
because my feeling is that though people have been studying

504
00:33:11,640 --> 00:33:16,000
like probabilistic queries like marginal map, marginal,

505
00:33:18,240 --> 00:33:21,720
and all these kinds of stuff extensively,

506
00:33:21,720 --> 00:33:25,560
but in practice, we really care

507
00:33:25,560 --> 00:33:29,280
about some more complicated, less generic ones,

508
00:33:29,280 --> 00:33:32,360
and whether is there a language to define

509
00:33:32,360 --> 00:33:34,160
or to describe their tractability

510
00:33:34,160 --> 00:33:37,520
is kind of missing from the literature.

511
00:33:37,520 --> 00:33:40,720
But I could relate this to say

512
00:33:40,720 --> 00:33:45,720
there's some work on compiling DFAs to circuits.

513
00:33:46,720 --> 00:33:50,400
I think that could be something as a starting point

514
00:33:50,400 --> 00:33:51,360
to look at.

515
00:33:51,400 --> 00:33:52,920
I'm not sure if that answers your question.

516
00:33:52,920 --> 00:33:53,920
I don't have an answer.

517
00:33:53,920 --> 00:33:56,240
My answer is, okay.

518
00:33:57,840 --> 00:34:00,120
Okay, so experiments and benchmarks.

519
00:34:01,160 --> 00:34:06,040
So we evaluate our method on this common sense generation,

520
00:34:06,040 --> 00:34:07,360
common gem benchmark.

521
00:34:07,360 --> 00:34:10,080
Well, it's very similar to the example I gave you.

522
00:34:10,080 --> 00:34:12,280
It gave you a bunch of keywords,

523
00:34:12,280 --> 00:34:17,280
and each example comes with a bunch of gold sentences,

524
00:34:17,600 --> 00:34:20,360
and you want to generate something

525
00:34:20,360 --> 00:34:25,040
that looks similar to the gold sentences using keywords.

526
00:34:26,000 --> 00:34:29,600
And here, it's like the most general case.

527
00:34:29,600 --> 00:34:31,440
So basically these keywords,

528
00:34:31,440 --> 00:34:32,920
they can appear in any order,

529
00:34:32,920 --> 00:34:37,160
and they can appear in any form of their inflections.

530
00:34:38,720 --> 00:34:42,480
Okay, so this is the, yes, then.

531
00:34:43,520 --> 00:34:46,520
And it's a gigantic table.

532
00:34:46,520 --> 00:34:48,920
The numbers themselves are not that important.

533
00:34:51,080 --> 00:34:53,080
So one thing to note that is compared

534
00:34:53,080 --> 00:34:54,880
to all the other baselines,

535
00:34:56,040 --> 00:35:00,760
our method, gelato, achieves a state-of-the-art performance

536
00:35:00,760 --> 00:35:04,360
with respect to basically all metrics.

537
00:35:04,360 --> 00:35:08,200
So these metrics here, Rouge L, Blue Four Ciders,

538
00:35:08,200 --> 00:35:12,320
Spice State, there are just some standard NLP metrics

539
00:35:12,320 --> 00:35:17,320
that people use to evaluate the quality of your text.

540
00:35:17,600 --> 00:35:19,920
So basically you have a gold sentence,

541
00:35:19,920 --> 00:35:22,360
and you have your generated sentence.

542
00:35:22,360 --> 00:35:25,720
They compute somehow the NBREM overlap

543
00:35:25,720 --> 00:35:27,800
to measure the quality.

544
00:35:31,640 --> 00:35:35,440
But the other thing is that all of the previous method

545
00:35:37,440 --> 00:35:42,160
cannot achieve 100% constraint satisfiability,

546
00:35:42,160 --> 00:35:45,200
but ours does in practice as well.

547
00:35:45,200 --> 00:35:46,720
Well, there is this one baseline,

548
00:35:46,720 --> 00:35:49,600
they achieve 100% accuracy as well,

549
00:35:49,600 --> 00:35:54,600
but they kind of did it by starting from the keywords.

550
00:35:54,840 --> 00:35:57,400
So they're always gonna be there.

551
00:35:58,360 --> 00:36:01,240
And you can see their generation quality

552
00:36:01,240 --> 00:36:02,600
is really poor, so.

553
00:36:07,840 --> 00:36:11,440
We also conducted some sort of human evaluation.

554
00:36:14,040 --> 00:36:15,280
Okay, I guess, yes?

555
00:36:15,280 --> 00:36:17,280
Which language model does that look like?

556
00:36:17,280 --> 00:36:22,280
Oh, the language model, we use GPT-2, GPT-2 large.

557
00:36:23,160 --> 00:36:25,000
Yeah, and all of the baseline state,

558
00:36:25,000 --> 00:36:26,400
they use GPT-2 large, yeah.

559
00:36:28,960 --> 00:36:31,440
And in case people don't really trust

560
00:36:31,440 --> 00:36:33,520
these automatic evaluation metrics,

561
00:36:33,520 --> 00:36:35,720
we also conducted human evaluation.

562
00:36:38,600 --> 00:36:41,200
And you can see that our model performs

563
00:36:42,200 --> 00:36:44,400
much better than previous state-of-the-art.

564
00:36:45,400 --> 00:36:47,520
Okay, well, are they very significant?

565
00:36:47,520 --> 00:36:49,520
I mean, they're pretty close.

566
00:36:49,520 --> 00:36:52,640
Yeah, so, yeah, so basically the,

567
00:36:54,320 --> 00:36:56,080
I'm not sure if you can see it clearly,

568
00:36:56,080 --> 00:36:59,240
but the bold-faced ones are statistically

569
00:37:00,560 --> 00:37:04,520
significantly, what equivalent?

570
00:37:04,520 --> 00:37:09,520
So these ones, these two are statistically equivalent.

571
00:37:09,840 --> 00:37:12,580
This one is statistically significant.

572
00:37:13,580 --> 00:37:15,420
And we're looking at, when defibrating,

573
00:37:15,420 --> 00:37:17,620
what's the number here?

574
00:37:17,620 --> 00:37:20,660
Oh, yeah, so basically we provide the annotator

575
00:37:20,660 --> 00:37:23,620
some sentence and we provide description

576
00:37:23,620 --> 00:37:26,940
of one of each of the aspects.

577
00:37:26,940 --> 00:37:29,420
So basically, concept means that does it use

578
00:37:29,420 --> 00:37:31,540
all the concepts naturally?

579
00:37:31,540 --> 00:37:34,300
And plausibility means that is the sentence

580
00:37:34,300 --> 00:37:38,180
like a plausible sentence describing a realistic scene.

581
00:37:38,180 --> 00:37:41,920
And quality is basically fluency, grammar, and stuff.

582
00:37:41,960 --> 00:37:44,040
Overall is the like another,

583
00:37:44,040 --> 00:37:45,800
how do you feel about the sentence?

584
00:37:45,800 --> 00:37:48,400
And the numbers are from one, two, three.

585
00:37:48,400 --> 00:37:50,520
One, two, three, go, yeah.

586
00:37:50,520 --> 00:37:51,360
Okay.

587
00:37:53,720 --> 00:37:57,800
Okay, so let's get back to the very first

588
00:37:57,800 --> 00:38:01,240
motivating example and you can see that gelato,

589
00:38:01,240 --> 00:38:03,860
we use our model to add, and it is actually able

590
00:38:03,860 --> 00:38:07,020
to get everything correct and generate a fluent sentence.

591
00:38:07,860 --> 00:38:12,860
And another, I don't, okay, so we also found this one

592
00:38:16,540 --> 00:38:18,540
in one of the generated candidates.

593
00:38:18,540 --> 00:38:21,580
A pair of Frisbee players are caught in a dog fight,

594
00:38:21,580 --> 00:38:26,580
which is not like the thing that most people

595
00:38:26,820 --> 00:38:27,740
would like to think of.

596
00:38:27,740 --> 00:38:32,740
So it also shows some sort of creativity here.

597
00:38:32,860 --> 00:38:36,780
Here, okay, that's my talk.

598
00:38:38,060 --> 00:38:40,460
Please ask questions if you have,

599
00:38:40,460 --> 00:38:42,620
and otherwise we can go to lunch.

600
00:38:42,620 --> 00:38:43,460
Thank you.

601
00:38:43,460 --> 00:38:44,300
Thank you.

602
00:38:47,300 --> 00:38:48,300
One more question.

603
00:38:50,860 --> 00:38:52,700
Thanks for your talk, if I understand right,

604
00:38:52,700 --> 00:38:55,540
you said that to generate every word,

605
00:38:55,540 --> 00:38:58,740
you have to do a linear pass over the circuit.

606
00:38:58,740 --> 00:39:02,100
So how much is this overhead compared to the kind

607
00:39:02,260 --> 00:39:03,780
of latency from the language model?

608
00:39:03,780 --> 00:39:06,180
Okay, I like that question.

609
00:39:06,180 --> 00:39:10,780
So we don't really need to take a pass over the whole circuit.

610
00:39:10,780 --> 00:39:14,280
With some caching, we can do like constant time.

611
00:39:15,500 --> 00:39:17,620
So basically to generate each word,

612
00:39:17,620 --> 00:39:19,340
the cost is like constant time.

613
00:39:19,340 --> 00:39:21,140
It's like a pass through one layer.

614
00:39:22,300 --> 00:39:23,300
Is it in any practice?

615
00:39:23,300 --> 00:39:24,140
How fast is it?

616
00:39:24,140 --> 00:39:25,500
Oh, how fast is it?

617
00:39:25,500 --> 00:39:26,900
We actually, so.

618
00:39:33,100 --> 00:39:35,860
I don't have a table here,

619
00:39:35,860 --> 00:39:37,460
but we have a table in the paper.

620
00:39:37,460 --> 00:39:42,460
So if say generating a sentence with five keywords,

621
00:39:43,460 --> 00:39:48,460
a GPT-2 large would take around 20 seconds,

622
00:39:50,020 --> 00:39:54,500
and our method is like 100-ish seconds.

623
00:39:54,500 --> 00:39:57,140
So it's not terrible.

624
00:39:57,140 --> 00:39:59,660
And one of the baselines here,

625
00:39:59,660 --> 00:40:03,020
well, which was actually like the best paper award

626
00:40:03,020 --> 00:40:05,140
at one of the top NLP conferences,

627
00:40:05,140 --> 00:40:07,060
they use search-based method.

628
00:40:07,060 --> 00:40:09,260
And for them to generate a sentence,

629
00:40:09,260 --> 00:40:13,100
they take like 700 seconds, 800 seconds.

630
00:40:13,100 --> 00:40:15,740
So because they're search-based,

631
00:40:15,740 --> 00:40:18,340
so when a search-based gets large,

632
00:40:21,740 --> 00:40:23,540
their method shows like a bottleneck.

633
00:40:23,620 --> 00:40:25,460
That's pretty well.

634
00:40:25,460 --> 00:40:26,300
Thanks.

635
00:40:28,100 --> 00:40:31,300
Can you also give an idea about the time required

636
00:40:31,300 --> 00:40:33,620
to derive the hidden Markov model?

637
00:40:33,620 --> 00:40:36,300
Oh, yeah, so I do have the time for that.

638
00:40:36,300 --> 00:40:40,820
So our hidden Markov model has like 4,000 states,

639
00:40:40,820 --> 00:40:43,340
and the emission is 50,000.

640
00:40:43,340 --> 00:40:48,020
We trained them with the juice framework

641
00:40:48,020 --> 00:40:49,500
that we developed in our lab.

642
00:40:50,340 --> 00:40:55,340
The training takes like 20 hours.

643
00:40:55,940 --> 00:40:59,860
So it's, we sample about 200,

644
00:41:02,940 --> 00:41:06,660
we sampled eight million sentences from GPT,

645
00:41:06,660 --> 00:41:11,060
and we trained them for 40 epochs, 20 hours.

646
00:41:16,140 --> 00:41:18,020
What's the number of parameters?

647
00:41:18,020 --> 00:41:19,060
Number of parameters?

648
00:41:20,500 --> 00:41:21,820
Of HMM.

649
00:41:21,820 --> 00:41:26,820
Yeah, so HMM is 4,000 hidden states and 50,000 emissions.

650
00:41:28,180 --> 00:41:31,820
So the main, mainly that the parameters are centered

651
00:41:31,820 --> 00:41:33,420
on the emission table.

652
00:41:33,420 --> 00:41:37,500
So it's like 40,000, 4,000 times 50,000.

653
00:41:37,500 --> 00:41:39,700
I didn't do that in my head.

654
00:41:39,700 --> 00:41:41,380
So those are unique parameters, right?

655
00:41:41,380 --> 00:41:42,620
Yes, yeah.

656
00:41:42,620 --> 00:41:46,020
Yes, so in the PC, yes, you kind of like,

657
00:41:46,020 --> 00:41:48,660
you've rolled out all the parameters.

658
00:41:49,620 --> 00:41:50,620
All the positions.

659
00:41:53,180 --> 00:41:56,140
Controlable generation is huge, right?

660
00:41:56,140 --> 00:41:57,580
So this is great.

661
00:41:57,580 --> 00:41:59,780
What, I mean, and I probably understand you,

662
00:41:59,780 --> 00:42:01,540
you gave a logical perspective for this audience,

663
00:42:01,540 --> 00:42:04,100
but I mean, a reg acts as a much more natural

664
00:42:04,100 --> 00:42:06,540
sort of control structure.

665
00:42:06,540 --> 00:42:09,460
So presumably you can handle any of our complication

666
00:42:09,460 --> 00:42:11,340
given the DFA interpretation, right?

667
00:42:11,340 --> 00:42:12,420
Yes, yes.

668
00:42:12,420 --> 00:42:15,060
I think that's a very important follow up.

669
00:42:15,060 --> 00:42:17,780
We are kind of looking at considering like,

670
00:42:17,820 --> 00:42:20,580
compiling DFA's to circuits

671
00:42:20,580 --> 00:42:24,580
and kind of automate all these process.

672
00:42:24,580 --> 00:42:26,700
Yeah, so ACLs in January, you submit there.

673
00:42:26,700 --> 00:42:28,260
I mean, I think they'll love it.

674
00:42:29,180 --> 00:42:31,980
Your initial example, you wanted a winter tree,

675
00:42:31,980 --> 00:42:35,780
you wanted the presence of winter to select for warm, right?

676
00:42:35,780 --> 00:42:37,980
Which is more of a natural language entailment.

677
00:42:37,980 --> 00:42:40,060
And I don't think you handle that, right?

678
00:42:40,060 --> 00:42:42,540
You handle variations and factual variations

679
00:42:42,540 --> 00:42:43,380
because you actually code them.

680
00:42:43,380 --> 00:42:48,300
I think we can totally do that winter example.

681
00:42:48,300 --> 00:42:52,580
So basically, let me go back.

682
00:42:53,580 --> 00:42:56,220
It seems like you encoded the specific variations

683
00:42:56,220 --> 00:42:57,580
that you were allowing, right?

684
00:42:57,580 --> 00:43:01,780
Oh, you mean like, so we can have winter, winters,

685
00:43:01,780 --> 00:43:04,620
but like, maybe if the winter,

686
00:43:04,620 --> 00:43:06,460
the word is not explicitly mentioned,

687
00:43:06,460 --> 00:43:07,780
you cannot do that.

688
00:43:07,780 --> 00:43:09,660
That's what I understand from your formalism, right?

689
00:43:09,660 --> 00:43:11,900
If it's in your or, you'll generate it.

690
00:43:11,900 --> 00:43:15,580
If it's not, you won't, you won't capture the entailment.

691
00:43:15,580 --> 00:43:19,300
No, I mean, so basically, so that's the other thing.

692
00:43:19,300 --> 00:43:23,220
So sometimes people, or most of the time,

693
00:43:23,220 --> 00:43:26,380
people want to have like kind of soft control.

694
00:43:26,380 --> 00:43:31,460
They can't really write their constraint in logical form.

695
00:43:31,460 --> 00:43:36,380
For example, toxic, like how can you tell a sentence is toxic?

696
00:43:36,380 --> 00:43:38,780
But there are many ways to approximate it.

697
00:43:38,780 --> 00:43:42,860
One of the ways is we have like a long list of phrases or words

698
00:43:42,860 --> 00:43:46,460
that you're now allowed to use in a toxic sentence

699
00:43:46,460 --> 00:43:49,740
and we can basically just write down a logical formula

700
00:43:49,740 --> 00:43:53,620
approximating that toxicity constraint.

701
00:43:53,620 --> 00:43:55,860
Yeah, I mean, there's also plug-and-play generation tricks

702
00:43:55,860 --> 00:43:58,460
that are actually quite interesting, I think that...

703
00:43:58,460 --> 00:44:04,580
So basically, there is a, of course, there's a naive way to do it.

704
00:44:04,580 --> 00:44:07,860
You can say whenever I encounter one of the words in the list,

705
00:44:07,900 --> 00:44:11,660
I just remote, like, prevent it from generating,

706
00:44:11,660 --> 00:44:14,300
prevent it from being sampled.

707
00:44:14,300 --> 00:44:18,580
But that's not exactly what we're trying to do probabilistically, right?

708
00:44:18,580 --> 00:44:19,300
So...

709
00:44:19,300 --> 00:44:22,300
Yeah, but I just, maybe look into plug-and-play,

710
00:44:22,300 --> 00:44:24,420
control generation is actually a paper.

711
00:44:24,420 --> 00:44:25,460
Yes, yeah.

712
00:44:25,460 --> 00:44:28,420
So where I think they're doing much more than that, right?

713
00:44:28,420 --> 00:44:31,100
Well, so, yeah, so it's like...

714
00:44:31,100 --> 00:44:34,580
They're also modifying the posterior selection.

715
00:44:34,580 --> 00:44:39,300
So if I'm not wrong, if I remember this correctly,

716
00:44:39,300 --> 00:44:42,940
they're basically trying to train some sort of...

717
00:44:47,020 --> 00:44:48,860
They use a classifier, right?

718
00:44:48,860 --> 00:44:51,220
Yeah, so basically, kind of, they're trying to train

719
00:44:51,220 --> 00:44:53,500
a newer model to approximate this.

720
00:44:53,500 --> 00:44:54,740
Yeah.

721
00:44:54,740 --> 00:44:56,260
So, but they're...

722
00:44:56,260 --> 00:45:00,100
So, yeah, so their methods has like two disadvantages

723
00:45:00,100 --> 00:45:02,220
compared to our advantages.

724
00:45:02,260 --> 00:45:06,020
So one of them is that they cannot guarantee that this is...

725
00:45:06,020 --> 00:45:06,660
Right.

726
00:45:06,660 --> 00:45:10,460
100% logically satisfiable.

727
00:45:10,460 --> 00:45:13,740
And the other is that they have to retrain their model

728
00:45:13,740 --> 00:45:15,660
for all different constraints.

729
00:45:15,660 --> 00:45:17,300
Their model for all...

730
00:45:17,300 --> 00:45:18,540
Yes, okay, but...

731
00:45:18,540 --> 00:45:21,300
Okay, so suppose in their training data,

732
00:45:21,300 --> 00:45:23,860
they're only trained to kind of satisfy...

733
00:45:23,860 --> 00:45:28,380
They only have seen using, say, less than 10 keywords

734
00:45:28,380 --> 00:45:30,660
to generate a sentence, right?

735
00:45:30,820 --> 00:45:32,980
What if today I want to use 20 keywords?

736
00:45:32,980 --> 00:45:35,300
They would not be able to generalize well to that one.

737
00:45:35,300 --> 00:45:37,580
Right, so they're examples of toxicity, right?

738
00:45:37,580 --> 00:45:40,140
So they pretrain for toxicity and they can use that anywhere.

739
00:45:40,140 --> 00:45:43,180
So there's some tasks that you're just going to use repeatedly.

740
00:45:43,180 --> 00:45:45,340
I guess I see a combination of what you're doing

741
00:45:45,340 --> 00:45:46,620
and what they're doing together,

742
00:45:46,620 --> 00:45:49,100
which gives you this sort of entailment during the...

743
00:45:49,100 --> 00:45:51,580
Like this broader sense of satisfaction, right?

744
00:45:51,580 --> 00:45:54,340
Which I think was an interesting motivation.

745
00:45:54,340 --> 00:45:57,260
You didn't quite deliver on, but you could deliver on.

746
00:45:57,260 --> 00:45:59,500
That's actually the current project we're working on.

747
00:45:59,500 --> 00:46:01,460
So we're trying to combine the models

748
00:46:01,460 --> 00:46:04,780
that can handle soft constraints with our model, too.

749
00:46:04,780 --> 00:46:06,260
Okay, beautiful.

750
00:46:06,260 --> 00:46:08,940
And finally, the reason large types of models work so well

751
00:46:08,940 --> 00:46:10,940
is just because of attention, right?

752
00:46:10,940 --> 00:46:13,620
And so when you're using a hidden marker model, right,

753
00:46:13,620 --> 00:46:15,860
you're losing the power of attention.

754
00:46:15,860 --> 00:46:19,460
But I guess I'm looking at this and trying to convince myself

755
00:46:19,460 --> 00:46:21,940
that, well, you get all the attention in the right-hand part

756
00:46:21,940 --> 00:46:24,500
and then you just need a little bit of bias of selection

757
00:46:24,500 --> 00:46:28,020
in the left-hand part here, and that's what's happening.

758
00:46:28,020 --> 00:46:30,700
Yeah, so basically, intuitively, you can think of this part

759
00:46:30,700 --> 00:46:33,540
as only providing, like, guide and suggestions,

760
00:46:33,540 --> 00:46:38,540
leading the model, leading GPT to satisfy the constraint.

761
00:46:38,620 --> 00:46:40,620
But on this part, it's kind of responsible

762
00:46:40,620 --> 00:46:42,980
for fluency, grammar, and everything.

763
00:46:46,420 --> 00:46:47,540
Beautiful work.

764
00:46:47,540 --> 00:46:48,380
Thank you.

765
00:46:50,540 --> 00:46:51,380
All right, thanks.

766
00:46:51,380 --> 00:46:54,660
Maybe we should wrap up, and thanks again.

767
00:46:55,660 --> 00:47:01,300
And we can have lunch with the database folks

768
00:47:01,300 --> 00:47:03,300
and talk about generating SQL query.

