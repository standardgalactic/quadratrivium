start	end	text
0	5000	And we are going to see connections of language models
5180	8080	that maybe you did not quite expect to anticipate,
8080	9080	but they are very real.
9080	11920	And to start with, we are very lucky to have
11920	15880	Steven Pantadosi, he is a professor of psychology
15880	17520	and neuroscience at UC Berkeley.
17520	22520	And a friend of AI, we actually have a grant together.
23480	28480	And a lot of us in AI are excited and interested
29480	32080	in learning from the psychologists
32080	35200	and seeing how they can inspire our work.
35200	37320	Well, actually, this is a two-way street
37320	40320	and because the psychologists are also getting excited
40320	43280	about the language models to understand
43280	44280	something about humans.
44280	47840	And Steven will tell us about some of the work
47840	49800	he has been doing, which I think is very exciting.
49800	50640	So, Steven.
52720	55840	So thank you for the invitation to speak here.
55840	57560	I'm going to be talking about the meaning
57560	59040	in the age of large language models
59040	60640	and maybe finding meaning in the age
60640	61480	of large language models.
61480	64880	And this talk isn't a kind of technical talk
64880	68640	about language models or evaluation or anything.
68640	72040	It's almost closer to philosophy
72040	74360	or closer to kind of high level theories
74360	77200	in cognitive science and psychology,
77200	78880	which are about meaning.
78880	80680	And some of the kind of history of work
80680	84280	of people's ideas about where meaning comes from
84280	86720	in language or in semantics
86720	87800	and how we can think about those
87800	89480	in the context of large language models
89480	91640	and in particular in the context of debates
91640	93000	about large language models
93000	96240	and whether they're just stochastic parrots
96240	98920	that don't have any form of understanding
98920	101360	or whether there's some sense in which they have understanding
101360	104200	and if they do have some form of understanding,
104200	106800	if that form of understanding is at all related
106800	110800	to the types of understanding that people have.
110800	114680	So my plan for the talk is first to talk about meaning
114680	116120	and reference kind of generally.
116120	120080	I'll talk about some claims in large language models
120080	122840	and why people often think that there's no sense
122840	126640	of meaning or kind of real semantics in them.
126640	130840	Then I'll talk about some psychological theories
130840	132760	about meaning and how meaning arises
132760	135120	from what are called conceptual roles.
136000	137400	I'll come back to large language models
137400	141200	and talk a little bit about learning conceptual roles
141200	142360	in large language models
142360	144840	or in kind of general machine learning systems.
145080	147560	And then some very kind of brief overview
147560	151920	of a learning conceptual role experiment in people.
151920	156800	So let me start with kind of how I got interested in this
156800	160320	which was this paper by Bender and Kohler in 2020.
160320	163680	Emily Bender is a computer scientist
163680	167560	trained as a linguist also at the University of Washington
167560	169760	who people may know as a very vocal critic
169760	173480	of many aspects of large language models.
173520	176120	The one that initially I think interested me
176120	179720	was claims about the meaningfulness of large language models
179720	183600	and essentially arguments that there's nothing meaningful
183600	186760	at all in what statistical models
186760	189000	that are trained on text can do.
190080	192600	And Bender and Kohler came up with a very nice way
192600	197600	of making this point, what they call the octopus test.
197600	199560	The octopus test goes as follows.
199560	202440	So kind of starting point is for them,
202440	206000	meaning is an association between a word, say,
206000	208000	and something external to language.
208000	210120	Okay, so the meaning of the book
210120	214400	is some physical object, a book that's out in the world.
214400	215400	You can see that that's right
215400	217920	because that's how we label physical objects
217920	219320	and when we're learning words, right?
219320	221400	We hear the word book when there's books around
221400	222960	and we pick up on that association.
222960	224560	And so that's kind of fundamentally
224560	227920	what the, that kind of external reference, right?
227920	230680	To something in the real world is fundamentally
231560	232800	what the word means.
234120	235640	And Bender and Kohler say, okay,
235640	238520	let's take that as our definition of meaning
238520	240160	and let's imagine an octopus.
240160	243080	So an octopus who lives under the ocean
243080	246240	and has tapped into a communication channel,
246240	250520	say a telephone line between two islands, okay?
250520	251360	So the octopus is there,
251360	253240	it's eavesdropping on all of the communication
253240	255160	that happens between those two islands.
255160	257960	You can get a huge amount of linguistic input
258960	262160	and you could imagine very smart octopus
262160	265160	might be able to learn all of the statistical properties
265160	268440	of what's happening across that communication channel, right?
268440	272040	Might be able to learn, become very good at predicting text.
272040	274720	Maybe it could predict it optimally, whatever.
275720	278080	Their argument is that the octopus
278080	280000	could never actually learn the meanings, right?
280000	281320	Because it would never have access
281320	283520	to the physical reference, right?
283520	284360	So yeah.
284360	286520	Is it really different from the Chinese room?
286680	290040	Yeah, so it's interestingly a little different
290040	294160	from the Chinese room and maybe can I defer that question
294160	295600	to the end if that's okay?
296560	298200	Because I think what's going on with the Chinese room
298200	301160	might make more sense with what I say later, okay?
301160	303480	But very similar in spirit, I would say.
304920	308720	So this octopus has no access to the physical reference
308720	310080	and therefore couldn't solve tasks
310080	311800	involving the physical objects, right?
311800	314880	If you ask them to visually recognize what a coconut was
314880	317440	even if they knew all of the statistical properties
317440	319360	of where the word coconut would be used, right?
319360	322960	They wouldn't be able to solve the physical stuff, okay?
322960	324320	And of course this is the situation
324320	326400	that a large language model is in,
326400	329080	at least one that's only trained on text, right?
329080	330920	It doesn't get access to stuff from the world,
330920	335120	it only has kind of text, okay?
336200	339600	Therefore this octopus doesn't have meanings for the words.
340440	342120	Such an octopus is like a large language model
342120	343960	because they only have text.
343960	345440	And predictive ability therefore
345440	347320	can't give you the meanings, right?
347320	349400	Meaning is something just fundamentally different
349400	351760	than predictive ability.
353440	355480	Let me give you one other example of this.
355480	357880	Well, I'll just say, I think that on the surface
357880	360560	this is a somewhat convincing argument, right?
360560	365560	It's kind of compelling to think of meaning in this way.
365840	368960	And certainly if you do, it seems pretty convincing, yeah.
369800	372040	It may not be right in terms of your mathematical history
372040	374520	but it's given me a couple of axioms.
374520	377640	And you know, you're not supposed to have,
377640	380800	because this is really referring to
380800	383160	this particular interpretation of that one.
383160	385240	This would make that up in your mind.
385240	387840	Yeah, great, so hold on to that thought too
387840	389640	because a mathematician thinking about axioms
389640	392680	is very much related to another version of meaning
392680	396040	that I'll talk about in a few minutes, okay?
396040	399200	Even as Hilbert wanted it to be, right?
399200	402520	Hilbert's desire was to convert Euclidean geometry
402520	405360	to a set of axioms such that every symbol
405360	408120	could be replaced by some arbitrary squiggle
408120	410320	and the system should still work.
410320	414400	That is what Hilbert did in the axiomatization of geometry.
414400	419400	Yeah, so I think most mathematicians
419480	421680	would have a slightly different sense of meaning
421680	423760	but one which matches what I'll say
423960	425760	in a few minutes, so yeah.
426760	429720	Let me give you just another kind of gloss
429720	432060	on meaning and language models.
432060	435200	Here's Gary Marcus talking about lambda.
435200	437760	In truth, literally everything the system says is bullshit.
437760	440200	The sooner we realize that lambda's utterances are bullshit,
440200	442000	just games with predictive word tools
442000	444360	and no real meaning, the better off we'll be.
445480	447320	Software like lambda doesn't even try to connect
447320	448560	to the world at large, right?
448560	450880	That's what he thinks makes it bullshit.
450880	452840	Just tries to be the best version of autocomplete
452920	453760	that it can be.
453760	454760	They don't understand language
454760	457720	in the sense of relating sentences to the world
457720	461100	but just sequences of words to one another, okay?
462600	466600	So I got interested in this in part
466600	469440	because I find this view kind of compelling.
469440	471840	On the other hand, I also think it's kind of deeply wrong
471840	473640	and the way in which it's deeply wrong
473640	476320	is really interesting for what it has to say
476320	480040	about conceptual representations and meanings
480040	484160	in human minds as well as in machine learning models.
484160	489160	So a few years ago, I teamed up with Felix Hill,
490000	492840	who's a researcher at DeepMind
492840	497840	and wrote an article basically going through arguments
498200	500640	that meaning is not this form of reference, right?
500640	503560	So in fact, this idea that meaning should be equated
503560	506000	with some mapping to things in the world
506000	508280	has often been rejected by people in linguistics
508280	511040	and philosophy and cognitive science.
511040	513240	And I think for good reason,
513240	514840	so just to give you a kind of flavor
514840	518720	of why people often reject this,
518720	521120	there's many concepts, many words, for example,
521120	524320	that have no reference to the external world, right?
524320	526120	Function words are a good example of this.
526120	529040	Words like to or is or many, right?
529040	531680	There's no to, to, to out in the world
531680	533960	that that word refers to.
533960	537360	There's also no is out in the world or no many.
537400	539240	Those are function words in language
539240	542600	and what they do is actually much more like
542600	546720	kind of what an operator in mathematics or something does,
546720	547560	right?
547560	549840	These words have an internal meaning.
549840	553640	They control the kind of compositional meaning of a sentence,
553640	555880	meaning that they have to be composed
555880	558320	internally in linguistic representations
558320	559960	in order to express their meaning, right?
559960	563240	They're not pointing to something out in the world.
563240	565240	Even if you don't go to function words like this,
565280	568960	there's other words which are very hard to make sense of
568960	571680	in a kind of view that meaning is stuff in the world.
571680	575080	So you can think of very abstract words like justice, right?
575080	576800	There's probably not a justice out there.
576800	580120	That's some kind of construct that we have or wit,
580120	582560	or you can think of things that don't exist like dragons,
582560	583400	right?
583400	587240	There's no external thing in the world, which is a dragon.
587240	589360	There's even words and concepts we have
589360	592080	that have no possible reference to the world, right?
592080	594280	So if I think about an imaginary bicycle,
594320	596800	that's something which is by definition imaginary, right?
596800	600440	It's not out there, or a perpetual motion machine, right?
600440	603040	We can have a concept of a perpetual motion machine
603040	604920	and think about it and reason about it,
604920	607920	but there's certainly not one that exists out in the world.
609760	611840	Even for ordinary concepts,
611840	614880	we likely haven't even considered all of the possible things
614880	617360	which could be reference of those concepts, right?
617360	621680	So I could walk in wearing a shoe made out of eggplants
621680	624000	and you could look at them and everybody might agree
624040	626520	that they're shoes, right?
626520	627720	But you would agree that they're shoes
627720	630960	without ever having seen shoes made of eggplants before, right?
630960	632720	So there's some object in the world
632720	635160	which everybody would agree is a shoe.
635160	637140	Even though you've never encountered that thing before,
637140	638560	that means that it couldn't have been the stuff
638560	641560	in the world which determined whether it was a shoe, right?
641560	643720	Had to be some kind of more abstract conception
643720	646280	of what makes something a shoe, right?
646280	648040	You can think about things like the function
648040	651000	or the origin, how they're used.
651000	653200	These other kinds of properties of objects
653200	655760	seem to be much more important for the categorization
655760	658000	of the concept, yeah.
658000	661320	Well, here this is a little bit of a survivorship bias
661320	665400	because eggplants shoes might still be considered shoes,
665400	668120	but then ice cream shoes are probably,
668120	670360	nobody will recognize them as shoes, right?
670360	671880	But then you don't think about ice cream shoes.
671880	675440	So it's like the things that you can think of as shoes
675440	677440	are in your little bowl
677440	678800	and then you don't think about things
678800	679840	that are already too far.
679840	681920	So it seems like there is still kind of some distance
681920	684520	to the closest real object.
684520	685360	Yeah, yeah.
685360	690360	So all of this is not to say that the real objects
691240	692640	are irrelevant, right?
692640	695280	Like I agree that eggplants are much more plausible issues
695280	698840	than ice cream and that has to do with the kind of real
698840	702080	physical properties of those substances.
702080	703880	My point is just that the physical thing
703880	705600	is not the defining thing, right?
705600	707400	It's not something in the object you look to
707400	709080	to decide whether it's a shoe or not, right?
709080	711900	It's something more abstract about how it's used
711900	713300	or made or something like that.
715380	717380	I'll actually talk a bit in this paper
717380	721780	about the concept of a postage stamp, right?
721780	725060	Which is just an example of one that people probably
725060	728980	have some intuitions about where you could easily think
728980	731580	of postage stamps which are fundamentally different
731580	732940	than anyone's you've seen before.
732940	735180	You could think of one made of glass, for example,
735180	737900	or you could think of one that was an RFID tag,
737900	740500	which is probably physical incarnations
740500	742940	of postage stamps like that that everybody would agree
742940	745420	should be called a postage stamp.
745420	747940	And if you try to get people to define it, right?
747940	749180	You might say something like, well,
749180	750620	a postage stamp is something you pay for
750620	752260	and you put on a letter so that the letter
752260	753540	will be delivered by the government
753540	754620	or something like that, right?
754620	759620	So what the term means is intrinsically connected
759620	762900	to a bunch of these other terms like payment and letters
762900	765060	and being delivered and those things.
765060	767940	And in fact, if those terms change meaning, right?
767940	770980	So if, for example, people develop a new way
770980	773540	of paying for things, paying on the blockchain
773540	774380	or something, right?
774380	776280	Then you kind of know automatically
776280	779060	that a postage stamp can be paid for in that way,
779060	780500	at least in principle, right?
780500	783380	So it's not just that the word is associated
783380	785140	with those other things, but that its meaning
785140	787940	is inherently connected to those other things.
789940	794940	So that's one kind of take on why reference to stuff
798020	799060	out in the world, right?
799060	802860	Is not a good way of thinking about meaning.
802860	805500	Let me tell you what one alternative is.
805500	808460	Actually, before I do that, let me just show
808460	809620	a couple of other alternatives,
809620	811620	which I think are also not plausible,
811620	814020	but might be familiar to people, okay?
814020	816460	So what I just talked about is this kind of
816460	817620	world mapping view, right?
817620	819500	That there's some word and its meaning
819500	822400	is some physical object or some thing.
824100	825500	You can think about other kinds of views
825740	829140	of concepts and meaning might have a kind of
829140	830500	feature spacey kind of views,
830500	832020	port vector machines or something, right?
832020	834060	There's some abstract feature space
834060	836640	and a concept is some dividing line
836640	839100	or some region or something in this space.
841340	846180	That I think is maybe fine in some narrow applications,
846180	849980	but what I'll talk about next are cases of, say,
849980	852860	human cognition, which really don't fit well
852860	854820	into that picture, in the sense that things
854820	856660	are much more complicated for how people think
856660	859740	about concepts and their relationships.
859740	862060	People might also have this sort of hierarchy
862060	863380	or network view, right?
863380	865580	So sometimes people think, oh, sorry, you can't see this.
865580	867880	There's supposed to be lines connecting
867880	870100	one concept book in the middle to a bunch
870100	871460	of other concepts, right?
871460	874380	And you might, you know, there's old theories
874380	876980	of, say, semantic organization or very old,
876980	879100	old AI kind of approaches, right?
879100	883020	That think about building hierarchies of concepts
883940	886620	or sometimes networks of concepts
886620	889500	and trying to define meaning in terms of those relationships.
889500	892060	I actually think that both this and the feature
892060	895380	and the world mapping view have some of the,
895380	898620	some kind of useful properties or useful insights
898620	901340	about concepts, but just aren't quite the whole thing
901340	904020	for reasons that I'll talk about next.
904020	908500	So let me just start with, start trying to introduce
908500	911060	this kind of other view of concepts
911060	913580	by trying to get people's intuitions
913580	916140	on a recent news story, okay?
916140	920460	So here's a little recent news from the US versus Trump.
920460	922140	I think this is not the most recent indictment,
922140	923900	but one or two indictments ago.
925180	928140	If you look through it, you can read all about
929020	931620	Pence and Trump and efforts to manipulate
931620	933300	the election and things.
933300	936500	Here's a little paraphrase of one of the paragraphs, 90 C.
936500	938340	So Pence, the vice president, right,
938340	940740	opposed a Trump team lawsuit arguing
940740	943500	that the vice president could reject electoral votes.
943500	945660	So Pence didn't want them to argue
945660	948220	that he could reject electoral votes.
948220	950100	He said to Trump that he didn't have a constitutional
950100	952660	authority and that the action would be improper.
952660	955700	So it's according to Pence's notes at the time.
955700	958940	And Trump responded, you're too honest, okay?
958940	960660	According to Pence's notes.
960660	962460	So think about that situation and everything
962460	964780	you know about this context, right?
964780	967340	And think about an answer to a question like,
967340	969500	why did Trump say this?
969500	970340	Right.
971780	975940	You think about that, probably what's going on
975940	977260	as you think about it, right?
977260	980060	As you're thinking about lots of other things
980060	981980	and how they're related to this situation,
981980	983980	like what Trump was trying to achieve,
983980	986900	maybe what kind of personality Trump had,
986900	989060	what Trump was trying to do to Pence.
989060	990740	Is he trying to manipulate him
990740	993020	into taking some kinds of actions?
993020	995900	What exactly that action would have, right?
995900	998020	In terms of the election.
998020	1000820	Everybody is perfectly capable of reasoning through these
1000820	1005540	and coming up with kind of plausible causal story
1005540	1006580	about what's happening, right?
1006580	1008460	It feels like we can come up with our own
1008460	1012740	kind of internal explanations about events like these.
1012740	1015100	And in fact, that process of coming up
1015100	1018980	with internal explanations, interrelated kind of concepts
1018980	1023020	and meanings is one that people in developmental psychology
1023020	1025420	have been very interested in and excited about
1025700	1028820	as a theory of kind of human cognition.
1028820	1033180	So basic observation is that people form these
1033180	1036660	very richly interconnected systems of concepts, right?
1036660	1040060	All of the kind of interconnected stuff you would need to draw
1040060	1042620	in order to answer a question like that,
1042620	1045140	which feels totally, totally normal.
1045140	1047300	People sometimes call these intuitive theories, right?
1047300	1049820	You have some intuitive theory of how Trump is acting
1049820	1051540	or how the political system would work
1051540	1056140	or some intuitive theory of what Pence might be doing
1056140	1058140	or might be trying to achieve.
1058140	1059860	And these things are often compared
1059860	1061820	to theories in science, right?
1061820	1066820	So you can think of your theory of why Trump might do this
1067780	1070780	as kind of analogous to a little scientific theory, right?
1070780	1073300	It has some pieces, it has some relationships
1073300	1076100	between the pieces, it has some dynamics.
1076100	1078540	And maybe you can look at all of that
1078540	1080340	and kind of reason about it causally
1080380	1082660	as you might reason about any other kind of system
1082660	1084380	that you've encountered.
1085220	1087580	So the idea that people,
1087580	1090620	and maybe most notably kids do this is one
1090620	1094140	which has really been very popular
1094140	1098700	in cognitive development, championed maybe most prominently
1098700	1101460	by Alison Gopnik, who's a developmental psychologist
1101460	1103060	here at Berkeley.
1103060	1106180	Let me just give you a quick example of how kids,
1106180	1110220	how experiments with kids like kids sometimes go
1110420	1111260	in this domain.
1111260	1116260	So here's an experiment from LaZot and Gelman.
1117060	1119380	So kids are shown these two foxes, right?
1119380	1123220	Which you might notice are identical pictures, okay?
1124060	1126740	And then they're told things about these foxes
1126740	1130740	and asked what they could do in order to answer a question,
1130740	1131580	right?
1131580	1133940	So this is like a simple version of why did Trump say that?
1133940	1137220	You might be told that one is an animal and one is a toy,
1137220	1138060	okay?
1138060	1139820	So what could you do in order to determine
1139820	1143340	which one is an animal and which one is a toy, right?
1146820	1150340	In this experiment, kids will say that you should do things
1150340	1152020	like check the insides, right?
1152020	1153860	Like check their guts or whatever, right?
1153860	1156140	Open them up and see.
1156140	1159220	Or look at their behavior, right?
1159220	1161820	If it acts like an animal then you could use that
1161820	1164660	to figure out which one is the animal, which one's the toy.
1164660	1166180	Or look at their parents, right?
1166220	1170260	Like, you know, the animal will have animal parents
1170260	1171820	and the toy won't, right?
1171820	1173740	And importantly, they don't just say, yeah,
1173740	1175620	you can check everything about these.
1175620	1178180	They know, for example, that age is not relevant, right?
1178180	1181540	So they won't tell you that age would tell you
1181540	1184140	which one is an animal and which one is a toy.
1184140	1185900	It's worth pausing and just thinking about this
1185900	1189460	and what this means in terms of conceptual representations,
1189460	1190660	right?
1190660	1192820	Because you can think about your concept of what makes
1192820	1195460	something an animal or what makes something a toy
1195460	1197460	and kind of like the postage stamp example, right?
1197460	1199540	It's intrinsically connected to these other things,
1199540	1202300	like what parents are or what's going on
1202300	1204340	with your guts inside, right?
1204340	1206020	Or what your behavior is, right?
1206020	1207900	That concept is just intrinsically linked there
1207900	1211500	and kids, I think these are preschoolers know that
1211500	1213260	from a pretty young age.
1214660	1216340	Gals asked them a question like one is a dog,
1216340	1219660	one is a wolf, what would you do, okay?
1219660	1221180	Kids basically say the same things there.
1221180	1223460	You could check the insides, you could look at behavior,
1223460	1224700	you could look at their parents,
1224700	1228380	see if they had a dog parent or a wolf parent.
1228380	1230140	Some of these are actually kind of interesting, right?
1230140	1232620	Because I don't think anybody knows,
1232620	1235980	at least I don't, what you would look for on the insides
1235980	1238020	to distinguish a dog versus a wolf, right?
1238020	1241140	Like maybe you could go down to DNA
1241140	1245060	or I'm sure you could go down to DNA to tell that.
1245060	1246660	But people have the intuition that like, okay,
1246660	1248380	there's something about being in this category
1248380	1250660	which depends on these other aspects
1250660	1252420	of being in the concept.
1253300	1254740	To me, I read this much simpler.
1254740	1257340	It's like basically what they're saying is look,
1257340	1259500	like all of this are visual things.
1259500	1261420	It's just that you're trying to project it into language
1261420	1264500	but actually what the kids are probably meaning is,
1264500	1266060	you will know it when you see it
1266060	1267980	than when you play with it, right?
1267980	1270300	It's vision and interaction.
1271180	1272020	Yeah, yeah.
1272020	1273580	And age is neither.
1273580	1274420	Yeah, yeah.
1274420	1279100	So I think it's true that, yes, all of these are visual cues.
1279100	1282140	I don't know of experiments that look at non-visual cues
1283380	1285140	but I agree, yeah, that's interesting.
1285140	1286420	I'll give you one other example
1286420	1290580	where they know that there's no cue, right?
1290580	1292140	So if you tell them that one is named Amanda
1292140	1294500	and one is named Melissa,
1294500	1297220	then they'll reject all of these as tests, right?
1297220	1299020	They'll say, okay, the insides are not gonna tell you
1299020	1302020	which one is Amanda, the behavior and the parents
1302020	1304820	and the age and these things are not going to, okay?
1304820	1306340	So all of this is just to say
1306340	1309780	that people have a, even kids, right,
1310060	1312060	have pretty sophisticated theories
1312060	1315260	of how concepts relate to other concepts, right?
1315260	1317380	And in fact, in a situation like this, right,
1317380	1320980	there's nothing visual apparently that could tell you, right?
1320980	1322620	So it's not a visual discrimination task.
1322620	1325340	It's really a kind of conceptual one
1325340	1327660	that's asking you to look at other kinds
1327660	1329300	of conceptual features and things.
1331060	1334580	So people have these intuitive theories
1334580	1336900	then one kind of proposal,
1336900	1339380	quite a few people have argued for is that meaning arises
1339380	1341940	from essentially the role that a word or a symbol
1341940	1344660	or a concept plays in this theory, right?
1344660	1348580	Like the meaning of animals really just intrinsically related
1348580	1351500	to these ways of testing it and these kinds of features
1351500	1353540	and all of the other things
1353540	1356460	that are not kind of simple semantic associates with animal
1356460	1359620	but are kind of deeply connected
1359620	1362620	in the sense of an intuitive theory.
1363460	1364860	I was trying to come up with examples
1364860	1368460	where this, you know,
1368540	1370500	we'll give people this intuition, right,
1370500	1374020	that, you know, if you try to define these words,
1374020	1377060	if you try to define what an indictment is, for example,
1377060	1378340	it's very hard to do it in a way
1378340	1380660	that doesn't reference other legal terms
1380660	1382420	and other kind of social constructs
1382420	1387100	and concepts that you already have, right?
1387100	1388540	It's kind of intrinsically related
1388540	1391460	to the system of other concepts and terms, right?
1391460	1393580	Chord change is kind of like this too in music, right?
1393580	1395380	You have to talk about chords and notes
1395860	1398940	and circle of fifths or whatever, right?
1398940	1400860	Like these things are just intrinsically related.
1400860	1403580	I think force in physics is like this.
1403580	1405820	It's very hard to talk about it in isolation
1405820	1408300	independent of, you know, experiments
1408300	1409740	or other concepts or things.
1409740	1411060	Or if I said, like, what does a bobbin do
1411060	1412700	in a sewing machine, okay, right?
1412700	1414340	Like you have to talk about thread
1414340	1416060	and you have to talk about the processes of sewing.
1416060	1417220	Just the meaning of these things
1417220	1419980	are just all intrinsically linked together.
1421260	1424500	So this idea that meaning,
1424540	1426020	not about reference, it's about the role
1426020	1428740	that something plays is called conceptual role theory.
1429660	1431220	Meaning of a word or concept is determined
1431220	1433020	by the role it plays.
1434380	1439380	And this has been argued for,
1439820	1441940	I think maybe most prominently by Ned Block
1441940	1444220	who's a philosopher of mind,
1444220	1447100	who wrote one of my favorite paper titles,
1447100	1450260	Advertisement for a Semantics of Psychology,
1450260	1451900	which is basically all about, you know,
1451900	1454740	how psychology needs a theory of meaning
1454740	1456620	and a theory of semantics.
1456620	1458180	And this idea of conceptual role
1458180	1460020	is something that could do that.
1460020	1462380	So it can explain kind of where meaning comes from.
1462380	1465660	It can address questions of how meaning depends
1465660	1467340	on things like your representations
1467340	1469300	or categories that you know,
1469300	1471380	can play nicely with compositionality
1471380	1474620	or other aspects of language.
1474620	1476540	And I think maybe most compellingly
1476540	1478980	can explain how you could find meaning in brains, right?
1478980	1480260	So if you open up a brain
1480260	1482460	and you start recording from neurons,
1483740	1485100	you know, it's really unclear what it means
1485100	1486780	for there to be reference in there,
1486780	1489260	reference to the external world in there.
1489260	1490740	But maybe you could kind of make sense
1490740	1493340	of patterns of activity in a way
1493340	1497900	that lets you kind of interpret systems
1497900	1501100	of signals and representations.
1502420	1505020	Let me give you just one other example of this
1505020	1508820	that maybe might make things more clear.
1508820	1510940	This idea of conceptual role semantics,
1510940	1515820	I think is also how meaning works in, say, a computer, okay?
1515820	1517300	Also, I think in mathematics,
1517300	1519500	which is why I was deferring the questions
1519500	1521180	about mathematics,
1521180	1522540	but you could look at something like this, right?
1522540	1525660	This is a floating point representation
1525660	1528260	and ask what makes the bits in this representation
1528260	1529540	mean what they do, right?
1529540	1532540	In particular, what makes the first bit mean the sign bit,
1532540	1533380	right?
1533380	1534780	It's nothing about being the first one
1534780	1536900	because there've been dozens of different conventions
1536900	1537940	for floating point numbers
1538060	1541660	which put the sign bit in all kinds of different places, right?
1542740	1545180	What makes it mean the sign bit
1545180	1547780	is how it interacts with all of the other operations
1547780	1551020	that you can do with floating point numbers, right?
1551020	1551980	So meaning, in some sense,
1551980	1554260	comes from the interaction between symbols,
1554260	1555900	or in other words, their conceptual role.
1555900	1558660	So in particular, like what does negation do, right?
1558660	1561660	If I have a negation operator, okay, it flips the sign bit.
1561660	1566420	Great, okay, that's where it gets its meaning from, right?
1566420	1567340	Or what does addition do?
1567900	1570580	The right thing with respect to the sign bit
1570580	1574180	or multiplication or rounding or whatever, right?
1574180	1577140	So what makes this the floating point representation
1577140	1580380	or what makes that first bit represent sign
1580380	1582820	is nothing intrinsic in the representation itself.
1582820	1585100	It's how it interacts with all of the other components
1585100	1587900	of the system, okay, yeah?
1587900	1590420	You explain the difference between this way of thinking
1590420	1593300	about the sense of here's a hierarchical approach
1593300	1595980	where there's concepts and there's sort of numbers.
1596980	1601980	Yeah, so I think that there's certainly concepts people have
1603300	1604700	that are hierarchical, right?
1604700	1606860	So we know that dogs are kind of animal
1606860	1609340	and animals are a kind of living thing.
1609340	1612020	I think what that kind of picture is missing
1612020	1614540	is that our representations are actually,
1614540	1617140	like computational objects, like they do something, right?
1617140	1621140	They interact with each other and they allow us to solve
1621140	1622700	certain kinds of inference problems
1622700	1625580	and all of the stuff you could do with your concepts
1625660	1627820	like the Trump example, right?
1629260	1632220	Yeah, so the claim is not that they're not hierarchical, right?
1632220	1635620	It's that the interesting important things they do
1635620	1638340	come from interactions kind of internally between concepts
1638340	1641020	much like the way that the sign bit is interesting
1641020	1643620	or important here comes from its interactions
1643620	1646620	with things like negation and multiplication, yeah, yeah.
1648220	1650420	What you're saying here is perfectly good,
1650420	1653420	but what I have trouble with is buying this
1653460	1656420	as an exclusive theory of semantics.
1656420	1658980	Just like you gave good arguments
1658980	1663260	against that meaning is just reference, okay?
1663260	1665540	I think you demolished that theory,
1665540	1669740	but now you put up another theory which also I find
1669740	1671340	that it has some good aspects,
1671340	1675140	but to make that an exclusive theory is problematic.
1675140	1677340	So if we look at children growing up,
1677340	1680540	there are these studies on sort of concreteness judgments.
1680540	1683140	So the vocabulary of a child at two,
1683140	1686660	there are a lot of words in there like milk and bottle
1686660	1690100	and jump and sit and so forth, which are very concrete,
1690100	1692020	concrete in a visual sense,
1692020	1694180	concrete in a motor program sense.
1695260	1699020	At the age of 10, they have words like justice and fairness
1699020	1702380	and so forth, which are very much,
1702380	1705300	which fit much better into this conceptual road story
1705300	1708380	where is the vocabulary of a child at two,
1708380	1711460	maybe one where this kind of groundedness
1711460	1714940	to sensory motor experience is a much better account.
1714940	1717460	And this is not problematic for me.
1717460	1722060	Why do we need to have one exclusive theory for meaning?
1722060	1725860	Both of these are aspects of meaning.
1725860	1727300	Yeah, I agree.
1727300	1732300	So I think that you can think of the physical reference,
1732820	1734700	as in some sense one of the conceptual roles
1734700	1735940	that something can have.
1737300	1738220	It is important.
1738220	1741740	I'm not sure we know kind of how abstract kids early meanings
1741740	1743980	are for those kinds of words,
1743980	1746340	because it has to be a little bit abstract
1746340	1748940	because you'll call a new bottle that you see a bottle still.
1748940	1751940	So you have some abstraction away from the examples
1751940	1753380	of bottles that you've seen,
1753380	1757100	but I agree it feels early on very concrete
1757100	1760380	and much less abstract than things we come later.
1763260	1765420	Just trying to make sure I understand
1765420	1766700	what this theory is saying.
1766700	1769700	So is there a character to think of this
1769700	1772340	that you're saying that meaning is basically
1772340	1776540	like some homomorphism onto some either intuitive
1776540	1778080	or formal theory?
1779340	1780260	Ah, sure.
1781180	1784460	So then maybe a follow up question is like,
1784460	1786780	how do we know which homomorphisms are valid?
1786780	1788220	Because I could always,
1788220	1791300	if I can have some arbitrary correspondence mapping,
1791300	1793780	I could make anything correspond to anything else.
1794780	1796540	You know what's so loud here?
1796540	1799420	Yeah, so I don't think anybody has been that formal.
1799420	1802020	People like Putnam have made this kind of argument
1802020	1804420	about understanding computation in physical systems,
1804420	1806700	basically saying like physical systems,
1806700	1809300	like a brain or in his example, a wall, right?
1809300	1812980	Are so complicated that I could come up with
1812980	1815260	kind of any mapping back and forth between the states of it
1815260	1819100	and the states of the kind of arbitrary computational system.
1819100	1822300	And that's probably a much longer thing to get into.
1822300	1824540	I'll just say that I don't think I have a very easy answer
1824540	1825740	about that, right?
1826740	1828980	I think of this as not kind of,
1830180	1832340	certainly not formalized in that sense,
1832340	1835160	but in sort of a higher level in terms of like
1835160	1837740	what kinds of theories we should be looking for, right?
1837740	1838940	And then there's lots of work to do
1838940	1841780	in terms of making that precise.
1841780	1842820	So yeah, yeah.
1842820	1844660	Yeah, Quine actually uses that example
1844660	1849220	to motivate this kind of theory in like 1950s philosophy.
1849220	1850380	Sorry, which example?
1850380	1852100	Quine uses this example.
1852100	1853580	He uses an example of Gavagai.
1853580	1856100	You see something popping out and you're like,
1856100	1858620	how do you know Gavagai means rabbit, not running,
1858620	1860660	and not hole, and not something else
1860660	1865100	because the real world doesn't determine what a meaning is.
1865100	1866260	Yeah, yeah.
1866260	1867100	Yeah.
1867100	1869700	The hierarchical concepts and semantics
1869700	1871860	have been extensively terminated,
1871860	1873180	scripting orders and so on.
1873180	1875660	Has any of this been operationalized at all?
1875660	1877580	Can you comment on that?
1877580	1878780	I don't think so, yeah.
1878780	1880620	So I mean, I can talk,
1881620	1884580	I have a couple of examples of kind of learning
1884580	1889380	intuitive theories, which essentially have this kind
1889380	1893740	of character, so of taking data and then trying to come up
1893740	1897340	with some structures that obey the right relations, right?
1897340	1901180	And yeah, I'll talk a little bit about that,
1901180	1905860	but there hasn't been a ton of work on that, so yeah.
1905860	1908660	Could you talk about how this theory deals with
1908660	1911900	when the same symbols or words are in different kind
1911900	1913900	of theories or settings?
1913900	1916900	Is it kind of mean that the symbols themselves
1916900	1919260	don't have meaning or how are the kind of the meanings
1919260	1921140	shared across different contexts?
1921140	1925620	Yeah, so that's an interesting question
1925620	1928140	that I think people have not resolved very well.
1928140	1933140	So your symbol for your father might play
1934420	1936140	a bunch of different roles, right?
1936140	1938700	Because you know what job your father has
1938700	1941300	and you know what family relations and you know
1941300	1945940	what hobbies and I don't think that there's good
1945940	1948940	kind of formalized accounts of how to make sense
1948940	1951540	of all of that, so there's not great theories
1951540	1953580	of kind of formalizing conceptual roles.
1954820	1957420	I'll give some arguments why I think it's possible
1957420	1959260	that language models are doing this at least
1959260	1962580	in a tiny version, but in terms of like rich
1962580	1964740	and kind of human like conceptual roles,
1964740	1967060	I think that's one of the key problems that's hard to solve,
1967060	1969740	so, is there another one?
1969740	1970580	Yeah.
1970580	1975260	I think that the oncology of the kind of physical world
1975260	1978140	people think that we're using right now, for example,
1978140	1983140	is a subset of the oncology of a human's mind
1985300	1988380	and probably also a subset of all possible
1988380	1992500	future invented concepts and so on.
1993340	1995660	Sorry, what was the, I missed the very first part,
1995660	1996660	what was the question part?
1996660	1999020	So the question is whether you think
2000300	2003940	that the existing ontology that you are using now
2003940	2007420	is a subset of the ontology of human's mind
2007420	2010540	that we haven't fully explored
2010540	2013540	and probably that is also a subset of what kind of
2013540	2018340	can be inventive or creative produced concept
2018340	2019620	that you mentioned in the beginning.
2019620	2022300	By ontology, do you mean these theories?
2022300	2025140	I mean terms, for example, yeah, concepts.
2025140	2026140	Yeah, concepts.
2026140	2029140	I mean, I don't think any of these
2029140	2030940	is quite the right answer, right?
2030940	2034260	Like these things are actually very difficult to figure out,
2035700	2037180	but I think they're kind of pointing
2037180	2039420	in some useful directions or something.
2039420	2043220	So I don't know if that answers your question, but yeah.
2043220	2045740	And if you know the symbols that you're using,
2045740	2046580	why does it matter?
2046580	2049700	Because that's not to define certain meaning,
2049700	2054700	whether you use words to represent or you find to represent,
2055660	2056980	it doesn't matter, right?
2058100	2062380	In terms of which symbols, like mental representations or?
2062380	2065860	Yeah, without a concept, whether you use the words
2065860	2068860	to represent that or you think that's fine,
2068860	2071300	you think that's all that comes out.
2071300	2073740	Or like, I'm sure that this kind of concept
2073740	2076380	has a lot to do with the form,
2077340	2079700	but I think it's just a little bit of a question.
2079700	2081220	Yeah, so yeah.
2081220	2084620	Just to make sure that you, how far do you still have to,
2084620	2085940	like do you have any?
2085940	2087300	I have a little ways to go.
2087300	2090660	Okay, so maybe we should push this the word
2090660	2091900	after, after at the end,
2091900	2094780	because it seems like a deeper discussion.
2094780	2096420	Yeah, yeah, okay, great.
2098620	2103620	Okay, so I talked about these kinds of accounts of meaning,
2104340	2108780	particular meaning as conceptual role.
2108780	2111460	And let me talk a little bit about learning conceptual roles
2111460	2115820	and why we might think that's plausible or useful.
2117740	2120540	Seems to me at least that large language models
2120540	2122620	almost certainly need to learn some of these pieces
2122620	2124340	of conceptual role,
2124340	2126140	that these kinds of things seem really necessary
2126140	2129020	for the stuff large language models are good at, right?
2129020	2131860	Writing coherent texts or doing translations
2132220	2134580	or providing definitions or providing elaborations
2134580	2136700	or explanations, all of those things require you
2136700	2138740	to put symbols in the right relationships
2138740	2140740	with other symbols, right?
2140740	2144660	And that means that to do those things well,
2144660	2146700	you essentially have to have some little components
2146700	2148540	of conceptual roles, right?
2151620	2153780	One way to think about this is that human meanings
2153780	2156660	or human conceptual roles generated the text, right?
2156660	2159900	So maybe a smart inferential model could invert that
2159900	2162660	and figure out what were the likely conceptual roles
2162660	2165980	that generated the thing that I saw.
2165980	2167500	I like this, the Stringer quote, right?
2167500	2169220	The structure of sentences serves as an image
2169220	2170540	of the structure of thoughts, right?
2170540	2173820	Some projection of our thoughts or our meanings,
2173820	2176740	our conceptual roles that gets realized into sentences.
2176740	2177580	Yeah.
2177580	2178980	Yeah, so are you gonna follow up on something
2178980	2181460	that was a great, a recontextuality kind of example?
2181460	2184700	Is that a recontextuality kind of thing?
2184700	2185540	Which examples are you talking about?
2185540	2186660	Are you gonna recontextualize,
2186660	2188500	let's say a gentleman's two boxes example,
2188500	2190420	or is that a hyper-projection?
2191860	2193940	No, I wasn't gonna go back to that.
2193940	2196380	Yeah, but I'll talk about a study
2196380	2200020	in Large-Range Models in a minute, okay.
2201260	2202980	Okay, a lot of people have the intuition
2202980	2203820	this is not possible.
2203820	2206740	I think this is kind of the Bender and Marcus intuition
2206740	2208380	that our thoughts really get projected
2208380	2210780	into this kind of impoverished sequence of sounds, right?
2210780	2212180	How could you discover something
2212180	2215740	like rich conceptual roles there, right?
2215740	2218060	If you just have this projection of language,
2218060	2220340	how could that ever support rich and interesting
2220340	2222220	kinds of conceptual roles?
2224580	2227260	One kind of way that I think is a helpful analogy,
2227260	2230180	although not kind of a mathematically precise
2230180	2231860	implementation or something,
2231860	2233660	people may know these embedding theorems
2233660	2237260	from dynamical systems, which I think are very cool.
2237260	2240140	There's this paper called Geometry from a Time Series,
2241140	2243140	which essentially shows that in some cases
2243140	2245900	you can take projections of dynamical systems
2245940	2248700	and recover things which capture the structure
2248700	2251420	of the dynamics from that projection.
2251420	2253300	So in particular, in this paper, they go through this,
2253300	2255660	which is the Rossler Attractor.
2255660	2258940	This is a three-dimensional system
2258940	2261340	of differential equations.
2261340	2264140	And what you can do is take a one-dimensional projection
2264140	2265220	of those dynamics.
2265220	2267060	So you can look at just the X location
2267060	2269580	of what's happening there.
2269580	2273180	And through a clever trick essentially translating
2273180	2276060	the one-dimensions into three-dimensions
2276060	2281060	using, by going backwards in time, some number of steps,
2281260	2283060	you can actually recover the structure of this
2283060	2284900	from the one-dimensional projection.
2286780	2289420	And there's other kind of general theorems
2289420	2290820	about when this is possible,
2290820	2293340	Parkinson's embedding theorem and things like that.
2293340	2297580	The point here is that we shouldn't really have
2297580	2299300	strong intuitions about what's possible
2299300	2301620	from some projection of thoughts,
2301620	2304980	because oftentimes there might be possible
2304980	2308100	for people to, or for learning models
2308100	2311420	to reconstruct kind of interesting parts
2311420	2313820	of the structure of some system
2313820	2317220	just from simple kind of measurements of that system.
2317220	2321100	Actually Shaw here, the senior author wrote an entire book
2321100	2324300	on recovering the kind of dynamical properties
2324300	2325620	of a dripping water faucet,
2326840	2328940	where you can measure the time between drips
2328940	2331340	and figure out things about the kind of latent variables
2331340	2332940	and latent structures they're using techniques
2332940	2334460	that are a lot like these.
2335820	2338220	In psychology, actually people have also been interested
2338220	2341340	in kind of closely related types of models.
2341340	2346000	So there's this work by Roger Shepard in the 80s,
2346000	2349020	which essentially would take behavioral judgments
2349020	2352780	and try to infer the underlying structures behind them.
2352780	2357260	So for example, this matrix here is different colors
2357260	2358700	or different wavelengths of light
2358700	2362260	and then confusability between them on judgment tasks.
2362260	2364100	So just how similar are these things
2364100	2366300	or how confusable is one color with another.
2367380	2369960	And Shepard was using multi-dimensional scaling
2369960	2375020	to go from data like this up to representation like this,
2375020	2377180	which you might recognize as a color wheel.
2377180	2380660	Basically you can arrange points
2380660	2382380	so that their distances correspond
2382380	2385700	to the distances in the confusion matrix
2385700	2388220	and therefore recover something
2388220	2389500	about the kind of underlying,
2389500	2391540	in this case, psychological structure
2391540	2393180	that generated that data.
2395860	2399460	People have also done similar kinds of things
2399460	2402980	in learning kind of real formalized versions
2402980	2406620	of theories or of intuitive theories.
2406620	2409300	I really like this paper by Tomer Ullmann
2409300	2411940	and Noah Goodman and Josh who's speaking next
2411940	2414100	on learning a theory of magnetism.
2414100	2415780	So basically you take observations
2415780	2420780	of which objects interact with other objects
2420860	2425860	and do some learning to acquire a kind of high level theory
2426340	2430120	of the fact that there are two different kinds
2430120	2433360	of magnetic objects and those two different kinds
2433360	2434600	of things will interact with each other,
2434600	2435740	but they won't interact with things
2435740	2437380	that are non-magnetic.
2437380	2440020	So this is like a little tiny mini intuitive theory
2440020	2443060	that you can acquire just from very simple,
2443060	2444580	you might think kind of impoverished data
2444580	2446240	about interactions.
2447740	2451640	So when people talk about LOMs just being based on text,
2451640	2452980	I think that isn't really enough
2452980	2455620	to conclude anything about what theories they might induce,
2455620	2457140	or what kinds of internal structures
2457140	2461020	and conceptual roles they might induce from that text.
2461020	2462460	And in fact, there's some evidence I think
2462460	2467460	that what they are inducing looks pretty plausible
2467780	2469860	at least in kind of simple domains.
2469860	2474700	So there's this paper by Grandin and colleagues
2474700	2478020	which essentially looked at word embedding vectors
2478020	2481980	and projected them onto say intuitive dimensions.
2481980	2484580	So here you have a bunch of words,
2484580	2486540	you project them onto this line,
2486540	2489020	which is the line connecting small and large.
2489020	2491900	Okay, so all of our high dimensional word vectors
2491900	2494780	get projected onto the small versus large line.
2494780	2496780	And we take that as a way of measuring
2496780	2499660	how large versus small different objects are.
2500580	2502020	And then the question is,
2502020	2506020	is in a model trained only on text prediction,
2506020	2511020	is that, does that projection recover anything human like
2511120	2514180	about the underlying conceptual spaces?
2514180	2515300	And they show yes it does.
2515300	2518780	So here's six plots where the x-axis
2518780	2521180	is the semantic projection, right?
2521180	2523860	So how far on that small to large line something is.
2523860	2525800	And then the y-axis is human ratings
2525800	2529280	of how small versus large an object is, right?
2529280	2531080	You can see that the correlations here are not perfect
2531080	2533080	but they're also not garbage, right?
2533080	2537960	They're actually quite strong I think for a model like this
2537960	2541020	that things which the model calls wet versus dry
2541020	2544400	or big versus small or dangerous versus safe,
2545440	2547900	people also agree with, right?
2547900	2549740	So just in predicting text,
2549740	2552080	this thing has recovered these kinds of aspects
2552080	2554720	of semantic structure latent
2554720	2557680	in the word vector representations, yeah.
2557680	2560360	But this is probably sitting there in n-grams
2560360	2562760	and in bi-grams in fact, that information, right?
2562760	2565360	It doesn't have to do anything with the real world.
2566360	2568440	Well, it does have something to do with the real world.
2568440	2572760	It's like a small and large
2572760	2575080	could just be linguistic constructs
2575080	2577320	and you're testing it on language.
2577320	2580200	Oh, I see, you think it's that you say small tiger
2580200	2581880	versus large tiger or something.
2582320	2584160	Small puppy, right?
2584160	2586680	Puppy is always small, tiger is always large, yeah.
2586680	2589960	So it might be true in n-grams, I'm not sure.
2589960	2592160	I don't think that they looked at that.
2593800	2598640	I don't think that defeats the argument though, right?
2598640	2600960	Because I think it is the case that
2602080	2604200	even n-gram statistics are statistics
2604200	2606400	about word relations, right?
2606400	2608240	So it might be that you don't need fancy language models
2608240	2609720	or something to do this.
2610200	2611360	But you're not actually like,
2611360	2613600	you don't need real world for this to work.
2615120	2617920	Well, the real world generated
2617920	2620840	how often you hear small puppy versus large puppy, right?
2620840	2624920	So the real world is mirrored in those statistics
2624920	2627400	and then the configuration that system comes up with
2627400	2630520	is also one that mirrors those properties of the real world.
2630520	2631360	Yeah.
2632640	2633960	Can I put back on that as well?
2633960	2636320	I do kind of feel like what others are saying is right,
2636320	2638720	but this is much closer to n-grams
2638720	2640680	than it is to large language models.
2640680	2643440	And the properties we're seeing are just so much wilder
2643440	2647240	than any of these embedding tricks in practice.
2647240	2648520	Oh, you mean that large language model
2648520	2650160	is much smarter than this?
2650160	2652880	I don't even see how this is comparable in a way, right?
2652880	2655320	Like this pops out of PCA,
2655320	2658040	whereas we're seeing these wild emergent behaviors
2658040	2658920	come out of large language models.
2658920	2660400	Yeah, yeah, so I mean,
2660400	2662680	I don't think this explains wild emergent behaviors.
2662680	2665120	I think that this was just trying to say
2665120	2667840	that when you train on text prediction,
2667840	2669480	you configure yourself to align
2669480	2671840	with some of the true properties of the world,
2671840	2673960	which are reflected in the text analysis.
2673960	2674960	That's all, yeah.
2677320	2678320	Okay, so I'm short on time.
2678320	2679160	I'll skip this.
2679160	2680840	I'll just say that there's other papers
2680840	2685600	looking at transformers and kind of how they relate
2685600	2688800	to classic studies on concepts in cognitive science,
2688800	2691360	classic kinds of effects, but I'll skip that.
2692880	2694080	Maybe I'll go very briefly
2694080	2696200	just through this kind of fun experiment.
2696200	2699080	This is Mark Gorenstein in my lab
2699080	2702720	has been interested in learning concepts
2702720	2705880	just from linguistic experience,
2705880	2707760	maybe linguistic prediction.
2707760	2710360	He's been doing these kind of cool experiments
2710360	2714400	where we give people passages of natural language
2714400	2716400	where there's some blanks.
2716400	2718120	So here's a passage.
2718120	2719640	The myth of blank is so powerful
2719640	2721160	that the very words conjure up blank,
2721160	2725040	of strudel and blank in a cozy Vietnese cafe, blah, blah, blah.
2725040	2727720	And the job of participants in this
2727720	2731000	is to learn where to put the word DAX.
2731000	2732240	Okay, so DAX is a novel word
2732240	2734160	they've never encountered before.
2735360	2738440	You have to read this and understand the context and stuff
2738440	2742760	in order to figure that out and see where DAX should go.
2742760	2747440	Secretly behind the scenes, this example has been chosen
2747440	2751600	as just from a big corpus of text of a really rare word
2751600	2753400	that people probably don't know.
2753800	2756600	So the rare word here is soccer tort.
2757520	2760240	And that means that this language,
2760240	2762840	like where soccer tort actually occurred here,
2763840	2765960	was generated from real people
2765960	2767960	and presumably reflects the underlying meaning
2767960	2769680	and things of soccer tort.
2770680	2773200	Maybe I don't know if I'm saying that correctly.
2773200	2775560	But with enough examples of these people
2775560	2776880	will learn where DAX is,
2776880	2779080	they get feedback on whether or not they were correct,
2779080	2780960	according to whether they chose the place
2781000	2783640	where soccer tort actually appeared.
2783640	2787080	Okay, so we're having them do kind of a version
2787080	2788880	of a prediction task,
2790000	2793560	trying to figure out where this word goes,
2793560	2794920	but they don't actually see the word,
2794920	2796760	they see it as DAX.
2798520	2800440	People get pretty decent at this,
2800440	2804760	so up to 80% or so, depending on the word.
2806080	2808960	These are just, sorry, these are the examples of the words
2809000	2811600	which generated the unseen context.
2812760	2814400	And after that, we asked them a bunch of questions.
2814400	2816400	So we asked them some reading comprehension,
2816400	2818840	we asked them feature questions about DAX,
2818840	2820320	is DAX a man-made object?
2820320	2822240	Do biologists typically study DAX?
2822240	2824200	Do people use DAX in painting?
2824200	2826960	Just a whole collection of basic feature,
2826960	2828800	kind of concept-y questions.
2828800	2830680	We give them an image recognition task,
2830680	2833160	right, picking out soccer tort,
2833160	2837040	the real thing versus alternatives.
2837280	2839720	And we also asked them for explicit definitions, right?
2839720	2841440	These contexts are not definitions,
2841440	2843640	they're not saying here's what a soccer tort is,
2843640	2847560	it's naturalistic usages of the object.
2847560	2850080	And what we find actually is within,
2853360	2856600	within about 20 trials or so,
2856600	2859240	sorry, everything is after 20 trials,
2859240	2860760	people are actually very, very good
2860760	2865240	at judging conceptual features for these concepts,
2865280	2868040	almost at ceiling in most of the kinds
2868040	2870560	of feature questions we asked them.
2870560	2874200	Here's each word on a row,
2874200	2877640	and then features here on the x-axis,
2877640	2880080	almost everything is read, meaning they're good at this.
2880080	2885080	They're also good at picking which picture is the object,
2885080	2886760	so they've never encountered any pictures at all,
2886760	2890920	but they're 80% or so good at picking these things out.
2890920	2892760	And they're even good at giving definitions
2892760	2894240	for these terms, okay?
2894240	2896680	So here's a dictionary or dictionary
2896680	2898440	or something definition of soccer tort,
2898440	2900800	it's a chocolate cake or tort of Austrian origin
2900800	2904040	invented by Franz soccer supposedly in 1832
2904040	2906600	for some prince in Vienna.
2906600	2908120	And people just from these contexts,
2908120	2910320	20 of them will learn things like it's a chocolate dessert
2910320	2911760	similar to a cake that was originally
2911760	2913600	and most commonly made in Vienna.
2913600	2914800	I think it's a type of chocolate cake
2914800	2917280	that can be ordered for dessert in Austria,
2917280	2919760	kind of rich chocolate cake from Vienna and so on, okay?
2919760	2924760	So people are pretty good at taking
2926520	2928200	kind of in-context language use
2928200	2930320	and figuring out underlying aspects
2930320	2932680	of conceptual representation from that.
2936120	2937760	But they will never be able to taste it.
2937760	2938920	I mean, it tastes so good.
2938920	2940920	Have you had it?
2940920	2943800	I've never heard of it, so okay.
2943800	2945080	It's great.
2945080	2946800	Okay, so let me just wrap up here.
2946800	2951800	So I think of these kind of conceptual roles or theories
2952280	2954080	as really both a strength and a weakness
2954080	2957200	of these current large language models.
2958040	2959520	One is that large language models,
2959520	2961200	I think, seem very good at learning
2961200	2963640	kind of shallow but broad theories, right?
2963640	2966120	So things like could shoes be made out of eggplants
2966120	2968760	or what would happen if shoes were made out of eggplants, right?
2968760	2973760	They would know what some bad downsides
2973760	2976000	of that kind of thing might be, right?
2976000	2980080	Or answer basic kinds of questions
2980080	2983880	that might rely on kind of reasoning
2983880	2987440	through one or two kind of links about the relationships
2987440	2990420	between the objects involved in a situation like that.
2991440	2992960	I think it's been very surprising to people
2992960	2995520	that this works so well, right?
2995520	2998480	And part of, I think, what makes it surprising
2998480	3003480	is that these models are able to be trained
3003680	3005880	on a huge number of words, right?
3005880	3007960	And so sort of superficially knowing a little bit
3007960	3010120	about conceptual roles of a huge number of words
3010120	3011920	seems to get you pretty far
3012920	3014440	in terms of seeming convincing
3014440	3016660	and in terms of language production.
3017600	3019680	But it's also these conceptual roles and theories
3019680	3021720	are also weakness and they don't seem very good
3021720	3023520	at robust and precise theories, right?
3023520	3025480	So if you think about conceptual roles
3025480	3026780	like in mathematics, right?
3026780	3028960	How you define say a natural number
3028960	3030800	or how you define an integral or something, right?
3030800	3034200	Like all of those are symbolic kinds of theories
3034200	3037080	which are precise and which support
3037080	3039720	chains of reasoning of arbitrary length, right?
3039720	3041440	And that's what these systems really seem
3041440	3043960	not to be very good at.
3043960	3046000	Likely that's because there's some important things
3046000	3047040	which are missing, right?
3047040	3050880	Things like grounding, things like reasoning
3050880	3053440	or even richer kinds of theories.
3053440	3057100	I think Josh will talk about this some next.
3058100	3061080	I think that there's a kind of broader view
3061080	3063900	of concepts and meanings, which is really,
3063900	3065260	I think the most exciting for people
3065260	3069420	that work on concepts and concept representations
3069420	3072940	in cognitive psychology, which is that large language models
3072940	3074540	have really shown how vectors can do things
3074540	3076460	that were long thought to be impossible
3076460	3078180	for non-symbolic models, right?
3078180	3079760	In particular, these kinds of arguments
3079760	3082060	from people like Fodor and Polition
3082060	3085540	about compositionality and systematicity and productivity,
3085540	3086380	right?
3086380	3089020	All of these kinds of things that people have pointed to
3089020	3092140	as characteristic features of thinking
3092660	3094700	have argued were characteristic features
3094700	3097820	of symbolic thinking just turn out not to be right, right?
3097820	3101540	It turns out you can get vectors to do those things.
3101540	3103120	And I would argue that the solution
3103120	3104980	to why vectors could do those things
3104980	3107900	is probably that what these models are doing
3107900	3111500	is training vectors that encode conceptual roles, right?
3111500	3114580	Like what they're learning is representations of meaning
3114580	3118140	which capture the important parts of conceptual roles.
3118140	3121980	This is actually something which has been long sought after
3121980	3124140	in say computational neuroscience.
3124140	3126260	There's things like vector symbolic architectures
3126260	3129940	that are very exciting ways of encoding
3129940	3132220	say arbitrary symbolic systems
3132220	3136700	or arbitrary mathematical systems into vectors.
3136700	3139700	And I think that some marriage of those two things
3139700	3142900	is going to be very exciting.
3144500	3146380	So large language models point to a theory of meaning
3146380	3150660	that's based on essentially vector based conceptual roles,
3151020	3154060	and perhaps can capture a lot of the different features
3154060	3158820	of meaning that people in say cognitive science
3158820	3162900	or cognitive development have tried to kind of bring out
3162900	3164540	in human conceptual systems, right?
3164540	3165980	Like that our meanings are gradient
3165980	3167980	or that they have hierarchies,
3167980	3169620	that we know things like definitions
3169620	3171700	and we can make inferences about relationships
3171700	3175500	and similarities and all of those things seem like things
3175500	3178540	that you can encode at least in principle in vectors
3179540	3180980	which is great.
3181940	3186020	So let's get that, let me just end there.
3186020	3187940	I'll thank you again for the invitation
3187940	3191540	and thanks also to all of my co-authors on the work here.
3191540	3196540	All right.
3196700	3197540	Yes.
3199860	3201740	Maybe I'm reading too much into it
3201740	3204380	but it seemed to me that you hinted at
3206020	3207940	what these vector representations
3208700	3211260	large language models tell us about
3211260	3215700	both human cognition as well as about language,
3215700	3217020	the nature of language.
3218020	3220900	Could I ask you to do a projective measurement
3220900	3223700	and come out and say something about that?
3225940	3230940	I think that they tell us that vectors are really plausible.
3231940	3235300	They kind of show us how vectors are plausible for meanings,
3235820	3238940	and the way in which I think that they're plausible
3238940	3241660	for meanings is that they encode conceptual roles.
3242820	3243660	That's what I would say.
3243660	3245860	And I think until them,
3245860	3247380	until kind of recent deep learning,
3247380	3248980	I think it was really unclear.
3248980	3252380	So people had argued for decades about whether
3252380	3254500	the foundation of concepts was definitions
3254500	3256300	or is it like somehow similarities
3256300	3257660	or is it that you just know a word
3257660	3261140	and you know a bunch of associated features or whatever.
3262100	3266060	And I think one of the main insights, for example,
3266060	3269860	is that you can extract a definition
3269860	3271180	from a large language model.
3271180	3272340	We've even given it some of these
3272340	3273820	kind of human experiments we've done
3273820	3275740	and they're pretty good at coming up
3275740	3279220	with the chocolate torch kind of definitions from those.
3279220	3280780	And that tells you that the definitions
3280780	3283740	can be encoded into vectors, right?
3283740	3284740	And that's great, right?
3284740	3289100	That means that you don't need to think about definitions
3289100	3291860	as the defining part of concepts, right?
3291860	3294540	There's some other kind of more abstract,
3294540	3297100	you know, high dimensional space or whatever
3297100	3298460	that defines the meanings.
3298460	3300180	And the sense in which it defines the meanings
3300180	3302140	is in terms of the relationships between vectors
3302140	3304340	on the tasks that you use the concepts for.
3305340	3308500	Is it just natural since the brain encodes information
3308500	3311420	with lots of neurons firing through your brain?
3311420	3313100	This should not be surprising.
3313100	3317060	So it's not surprising, it was always unclear
3317060	3318540	how that was even possible.
3319100	3323900	Yeah, exactly, yeah, yeah, yeah.
3323900	3325540	It's like everybody always kind of knew
3325540	3327580	that there had to be a continuous system
3327580	3328820	which could support these things.
3328820	3330900	But when you look at, you know, the discreteness
3330900	3332860	in language or this discreteness in mathematics,
3332860	3335340	it was always kind of unclear where that could come from.
3335340	3336900	So that's why I think things like vector symbolic
3336900	3339660	architectures are very exciting too.
3339660	3342900	So when a human fills in the meaning,
3342900	3344300	they're using a lot of context
3344300	3346140	that they've gotten from the real world.
3346140	3349180	When an octopus tries to fill in the meaning,
3349180	3352180	they have much less context to work with.
3352180	3354300	And when a LLM fills in the meaning,
3354300	3357500	they have no kind of context to work with.
3357500	3358620	So I was wondering if you had thoughts
3358620	3361980	about the differences and that implies.
3361980	3364620	It's really interesting to think of what's exactly
3364620	3366060	happening in that human experiment
3366060	3368260	because I agree it's transfer of stuff you know,
3368260	3369420	like you've encountered cakes
3369420	3372660	and you've encountered fancy pastries or whatever.
3372700	3376140	And part of what you know about those meanings
3376140	3377980	are the grounded parts, right?
3377980	3378980	You know what a cake looks like,
3378980	3383620	which is why you can recognize the pictures and things.
3383620	3385380	I always have a little bit of trouble thinking about it
3385380	3387700	because it's never quite clear to me exactly
3387700	3389780	what it means to transfer something grounded.
3389780	3391780	Like it feels a little bit like in order for it
3391780	3394500	to transfer at all, it has to be a little bit abstract.
3396140	3398060	But I agree that that's the right question to ask
3398060	3400700	and we don't have any theories or certainly no evidence
3400700	3403780	about how exactly people solve that problem
3403780	3406700	or the way in which it relies on conceptual roles
3406700	3409460	versus grounded experience or something.
3409460	3411060	Okay, so we'll have one more question.
3411060	3413580	Meanwhile, maybe we can have the next speakers
3413580	3414660	start setting up.
3418340	3420980	Yeah, thanks Steve for the great talk.
3420980	3423340	I wanted to understand better what the argument was
3423340	3427660	in this kind of conceptual embedding experiment
3427660	3429980	because it seems like you could just ask the LLM
3430460	3431620	whether it's tall or not.
3431620	3434260	Like you didn't really need to do this projection
3434260	3436420	to know that it can do this task.
3436420	3439020	So is it somehow, is there something special
3439020	3440780	about the fact that you're looking at embeddings
3440780	3443860	rather than the outputs or what's kind of going on there?
3444900	3446180	That's a good question.
3446180	3448340	So I think you probably could do that.
3448340	3452100	I don't know how the results would compare
3452100	3454020	if you just asked versus not.
3456260	3457740	Yeah, I'm not sure.
3457740	3461420	I mean, I think it's like if it doesn't succeed
3461420	3463300	on just asking, then it's interesting to know
3463300	3465900	whether it's kind of latent representation
3465900	3467580	still has that information or not.
3467580	3472580	So, but I don't know the whole space of kind of how you,
3472900	3474580	how you can interrogate these models
3474580	3475900	for those questions, so.
3477420	3480900	All right, let's thank Steve again.
3483780	3486980	We'll have plenty of time to talk to him more
3487940	3491140	at the refreshments after this talk.
3491140	3494220	And who knows, maybe there will be Zafar Turkish in there.
3494220	3495780	Which is, by the way, amazing.
3495780	3498660	If you're in Vienna, you should absolutely try it.
3498660	3503660	So unfortunately, the next speaker could not be here.
3503660	3508660	Josh Tenenbaum is a latent variable in this session
3509980	3514660	because both Stephen was a student of Josh's
3514660	3515620	and...
