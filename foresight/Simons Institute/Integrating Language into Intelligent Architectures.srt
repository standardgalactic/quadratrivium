1
00:00:00,000 --> 00:00:01,920
I mentioned Gabe Grand, another student collaborator

2
00:00:01,920 --> 00:00:04,560
who is one of Jacob and Dreyas's students,

3
00:00:04,560 --> 00:00:09,120
and Tanja Shren, a student also co-advised

4
00:00:09,120 --> 00:00:13,040
by Josh and Vikash, as well as Noah Goodman,

5
00:00:13,040 --> 00:00:15,280
my advisor Vikash and Jacob and Dreyas.

6
00:00:17,640 --> 00:00:19,480
So our broader goal today in this talk

7
00:00:19,480 --> 00:00:21,480
is actually going to be to reflect

8
00:00:21,480 --> 00:00:23,080
based on many of the recent advances

9
00:00:23,080 --> 00:00:25,360
that we all know of modeling natural language,

10
00:00:25,360 --> 00:00:26,920
but I think also drawing on evidence

11
00:00:26,920 --> 00:00:29,720
from toolkits, other toolkits in AI.

12
00:00:30,720 --> 00:00:33,040
Evidence from cognitive science and from neuroscience

13
00:00:33,040 --> 00:00:35,680
on kind of the broader spectrum of different ways

14
00:00:35,680 --> 00:00:38,360
that we might think about building intelligent architectures

15
00:00:38,360 --> 00:00:41,320
that use and produce and learn from language,

16
00:00:41,320 --> 00:00:43,960
as well as more generally, I think what role language

17
00:00:43,960 --> 00:00:46,880
might play or could play in a computational system

18
00:00:46,880 --> 00:00:48,320
that we say thinks.

19
00:00:48,320 --> 00:00:50,360
And obviously this is a question that many people

20
00:00:50,360 --> 00:00:52,160
in this room, but many other people who probably

21
00:00:52,160 --> 00:00:53,680
aren't in this room have thought about

22
00:00:53,680 --> 00:00:57,280
from philosophers of language to linguists and neuroscientists.

23
00:00:57,280 --> 00:00:59,640
And we thought it actually might be a useful exercise

24
00:00:59,640 --> 00:01:02,760
to kind of start by reflecting on the underlying answers

25
00:01:02,760 --> 00:01:05,320
to this question that are or aren't suggested

26
00:01:05,320 --> 00:01:06,560
by some of the most prominent directions

27
00:01:06,560 --> 00:01:09,520
that we're taking in AI research right now.

28
00:01:09,520 --> 00:01:10,800
And of course, one of the reasons

29
00:01:10,800 --> 00:01:12,920
why we're even asking this question at this scale,

30
00:01:12,920 --> 00:01:15,000
what is the role of language in intelligence

31
00:01:15,000 --> 00:01:17,960
is in large part driven by this remarkable observation

32
00:01:17,960 --> 00:01:20,480
that we all know about from just a few years ago,

33
00:01:20,480 --> 00:01:23,080
which is if you train these large

34
00:01:23,080 --> 00:01:26,240
and specifically transformer based neural architectures

35
00:01:26,240 --> 00:01:27,960
as language models, just to do this

36
00:01:27,960 --> 00:01:32,200
next word prediction task, with enough language data,

37
00:01:32,200 --> 00:01:34,480
they start to show behaviors that really suggest

38
00:01:34,480 --> 00:01:36,280
that there's something more than language at play.

39
00:01:36,280 --> 00:01:37,880
They look like they're thinking.

40
00:01:38,800 --> 00:01:41,880
They can induce patterns from just a few examples in data,

41
00:01:41,880 --> 00:01:43,560
or they can even read the definitions

42
00:01:43,560 --> 00:01:46,480
of totally novel words like Zaka Tota in context

43
00:01:46,480 --> 00:01:48,720
and produce realistic sentences that appear

44
00:01:48,720 --> 00:01:52,080
as if they understand how to use those words immediately.

45
00:01:52,080 --> 00:01:54,360
And so a lot of the excitement, I think it's fair to say

46
00:01:54,360 --> 00:01:57,080
around language models is that maybe for the first time,

47
00:01:57,080 --> 00:01:59,880
we're seeing something that is offering a scalable route

48
00:01:59,880 --> 00:02:03,840
towards implementing more generally intelligent architectures

49
00:02:03,840 --> 00:02:07,200
just by directly scaling, or largely by directly scaling

50
00:02:07,200 --> 00:02:08,160
the amount of language data

51
00:02:08,160 --> 00:02:09,960
that they're being trained to predict.

52
00:02:11,640 --> 00:02:13,280
All right, so what is the underlying idea here?

53
00:02:13,280 --> 00:02:15,200
What does this have to say about language?

54
00:02:15,200 --> 00:02:16,240
Well, I think it's fair to say

55
00:02:16,240 --> 00:02:18,880
that one of the dominant hypotheses that's underlying

56
00:02:18,880 --> 00:02:21,960
why we were even starting to see some of this behavior

57
00:02:21,960 --> 00:02:23,840
rests on kind of a two-part idea.

58
00:02:23,840 --> 00:02:27,120
One is something about the nature of language itself, right?

59
00:02:27,120 --> 00:02:31,280
It's the suggestion that language is sufficiently diverse

60
00:02:31,280 --> 00:02:34,360
and so broad, maybe because we suspect that humans

61
00:02:34,360 --> 00:02:35,760
express so much of their thoughts

62
00:02:35,760 --> 00:02:38,360
in such a diverse range of their thoughts and language

63
00:02:38,360 --> 00:02:41,520
that being able to perfectly solve this task,

64
00:02:41,520 --> 00:02:44,680
to be a perfect language model, or at least a really good one,

65
00:02:44,680 --> 00:02:48,440
is essentially an AGI complete task,

66
00:02:48,440 --> 00:02:50,400
and maybe also one that conveniently,

67
00:02:50,400 --> 00:02:51,560
unlike other kinds of tasks,

68
00:02:51,560 --> 00:02:53,320
like predicting all of the videos in the world,

69
00:02:54,000 --> 00:02:55,800
we have maybe efficient architectures to do

70
00:02:55,800 --> 00:02:57,200
and enough data to do.

71
00:02:57,200 --> 00:03:00,360
And I think it's worth noting that this doesn't actually

72
00:03:00,360 --> 00:03:03,200
really have to be a particularly strong hypothesis

73
00:03:03,200 --> 00:03:06,880
about what it is computationally that thinking looks like,

74
00:03:06,880 --> 00:03:10,520
or even how language necessarily is implicated internally

75
00:03:10,520 --> 00:03:13,920
inside the computational processes that we call thinking.

76
00:03:13,920 --> 00:03:15,360
Rather, it's really just a hypothesis

77
00:03:15,360 --> 00:03:18,360
about the nature of the language modeling task itself.

78
00:03:20,280 --> 00:03:22,200
And I think what a lot of people

79
00:03:22,400 --> 00:03:25,640
in this room would agree now is that,

80
00:03:25,640 --> 00:03:28,240
well, that might be true about the language modeling task

81
00:03:28,240 --> 00:03:31,280
in principle, much of the most exciting research

82
00:03:31,280 --> 00:03:32,640
that we're seeing using LMS right now

83
00:03:32,640 --> 00:03:35,920
actually isn't predicated on really the hope or the idea

84
00:03:35,920 --> 00:03:37,960
that just by scaling to more and more data,

85
00:03:37,960 --> 00:03:40,760
we're gonna expect transformers trained and used in this way

86
00:03:40,760 --> 00:03:42,200
to actually solve that task

87
00:03:42,200 --> 00:03:44,560
or become perfect next-world prediction models.

88
00:03:44,560 --> 00:03:46,520
And the intuition behind that,

89
00:03:46,520 --> 00:03:48,400
which I think many people have pointed out,

90
00:03:48,400 --> 00:03:51,600
comes both from the way in which we're looking

91
00:03:51,960 --> 00:03:53,800
in which transformers work, right?

92
00:03:53,800 --> 00:03:55,320
There are autoregressive language models

93
00:03:55,320 --> 00:03:57,520
that are doing a fixed, finite amount

94
00:03:57,520 --> 00:03:59,720
of internal computation that only scales

95
00:03:59,720 --> 00:04:01,520
based on the previous linguistic context.

96
00:04:01,520 --> 00:04:03,360
And it doesn't really make sense that,

97
00:04:03,360 --> 00:04:05,960
of course, you can pose questions in language

98
00:04:05,960 --> 00:04:08,240
like arbitrarily difficult math problems

99
00:04:08,240 --> 00:04:11,440
or planning problems that intuitively require

100
00:04:11,440 --> 00:04:12,360
an amount of computation,

101
00:04:12,360 --> 00:04:13,840
whose complexity doesn't actually depend

102
00:04:13,840 --> 00:04:16,720
linearly on the previous token context.

103
00:04:16,720 --> 00:04:18,680
And I think that's been matched empirically

104
00:04:18,680 --> 00:04:20,800
by lots of different observations

105
00:04:20,800 --> 00:04:23,480
about which kinds of sentences are hard to complete

106
00:04:23,480 --> 00:04:25,160
if you treat them as next-world prediction tasks

107
00:04:25,160 --> 00:04:26,160
in this way, right?

108
00:04:26,160 --> 00:04:27,360
Hard arbitrary math problems

109
00:04:27,360 --> 00:04:28,960
or planning problems like these.

110
00:04:29,960 --> 00:04:33,640
Right, so some of, you know,

111
00:04:33,640 --> 00:04:36,400
if you're thinking about the role of language,

112
00:04:36,400 --> 00:04:39,720
why is language, why are language models so big right now

113
00:04:39,720 --> 00:04:42,640
as, oh, well, language just has all the evidence necessary

114
00:04:42,640 --> 00:04:46,280
to train something to think.

115
00:04:48,080 --> 00:04:50,000
Like Leo said, that's not actually committing

116
00:04:50,000 --> 00:04:52,320
to any hypothesis about sort of how language

117
00:04:52,320 --> 00:04:53,520
is used internally in thinking.

118
00:04:53,520 --> 00:04:56,080
The transformers' computations are not necessarily

119
00:04:56,080 --> 00:04:59,040
doing anything linguistic as it's computing

120
00:04:59,040 --> 00:05:00,680
the distribution of the next word.

121
00:05:00,680 --> 00:05:04,440
But more recently, we've seen people use language models

122
00:05:04,440 --> 00:05:07,640
to solve these harder tasks by letting them think more.

123
00:05:07,640 --> 00:05:09,400
And what letting a transformer think more means

124
00:05:09,400 --> 00:05:11,800
is letting it generate more tokens.

125
00:05:11,800 --> 00:05:15,880
So in this view, thinking doesn't necessarily emerge

126
00:05:15,880 --> 00:05:17,880
just as predicting the next token.

127
00:05:18,880 --> 00:05:22,920
Rather, thinking happens in language, right?

128
00:05:22,920 --> 00:05:27,840
It happens by sort of producing a chain of thoughts

129
00:05:27,840 --> 00:05:30,960
or using a scratch pad or deciding

130
00:05:30,960 --> 00:05:32,640
that you're going to invoke a calculator

131
00:05:32,640 --> 00:05:34,640
or write some code and execute the code.

132
00:05:35,640 --> 00:05:40,280
And the sort of hypothesis embodied,

133
00:05:40,280 --> 00:05:42,000
I think by this line of work,

134
00:05:42,000 --> 00:05:43,160
is that the role of language

135
00:05:43,160 --> 00:05:45,880
and intelligence architecture is bigger.

136
00:05:45,880 --> 00:05:48,760
That language or some kind of like running monologue

137
00:05:48,760 --> 00:05:51,600
of language is the central controller of thought.

138
00:05:51,600 --> 00:05:54,760
And thinking is taking place primarily in language.

139
00:05:57,760 --> 00:06:01,840
And what's maybe striking about these proposals

140
00:06:01,840 --> 00:06:03,840
is that if you ask most cognitive scientists

141
00:06:03,840 --> 00:06:05,680
or neuroscientists, they'd probably say,

142
00:06:05,680 --> 00:06:07,520
this isn't the role that language plays

143
00:06:07,520 --> 00:06:09,320
in relation to general intelligence in humans,

144
00:06:09,320 --> 00:06:11,680
or at least it's not the dominant hypothesis.

145
00:06:12,760 --> 00:06:13,840
Until just a few years ago,

146
00:06:13,840 --> 00:06:14,920
this probably wouldn't have been the role

147
00:06:14,920 --> 00:06:16,000
that many AI researchers

148
00:06:16,000 --> 00:06:17,880
would have necessarily posited for language

149
00:06:17,880 --> 00:06:22,160
as the main controller or the main substrate

150
00:06:22,160 --> 00:06:25,240
of a running monologue that controls all thinking.

151
00:06:25,240 --> 00:06:27,320
And so we thought we'd review some of the background

152
00:06:27,320 --> 00:06:29,520
for what language seems to look like in humans

153
00:06:29,520 --> 00:06:32,000
as a basis for informing how we might build architectures

154
00:06:32,000 --> 00:06:34,360
that better capture those more human-like roles

155
00:06:34,360 --> 00:06:35,200
for language.

156
00:06:36,840 --> 00:06:38,080
Yeah, and so I just want to be clear

157
00:06:38,080 --> 00:06:39,760
that we're going to do kind of a quick background

158
00:06:39,760 --> 00:06:42,000
on some of the neuroscientific and cognitive evidence

159
00:06:42,000 --> 00:06:44,120
about what language might look like in people.

160
00:06:44,120 --> 00:06:47,080
And our goal really isn't to push back in any way

161
00:06:47,080 --> 00:06:48,840
against these other kinds of paradigms

162
00:06:48,840 --> 00:06:50,320
for where language might fit in.

163
00:06:50,320 --> 00:06:52,240
It's rather to present at a mode

164
00:06:52,240 --> 00:06:53,800
where we're really thinking a lot

165
00:06:53,800 --> 00:06:55,440
about scaling up language models

166
00:06:55,440 --> 00:06:57,960
as one really dominant role,

167
00:06:57,960 --> 00:07:00,200
what our other roots might be

168
00:07:00,200 --> 00:07:03,480
because it doesn't seem like that that really matches

169
00:07:03,480 --> 00:07:05,760
a different prominent intelligent architecture

170
00:07:05,760 --> 00:07:07,400
that's sitting here in the room today.

171
00:07:07,400 --> 00:07:09,280
So one source of evidence for the role

172
00:07:09,280 --> 00:07:12,080
that language appears to play in human cognition

173
00:07:12,080 --> 00:07:14,760
comes from neuroimaging data, fMRI data,

174
00:07:14,760 --> 00:07:18,160
suggesting how language is processed in the human brain.

175
00:07:18,160 --> 00:07:22,280
And at this point, convergent imaging data from many people,

176
00:07:22,280 --> 00:07:24,120
including Ed Fedorenko at MIT,

177
00:07:24,120 --> 00:07:26,120
Standa Hain and recent graduate students

178
00:07:26,120 --> 00:07:29,640
in our department, Kyle Malewald and Anya Ivanova,

179
00:07:29,640 --> 00:07:30,920
suggests that human brains

180
00:07:30,920 --> 00:07:33,440
have this language-specific network

181
00:07:33,440 --> 00:07:35,600
that handles many of the tasks that we associate with language.

182
00:07:35,600 --> 00:07:37,000
It's activated when people do tasks

183
00:07:37,000 --> 00:07:39,560
like listening to sentences or reading them

184
00:07:39,560 --> 00:07:41,440
or speaking or writing words.

185
00:07:42,280 --> 00:07:44,560
And this is not just an English-specific network.

186
00:07:44,560 --> 00:07:46,240
The same general region is activated

187
00:07:46,240 --> 00:07:48,720
no matter what language someone is speaking.

188
00:07:48,720 --> 00:07:51,640
It even fires when they're producing constructed languages

189
00:07:51,640 --> 00:07:53,360
for people who come to learn and become fluent

190
00:07:53,360 --> 00:07:55,320
in languages like Dothraki or Klingon.

191
00:07:56,960 --> 00:07:59,480
And right, what is it that this language network does?

192
00:07:59,480 --> 00:08:01,800
Well, increasingly, one dominant hypothesis

193
00:08:01,800 --> 00:08:03,480
is that it actually does do something

194
00:08:03,480 --> 00:08:05,120
like next-word prediction.

195
00:08:05,120 --> 00:08:08,600
And in fact, among many other kinds of alternative models,

196
00:08:08,600 --> 00:08:10,880
like people were earlier,

197
00:08:10,880 --> 00:08:12,480
for a long time I've been trying to do things

198
00:08:12,480 --> 00:08:14,560
like align words back to the brain,

199
00:08:14,560 --> 00:08:16,760
transformer architectures really do seem to be

200
00:08:16,760 --> 00:08:18,760
among the best models that we have right now

201
00:08:18,760 --> 00:08:21,280
of the neural activity of this language network,

202
00:08:21,280 --> 00:08:22,640
specifically when they're trained

203
00:08:22,640 --> 00:08:23,960
on next-word prediction tasks

204
00:08:23,960 --> 00:08:26,600
and not other linguistic tasks like NOI.

205
00:08:28,000 --> 00:08:30,600
But I think what we see in this neuroimaging evidence

206
00:08:30,600 --> 00:08:32,400
also highlights the ways in which the role

207
00:08:32,400 --> 00:08:34,360
of this human language network

208
00:08:34,360 --> 00:08:36,440
probably is not as the controller

209
00:08:36,440 --> 00:08:38,360
or the central seat of cognition.

210
00:08:38,360 --> 00:08:39,920
It's selective for language

211
00:08:39,920 --> 00:08:41,720
and it isn't involved in many other kinds

212
00:08:41,720 --> 00:08:43,440
of thinking activities,

213
00:08:43,440 --> 00:08:45,840
even ones that might seem to involve symbols

214
00:08:45,840 --> 00:08:47,320
like solving math problems

215
00:08:47,320 --> 00:08:49,680
or reasoning about logic and physics and social reasoning.

216
00:08:49,680 --> 00:08:53,160
Those invoke other regions of the brain

217
00:08:53,160 --> 00:08:55,360
and language interfaces modulary with them.

218
00:08:55,360 --> 00:08:57,320
It can interface very generally with them,

219
00:08:57,320 --> 00:08:58,160
but it doesn't seem to be

220
00:08:58,160 --> 00:09:00,360
where the bulk of thinking is taking place.

221
00:09:01,600 --> 00:09:02,440
Oh, yes.

222
00:09:02,440 --> 00:09:03,440
Oh yeah, I just had a question

223
00:09:03,440 --> 00:09:05,040
about the next-word prediction findings.

224
00:09:05,040 --> 00:09:08,840
So did you also try to say like math word prediction

225
00:09:08,960 --> 00:09:11,440
and this is actually worse than next-word?

226
00:09:11,440 --> 00:09:13,440
Yeah, so I think they do compare

227
00:09:13,440 --> 00:09:15,360
or like closed tasks are one of the alternates

228
00:09:15,360 --> 00:09:16,200
that they're looking for.

229
00:09:16,200 --> 00:09:18,440
And also, yeah, people should feel free to just shout out

230
00:09:18,440 --> 00:09:20,720
as we've been doing all along.

231
00:09:20,720 --> 00:09:24,880
So also, right, actually you can see adult patients

232
00:09:24,880 --> 00:09:28,120
that suffer strokes that only damage this area of their brain

233
00:09:28,120 --> 00:09:31,720
and we find that they can still think about,

234
00:09:31,720 --> 00:09:33,800
well, they can't comprehend spoken language,

235
00:09:33,800 --> 00:09:35,800
but they can still think about all these different kinds

236
00:09:35,800 --> 00:09:37,320
of tasks fronted in different mediums,

237
00:09:37,320 --> 00:09:38,920
like they can draw physical inferences

238
00:09:38,920 --> 00:09:40,760
from videos that they're watching.

239
00:09:40,760 --> 00:09:43,400
And conversely, maybe most tantalizingly,

240
00:09:43,400 --> 00:09:46,720
it's actually also possible to sustain localized damage

241
00:09:46,720 --> 00:09:49,360
that leaves you still able to produce these long,

242
00:09:49,360 --> 00:09:50,600
very relatively fluent

243
00:09:50,600 --> 00:09:53,080
and quite syntactically coherent sentences,

244
00:09:53,080 --> 00:09:55,400
almost as if they're maybe just like a very rudimentary

245
00:09:55,400 --> 00:09:58,080
and local word-based language model,

246
00:09:58,080 --> 00:09:59,880
while not really conditioning meaningfully

247
00:09:59,880 --> 00:10:01,920
on what someone else is saying.

248
00:10:01,920 --> 00:10:03,880
So they say, do you like it here in Kansas City?

249
00:10:03,880 --> 00:10:06,080
And this person says, yes, I am.

250
00:10:07,800 --> 00:10:09,120
Or really producing sentences

251
00:10:09,120 --> 00:10:10,720
that are obviously more globally meaningful

252
00:10:10,720 --> 00:10:12,080
or goal directed towards the question.

253
00:10:12,080 --> 00:10:13,880
And I don't want us to overindex on those results,

254
00:10:13,880 --> 00:10:17,000
but they kind of fit in with this more coherent picture.

255
00:10:17,000 --> 00:10:20,120
And in many ways, I think this kind of recent evidence

256
00:10:20,120 --> 00:10:22,800
from neuroscience actually supports the broader picture

257
00:10:22,800 --> 00:10:25,240
of human cognition and the place of language in it

258
00:10:25,240 --> 00:10:26,720
that most cognitive scientists

259
00:10:26,720 --> 00:10:28,800
and many linguists have already believed

260
00:10:28,800 --> 00:10:30,880
based on what we see and have come to learn

261
00:10:30,880 --> 00:10:32,800
through developmental science,

262
00:10:32,800 --> 00:10:34,200
studying how kids think

263
00:10:34,200 --> 00:10:36,320
and where language seems to fit into that picture, right?

264
00:10:36,320 --> 00:10:38,560
So a broad body of work at this point

265
00:10:38,560 --> 00:10:41,560
suggests that infants, well before they learn language

266
00:10:41,560 --> 00:10:43,320
of any kind or are speaking,

267
00:10:43,320 --> 00:10:45,880
already independently perform many of the tasks

268
00:10:45,880 --> 00:10:48,000
I think we associate with coherent thinking,

269
00:10:48,000 --> 00:10:50,120
from reasoning about physics to planning

270
00:10:50,120 --> 00:10:53,120
to drawing causal and probabilistic inferences.

271
00:10:53,120 --> 00:10:55,920
And language seems to be something that humans learn

272
00:10:55,920 --> 00:10:59,080
and scaffold on top of this structured basis for thinking.

273
00:10:59,080 --> 00:11:00,960
We take much less input data

274
00:11:00,960 --> 00:11:02,320
to learn to produce fluent language

275
00:11:02,320 --> 00:11:03,680
than a large language model.

276
00:11:03,680 --> 00:11:06,160
And humans that actually aren't exposed

277
00:11:06,160 --> 00:11:08,040
to any language input at all.

278
00:11:08,040 --> 00:11:11,000
So famously there are these deaf children in Nicaragua

279
00:11:11,000 --> 00:11:13,440
who grow up in isolated hearing families

280
00:11:13,440 --> 00:11:16,840
actually spontaneously come to produce early sign languages

281
00:11:16,840 --> 00:11:18,760
as a product of trying to communicate events

282
00:11:18,760 --> 00:11:20,640
that bear many of the hallmarks of the syntax

283
00:11:20,640 --> 00:11:23,480
that we associate of our own natural languages,

284
00:11:23,480 --> 00:11:26,200
like distinguishing between the subject who's punching

285
00:11:26,200 --> 00:11:28,440
and the person who's getting punched.

286
00:11:28,440 --> 00:11:30,000
Suggesting that in many ways

287
00:11:30,000 --> 00:11:32,640
the language we produce somehow externalizes

288
00:11:32,640 --> 00:11:36,040
the underlying structure of the thought that we already have.

289
00:11:36,080 --> 00:11:38,280
And of course, there are many other animals

290
00:11:38,280 --> 00:11:40,680
that we associate as being intelligent in some way

291
00:11:40,680 --> 00:11:42,960
and that have been modeled using the models

292
00:11:42,960 --> 00:11:46,400
that we associate with probabilistic reasoning and planning

293
00:11:46,400 --> 00:11:47,720
that don't use language at all.

294
00:11:47,720 --> 00:11:51,520
So it doesn't feel like language is by any means necessary

295
00:11:51,520 --> 00:11:53,040
for what we think of as thought.

296
00:11:54,560 --> 00:11:58,160
So formalizing this picture of an intelligent system,

297
00:11:58,160 --> 00:12:01,240
one that's shared across animals and non-linguistic infants

298
00:12:01,240 --> 00:12:03,880
and maybe captures some of the computations involved

299
00:12:03,880 --> 00:12:05,520
in many of those thinking tasks

300
00:12:05,520 --> 00:12:10,440
that don't involve the language network in our brains

301
00:12:10,440 --> 00:12:12,840
is both the central goal of a lot of cognitive science

302
00:12:12,840 --> 00:12:16,120
and has been a historic motivation

303
00:12:16,120 --> 00:12:18,360
for diverse fields within AI

304
00:12:18,360 --> 00:12:21,480
before the current sort of LLM centric moment.

305
00:12:21,480 --> 00:12:25,280
So if you open up Russell and Norvig's textbook,

306
00:12:25,280 --> 00:12:28,080
AI a modern approach, you'll see an equation like this

307
00:12:28,080 --> 00:12:30,960
that's supposed to sort of capture what we believe

308
00:12:30,960 --> 00:12:32,640
about how intelligence works.

309
00:12:34,440 --> 00:12:37,200
Computationally or a computational model of intelligence.

310
00:12:37,200 --> 00:12:39,200
Where the idea is that an intelligent agent

311
00:12:39,200 --> 00:12:42,720
is one that has sort of structured internal world models

312
00:12:42,720 --> 00:12:46,280
that can be updated based on observations of the world

313
00:12:46,280 --> 00:12:49,480
and in which we can sort of predict the results

314
00:12:49,480 --> 00:12:53,520
of our actions that the agent has some sort of values

315
00:12:53,520 --> 00:12:57,400
or desires that are captured in some kind of utility function

316
00:12:57,400 --> 00:12:59,480
that the agent can do probabilistic reasoning

317
00:12:59,480 --> 00:13:03,160
over its observations and the possible actions it might take

318
00:13:03,480 --> 00:13:05,120
and what their expected utilities are

319
00:13:05,120 --> 00:13:06,680
and that it can do some kind of planning

320
00:13:06,680 --> 00:13:08,760
to optimize the value of the back end.

321
00:13:08,760 --> 00:13:09,600
Yeah.

322
00:13:09,600 --> 00:13:12,320
Is there a normative statement or descriptive statement?

323
00:13:12,320 --> 00:13:15,320
Is it the case that we are defining intelligent agent

324
00:13:15,320 --> 00:13:17,200
to be having this kind of property

325
00:13:17,200 --> 00:13:20,000
or it's like if we want to do intelligent architecture,

326
00:13:20,000 --> 00:13:21,360
we should have these?

327
00:13:21,360 --> 00:13:23,040
Yeah, so I think some, it's a great question.

328
00:13:23,040 --> 00:13:24,960
I think some of the work definitely is coming at it

329
00:13:24,960 --> 00:13:26,160
from a normative perspective.

330
00:13:26,160 --> 00:13:28,920
This is like a definition of what rational actions

331
00:13:28,920 --> 00:13:30,080
might look like, right?

332
00:13:30,080 --> 00:13:31,240
But I think there is a lot of work

333
00:13:31,560 --> 00:13:32,960
computational cognitive science

334
00:13:32,960 --> 00:13:34,880
that sort of in various domains

335
00:13:34,880 --> 00:13:36,000
has collected a lot of evidence

336
00:13:36,000 --> 00:13:39,400
that people either like recognize this as normative

337
00:13:39,400 --> 00:13:42,920
or behave like this in computation,

338
00:13:42,920 --> 00:13:45,720
according to the limits of what they can do computationally.

339
00:13:48,240 --> 00:13:53,240
So in this architecture, the controller for thinking

340
00:13:53,320 --> 00:13:54,600
would be some kind of system

341
00:13:54,600 --> 00:13:56,240
that's capable of general world modeling

342
00:13:56,240 --> 00:13:57,800
and probabilistic reasoning

343
00:13:57,800 --> 00:13:59,800
or planning and utility maximization

344
00:13:59,840 --> 00:14:04,120
rather than sort of a primarily linguistic system necessarily.

345
00:14:05,840 --> 00:14:07,720
Now largely attempts in artificial intelligence

346
00:14:07,720 --> 00:14:09,360
to directly implement this equation

347
00:14:09,360 --> 00:14:11,920
by building out components that sort of map on

348
00:14:11,920 --> 00:14:15,520
to the different elements of this equation

349
00:14:15,520 --> 00:14:18,200
have struggled to match the computational efficiency

350
00:14:18,200 --> 00:14:20,640
and the generality of natural intelligence.

351
00:14:20,640 --> 00:14:23,200
But we have seen over the past couple of decades or so

352
00:14:23,200 --> 00:14:24,680
the emergence of this new class of tools

353
00:14:24,680 --> 00:14:26,480
called probabilistic programming languages

354
00:14:26,480 --> 00:14:27,320
for building software

355
00:14:27,320 --> 00:14:29,480
that can solve probabilistic reasoning tasks

356
00:14:29,480 --> 00:14:31,760
at least in limited domains.

357
00:14:31,760 --> 00:14:35,280
And sorry, and these languages have been applied

358
00:14:35,280 --> 00:14:37,280
to create systems that do everything

359
00:14:37,280 --> 00:14:39,720
from perceiving cluttered 3D scenes

360
00:14:39,720 --> 00:14:41,080
more accurately sometimes

361
00:14:41,080 --> 00:14:43,520
than object detector neural nets

362
00:14:43,520 --> 00:14:46,400
to interpreting and predicting economic trends

363
00:14:46,400 --> 00:14:48,360
more accurately than leading industry solutions

364
00:14:48,360 --> 00:14:49,760
like Facebook neural profit.

365
00:14:50,680 --> 00:14:52,480
And these probabilistic programming systems

366
00:14:52,480 --> 00:14:53,680
are enabling these applications

367
00:14:53,680 --> 00:14:55,200
with two key technical features.

368
00:14:55,200 --> 00:14:57,000
They feature modeling languages

369
00:14:57,000 --> 00:14:59,960
that let users express complicated probabilistic models

370
00:14:59,960 --> 00:15:01,440
of the world as programs

371
00:15:01,440 --> 00:15:04,240
making it easy to write down rich probabilistic models

372
00:15:04,240 --> 00:15:06,040
that are defined in terms of expressive

373
00:15:06,040 --> 00:15:07,760
program-like components.

374
00:15:07,760 --> 00:15:10,160
So for example, the probabilistic models behind

375
00:15:10,160 --> 00:15:11,560
these example applications

376
00:15:11,560 --> 00:15:14,120
are defined in terms of 3D renderers, symbolic planners,

377
00:15:14,120 --> 00:15:16,440
scientific simulators and so on.

378
00:15:16,440 --> 00:15:18,440
And the second thing that probabilistic programming systems do

379
00:15:18,440 --> 00:15:21,000
is that they automate various mathematical operations

380
00:15:21,000 --> 00:15:22,120
on these models

381
00:15:22,120 --> 00:15:23,960
and that automation makes it possible for users

382
00:15:23,960 --> 00:15:25,880
to concisely implement sophisticated algorithms

383
00:15:25,880 --> 00:15:27,200
for probabilistic reasoning,

384
00:15:27,200 --> 00:15:28,880
such as the variety of Monte Carlo

385
00:15:28,880 --> 00:15:30,160
and variational inference algorithms

386
00:15:30,160 --> 00:15:32,000
that power these example applications.

387
00:15:32,000 --> 00:15:33,360
One way of thinking about these tools

388
00:15:33,360 --> 00:15:38,160
is as kind of like PyTorch or TensorFlow,

389
00:15:38,160 --> 00:15:40,760
but instead of writing differentiable models

390
00:15:40,760 --> 00:15:41,720
and doing optimization,

391
00:15:41,720 --> 00:15:42,960
you're writing probabilistic models

392
00:15:42,960 --> 00:15:45,200
and doing various probabilistic reasoning tasks.

393
00:15:46,760 --> 00:15:48,840
So far, these AI engineering efforts

394
00:15:48,840 --> 00:15:50,000
haven't really made contact

395
00:15:50,000 --> 00:15:54,080
with the sort of language model part of AI.

396
00:15:55,000 --> 00:15:56,760
Much like there hasn't yet been a concerted effort

397
00:15:56,760 --> 00:15:59,080
to take this classical picture of intelligence architecture

398
00:15:59,080 --> 00:16:00,400
and figure out how language might be

399
00:16:00,400 --> 00:16:01,520
richly integrated into it.

400
00:16:01,520 --> 00:16:03,280
So for the rest of the talk today,

401
00:16:03,280 --> 00:16:04,920
we're gonna explore two different approaches

402
00:16:04,920 --> 00:16:06,360
for thinking about where language

403
00:16:06,360 --> 00:16:09,000
might fit into this picture and approach.

404
00:16:09,000 --> 00:16:11,160
So probably Leo's gonna begin

405
00:16:11,160 --> 00:16:12,800
by talking about how an intelligent agent

406
00:16:12,800 --> 00:16:16,120
might incorporate lots of externally produced languages,

407
00:16:16,120 --> 00:16:19,200
explanations, observations, questions,

408
00:16:19,200 --> 00:16:22,160
how an agent can incorporate all of those forms of language

409
00:16:22,200 --> 00:16:24,560
into the way that it updates its beliefs

410
00:16:24,560 --> 00:16:26,240
and decides how to act in the world.

411
00:16:26,240 --> 00:16:27,720
Then I'll talk a bit about systems

412
00:16:27,720 --> 00:16:30,120
that leverage language as a tool for thinking

413
00:16:30,120 --> 00:16:31,960
within its models of the world

414
00:16:31,960 --> 00:16:33,840
or its probabilistic reasoning algorithms.

415
00:16:33,840 --> 00:16:35,000
And each of these corresponds

416
00:16:35,000 --> 00:16:37,680
to very recent preprints of work.

417
00:16:37,680 --> 00:16:39,400
So I also wanna give a disclaimer.

418
00:16:39,400 --> 00:16:42,440
We should really emphasize that both are preliminary proposals

419
00:16:42,440 --> 00:16:44,800
and we're giving a very speculative talk,

420
00:16:44,800 --> 00:16:46,800
not scaled architectural solutions here.

421
00:16:48,320 --> 00:16:49,160
Cool, right.

422
00:16:49,160 --> 00:16:50,800
So the first person that's portion of this talk

423
00:16:50,800 --> 00:16:51,640
is summarizing work.

424
00:16:51,640 --> 00:16:52,640
If you wanna read more,

425
00:16:52,640 --> 00:16:54,360
you can find this very long paper

426
00:16:54,360 --> 00:16:56,320
from word models to world models.

427
00:16:56,320 --> 00:16:57,160
That's up on archive

428
00:16:57,160 --> 00:16:59,120
and this is primarily done with another student

429
00:16:59,120 --> 00:17:01,320
who's not here today, Gabe Grant.

430
00:17:01,320 --> 00:17:03,280
And as I just mentioned, the context for this work

431
00:17:03,280 --> 00:17:05,200
is thinking about how we can build systems

432
00:17:05,200 --> 00:17:07,620
that capture the breadth with which all the external language

433
00:17:07,620 --> 00:17:10,920
we hear seems to inform at least our human thinking.

434
00:17:10,920 --> 00:17:11,920
And what's clear, right,

435
00:17:11,920 --> 00:17:13,680
is that this role seems to be very broad.

436
00:17:13,680 --> 00:17:15,880
If we take the basic model of an agent

437
00:17:15,880 --> 00:17:17,200
with beliefs and goals,

438
00:17:17,200 --> 00:17:19,480
well, it seems like there's an incredibly diverse range

439
00:17:19,480 --> 00:17:21,960
of situations in which we can update our beliefs

440
00:17:21,960 --> 00:17:23,560
about a situation from observations

441
00:17:23,560 --> 00:17:25,160
that we express in language,

442
00:17:25,160 --> 00:17:26,640
or in which the goals of our thought

443
00:17:26,640 --> 00:17:29,480
are to answer questions that we specify linguistically.

444
00:17:29,480 --> 00:17:32,280
And these might implicate our knowledge of other agents

445
00:17:32,280 --> 00:17:33,760
drawing on our intuitive psychology

446
00:17:33,760 --> 00:17:36,240
to reason about what they think and what they'll do.

447
00:17:36,240 --> 00:17:38,640
Or we can talk about the physical world around us,

448
00:17:38,640 --> 00:17:40,280
what we perceive and ask questions

449
00:17:40,280 --> 00:17:41,760
that require us to reason about

450
00:17:41,760 --> 00:17:44,080
what we see and draw on our physical intuitions.

451
00:17:45,160 --> 00:17:46,520
And of course, one of the remarkable things

452
00:17:46,520 --> 00:17:48,280
where we're also excited about language

453
00:17:48,280 --> 00:17:50,400
is that it doesn't just draw on what we already know,

454
00:17:50,400 --> 00:17:52,360
it's this means by which humans seem to learn

455
00:17:52,360 --> 00:17:54,600
and pass on profoundly new knowledge,

456
00:17:54,600 --> 00:17:58,640
whether that's new concepts that we define in words

457
00:17:58,640 --> 00:18:01,360
or learn from words to really profound new theories

458
00:18:01,360 --> 00:18:02,960
and conceptual systems, right?

459
00:18:02,960 --> 00:18:04,400
Much of what we know about the world,

460
00:18:04,400 --> 00:18:06,960
the fact that there are wars, what wars are,

461
00:18:06,960 --> 00:18:08,720
legal systems, sciences,

462
00:18:08,720 --> 00:18:11,240
comes from information that it feels that we acquire

463
00:18:11,240 --> 00:18:12,880
from language in some way.

464
00:18:13,840 --> 00:18:15,320
But how do we do that?

465
00:18:15,320 --> 00:18:17,720
Well, I think one longstanding lens

466
00:18:17,720 --> 00:18:19,720
for thinking about language

467
00:18:19,720 --> 00:18:23,520
that's kind of persisted before the LLM-based moment

468
00:18:23,520 --> 00:18:25,400
is that what language is,

469
00:18:25,400 --> 00:18:27,280
is this external symbolic medium

470
00:18:27,280 --> 00:18:29,360
for communicating human thoughts.

471
00:18:29,360 --> 00:18:30,600
And the way that it does that

472
00:18:30,600 --> 00:18:33,360
is because there's some kind of general mapping function

473
00:18:33,360 --> 00:18:35,680
from our internal representations of thought

474
00:18:35,680 --> 00:18:38,720
into this external symbol system, that's language.

475
00:18:38,720 --> 00:18:41,360
And so in this kind of older framework,

476
00:18:41,360 --> 00:18:44,400
what it means to understand language or make meaning

477
00:18:44,400 --> 00:18:47,520
means mapping back from external sentences that we hear

478
00:18:47,520 --> 00:18:50,440
into structured internal representations.

479
00:18:50,440 --> 00:18:52,640
And what we explore in this paper

480
00:18:52,640 --> 00:18:56,880
is a general proposal that casts the meanings of language

481
00:18:56,880 --> 00:18:59,880
as these mappings or probabilistic distributions

482
00:18:59,880 --> 00:19:02,960
over expressions in a probabilistic programming language.

483
00:19:02,960 --> 00:19:05,200
And I'm gonna come back after some concrete examples

484
00:19:05,200 --> 00:19:07,720
to how I think this relates to the conceptual rule semantics

485
00:19:07,720 --> 00:19:09,480
that Steve articulated in the first talk,

486
00:19:09,480 --> 00:19:10,320
because I think there are actually

487
00:19:10,320 --> 00:19:11,600
some really deep connections here,

488
00:19:11,600 --> 00:19:14,560
the ways in which this might be one way to formalize

489
00:19:14,560 --> 00:19:16,160
or enrich some of those ideas.

490
00:19:17,120 --> 00:19:19,680
And this architecture, this proposal here

491
00:19:19,680 --> 00:19:21,720
also suggests how language can be integrated

492
00:19:21,720 --> 00:19:23,480
into a more general architecture,

493
00:19:23,480 --> 00:19:26,080
because it's one that already starts out with,

494
00:19:26,080 --> 00:19:27,280
as Alex mentioned,

495
00:19:27,280 --> 00:19:29,480
some kind of existing internal modeling language,

496
00:19:29,480 --> 00:19:32,440
whose goal is to represent the world probabilistically,

497
00:19:32,440 --> 00:19:34,560
query those models and specify what it means

498
00:19:34,560 --> 00:19:36,880
to draw coherent inferences over them.

499
00:19:36,880 --> 00:19:38,880
And it also suggests, I think, a framework

500
00:19:38,880 --> 00:19:40,920
for formally modeling the content

501
00:19:40,920 --> 00:19:43,040
of different kinds of sentences in language

502
00:19:43,040 --> 00:19:44,960
as the kinds of probabilistic programming expressions

503
00:19:45,040 --> 00:19:46,320
that they might map into.

504
00:19:48,320 --> 00:19:52,600
So, and so in this paper,

505
00:19:52,600 --> 00:19:54,640
we begin by exploring how this proposal

506
00:19:54,640 --> 00:19:56,920
might be instantiated with respect to

507
00:19:56,920 --> 00:19:59,200
a bunch of different domains of reasoning,

508
00:20:00,560 --> 00:20:03,520
sorry, including general probabilistic reasoning,

509
00:20:03,520 --> 00:20:05,160
but also reasoning about relations

510
00:20:05,160 --> 00:20:07,360
or physics and social situations.

511
00:20:07,360 --> 00:20:10,120
And in all of these, we're gonna propose

512
00:20:10,120 --> 00:20:11,280
that we might think about the language

513
00:20:11,280 --> 00:20:13,360
that communicates general conceptual knowledge

514
00:20:13,360 --> 00:20:16,520
about the world definitions or causal knowledge

515
00:20:16,520 --> 00:20:18,880
as constructing these probabilistic expressions

516
00:20:18,880 --> 00:20:22,160
that are those that build up probabilistic generative models.

517
00:20:22,160 --> 00:20:25,960
And then in this framework, observations in the language,

518
00:20:25,960 --> 00:20:29,160
like there is at least one red mug in the scene

519
00:20:29,160 --> 00:20:31,680
or Charlie is Dana's grandfather,

520
00:20:31,680 --> 00:20:34,120
construct formal conditioning statements,

521
00:20:34,120 --> 00:20:36,920
which update the state of this probabilistic model.

522
00:20:36,920 --> 00:20:39,480
And then questions map into query expressions

523
00:20:39,480 --> 00:20:42,320
that specify the formal target of probabilistic inference

524
00:20:42,320 --> 00:20:43,720
with respect to a model.

525
00:20:44,880 --> 00:20:46,120
And in this framework,

526
00:20:46,120 --> 00:20:49,760
we're thinking essentially cast as probabilistic reasoning.

527
00:20:49,760 --> 00:20:52,120
We suggest that another way that we can think about

528
00:20:52,120 --> 00:20:53,720
the role of a language model,

529
00:20:53,720 --> 00:20:55,160
one that's much smaller,

530
00:20:55,160 --> 00:20:56,800
like the language network in the brain

531
00:20:56,800 --> 00:20:58,800
is actually as a means of instantiating

532
00:20:58,800 --> 00:21:00,480
this meaning function in a way that

533
00:21:00,480 --> 00:21:02,680
we've really never had before

534
00:21:02,680 --> 00:21:05,240
to protect these kinds of context specific

535
00:21:05,240 --> 00:21:07,440
and previous discourse conditioned mappings

536
00:21:07,440 --> 00:21:09,640
from sentences in natural language

537
00:21:09,640 --> 00:21:11,520
into distributions over expressions

538
00:21:11,520 --> 00:21:14,120
that convey meaning in a probabilistic programming language.

539
00:21:16,040 --> 00:21:19,320
And so as a part of this long running disclaimer,

540
00:21:19,320 --> 00:21:20,880
what I'm gonna be showing is a really minimal

541
00:21:20,880 --> 00:21:22,880
implementation of this framework,

542
00:21:22,880 --> 00:21:25,240
but really intended as a pointer to different directions

543
00:21:25,240 --> 00:21:26,800
with which we might scale this approach

544
00:21:26,800 --> 00:21:28,880
to implement a more general interface between language

545
00:21:28,880 --> 00:21:31,000
and arrange a different core cognitive domains.

546
00:21:31,000 --> 00:21:32,120
So just to be clear,

547
00:21:32,120 --> 00:21:34,760
concretely in the examples that you see next,

548
00:21:34,760 --> 00:21:37,240
our meaning function is gonna be implemented using codex,

549
00:21:37,240 --> 00:21:40,240
right, an open AM model much smaller

550
00:21:40,240 --> 00:21:41,520
than the state of the art right now

551
00:21:41,520 --> 00:21:42,960
that's trained to learn joint distributions

552
00:21:42,960 --> 00:21:44,440
over language and code.

553
00:21:44,440 --> 00:21:46,080
And the probabilistic programming language

554
00:21:46,080 --> 00:21:47,480
we're gonna show is church,

555
00:21:47,480 --> 00:21:50,920
which is this very simple probabilistic programming language

556
00:21:50,920 --> 00:21:52,440
that supports kind of very general

557
00:21:52,440 --> 00:21:54,240
sample based inference procedures.

558
00:21:54,240 --> 00:21:56,360
And our goal is to demonstrate how this framework

559
00:21:56,360 --> 00:21:58,200
might broadly interface between language

560
00:21:58,200 --> 00:22:00,520
and a bunch of different core cognitive domains.

561
00:22:01,640 --> 00:22:03,840
So first to illustrate the basic sense

562
00:22:03,840 --> 00:22:06,280
in which a proposal like this might allow language

563
00:22:06,280 --> 00:22:09,440
to update an agent's beliefs and query a world model,

564
00:22:09,440 --> 00:22:11,480
I'm gonna begin with a really simple toy example

565
00:22:11,480 --> 00:22:13,160
that actually draws on a bunch of prior

566
00:22:13,160 --> 00:22:15,160
cognitive science experiments

567
00:22:15,160 --> 00:22:18,680
in which real people were asked to draw various inferences

568
00:22:18,680 --> 00:22:21,160
about which teams of players might win different games

569
00:22:21,160 --> 00:22:22,840
of tug of war based on the games

570
00:22:22,840 --> 00:22:25,360
that you'd previously seen players win or lose.

571
00:22:25,360 --> 00:22:27,920
And so this is older work from Josh's group

572
00:22:27,920 --> 00:22:30,560
that demonstrated I think the sense in which

573
00:22:30,560 --> 00:22:32,640
this normative model, this Norvig model

574
00:22:32,640 --> 00:22:34,960
of probabilistic inference actually in many ways

575
00:22:34,960 --> 00:22:37,720
predicts the actual behaviors and predictions made by humans

576
00:22:38,560 --> 00:22:40,560
using a very general probabilistic model

577
00:22:40,560 --> 00:22:43,120
of the mechanics of this tug of war game.

578
00:22:43,120 --> 00:22:45,840
And our goal here is to show how our framework

579
00:22:45,840 --> 00:22:48,280
we can implement an interface between natural language

580
00:22:48,280 --> 00:22:51,880
and all of the core examples of this older experiment.

581
00:22:51,880 --> 00:22:54,400
So just to go through here, I think what you're seeing

582
00:22:54,400 --> 00:22:56,600
in this little toy example, the world model

583
00:22:56,600 --> 00:22:58,640
that's being defined on the screen

584
00:22:58,640 --> 00:23:00,560
is capturing the basic causal relationship

585
00:23:00,560 --> 00:23:04,720
by which properties of different human players

586
00:23:04,720 --> 00:23:06,600
might influence the outcomes of different tournaments

587
00:23:06,640 --> 00:23:08,400
that they play in tug of war.

588
00:23:08,400 --> 00:23:12,440
So for instance, here we're modeling players

589
00:23:12,440 --> 00:23:16,360
as having some internal inherent strength value

590
00:23:16,360 --> 00:23:18,360
where strength varies approximately normally

591
00:23:18,360 --> 00:23:20,200
but as this unobserved latent variable

592
00:23:20,200 --> 00:23:21,880
over different kinds of players.

593
00:23:21,880 --> 00:23:24,200
And we also think of players as having

594
00:23:24,200 --> 00:23:26,600
some kind of internal laziness value

595
00:23:26,600 --> 00:23:28,400
which represents the percentage of the time

596
00:23:28,400 --> 00:23:29,440
that they actually don't act

597
00:23:29,440 --> 00:23:32,000
according to their underlying strength.

598
00:23:32,000 --> 00:23:35,000
And how did these variables determine the outcomes

599
00:23:35,000 --> 00:23:37,920
that we observe of given games of tug of war?

600
00:23:37,920 --> 00:23:40,280
Well, the strength of a whole team of players

601
00:23:40,280 --> 00:23:44,080
depends on the cumulative sum of its player strengths.

602
00:23:44,080 --> 00:23:46,280
But if a player is deciding to be lazy in this game,

603
00:23:46,280 --> 00:23:48,040
they might not pull as hard as they could.

604
00:23:48,040 --> 00:23:49,720
And whichever team pulls with the most strength

605
00:23:49,720 --> 00:23:52,560
in a given match is going to win that match, yeah.

606
00:23:52,560 --> 00:23:55,080
Did you design the primitives of strength and laziness

607
00:23:55,080 --> 00:23:57,400
or a codex come up with the primitives themselves?

608
00:23:57,400 --> 00:23:59,680
So in this one, we're looking at a model

609
00:23:59,680 --> 00:24:00,720
that's derived from the older work.

610
00:24:00,720 --> 00:24:03,200
So these are designed, but yes, that's later in this work.

611
00:24:03,200 --> 00:24:04,160
We're gonna show some examples

612
00:24:04,160 --> 00:24:05,720
of how you can learn this kind of model

613
00:24:05,720 --> 00:24:07,240
from someone just talking about it in language

614
00:24:07,240 --> 00:24:09,720
like the definition that I just gave.

615
00:24:09,720 --> 00:24:10,920
Right, and so again, you know,

616
00:24:10,920 --> 00:24:12,200
this is a really simple example,

617
00:24:12,200 --> 00:24:14,200
but I think also one that actually captures

618
00:24:14,200 --> 00:24:16,520
a surprising amount of the basic causal knowledge

619
00:24:16,520 --> 00:24:18,280
that people have if you tell them

620
00:24:18,280 --> 00:24:20,240
that you're gonna be listening to tug of war games,

621
00:24:20,240 --> 00:24:21,760
but sometimes people can be lazy

622
00:24:21,760 --> 00:24:23,440
and not pull as hard as they could.

623
00:24:23,440 --> 00:24:26,480
So how do we go about relating language in this domain?

624
00:24:27,680 --> 00:24:31,200
Right, well, one means by which we can induce

625
00:24:31,200 --> 00:24:33,120
a simple notion of a meaning function

626
00:24:33,120 --> 00:24:35,440
that actually fits the definition we just gave

627
00:24:35,440 --> 00:24:37,360
is by conditioning a language model

628
00:24:37,360 --> 00:24:40,840
both on this context-specific generative world model

629
00:24:40,840 --> 00:24:44,000
and on a few examples showing how language is mapped

630
00:24:44,000 --> 00:24:46,560
into sampled probabilistic programming expressions

631
00:24:46,560 --> 00:24:47,460
in this domain.

632
00:24:48,800 --> 00:24:50,520
And what we've done now, right,

633
00:24:50,520 --> 00:24:54,560
is effectively induce this kind of situation-specific

634
00:24:54,560 --> 00:24:57,680
contextual mapping from arbitrary new sentences

635
00:24:57,680 --> 00:24:59,000
to expressions that conditions

636
00:24:59,000 --> 00:25:02,680
both on the general prior distribution that codex is

637
00:25:02,680 --> 00:25:04,480
over language and code,

638
00:25:04,480 --> 00:25:07,320
and this kind of specific discourse thinking context

639
00:25:07,320 --> 00:25:10,720
of how language is being used in this situation.

640
00:25:10,720 --> 00:25:12,680
And there are clearly other ways to do this,

641
00:25:12,680 --> 00:25:14,080
some of which we'll talk about later,

642
00:25:14,080 --> 00:25:16,640
but we're using this example to illustrate

643
00:25:16,640 --> 00:25:17,660
just how much you might be able to do

644
00:25:17,660 --> 00:25:19,540
with this kind of minimal implementation,

645
00:25:19,540 --> 00:25:20,960
a notion of a model that translates

646
00:25:20,960 --> 00:25:22,800
between language to code.

647
00:25:22,800 --> 00:25:26,260
Right, so what kinds of language might we say here

648
00:25:26,260 --> 00:25:27,520
and how might we think about them

649
00:25:27,520 --> 00:25:29,640
in relation to probabilistic programming expressions?

650
00:25:29,640 --> 00:25:34,080
Well, a general proposition, like Josh won against Leo,

651
00:25:34,080 --> 00:25:37,000
gets translated into or might map,

652
00:25:37,000 --> 00:25:40,440
we might think of mapping or meaning a conditioned statement,

653
00:25:40,440 --> 00:25:42,880
an observation that Josh won against Leo.

654
00:25:43,740 --> 00:25:45,520
If we make subsequent observations,

655
00:25:45,520 --> 00:25:49,200
like then Josh went on to claim victory against Alex,

656
00:25:49,200 --> 00:25:51,840
we can continue to kind of generally use this

657
00:25:51,840 --> 00:25:54,640
meaning function that we've induced to turn that

658
00:25:54,640 --> 00:25:56,400
into a probabilistic programming language

659
00:25:56,400 --> 00:25:58,540
that captures the fact that Josh won against Alex.

660
00:25:58,580 --> 00:26:01,660
If we then say that even working together as a team,

661
00:26:01,660 --> 00:26:03,740
Leo and Alex still couldn't beat Josh

662
00:26:03,740 --> 00:26:05,260
in this game of tech of war.

663
00:26:06,820 --> 00:26:09,140
At this point, if we want to answer a query,

664
00:26:09,140 --> 00:26:12,020
like, okay, wait, how strong is Josh?

665
00:26:12,940 --> 00:26:17,020
What we think of as thinking in this situation

666
00:26:17,020 --> 00:26:20,860
is actually sampling from the posterior

667
00:26:20,860 --> 00:26:22,900
over possible worlds from the generative model

668
00:26:22,900 --> 00:26:25,220
that we just defined, subject to the observations

669
00:26:25,220 --> 00:26:26,340
that we've just made.

670
00:26:26,340 --> 00:26:29,540
And indeed, that means that the meaning of a sentence

671
00:26:29,540 --> 00:26:31,380
like how strong is Josh is really

672
00:26:31,380 --> 00:26:34,420
a structured publicistic inference query.

673
00:26:34,420 --> 00:26:37,780
What is the latent variable that is Josh's strength?

674
00:26:37,780 --> 00:26:40,460
And what we see here is that given his track record,

675
00:26:40,460 --> 00:26:42,660
all these people that he's beating, even playing together,

676
00:26:42,660 --> 00:26:44,580
our inference is that Josh is likely

677
00:26:44,580 --> 00:26:46,380
a good bit stronger than average.

678
00:26:46,380 --> 00:26:48,140
And that also means coherently,

679
00:26:48,140 --> 00:26:50,180
we might expect that a priori,

680
00:26:50,180 --> 00:26:52,820
a new player we've never seen, like Gabe,

681
00:26:52,820 --> 00:26:54,580
is going to be unlikely to beat him.

682
00:26:55,580 --> 00:26:59,260
So if we ask what are the odds of Gabe at beating Josh,

683
00:26:59,260 --> 00:27:01,300
we see that we think it's somewhat unlikely.

684
00:27:04,420 --> 00:27:05,260
Question?

685
00:27:05,260 --> 00:27:06,100
Oh, yes.

686
00:27:06,100 --> 00:27:08,420
So on the how strong is Josh,

687
00:27:08,420 --> 00:27:10,220
it seems like there's an interesting thing here

688
00:27:10,220 --> 00:27:11,860
where there's also an implicit question

689
00:27:11,860 --> 00:27:15,020
of what the word strong means in this context.

690
00:27:16,380 --> 00:27:20,300
And that, like, right, it's like not a number,

691
00:27:20,300 --> 00:27:22,580
it's kind of some like, comparative adjective.

692
00:27:22,580 --> 00:27:24,580
It's like, probably a non-intersective.

693
00:27:25,580 --> 00:27:27,780
So I guess, is that something you think about?

694
00:27:27,780 --> 00:27:29,740
This framework, or should I just be kind of, like,

695
00:27:29,740 --> 00:27:31,860
ignoring this sort of issue?

696
00:27:31,860 --> 00:27:33,340
Yeah, well, okay.

697
00:27:33,340 --> 00:27:34,740
So I think there's a number of ways

698
00:27:34,740 --> 00:27:35,860
that we can think about that.

699
00:27:35,860 --> 00:27:40,860
I mean, so, right, so the one sense we could say is like,

700
00:27:41,180 --> 00:27:42,380
right, how strong is Josh?

701
00:27:42,380 --> 00:27:44,980
Isn't, the answer to how strong of Josh isn't a number.

702
00:27:44,980 --> 00:27:47,380
Rather, it's kind of this distribution

703
00:27:47,380 --> 00:27:49,100
over this posterior distribution

704
00:27:49,100 --> 00:27:51,700
of various underlying strength values

705
00:27:51,700 --> 00:27:54,860
that we currently might infer that Josh has

706
00:27:54,860 --> 00:27:56,940
with respect to the general value that we have.

707
00:27:56,940 --> 00:27:59,660
I think another kind of popular definition

708
00:27:59,660 --> 00:28:02,020
of various uncertain adjectives,

709
00:28:02,020 --> 00:28:03,580
like a word like strong, right,

710
00:28:03,580 --> 00:28:06,160
is that you have some internal threshold value,

711
00:28:06,160 --> 00:28:08,020
or the person speaking has some kind of internal

712
00:28:08,020 --> 00:28:12,340
threshold value that you must kind of jointly infer

713
00:28:12,340 --> 00:28:14,860
with respect to the context in what you've seen.

714
00:28:14,860 --> 00:28:17,540
And some of the examples that I'll actually give later,

715
00:28:17,540 --> 00:28:19,860
so, right, there's kind of a long line of work

716
00:28:19,860 --> 00:28:21,540
in linguistics, including some work

717
00:28:21,540 --> 00:28:23,700
that treats that as like a pragmatic inference.

718
00:28:23,700 --> 00:28:25,340
I think some of the interesting work

719
00:28:25,340 --> 00:28:27,900
that we'll show a little bit later is that,

720
00:28:27,900 --> 00:28:29,340
there are some ways in which you might think

721
00:28:29,340 --> 00:28:32,820
of this mapping function as actually being a general one

722
00:28:32,820 --> 00:28:34,860
that includes that notion of pragmatic inference.

723
00:28:34,860 --> 00:28:36,940
And also, I think captures the sense

724
00:28:36,940 --> 00:28:38,740
in which if you continually,

725
00:28:38,740 --> 00:28:41,180
are you really doing this kind of pragmatic inference

726
00:28:41,180 --> 00:28:43,020
all the time, or do you actually,

727
00:28:43,020 --> 00:28:44,420
in many general settings,

728
00:28:44,420 --> 00:28:46,460
like talking about the strengths of people,

729
00:28:46,460 --> 00:28:49,340
actually have some kind of cashed older notion of strength

730
00:28:49,340 --> 00:28:50,180
that you can draw in.

731
00:28:50,180 --> 00:28:52,940
And I think actually, this notion of large language models

732
00:28:52,940 --> 00:28:55,460
as just being this learned mapping function

733
00:28:55,460 --> 00:28:57,580
from language into expressions include,

734
00:28:57,580 --> 00:28:59,220
can also capture the sense in which

735
00:28:59,220 --> 00:29:01,140
that knowledge is amortized away.

736
00:29:01,140 --> 00:29:02,860
And you might not be having that inference.

737
00:29:02,860 --> 00:29:03,700
Yeah, Chris.

738
00:29:08,500 --> 00:29:11,700
She's sort of just denying or ignoring

739
00:29:11,700 --> 00:29:16,420
what makes people so excited about large language models

740
00:29:16,460 --> 00:29:21,460
in their meaning representation and ability to do inference.

741
00:29:24,540 --> 00:29:27,100
I mean, because, okay, you've got sort of

742
00:29:27,100 --> 00:29:29,100
cooler probabilistic programming language

743
00:29:29,100 --> 00:29:30,860
on the right hand side,

744
00:29:30,860 --> 00:29:34,580
but in some sense, the picture is still,

745
00:29:34,580 --> 00:29:39,580
this is semantic parsing, like it was 2010 to 2015.

746
00:29:39,580 --> 00:29:44,580
And yes, you're using a large language model,

747
00:29:44,780 --> 00:29:48,300
but you're not actually using the excitement

748
00:29:48,300 --> 00:29:52,020
of a large language model as a representation system.

749
00:29:52,020 --> 00:29:54,540
Yeah, and I think, so probably each of us

750
00:29:54,540 --> 00:29:55,700
would have different answers to this,

751
00:29:55,700 --> 00:29:57,380
but part of what we're hoping to paint out

752
00:29:57,380 --> 00:29:59,220
over the course of this talk is I think

753
00:29:59,220 --> 00:30:02,100
some of the ways in which actually, right,

754
00:30:02,100 --> 00:30:04,900
of course, no one wants to say we're gonna go back

755
00:30:04,900 --> 00:30:07,020
to kind of the brittleness of semantic parsing,

756
00:30:07,020 --> 00:30:08,780
but I think one thing that large language models

757
00:30:08,780 --> 00:30:11,460
actually give us, or one proposal in this talk,

758
00:30:11,500 --> 00:30:15,580
is that there are some aspects of the theory,

759
00:30:15,580 --> 00:30:17,260
kind of the classic notion of linguistics,

760
00:30:17,260 --> 00:30:19,420
and certainly the classic notions of semantic parsing

761
00:30:19,420 --> 00:30:20,860
that actually normatively capture

762
00:30:20,860 --> 00:30:23,580
a lot of what we really might want when we think about,

763
00:30:23,580 --> 00:30:28,580
so one answer for an AI system is, well, yes,

764
00:30:28,580 --> 00:30:29,700
in some way, we don't wanna throw,

765
00:30:29,700 --> 00:30:31,100
certainly we don't wanna throw away

766
00:30:31,100 --> 00:30:32,980
everything that we're learning from large language models,

767
00:30:32,980 --> 00:30:34,340
and I think one answer to that

768
00:30:34,340 --> 00:30:37,260
is kind of the answer that I gave to Jacob, right?

769
00:30:37,260 --> 00:30:39,860
If we think about not always, you know,

770
00:30:39,860 --> 00:30:42,980
in these examples, we're showing this very direct system

771
00:30:42,980 --> 00:30:44,940
in which we always start with language

772
00:30:44,940 --> 00:30:46,700
and we always map into some sort of

773
00:30:46,700 --> 00:30:47,620
probabilistic programming expression,

774
00:30:47,620 --> 00:30:49,540
and that's where all of the thinking happens,

775
00:30:49,540 --> 00:30:51,980
and we might think, well, that doesn't totally make sense

776
00:30:51,980 --> 00:30:55,100
because there are lots of cases where, as you're saying,

777
00:30:55,100 --> 00:30:56,380
we have every reason to believe

778
00:30:56,380 --> 00:30:58,380
that large language models have learned

779
00:30:58,380 --> 00:31:00,020
a lot of latent information.

780
00:31:00,020 --> 00:31:01,940
They can do a lot of, they certainly have

781
00:31:01,940 --> 00:31:03,780
a lot of latent conceptual information,

782
00:31:03,780 --> 00:31:05,580
and maybe to some degree, they can even perform

783
00:31:05,580 --> 00:31:08,260
certain kinds of limited, amortized inferences,

784
00:31:08,260 --> 00:31:10,780
or reuse old inferences that they've learned

785
00:31:10,780 --> 00:31:11,740
from other people I've had,

786
00:31:11,740 --> 00:31:13,420
and so in the second part of this talk,

787
00:31:13,420 --> 00:31:15,220
we're going to show different ways in which,

788
00:31:15,220 --> 00:31:17,500
well, this probabilistic programming language itself

789
00:31:17,500 --> 00:31:20,340
doesn't necessarily need to be something that's isolated

790
00:31:20,340 --> 00:31:22,140
from what large language models have learned.

791
00:31:22,140 --> 00:31:25,740
It also can embed calls to large language models

792
00:31:25,740 --> 00:31:28,100
within it to kind of draw in that sort of knowledge.

793
00:31:28,100 --> 00:31:30,860
Haven't you gone back to the visualness of semantic housing

794
00:31:30,860 --> 00:31:33,340
because you're doing this translation

795
00:31:34,340 --> 00:31:39,260
into symbolic semantic representation,

796
00:31:39,260 --> 00:31:42,540
which really ends with your actual result,

797
00:31:42,540 --> 00:31:44,860
and it's riddled in the same way?

798
00:31:44,860 --> 00:31:48,820
Well, right, and also, so no, I think I would say,

799
00:31:48,820 --> 00:31:53,340
I don't totally think that the way in which we're using,

800
00:31:53,340 --> 00:31:55,620
or the sense in which, or I think there are some ways

801
00:31:55,620 --> 00:31:58,980
in which this kind of broader definition

802
00:31:58,980 --> 00:32:01,540
in which you are saying, well, the meaning of a distribution,

803
00:32:01,540 --> 00:32:04,140
or the meaning of a sentence in language

804
00:32:04,140 --> 00:32:06,900
isn't just one probabilistic programming expression, right?

805
00:32:06,900 --> 00:32:10,260
That's what we're showing here for pedagogical purposes,

806
00:32:10,260 --> 00:32:12,180
but you might say, well, okay, right,

807
00:32:12,180 --> 00:32:16,460
how are you going to obey kind of the ambiguity of language?

808
00:32:16,460 --> 00:32:19,620
There are kinds of sentences that are definitely ambiguous.

809
00:32:19,620 --> 00:32:23,100
So one example that we've looked at are sentences

810
00:32:23,100 --> 00:32:24,580
in which you say something like,

811
00:32:24,580 --> 00:32:26,780
Josh beat Alex and Leo, right?

812
00:32:26,780 --> 00:32:28,260
And you might ask, well, you know,

813
00:32:28,260 --> 00:32:30,540
that's kind of a classic syntactic construction.

814
00:32:30,580 --> 00:32:33,420
Does that mean that Josh beat Alex and Leo,

815
00:32:33,420 --> 00:32:34,940
and they were playing on the same team,

816
00:32:34,940 --> 00:32:38,260
or Josh beat Alex, and then Josh went on to beat Leo?

817
00:32:38,260 --> 00:32:42,380
And what we see, or generally, what you might say is,

818
00:32:42,380 --> 00:32:44,060
well, the meaning of that sentence

819
00:32:44,060 --> 00:32:47,820
actually shouldn't be picking one expression or the other.

820
00:32:47,820 --> 00:32:49,780
It should be kind of the distribution

821
00:32:49,780 --> 00:32:52,820
over those possible parses,

822
00:32:52,820 --> 00:32:55,460
and that distribution also shouldn't just be something

823
00:32:55,460 --> 00:32:58,060
that we can determine in this totally context

824
00:32:58,060 --> 00:33:00,180
and sensitive way, it should actually depend on

825
00:33:00,180 --> 00:33:02,300
all the previous patterns in the discourse.

826
00:33:02,300 --> 00:33:06,180
So if someone's continually been using this conjunctive

827
00:33:06,180 --> 00:33:10,660
and to refer to teams of players playing together,

828
00:33:10,660 --> 00:33:13,300
we should take that kind of discourse bias into account.

829
00:33:13,300 --> 00:33:16,780
And I think actually, right, this provides,

830
00:33:16,780 --> 00:33:19,580
or thinking of large language models

831
00:33:19,580 --> 00:33:21,260
as kind of generally having learned

832
00:33:21,260 --> 00:33:22,860
this broad joint distribution,

833
00:33:22,860 --> 00:33:26,500
but one that can be kind of conditioned quite richly

834
00:33:26,500 --> 00:33:30,620
both on the content of this generative model.

835
00:33:30,620 --> 00:33:32,220
So it's not trying to come up

836
00:33:32,220 --> 00:33:34,460
with a universal definition of strength.

837
00:33:34,460 --> 00:33:36,140
It's not even necessarily trying to come up

838
00:33:36,140 --> 00:33:38,500
with a universal definition of any of these words.

839
00:33:38,500 --> 00:33:40,500
It's thinking about how they might map contextually

840
00:33:40,500 --> 00:33:43,620
into the best possible expression in the context

841
00:33:43,620 --> 00:33:48,500
of a particular local model built for a particular situation.

842
00:33:48,500 --> 00:33:52,180
I think is obviously related to,

843
00:33:52,180 --> 00:33:57,180
but attempting to address some of the brittleness challenges

844
00:33:57,220 --> 00:33:59,420
of semantic parsing in the past.

845
00:33:59,420 --> 00:34:00,780
I think another answer to this, right,

846
00:34:00,780 --> 00:34:04,540
is that part of the problem of semantic parsing previously

847
00:34:04,540 --> 00:34:06,100
has been actually that the mapping functions

848
00:34:06,100 --> 00:34:09,020
have historically been difficult to get right.

849
00:34:09,020 --> 00:34:10,820
Whether you were thinking about those

850
00:34:10,820 --> 00:34:13,380
as kind of old hard-coded grammars

851
00:34:13,380 --> 00:34:16,060
or many of the attempts to kind of learn these things,

852
00:34:16,060 --> 00:34:17,940
we are very domain-specific supervision.

853
00:34:17,940 --> 00:34:19,740
So you wanna have a semantic parser

854
00:34:19,740 --> 00:34:21,740
for a particular robotics domain.

855
00:34:21,740 --> 00:34:25,660
You need a thousand examples of sentences

856
00:34:25,660 --> 00:34:27,660
about that particular robotics domain

857
00:34:27,660 --> 00:34:30,340
and a thousand paired with a thousand examples

858
00:34:30,340 --> 00:34:35,340
of programs that are operating on that particular domain.

859
00:34:35,660 --> 00:34:38,420
What we're seeing here is I think something that says no.

860
00:34:38,420 --> 00:34:40,460
What it means to learn language generally

861
00:34:40,460 --> 00:34:42,780
is to learn kind of this general mapping

862
00:34:42,780 --> 00:34:45,980
between language and some kind of underlying representation.

863
00:34:47,340 --> 00:34:51,420
And also, one reason why we might want a system like this

864
00:34:51,420 --> 00:34:53,980
is because we want to be able to condition coherently

865
00:34:53,980 --> 00:34:57,940
on information that's not just coming from language.

866
00:34:57,940 --> 00:35:01,860
And we want to think about how a general substrate

867
00:35:01,860 --> 00:35:05,660
in which the only, yes, we might be told

868
00:35:05,660 --> 00:35:06,900
that Josh went against Leo,

869
00:35:06,900 --> 00:35:09,700
but we might also be watching videos

870
00:35:09,700 --> 00:35:12,460
that give us information about Josh's strength,

871
00:35:12,460 --> 00:35:13,460
that convey our observations.

872
00:35:13,460 --> 00:35:16,340
We might also have seen pictures

873
00:35:16,340 --> 00:35:18,860
like the ones in the stimuli that we saw before

874
00:35:18,900 --> 00:35:22,620
demonstrating the results of previous outcomes of matches.

875
00:35:22,620 --> 00:35:25,860
And I think one thing that suggests

876
00:35:25,860 --> 00:35:29,740
is we want this kind of general substrate

877
00:35:30,740 --> 00:35:32,940
in which we can think about how those observations,

878
00:35:32,940 --> 00:35:35,340
including the observations from language,

879
00:35:35,340 --> 00:35:37,900
but without prioritizing language in any way,

880
00:35:37,900 --> 00:35:40,380
I think are coherently considered.

881
00:35:42,900 --> 00:35:47,260
So I think it depends on what part of semantic parsing

882
00:35:47,260 --> 00:35:50,940
you, or yeah, I think the answer to that depends

883
00:35:50,940 --> 00:35:53,380
on what part of semantic parsing we think of

884
00:35:53,380 --> 00:35:55,940
as being the source of the brittleness

885
00:35:55,940 --> 00:35:59,500
that caused us to throw that paradigm into question.

886
00:36:01,340 --> 00:36:04,980
Yeah, and maybe I'll just offer one more perspective.

887
00:36:04,980 --> 00:36:07,700
So one part of it is what Leo is saying.

888
00:36:07,700 --> 00:36:09,740
Traditional semantic parsing is brittle in two ways.

889
00:36:09,740 --> 00:36:11,540
One is, do you have broad coverage of language

890
00:36:11,540 --> 00:36:13,580
that you can parse into your system?

891
00:36:13,580 --> 00:36:15,940
And two is like how broad coverage

892
00:36:15,940 --> 00:36:18,060
are the set of query, like the set of semantic queries

893
00:36:18,060 --> 00:36:19,420
that you can actually answer.

894
00:36:19,420 --> 00:36:21,020
And I think what you're pointing out is

895
00:36:21,020 --> 00:36:23,100
this doesn't seem to address the second source

896
00:36:23,100 --> 00:36:24,580
of brittleness, which is that your system

897
00:36:24,580 --> 00:36:25,580
can only answer certain things,

898
00:36:25,580 --> 00:36:27,460
it can only reason about certain things.

899
00:36:28,460 --> 00:36:32,180
Brittleness of the formal representation language

900
00:36:32,180 --> 00:36:33,020
that you're using.

901
00:36:33,020 --> 00:36:38,020
Right, that's true of large language model representations.

902
00:36:38,860 --> 00:36:43,860
Right, so I think my sort of take on that

903
00:36:44,380 --> 00:36:48,100
is from a kind of AI engineering perspective

904
00:36:48,100 --> 00:36:52,860
is sort of a branching in two directions.

905
00:36:52,860 --> 00:36:55,740
One is, I think we have made some progress

906
00:36:55,740 --> 00:36:59,340
that this is not really evoking, probably,

907
00:36:59,340 --> 00:37:01,980
toward systems that within restricted domains

908
00:37:03,100 --> 00:37:06,140
can reason coherently and probabilistically

909
00:37:06,140 --> 00:37:07,900
about a wide range of queries.

910
00:37:07,900 --> 00:37:12,900
So we have systems like this inference QL system

911
00:37:13,180 --> 00:37:16,180
that uses nonparametric bays to analyze huge data tables

912
00:37:16,180 --> 00:37:19,020
and come up with a model of that system

913
00:37:19,020 --> 00:37:22,660
that or of your data that can answer all sorts of questions

914
00:37:22,660 --> 00:37:27,660
like, oh, you know, show me like which people

915
00:37:29,380 --> 00:37:32,420
in this data set are like probably overpaid

916
00:37:32,420 --> 00:37:34,340
given their experience or something like that.

917
00:37:34,340 --> 00:37:36,820
So in the same way that people are kind of excited

918
00:37:36,820 --> 00:37:39,860
about using natural language or using language models

919
00:37:39,860 --> 00:37:42,500
to parse into SQL, right?

920
00:37:42,500 --> 00:37:44,980
Because so much data is in SQL

921
00:37:44,980 --> 00:37:48,220
and it's a very SQL is a very expressive language

922
00:37:48,220 --> 00:37:50,220
for asking questions about that data.

923
00:37:51,300 --> 00:37:53,220
When we have a probabilistic system,

924
00:37:53,220 --> 00:37:55,740
like a good probabilistic model of that data under the hood,

925
00:37:55,740 --> 00:37:58,580
it enables conversational patterns that are not enabled

926
00:37:58,580 --> 00:38:01,020
when you have like SQL as the database

927
00:38:01,020 --> 00:38:03,020
because we expect our conversational partners

928
00:38:03,020 --> 00:38:05,100
to have coherent beliefs about the world,

929
00:38:05,100 --> 00:38:07,620
to update those beliefs in response to new evidence

930
00:38:07,660 --> 00:38:11,100
that we give it to be able to report uncertainty

931
00:38:12,540 --> 00:38:14,300
and make sort of modal judgments.

932
00:38:14,300 --> 00:38:18,860
And so one engineering path is to take those kinds of systems

933
00:38:18,860 --> 00:38:22,700
and sort of build conversational interfaces to them

934
00:38:22,700 --> 00:38:25,860
that behave more like an intelligent person would behave

935
00:38:25,860 --> 00:38:28,700
and can draw inferences that you might not draw

936
00:38:28,700 --> 00:38:30,820
if you're just talking to a SQL backup.

937
00:38:30,820 --> 00:38:33,020
The other path that sort of we'll talk about

938
00:38:33,020 --> 00:38:35,540
in the next part of the talk

939
00:38:35,580 --> 00:38:37,780
is how can we use those representations

940
00:38:37,780 --> 00:38:39,220
that language models have learned

941
00:38:39,220 --> 00:38:43,180
to make the probabilistic inferences more interesting

942
00:38:43,180 --> 00:38:45,100
and more robust, less brittle,

943
00:38:45,100 --> 00:38:48,580
without sort of totally embracing the other kind of brittleness

944
00:38:48,580 --> 00:38:50,180
which is the kind of brittleness that language models

945
00:38:50,180 --> 00:38:51,020
seem to have right now,

946
00:38:51,020 --> 00:38:52,540
which is that they draw,

947
00:38:52,540 --> 00:38:53,940
that they don't really necessarily reason

948
00:38:53,940 --> 00:38:56,340
with coherent probabilistic beliefs.

949
00:38:56,340 --> 00:38:59,340
So maybe, yeah, let's go into that next part.

950
00:38:59,340 --> 00:39:03,340
Yes, yes, okay, right.

951
00:39:04,300 --> 00:39:06,540
Well, so right, so in the interest of time,

952
00:39:06,540 --> 00:39:07,940
I'm actually gonna like skip through

953
00:39:07,940 --> 00:39:09,220
some of the rest of this example,

954
00:39:09,220 --> 00:39:11,660
which I think is just more of what you've seen,

955
00:39:11,660 --> 00:39:13,860
but one sense in which I think,

956
00:39:13,860 --> 00:39:16,540
yeah, maybe a third part of the answer to Chris,

957
00:39:16,540 --> 00:39:17,740
I would say, is that,

958
00:39:20,460 --> 00:39:22,300
right, you know, yeah.

959
00:39:22,300 --> 00:39:23,900
I think part of what this is trying to do

960
00:39:23,900 --> 00:39:26,700
is explore some of the ways in which we might answer ways,

961
00:39:26,700 --> 00:39:29,820
right, without giving an answer in all the ways in which

962
00:39:29,820 --> 00:39:32,780
we might answer the ways in which language models themselves

963
00:39:32,780 --> 00:39:35,260
are brittle with respect to what we also want

964
00:39:35,260 --> 00:39:36,860
from a model of intelligence, right?

965
00:39:36,860 --> 00:39:38,980
We might suspect that when we answer,

966
00:39:38,980 --> 00:39:40,540
ask questions like this,

967
00:39:40,540 --> 00:39:42,980
really what we are trying to do is specify

968
00:39:42,980 --> 00:39:46,980
some kind of normative query that captures formally

969
00:39:46,980 --> 00:39:50,260
a sense of, well, we want something like the posterior

970
00:39:50,260 --> 00:39:52,980
with respect to some kind of internal model of the world.

971
00:39:52,980 --> 00:39:55,780
And, you know, this is kind of the simplest means,

972
00:39:55,780 --> 00:39:57,620
or this is a very simple example

973
00:39:57,620 --> 00:40:00,380
of how we might formally impose that kind of structure,

974
00:40:01,380 --> 00:40:03,620
but one that I think can be elaborated on,

975
00:40:05,300 --> 00:40:07,660
depending on the kinds of primitives

976
00:40:07,660 --> 00:40:09,140
and the ways in which you're thinking about

977
00:40:09,140 --> 00:40:12,220
what it is that the probabilistic programs can express, right?

978
00:40:12,220 --> 00:40:15,460
So one way in which we might think about doing that

979
00:40:16,580 --> 00:40:19,820
is by thinking about probabilistic programs

980
00:40:19,820 --> 00:40:23,660
that themselves have access to other kinds of means

981
00:40:23,660 --> 00:40:26,820
of calling other different mechanisms and cognition, right?

982
00:40:26,820 --> 00:40:29,180
So I think I would draw a contrast here

983
00:40:29,300 --> 00:40:31,900
between the notion of the large language model

984
00:40:31,900 --> 00:40:34,620
as a controller, the one that's making the decisions

985
00:40:34,620 --> 00:40:37,340
about when to write little snippets of code

986
00:40:37,340 --> 00:40:41,460
and to execute them, when to call out to little planners

987
00:40:41,460 --> 00:40:44,340
and incorporate them, or stuff like the Minds Eye work,

988
00:40:44,340 --> 00:40:46,140
right, where there's a language model,

989
00:40:46,140 --> 00:40:48,540
it decides when to call out to a physics simulator,

990
00:40:48,540 --> 00:40:51,460
but the way it interprets the outputs

991
00:40:51,460 --> 00:40:53,620
of that physics simulator is to paste those back

992
00:40:53,620 --> 00:40:55,700
into the language model context

993
00:40:55,700 --> 00:40:58,540
and try to draw inferences on them in turn.

994
00:40:58,540 --> 00:41:00,580
Rather, in this kind of framework, right,

995
00:41:00,580 --> 00:41:03,460
what you can kind of see a, or yeah,

996
00:41:03,460 --> 00:41:05,700
the direction that this framework would be pointing towards

997
00:41:05,700 --> 00:41:09,300
is to say, well, on the other hand,

998
00:41:09,300 --> 00:41:12,700
we already have languages that allow us to do things

999
00:41:12,700 --> 00:41:14,980
like build expressive generative models

1000
00:41:14,980 --> 00:41:17,900
over three-dimensional scenes that also capture things

1001
00:41:17,900 --> 00:41:20,020
that we might want only from perception,

1002
00:41:20,020 --> 00:41:22,540
like knowledge about how the shapes of objects

1003
00:41:22,540 --> 00:41:24,380
tend to accude each other,

1004
00:41:24,380 --> 00:41:27,460
or incorporate rich models of physics,

1005
00:41:27,460 --> 00:41:31,340
or that model theory of mind as taking place recursively

1006
00:41:31,340 --> 00:41:33,860
and thinking about agents who themselves have beliefs

1007
00:41:33,860 --> 00:41:35,780
about their own internal world models

1008
00:41:35,780 --> 00:41:39,420
and are actually choosing their actions as planners, right?

1009
00:41:39,420 --> 00:41:42,060
And in this kind of framework,

1010
00:41:42,060 --> 00:41:44,900
you can point the way towards a kind of model that says,

1011
00:41:44,900 --> 00:41:47,780
well, how is it that I might incorporate language

1012
00:41:47,780 --> 00:41:50,340
into these kinds of models sitting alongside

1013
00:41:50,340 --> 00:41:53,140
these other kind of observations that I might make, right?

1014
00:41:53,140 --> 00:41:56,140
So how might I think about the meaning

1015
00:41:56,380 --> 00:41:58,940
of images that I want to generate

1016
00:41:58,940 --> 00:42:01,260
that specify specific constraints,

1017
00:42:01,260 --> 00:42:05,420
or imagination, or, right, go ahead, Jacob.

1018
00:42:05,420 --> 00:42:07,060
Yeah, I just had a clarification question.

1019
00:42:07,060 --> 00:42:11,060
So you were talking earlier about having this meeting function,

1020
00:42:11,060 --> 00:42:13,340
and then I think also we're mentioning something

1021
00:42:13,340 --> 00:42:18,340
about like code x, in terms of the questions.

1022
00:42:18,420 --> 00:42:21,540
I'm just trying to understand which of these is that.

1023
00:42:21,540 --> 00:42:23,460
Is that here, or is the meeting function

1024
00:42:23,460 --> 00:42:24,700
when you come later?

1025
00:42:25,660 --> 00:42:26,780
So that's maybe the first question

1026
00:42:26,780 --> 00:42:27,860
and then the other clarification is,

1027
00:42:27,860 --> 00:42:30,420
so are these statements actually just been programmatically

1028
00:42:30,420 --> 00:42:34,620
created from code x by prompting the text that was on?

1029
00:42:34,620 --> 00:42:35,460
Yes, that's right.

1030
00:42:35,460 --> 00:42:40,460
So by meeting function in this framework, we say,

1031
00:42:40,500 --> 00:42:42,100
well, there's kind of two generalizations

1032
00:42:42,100 --> 00:42:42,940
of a meeting function.

1033
00:42:42,940 --> 00:42:45,140
There's a general joint prior, right?

1034
00:42:45,140 --> 00:42:46,660
That code x is already,

1035
00:42:46,660 --> 00:42:47,940
that it's learned between language and code,

1036
00:42:47,940 --> 00:42:50,740
and then there's this kind of context specific

1037
00:42:50,740 --> 00:42:53,300
meeting function in the sense that it's conditioned

1038
00:42:53,300 --> 00:42:55,860
on whatever's in the prompt, the generative model,

1039
00:42:55,860 --> 00:42:59,940
and some examples of how language relates to expressions

1040
00:42:59,940 --> 00:43:01,660
that it's doing, so that's a meeting function.

1041
00:43:01,660 --> 00:43:03,500
And yes, all these examples that you're seeing

1042
00:43:03,500 --> 00:43:05,980
are one sample from that distribution.

1043
00:43:09,500 --> 00:43:13,060
Right, and one of the things that I wanna point to here,

1044
00:43:13,060 --> 00:43:15,740
right, is it does, I think it suggests a framework

1045
00:43:15,740 --> 00:43:18,220
or another means of thinking about what it means

1046
00:43:18,220 --> 00:43:21,660
for language to construct new concepts from definitions,

1047
00:43:21,700 --> 00:43:23,460
or even come to construct new world models

1048
00:43:23,460 --> 00:43:25,820
from thinking like somebody in the beginning asked, right?

1049
00:43:25,820 --> 00:43:29,500
So how, for instance, might we think about enriching

1050
00:43:29,500 --> 00:43:31,940
an existing structured relational model

1051
00:43:31,940 --> 00:43:33,900
with concepts that we learn from language?

1052
00:43:33,900 --> 00:43:38,060
So for example, if we consider kind of a formal model

1053
00:43:38,060 --> 00:43:43,060
of kinship relations, we might say that, well,

1054
00:43:43,900 --> 00:43:46,020
the generative model of this domain

1055
00:43:46,020 --> 00:43:49,340
is itself represented as a probabilistic program.

1056
00:43:49,340 --> 00:43:52,940
It captures both the causal means by which

1057
00:43:54,100 --> 00:43:56,900
people give rise to their children,

1058
00:43:56,900 --> 00:44:00,540
and also the definitions or one notion of the definitions

1059
00:44:00,540 --> 00:44:02,620
of what it means to be something like a sister or a father

1060
00:44:02,620 --> 00:44:04,780
with respect to this core notion

1061
00:44:04,780 --> 00:44:07,300
of how family trees come to be.

1062
00:44:07,300 --> 00:44:10,500
And so if you take this kind of general notion

1063
00:44:10,500 --> 00:44:13,900
of the meaning of language as being the distribution

1064
00:44:13,900 --> 00:44:14,900
over expressions that it creates

1065
00:44:14,900 --> 00:44:16,340
in a probabilistic programming language,

1066
00:44:16,340 --> 00:44:20,380
you might start to think how we can formally think about

1067
00:44:20,380 --> 00:44:24,260
relating definitions for various kinds of relational terms.

1068
00:44:25,260 --> 00:44:26,860
An uncle is the brother of one's parent

1069
00:44:26,860 --> 00:44:28,420
or the husband of one's aunt.

1070
00:44:28,420 --> 00:44:31,420
A pibling is a gender neutral term for an aunt or uncle,

1071
00:44:31,420 --> 00:44:33,220
that's the sibling of one's parent,

1072
00:44:34,260 --> 00:44:39,260
or this relational notion of a sister of one's father

1073
00:44:39,980 --> 00:44:41,420
from a language that's actually not found

1074
00:44:41,420 --> 00:44:42,780
anywhere on the internet.

1075
00:44:42,780 --> 00:44:45,620
And I think the core thing that we wanna suggest here, right,

1076
00:44:46,020 --> 00:44:48,140
is why do we even have definitions at all?

1077
00:44:49,500 --> 00:44:53,260
Well, one notion of what it even means

1078
00:44:53,260 --> 00:44:55,660
to have learned the definition of this term

1079
00:44:55,660 --> 00:44:58,340
is that it should drive coherently

1080
00:44:58,340 --> 00:45:01,260
all of the downstream inferences that you make with that term,

1081
00:45:01,260 --> 00:45:04,340
and it should graft onto the conceptual knowledge

1082
00:45:04,340 --> 00:45:05,580
that you already have.

1083
00:45:05,580 --> 00:45:10,020
And so you can think about forming new sentences directly

1084
00:45:10,020 --> 00:45:13,380
that refer to someone's paani or one's pibling

1085
00:45:13,380 --> 00:45:15,460
in this situation and expecting them

1086
00:45:15,460 --> 00:45:18,620
to draw both on your existing conceptual knowledge

1087
00:45:18,620 --> 00:45:22,180
of what it even means to have a family tree

1088
00:45:22,180 --> 00:45:24,540
as well as all the other conceptual terms for a friendship

1089
00:45:24,540 --> 00:45:25,780
that you may already have.

1090
00:45:28,060 --> 00:45:32,100
And the same framework also suggests one mechanism

1091
00:45:32,100 --> 00:45:34,340
by which we might formalize what it means

1092
00:45:34,340 --> 00:45:36,660
to learn world models from language.

1093
00:45:36,660 --> 00:45:39,380
So as I mentioned, if we return to the situation

1094
00:45:39,380 --> 00:45:42,580
that opens this talk, tug of war games,

1095
00:45:43,420 --> 00:45:45,820
we might think about how the definition that I gave

1096
00:45:45,820 --> 00:45:48,140
when I sat up here at the podium, right,

1097
00:45:48,140 --> 00:45:50,380
saying there are people whose strength levels vary

1098
00:45:50,380 --> 00:45:52,420
from person to person.

1099
00:45:52,420 --> 00:45:55,940
People have a percentage of time in which they're lazy.

1100
00:45:55,940 --> 00:45:58,900
Strengths of the teams depend on the underlying strengths

1101
00:45:58,900 --> 00:46:01,620
of the members of that team,

1102
00:46:01,620 --> 00:46:03,420
and whether one team beats another

1103
00:46:03,420 --> 00:46:05,980
just depends on which team pulls stronger that match.

1104
00:46:05,980 --> 00:46:09,060
And this kind of setting is actually language, right,

1105
00:46:09,060 --> 00:46:12,380
is building up the actual generative model itself.

1106
00:46:12,380 --> 00:46:14,580
And you might think of a system like this

1107
00:46:14,580 --> 00:46:17,300
that both learns these kinds of theories from language

1108
00:46:17,300 --> 00:46:21,380
and then is appending to this kind of local problem-based

1109
00:46:21,380 --> 00:46:23,700
context to answer arbitrary questions

1110
00:46:23,700 --> 00:46:25,220
like the kinds that we gave or conditioned

1111
00:46:25,220 --> 00:46:28,220
on various observations like Josh being stronger than Leo

1112
00:46:28,220 --> 00:46:30,740
with respect to this kind of local notion

1113
00:46:31,980 --> 00:46:36,300
of what strength means in this particular problem context

1114
00:46:36,300 --> 00:46:38,300
that we're thinking about.

1115
00:46:38,300 --> 00:46:39,700
In interest of time, why don't we just jump on

1116
00:46:39,700 --> 00:46:40,540
to your section.

1117
00:46:40,620 --> 00:46:41,460
Yeah, thanks.

1118
00:46:48,540 --> 00:46:51,300
So we've just been talking about how natural language

1119
00:46:51,300 --> 00:46:55,620
can sort of be interpreted or semantically parsed

1120
00:46:55,620 --> 00:46:57,980
to a probabilistic language of thought,

1121
00:46:57,980 --> 00:47:00,500
but we haven't talked about how cognition itself,

1122
00:47:01,780 --> 00:47:03,620
which is sort of, we've been talking about

1123
00:47:03,620 --> 00:47:05,180
as the product of general purpose

1124
00:47:05,180 --> 00:47:06,900
probabilistic inference machinery,

1125
00:47:06,900 --> 00:47:09,500
might interact with language cognitively

1126
00:47:09,540 --> 00:47:12,420
or how our tools for, you know,

1127
00:47:12,420 --> 00:47:14,980
our algorithms for inference, our model representations

1128
00:47:14,980 --> 00:47:18,580
might benefit from recent advances in language models.

1129
00:47:18,580 --> 00:47:21,260
So in the rest of the talk,

1130
00:47:21,260 --> 00:47:23,740
I'll sort of talk about this also very preliminary work

1131
00:47:23,740 --> 00:47:26,180
that we just presented at a workshop at ICML

1132
00:47:27,660 --> 00:47:29,420
that is more about a role for natural language

1133
00:47:29,420 --> 00:47:31,660
and language models in this part of the picture.

1134
00:47:32,900 --> 00:47:34,700
And one reason to think that natural language

1135
00:47:34,700 --> 00:47:37,260
must play some role in this part of the picture

1136
00:47:37,340 --> 00:47:41,140
is that sometimes we set ourselves reasoning tasks

1137
00:47:41,140 --> 00:47:43,340
whose specifications, what it would mean

1138
00:47:43,340 --> 00:47:44,980
to solve the reasoning task correctly

1139
00:47:44,980 --> 00:47:46,180
must involve natural language.

1140
00:47:46,180 --> 00:47:47,820
So for example, if you have an iPhone,

1141
00:47:47,820 --> 00:47:50,300
you might have used the visual voicemail feature,

1142
00:47:50,300 --> 00:47:51,700
which automatically, but somewhat

1143
00:47:51,700 --> 00:47:53,780
incompletely transcribes your voicemails.

1144
00:47:53,780 --> 00:47:56,060
And these transcripts have gaps marked

1145
00:47:56,060 --> 00:47:58,140
by underscored sequences of varying lengths,

1146
00:47:58,140 --> 00:48:00,740
indicating Apple couldn't quite work out what was said.

1147
00:48:01,740 --> 00:48:03,500
And an inference task that I sometimes face

1148
00:48:03,500 --> 00:48:04,820
is squinting at these transcripts

1149
00:48:04,820 --> 00:48:06,820
and trying to think what could the person have said

1150
00:48:06,820 --> 00:48:09,740
during those bits that it didn't transcribe correctly.

1151
00:48:09,740 --> 00:48:12,980
And is it worth my time to listen to this voicemail

1152
00:48:12,980 --> 00:48:16,660
or am I pretty certain that I got all the relevant information

1153
00:48:16,660 --> 00:48:18,980
from the part of the transcript that I've seen?

1154
00:48:20,100 --> 00:48:23,100
So even if I'm representing that kind of inference problem

1155
00:48:23,100 --> 00:48:24,940
in some kind of probabilistic language of thought

1156
00:48:24,940 --> 00:48:27,540
and not in natural language, it must reference natural language

1157
00:48:27,540 --> 00:48:29,300
because a key part of the reasoning that I'm doing

1158
00:48:29,300 --> 00:48:30,660
is about how long those gaps are,

1159
00:48:30,660 --> 00:48:32,220
about what words could go in those gaps,

1160
00:48:32,220 --> 00:48:34,580
how they could semantically and syntactically

1161
00:48:34,580 --> 00:48:35,860
with the words around them.

1162
00:48:37,820 --> 00:48:39,980
And there are a lot of other tasks like this

1163
00:48:39,980 --> 00:48:42,620
where the specification of some reasoning problem

1164
00:48:42,620 --> 00:48:44,020
must in some way involve language.

1165
00:48:44,020 --> 00:48:45,340
Maybe we're writing something

1166
00:48:45,340 --> 00:48:47,100
that has to obey certain structural constraints

1167
00:48:47,100 --> 00:48:49,340
like a poem or code.

1168
00:48:50,260 --> 00:48:52,380
Maybe we're puzzling over a message from our advisor,

1169
00:48:52,380 --> 00:48:53,700
trying to infer all the different meanings

1170
00:48:53,700 --> 00:48:55,780
consistent with what they said.

1171
00:48:55,780 --> 00:48:56,980
Maybe we're trying to figure out

1172
00:48:56,980 --> 00:48:59,340
how to put together some words that we predict

1173
00:48:59,340 --> 00:49:01,780
could achieve some desired effect in a listener.

1174
00:49:03,140 --> 00:49:05,740
And beyond the fact that some inference problems

1175
00:49:05,740 --> 00:49:07,780
implicate language and their specification,

1176
00:49:07,780 --> 00:49:08,980
it seems like at least sometimes

1177
00:49:08,980 --> 00:49:12,580
we sort of use language for thinking, right?

1178
00:49:12,580 --> 00:49:15,980
Rubber duck debugging is when we successfully debug

1179
00:49:15,980 --> 00:49:17,260
something that's been something us

1180
00:49:17,260 --> 00:49:20,500
by just talking about it to ourselves or to a rubber duck.

1181
00:49:22,180 --> 00:49:25,020
And I think this is the intuition also behind

1182
00:49:25,020 --> 00:49:27,500
sort of chain of thought, scrap pad,

1183
00:49:27,500 --> 00:49:31,380
those kinds of innovations in language model land.

1184
00:49:32,540 --> 00:49:34,100
But one reason I'm drawing this distinction

1185
00:49:34,100 --> 00:49:36,700
between task specification and algorithm

1186
00:49:36,700 --> 00:49:39,820
is that this has long been a really important distinction

1187
00:49:39,820 --> 00:49:41,140
in probabilistic modeling and inference.

1188
00:49:41,140 --> 00:49:42,980
And it's something that I think we lose

1189
00:49:42,980 --> 00:49:45,940
when we move just to asking a language model a question

1190
00:49:45,940 --> 00:49:48,180
and hoping that it gives us the right answer.

1191
00:49:48,180 --> 00:49:52,380
So in the kind of work that our lab does

1192
00:49:52,380 --> 00:49:54,060
in modeling and inference,

1193
00:49:54,060 --> 00:49:58,560
we sort of separately create a model probabilistic program

1194
00:49:58,560 --> 00:50:00,180
that includes a task specification

1195
00:50:00,180 --> 00:50:02,620
as a posterior distribution we wanna sample from

1196
00:50:02,620 --> 00:50:04,020
and separately an inference program

1197
00:50:04,020 --> 00:50:06,260
that compositionally encodes some kind of algorithm

1198
00:50:06,260 --> 00:50:08,260
or strategy for solving that inference task.

1199
00:50:08,260 --> 00:50:10,100
And when you use a probabilistic programming language

1200
00:50:10,100 --> 00:50:11,740
to do this, you get some benefits

1201
00:50:11,740 --> 00:50:14,700
from taking this approach of separating model inference.

1202
00:50:14,700 --> 00:50:16,380
We know that we have soundness theorems

1203
00:50:16,380 --> 00:50:18,500
guaranteeing that as computation increases,

1204
00:50:18,500 --> 00:50:20,820
the inference is going to approach the posterior.

1205
00:50:20,820 --> 00:50:22,340
We have automated tools and tests

1206
00:50:22,340 --> 00:50:24,260
for measuring how accurate our inferences are

1207
00:50:24,260 --> 00:50:27,060
relative to the model with finite computation.

1208
00:50:27,060 --> 00:50:28,540
And we also have gradient estimators

1209
00:50:28,540 --> 00:50:31,020
that help us tune any parameters of our inference programs

1210
00:50:31,020 --> 00:50:33,860
to be better inference algorithms.

1211
00:50:33,860 --> 00:50:35,820
And beyond being useful properties for engineering,

1212
00:50:35,820 --> 00:50:37,300
these guarantees also reflect

1213
00:50:37,300 --> 00:50:38,940
some key aspects of human cognition.

1214
00:50:38,940 --> 00:50:41,900
We can often think more to reach more accurate conclusions.

1215
00:50:41,900 --> 00:50:43,300
We can critically evaluate the extent

1216
00:50:43,300 --> 00:50:45,260
to which our current hypotheses actually make sense

1217
00:50:45,260 --> 00:50:46,900
given our model of the world.

1218
00:50:46,900 --> 00:50:48,980
And if we repeatedly face the same kind of inference task,

1219
00:50:48,980 --> 00:50:52,260
we can train ourselves to get better at solving it.

1220
00:50:52,260 --> 00:50:53,620
So something we've begun to explore

1221
00:50:53,620 --> 00:50:55,660
is whether adding LLMs to this picture

1222
00:50:55,660 --> 00:50:58,460
might let us both specify various linguistic tasks

1223
00:50:58,460 --> 00:50:59,900
as formal probabilistic models

1224
00:50:59,940 --> 00:51:01,340
and enhance our inference algorithms

1225
00:51:01,340 --> 00:51:03,060
by letting them do some of their thinking

1226
00:51:03,060 --> 00:51:04,580
using languages as a tool.

1227
00:51:04,580 --> 00:51:07,660
So I'll first talk about the modeling side of things.

1228
00:51:07,660 --> 00:51:10,340
So we all know that an autoregressive language model

1229
00:51:10,340 --> 00:51:12,180
defines a probability distribution

1230
00:51:12,180 --> 00:51:13,500
over sequences of tokens.

1231
00:51:14,420 --> 00:51:15,700
But we rarely just want to sample

1232
00:51:15,700 --> 00:51:18,980
that unconditional distribution.

1233
00:51:18,980 --> 00:51:21,380
You know, in the same way that in order to use a SAT solver,

1234
00:51:21,380 --> 00:51:23,900
we need to reduce the problem we care about to a SAT formula,

1235
00:51:23,900 --> 00:51:26,740
use a language model, we need to reduction

1236
00:51:26,740 --> 00:51:28,900
from the task instance that we care about

1237
00:51:28,900 --> 00:51:32,540
to a prompt.

1238
00:51:32,540 --> 00:51:33,940
And the idea is that we're saying

1239
00:51:33,940 --> 00:51:35,220
that the conditional distribution

1240
00:51:35,220 --> 00:51:37,020
of the language model conditioned on that prompt

1241
00:51:37,020 --> 00:51:39,260
is somehow a good specification of the task

1242
00:51:39,260 --> 00:51:40,100
that we want to solve,

1243
00:51:40,100 --> 00:51:42,020
or a good approximation of the task that we want to solve.

1244
00:51:42,020 --> 00:51:43,580
But unlike the reductions to SAT,

1245
00:51:43,580 --> 00:51:46,700
of course, this reduction is lossy.

1246
00:51:46,700 --> 00:51:49,220
One problem is that sort of hard constraints,

1247
00:51:49,220 --> 00:51:51,940
sort of instructions that we give the language model

1248
00:51:51,940 --> 00:51:54,700
might be, you know, it might fail to follow them.

1249
00:51:54,700 --> 00:51:57,060
So this conditional distribution, p-task,

1250
00:51:59,140 --> 00:52:01,460
is not really the specification that we have in mind,

1251
00:52:01,460 --> 00:52:03,620
it's just some close thing that we can get.

1252
00:52:07,380 --> 00:52:09,820
Another problem is that the entropy of this distribution

1253
00:52:09,820 --> 00:52:12,140
may not meaningfully reflect uncertainties.

1254
00:52:12,140 --> 00:52:15,300
So you may have seen in the GPT-4 paper

1255
00:52:15,300 --> 00:52:20,540
that on multiple choice tasks,

1256
00:52:20,540 --> 00:52:22,900
where there's some multiple choice question

1257
00:52:22,900 --> 00:52:26,060
and then the language model is asked to output A, B, C, or D.

1258
00:52:26,060 --> 00:52:29,300
Before they did any RLHF and instruction tuning,

1259
00:52:29,300 --> 00:52:31,460
if they create a calibration plot,

1260
00:52:31,460 --> 00:52:34,980
where they plot sort of, you know,

1261
00:52:34,980 --> 00:52:37,740
of all the answers in which GPT was, you know,

1262
00:52:37,740 --> 00:52:41,580
0.4% confident, how often was that the correct answer?

1263
00:52:41,580 --> 00:52:44,220
GPT-4 is strikingly well calibrated.

1264
00:52:44,220 --> 00:52:46,140
And that's what you might expect from a model

1265
00:52:46,140 --> 00:52:48,980
that's doing a very good job of next token prediction,

1266
00:52:48,980 --> 00:52:50,980
of matching the distribution of language.

1267
00:52:50,980 --> 00:52:53,260
When it's uncertain, the loss function is telling it,

1268
00:52:53,260 --> 00:52:55,820
it should allocate its mass, its probability mass,

1269
00:52:55,820 --> 00:52:57,380
according to that distribution.

1270
00:52:57,380 --> 00:52:59,820
Whereas after RLHF, the calibration is shot.

1271
00:52:59,820 --> 00:53:02,700
And this is also what you might expect,

1272
00:53:02,700 --> 00:53:06,820
even if the humans who were sort of

1273
00:53:06,820 --> 00:53:10,700
providing the human feedback in RLHF

1274
00:53:10,700 --> 00:53:13,060
preferred the right answer, okay?

1275
00:53:13,060 --> 00:53:17,980
The distribution that you get after performing RLHF

1276
00:53:17,980 --> 00:53:20,780
with the objective that's commonly used for RLHF

1277
00:53:20,780 --> 00:53:23,220
sort of creates a reduction in temperature.

1278
00:53:23,220 --> 00:53:26,060
It's equivalent to reducing the temperature

1279
00:53:26,060 --> 00:53:27,900
of the parts where the human feedback

1280
00:53:27,900 --> 00:53:31,820
is exactly aligned with sort of the correct answer.

1281
00:53:31,820 --> 00:53:33,540
And so it becomes overconfident.

1282
00:53:34,500 --> 00:53:37,140
And, you know, this is very prompt dependent.

1283
00:53:37,140 --> 00:53:39,060
I don't mean to say that this is like always gonna happen

1284
00:53:39,060 --> 00:53:42,140
if you go and use GPT, but I went and used GPT-3.5

1285
00:53:42,140 --> 00:53:44,300
to do this like infilling task,

1286
00:53:44,300 --> 00:53:46,460
and it did it correctly, but also every time

1287
00:53:46,460 --> 00:53:48,140
that I generated at temperature one,

1288
00:53:48,140 --> 00:53:49,980
it gave me basically the same answer.

1289
00:53:50,860 --> 00:53:53,660
So if I want to think of this distribution

1290
00:53:53,660 --> 00:53:55,420
as sort of representing uncertainties

1291
00:53:55,420 --> 00:53:57,100
that I can make decisions about whether to listen

1292
00:53:57,100 --> 00:53:59,060
to my voicemail because it might contain things

1293
00:53:59,060 --> 00:54:04,060
I don't know, this P-task is not up to that task.

1294
00:54:04,260 --> 00:54:08,460
So our idea is to instead of reducing to a prompt,

1295
00:54:08,460 --> 00:54:10,220
reduce to a probabilistic program

1296
00:54:10,220 --> 00:54:12,860
that may call a language model,

1297
00:54:12,860 --> 00:54:14,460
which is sort of a more flexible way

1298
00:54:14,460 --> 00:54:17,020
of specifying what P-task distributions

1299
00:54:17,020 --> 00:54:18,220
we want to sample from.

1300
00:54:18,300 --> 00:54:20,380
And I know I'm running low on time,

1301
00:54:20,380 --> 00:54:23,060
but the idea is that these models

1302
00:54:23,060 --> 00:54:25,300
can mix calls to the language model

1303
00:54:25,300 --> 00:54:28,220
with conditioning statements and other logic.

1304
00:54:28,220 --> 00:54:31,580
So in this probabilistic program for this infilling task,

1305
00:54:33,060 --> 00:54:36,540
it is in a loop going through each sort of blank

1306
00:54:36,540 --> 00:54:38,660
in the that we need to infill,

1307
00:54:38,660 --> 00:54:40,140
sampling a random number of tokens

1308
00:54:40,140 --> 00:54:44,020
that should fill that spot, sampling those tokens

1309
00:54:44,020 --> 00:54:47,700
and then observing the next sort of fixed fragment

1310
00:54:48,660 --> 00:54:51,220
or conditioning on the next part being a fixed fragment.

1311
00:54:51,220 --> 00:54:53,660
And this just lets us specify a model

1312
00:54:53,660 --> 00:54:55,780
that doesn't just have a prefix prompt

1313
00:54:55,780 --> 00:54:57,980
that sort of has a prompt with blanks in it.

1314
00:54:59,100 --> 00:55:00,780
I haven't said yet how we're going to sample this,

1315
00:55:00,780 --> 00:55:02,980
but the idea is that this defines a specification

1316
00:55:02,980 --> 00:55:04,380
for the task that we want.

1317
00:55:04,380 --> 00:55:08,140
And similarly, we have programs that sort of specify

1318
00:55:08,140 --> 00:55:11,020
a variety of tasks that involve sort of thinking

1319
00:55:11,020 --> 00:55:12,460
with language, right?

1320
00:55:12,460 --> 00:55:13,940
We can condition on hard constraints

1321
00:55:13,940 --> 00:55:17,020
if we want to parse into a formal grammar or write a high two.

1322
00:55:17,020 --> 00:55:19,540
We can do kind of a product of experts model

1323
00:55:19,540 --> 00:55:20,500
using multiple prompts.

1324
00:55:20,500 --> 00:55:22,860
So maybe I want to think about a fun fact

1325
00:55:22,860 --> 00:55:24,460
that's about both London and Paris.

1326
00:55:24,460 --> 00:55:26,300
Well, you could just ask the prompt,

1327
00:55:26,300 --> 00:55:28,700
hey, please give me a fun fact about both London and Paris,

1328
00:55:28,700 --> 00:55:30,700
but you could also create a product of experts model

1329
00:55:30,700 --> 00:55:32,300
where it has to come up with a completion

1330
00:55:32,300 --> 00:55:34,580
that is both a completion to the sentence,

1331
00:55:34,580 --> 00:55:36,060
a fun fact about London is,

1332
00:55:36,060 --> 00:55:37,220
and a completion to the sentence,

1333
00:55:37,220 --> 00:55:38,620
a fun fact about Paris is.

1334
00:55:38,620 --> 00:55:41,420
And that's kind of a hybrid where the idea of and

1335
00:55:41,420 --> 00:55:43,460
and both is symbolically encoded,

1336
00:55:43,460 --> 00:55:46,100
but we're still using the language models representation

1337
00:55:46,100 --> 00:55:49,860
of knowledge about fun facts and these things.

1338
00:55:49,860 --> 00:55:52,860
Similarly, we can sort of represent reward steering

1339
00:55:52,860 --> 00:55:55,420
or a classifier guidance by conditioning,

1340
00:55:55,420 --> 00:55:58,020
by sort of soft conditioning on a reward function.

1341
00:55:59,500 --> 00:56:01,020
And we can also include things like,

1342
00:56:01,020 --> 00:56:04,220
hey, please generate a gloss of this code

1343
00:56:04,220 --> 00:56:09,220
that when I try to semantic parse it back into code,

1344
00:56:09,580 --> 00:56:12,700
gives me the same code I started with, things like that.

1345
00:56:12,740 --> 00:56:16,700
So those are model programs for specifying various tasks.

1346
00:56:16,700 --> 00:56:17,980
We need inference algorithms

1347
00:56:17,980 --> 00:56:20,260
for actually sampling from these distributions.

1348
00:56:20,260 --> 00:56:22,420
And so far we've been focusing

1349
00:56:22,420 --> 00:56:24,620
on sequential Monte Carlo inference algorithms.

1350
00:56:25,660 --> 00:56:27,780
And we kind of have a default version of this method

1351
00:56:27,780 --> 00:56:29,260
and then fancier versions of this method

1352
00:56:29,260 --> 00:56:31,100
that are necessary for harder tasks.

1353
00:56:31,100 --> 00:56:33,840
In many ways, sequential Monte Carlo looks like beam search.

1354
00:56:33,840 --> 00:56:36,300
You kind of keep multiple hypotheses around,

1355
00:56:36,300 --> 00:56:38,900
you extend them, you reweight them

1356
00:56:38,900 --> 00:56:40,780
according to model specific way,

1357
00:56:40,780 --> 00:56:41,900
and then you resample,

1358
00:56:41,900 --> 00:56:43,700
which is kind of like the part of beam search

1359
00:56:43,700 --> 00:56:45,620
where you sort of down sample

1360
00:56:45,620 --> 00:56:48,740
from your big expanded beam back to your beam size.

1361
00:56:48,740 --> 00:56:50,780
But unlike beam search,

1362
00:56:50,780 --> 00:56:52,260
sequential Monte Carlo,

1363
00:56:52,260 --> 00:56:55,460
as you scale up the number of hypotheses that you're using,

1364
00:56:55,460 --> 00:56:57,540
instead of converging to an arg max

1365
00:56:57,540 --> 00:56:59,260
of your objective function,

1366
00:56:59,260 --> 00:57:01,620
converges to the posterior distribution,

1367
00:57:01,620 --> 00:57:03,820
sampling from the posterior distribution.

1368
00:57:03,820 --> 00:57:06,220
And this sort of default version of SMC

1369
00:57:06,220 --> 00:57:09,820
has worked for a few simple tasks that we've tried it on.

1370
00:57:10,820 --> 00:57:13,900
For example, if I want a completion that follows

1371
00:57:13,900 --> 00:57:15,380
my favorite physicists is probably,

1372
00:57:15,380 --> 00:57:18,100
and my favorite writer is probably equally well,

1373
00:57:18,100 --> 00:57:20,020
SMC can give me Richard Feynman.

1374
00:57:20,020 --> 00:57:23,340
I really admire how he communicates complex ideas so clearly.

1375
00:57:24,700 --> 00:57:27,220
Or if I want to finish the fed says,

1376
00:57:27,220 --> 00:57:29,100
but only using words less than five letters,

1377
00:57:29,100 --> 00:57:30,500
I get the fed says it will taper,

1378
00:57:30,500 --> 00:57:33,340
but the rate hikes are still years away

1379
00:57:33,340 --> 00:57:34,180
to something like that.

1380
00:57:34,180 --> 00:57:35,860
And it's worth noting that if you do something

1381
00:57:35,860 --> 00:57:38,420
like token masking to enforce this constraint,

1382
00:57:38,460 --> 00:57:40,780
you just forbid the language model

1383
00:57:40,780 --> 00:57:43,580
from generating anything that's longer than five letters.

1384
00:57:43,580 --> 00:57:45,980
You get all sorts of weird completions.

1385
00:57:45,980 --> 00:57:47,860
It's different from the posterior here.

1386
00:57:47,860 --> 00:57:50,260
You get completions that set up an idiom

1387
00:57:50,260 --> 00:57:51,260
that it could only complete

1388
00:57:51,260 --> 00:57:52,860
if it used a word longer than five letters

1389
00:57:52,860 --> 00:57:53,860
or something like that.

1390
00:57:53,860 --> 00:57:55,540
And then it just gets very confused

1391
00:57:55,540 --> 00:57:57,460
and right stop, that, that, read more or something.

1392
00:57:57,460 --> 00:57:59,180
It tries to come up with some context

1393
00:57:59,180 --> 00:58:01,300
in which the sentence would be cut off early.

1394
00:58:02,540 --> 00:58:03,820
And for infilling tasks,

1395
00:58:03,820 --> 00:58:07,660
we get a variety of samples that sort of fit semantically

1396
00:58:07,700 --> 00:58:11,540
and syntactically with the text,

1397
00:58:11,540 --> 00:58:14,500
but infilling tasks can be made much harder than this one.

1398
00:58:14,500 --> 00:58:17,140
So I don't want to claim that this method yet solves

1399
00:58:17,140 --> 00:58:18,220
all these infilling tasks.

1400
00:58:18,220 --> 00:58:19,420
So for harder tasks,

1401
00:58:19,420 --> 00:58:21,340
we think we're going to need to use fancier

1402
00:58:21,340 --> 00:58:22,500
sequential Monte Carlo algorithms.

1403
00:58:22,500 --> 00:58:24,540
And both of these sort of steps

1404
00:58:24,540 --> 00:58:26,140
can actually be extended in various ways.

1405
00:58:26,140 --> 00:58:28,900
We can use better proposal distributions

1406
00:58:28,900 --> 00:58:32,300
and sort of better reweighting strategies

1407
00:58:32,300 --> 00:58:34,340
that are trying to guess, okay,

1408
00:58:34,340 --> 00:58:36,900
are we on the path to getting a good sample here?

1409
00:58:36,900 --> 00:58:38,620
And we think that techniques

1410
00:58:38,620 --> 00:58:40,140
that have already been developed in the literature

1411
00:58:40,140 --> 00:58:43,460
for proposing good things in line with constraints

1412
00:58:43,460 --> 00:58:46,340
or sort of discriminating whether we're likely

1413
00:58:46,340 --> 00:58:51,140
to land in a constraint could be good ingredients to put here.

1414
00:58:51,140 --> 00:58:54,660
But the important thing is, and this is the very end,

1415
00:58:54,660 --> 00:58:56,540
the important thing is all of those things

1416
00:58:56,540 --> 00:58:58,820
become part of the inference program.

1417
00:58:58,820 --> 00:58:59,940
And there are still guarantees

1418
00:58:59,940 --> 00:59:01,860
that as we scale up the number of particles,

1419
00:59:01,860 --> 00:59:05,220
we're still targeting the original specification of the model.

1420
00:59:05,220 --> 00:59:08,260
So all of those heuristics or biases don't sort of,

1421
00:59:08,260 --> 00:59:09,740
we don't just trust them blindly.

1422
00:59:09,740 --> 00:59:12,740
We don't hand the keys to those techniques.

1423
00:59:12,740 --> 00:59:14,940
We still have a specification that we can understand.

1424
00:59:14,940 --> 00:59:16,660
Okay, I'll stop there.

1425
00:59:16,660 --> 00:59:17,500
Thanks.

1426
00:59:17,500 --> 00:59:23,740
All right, we're a little bit behind time.

1427
00:59:23,740 --> 00:59:25,780
So unless there is a burning question,

1428
00:59:25,780 --> 00:59:27,740
burning question, no burning questions.

1429
00:59:27,740 --> 00:59:31,980
Let's break for tea and maybe Zachertorte

1430
00:59:31,980 --> 00:59:35,820
and you guys can talk to the speakers.

1431
00:59:35,820 --> 00:59:39,620
And at 11.30, we are going to...

