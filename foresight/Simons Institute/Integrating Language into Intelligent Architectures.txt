I mentioned Gabe Grand, another student collaborator
who is one of Jacob and Dreyas's students,
and Tanja Shren, a student also co-advised
by Josh and Vikash, as well as Noah Goodman,
my advisor Vikash and Jacob and Dreyas.
So our broader goal today in this talk
is actually going to be to reflect
based on many of the recent advances
that we all know of modeling natural language,
but I think also drawing on evidence
from toolkits, other toolkits in AI.
Evidence from cognitive science and from neuroscience
on kind of the broader spectrum of different ways
that we might think about building intelligent architectures
that use and produce and learn from language,
as well as more generally, I think what role language
might play or could play in a computational system
that we say thinks.
And obviously this is a question that many people
in this room, but many other people who probably
aren't in this room have thought about
from philosophers of language to linguists and neuroscientists.
And we thought it actually might be a useful exercise
to kind of start by reflecting on the underlying answers
to this question that are or aren't suggested
by some of the most prominent directions
that we're taking in AI research right now.
And of course, one of the reasons
why we're even asking this question at this scale,
what is the role of language in intelligence
is in large part driven by this remarkable observation
that we all know about from just a few years ago,
which is if you train these large
and specifically transformer based neural architectures
as language models, just to do this
next word prediction task, with enough language data,
they start to show behaviors that really suggest
that there's something more than language at play.
They look like they're thinking.
They can induce patterns from just a few examples in data,
or they can even read the definitions
of totally novel words like Zaka Tota in context
and produce realistic sentences that appear
as if they understand how to use those words immediately.
And so a lot of the excitement, I think it's fair to say
around language models is that maybe for the first time,
we're seeing something that is offering a scalable route
towards implementing more generally intelligent architectures
just by directly scaling, or largely by directly scaling
the amount of language data
that they're being trained to predict.
All right, so what is the underlying idea here?
What does this have to say about language?
Well, I think it's fair to say
that one of the dominant hypotheses that's underlying
why we were even starting to see some of this behavior
rests on kind of a two-part idea.
One is something about the nature of language itself, right?
It's the suggestion that language is sufficiently diverse
and so broad, maybe because we suspect that humans
express so much of their thoughts
in such a diverse range of their thoughts and language
that being able to perfectly solve this task,
to be a perfect language model, or at least a really good one,
is essentially an AGI complete task,
and maybe also one that conveniently,
unlike other kinds of tasks,
like predicting all of the videos in the world,
we have maybe efficient architectures to do
and enough data to do.
And I think it's worth noting that this doesn't actually
really have to be a particularly strong hypothesis
about what it is computationally that thinking looks like,
or even how language necessarily is implicated internally
inside the computational processes that we call thinking.
Rather, it's really just a hypothesis
about the nature of the language modeling task itself.
And I think what a lot of people
in this room would agree now is that,
well, that might be true about the language modeling task
in principle, much of the most exciting research
that we're seeing using LMS right now
actually isn't predicated on really the hope or the idea
that just by scaling to more and more data,
we're gonna expect transformers trained and used in this way
to actually solve that task
or become perfect next-world prediction models.
And the intuition behind that,
which I think many people have pointed out,
comes both from the way in which we're looking
in which transformers work, right?
There are autoregressive language models
that are doing a fixed, finite amount
of internal computation that only scales
based on the previous linguistic context.
And it doesn't really make sense that,
of course, you can pose questions in language
like arbitrarily difficult math problems
or planning problems that intuitively require
an amount of computation,
whose complexity doesn't actually depend
linearly on the previous token context.
And I think that's been matched empirically
by lots of different observations
about which kinds of sentences are hard to complete
if you treat them as next-world prediction tasks
in this way, right?
Hard arbitrary math problems
or planning problems like these.
Right, so some of, you know,
if you're thinking about the role of language,
why is language, why are language models so big right now
as, oh, well, language just has all the evidence necessary
to train something to think.
Like Leo said, that's not actually committing
to any hypothesis about sort of how language
is used internally in thinking.
The transformers' computations are not necessarily
doing anything linguistic as it's computing
the distribution of the next word.
But more recently, we've seen people use language models
to solve these harder tasks by letting them think more.
And what letting a transformer think more means
is letting it generate more tokens.
So in this view, thinking doesn't necessarily emerge
just as predicting the next token.
Rather, thinking happens in language, right?
It happens by sort of producing a chain of thoughts
or using a scratch pad or deciding
that you're going to invoke a calculator
or write some code and execute the code.
And the sort of hypothesis embodied,
I think by this line of work,
is that the role of language
and intelligence architecture is bigger.
That language or some kind of like running monologue
of language is the central controller of thought.
And thinking is taking place primarily in language.
And what's maybe striking about these proposals
is that if you ask most cognitive scientists
or neuroscientists, they'd probably say,
this isn't the role that language plays
in relation to general intelligence in humans,
or at least it's not the dominant hypothesis.
Until just a few years ago,
this probably wouldn't have been the role
that many AI researchers
would have necessarily posited for language
as the main controller or the main substrate
of a running monologue that controls all thinking.
And so we thought we'd review some of the background
for what language seems to look like in humans
as a basis for informing how we might build architectures
that better capture those more human-like roles
for language.
Yeah, and so I just want to be clear
that we're going to do kind of a quick background
on some of the neuroscientific and cognitive evidence
about what language might look like in people.
And our goal really isn't to push back in any way
against these other kinds of paradigms
for where language might fit in.
It's rather to present at a mode
where we're really thinking a lot
about scaling up language models
as one really dominant role,
what our other roots might be
because it doesn't seem like that that really matches
a different prominent intelligent architecture
that's sitting here in the room today.
So one source of evidence for the role
that language appears to play in human cognition
comes from neuroimaging data, fMRI data,
suggesting how language is processed in the human brain.
And at this point, convergent imaging data from many people,
including Ed Fedorenko at MIT,
Standa Hain and recent graduate students
in our department, Kyle Malewald and Anya Ivanova,
suggests that human brains
have this language-specific network
that handles many of the tasks that we associate with language.
It's activated when people do tasks
like listening to sentences or reading them
or speaking or writing words.
And this is not just an English-specific network.
The same general region is activated
no matter what language someone is speaking.
It even fires when they're producing constructed languages
for people who come to learn and become fluent
in languages like Dothraki or Klingon.
And right, what is it that this language network does?
Well, increasingly, one dominant hypothesis
is that it actually does do something
like next-word prediction.
And in fact, among many other kinds of alternative models,
like people were earlier,
for a long time I've been trying to do things
like align words back to the brain,
transformer architectures really do seem to be
among the best models that we have right now
of the neural activity of this language network,
specifically when they're trained
on next-word prediction tasks
and not other linguistic tasks like NOI.
But I think what we see in this neuroimaging evidence
also highlights the ways in which the role
of this human language network
probably is not as the controller
or the central seat of cognition.
It's selective for language
and it isn't involved in many other kinds
of thinking activities,
even ones that might seem to involve symbols
like solving math problems
or reasoning about logic and physics and social reasoning.
Those invoke other regions of the brain
and language interfaces modulary with them.
It can interface very generally with them,
but it doesn't seem to be
where the bulk of thinking is taking place.
Oh, yes.
Oh yeah, I just had a question
about the next-word prediction findings.
So did you also try to say like math word prediction
and this is actually worse than next-word?
Yeah, so I think they do compare
or like closed tasks are one of the alternates
that they're looking for.
And also, yeah, people should feel free to just shout out
as we've been doing all along.
So also, right, actually you can see adult patients
that suffer strokes that only damage this area of their brain
and we find that they can still think about,
well, they can't comprehend spoken language,
but they can still think about all these different kinds
of tasks fronted in different mediums,
like they can draw physical inferences
from videos that they're watching.
And conversely, maybe most tantalizingly,
it's actually also possible to sustain localized damage
that leaves you still able to produce these long,
very relatively fluent
and quite syntactically coherent sentences,
almost as if they're maybe just like a very rudimentary
and local word-based language model,
while not really conditioning meaningfully
on what someone else is saying.
So they say, do you like it here in Kansas City?
And this person says, yes, I am.
Or really producing sentences
that are obviously more globally meaningful
or goal directed towards the question.
And I don't want us to overindex on those results,
but they kind of fit in with this more coherent picture.
And in many ways, I think this kind of recent evidence
from neuroscience actually supports the broader picture
of human cognition and the place of language in it
that most cognitive scientists
and many linguists have already believed
based on what we see and have come to learn
through developmental science,
studying how kids think
and where language seems to fit into that picture, right?
So a broad body of work at this point
suggests that infants, well before they learn language
of any kind or are speaking,
already independently perform many of the tasks
I think we associate with coherent thinking,
from reasoning about physics to planning
to drawing causal and probabilistic inferences.
And language seems to be something that humans learn
and scaffold on top of this structured basis for thinking.
We take much less input data
to learn to produce fluent language
than a large language model.
And humans that actually aren't exposed
to any language input at all.
So famously there are these deaf children in Nicaragua
who grow up in isolated hearing families
actually spontaneously come to produce early sign languages
as a product of trying to communicate events
that bear many of the hallmarks of the syntax
that we associate of our own natural languages,
like distinguishing between the subject who's punching
and the person who's getting punched.
Suggesting that in many ways
the language we produce somehow externalizes
the underlying structure of the thought that we already have.
And of course, there are many other animals
that we associate as being intelligent in some way
and that have been modeled using the models
that we associate with probabilistic reasoning and planning
that don't use language at all.
So it doesn't feel like language is by any means necessary
for what we think of as thought.
So formalizing this picture of an intelligent system,
one that's shared across animals and non-linguistic infants
and maybe captures some of the computations involved
in many of those thinking tasks
that don't involve the language network in our brains
is both the central goal of a lot of cognitive science
and has been a historic motivation
for diverse fields within AI
before the current sort of LLM centric moment.
So if you open up Russell and Norvig's textbook,
AI a modern approach, you'll see an equation like this
that's supposed to sort of capture what we believe
about how intelligence works.
Computationally or a computational model of intelligence.
Where the idea is that an intelligent agent
is one that has sort of structured internal world models
that can be updated based on observations of the world
and in which we can sort of predict the results
of our actions that the agent has some sort of values
or desires that are captured in some kind of utility function
that the agent can do probabilistic reasoning
over its observations and the possible actions it might take
and what their expected utilities are
and that it can do some kind of planning
to optimize the value of the back end.
Yeah.
Is there a normative statement or descriptive statement?
Is it the case that we are defining intelligent agent
to be having this kind of property
or it's like if we want to do intelligent architecture,
we should have these?
Yeah, so I think some, it's a great question.
I think some of the work definitely is coming at it
from a normative perspective.
This is like a definition of what rational actions
might look like, right?
But I think there is a lot of work
computational cognitive science
that sort of in various domains
has collected a lot of evidence
that people either like recognize this as normative
or behave like this in computation,
according to the limits of what they can do computationally.
So in this architecture, the controller for thinking
would be some kind of system
that's capable of general world modeling
and probabilistic reasoning
or planning and utility maximization
rather than sort of a primarily linguistic system necessarily.
Now largely attempts in artificial intelligence
to directly implement this equation
by building out components that sort of map on
to the different elements of this equation
have struggled to match the computational efficiency
and the generality of natural intelligence.
But we have seen over the past couple of decades or so
the emergence of this new class of tools
called probabilistic programming languages
for building software
that can solve probabilistic reasoning tasks
at least in limited domains.
And sorry, and these languages have been applied
to create systems that do everything
from perceiving cluttered 3D scenes
more accurately sometimes
than object detector neural nets
to interpreting and predicting economic trends
more accurately than leading industry solutions
like Facebook neural profit.
And these probabilistic programming systems
are enabling these applications
with two key technical features.
They feature modeling languages
that let users express complicated probabilistic models
of the world as programs
making it easy to write down rich probabilistic models
that are defined in terms of expressive
program-like components.
So for example, the probabilistic models behind
these example applications
are defined in terms of 3D renderers, symbolic planners,
scientific simulators and so on.
And the second thing that probabilistic programming systems do
is that they automate various mathematical operations
on these models
and that automation makes it possible for users
to concisely implement sophisticated algorithms
for probabilistic reasoning,
such as the variety of Monte Carlo
and variational inference algorithms
that power these example applications.
One way of thinking about these tools
is as kind of like PyTorch or TensorFlow,
but instead of writing differentiable models
and doing optimization,
you're writing probabilistic models
and doing various probabilistic reasoning tasks.
So far, these AI engineering efforts
haven't really made contact
with the sort of language model part of AI.
Much like there hasn't yet been a concerted effort
to take this classical picture of intelligence architecture
and figure out how language might be
richly integrated into it.
So for the rest of the talk today,
we're gonna explore two different approaches
for thinking about where language
might fit into this picture and approach.
So probably Leo's gonna begin
by talking about how an intelligent agent
might incorporate lots of externally produced languages,
explanations, observations, questions,
how an agent can incorporate all of those forms of language
into the way that it updates its beliefs
and decides how to act in the world.
Then I'll talk a bit about systems
that leverage language as a tool for thinking
within its models of the world
or its probabilistic reasoning algorithms.
And each of these corresponds
to very recent preprints of work.
So I also wanna give a disclaimer.
We should really emphasize that both are preliminary proposals
and we're giving a very speculative talk,
not scaled architectural solutions here.
Cool, right.
So the first person that's portion of this talk
is summarizing work.
If you wanna read more,
you can find this very long paper
from word models to world models.
That's up on archive
and this is primarily done with another student
who's not here today, Gabe Grant.
And as I just mentioned, the context for this work
is thinking about how we can build systems
that capture the breadth with which all the external language
we hear seems to inform at least our human thinking.
And what's clear, right,
is that this role seems to be very broad.
If we take the basic model of an agent
with beliefs and goals,
well, it seems like there's an incredibly diverse range
of situations in which we can update our beliefs
about a situation from observations
that we express in language,
or in which the goals of our thought
are to answer questions that we specify linguistically.
And these might implicate our knowledge of other agents
drawing on our intuitive psychology
to reason about what they think and what they'll do.
Or we can talk about the physical world around us,
what we perceive and ask questions
that require us to reason about
what we see and draw on our physical intuitions.
And of course, one of the remarkable things
where we're also excited about language
is that it doesn't just draw on what we already know,
it's this means by which humans seem to learn
and pass on profoundly new knowledge,
whether that's new concepts that we define in words
or learn from words to really profound new theories
and conceptual systems, right?
Much of what we know about the world,
the fact that there are wars, what wars are,
legal systems, sciences,
comes from information that it feels that we acquire
from language in some way.
But how do we do that?
Well, I think one longstanding lens
for thinking about language
that's kind of persisted before the LLM-based moment
is that what language is,
is this external symbolic medium
for communicating human thoughts.
And the way that it does that
is because there's some kind of general mapping function
from our internal representations of thought
into this external symbol system, that's language.
And so in this kind of older framework,
what it means to understand language or make meaning
means mapping back from external sentences that we hear
into structured internal representations.
And what we explore in this paper
is a general proposal that casts the meanings of language
as these mappings or probabilistic distributions
over expressions in a probabilistic programming language.
And I'm gonna come back after some concrete examples
to how I think this relates to the conceptual rule semantics
that Steve articulated in the first talk,
because I think there are actually
some really deep connections here,
the ways in which this might be one way to formalize
or enrich some of those ideas.
And this architecture, this proposal here
also suggests how language can be integrated
into a more general architecture,
because it's one that already starts out with,
as Alex mentioned,
some kind of existing internal modeling language,
whose goal is to represent the world probabilistically,
query those models and specify what it means
to draw coherent inferences over them.
And it also suggests, I think, a framework
for formally modeling the content
of different kinds of sentences in language
as the kinds of probabilistic programming expressions
that they might map into.
So, and so in this paper,
we begin by exploring how this proposal
might be instantiated with respect to
a bunch of different domains of reasoning,
sorry, including general probabilistic reasoning,
but also reasoning about relations
or physics and social situations.
And in all of these, we're gonna propose
that we might think about the language
that communicates general conceptual knowledge
about the world definitions or causal knowledge
as constructing these probabilistic expressions
that are those that build up probabilistic generative models.
And then in this framework, observations in the language,
like there is at least one red mug in the scene
or Charlie is Dana's grandfather,
construct formal conditioning statements,
which update the state of this probabilistic model.
And then questions map into query expressions
that specify the formal target of probabilistic inference
with respect to a model.
And in this framework,
we're thinking essentially cast as probabilistic reasoning.
We suggest that another way that we can think about
the role of a language model,
one that's much smaller,
like the language network in the brain
is actually as a means of instantiating
this meaning function in a way that
we've really never had before
to protect these kinds of context specific
and previous discourse conditioned mappings
from sentences in natural language
into distributions over expressions
that convey meaning in a probabilistic programming language.
And so as a part of this long running disclaimer,
what I'm gonna be showing is a really minimal
implementation of this framework,
but really intended as a pointer to different directions
with which we might scale this approach
to implement a more general interface between language
and arrange a different core cognitive domains.
So just to be clear,
concretely in the examples that you see next,
our meaning function is gonna be implemented using codex,
right, an open AM model much smaller
than the state of the art right now
that's trained to learn joint distributions
over language and code.
And the probabilistic programming language
we're gonna show is church,
which is this very simple probabilistic programming language
that supports kind of very general
sample based inference procedures.
And our goal is to demonstrate how this framework
might broadly interface between language
and a bunch of different core cognitive domains.
So first to illustrate the basic sense
in which a proposal like this might allow language
to update an agent's beliefs and query a world model,
I'm gonna begin with a really simple toy example
that actually draws on a bunch of prior
cognitive science experiments
in which real people were asked to draw various inferences
about which teams of players might win different games
of tug of war based on the games
that you'd previously seen players win or lose.
And so this is older work from Josh's group
that demonstrated I think the sense in which
this normative model, this Norvig model
of probabilistic inference actually in many ways
predicts the actual behaviors and predictions made by humans
using a very general probabilistic model
of the mechanics of this tug of war game.
And our goal here is to show how our framework
we can implement an interface between natural language
and all of the core examples of this older experiment.
So just to go through here, I think what you're seeing
in this little toy example, the world model
that's being defined on the screen
is capturing the basic causal relationship
by which properties of different human players
might influence the outcomes of different tournaments
that they play in tug of war.
So for instance, here we're modeling players
as having some internal inherent strength value
where strength varies approximately normally
but as this unobserved latent variable
over different kinds of players.
And we also think of players as having
some kind of internal laziness value
which represents the percentage of the time
that they actually don't act
according to their underlying strength.
And how did these variables determine the outcomes
that we observe of given games of tug of war?
Well, the strength of a whole team of players
depends on the cumulative sum of its player strengths.
But if a player is deciding to be lazy in this game,
they might not pull as hard as they could.
And whichever team pulls with the most strength
in a given match is going to win that match, yeah.
Did you design the primitives of strength and laziness
or a codex come up with the primitives themselves?
So in this one, we're looking at a model
that's derived from the older work.
So these are designed, but yes, that's later in this work.
We're gonna show some examples
of how you can learn this kind of model
from someone just talking about it in language
like the definition that I just gave.
Right, and so again, you know,
this is a really simple example,
but I think also one that actually captures
a surprising amount of the basic causal knowledge
that people have if you tell them
that you're gonna be listening to tug of war games,
but sometimes people can be lazy
and not pull as hard as they could.
So how do we go about relating language in this domain?
Right, well, one means by which we can induce
a simple notion of a meaning function
that actually fits the definition we just gave
is by conditioning a language model
both on this context-specific generative world model
and on a few examples showing how language is mapped
into sampled probabilistic programming expressions
in this domain.
And what we've done now, right,
is effectively induce this kind of situation-specific
contextual mapping from arbitrary new sentences
to expressions that conditions
both on the general prior distribution that codex is
over language and code,
and this kind of specific discourse thinking context
of how language is being used in this situation.
And there are clearly other ways to do this,
some of which we'll talk about later,
but we're using this example to illustrate
just how much you might be able to do
with this kind of minimal implementation,
a notion of a model that translates
between language to code.
Right, so what kinds of language might we say here
and how might we think about them
in relation to probabilistic programming expressions?
Well, a general proposition, like Josh won against Leo,
gets translated into or might map,
we might think of mapping or meaning a conditioned statement,
an observation that Josh won against Leo.
If we make subsequent observations,
like then Josh went on to claim victory against Alex,
we can continue to kind of generally use this
meaning function that we've induced to turn that
into a probabilistic programming language
that captures the fact that Josh won against Alex.
If we then say that even working together as a team,
Leo and Alex still couldn't beat Josh
in this game of tech of war.
At this point, if we want to answer a query,
like, okay, wait, how strong is Josh?
What we think of as thinking in this situation
is actually sampling from the posterior
over possible worlds from the generative model
that we just defined, subject to the observations
that we've just made.
And indeed, that means that the meaning of a sentence
like how strong is Josh is really
a structured publicistic inference query.
What is the latent variable that is Josh's strength?
And what we see here is that given his track record,
all these people that he's beating, even playing together,
our inference is that Josh is likely
a good bit stronger than average.
And that also means coherently,
we might expect that a priori,
a new player we've never seen, like Gabe,
is going to be unlikely to beat him.
So if we ask what are the odds of Gabe at beating Josh,
we see that we think it's somewhat unlikely.
Question?
Oh, yes.
So on the how strong is Josh,
it seems like there's an interesting thing here
where there's also an implicit question
of what the word strong means in this context.
And that, like, right, it's like not a number,
it's kind of some like, comparative adjective.
It's like, probably a non-intersective.
So I guess, is that something you think about?
This framework, or should I just be kind of, like,
ignoring this sort of issue?
Yeah, well, okay.
So I think there's a number of ways
that we can think about that.
I mean, so, right, so the one sense we could say is like,
right, how strong is Josh?
Isn't, the answer to how strong of Josh isn't a number.
Rather, it's kind of this distribution
over this posterior distribution
of various underlying strength values
that we currently might infer that Josh has
with respect to the general value that we have.
I think another kind of popular definition
of various uncertain adjectives,
like a word like strong, right,
is that you have some internal threshold value,
or the person speaking has some kind of internal
threshold value that you must kind of jointly infer
with respect to the context in what you've seen.
And some of the examples that I'll actually give later,
so, right, there's kind of a long line of work
in linguistics, including some work
that treats that as like a pragmatic inference.
I think some of the interesting work
that we'll show a little bit later is that,
there are some ways in which you might think
of this mapping function as actually being a general one
that includes that notion of pragmatic inference.
And also, I think captures the sense
in which if you continually,
are you really doing this kind of pragmatic inference
all the time, or do you actually,
in many general settings,
like talking about the strengths of people,
actually have some kind of cashed older notion of strength
that you can draw in.
And I think actually, this notion of large language models
as just being this learned mapping function
from language into expressions include,
can also capture the sense in which
that knowledge is amortized away.
And you might not be having that inference.
Yeah, Chris.
She's sort of just denying or ignoring
what makes people so excited about large language models
in their meaning representation and ability to do inference.
I mean, because, okay, you've got sort of
cooler probabilistic programming language
on the right hand side,
but in some sense, the picture is still,
this is semantic parsing, like it was 2010 to 2015.
And yes, you're using a large language model,
but you're not actually using the excitement
of a large language model as a representation system.
Yeah, and I think, so probably each of us
would have different answers to this,
but part of what we're hoping to paint out
over the course of this talk is I think
some of the ways in which actually, right,
of course, no one wants to say we're gonna go back
to kind of the brittleness of semantic parsing,
but I think one thing that large language models
actually give us, or one proposal in this talk,
is that there are some aspects of the theory,
kind of the classic notion of linguistics,
and certainly the classic notions of semantic parsing
that actually normatively capture
a lot of what we really might want when we think about,
so one answer for an AI system is, well, yes,
in some way, we don't wanna throw,
certainly we don't wanna throw away
everything that we're learning from large language models,
and I think one answer to that
is kind of the answer that I gave to Jacob, right?
If we think about not always, you know,
in these examples, we're showing this very direct system
in which we always start with language
and we always map into some sort of
probabilistic programming expression,
and that's where all of the thinking happens,
and we might think, well, that doesn't totally make sense
because there are lots of cases where, as you're saying,
we have every reason to believe
that large language models have learned
a lot of latent information.
They can do a lot of, they certainly have
a lot of latent conceptual information,
and maybe to some degree, they can even perform
certain kinds of limited, amortized inferences,
or reuse old inferences that they've learned
from other people I've had,
and so in the second part of this talk,
we're going to show different ways in which,
well, this probabilistic programming language itself
doesn't necessarily need to be something that's isolated
from what large language models have learned.
It also can embed calls to large language models
within it to kind of draw in that sort of knowledge.
Haven't you gone back to the visualness of semantic housing
because you're doing this translation
into symbolic semantic representation,
which really ends with your actual result,
and it's riddled in the same way?
Well, right, and also, so no, I think I would say,
I don't totally think that the way in which we're using,
or the sense in which, or I think there are some ways
in which this kind of broader definition
in which you are saying, well, the meaning of a distribution,
or the meaning of a sentence in language
isn't just one probabilistic programming expression, right?
That's what we're showing here for pedagogical purposes,
but you might say, well, okay, right,
how are you going to obey kind of the ambiguity of language?
There are kinds of sentences that are definitely ambiguous.
So one example that we've looked at are sentences
in which you say something like,
Josh beat Alex and Leo, right?
And you might ask, well, you know,
that's kind of a classic syntactic construction.
Does that mean that Josh beat Alex and Leo,
and they were playing on the same team,
or Josh beat Alex, and then Josh went on to beat Leo?
And what we see, or generally, what you might say is,
well, the meaning of that sentence
actually shouldn't be picking one expression or the other.
It should be kind of the distribution
over those possible parses,
and that distribution also shouldn't just be something
that we can determine in this totally context
and sensitive way, it should actually depend on
all the previous patterns in the discourse.
So if someone's continually been using this conjunctive
and to refer to teams of players playing together,
we should take that kind of discourse bias into account.
And I think actually, right, this provides,
or thinking of large language models
as kind of generally having learned
this broad joint distribution,
but one that can be kind of conditioned quite richly
both on the content of this generative model.
So it's not trying to come up
with a universal definition of strength.
It's not even necessarily trying to come up
with a universal definition of any of these words.
It's thinking about how they might map contextually
into the best possible expression in the context
of a particular local model built for a particular situation.
I think is obviously related to,
but attempting to address some of the brittleness challenges
of semantic parsing in the past.
I think another answer to this, right,
is that part of the problem of semantic parsing previously
has been actually that the mapping functions
have historically been difficult to get right.
Whether you were thinking about those
as kind of old hard-coded grammars
or many of the attempts to kind of learn these things,
we are very domain-specific supervision.
So you wanna have a semantic parser
for a particular robotics domain.
You need a thousand examples of sentences
about that particular robotics domain
and a thousand paired with a thousand examples
of programs that are operating on that particular domain.
What we're seeing here is I think something that says no.
What it means to learn language generally
is to learn kind of this general mapping
between language and some kind of underlying representation.
And also, one reason why we might want a system like this
is because we want to be able to condition coherently
on information that's not just coming from language.
And we want to think about how a general substrate
in which the only, yes, we might be told
that Josh went against Leo,
but we might also be watching videos
that give us information about Josh's strength,
that convey our observations.
We might also have seen pictures
like the ones in the stimuli that we saw before
demonstrating the results of previous outcomes of matches.
And I think one thing that suggests
is we want this kind of general substrate
in which we can think about how those observations,
including the observations from language,
but without prioritizing language in any way,
I think are coherently considered.
So I think it depends on what part of semantic parsing
you, or yeah, I think the answer to that depends
on what part of semantic parsing we think of
as being the source of the brittleness
that caused us to throw that paradigm into question.
Yeah, and maybe I'll just offer one more perspective.
So one part of it is what Leo is saying.
Traditional semantic parsing is brittle in two ways.
One is, do you have broad coverage of language
that you can parse into your system?
And two is like how broad coverage
are the set of query, like the set of semantic queries
that you can actually answer.
And I think what you're pointing out is
this doesn't seem to address the second source
of brittleness, which is that your system
can only answer certain things,
it can only reason about certain things.
Brittleness of the formal representation language
that you're using.
Right, that's true of large language model representations.
Right, so I think my sort of take on that
is from a kind of AI engineering perspective
is sort of a branching in two directions.
One is, I think we have made some progress
that this is not really evoking, probably,
toward systems that within restricted domains
can reason coherently and probabilistically
about a wide range of queries.
So we have systems like this inference QL system
that uses nonparametric bays to analyze huge data tables
and come up with a model of that system
that or of your data that can answer all sorts of questions
like, oh, you know, show me like which people
in this data set are like probably overpaid
given their experience or something like that.
So in the same way that people are kind of excited
about using natural language or using language models
to parse into SQL, right?
Because so much data is in SQL
and it's a very SQL is a very expressive language
for asking questions about that data.
When we have a probabilistic system,
like a good probabilistic model of that data under the hood,
it enables conversational patterns that are not enabled
when you have like SQL as the database
because we expect our conversational partners
to have coherent beliefs about the world,
to update those beliefs in response to new evidence
that we give it to be able to report uncertainty
and make sort of modal judgments.
And so one engineering path is to take those kinds of systems
and sort of build conversational interfaces to them
that behave more like an intelligent person would behave
and can draw inferences that you might not draw
if you're just talking to a SQL backup.
The other path that sort of we'll talk about
in the next part of the talk
is how can we use those representations
that language models have learned
to make the probabilistic inferences more interesting
and more robust, less brittle,
without sort of totally embracing the other kind of brittleness
which is the kind of brittleness that language models
seem to have right now,
which is that they draw,
that they don't really necessarily reason
with coherent probabilistic beliefs.
So maybe, yeah, let's go into that next part.
Yes, yes, okay, right.
Well, so right, so in the interest of time,
I'm actually gonna like skip through
some of the rest of this example,
which I think is just more of what you've seen,
but one sense in which I think,
yeah, maybe a third part of the answer to Chris,
I would say, is that,
right, you know, yeah.
I think part of what this is trying to do
is explore some of the ways in which we might answer ways,
right, without giving an answer in all the ways in which
we might answer the ways in which language models themselves
are brittle with respect to what we also want
from a model of intelligence, right?
We might suspect that when we answer,
ask questions like this,
really what we are trying to do is specify
some kind of normative query that captures formally
a sense of, well, we want something like the posterior
with respect to some kind of internal model of the world.
And, you know, this is kind of the simplest means,
or this is a very simple example
of how we might formally impose that kind of structure,
but one that I think can be elaborated on,
depending on the kinds of primitives
and the ways in which you're thinking about
what it is that the probabilistic programs can express, right?
So one way in which we might think about doing that
is by thinking about probabilistic programs
that themselves have access to other kinds of means
of calling other different mechanisms and cognition, right?
So I think I would draw a contrast here
between the notion of the large language model
as a controller, the one that's making the decisions
about when to write little snippets of code
and to execute them, when to call out to little planners
and incorporate them, or stuff like the Minds Eye work,
right, where there's a language model,
it decides when to call out to a physics simulator,
but the way it interprets the outputs
of that physics simulator is to paste those back
into the language model context
and try to draw inferences on them in turn.
Rather, in this kind of framework, right,
what you can kind of see a, or yeah,
the direction that this framework would be pointing towards
is to say, well, on the other hand,
we already have languages that allow us to do things
like build expressive generative models
over three-dimensional scenes that also capture things
that we might want only from perception,
like knowledge about how the shapes of objects
tend to accude each other,
or incorporate rich models of physics,
or that model theory of mind as taking place recursively
and thinking about agents who themselves have beliefs
about their own internal world models
and are actually choosing their actions as planners, right?
And in this kind of framework,
you can point the way towards a kind of model that says,
well, how is it that I might incorporate language
into these kinds of models sitting alongside
these other kind of observations that I might make, right?
So how might I think about the meaning
of images that I want to generate
that specify specific constraints,
or imagination, or, right, go ahead, Jacob.
Yeah, I just had a clarification question.
So you were talking earlier about having this meeting function,
and then I think also we're mentioning something
about like code x, in terms of the questions.
I'm just trying to understand which of these is that.
Is that here, or is the meeting function
when you come later?
So that's maybe the first question
and then the other clarification is,
so are these statements actually just been programmatically
created from code x by prompting the text that was on?
Yes, that's right.
So by meeting function in this framework, we say,
well, there's kind of two generalizations
of a meeting function.
There's a general joint prior, right?
That code x is already,
that it's learned between language and code,
and then there's this kind of context specific
meeting function in the sense that it's conditioned
on whatever's in the prompt, the generative model,
and some examples of how language relates to expressions
that it's doing, so that's a meeting function.
And yes, all these examples that you're seeing
are one sample from that distribution.
Right, and one of the things that I wanna point to here,
right, is it does, I think it suggests a framework
or another means of thinking about what it means
for language to construct new concepts from definitions,
or even come to construct new world models
from thinking like somebody in the beginning asked, right?
So how, for instance, might we think about enriching
an existing structured relational model
with concepts that we learn from language?
So for example, if we consider kind of a formal model
of kinship relations, we might say that, well,
the generative model of this domain
is itself represented as a probabilistic program.
It captures both the causal means by which
people give rise to their children,
and also the definitions or one notion of the definitions
of what it means to be something like a sister or a father
with respect to this core notion
of how family trees come to be.
And so if you take this kind of general notion
of the meaning of language as being the distribution
over expressions that it creates
in a probabilistic programming language,
you might start to think how we can formally think about
relating definitions for various kinds of relational terms.
An uncle is the brother of one's parent
or the husband of one's aunt.
A pibling is a gender neutral term for an aunt or uncle,
that's the sibling of one's parent,
or this relational notion of a sister of one's father
from a language that's actually not found
anywhere on the internet.
And I think the core thing that we wanna suggest here, right,
is why do we even have definitions at all?
Well, one notion of what it even means
to have learned the definition of this term
is that it should drive coherently
all of the downstream inferences that you make with that term,
and it should graft onto the conceptual knowledge
that you already have.
And so you can think about forming new sentences directly
that refer to someone's paani or one's pibling
in this situation and expecting them
to draw both on your existing conceptual knowledge
of what it even means to have a family tree
as well as all the other conceptual terms for a friendship
that you may already have.
And the same framework also suggests one mechanism
by which we might formalize what it means
to learn world models from language.
So as I mentioned, if we return to the situation
that opens this talk, tug of war games,
we might think about how the definition that I gave
when I sat up here at the podium, right,
saying there are people whose strength levels vary
from person to person.
People have a percentage of time in which they're lazy.
Strengths of the teams depend on the underlying strengths
of the members of that team,
and whether one team beats another
just depends on which team pulls stronger that match.
And this kind of setting is actually language, right,
is building up the actual generative model itself.
And you might think of a system like this
that both learns these kinds of theories from language
and then is appending to this kind of local problem-based
context to answer arbitrary questions
like the kinds that we gave or conditioned
on various observations like Josh being stronger than Leo
with respect to this kind of local notion
of what strength means in this particular problem context
that we're thinking about.
In interest of time, why don't we just jump on
to your section.
Yeah, thanks.
So we've just been talking about how natural language
can sort of be interpreted or semantically parsed
to a probabilistic language of thought,
but we haven't talked about how cognition itself,
which is sort of, we've been talking about
as the product of general purpose
probabilistic inference machinery,
might interact with language cognitively
or how our tools for, you know,
our algorithms for inference, our model representations
might benefit from recent advances in language models.
So in the rest of the talk,
I'll sort of talk about this also very preliminary work
that we just presented at a workshop at ICML
that is more about a role for natural language
and language models in this part of the picture.
And one reason to think that natural language
must play some role in this part of the picture
is that sometimes we set ourselves reasoning tasks
whose specifications, what it would mean
to solve the reasoning task correctly
must involve natural language.
So for example, if you have an iPhone,
you might have used the visual voicemail feature,
which automatically, but somewhat
incompletely transcribes your voicemails.
And these transcripts have gaps marked
by underscored sequences of varying lengths,
indicating Apple couldn't quite work out what was said.
And an inference task that I sometimes face
is squinting at these transcripts
and trying to think what could the person have said
during those bits that it didn't transcribe correctly.
And is it worth my time to listen to this voicemail
or am I pretty certain that I got all the relevant information
from the part of the transcript that I've seen?
So even if I'm representing that kind of inference problem
in some kind of probabilistic language of thought
and not in natural language, it must reference natural language
because a key part of the reasoning that I'm doing
is about how long those gaps are,
about what words could go in those gaps,
how they could semantically and syntactically
with the words around them.
And there are a lot of other tasks like this
where the specification of some reasoning problem
must in some way involve language.
Maybe we're writing something
that has to obey certain structural constraints
like a poem or code.
Maybe we're puzzling over a message from our advisor,
trying to infer all the different meanings
consistent with what they said.
Maybe we're trying to figure out
how to put together some words that we predict
could achieve some desired effect in a listener.
And beyond the fact that some inference problems
implicate language and their specification,
it seems like at least sometimes
we sort of use language for thinking, right?
Rubber duck debugging is when we successfully debug
something that's been something us
by just talking about it to ourselves or to a rubber duck.
And I think this is the intuition also behind
sort of chain of thought, scrap pad,
those kinds of innovations in language model land.
But one reason I'm drawing this distinction
between task specification and algorithm
is that this has long been a really important distinction
in probabilistic modeling and inference.
And it's something that I think we lose
when we move just to asking a language model a question
and hoping that it gives us the right answer.
So in the kind of work that our lab does
in modeling and inference,
we sort of separately create a model probabilistic program
that includes a task specification
as a posterior distribution we wanna sample from
and separately an inference program
that compositionally encodes some kind of algorithm
or strategy for solving that inference task.
And when you use a probabilistic programming language
to do this, you get some benefits
from taking this approach of separating model inference.
We know that we have soundness theorems
guaranteeing that as computation increases,
the inference is going to approach the posterior.
We have automated tools and tests
for measuring how accurate our inferences are
relative to the model with finite computation.
And we also have gradient estimators
that help us tune any parameters of our inference programs
to be better inference algorithms.
And beyond being useful properties for engineering,
these guarantees also reflect
some key aspects of human cognition.
We can often think more to reach more accurate conclusions.
We can critically evaluate the extent
to which our current hypotheses actually make sense
given our model of the world.
And if we repeatedly face the same kind of inference task,
we can train ourselves to get better at solving it.
So something we've begun to explore
is whether adding LLMs to this picture
might let us both specify various linguistic tasks
as formal probabilistic models
and enhance our inference algorithms
by letting them do some of their thinking
using languages as a tool.
So I'll first talk about the modeling side of things.
So we all know that an autoregressive language model
defines a probability distribution
over sequences of tokens.
But we rarely just want to sample
that unconditional distribution.
You know, in the same way that in order to use a SAT solver,
we need to reduce the problem we care about to a SAT formula,
use a language model, we need to reduction
from the task instance that we care about
to a prompt.
And the idea is that we're saying
that the conditional distribution
of the language model conditioned on that prompt
is somehow a good specification of the task
that we want to solve,
or a good approximation of the task that we want to solve.
But unlike the reductions to SAT,
of course, this reduction is lossy.
One problem is that sort of hard constraints,
sort of instructions that we give the language model
might be, you know, it might fail to follow them.
So this conditional distribution, p-task,
is not really the specification that we have in mind,
it's just some close thing that we can get.
Another problem is that the entropy of this distribution
may not meaningfully reflect uncertainties.
So you may have seen in the GPT-4 paper
that on multiple choice tasks,
where there's some multiple choice question
and then the language model is asked to output A, B, C, or D.
Before they did any RLHF and instruction tuning,
if they create a calibration plot,
where they plot sort of, you know,
of all the answers in which GPT was, you know,
0.4% confident, how often was that the correct answer?
GPT-4 is strikingly well calibrated.
And that's what you might expect from a model
that's doing a very good job of next token prediction,
of matching the distribution of language.
When it's uncertain, the loss function is telling it,
it should allocate its mass, its probability mass,
according to that distribution.
Whereas after RLHF, the calibration is shot.
And this is also what you might expect,
even if the humans who were sort of
providing the human feedback in RLHF
preferred the right answer, okay?
The distribution that you get after performing RLHF
with the objective that's commonly used for RLHF
sort of creates a reduction in temperature.
It's equivalent to reducing the temperature
of the parts where the human feedback
is exactly aligned with sort of the correct answer.
And so it becomes overconfident.
And, you know, this is very prompt dependent.
I don't mean to say that this is like always gonna happen
if you go and use GPT, but I went and used GPT-3.5
to do this like infilling task,
and it did it correctly, but also every time
that I generated at temperature one,
it gave me basically the same answer.
So if I want to think of this distribution
as sort of representing uncertainties
that I can make decisions about whether to listen
to my voicemail because it might contain things
I don't know, this P-task is not up to that task.
So our idea is to instead of reducing to a prompt,
reduce to a probabilistic program
that may call a language model,
which is sort of a more flexible way
of specifying what P-task distributions
we want to sample from.
And I know I'm running low on time,
but the idea is that these models
can mix calls to the language model
with conditioning statements and other logic.
So in this probabilistic program for this infilling task,
it is in a loop going through each sort of blank
in the that we need to infill,
sampling a random number of tokens
that should fill that spot, sampling those tokens
and then observing the next sort of fixed fragment
or conditioning on the next part being a fixed fragment.
And this just lets us specify a model
that doesn't just have a prefix prompt
that sort of has a prompt with blanks in it.
I haven't said yet how we're going to sample this,
but the idea is that this defines a specification
for the task that we want.
And similarly, we have programs that sort of specify
a variety of tasks that involve sort of thinking
with language, right?
We can condition on hard constraints
if we want to parse into a formal grammar or write a high two.
We can do kind of a product of experts model
using multiple prompts.
So maybe I want to think about a fun fact
that's about both London and Paris.
Well, you could just ask the prompt,
hey, please give me a fun fact about both London and Paris,
but you could also create a product of experts model
where it has to come up with a completion
that is both a completion to the sentence,
a fun fact about London is,
and a completion to the sentence,
a fun fact about Paris is.
And that's kind of a hybrid where the idea of and
and both is symbolically encoded,
but we're still using the language models representation
of knowledge about fun facts and these things.
Similarly, we can sort of represent reward steering
or a classifier guidance by conditioning,
by sort of soft conditioning on a reward function.
And we can also include things like,
hey, please generate a gloss of this code
that when I try to semantic parse it back into code,
gives me the same code I started with, things like that.
So those are model programs for specifying various tasks.
We need inference algorithms
for actually sampling from these distributions.
And so far we've been focusing
on sequential Monte Carlo inference algorithms.
And we kind of have a default version of this method
and then fancier versions of this method
that are necessary for harder tasks.
In many ways, sequential Monte Carlo looks like beam search.
You kind of keep multiple hypotheses around,
you extend them, you reweight them
according to model specific way,
and then you resample,
which is kind of like the part of beam search
where you sort of down sample
from your big expanded beam back to your beam size.
But unlike beam search,
sequential Monte Carlo,
as you scale up the number of hypotheses that you're using,
instead of converging to an arg max
of your objective function,
converges to the posterior distribution,
sampling from the posterior distribution.
And this sort of default version of SMC
has worked for a few simple tasks that we've tried it on.
For example, if I want a completion that follows
my favorite physicists is probably,
and my favorite writer is probably equally well,
SMC can give me Richard Feynman.
I really admire how he communicates complex ideas so clearly.
Or if I want to finish the fed says,
but only using words less than five letters,
I get the fed says it will taper,
but the rate hikes are still years away
to something like that.
And it's worth noting that if you do something
like token masking to enforce this constraint,
you just forbid the language model
from generating anything that's longer than five letters.
You get all sorts of weird completions.
It's different from the posterior here.
You get completions that set up an idiom
that it could only complete
if it used a word longer than five letters
or something like that.
And then it just gets very confused
and right stop, that, that, read more or something.
It tries to come up with some context
in which the sentence would be cut off early.
And for infilling tasks,
we get a variety of samples that sort of fit semantically
and syntactically with the text,
but infilling tasks can be made much harder than this one.
So I don't want to claim that this method yet solves
all these infilling tasks.
So for harder tasks,
we think we're going to need to use fancier
sequential Monte Carlo algorithms.
And both of these sort of steps
can actually be extended in various ways.
We can use better proposal distributions
and sort of better reweighting strategies
that are trying to guess, okay,
are we on the path to getting a good sample here?
And we think that techniques
that have already been developed in the literature
for proposing good things in line with constraints
or sort of discriminating whether we're likely
to land in a constraint could be good ingredients to put here.
But the important thing is, and this is the very end,
the important thing is all of those things
become part of the inference program.
And there are still guarantees
that as we scale up the number of particles,
we're still targeting the original specification of the model.
So all of those heuristics or biases don't sort of,
we don't just trust them blindly.
We don't hand the keys to those techniques.
We still have a specification that we can understand.
Okay, I'll stop there.
Thanks.
All right, we're a little bit behind time.
So unless there is a burning question,
burning question, no burning questions.
Let's break for tea and maybe Zachertorte
and you guys can talk to the speakers.
And at 11.30, we are going to...
