1
00:00:00,000 --> 00:00:14,320
So our first speaker for the afternoon session is Jacob Steinhardt.

2
00:00:14,320 --> 00:00:21,400
Jacob is a remarkable researcher who kind of combines like crazy creativity and technical

3
00:00:21,400 --> 00:00:22,400
know-how.

4
00:00:22,400 --> 00:00:26,300
And he's gotten me to think very differently about a lot of problems both technically

5
00:00:26,300 --> 00:00:29,200
and I guess societally.

6
00:00:29,200 --> 00:00:36,200
So I predict it will be a very interesting talk.

7
00:00:36,200 --> 00:00:37,760
Thank you, Sasha.

8
00:00:37,760 --> 00:00:45,320
So I decided that I wanted to talk about something new kind of in the spirit of the Simon's

9
00:00:45,320 --> 00:00:46,320
workshop.

10
00:00:46,320 --> 00:00:49,120
So these slides are actually prepared totally from scratch.

11
00:00:49,120 --> 00:00:52,900
No one has seen them before except me on my laptop.

12
00:00:52,900 --> 00:00:55,240
So hopefully it will be interesting.

13
00:00:55,240 --> 00:01:01,720
So the kind of overall motivation here is, you know, at least I really want to understand

14
00:01:01,720 --> 00:01:07,480
what, you know, these ML systems that we're deploying all over the world are doing, how

15
00:01:07,480 --> 00:01:11,480
they behave, what's going on under the hood.

16
00:01:11,480 --> 00:01:15,960
But I find this very challenging because there's so many new ones, you know, maybe like every

17
00:01:15,960 --> 00:01:21,880
month some new model is released and they get bigger and more capable and more complex.

18
00:01:21,880 --> 00:01:26,960
And so how kind of understanding of what these models are doing keep up, especially given

19
00:01:26,960 --> 00:01:32,240
that we often get kind of new capabilities or qualitative behaviors just kind of emerging

20
00:01:32,240 --> 00:01:34,120
every time we scale up.

21
00:01:34,120 --> 00:01:42,200
And so I think, you know, there's maybe more than one answer to this, but something that

22
00:01:42,200 --> 00:01:48,320
will be the focus of this talk is the idea that, well, if we can somehow use LLMs to

23
00:01:48,320 --> 00:01:54,640
understand LLMs, then maybe we're in better shape because then every time a new better

24
00:01:54,640 --> 00:01:58,320
LLM is released, well, there's a new thing to understand, but we've also gotten better

25
00:01:58,320 --> 00:01:59,760
at understanding things.

26
00:01:59,760 --> 00:02:07,000
So, you know, that's kind of the idea, can we get this virtuous cycle?

27
00:02:07,000 --> 00:02:10,120
And then, you know, hopefully as models get better, understanding will as well.

28
00:02:10,120 --> 00:02:15,400
So this is going to be based on work with actually a lot of different collaborators,

29
00:02:15,400 --> 00:02:22,080
but three that I wanted to highlight are my students, Rachie, Eric, and Colin, in particular,

30
00:02:22,080 --> 00:02:29,960
a lot of the kind of LLM as statistician perspective in this talk was developed by Rachie.

31
00:02:29,960 --> 00:02:34,360
So what do I mean by LLMs as statisticians?

32
00:02:34,360 --> 00:02:40,000
So what I want to argue is that many forms of understanding that we could care about

33
00:02:40,000 --> 00:02:45,120
essentially reduce to some sort of statistical or data science problem, right?

34
00:02:45,120 --> 00:02:52,880
So maybe we're given a model, we just see what it outputs on some huge number of inputs,

35
00:02:52,880 --> 00:02:53,880
right?

36
00:02:53,880 --> 00:02:56,720
We could easily do that by just taking all of the data sets we have and seeing what

37
00:02:56,720 --> 00:02:57,840
the model does.

38
00:02:57,840 --> 00:02:59,920
And then maybe we want to kind of identify patterns, right?

39
00:02:59,920 --> 00:03:04,400
Are there some things that the model is good at, some things it's bad at, cases where there's

40
00:03:04,400 --> 00:03:09,800
something surprising, and then, you know, you could try to formalize that as some statistical

41
00:03:09,800 --> 00:03:11,520
hypothesis and test it.

42
00:03:11,520 --> 00:03:14,480
So that's kind of a statistical problem.

43
00:03:14,480 --> 00:03:19,480
Maybe we want to understand the training set and understand, you know, what are the important

44
00:03:19,480 --> 00:03:23,040
sources of variation?

45
00:03:23,040 --> 00:03:27,960
You know, if it turns out that large fractions of the data set are in some weird language

46
00:03:27,960 --> 00:03:32,840
called base 64, maybe we want to know about that.

47
00:03:32,840 --> 00:03:36,720
And then you could also have kind of more active learning or active sampling problems

48
00:03:36,720 --> 00:03:41,640
where we want to, say, generate new inputs that elicit problematic behaviors so that

49
00:03:41,640 --> 00:03:43,440
we can identify and fix it, right?

50
00:03:43,440 --> 00:03:49,280
So I think of these all as on some level kind of statistics or data problems.

51
00:03:49,280 --> 00:03:55,680
And so if we can, in some sort of general sense, get large language models to do statistics,

52
00:03:55,680 --> 00:04:00,720
then they can help us tackle all of these problems, and of course, many other useful

53
00:04:00,720 --> 00:04:02,440
problems as well.

54
00:04:02,440 --> 00:04:05,640
So to do that, what would we need to do?

55
00:04:05,640 --> 00:04:10,120
So I'm going to kind of take a very high level view of, you know, what is, like, the

56
00:04:10,120 --> 00:04:12,880
pipeline of doing statistics?

57
00:04:12,880 --> 00:04:16,160
And I'd say it kind of has four steps.

58
00:04:16,160 --> 00:04:19,280
The first is we look at some initial data.

59
00:04:19,280 --> 00:04:23,040
Maybe we want to think of this as training data, but, you know, just some sort of data

60
00:04:23,040 --> 00:04:25,560
that kind of helps us get our bearings.

61
00:04:25,560 --> 00:04:31,120
From this data, we maybe want to form some hypothesis, you know, and maybe, like, the

62
00:04:31,120 --> 00:04:38,160
hypothesis that models do worse on long inputs than short inputs, right?

63
00:04:38,160 --> 00:04:42,720
And then formalize that quantitatively and then test this on new data.

64
00:04:42,720 --> 00:04:46,120
This data could just be, you know, a held out set from the same distribution.

65
00:04:46,120 --> 00:04:52,280
We might care about kind of, you know, generalization to new domains, or maybe if, maybe we want

66
00:04:52,280 --> 00:04:56,880
to even, like, actively collect data to really stress test our hypothesis.

67
00:04:56,880 --> 00:05:02,560
And so I'm going to go over a couple of case studies where we'll have problems that kind

68
00:05:02,560 --> 00:05:06,800
of follow this structure and will automate each step with LLMs.

69
00:05:06,800 --> 00:05:12,600
The kind of key difference from maybe traditional ways of looking at statistics is going to

70
00:05:12,600 --> 00:05:19,440
be in this hypothesis H. So a key difference is, you know, often H is, like, maybe expressed

71
00:05:19,440 --> 00:05:25,960
as some mathematical function, like, you know, patients with this future in a data set are

72
00:05:25,960 --> 00:05:27,240
more likely to get this disease.

73
00:05:27,240 --> 00:05:31,360
I can write this as, like, expectation of some feature function.

74
00:05:31,360 --> 00:05:35,720
But here, because we're working with language models, H is going to be a natural language

75
00:05:35,720 --> 00:05:36,840
string.

76
00:05:36,840 --> 00:05:41,920
And so this third step, which maybe is often almost trivial, it's just, like, taking the

77
00:05:41,920 --> 00:05:47,520
average of your feature over the data set, now actually becomes kind of non-trivial because

78
00:05:47,520 --> 00:05:51,880
we have to formalize what it means for, say, a natural language string to be true about

79
00:05:51,880 --> 00:05:54,280
a data set or not.

80
00:05:54,280 --> 00:05:58,600
So if that's kind of abstract, don't worry because we're going to go into some case studies

81
00:05:58,600 --> 00:05:59,600
very soon.

82
00:05:59,600 --> 00:06:00,920
In fact, right now.

83
00:06:00,920 --> 00:06:01,920
Okay.

84
00:06:01,920 --> 00:06:08,960
So the first case study I want to talk about is finding failures in a model called Clip.

85
00:06:08,960 --> 00:06:14,420
So for those who aren't familiar with it, Clip is an encoder model.

86
00:06:14,420 --> 00:06:20,840
So it takes in inputs and kind of embeds them as some feature vector.

87
00:06:20,840 --> 00:06:24,760
And it's kind of used as the backbone of many later models, right?

88
00:06:24,760 --> 00:06:27,440
So it's often useful to have good embeddings.

89
00:06:27,440 --> 00:06:31,560
So many models kind of use these embeddings and then do something with them.

90
00:06:31,560 --> 00:06:38,720
And the thing that is kind of special about Clip is that it embeds both images and text

91
00:06:38,720 --> 00:06:41,880
into a kind of shared space.

92
00:06:41,880 --> 00:06:48,880
So it was one of the most early multimodal models that kind of did this effectively.

93
00:06:48,880 --> 00:06:52,080
And as sort of shown here, lots of models use it.

94
00:06:52,080 --> 00:06:55,320
So mid-journey uses it as its kind of first embedding step.

95
00:06:55,320 --> 00:07:01,880
Dolly does, stable diffusion does, and lots of like 3D and video models do as well.

96
00:07:01,880 --> 00:07:06,160
And you can kind of get pretty amazing results using these embeddings.

97
00:07:06,160 --> 00:07:10,080
Often they're used to kind of do text to image generation.

98
00:07:10,080 --> 00:07:14,720
So you have some text description of what you want, and then these models kind of try

99
00:07:14,720 --> 00:07:20,720
to generate an image that matches this description in some way.

100
00:07:20,720 --> 00:07:26,440
So you can see here, these are maybe a little bit small, but this is an empty glass.

101
00:07:26,440 --> 00:07:29,640
And then you get this glass, a family of five members.

102
00:07:29,640 --> 00:07:32,560
And then you get this, a man descending a mountain.

103
00:07:32,560 --> 00:07:33,560
You get this.

104
00:07:33,680 --> 00:07:37,840
So these are all amazing, except for the problem that they're also all completely wrong.

105
00:07:37,840 --> 00:07:39,840
This is a full glass.

106
00:07:39,840 --> 00:07:41,920
This is a family of six members.

107
00:07:41,920 --> 00:07:43,960
This is somewhat us sending a mountain.

108
00:07:43,960 --> 00:07:48,680
This says there's no star in the night sky, and it shows the Milky Way.

109
00:07:48,680 --> 00:07:53,520
And so you get these amazing images, but you often get these kind of like very semantically

110
00:07:53,520 --> 00:07:57,400
obvious failures.

111
00:07:57,400 --> 00:08:03,560
So the kind of plot twist here, aside from the fact that here's a bunch of failures that

112
00:08:03,560 --> 00:08:09,000
we can find, is actually that we didn't find these failures, and LLM found these failures.

113
00:08:09,000 --> 00:08:13,920
So these are all automatically generated by actually not a single LLM, but a kind of

114
00:08:13,920 --> 00:08:20,160
pipeline of several LLMs working together on kind of complimentary tasks that played

115
00:08:20,160 --> 00:08:22,600
to their individual strengths.

116
00:08:22,600 --> 00:08:28,200
So we'll talk about how could we actually build a system that, just given Clip, could

117
00:08:28,200 --> 00:08:33,120
kind of automatically generate all of these failures at a large scale.

118
00:08:33,120 --> 00:08:36,480
So are there any questions so far before I go into the details of this?

119
00:08:36,480 --> 00:08:37,480
Yes?

120
00:08:37,480 --> 00:08:38,480
One quick question.

121
00:08:38,480 --> 00:08:45,520
Why do you say Dolly, new being, what does that mean in this context?

122
00:08:45,520 --> 00:08:52,800
I think different versions of Dolly, and one of them, I'm not totally sure of the details

123
00:08:52,800 --> 00:08:59,280
here, but I believe that Dolly was developed by OpenAI, but then Microsoft served some

124
00:08:59,280 --> 00:09:02,120
version of Dolly as part of their being product.

125
00:09:02,120 --> 00:09:10,560
And so this is that, which we use because we can actually query it kind of publicly.

126
00:09:10,560 --> 00:09:11,560
Yes?

127
00:09:11,560 --> 00:09:18,640
Are there any new type of errors that pop up from this process?

128
00:09:18,640 --> 00:09:24,120
For example, like the counting error or the direction error, those are already kind of

129
00:09:24,120 --> 00:09:25,120
known, right?

130
00:09:25,120 --> 00:09:28,080
So are there any new hypothesized forms in this process?

131
00:09:28,080 --> 00:09:29,800
Yes, that's a good question.

132
00:09:29,800 --> 00:09:33,280
So I'll get to kind of numbers later.

133
00:09:33,280 --> 00:09:38,160
We generated 14 categories of errors.

134
00:09:38,160 --> 00:09:42,480
We looked for papers that kind of described these errors.

135
00:09:42,480 --> 00:09:47,680
I think there were a handful that had already been described in the literature.

136
00:09:47,680 --> 00:09:49,120
The others were not in the literature.

137
00:09:49,120 --> 00:09:54,640
I would probably guess that someone whose day job was to play around with these models

138
00:09:54,640 --> 00:09:57,240
would be familiar with these errors.

139
00:09:57,240 --> 00:10:04,320
But the nice thing is even without spending lots and lots of time, you can do this.

140
00:10:04,320 --> 00:10:12,240
In fact, I think there were cases where reviewers asked us to add a new system to this pipeline

141
00:10:12,240 --> 00:10:17,760
and in a half hour we just got all of the new errors that that system had.

142
00:10:17,760 --> 00:10:22,800
So I think right now that's the advantage is that it's much quicker.

143
00:10:22,800 --> 00:10:27,040
I think as models become better, I'm hoping that we'll also get things that even an expert

144
00:10:27,040 --> 00:10:30,040
who spent lots and lots of time might not necessarily find.

145
00:10:30,040 --> 00:10:32,200
Yeah, great question.

146
00:10:32,200 --> 00:10:33,200
Any other questions?

147
00:10:33,200 --> 00:10:38,760
Okay, so how do we do this?

148
00:10:38,760 --> 00:10:45,320
So first maybe let me give a kind of overview of the key ideas here.

149
00:10:45,320 --> 00:10:50,240
So again, remember, Clip is a feature encoder that is encoding both text and images.

150
00:10:50,240 --> 00:10:55,800
So the kind of main key idea in terms of like where we get off the ground is there's going

151
00:10:55,800 --> 00:10:59,600
to be a sort of notion of what I'll call a hash collision in the encoder.

152
00:10:59,600 --> 00:11:03,560
And we'll come up with a sort of automated way of identifying lots of hash collisions

153
00:11:03,560 --> 00:11:04,560
in the encoder.

154
00:11:04,560 --> 00:11:09,600
So this is going to give us a lot of kind of like individual examples where if you have

155
00:11:09,600 --> 00:11:12,920
a collision, you don't know which of those two is wrong, but you know at least one of

156
00:11:12,920 --> 00:11:16,760
them has to be wrong.

157
00:11:16,760 --> 00:11:21,680
Then kind of given those failures, we're going to use LLMs to kind of categorize them into

158
00:11:21,680 --> 00:11:27,280
coherent patterns and test that those patterns are actually correct by generating new examples

159
00:11:27,760 --> 00:11:33,520
from those patterns and making sure that they actually in fact induce failures consistently

160
00:11:33,520 --> 00:11:39,440
both in the encoder and on downstream tasks and even kind of also generalizing to new

161
00:11:39,440 --> 00:11:44,600
domains that are kind of different from what we found these patterns on.

162
00:11:44,600 --> 00:11:46,720
So that's kind of the high level.

163
00:11:46,720 --> 00:11:50,160
Let me kind of step through these all one by one.

164
00:11:50,160 --> 00:11:55,440
So again, remember we had this statistical pipeline of get some data, generate a hypothesis,

165
00:11:56,440 --> 00:11:57,880
test it on new data.

166
00:11:57,880 --> 00:11:59,920
So let's just go through these one by one.

167
00:11:59,920 --> 00:12:03,960
So first let's just talk about where are we getting this initial data?

168
00:12:03,960 --> 00:12:06,240
What is this kind of hash collision idea?

169
00:12:06,240 --> 00:12:11,160
So to talk about this, I need to give you a little bit more background on Clip.

170
00:12:11,160 --> 00:12:18,160
So as I said before, Clip embeds either an image I or text T, but what is it actually

171
00:12:18,160 --> 00:12:20,680
designed to do or what was it sort of trained on?

172
00:12:20,680 --> 00:12:24,160
So it's actually trained on a bunch of pairs of images and their captions.

173
00:12:24,880 --> 00:12:31,880
And in general, the idea is that if there's text that describes an image, then that text

174
00:12:31,880 --> 00:12:37,720
in that image should have similar embeddings, ideally more similar embeddings to each other

175
00:12:37,720 --> 00:12:39,120
than to anything else.

176
00:12:39,120 --> 00:12:44,320
So the training process was basically you got a bunch of images, you got a bunch of

177
00:12:44,320 --> 00:12:50,280
captions, and then you want to make sure that under this embedding, the cosine similarity

178
00:12:50,320 --> 00:12:56,200
between an image and its caption is higher than between that image and any other captions.

179
00:12:56,200 --> 00:12:59,760
So you kind of want, if we form this matrix of dot products, you want the diagonal to

180
00:12:59,760 --> 00:13:03,200
be really big and everything else to be really small.

181
00:13:03,200 --> 00:13:08,160
And so the kind of point here is that if T is the description of I, they should have

182
00:13:08,160 --> 00:13:11,520
similar embeddings.

183
00:13:11,520 --> 00:13:15,680
Now how can I use this to find problems?

184
00:13:15,800 --> 00:13:22,000
Well, if T and T prime describe different images, but have the same embedding or have

185
00:13:22,000 --> 00:13:28,000
very similar embeddings, then at least one of them has to be wrong in some sense, right?

186
00:13:28,000 --> 00:13:33,760
Because they, you know, like whatever images these corresponded to, they kind of can't

187
00:13:33,760 --> 00:13:37,160
both be, they can't both be right.

188
00:13:37,160 --> 00:13:38,160
Ah, yes, Aliosha.

189
00:13:38,160 --> 00:13:43,480
That's the subjection between images and text, and we know that, you know, pictures

190
00:13:43,480 --> 00:13:45,680
weren't a thousand words.

191
00:13:45,680 --> 00:13:52,200
Right, so the examples I have in mind would be something like an empty cup and a full

192
00:13:52,200 --> 00:13:53,200
cup.

193
00:13:53,200 --> 00:13:56,680
And if those, like if those sentences had the same embedding.

194
00:13:56,680 --> 00:14:00,480
But those are, that's, that's a very small subset of all the, like there is a lot of

195
00:14:00,480 --> 00:14:04,440
synonyms, right, visual synonyms.

196
00:14:04,440 --> 00:14:05,440
That's right.

197
00:14:05,440 --> 00:14:07,480
So you have to worry about visual synonyms.

198
00:14:07,480 --> 00:14:13,320
So we need, we need some way of measuring kind of semantic difference that hopefully

199
00:14:13,320 --> 00:14:16,200
implies that things are actually visually different.

200
00:14:16,200 --> 00:14:19,040
So I'll get to that on the next slide.

201
00:14:19,040 --> 00:14:20,800
Other questions though?

202
00:14:20,800 --> 00:14:28,720
So one thing here also is the nice thing is that I can do this only looking at text, right?

203
00:14:28,720 --> 00:14:32,240
I mean, I have to use the clip encoder, but I'm only encoding text.

204
00:14:32,240 --> 00:14:33,800
And why do I want to only look at text?

205
00:14:33,800 --> 00:14:37,680
Well, basically, because language models work really well and image models don't.

206
00:14:37,680 --> 00:14:42,920
Sorry, Aliyosha.

207
00:14:42,920 --> 00:14:47,280
But you know, according to Aliyosha, image models will work really well soon.

208
00:14:47,280 --> 00:14:48,280
Even better.

209
00:14:48,280 --> 00:14:50,960
But right now, right now, we want to stick with language.

210
00:14:50,960 --> 00:14:53,160
So, so what do we do?

211
00:14:53,160 --> 00:14:57,160
We're going to collect some initial corpus of text inputs, and we want these inputs to

212
00:14:57,160 --> 00:15:00,880
be inputs that have some visual significance.

213
00:15:00,880 --> 00:15:06,920
So we'll often take them from some, say, captioning data set or other kind of data

214
00:15:06,920 --> 00:15:10,120
set that has visual descriptions.

215
00:15:10,120 --> 00:15:12,760
And then we're going to embed them all under clip.

216
00:15:12,760 --> 00:15:16,800
And we're also going to embed them under another model called Distill Roberta, which

217
00:15:16,800 --> 00:15:22,320
is a very good text model, especially for embeddings.

218
00:15:22,320 --> 00:15:24,720
So it's a text-only model.

219
00:15:24,720 --> 00:15:28,560
And it also has a higher-dimensional embedding space than clip.

220
00:15:28,560 --> 00:15:31,920
And so for both of these reasons, it's kind of, you know, has a better understanding of

221
00:15:31,920 --> 00:15:37,000
text than clip does, because it gets to only focus on text and it has more parameters.

222
00:15:37,000 --> 00:15:45,080
And so the basic idea is, you know, so we have all these clip embeddings.

223
00:15:45,080 --> 00:15:52,640
And if there's two inputs that are very close in clip space, but actually have low Roberta

224
00:15:52,640 --> 00:15:54,560
similarity, right?

225
00:15:54,560 --> 00:15:58,760
So that means they're kind of different, like Roberta thinks that they're semantically

226
00:15:58,760 --> 00:16:03,200
different sentences, but clip says they're the same, then we're going to say, OK, that's

227
00:16:03,200 --> 00:16:06,720
a hash collision, probably something is wrong there.

228
00:16:06,720 --> 00:16:12,880
Now, I agree with you that we also want to check that these really are semantically different

229
00:16:12,880 --> 00:16:14,920
in a way that matters visually.

230
00:16:14,920 --> 00:16:19,960
Empirically, it turns out that that's the case about 90% of the time.

231
00:16:19,960 --> 00:16:24,800
So this is kind of good enough, and this is something we kind of verified with human

232
00:16:24,800 --> 00:16:25,800
subject studies.

233
00:16:25,800 --> 00:16:28,080
Jacob, how do you find these things?

234
00:16:28,080 --> 00:16:32,640
Because aren't the clip vectors like 2048 dimensional or something?

235
00:16:32,640 --> 00:16:39,880
Yeah, so we're just taking the cosine similarity.

236
00:16:39,880 --> 00:16:42,800
So this is like, this is an n squared algorithm.

237
00:16:42,800 --> 00:16:47,440
Oh, yeah, but OK, but I mean, if we imagine that you were just looking for collisions

238
00:16:47,440 --> 00:16:53,120
in this 2048 dimensional space, we would say a priori that could take astronomical time.

239
00:16:53,120 --> 00:16:57,840
You're saying like in practice, it takes much less time because, you know, there's something

240
00:16:57,840 --> 00:17:03,400
about these text inputs that makes the collisions likelier.

241
00:17:03,400 --> 00:17:04,760
So yeah, a couple of things.

242
00:17:04,760 --> 00:17:08,240
I guess first we don't need exact collisions.

243
00:17:08,240 --> 00:17:13,920
If the cosine similarity is large enough, I guess empirically, if it's larger than 0.88,

244
00:17:13,920 --> 00:17:19,720
it turns out that it's pretty likely to create a problem.

245
00:17:19,720 --> 00:17:21,440
So you don't need them to be exactly the same.

246
00:17:21,440 --> 00:17:23,400
That kind of helped you somewhat.

247
00:17:23,400 --> 00:17:25,400
And yeah, so somehow this...

248
00:17:25,400 --> 00:17:33,040
But I mean, two unit vectors having an inner product of 0.88 in a 2048 dimensional space,

249
00:17:33,040 --> 00:17:36,400
we might as well call that a collision, right, at that point.

250
00:17:36,400 --> 00:17:41,120
But he's assuming n squared time for u n with the exponential, right?

251
00:17:41,800 --> 00:17:42,800
So, okay, right.

252
00:17:42,800 --> 00:17:46,040
So Scott's claiming we would need an exponential large data set, but this is not...

253
00:17:46,040 --> 00:17:51,320
Yeah, yeah, yeah, but okay, but it happens that...

254
00:17:51,320 --> 00:17:54,840
So basically the reason why there are collisions is not...

255
00:17:54,840 --> 00:17:59,040
has nothing to do with like the pigeonhole principle with the space, right?

256
00:17:59,040 --> 00:18:04,320
It's just that, you know, the way that it's something special about this mapping that

257
00:18:04,320 --> 00:18:09,880
causes there to be collisions, even though a priori there could have been no collisions.

258
00:18:09,880 --> 00:18:10,880
Yeah, that's right.

259
00:18:10,880 --> 00:18:15,240
Okay, and if you care, couldn't you do it much faster by using the same kind of h and

260
00:18:15,240 --> 00:18:20,320
s, w in the cities with all of the embeddings?

261
00:18:20,320 --> 00:18:23,000
Yeah, you could do this much faster than n squared.

262
00:18:23,000 --> 00:18:28,280
It just turned out that this is not the bottleneck, like the bottleneck is running the forward

263
00:18:28,280 --> 00:18:35,080
passes of all the models and kind of looping over a bunch of pairs of, you know, thousand

264
00:18:35,080 --> 00:18:39,440
dimensional vectors is pretty cheap compared to running an LLM.

265
00:18:40,160 --> 00:18:44,240
Yeah, so we tried two different corpora.

266
00:18:44,240 --> 00:18:52,280
One is Cocoa and the other is, what's the other one, SNLI, yeah.

267
00:18:52,280 --> 00:18:58,000
These are both kind of text data sets that have visual significance.

268
00:18:58,000 --> 00:19:01,720
And yeah, I mean, basically the point is that there's enough, yeah, there's enough structure

269
00:19:01,720 --> 00:19:04,120
in text that you actually do get collisions.

270
00:19:04,120 --> 00:19:06,880
I don't care about the n squared.

271
00:19:06,880 --> 00:19:09,040
Okay, okay, got it, got it.

272
00:19:09,240 --> 00:19:10,920
Okay, so this is the first step, right?

273
00:19:10,920 --> 00:19:14,520
This is going to give us a bunch of pairs where we kind of know that like one of the

274
00:19:14,520 --> 00:19:18,480
two things in the pair is wrong.

275
00:19:18,480 --> 00:19:21,560
And so now we want to do something with that.

276
00:19:21,560 --> 00:19:26,520
So this is kind of the next stage is we want to generate some hypotheses based on these

277
00:19:26,520 --> 00:19:27,720
pairs, right?

278
00:19:27,720 --> 00:19:32,600
So this is where we're going to use the fact that we have text and not images, right?

279
00:19:32,600 --> 00:19:37,240
So the individual pair fillers are text inputs, they can feed them to GPT4.

280
00:19:37,240 --> 00:19:42,160
And so, you know, here's the magical prompt, it says I'll provide a series of data for

281
00:19:42,160 --> 00:19:43,160
you to remember.

282
00:19:43,160 --> 00:19:46,600
Subsequently, I'll ask you some questions to test your performance.

283
00:19:46,600 --> 00:19:51,400
Here's some pairs of prompts to memorize, and then you give it all of, well, you give

284
00:19:51,400 --> 00:19:55,040
it as many of these failures as you can fit in the context window.

285
00:19:55,040 --> 00:19:59,000
And then you tell it, hey, I'm trying to find failures as an embedding model.

286
00:19:59,000 --> 00:20:03,840
These are pairs of sentences that are encoded very similarly using the specific examples.

287
00:20:03,840 --> 00:20:07,160
Are there any general types of failures you notice the embedding is making?

288
00:20:07,160 --> 00:20:11,760
And then, you know, you kind of give it some more context and you'd say, okay, what does

289
00:20:11,760 --> 00:20:12,760
it generate, right?

290
00:20:12,760 --> 00:20:18,120
So you're basically saying, here's some data, please look at it, please tell me some patterns.

291
00:20:18,120 --> 00:20:23,080
And so then it, you know, it does a pretty good job of coming up with things.

292
00:20:23,080 --> 00:20:28,840
It says, okay, there's negation, temporal differences, quantifiers, and the nice thing

293
00:20:28,840 --> 00:20:33,080
is actually it doesn't just say negation, but it gives a bunch of elaborations.

294
00:20:33,080 --> 00:20:36,880
So it says embedding models may not correctly capture the negative context in a sentence

295
00:20:36,880 --> 00:20:41,920
leading to similarities between sentences with and without negation.

296
00:20:41,920 --> 00:20:45,960
This can result in incorrect visual representations if the presence or absence of an action is

297
00:20:45,960 --> 00:20:49,200
significant in video generation.

298
00:20:49,200 --> 00:20:53,120
And it kind of keeps going.

299
00:20:53,120 --> 00:20:59,240
And you get, I guess in this case, you get kind of 14 distinct failures in total on this

300
00:20:59,240 --> 00:21:02,400
list.

301
00:21:02,400 --> 00:21:05,880
And the other thing is empirically, GPD4 just kind of always uses this consistent list

302
00:21:05,880 --> 00:21:09,600
format so you can automatically just parse out the individual hypotheses.

303
00:21:09,600 --> 00:21:10,600
Yeah, Lisa.

304
00:21:10,600 --> 00:21:18,080
Have you tried just like asking GPT without these like inputs, like what are common failure

305
00:21:18,080 --> 00:21:21,120
cases of image embedding models or something?

306
00:21:21,120 --> 00:21:22,120
Yes.

307
00:21:22,120 --> 00:21:27,240
So that's a baseline that I will show results on later.

308
00:21:27,240 --> 00:21:32,560
And yeah, it works a lot less well.

309
00:21:33,200 --> 00:21:38,280
So this kind of gives us hypotheses.

310
00:21:38,280 --> 00:21:40,080
So this was step two.

311
00:21:40,080 --> 00:21:44,520
But now we're running into this problem of, OK, usually in statistics a hypothesis is

312
00:21:44,520 --> 00:21:49,840
like actually some mathematical function, but here these are just sentences.

313
00:21:49,840 --> 00:21:52,400
So now we need to formalize this hypothesis.

314
00:21:52,400 --> 00:21:57,920
So we have this list of hypotheses, h1 through hk, that are all natural language descriptions.

315
00:21:57,920 --> 00:22:03,440
So how can we test if one of these hypotheses is actually any good?

316
00:22:03,440 --> 00:22:07,440
So I think this is a pretty interesting conceptual question to think about.

317
00:22:07,440 --> 00:22:14,400
So maybe I'll pose it to the audience if anyone has ideas for how we could formalize this.

318
00:22:14,400 --> 00:22:16,400
Hi, Chris.

319
00:22:16,400 --> 00:22:18,400
Good afternoon.

320
00:22:18,400 --> 00:22:25,520
Two or more about pairs as to whether they match this hypothesis and then see if the

321
00:22:25,520 --> 00:22:26,520
images are wrong.

322
00:22:26,520 --> 00:22:27,520
OK.

323
00:22:27,520 --> 00:22:31,520
In this analysis of DPT4, if you have access.

324
00:22:31,520 --> 00:22:32,520
Good.

325
00:22:32,520 --> 00:22:33,520
Yes.

326
00:22:33,520 --> 00:22:34,520
So that is Yan Yi.

327
00:22:34,520 --> 00:22:41,680
I mean, if we think about research hypothesis, there are a few dimensions that you can use

328
00:22:41,680 --> 00:22:44,160
to categorize whether some say it's a hypothesis.

329
00:22:44,160 --> 00:22:47,160
So for example, it should be testable, right?

330
00:22:47,280 --> 00:22:49,280
There should be a clear scope.

331
00:22:49,280 --> 00:22:53,280
There are a few dimensions I think that you can come up with based on experts.

332
00:22:53,280 --> 00:22:54,280
Right.

333
00:22:54,280 --> 00:23:01,000
So you can kind of ask experts or DPT4 if it had those properties.

334
00:23:01,000 --> 00:23:05,960
So it turns out we're going to do something pretty similar to what Chris said, although

335
00:23:05,960 --> 00:23:09,120
we're going to look at generation rather than classification.

336
00:23:09,120 --> 00:23:17,880
So we're going to say H is a good hypothesis if when you hand that description to some

337
00:23:17,880 --> 00:23:24,400
intelligent agent, in this case not humans, because humans are expensive, but DPT4, they

338
00:23:24,400 --> 00:23:31,440
do a better job of generating new failures than they would otherwise.

339
00:23:31,440 --> 00:23:36,360
So this is the way we're going to quantify this.

340
00:23:36,360 --> 00:23:43,040
So we'll say H is a good hypothesis if it can be used to generate new failures better

341
00:23:43,040 --> 00:23:46,720
than some just baseline method of generating failures.

342
00:23:46,720 --> 00:23:50,080
And this is where we're going to get to your question, Lisa.

343
00:23:50,080 --> 00:23:57,480
So we can either hand DPT4 these hypotheses or we could just ask DPT4 to brainstorm without

344
00:23:57,480 --> 00:24:03,520
any data ways in which vision models might be bad and kind of test those against each

345
00:24:03,520 --> 00:24:06,200
other and see which one does better.

346
00:24:06,200 --> 00:24:09,920
So we're going to test this by prompting an LLM with H as a context.

347
00:24:09,920 --> 00:24:12,680
And so again, what is the magical prompt?

348
00:24:12,680 --> 00:24:18,000
The magical prompt is to say write down 41 pairs of prompts that an embedding model with

349
00:24:18,000 --> 00:24:22,240
the following failure mode might encode similarly, even though they would correspond to different

350
00:24:22,240 --> 00:24:24,280
images if used as captions.

351
00:24:24,280 --> 00:24:30,240
Use the following format so you give it kind of a format so that we can extract things programmatically.

352
00:24:30,240 --> 00:24:34,680
And then we say some other stuff to motivate it, saying that it will be evaluated based

353
00:24:34,680 --> 00:24:37,680
on how well it performs.

354
00:24:37,680 --> 00:24:44,840
And then you give the failure mode and as kind of the description that we extracted before.

355
00:24:44,840 --> 00:24:54,080
Y41, that's the length, basically the length of the output context window that can be fit.

356
00:24:54,080 --> 00:24:59,160
So if you want more than 41, you just have to ask it a couple of times.

357
00:24:59,200 --> 00:25:01,600
So this is what we did.

358
00:25:01,600 --> 00:25:04,600
Yeah, Nicholas.

359
00:25:04,600 --> 00:25:10,240
How much does this be creative and cautious and these kinds of things like actually help?

360
00:25:10,240 --> 00:25:15,440
Or is this just like black magic that you sort of sprinkle on top?

361
00:25:15,440 --> 00:25:20,240
We didn't do careful ablations on the prompt.

362
00:25:20,240 --> 00:25:26,200
I think, yeah, we added a bunch of stuff until it worked.

363
00:25:26,600 --> 00:25:29,960
I don't think we tried removing things to see what was actually necessary.

364
00:25:29,960 --> 00:25:36,480
So it seems totally like I would guess that if you tried to distill this to its bare essentials,

365
00:25:36,480 --> 00:25:39,080
you could get something simpler.

366
00:25:39,080 --> 00:25:41,320
But we didn't try to do that.

367
00:25:41,320 --> 00:25:44,120
But yeah, great question.

368
00:25:44,120 --> 00:25:48,240
OK, so then we want to quantify by measuring the success rate.

369
00:25:48,240 --> 00:25:54,600
So we get all these pairs of prompts that are supposedly supposed to be new failures.

370
00:25:54,600 --> 00:25:56,960
So we can do this in two ways.

371
00:25:56,960 --> 00:26:04,640
We can look at the fraction of things generated that are hash collisions in the same sense as before.

372
00:26:04,640 --> 00:26:07,680
So that's kind of an easy thing to do.

373
00:26:07,680 --> 00:26:11,120
At some point, we want to make sure that the system is actually doing something

374
00:26:11,120 --> 00:26:16,440
and that the something doesn't involve trusting that LLMs are good at their job.

375
00:26:16,440 --> 00:26:23,880
So we also do a human evaluation where we look at downstream systems that rely on clip

376
00:26:23,960 --> 00:26:29,440
and ask humans if there's a failure to make sure that these actually are failures

377
00:26:29,440 --> 00:26:33,560
and not just happen to have high cosine similarity.

378
00:26:33,560 --> 00:26:36,040
So those are the two things we do.

379
00:26:36,040 --> 00:26:39,000
So let's kind of go over the results.

380
00:26:39,000 --> 00:26:45,240
So first, just kind of looking at hash collisions,

381
00:26:45,240 --> 00:26:50,400
but testing on these new inputs that were generated.

382
00:26:50,400 --> 00:26:58,480
So we say an input has a success if these similarities are above some threshold.

383
00:26:58,480 --> 00:27:00,760
And what is this table saying?

384
00:27:00,760 --> 00:27:06,920
So these rows are kind of the different failures generated by the system.

385
00:27:06,920 --> 00:27:10,480
And actually, we considered six different systems.

386
00:27:10,480 --> 00:27:16,600
So you can ask different models to look at the data and propose hypotheses.

387
00:27:16,600 --> 00:27:21,800
So these are different kind of proposal models, GPT-4-Claude or GPT-3.5.

388
00:27:21,800 --> 00:27:28,680
And then you can also vary the data set that you used to actually get these failures out.

389
00:27:28,680 --> 00:27:34,160
So these are the two data sets that people asked about before, COCO and SNLI.

390
00:27:34,160 --> 00:27:36,840
So I guess a couple interesting things.

391
00:27:36,840 --> 00:27:46,320
One is that, oh, and a check mark means that the model generated the failure at all in its list.

392
00:27:46,360 --> 00:27:55,680
And then the color is kind of the success rate of generating new inputs conditional on that failure description.

393
00:27:55,680 --> 00:27:58,080
So a couple interesting things.

394
00:27:58,080 --> 00:28:01,360
First of all, the data set seems to actually matter.

395
00:28:01,360 --> 00:28:13,640
So kind of for both GPT-4 and Claude, action, well, OK, maybe let's pick a more intuitive one.

396
00:28:13,640 --> 00:28:26,160
OK, so for both GPT-4 and Claude and GPT-3.5, SNLI elicits granularity as a failure, whereas COCO never does.

397
00:28:26,160 --> 00:28:28,960
And sometimes it's kind of not quite so systematic.

398
00:28:28,960 --> 00:28:35,120
But in general, it sort of seems like these data sets actually do kind of like elicit different failures.

399
00:28:35,120 --> 00:28:41,400
So there is at least some dependence on the data, which is somewhat reassuring.

400
00:28:41,400 --> 00:28:48,280
The other thing is maybe as expected, GPT-4 and Claude in general find many more failures than GPT-3.5 does.

401
00:28:48,280 --> 00:28:54,040
So these better models actually generate more distinct hypotheses.

402
00:28:54,040 --> 00:29:07,560
And then a final thing that is interesting is actually even for the same failure, bigger models often are giving you higher success rates.

403
00:29:07,560 --> 00:29:12,480
So you can see this in a couple places like for granularity.

404
00:29:12,480 --> 00:29:22,200
The description of the granularity failure that GPT-4 generated was apparently better in terms of if you then hand that back to GPT-4,

405
00:29:22,200 --> 00:29:32,280
it more successfully generates novel failure instances compared to the kind of description that GPT-3.5 gave.

406
00:29:32,280 --> 00:29:37,360
In all of these cases, we're fixing GPT-4 as kind of the thing that's generating new failures.

407
00:29:37,360 --> 00:29:40,480
So there's no effect from that.

408
00:29:40,480 --> 00:29:47,000
So this kind of difference is just coming from the actual text description output by them all.

409
00:29:47,000 --> 00:29:56,720
So are there any questions about this data?

410
00:29:56,720 --> 00:30:11,640
What more is left about when GPT-4 does better than GPT-3.5, is it because it better understood the instruction versus maybe GPT-3.5 also understood the description,

411
00:30:11,640 --> 00:30:24,160
but somehow the examples were very just curious what sort of qualitative differences are there between different models.

412
00:30:25,080 --> 00:30:29,720
So I haven't thought about this a ton.

413
00:30:29,720 --> 00:30:41,280
I think my two main hypotheses here would be, I believe GPT-4 has a larger context window so it can see more examples, which might be useful.

414
00:30:41,280 --> 00:30:54,040
But I think probably the more important thing is actually just that the task of proposing hypotheses from a data set is actually a pretty challenging task.

415
00:30:54,040 --> 00:31:00,240
And so even kind of frontier models are not that good at it.

416
00:31:00,240 --> 00:31:11,520
So then once you drop down from GPT-4 to GPT-3.5, you're kind of losing, probably just losing too much capability for it to be super consistent.

417
00:31:11,520 --> 00:31:13,080
That would be my hypothesis.

418
00:31:13,080 --> 00:31:13,880
I don't know.

419
00:31:13,880 --> 00:31:14,560
Yeah, Richie.

420
00:31:14,560 --> 00:31:23,040
Yeah, so in practice, we found that GPT-3.5 doesn't really condition on the data very well, while GPT-4 actually does condition on the data.

421
00:31:23,040 --> 00:31:25,040
And then describe the same thing that they do.

422
00:31:28,040 --> 00:31:29,040
Yes?

423
00:31:29,040 --> 00:31:38,600
If I have the samples that the model generated actually fall into those categories, so maybe they overlap with some other categories.

424
00:31:38,600 --> 00:31:48,800
So I don't think we did a systematic evaluation of whether all of the examples fall into those categories.

425
00:31:49,560 --> 00:31:56,560
Yeah, I can give some orders on the next slide that get at least implicitly at that.

426
00:31:56,560 --> 00:32:00,560
It looked like maybe someone else had a question.

427
00:32:00,560 --> 00:32:08,560
OK, so maybe let's go to the human evaluation, which will at least partially answer your question.

428
00:32:08,560 --> 00:32:18,560
So we wanted to not just stick with saying that you actually get these hash collisions, but show that these actually lead to images that humans say are wrong.

429
00:32:19,320 --> 00:32:30,320
So, OK, darn, the text here is a little bit small, but this is kind of the human annotator interface that we gave.

430
00:32:30,320 --> 00:32:39,320
So we had prompt one, a city skyline with a bridge, prompt two, a city skyline without a bridge.

431
00:32:39,320 --> 00:32:42,320
So this is this kind of collision pair.

432
00:32:42,320 --> 00:32:52,320
This is an image that came from one of these two prompts chosen at random.

433
00:32:52,320 --> 00:33:04,320
And so the annotator has to say either that this corresponds to prompt one, it corresponds to prompt two, it corresponds to neither of them,

434
00:33:05,320 --> 00:33:10,320
or these prompts are described visually identical situations.

435
00:33:10,320 --> 00:33:17,320
So this kind of gets at your earlier question, Alyosha, on whether these are actually semantically different.

436
00:33:17,320 --> 00:33:29,320
And so we're kind of measuring what counts as a successful failure, either if the annotator says that it's wrong,

437
00:33:29,320 --> 00:33:35,320
or if they say it's prompt two, but it was actually generated by prompt one, or vice versa.

438
00:33:35,320 --> 00:33:49,320
And so then you can look at the kind of rate at which mistakes are made conditional on different levels of clip similarity in the two prompts.

439
00:33:49,320 --> 00:33:53,320
So this is kind of testing that high similarity actually leads to failures.

440
00:33:53,320 --> 00:33:59,320
And you can kind of see there's this like inflection point at like 0.88.

441
00:33:59,320 --> 00:34:07,320
So this is kind of one verifying that actually we do get pretty high rates of failure,

442
00:34:07,320 --> 00:34:13,320
and two that this magical threshold I told you about earlier is actually kind of reasonable threshold.

443
00:34:13,320 --> 00:34:20,320
And then finally, to get at this question of whether these descriptions are actually doing anything.

444
00:34:20,320 --> 00:34:24,320
So I guess this doesn't test whether the failures correspond to the descriptions,

445
00:34:24,320 --> 00:34:29,320
but it tests that the descriptions are actually needed to get high failure rates.

446
00:34:29,320 --> 00:34:35,320
If you have a baseline system where you just ask it to like brainstorm possible failures images might have,

447
00:34:35,320 --> 00:34:42,320
and then condition on those, you only get about 20% failure rate,

448
00:34:42,320 --> 00:34:47,320
whereas you get an 80% failure rate if you use this data conditioned system.

449
00:34:47,320 --> 00:34:55,320
So these are the human evaluated rather than model evaluated equals.

450
00:34:55,320 --> 00:35:02,320
So are there questions about this?

451
00:35:02,320 --> 00:35:05,320
This is a talk where high failure rate is better.

452
00:35:05,320 --> 00:35:12,320
Yes, yeah, you want high failure because we're trying to find failures so that we can fix them.

453
00:35:12,320 --> 00:35:21,320
Okay, and then I guess a final cool thing is, you know, a kind of really big bonus of having this come from language models

454
00:35:21,320 --> 00:35:24,320
is language models are kind of automatically steerable.

455
00:35:24,320 --> 00:35:26,320
Right, so I have this way of generating failures,

456
00:35:26,320 --> 00:35:33,320
but I can then just ask the model to give me failures that are relevant to some new domain.

457
00:35:33,320 --> 00:35:40,320
And so in this case, we kind of asked it to generate failures that are relevant to self-driving.

458
00:35:40,320 --> 00:35:46,320
The data sets are still cocoa and SNLI so we didn't give it data that would specialize to self-driving,

459
00:35:46,320 --> 00:35:54,320
but it can still kind of generate these failures in this novel domain and still have a good success rate.

460
00:35:54,320 --> 00:35:57,320
And so these are just kind of examples.

461
00:35:57,320 --> 00:36:01,320
Stable diffusion, you have the cars on the right side of the lane, but it's on the left side.

462
00:36:01,320 --> 00:36:04,320
This is not a green light, gives you a green light.

463
00:36:04,320 --> 00:36:13,320
A yield sign gives you something that is at least not shaped like a yield sign, probably a stop sign.

464
00:36:13,320 --> 00:36:15,320
And then a car stops for a red light.

465
00:36:15,320 --> 00:36:20,320
This is actually a text to video model and the light is green.

466
00:36:21,320 --> 00:36:28,320
What data from cocoa and SNLI are you passing in?

467
00:36:28,320 --> 00:36:31,320
No, no, so you're passing in the text from...

468
00:36:31,320 --> 00:36:39,320
So this is for the very first stage where you're giving it a bunch of just text inputs and embedding them to check for hash collisions.

469
00:36:39,320 --> 00:36:46,320
So those hash collisions were from embedding text sentences from cocoa and SNLI.

470
00:36:46,320 --> 00:36:54,320
So there's actually no images anywhere here except in the output of the systems.

471
00:36:54,320 --> 00:37:00,320
I'm kind of curious about the hypothesis part of this and whether that's kind of necessary.

472
00:37:00,320 --> 00:37:05,320
So we had a paper a couple years ago and just tried finding these collisions.

473
00:37:05,320 --> 00:37:11,320
And I kind of wonder if you could just give it a sentence and like search for a collision and just cut out the language model.

474
00:37:11,320 --> 00:37:15,320
What is it adding in the process?

475
00:37:15,320 --> 00:37:19,320
Well, finding the initial... Oh, I see.

476
00:37:19,320 --> 00:37:34,320
So I think one thing is this steerability I think would be challenging in some cases if you were doing that because you would need a large data set of text in whatever new domain you were looking at.

477
00:37:34,320 --> 00:37:42,320
We don't need a bunch of sentences about self-driving cars to do this, but if you were looking for collisions manually, then you would have to do that.

478
00:37:42,320 --> 00:37:45,320
I see.

479
00:37:45,320 --> 00:37:48,320
Yeah, cool.

480
00:37:48,320 --> 00:37:57,320
Okay, so to summarize this, right, we had these four stages of, you know, first we want to get initial data, which we did by scraping hash collisions from this text data set.

481
00:37:57,320 --> 00:38:02,320
This kind of invoked these two models clipped into still Roberta.

482
00:38:02,320 --> 00:38:06,320
Then we generate hypotheses by prompting GPT-4.

483
00:38:06,320 --> 00:38:11,320
Then we kind of formalize these hypotheses by looking at the success rate of generating new failures.

484
00:38:11,320 --> 00:38:16,320
So we use GPT-4 to generate the failures and clip to evaluate them.

485
00:38:16,320 --> 00:38:21,320
And then we can also do this active steering, again, prompting GPT-4.

486
00:38:21,320 --> 00:38:35,320
So I think one thing I want to highlight here is that, you know, often we think about just having this one language model and we just come up with our super clever prompt that solves everything and maybe do chain of thoughts.

487
00:38:35,320 --> 00:38:37,320
And this sort of thing.

488
00:38:37,320 --> 00:38:52,320
But I think you can actually get a lot further if you're willing to kind of use this kind of, you know, ecosystem of models together in creative ways.

489
00:38:52,320 --> 00:39:02,320
And I think statistics is a particularly kind of good use case for this because there are these different stages of the pipeline that require kind of different skills.

490
00:39:02,320 --> 00:39:04,320
And statistics also has some nice properties, right?

491
00:39:04,320 --> 00:39:10,320
Like many parts of it are kind of automatically measurable and verifiable.

492
00:39:10,320 --> 00:39:21,320
And so you get a lot of the same strengths as Adam was talking about yesterday with computer programming where, you know, you can...

493
00:39:21,320 --> 00:39:25,320
We haven't done much of this, but like you can, you know, maybe do this self-training.

494
00:39:25,320 --> 00:39:35,320
And maybe if you get models that were really, really, really good at statistics, like superhuman at statistics, because there's so much automatically-generatable data.

495
00:39:35,320 --> 00:39:36,320
Okay.

496
00:39:36,320 --> 00:39:40,320
So that was the first case study.

497
00:39:40,320 --> 00:39:42,320
Let me go over the second one.

498
00:39:42,320 --> 00:39:48,320
We'll go a bit more quickly now that we've kind of built up a lot of these conceptual ideas.

499
00:39:48,320 --> 00:40:00,320
So here I'm actually going to talk about a kind of meta task that is then going to be useful for lots of individual ways in which we would want to understand language models.

500
00:40:00,320 --> 00:40:04,320
So this meta task is classifying with natural language predicates.

501
00:40:04,320 --> 00:40:08,320
So the task here is we're going to be given two text data sets, D1 and D2.

502
00:40:08,320 --> 00:40:11,320
We want to find out what's different between them.

503
00:40:11,320 --> 00:40:15,320
And this difference, again, should be some natural language string H.

504
00:40:15,320 --> 00:40:20,320
And so we can kind of think about this as isomorphic to binary classification, right?

505
00:40:20,320 --> 00:40:26,320
We're kind of trying to classify between D1 and D2, but where the function is described in natural language.

506
00:40:26,320 --> 00:40:30,320
So let me just give you an example of what this task might look like, right?

507
00:40:30,320 --> 00:40:34,320
So maybe these are my two data sets, D1 and D2.

508
00:40:34,320 --> 00:40:39,320
We want to come up with a natural language description of how they're different.

509
00:40:39,320 --> 00:40:44,320
So can anyone figure this one out?

510
00:40:45,320 --> 00:40:47,320
Okay, yes.

511
00:40:47,320 --> 00:40:50,320
So the left is French and the right is English.

512
00:40:50,320 --> 00:40:56,320
So our H would be D1 contains more French sentences compared to D2.

513
00:40:56,320 --> 00:41:00,320
Okay, so here's a harder example.

514
00:41:00,320 --> 00:41:06,320
Maybe partly hard due to text size.

515
00:41:06,320 --> 00:41:14,320
So I claim that even if you looked at this for a while, it would be hard to tell what the difference was.

516
00:41:14,320 --> 00:41:23,320
And in fact, the difference is that sentences in D1 contain at least two female characters in them, whereas sentences in D2 do not.

517
00:41:23,320 --> 00:41:32,320
This is actually pretty challenging because there's things like she carried a total of eight torpedoes where she refers to a ship,

518
00:41:32,320 --> 00:41:35,320
which is not in fact a female character.

519
00:41:35,320 --> 00:41:40,320
And you also have to know that Professor McKeown is female.

520
00:41:40,320 --> 00:41:49,320
And so there's kind of a lot of world knowledge and kind of non-trivial stuff in solving a problem like this.

521
00:41:49,320 --> 00:41:52,320
So I guess that's the kind of meta task.

522
00:41:52,320 --> 00:41:54,320
Why should we care about this?

523
00:41:54,320 --> 00:42:00,320
So first, I'll give you three use cases that would help us better understand LLMs.

524
00:42:00,320 --> 00:42:05,320
So we could want to understand distribution shift.

525
00:42:05,320 --> 00:42:10,320
So especially if a model is doing poorly out of distribution, maybe we want to diagnose what's different.

526
00:42:10,320 --> 00:42:16,320
And so we might find out that the test distribution involves more formal writing than the training distribution.

527
00:42:16,320 --> 00:42:20,320
And that might help us diagnose failures or tell us what we should fine tune on.

528
00:42:20,320 --> 00:42:24,320
The positive class contains more URLs than the negative class.

529
00:42:24,320 --> 00:42:30,320
If this were a spam classification data set, this would tell us that there was this potential spurious queue.

530
00:42:30,320 --> 00:42:37,320
That maybe the model just looks at the presence or absence of URLs and we should make sure that that's not just what it's doing.

531
00:42:37,320 --> 00:42:39,320
We could do error analysis.

532
00:42:39,320 --> 00:42:47,320
I could give you two models and I could look at the difference between inputs where one of them versus the other one makes mistakes.

533
00:42:47,320 --> 00:42:51,320
And then we could also go beyond just trying to understand LLMs.

534
00:42:51,320 --> 00:42:57,320
You know, you could start trying to do, you know, for like say like social science, right?

535
00:42:57,320 --> 00:43:07,320
I look at a bunch of tweets from one year versus another year and it turns out and say, okay, public opinion from this year is more optimistic about the pandemic than last year, right?

536
00:43:07,320 --> 00:43:12,320
We can kind of generate at least descriptive hypotheses like this.

537
00:43:12,320 --> 00:43:25,320
Of course, you would then want to carefully do all of the causal inference and other stuff to validate these, but this at least can generate hypotheses for you.

538
00:43:25,320 --> 00:43:27,320
Okay, so how will we do this?

539
00:43:27,320 --> 00:43:30,320
Well, yes, D.

540
00:43:30,320 --> 00:43:38,320
So, if you go back to the examples you have, it feels like the hypothesized space is huge, right?

541
00:43:38,320 --> 00:43:45,320
So there seems to be a recall precision issue here, and it seems your ground truth only has some of them.

542
00:43:45,320 --> 00:43:51,320
Like for them, the very easy example, my hypothesis could be there is no relation between those two sentences.

543
00:43:51,320 --> 00:43:55,320
I mean, you have like, it has more French versus the second sentence.

544
00:43:55,320 --> 00:43:57,320
The space seems huge.

545
00:43:57,320 --> 00:44:00,320
How do you locate it to the final ones we want?

546
00:44:00,320 --> 00:44:04,320
So we're not going to have ground truth in most cases.

547
00:44:04,320 --> 00:44:13,320
There are a couple of cases where we did create a ground truth so that we could kind of test in the traditional setting with kind of gold labels,

548
00:44:13,320 --> 00:44:20,320
but we're going to kind of take a similar perspective to how you'd evaluate any other classifier, right?

549
00:44:20,320 --> 00:44:25,320
So we're going to come up with a way to quantify these in terms of some classification error rate,

550
00:44:25,320 --> 00:44:33,320
and then we'll say that one hypothesis is better than another if it has a lower error rate in distinguishing D1 and D2.

551
00:44:33,320 --> 00:44:38,320
And so rather than having a fixed ground truth, we can just talk about which systems are better or worse,

552
00:44:38,320 --> 00:44:44,320
and maybe we could also say compare to humans to get some like overall benchmark.

553
00:44:44,320 --> 00:44:50,320
Some of the hypothesis might be very true, and others may be more usable, right?

554
00:44:50,320 --> 00:44:51,320
Okay, right.

555
00:44:51,320 --> 00:44:58,320
So there's, right, so you might also want to evaluate if there's some like goal,

556
00:44:58,320 --> 00:45:03,320
you might want to evaluate like relevance to the goal and some notion of novelty.

557
00:45:03,320 --> 00:45:05,320
These become, start to become subjective.

558
00:45:05,320 --> 00:45:15,320
So we have done some of this where we get human ratings of novelty and relevance.

559
00:45:15,320 --> 00:45:24,320
Reviewers didn't like it actually because it's too hard to define novelty, but anyways, like you can do this and like I think,

560
00:45:24,320 --> 00:45:27,320
but I do agree it's kind of a tricky problem.

561
00:45:27,320 --> 00:45:37,320
So just building on these questions, are you constraining some complexity of your hypothesis?

562
00:45:37,320 --> 00:45:40,320
Are you looking for short hypotheses?

563
00:45:40,320 --> 00:45:51,320
So we'll implicitly have short hypotheses here, although that kind of just comes from the fact that these are going to be generated by an LLM,

564
00:45:51,320 --> 00:45:56,320
and LLMs will only output things that are so long.

565
00:45:56,320 --> 00:46:06,320
But yeah, we like, for usability reasons, you would want this to be kind of short and interpretable.

566
00:46:06,320 --> 00:46:08,320
So yeah, how are we going to do this?

567
00:46:08,320 --> 00:46:14,320
We're basically just going to use LLMs somewhat similar to before, right?

568
00:46:14,320 --> 00:46:22,320
So I won't give you the full magical prompt that Rachel came up with, but just kind of schematically,

569
00:46:22,320 --> 00:46:30,320
give it a bunch of examples from the first distribution and label them as A, a bunch from the second label them as B.

570
00:46:30,320 --> 00:46:36,320
And then we say, you know, compared to group B, each sentence from group A, and then we ask it to complete it.

571
00:46:36,320 --> 00:46:40,320
This was done at a point where we were using GBD3.

572
00:46:40,320 --> 00:46:43,320
So we needed it to be in this completion format.

573
00:46:43,320 --> 00:46:51,320
Once you have instruction tuned models, there's maybe nicer prompts you can have.

574
00:46:51,320 --> 00:46:53,320
But this is kind of the basic idea.

575
00:46:53,320 --> 00:46:59,320
And then you sample this a bunch of times with different, you know, sub-samples of the data, right?

576
00:46:59,320 --> 00:47:03,320
So keep in mind that we can only fit maybe like 30 or so examples into the context window.

577
00:47:03,320 --> 00:47:08,320
So we have a data set with thousands of examples is like a very tiny fraction.

578
00:47:08,320 --> 00:47:13,320
So we kind of keep sub-sampling to generate different hypotheses.

579
00:47:13,320 --> 00:47:20,320
And then, you know, you'll get things like is more positive, contains the word chapter, is longer.

580
00:47:20,320 --> 00:47:26,320
Some of them actually kind of have, yeah, some of them end up being kind of trivial.

581
00:47:26,320 --> 00:47:28,320
So you might want ways of filtering them out.

582
00:47:28,320 --> 00:47:32,320
But you kind of get this set of candidate hypotheses.

583
00:47:32,320 --> 00:47:38,320
So this is kind of telling us how we do the first two steps of this statistics pipeline, right?

584
00:47:38,320 --> 00:47:40,320
We look at this initial data.

585
00:47:40,320 --> 00:47:47,320
We prompt GPTN for some N with examples from D1 and D2.

586
00:47:47,320 --> 00:47:50,320
And then we ask how they're different in this form of the hypothesis.

587
00:47:50,320 --> 00:47:55,320
But now we need to somehow formalize each quantitatively and test it on new data.

588
00:47:55,320 --> 00:48:07,320
So I guess, again, maybe I'll pose the question, you know, how could we formalize this each quantitatively?

589
00:48:07,320 --> 00:48:14,320
How could we sort of say quantitatively how good is it?

590
00:48:14,320 --> 00:48:16,320
Sorry, what do you say first?

591
00:48:16,320 --> 00:48:18,320
Okay, good.

592
00:48:18,320 --> 00:48:26,320
So we can say, you know, good hypothesis is something that helps tell D1 and D2 apart.

593
00:48:26,320 --> 00:48:36,320
So we'll, you know, take a sample from D1, a sample from D2, mix them up randomly so you don't know which is which.

594
00:48:36,320 --> 00:48:45,320
Tell either a human or an LLM the hypothesis and ask them to say which is which.

595
00:48:45,320 --> 00:48:53,320
So, you know, as an example, say H is involves more formal writing, we can interpret this as basically a two argument predicate, right?

596
00:48:53,320 --> 00:49:05,320
So if I have sentences X1 and X2, H of X1, X2 is some binary predicate that is the truth value of, you know, the utterance X1 involves more formal writing than X2.

597
00:49:06,320 --> 00:49:10,320
And so this should be true or false.

598
00:49:10,320 --> 00:49:16,320
And so then we just ask a human or a language model if it's true or false.

599
00:49:16,320 --> 00:49:30,320
And so then we'll say H is a correct hypothesis about D1 versus D2 if in expectation over samples X1 from D1 and X2 from D2, this is much less than 0.5, right?

600
00:49:30,320 --> 00:49:44,320
So 0.5 would be chance if this is sort of the, like some measure of the classification error, if it's much less than 0.5, then we've, we figured out something non-trivial about D1 and D2.

601
00:49:44,320 --> 00:49:54,320
And so yeah, how to implement this, I guess you could ask humans or you could just query an LLM.

602
00:49:55,320 --> 00:50:11,320
And so what does this look like, right? So just to illustrate this, if H is samples from D1 or more positive than those from D2, we, you know, we give, in this case, Charlie Snell, who was an undergrad at Berkeley at the time and is now a PhD student at Berkeley,

603
00:50:12,320 --> 00:50:30,320
this paper proposes an impactful task or the approach of this paper is too trivial, and ask him which of these it's true about, and then he says something, and then, you know, maybe Charlie's time is pretty valuable, so you can hire crowd workers to do this instead.

604
00:50:30,320 --> 00:50:48,320
But the problem is, you know, even if we just wanted to average over, say, like 100 samples from this distribution to get some notion of accuracy, this would cost $10 per text description, and this is very expensive.

605
00:50:48,320 --> 00:50:50,320
You don't want to do this.

606
00:50:50,320 --> 00:51:02,320
And so the nice thing is LLMs kind of reduce the cost of this pipeline by about a factor of 1000. You can do this 100 samples for only seven cents with GP3.5 turbo.

607
00:51:02,320 --> 00:51:11,320
And so then this gives you a kind of automatic, quantifiable measure of how successful this hypothesis is.

608
00:51:11,320 --> 00:51:25,320
And the nice thing is also that it's somewhat more reproducible than humans, like you don't have to worry about getting back the same human label errors again, because, you know, the model, well, the model's not actually fixed, open AI keeps updating it, that's kind of annoying.

609
00:51:25,320 --> 00:51:33,320
But if they were to serve a stable version of the model, then this would be reproducible.

610
00:51:34,320 --> 00:51:51,320
Right, so now we've kind of gotten this whole pipeline, right, so the overall system as we have this proposal, which at the time we initially wrote this paper was a fine tune to GP3 that generates these candidate hypotheses.

611
00:51:51,320 --> 00:51:57,320
Then we have a verifier that kind of, you know, does this check on each of the hypotheses.

612
00:51:57,320 --> 00:52:04,320
And at the time it was fine tune unified QA. Now you can kind of replace with newer models.

613
00:52:04,320 --> 00:52:11,320
And then you kind of, you know, re rank the hypotheses based on their actual success rate at this classification task.

614
00:52:11,320 --> 00:52:18,320
And, you know, why is this decomposition useful. Well, from an engineering perspective, and I think this is actually very important.

615
00:52:18,320 --> 00:52:24,320
The proposer only sees 30 examples, because that's length of its context window.

616
00:52:24,320 --> 00:52:41,320
So it's in that sense fairly limited, even though it's this very smart, like GPT three or four system, whereas the verifier can see thousands of examples and so you can get much better tests of statistical significance.

617
00:52:41,320 --> 00:52:43,320
How am I doing on time.

618
00:52:43,320 --> 00:52:46,320
I think we have about five minutes left.

619
00:52:46,320 --> 00:52:48,320
Okay, cool.

620
00:52:48,320 --> 00:52:54,320
So maybe I'll just say, you know, you can use this for a bunch of things. So for describing distribution shifts.

621
00:52:54,320 --> 00:53:03,320
There's these two data sets MNLI and SNLI where SNLI is often used as like an OD version of MNLI.

622
00:53:03,320 --> 00:53:09,320
Here's four samples to which are from SNLI and to which are from MNLI.

623
00:53:09,320 --> 00:53:14,320
It's not immediately obvious what distinguishes them.

624
00:53:14,320 --> 00:53:28,320
But if I say that SNLI describes a picture, then it's very clear that the green ones are SNLI because it says the church choir sings to the masses and old man with the packages poses in front of an advertisement.

625
00:53:28,320 --> 00:53:31,320
And the other two are not about pictures.

626
00:53:31,320 --> 00:53:36,320
So you kind of immediately see what the distribution shift is here.

627
00:53:36,320 --> 00:53:44,320
There's two paraphrase data sets, Twitter, PPTV and QQP, which stands for core question pairs.

628
00:53:44,320 --> 00:53:48,320
It says Twitter talks about a news story and core contains a question.

629
00:53:48,320 --> 00:53:55,320
These are just kind of sanity checks like these would be kind of totally obvious to anyone who was familiar with these data sets.

630
00:53:55,320 --> 00:53:57,320
But you can do more interesting things.

631
00:53:57,320 --> 00:54:04,320
So this was one that I think to our knowledge was novel at the time we discovered it detecting spurious cues in a data set.

632
00:54:04,320 --> 00:54:10,320
So we handed it this data set called SUBJ, which is a data set for subjectivity analysis.

633
00:54:10,320 --> 00:54:13,320
And it said the objective class was a plot summary of a film.

634
00:54:13,320 --> 00:54:23,320
The subjective class is a quote from a film review, which seems like it should be wrong for a data set that's about subjectivity analysis.

635
00:54:23,320 --> 00:54:28,320
But if you actually go back and read the paper, it says,

636
00:54:28,320 --> 00:54:34,320
to gather subjective sentences, we collected 5000 movie reviews snippets from Rotten Tomatoes to obtain mostly objective data.

637
00:54:34,320 --> 00:54:38,320
We took 5000 sentences from plot summaries available from IMDB.

638
00:54:38,320 --> 00:54:47,320
So actually, if you did well in this data set, you were basically learning this rather than stuff about subjectivity.

639
00:54:47,320 --> 00:54:54,320
There's like other shortcuts we found somewhere new somewhere old, but you can sort of find all these various cues.

640
00:54:54,320 --> 00:54:57,320
You can use this for error analysis and so on.

641
00:54:57,320 --> 00:55:02,320
So maybe just to summarize, right, we have these four steps of this pipeline.

642
00:55:02,320 --> 00:55:08,320
Initial data was just the two text distributions, generate the hypothesis by prompting GPT-3,

643
00:55:08,320 --> 00:55:16,320
formalize the hypothesis by measuring the success rate, and then you kind of test on new held out samples.

644
00:55:16,320 --> 00:55:26,320
The final thing I'll just leave up is we have ongoing work that is kind of taking this far beyond classification to just sort of like generally using natural language

645
00:55:26,320 --> 00:55:31,320
predicates as features in statistical models.

646
00:55:31,320 --> 00:55:41,320
So this example here is trying to describe temporal drift in news headlines from the Australian Broadcasting Company,

647
00:55:41,320 --> 00:55:48,320
and it kind of identifies these five features that kind of vary.

648
00:55:48,320 --> 00:55:54,320
And you could think of as maybe the top five principal components explain the variation in this data set.

649
00:55:54,320 --> 00:55:58,320
Although the percent of variance explained is actually still pretty low here.

650
00:55:58,320 --> 00:56:05,320
So I think you should think of this as just like a initial result.

651
00:56:05,320 --> 00:56:07,320
So maybe I'll end there and take questions.

652
00:56:07,320 --> 00:56:26,320
So for the proposal, there can be just like do prompt optimization like overtime,

653
00:56:26,320 --> 00:56:30,320
you keep selecting like an example and then see where it fails and then show it again.

654
00:56:30,320 --> 00:56:36,320
And then after a while you have like this good hypothesis and that you can somehow guarantee that is correct

655
00:56:36,320 --> 00:56:38,320
in a set of checking things.

656
00:56:38,320 --> 00:56:40,320
So you could.

657
00:56:40,320 --> 00:56:44,320
So is the idea like basically like fine tune the proposal to get better and better?

658
00:56:44,320 --> 00:56:46,320
Yeah, fine tune the prompt.

659
00:56:46,320 --> 00:56:48,320
You do that prompt optimization.

660
00:56:48,320 --> 00:56:50,320
Oh, prompt optimization.

661
00:56:50,320 --> 00:56:59,320
So I think so my general sense is if you just use the proposal,

662
00:56:59,320 --> 00:57:02,320
I mean, we didn't try to do like the full prompt optimization,

663
00:57:02,320 --> 00:57:09,320
but we do do ablations of like just using the proposal and that generally does a lot worse.

664
00:57:09,320 --> 00:57:12,320
I think that there's a couple issues.

665
00:57:12,320 --> 00:57:16,320
What one is just that the proposal gets much less data than the verifier.

666
00:57:16,320 --> 00:57:20,320
So, you know, even like if you do prompt optimization.

667
00:57:20,320 --> 00:57:27,320
Oh, no, what's your I thought that your proposal like get like a few number of data.

668
00:57:27,320 --> 00:57:29,320
That's why you should do the checking.

669
00:57:29,320 --> 00:57:30,320
Yes.

670
00:57:30,320 --> 00:57:35,320
And then I was just saying that can you make the proposal more strong by just to prompt.

671
00:57:35,320 --> 00:57:41,320
I see everything and then you can guarantee that these hypothesis is correct.

672
00:57:41,320 --> 00:57:44,320
Yeah, so that's that's an interesting idea.

673
00:57:44,320 --> 00:57:51,320
So you're basically saying, okay, like do prompt optimization to find a prompt that gets this feel like we discussed this.

674
00:57:51,320 --> 00:57:55,320
What was your claim is that you don't get semantically.

675
00:57:55,320 --> 00:58:03,320
The point that you do gradient is gradient is that you find the prompts and you are you very easily get like a readable prompt that is not natural.

676
00:58:03,320 --> 00:58:06,320
I would stream because what you typically find with adversarial examples.

677
00:58:06,320 --> 00:58:08,320
They are relatively possible.

678
00:58:08,320 --> 00:58:09,320
Yeah.

679
00:58:09,320 --> 00:58:11,320
So I think the issue is at least right now.

680
00:58:11,320 --> 00:58:16,320
If you do this, it's hard to get kind of like natural language out right now.

681
00:58:16,320 --> 00:58:27,320
Right now you asked you for to give the gradients but

682
00:58:27,320 --> 00:58:30,320
Thank you for this amazing talk.

683
00:58:30,320 --> 00:58:38,320
The quick question maybe this is not so you're asking you're asking for hypotheses that separate the two data set.

684
00:58:38,320 --> 00:58:44,320
And when you were testing them, they were like, okay, let me pick two examples and say one is more positive than the other.

685
00:58:44,320 --> 00:58:56,320
But could you tweak this to possibly look for hypotheses that certainly answer yes in one case and no in the other case that you don't have to look at two comparative examples.

686
00:58:56,320 --> 00:58:59,320
Like, okay, this one talks of a female character.

687
00:58:59,320 --> 00:59:02,320
The other one talks of a male character.

688
00:59:02,320 --> 00:59:07,320
So what would be just so I can understand like what would be an example.

689
00:59:07,320 --> 00:59:16,320
So as an example, the second data, the second example you showed was that this statement talks of two female characters.

690
00:59:16,320 --> 00:59:22,320
That's a statement you can answer in yes or no without looking at two different samples from the two data sets.

691
00:59:22,320 --> 00:59:23,320
Yeah.

692
00:59:23,320 --> 00:59:32,320
So could you tweak this to possibly just look for hypotheses that could be answered on a single data point rather than a comparison between data points from different data sets.

693
00:59:32,320 --> 00:59:41,320
Yeah, so we actually, I think, worked with both versions of the system, one that is kind of unary predicates and one that's binary predicates.

694
00:59:41,320 --> 00:59:47,320
I think in practice, a lot of the interesting things you want out are kind of binary predicates.

695
00:59:47,320 --> 00:59:57,320
So you can get like you can get somewhere with this unary thing, but you're kind of losing something if you don't consider comparatives.

696
00:59:57,320 --> 00:59:59,320
Okay, let's thank the speaker.

