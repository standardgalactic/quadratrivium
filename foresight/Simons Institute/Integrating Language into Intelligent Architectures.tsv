start	end	text
0	1920	I mentioned Gabe Grand, another student collaborator
1920	4560	who is one of Jacob and Dreyas's students,
4560	9120	and Tanja Shren, a student also co-advised
9120	13040	by Josh and Vikash, as well as Noah Goodman,
13040	15280	my advisor Vikash and Jacob and Dreyas.
17640	19480	So our broader goal today in this talk
19480	21480	is actually going to be to reflect
21480	23080	based on many of the recent advances
23080	25360	that we all know of modeling natural language,
25360	26920	but I think also drawing on evidence
26920	29720	from toolkits, other toolkits in AI.
30720	33040	Evidence from cognitive science and from neuroscience
33040	35680	on kind of the broader spectrum of different ways
35680	38360	that we might think about building intelligent architectures
38360	41320	that use and produce and learn from language,
41320	43960	as well as more generally, I think what role language
43960	46880	might play or could play in a computational system
46880	48320	that we say thinks.
48320	50360	And obviously this is a question that many people
50360	52160	in this room, but many other people who probably
52160	53680	aren't in this room have thought about
53680	57280	from philosophers of language to linguists and neuroscientists.
57280	59640	And we thought it actually might be a useful exercise
59640	62760	to kind of start by reflecting on the underlying answers
62760	65320	to this question that are or aren't suggested
65320	66560	by some of the most prominent directions
66560	69520	that we're taking in AI research right now.
69520	70800	And of course, one of the reasons
70800	72920	why we're even asking this question at this scale,
72920	75000	what is the role of language in intelligence
75000	77960	is in large part driven by this remarkable observation
77960	80480	that we all know about from just a few years ago,
80480	83080	which is if you train these large
83080	86240	and specifically transformer based neural architectures
86240	87960	as language models, just to do this
87960	92200	next word prediction task, with enough language data,
92200	94480	they start to show behaviors that really suggest
94480	96280	that there's something more than language at play.
96280	97880	They look like they're thinking.
98800	101880	They can induce patterns from just a few examples in data,
101880	103560	or they can even read the definitions
103560	106480	of totally novel words like Zaka Tota in context
106480	108720	and produce realistic sentences that appear
108720	112080	as if they understand how to use those words immediately.
112080	114360	And so a lot of the excitement, I think it's fair to say
114360	117080	around language models is that maybe for the first time,
117080	119880	we're seeing something that is offering a scalable route
119880	123840	towards implementing more generally intelligent architectures
123840	127200	just by directly scaling, or largely by directly scaling
127200	128160	the amount of language data
128160	129960	that they're being trained to predict.
131640	133280	All right, so what is the underlying idea here?
133280	135200	What does this have to say about language?
135200	136240	Well, I think it's fair to say
136240	138880	that one of the dominant hypotheses that's underlying
138880	141960	why we were even starting to see some of this behavior
141960	143840	rests on kind of a two-part idea.
143840	147120	One is something about the nature of language itself, right?
147120	151280	It's the suggestion that language is sufficiently diverse
151280	154360	and so broad, maybe because we suspect that humans
154360	155760	express so much of their thoughts
155760	158360	in such a diverse range of their thoughts and language
158360	161520	that being able to perfectly solve this task,
161520	164680	to be a perfect language model, or at least a really good one,
164680	168440	is essentially an AGI complete task,
168440	170400	and maybe also one that conveniently,
170400	171560	unlike other kinds of tasks,
171560	173320	like predicting all of the videos in the world,
174000	175800	we have maybe efficient architectures to do
175800	177200	and enough data to do.
177200	180360	And I think it's worth noting that this doesn't actually
180360	183200	really have to be a particularly strong hypothesis
183200	186880	about what it is computationally that thinking looks like,
186880	190520	or even how language necessarily is implicated internally
190520	193920	inside the computational processes that we call thinking.
193920	195360	Rather, it's really just a hypothesis
195360	198360	about the nature of the language modeling task itself.
200280	202200	And I think what a lot of people
202400	205640	in this room would agree now is that,
205640	208240	well, that might be true about the language modeling task
208240	211280	in principle, much of the most exciting research
211280	212640	that we're seeing using LMS right now
212640	215920	actually isn't predicated on really the hope or the idea
215920	217960	that just by scaling to more and more data,
217960	220760	we're gonna expect transformers trained and used in this way
220760	222200	to actually solve that task
222200	224560	or become perfect next-world prediction models.
224560	226520	And the intuition behind that,
226520	228400	which I think many people have pointed out,
228400	231600	comes both from the way in which we're looking
231960	233800	in which transformers work, right?
233800	235320	There are autoregressive language models
235320	237520	that are doing a fixed, finite amount
237520	239720	of internal computation that only scales
239720	241520	based on the previous linguistic context.
241520	243360	And it doesn't really make sense that,
243360	245960	of course, you can pose questions in language
245960	248240	like arbitrarily difficult math problems
248240	251440	or planning problems that intuitively require
251440	252360	an amount of computation,
252360	253840	whose complexity doesn't actually depend
253840	256720	linearly on the previous token context.
256720	258680	And I think that's been matched empirically
258680	260800	by lots of different observations
260800	263480	about which kinds of sentences are hard to complete
263480	265160	if you treat them as next-world prediction tasks
265160	266160	in this way, right?
266160	267360	Hard arbitrary math problems
267360	268960	or planning problems like these.
269960	273640	Right, so some of, you know,
273640	276400	if you're thinking about the role of language,
276400	279720	why is language, why are language models so big right now
279720	282640	as, oh, well, language just has all the evidence necessary
282640	286280	to train something to think.
288080	290000	Like Leo said, that's not actually committing
290000	292320	to any hypothesis about sort of how language
292320	293520	is used internally in thinking.
293520	296080	The transformers' computations are not necessarily
296080	299040	doing anything linguistic as it's computing
299040	300680	the distribution of the next word.
300680	304440	But more recently, we've seen people use language models
304440	307640	to solve these harder tasks by letting them think more.
307640	309400	And what letting a transformer think more means
309400	311800	is letting it generate more tokens.
311800	315880	So in this view, thinking doesn't necessarily emerge
315880	317880	just as predicting the next token.
318880	322920	Rather, thinking happens in language, right?
322920	327840	It happens by sort of producing a chain of thoughts
327840	330960	or using a scratch pad or deciding
330960	332640	that you're going to invoke a calculator
332640	334640	or write some code and execute the code.
335640	340280	And the sort of hypothesis embodied,
340280	342000	I think by this line of work,
342000	343160	is that the role of language
343160	345880	and intelligence architecture is bigger.
345880	348760	That language or some kind of like running monologue
348760	351600	of language is the central controller of thought.
351600	354760	And thinking is taking place primarily in language.
357760	361840	And what's maybe striking about these proposals
361840	363840	is that if you ask most cognitive scientists
363840	365680	or neuroscientists, they'd probably say,
365680	367520	this isn't the role that language plays
367520	369320	in relation to general intelligence in humans,
369320	371680	or at least it's not the dominant hypothesis.
372760	373840	Until just a few years ago,
373840	374920	this probably wouldn't have been the role
374920	376000	that many AI researchers
376000	377880	would have necessarily posited for language
377880	382160	as the main controller or the main substrate
382160	385240	of a running monologue that controls all thinking.
385240	387320	And so we thought we'd review some of the background
387320	389520	for what language seems to look like in humans
389520	392000	as a basis for informing how we might build architectures
392000	394360	that better capture those more human-like roles
394360	395200	for language.
396840	398080	Yeah, and so I just want to be clear
398080	399760	that we're going to do kind of a quick background
399760	402000	on some of the neuroscientific and cognitive evidence
402000	404120	about what language might look like in people.
404120	407080	And our goal really isn't to push back in any way
407080	408840	against these other kinds of paradigms
408840	410320	for where language might fit in.
410320	412240	It's rather to present at a mode
412240	413800	where we're really thinking a lot
413800	415440	about scaling up language models
415440	417960	as one really dominant role,
417960	420200	what our other roots might be
420200	423480	because it doesn't seem like that that really matches
423480	425760	a different prominent intelligent architecture
425760	427400	that's sitting here in the room today.
427400	429280	So one source of evidence for the role
429280	432080	that language appears to play in human cognition
432080	434760	comes from neuroimaging data, fMRI data,
434760	438160	suggesting how language is processed in the human brain.
438160	442280	And at this point, convergent imaging data from many people,
442280	444120	including Ed Fedorenko at MIT,
444120	446120	Standa Hain and recent graduate students
446120	449640	in our department, Kyle Malewald and Anya Ivanova,
449640	450920	suggests that human brains
450920	453440	have this language-specific network
453440	455600	that handles many of the tasks that we associate with language.
455600	457000	It's activated when people do tasks
457000	459560	like listening to sentences or reading them
459560	461440	or speaking or writing words.
462280	464560	And this is not just an English-specific network.
464560	466240	The same general region is activated
466240	468720	no matter what language someone is speaking.
468720	471640	It even fires when they're producing constructed languages
471640	473360	for people who come to learn and become fluent
473360	475320	in languages like Dothraki or Klingon.
476960	479480	And right, what is it that this language network does?
479480	481800	Well, increasingly, one dominant hypothesis
481800	483480	is that it actually does do something
483480	485120	like next-word prediction.
485120	488600	And in fact, among many other kinds of alternative models,
488600	490880	like people were earlier,
490880	492480	for a long time I've been trying to do things
492480	494560	like align words back to the brain,
494560	496760	transformer architectures really do seem to be
496760	498760	among the best models that we have right now
498760	501280	of the neural activity of this language network,
501280	502640	specifically when they're trained
502640	503960	on next-word prediction tasks
503960	506600	and not other linguistic tasks like NOI.
508000	510600	But I think what we see in this neuroimaging evidence
510600	512400	also highlights the ways in which the role
512400	514360	of this human language network
514360	516440	probably is not as the controller
516440	518360	or the central seat of cognition.
518360	519920	It's selective for language
519920	521720	and it isn't involved in many other kinds
521720	523440	of thinking activities,
523440	525840	even ones that might seem to involve symbols
525840	527320	like solving math problems
527320	529680	or reasoning about logic and physics and social reasoning.
529680	533160	Those invoke other regions of the brain
533160	535360	and language interfaces modulary with them.
535360	537320	It can interface very generally with them,
537320	538160	but it doesn't seem to be
538160	540360	where the bulk of thinking is taking place.
541600	542440	Oh, yes.
542440	543440	Oh yeah, I just had a question
543440	545040	about the next-word prediction findings.
545040	548840	So did you also try to say like math word prediction
548960	551440	and this is actually worse than next-word?
551440	553440	Yeah, so I think they do compare
553440	555360	or like closed tasks are one of the alternates
555360	556200	that they're looking for.
556200	558440	And also, yeah, people should feel free to just shout out
558440	560720	as we've been doing all along.
560720	564880	So also, right, actually you can see adult patients
564880	568120	that suffer strokes that only damage this area of their brain
568120	571720	and we find that they can still think about,
571720	573800	well, they can't comprehend spoken language,
573800	575800	but they can still think about all these different kinds
575800	577320	of tasks fronted in different mediums,
577320	578920	like they can draw physical inferences
578920	580760	from videos that they're watching.
580760	583400	And conversely, maybe most tantalizingly,
583400	586720	it's actually also possible to sustain localized damage
586720	589360	that leaves you still able to produce these long,
589360	590600	very relatively fluent
590600	593080	and quite syntactically coherent sentences,
593080	595400	almost as if they're maybe just like a very rudimentary
595400	598080	and local word-based language model,
598080	599880	while not really conditioning meaningfully
599880	601920	on what someone else is saying.
601920	603880	So they say, do you like it here in Kansas City?
603880	606080	And this person says, yes, I am.
607800	609120	Or really producing sentences
609120	610720	that are obviously more globally meaningful
610720	612080	or goal directed towards the question.
612080	613880	And I don't want us to overindex on those results,
613880	617000	but they kind of fit in with this more coherent picture.
617000	620120	And in many ways, I think this kind of recent evidence
620120	622800	from neuroscience actually supports the broader picture
622800	625240	of human cognition and the place of language in it
625240	626720	that most cognitive scientists
626720	628800	and many linguists have already believed
628800	630880	based on what we see and have come to learn
630880	632800	through developmental science,
632800	634200	studying how kids think
634200	636320	and where language seems to fit into that picture, right?
636320	638560	So a broad body of work at this point
638560	641560	suggests that infants, well before they learn language
641560	643320	of any kind or are speaking,
643320	645880	already independently perform many of the tasks
645880	648000	I think we associate with coherent thinking,
648000	650120	from reasoning about physics to planning
650120	653120	to drawing causal and probabilistic inferences.
653120	655920	And language seems to be something that humans learn
655920	659080	and scaffold on top of this structured basis for thinking.
659080	660960	We take much less input data
660960	662320	to learn to produce fluent language
662320	663680	than a large language model.
663680	666160	And humans that actually aren't exposed
666160	668040	to any language input at all.
668040	671000	So famously there are these deaf children in Nicaragua
671000	673440	who grow up in isolated hearing families
673440	676840	actually spontaneously come to produce early sign languages
676840	678760	as a product of trying to communicate events
678760	680640	that bear many of the hallmarks of the syntax
680640	683480	that we associate of our own natural languages,
683480	686200	like distinguishing between the subject who's punching
686200	688440	and the person who's getting punched.
688440	690000	Suggesting that in many ways
690000	692640	the language we produce somehow externalizes
692640	696040	the underlying structure of the thought that we already have.
696080	698280	And of course, there are many other animals
698280	700680	that we associate as being intelligent in some way
700680	702960	and that have been modeled using the models
702960	706400	that we associate with probabilistic reasoning and planning
706400	707720	that don't use language at all.
707720	711520	So it doesn't feel like language is by any means necessary
711520	713040	for what we think of as thought.
714560	718160	So formalizing this picture of an intelligent system,
718160	721240	one that's shared across animals and non-linguistic infants
721240	723880	and maybe captures some of the computations involved
723880	725520	in many of those thinking tasks
725520	730440	that don't involve the language network in our brains
730440	732840	is both the central goal of a lot of cognitive science
732840	736120	and has been a historic motivation
736120	738360	for diverse fields within AI
738360	741480	before the current sort of LLM centric moment.
741480	745280	So if you open up Russell and Norvig's textbook,
745280	748080	AI a modern approach, you'll see an equation like this
748080	750960	that's supposed to sort of capture what we believe
750960	752640	about how intelligence works.
754440	757200	Computationally or a computational model of intelligence.
757200	759200	Where the idea is that an intelligent agent
759200	762720	is one that has sort of structured internal world models
762720	766280	that can be updated based on observations of the world
766280	769480	and in which we can sort of predict the results
769480	773520	of our actions that the agent has some sort of values
773520	777400	or desires that are captured in some kind of utility function
777400	779480	that the agent can do probabilistic reasoning
779480	783160	over its observations and the possible actions it might take
783480	785120	and what their expected utilities are
785120	786680	and that it can do some kind of planning
786680	788760	to optimize the value of the back end.
788760	789600	Yeah.
789600	792320	Is there a normative statement or descriptive statement?
792320	795320	Is it the case that we are defining intelligent agent
795320	797200	to be having this kind of property
797200	800000	or it's like if we want to do intelligent architecture,
800000	801360	we should have these?
801360	803040	Yeah, so I think some, it's a great question.
803040	804960	I think some of the work definitely is coming at it
804960	806160	from a normative perspective.
806160	808920	This is like a definition of what rational actions
808920	810080	might look like, right?
810080	811240	But I think there is a lot of work
811560	812960	computational cognitive science
812960	814880	that sort of in various domains
814880	816000	has collected a lot of evidence
816000	819400	that people either like recognize this as normative
819400	822920	or behave like this in computation,
822920	825720	according to the limits of what they can do computationally.
828240	833240	So in this architecture, the controller for thinking
833320	834600	would be some kind of system
834600	836240	that's capable of general world modeling
836240	837800	and probabilistic reasoning
837800	839800	or planning and utility maximization
839840	844120	rather than sort of a primarily linguistic system necessarily.
845840	847720	Now largely attempts in artificial intelligence
847720	849360	to directly implement this equation
849360	851920	by building out components that sort of map on
851920	855520	to the different elements of this equation
855520	858200	have struggled to match the computational efficiency
858200	860640	and the generality of natural intelligence.
860640	863200	But we have seen over the past couple of decades or so
863200	864680	the emergence of this new class of tools
864680	866480	called probabilistic programming languages
866480	867320	for building software
867320	869480	that can solve probabilistic reasoning tasks
869480	871760	at least in limited domains.
871760	875280	And sorry, and these languages have been applied
875280	877280	to create systems that do everything
877280	879720	from perceiving cluttered 3D scenes
879720	881080	more accurately sometimes
881080	883520	than object detector neural nets
883520	886400	to interpreting and predicting economic trends
886400	888360	more accurately than leading industry solutions
888360	889760	like Facebook neural profit.
890680	892480	And these probabilistic programming systems
892480	893680	are enabling these applications
893680	895200	with two key technical features.
895200	897000	They feature modeling languages
897000	899960	that let users express complicated probabilistic models
899960	901440	of the world as programs
901440	904240	making it easy to write down rich probabilistic models
904240	906040	that are defined in terms of expressive
906040	907760	program-like components.
907760	910160	So for example, the probabilistic models behind
910160	911560	these example applications
911560	914120	are defined in terms of 3D renderers, symbolic planners,
914120	916440	scientific simulators and so on.
916440	918440	And the second thing that probabilistic programming systems do
918440	921000	is that they automate various mathematical operations
921000	922120	on these models
922120	923960	and that automation makes it possible for users
923960	925880	to concisely implement sophisticated algorithms
925880	927200	for probabilistic reasoning,
927200	928880	such as the variety of Monte Carlo
928880	930160	and variational inference algorithms
930160	932000	that power these example applications.
932000	933360	One way of thinking about these tools
933360	938160	is as kind of like PyTorch or TensorFlow,
938160	940760	but instead of writing differentiable models
940760	941720	and doing optimization,
941720	942960	you're writing probabilistic models
942960	945200	and doing various probabilistic reasoning tasks.
946760	948840	So far, these AI engineering efforts
948840	950000	haven't really made contact
950000	954080	with the sort of language model part of AI.
955000	956760	Much like there hasn't yet been a concerted effort
956760	959080	to take this classical picture of intelligence architecture
959080	960400	and figure out how language might be
960400	961520	richly integrated into it.
961520	963280	So for the rest of the talk today,
963280	964920	we're gonna explore two different approaches
964920	966360	for thinking about where language
966360	969000	might fit into this picture and approach.
969000	971160	So probably Leo's gonna begin
971160	972800	by talking about how an intelligent agent
972800	976120	might incorporate lots of externally produced languages,
976120	979200	explanations, observations, questions,
979200	982160	how an agent can incorporate all of those forms of language
982200	984560	into the way that it updates its beliefs
984560	986240	and decides how to act in the world.
986240	987720	Then I'll talk a bit about systems
987720	990120	that leverage language as a tool for thinking
990120	991960	within its models of the world
991960	993840	or its probabilistic reasoning algorithms.
993840	995000	And each of these corresponds
995000	997680	to very recent preprints of work.
997680	999400	So I also wanna give a disclaimer.
999400	1002440	We should really emphasize that both are preliminary proposals
1002440	1004800	and we're giving a very speculative talk,
1004800	1006800	not scaled architectural solutions here.
1008320	1009160	Cool, right.
1009160	1010800	So the first person that's portion of this talk
1010800	1011640	is summarizing work.
1011640	1012640	If you wanna read more,
1012640	1014360	you can find this very long paper
1014360	1016320	from word models to world models.
1016320	1017160	That's up on archive
1017160	1019120	and this is primarily done with another student
1019120	1021320	who's not here today, Gabe Grant.
1021320	1023280	And as I just mentioned, the context for this work
1023280	1025200	is thinking about how we can build systems
1025200	1027620	that capture the breadth with which all the external language
1027620	1030920	we hear seems to inform at least our human thinking.
1030920	1031920	And what's clear, right,
1031920	1033680	is that this role seems to be very broad.
1033680	1035880	If we take the basic model of an agent
1035880	1037200	with beliefs and goals,
1037200	1039480	well, it seems like there's an incredibly diverse range
1039480	1041960	of situations in which we can update our beliefs
1041960	1043560	about a situation from observations
1043560	1045160	that we express in language,
1045160	1046640	or in which the goals of our thought
1046640	1049480	are to answer questions that we specify linguistically.
1049480	1052280	And these might implicate our knowledge of other agents
1052280	1053760	drawing on our intuitive psychology
1053760	1056240	to reason about what they think and what they'll do.
1056240	1058640	Or we can talk about the physical world around us,
1058640	1060280	what we perceive and ask questions
1060280	1061760	that require us to reason about
1061760	1064080	what we see and draw on our physical intuitions.
1065160	1066520	And of course, one of the remarkable things
1066520	1068280	where we're also excited about language
1068280	1070400	is that it doesn't just draw on what we already know,
1070400	1072360	it's this means by which humans seem to learn
1072360	1074600	and pass on profoundly new knowledge,
1074600	1078640	whether that's new concepts that we define in words
1078640	1081360	or learn from words to really profound new theories
1081360	1082960	and conceptual systems, right?
1082960	1084400	Much of what we know about the world,
1084400	1086960	the fact that there are wars, what wars are,
1086960	1088720	legal systems, sciences,
1088720	1091240	comes from information that it feels that we acquire
1091240	1092880	from language in some way.
1093840	1095320	But how do we do that?
1095320	1097720	Well, I think one longstanding lens
1097720	1099720	for thinking about language
1099720	1103520	that's kind of persisted before the LLM-based moment
1103520	1105400	is that what language is,
1105400	1107280	is this external symbolic medium
1107280	1109360	for communicating human thoughts.
1109360	1110600	And the way that it does that
1110600	1113360	is because there's some kind of general mapping function
1113360	1115680	from our internal representations of thought
1115680	1118720	into this external symbol system, that's language.
1118720	1121360	And so in this kind of older framework,
1121360	1124400	what it means to understand language or make meaning
1124400	1127520	means mapping back from external sentences that we hear
1127520	1130440	into structured internal representations.
1130440	1132640	And what we explore in this paper
1132640	1136880	is a general proposal that casts the meanings of language
1136880	1139880	as these mappings or probabilistic distributions
1139880	1142960	over expressions in a probabilistic programming language.
1142960	1145200	And I'm gonna come back after some concrete examples
1145200	1147720	to how I think this relates to the conceptual rule semantics
1147720	1149480	that Steve articulated in the first talk,
1149480	1150320	because I think there are actually
1150320	1151600	some really deep connections here,
1151600	1154560	the ways in which this might be one way to formalize
1154560	1156160	or enrich some of those ideas.
1157120	1159680	And this architecture, this proposal here
1159680	1161720	also suggests how language can be integrated
1161720	1163480	into a more general architecture,
1163480	1166080	because it's one that already starts out with,
1166080	1167280	as Alex mentioned,
1167280	1169480	some kind of existing internal modeling language,
1169480	1172440	whose goal is to represent the world probabilistically,
1172440	1174560	query those models and specify what it means
1174560	1176880	to draw coherent inferences over them.
1176880	1178880	And it also suggests, I think, a framework
1178880	1180920	for formally modeling the content
1180920	1183040	of different kinds of sentences in language
1183040	1184960	as the kinds of probabilistic programming expressions
1185040	1186320	that they might map into.
1188320	1192600	So, and so in this paper,
1192600	1194640	we begin by exploring how this proposal
1194640	1196920	might be instantiated with respect to
1196920	1199200	a bunch of different domains of reasoning,
1200560	1203520	sorry, including general probabilistic reasoning,
1203520	1205160	but also reasoning about relations
1205160	1207360	or physics and social situations.
1207360	1210120	And in all of these, we're gonna propose
1210120	1211280	that we might think about the language
1211280	1213360	that communicates general conceptual knowledge
1213360	1216520	about the world definitions or causal knowledge
1216520	1218880	as constructing these probabilistic expressions
1218880	1222160	that are those that build up probabilistic generative models.
1222160	1225960	And then in this framework, observations in the language,
1225960	1229160	like there is at least one red mug in the scene
1229160	1231680	or Charlie is Dana's grandfather,
1231680	1234120	construct formal conditioning statements,
1234120	1236920	which update the state of this probabilistic model.
1236920	1239480	And then questions map into query expressions
1239480	1242320	that specify the formal target of probabilistic inference
1242320	1243720	with respect to a model.
1244880	1246120	And in this framework,
1246120	1249760	we're thinking essentially cast as probabilistic reasoning.
1249760	1252120	We suggest that another way that we can think about
1252120	1253720	the role of a language model,
1253720	1255160	one that's much smaller,
1255160	1256800	like the language network in the brain
1256800	1258800	is actually as a means of instantiating
1258800	1260480	this meaning function in a way that
1260480	1262680	we've really never had before
1262680	1265240	to protect these kinds of context specific
1265240	1267440	and previous discourse conditioned mappings
1267440	1269640	from sentences in natural language
1269640	1271520	into distributions over expressions
1271520	1274120	that convey meaning in a probabilistic programming language.
1276040	1279320	And so as a part of this long running disclaimer,
1279320	1280880	what I'm gonna be showing is a really minimal
1280880	1282880	implementation of this framework,
1282880	1285240	but really intended as a pointer to different directions
1285240	1286800	with which we might scale this approach
1286800	1288880	to implement a more general interface between language
1288880	1291000	and arrange a different core cognitive domains.
1291000	1292120	So just to be clear,
1292120	1294760	concretely in the examples that you see next,
1294760	1297240	our meaning function is gonna be implemented using codex,
1297240	1300240	right, an open AM model much smaller
1300240	1301520	than the state of the art right now
1301520	1302960	that's trained to learn joint distributions
1302960	1304440	over language and code.
1304440	1306080	And the probabilistic programming language
1306080	1307480	we're gonna show is church,
1307480	1310920	which is this very simple probabilistic programming language
1310920	1312440	that supports kind of very general
1312440	1314240	sample based inference procedures.
1314240	1316360	And our goal is to demonstrate how this framework
1316360	1318200	might broadly interface between language
1318200	1320520	and a bunch of different core cognitive domains.
1321640	1323840	So first to illustrate the basic sense
1323840	1326280	in which a proposal like this might allow language
1326280	1329440	to update an agent's beliefs and query a world model,
1329440	1331480	I'm gonna begin with a really simple toy example
1331480	1333160	that actually draws on a bunch of prior
1333160	1335160	cognitive science experiments
1335160	1338680	in which real people were asked to draw various inferences
1338680	1341160	about which teams of players might win different games
1341160	1342840	of tug of war based on the games
1342840	1345360	that you'd previously seen players win or lose.
1345360	1347920	And so this is older work from Josh's group
1347920	1350560	that demonstrated I think the sense in which
1350560	1352640	this normative model, this Norvig model
1352640	1354960	of probabilistic inference actually in many ways
1354960	1357720	predicts the actual behaviors and predictions made by humans
1358560	1360560	using a very general probabilistic model
1360560	1363120	of the mechanics of this tug of war game.
1363120	1365840	And our goal here is to show how our framework
1365840	1368280	we can implement an interface between natural language
1368280	1371880	and all of the core examples of this older experiment.
1371880	1374400	So just to go through here, I think what you're seeing
1374400	1376600	in this little toy example, the world model
1376600	1378640	that's being defined on the screen
1378640	1380560	is capturing the basic causal relationship
1380560	1384720	by which properties of different human players
1384720	1386600	might influence the outcomes of different tournaments
1386640	1388400	that they play in tug of war.
1388400	1392440	So for instance, here we're modeling players
1392440	1396360	as having some internal inherent strength value
1396360	1398360	where strength varies approximately normally
1398360	1400200	but as this unobserved latent variable
1400200	1401880	over different kinds of players.
1401880	1404200	And we also think of players as having
1404200	1406600	some kind of internal laziness value
1406600	1408400	which represents the percentage of the time
1408400	1409440	that they actually don't act
1409440	1412000	according to their underlying strength.
1412000	1415000	And how did these variables determine the outcomes
1415000	1417920	that we observe of given games of tug of war?
1417920	1420280	Well, the strength of a whole team of players
1420280	1424080	depends on the cumulative sum of its player strengths.
1424080	1426280	But if a player is deciding to be lazy in this game,
1426280	1428040	they might not pull as hard as they could.
1428040	1429720	And whichever team pulls with the most strength
1429720	1432560	in a given match is going to win that match, yeah.
1432560	1435080	Did you design the primitives of strength and laziness
1435080	1437400	or a codex come up with the primitives themselves?
1437400	1439680	So in this one, we're looking at a model
1439680	1440720	that's derived from the older work.
1440720	1443200	So these are designed, but yes, that's later in this work.
1443200	1444160	We're gonna show some examples
1444160	1445720	of how you can learn this kind of model
1445720	1447240	from someone just talking about it in language
1447240	1449720	like the definition that I just gave.
1449720	1450920	Right, and so again, you know,
1450920	1452200	this is a really simple example,
1452200	1454200	but I think also one that actually captures
1454200	1456520	a surprising amount of the basic causal knowledge
1456520	1458280	that people have if you tell them
1458280	1460240	that you're gonna be listening to tug of war games,
1460240	1461760	but sometimes people can be lazy
1461760	1463440	and not pull as hard as they could.
1463440	1466480	So how do we go about relating language in this domain?
1467680	1471200	Right, well, one means by which we can induce
1471200	1473120	a simple notion of a meaning function
1473120	1475440	that actually fits the definition we just gave
1475440	1477360	is by conditioning a language model
1477360	1480840	both on this context-specific generative world model
1480840	1484000	and on a few examples showing how language is mapped
1484000	1486560	into sampled probabilistic programming expressions
1486560	1487460	in this domain.
1488800	1490520	And what we've done now, right,
1490520	1494560	is effectively induce this kind of situation-specific
1494560	1497680	contextual mapping from arbitrary new sentences
1497680	1499000	to expressions that conditions
1499000	1502680	both on the general prior distribution that codex is
1502680	1504480	over language and code,
1504480	1507320	and this kind of specific discourse thinking context
1507320	1510720	of how language is being used in this situation.
1510720	1512680	And there are clearly other ways to do this,
1512680	1514080	some of which we'll talk about later,
1514080	1516640	but we're using this example to illustrate
1516640	1517660	just how much you might be able to do
1517660	1519540	with this kind of minimal implementation,
1519540	1520960	a notion of a model that translates
1520960	1522800	between language to code.
1522800	1526260	Right, so what kinds of language might we say here
1526260	1527520	and how might we think about them
1527520	1529640	in relation to probabilistic programming expressions?
1529640	1534080	Well, a general proposition, like Josh won against Leo,
1534080	1537000	gets translated into or might map,
1537000	1540440	we might think of mapping or meaning a conditioned statement,
1540440	1542880	an observation that Josh won against Leo.
1543740	1545520	If we make subsequent observations,
1545520	1549200	like then Josh went on to claim victory against Alex,
1549200	1551840	we can continue to kind of generally use this
1551840	1554640	meaning function that we've induced to turn that
1554640	1556400	into a probabilistic programming language
1556400	1558540	that captures the fact that Josh won against Alex.
1558580	1561660	If we then say that even working together as a team,
1561660	1563740	Leo and Alex still couldn't beat Josh
1563740	1565260	in this game of tech of war.
1566820	1569140	At this point, if we want to answer a query,
1569140	1572020	like, okay, wait, how strong is Josh?
1572940	1577020	What we think of as thinking in this situation
1577020	1580860	is actually sampling from the posterior
1580860	1582900	over possible worlds from the generative model
1582900	1585220	that we just defined, subject to the observations
1585220	1586340	that we've just made.
1586340	1589540	And indeed, that means that the meaning of a sentence
1589540	1591380	like how strong is Josh is really
1591380	1594420	a structured publicistic inference query.
1594420	1597780	What is the latent variable that is Josh's strength?
1597780	1600460	And what we see here is that given his track record,
1600460	1602660	all these people that he's beating, even playing together,
1602660	1604580	our inference is that Josh is likely
1604580	1606380	a good bit stronger than average.
1606380	1608140	And that also means coherently,
1608140	1610180	we might expect that a priori,
1610180	1612820	a new player we've never seen, like Gabe,
1612820	1614580	is going to be unlikely to beat him.
1615580	1619260	So if we ask what are the odds of Gabe at beating Josh,
1619260	1621300	we see that we think it's somewhat unlikely.
1624420	1625260	Question?
1625260	1626100	Oh, yes.
1626100	1628420	So on the how strong is Josh,
1628420	1630220	it seems like there's an interesting thing here
1630220	1631860	where there's also an implicit question
1631860	1635020	of what the word strong means in this context.
1636380	1640300	And that, like, right, it's like not a number,
1640300	1642580	it's kind of some like, comparative adjective.
1642580	1644580	It's like, probably a non-intersective.
1645580	1647780	So I guess, is that something you think about?
1647780	1649740	This framework, or should I just be kind of, like,
1649740	1651860	ignoring this sort of issue?
1651860	1653340	Yeah, well, okay.
1653340	1654740	So I think there's a number of ways
1654740	1655860	that we can think about that.
1655860	1660860	I mean, so, right, so the one sense we could say is like,
1661180	1662380	right, how strong is Josh?
1662380	1664980	Isn't, the answer to how strong of Josh isn't a number.
1664980	1667380	Rather, it's kind of this distribution
1667380	1669100	over this posterior distribution
1669100	1671700	of various underlying strength values
1671700	1674860	that we currently might infer that Josh has
1674860	1676940	with respect to the general value that we have.
1676940	1679660	I think another kind of popular definition
1679660	1682020	of various uncertain adjectives,
1682020	1683580	like a word like strong, right,
1683580	1686160	is that you have some internal threshold value,
1686160	1688020	or the person speaking has some kind of internal
1688020	1692340	threshold value that you must kind of jointly infer
1692340	1694860	with respect to the context in what you've seen.
1694860	1697540	And some of the examples that I'll actually give later,
1697540	1699860	so, right, there's kind of a long line of work
1699860	1701540	in linguistics, including some work
1701540	1703700	that treats that as like a pragmatic inference.
1703700	1705340	I think some of the interesting work
1705340	1707900	that we'll show a little bit later is that,
1707900	1709340	there are some ways in which you might think
1709340	1712820	of this mapping function as actually being a general one
1712820	1714860	that includes that notion of pragmatic inference.
1714860	1716940	And also, I think captures the sense
1716940	1718740	in which if you continually,
1718740	1721180	are you really doing this kind of pragmatic inference
1721180	1723020	all the time, or do you actually,
1723020	1724420	in many general settings,
1724420	1726460	like talking about the strengths of people,
1726460	1729340	actually have some kind of cashed older notion of strength
1729340	1730180	that you can draw in.
1730180	1732940	And I think actually, this notion of large language models
1732940	1735460	as just being this learned mapping function
1735460	1737580	from language into expressions include,
1737580	1739220	can also capture the sense in which
1739220	1741140	that knowledge is amortized away.
1741140	1742860	And you might not be having that inference.
1742860	1743700	Yeah, Chris.
1748500	1751700	She's sort of just denying or ignoring
1751700	1756420	what makes people so excited about large language models
1756460	1761460	in their meaning representation and ability to do inference.
1764540	1767100	I mean, because, okay, you've got sort of
1767100	1769100	cooler probabilistic programming language
1769100	1770860	on the right hand side,
1770860	1774580	but in some sense, the picture is still,
1774580	1779580	this is semantic parsing, like it was 2010 to 2015.
1779580	1784580	And yes, you're using a large language model,
1784780	1788300	but you're not actually using the excitement
1788300	1792020	of a large language model as a representation system.
1792020	1794540	Yeah, and I think, so probably each of us
1794540	1795700	would have different answers to this,
1795700	1797380	but part of what we're hoping to paint out
1797380	1799220	over the course of this talk is I think
1799220	1802100	some of the ways in which actually, right,
1802100	1804900	of course, no one wants to say we're gonna go back
1804900	1807020	to kind of the brittleness of semantic parsing,
1807020	1808780	but I think one thing that large language models
1808780	1811460	actually give us, or one proposal in this talk,
1811500	1815580	is that there are some aspects of the theory,
1815580	1817260	kind of the classic notion of linguistics,
1817260	1819420	and certainly the classic notions of semantic parsing
1819420	1820860	that actually normatively capture
1820860	1823580	a lot of what we really might want when we think about,
1823580	1828580	so one answer for an AI system is, well, yes,
1828580	1829700	in some way, we don't wanna throw,
1829700	1831100	certainly we don't wanna throw away
1831100	1832980	everything that we're learning from large language models,
1832980	1834340	and I think one answer to that
1834340	1837260	is kind of the answer that I gave to Jacob, right?
1837260	1839860	If we think about not always, you know,
1839860	1842980	in these examples, we're showing this very direct system
1842980	1844940	in which we always start with language
1844940	1846700	and we always map into some sort of
1846700	1847620	probabilistic programming expression,
1847620	1849540	and that's where all of the thinking happens,
1849540	1851980	and we might think, well, that doesn't totally make sense
1851980	1855100	because there are lots of cases where, as you're saying,
1855100	1856380	we have every reason to believe
1856380	1858380	that large language models have learned
1858380	1860020	a lot of latent information.
1860020	1861940	They can do a lot of, they certainly have
1861940	1863780	a lot of latent conceptual information,
1863780	1865580	and maybe to some degree, they can even perform
1865580	1868260	certain kinds of limited, amortized inferences,
1868260	1870780	or reuse old inferences that they've learned
1870780	1871740	from other people I've had,
1871740	1873420	and so in the second part of this talk,
1873420	1875220	we're going to show different ways in which,
1875220	1877500	well, this probabilistic programming language itself
1877500	1880340	doesn't necessarily need to be something that's isolated
1880340	1882140	from what large language models have learned.
1882140	1885740	It also can embed calls to large language models
1885740	1888100	within it to kind of draw in that sort of knowledge.
1888100	1890860	Haven't you gone back to the visualness of semantic housing
1890860	1893340	because you're doing this translation
1894340	1899260	into symbolic semantic representation,
1899260	1902540	which really ends with your actual result,
1902540	1904860	and it's riddled in the same way?
1904860	1908820	Well, right, and also, so no, I think I would say,
1908820	1913340	I don't totally think that the way in which we're using,
1913340	1915620	or the sense in which, or I think there are some ways
1915620	1918980	in which this kind of broader definition
1918980	1921540	in which you are saying, well, the meaning of a distribution,
1921540	1924140	or the meaning of a sentence in language
1924140	1926900	isn't just one probabilistic programming expression, right?
1926900	1930260	That's what we're showing here for pedagogical purposes,
1930260	1932180	but you might say, well, okay, right,
1932180	1936460	how are you going to obey kind of the ambiguity of language?
1936460	1939620	There are kinds of sentences that are definitely ambiguous.
1939620	1943100	So one example that we've looked at are sentences
1943100	1944580	in which you say something like,
1944580	1946780	Josh beat Alex and Leo, right?
1946780	1948260	And you might ask, well, you know,
1948260	1950540	that's kind of a classic syntactic construction.
1950580	1953420	Does that mean that Josh beat Alex and Leo,
1953420	1954940	and they were playing on the same team,
1954940	1958260	or Josh beat Alex, and then Josh went on to beat Leo?
1958260	1962380	And what we see, or generally, what you might say is,
1962380	1964060	well, the meaning of that sentence
1964060	1967820	actually shouldn't be picking one expression or the other.
1967820	1969780	It should be kind of the distribution
1969780	1972820	over those possible parses,
1972820	1975460	and that distribution also shouldn't just be something
1975460	1978060	that we can determine in this totally context
1978060	1980180	and sensitive way, it should actually depend on
1980180	1982300	all the previous patterns in the discourse.
1982300	1986180	So if someone's continually been using this conjunctive
1986180	1990660	and to refer to teams of players playing together,
1990660	1993300	we should take that kind of discourse bias into account.
1993300	1996780	And I think actually, right, this provides,
1996780	1999580	or thinking of large language models
1999580	2001260	as kind of generally having learned
2001260	2002860	this broad joint distribution,
2002860	2006500	but one that can be kind of conditioned quite richly
2006500	2010620	both on the content of this generative model.
2010620	2012220	So it's not trying to come up
2012220	2014460	with a universal definition of strength.
2014460	2016140	It's not even necessarily trying to come up
2016140	2018500	with a universal definition of any of these words.
2018500	2020500	It's thinking about how they might map contextually
2020500	2023620	into the best possible expression in the context
2023620	2028500	of a particular local model built for a particular situation.
2028500	2032180	I think is obviously related to,
2032180	2037180	but attempting to address some of the brittleness challenges
2037220	2039420	of semantic parsing in the past.
2039420	2040780	I think another answer to this, right,
2040780	2044540	is that part of the problem of semantic parsing previously
2044540	2046100	has been actually that the mapping functions
2046100	2049020	have historically been difficult to get right.
2049020	2050820	Whether you were thinking about those
2050820	2053380	as kind of old hard-coded grammars
2053380	2056060	or many of the attempts to kind of learn these things,
2056060	2057940	we are very domain-specific supervision.
2057940	2059740	So you wanna have a semantic parser
2059740	2061740	for a particular robotics domain.
2061740	2065660	You need a thousand examples of sentences
2065660	2067660	about that particular robotics domain
2067660	2070340	and a thousand paired with a thousand examples
2070340	2075340	of programs that are operating on that particular domain.
2075660	2078420	What we're seeing here is I think something that says no.
2078420	2080460	What it means to learn language generally
2080460	2082780	is to learn kind of this general mapping
2082780	2085980	between language and some kind of underlying representation.
2087340	2091420	And also, one reason why we might want a system like this
2091420	2093980	is because we want to be able to condition coherently
2093980	2097940	on information that's not just coming from language.
2097940	2101860	And we want to think about how a general substrate
2101860	2105660	in which the only, yes, we might be told
2105660	2106900	that Josh went against Leo,
2106900	2109700	but we might also be watching videos
2109700	2112460	that give us information about Josh's strength,
2112460	2113460	that convey our observations.
2113460	2116340	We might also have seen pictures
2116340	2118860	like the ones in the stimuli that we saw before
2118900	2122620	demonstrating the results of previous outcomes of matches.
2122620	2125860	And I think one thing that suggests
2125860	2129740	is we want this kind of general substrate
2130740	2132940	in which we can think about how those observations,
2132940	2135340	including the observations from language,
2135340	2137900	but without prioritizing language in any way,
2137900	2140380	I think are coherently considered.
2142900	2147260	So I think it depends on what part of semantic parsing
2147260	2150940	you, or yeah, I think the answer to that depends
2150940	2153380	on what part of semantic parsing we think of
2153380	2155940	as being the source of the brittleness
2155940	2159500	that caused us to throw that paradigm into question.
2161340	2164980	Yeah, and maybe I'll just offer one more perspective.
2164980	2167700	So one part of it is what Leo is saying.
2167700	2169740	Traditional semantic parsing is brittle in two ways.
2169740	2171540	One is, do you have broad coverage of language
2171540	2173580	that you can parse into your system?
2173580	2175940	And two is like how broad coverage
2175940	2178060	are the set of query, like the set of semantic queries
2178060	2179420	that you can actually answer.
2179420	2181020	And I think what you're pointing out is
2181020	2183100	this doesn't seem to address the second source
2183100	2184580	of brittleness, which is that your system
2184580	2185580	can only answer certain things,
2185580	2187460	it can only reason about certain things.
2188460	2192180	Brittleness of the formal representation language
2192180	2193020	that you're using.
2193020	2198020	Right, that's true of large language model representations.
2198860	2203860	Right, so I think my sort of take on that
2204380	2208100	is from a kind of AI engineering perspective
2208100	2212860	is sort of a branching in two directions.
2212860	2215740	One is, I think we have made some progress
2215740	2219340	that this is not really evoking, probably,
2219340	2221980	toward systems that within restricted domains
2223100	2226140	can reason coherently and probabilistically
2226140	2227900	about a wide range of queries.
2227900	2232900	So we have systems like this inference QL system
2233180	2236180	that uses nonparametric bays to analyze huge data tables
2236180	2239020	and come up with a model of that system
2239020	2242660	that or of your data that can answer all sorts of questions
2242660	2247660	like, oh, you know, show me like which people
2249380	2252420	in this data set are like probably overpaid
2252420	2254340	given their experience or something like that.
2254340	2256820	So in the same way that people are kind of excited
2256820	2259860	about using natural language or using language models
2259860	2262500	to parse into SQL, right?
2262500	2264980	Because so much data is in SQL
2264980	2268220	and it's a very SQL is a very expressive language
2268220	2270220	for asking questions about that data.
2271300	2273220	When we have a probabilistic system,
2273220	2275740	like a good probabilistic model of that data under the hood,
2275740	2278580	it enables conversational patterns that are not enabled
2278580	2281020	when you have like SQL as the database
2281020	2283020	because we expect our conversational partners
2283020	2285100	to have coherent beliefs about the world,
2285100	2287620	to update those beliefs in response to new evidence
2287660	2291100	that we give it to be able to report uncertainty
2292540	2294300	and make sort of modal judgments.
2294300	2298860	And so one engineering path is to take those kinds of systems
2298860	2302700	and sort of build conversational interfaces to them
2302700	2305860	that behave more like an intelligent person would behave
2305860	2308700	and can draw inferences that you might not draw
2308700	2310820	if you're just talking to a SQL backup.
2310820	2313020	The other path that sort of we'll talk about
2313020	2315540	in the next part of the talk
2315580	2317780	is how can we use those representations
2317780	2319220	that language models have learned
2319220	2323180	to make the probabilistic inferences more interesting
2323180	2325100	and more robust, less brittle,
2325100	2328580	without sort of totally embracing the other kind of brittleness
2328580	2330180	which is the kind of brittleness that language models
2330180	2331020	seem to have right now,
2331020	2332540	which is that they draw,
2332540	2333940	that they don't really necessarily reason
2333940	2336340	with coherent probabilistic beliefs.
2336340	2339340	So maybe, yeah, let's go into that next part.
2339340	2343340	Yes, yes, okay, right.
2344300	2346540	Well, so right, so in the interest of time,
2346540	2347940	I'm actually gonna like skip through
2347940	2349220	some of the rest of this example,
2349220	2351660	which I think is just more of what you've seen,
2351660	2353860	but one sense in which I think,
2353860	2356540	yeah, maybe a third part of the answer to Chris,
2356540	2357740	I would say, is that,
2360460	2362300	right, you know, yeah.
2362300	2363900	I think part of what this is trying to do
2363900	2366700	is explore some of the ways in which we might answer ways,
2366700	2369820	right, without giving an answer in all the ways in which
2369820	2372780	we might answer the ways in which language models themselves
2372780	2375260	are brittle with respect to what we also want
2375260	2376860	from a model of intelligence, right?
2376860	2378980	We might suspect that when we answer,
2378980	2380540	ask questions like this,
2380540	2382980	really what we are trying to do is specify
2382980	2386980	some kind of normative query that captures formally
2386980	2390260	a sense of, well, we want something like the posterior
2390260	2392980	with respect to some kind of internal model of the world.
2392980	2395780	And, you know, this is kind of the simplest means,
2395780	2397620	or this is a very simple example
2397620	2400380	of how we might formally impose that kind of structure,
2401380	2403620	but one that I think can be elaborated on,
2405300	2407660	depending on the kinds of primitives
2407660	2409140	and the ways in which you're thinking about
2409140	2412220	what it is that the probabilistic programs can express, right?
2412220	2415460	So one way in which we might think about doing that
2416580	2419820	is by thinking about probabilistic programs
2419820	2423660	that themselves have access to other kinds of means
2423660	2426820	of calling other different mechanisms and cognition, right?
2426820	2429180	So I think I would draw a contrast here
2429300	2431900	between the notion of the large language model
2431900	2434620	as a controller, the one that's making the decisions
2434620	2437340	about when to write little snippets of code
2437340	2441460	and to execute them, when to call out to little planners
2441460	2444340	and incorporate them, or stuff like the Minds Eye work,
2444340	2446140	right, where there's a language model,
2446140	2448540	it decides when to call out to a physics simulator,
2448540	2451460	but the way it interprets the outputs
2451460	2453620	of that physics simulator is to paste those back
2453620	2455700	into the language model context
2455700	2458540	and try to draw inferences on them in turn.
2458540	2460580	Rather, in this kind of framework, right,
2460580	2463460	what you can kind of see a, or yeah,
2463460	2465700	the direction that this framework would be pointing towards
2465700	2469300	is to say, well, on the other hand,
2469300	2472700	we already have languages that allow us to do things
2472700	2474980	like build expressive generative models
2474980	2477900	over three-dimensional scenes that also capture things
2477900	2480020	that we might want only from perception,
2480020	2482540	like knowledge about how the shapes of objects
2482540	2484380	tend to accude each other,
2484380	2487460	or incorporate rich models of physics,
2487460	2491340	or that model theory of mind as taking place recursively
2491340	2493860	and thinking about agents who themselves have beliefs
2493860	2495780	about their own internal world models
2495780	2499420	and are actually choosing their actions as planners, right?
2499420	2502060	And in this kind of framework,
2502060	2504900	you can point the way towards a kind of model that says,
2504900	2507780	well, how is it that I might incorporate language
2507780	2510340	into these kinds of models sitting alongside
2510340	2513140	these other kind of observations that I might make, right?
2513140	2516140	So how might I think about the meaning
2516380	2518940	of images that I want to generate
2518940	2521260	that specify specific constraints,
2521260	2525420	or imagination, or, right, go ahead, Jacob.
2525420	2527060	Yeah, I just had a clarification question.
2527060	2531060	So you were talking earlier about having this meeting function,
2531060	2533340	and then I think also we're mentioning something
2533340	2538340	about like code x, in terms of the questions.
2538420	2541540	I'm just trying to understand which of these is that.
2541540	2543460	Is that here, or is the meeting function
2543460	2544700	when you come later?
2545660	2546780	So that's maybe the first question
2546780	2547860	and then the other clarification is,
2547860	2550420	so are these statements actually just been programmatically
2550420	2554620	created from code x by prompting the text that was on?
2554620	2555460	Yes, that's right.
2555460	2560460	So by meeting function in this framework, we say,
2560500	2562100	well, there's kind of two generalizations
2562100	2562940	of a meeting function.
2562940	2565140	There's a general joint prior, right?
2565140	2566660	That code x is already,
2566660	2567940	that it's learned between language and code,
2567940	2570740	and then there's this kind of context specific
2570740	2573300	meeting function in the sense that it's conditioned
2573300	2575860	on whatever's in the prompt, the generative model,
2575860	2579940	and some examples of how language relates to expressions
2579940	2581660	that it's doing, so that's a meeting function.
2581660	2583500	And yes, all these examples that you're seeing
2583500	2585980	are one sample from that distribution.
2589500	2593060	Right, and one of the things that I wanna point to here,
2593060	2595740	right, is it does, I think it suggests a framework
2595740	2598220	or another means of thinking about what it means
2598220	2601660	for language to construct new concepts from definitions,
2601700	2603460	or even come to construct new world models
2603460	2605820	from thinking like somebody in the beginning asked, right?
2605820	2609500	So how, for instance, might we think about enriching
2609500	2611940	an existing structured relational model
2611940	2613900	with concepts that we learn from language?
2613900	2618060	So for example, if we consider kind of a formal model
2618060	2623060	of kinship relations, we might say that, well,
2623900	2626020	the generative model of this domain
2626020	2629340	is itself represented as a probabilistic program.
2629340	2632940	It captures both the causal means by which
2634100	2636900	people give rise to their children,
2636900	2640540	and also the definitions or one notion of the definitions
2640540	2642620	of what it means to be something like a sister or a father
2642620	2644780	with respect to this core notion
2644780	2647300	of how family trees come to be.
2647300	2650500	And so if you take this kind of general notion
2650500	2653900	of the meaning of language as being the distribution
2653900	2654900	over expressions that it creates
2654900	2656340	in a probabilistic programming language,
2656340	2660380	you might start to think how we can formally think about
2660380	2664260	relating definitions for various kinds of relational terms.
2665260	2666860	An uncle is the brother of one's parent
2666860	2668420	or the husband of one's aunt.
2668420	2671420	A pibling is a gender neutral term for an aunt or uncle,
2671420	2673220	that's the sibling of one's parent,
2674260	2679260	or this relational notion of a sister of one's father
2679980	2681420	from a language that's actually not found
2681420	2682780	anywhere on the internet.
2682780	2685620	And I think the core thing that we wanna suggest here, right,
2686020	2688140	is why do we even have definitions at all?
2689500	2693260	Well, one notion of what it even means
2693260	2695660	to have learned the definition of this term
2695660	2698340	is that it should drive coherently
2698340	2701260	all of the downstream inferences that you make with that term,
2701260	2704340	and it should graft onto the conceptual knowledge
2704340	2705580	that you already have.
2705580	2710020	And so you can think about forming new sentences directly
2710020	2713380	that refer to someone's paani or one's pibling
2713380	2715460	in this situation and expecting them
2715460	2718620	to draw both on your existing conceptual knowledge
2718620	2722180	of what it even means to have a family tree
2722180	2724540	as well as all the other conceptual terms for a friendship
2724540	2725780	that you may already have.
2728060	2732100	And the same framework also suggests one mechanism
2732100	2734340	by which we might formalize what it means
2734340	2736660	to learn world models from language.
2736660	2739380	So as I mentioned, if we return to the situation
2739380	2742580	that opens this talk, tug of war games,
2743420	2745820	we might think about how the definition that I gave
2745820	2748140	when I sat up here at the podium, right,
2748140	2750380	saying there are people whose strength levels vary
2750380	2752420	from person to person.
2752420	2755940	People have a percentage of time in which they're lazy.
2755940	2758900	Strengths of the teams depend on the underlying strengths
2758900	2761620	of the members of that team,
2761620	2763420	and whether one team beats another
2763420	2765980	just depends on which team pulls stronger that match.
2765980	2769060	And this kind of setting is actually language, right,
2769060	2772380	is building up the actual generative model itself.
2772380	2774580	And you might think of a system like this
2774580	2777300	that both learns these kinds of theories from language
2777300	2781380	and then is appending to this kind of local problem-based
2781380	2783700	context to answer arbitrary questions
2783700	2785220	like the kinds that we gave or conditioned
2785220	2788220	on various observations like Josh being stronger than Leo
2788220	2790740	with respect to this kind of local notion
2791980	2796300	of what strength means in this particular problem context
2796300	2798300	that we're thinking about.
2798300	2799700	In interest of time, why don't we just jump on
2799700	2800540	to your section.
2800620	2801460	Yeah, thanks.
2808540	2811300	So we've just been talking about how natural language
2811300	2815620	can sort of be interpreted or semantically parsed
2815620	2817980	to a probabilistic language of thought,
2817980	2820500	but we haven't talked about how cognition itself,
2821780	2823620	which is sort of, we've been talking about
2823620	2825180	as the product of general purpose
2825180	2826900	probabilistic inference machinery,
2826900	2829500	might interact with language cognitively
2829540	2832420	or how our tools for, you know,
2832420	2834980	our algorithms for inference, our model representations
2834980	2838580	might benefit from recent advances in language models.
2838580	2841260	So in the rest of the talk,
2841260	2843740	I'll sort of talk about this also very preliminary work
2843740	2846180	that we just presented at a workshop at ICML
2847660	2849420	that is more about a role for natural language
2849420	2851660	and language models in this part of the picture.
2852900	2854700	And one reason to think that natural language
2854700	2857260	must play some role in this part of the picture
2857340	2861140	is that sometimes we set ourselves reasoning tasks
2861140	2863340	whose specifications, what it would mean
2863340	2864980	to solve the reasoning task correctly
2864980	2866180	must involve natural language.
2866180	2867820	So for example, if you have an iPhone,
2867820	2870300	you might have used the visual voicemail feature,
2870300	2871700	which automatically, but somewhat
2871700	2873780	incompletely transcribes your voicemails.
2873780	2876060	And these transcripts have gaps marked
2876060	2878140	by underscored sequences of varying lengths,
2878140	2880740	indicating Apple couldn't quite work out what was said.
2881740	2883500	And an inference task that I sometimes face
2883500	2884820	is squinting at these transcripts
2884820	2886820	and trying to think what could the person have said
2886820	2889740	during those bits that it didn't transcribe correctly.
2889740	2892980	And is it worth my time to listen to this voicemail
2892980	2896660	or am I pretty certain that I got all the relevant information
2896660	2898980	from the part of the transcript that I've seen?
2900100	2903100	So even if I'm representing that kind of inference problem
2903100	2904940	in some kind of probabilistic language of thought
2904940	2907540	and not in natural language, it must reference natural language
2907540	2909300	because a key part of the reasoning that I'm doing
2909300	2910660	is about how long those gaps are,
2910660	2912220	about what words could go in those gaps,
2912220	2914580	how they could semantically and syntactically
2914580	2915860	with the words around them.
2917820	2919980	And there are a lot of other tasks like this
2919980	2922620	where the specification of some reasoning problem
2922620	2924020	must in some way involve language.
2924020	2925340	Maybe we're writing something
2925340	2927100	that has to obey certain structural constraints
2927100	2929340	like a poem or code.
2930260	2932380	Maybe we're puzzling over a message from our advisor,
2932380	2933700	trying to infer all the different meanings
2933700	2935780	consistent with what they said.
2935780	2936980	Maybe we're trying to figure out
2936980	2939340	how to put together some words that we predict
2939340	2941780	could achieve some desired effect in a listener.
2943140	2945740	And beyond the fact that some inference problems
2945740	2947780	implicate language and their specification,
2947780	2948980	it seems like at least sometimes
2948980	2952580	we sort of use language for thinking, right?
2952580	2955980	Rubber duck debugging is when we successfully debug
2955980	2957260	something that's been something us
2957260	2960500	by just talking about it to ourselves or to a rubber duck.
2962180	2965020	And I think this is the intuition also behind
2965020	2967500	sort of chain of thought, scrap pad,
2967500	2971380	those kinds of innovations in language model land.
2972540	2974100	But one reason I'm drawing this distinction
2974100	2976700	between task specification and algorithm
2976700	2979820	is that this has long been a really important distinction
2979820	2981140	in probabilistic modeling and inference.
2981140	2982980	And it's something that I think we lose
2982980	2985940	when we move just to asking a language model a question
2985940	2988180	and hoping that it gives us the right answer.
2988180	2992380	So in the kind of work that our lab does
2992380	2994060	in modeling and inference,
2994060	2998560	we sort of separately create a model probabilistic program
2998560	3000180	that includes a task specification
3000180	3002620	as a posterior distribution we wanna sample from
3002620	3004020	and separately an inference program
3004020	3006260	that compositionally encodes some kind of algorithm
3006260	3008260	or strategy for solving that inference task.
3008260	3010100	And when you use a probabilistic programming language
3010100	3011740	to do this, you get some benefits
3011740	3014700	from taking this approach of separating model inference.
3014700	3016380	We know that we have soundness theorems
3016380	3018500	guaranteeing that as computation increases,
3018500	3020820	the inference is going to approach the posterior.
3020820	3022340	We have automated tools and tests
3022340	3024260	for measuring how accurate our inferences are
3024260	3027060	relative to the model with finite computation.
3027060	3028540	And we also have gradient estimators
3028540	3031020	that help us tune any parameters of our inference programs
3031020	3033860	to be better inference algorithms.
3033860	3035820	And beyond being useful properties for engineering,
3035820	3037300	these guarantees also reflect
3037300	3038940	some key aspects of human cognition.
3038940	3041900	We can often think more to reach more accurate conclusions.
3041900	3043300	We can critically evaluate the extent
3043300	3045260	to which our current hypotheses actually make sense
3045260	3046900	given our model of the world.
3046900	3048980	And if we repeatedly face the same kind of inference task,
3048980	3052260	we can train ourselves to get better at solving it.
3052260	3053620	So something we've begun to explore
3053620	3055660	is whether adding LLMs to this picture
3055660	3058460	might let us both specify various linguistic tasks
3058460	3059900	as formal probabilistic models
3059940	3061340	and enhance our inference algorithms
3061340	3063060	by letting them do some of their thinking
3063060	3064580	using languages as a tool.
3064580	3067660	So I'll first talk about the modeling side of things.
3067660	3070340	So we all know that an autoregressive language model
3070340	3072180	defines a probability distribution
3072180	3073500	over sequences of tokens.
3074420	3075700	But we rarely just want to sample
3075700	3078980	that unconditional distribution.
3078980	3081380	You know, in the same way that in order to use a SAT solver,
3081380	3083900	we need to reduce the problem we care about to a SAT formula,
3083900	3086740	use a language model, we need to reduction
3086740	3088900	from the task instance that we care about
3088900	3092540	to a prompt.
3092540	3093940	And the idea is that we're saying
3093940	3095220	that the conditional distribution
3095220	3097020	of the language model conditioned on that prompt
3097020	3099260	is somehow a good specification of the task
3099260	3100100	that we want to solve,
3100100	3102020	or a good approximation of the task that we want to solve.
3102020	3103580	But unlike the reductions to SAT,
3103580	3106700	of course, this reduction is lossy.
3106700	3109220	One problem is that sort of hard constraints,
3109220	3111940	sort of instructions that we give the language model
3111940	3114700	might be, you know, it might fail to follow them.
3114700	3117060	So this conditional distribution, p-task,
3119140	3121460	is not really the specification that we have in mind,
3121460	3123620	it's just some close thing that we can get.
3127380	3129820	Another problem is that the entropy of this distribution
3129820	3132140	may not meaningfully reflect uncertainties.
3132140	3135300	So you may have seen in the GPT-4 paper
3135300	3140540	that on multiple choice tasks,
3140540	3142900	where there's some multiple choice question
3142900	3146060	and then the language model is asked to output A, B, C, or D.
3146060	3149300	Before they did any RLHF and instruction tuning,
3149300	3151460	if they create a calibration plot,
3151460	3154980	where they plot sort of, you know,
3154980	3157740	of all the answers in which GPT was, you know,
3157740	3161580	0.4% confident, how often was that the correct answer?
3161580	3164220	GPT-4 is strikingly well calibrated.
3164220	3166140	And that's what you might expect from a model
3166140	3168980	that's doing a very good job of next token prediction,
3168980	3170980	of matching the distribution of language.
3170980	3173260	When it's uncertain, the loss function is telling it,
3173260	3175820	it should allocate its mass, its probability mass,
3175820	3177380	according to that distribution.
3177380	3179820	Whereas after RLHF, the calibration is shot.
3179820	3182700	And this is also what you might expect,
3182700	3186820	even if the humans who were sort of
3186820	3190700	providing the human feedback in RLHF
3190700	3193060	preferred the right answer, okay?
3193060	3197980	The distribution that you get after performing RLHF
3197980	3200780	with the objective that's commonly used for RLHF
3200780	3203220	sort of creates a reduction in temperature.
3203220	3206060	It's equivalent to reducing the temperature
3206060	3207900	of the parts where the human feedback
3207900	3211820	is exactly aligned with sort of the correct answer.
3211820	3213540	And so it becomes overconfident.
3214500	3217140	And, you know, this is very prompt dependent.
3217140	3219060	I don't mean to say that this is like always gonna happen
3219060	3222140	if you go and use GPT, but I went and used GPT-3.5
3222140	3224300	to do this like infilling task,
3224300	3226460	and it did it correctly, but also every time
3226460	3228140	that I generated at temperature one,
3228140	3229980	it gave me basically the same answer.
3230860	3233660	So if I want to think of this distribution
3233660	3235420	as sort of representing uncertainties
3235420	3237100	that I can make decisions about whether to listen
3237100	3239060	to my voicemail because it might contain things
3239060	3244060	I don't know, this P-task is not up to that task.
3244260	3248460	So our idea is to instead of reducing to a prompt,
3248460	3250220	reduce to a probabilistic program
3250220	3252860	that may call a language model,
3252860	3254460	which is sort of a more flexible way
3254460	3257020	of specifying what P-task distributions
3257020	3258220	we want to sample from.
3258300	3260380	And I know I'm running low on time,
3260380	3263060	but the idea is that these models
3263060	3265300	can mix calls to the language model
3265300	3268220	with conditioning statements and other logic.
3268220	3271580	So in this probabilistic program for this infilling task,
3273060	3276540	it is in a loop going through each sort of blank
3276540	3278660	in the that we need to infill,
3278660	3280140	sampling a random number of tokens
3280140	3284020	that should fill that spot, sampling those tokens
3284020	3287700	and then observing the next sort of fixed fragment
3288660	3291220	or conditioning on the next part being a fixed fragment.
3291220	3293660	And this just lets us specify a model
3293660	3295780	that doesn't just have a prefix prompt
3295780	3297980	that sort of has a prompt with blanks in it.
3299100	3300780	I haven't said yet how we're going to sample this,
3300780	3302980	but the idea is that this defines a specification
3302980	3304380	for the task that we want.
3304380	3308140	And similarly, we have programs that sort of specify
3308140	3311020	a variety of tasks that involve sort of thinking
3311020	3312460	with language, right?
3312460	3313940	We can condition on hard constraints
3313940	3317020	if we want to parse into a formal grammar or write a high two.
3317020	3319540	We can do kind of a product of experts model
3319540	3320500	using multiple prompts.
3320500	3322860	So maybe I want to think about a fun fact
3322860	3324460	that's about both London and Paris.
3324460	3326300	Well, you could just ask the prompt,
3326300	3328700	hey, please give me a fun fact about both London and Paris,
3328700	3330700	but you could also create a product of experts model
3330700	3332300	where it has to come up with a completion
3332300	3334580	that is both a completion to the sentence,
3334580	3336060	a fun fact about London is,
3336060	3337220	and a completion to the sentence,
3337220	3338620	a fun fact about Paris is.
3338620	3341420	And that's kind of a hybrid where the idea of and
3341420	3343460	and both is symbolically encoded,
3343460	3346100	but we're still using the language models representation
3346100	3349860	of knowledge about fun facts and these things.
3349860	3352860	Similarly, we can sort of represent reward steering
3352860	3355420	or a classifier guidance by conditioning,
3355420	3358020	by sort of soft conditioning on a reward function.
3359500	3361020	And we can also include things like,
3361020	3364220	hey, please generate a gloss of this code
3364220	3369220	that when I try to semantic parse it back into code,
3369580	3372700	gives me the same code I started with, things like that.
3372740	3376700	So those are model programs for specifying various tasks.
3376700	3377980	We need inference algorithms
3377980	3380260	for actually sampling from these distributions.
3380260	3382420	And so far we've been focusing
3382420	3384620	on sequential Monte Carlo inference algorithms.
3385660	3387780	And we kind of have a default version of this method
3387780	3389260	and then fancier versions of this method
3389260	3391100	that are necessary for harder tasks.
3391100	3393840	In many ways, sequential Monte Carlo looks like beam search.
3393840	3396300	You kind of keep multiple hypotheses around,
3396300	3398900	you extend them, you reweight them
3398900	3400780	according to model specific way,
3400780	3401900	and then you resample,
3401900	3403700	which is kind of like the part of beam search
3403700	3405620	where you sort of down sample
3405620	3408740	from your big expanded beam back to your beam size.
3408740	3410780	But unlike beam search,
3410780	3412260	sequential Monte Carlo,
3412260	3415460	as you scale up the number of hypotheses that you're using,
3415460	3417540	instead of converging to an arg max
3417540	3419260	of your objective function,
3419260	3421620	converges to the posterior distribution,
3421620	3423820	sampling from the posterior distribution.
3423820	3426220	And this sort of default version of SMC
3426220	3429820	has worked for a few simple tasks that we've tried it on.
3430820	3433900	For example, if I want a completion that follows
3433900	3435380	my favorite physicists is probably,
3435380	3438100	and my favorite writer is probably equally well,
3438100	3440020	SMC can give me Richard Feynman.
3440020	3443340	I really admire how he communicates complex ideas so clearly.
3444700	3447220	Or if I want to finish the fed says,
3447220	3449100	but only using words less than five letters,
3449100	3450500	I get the fed says it will taper,
3450500	3453340	but the rate hikes are still years away
3453340	3454180	to something like that.
3454180	3455860	And it's worth noting that if you do something
3455860	3458420	like token masking to enforce this constraint,
3458460	3460780	you just forbid the language model
3460780	3463580	from generating anything that's longer than five letters.
3463580	3465980	You get all sorts of weird completions.
3465980	3467860	It's different from the posterior here.
3467860	3470260	You get completions that set up an idiom
3470260	3471260	that it could only complete
3471260	3472860	if it used a word longer than five letters
3472860	3473860	or something like that.
3473860	3475540	And then it just gets very confused
3475540	3477460	and right stop, that, that, read more or something.
3477460	3479180	It tries to come up with some context
3479180	3481300	in which the sentence would be cut off early.
3482540	3483820	And for infilling tasks,
3483820	3487660	we get a variety of samples that sort of fit semantically
3487700	3491540	and syntactically with the text,
3491540	3494500	but infilling tasks can be made much harder than this one.
3494500	3497140	So I don't want to claim that this method yet solves
3497140	3498220	all these infilling tasks.
3498220	3499420	So for harder tasks,
3499420	3501340	we think we're going to need to use fancier
3501340	3502500	sequential Monte Carlo algorithms.
3502500	3504540	And both of these sort of steps
3504540	3506140	can actually be extended in various ways.
3506140	3508900	We can use better proposal distributions
3508900	3512300	and sort of better reweighting strategies
3512300	3514340	that are trying to guess, okay,
3514340	3516900	are we on the path to getting a good sample here?
3516900	3518620	And we think that techniques
3518620	3520140	that have already been developed in the literature
3520140	3523460	for proposing good things in line with constraints
3523460	3526340	or sort of discriminating whether we're likely
3526340	3531140	to land in a constraint could be good ingredients to put here.
3531140	3534660	But the important thing is, and this is the very end,
3534660	3536540	the important thing is all of those things
3536540	3538820	become part of the inference program.
3538820	3539940	And there are still guarantees
3539940	3541860	that as we scale up the number of particles,
3541860	3545220	we're still targeting the original specification of the model.
3545220	3548260	So all of those heuristics or biases don't sort of,
3548260	3549740	we don't just trust them blindly.
3549740	3552740	We don't hand the keys to those techniques.
3552740	3554940	We still have a specification that we can understand.
3554940	3556660	Okay, I'll stop there.
3556660	3557500	Thanks.
3557500	3563740	All right, we're a little bit behind time.
3563740	3565780	So unless there is a burning question,
3565780	3567740	burning question, no burning questions.
3567740	3571980	Let's break for tea and maybe Zachertorte
3571980	3575820	and you guys can talk to the speakers.
3575820	3579620	And at 11.30, we are going to...
