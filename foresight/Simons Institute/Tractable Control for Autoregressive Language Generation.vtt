WEBVTT

00:00.000 --> 00:03.360
pro six circuits on tiny binary assets.

00:03.360 --> 00:06.880
So what we'll do today is actually train PCs

00:06.880 --> 00:09.120
on language, language models,

00:09.120 --> 00:11.400
and Juan-Wes going to tell us how that works

00:11.400 --> 00:12.800
and what that's useful for.

00:15.160 --> 00:18.800
So today I'll present our paper,

00:18.800 --> 00:22.200
Attractable Control for Autoregressive Language Generation.

00:23.480 --> 00:28.480
So unlike probably the interesting ones,

00:28.640 --> 00:32.040
today I'll focus more on the application side

00:32.040 --> 00:34.080
of probabilistic circuits.

00:34.080 --> 00:36.680
Like more machine learning oriented.

00:38.480 --> 00:43.480
Okay, so let's get started with the basic concept

00:43.840 --> 00:45.560
of large language models.

00:46.960 --> 00:49.520
They have been like really popular recently.

00:49.520 --> 00:52.760
I'm not sure if people here really care or not.

00:53.760 --> 00:58.760
Like I see like this interested face you do.

01:00.280 --> 01:02.000
Okay, that's a good sign.

01:02.000 --> 01:03.680
That's a good sign.

01:03.680 --> 01:07.800
So basically the idea is very simple.

01:07.800 --> 01:11.320
In the sense that you collect large amount of text

01:11.320 --> 01:13.760
consisting of trillions of words,

01:13.760 --> 01:17.400
and you train neural networks with billions of parameters

01:17.400 --> 01:21.400
on these data, then you have these so-called large

01:21.400 --> 01:22.520
language models.

01:24.880 --> 01:29.880
These chat GPT or GPT-4 thing has become like really popular.

01:32.600 --> 01:34.440
People talking to them on the internet,

01:34.440 --> 01:38.200
asking them to polish their papers,

01:38.200 --> 01:40.720
using them to do their homeworks,

01:40.720 --> 01:45.720
and even asking them to play D&D together.

01:47.520 --> 01:50.320
So yeah, that is actually true.

01:51.320 --> 01:56.320
So the idea is, so people have now having this feeling

01:57.280 --> 01:59.840
that we probably have solved AI,

01:59.840 --> 02:03.560
we have solved artificial intelligence,

02:05.680 --> 02:07.640
and chat GPT has passed Turing test,

02:07.640 --> 02:10.840
but is that really the case, okay?

02:10.840 --> 02:15.840
So I tried this a few months ago.

02:16.840 --> 02:21.840
It was an actual conversation between me and chat GPT.

02:21.920 --> 02:25.600
So I asked the model to generate a sentence

02:25.600 --> 02:29.560
using Frisbee, Cod, and Dog following the given order.

02:29.560 --> 02:32.280
Well, I mean, this should be quite simple

02:32.280 --> 02:36.400
for a super intelligent AI model, right?

02:36.400 --> 02:39.760
So it does give me a sentence.

02:39.760 --> 02:42.840
The sentence look quite decent,

02:43.840 --> 02:46.080
and all the key words are in there,

02:46.080 --> 02:50.960
but Dog and Cod are in the wrong order, right?

02:50.960 --> 02:53.240
Well, I mean, humans make mistakes,

02:53.240 --> 02:55.120
so super intelligent AI does.

02:56.120 --> 03:00.120
So I was being patient, I tried again, okay?

03:00.120 --> 03:01.920
Again, it gives me a sentence,

03:01.920 --> 03:04.440
but it's even worse now.

03:04.440 --> 03:06.480
All of the words are in the wrong order.

03:06.480 --> 03:11.480
So you see that chat GPT fails to follow

03:11.760 --> 03:15.040
even this simple logical constraint, okay?

03:15.040 --> 03:17.680
So people have been trying to fix this.

03:17.680 --> 03:20.240
Some of the methods, including like,

03:20.240 --> 03:22.320
okay, we can prompt them in a different way,

03:22.320 --> 03:25.100
like chain of thought.

03:26.160 --> 03:28.200
I'm not comfortable saying this phrase,

03:28.200 --> 03:30.440
but that's one of the methods.

03:31.680 --> 03:34.400
Or people try to use search algorithms

03:34.400 --> 03:36.400
trying to find the correct sentence

03:36.400 --> 03:39.880
in this huge search space, so on and so forth.

03:39.880 --> 03:43.680
But none of them actually guarantees that chat GPT

03:43.680 --> 03:47.720
or whatever other language model give us what we want.

03:47.720 --> 03:52.440
So what we actually want is like 100% guarantee.

03:52.440 --> 03:55.400
When we have like a huge powerful model,

03:55.400 --> 03:57.360
when we instructed to do something,

03:57.360 --> 04:01.480
we wanted to follow our instructions exactly.

04:01.480 --> 04:05.160
So in today's talk, I'll show you how we can do this

04:05.160 --> 04:07.240
with probabilistic circuits.

04:08.240 --> 04:11.320
Before some of the detail,

04:11.320 --> 04:14.840
so let's get started with some basics of language modeling.

04:14.840 --> 04:17.720
So language modeling is really not like a fancy idea.

04:17.720 --> 04:21.280
It's just a joined distribution over some text,

04:22.400 --> 04:25.480
probably less than or equal to n words.

04:27.400 --> 04:32.400
Each random variable here is a word taking value

04:33.320 --> 04:38.320
in a fixed, in a vocabulary of finite size.

04:40.120 --> 04:44.520
The size usually in practice ranges from 10,000 to 50,000

04:44.520 --> 04:46.200
or something, okay?

04:47.280 --> 04:50.000
And here's some examples.

04:50.000 --> 04:53.920
You might notice there's some special word called EOS here,

04:53.920 --> 04:55.120
which means end of sentence,

04:55.120 --> 04:58.000
which is only like a special token or word

04:58.000 --> 05:00.840
we use to pad sentences of different length

05:00.840 --> 05:01.920
to the maximum length.

05:01.920 --> 05:06.120
So we have this like joined distribution well defined, okay?

05:06.120 --> 05:10.000
So we collect a lot of text of this form,

05:10.000 --> 05:13.200
sentence fragments, paragraph fragments,

05:13.200 --> 05:16.000
and we train this joined distribution

05:16.000 --> 05:19.800
by maximizing a lot of likelihood, a very, very simple idea.

05:20.640 --> 05:22.200
Okay, so what about the architecture

05:22.200 --> 05:24.600
of this distribution of our model?

05:24.600 --> 05:29.600
Well, like the most popular ones, like GPT,

05:30.320 --> 05:32.440
they are autoregressive models.

05:32.440 --> 05:36.800
Well, so by the chain rule of probability,

05:36.800 --> 05:40.680
we can decompose our joined distribution this way.

05:42.360 --> 05:45.760
Represented as a graphical model is basically

05:45.760 --> 05:47.760
for each random variable,

05:47.760 --> 05:51.520
we have arrows going from all the previous random variables.

05:52.600 --> 05:55.000
From a generative point of view,

05:56.000 --> 05:58.640
it's basically we start with,

05:59.800 --> 06:01.040
I can't see my cursor.

06:01.040 --> 06:05.360
So we start generating the first word

06:05.360 --> 06:07.480
from this prior distribution.

06:07.480 --> 06:08.920
Once we have the first word,

06:08.920 --> 06:10.920
we move on to the second one,

06:10.920 --> 06:12.320
conditioning on the first one,

06:12.320 --> 06:13.960
and then we generate the third one,

06:13.960 --> 06:17.160
conditioning on the first two words, very simple, okay?

06:19.560 --> 06:22.840
By some classic results,

06:22.920 --> 06:26.680
marginose for Bayesian networks

06:26.680 --> 06:29.400
with these structures is intractable.

06:31.360 --> 06:36.360
Okay, so before when we asked GPT

06:39.200 --> 06:40.600
to generate a sentence for us,

06:40.600 --> 06:44.440
we are like talking to it as if it's human, right?

06:44.440 --> 06:48.080
So the assumption is if our distribution over language

06:48.080 --> 06:51.120
is a perfect distribution,

06:51.120 --> 06:54.960
then we should be able to interact with it as a human.

06:56.400 --> 06:58.640
When we're doing these kind of like prompting,

06:58.640 --> 06:59.840
when we're talking to it,

06:59.840 --> 07:03.560
most of the time we are actually trying to do conditioning.

07:03.560 --> 07:06.560
So let's consider an even simpler example.

07:07.680 --> 07:10.560
Suppose in our autoregressive generation,

07:10.560 --> 07:14.400
we have generated the first three words, the weather is,

07:14.400 --> 07:17.600
and our constraint is that we want the sentence

07:17.600 --> 07:19.280
to contain the keyword winter.

07:19.280 --> 07:21.200
Well, I mean, we wanna write something

07:23.000 --> 07:25.840
with the topic of winter, so that's our constraint.

07:26.920 --> 07:30.160
What we really want is, okay,

07:30.160 --> 07:34.640
so what language model gives us is the next token,

07:34.640 --> 07:36.440
next word probability.

07:36.440 --> 07:39.840
So basically given the weather is,

07:39.840 --> 07:44.040
it might prefer whether it's warm or cold,

07:45.120 --> 07:47.440
while its prior distribution might want the weather

07:47.440 --> 07:48.920
to be warm.

07:48.920 --> 07:51.440
But what we actually want,

07:52.480 --> 07:53.920
but what we'll actually need

07:53.920 --> 07:57.120
is this conditional next word probability, right?

07:57.120 --> 08:00.600
We want our sentence to satisfy the constraint,

08:00.600 --> 08:01.960
once it contain the winter,

08:03.440 --> 08:05.440
and well, intuitively,

08:05.440 --> 08:08.440
if we want the winter to be in the sentence,

08:08.440 --> 08:11.440
weather is more likely to be cold, okay?

08:11.440 --> 08:16.440
But this is intractable for those autoregressive models.

08:21.240 --> 08:23.120
So why is it intractable?

08:23.120 --> 08:25.240
Let's go more into the details.

08:25.240 --> 08:29.440
So by Bayes' rule, this conditional probability

08:29.440 --> 08:32.160
can be decomposed into these two terms.

08:33.640 --> 08:35.440
The next term here is very simple,

08:35.440 --> 08:38.400
it's just a language model next word probability.

08:38.400 --> 08:43.400
But the first term here is actually a marginal probability.

08:45.000 --> 08:47.800
So it's basically the marginal probability

08:47.800 --> 08:51.360
over all possible suffixes that we could generate

08:51.360 --> 08:53.800
that contains winter, okay?

08:55.480 --> 08:58.200
So this is intractable for autoregressive models,

08:58.200 --> 09:00.080
but we have PCs, right?

09:00.080 --> 09:03.200
PCs are tractable, at least,

09:04.960 --> 09:07.880
we know how to compute marginal probabilities with them.

09:07.880 --> 09:12.880
So why don't we just approximate this term here

09:13.280 --> 09:14.840
with probabilistic circuits?

09:18.120 --> 09:22.720
And we refer to our pipeline as gelato,

09:23.760 --> 09:26.120
in generating language with tractable constraints.

09:28.840 --> 09:31.400
Okay, so how do we do this?

09:31.400 --> 09:36.400
So the first step is that we pick our favorite PC

09:36.400 --> 09:40.360
for sequential modeling, which is a hidden markup model,

09:40.360 --> 09:41.880
the simplest we can have.

09:43.160 --> 09:46.480
We have, on the other hand,

09:46.480 --> 09:48.120
we have our pre-trained language model

09:48.120 --> 09:50.360
we want to control or guide.

09:50.360 --> 09:53.360
We sample a lot of data from our language model

09:53.360 --> 09:57.680
unconditionally, and then we train our hidden markup model

09:57.680 --> 10:01.780
on these data with maximum likelihood estimation.

10:01.780 --> 10:04.320
So effectively minimizing the KL divergence

10:04.320 --> 10:08.000
between the two joint distributions, okay?

10:08.000 --> 10:13.000
And our assumption is that if the joint distributions

10:13.280 --> 10:17.920
are close enough, then all of the marginal distributions

10:17.920 --> 10:22.560
and conditional distributions are gonna be similar, okay?

10:22.560 --> 10:23.400
Any questions?

10:25.040 --> 10:27.920
So the intuition is that we have a black box

10:27.920 --> 10:30.280
autoregressive model, and we train

10:30.280 --> 10:32.000
some sort of white box PC,

10:32.040 --> 10:37.040
so we can use as a representative of the black box, okay?

10:41.400 --> 10:45.800
So we don't want to, since this is the workshop

10:45.800 --> 10:50.800
for circuits and logic, let's represent HMMs as PCs.

10:51.280 --> 10:54.760
So here is the graphical model version of PC.

10:55.600 --> 11:00.360
The Zs here are the hidden variables, the latent variables.

11:01.360 --> 11:05.480
And the X here are the observed variables, or the words.

11:07.040 --> 11:11.360
And let me use the whiteboard to show you how to do this.

11:16.560 --> 11:21.560
Okay, so one assumption we have is that our hidden states,

11:21.720 --> 11:25.440
our latent variables, are discrete variables

11:25.440 --> 11:30.200
taking values from one to H, and H is our hidden state.

11:30.200 --> 11:31.040
Any questions?

11:32.560 --> 11:34.840
Okay, so we start from the very beginning.

11:34.840 --> 11:39.200
In an HMM, we start with the initial state, Z1,

11:39.200 --> 11:42.120
and it has H choices.

11:52.000 --> 11:57.000
So we want these eight product nodes to be representing

11:57.920 --> 12:01.440
the probability, can people see it actually?

12:01.440 --> 12:06.440
So representing the probability of X1 to N

12:06.960 --> 12:09.640
conditioning on Z1 equals to I.

12:11.960 --> 12:15.080
So I would be the hidden state, so I will,

12:15.080 --> 12:19.400
this will be one, two, and H, okay?

12:19.400 --> 12:24.400
And the edge will have weights

12:24.400 --> 12:27.320
of the prior distribution on Z1.

12:33.320 --> 12:36.120
Okay, I see nodding, I see confusing faces,

12:36.120 --> 12:37.480
but I'll move on whatever.

12:39.280 --> 12:41.960
Okay, so it'll be clear later.

12:41.960 --> 12:46.960
So given, so by the Markov property, given Z1, X1,

12:47.840 --> 12:51.600
and so basically given Z1, this part

12:51.600 --> 12:53.720
and the remaining part will be independent.

12:53.720 --> 12:56.160
So let's deal with X1 first.

13:03.880 --> 13:06.080
We have some input distributions,

13:09.720 --> 13:14.080
and this input distribution will be representing

13:14.080 --> 13:17.200
the probability, the emission probability,

13:17.200 --> 13:20.760
basically X1 given Z1 equals to I.

13:21.440 --> 13:25.480
Okay, so basically here, we're in the states

13:25.480 --> 13:28.280
of Z1 equals to I, and this will represent

13:28.280 --> 13:31.280
the probability of X1 given at Z1 equals to I.

13:34.920 --> 13:36.560
I see more nodding faces now.

13:40.480 --> 13:43.120
Okay, and we want this part to describe

13:43.120 --> 13:47.680
the remaining distribution, so we have some nodes here,

13:47.840 --> 13:52.840
and they will represent the distribution of P of X2

14:01.120 --> 14:06.120
to N given Z1 equals to I, okay?

14:08.480 --> 14:10.680
Corresponding to the inputs.

14:10.680 --> 14:14.640
And so we proceed to the next layer.

14:17.680 --> 14:22.680
And this part will be the transition probabilities,

14:29.080 --> 14:32.920
will be the transition probability of Z2 equals to J

14:32.920 --> 14:37.920
given Z1 equals to I, and these will recursively,

14:39.720 --> 14:42.320
we do this construction recursively,

14:42.320 --> 14:44.200
will be representing the probability

14:44.240 --> 14:48.240
from X2 to N, conditioning on Z2 equals to I, okay?

14:50.280 --> 14:51.120
Does that make sense?

14:51.120 --> 14:56.120
So we just proceed, so it's like Antonio and Robert

14:57.400 --> 14:59.720
was trying to make the point,

14:59.720 --> 15:03.160
this is a tensorized layer representation of a PC.

15:06.600 --> 15:11.600
Okay, let's move on, okay, so now we have our,

15:14.320 --> 15:15.140
oh, sorry.

15:18.000 --> 15:21.720
So let's move on, so we have our circuit

15:21.720 --> 15:26.440
representing the distribution over text,

15:26.440 --> 15:28.960
then we need to answer the query,

15:28.960 --> 15:31.040
we need to encode the logical constraint

15:31.040 --> 15:32.260
as the logical circuit.

15:35.600 --> 15:40.600
Let's go back to the constraint we have in the very beginning,

15:41.120 --> 15:43.180
we want Frisbee, dog and dog.

15:44.480 --> 15:47.720
Frisbee, caught and dog to appear in the given order.

15:50.280 --> 15:53.760
This is like a naive way to represent this constraint,

15:54.640 --> 15:59.640
basically the IJK here are enumerating all possible

16:02.120 --> 16:06.040
positions of these three words,

16:06.040 --> 16:11.040
and we take a conjunction whenever we know the positions.

16:11.920 --> 16:13.020
Any questions?

16:15.200 --> 16:18.800
Okay, okay, but this is not really ideal,

16:18.800 --> 16:22.800
we can directly convert it into a PC

16:22.800 --> 16:24.280
that represents the constraint,

16:24.280 --> 16:29.280
but what are the problems, do people see that?

16:37.720 --> 16:41.040
I'll give a hint, complexity,

16:41.040 --> 16:42.800
what's the complexity of this,

16:42.840 --> 16:44.480
what's the size of this DNF?

16:47.880 --> 16:50.680
Yes, cubic, cubic, and to be more precise,

16:50.680 --> 16:53.400
it's n choose number of keywords, right?

16:53.400 --> 16:55.480
Well, I mean, we can do it like cubic,

16:55.480 --> 16:57.840
but suppose we have five or 10 keywords,

16:57.840 --> 17:01.160
we can no longer take that complexity, okay?

17:01.160 --> 17:03.880
And the other problem is more subtle,

17:03.880 --> 17:08.880
which is that this DNF is not deterministic, okay?

17:09.760 --> 17:13.600
So we know that we can multiply circuits

17:13.600 --> 17:15.760
when they are compatible,

17:15.760 --> 17:18.280
or when they are structured decomposable

17:18.280 --> 17:20.320
with respect to the same vitri,

17:20.320 --> 17:22.920
but here we want exact conditioning,

17:22.920 --> 17:27.920
so we actually need to make sure that our circuit

17:29.280 --> 17:31.360
represents a uniform distribution

17:31.360 --> 17:35.120
over the support specified by this DNF.

17:35.560 --> 17:40.560
DNF, and in general, it is sharply hard

17:42.400 --> 17:45.240
to do model counting to normalize everything.

17:45.240 --> 17:48.320
Okay, I'll go into the details on the board.

17:49.280 --> 17:53.720
Yes, yes, I'll talk about it on the board.

17:59.600 --> 18:01.440
Okay, I'll move to the other side.

18:05.120 --> 18:10.120
Okay, so for the first question,

18:10.560 --> 18:13.240
why does it need to be deterministic?

18:13.240 --> 18:16.340
It is because so, okay, suppose we have a very,

18:18.200 --> 18:21.280
okay, so suppose we have a very simple distribution,

18:21.280 --> 18:24.320
X1, X2.

18:35.560 --> 18:40.560
So, we have some weights,

18:48.360 --> 18:52.160
so this is a distribution over two random variables,

18:52.160 --> 18:55.360
okay, and this is its probability mass function.

19:00.520 --> 19:02.000
So when we are conditioning,

19:02.000 --> 19:04.840
we are actually selecting all the terms

19:04.880 --> 19:06.880
that satisfy our constraint

19:06.880 --> 19:10.400
and zeroing out everything else, right?

19:10.400 --> 19:14.200
So if we want to do that with circuit multiplication,

19:15.040 --> 19:17.720
suppose our constraint, our support,

19:17.720 --> 19:22.720
would be something like X1, X2, and X1 bar, X2, okay?

19:25.840 --> 19:28.080
So the constraint circuit

19:28.520 --> 19:33.520
should be a uniform distribution over its support,

19:37.440 --> 19:42.440
otherwise it messes up with the original weights, right?

19:43.360 --> 19:44.440
Does that make sense?

19:45.600 --> 19:48.880
Okay, so however, okay, let me,

19:48.880 --> 19:53.880
so this will be 0.5, X1, X2 plus 0.5, X1 bar.

19:59.040 --> 20:01.280
Well, I mean, suppose,

20:03.480 --> 20:04.760
and we have a bunch of zeros,

20:04.760 --> 20:08.240
and we multiply these two circuits point-wise,

20:09.240 --> 20:13.280
and we kind, we keep these two terms

20:13.280 --> 20:15.440
and erase these two terms, right?

20:15.440 --> 20:19.000
But we want W and W3 to be proportional to each other,

20:19.000 --> 20:21.800
like the ratio should stay the same.

20:21.800 --> 20:24.720
So this circuit must be a uniform distribution

20:24.720 --> 20:27.640
over the support of constraint, okay?

20:27.680 --> 20:30.240
So given a logical circuit,

20:30.240 --> 20:32.440
how do we convert it into a PC

20:32.440 --> 20:36.280
that represents a uniform distribution over the support?

20:37.480 --> 20:40.440
To do that, we need to do model counting,

20:40.440 --> 20:43.040
but model counting in general is hard

20:43.040 --> 20:44.720
if determinism is missing.

20:46.240 --> 20:49.760
So this one won't work.

20:52.200 --> 20:53.520
Does that make sense?

20:55.640 --> 20:57.200
Yes, no?

20:57.200 --> 20:58.040
Robert?

20:59.640 --> 21:02.040
Isn't that full of a new mistake?

21:02.040 --> 21:04.640
Oh yeah, so it's a very subtle, subtle thing.

21:04.640 --> 21:09.640
So, well, we only require Frisbee, Caught, and Dog

21:10.240 --> 21:11.920
to appear in some positions,

21:11.920 --> 21:16.760
but we do not limit that they have to appear exactly once.

21:16.760 --> 21:21.760
So you can totally have Frisbee, Frisbee, Caught,

21:23.120 --> 21:26.320
Caught, Dog, Dog, something like this.

21:27.560 --> 21:29.600
So we have two sub-sequences.

21:30.400 --> 21:32.680
We have a lot of sub-sequences, right?

21:34.320 --> 21:39.320
So this is position one, two, three, four, five, six.

21:41.160 --> 21:45.320
So when IJK are one, three, five,

21:46.240 --> 21:49.240
it satisfies the clauses, right?

21:49.240 --> 21:50.760
One of them, right?

21:57.200 --> 21:59.920
Let me choose a different way.

21:59.920 --> 22:04.800
Okay, so basically for this huge disjunction,

22:04.800 --> 22:07.440
for each instantiation to the variable,

22:07.440 --> 22:09.720
we only satisfy one of the clauses, right?

22:09.720 --> 22:11.320
We want, that's determinism.

22:13.280 --> 22:14.480
Yes.

22:14.480 --> 22:19.480
And suppose IJK are one, three, five.

22:20.240 --> 22:22.280
Does this instance satisfy that?

22:22.280 --> 22:23.400
Yes, it does, right?

22:24.400 --> 22:28.040
But when IJK are one, four, five,

22:28.040 --> 22:29.560
it also satisfies that.

22:29.560 --> 22:30.400
So it's not.

22:31.360 --> 22:33.120
Does that make sense?

22:33.120 --> 22:33.960
Okay.

22:33.960 --> 22:35.720
Okay, so now let's construct

22:35.720 --> 22:38.320
a deterministic circuit representing this constraint.

22:40.520 --> 22:45.520
The idea is similar to a deterministic finite automata.

22:46.640 --> 22:51.440
Basically, you track how many words have you included,

22:51.440 --> 22:52.600
how many words have you used,

22:52.600 --> 22:55.480
which state are you in satisfying the constraint?

22:56.720 --> 22:59.520
So we want to construct these sub formulas,

22:59.520 --> 23:03.520
V, T, say cod and dog.

23:07.720 --> 23:10.560
The sub formula representing the constraint

23:10.560 --> 23:15.560
that cod and dog appears

23:16.560 --> 23:21.560
in XT, XT plus one.

23:27.400 --> 23:31.640
So how do we construct this sub circuit or this sub formula?

23:31.640 --> 23:32.680
Oh, it's pretty simple.

23:32.680 --> 23:34.680
It's a sum node.

23:34.680 --> 23:36.640
We consider two different cases.

23:37.800 --> 23:41.480
One of the cases that XT is cod.

23:42.320 --> 23:47.320
And the other case is that XT is not cod.

23:52.840 --> 23:54.640
Does this make sense?

23:54.640 --> 23:55.480
Okay.

23:55.480 --> 24:00.280
So if XT is cod, then we have like step one.

24:02.680 --> 24:05.400
We have made one step towards satisfying the constraint.

24:07.480 --> 24:11.120
So we can reduce it to V, T plus one.

24:12.080 --> 24:12.920
Dog.

24:14.400 --> 24:16.400
And suppose XT is not cod.

24:16.400 --> 24:17.720
We're in the same state.

24:19.000 --> 24:21.040
We reduce it to T plus one.

24:24.760 --> 24:25.600
Dog.

24:28.560 --> 24:29.400
Is that clear?

24:30.280 --> 24:33.720
So suppose we have constructed FI

24:33.720 --> 24:38.720
for all time step greater than T

24:39.560 --> 24:42.800
then we can construct all the fees for T.

24:45.800 --> 24:47.680
So it's a recursive algorithm.

24:51.400 --> 24:52.520
Oh, what's the base case?

24:52.520 --> 24:54.120
Yes, that's very interesting.

24:58.240 --> 24:59.680
Okay, I'm gonna erase this.

25:03.120 --> 25:06.520
So just cod and dog in the part.

25:06.520 --> 25:10.320
So yeah, I mean, we can make it even simpler.

25:10.320 --> 25:12.320
I'm lazy, so I wanna make things simpler.

25:14.160 --> 25:16.440
So suppose in the nth position,

25:18.160 --> 25:20.520
we only have one word, right?

25:21.760 --> 25:23.760
So fee and,

25:30.240 --> 25:32.120
what is this formula?

25:32.120 --> 25:37.120
It means that from Xn to Xn,

25:38.240 --> 25:40.960
we need to have cod and dog.

25:40.960 --> 25:43.520
So this is false, right?

25:43.520 --> 25:44.360
Zero.

25:47.360 --> 25:49.280
What if we only have dog?

25:49.280 --> 25:54.280
So this is true only if Xn equals dog.

25:57.520 --> 25:59.640
So yeah, those are the base cases.

26:00.480 --> 26:05.480
Okay, so what's the size of the circuit?

26:07.040 --> 26:08.760
So at each time step,

26:10.400 --> 26:15.400
we need fee T, maybe we have already satisfied the constraint.

26:15.400 --> 26:18.880
So something empty, right?

26:18.880 --> 26:23.880
We need fee T, we have one word left.

26:24.880 --> 26:29.240
Fee T, we have cod and dog, we have two word left.

26:30.840 --> 26:34.200
And we have generated none of them yet.

26:38.520 --> 26:41.800
Okay, so at each time step,

26:41.800 --> 26:46.800
we have four sum notes or four more notes

26:47.320 --> 26:49.440
representing all these four cases.

26:50.400 --> 26:54.680
And the size, the number of notes in the circuit

26:54.680 --> 26:55.920
would be four times.

26:57.760 --> 26:59.240
Okay, does that make sense?

27:02.240 --> 27:05.440
And you can notice that when we are constructing this circuit,

27:05.440 --> 27:08.280
we are always conditioning on XT,

27:08.280 --> 27:10.680
current variable, which is very similar to HMM.

27:10.680 --> 27:14.240
It generates one variable at a time, one word at a time.

27:14.240 --> 27:16.600
So it is compatible with HMM.

27:16.600 --> 27:18.280
And also it's deterministic.

27:20.320 --> 27:23.160
And we can do the apply operation,

27:23.160 --> 27:25.400
the product operation layer-wise.

27:25.400 --> 27:28.280
So the size of the eventual circuit

27:29.760 --> 27:33.560
would be four, so originally,

27:33.560 --> 27:38.560
so originally, here for each layer, we have H nodes.

27:39.600 --> 27:43.040
Now we, here for each layer, we have four nodes.

27:43.040 --> 27:46.600
So eventually we have four H nodes just for each layer.

27:47.600 --> 27:51.600
Acceptable, okay, does that make sense?

27:53.280 --> 27:55.360
So we can still generate this.

27:56.360 --> 27:58.480
We can generate this even further.

27:58.480 --> 28:02.000
Well, some time, well, one simple generalization is,

28:02.880 --> 28:06.560
you might think it doesn't have to be caught, right?

28:06.560 --> 28:10.520
It can be caught or either catch, right?

28:10.520 --> 28:15.520
Or catches, they all kind of represent the same concept.

28:17.120 --> 28:20.560
So in the construction, we can

28:23.840 --> 28:26.560
modify our original circuit a little bit.

28:27.520 --> 28:31.560
Before it was XT equals caught,

28:33.160 --> 28:36.120
we can replace it with another OR node,

28:38.560 --> 28:41.720
enumerating caught, catch, and catches.

28:42.320 --> 28:46.520
Okay, and the circuit size stays roughly the same.

28:48.080 --> 28:49.920
And there are many other things we can do.

28:49.920 --> 28:52.600
We can also say, oh, we don't need them

28:52.600 --> 28:55.160
to be in the same order, in a fixed order.

28:55.160 --> 28:58.440
We can, we want them to be like in arbitrary order.

28:58.440 --> 28:59.640
We don't care about the order.

28:59.640 --> 29:01.400
How do we do that?

29:01.400 --> 29:04.640
So in this case, we have four states, right?

29:04.640 --> 29:09.640
Basically enumerating all suffixes of these three keywords.

29:10.640 --> 29:15.040
In that case, we would have all subsets of these three keywords.

29:15.040 --> 29:20.040
So in that way, the complexity would be going from four,

29:21.040 --> 29:25.400
we have three keywords, and this is four, two, two to the three.

29:25.400 --> 29:29.400
That's there, eight subsets, two to the three subsets, okay?

29:29.400 --> 29:34.400
So still acceptable, two to the n is not really,

29:36.040 --> 29:38.880
say we have 10 keywords, two to the n, it's just 1024.

29:39.160 --> 29:41.720
It's still doable in practice, yes.

29:41.720 --> 29:45.720
So it seems that you can do any kind of regular expression

29:45.720 --> 29:50.200
constrained, but you're focusing here on language

29:50.200 --> 29:52.080
as a fixed length, right?

29:52.080 --> 29:57.080
Because you have a fixed number of random variables.

29:59.000 --> 30:03.400
So is that the kind of equivalent of what you're saying?

30:03.400 --> 30:05.360
Yes, yes, that's a very, very good point.

30:05.360 --> 30:09.680
So fixed length here is a very essential assumption.

30:09.680 --> 30:14.680
So suppose we have a distribution over language of arbitrary length,

30:16.560 --> 30:21.040
then applying the constraint could be hard to define.

30:22.160 --> 30:24.720
So basically these three words,

30:24.720 --> 30:29.760
they can appear in arbitrarily far positions,

30:29.760 --> 30:32.560
and we need probably you take like an infinite sum

30:32.560 --> 30:34.560
to define this conditional probability.

30:36.080 --> 30:39.880
But this is not terrible in practice,

30:39.880 --> 30:43.120
because in practice, even for models like GPT,

30:43.120 --> 30:46.400
they have a finite sequence length, yeah,

30:46.400 --> 30:51.400
that's a very good point, okay, I'm trying to be fast.

30:55.080 --> 30:57.560
Oh, I mean, there are variations of constraints

30:57.560 --> 31:02.440
we have talked about, and okay,

31:02.440 --> 31:04.800
so now we have our,

31:07.960 --> 31:12.240
sorry, now we have our probabilistic circuit

31:12.240 --> 31:14.200
representing the distribution,

31:14.200 --> 31:16.480
and we have our constraint circuit,

31:16.480 --> 31:18.520
and we'll close them to take the product.

31:20.120 --> 31:21.720
So we have our constraint circuit now,

31:21.720 --> 31:24.080
we can compute the probabilities we need.

31:24.080 --> 31:26.400
So the step two is very simple.

31:26.400 --> 31:30.680
So this is the original like base decomposition

31:30.680 --> 31:33.280
of the conditional probability we have.

31:33.280 --> 31:36.080
So this term here is intractable,

31:36.920 --> 31:40.560
so we secretly replace the subscript of LM with HMM,

31:40.560 --> 31:45.560
the one we notice, and we define this conditional distribution,

31:47.360 --> 31:51.960
the gelato distribution, and we can compute this

31:51.960 --> 31:55.480
with the linear pass of the circuit.

31:55.840 --> 32:01.480
Okay, so what are the advantages?

32:02.480 --> 32:04.800
So number one, by definition,

32:04.800 --> 32:07.240
the constraint alpha is guaranteed to be satisfied,

32:07.240 --> 32:11.160
so finally, other than compared to all the other methods,

32:11.160 --> 32:15.280
we have 100% reliable thing we can trust,

32:15.280 --> 32:18.280
and number two is that the training of this HMM

32:20.280 --> 32:22.120
does not depend on the constraint.

32:22.120 --> 32:25.200
So basically once we have this HMM trained,

32:25.200 --> 32:29.240
we can use it to enforce whatever constraint we want.

32:29.240 --> 32:32.160
So maybe today I want to write something

32:32.160 --> 32:34.920
using some keywords and key concepts,

32:34.920 --> 32:37.080
and tomorrow I feel like this language model

32:37.080 --> 32:41.720
is like using a lot of inappropriate languages,

32:41.720 --> 32:45.440
and we can detoxify it by specifying a list of bad words

32:45.440 --> 32:50.120
that it cannot use, and all the same model,

32:50.120 --> 32:53.600
and all the constraints are only enforced at inference time.

32:54.600 --> 32:55.600
Yes?

32:55.600 --> 32:57.520
What kind of constraints can be represented

32:57.520 --> 33:01.360
as the composable pieces?

33:01.360 --> 33:03.840
Yes, that's a very, very good question.

33:03.840 --> 33:06.000
That's actually one of the main reason

33:06.000 --> 33:08.200
in presenting this work here,

33:08.200 --> 33:11.640
because my feeling is that though people have been studying

33:11.640 --> 33:16.000
like probabilistic queries like marginal map, marginal,

33:18.240 --> 33:21.720
and all these kinds of stuff extensively,

33:21.720 --> 33:25.560
but in practice, we really care

33:25.560 --> 33:29.280
about some more complicated, less generic ones,

33:29.280 --> 33:32.360
and whether is there a language to define

33:32.360 --> 33:34.160
or to describe their tractability

33:34.160 --> 33:37.520
is kind of missing from the literature.

33:37.520 --> 33:40.720
But I could relate this to say

33:40.720 --> 33:45.720
there's some work on compiling DFAs to circuits.

33:46.720 --> 33:50.400
I think that could be something as a starting point

33:50.400 --> 33:51.360
to look at.

33:51.400 --> 33:52.920
I'm not sure if that answers your question.

33:52.920 --> 33:53.920
I don't have an answer.

33:53.920 --> 33:56.240
My answer is, okay.

33:57.840 --> 34:00.120
Okay, so experiments and benchmarks.

34:01.160 --> 34:06.040
So we evaluate our method on this common sense generation,

34:06.040 --> 34:07.360
common gem benchmark.

34:07.360 --> 34:10.080
Well, it's very similar to the example I gave you.

34:10.080 --> 34:12.280
It gave you a bunch of keywords,

34:12.280 --> 34:17.280
and each example comes with a bunch of gold sentences,

34:17.600 --> 34:20.360
and you want to generate something

34:20.360 --> 34:25.040
that looks similar to the gold sentences using keywords.

34:26.000 --> 34:29.600
And here, it's like the most general case.

34:29.600 --> 34:31.440
So basically these keywords,

34:31.440 --> 34:32.920
they can appear in any order,

34:32.920 --> 34:37.160
and they can appear in any form of their inflections.

34:38.720 --> 34:42.480
Okay, so this is the, yes, then.

34:43.520 --> 34:46.520
And it's a gigantic table.

34:46.520 --> 34:48.920
The numbers themselves are not that important.

34:51.080 --> 34:53.080
So one thing to note that is compared

34:53.080 --> 34:54.880
to all the other baselines,

34:56.040 --> 35:00.760
our method, gelato, achieves a state-of-the-art performance

35:00.760 --> 35:04.360
with respect to basically all metrics.

35:04.360 --> 35:08.200
So these metrics here, Rouge L, Blue Four Ciders,

35:08.200 --> 35:12.320
Spice State, there are just some standard NLP metrics

35:12.320 --> 35:17.320
that people use to evaluate the quality of your text.

35:17.600 --> 35:19.920
So basically you have a gold sentence,

35:19.920 --> 35:22.360
and you have your generated sentence.

35:22.360 --> 35:25.720
They compute somehow the NBREM overlap

35:25.720 --> 35:27.800
to measure the quality.

35:31.640 --> 35:35.440
But the other thing is that all of the previous method

35:37.440 --> 35:42.160
cannot achieve 100% constraint satisfiability,

35:42.160 --> 35:45.200
but ours does in practice as well.

35:45.200 --> 35:46.720
Well, there is this one baseline,

35:46.720 --> 35:49.600
they achieve 100% accuracy as well,

35:49.600 --> 35:54.600
but they kind of did it by starting from the keywords.

35:54.840 --> 35:57.400
So they're always gonna be there.

35:58.360 --> 36:01.240
And you can see their generation quality

36:01.240 --> 36:02.600
is really poor, so.

36:07.840 --> 36:11.440
We also conducted some sort of human evaluation.

36:14.040 --> 36:15.280
Okay, I guess, yes?

36:15.280 --> 36:17.280
Which language model does that look like?

36:17.280 --> 36:22.280
Oh, the language model, we use GPT-2, GPT-2 large.

36:23.160 --> 36:25.000
Yeah, and all of the baseline state,

36:25.000 --> 36:26.400
they use GPT-2 large, yeah.

36:28.960 --> 36:31.440
And in case people don't really trust

36:31.440 --> 36:33.520
these automatic evaluation metrics,

36:33.520 --> 36:35.720
we also conducted human evaluation.

36:38.600 --> 36:41.200
And you can see that our model performs

36:42.200 --> 36:44.400
much better than previous state-of-the-art.

36:45.400 --> 36:47.520
Okay, well, are they very significant?

36:47.520 --> 36:49.520
I mean, they're pretty close.

36:49.520 --> 36:52.640
Yeah, so, yeah, so basically the,

36:54.320 --> 36:56.080
I'm not sure if you can see it clearly,

36:56.080 --> 36:59.240
but the bold-faced ones are statistically

37:00.560 --> 37:04.520
significantly, what equivalent?

37:04.520 --> 37:09.520
So these ones, these two are statistically equivalent.

37:09.840 --> 37:12.580
This one is statistically significant.

37:13.580 --> 37:15.420
And we're looking at, when defibrating,

37:15.420 --> 37:17.620
what's the number here?

37:17.620 --> 37:20.660
Oh, yeah, so basically we provide the annotator

37:20.660 --> 37:23.620
some sentence and we provide description

37:23.620 --> 37:26.940
of one of each of the aspects.

37:26.940 --> 37:29.420
So basically, concept means that does it use

37:29.420 --> 37:31.540
all the concepts naturally?

37:31.540 --> 37:34.300
And plausibility means that is the sentence

37:34.300 --> 37:38.180
like a plausible sentence describing a realistic scene.

37:38.180 --> 37:41.920
And quality is basically fluency, grammar, and stuff.

37:41.960 --> 37:44.040
Overall is the like another,

37:44.040 --> 37:45.800
how do you feel about the sentence?

37:45.800 --> 37:48.400
And the numbers are from one, two, three.

37:48.400 --> 37:50.520
One, two, three, go, yeah.

37:50.520 --> 37:51.360
Okay.

37:53.720 --> 37:57.800
Okay, so let's get back to the very first

37:57.800 --> 38:01.240
motivating example and you can see that gelato,

38:01.240 --> 38:03.860
we use our model to add, and it is actually able

38:03.860 --> 38:07.020
to get everything correct and generate a fluent sentence.

38:07.860 --> 38:12.860
And another, I don't, okay, so we also found this one

38:16.540 --> 38:18.540
in one of the generated candidates.

38:18.540 --> 38:21.580
A pair of Frisbee players are caught in a dog fight,

38:21.580 --> 38:26.580
which is not like the thing that most people

38:26.820 --> 38:27.740
would like to think of.

38:27.740 --> 38:32.740
So it also shows some sort of creativity here.

38:32.860 --> 38:36.780
Here, okay, that's my talk.

38:38.060 --> 38:40.460
Please ask questions if you have,

38:40.460 --> 38:42.620
and otherwise we can go to lunch.

38:42.620 --> 38:43.460
Thank you.

38:43.460 --> 38:44.300
Thank you.

38:47.300 --> 38:48.300
One more question.

38:50.860 --> 38:52.700
Thanks for your talk, if I understand right,

38:52.700 --> 38:55.540
you said that to generate every word,

38:55.540 --> 38:58.740
you have to do a linear pass over the circuit.

38:58.740 --> 39:02.100
So how much is this overhead compared to the kind

39:02.260 --> 39:03.780
of latency from the language model?

39:03.780 --> 39:06.180
Okay, I like that question.

39:06.180 --> 39:10.780
So we don't really need to take a pass over the whole circuit.

39:10.780 --> 39:14.280
With some caching, we can do like constant time.

39:15.500 --> 39:17.620
So basically to generate each word,

39:17.620 --> 39:19.340
the cost is like constant time.

39:19.340 --> 39:21.140
It's like a pass through one layer.

39:22.300 --> 39:23.300
Is it in any practice?

39:23.300 --> 39:24.140
How fast is it?

39:24.140 --> 39:25.500
Oh, how fast is it?

39:25.500 --> 39:26.900
We actually, so.

39:33.100 --> 39:35.860
I don't have a table here,

39:35.860 --> 39:37.460
but we have a table in the paper.

39:37.460 --> 39:42.460
So if say generating a sentence with five keywords,

39:43.460 --> 39:48.460
a GPT-2 large would take around 20 seconds,

39:50.020 --> 39:54.500
and our method is like 100-ish seconds.

39:54.500 --> 39:57.140
So it's not terrible.

39:57.140 --> 39:59.660
And one of the baselines here,

39:59.660 --> 40:03.020
well, which was actually like the best paper award

40:03.020 --> 40:05.140
at one of the top NLP conferences,

40:05.140 --> 40:07.060
they use search-based method.

40:07.060 --> 40:09.260
And for them to generate a sentence,

40:09.260 --> 40:13.100
they take like 700 seconds, 800 seconds.

40:13.100 --> 40:15.740
So because they're search-based,

40:15.740 --> 40:18.340
so when a search-based gets large,

40:21.740 --> 40:23.540
their method shows like a bottleneck.

40:23.620 --> 40:25.460
That's pretty well.

40:25.460 --> 40:26.300
Thanks.

40:28.100 --> 40:31.300
Can you also give an idea about the time required

40:31.300 --> 40:33.620
to derive the hidden Markov model?

40:33.620 --> 40:36.300
Oh, yeah, so I do have the time for that.

40:36.300 --> 40:40.820
So our hidden Markov model has like 4,000 states,

40:40.820 --> 40:43.340
and the emission is 50,000.

40:43.340 --> 40:48.020
We trained them with the juice framework

40:48.020 --> 40:49.500
that we developed in our lab.

40:50.340 --> 40:55.340
The training takes like 20 hours.

40:55.940 --> 40:59.860
So it's, we sample about 200,

41:02.940 --> 41:06.660
we sampled eight million sentences from GPT,

41:06.660 --> 41:11.060
and we trained them for 40 epochs, 20 hours.

41:16.140 --> 41:18.020
What's the number of parameters?

41:18.020 --> 41:19.060
Number of parameters?

41:20.500 --> 41:21.820
Of HMM.

41:21.820 --> 41:26.820
Yeah, so HMM is 4,000 hidden states and 50,000 emissions.

41:28.180 --> 41:31.820
So the main, mainly that the parameters are centered

41:31.820 --> 41:33.420
on the emission table.

41:33.420 --> 41:37.500
So it's like 40,000, 4,000 times 50,000.

41:37.500 --> 41:39.700
I didn't do that in my head.

41:39.700 --> 41:41.380
So those are unique parameters, right?

41:41.380 --> 41:42.620
Yes, yeah.

41:42.620 --> 41:46.020
Yes, so in the PC, yes, you kind of like,

41:46.020 --> 41:48.660
you've rolled out all the parameters.

41:49.620 --> 41:50.620
All the positions.

41:53.180 --> 41:56.140
Controlable generation is huge, right?

41:56.140 --> 41:57.580
So this is great.

41:57.580 --> 41:59.780
What, I mean, and I probably understand you,

41:59.780 --> 42:01.540
you gave a logical perspective for this audience,

42:01.540 --> 42:04.100
but I mean, a reg acts as a much more natural

42:04.100 --> 42:06.540
sort of control structure.

42:06.540 --> 42:09.460
So presumably you can handle any of our complication

42:09.460 --> 42:11.340
given the DFA interpretation, right?

42:11.340 --> 42:12.420
Yes, yes.

42:12.420 --> 42:15.060
I think that's a very important follow up.

42:15.060 --> 42:17.780
We are kind of looking at considering like,

42:17.820 --> 42:20.580
compiling DFA's to circuits

42:20.580 --> 42:24.580
and kind of automate all these process.

42:24.580 --> 42:26.700
Yeah, so ACLs in January, you submit there.

42:26.700 --> 42:28.260
I mean, I think they'll love it.

42:29.180 --> 42:31.980
Your initial example, you wanted a winter tree,

42:31.980 --> 42:35.780
you wanted the presence of winter to select for warm, right?

42:35.780 --> 42:37.980
Which is more of a natural language entailment.

42:37.980 --> 42:40.060
And I don't think you handle that, right?

42:40.060 --> 42:42.540
You handle variations and factual variations

42:42.540 --> 42:43.380
because you actually code them.

42:43.380 --> 42:48.300
I think we can totally do that winter example.

42:48.300 --> 42:52.580
So basically, let me go back.

42:53.580 --> 42:56.220
It seems like you encoded the specific variations

42:56.220 --> 42:57.580
that you were allowing, right?

42:57.580 --> 43:01.780
Oh, you mean like, so we can have winter, winters,

43:01.780 --> 43:04.620
but like, maybe if the winter,

43:04.620 --> 43:06.460
the word is not explicitly mentioned,

43:06.460 --> 43:07.780
you cannot do that.

43:07.780 --> 43:09.660
That's what I understand from your formalism, right?

43:09.660 --> 43:11.900
If it's in your or, you'll generate it.

43:11.900 --> 43:15.580
If it's not, you won't, you won't capture the entailment.

43:15.580 --> 43:19.300
No, I mean, so basically, so that's the other thing.

43:19.300 --> 43:23.220
So sometimes people, or most of the time,

43:23.220 --> 43:26.380
people want to have like kind of soft control.

43:26.380 --> 43:31.460
They can't really write their constraint in logical form.

43:31.460 --> 43:36.380
For example, toxic, like how can you tell a sentence is toxic?

43:36.380 --> 43:38.780
But there are many ways to approximate it.

43:38.780 --> 43:42.860
One of the ways is we have like a long list of phrases or words

43:42.860 --> 43:46.460
that you're now allowed to use in a toxic sentence

43:46.460 --> 43:49.740
and we can basically just write down a logical formula

43:49.740 --> 43:53.620
approximating that toxicity constraint.

43:53.620 --> 43:55.860
Yeah, I mean, there's also plug-and-play generation tricks

43:55.860 --> 43:58.460
that are actually quite interesting, I think that...

43:58.460 --> 44:04.580
So basically, there is a, of course, there's a naive way to do it.

44:04.580 --> 44:07.860
You can say whenever I encounter one of the words in the list,

44:07.900 --> 44:11.660
I just remote, like, prevent it from generating,

44:11.660 --> 44:14.300
prevent it from being sampled.

44:14.300 --> 44:18.580
But that's not exactly what we're trying to do probabilistically, right?

44:18.580 --> 44:19.300
So...

44:19.300 --> 44:22.300
Yeah, but I just, maybe look into plug-and-play,

44:22.300 --> 44:24.420
control generation is actually a paper.

44:24.420 --> 44:25.460
Yes, yeah.

44:25.460 --> 44:28.420
So where I think they're doing much more than that, right?

44:28.420 --> 44:31.100
Well, so, yeah, so it's like...

44:31.100 --> 44:34.580
They're also modifying the posterior selection.

44:34.580 --> 44:39.300
So if I'm not wrong, if I remember this correctly,

44:39.300 --> 44:42.940
they're basically trying to train some sort of...

44:47.020 --> 44:48.860
They use a classifier, right?

44:48.860 --> 44:51.220
Yeah, so basically, kind of, they're trying to train

44:51.220 --> 44:53.500
a newer model to approximate this.

44:53.500 --> 44:54.740
Yeah.

44:54.740 --> 44:56.260
So, but they're...

44:56.260 --> 45:00.100
So, yeah, so their methods has like two disadvantages

45:00.100 --> 45:02.220
compared to our advantages.

45:02.260 --> 45:06.020
So one of them is that they cannot guarantee that this is...

45:06.020 --> 45:06.660
Right.

45:06.660 --> 45:10.460
100% logically satisfiable.

45:10.460 --> 45:13.740
And the other is that they have to retrain their model

45:13.740 --> 45:15.660
for all different constraints.

45:15.660 --> 45:17.300
Their model for all...

45:17.300 --> 45:18.540
Yes, okay, but...

45:18.540 --> 45:21.300
Okay, so suppose in their training data,

45:21.300 --> 45:23.860
they're only trained to kind of satisfy...

45:23.860 --> 45:28.380
They only have seen using, say, less than 10 keywords

45:28.380 --> 45:30.660
to generate a sentence, right?

45:30.820 --> 45:32.980
What if today I want to use 20 keywords?

45:32.980 --> 45:35.300
They would not be able to generalize well to that one.

45:35.300 --> 45:37.580
Right, so they're examples of toxicity, right?

45:37.580 --> 45:40.140
So they pretrain for toxicity and they can use that anywhere.

45:40.140 --> 45:43.180
So there's some tasks that you're just going to use repeatedly.

45:43.180 --> 45:45.340
I guess I see a combination of what you're doing

45:45.340 --> 45:46.620
and what they're doing together,

45:46.620 --> 45:49.100
which gives you this sort of entailment during the...

45:49.100 --> 45:51.580
Like this broader sense of satisfaction, right?

45:51.580 --> 45:54.340
Which I think was an interesting motivation.

45:54.340 --> 45:57.260
You didn't quite deliver on, but you could deliver on.

45:57.260 --> 45:59.500
That's actually the current project we're working on.

45:59.500 --> 46:01.460
So we're trying to combine the models

46:01.460 --> 46:04.780
that can handle soft constraints with our model, too.

46:04.780 --> 46:06.260
Okay, beautiful.

46:06.260 --> 46:08.940
And finally, the reason large types of models work so well

46:08.940 --> 46:10.940
is just because of attention, right?

46:10.940 --> 46:13.620
And so when you're using a hidden marker model, right,

46:13.620 --> 46:15.860
you're losing the power of attention.

46:15.860 --> 46:19.460
But I guess I'm looking at this and trying to convince myself

46:19.460 --> 46:21.940
that, well, you get all the attention in the right-hand part

46:21.940 --> 46:24.500
and then you just need a little bit of bias of selection

46:24.500 --> 46:28.020
in the left-hand part here, and that's what's happening.

46:28.020 --> 46:30.700
Yeah, so basically, intuitively, you can think of this part

46:30.700 --> 46:33.540
as only providing, like, guide and suggestions,

46:33.540 --> 46:38.540
leading the model, leading GPT to satisfy the constraint.

46:38.620 --> 46:40.620
But on this part, it's kind of responsible

46:40.620 --> 46:42.980
for fluency, grammar, and everything.

46:46.420 --> 46:47.540
Beautiful work.

46:47.540 --> 46:48.380
Thank you.

46:50.540 --> 46:51.380
All right, thanks.

46:51.380 --> 46:54.660
Maybe we should wrap up, and thanks again.

46:55.660 --> 47:01.300
And we can have lunch with the database folks

47:01.300 --> 47:03.300
and talk about generating SQL query.

