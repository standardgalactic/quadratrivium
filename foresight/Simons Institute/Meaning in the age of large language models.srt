1
00:00:00,000 --> 00:00:05,000
And we are going to see connections of language models

2
00:00:05,180 --> 00:00:08,080
that maybe you did not quite expect to anticipate,

3
00:00:08,080 --> 00:00:09,080
but they are very real.

4
00:00:09,080 --> 00:00:11,920
And to start with, we are very lucky to have

5
00:00:11,920 --> 00:00:15,880
Steven Pantadosi, he is a professor of psychology

6
00:00:15,880 --> 00:00:17,520
and neuroscience at UC Berkeley.

7
00:00:17,520 --> 00:00:22,520
And a friend of AI, we actually have a grant together.

8
00:00:23,480 --> 00:00:28,480
And a lot of us in AI are excited and interested

9
00:00:29,480 --> 00:00:32,080
in learning from the psychologists

10
00:00:32,080 --> 00:00:35,200
and seeing how they can inspire our work.

11
00:00:35,200 --> 00:00:37,320
Well, actually, this is a two-way street

12
00:00:37,320 --> 00:00:40,320
and because the psychologists are also getting excited

13
00:00:40,320 --> 00:00:43,280
about the language models to understand

14
00:00:43,280 --> 00:00:44,280
something about humans.

15
00:00:44,280 --> 00:00:47,840
And Steven will tell us about some of the work

16
00:00:47,840 --> 00:00:49,800
he has been doing, which I think is very exciting.

17
00:00:49,800 --> 00:00:50,640
So, Steven.

18
00:00:52,720 --> 00:00:55,840
So thank you for the invitation to speak here.

19
00:00:55,840 --> 00:00:57,560
I'm going to be talking about the meaning

20
00:00:57,560 --> 00:00:59,040
in the age of large language models

21
00:00:59,040 --> 00:01:00,640
and maybe finding meaning in the age

22
00:01:00,640 --> 00:01:01,480
of large language models.

23
00:01:01,480 --> 00:01:04,880
And this talk isn't a kind of technical talk

24
00:01:04,880 --> 00:01:08,640
about language models or evaluation or anything.

25
00:01:08,640 --> 00:01:12,040
It's almost closer to philosophy

26
00:01:12,040 --> 00:01:14,360
or closer to kind of high level theories

27
00:01:14,360 --> 00:01:17,200
in cognitive science and psychology,

28
00:01:17,200 --> 00:01:18,880
which are about meaning.

29
00:01:18,880 --> 00:01:20,680
And some of the kind of history of work

30
00:01:20,680 --> 00:01:24,280
of people's ideas about where meaning comes from

31
00:01:24,280 --> 00:01:26,720
in language or in semantics

32
00:01:26,720 --> 00:01:27,800
and how we can think about those

33
00:01:27,800 --> 00:01:29,480
in the context of large language models

34
00:01:29,480 --> 00:01:31,640
and in particular in the context of debates

35
00:01:31,640 --> 00:01:33,000
about large language models

36
00:01:33,000 --> 00:01:36,240
and whether they're just stochastic parrots

37
00:01:36,240 --> 00:01:38,920
that don't have any form of understanding

38
00:01:38,920 --> 00:01:41,360
or whether there's some sense in which they have understanding

39
00:01:41,360 --> 00:01:44,200
and if they do have some form of understanding,

40
00:01:44,200 --> 00:01:46,800
if that form of understanding is at all related

41
00:01:46,800 --> 00:01:50,800
to the types of understanding that people have.

42
00:01:50,800 --> 00:01:54,680
So my plan for the talk is first to talk about meaning

43
00:01:54,680 --> 00:01:56,120
and reference kind of generally.

44
00:01:56,120 --> 00:02:00,080
I'll talk about some claims in large language models

45
00:02:00,080 --> 00:02:02,840
and why people often think that there's no sense

46
00:02:02,840 --> 00:02:06,640
of meaning or kind of real semantics in them.

47
00:02:06,640 --> 00:02:10,840
Then I'll talk about some psychological theories

48
00:02:10,840 --> 00:02:12,760
about meaning and how meaning arises

49
00:02:12,760 --> 00:02:15,120
from what are called conceptual roles.

50
00:02:16,000 --> 00:02:17,400
I'll come back to large language models

51
00:02:17,400 --> 00:02:21,200
and talk a little bit about learning conceptual roles

52
00:02:21,200 --> 00:02:22,360
in large language models

53
00:02:22,360 --> 00:02:24,840
or in kind of general machine learning systems.

54
00:02:25,080 --> 00:02:27,560
And then some very kind of brief overview

55
00:02:27,560 --> 00:02:31,920
of a learning conceptual role experiment in people.

56
00:02:31,920 --> 00:02:36,800
So let me start with kind of how I got interested in this

57
00:02:36,800 --> 00:02:40,320
which was this paper by Bender and Kohler in 2020.

58
00:02:40,320 --> 00:02:43,680
Emily Bender is a computer scientist

59
00:02:43,680 --> 00:02:47,560
trained as a linguist also at the University of Washington

60
00:02:47,560 --> 00:02:49,760
who people may know as a very vocal critic

61
00:02:49,760 --> 00:02:53,480
of many aspects of large language models.

62
00:02:53,520 --> 00:02:56,120
The one that initially I think interested me

63
00:02:56,120 --> 00:02:59,720
was claims about the meaningfulness of large language models

64
00:02:59,720 --> 00:03:03,600
and essentially arguments that there's nothing meaningful

65
00:03:03,600 --> 00:03:06,760
at all in what statistical models

66
00:03:06,760 --> 00:03:09,000
that are trained on text can do.

67
00:03:10,080 --> 00:03:12,600
And Bender and Kohler came up with a very nice way

68
00:03:12,600 --> 00:03:17,600
of making this point, what they call the octopus test.

69
00:03:17,600 --> 00:03:19,560
The octopus test goes as follows.

70
00:03:19,560 --> 00:03:22,440
So kind of starting point is for them,

71
00:03:22,440 --> 00:03:26,000
meaning is an association between a word, say,

72
00:03:26,000 --> 00:03:28,000
and something external to language.

73
00:03:28,000 --> 00:03:30,120
Okay, so the meaning of the book

74
00:03:30,120 --> 00:03:34,400
is some physical object, a book that's out in the world.

75
00:03:34,400 --> 00:03:35,400
You can see that that's right

76
00:03:35,400 --> 00:03:37,920
because that's how we label physical objects

77
00:03:37,920 --> 00:03:39,320
and when we're learning words, right?

78
00:03:39,320 --> 00:03:41,400
We hear the word book when there's books around

79
00:03:41,400 --> 00:03:42,960
and we pick up on that association.

80
00:03:42,960 --> 00:03:44,560
And so that's kind of fundamentally

81
00:03:44,560 --> 00:03:47,920
what the, that kind of external reference, right?

82
00:03:47,920 --> 00:03:50,680
To something in the real world is fundamentally

83
00:03:51,560 --> 00:03:52,800
what the word means.

84
00:03:54,120 --> 00:03:55,640
And Bender and Kohler say, okay,

85
00:03:55,640 --> 00:03:58,520
let's take that as our definition of meaning

86
00:03:58,520 --> 00:04:00,160
and let's imagine an octopus.

87
00:04:00,160 --> 00:04:03,080
So an octopus who lives under the ocean

88
00:04:03,080 --> 00:04:06,240
and has tapped into a communication channel,

89
00:04:06,240 --> 00:04:10,520
say a telephone line between two islands, okay?

90
00:04:10,520 --> 00:04:11,360
So the octopus is there,

91
00:04:11,360 --> 00:04:13,240
it's eavesdropping on all of the communication

92
00:04:13,240 --> 00:04:15,160
that happens between those two islands.

93
00:04:15,160 --> 00:04:17,960
You can get a huge amount of linguistic input

94
00:04:18,960 --> 00:04:22,160
and you could imagine very smart octopus

95
00:04:22,160 --> 00:04:25,160
might be able to learn all of the statistical properties

96
00:04:25,160 --> 00:04:28,440
of what's happening across that communication channel, right?

97
00:04:28,440 --> 00:04:32,040
Might be able to learn, become very good at predicting text.

98
00:04:32,040 --> 00:04:34,720
Maybe it could predict it optimally, whatever.

99
00:04:35,720 --> 00:04:38,080
Their argument is that the octopus

100
00:04:38,080 --> 00:04:40,000
could never actually learn the meanings, right?

101
00:04:40,000 --> 00:04:41,320
Because it would never have access

102
00:04:41,320 --> 00:04:43,520
to the physical reference, right?

103
00:04:43,520 --> 00:04:44,360
So yeah.

104
00:04:44,360 --> 00:04:46,520
Is it really different from the Chinese room?

105
00:04:46,680 --> 00:04:50,040
Yeah, so it's interestingly a little different

106
00:04:50,040 --> 00:04:54,160
from the Chinese room and maybe can I defer that question

107
00:04:54,160 --> 00:04:55,600
to the end if that's okay?

108
00:04:56,560 --> 00:04:58,200
Because I think what's going on with the Chinese room

109
00:04:58,200 --> 00:05:01,160
might make more sense with what I say later, okay?

110
00:05:01,160 --> 00:05:03,480
But very similar in spirit, I would say.

111
00:05:04,920 --> 00:05:08,720
So this octopus has no access to the physical reference

112
00:05:08,720 --> 00:05:10,080
and therefore couldn't solve tasks

113
00:05:10,080 --> 00:05:11,800
involving the physical objects, right?

114
00:05:11,800 --> 00:05:14,880
If you ask them to visually recognize what a coconut was

115
00:05:14,880 --> 00:05:17,440
even if they knew all of the statistical properties

116
00:05:17,440 --> 00:05:19,360
of where the word coconut would be used, right?

117
00:05:19,360 --> 00:05:22,960
They wouldn't be able to solve the physical stuff, okay?

118
00:05:22,960 --> 00:05:24,320
And of course this is the situation

119
00:05:24,320 --> 00:05:26,400
that a large language model is in,

120
00:05:26,400 --> 00:05:29,080
at least one that's only trained on text, right?

121
00:05:29,080 --> 00:05:30,920
It doesn't get access to stuff from the world,

122
00:05:30,920 --> 00:05:35,120
it only has kind of text, okay?

123
00:05:36,200 --> 00:05:39,600
Therefore this octopus doesn't have meanings for the words.

124
00:05:40,440 --> 00:05:42,120
Such an octopus is like a large language model

125
00:05:42,120 --> 00:05:43,960
because they only have text.

126
00:05:43,960 --> 00:05:45,440
And predictive ability therefore

127
00:05:45,440 --> 00:05:47,320
can't give you the meanings, right?

128
00:05:47,320 --> 00:05:49,400
Meaning is something just fundamentally different

129
00:05:49,400 --> 00:05:51,760
than predictive ability.

130
00:05:53,440 --> 00:05:55,480
Let me give you one other example of this.

131
00:05:55,480 --> 00:05:57,880
Well, I'll just say, I think that on the surface

132
00:05:57,880 --> 00:06:00,560
this is a somewhat convincing argument, right?

133
00:06:00,560 --> 00:06:05,560
It's kind of compelling to think of meaning in this way.

134
00:06:05,840 --> 00:06:08,960
And certainly if you do, it seems pretty convincing, yeah.

135
00:06:09,800 --> 00:06:12,040
It may not be right in terms of your mathematical history

136
00:06:12,040 --> 00:06:14,520
but it's given me a couple of axioms.

137
00:06:14,520 --> 00:06:17,640
And you know, you're not supposed to have,

138
00:06:17,640 --> 00:06:20,800
because this is really referring to

139
00:06:20,800 --> 00:06:23,160
this particular interpretation of that one.

140
00:06:23,160 --> 00:06:25,240
This would make that up in your mind.

141
00:06:25,240 --> 00:06:27,840
Yeah, great, so hold on to that thought too

142
00:06:27,840 --> 00:06:29,640
because a mathematician thinking about axioms

143
00:06:29,640 --> 00:06:32,680
is very much related to another version of meaning

144
00:06:32,680 --> 00:06:36,040
that I'll talk about in a few minutes, okay?

145
00:06:36,040 --> 00:06:39,200
Even as Hilbert wanted it to be, right?

146
00:06:39,200 --> 00:06:42,520
Hilbert's desire was to convert Euclidean geometry

147
00:06:42,520 --> 00:06:45,360
to a set of axioms such that every symbol

148
00:06:45,360 --> 00:06:48,120
could be replaced by some arbitrary squiggle

149
00:06:48,120 --> 00:06:50,320
and the system should still work.

150
00:06:50,320 --> 00:06:54,400
That is what Hilbert did in the axiomatization of geometry.

151
00:06:54,400 --> 00:06:59,400
Yeah, so I think most mathematicians

152
00:06:59,480 --> 00:07:01,680
would have a slightly different sense of meaning

153
00:07:01,680 --> 00:07:03,760
but one which matches what I'll say

154
00:07:03,960 --> 00:07:05,760
in a few minutes, so yeah.

155
00:07:06,760 --> 00:07:09,720
Let me give you just another kind of gloss

156
00:07:09,720 --> 00:07:12,060
on meaning and language models.

157
00:07:12,060 --> 00:07:15,200
Here's Gary Marcus talking about lambda.

158
00:07:15,200 --> 00:07:17,760
In truth, literally everything the system says is bullshit.

159
00:07:17,760 --> 00:07:20,200
The sooner we realize that lambda's utterances are bullshit,

160
00:07:20,200 --> 00:07:22,000
just games with predictive word tools

161
00:07:22,000 --> 00:07:24,360
and no real meaning, the better off we'll be.

162
00:07:25,480 --> 00:07:27,320
Software like lambda doesn't even try to connect

163
00:07:27,320 --> 00:07:28,560
to the world at large, right?

164
00:07:28,560 --> 00:07:30,880
That's what he thinks makes it bullshit.

165
00:07:30,880 --> 00:07:32,840
Just tries to be the best version of autocomplete

166
00:07:32,920 --> 00:07:33,760
that it can be.

167
00:07:33,760 --> 00:07:34,760
They don't understand language

168
00:07:34,760 --> 00:07:37,720
in the sense of relating sentences to the world

169
00:07:37,720 --> 00:07:41,100
but just sequences of words to one another, okay?

170
00:07:42,600 --> 00:07:46,600
So I got interested in this in part

171
00:07:46,600 --> 00:07:49,440
because I find this view kind of compelling.

172
00:07:49,440 --> 00:07:51,840
On the other hand, I also think it's kind of deeply wrong

173
00:07:51,840 --> 00:07:53,640
and the way in which it's deeply wrong

174
00:07:53,640 --> 00:07:56,320
is really interesting for what it has to say

175
00:07:56,320 --> 00:08:00,040
about conceptual representations and meanings

176
00:08:00,040 --> 00:08:04,160
in human minds as well as in machine learning models.

177
00:08:04,160 --> 00:08:09,160
So a few years ago, I teamed up with Felix Hill,

178
00:08:10,000 --> 00:08:12,840
who's a researcher at DeepMind

179
00:08:12,840 --> 00:08:17,840
and wrote an article basically going through arguments

180
00:08:18,200 --> 00:08:20,640
that meaning is not this form of reference, right?

181
00:08:20,640 --> 00:08:23,560
So in fact, this idea that meaning should be equated

182
00:08:23,560 --> 00:08:26,000
with some mapping to things in the world

183
00:08:26,000 --> 00:08:28,280
has often been rejected by people in linguistics

184
00:08:28,280 --> 00:08:31,040
and philosophy and cognitive science.

185
00:08:31,040 --> 00:08:33,240
And I think for good reason,

186
00:08:33,240 --> 00:08:34,840
so just to give you a kind of flavor

187
00:08:34,840 --> 00:08:38,720
of why people often reject this,

188
00:08:38,720 --> 00:08:41,120
there's many concepts, many words, for example,

189
00:08:41,120 --> 00:08:44,320
that have no reference to the external world, right?

190
00:08:44,320 --> 00:08:46,120
Function words are a good example of this.

191
00:08:46,120 --> 00:08:49,040
Words like to or is or many, right?

192
00:08:49,040 --> 00:08:51,680
There's no to, to, to out in the world

193
00:08:51,680 --> 00:08:53,960
that that word refers to.

194
00:08:53,960 --> 00:08:57,360
There's also no is out in the world or no many.

195
00:08:57,400 --> 00:08:59,240
Those are function words in language

196
00:08:59,240 --> 00:09:02,600
and what they do is actually much more like

197
00:09:02,600 --> 00:09:06,720
kind of what an operator in mathematics or something does,

198
00:09:06,720 --> 00:09:07,560
right?

199
00:09:07,560 --> 00:09:09,840
These words have an internal meaning.

200
00:09:09,840 --> 00:09:13,640
They control the kind of compositional meaning of a sentence,

201
00:09:13,640 --> 00:09:15,880
meaning that they have to be composed

202
00:09:15,880 --> 00:09:18,320
internally in linguistic representations

203
00:09:18,320 --> 00:09:19,960
in order to express their meaning, right?

204
00:09:19,960 --> 00:09:23,240
They're not pointing to something out in the world.

205
00:09:23,240 --> 00:09:25,240
Even if you don't go to function words like this,

206
00:09:25,280 --> 00:09:28,960
there's other words which are very hard to make sense of

207
00:09:28,960 --> 00:09:31,680
in a kind of view that meaning is stuff in the world.

208
00:09:31,680 --> 00:09:35,080
So you can think of very abstract words like justice, right?

209
00:09:35,080 --> 00:09:36,800
There's probably not a justice out there.

210
00:09:36,800 --> 00:09:40,120
That's some kind of construct that we have or wit,

211
00:09:40,120 --> 00:09:42,560
or you can think of things that don't exist like dragons,

212
00:09:42,560 --> 00:09:43,400
right?

213
00:09:43,400 --> 00:09:47,240
There's no external thing in the world, which is a dragon.

214
00:09:47,240 --> 00:09:49,360
There's even words and concepts we have

215
00:09:49,360 --> 00:09:52,080
that have no possible reference to the world, right?

216
00:09:52,080 --> 00:09:54,280
So if I think about an imaginary bicycle,

217
00:09:54,320 --> 00:09:56,800
that's something which is by definition imaginary, right?

218
00:09:56,800 --> 00:10:00,440
It's not out there, or a perpetual motion machine, right?

219
00:10:00,440 --> 00:10:03,040
We can have a concept of a perpetual motion machine

220
00:10:03,040 --> 00:10:04,920
and think about it and reason about it,

221
00:10:04,920 --> 00:10:07,920
but there's certainly not one that exists out in the world.

222
00:10:09,760 --> 00:10:11,840
Even for ordinary concepts,

223
00:10:11,840 --> 00:10:14,880
we likely haven't even considered all of the possible things

224
00:10:14,880 --> 00:10:17,360
which could be reference of those concepts, right?

225
00:10:17,360 --> 00:10:21,680
So I could walk in wearing a shoe made out of eggplants

226
00:10:21,680 --> 00:10:24,000
and you could look at them and everybody might agree

227
00:10:24,040 --> 00:10:26,520
that they're shoes, right?

228
00:10:26,520 --> 00:10:27,720
But you would agree that they're shoes

229
00:10:27,720 --> 00:10:30,960
without ever having seen shoes made of eggplants before, right?

230
00:10:30,960 --> 00:10:32,720
So there's some object in the world

231
00:10:32,720 --> 00:10:35,160
which everybody would agree is a shoe.

232
00:10:35,160 --> 00:10:37,140
Even though you've never encountered that thing before,

233
00:10:37,140 --> 00:10:38,560
that means that it couldn't have been the stuff

234
00:10:38,560 --> 00:10:41,560
in the world which determined whether it was a shoe, right?

235
00:10:41,560 --> 00:10:43,720
Had to be some kind of more abstract conception

236
00:10:43,720 --> 00:10:46,280
of what makes something a shoe, right?

237
00:10:46,280 --> 00:10:48,040
You can think about things like the function

238
00:10:48,040 --> 00:10:51,000
or the origin, how they're used.

239
00:10:51,000 --> 00:10:53,200
These other kinds of properties of objects

240
00:10:53,200 --> 00:10:55,760
seem to be much more important for the categorization

241
00:10:55,760 --> 00:10:58,000
of the concept, yeah.

242
00:10:58,000 --> 00:11:01,320
Well, here this is a little bit of a survivorship bias

243
00:11:01,320 --> 00:11:05,400
because eggplants shoes might still be considered shoes,

244
00:11:05,400 --> 00:11:08,120
but then ice cream shoes are probably,

245
00:11:08,120 --> 00:11:10,360
nobody will recognize them as shoes, right?

246
00:11:10,360 --> 00:11:11,880
But then you don't think about ice cream shoes.

247
00:11:11,880 --> 00:11:15,440
So it's like the things that you can think of as shoes

248
00:11:15,440 --> 00:11:17,440
are in your little bowl

249
00:11:17,440 --> 00:11:18,800
and then you don't think about things

250
00:11:18,800 --> 00:11:19,840
that are already too far.

251
00:11:19,840 --> 00:11:21,920
So it seems like there is still kind of some distance

252
00:11:21,920 --> 00:11:24,520
to the closest real object.

253
00:11:24,520 --> 00:11:25,360
Yeah, yeah.

254
00:11:25,360 --> 00:11:30,360
So all of this is not to say that the real objects

255
00:11:31,240 --> 00:11:32,640
are irrelevant, right?

256
00:11:32,640 --> 00:11:35,280
Like I agree that eggplants are much more plausible issues

257
00:11:35,280 --> 00:11:38,840
than ice cream and that has to do with the kind of real

258
00:11:38,840 --> 00:11:42,080
physical properties of those substances.

259
00:11:42,080 --> 00:11:43,880
My point is just that the physical thing

260
00:11:43,880 --> 00:11:45,600
is not the defining thing, right?

261
00:11:45,600 --> 00:11:47,400
It's not something in the object you look to

262
00:11:47,400 --> 00:11:49,080
to decide whether it's a shoe or not, right?

263
00:11:49,080 --> 00:11:51,900
It's something more abstract about how it's used

264
00:11:51,900 --> 00:11:53,300
or made or something like that.

265
00:11:55,380 --> 00:11:57,380
I'll actually talk a bit in this paper

266
00:11:57,380 --> 00:12:01,780
about the concept of a postage stamp, right?

267
00:12:01,780 --> 00:12:05,060
Which is just an example of one that people probably

268
00:12:05,060 --> 00:12:08,980
have some intuitions about where you could easily think

269
00:12:08,980 --> 00:12:11,580
of postage stamps which are fundamentally different

270
00:12:11,580 --> 00:12:12,940
than anyone's you've seen before.

271
00:12:12,940 --> 00:12:15,180
You could think of one made of glass, for example,

272
00:12:15,180 --> 00:12:17,900
or you could think of one that was an RFID tag,

273
00:12:17,900 --> 00:12:20,500
which is probably physical incarnations

274
00:12:20,500 --> 00:12:22,940
of postage stamps like that that everybody would agree

275
00:12:22,940 --> 00:12:25,420
should be called a postage stamp.

276
00:12:25,420 --> 00:12:27,940
And if you try to get people to define it, right?

277
00:12:27,940 --> 00:12:29,180
You might say something like, well,

278
00:12:29,180 --> 00:12:30,620
a postage stamp is something you pay for

279
00:12:30,620 --> 00:12:32,260
and you put on a letter so that the letter

280
00:12:32,260 --> 00:12:33,540
will be delivered by the government

281
00:12:33,540 --> 00:12:34,620
or something like that, right?

282
00:12:34,620 --> 00:12:39,620
So what the term means is intrinsically connected

283
00:12:39,620 --> 00:12:42,900
to a bunch of these other terms like payment and letters

284
00:12:42,900 --> 00:12:45,060
and being delivered and those things.

285
00:12:45,060 --> 00:12:47,940
And in fact, if those terms change meaning, right?

286
00:12:47,940 --> 00:12:50,980
So if, for example, people develop a new way

287
00:12:50,980 --> 00:12:53,540
of paying for things, paying on the blockchain

288
00:12:53,540 --> 00:12:54,380
or something, right?

289
00:12:54,380 --> 00:12:56,280
Then you kind of know automatically

290
00:12:56,280 --> 00:12:59,060
that a postage stamp can be paid for in that way,

291
00:12:59,060 --> 00:13:00,500
at least in principle, right?

292
00:13:00,500 --> 00:13:03,380
So it's not just that the word is associated

293
00:13:03,380 --> 00:13:05,140
with those other things, but that its meaning

294
00:13:05,140 --> 00:13:07,940
is inherently connected to those other things.

295
00:13:09,940 --> 00:13:14,940
So that's one kind of take on why reference to stuff

296
00:13:18,020 --> 00:13:19,060
out in the world, right?

297
00:13:19,060 --> 00:13:22,860
Is not a good way of thinking about meaning.

298
00:13:22,860 --> 00:13:25,500
Let me tell you what one alternative is.

299
00:13:25,500 --> 00:13:28,460
Actually, before I do that, let me just show

300
00:13:28,460 --> 00:13:29,620
a couple of other alternatives,

301
00:13:29,620 --> 00:13:31,620
which I think are also not plausible,

302
00:13:31,620 --> 00:13:34,020
but might be familiar to people, okay?

303
00:13:34,020 --> 00:13:36,460
So what I just talked about is this kind of

304
00:13:36,460 --> 00:13:37,620
world mapping view, right?

305
00:13:37,620 --> 00:13:39,500
That there's some word and its meaning

306
00:13:39,500 --> 00:13:42,400
is some physical object or some thing.

307
00:13:44,100 --> 00:13:45,500
You can think about other kinds of views

308
00:13:45,740 --> 00:13:49,140
of concepts and meaning might have a kind of

309
00:13:49,140 --> 00:13:50,500
feature spacey kind of views,

310
00:13:50,500 --> 00:13:52,020
port vector machines or something, right?

311
00:13:52,020 --> 00:13:54,060
There's some abstract feature space

312
00:13:54,060 --> 00:13:56,640
and a concept is some dividing line

313
00:13:56,640 --> 00:13:59,100
or some region or something in this space.

314
00:14:01,340 --> 00:14:06,180
That I think is maybe fine in some narrow applications,

315
00:14:06,180 --> 00:14:09,980
but what I'll talk about next are cases of, say,

316
00:14:09,980 --> 00:14:12,860
human cognition, which really don't fit well

317
00:14:12,860 --> 00:14:14,820
into that picture, in the sense that things

318
00:14:14,820 --> 00:14:16,660
are much more complicated for how people think

319
00:14:16,660 --> 00:14:19,740
about concepts and their relationships.

320
00:14:19,740 --> 00:14:22,060
People might also have this sort of hierarchy

321
00:14:22,060 --> 00:14:23,380
or network view, right?

322
00:14:23,380 --> 00:14:25,580
So sometimes people think, oh, sorry, you can't see this.

323
00:14:25,580 --> 00:14:27,880
There's supposed to be lines connecting

324
00:14:27,880 --> 00:14:30,100
one concept book in the middle to a bunch

325
00:14:30,100 --> 00:14:31,460
of other concepts, right?

326
00:14:31,460 --> 00:14:34,380
And you might, you know, there's old theories

327
00:14:34,380 --> 00:14:36,980
of, say, semantic organization or very old,

328
00:14:36,980 --> 00:14:39,100
old AI kind of approaches, right?

329
00:14:39,100 --> 00:14:43,020
That think about building hierarchies of concepts

330
00:14:43,940 --> 00:14:46,620
or sometimes networks of concepts

331
00:14:46,620 --> 00:14:49,500
and trying to define meaning in terms of those relationships.

332
00:14:49,500 --> 00:14:52,060
I actually think that both this and the feature

333
00:14:52,060 --> 00:14:55,380
and the world mapping view have some of the,

334
00:14:55,380 --> 00:14:58,620
some kind of useful properties or useful insights

335
00:14:58,620 --> 00:15:01,340
about concepts, but just aren't quite the whole thing

336
00:15:01,340 --> 00:15:04,020
for reasons that I'll talk about next.

337
00:15:04,020 --> 00:15:08,500
So let me just start with, start trying to introduce

338
00:15:08,500 --> 00:15:11,060
this kind of other view of concepts

339
00:15:11,060 --> 00:15:13,580
by trying to get people's intuitions

340
00:15:13,580 --> 00:15:16,140
on a recent news story, okay?

341
00:15:16,140 --> 00:15:20,460
So here's a little recent news from the US versus Trump.

342
00:15:20,460 --> 00:15:22,140
I think this is not the most recent indictment,

343
00:15:22,140 --> 00:15:23,900
but one or two indictments ago.

344
00:15:25,180 --> 00:15:28,140
If you look through it, you can read all about

345
00:15:29,020 --> 00:15:31,620
Pence and Trump and efforts to manipulate

346
00:15:31,620 --> 00:15:33,300
the election and things.

347
00:15:33,300 --> 00:15:36,500
Here's a little paraphrase of one of the paragraphs, 90 C.

348
00:15:36,500 --> 00:15:38,340
So Pence, the vice president, right,

349
00:15:38,340 --> 00:15:40,740
opposed a Trump team lawsuit arguing

350
00:15:40,740 --> 00:15:43,500
that the vice president could reject electoral votes.

351
00:15:43,500 --> 00:15:45,660
So Pence didn't want them to argue

352
00:15:45,660 --> 00:15:48,220
that he could reject electoral votes.

353
00:15:48,220 --> 00:15:50,100
He said to Trump that he didn't have a constitutional

354
00:15:50,100 --> 00:15:52,660
authority and that the action would be improper.

355
00:15:52,660 --> 00:15:55,700
So it's according to Pence's notes at the time.

356
00:15:55,700 --> 00:15:58,940
And Trump responded, you're too honest, okay?

357
00:15:58,940 --> 00:16:00,660
According to Pence's notes.

358
00:16:00,660 --> 00:16:02,460
So think about that situation and everything

359
00:16:02,460 --> 00:16:04,780
you know about this context, right?

360
00:16:04,780 --> 00:16:07,340
And think about an answer to a question like,

361
00:16:07,340 --> 00:16:09,500
why did Trump say this?

362
00:16:09,500 --> 00:16:10,340
Right.

363
00:16:11,780 --> 00:16:15,940
You think about that, probably what's going on

364
00:16:15,940 --> 00:16:17,260
as you think about it, right?

365
00:16:17,260 --> 00:16:20,060
As you're thinking about lots of other things

366
00:16:20,060 --> 00:16:21,980
and how they're related to this situation,

367
00:16:21,980 --> 00:16:23,980
like what Trump was trying to achieve,

368
00:16:23,980 --> 00:16:26,900
maybe what kind of personality Trump had,

369
00:16:26,900 --> 00:16:29,060
what Trump was trying to do to Pence.

370
00:16:29,060 --> 00:16:30,740
Is he trying to manipulate him

371
00:16:30,740 --> 00:16:33,020
into taking some kinds of actions?

372
00:16:33,020 --> 00:16:35,900
What exactly that action would have, right?

373
00:16:35,900 --> 00:16:38,020
In terms of the election.

374
00:16:38,020 --> 00:16:40,820
Everybody is perfectly capable of reasoning through these

375
00:16:40,820 --> 00:16:45,540
and coming up with kind of plausible causal story

376
00:16:45,540 --> 00:16:46,580
about what's happening, right?

377
00:16:46,580 --> 00:16:48,460
It feels like we can come up with our own

378
00:16:48,460 --> 00:16:52,740
kind of internal explanations about events like these.

379
00:16:52,740 --> 00:16:55,100
And in fact, that process of coming up

380
00:16:55,100 --> 00:16:58,980
with internal explanations, interrelated kind of concepts

381
00:16:58,980 --> 00:17:03,020
and meanings is one that people in developmental psychology

382
00:17:03,020 --> 00:17:05,420
have been very interested in and excited about

383
00:17:05,700 --> 00:17:08,820
as a theory of kind of human cognition.

384
00:17:08,820 --> 00:17:13,180
So basic observation is that people form these

385
00:17:13,180 --> 00:17:16,660
very richly interconnected systems of concepts, right?

386
00:17:16,660 --> 00:17:20,060
All of the kind of interconnected stuff you would need to draw

387
00:17:20,060 --> 00:17:22,620
in order to answer a question like that,

388
00:17:22,620 --> 00:17:25,140
which feels totally, totally normal.

389
00:17:25,140 --> 00:17:27,300
People sometimes call these intuitive theories, right?

390
00:17:27,300 --> 00:17:29,820
You have some intuitive theory of how Trump is acting

391
00:17:29,820 --> 00:17:31,540
or how the political system would work

392
00:17:31,540 --> 00:17:36,140
or some intuitive theory of what Pence might be doing

393
00:17:36,140 --> 00:17:38,140
or might be trying to achieve.

394
00:17:38,140 --> 00:17:39,860
And these things are often compared

395
00:17:39,860 --> 00:17:41,820
to theories in science, right?

396
00:17:41,820 --> 00:17:46,820
So you can think of your theory of why Trump might do this

397
00:17:47,780 --> 00:17:50,780
as kind of analogous to a little scientific theory, right?

398
00:17:50,780 --> 00:17:53,300
It has some pieces, it has some relationships

399
00:17:53,300 --> 00:17:56,100
between the pieces, it has some dynamics.

400
00:17:56,100 --> 00:17:58,540
And maybe you can look at all of that

401
00:17:58,540 --> 00:18:00,340
and kind of reason about it causally

402
00:18:00,380 --> 00:18:02,660
as you might reason about any other kind of system

403
00:18:02,660 --> 00:18:04,380
that you've encountered.

404
00:18:05,220 --> 00:18:07,580
So the idea that people,

405
00:18:07,580 --> 00:18:10,620
and maybe most notably kids do this is one

406
00:18:10,620 --> 00:18:14,140
which has really been very popular

407
00:18:14,140 --> 00:18:18,700
in cognitive development, championed maybe most prominently

408
00:18:18,700 --> 00:18:21,460
by Alison Gopnik, who's a developmental psychologist

409
00:18:21,460 --> 00:18:23,060
here at Berkeley.

410
00:18:23,060 --> 00:18:26,180
Let me just give you a quick example of how kids,

411
00:18:26,180 --> 00:18:30,220
how experiments with kids like kids sometimes go

412
00:18:30,420 --> 00:18:31,260
in this domain.

413
00:18:31,260 --> 00:18:36,260
So here's an experiment from LaZot and Gelman.

414
00:18:37,060 --> 00:18:39,380
So kids are shown these two foxes, right?

415
00:18:39,380 --> 00:18:43,220
Which you might notice are identical pictures, okay?

416
00:18:44,060 --> 00:18:46,740
And then they're told things about these foxes

417
00:18:46,740 --> 00:18:50,740
and asked what they could do in order to answer a question,

418
00:18:50,740 --> 00:18:51,580
right?

419
00:18:51,580 --> 00:18:53,940
So this is like a simple version of why did Trump say that?

420
00:18:53,940 --> 00:18:57,220
You might be told that one is an animal and one is a toy,

421
00:18:57,220 --> 00:18:58,060
okay?

422
00:18:58,060 --> 00:18:59,820
So what could you do in order to determine

423
00:18:59,820 --> 00:19:03,340
which one is an animal and which one is a toy, right?

424
00:19:06,820 --> 00:19:10,340
In this experiment, kids will say that you should do things

425
00:19:10,340 --> 00:19:12,020
like check the insides, right?

426
00:19:12,020 --> 00:19:13,860
Like check their guts or whatever, right?

427
00:19:13,860 --> 00:19:16,140
Open them up and see.

428
00:19:16,140 --> 00:19:19,220
Or look at their behavior, right?

429
00:19:19,220 --> 00:19:21,820
If it acts like an animal then you could use that

430
00:19:21,820 --> 00:19:24,660
to figure out which one is the animal, which one's the toy.

431
00:19:24,660 --> 00:19:26,180
Or look at their parents, right?

432
00:19:26,220 --> 00:19:30,260
Like, you know, the animal will have animal parents

433
00:19:30,260 --> 00:19:31,820
and the toy won't, right?

434
00:19:31,820 --> 00:19:33,740
And importantly, they don't just say, yeah,

435
00:19:33,740 --> 00:19:35,620
you can check everything about these.

436
00:19:35,620 --> 00:19:38,180
They know, for example, that age is not relevant, right?

437
00:19:38,180 --> 00:19:41,540
So they won't tell you that age would tell you

438
00:19:41,540 --> 00:19:44,140
which one is an animal and which one is a toy.

439
00:19:44,140 --> 00:19:45,900
It's worth pausing and just thinking about this

440
00:19:45,900 --> 00:19:49,460
and what this means in terms of conceptual representations,

441
00:19:49,460 --> 00:19:50,660
right?

442
00:19:50,660 --> 00:19:52,820
Because you can think about your concept of what makes

443
00:19:52,820 --> 00:19:55,460
something an animal or what makes something a toy

444
00:19:55,460 --> 00:19:57,460
and kind of like the postage stamp example, right?

445
00:19:57,460 --> 00:19:59,540
It's intrinsically connected to these other things,

446
00:19:59,540 --> 00:20:02,300
like what parents are or what's going on

447
00:20:02,300 --> 00:20:04,340
with your guts inside, right?

448
00:20:04,340 --> 00:20:06,020
Or what your behavior is, right?

449
00:20:06,020 --> 00:20:07,900
That concept is just intrinsically linked there

450
00:20:07,900 --> 00:20:11,500
and kids, I think these are preschoolers know that

451
00:20:11,500 --> 00:20:13,260
from a pretty young age.

452
00:20:14,660 --> 00:20:16,340
Gals asked them a question like one is a dog,

453
00:20:16,340 --> 00:20:19,660
one is a wolf, what would you do, okay?

454
00:20:19,660 --> 00:20:21,180
Kids basically say the same things there.

455
00:20:21,180 --> 00:20:23,460
You could check the insides, you could look at behavior,

456
00:20:23,460 --> 00:20:24,700
you could look at their parents,

457
00:20:24,700 --> 00:20:28,380
see if they had a dog parent or a wolf parent.

458
00:20:28,380 --> 00:20:30,140
Some of these are actually kind of interesting, right?

459
00:20:30,140 --> 00:20:32,620
Because I don't think anybody knows,

460
00:20:32,620 --> 00:20:35,980
at least I don't, what you would look for on the insides

461
00:20:35,980 --> 00:20:38,020
to distinguish a dog versus a wolf, right?

462
00:20:38,020 --> 00:20:41,140
Like maybe you could go down to DNA

463
00:20:41,140 --> 00:20:45,060
or I'm sure you could go down to DNA to tell that.

464
00:20:45,060 --> 00:20:46,660
But people have the intuition that like, okay,

465
00:20:46,660 --> 00:20:48,380
there's something about being in this category

466
00:20:48,380 --> 00:20:50,660
which depends on these other aspects

467
00:20:50,660 --> 00:20:52,420
of being in the concept.

468
00:20:53,300 --> 00:20:54,740
To me, I read this much simpler.

469
00:20:54,740 --> 00:20:57,340
It's like basically what they're saying is look,

470
00:20:57,340 --> 00:20:59,500
like all of this are visual things.

471
00:20:59,500 --> 00:21:01,420
It's just that you're trying to project it into language

472
00:21:01,420 --> 00:21:04,500
but actually what the kids are probably meaning is,

473
00:21:04,500 --> 00:21:06,060
you will know it when you see it

474
00:21:06,060 --> 00:21:07,980
than when you play with it, right?

475
00:21:07,980 --> 00:21:10,300
It's vision and interaction.

476
00:21:11,180 --> 00:21:12,020
Yeah, yeah.

477
00:21:12,020 --> 00:21:13,580
And age is neither.

478
00:21:13,580 --> 00:21:14,420
Yeah, yeah.

479
00:21:14,420 --> 00:21:19,100
So I think it's true that, yes, all of these are visual cues.

480
00:21:19,100 --> 00:21:22,140
I don't know of experiments that look at non-visual cues

481
00:21:23,380 --> 00:21:25,140
but I agree, yeah, that's interesting.

482
00:21:25,140 --> 00:21:26,420
I'll give you one other example

483
00:21:26,420 --> 00:21:30,580
where they know that there's no cue, right?

484
00:21:30,580 --> 00:21:32,140
So if you tell them that one is named Amanda

485
00:21:32,140 --> 00:21:34,500
and one is named Melissa,

486
00:21:34,500 --> 00:21:37,220
then they'll reject all of these as tests, right?

487
00:21:37,220 --> 00:21:39,020
They'll say, okay, the insides are not gonna tell you

488
00:21:39,020 --> 00:21:42,020
which one is Amanda, the behavior and the parents

489
00:21:42,020 --> 00:21:44,820
and the age and these things are not going to, okay?

490
00:21:44,820 --> 00:21:46,340
So all of this is just to say

491
00:21:46,340 --> 00:21:49,780
that people have a, even kids, right,

492
00:21:50,060 --> 00:21:52,060
have pretty sophisticated theories

493
00:21:52,060 --> 00:21:55,260
of how concepts relate to other concepts, right?

494
00:21:55,260 --> 00:21:57,380
And in fact, in a situation like this, right,

495
00:21:57,380 --> 00:22:00,980
there's nothing visual apparently that could tell you, right?

496
00:22:00,980 --> 00:22:02,620
So it's not a visual discrimination task.

497
00:22:02,620 --> 00:22:05,340
It's really a kind of conceptual one

498
00:22:05,340 --> 00:22:07,660
that's asking you to look at other kinds

499
00:22:07,660 --> 00:22:09,300
of conceptual features and things.

500
00:22:11,060 --> 00:22:14,580
So people have these intuitive theories

501
00:22:14,580 --> 00:22:16,900
then one kind of proposal,

502
00:22:16,900 --> 00:22:19,380
quite a few people have argued for is that meaning arises

503
00:22:19,380 --> 00:22:21,940
from essentially the role that a word or a symbol

504
00:22:21,940 --> 00:22:24,660
or a concept plays in this theory, right?

505
00:22:24,660 --> 00:22:28,580
Like the meaning of animals really just intrinsically related

506
00:22:28,580 --> 00:22:31,500
to these ways of testing it and these kinds of features

507
00:22:31,500 --> 00:22:33,540
and all of the other things

508
00:22:33,540 --> 00:22:36,460
that are not kind of simple semantic associates with animal

509
00:22:36,460 --> 00:22:39,620
but are kind of deeply connected

510
00:22:39,620 --> 00:22:42,620
in the sense of an intuitive theory.

511
00:22:43,460 --> 00:22:44,860
I was trying to come up with examples

512
00:22:44,860 --> 00:22:48,460
where this, you know,

513
00:22:48,540 --> 00:22:50,500
we'll give people this intuition, right,

514
00:22:50,500 --> 00:22:54,020
that, you know, if you try to define these words,

515
00:22:54,020 --> 00:22:57,060
if you try to define what an indictment is, for example,

516
00:22:57,060 --> 00:22:58,340
it's very hard to do it in a way

517
00:22:58,340 --> 00:23:00,660
that doesn't reference other legal terms

518
00:23:00,660 --> 00:23:02,420
and other kind of social constructs

519
00:23:02,420 --> 00:23:07,100
and concepts that you already have, right?

520
00:23:07,100 --> 00:23:08,540
It's kind of intrinsically related

521
00:23:08,540 --> 00:23:11,460
to the system of other concepts and terms, right?

522
00:23:11,460 --> 00:23:13,580
Chord change is kind of like this too in music, right?

523
00:23:13,580 --> 00:23:15,380
You have to talk about chords and notes

524
00:23:15,860 --> 00:23:18,940
and circle of fifths or whatever, right?

525
00:23:18,940 --> 00:23:20,860
Like these things are just intrinsically related.

526
00:23:20,860 --> 00:23:23,580
I think force in physics is like this.

527
00:23:23,580 --> 00:23:25,820
It's very hard to talk about it in isolation

528
00:23:25,820 --> 00:23:28,300
independent of, you know, experiments

529
00:23:28,300 --> 00:23:29,740
or other concepts or things.

530
00:23:29,740 --> 00:23:31,060
Or if I said, like, what does a bobbin do

531
00:23:31,060 --> 00:23:32,700
in a sewing machine, okay, right?

532
00:23:32,700 --> 00:23:34,340
Like you have to talk about thread

533
00:23:34,340 --> 00:23:36,060
and you have to talk about the processes of sewing.

534
00:23:36,060 --> 00:23:37,220
Just the meaning of these things

535
00:23:37,220 --> 00:23:39,980
are just all intrinsically linked together.

536
00:23:41,260 --> 00:23:44,500
So this idea that meaning,

537
00:23:44,540 --> 00:23:46,020
not about reference, it's about the role

538
00:23:46,020 --> 00:23:48,740
that something plays is called conceptual role theory.

539
00:23:49,660 --> 00:23:51,220
Meaning of a word or concept is determined

540
00:23:51,220 --> 00:23:53,020
by the role it plays.

541
00:23:54,380 --> 00:23:59,380
And this has been argued for,

542
00:23:59,820 --> 00:24:01,940
I think maybe most prominently by Ned Block

543
00:24:01,940 --> 00:24:04,220
who's a philosopher of mind,

544
00:24:04,220 --> 00:24:07,100
who wrote one of my favorite paper titles,

545
00:24:07,100 --> 00:24:10,260
Advertisement for a Semantics of Psychology,

546
00:24:10,260 --> 00:24:11,900
which is basically all about, you know,

547
00:24:11,900 --> 00:24:14,740
how psychology needs a theory of meaning

548
00:24:14,740 --> 00:24:16,620
and a theory of semantics.

549
00:24:16,620 --> 00:24:18,180
And this idea of conceptual role

550
00:24:18,180 --> 00:24:20,020
is something that could do that.

551
00:24:20,020 --> 00:24:22,380
So it can explain kind of where meaning comes from.

552
00:24:22,380 --> 00:24:25,660
It can address questions of how meaning depends

553
00:24:25,660 --> 00:24:27,340
on things like your representations

554
00:24:27,340 --> 00:24:29,300
or categories that you know,

555
00:24:29,300 --> 00:24:31,380
can play nicely with compositionality

556
00:24:31,380 --> 00:24:34,620
or other aspects of language.

557
00:24:34,620 --> 00:24:36,540
And I think maybe most compellingly

558
00:24:36,540 --> 00:24:38,980
can explain how you could find meaning in brains, right?

559
00:24:38,980 --> 00:24:40,260
So if you open up a brain

560
00:24:40,260 --> 00:24:42,460
and you start recording from neurons,

561
00:24:43,740 --> 00:24:45,100
you know, it's really unclear what it means

562
00:24:45,100 --> 00:24:46,780
for there to be reference in there,

563
00:24:46,780 --> 00:24:49,260
reference to the external world in there.

564
00:24:49,260 --> 00:24:50,740
But maybe you could kind of make sense

565
00:24:50,740 --> 00:24:53,340
of patterns of activity in a way

566
00:24:53,340 --> 00:24:57,900
that lets you kind of interpret systems

567
00:24:57,900 --> 00:25:01,100
of signals and representations.

568
00:25:02,420 --> 00:25:05,020
Let me give you just one other example of this

569
00:25:05,020 --> 00:25:08,820
that maybe might make things more clear.

570
00:25:08,820 --> 00:25:10,940
This idea of conceptual role semantics,

571
00:25:10,940 --> 00:25:15,820
I think is also how meaning works in, say, a computer, okay?

572
00:25:15,820 --> 00:25:17,300
Also, I think in mathematics,

573
00:25:17,300 --> 00:25:19,500
which is why I was deferring the questions

574
00:25:19,500 --> 00:25:21,180
about mathematics,

575
00:25:21,180 --> 00:25:22,540
but you could look at something like this, right?

576
00:25:22,540 --> 00:25:25,660
This is a floating point representation

577
00:25:25,660 --> 00:25:28,260
and ask what makes the bits in this representation

578
00:25:28,260 --> 00:25:29,540
mean what they do, right?

579
00:25:29,540 --> 00:25:32,540
In particular, what makes the first bit mean the sign bit,

580
00:25:32,540 --> 00:25:33,380
right?

581
00:25:33,380 --> 00:25:34,780
It's nothing about being the first one

582
00:25:34,780 --> 00:25:36,900
because there've been dozens of different conventions

583
00:25:36,900 --> 00:25:37,940
for floating point numbers

584
00:25:38,060 --> 00:25:41,660
which put the sign bit in all kinds of different places, right?

585
00:25:42,740 --> 00:25:45,180
What makes it mean the sign bit

586
00:25:45,180 --> 00:25:47,780
is how it interacts with all of the other operations

587
00:25:47,780 --> 00:25:51,020
that you can do with floating point numbers, right?

588
00:25:51,020 --> 00:25:51,980
So meaning, in some sense,

589
00:25:51,980 --> 00:25:54,260
comes from the interaction between symbols,

590
00:25:54,260 --> 00:25:55,900
or in other words, their conceptual role.

591
00:25:55,900 --> 00:25:58,660
So in particular, like what does negation do, right?

592
00:25:58,660 --> 00:26:01,660
If I have a negation operator, okay, it flips the sign bit.

593
00:26:01,660 --> 00:26:06,420
Great, okay, that's where it gets its meaning from, right?

594
00:26:06,420 --> 00:26:07,340
Or what does addition do?

595
00:26:07,900 --> 00:26:10,580
The right thing with respect to the sign bit

596
00:26:10,580 --> 00:26:14,180
or multiplication or rounding or whatever, right?

597
00:26:14,180 --> 00:26:17,140
So what makes this the floating point representation

598
00:26:17,140 --> 00:26:20,380
or what makes that first bit represent sign

599
00:26:20,380 --> 00:26:22,820
is nothing intrinsic in the representation itself.

600
00:26:22,820 --> 00:26:25,100
It's how it interacts with all of the other components

601
00:26:25,100 --> 00:26:27,900
of the system, okay, yeah?

602
00:26:27,900 --> 00:26:30,420
You explain the difference between this way of thinking

603
00:26:30,420 --> 00:26:33,300
about the sense of here's a hierarchical approach

604
00:26:33,300 --> 00:26:35,980
where there's concepts and there's sort of numbers.

605
00:26:36,980 --> 00:26:41,980
Yeah, so I think that there's certainly concepts people have

606
00:26:43,300 --> 00:26:44,700
that are hierarchical, right?

607
00:26:44,700 --> 00:26:46,860
So we know that dogs are kind of animal

608
00:26:46,860 --> 00:26:49,340
and animals are a kind of living thing.

609
00:26:49,340 --> 00:26:52,020
I think what that kind of picture is missing

610
00:26:52,020 --> 00:26:54,540
is that our representations are actually,

611
00:26:54,540 --> 00:26:57,140
like computational objects, like they do something, right?

612
00:26:57,140 --> 00:27:01,140
They interact with each other and they allow us to solve

613
00:27:01,140 --> 00:27:02,700
certain kinds of inference problems

614
00:27:02,700 --> 00:27:05,580
and all of the stuff you could do with your concepts

615
00:27:05,660 --> 00:27:07,820
like the Trump example, right?

616
00:27:09,260 --> 00:27:12,220
Yeah, so the claim is not that they're not hierarchical, right?

617
00:27:12,220 --> 00:27:15,620
It's that the interesting important things they do

618
00:27:15,620 --> 00:27:18,340
come from interactions kind of internally between concepts

619
00:27:18,340 --> 00:27:21,020
much like the way that the sign bit is interesting

620
00:27:21,020 --> 00:27:23,620
or important here comes from its interactions

621
00:27:23,620 --> 00:27:26,620
with things like negation and multiplication, yeah, yeah.

622
00:27:28,220 --> 00:27:30,420
What you're saying here is perfectly good,

623
00:27:30,420 --> 00:27:33,420
but what I have trouble with is buying this

624
00:27:33,460 --> 00:27:36,420
as an exclusive theory of semantics.

625
00:27:36,420 --> 00:27:38,980
Just like you gave good arguments

626
00:27:38,980 --> 00:27:43,260
against that meaning is just reference, okay?

627
00:27:43,260 --> 00:27:45,540
I think you demolished that theory,

628
00:27:45,540 --> 00:27:49,740
but now you put up another theory which also I find

629
00:27:49,740 --> 00:27:51,340
that it has some good aspects,

630
00:27:51,340 --> 00:27:55,140
but to make that an exclusive theory is problematic.

631
00:27:55,140 --> 00:27:57,340
So if we look at children growing up,

632
00:27:57,340 --> 00:28:00,540
there are these studies on sort of concreteness judgments.

633
00:28:00,540 --> 00:28:03,140
So the vocabulary of a child at two,

634
00:28:03,140 --> 00:28:06,660
there are a lot of words in there like milk and bottle

635
00:28:06,660 --> 00:28:10,100
and jump and sit and so forth, which are very concrete,

636
00:28:10,100 --> 00:28:12,020
concrete in a visual sense,

637
00:28:12,020 --> 00:28:14,180
concrete in a motor program sense.

638
00:28:15,260 --> 00:28:19,020
At the age of 10, they have words like justice and fairness

639
00:28:19,020 --> 00:28:22,380
and so forth, which are very much,

640
00:28:22,380 --> 00:28:25,300
which fit much better into this conceptual road story

641
00:28:25,300 --> 00:28:28,380
where is the vocabulary of a child at two,

642
00:28:28,380 --> 00:28:31,460
maybe one where this kind of groundedness

643
00:28:31,460 --> 00:28:34,940
to sensory motor experience is a much better account.

644
00:28:34,940 --> 00:28:37,460
And this is not problematic for me.

645
00:28:37,460 --> 00:28:42,060
Why do we need to have one exclusive theory for meaning?

646
00:28:42,060 --> 00:28:45,860
Both of these are aspects of meaning.

647
00:28:45,860 --> 00:28:47,300
Yeah, I agree.

648
00:28:47,300 --> 00:28:52,300
So I think that you can think of the physical reference,

649
00:28:52,820 --> 00:28:54,700
as in some sense one of the conceptual roles

650
00:28:54,700 --> 00:28:55,940
that something can have.

651
00:28:57,300 --> 00:28:58,220
It is important.

652
00:28:58,220 --> 00:29:01,740
I'm not sure we know kind of how abstract kids early meanings

653
00:29:01,740 --> 00:29:03,980
are for those kinds of words,

654
00:29:03,980 --> 00:29:06,340
because it has to be a little bit abstract

655
00:29:06,340 --> 00:29:08,940
because you'll call a new bottle that you see a bottle still.

656
00:29:08,940 --> 00:29:11,940
So you have some abstraction away from the examples

657
00:29:11,940 --> 00:29:13,380
of bottles that you've seen,

658
00:29:13,380 --> 00:29:17,100
but I agree it feels early on very concrete

659
00:29:17,100 --> 00:29:20,380
and much less abstract than things we come later.

660
00:29:23,260 --> 00:29:25,420
Just trying to make sure I understand

661
00:29:25,420 --> 00:29:26,700
what this theory is saying.

662
00:29:26,700 --> 00:29:29,700
So is there a character to think of this

663
00:29:29,700 --> 00:29:32,340
that you're saying that meaning is basically

664
00:29:32,340 --> 00:29:36,540
like some homomorphism onto some either intuitive

665
00:29:36,540 --> 00:29:38,080
or formal theory?

666
00:29:39,340 --> 00:29:40,260
Ah, sure.

667
00:29:41,180 --> 00:29:44,460
So then maybe a follow up question is like,

668
00:29:44,460 --> 00:29:46,780
how do we know which homomorphisms are valid?

669
00:29:46,780 --> 00:29:48,220
Because I could always,

670
00:29:48,220 --> 00:29:51,300
if I can have some arbitrary correspondence mapping,

671
00:29:51,300 --> 00:29:53,780
I could make anything correspond to anything else.

672
00:29:54,780 --> 00:29:56,540
You know what's so loud here?

673
00:29:56,540 --> 00:29:59,420
Yeah, so I don't think anybody has been that formal.

674
00:29:59,420 --> 00:30:02,020
People like Putnam have made this kind of argument

675
00:30:02,020 --> 00:30:04,420
about understanding computation in physical systems,

676
00:30:04,420 --> 00:30:06,700
basically saying like physical systems,

677
00:30:06,700 --> 00:30:09,300
like a brain or in his example, a wall, right?

678
00:30:09,300 --> 00:30:12,980
Are so complicated that I could come up with

679
00:30:12,980 --> 00:30:15,260
kind of any mapping back and forth between the states of it

680
00:30:15,260 --> 00:30:19,100
and the states of the kind of arbitrary computational system.

681
00:30:19,100 --> 00:30:22,300
And that's probably a much longer thing to get into.

682
00:30:22,300 --> 00:30:24,540
I'll just say that I don't think I have a very easy answer

683
00:30:24,540 --> 00:30:25,740
about that, right?

684
00:30:26,740 --> 00:30:28,980
I think of this as not kind of,

685
00:30:30,180 --> 00:30:32,340
certainly not formalized in that sense,

686
00:30:32,340 --> 00:30:35,160
but in sort of a higher level in terms of like

687
00:30:35,160 --> 00:30:37,740
what kinds of theories we should be looking for, right?

688
00:30:37,740 --> 00:30:38,940
And then there's lots of work to do

689
00:30:38,940 --> 00:30:41,780
in terms of making that precise.

690
00:30:41,780 --> 00:30:42,820
So yeah, yeah.

691
00:30:42,820 --> 00:30:44,660
Yeah, Quine actually uses that example

692
00:30:44,660 --> 00:30:49,220
to motivate this kind of theory in like 1950s philosophy.

693
00:30:49,220 --> 00:30:50,380
Sorry, which example?

694
00:30:50,380 --> 00:30:52,100
Quine uses this example.

695
00:30:52,100 --> 00:30:53,580
He uses an example of Gavagai.

696
00:30:53,580 --> 00:30:56,100
You see something popping out and you're like,

697
00:30:56,100 --> 00:30:58,620
how do you know Gavagai means rabbit, not running,

698
00:30:58,620 --> 00:31:00,660
and not hole, and not something else

699
00:31:00,660 --> 00:31:05,100
because the real world doesn't determine what a meaning is.

700
00:31:05,100 --> 00:31:06,260
Yeah, yeah.

701
00:31:06,260 --> 00:31:07,100
Yeah.

702
00:31:07,100 --> 00:31:09,700
The hierarchical concepts and semantics

703
00:31:09,700 --> 00:31:11,860
have been extensively terminated,

704
00:31:11,860 --> 00:31:13,180
scripting orders and so on.

705
00:31:13,180 --> 00:31:15,660
Has any of this been operationalized at all?

706
00:31:15,660 --> 00:31:17,580
Can you comment on that?

707
00:31:17,580 --> 00:31:18,780
I don't think so, yeah.

708
00:31:18,780 --> 00:31:20,620
So I mean, I can talk,

709
00:31:21,620 --> 00:31:24,580
I have a couple of examples of kind of learning

710
00:31:24,580 --> 00:31:29,380
intuitive theories, which essentially have this kind

711
00:31:29,380 --> 00:31:33,740
of character, so of taking data and then trying to come up

712
00:31:33,740 --> 00:31:37,340
with some structures that obey the right relations, right?

713
00:31:37,340 --> 00:31:41,180
And yeah, I'll talk a little bit about that,

714
00:31:41,180 --> 00:31:45,860
but there hasn't been a ton of work on that, so yeah.

715
00:31:45,860 --> 00:31:48,660
Could you talk about how this theory deals with

716
00:31:48,660 --> 00:31:51,900
when the same symbols or words are in different kind

717
00:31:51,900 --> 00:31:53,900
of theories or settings?

718
00:31:53,900 --> 00:31:56,900
Is it kind of mean that the symbols themselves

719
00:31:56,900 --> 00:31:59,260
don't have meaning or how are the kind of the meanings

720
00:31:59,260 --> 00:32:01,140
shared across different contexts?

721
00:32:01,140 --> 00:32:05,620
Yeah, so that's an interesting question

722
00:32:05,620 --> 00:32:08,140
that I think people have not resolved very well.

723
00:32:08,140 --> 00:32:13,140
So your symbol for your father might play

724
00:32:14,420 --> 00:32:16,140
a bunch of different roles, right?

725
00:32:16,140 --> 00:32:18,700
Because you know what job your father has

726
00:32:18,700 --> 00:32:21,300
and you know what family relations and you know

727
00:32:21,300 --> 00:32:25,940
what hobbies and I don't think that there's good

728
00:32:25,940 --> 00:32:28,940
kind of formalized accounts of how to make sense

729
00:32:28,940 --> 00:32:31,540
of all of that, so there's not great theories

730
00:32:31,540 --> 00:32:33,580
of kind of formalizing conceptual roles.

731
00:32:34,820 --> 00:32:37,420
I'll give some arguments why I think it's possible

732
00:32:37,420 --> 00:32:39,260
that language models are doing this at least

733
00:32:39,260 --> 00:32:42,580
in a tiny version, but in terms of like rich

734
00:32:42,580 --> 00:32:44,740
and kind of human like conceptual roles,

735
00:32:44,740 --> 00:32:47,060
I think that's one of the key problems that's hard to solve,

736
00:32:47,060 --> 00:32:49,740
so, is there another one?

737
00:32:49,740 --> 00:32:50,580
Yeah.

738
00:32:50,580 --> 00:32:55,260
I think that the oncology of the kind of physical world

739
00:32:55,260 --> 00:32:58,140
people think that we're using right now, for example,

740
00:32:58,140 --> 00:33:03,140
is a subset of the oncology of a human's mind

741
00:33:05,300 --> 00:33:08,380
and probably also a subset of all possible

742
00:33:08,380 --> 00:33:12,500
future invented concepts and so on.

743
00:33:13,340 --> 00:33:15,660
Sorry, what was the, I missed the very first part,

744
00:33:15,660 --> 00:33:16,660
what was the question part?

745
00:33:16,660 --> 00:33:19,020
So the question is whether you think

746
00:33:20,300 --> 00:33:23,940
that the existing ontology that you are using now

747
00:33:23,940 --> 00:33:27,420
is a subset of the ontology of human's mind

748
00:33:27,420 --> 00:33:30,540
that we haven't fully explored

749
00:33:30,540 --> 00:33:33,540
and probably that is also a subset of what kind of

750
00:33:33,540 --> 00:33:38,340
can be inventive or creative produced concept

751
00:33:38,340 --> 00:33:39,620
that you mentioned in the beginning.

752
00:33:39,620 --> 00:33:42,300
By ontology, do you mean these theories?

753
00:33:42,300 --> 00:33:45,140
I mean terms, for example, yeah, concepts.

754
00:33:45,140 --> 00:33:46,140
Yeah, concepts.

755
00:33:46,140 --> 00:33:49,140
I mean, I don't think any of these

756
00:33:49,140 --> 00:33:50,940
is quite the right answer, right?

757
00:33:50,940 --> 00:33:54,260
Like these things are actually very difficult to figure out,

758
00:33:55,700 --> 00:33:57,180
but I think they're kind of pointing

759
00:33:57,180 --> 00:33:59,420
in some useful directions or something.

760
00:33:59,420 --> 00:34:03,220
So I don't know if that answers your question, but yeah.

761
00:34:03,220 --> 00:34:05,740
And if you know the symbols that you're using,

762
00:34:05,740 --> 00:34:06,580
why does it matter?

763
00:34:06,580 --> 00:34:09,700
Because that's not to define certain meaning,

764
00:34:09,700 --> 00:34:14,700
whether you use words to represent or you find to represent,

765
00:34:15,660 --> 00:34:16,980
it doesn't matter, right?

766
00:34:18,100 --> 00:34:22,380
In terms of which symbols, like mental representations or?

767
00:34:22,380 --> 00:34:25,860
Yeah, without a concept, whether you use the words

768
00:34:25,860 --> 00:34:28,860
to represent that or you think that's fine,

769
00:34:28,860 --> 00:34:31,300
you think that's all that comes out.

770
00:34:31,300 --> 00:34:33,740
Or like, I'm sure that this kind of concept

771
00:34:33,740 --> 00:34:36,380
has a lot to do with the form,

772
00:34:37,340 --> 00:34:39,700
but I think it's just a little bit of a question.

773
00:34:39,700 --> 00:34:41,220
Yeah, so yeah.

774
00:34:41,220 --> 00:34:44,620
Just to make sure that you, how far do you still have to,

775
00:34:44,620 --> 00:34:45,940
like do you have any?

776
00:34:45,940 --> 00:34:47,300
I have a little ways to go.

777
00:34:47,300 --> 00:34:50,660
Okay, so maybe we should push this the word

778
00:34:50,660 --> 00:34:51,900
after, after at the end,

779
00:34:51,900 --> 00:34:54,780
because it seems like a deeper discussion.

780
00:34:54,780 --> 00:34:56,420
Yeah, yeah, okay, great.

781
00:34:58,620 --> 00:35:03,620
Okay, so I talked about these kinds of accounts of meaning,

782
00:35:04,340 --> 00:35:08,780
particular meaning as conceptual role.

783
00:35:08,780 --> 00:35:11,460
And let me talk a little bit about learning conceptual roles

784
00:35:11,460 --> 00:35:15,820
and why we might think that's plausible or useful.

785
00:35:17,740 --> 00:35:20,540
Seems to me at least that large language models

786
00:35:20,540 --> 00:35:22,620
almost certainly need to learn some of these pieces

787
00:35:22,620 --> 00:35:24,340
of conceptual role,

788
00:35:24,340 --> 00:35:26,140
that these kinds of things seem really necessary

789
00:35:26,140 --> 00:35:29,020
for the stuff large language models are good at, right?

790
00:35:29,020 --> 00:35:31,860
Writing coherent texts or doing translations

791
00:35:32,220 --> 00:35:34,580
or providing definitions or providing elaborations

792
00:35:34,580 --> 00:35:36,700
or explanations, all of those things require you

793
00:35:36,700 --> 00:35:38,740
to put symbols in the right relationships

794
00:35:38,740 --> 00:35:40,740
with other symbols, right?

795
00:35:40,740 --> 00:35:44,660
And that means that to do those things well,

796
00:35:44,660 --> 00:35:46,700
you essentially have to have some little components

797
00:35:46,700 --> 00:35:48,540
of conceptual roles, right?

798
00:35:51,620 --> 00:35:53,780
One way to think about this is that human meanings

799
00:35:53,780 --> 00:35:56,660
or human conceptual roles generated the text, right?

800
00:35:56,660 --> 00:35:59,900
So maybe a smart inferential model could invert that

801
00:35:59,900 --> 00:36:02,660
and figure out what were the likely conceptual roles

802
00:36:02,660 --> 00:36:05,980
that generated the thing that I saw.

803
00:36:05,980 --> 00:36:07,500
I like this, the Stringer quote, right?

804
00:36:07,500 --> 00:36:09,220
The structure of sentences serves as an image

805
00:36:09,220 --> 00:36:10,540
of the structure of thoughts, right?

806
00:36:10,540 --> 00:36:13,820
Some projection of our thoughts or our meanings,

807
00:36:13,820 --> 00:36:16,740
our conceptual roles that gets realized into sentences.

808
00:36:16,740 --> 00:36:17,580
Yeah.

809
00:36:17,580 --> 00:36:18,980
Yeah, so are you gonna follow up on something

810
00:36:18,980 --> 00:36:21,460
that was a great, a recontextuality kind of example?

811
00:36:21,460 --> 00:36:24,700
Is that a recontextuality kind of thing?

812
00:36:24,700 --> 00:36:25,540
Which examples are you talking about?

813
00:36:25,540 --> 00:36:26,660
Are you gonna recontextualize,

814
00:36:26,660 --> 00:36:28,500
let's say a gentleman's two boxes example,

815
00:36:28,500 --> 00:36:30,420
or is that a hyper-projection?

816
00:36:31,860 --> 00:36:33,940
No, I wasn't gonna go back to that.

817
00:36:33,940 --> 00:36:36,380
Yeah, but I'll talk about a study

818
00:36:36,380 --> 00:36:40,020
in Large-Range Models in a minute, okay.

819
00:36:41,260 --> 00:36:42,980
Okay, a lot of people have the intuition

820
00:36:42,980 --> 00:36:43,820
this is not possible.

821
00:36:43,820 --> 00:36:46,740
I think this is kind of the Bender and Marcus intuition

822
00:36:46,740 --> 00:36:48,380
that our thoughts really get projected

823
00:36:48,380 --> 00:36:50,780
into this kind of impoverished sequence of sounds, right?

824
00:36:50,780 --> 00:36:52,180
How could you discover something

825
00:36:52,180 --> 00:36:55,740
like rich conceptual roles there, right?

826
00:36:55,740 --> 00:36:58,060
If you just have this projection of language,

827
00:36:58,060 --> 00:37:00,340
how could that ever support rich and interesting

828
00:37:00,340 --> 00:37:02,220
kinds of conceptual roles?

829
00:37:04,580 --> 00:37:07,260
One kind of way that I think is a helpful analogy,

830
00:37:07,260 --> 00:37:10,180
although not kind of a mathematically precise

831
00:37:10,180 --> 00:37:11,860
implementation or something,

832
00:37:11,860 --> 00:37:13,660
people may know these embedding theorems

833
00:37:13,660 --> 00:37:17,260
from dynamical systems, which I think are very cool.

834
00:37:17,260 --> 00:37:20,140
There's this paper called Geometry from a Time Series,

835
00:37:21,140 --> 00:37:23,140
which essentially shows that in some cases

836
00:37:23,140 --> 00:37:25,900
you can take projections of dynamical systems

837
00:37:25,940 --> 00:37:28,700
and recover things which capture the structure

838
00:37:28,700 --> 00:37:31,420
of the dynamics from that projection.

839
00:37:31,420 --> 00:37:33,300
So in particular, in this paper, they go through this,

840
00:37:33,300 --> 00:37:35,660
which is the Rossler Attractor.

841
00:37:35,660 --> 00:37:38,940
This is a three-dimensional system

842
00:37:38,940 --> 00:37:41,340
of differential equations.

843
00:37:41,340 --> 00:37:44,140
And what you can do is take a one-dimensional projection

844
00:37:44,140 --> 00:37:45,220
of those dynamics.

845
00:37:45,220 --> 00:37:47,060
So you can look at just the X location

846
00:37:47,060 --> 00:37:49,580
of what's happening there.

847
00:37:49,580 --> 00:37:53,180
And through a clever trick essentially translating

848
00:37:53,180 --> 00:37:56,060
the one-dimensions into three-dimensions

849
00:37:56,060 --> 00:38:01,060
using, by going backwards in time, some number of steps,

850
00:38:01,260 --> 00:38:03,060
you can actually recover the structure of this

851
00:38:03,060 --> 00:38:04,900
from the one-dimensional projection.

852
00:38:06,780 --> 00:38:09,420
And there's other kind of general theorems

853
00:38:09,420 --> 00:38:10,820
about when this is possible,

854
00:38:10,820 --> 00:38:13,340
Parkinson's embedding theorem and things like that.

855
00:38:13,340 --> 00:38:17,580
The point here is that we shouldn't really have

856
00:38:17,580 --> 00:38:19,300
strong intuitions about what's possible

857
00:38:19,300 --> 00:38:21,620
from some projection of thoughts,

858
00:38:21,620 --> 00:38:24,980
because oftentimes there might be possible

859
00:38:24,980 --> 00:38:28,100
for people to, or for learning models

860
00:38:28,100 --> 00:38:31,420
to reconstruct kind of interesting parts

861
00:38:31,420 --> 00:38:33,820
of the structure of some system

862
00:38:33,820 --> 00:38:37,220
just from simple kind of measurements of that system.

863
00:38:37,220 --> 00:38:41,100
Actually Shaw here, the senior author wrote an entire book

864
00:38:41,100 --> 00:38:44,300
on recovering the kind of dynamical properties

865
00:38:44,300 --> 00:38:45,620
of a dripping water faucet,

866
00:38:46,840 --> 00:38:48,940
where you can measure the time between drips

867
00:38:48,940 --> 00:38:51,340
and figure out things about the kind of latent variables

868
00:38:51,340 --> 00:38:52,940
and latent structures they're using techniques

869
00:38:52,940 --> 00:38:54,460
that are a lot like these.

870
00:38:55,820 --> 00:38:58,220
In psychology, actually people have also been interested

871
00:38:58,220 --> 00:39:01,340
in kind of closely related types of models.

872
00:39:01,340 --> 00:39:06,000
So there's this work by Roger Shepard in the 80s,

873
00:39:06,000 --> 00:39:09,020
which essentially would take behavioral judgments

874
00:39:09,020 --> 00:39:12,780
and try to infer the underlying structures behind them.

875
00:39:12,780 --> 00:39:17,260
So for example, this matrix here is different colors

876
00:39:17,260 --> 00:39:18,700
or different wavelengths of light

877
00:39:18,700 --> 00:39:22,260
and then confusability between them on judgment tasks.

878
00:39:22,260 --> 00:39:24,100
So just how similar are these things

879
00:39:24,100 --> 00:39:26,300
or how confusable is one color with another.

880
00:39:27,380 --> 00:39:29,960
And Shepard was using multi-dimensional scaling

881
00:39:29,960 --> 00:39:35,020
to go from data like this up to representation like this,

882
00:39:35,020 --> 00:39:37,180
which you might recognize as a color wheel.

883
00:39:37,180 --> 00:39:40,660
Basically you can arrange points

884
00:39:40,660 --> 00:39:42,380
so that their distances correspond

885
00:39:42,380 --> 00:39:45,700
to the distances in the confusion matrix

886
00:39:45,700 --> 00:39:48,220
and therefore recover something

887
00:39:48,220 --> 00:39:49,500
about the kind of underlying,

888
00:39:49,500 --> 00:39:51,540
in this case, psychological structure

889
00:39:51,540 --> 00:39:53,180
that generated that data.

890
00:39:55,860 --> 00:39:59,460
People have also done similar kinds of things

891
00:39:59,460 --> 00:40:02,980
in learning kind of real formalized versions

892
00:40:02,980 --> 00:40:06,620
of theories or of intuitive theories.

893
00:40:06,620 --> 00:40:09,300
I really like this paper by Tomer Ullmann

894
00:40:09,300 --> 00:40:11,940
and Noah Goodman and Josh who's speaking next

895
00:40:11,940 --> 00:40:14,100
on learning a theory of magnetism.

896
00:40:14,100 --> 00:40:15,780
So basically you take observations

897
00:40:15,780 --> 00:40:20,780
of which objects interact with other objects

898
00:40:20,860 --> 00:40:25,860
and do some learning to acquire a kind of high level theory

899
00:40:26,340 --> 00:40:30,120
of the fact that there are two different kinds

900
00:40:30,120 --> 00:40:33,360
of magnetic objects and those two different kinds

901
00:40:33,360 --> 00:40:34,600
of things will interact with each other,

902
00:40:34,600 --> 00:40:35,740
but they won't interact with things

903
00:40:35,740 --> 00:40:37,380
that are non-magnetic.

904
00:40:37,380 --> 00:40:40,020
So this is like a little tiny mini intuitive theory

905
00:40:40,020 --> 00:40:43,060
that you can acquire just from very simple,

906
00:40:43,060 --> 00:40:44,580
you might think kind of impoverished data

907
00:40:44,580 --> 00:40:46,240
about interactions.

908
00:40:47,740 --> 00:40:51,640
So when people talk about LOMs just being based on text,

909
00:40:51,640 --> 00:40:52,980
I think that isn't really enough

910
00:40:52,980 --> 00:40:55,620
to conclude anything about what theories they might induce,

911
00:40:55,620 --> 00:40:57,140
or what kinds of internal structures

912
00:40:57,140 --> 00:41:01,020
and conceptual roles they might induce from that text.

913
00:41:01,020 --> 00:41:02,460
And in fact, there's some evidence I think

914
00:41:02,460 --> 00:41:07,460
that what they are inducing looks pretty plausible

915
00:41:07,780 --> 00:41:09,860
at least in kind of simple domains.

916
00:41:09,860 --> 00:41:14,700
So there's this paper by Grandin and colleagues

917
00:41:14,700 --> 00:41:18,020
which essentially looked at word embedding vectors

918
00:41:18,020 --> 00:41:21,980
and projected them onto say intuitive dimensions.

919
00:41:21,980 --> 00:41:24,580
So here you have a bunch of words,

920
00:41:24,580 --> 00:41:26,540
you project them onto this line,

921
00:41:26,540 --> 00:41:29,020
which is the line connecting small and large.

922
00:41:29,020 --> 00:41:31,900
Okay, so all of our high dimensional word vectors

923
00:41:31,900 --> 00:41:34,780
get projected onto the small versus large line.

924
00:41:34,780 --> 00:41:36,780
And we take that as a way of measuring

925
00:41:36,780 --> 00:41:39,660
how large versus small different objects are.

926
00:41:40,580 --> 00:41:42,020
And then the question is,

927
00:41:42,020 --> 00:41:46,020
is in a model trained only on text prediction,

928
00:41:46,020 --> 00:41:51,020
is that, does that projection recover anything human like

929
00:41:51,120 --> 00:41:54,180
about the underlying conceptual spaces?

930
00:41:54,180 --> 00:41:55,300
And they show yes it does.

931
00:41:55,300 --> 00:41:58,780
So here's six plots where the x-axis

932
00:41:58,780 --> 00:42:01,180
is the semantic projection, right?

933
00:42:01,180 --> 00:42:03,860
So how far on that small to large line something is.

934
00:42:03,860 --> 00:42:05,800
And then the y-axis is human ratings

935
00:42:05,800 --> 00:42:09,280
of how small versus large an object is, right?

936
00:42:09,280 --> 00:42:11,080
You can see that the correlations here are not perfect

937
00:42:11,080 --> 00:42:13,080
but they're also not garbage, right?

938
00:42:13,080 --> 00:42:17,960
They're actually quite strong I think for a model like this

939
00:42:17,960 --> 00:42:21,020
that things which the model calls wet versus dry

940
00:42:21,020 --> 00:42:24,400
or big versus small or dangerous versus safe,

941
00:42:25,440 --> 00:42:27,900
people also agree with, right?

942
00:42:27,900 --> 00:42:29,740
So just in predicting text,

943
00:42:29,740 --> 00:42:32,080
this thing has recovered these kinds of aspects

944
00:42:32,080 --> 00:42:34,720
of semantic structure latent

945
00:42:34,720 --> 00:42:37,680
in the word vector representations, yeah.

946
00:42:37,680 --> 00:42:40,360
But this is probably sitting there in n-grams

947
00:42:40,360 --> 00:42:42,760
and in bi-grams in fact, that information, right?

948
00:42:42,760 --> 00:42:45,360
It doesn't have to do anything with the real world.

949
00:42:46,360 --> 00:42:48,440
Well, it does have something to do with the real world.

950
00:42:48,440 --> 00:42:52,760
It's like a small and large

951
00:42:52,760 --> 00:42:55,080
could just be linguistic constructs

952
00:42:55,080 --> 00:42:57,320
and you're testing it on language.

953
00:42:57,320 --> 00:43:00,200
Oh, I see, you think it's that you say small tiger

954
00:43:00,200 --> 00:43:01,880
versus large tiger or something.

955
00:43:02,320 --> 00:43:04,160
Small puppy, right?

956
00:43:04,160 --> 00:43:06,680
Puppy is always small, tiger is always large, yeah.

957
00:43:06,680 --> 00:43:09,960
So it might be true in n-grams, I'm not sure.

958
00:43:09,960 --> 00:43:12,160
I don't think that they looked at that.

959
00:43:13,800 --> 00:43:18,640
I don't think that defeats the argument though, right?

960
00:43:18,640 --> 00:43:20,960
Because I think it is the case that

961
00:43:22,080 --> 00:43:24,200
even n-gram statistics are statistics

962
00:43:24,200 --> 00:43:26,400
about word relations, right?

963
00:43:26,400 --> 00:43:28,240
So it might be that you don't need fancy language models

964
00:43:28,240 --> 00:43:29,720
or something to do this.

965
00:43:30,200 --> 00:43:31,360
But you're not actually like,

966
00:43:31,360 --> 00:43:33,600
you don't need real world for this to work.

967
00:43:35,120 --> 00:43:37,920
Well, the real world generated

968
00:43:37,920 --> 00:43:40,840
how often you hear small puppy versus large puppy, right?

969
00:43:40,840 --> 00:43:44,920
So the real world is mirrored in those statistics

970
00:43:44,920 --> 00:43:47,400
and then the configuration that system comes up with

971
00:43:47,400 --> 00:43:50,520
is also one that mirrors those properties of the real world.

972
00:43:50,520 --> 00:43:51,360
Yeah.

973
00:43:52,640 --> 00:43:53,960
Can I put back on that as well?

974
00:43:53,960 --> 00:43:56,320
I do kind of feel like what others are saying is right,

975
00:43:56,320 --> 00:43:58,720
but this is much closer to n-grams

976
00:43:58,720 --> 00:44:00,680
than it is to large language models.

977
00:44:00,680 --> 00:44:03,440
And the properties we're seeing are just so much wilder

978
00:44:03,440 --> 00:44:07,240
than any of these embedding tricks in practice.

979
00:44:07,240 --> 00:44:08,520
Oh, you mean that large language model

980
00:44:08,520 --> 00:44:10,160
is much smarter than this?

981
00:44:10,160 --> 00:44:12,880
I don't even see how this is comparable in a way, right?

982
00:44:12,880 --> 00:44:15,320
Like this pops out of PCA,

983
00:44:15,320 --> 00:44:18,040
whereas we're seeing these wild emergent behaviors

984
00:44:18,040 --> 00:44:18,920
come out of large language models.

985
00:44:18,920 --> 00:44:20,400
Yeah, yeah, so I mean,

986
00:44:20,400 --> 00:44:22,680
I don't think this explains wild emergent behaviors.

987
00:44:22,680 --> 00:44:25,120
I think that this was just trying to say

988
00:44:25,120 --> 00:44:27,840
that when you train on text prediction,

989
00:44:27,840 --> 00:44:29,480
you configure yourself to align

990
00:44:29,480 --> 00:44:31,840
with some of the true properties of the world,

991
00:44:31,840 --> 00:44:33,960
which are reflected in the text analysis.

992
00:44:33,960 --> 00:44:34,960
That's all, yeah.

993
00:44:37,320 --> 00:44:38,320
Okay, so I'm short on time.

994
00:44:38,320 --> 00:44:39,160
I'll skip this.

995
00:44:39,160 --> 00:44:40,840
I'll just say that there's other papers

996
00:44:40,840 --> 00:44:45,600
looking at transformers and kind of how they relate

997
00:44:45,600 --> 00:44:48,800
to classic studies on concepts in cognitive science,

998
00:44:48,800 --> 00:44:51,360
classic kinds of effects, but I'll skip that.

999
00:44:52,880 --> 00:44:54,080
Maybe I'll go very briefly

1000
00:44:54,080 --> 00:44:56,200
just through this kind of fun experiment.

1001
00:44:56,200 --> 00:44:59,080
This is Mark Gorenstein in my lab

1002
00:44:59,080 --> 00:45:02,720
has been interested in learning concepts

1003
00:45:02,720 --> 00:45:05,880
just from linguistic experience,

1004
00:45:05,880 --> 00:45:07,760
maybe linguistic prediction.

1005
00:45:07,760 --> 00:45:10,360
He's been doing these kind of cool experiments

1006
00:45:10,360 --> 00:45:14,400
where we give people passages of natural language

1007
00:45:14,400 --> 00:45:16,400
where there's some blanks.

1008
00:45:16,400 --> 00:45:18,120
So here's a passage.

1009
00:45:18,120 --> 00:45:19,640
The myth of blank is so powerful

1010
00:45:19,640 --> 00:45:21,160
that the very words conjure up blank,

1011
00:45:21,160 --> 00:45:25,040
of strudel and blank in a cozy Vietnese cafe, blah, blah, blah.

1012
00:45:25,040 --> 00:45:27,720
And the job of participants in this

1013
00:45:27,720 --> 00:45:31,000
is to learn where to put the word DAX.

1014
00:45:31,000 --> 00:45:32,240
Okay, so DAX is a novel word

1015
00:45:32,240 --> 00:45:34,160
they've never encountered before.

1016
00:45:35,360 --> 00:45:38,440
You have to read this and understand the context and stuff

1017
00:45:38,440 --> 00:45:42,760
in order to figure that out and see where DAX should go.

1018
00:45:42,760 --> 00:45:47,440
Secretly behind the scenes, this example has been chosen

1019
00:45:47,440 --> 00:45:51,600
as just from a big corpus of text of a really rare word

1020
00:45:51,600 --> 00:45:53,400
that people probably don't know.

1021
00:45:53,800 --> 00:45:56,600
So the rare word here is soccer tort.

1022
00:45:57,520 --> 00:46:00,240
And that means that this language,

1023
00:46:00,240 --> 00:46:02,840
like where soccer tort actually occurred here,

1024
00:46:03,840 --> 00:46:05,960
was generated from real people

1025
00:46:05,960 --> 00:46:07,960
and presumably reflects the underlying meaning

1026
00:46:07,960 --> 00:46:09,680
and things of soccer tort.

1027
00:46:10,680 --> 00:46:13,200
Maybe I don't know if I'm saying that correctly.

1028
00:46:13,200 --> 00:46:15,560
But with enough examples of these people

1029
00:46:15,560 --> 00:46:16,880
will learn where DAX is,

1030
00:46:16,880 --> 00:46:19,080
they get feedback on whether or not they were correct,

1031
00:46:19,080 --> 00:46:20,960
according to whether they chose the place

1032
00:46:21,000 --> 00:46:23,640
where soccer tort actually appeared.

1033
00:46:23,640 --> 00:46:27,080
Okay, so we're having them do kind of a version

1034
00:46:27,080 --> 00:46:28,880
of a prediction task,

1035
00:46:30,000 --> 00:46:33,560
trying to figure out where this word goes,

1036
00:46:33,560 --> 00:46:34,920
but they don't actually see the word,

1037
00:46:34,920 --> 00:46:36,760
they see it as DAX.

1038
00:46:38,520 --> 00:46:40,440
People get pretty decent at this,

1039
00:46:40,440 --> 00:46:44,760
so up to 80% or so, depending on the word.

1040
00:46:46,080 --> 00:46:48,960
These are just, sorry, these are the examples of the words

1041
00:46:49,000 --> 00:46:51,600
which generated the unseen context.

1042
00:46:52,760 --> 00:46:54,400
And after that, we asked them a bunch of questions.

1043
00:46:54,400 --> 00:46:56,400
So we asked them some reading comprehension,

1044
00:46:56,400 --> 00:46:58,840
we asked them feature questions about DAX,

1045
00:46:58,840 --> 00:47:00,320
is DAX a man-made object?

1046
00:47:00,320 --> 00:47:02,240
Do biologists typically study DAX?

1047
00:47:02,240 --> 00:47:04,200
Do people use DAX in painting?

1048
00:47:04,200 --> 00:47:06,960
Just a whole collection of basic feature,

1049
00:47:06,960 --> 00:47:08,800
kind of concept-y questions.

1050
00:47:08,800 --> 00:47:10,680
We give them an image recognition task,

1051
00:47:10,680 --> 00:47:13,160
right, picking out soccer tort,

1052
00:47:13,160 --> 00:47:17,040
the real thing versus alternatives.

1053
00:47:17,280 --> 00:47:19,720
And we also asked them for explicit definitions, right?

1054
00:47:19,720 --> 00:47:21,440
These contexts are not definitions,

1055
00:47:21,440 --> 00:47:23,640
they're not saying here's what a soccer tort is,

1056
00:47:23,640 --> 00:47:27,560
it's naturalistic usages of the object.

1057
00:47:27,560 --> 00:47:30,080
And what we find actually is within,

1058
00:47:33,360 --> 00:47:36,600
within about 20 trials or so,

1059
00:47:36,600 --> 00:47:39,240
sorry, everything is after 20 trials,

1060
00:47:39,240 --> 00:47:40,760
people are actually very, very good

1061
00:47:40,760 --> 00:47:45,240
at judging conceptual features for these concepts,

1062
00:47:45,280 --> 00:47:48,040
almost at ceiling in most of the kinds

1063
00:47:48,040 --> 00:47:50,560
of feature questions we asked them.

1064
00:47:50,560 --> 00:47:54,200
Here's each word on a row,

1065
00:47:54,200 --> 00:47:57,640
and then features here on the x-axis,

1066
00:47:57,640 --> 00:48:00,080
almost everything is read, meaning they're good at this.

1067
00:48:00,080 --> 00:48:05,080
They're also good at picking which picture is the object,

1068
00:48:05,080 --> 00:48:06,760
so they've never encountered any pictures at all,

1069
00:48:06,760 --> 00:48:10,920
but they're 80% or so good at picking these things out.

1070
00:48:10,920 --> 00:48:12,760
And they're even good at giving definitions

1071
00:48:12,760 --> 00:48:14,240
for these terms, okay?

1072
00:48:14,240 --> 00:48:16,680
So here's a dictionary or dictionary

1073
00:48:16,680 --> 00:48:18,440
or something definition of soccer tort,

1074
00:48:18,440 --> 00:48:20,800
it's a chocolate cake or tort of Austrian origin

1075
00:48:20,800 --> 00:48:24,040
invented by Franz soccer supposedly in 1832

1076
00:48:24,040 --> 00:48:26,600
for some prince in Vienna.

1077
00:48:26,600 --> 00:48:28,120
And people just from these contexts,

1078
00:48:28,120 --> 00:48:30,320
20 of them will learn things like it's a chocolate dessert

1079
00:48:30,320 --> 00:48:31,760
similar to a cake that was originally

1080
00:48:31,760 --> 00:48:33,600
and most commonly made in Vienna.

1081
00:48:33,600 --> 00:48:34,800
I think it's a type of chocolate cake

1082
00:48:34,800 --> 00:48:37,280
that can be ordered for dessert in Austria,

1083
00:48:37,280 --> 00:48:39,760
kind of rich chocolate cake from Vienna and so on, okay?

1084
00:48:39,760 --> 00:48:44,760
So people are pretty good at taking

1085
00:48:46,520 --> 00:48:48,200
kind of in-context language use

1086
00:48:48,200 --> 00:48:50,320
and figuring out underlying aspects

1087
00:48:50,320 --> 00:48:52,680
of conceptual representation from that.

1088
00:48:56,120 --> 00:48:57,760
But they will never be able to taste it.

1089
00:48:57,760 --> 00:48:58,920
I mean, it tastes so good.

1090
00:48:58,920 --> 00:49:00,920
Have you had it?

1091
00:49:00,920 --> 00:49:03,800
I've never heard of it, so okay.

1092
00:49:03,800 --> 00:49:05,080
It's great.

1093
00:49:05,080 --> 00:49:06,800
Okay, so let me just wrap up here.

1094
00:49:06,800 --> 00:49:11,800
So I think of these kind of conceptual roles or theories

1095
00:49:12,280 --> 00:49:14,080
as really both a strength and a weakness

1096
00:49:14,080 --> 00:49:17,200
of these current large language models.

1097
00:49:18,040 --> 00:49:19,520
One is that large language models,

1098
00:49:19,520 --> 00:49:21,200
I think, seem very good at learning

1099
00:49:21,200 --> 00:49:23,640
kind of shallow but broad theories, right?

1100
00:49:23,640 --> 00:49:26,120
So things like could shoes be made out of eggplants

1101
00:49:26,120 --> 00:49:28,760
or what would happen if shoes were made out of eggplants, right?

1102
00:49:28,760 --> 00:49:33,760
They would know what some bad downsides

1103
00:49:33,760 --> 00:49:36,000
of that kind of thing might be, right?

1104
00:49:36,000 --> 00:49:40,080
Or answer basic kinds of questions

1105
00:49:40,080 --> 00:49:43,880
that might rely on kind of reasoning

1106
00:49:43,880 --> 00:49:47,440
through one or two kind of links about the relationships

1107
00:49:47,440 --> 00:49:50,420
between the objects involved in a situation like that.

1108
00:49:51,440 --> 00:49:52,960
I think it's been very surprising to people

1109
00:49:52,960 --> 00:49:55,520
that this works so well, right?

1110
00:49:55,520 --> 00:49:58,480
And part of, I think, what makes it surprising

1111
00:49:58,480 --> 00:50:03,480
is that these models are able to be trained

1112
00:50:03,680 --> 00:50:05,880
on a huge number of words, right?

1113
00:50:05,880 --> 00:50:07,960
And so sort of superficially knowing a little bit

1114
00:50:07,960 --> 00:50:10,120
about conceptual roles of a huge number of words

1115
00:50:10,120 --> 00:50:11,920
seems to get you pretty far

1116
00:50:12,920 --> 00:50:14,440
in terms of seeming convincing

1117
00:50:14,440 --> 00:50:16,660
and in terms of language production.

1118
00:50:17,600 --> 00:50:19,680
But it's also these conceptual roles and theories

1119
00:50:19,680 --> 00:50:21,720
are also weakness and they don't seem very good

1120
00:50:21,720 --> 00:50:23,520
at robust and precise theories, right?

1121
00:50:23,520 --> 00:50:25,480
So if you think about conceptual roles

1122
00:50:25,480 --> 00:50:26,780
like in mathematics, right?

1123
00:50:26,780 --> 00:50:28,960
How you define say a natural number

1124
00:50:28,960 --> 00:50:30,800
or how you define an integral or something, right?

1125
00:50:30,800 --> 00:50:34,200
Like all of those are symbolic kinds of theories

1126
00:50:34,200 --> 00:50:37,080
which are precise and which support

1127
00:50:37,080 --> 00:50:39,720
chains of reasoning of arbitrary length, right?

1128
00:50:39,720 --> 00:50:41,440
And that's what these systems really seem

1129
00:50:41,440 --> 00:50:43,960
not to be very good at.

1130
00:50:43,960 --> 00:50:46,000
Likely that's because there's some important things

1131
00:50:46,000 --> 00:50:47,040
which are missing, right?

1132
00:50:47,040 --> 00:50:50,880
Things like grounding, things like reasoning

1133
00:50:50,880 --> 00:50:53,440
or even richer kinds of theories.

1134
00:50:53,440 --> 00:50:57,100
I think Josh will talk about this some next.

1135
00:50:58,100 --> 00:51:01,080
I think that there's a kind of broader view

1136
00:51:01,080 --> 00:51:03,900
of concepts and meanings, which is really,

1137
00:51:03,900 --> 00:51:05,260
I think the most exciting for people

1138
00:51:05,260 --> 00:51:09,420
that work on concepts and concept representations

1139
00:51:09,420 --> 00:51:12,940
in cognitive psychology, which is that large language models

1140
00:51:12,940 --> 00:51:14,540
have really shown how vectors can do things

1141
00:51:14,540 --> 00:51:16,460
that were long thought to be impossible

1142
00:51:16,460 --> 00:51:18,180
for non-symbolic models, right?

1143
00:51:18,180 --> 00:51:19,760
In particular, these kinds of arguments

1144
00:51:19,760 --> 00:51:22,060
from people like Fodor and Polition

1145
00:51:22,060 --> 00:51:25,540
about compositionality and systematicity and productivity,

1146
00:51:25,540 --> 00:51:26,380
right?

1147
00:51:26,380 --> 00:51:29,020
All of these kinds of things that people have pointed to

1148
00:51:29,020 --> 00:51:32,140
as characteristic features of thinking

1149
00:51:32,660 --> 00:51:34,700
have argued were characteristic features

1150
00:51:34,700 --> 00:51:37,820
of symbolic thinking just turn out not to be right, right?

1151
00:51:37,820 --> 00:51:41,540
It turns out you can get vectors to do those things.

1152
00:51:41,540 --> 00:51:43,120
And I would argue that the solution

1153
00:51:43,120 --> 00:51:44,980
to why vectors could do those things

1154
00:51:44,980 --> 00:51:47,900
is probably that what these models are doing

1155
00:51:47,900 --> 00:51:51,500
is training vectors that encode conceptual roles, right?

1156
00:51:51,500 --> 00:51:54,580
Like what they're learning is representations of meaning

1157
00:51:54,580 --> 00:51:58,140
which capture the important parts of conceptual roles.

1158
00:51:58,140 --> 00:52:01,980
This is actually something which has been long sought after

1159
00:52:01,980 --> 00:52:04,140
in say computational neuroscience.

1160
00:52:04,140 --> 00:52:06,260
There's things like vector symbolic architectures

1161
00:52:06,260 --> 00:52:09,940
that are very exciting ways of encoding

1162
00:52:09,940 --> 00:52:12,220
say arbitrary symbolic systems

1163
00:52:12,220 --> 00:52:16,700
or arbitrary mathematical systems into vectors.

1164
00:52:16,700 --> 00:52:19,700
And I think that some marriage of those two things

1165
00:52:19,700 --> 00:52:22,900
is going to be very exciting.

1166
00:52:24,500 --> 00:52:26,380
So large language models point to a theory of meaning

1167
00:52:26,380 --> 00:52:30,660
that's based on essentially vector based conceptual roles,

1168
00:52:31,020 --> 00:52:34,060
and perhaps can capture a lot of the different features

1169
00:52:34,060 --> 00:52:38,820
of meaning that people in say cognitive science

1170
00:52:38,820 --> 00:52:42,900
or cognitive development have tried to kind of bring out

1171
00:52:42,900 --> 00:52:44,540
in human conceptual systems, right?

1172
00:52:44,540 --> 00:52:45,980
Like that our meanings are gradient

1173
00:52:45,980 --> 00:52:47,980
or that they have hierarchies,

1174
00:52:47,980 --> 00:52:49,620
that we know things like definitions

1175
00:52:49,620 --> 00:52:51,700
and we can make inferences about relationships

1176
00:52:51,700 --> 00:52:55,500
and similarities and all of those things seem like things

1177
00:52:55,500 --> 00:52:58,540
that you can encode at least in principle in vectors

1178
00:52:59,540 --> 00:53:00,980
which is great.

1179
00:53:01,940 --> 00:53:06,020
So let's get that, let me just end there.

1180
00:53:06,020 --> 00:53:07,940
I'll thank you again for the invitation

1181
00:53:07,940 --> 00:53:11,540
and thanks also to all of my co-authors on the work here.

1182
00:53:11,540 --> 00:53:16,540
All right.

1183
00:53:16,700 --> 00:53:17,540
Yes.

1184
00:53:19,860 --> 00:53:21,740
Maybe I'm reading too much into it

1185
00:53:21,740 --> 00:53:24,380
but it seemed to me that you hinted at

1186
00:53:26,020 --> 00:53:27,940
what these vector representations

1187
00:53:28,700 --> 00:53:31,260
large language models tell us about

1188
00:53:31,260 --> 00:53:35,700
both human cognition as well as about language,

1189
00:53:35,700 --> 00:53:37,020
the nature of language.

1190
00:53:38,020 --> 00:53:40,900
Could I ask you to do a projective measurement

1191
00:53:40,900 --> 00:53:43,700
and come out and say something about that?

1192
00:53:45,940 --> 00:53:50,940
I think that they tell us that vectors are really plausible.

1193
00:53:51,940 --> 00:53:55,300
They kind of show us how vectors are plausible for meanings,

1194
00:53:55,820 --> 00:53:58,940
and the way in which I think that they're plausible

1195
00:53:58,940 --> 00:54:01,660
for meanings is that they encode conceptual roles.

1196
00:54:02,820 --> 00:54:03,660
That's what I would say.

1197
00:54:03,660 --> 00:54:05,860
And I think until them,

1198
00:54:05,860 --> 00:54:07,380
until kind of recent deep learning,

1199
00:54:07,380 --> 00:54:08,980
I think it was really unclear.

1200
00:54:08,980 --> 00:54:12,380
So people had argued for decades about whether

1201
00:54:12,380 --> 00:54:14,500
the foundation of concepts was definitions

1202
00:54:14,500 --> 00:54:16,300
or is it like somehow similarities

1203
00:54:16,300 --> 00:54:17,660
or is it that you just know a word

1204
00:54:17,660 --> 00:54:21,140
and you know a bunch of associated features or whatever.

1205
00:54:22,100 --> 00:54:26,060
And I think one of the main insights, for example,

1206
00:54:26,060 --> 00:54:29,860
is that you can extract a definition

1207
00:54:29,860 --> 00:54:31,180
from a large language model.

1208
00:54:31,180 --> 00:54:32,340
We've even given it some of these

1209
00:54:32,340 --> 00:54:33,820
kind of human experiments we've done

1210
00:54:33,820 --> 00:54:35,740
and they're pretty good at coming up

1211
00:54:35,740 --> 00:54:39,220
with the chocolate torch kind of definitions from those.

1212
00:54:39,220 --> 00:54:40,780
And that tells you that the definitions

1213
00:54:40,780 --> 00:54:43,740
can be encoded into vectors, right?

1214
00:54:43,740 --> 00:54:44,740
And that's great, right?

1215
00:54:44,740 --> 00:54:49,100
That means that you don't need to think about definitions

1216
00:54:49,100 --> 00:54:51,860
as the defining part of concepts, right?

1217
00:54:51,860 --> 00:54:54,540
There's some other kind of more abstract,

1218
00:54:54,540 --> 00:54:57,100
you know, high dimensional space or whatever

1219
00:54:57,100 --> 00:54:58,460
that defines the meanings.

1220
00:54:58,460 --> 00:55:00,180
And the sense in which it defines the meanings

1221
00:55:00,180 --> 00:55:02,140
is in terms of the relationships between vectors

1222
00:55:02,140 --> 00:55:04,340
on the tasks that you use the concepts for.

1223
00:55:05,340 --> 00:55:08,500
Is it just natural since the brain encodes information

1224
00:55:08,500 --> 00:55:11,420
with lots of neurons firing through your brain?

1225
00:55:11,420 --> 00:55:13,100
This should not be surprising.

1226
00:55:13,100 --> 00:55:17,060
So it's not surprising, it was always unclear

1227
00:55:17,060 --> 00:55:18,540
how that was even possible.

1228
00:55:19,100 --> 00:55:23,900
Yeah, exactly, yeah, yeah, yeah.

1229
00:55:23,900 --> 00:55:25,540
It's like everybody always kind of knew

1230
00:55:25,540 --> 00:55:27,580
that there had to be a continuous system

1231
00:55:27,580 --> 00:55:28,820
which could support these things.

1232
00:55:28,820 --> 00:55:30,900
But when you look at, you know, the discreteness

1233
00:55:30,900 --> 00:55:32,860
in language or this discreteness in mathematics,

1234
00:55:32,860 --> 00:55:35,340
it was always kind of unclear where that could come from.

1235
00:55:35,340 --> 00:55:36,900
So that's why I think things like vector symbolic

1236
00:55:36,900 --> 00:55:39,660
architectures are very exciting too.

1237
00:55:39,660 --> 00:55:42,900
So when a human fills in the meaning,

1238
00:55:42,900 --> 00:55:44,300
they're using a lot of context

1239
00:55:44,300 --> 00:55:46,140
that they've gotten from the real world.

1240
00:55:46,140 --> 00:55:49,180
When an octopus tries to fill in the meaning,

1241
00:55:49,180 --> 00:55:52,180
they have much less context to work with.

1242
00:55:52,180 --> 00:55:54,300
And when a LLM fills in the meaning,

1243
00:55:54,300 --> 00:55:57,500
they have no kind of context to work with.

1244
00:55:57,500 --> 00:55:58,620
So I was wondering if you had thoughts

1245
00:55:58,620 --> 00:56:01,980
about the differences and that implies.

1246
00:56:01,980 --> 00:56:04,620
It's really interesting to think of what's exactly

1247
00:56:04,620 --> 00:56:06,060
happening in that human experiment

1248
00:56:06,060 --> 00:56:08,260
because I agree it's transfer of stuff you know,

1249
00:56:08,260 --> 00:56:09,420
like you've encountered cakes

1250
00:56:09,420 --> 00:56:12,660
and you've encountered fancy pastries or whatever.

1251
00:56:12,700 --> 00:56:16,140
And part of what you know about those meanings

1252
00:56:16,140 --> 00:56:17,980
are the grounded parts, right?

1253
00:56:17,980 --> 00:56:18,980
You know what a cake looks like,

1254
00:56:18,980 --> 00:56:23,620
which is why you can recognize the pictures and things.

1255
00:56:23,620 --> 00:56:25,380
I always have a little bit of trouble thinking about it

1256
00:56:25,380 --> 00:56:27,700
because it's never quite clear to me exactly

1257
00:56:27,700 --> 00:56:29,780
what it means to transfer something grounded.

1258
00:56:29,780 --> 00:56:31,780
Like it feels a little bit like in order for it

1259
00:56:31,780 --> 00:56:34,500
to transfer at all, it has to be a little bit abstract.

1260
00:56:36,140 --> 00:56:38,060
But I agree that that's the right question to ask

1261
00:56:38,060 --> 00:56:40,700
and we don't have any theories or certainly no evidence

1262
00:56:40,700 --> 00:56:43,780
about how exactly people solve that problem

1263
00:56:43,780 --> 00:56:46,700
or the way in which it relies on conceptual roles

1264
00:56:46,700 --> 00:56:49,460
versus grounded experience or something.

1265
00:56:49,460 --> 00:56:51,060
Okay, so we'll have one more question.

1266
00:56:51,060 --> 00:56:53,580
Meanwhile, maybe we can have the next speakers

1267
00:56:53,580 --> 00:56:54,660
start setting up.

1268
00:56:58,340 --> 00:57:00,980
Yeah, thanks Steve for the great talk.

1269
00:57:00,980 --> 00:57:03,340
I wanted to understand better what the argument was

1270
00:57:03,340 --> 00:57:07,660
in this kind of conceptual embedding experiment

1271
00:57:07,660 --> 00:57:09,980
because it seems like you could just ask the LLM

1272
00:57:10,460 --> 00:57:11,620
whether it's tall or not.

1273
00:57:11,620 --> 00:57:14,260
Like you didn't really need to do this projection

1274
00:57:14,260 --> 00:57:16,420
to know that it can do this task.

1275
00:57:16,420 --> 00:57:19,020
So is it somehow, is there something special

1276
00:57:19,020 --> 00:57:20,780
about the fact that you're looking at embeddings

1277
00:57:20,780 --> 00:57:23,860
rather than the outputs or what's kind of going on there?

1278
00:57:24,900 --> 00:57:26,180
That's a good question.

1279
00:57:26,180 --> 00:57:28,340
So I think you probably could do that.

1280
00:57:28,340 --> 00:57:32,100
I don't know how the results would compare

1281
00:57:32,100 --> 00:57:34,020
if you just asked versus not.

1282
00:57:36,260 --> 00:57:37,740
Yeah, I'm not sure.

1283
00:57:37,740 --> 00:57:41,420
I mean, I think it's like if it doesn't succeed

1284
00:57:41,420 --> 00:57:43,300
on just asking, then it's interesting to know

1285
00:57:43,300 --> 00:57:45,900
whether it's kind of latent representation

1286
00:57:45,900 --> 00:57:47,580
still has that information or not.

1287
00:57:47,580 --> 00:57:52,580
So, but I don't know the whole space of kind of how you,

1288
00:57:52,900 --> 00:57:54,580
how you can interrogate these models

1289
00:57:54,580 --> 00:57:55,900
for those questions, so.

1290
00:57:57,420 --> 00:58:00,900
All right, let's thank Steve again.

1291
00:58:03,780 --> 00:58:06,980
We'll have plenty of time to talk to him more

1292
00:58:07,940 --> 00:58:11,140
at the refreshments after this talk.

1293
00:58:11,140 --> 00:58:14,220
And who knows, maybe there will be Zafar Turkish in there.

1294
00:58:14,220 --> 00:58:15,780
Which is, by the way, amazing.

1295
00:58:15,780 --> 00:58:18,660
If you're in Vienna, you should absolutely try it.

1296
00:58:18,660 --> 00:58:23,660
So unfortunately, the next speaker could not be here.

1297
00:58:23,660 --> 00:58:28,660
Josh Tenenbaum is a latent variable in this session

1298
00:58:29,980 --> 00:58:34,660
because both Stephen was a student of Josh's

1299
00:58:34,660 --> 00:58:35,620
and...

