start	end	text
0	3360	pro six circuits on tiny binary assets.
3360	6880	So what we'll do today is actually train PCs
6880	9120	on language, language models,
9120	11400	and Juan-Wes going to tell us how that works
11400	12800	and what that's useful for.
15160	18800	So today I'll present our paper,
18800	22200	Attractable Control for Autoregressive Language Generation.
23480	28480	So unlike probably the interesting ones,
28640	32040	today I'll focus more on the application side
32040	34080	of probabilistic circuits.
34080	36680	Like more machine learning oriented.
38480	43480	Okay, so let's get started with the basic concept
43840	45560	of large language models.
46960	49520	They have been like really popular recently.
49520	52760	I'm not sure if people here really care or not.
53760	58760	Like I see like this interested face you do.
60280	62000	Okay, that's a good sign.
62000	63680	That's a good sign.
63680	67800	So basically the idea is very simple.
67800	71320	In the sense that you collect large amount of text
71320	73760	consisting of trillions of words,
73760	77400	and you train neural networks with billions of parameters
77400	81400	on these data, then you have these so-called large
81400	82520	language models.
84880	89880	These chat GPT or GPT-4 thing has become like really popular.
92600	94440	People talking to them on the internet,
94440	98200	asking them to polish their papers,
98200	100720	using them to do their homeworks,
100720	105720	and even asking them to play D&D together.
107520	110320	So yeah, that is actually true.
111320	116320	So the idea is, so people have now having this feeling
117280	119840	that we probably have solved AI,
119840	123560	we have solved artificial intelligence,
125680	127640	and chat GPT has passed Turing test,
127640	130840	but is that really the case, okay?
130840	135840	So I tried this a few months ago.
136840	141840	It was an actual conversation between me and chat GPT.
141920	145600	So I asked the model to generate a sentence
145600	149560	using Frisbee, Cod, and Dog following the given order.
149560	152280	Well, I mean, this should be quite simple
152280	156400	for a super intelligent AI model, right?
156400	159760	So it does give me a sentence.
159760	162840	The sentence look quite decent,
163840	166080	and all the key words are in there,
166080	170960	but Dog and Cod are in the wrong order, right?
170960	173240	Well, I mean, humans make mistakes,
173240	175120	so super intelligent AI does.
176120	180120	So I was being patient, I tried again, okay?
180120	181920	Again, it gives me a sentence,
181920	184440	but it's even worse now.
184440	186480	All of the words are in the wrong order.
186480	191480	So you see that chat GPT fails to follow
191760	195040	even this simple logical constraint, okay?
195040	197680	So people have been trying to fix this.
197680	200240	Some of the methods, including like,
200240	202320	okay, we can prompt them in a different way,
202320	205100	like chain of thought.
206160	208200	I'm not comfortable saying this phrase,
208200	210440	but that's one of the methods.
211680	214400	Or people try to use search algorithms
214400	216400	trying to find the correct sentence
216400	219880	in this huge search space, so on and so forth.
219880	223680	But none of them actually guarantees that chat GPT
223680	227720	or whatever other language model give us what we want.
227720	232440	So what we actually want is like 100% guarantee.
232440	235400	When we have like a huge powerful model,
235400	237360	when we instructed to do something,
237360	241480	we wanted to follow our instructions exactly.
241480	245160	So in today's talk, I'll show you how we can do this
245160	247240	with probabilistic circuits.
248240	251320	Before some of the detail,
251320	254840	so let's get started with some basics of language modeling.
254840	257720	So language modeling is really not like a fancy idea.
257720	261280	It's just a joined distribution over some text,
262400	265480	probably less than or equal to n words.
267400	272400	Each random variable here is a word taking value
273320	278320	in a fixed, in a vocabulary of finite size.
280120	284520	The size usually in practice ranges from 10,000 to 50,000
284520	286200	or something, okay?
287280	290000	And here's some examples.
290000	293920	You might notice there's some special word called EOS here,
293920	295120	which means end of sentence,
295120	298000	which is only like a special token or word
298000	300840	we use to pad sentences of different length
300840	301920	to the maximum length.
301920	306120	So we have this like joined distribution well defined, okay?
306120	310000	So we collect a lot of text of this form,
310000	313200	sentence fragments, paragraph fragments,
313200	316000	and we train this joined distribution
316000	319800	by maximizing a lot of likelihood, a very, very simple idea.
320640	322200	Okay, so what about the architecture
322200	324600	of this distribution of our model?
324600	329600	Well, like the most popular ones, like GPT,
330320	332440	they are autoregressive models.
332440	336800	Well, so by the chain rule of probability,
336800	340680	we can decompose our joined distribution this way.
342360	345760	Represented as a graphical model is basically
345760	347760	for each random variable,
347760	351520	we have arrows going from all the previous random variables.
352600	355000	From a generative point of view,
356000	358640	it's basically we start with,
359800	361040	I can't see my cursor.
361040	365360	So we start generating the first word
365360	367480	from this prior distribution.
367480	368920	Once we have the first word,
368920	370920	we move on to the second one,
370920	372320	conditioning on the first one,
372320	373960	and then we generate the third one,
373960	377160	conditioning on the first two words, very simple, okay?
379560	382840	By some classic results,
382920	386680	marginose for Bayesian networks
386680	389400	with these structures is intractable.
391360	396360	Okay, so before when we asked GPT
399200	400600	to generate a sentence for us,
400600	404440	we are like talking to it as if it's human, right?
404440	408080	So the assumption is if our distribution over language
408080	411120	is a perfect distribution,
411120	414960	then we should be able to interact with it as a human.
416400	418640	When we're doing these kind of like prompting,
418640	419840	when we're talking to it,
419840	423560	most of the time we are actually trying to do conditioning.
423560	426560	So let's consider an even simpler example.
427680	430560	Suppose in our autoregressive generation,
430560	434400	we have generated the first three words, the weather is,
434400	437600	and our constraint is that we want the sentence
437600	439280	to contain the keyword winter.
439280	441200	Well, I mean, we wanna write something
443000	445840	with the topic of winter, so that's our constraint.
446920	450160	What we really want is, okay,
450160	454640	so what language model gives us is the next token,
454640	456440	next word probability.
456440	459840	So basically given the weather is,
459840	464040	it might prefer whether it's warm or cold,
465120	467440	while its prior distribution might want the weather
467440	468920	to be warm.
468920	471440	But what we actually want,
472480	473920	but what we'll actually need
473920	477120	is this conditional next word probability, right?
477120	480600	We want our sentence to satisfy the constraint,
480600	481960	once it contain the winter,
483440	485440	and well, intuitively,
485440	488440	if we want the winter to be in the sentence,
488440	491440	weather is more likely to be cold, okay?
491440	496440	But this is intractable for those autoregressive models.
501240	503120	So why is it intractable?
503120	505240	Let's go more into the details.
505240	509440	So by Bayes' rule, this conditional probability
509440	512160	can be decomposed into these two terms.
513640	515440	The next term here is very simple,
515440	518400	it's just a language model next word probability.
518400	523400	But the first term here is actually a marginal probability.
525000	527800	So it's basically the marginal probability
527800	531360	over all possible suffixes that we could generate
531360	533800	that contains winter, okay?
535480	538200	So this is intractable for autoregressive models,
538200	540080	but we have PCs, right?
540080	543200	PCs are tractable, at least,
544960	547880	we know how to compute marginal probabilities with them.
547880	552880	So why don't we just approximate this term here
553280	554840	with probabilistic circuits?
558120	562720	And we refer to our pipeline as gelato,
563760	566120	in generating language with tractable constraints.
568840	571400	Okay, so how do we do this?
571400	576400	So the first step is that we pick our favorite PC
576400	580360	for sequential modeling, which is a hidden markup model,
580360	581880	the simplest we can have.
583160	586480	We have, on the other hand,
586480	588120	we have our pre-trained language model
588120	590360	we want to control or guide.
590360	593360	We sample a lot of data from our language model
593360	597680	unconditionally, and then we train our hidden markup model
597680	601780	on these data with maximum likelihood estimation.
601780	604320	So effectively minimizing the KL divergence
604320	608000	between the two joint distributions, okay?
608000	613000	And our assumption is that if the joint distributions
613280	617920	are close enough, then all of the marginal distributions
617920	622560	and conditional distributions are gonna be similar, okay?
622560	623400	Any questions?
625040	627920	So the intuition is that we have a black box
627920	630280	autoregressive model, and we train
630280	632000	some sort of white box PC,
632040	637040	so we can use as a representative of the black box, okay?
641400	645800	So we don't want to, since this is the workshop
645800	650800	for circuits and logic, let's represent HMMs as PCs.
651280	654760	So here is the graphical model version of PC.
655600	660360	The Zs here are the hidden variables, the latent variables.
661360	665480	And the X here are the observed variables, or the words.
667040	671360	And let me use the whiteboard to show you how to do this.
676560	681560	Okay, so one assumption we have is that our hidden states,
681720	685440	our latent variables, are discrete variables
685440	690200	taking values from one to H, and H is our hidden state.
690200	691040	Any questions?
692560	694840	Okay, so we start from the very beginning.
694840	699200	In an HMM, we start with the initial state, Z1,
699200	702120	and it has H choices.
712000	717000	So we want these eight product nodes to be representing
717920	721440	the probability, can people see it actually?
721440	726440	So representing the probability of X1 to N
726960	729640	conditioning on Z1 equals to I.
731960	735080	So I would be the hidden state, so I will,
735080	739400	this will be one, two, and H, okay?
739400	744400	And the edge will have weights
744400	747320	of the prior distribution on Z1.
753320	756120	Okay, I see nodding, I see confusing faces,
756120	757480	but I'll move on whatever.
759280	761960	Okay, so it'll be clear later.
761960	766960	So given, so by the Markov property, given Z1, X1,
767840	771600	and so basically given Z1, this part
771600	773720	and the remaining part will be independent.
773720	776160	So let's deal with X1 first.
783880	786080	We have some input distributions,
789720	794080	and this input distribution will be representing
794080	797200	the probability, the emission probability,
797200	800760	basically X1 given Z1 equals to I.
801440	805480	Okay, so basically here, we're in the states
805480	808280	of Z1 equals to I, and this will represent
808280	811280	the probability of X1 given at Z1 equals to I.
814920	816560	I see more nodding faces now.
820480	823120	Okay, and we want this part to describe
823120	827680	the remaining distribution, so we have some nodes here,
827840	832840	and they will represent the distribution of P of X2
841120	846120	to N given Z1 equals to I, okay?
848480	850680	Corresponding to the inputs.
850680	854640	And so we proceed to the next layer.
857680	862680	And this part will be the transition probabilities,
869080	872920	will be the transition probability of Z2 equals to J
872920	877920	given Z1 equals to I, and these will recursively,
879720	882320	we do this construction recursively,
882320	884200	will be representing the probability
884240	888240	from X2 to N, conditioning on Z2 equals to I, okay?
890280	891120	Does that make sense?
891120	896120	So we just proceed, so it's like Antonio and Robert
897400	899720	was trying to make the point,
899720	903160	this is a tensorized layer representation of a PC.
906600	911600	Okay, let's move on, okay, so now we have our,
914320	915140	oh, sorry.
918000	921720	So let's move on, so we have our circuit
921720	926440	representing the distribution over text,
926440	928960	then we need to answer the query,
928960	931040	we need to encode the logical constraint
931040	932260	as the logical circuit.
935600	940600	Let's go back to the constraint we have in the very beginning,
941120	943180	we want Frisbee, dog and dog.
944480	947720	Frisbee, caught and dog to appear in the given order.
950280	953760	This is like a naive way to represent this constraint,
954640	959640	basically the IJK here are enumerating all possible
962120	966040	positions of these three words,
966040	971040	and we take a conjunction whenever we know the positions.
971920	973020	Any questions?
975200	978800	Okay, okay, but this is not really ideal,
978800	982800	we can directly convert it into a PC
982800	984280	that represents the constraint,
984280	989280	but what are the problems, do people see that?
997720	1001040	I'll give a hint, complexity,
1001040	1002800	what's the complexity of this,
1002840	1004480	what's the size of this DNF?
1007880	1010680	Yes, cubic, cubic, and to be more precise,
1010680	1013400	it's n choose number of keywords, right?
1013400	1015480	Well, I mean, we can do it like cubic,
1015480	1017840	but suppose we have five or 10 keywords,
1017840	1021160	we can no longer take that complexity, okay?
1021160	1023880	And the other problem is more subtle,
1023880	1028880	which is that this DNF is not deterministic, okay?
1029760	1033600	So we know that we can multiply circuits
1033600	1035760	when they are compatible,
1035760	1038280	or when they are structured decomposable
1038280	1040320	with respect to the same vitri,
1040320	1042920	but here we want exact conditioning,
1042920	1047920	so we actually need to make sure that our circuit
1049280	1051360	represents a uniform distribution
1051360	1055120	over the support specified by this DNF.
1055560	1060560	DNF, and in general, it is sharply hard
1062400	1065240	to do model counting to normalize everything.
1065240	1068320	Okay, I'll go into the details on the board.
1069280	1073720	Yes, yes, I'll talk about it on the board.
1079600	1081440	Okay, I'll move to the other side.
1085120	1090120	Okay, so for the first question,
1090560	1093240	why does it need to be deterministic?
1093240	1096340	It is because so, okay, suppose we have a very,
1098200	1101280	okay, so suppose we have a very simple distribution,
1101280	1104320	X1, X2.
1115560	1120560	So, we have some weights,
1128360	1132160	so this is a distribution over two random variables,
1132160	1135360	okay, and this is its probability mass function.
1140520	1142000	So when we are conditioning,
1142000	1144840	we are actually selecting all the terms
1144880	1146880	that satisfy our constraint
1146880	1150400	and zeroing out everything else, right?
1150400	1154200	So if we want to do that with circuit multiplication,
1155040	1157720	suppose our constraint, our support,
1157720	1162720	would be something like X1, X2, and X1 bar, X2, okay?
1165840	1168080	So the constraint circuit
1168520	1173520	should be a uniform distribution over its support,
1177440	1182440	otherwise it messes up with the original weights, right?
1183360	1184440	Does that make sense?
1185600	1188880	Okay, so however, okay, let me,
1188880	1193880	so this will be 0.5, X1, X2 plus 0.5, X1 bar.
1199040	1201280	Well, I mean, suppose,
1203480	1204760	and we have a bunch of zeros,
1204760	1208240	and we multiply these two circuits point-wise,
1209240	1213280	and we kind, we keep these two terms
1213280	1215440	and erase these two terms, right?
1215440	1219000	But we want W and W3 to be proportional to each other,
1219000	1221800	like the ratio should stay the same.
1221800	1224720	So this circuit must be a uniform distribution
1224720	1227640	over the support of constraint, okay?
1227680	1230240	So given a logical circuit,
1230240	1232440	how do we convert it into a PC
1232440	1236280	that represents a uniform distribution over the support?
1237480	1240440	To do that, we need to do model counting,
1240440	1243040	but model counting in general is hard
1243040	1244720	if determinism is missing.
1246240	1249760	So this one won't work.
1252200	1253520	Does that make sense?
1255640	1257200	Yes, no?
1257200	1258040	Robert?
1259640	1262040	Isn't that full of a new mistake?
1262040	1264640	Oh yeah, so it's a very subtle, subtle thing.
1264640	1269640	So, well, we only require Frisbee, Caught, and Dog
1270240	1271920	to appear in some positions,
1271920	1276760	but we do not limit that they have to appear exactly once.
1276760	1281760	So you can totally have Frisbee, Frisbee, Caught,
1283120	1286320	Caught, Dog, Dog, something like this.
1287560	1289600	So we have two sub-sequences.
1290400	1292680	We have a lot of sub-sequences, right?
1294320	1299320	So this is position one, two, three, four, five, six.
1301160	1305320	So when IJK are one, three, five,
1306240	1309240	it satisfies the clauses, right?
1309240	1310760	One of them, right?
1317200	1319920	Let me choose a different way.
1319920	1324800	Okay, so basically for this huge disjunction,
1324800	1327440	for each instantiation to the variable,
1327440	1329720	we only satisfy one of the clauses, right?
1329720	1331320	We want, that's determinism.
1333280	1334480	Yes.
1334480	1339480	And suppose IJK are one, three, five.
1340240	1342280	Does this instance satisfy that?
1342280	1343400	Yes, it does, right?
1344400	1348040	But when IJK are one, four, five,
1348040	1349560	it also satisfies that.
1349560	1350400	So it's not.
1351360	1353120	Does that make sense?
1353120	1353960	Okay.
1353960	1355720	Okay, so now let's construct
1355720	1358320	a deterministic circuit representing this constraint.
1360520	1365520	The idea is similar to a deterministic finite automata.
1366640	1371440	Basically, you track how many words have you included,
1371440	1372600	how many words have you used,
1372600	1375480	which state are you in satisfying the constraint?
1376720	1379520	So we want to construct these sub formulas,
1379520	1383520	V, T, say cod and dog.
1387720	1390560	The sub formula representing the constraint
1390560	1395560	that cod and dog appears
1396560	1401560	in XT, XT plus one.
1407400	1411640	So how do we construct this sub circuit or this sub formula?
1411640	1412680	Oh, it's pretty simple.
1412680	1414680	It's a sum node.
1414680	1416640	We consider two different cases.
1417800	1421480	One of the cases that XT is cod.
1422320	1427320	And the other case is that XT is not cod.
1432840	1434640	Does this make sense?
1434640	1435480	Okay.
1435480	1440280	So if XT is cod, then we have like step one.
1442680	1445400	We have made one step towards satisfying the constraint.
1447480	1451120	So we can reduce it to V, T plus one.
1452080	1452920	Dog.
1454400	1456400	And suppose XT is not cod.
1456400	1457720	We're in the same state.
1459000	1461040	We reduce it to T plus one.
1464760	1465600	Dog.
1468560	1469400	Is that clear?
1470280	1473720	So suppose we have constructed FI
1473720	1478720	for all time step greater than T
1479560	1482800	then we can construct all the fees for T.
1485800	1487680	So it's a recursive algorithm.
1491400	1492520	Oh, what's the base case?
1492520	1494120	Yes, that's very interesting.
1498240	1499680	Okay, I'm gonna erase this.
1503120	1506520	So just cod and dog in the part.
1506520	1510320	So yeah, I mean, we can make it even simpler.
1510320	1512320	I'm lazy, so I wanna make things simpler.
1514160	1516440	So suppose in the nth position,
1518160	1520520	we only have one word, right?
1521760	1523760	So fee and,
1530240	1532120	what is this formula?
1532120	1537120	It means that from Xn to Xn,
1538240	1540960	we need to have cod and dog.
1540960	1543520	So this is false, right?
1543520	1544360	Zero.
1547360	1549280	What if we only have dog?
1549280	1554280	So this is true only if Xn equals dog.
1557520	1559640	So yeah, those are the base cases.
1560480	1565480	Okay, so what's the size of the circuit?
1567040	1568760	So at each time step,
1570400	1575400	we need fee T, maybe we have already satisfied the constraint.
1575400	1578880	So something empty, right?
1578880	1583880	We need fee T, we have one word left.
1584880	1589240	Fee T, we have cod and dog, we have two word left.
1590840	1594200	And we have generated none of them yet.
1598520	1601800	Okay, so at each time step,
1601800	1606800	we have four sum notes or four more notes
1607320	1609440	representing all these four cases.
1610400	1614680	And the size, the number of notes in the circuit
1614680	1615920	would be four times.
1617760	1619240	Okay, does that make sense?
1622240	1625440	And you can notice that when we are constructing this circuit,
1625440	1628280	we are always conditioning on XT,
1628280	1630680	current variable, which is very similar to HMM.
1630680	1634240	It generates one variable at a time, one word at a time.
1634240	1636600	So it is compatible with HMM.
1636600	1638280	And also it's deterministic.
1640320	1643160	And we can do the apply operation,
1643160	1645400	the product operation layer-wise.
1645400	1648280	So the size of the eventual circuit
1649760	1653560	would be four, so originally,
1653560	1658560	so originally, here for each layer, we have H nodes.
1659600	1663040	Now we, here for each layer, we have four nodes.
1663040	1666600	So eventually we have four H nodes just for each layer.
1667600	1671600	Acceptable, okay, does that make sense?
1673280	1675360	So we can still generate this.
1676360	1678480	We can generate this even further.
1678480	1682000	Well, some time, well, one simple generalization is,
1682880	1686560	you might think it doesn't have to be caught, right?
1686560	1690520	It can be caught or either catch, right?
1690520	1695520	Or catches, they all kind of represent the same concept.
1697120	1700560	So in the construction, we can
1703840	1706560	modify our original circuit a little bit.
1707520	1711560	Before it was XT equals caught,
1713160	1716120	we can replace it with another OR node,
1718560	1721720	enumerating caught, catch, and catches.
1722320	1726520	Okay, and the circuit size stays roughly the same.
1728080	1729920	And there are many other things we can do.
1729920	1732600	We can also say, oh, we don't need them
1732600	1735160	to be in the same order, in a fixed order.
1735160	1738440	We can, we want them to be like in arbitrary order.
1738440	1739640	We don't care about the order.
1739640	1741400	How do we do that?
1741400	1744640	So in this case, we have four states, right?
1744640	1749640	Basically enumerating all suffixes of these three keywords.
1750640	1755040	In that case, we would have all subsets of these three keywords.
1755040	1760040	So in that way, the complexity would be going from four,
1761040	1765400	we have three keywords, and this is four, two, two to the three.
1765400	1769400	That's there, eight subsets, two to the three subsets, okay?
1769400	1774400	So still acceptable, two to the n is not really,
1776040	1778880	say we have 10 keywords, two to the n, it's just 1024.
1779160	1781720	It's still doable in practice, yes.
1781720	1785720	So it seems that you can do any kind of regular expression
1785720	1790200	constrained, but you're focusing here on language
1790200	1792080	as a fixed length, right?
1792080	1797080	Because you have a fixed number of random variables.
1799000	1803400	So is that the kind of equivalent of what you're saying?
1803400	1805360	Yes, yes, that's a very, very good point.
1805360	1809680	So fixed length here is a very essential assumption.
1809680	1814680	So suppose we have a distribution over language of arbitrary length,
1816560	1821040	then applying the constraint could be hard to define.
1822160	1824720	So basically these three words,
1824720	1829760	they can appear in arbitrarily far positions,
1829760	1832560	and we need probably you take like an infinite sum
1832560	1834560	to define this conditional probability.
1836080	1839880	But this is not terrible in practice,
1839880	1843120	because in practice, even for models like GPT,
1843120	1846400	they have a finite sequence length, yeah,
1846400	1851400	that's a very good point, okay, I'm trying to be fast.
1855080	1857560	Oh, I mean, there are variations of constraints
1857560	1862440	we have talked about, and okay,
1862440	1864800	so now we have our,
1867960	1872240	sorry, now we have our probabilistic circuit
1872240	1874200	representing the distribution,
1874200	1876480	and we have our constraint circuit,
1876480	1878520	and we'll close them to take the product.
1880120	1881720	So we have our constraint circuit now,
1881720	1884080	we can compute the probabilities we need.
1884080	1886400	So the step two is very simple.
1886400	1890680	So this is the original like base decomposition
1890680	1893280	of the conditional probability we have.
1893280	1896080	So this term here is intractable,
1896920	1900560	so we secretly replace the subscript of LM with HMM,
1900560	1905560	the one we notice, and we define this conditional distribution,
1907360	1911960	the gelato distribution, and we can compute this
1911960	1915480	with the linear pass of the circuit.
1915840	1921480	Okay, so what are the advantages?
1922480	1924800	So number one, by definition,
1924800	1927240	the constraint alpha is guaranteed to be satisfied,
1927240	1931160	so finally, other than compared to all the other methods,
1931160	1935280	we have 100% reliable thing we can trust,
1935280	1938280	and number two is that the training of this HMM
1940280	1942120	does not depend on the constraint.
1942120	1945200	So basically once we have this HMM trained,
1945200	1949240	we can use it to enforce whatever constraint we want.
1949240	1952160	So maybe today I want to write something
1952160	1954920	using some keywords and key concepts,
1954920	1957080	and tomorrow I feel like this language model
1957080	1961720	is like using a lot of inappropriate languages,
1961720	1965440	and we can detoxify it by specifying a list of bad words
1965440	1970120	that it cannot use, and all the same model,
1970120	1973600	and all the constraints are only enforced at inference time.
1974600	1975600	Yes?
1975600	1977520	What kind of constraints can be represented
1977520	1981360	as the composable pieces?
1981360	1983840	Yes, that's a very, very good question.
1983840	1986000	That's actually one of the main reason
1986000	1988200	in presenting this work here,
1988200	1991640	because my feeling is that though people have been studying
1991640	1996000	like probabilistic queries like marginal map, marginal,
1998240	2001720	and all these kinds of stuff extensively,
2001720	2005560	but in practice, we really care
2005560	2009280	about some more complicated, less generic ones,
2009280	2012360	and whether is there a language to define
2012360	2014160	or to describe their tractability
2014160	2017520	is kind of missing from the literature.
2017520	2020720	But I could relate this to say
2020720	2025720	there's some work on compiling DFAs to circuits.
2026720	2030400	I think that could be something as a starting point
2030400	2031360	to look at.
2031400	2032920	I'm not sure if that answers your question.
2032920	2033920	I don't have an answer.
2033920	2036240	My answer is, okay.
2037840	2040120	Okay, so experiments and benchmarks.
2041160	2046040	So we evaluate our method on this common sense generation,
2046040	2047360	common gem benchmark.
2047360	2050080	Well, it's very similar to the example I gave you.
2050080	2052280	It gave you a bunch of keywords,
2052280	2057280	and each example comes with a bunch of gold sentences,
2057600	2060360	and you want to generate something
2060360	2065040	that looks similar to the gold sentences using keywords.
2066000	2069600	And here, it's like the most general case.
2069600	2071440	So basically these keywords,
2071440	2072920	they can appear in any order,
2072920	2077160	and they can appear in any form of their inflections.
2078720	2082480	Okay, so this is the, yes, then.
2083520	2086520	And it's a gigantic table.
2086520	2088920	The numbers themselves are not that important.
2091080	2093080	So one thing to note that is compared
2093080	2094880	to all the other baselines,
2096040	2100760	our method, gelato, achieves a state-of-the-art performance
2100760	2104360	with respect to basically all metrics.
2104360	2108200	So these metrics here, Rouge L, Blue Four Ciders,
2108200	2112320	Spice State, there are just some standard NLP metrics
2112320	2117320	that people use to evaluate the quality of your text.
2117600	2119920	So basically you have a gold sentence,
2119920	2122360	and you have your generated sentence.
2122360	2125720	They compute somehow the NBREM overlap
2125720	2127800	to measure the quality.
2131640	2135440	But the other thing is that all of the previous method
2137440	2142160	cannot achieve 100% constraint satisfiability,
2142160	2145200	but ours does in practice as well.
2145200	2146720	Well, there is this one baseline,
2146720	2149600	they achieve 100% accuracy as well,
2149600	2154600	but they kind of did it by starting from the keywords.
2154840	2157400	So they're always gonna be there.
2158360	2161240	And you can see their generation quality
2161240	2162600	is really poor, so.
2167840	2171440	We also conducted some sort of human evaluation.
2174040	2175280	Okay, I guess, yes?
2175280	2177280	Which language model does that look like?
2177280	2182280	Oh, the language model, we use GPT-2, GPT-2 large.
2183160	2185000	Yeah, and all of the baseline state,
2185000	2186400	they use GPT-2 large, yeah.
2188960	2191440	And in case people don't really trust
2191440	2193520	these automatic evaluation metrics,
2193520	2195720	we also conducted human evaluation.
2198600	2201200	And you can see that our model performs
2202200	2204400	much better than previous state-of-the-art.
2205400	2207520	Okay, well, are they very significant?
2207520	2209520	I mean, they're pretty close.
2209520	2212640	Yeah, so, yeah, so basically the,
2214320	2216080	I'm not sure if you can see it clearly,
2216080	2219240	but the bold-faced ones are statistically
2220560	2224520	significantly, what equivalent?
2224520	2229520	So these ones, these two are statistically equivalent.
2229840	2232580	This one is statistically significant.
2233580	2235420	And we're looking at, when defibrating,
2235420	2237620	what's the number here?
2237620	2240660	Oh, yeah, so basically we provide the annotator
2240660	2243620	some sentence and we provide description
2243620	2246940	of one of each of the aspects.
2246940	2249420	So basically, concept means that does it use
2249420	2251540	all the concepts naturally?
2251540	2254300	And plausibility means that is the sentence
2254300	2258180	like a plausible sentence describing a realistic scene.
2258180	2261920	And quality is basically fluency, grammar, and stuff.
2261960	2264040	Overall is the like another,
2264040	2265800	how do you feel about the sentence?
2265800	2268400	And the numbers are from one, two, three.
2268400	2270520	One, two, three, go, yeah.
2270520	2271360	Okay.
2273720	2277800	Okay, so let's get back to the very first
2277800	2281240	motivating example and you can see that gelato,
2281240	2283860	we use our model to add, and it is actually able
2283860	2287020	to get everything correct and generate a fluent sentence.
2287860	2292860	And another, I don't, okay, so we also found this one
2296540	2298540	in one of the generated candidates.
2298540	2301580	A pair of Frisbee players are caught in a dog fight,
2301580	2306580	which is not like the thing that most people
2306820	2307740	would like to think of.
2307740	2312740	So it also shows some sort of creativity here.
2312860	2316780	Here, okay, that's my talk.
2318060	2320460	Please ask questions if you have,
2320460	2322620	and otherwise we can go to lunch.
2322620	2323460	Thank you.
2323460	2324300	Thank you.
2327300	2328300	One more question.
2330860	2332700	Thanks for your talk, if I understand right,
2332700	2335540	you said that to generate every word,
2335540	2338740	you have to do a linear pass over the circuit.
2338740	2342100	So how much is this overhead compared to the kind
2342260	2343780	of latency from the language model?
2343780	2346180	Okay, I like that question.
2346180	2350780	So we don't really need to take a pass over the whole circuit.
2350780	2354280	With some caching, we can do like constant time.
2355500	2357620	So basically to generate each word,
2357620	2359340	the cost is like constant time.
2359340	2361140	It's like a pass through one layer.
2362300	2363300	Is it in any practice?
2363300	2364140	How fast is it?
2364140	2365500	Oh, how fast is it?
2365500	2366900	We actually, so.
2373100	2375860	I don't have a table here,
2375860	2377460	but we have a table in the paper.
2377460	2382460	So if say generating a sentence with five keywords,
2383460	2388460	a GPT-2 large would take around 20 seconds,
2390020	2394500	and our method is like 100-ish seconds.
2394500	2397140	So it's not terrible.
2397140	2399660	And one of the baselines here,
2399660	2403020	well, which was actually like the best paper award
2403020	2405140	at one of the top NLP conferences,
2405140	2407060	they use search-based method.
2407060	2409260	And for them to generate a sentence,
2409260	2413100	they take like 700 seconds, 800 seconds.
2413100	2415740	So because they're search-based,
2415740	2418340	so when a search-based gets large,
2421740	2423540	their method shows like a bottleneck.
2423620	2425460	That's pretty well.
2425460	2426300	Thanks.
2428100	2431300	Can you also give an idea about the time required
2431300	2433620	to derive the hidden Markov model?
2433620	2436300	Oh, yeah, so I do have the time for that.
2436300	2440820	So our hidden Markov model has like 4,000 states,
2440820	2443340	and the emission is 50,000.
2443340	2448020	We trained them with the juice framework
2448020	2449500	that we developed in our lab.
2450340	2455340	The training takes like 20 hours.
2455940	2459860	So it's, we sample about 200,
2462940	2466660	we sampled eight million sentences from GPT,
2466660	2471060	and we trained them for 40 epochs, 20 hours.
2476140	2478020	What's the number of parameters?
2478020	2479060	Number of parameters?
2480500	2481820	Of HMM.
2481820	2486820	Yeah, so HMM is 4,000 hidden states and 50,000 emissions.
2488180	2491820	So the main, mainly that the parameters are centered
2491820	2493420	on the emission table.
2493420	2497500	So it's like 40,000, 4,000 times 50,000.
2497500	2499700	I didn't do that in my head.
2499700	2501380	So those are unique parameters, right?
2501380	2502620	Yes, yeah.
2502620	2506020	Yes, so in the PC, yes, you kind of like,
2506020	2508660	you've rolled out all the parameters.
2509620	2510620	All the positions.
2513180	2516140	Controlable generation is huge, right?
2516140	2517580	So this is great.
2517580	2519780	What, I mean, and I probably understand you,
2519780	2521540	you gave a logical perspective for this audience,
2521540	2524100	but I mean, a reg acts as a much more natural
2524100	2526540	sort of control structure.
2526540	2529460	So presumably you can handle any of our complication
2529460	2531340	given the DFA interpretation, right?
2531340	2532420	Yes, yes.
2532420	2535060	I think that's a very important follow up.
2535060	2537780	We are kind of looking at considering like,
2537820	2540580	compiling DFA's to circuits
2540580	2544580	and kind of automate all these process.
2544580	2546700	Yeah, so ACLs in January, you submit there.
2546700	2548260	I mean, I think they'll love it.
2549180	2551980	Your initial example, you wanted a winter tree,
2551980	2555780	you wanted the presence of winter to select for warm, right?
2555780	2557980	Which is more of a natural language entailment.
2557980	2560060	And I don't think you handle that, right?
2560060	2562540	You handle variations and factual variations
2562540	2563380	because you actually code them.
2563380	2568300	I think we can totally do that winter example.
2568300	2572580	So basically, let me go back.
2573580	2576220	It seems like you encoded the specific variations
2576220	2577580	that you were allowing, right?
2577580	2581780	Oh, you mean like, so we can have winter, winters,
2581780	2584620	but like, maybe if the winter,
2584620	2586460	the word is not explicitly mentioned,
2586460	2587780	you cannot do that.
2587780	2589660	That's what I understand from your formalism, right?
2589660	2591900	If it's in your or, you'll generate it.
2591900	2595580	If it's not, you won't, you won't capture the entailment.
2595580	2599300	No, I mean, so basically, so that's the other thing.
2599300	2603220	So sometimes people, or most of the time,
2603220	2606380	people want to have like kind of soft control.
2606380	2611460	They can't really write their constraint in logical form.
2611460	2616380	For example, toxic, like how can you tell a sentence is toxic?
2616380	2618780	But there are many ways to approximate it.
2618780	2622860	One of the ways is we have like a long list of phrases or words
2622860	2626460	that you're now allowed to use in a toxic sentence
2626460	2629740	and we can basically just write down a logical formula
2629740	2633620	approximating that toxicity constraint.
2633620	2635860	Yeah, I mean, there's also plug-and-play generation tricks
2635860	2638460	that are actually quite interesting, I think that...
2638460	2644580	So basically, there is a, of course, there's a naive way to do it.
2644580	2647860	You can say whenever I encounter one of the words in the list,
2647900	2651660	I just remote, like, prevent it from generating,
2651660	2654300	prevent it from being sampled.
2654300	2658580	But that's not exactly what we're trying to do probabilistically, right?
2658580	2659300	So...
2659300	2662300	Yeah, but I just, maybe look into plug-and-play,
2662300	2664420	control generation is actually a paper.
2664420	2665460	Yes, yeah.
2665460	2668420	So where I think they're doing much more than that, right?
2668420	2671100	Well, so, yeah, so it's like...
2671100	2674580	They're also modifying the posterior selection.
2674580	2679300	So if I'm not wrong, if I remember this correctly,
2679300	2682940	they're basically trying to train some sort of...
2687020	2688860	They use a classifier, right?
2688860	2691220	Yeah, so basically, kind of, they're trying to train
2691220	2693500	a newer model to approximate this.
2693500	2694740	Yeah.
2694740	2696260	So, but they're...
2696260	2700100	So, yeah, so their methods has like two disadvantages
2700100	2702220	compared to our advantages.
2702260	2706020	So one of them is that they cannot guarantee that this is...
2706020	2706660	Right.
2706660	2710460	100% logically satisfiable.
2710460	2713740	And the other is that they have to retrain their model
2713740	2715660	for all different constraints.
2715660	2717300	Their model for all...
2717300	2718540	Yes, okay, but...
2718540	2721300	Okay, so suppose in their training data,
2721300	2723860	they're only trained to kind of satisfy...
2723860	2728380	They only have seen using, say, less than 10 keywords
2728380	2730660	to generate a sentence, right?
2730820	2732980	What if today I want to use 20 keywords?
2732980	2735300	They would not be able to generalize well to that one.
2735300	2737580	Right, so they're examples of toxicity, right?
2737580	2740140	So they pretrain for toxicity and they can use that anywhere.
2740140	2743180	So there's some tasks that you're just going to use repeatedly.
2743180	2745340	I guess I see a combination of what you're doing
2745340	2746620	and what they're doing together,
2746620	2749100	which gives you this sort of entailment during the...
2749100	2751580	Like this broader sense of satisfaction, right?
2751580	2754340	Which I think was an interesting motivation.
2754340	2757260	You didn't quite deliver on, but you could deliver on.
2757260	2759500	That's actually the current project we're working on.
2759500	2761460	So we're trying to combine the models
2761460	2764780	that can handle soft constraints with our model, too.
2764780	2766260	Okay, beautiful.
2766260	2768940	And finally, the reason large types of models work so well
2768940	2770940	is just because of attention, right?
2770940	2773620	And so when you're using a hidden marker model, right,
2773620	2775860	you're losing the power of attention.
2775860	2779460	But I guess I'm looking at this and trying to convince myself
2779460	2781940	that, well, you get all the attention in the right-hand part
2781940	2784500	and then you just need a little bit of bias of selection
2784500	2788020	in the left-hand part here, and that's what's happening.
2788020	2790700	Yeah, so basically, intuitively, you can think of this part
2790700	2793540	as only providing, like, guide and suggestions,
2793540	2798540	leading the model, leading GPT to satisfy the constraint.
2798620	2800620	But on this part, it's kind of responsible
2800620	2802980	for fluency, grammar, and everything.
2806420	2807540	Beautiful work.
2807540	2808380	Thank you.
2810540	2811380	All right, thanks.
2811380	2814660	Maybe we should wrap up, and thanks again.
2815660	2821300	And we can have lunch with the database folks
2821300	2823300	and talk about generating SQL query.
