{"text": " After that, we have our discussion session for about half an hour from 4 to 4.30. So, the next talk will be given by my colleague, Major Naik, who will talk about scallops and other seafood. Hi, everyone. Thanks, Val. So, I'll be talking about this programming language and compiler that we have been building over the last three years on this exciting topic called Neurosymbolic Programming. As a disclaimer, I'm a programming languages researcher, mostly consuming all the cool stuff that folks in databases and AI have been doing so. So, you won't see much in terms of novelty, but you'll see a lot of interesting synergies and empirical results that come from importing good theory from other fields. Let me first motivate the need for Neurosymbolic programming. So, there are these two prevalent paradigms of modern programming, as all of you know. So, these are commonly called System 1 and System 2 by noted psychologist Daniel Kahneman in his book Thinking Fast and Slow. So, if you see System 1 is deep learning where you have a lot of benefits. For example, you have subsymbolic knowledge. What that means is you have meaning associated with, say, a name Tom that likely is a male and so on, right? There's open domain knowledge. You don't have to write and code everything explicitly. They're good at rapid reasoning, handling noise and naturalness, and what we nowadays see with foundation models is in context learning or more accurately prompt engineering. On the other hand, we have classical algorithms, which is System 2, where you can explicitly encode knowledge and get more data efficient solutions. You can also do complex reasoning, things like multi-hop reasoning using recursion, but also negation, aggregation, and so on, which traditionally deep learning isn't great at. You have other benefits like interpretability, modular reasoning, and better generalizability. Traditionally, these two systems don't talk to each other very well. But we do want features of both for many AI applications, right? And so that has given rise to this field called neurosymbolic programming. Now, this term means slightly different things to different people, and so I'm going to define more specifically what it means to us. There is a much richer taxonomy of different styles of neurosymbolic programming that we don't necessarily encompass. But I will show you that at least this form that we consider is interesting enough to write a lot of useful applications, right? So what I've shown here is a picture of the three approaches before proceeding to why it is hard to combine deep learning and classical algorithms in a single system. So in deep learning, much of the success comes from gradient descent algorithms for backpropagating the loss to learn the parameter theta of this neural model. X and Y are in these double boxes indicating that you have supervision on them, right? On the other hand, classical algorithms such as this program P can typically work on structured data, which I'm going to indicate with R, okay? And in neurosymbolic, this is a simple neurosymbolic program where there's initially a neural component and theta, which takes input X, produces an intermediate representation R, which is structured, which in turn is fed to a classical algorithm P to produce an output Y, right? And we have gradient descent here, dr by d theta, but we'd also like to somehow backpropagate the loss across this program, this discrete program P, okay? And even though this looks like a supervised learning setting, we have actually used neurosymbolic programming in many different settings that I'll show you today, including RL and even unsupervised learning with foundation models. So what are some of the challenges that we tackled in building Scallop? So the first question is what is the symbolic representation we use for R? And the second is what is the reasoning language for programs P, right? As you can imagine, there's a lot of different choices for these decisions. Most importantly, how can we obtain an automatic and efficient differentiable reasoning engine to learn this dy by dr under what we call algorithmic supervision, right? What that means is you're not given supervision on R, right? And that makes this whole problem more challenging. We only have supervision end to end on end to end observable input output data X and Y, okay? But not on sort of intermediate data R. And this makes sense for a lot of applications. If you think of a healthcare application, right? You have data about a patient, all of their lab measurements and so on. We also have some outcome Y of, say, a treatment. But even an expert clinician might not have time to label every intermediate piece of information or might not even know how to label it, right? Even if they could. So that makes this problem particularly challenging. We also have two other challenges here. We'd like to tailor learning of this computing dy by dr by different applications, characteristics, okay? At this point, we are looking at approximate algorithms which go along well with gradient descent, which is already approximate. And lastly, we'd like to integrate with all the existing training pipelines. This is more of an empirical challenge here. We don't want to reinvent something like PyTorch. And so we'd like to reuse all the existing models and training pipelines. So here are some of the design decisions that we made. Much of this borrows from work by other researchers. I'm not necessarily inspired by DeepProblog, but would later learn that there was work by many other researchers as well. And our main contribution here was really using Datalog and putting all of this together in a practical system, right? So the first design decision here was to use a relational representation for R, right? And I don't need to tell a database community about how the relational model has withstood the test of time. It can represent very general forms of data in arbitrary graphs. There are many other nice properties about relations, as I will show you when we tag tuples with probabilities and more general kinds of information, so this relational representation is really helpful. The second is the choice of the language for P, and here we use a Datalog-based language. We started out literally with Datalog, but it has grown over the years. We have support for algebraic data types, foreign functions, and so on. Actually, it is at this point Turing complete. So depending on what subset of a language you use, you get different guarantees. Perhaps the most interesting piece here for this audience is we accidentally discovered provenance semirings. We were playing with different kinds of tags and eventually realized that they could generalize them in this very elegant work, which was mentioned at the beginning of this workshop on provenance semirings. I will show you the different semirings that we have in my talk. Lastly, we have integration with both PyTorch and JAX. PyTorch for getting models that might be pre-trained and so on that we might want to fine-tune, but also for computing the loss here using something like JAX. It is a pretty usable framework and to end with lots of moving pieces. Let me give a simple motivating example of the kinds of things we can do with Scala. This is a simple strategy game called Pac-Man. It is actually a simplified version which is called Static Pac-Man. The ghosts are not moving. The setup is as follows. There is this grid of 5x5 cells. Each time in each instance of this game, we randomly assign these ghosts, the Pac-Man agent and the goal in different cells. As I said, the ghosts do not move and so the goal is to learn to reach the goal without hitting any of the ghosts. We set this up as a simple RL reinforcement learning problem. We use a simple model here which is BQN, BQ networks to train this agent. The baseline here is an end-to-end neural model, a convolutional neural network, which is not given supervision on which cells contain ghosts or the goal post or the Pac-Man itself. All of this is the intermediate representation to be learned. The only supervision one gets is after an entire game episode where either the Pac-Man reached the goal, so you get a reward of 1 or it didn't and in which case it gets a reward of 0. We formulate this problem in Scalop by decomposing it into a neural model which is doing this low-level perception. The goal of the neural model is to simply learn what each cell might contain. There are four choices. It can either be empty or it can contain a ghost or it can contain the Pac-Man itself or it can contain the goal. These are the four choices. Now this neural model outputs these choices to a logic program in Scalop whose goal is to do the path planning. In summary, instead of having a monolithic neural network which is trying to learn end-to-end how to do both entity extraction and path planning, we decompose this task into entity extraction which is sort of low-level perception that is best done by a neural module and a logic program, a classical algorithm which does the path planning. At each step, the goal is to decide whether the Pac-Man should move up, down, right or left. You'll get the reward only after an entire episode of around 20 steps. Here is our empirical result. In just 50 such episodes with this Scalop program that I showed you, we can get an accuracy of 99.4%. Whereas if you do this with a baseline of end-to-end neural, you get a much lower accuracy and it requires over 50,000 episodes. There's some hand-waving here. This is not entirely a fair comparison because we have written a domain knowledge using logic rules. I probably skipped over the program itself so here goes. This is our syntax for our data log-based language, in this case for path planning. The goal here is to compute the next action, one of these four choices. That in turn depends on whether there's a path from an adjoining position where the Pac-Man currently is to the goal. The definition of a path is itself a recursive predicate. It's a path that does not collide with any of the hosts and that is encoded using what we call safe cells. Any questions so far? The programmer writes this discrete program without any probabilities and so on. What we will see happening under the hood when both at training and inference time is a neural model will compute these predicates such as actor, goal and enemy only with different degrees of certainty. In some sense, we have all of these possible words being tracked simultaneously by the scallop engine. All of that computation will be done through tags which you don't see here at the level of the values that are being propagated. Can I ask you a quick question on this? Is there a notion of a shortest path or is it any path? Great question. Here we say any path but if I understand this correctly, the tags will penalize paths that are longer. Cycles in the path you will... Let me get into the semirings. The short answer is the tags will be tracking a finite amount of information so they won't necessarily compute all paths with cycles and so on. In this example, the neural network is responsible for extracting the state of the program and then you have an actual program to perform the logic. The neural network extracts action information or something else. So the neural network only extracts... So the question was whether the neural network extracts these predicates actions. If it did. So that is the baseline that I was showing you that you don't have a logic program. So the neural network is taking in this grid of pixels, 200 by 200 pixels and producing one of these four outputs or other distribution over these four outputs. So that is the baseline. If you were using that, then you wouldn't need scallop. Here we are trying to show you that you can actually do more data efficient and robust and so on. By the way, this program, which is learned, generalizes very nicely to much larger grids, even 25 by 25. So you see, whereas a network which was trained end to end would probably not generalize well to other grid sizes. So let me briefly talk about what is going on in the scallop compiler. So we have this differentiable reasoning framework. First, a preview of our entire compiler. So the surface syntax looks like this. In this case, it even has limited forms of quantifiers. We have a front end which produces these abstract syntax trees and there are several passes here for type inference and so on. Then we have a back end where, which is based on extended data log where we do a lot of optimizations including query planning and magic set transformations and so on. And finally, we have this relational algebra machine or RAM, which is what is actually executed at training and inference time. And this is what the equivalent scallop RAM program looks like for that high level constraint. So where does prominence come in? So the semantics of SEL RAM, which is essentially just extended relational algebra, which is the semantics of data log. We have implemented a very general framework for tracking provenance. And this is inspired by the work on provenance semirings that was mentioned at the beginning of this workshop. And we even have this very clean interface to define new provenance structures. So again, covered in the original tutorial, but briefly there's this tag space that you have to define yourself. And then various operations for disjunction, conjunction, negation and saturation. I've shown one instance of this abstract structure here, which we call max min probabilities. Here the set of tags is real numbers between 0 and 1 and disjunction is max, conjunction is min and so on and so forth. If you apply this max min probe to a particular rule during the execution of the program I showed you, let's say the rule which computes whether a cell x comma y is safe. So it is safe if it is indeed a grid cell, a cell in the grid, in the 5 by 5 grid, and we do not believe that there's an enemy in the cell. So this is the standard semantics of data log, of discrete data log, untagged semantics. So let us say that in 1 comma 2 and 2 comma 3 are two different grid cells. And let us say the enemy is in the cell 2 comma 3, then we know how to compute this difference. So that's just the tuple 1 comma 2. But in the tagged semantics, something much richer is happening, which is that we have these tags t1, t2 and t3 now. And they are propagated here along with the output values of each rule. And once you use a particular provenance semiring, in this case the max min probe, we can for example say that in this case we believe the enemy is in cell 2 comma 3 with the probability of 0.2 coming from the convolutional neural network. And so now you can imagine every cell has some probability of an enemy being there. And accordingly you can now get estimates of which cells are safe, okay? Yes, go ahead. So the difference with prob log is that you use this fuzzy logic, we are propagating the probability. The difference to prob log? Yes, so the prob log has this weighted model accounting semantics, right? So you use the fuzzy semantics to propagate the probability. So we, so I wouldn't know the answer to that. We can probably take that offline, but we do have, so this was as I said inspired by deep prob log. We do have weighted model counting. I just showed you, so max, I see, so you mean fuzzy as in this max min. Okay, okay. So I just showed you a simple semiring. In practice we don't use any of those. We just use them early on while we are developing applications, but very quickly turns out these fuzzier semirings don't really help learn the model, okay? So the one that we really use, so as I said, there's the discrete execution. There's the probabilistic one, and then finally there's the differentiable one, which is what we use for learning, right? And the one that you are probably talking about is what we call top K proofs. So along with each tuple, we track, you know, what we call, you know, up to the top K proofs, which I think Eric in the first talk referred to as I believe W of X, okay? So we don't count how many times a tuple was used or anything like that. Yes, with respect to the negation and saturation operations, right? Can you expand a little bit on what your requirements for them are, for this to work? Yeah, this is sort of too low level for me to explain. So I wouldn't know. I'll be happy to get you in touch with the students. First of all, it will be stratified negation. But I think you are asking me a deeper question than that. What's the structure, what's the test, what's the negation has to prove that? So if it comes to me, I'll let you know. I do know exactly what you're asking, and I'll try to see if I can remember, okay? There are certain restrictions on all of these, on negation and saturation, okay? But you prove them once and for all when you're defining the semi-ring, okay? And so you can then use it. So I'm not going to go too much further into semi-rings other than to just say that the nice thing here, at least to me, is that the programmer writes as if they are programming against a deterministic neural model that is producing these outputs, right? But under the hood, you have all of these probabilistic and differentiable reasoning happening through these tags, okay? We have applied this to a wide range of benchmarks and are now moving to even more sophisticated ones in robotics and healthcare, for explainable healthcare and so on. But I'll just show you some core, you know, some challenges in the field of AI that we started out with. These include, you know, benchmarks in computer vision, which have images and video. For example, here, this is this Mugen benchmark where the goal is given a short video and a piece of text to give a score between zero and one that tells how likely is this text talking about the frames in this video, right? So this, as you can imagine, has applications in video captioning, video search, video recommendation and so on, right? There is, we have benchmarks in NLP as well. Again, fairly standard ones and then we also have multimodal benchmarks. And much of these benefits of relational, the relational model are useful here. For example, we extract scene graphs from images which can be represented as relations. We extract abstract syntax trees from in semantic parsing, which are again represented as relations, right? This is where the rubble meets the road. All of this theory is elegant. But if it doesn't work in practice, then it's not, then it doesn't help us, right? When we started this project, many of these baselines, both neural and neurosymbolic, were far behind us, right? But by the time we got all of this published, some of them had even crept back up ahead of us, right? So this is sort of the challenge we face against the end-to-end deep learning paradigm, right? Which is, it will, you know, as newer neural architectures and so on are designed, they might even outperform, say, the neurosymbolic approaches, okay? So, any questions before I proceed? Yes. Maybe also, right, so accuracy is one thing, right? But I could also see that since you're encoding some of the main knowledge into your program, right? I could see that, for example, you might need less data to learn the model and things like that, and maybe it's more performant. Yes, so great question. And we have all of these results in our PLDI paper, PLDI 2023, where we talk about better interpretability. So if you remember, the intermediate representation R on which no supervision was given, we can actually look back what did it actually learn the right representation, right? And the answer is yes, so it is more interpretable, it is more robust and better generalizable. So these better neural networks, are they kind of trying to do what you do with your structured constraints? Are they trying to do that through network structure? Are they trying to simulate, basically, what you can do in a more elegant way? So, first of all, these are end-to-end deep neural models, right, like transformers and so on. We wouldn't necessarily know what they are trying to do, but they are solving this problem. Let me show you one, right? So this pathfinder was a benchmark by, I think, Google Research a few years ago called PathX. You see, they're two tiny dots, and you have to tell whether there's a dotted path from one to the other, okay? And even for a human, it can take up to two minutes to tell for some of these difficult images whether there's actually a dotted path or not. So it's a binary classification problem, right? So, you know, so the state of the art now here is actually a transformer which, I'm sorry, which beats what we have. So you see for PathX, there's this transformer model which is doing even better than ours. In our case, we simply, you know, we have the rule for computing transitive closure. So once you know which, where are the two dots and where are the edges, you can use, you know, simple these two rules to tell whether they are reachable or not. But you don't know if their model is trying to do something like that in the deep learning model directly? Right, so we haven't actually seen, you know, like for explanations within them. So it would be nice to see that. There are also some neuro-symbolic baselines here. I mean, work that Guy and others have done. By the way, a lot of his work has gone into this with sentential decedent diagrams and so on in our weighted model accounting. You know, just ignore, you know, not mentioning here. But there are other neuro-symbolic works as well based on, you know, abductive reasoning and so on that we were able to outperform. Yeah. So I have a question about the gradient semi-ring, which was mentioned several times today. So I don't understand how it's useful in the context here because the gradient semi-ring really computes the forward derivatives, which means that if you have a neural network for object detection with a million parameters, you have to push forward a million dimensional vector through your whole computation path. And what you really need for machine learning is the backward derivatives, which are, you know, linear time. So even though mathematically the gradient semi-ring is a beautiful thing, it's actually the wrong tool for machine learning. You want the backward derivative not to forward. Yeah, so I think we'll have to talk about this more offline. Sorry about that. I wasn't paying too close attention to the gradient semi-ring. Let's talk more. Could you go back once? Yeah. So for the first two examples, we have the MNIST bit. Yeah. And then this is, like after you recognize, so another approach is that you recognize bit and then you just write Python to something to get. Right. So why is this any better? Why are we doing anything better? Your supervision is not given on the individual MNIST digits. Okay. It's only given on the final result. Yeah. So but this example is, it feels a little bit confined. Right. So I could have done this by doing the two basic approach. Yeah. Yeah. For example, here we are this kind of more streamlined approach has a clear benefit. So if you had supervision on the intermediate results, you wouldn't need scallop at all. Okay. Right. Right. So in, in none of these benchmarks, do we have intermediate supervision, even though many of them are synthetic and you actually know the intermediate labels. So that is how we actually, you know, measure whether you know, you know, the degree of interpretability, how much has it actually recovered the information. So I'm not showing you, you know, we have heat maps for all of this to show you actually what intermediate representation was learned. And it is, it matches the synthetic data's labels. Yes. Okay. So, you know, that is, you know, I'm just going to show you some fun things here. There's not much more I can say here with, you know, so now what happened was in these two years that we did this work, LLMs and more generally foundation models came on the scene. And we wondered, you know, can we catch up? Right. Can we somehow integrate this into scallop? And the answer surprisingly is yes. Okay. And this is still open. I think Joe also brought this up, you know, if I understood correctly what you're saying. So what is the, the programming, you know, abstraction for say, you know, these generative models, and surprisingly, the relational model still works. If you think of any foundation model, right, it is a binary relation which takes a prompt and gives a response. Right. And these are data types where the strings or tensors and so on are all supported in scallop. Okay. And it's actually a relation, not a function because based on the temperature and so on that you use the same prompt could have different responses. Right. You know, very well into scallop. And we built this library of plugins. We now have 12 foundation models integrated into scallop and you can add new ones very easily using our foreign function and predicate interface. I'm not going to go too much into these, but I can, you can sort of see how we are, we have these decorators for relations. And you can use a few short examples or you can use chain of thought, you can use auto GPT, you can even fine tune, you know, layers of these models in scallop using again just end to end supervision. In this case, you know, we break down this task into sort of this in context learning which extracts tuples, you know, which are the basic relationships between pairs of people mentioned in this passage. And then we write a few rules in this case just three, which can compute the answer to a question which is how is a particular pair of people in this passage related. Right. So this is sort of showing you multi hop reasoning. By the way, we even have rule learning here so the parameters don't just have to be in the neural model but for example this relation composition is itself noisy. And you could learn the weights of individual tuples of this relation. You can extend it to vision models as well. So here's a simple one, which is actually a multi model model clip from open AI, which also provides probabilities. So in this case, the input is an image. It's a bound argument and the output is the label. So in this case, if you give a set of labels such as cat and dog, it will tell you the probability of this image being cat or a dog. We have also integrated meta segment anything models. So this in this case, you are given an image as an input, and it produces a set of tuples with an ID of each segment and the tensor representation of the segment. Right. You can put these all together and build very interesting multi model applications in scallops. So in this case here, what you see is three different models put together to solve the problem from this clever benchmark, which asks in this case, some some question that involves elementary reasoning about a scene. Right. How many green objects are there in the image. I'm not showing you all of the rules that we wrote in scallop. There are about 100 rules that we wrote for this particular task. But we use these three different models to extract basic information. In this case, doing the semantic parsing of this question, extracting segments from this image and finally labeling each segment with a piece of text. Finally, we can get the answer that there are three main objects in this image. Okay. So I'm not going to show you the imperial results. This work is still under review. We have applied it to a wide range of benchmarks, including those involving vector databases. So, you know, you're having retrieval and generation, but also image generation and so on. Right. And you can actually run many of these applications at this URL. And there's a lot more resources at this particular URL. Thank you very much for your attention. Any questions? Thank you. And once", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.0, "text": " After that, we have our discussion session for about half an hour from 4 to 4.30.", "tokens": [50364, 2381, 300, 11, 321, 362, 527, 5017, 5481, 337, 466, 1922, 364, 1773, 490, 1017, 281, 1017, 13, 3446, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29940079104515815, "compression_ratio": 1.3312883435582823, "no_speech_prob": 0.017766112461686134}, {"id": 1, "seek": 0, "start": 8.0, "end": 20.0, "text": " So, the next talk will be given by my colleague, Major Naik, who will talk about scallops and other seafood.", "tokens": [50764, 407, 11, 264, 958, 751, 486, 312, 2212, 538, 452, 13532, 11, 15581, 6056, 1035, 11, 567, 486, 751, 466, 30509, 3370, 293, 661, 23206, 13, 51364], "temperature": 0.0, "avg_logprob": -0.29940079104515815, "compression_ratio": 1.3312883435582823, "no_speech_prob": 0.017766112461686134}, {"id": 2, "seek": 0, "start": 20.0, "end": 25.0, "text": " Hi, everyone. Thanks, Val.", "tokens": [51364, 2421, 11, 1518, 13, 2561, 11, 7188, 13, 51614], "temperature": 0.0, "avg_logprob": -0.29940079104515815, "compression_ratio": 1.3312883435582823, "no_speech_prob": 0.017766112461686134}, {"id": 3, "seek": 2500, "start": 25.0, "end": 38.0, "text": " So, I'll be talking about this programming language and compiler that we have been building over the last three years on this exciting topic called Neurosymbolic Programming.", "tokens": [50364, 407, 11, 286, 603, 312, 1417, 466, 341, 9410, 2856, 293, 31958, 300, 321, 362, 668, 2390, 670, 264, 1036, 1045, 924, 322, 341, 4670, 4829, 1219, 1734, 8977, 88, 5612, 299, 8338, 2810, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11239871425905089, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.10116131603717804}, {"id": 4, "seek": 2500, "start": 38.0, "end": 51.0, "text": " As a disclaimer, I'm a programming languages researcher, mostly consuming all the cool stuff that folks in databases and AI have been doing so.", "tokens": [51014, 1018, 257, 40896, 11, 286, 478, 257, 9410, 8650, 21751, 11, 5240, 19867, 439, 264, 1627, 1507, 300, 4024, 294, 22380, 293, 7318, 362, 668, 884, 370, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11239871425905089, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.10116131603717804}, {"id": 5, "seek": 5100, "start": 51.0, "end": 64.0, "text": " So, you won't see much in terms of novelty, but you'll see a lot of interesting synergies and empirical results that come from importing good theory from other fields.", "tokens": [50364, 407, 11, 291, 1582, 380, 536, 709, 294, 2115, 295, 44805, 11, 457, 291, 603, 536, 257, 688, 295, 1880, 33781, 25480, 293, 31886, 3542, 300, 808, 490, 43866, 665, 5261, 490, 661, 7909, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06071758270263672, "compression_ratio": 1.5265700483091786, "no_speech_prob": 0.018199704587459564}, {"id": 6, "seek": 5100, "start": 64.0, "end": 70.0, "text": " Let me first motivate the need for Neurosymbolic programming.", "tokens": [51014, 961, 385, 700, 28497, 264, 643, 337, 1734, 8977, 88, 5612, 299, 9410, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06071758270263672, "compression_ratio": 1.5265700483091786, "no_speech_prob": 0.018199704587459564}, {"id": 7, "seek": 5100, "start": 70.0, "end": 76.0, "text": " So, there are these two prevalent paradigms of modern programming, as all of you know.", "tokens": [51314, 407, 11, 456, 366, 613, 732, 30652, 13480, 328, 2592, 295, 4363, 9410, 11, 382, 439, 295, 291, 458, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06071758270263672, "compression_ratio": 1.5265700483091786, "no_speech_prob": 0.018199704587459564}, {"id": 8, "seek": 7600, "start": 76.0, "end": 87.0, "text": " So, these are commonly called System 1 and System 2 by noted psychologist Daniel Kahneman in his book Thinking Fast and Slow.", "tokens": [50364, 407, 11, 613, 366, 12719, 1219, 8910, 502, 293, 8910, 568, 538, 12964, 29514, 8033, 591, 12140, 15023, 294, 702, 1446, 24460, 15968, 293, 17703, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13039774894714357, "compression_ratio": 1.3605442176870748, "no_speech_prob": 0.06340079754590988}, {"id": 9, "seek": 7600, "start": 87.0, "end": 96.0, "text": " So, if you see System 1 is deep learning where you have a lot of benefits.", "tokens": [50914, 407, 11, 498, 291, 536, 8910, 502, 307, 2452, 2539, 689, 291, 362, 257, 688, 295, 5311, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13039774894714357, "compression_ratio": 1.3605442176870748, "no_speech_prob": 0.06340079754590988}, {"id": 10, "seek": 9600, "start": 96.0, "end": 101.0, "text": " For example, you have subsymbolic knowledge.", "tokens": [50364, 1171, 1365, 11, 291, 362, 2090, 88, 5612, 299, 3601, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17138822617069369, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.05913146585226059}, {"id": 11, "seek": 9600, "start": 101.0, "end": 110.0, "text": " What that means is you have meaning associated with, say, a name Tom that likely is a male and so on, right?", "tokens": [50614, 708, 300, 1355, 307, 291, 362, 3620, 6615, 365, 11, 584, 11, 257, 1315, 5041, 300, 3700, 307, 257, 7133, 293, 370, 322, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.17138822617069369, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.05913146585226059}, {"id": 12, "seek": 9600, "start": 110.0, "end": 117.0, "text": " There's open domain knowledge. You don't have to write and code everything explicitly.", "tokens": [51064, 821, 311, 1269, 9274, 3601, 13, 509, 500, 380, 362, 281, 2464, 293, 3089, 1203, 20803, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17138822617069369, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.05913146585226059}, {"id": 13, "seek": 11700, "start": 117.0, "end": 130.0, "text": " They're good at rapid reasoning, handling noise and naturalness, and what we nowadays see with foundation models is in context learning or more accurately prompt engineering.", "tokens": [50364, 814, 434, 665, 412, 7558, 21577, 11, 13175, 5658, 293, 3303, 1287, 11, 293, 437, 321, 13434, 536, 365, 7030, 5245, 307, 294, 4319, 2539, 420, 544, 20095, 12391, 7043, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14975031939419833, "compression_ratio": 1.5023255813953489, "no_speech_prob": 0.01825844496488571}, {"id": 14, "seek": 11700, "start": 130.0, "end": 144.0, "text": " On the other hand, we have classical algorithms, which is System 2, where you can explicitly encode knowledge and get more data efficient solutions.", "tokens": [51014, 1282, 264, 661, 1011, 11, 321, 362, 13735, 14642, 11, 597, 307, 8910, 568, 11, 689, 291, 393, 20803, 2058, 1429, 3601, 293, 483, 544, 1412, 7148, 6547, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14975031939419833, "compression_ratio": 1.5023255813953489, "no_speech_prob": 0.01825844496488571}, {"id": 15, "seek": 14400, "start": 144.0, "end": 157.0, "text": " You can also do complex reasoning, things like multi-hop reasoning using recursion, but also negation, aggregation, and so on, which traditionally deep learning isn't great at.", "tokens": [50364, 509, 393, 611, 360, 3997, 21577, 11, 721, 411, 4825, 12, 9050, 21577, 1228, 20560, 313, 11, 457, 611, 2485, 399, 11, 16743, 399, 11, 293, 370, 322, 11, 597, 19067, 2452, 2539, 1943, 380, 869, 412, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1183929810157189, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0015007631154730916}, {"id": 16, "seek": 14400, "start": 157.0, "end": 164.0, "text": " You have other benefits like interpretability, modular reasoning, and better generalizability.", "tokens": [51014, 509, 362, 661, 5311, 411, 7302, 2310, 11, 31111, 21577, 11, 293, 1101, 2674, 590, 2310, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1183929810157189, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0015007631154730916}, {"id": 17, "seek": 14400, "start": 164.0, "end": 173.0, "text": " Traditionally, these two systems don't talk to each other very well.", "tokens": [51364, 22017, 15899, 11, 613, 732, 3652, 500, 380, 751, 281, 1184, 661, 588, 731, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1183929810157189, "compression_ratio": 1.6346153846153846, "no_speech_prob": 0.0015007631154730916}, {"id": 18, "seek": 17300, "start": 173.0, "end": 178.0, "text": " But we do want features of both for many AI applications, right?", "tokens": [50364, 583, 321, 360, 528, 4122, 295, 1293, 337, 867, 7318, 5821, 11, 558, 30, 50614], "temperature": 0.0, "avg_logprob": -0.1148750926509048, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0037590384017676115}, {"id": 19, "seek": 17300, "start": 178.0, "end": 184.0, "text": " And so that has given rise to this field called neurosymbolic programming.", "tokens": [50614, 400, 370, 300, 575, 2212, 6272, 281, 341, 2519, 1219, 12087, 329, 88, 5612, 299, 9410, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1148750926509048, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0037590384017676115}, {"id": 20, "seek": 17300, "start": 184.0, "end": 194.0, "text": " Now, this term means slightly different things to different people, and so I'm going to define more specifically what it means to us.", "tokens": [50914, 823, 11, 341, 1433, 1355, 4748, 819, 721, 281, 819, 561, 11, 293, 370, 286, 478, 516, 281, 6964, 544, 4682, 437, 309, 1355, 281, 505, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1148750926509048, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0037590384017676115}, {"id": 21, "seek": 19400, "start": 194.0, "end": 205.0, "text": " There is a much richer taxonomy of different styles of neurosymbolic programming that we don't necessarily encompass.", "tokens": [50364, 821, 307, 257, 709, 29021, 3366, 23423, 295, 819, 13273, 295, 12087, 329, 88, 5612, 299, 9410, 300, 321, 500, 380, 4725, 28268, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07469864402498518, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.024018222466111183}, {"id": 22, "seek": 19400, "start": 205.0, "end": 215.0, "text": " But I will show you that at least this form that we consider is interesting enough to write a lot of useful applications, right?", "tokens": [50914, 583, 286, 486, 855, 291, 300, 412, 1935, 341, 1254, 300, 321, 1949, 307, 1880, 1547, 281, 2464, 257, 688, 295, 4420, 5821, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07469864402498518, "compression_ratio": 1.4057142857142857, "no_speech_prob": 0.024018222466111183}, {"id": 23, "seek": 21500, "start": 215.0, "end": 230.0, "text": " So what I've shown here is a picture of the three approaches before proceeding to why it is hard to combine deep learning and classical algorithms in a single system.", "tokens": [50364, 407, 437, 286, 600, 4898, 510, 307, 257, 3036, 295, 264, 1045, 11587, 949, 41163, 281, 983, 309, 307, 1152, 281, 10432, 2452, 2539, 293, 13735, 14642, 294, 257, 2167, 1185, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07172521617677477, "compression_ratio": 1.3174603174603174, "no_speech_prob": 0.01362304762005806}, {"id": 24, "seek": 23000, "start": 230.0, "end": 247.0, "text": " So in deep learning, much of the success comes from gradient descent algorithms for backpropagating the loss to learn the parameter theta of this neural model.", "tokens": [50364, 407, 294, 2452, 2539, 11, 709, 295, 264, 2245, 1487, 490, 16235, 23475, 14642, 337, 646, 79, 1513, 559, 990, 264, 4470, 281, 1466, 264, 13075, 9725, 295, 341, 18161, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10775948422295707, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.026340382173657417}, {"id": 25, "seek": 23000, "start": 247.0, "end": 256.0, "text": " X and Y are in these double boxes indicating that you have supervision on them, right?", "tokens": [51214, 1783, 293, 398, 366, 294, 613, 3834, 9002, 25604, 300, 291, 362, 32675, 322, 552, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.10775948422295707, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.026340382173657417}, {"id": 26, "seek": 25600, "start": 256.0, "end": 269.0, "text": " On the other hand, classical algorithms such as this program P can typically work on structured data, which I'm going to indicate with R, okay?", "tokens": [50364, 1282, 264, 661, 1011, 11, 13735, 14642, 1270, 382, 341, 1461, 430, 393, 5850, 589, 322, 18519, 1412, 11, 597, 286, 478, 516, 281, 13330, 365, 497, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.10995713970329188, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.003763315500691533}, {"id": 27, "seek": 25600, "start": 269.0, "end": 284.0, "text": " And in neurosymbolic, this is a simple neurosymbolic program where there's initially a neural component and theta, which takes input X, produces an intermediate representation R, which is structured,", "tokens": [51014, 400, 294, 12087, 329, 88, 5612, 299, 11, 341, 307, 257, 2199, 12087, 329, 88, 5612, 299, 1461, 689, 456, 311, 9105, 257, 18161, 6542, 293, 9725, 11, 597, 2516, 4846, 1783, 11, 14725, 364, 19376, 10290, 497, 11, 597, 307, 18519, 11, 51764], "temperature": 0.0, "avg_logprob": -0.10995713970329188, "compression_ratio": 1.5953488372093023, "no_speech_prob": 0.003763315500691533}, {"id": 28, "seek": 28400, "start": 284.0, "end": 291.0, "text": " which in turn is fed to a classical algorithm P to produce an output Y, right?", "tokens": [50364, 597, 294, 1261, 307, 4636, 281, 257, 13735, 9284, 430, 281, 5258, 364, 5598, 398, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.12189925303224658, "compression_ratio": 1.4036144578313252, "no_speech_prob": 0.0038821420166641474}, {"id": 29, "seek": 28400, "start": 291.0, "end": 308.0, "text": " And we have gradient descent here, dr by d theta, but we'd also like to somehow backpropagate the loss across this program, this discrete program P, okay?", "tokens": [50714, 400, 321, 362, 16235, 23475, 510, 11, 1224, 538, 274, 9725, 11, 457, 321, 1116, 611, 411, 281, 6063, 646, 79, 1513, 559, 473, 264, 4470, 2108, 341, 1461, 11, 341, 27706, 1461, 430, 11, 1392, 30, 51564], "temperature": 0.0, "avg_logprob": -0.12189925303224658, "compression_ratio": 1.4036144578313252, "no_speech_prob": 0.0038821420166641474}, {"id": 30, "seek": 30800, "start": 308.0, "end": 322.0, "text": " And even though this looks like a supervised learning setting, we have actually used neurosymbolic programming in many different settings that I'll show you today, including RL and even unsupervised learning with foundation models.", "tokens": [50364, 400, 754, 1673, 341, 1542, 411, 257, 46533, 2539, 3287, 11, 321, 362, 767, 1143, 12087, 329, 88, 5612, 299, 9410, 294, 867, 819, 6257, 300, 286, 603, 855, 291, 965, 11, 3009, 497, 43, 293, 754, 2693, 12879, 24420, 2539, 365, 7030, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09952216288622688, "compression_ratio": 1.515, "no_speech_prob": 0.005299573764204979}, {"id": 31, "seek": 30800, "start": 322.0, "end": 326.0, "text": " So what are some of the challenges that we tackled in building Scallop?", "tokens": [51064, 407, 437, 366, 512, 295, 264, 4759, 300, 321, 9426, 1493, 294, 2390, 2747, 336, 404, 30, 51264], "temperature": 0.0, "avg_logprob": -0.09952216288622688, "compression_ratio": 1.515, "no_speech_prob": 0.005299573764204979}, {"id": 32, "seek": 32600, "start": 326.0, "end": 332.0, "text": " So the first question is what is the symbolic representation we use for R?", "tokens": [50364, 407, 264, 700, 1168, 307, 437, 307, 264, 25755, 10290, 321, 764, 337, 497, 30, 50664], "temperature": 0.0, "avg_logprob": -0.0864268320578116, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.017170125618577003}, {"id": 33, "seek": 32600, "start": 332.0, "end": 338.0, "text": " And the second is what is the reasoning language for programs P, right?", "tokens": [50664, 400, 264, 1150, 307, 437, 307, 264, 21577, 2856, 337, 4268, 430, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.0864268320578116, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.017170125618577003}, {"id": 34, "seek": 32600, "start": 338.0, "end": 345.0, "text": " As you can imagine, there's a lot of different choices for these decisions.", "tokens": [50964, 1018, 291, 393, 3811, 11, 456, 311, 257, 688, 295, 819, 7994, 337, 613, 5327, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0864268320578116, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.017170125618577003}, {"id": 35, "seek": 34500, "start": 345.0, "end": 359.0, "text": " Most importantly, how can we obtain an automatic and efficient differentiable reasoning engine to learn this dy by dr under what we call algorithmic supervision, right?", "tokens": [50364, 4534, 8906, 11, 577, 393, 321, 12701, 364, 12509, 293, 7148, 819, 9364, 21577, 2848, 281, 1466, 341, 14584, 538, 1224, 833, 437, 321, 818, 9284, 299, 32675, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.0955468685396256, "compression_ratio": 1.4712041884816753, "no_speech_prob": 0.016396917402744293}, {"id": 36, "seek": 34500, "start": 359.0, "end": 362.0, "text": " What that means is you're not given supervision on R, right?", "tokens": [51064, 708, 300, 1355, 307, 291, 434, 406, 2212, 32675, 322, 497, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.0955468685396256, "compression_ratio": 1.4712041884816753, "no_speech_prob": 0.016396917402744293}, {"id": 37, "seek": 34500, "start": 362.0, "end": 366.0, "text": " And that makes this whole problem more challenging.", "tokens": [51214, 400, 300, 1669, 341, 1379, 1154, 544, 7595, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0955468685396256, "compression_ratio": 1.4712041884816753, "no_speech_prob": 0.016396917402744293}, {"id": 38, "seek": 36600, "start": 366.0, "end": 373.0, "text": " We only have supervision end to end on end to end observable input output data X and Y, okay?", "tokens": [50364, 492, 787, 362, 32675, 917, 281, 917, 322, 917, 281, 917, 9951, 712, 4846, 5598, 1412, 1783, 293, 398, 11, 1392, 30, 50714], "temperature": 0.0, "avg_logprob": -0.13150460379464285, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.19912979006767273}, {"id": 39, "seek": 36600, "start": 373.0, "end": 379.0, "text": " But not on sort of intermediate data R. And this makes sense for a lot of applications.", "tokens": [50714, 583, 406, 322, 1333, 295, 19376, 1412, 497, 13, 400, 341, 1669, 2020, 337, 257, 688, 295, 5821, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13150460379464285, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.19912979006767273}, {"id": 40, "seek": 36600, "start": 379.0, "end": 381.0, "text": " If you think of a healthcare application, right?", "tokens": [51014, 759, 291, 519, 295, 257, 8884, 3861, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.13150460379464285, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.19912979006767273}, {"id": 41, "seek": 36600, "start": 381.0, "end": 387.0, "text": " You have data about a patient, all of their lab measurements and so on.", "tokens": [51114, 509, 362, 1412, 466, 257, 4537, 11, 439, 295, 641, 2715, 15383, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13150460379464285, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.19912979006767273}, {"id": 42, "seek": 38700, "start": 387.0, "end": 391.0, "text": " We also have some outcome Y of, say, a treatment.", "tokens": [50364, 492, 611, 362, 512, 9700, 398, 295, 11, 584, 11, 257, 5032, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17578187584877014, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.02126588486135006}, {"id": 43, "seek": 38700, "start": 391.0, "end": 401.0, "text": " But even an expert clinician might not have time to label every intermediate piece of information or might not even know how to label it, right?", "tokens": [50564, 583, 754, 364, 5844, 45962, 1062, 406, 362, 565, 281, 7645, 633, 19376, 2522, 295, 1589, 420, 1062, 406, 754, 458, 577, 281, 7645, 309, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.17578187584877014, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.02126588486135006}, {"id": 44, "seek": 38700, "start": 401.0, "end": 404.0, "text": " Even if they could.", "tokens": [51064, 2754, 498, 436, 727, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17578187584877014, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.02126588486135006}, {"id": 45, "seek": 38700, "start": 404.0, "end": 410.0, "text": " So that makes this problem particularly challenging.", "tokens": [51214, 407, 300, 1669, 341, 1154, 4098, 7595, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17578187584877014, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.02126588486135006}, {"id": 46, "seek": 41000, "start": 410.0, "end": 412.0, "text": " We also have two other challenges here.", "tokens": [50364, 492, 611, 362, 732, 661, 4759, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11216888929668226, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.01589975506067276}, {"id": 47, "seek": 41000, "start": 412.0, "end": 423.0, "text": " We'd like to tailor learning of this computing dy by dr by different applications, characteristics, okay?", "tokens": [50464, 492, 1116, 411, 281, 33068, 2539, 295, 341, 15866, 14584, 538, 1224, 538, 819, 5821, 11, 10891, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.11216888929668226, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.01589975506067276}, {"id": 48, "seek": 41000, "start": 423.0, "end": 432.0, "text": " At this point, we are looking at approximate algorithms which go along well with gradient descent, which is already approximate.", "tokens": [51014, 1711, 341, 935, 11, 321, 366, 1237, 412, 30874, 14642, 597, 352, 2051, 731, 365, 16235, 23475, 11, 597, 307, 1217, 30874, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11216888929668226, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.01589975506067276}, {"id": 49, "seek": 41000, "start": 432.0, "end": 438.0, "text": " And lastly, we'd like to integrate with all the existing training pipelines.", "tokens": [51464, 400, 16386, 11, 321, 1116, 411, 281, 13365, 365, 439, 264, 6741, 3097, 40168, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11216888929668226, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.01589975506067276}, {"id": 50, "seek": 43800, "start": 438.0, "end": 441.0, "text": " This is more of an empirical challenge here.", "tokens": [50364, 639, 307, 544, 295, 364, 31886, 3430, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10257840156555176, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0018361863913014531}, {"id": 51, "seek": 43800, "start": 441.0, "end": 444.0, "text": " We don't want to reinvent something like PyTorch.", "tokens": [50514, 492, 500, 380, 528, 281, 33477, 746, 411, 9953, 51, 284, 339, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10257840156555176, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0018361863913014531}, {"id": 52, "seek": 43800, "start": 444.0, "end": 453.0, "text": " And so we'd like to reuse all the existing models and training pipelines.", "tokens": [50664, 400, 370, 321, 1116, 411, 281, 26225, 439, 264, 6741, 5245, 293, 3097, 40168, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10257840156555176, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0018361863913014531}, {"id": 53, "seek": 43800, "start": 453.0, "end": 458.0, "text": " So here are some of the design decisions that we made.", "tokens": [51114, 407, 510, 366, 512, 295, 264, 1715, 5327, 300, 321, 1027, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10257840156555176, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0018361863913014531}, {"id": 54, "seek": 43800, "start": 458.0, "end": 462.0, "text": " Much of this borrows from work by other researchers.", "tokens": [51364, 12313, 295, 341, 14828, 28251, 490, 589, 538, 661, 10309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10257840156555176, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0018361863913014531}, {"id": 55, "seek": 46200, "start": 462.0, "end": 469.0, "text": " I'm not necessarily inspired by DeepProblog, but would later learn that there was work by many other researchers as well.", "tokens": [50364, 286, 478, 406, 4725, 7547, 538, 14895, 12681, 65, 4987, 11, 457, 576, 1780, 1466, 300, 456, 390, 589, 538, 867, 661, 10309, 382, 731, 13, 50714], "temperature": 0.0, "avg_logprob": -0.19009511535232132, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.010978007689118385}, {"id": 56, "seek": 46200, "start": 469.0, "end": 480.0, "text": " And our main contribution here was really using Datalog and putting all of this together in a practical system, right?", "tokens": [50714, 400, 527, 2135, 13150, 510, 390, 534, 1228, 9315, 44434, 293, 3372, 439, 295, 341, 1214, 294, 257, 8496, 1185, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.19009511535232132, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.010978007689118385}, {"id": 57, "seek": 46200, "start": 480.0, "end": 488.0, "text": " So the first design decision here was to use a relational representation for R, right?", "tokens": [51264, 407, 264, 700, 1715, 3537, 510, 390, 281, 764, 257, 38444, 10290, 337, 497, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.19009511535232132, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.010978007689118385}, {"id": 58, "seek": 48800, "start": 488.0, "end": 497.0, "text": " And I don't need to tell a database community about how the relational model has withstood the test of time.", "tokens": [50364, 400, 286, 500, 380, 643, 281, 980, 257, 8149, 1768, 466, 577, 264, 38444, 2316, 575, 365, 6431, 264, 1500, 295, 565, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12525160883513975, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.006585213355720043}, {"id": 59, "seek": 48800, "start": 497.0, "end": 505.0, "text": " It can represent very general forms of data in arbitrary graphs.", "tokens": [50814, 467, 393, 2906, 588, 2674, 6422, 295, 1412, 294, 23211, 24877, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12525160883513975, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.006585213355720043}, {"id": 60, "seek": 48800, "start": 505.0, "end": 514.0, "text": " There are many other nice properties about relations, as I will show you when we tag tuples with probabilities and more general kinds of information,", "tokens": [51214, 821, 366, 867, 661, 1481, 7221, 466, 2299, 11, 382, 286, 486, 855, 291, 562, 321, 6162, 2604, 2622, 365, 33783, 293, 544, 2674, 3685, 295, 1589, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12525160883513975, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.006585213355720043}, {"id": 61, "seek": 51400, "start": 514.0, "end": 518.0, "text": " so this relational representation is really helpful.", "tokens": [50364, 370, 341, 38444, 10290, 307, 534, 4961, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1298078649184283, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.004534980282187462}, {"id": 62, "seek": 51400, "start": 518.0, "end": 525.0, "text": " The second is the choice of the language for P, and here we use a Datalog-based language.", "tokens": [50564, 440, 1150, 307, 264, 3922, 295, 264, 2856, 337, 430, 11, 293, 510, 321, 764, 257, 9315, 44434, 12, 6032, 2856, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1298078649184283, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.004534980282187462}, {"id": 63, "seek": 51400, "start": 525.0, "end": 530.0, "text": " We started out literally with Datalog, but it has grown over the years.", "tokens": [50914, 492, 1409, 484, 3736, 365, 9315, 44434, 11, 457, 309, 575, 7709, 670, 264, 924, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1298078649184283, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.004534980282187462}, {"id": 64, "seek": 51400, "start": 530.0, "end": 535.0, "text": " We have support for algebraic data types, foreign functions, and so on.", "tokens": [51164, 492, 362, 1406, 337, 21989, 299, 1412, 3467, 11, 5329, 6828, 11, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1298078649184283, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.004534980282187462}, {"id": 65, "seek": 51400, "start": 535.0, "end": 538.0, "text": " Actually, it is at this point Turing complete.", "tokens": [51414, 5135, 11, 309, 307, 412, 341, 935, 314, 1345, 3566, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1298078649184283, "compression_ratio": 1.5633802816901408, "no_speech_prob": 0.004534980282187462}, {"id": 66, "seek": 53800, "start": 538.0, "end": 546.0, "text": " So depending on what subset of a language you use, you get different guarantees.", "tokens": [50364, 407, 5413, 322, 437, 25993, 295, 257, 2856, 291, 764, 11, 291, 483, 819, 32567, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10630725069743831, "compression_ratio": 1.3868613138686132, "no_speech_prob": 0.007445678114891052}, {"id": 67, "seek": 53800, "start": 546.0, "end": 554.0, "text": " Perhaps the most interesting piece here for this audience is we accidentally discovered provenance semirings.", "tokens": [50764, 10517, 264, 881, 1880, 2522, 510, 337, 341, 4034, 307, 321, 15715, 6941, 12785, 719, 4361, 347, 1109, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10630725069743831, "compression_ratio": 1.3868613138686132, "no_speech_prob": 0.007445678114891052}, {"id": 68, "seek": 55400, "start": 554.0, "end": 567.0, "text": " We were playing with different kinds of tags and eventually realized that they could generalize them in this very elegant work,", "tokens": [50364, 492, 645, 2433, 365, 819, 3685, 295, 18632, 293, 4728, 5334, 300, 436, 727, 2674, 1125, 552, 294, 341, 588, 21117, 589, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09879751054067461, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.05329643562436104}, {"id": 69, "seek": 55400, "start": 567.0, "end": 572.0, "text": " which was mentioned at the beginning of this workshop on provenance semirings.", "tokens": [51014, 597, 390, 2835, 412, 264, 2863, 295, 341, 13541, 322, 12785, 719, 4361, 347, 1109, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09879751054067461, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.05329643562436104}, {"id": 70, "seek": 55400, "start": 572.0, "end": 577.0, "text": " I will show you the different semirings that we have in my talk.", "tokens": [51264, 286, 486, 855, 291, 264, 819, 4361, 347, 1109, 300, 321, 362, 294, 452, 751, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09879751054067461, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.05329643562436104}, {"id": 71, "seek": 57700, "start": 577.0, "end": 583.0, "text": " Lastly, we have integration with both PyTorch and JAX.", "tokens": [50364, 18072, 11, 321, 362, 10980, 365, 1293, 9953, 51, 284, 339, 293, 26401, 55, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11506009101867676, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.0032699175644665956}, {"id": 72, "seek": 57700, "start": 583.0, "end": 590.0, "text": " PyTorch for getting models that might be pre-trained and so on that we might want to fine-tune,", "tokens": [50664, 9953, 51, 284, 339, 337, 1242, 5245, 300, 1062, 312, 659, 12, 17227, 2001, 293, 370, 322, 300, 321, 1062, 528, 281, 2489, 12, 83, 2613, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11506009101867676, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.0032699175644665956}, {"id": 73, "seek": 57700, "start": 590.0, "end": 598.0, "text": " but also for computing the loss here using something like JAX.", "tokens": [51014, 457, 611, 337, 15866, 264, 4470, 510, 1228, 746, 411, 26401, 55, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11506009101867676, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.0032699175644665956}, {"id": 74, "seek": 57700, "start": 598.0, "end": 605.0, "text": " It is a pretty usable framework and to end with lots of moving pieces.", "tokens": [51414, 467, 307, 257, 1238, 29975, 8388, 293, 281, 917, 365, 3195, 295, 2684, 3755, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11506009101867676, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.0032699175644665956}, {"id": 75, "seek": 60500, "start": 605.0, "end": 611.0, "text": " Let me give a simple motivating example of the kinds of things we can do with Scala.", "tokens": [50364, 961, 385, 976, 257, 2199, 41066, 1365, 295, 264, 3685, 295, 721, 321, 393, 360, 365, 2747, 5159, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16481410132514107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00413135951384902}, {"id": 76, "seek": 60500, "start": 611.0, "end": 616.0, "text": " This is a simple strategy game called Pac-Man.", "tokens": [50664, 639, 307, 257, 2199, 5206, 1216, 1219, 10702, 12, 6652, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16481410132514107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00413135951384902}, {"id": 77, "seek": 60500, "start": 616.0, "end": 619.0, "text": " It is actually a simplified version which is called Static Pac-Man.", "tokens": [50914, 467, 307, 767, 257, 26335, 3037, 597, 307, 1219, 745, 2399, 10702, 12, 6652, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16481410132514107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00413135951384902}, {"id": 78, "seek": 60500, "start": 619.0, "end": 621.0, "text": " The ghosts are not moving.", "tokens": [51064, 440, 21744, 366, 406, 2684, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16481410132514107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00413135951384902}, {"id": 79, "seek": 60500, "start": 621.0, "end": 623.0, "text": " The setup is as follows.", "tokens": [51164, 440, 8657, 307, 382, 10002, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16481410132514107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00413135951384902}, {"id": 80, "seek": 60500, "start": 623.0, "end": 626.0, "text": " There is this grid of 5x5 cells.", "tokens": [51264, 821, 307, 341, 10748, 295, 1025, 87, 20, 5438, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16481410132514107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00413135951384902}, {"id": 81, "seek": 62600, "start": 627.0, "end": 637.0, "text": " Each time in each instance of this game, we randomly assign these ghosts, the Pac-Man agent and the goal in different cells.", "tokens": [50414, 6947, 565, 294, 1184, 5197, 295, 341, 1216, 11, 321, 16979, 6269, 613, 21744, 11, 264, 10702, 12, 6652, 9461, 293, 264, 3387, 294, 819, 5438, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14719643592834472, "compression_ratio": 1.5320512820512822, "no_speech_prob": 0.0361822172999382}, {"id": 82, "seek": 62600, "start": 637.0, "end": 653.0, "text": " As I said, the ghosts do not move and so the goal is to learn to reach the goal without hitting any of the ghosts.", "tokens": [50914, 1018, 286, 848, 11, 264, 21744, 360, 406, 1286, 293, 370, 264, 3387, 307, 281, 1466, 281, 2524, 264, 3387, 1553, 8850, 604, 295, 264, 21744, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14719643592834472, "compression_ratio": 1.5320512820512822, "no_speech_prob": 0.0361822172999382}, {"id": 83, "seek": 65300, "start": 654.0, "end": 661.0, "text": " We set this up as a simple RL reinforcement learning problem.", "tokens": [50414, 492, 992, 341, 493, 382, 257, 2199, 497, 43, 29280, 2539, 1154, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14936208724975586, "compression_ratio": 1.4304635761589404, "no_speech_prob": 0.005054091103374958}, {"id": 84, "seek": 65300, "start": 661.0, "end": 672.0, "text": " We use a simple model here which is BQN, BQ networks to train this agent.", "tokens": [50764, 492, 764, 257, 2199, 2316, 510, 597, 307, 363, 48, 45, 11, 363, 48, 9590, 281, 3847, 341, 9461, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14936208724975586, "compression_ratio": 1.4304635761589404, "no_speech_prob": 0.005054091103374958}, {"id": 85, "seek": 65300, "start": 672.0, "end": 678.0, "text": " The baseline here is an end-to-end neural model, a convolutional neural network,", "tokens": [51314, 440, 20518, 510, 307, 364, 917, 12, 1353, 12, 521, 18161, 2316, 11, 257, 45216, 304, 18161, 3209, 11, 51614], "temperature": 0.0, "avg_logprob": -0.14936208724975586, "compression_ratio": 1.4304635761589404, "no_speech_prob": 0.005054091103374958}, {"id": 86, "seek": 67800, "start": 678.0, "end": 687.0, "text": " which is not given supervision on which cells contain ghosts or the goal post or the Pac-Man itself.", "tokens": [50364, 597, 307, 406, 2212, 32675, 322, 597, 5438, 5304, 21744, 420, 264, 3387, 2183, 420, 264, 10702, 12, 6652, 2564, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12617356917437386, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.006995773874223232}, {"id": 87, "seek": 67800, "start": 687.0, "end": 690.0, "text": " All of this is the intermediate representation to be learned.", "tokens": [50814, 1057, 295, 341, 307, 264, 19376, 10290, 281, 312, 3264, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12617356917437386, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.006995773874223232}, {"id": 88, "seek": 67800, "start": 690.0, "end": 698.0, "text": " The only supervision one gets is after an entire game episode where either the Pac-Man reached the goal,", "tokens": [50964, 440, 787, 32675, 472, 2170, 307, 934, 364, 2302, 1216, 3500, 689, 2139, 264, 10702, 12, 6652, 6488, 264, 3387, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12617356917437386, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.006995773874223232}, {"id": 89, "seek": 67800, "start": 698.0, "end": 705.0, "text": " so you get a reward of 1 or it didn't and in which case it gets a reward of 0.", "tokens": [51364, 370, 291, 483, 257, 7782, 295, 502, 420, 309, 994, 380, 293, 294, 597, 1389, 309, 2170, 257, 7782, 295, 1958, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12617356917437386, "compression_ratio": 1.6555023923444976, "no_speech_prob": 0.006995773874223232}, {"id": 90, "seek": 70500, "start": 706.0, "end": 714.0, "text": " We formulate this problem in Scalop by decomposing it into a neural model which is doing this low-level perception.", "tokens": [50414, 492, 47881, 341, 1154, 294, 2747, 304, 404, 538, 22867, 6110, 309, 666, 257, 18161, 2316, 597, 307, 884, 341, 2295, 12, 12418, 12860, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10703644385704628, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.0007670959457755089}, {"id": 91, "seek": 70500, "start": 714.0, "end": 721.0, "text": " The goal of the neural model is to simply learn what each cell might contain.", "tokens": [50814, 440, 3387, 295, 264, 18161, 2316, 307, 281, 2935, 1466, 437, 1184, 2815, 1062, 5304, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10703644385704628, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.0007670959457755089}, {"id": 92, "seek": 70500, "start": 721.0, "end": 723.0, "text": " There are four choices.", "tokens": [51164, 821, 366, 1451, 7994, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10703644385704628, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.0007670959457755089}, {"id": 93, "seek": 70500, "start": 723.0, "end": 730.0, "text": " It can either be empty or it can contain a ghost or it can contain the Pac-Man itself or it can contain the goal.", "tokens": [51264, 467, 393, 2139, 312, 6707, 420, 309, 393, 5304, 257, 8359, 420, 309, 393, 5304, 264, 10702, 12, 6652, 2564, 420, 309, 393, 5304, 264, 3387, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10703644385704628, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.0007670959457755089}, {"id": 94, "seek": 70500, "start": 730.0, "end": 732.0, "text": " These are the four choices.", "tokens": [51614, 1981, 366, 264, 1451, 7994, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10703644385704628, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.0007670959457755089}, {"id": 95, "seek": 73200, "start": 732.0, "end": 744.0, "text": " Now this neural model outputs these choices to a logic program in Scalop whose goal is to do the path planning.", "tokens": [50364, 823, 341, 18161, 2316, 23930, 613, 7994, 281, 257, 9952, 1461, 294, 2747, 304, 404, 6104, 3387, 307, 281, 360, 264, 3100, 5038, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08203338404170803, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.00037981331115588546}, {"id": 96, "seek": 73200, "start": 744.0, "end": 755.0, "text": " In summary, instead of having a monolithic neural network which is trying to learn end-to-end how to do both entity extraction and path planning,", "tokens": [50964, 682, 12691, 11, 2602, 295, 1419, 257, 1108, 42878, 18161, 3209, 597, 307, 1382, 281, 1466, 917, 12, 1353, 12, 521, 577, 281, 360, 1293, 13977, 30197, 293, 3100, 5038, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08203338404170803, "compression_ratio": 1.4770114942528736, "no_speech_prob": 0.00037981331115588546}, {"id": 97, "seek": 75500, "start": 755.0, "end": 762.0, "text": " we decompose this task into entity extraction which is sort of low-level perception that is best done by a neural module", "tokens": [50364, 321, 22867, 541, 341, 5633, 666, 13977, 30197, 597, 307, 1333, 295, 2295, 12, 12418, 12860, 300, 307, 1151, 1096, 538, 257, 18161, 10088, 50714], "temperature": 0.0, "avg_logprob": -0.11184714306360004, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.010798441246151924}, {"id": 98, "seek": 75500, "start": 762.0, "end": 768.0, "text": " and a logic program, a classical algorithm which does the path planning.", "tokens": [50714, 293, 257, 9952, 1461, 11, 257, 13735, 9284, 597, 775, 264, 3100, 5038, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11184714306360004, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.010798441246151924}, {"id": 99, "seek": 75500, "start": 768.0, "end": 775.0, "text": " At each step, the goal is to decide whether the Pac-Man should move up, down, right or left.", "tokens": [51014, 1711, 1184, 1823, 11, 264, 3387, 307, 281, 4536, 1968, 264, 10702, 12, 6652, 820, 1286, 493, 11, 760, 11, 558, 420, 1411, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11184714306360004, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.010798441246151924}, {"id": 100, "seek": 75500, "start": 775.0, "end": 781.0, "text": " You'll get the reward only after an entire episode of around 20 steps.", "tokens": [51364, 509, 603, 483, 264, 7782, 787, 934, 364, 2302, 3500, 295, 926, 945, 4439, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11184714306360004, "compression_ratio": 1.5063291139240507, "no_speech_prob": 0.010798441246151924}, {"id": 101, "seek": 78100, "start": 782.0, "end": 788.0, "text": " Here is our empirical result.", "tokens": [50414, 1692, 307, 527, 31886, 1874, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10262684754922356, "compression_ratio": 1.4207650273224044, "no_speech_prob": 0.004531839396804571}, {"id": 102, "seek": 78100, "start": 788.0, "end": 798.0, "text": " In just 50 such episodes with this Scalop program that I showed you, we can get an accuracy of 99.4%.", "tokens": [50714, 682, 445, 2625, 1270, 9313, 365, 341, 2747, 304, 404, 1461, 300, 286, 4712, 291, 11, 321, 393, 483, 364, 14170, 295, 11803, 13, 19, 6856, 51214], "temperature": 0.0, "avg_logprob": -0.10262684754922356, "compression_ratio": 1.4207650273224044, "no_speech_prob": 0.004531839396804571}, {"id": 103, "seek": 78100, "start": 798.0, "end": 809.0, "text": " Whereas if you do this with a baseline of end-to-end neural, you get a much lower accuracy and it requires over 50,000 episodes.", "tokens": [51214, 13813, 498, 291, 360, 341, 365, 257, 20518, 295, 917, 12, 1353, 12, 521, 18161, 11, 291, 483, 257, 709, 3126, 14170, 293, 309, 7029, 670, 2625, 11, 1360, 9313, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10262684754922356, "compression_ratio": 1.4207650273224044, "no_speech_prob": 0.004531839396804571}, {"id": 104, "seek": 80900, "start": 810.0, "end": 819.0, "text": " There's some hand-waving here. This is not entirely a fair comparison because we have written a domain knowledge using logic rules.", "tokens": [50414, 821, 311, 512, 1011, 12, 86, 6152, 510, 13, 639, 307, 406, 7696, 257, 3143, 9660, 570, 321, 362, 3720, 257, 9274, 3601, 1228, 9952, 4474, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19232950944166918, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0005192012758925557}, {"id": 105, "seek": 80900, "start": 819.0, "end": 823.0, "text": " I probably skipped over the program itself so here goes.", "tokens": [50864, 286, 1391, 30193, 670, 264, 1461, 2564, 370, 510, 1709, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19232950944166918, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0005192012758925557}, {"id": 106, "seek": 80900, "start": 823.0, "end": 831.0, "text": " This is our syntax for our data log-based language, in this case for path planning.", "tokens": [51064, 639, 307, 527, 28431, 337, 527, 1412, 3565, 12, 6032, 2856, 11, 294, 341, 1389, 337, 3100, 5038, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19232950944166918, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0005192012758925557}, {"id": 107, "seek": 83100, "start": 831.0, "end": 837.0, "text": " The goal here is to compute the next action, one of these four choices.", "tokens": [50364, 440, 3387, 510, 307, 281, 14722, 264, 958, 3069, 11, 472, 295, 613, 1451, 7994, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10847586393356323, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.0015728239668533206}, {"id": 108, "seek": 83100, "start": 837.0, "end": 847.0, "text": " That in turn depends on whether there's a path from an adjoining position where the Pac-Man currently is to the goal.", "tokens": [50664, 663, 294, 1261, 5946, 322, 1968, 456, 311, 257, 3100, 490, 364, 614, 5134, 1760, 2535, 689, 264, 10702, 12, 6652, 4362, 307, 281, 264, 3387, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10847586393356323, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.0015728239668533206}, {"id": 109, "seek": 83100, "start": 847.0, "end": 852.0, "text": " The definition of a path is itself a recursive predicate.", "tokens": [51164, 440, 7123, 295, 257, 3100, 307, 2564, 257, 20560, 488, 3852, 8700, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10847586393356323, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.0015728239668533206}, {"id": 110, "seek": 85200, "start": 852.0, "end": 861.0, "text": " It's a path that does not collide with any of the hosts and that is encoded using what we call safe cells.", "tokens": [50364, 467, 311, 257, 3100, 300, 775, 406, 49093, 365, 604, 295, 264, 21573, 293, 300, 307, 2058, 12340, 1228, 437, 321, 818, 3273, 5438, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1557924981210746, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.006386435590684414}, {"id": 111, "seek": 85200, "start": 861.0, "end": 864.0, "text": " Any questions so far?", "tokens": [50814, 2639, 1651, 370, 1400, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1557924981210746, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.006386435590684414}, {"id": 112, "seek": 85200, "start": 869.0, "end": 874.0, "text": " The programmer writes this discrete program without any probabilities and so on.", "tokens": [51214, 440, 32116, 13657, 341, 27706, 1461, 1553, 604, 33783, 293, 370, 322, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1557924981210746, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.006386435590684414}, {"id": 113, "seek": 87400, "start": 875.0, "end": 891.0, "text": " What we will see happening under the hood when both at training and inference time is a neural model will compute these predicates such as actor, goal and enemy only with different degrees of certainty.", "tokens": [50414, 708, 321, 486, 536, 2737, 833, 264, 13376, 562, 1293, 412, 3097, 293, 38253, 565, 307, 257, 18161, 2316, 486, 14722, 613, 47336, 1024, 1270, 382, 8747, 11, 3387, 293, 5945, 787, 365, 819, 5310, 295, 27022, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13199260830879211, "compression_ratio": 1.5326633165829147, "no_speech_prob": 0.0020186242181807756}, {"id": 114, "seek": 87400, "start": 891.0, "end": 900.0, "text": " In some sense, we have all of these possible words being tracked simultaneously by the scallop engine.", "tokens": [51214, 682, 512, 2020, 11, 321, 362, 439, 295, 613, 1944, 2283, 885, 31703, 16561, 538, 264, 30509, 404, 2848, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13199260830879211, "compression_ratio": 1.5326633165829147, "no_speech_prob": 0.0020186242181807756}, {"id": 115, "seek": 90000, "start": 901.0, "end": 912.0, "text": " All of that computation will be done through tags which you don't see here at the level of the values that are being propagated.", "tokens": [50414, 1057, 295, 300, 24903, 486, 312, 1096, 807, 18632, 597, 291, 500, 380, 536, 510, 412, 264, 1496, 295, 264, 4190, 300, 366, 885, 12425, 770, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1809390045347668, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.024774396792054176}, {"id": 116, "seek": 90000, "start": 912.0, "end": 917.0, "text": " Can I ask you a quick question on this? Is there a notion of a shortest path or is it any path?", "tokens": [50964, 1664, 286, 1029, 291, 257, 1702, 1168, 322, 341, 30, 1119, 456, 257, 10710, 295, 257, 31875, 3100, 420, 307, 309, 604, 3100, 30, 51214], "temperature": 0.0, "avg_logprob": -0.1809390045347668, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.024774396792054176}, {"id": 117, "seek": 90000, "start": 917.0, "end": 928.0, "text": " Great question. Here we say any path but if I understand this correctly, the tags will penalize paths that are longer.", "tokens": [51214, 3769, 1168, 13, 1692, 321, 584, 604, 3100, 457, 498, 286, 1223, 341, 8944, 11, 264, 18632, 486, 13661, 1125, 14518, 300, 366, 2854, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1809390045347668, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.024774396792054176}, {"id": 118, "seek": 92800, "start": 929.0, "end": 931.0, "text": " Cycles in the path you will...", "tokens": [50414, 10295, 6520, 294, 264, 3100, 291, 486, 485, 50514], "temperature": 0.0, "avg_logprob": -0.2811404770495845, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.00780999381095171}, {"id": 119, "seek": 92800, "start": 937.0, "end": 949.0, "text": " Let me get into the semirings. The short answer is the tags will be tracking a finite amount of information so they won't necessarily compute all paths with cycles and so on.", "tokens": [50814, 961, 385, 483, 666, 264, 4361, 347, 1109, 13, 440, 2099, 1867, 307, 264, 18632, 486, 312, 11603, 257, 19362, 2372, 295, 1589, 370, 436, 1582, 380, 4725, 14722, 439, 14518, 365, 17796, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2811404770495845, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.00780999381095171}, {"id": 120, "seek": 94900, "start": 950.0, "end": 963.0, "text": " In this example, the neural network is responsible for extracting the state of the program and then you have an actual program to perform the logic.", "tokens": [50414, 682, 341, 1365, 11, 264, 18161, 3209, 307, 6250, 337, 49844, 264, 1785, 295, 264, 1461, 293, 550, 291, 362, 364, 3539, 1461, 281, 2042, 264, 9952, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2530370288425022, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.03726273030042648}, {"id": 121, "seek": 94900, "start": 963.0, "end": 969.0, "text": " The neural network extracts action information or something else.", "tokens": [51064, 440, 18161, 3209, 8947, 82, 3069, 1589, 420, 746, 1646, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2530370288425022, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.03726273030042648}, {"id": 122, "seek": 94900, "start": 969.0, "end": 972.0, "text": " So the neural network only extracts...", "tokens": [51364, 407, 264, 18161, 3209, 787, 8947, 82, 485, 51514], "temperature": 0.0, "avg_logprob": -0.2530370288425022, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.03726273030042648}, {"id": 123, "seek": 94900, "start": 972.0, "end": 978.0, "text": " So the question was whether the neural network extracts these predicates actions.", "tokens": [51514, 407, 264, 1168, 390, 1968, 264, 18161, 3209, 8947, 82, 613, 47336, 1024, 5909, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2530370288425022, "compression_ratio": 1.8715083798882681, "no_speech_prob": 0.03726273030042648}, {"id": 124, "seek": 97800, "start": 979.0, "end": 980.0, "text": " If it did.", "tokens": [50414, 759, 309, 630, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14608584608987113, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.008306057192385197}, {"id": 125, "seek": 97800, "start": 983.0, "end": 987.0, "text": " So that is the baseline that I was showing you that you don't have a logic program.", "tokens": [50614, 407, 300, 307, 264, 20518, 300, 286, 390, 4099, 291, 300, 291, 500, 380, 362, 257, 9952, 1461, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14608584608987113, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.008306057192385197}, {"id": 126, "seek": 97800, "start": 987.0, "end": 997.0, "text": " So the neural network is taking in this grid of pixels, 200 by 200 pixels and producing one of these four outputs or other distribution over these four outputs.", "tokens": [50814, 407, 264, 18161, 3209, 307, 1940, 294, 341, 10748, 295, 18668, 11, 2331, 538, 2331, 18668, 293, 10501, 472, 295, 613, 1451, 23930, 420, 661, 7316, 670, 613, 1451, 23930, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14608584608987113, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.008306057192385197}, {"id": 127, "seek": 97800, "start": 997.0, "end": 1001.0, "text": " So that is the baseline. If you were using that, then you wouldn't need scallop.", "tokens": [51314, 407, 300, 307, 264, 20518, 13, 759, 291, 645, 1228, 300, 11, 550, 291, 2759, 380, 643, 30509, 404, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14608584608987113, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.008306057192385197}, {"id": 128, "seek": 97800, "start": 1001.0, "end": 1007.0, "text": " Here we are trying to show you that you can actually do more data efficient and robust and so on.", "tokens": [51514, 1692, 321, 366, 1382, 281, 855, 291, 300, 291, 393, 767, 360, 544, 1412, 7148, 293, 13956, 293, 370, 322, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14608584608987113, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.008306057192385197}, {"id": 129, "seek": 100700, "start": 1007.0, "end": 1013.0, "text": " By the way, this program, which is learned, generalizes very nicely to much larger grids, even 25 by 25.", "tokens": [50364, 3146, 264, 636, 11, 341, 1461, 11, 597, 307, 3264, 11, 2674, 5660, 588, 9594, 281, 709, 4833, 677, 3742, 11, 754, 3552, 538, 3552, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12613077724681182, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0005110293859615922}, {"id": 130, "seek": 100700, "start": 1013.0, "end": 1023.0, "text": " So you see, whereas a network which was trained end to end would probably not generalize well to other grid sizes.", "tokens": [50664, 407, 291, 536, 11, 9735, 257, 3209, 597, 390, 8895, 917, 281, 917, 576, 1391, 406, 2674, 1125, 731, 281, 661, 10748, 11602, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12613077724681182, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0005110293859615922}, {"id": 131, "seek": 100700, "start": 1023.0, "end": 1028.0, "text": " So let me briefly talk about what is going on in the scallop compiler.", "tokens": [51164, 407, 718, 385, 10515, 751, 466, 437, 307, 516, 322, 294, 264, 30509, 404, 31958, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12613077724681182, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0005110293859615922}, {"id": 132, "seek": 100700, "start": 1028.0, "end": 1031.0, "text": " So we have this differentiable reasoning framework.", "tokens": [51414, 407, 321, 362, 341, 819, 9364, 21577, 8388, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12613077724681182, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.0005110293859615922}, {"id": 133, "seek": 103100, "start": 1032.0, "end": 1035.0, "text": " First, a preview of our entire compiler.", "tokens": [50414, 2386, 11, 257, 14281, 295, 527, 2302, 31958, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15958259999752045, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0010318978456780314}, {"id": 134, "seek": 103100, "start": 1035.0, "end": 1038.0, "text": " So the surface syntax looks like this.", "tokens": [50564, 407, 264, 3753, 28431, 1542, 411, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15958259999752045, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0010318978456780314}, {"id": 135, "seek": 103100, "start": 1038.0, "end": 1043.0, "text": " In this case, it even has limited forms of quantifiers.", "tokens": [50714, 682, 341, 1389, 11, 309, 754, 575, 5567, 6422, 295, 4426, 23463, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15958259999752045, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0010318978456780314}, {"id": 136, "seek": 103100, "start": 1043.0, "end": 1052.0, "text": " We have a front end which produces these abstract syntax trees and there are several passes here for type inference and so on.", "tokens": [50964, 492, 362, 257, 1868, 917, 597, 14725, 613, 12649, 28431, 5852, 293, 456, 366, 2940, 11335, 510, 337, 2010, 38253, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15958259999752045, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0010318978456780314}, {"id": 137, "seek": 105200, "start": 1052.0, "end": 1063.0, "text": " Then we have a back end where, which is based on extended data log where we do a lot of optimizations including query planning and magic set transformations and so on.", "tokens": [50364, 1396, 321, 362, 257, 646, 917, 689, 11, 597, 307, 2361, 322, 10913, 1412, 3565, 689, 321, 360, 257, 688, 295, 5028, 14455, 3009, 14581, 5038, 293, 5585, 992, 34852, 293, 370, 322, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13632214069366455, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0033756622578948736}, {"id": 138, "seek": 105200, "start": 1063.0, "end": 1074.0, "text": " And finally, we have this relational algebra machine or RAM, which is what is actually executed at training and inference time.", "tokens": [50914, 400, 2721, 11, 321, 362, 341, 38444, 21989, 3479, 420, 14561, 11, 597, 307, 437, 307, 767, 17577, 412, 3097, 293, 38253, 565, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13632214069366455, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.0033756622578948736}, {"id": 139, "seek": 107400, "start": 1074.0, "end": 1082.0, "text": " And this is what the equivalent scallop RAM program looks like for that high level constraint.", "tokens": [50364, 400, 341, 307, 437, 264, 10344, 30509, 404, 14561, 1461, 1542, 411, 337, 300, 1090, 1496, 25534, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14042836266594963, "compression_ratio": 1.4929577464788732, "no_speech_prob": 0.003649122081696987}, {"id": 140, "seek": 107400, "start": 1082.0, "end": 1086.0, "text": " So where does prominence come in?", "tokens": [50764, 407, 689, 775, 39225, 655, 808, 294, 30, 50964], "temperature": 0.0, "avg_logprob": -0.14042836266594963, "compression_ratio": 1.4929577464788732, "no_speech_prob": 0.003649122081696987}, {"id": 141, "seek": 107400, "start": 1086.0, "end": 1096.0, "text": " So the semantics of SEL RAM, which is essentially just extended relational algebra, which is the semantics of data log.", "tokens": [50964, 407, 264, 4361, 45298, 295, 318, 3158, 14561, 11, 597, 307, 4476, 445, 10913, 38444, 21989, 11, 597, 307, 264, 4361, 45298, 295, 1412, 3565, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14042836266594963, "compression_ratio": 1.4929577464788732, "no_speech_prob": 0.003649122081696987}, {"id": 142, "seek": 107400, "start": 1096.0, "end": 1102.0, "text": " We have implemented a very general framework for tracking provenance.", "tokens": [51464, 492, 362, 12270, 257, 588, 2674, 8388, 337, 11603, 12785, 719, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14042836266594963, "compression_ratio": 1.4929577464788732, "no_speech_prob": 0.003649122081696987}, {"id": 143, "seek": 110200, "start": 1102.0, "end": 1110.0, "text": " And this is inspired by the work on provenance semirings that was mentioned at the beginning of this workshop.", "tokens": [50364, 400, 341, 307, 7547, 538, 264, 589, 322, 12785, 719, 4361, 347, 1109, 300, 390, 2835, 412, 264, 2863, 295, 341, 13541, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10770204101783643, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.00254843570291996}, {"id": 144, "seek": 110200, "start": 1110.0, "end": 1118.0, "text": " And we even have this very clean interface to define new provenance structures.", "tokens": [50764, 400, 321, 754, 362, 341, 588, 2541, 9226, 281, 6964, 777, 12785, 719, 9227, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10770204101783643, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.00254843570291996}, {"id": 145, "seek": 110200, "start": 1118.0, "end": 1126.0, "text": " So again, covered in the original tutorial, but briefly there's this tag space that you have to define yourself.", "tokens": [51164, 407, 797, 11, 5343, 294, 264, 3380, 7073, 11, 457, 10515, 456, 311, 341, 6162, 1901, 300, 291, 362, 281, 6964, 1803, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10770204101783643, "compression_ratio": 1.5618556701030928, "no_speech_prob": 0.00254843570291996}, {"id": 146, "seek": 112600, "start": 1126.0, "end": 1132.0, "text": " And then various operations for disjunction, conjunction, negation and saturation.", "tokens": [50364, 400, 550, 3683, 7705, 337, 717, 10010, 882, 11, 27482, 11, 2485, 399, 293, 27090, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12286653652997084, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004679559264332056}, {"id": 147, "seek": 112600, "start": 1132.0, "end": 1138.0, "text": " I've shown one instance of this abstract structure here, which we call max min probabilities.", "tokens": [50664, 286, 600, 4898, 472, 5197, 295, 341, 12649, 3877, 510, 11, 597, 321, 818, 11469, 923, 33783, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12286653652997084, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004679559264332056}, {"id": 148, "seek": 112600, "start": 1138.0, "end": 1150.0, "text": " Here the set of tags is real numbers between 0 and 1 and disjunction is max, conjunction is min and so on and so forth.", "tokens": [50964, 1692, 264, 992, 295, 18632, 307, 957, 3547, 1296, 1958, 293, 502, 293, 717, 10010, 882, 307, 11469, 11, 27482, 307, 923, 293, 370, 322, 293, 370, 5220, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12286653652997084, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.004679559264332056}, {"id": 149, "seek": 115000, "start": 1150.0, "end": 1160.0, "text": " If you apply this max min probe to a particular rule during the execution of the program I showed you,", "tokens": [50364, 759, 291, 3079, 341, 11469, 923, 22715, 281, 257, 1729, 4978, 1830, 264, 15058, 295, 264, 1461, 286, 4712, 291, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1159193365423529, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0025906735099852085}, {"id": 150, "seek": 115000, "start": 1160.0, "end": 1164.0, "text": " let's say the rule which computes whether a cell x comma y is safe.", "tokens": [50864, 718, 311, 584, 264, 4978, 597, 715, 1819, 1968, 257, 2815, 2031, 22117, 288, 307, 3273, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1159193365423529, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0025906735099852085}, {"id": 151, "seek": 115000, "start": 1164.0, "end": 1170.0, "text": " So it is safe if it is indeed a grid cell, a cell in the grid, in the 5 by 5 grid,", "tokens": [51064, 407, 309, 307, 3273, 498, 309, 307, 6451, 257, 10748, 2815, 11, 257, 2815, 294, 264, 10748, 11, 294, 264, 1025, 538, 1025, 10748, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1159193365423529, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0025906735099852085}, {"id": 152, "seek": 115000, "start": 1170.0, "end": 1174.0, "text": " and we do not believe that there's an enemy in the cell.", "tokens": [51364, 293, 321, 360, 406, 1697, 300, 456, 311, 364, 5945, 294, 264, 2815, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1159193365423529, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0025906735099852085}, {"id": 153, "seek": 115000, "start": 1174.0, "end": 1179.0, "text": " So this is the standard semantics of data log, of discrete data log, untagged semantics.", "tokens": [51564, 407, 341, 307, 264, 3832, 4361, 45298, 295, 1412, 3565, 11, 295, 27706, 1412, 3565, 11, 1701, 559, 3004, 4361, 45298, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1159193365423529, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0025906735099852085}, {"id": 154, "seek": 117900, "start": 1179.0, "end": 1184.0, "text": " So let us say that in 1 comma 2 and 2 comma 3 are two different grid cells.", "tokens": [50364, 407, 718, 505, 584, 300, 294, 502, 22117, 568, 293, 568, 22117, 805, 366, 732, 819, 10748, 5438, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12576002715736306, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0023220819421112537}, {"id": 155, "seek": 117900, "start": 1184.0, "end": 1190.0, "text": " And let us say the enemy is in the cell 2 comma 3, then we know how to compute this difference.", "tokens": [50614, 400, 718, 505, 584, 264, 5945, 307, 294, 264, 2815, 568, 22117, 805, 11, 550, 321, 458, 577, 281, 14722, 341, 2649, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12576002715736306, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0023220819421112537}, {"id": 156, "seek": 117900, "start": 1190.0, "end": 1193.0, "text": " So that's just the tuple 1 comma 2.", "tokens": [50914, 407, 300, 311, 445, 264, 2604, 781, 502, 22117, 568, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12576002715736306, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0023220819421112537}, {"id": 157, "seek": 117900, "start": 1193.0, "end": 1201.0, "text": " But in the tagged semantics, something much richer is happening, which is that we have these tags t1, t2 and t3 now.", "tokens": [51064, 583, 294, 264, 40239, 4361, 45298, 11, 746, 709, 29021, 307, 2737, 11, 597, 307, 300, 321, 362, 613, 18632, 256, 16, 11, 256, 17, 293, 256, 18, 586, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12576002715736306, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0023220819421112537}, {"id": 158, "seek": 120100, "start": 1202.0, "end": 1209.0, "text": " And they are propagated here along with the output values of each rule.", "tokens": [50414, 400, 436, 366, 12425, 770, 510, 2051, 365, 264, 5598, 4190, 295, 1184, 4978, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08261711169511844, "compression_ratio": 1.555, "no_speech_prob": 0.004068854730576277}, {"id": 159, "seek": 120100, "start": 1209.0, "end": 1216.0, "text": " And once you use a particular provenance semiring, in this case the max min probe,", "tokens": [50764, 400, 1564, 291, 764, 257, 1729, 12785, 719, 4361, 5057, 11, 294, 341, 1389, 264, 11469, 923, 22715, 11, 51114], "temperature": 0.0, "avg_logprob": -0.08261711169511844, "compression_ratio": 1.555, "no_speech_prob": 0.004068854730576277}, {"id": 160, "seek": 120100, "start": 1216.0, "end": 1222.0, "text": " we can for example say that in this case we believe the enemy is in cell 2 comma 3", "tokens": [51114, 321, 393, 337, 1365, 584, 300, 294, 341, 1389, 321, 1697, 264, 5945, 307, 294, 2815, 568, 22117, 805, 51414], "temperature": 0.0, "avg_logprob": -0.08261711169511844, "compression_ratio": 1.555, "no_speech_prob": 0.004068854730576277}, {"id": 161, "seek": 120100, "start": 1222.0, "end": 1227.0, "text": " with the probability of 0.2 coming from the convolutional neural network.", "tokens": [51414, 365, 264, 8482, 295, 1958, 13, 17, 1348, 490, 264, 45216, 304, 18161, 3209, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08261711169511844, "compression_ratio": 1.555, "no_speech_prob": 0.004068854730576277}, {"id": 162, "seek": 122700, "start": 1227.0, "end": 1233.0, "text": " And so now you can imagine every cell has some probability of an enemy being there.", "tokens": [50364, 400, 370, 586, 291, 393, 3811, 633, 2815, 575, 512, 8482, 295, 364, 5945, 885, 456, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1763820966084798, "compression_ratio": 1.3359375, "no_speech_prob": 0.002079930854961276}, {"id": 163, "seek": 122700, "start": 1233.0, "end": 1243.0, "text": " And accordingly you can now get estimates of which cells are safe, okay?", "tokens": [50664, 400, 19717, 291, 393, 586, 483, 20561, 295, 597, 5438, 366, 3273, 11, 1392, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1763820966084798, "compression_ratio": 1.3359375, "no_speech_prob": 0.002079930854961276}, {"id": 164, "seek": 122700, "start": 1243.0, "end": 1246.0, "text": " Yes, go ahead.", "tokens": [51164, 1079, 11, 352, 2286, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1763820966084798, "compression_ratio": 1.3359375, "no_speech_prob": 0.002079930854961276}, {"id": 165, "seek": 124600, "start": 1247.0, "end": 1254.0, "text": " So the difference with prob log is that you use this fuzzy logic, we are propagating the probability.", "tokens": [50414, 407, 264, 2649, 365, 1239, 3565, 307, 300, 291, 764, 341, 34710, 9952, 11, 321, 366, 12425, 990, 264, 8482, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 166, "seek": 124600, "start": 1254.0, "end": 1256.0, "text": " The difference to prob log?", "tokens": [50764, 440, 2649, 281, 1239, 3565, 30, 50864], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 167, "seek": 124600, "start": 1256.0, "end": 1259.0, "text": " Yes, so the prob log has this weighted model accounting semantics, right?", "tokens": [50864, 1079, 11, 370, 264, 1239, 3565, 575, 341, 32807, 2316, 19163, 4361, 45298, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 168, "seek": 124600, "start": 1259.0, "end": 1263.0, "text": " So you use the fuzzy semantics to propagate the probability.", "tokens": [51014, 407, 291, 764, 264, 34710, 4361, 45298, 281, 48256, 264, 8482, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 169, "seek": 124600, "start": 1263.0, "end": 1267.0, "text": " So we, so I wouldn't know the answer to that.", "tokens": [51214, 407, 321, 11, 370, 286, 2759, 380, 458, 264, 1867, 281, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 170, "seek": 124600, "start": 1267.0, "end": 1272.0, "text": " We can probably take that offline, but we do have, so this was as I said inspired by deep prob log.", "tokens": [51414, 492, 393, 1391, 747, 300, 21857, 11, 457, 321, 360, 362, 11, 370, 341, 390, 382, 286, 848, 7547, 538, 2452, 1239, 3565, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 171, "seek": 124600, "start": 1272.0, "end": 1275.0, "text": " We do have weighted model counting.", "tokens": [51664, 492, 360, 362, 32807, 2316, 13251, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2847917639690897, "compression_ratio": 1.8661087866108788, "no_speech_prob": 0.040172819048166275}, {"id": 172, "seek": 127500, "start": 1275.0, "end": 1279.0, "text": " I just showed you, so max, I see, so you mean fuzzy as in this max min.", "tokens": [50364, 286, 445, 4712, 291, 11, 370, 11469, 11, 286, 536, 11, 370, 291, 914, 34710, 382, 294, 341, 11469, 923, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 173, "seek": 127500, "start": 1279.0, "end": 1280.0, "text": " Okay, okay.", "tokens": [50564, 1033, 11, 1392, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 174, "seek": 127500, "start": 1280.0, "end": 1283.0, "text": " So I just showed you a simple semiring.", "tokens": [50614, 407, 286, 445, 4712, 291, 257, 2199, 4361, 5057, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 175, "seek": 127500, "start": 1283.0, "end": 1285.0, "text": " In practice we don't use any of those.", "tokens": [50764, 682, 3124, 321, 500, 380, 764, 604, 295, 729, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 176, "seek": 127500, "start": 1285.0, "end": 1289.0, "text": " We just use them early on while we are developing applications,", "tokens": [50864, 492, 445, 764, 552, 2440, 322, 1339, 321, 366, 6416, 5821, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 177, "seek": 127500, "start": 1289.0, "end": 1296.0, "text": " but very quickly turns out these fuzzier semirings don't really help learn the model, okay?", "tokens": [51064, 457, 588, 2661, 4523, 484, 613, 283, 16740, 811, 4361, 347, 1109, 500, 380, 534, 854, 1466, 264, 2316, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 178, "seek": 127500, "start": 1296.0, "end": 1302.0, "text": " So the one that we really use, so as I said, there's the discrete execution.", "tokens": [51414, 407, 264, 472, 300, 321, 534, 764, 11, 370, 382, 286, 848, 11, 456, 311, 264, 27706, 15058, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1502877954851117, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.006790922489017248}, {"id": 179, "seek": 130200, "start": 1302.0, "end": 1306.0, "text": " There's the probabilistic one, and then finally there's the differentiable one,", "tokens": [50364, 821, 311, 264, 31959, 3142, 472, 11, 293, 550, 2721, 456, 311, 264, 819, 9364, 472, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12784574582026556, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.01202979777008295}, {"id": 180, "seek": 130200, "start": 1306.0, "end": 1309.0, "text": " which is what we use for learning, right?", "tokens": [50564, 597, 307, 437, 321, 764, 337, 2539, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.12784574582026556, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.01202979777008295}, {"id": 181, "seek": 130200, "start": 1309.0, "end": 1314.0, "text": " And the one that you are probably talking about is what we call top K proofs.", "tokens": [50714, 400, 264, 472, 300, 291, 366, 1391, 1417, 466, 307, 437, 321, 818, 1192, 591, 8177, 82, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12784574582026556, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.01202979777008295}, {"id": 182, "seek": 130200, "start": 1314.0, "end": 1321.0, "text": " So along with each tuple, we track, you know, what we call, you know, up to the top K proofs,", "tokens": [50964, 407, 2051, 365, 1184, 2604, 781, 11, 321, 2837, 11, 291, 458, 11, 437, 321, 818, 11, 291, 458, 11, 493, 281, 264, 1192, 591, 8177, 82, 11, 51314], "temperature": 0.0, "avg_logprob": -0.12784574582026556, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.01202979777008295}, {"id": 183, "seek": 130200, "start": 1321.0, "end": 1328.0, "text": " which I think Eric in the first talk referred to as I believe W of X, okay?", "tokens": [51314, 597, 286, 519, 9336, 294, 264, 700, 751, 10839, 281, 382, 286, 1697, 343, 295, 1783, 11, 1392, 30, 51664], "temperature": 0.0, "avg_logprob": -0.12784574582026556, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.01202979777008295}, {"id": 184, "seek": 132800, "start": 1328.0, "end": 1335.0, "text": " So we don't count how many times a tuple was used or anything like that.", "tokens": [50364, 407, 321, 500, 380, 1207, 577, 867, 1413, 257, 2604, 781, 390, 1143, 420, 1340, 411, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1435928150099151, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.006282261107116938}, {"id": 185, "seek": 132800, "start": 1335.0, "end": 1341.0, "text": " Yes, with respect to the negation and saturation operations, right?", "tokens": [50714, 1079, 11, 365, 3104, 281, 264, 2485, 399, 293, 27090, 7705, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1435928150099151, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.006282261107116938}, {"id": 186, "seek": 132800, "start": 1341.0, "end": 1348.0, "text": " Can you expand a little bit on what your requirements for them are, for this to work?", "tokens": [51014, 1664, 291, 5268, 257, 707, 857, 322, 437, 428, 7728, 337, 552, 366, 11, 337, 341, 281, 589, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1435928150099151, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.006282261107116938}, {"id": 187, "seek": 132800, "start": 1348.0, "end": 1351.0, "text": " Yeah, this is sort of too low level for me to explain.", "tokens": [51364, 865, 11, 341, 307, 1333, 295, 886, 2295, 1496, 337, 385, 281, 2903, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1435928150099151, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.006282261107116938}, {"id": 188, "seek": 132800, "start": 1351.0, "end": 1352.0, "text": " So I wouldn't know.", "tokens": [51514, 407, 286, 2759, 380, 458, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1435928150099151, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.006282261107116938}, {"id": 189, "seek": 132800, "start": 1352.0, "end": 1357.0, "text": " I'll be happy to get you in touch with the students.", "tokens": [51564, 286, 603, 312, 2055, 281, 483, 291, 294, 2557, 365, 264, 1731, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1435928150099151, "compression_ratio": 1.5391304347826087, "no_speech_prob": 0.006282261107116938}, {"id": 190, "seek": 135700, "start": 1357.0, "end": 1359.0, "text": " First of all, it will be stratified negation.", "tokens": [50364, 2386, 295, 439, 11, 309, 486, 312, 23674, 2587, 2485, 399, 13, 50464], "temperature": 0.0, "avg_logprob": -0.17668490366892772, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.001955274026840925}, {"id": 191, "seek": 135700, "start": 1359.0, "end": 1363.0, "text": " But I think you are asking me a deeper question than that.", "tokens": [50464, 583, 286, 519, 291, 366, 3365, 385, 257, 7731, 1168, 813, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17668490366892772, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.001955274026840925}, {"id": 192, "seek": 135700, "start": 1363.0, "end": 1369.0, "text": " What's the structure, what's the test, what's the negation has to prove that?", "tokens": [50664, 708, 311, 264, 3877, 11, 437, 311, 264, 1500, 11, 437, 311, 264, 2485, 399, 575, 281, 7081, 300, 30, 50964], "temperature": 0.0, "avg_logprob": -0.17668490366892772, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.001955274026840925}, {"id": 193, "seek": 135700, "start": 1369.0, "end": 1371.0, "text": " So if it comes to me, I'll let you know.", "tokens": [50964, 407, 498, 309, 1487, 281, 385, 11, 286, 603, 718, 291, 458, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17668490366892772, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.001955274026840925}, {"id": 194, "seek": 135700, "start": 1371.0, "end": 1378.0, "text": " I do know exactly what you're asking, and I'll try to see if I can remember, okay?", "tokens": [51064, 286, 360, 458, 2293, 437, 291, 434, 3365, 11, 293, 286, 603, 853, 281, 536, 498, 286, 393, 1604, 11, 1392, 30, 51414], "temperature": 0.0, "avg_logprob": -0.17668490366892772, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.001955274026840925}, {"id": 195, "seek": 135700, "start": 1378.0, "end": 1386.0, "text": " There are certain restrictions on all of these, on negation and saturation, okay?", "tokens": [51414, 821, 366, 1629, 14191, 322, 439, 295, 613, 11, 322, 2485, 399, 293, 27090, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.17668490366892772, "compression_ratio": 1.651063829787234, "no_speech_prob": 0.001955274026840925}, {"id": 196, "seek": 138600, "start": 1386.0, "end": 1390.0, "text": " But you prove them once and for all when you're defining the semi-ring, okay?", "tokens": [50364, 583, 291, 7081, 552, 1564, 293, 337, 439, 562, 291, 434, 17827, 264, 12909, 12, 2937, 11, 1392, 30, 50564], "temperature": 0.0, "avg_logprob": -0.103898800025552, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.006089960224926472}, {"id": 197, "seek": 138600, "start": 1390.0, "end": 1393.0, "text": " And so you can then use it.", "tokens": [50564, 400, 370, 291, 393, 550, 764, 309, 13, 50714], "temperature": 0.0, "avg_logprob": -0.103898800025552, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.006089960224926472}, {"id": 198, "seek": 138600, "start": 1393.0, "end": 1399.0, "text": " So I'm not going to go too much further into semi-rings other than to just say that the nice thing here,", "tokens": [50714, 407, 286, 478, 406, 516, 281, 352, 886, 709, 3052, 666, 12909, 12, 25782, 661, 813, 281, 445, 584, 300, 264, 1481, 551, 510, 11, 51014], "temperature": 0.0, "avg_logprob": -0.103898800025552, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.006089960224926472}, {"id": 199, "seek": 138600, "start": 1399.0, "end": 1404.0, "text": " at least to me, is that the programmer writes as if they are programming against a deterministic", "tokens": [51014, 412, 1935, 281, 385, 11, 307, 300, 264, 32116, 13657, 382, 498, 436, 366, 9410, 1970, 257, 15957, 3142, 51264], "temperature": 0.0, "avg_logprob": -0.103898800025552, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.006089960224926472}, {"id": 200, "seek": 138600, "start": 1404.0, "end": 1407.0, "text": " neural model that is producing these outputs, right?", "tokens": [51264, 18161, 2316, 300, 307, 10501, 613, 23930, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.103898800025552, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.006089960224926472}, {"id": 201, "seek": 138600, "start": 1407.0, "end": 1415.0, "text": " But under the hood, you have all of these probabilistic and differentiable reasoning happening through these tags, okay?", "tokens": [51414, 583, 833, 264, 13376, 11, 291, 362, 439, 295, 613, 31959, 3142, 293, 819, 9364, 21577, 2737, 807, 613, 18632, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.103898800025552, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.006089960224926472}, {"id": 202, "seek": 141500, "start": 1416.0, "end": 1425.0, "text": " We have applied this to a wide range of benchmarks and are now moving to even more sophisticated ones in robotics", "tokens": [50414, 492, 362, 6456, 341, 281, 257, 4874, 3613, 295, 43751, 293, 366, 586, 2684, 281, 754, 544, 16950, 2306, 294, 34145, 50864], "temperature": 0.0, "avg_logprob": -0.0940095413298834, "compression_ratio": 1.59375, "no_speech_prob": 0.002508825156837702}, {"id": 203, "seek": 141500, "start": 1425.0, "end": 1429.0, "text": " and healthcare, for explainable healthcare and so on.", "tokens": [50864, 293, 8884, 11, 337, 2903, 712, 8884, 293, 370, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0940095413298834, "compression_ratio": 1.59375, "no_speech_prob": 0.002508825156837702}, {"id": 204, "seek": 141500, "start": 1429.0, "end": 1436.0, "text": " But I'll just show you some core, you know, some challenges in the field of AI that we started out with.", "tokens": [51064, 583, 286, 603, 445, 855, 291, 512, 4965, 11, 291, 458, 11, 512, 4759, 294, 264, 2519, 295, 7318, 300, 321, 1409, 484, 365, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0940095413298834, "compression_ratio": 1.59375, "no_speech_prob": 0.002508825156837702}, {"id": 205, "seek": 141500, "start": 1436.0, "end": 1442.0, "text": " These include, you know, benchmarks in computer vision, which have images and video.", "tokens": [51414, 1981, 4090, 11, 291, 458, 11, 43751, 294, 3820, 5201, 11, 597, 362, 5267, 293, 960, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0940095413298834, "compression_ratio": 1.59375, "no_speech_prob": 0.002508825156837702}, {"id": 206, "seek": 144200, "start": 1442.0, "end": 1449.0, "text": " For example, here, this is this Mugen benchmark where the goal is given a short video and a piece of text", "tokens": [50364, 1171, 1365, 11, 510, 11, 341, 307, 341, 376, 27915, 18927, 689, 264, 3387, 307, 2212, 257, 2099, 960, 293, 257, 2522, 295, 2487, 50714], "temperature": 0.0, "avg_logprob": -0.09862876184207876, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002395839896053076}, {"id": 207, "seek": 144200, "start": 1449.0, "end": 1459.0, "text": " to give a score between zero and one that tells how likely is this text talking about the frames in this video, right?", "tokens": [50714, 281, 976, 257, 6175, 1296, 4018, 293, 472, 300, 5112, 577, 3700, 307, 341, 2487, 1417, 466, 264, 12083, 294, 341, 960, 11, 558, 30, 51214], "temperature": 0.0, "avg_logprob": -0.09862876184207876, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002395839896053076}, {"id": 208, "seek": 144200, "start": 1459.0, "end": 1467.0, "text": " So this, as you can imagine, has applications in video captioning, video search, video recommendation and so on, right?", "tokens": [51214, 407, 341, 11, 382, 291, 393, 3811, 11, 575, 5821, 294, 960, 31974, 278, 11, 960, 3164, 11, 960, 11879, 293, 370, 322, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09862876184207876, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002395839896053076}, {"id": 209, "seek": 144200, "start": 1467.0, "end": 1470.0, "text": " There is, we have benchmarks in NLP as well.", "tokens": [51614, 821, 307, 11, 321, 362, 43751, 294, 426, 45196, 382, 731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09862876184207876, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002395839896053076}, {"id": 210, "seek": 147000, "start": 1470.0, "end": 1478.0, "text": " Again, fairly standard ones and then we also have multimodal benchmarks.", "tokens": [50364, 3764, 11, 6457, 3832, 2306, 293, 550, 321, 611, 362, 32972, 378, 304, 43751, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1272606473220022, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0005700209876522422}, {"id": 211, "seek": 147000, "start": 1478.0, "end": 1485.0, "text": " And much of these benefits of relational, the relational model are useful here.", "tokens": [50764, 400, 709, 295, 613, 5311, 295, 38444, 11, 264, 38444, 2316, 366, 4420, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1272606473220022, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0005700209876522422}, {"id": 212, "seek": 147000, "start": 1485.0, "end": 1490.0, "text": " For example, we extract scene graphs from images which can be represented as relations.", "tokens": [51114, 1171, 1365, 11, 321, 8947, 4145, 24877, 490, 5267, 597, 393, 312, 10379, 382, 2299, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1272606473220022, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0005700209876522422}, {"id": 213, "seek": 147000, "start": 1490.0, "end": 1498.0, "text": " We extract abstract syntax trees from in semantic parsing, which are again represented as relations, right?", "tokens": [51364, 492, 8947, 12649, 28431, 5852, 490, 294, 47982, 21156, 278, 11, 597, 366, 797, 10379, 382, 2299, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1272606473220022, "compression_ratio": 1.6650717703349283, "no_speech_prob": 0.0005700209876522422}, {"id": 214, "seek": 149800, "start": 1498.0, "end": 1502.0, "text": " This is where the rubble meets the road. All of this theory is elegant.", "tokens": [50364, 639, 307, 689, 264, 5915, 638, 13961, 264, 3060, 13, 1057, 295, 341, 5261, 307, 21117, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09711538828336276, "compression_ratio": 1.64, "no_speech_prob": 0.001454340061172843}, {"id": 215, "seek": 149800, "start": 1502.0, "end": 1508.0, "text": " But if it doesn't work in practice, then it's not, then it doesn't help us, right?", "tokens": [50564, 583, 498, 309, 1177, 380, 589, 294, 3124, 11, 550, 309, 311, 406, 11, 550, 309, 1177, 380, 854, 505, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.09711538828336276, "compression_ratio": 1.64, "no_speech_prob": 0.001454340061172843}, {"id": 216, "seek": 149800, "start": 1508.0, "end": 1517.0, "text": " When we started this project, many of these baselines, both neural and neurosymbolic, were far behind us, right?", "tokens": [50864, 1133, 321, 1409, 341, 1716, 11, 867, 295, 613, 987, 9173, 11, 1293, 18161, 293, 12087, 329, 88, 5612, 299, 11, 645, 1400, 2261, 505, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.09711538828336276, "compression_ratio": 1.64, "no_speech_prob": 0.001454340061172843}, {"id": 217, "seek": 149800, "start": 1517.0, "end": 1524.0, "text": " But by the time we got all of this published, some of them had even crept back up ahead of us, right?", "tokens": [51314, 583, 538, 264, 565, 321, 658, 439, 295, 341, 6572, 11, 512, 295, 552, 632, 754, 1197, 662, 646, 493, 2286, 295, 505, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.09711538828336276, "compression_ratio": 1.64, "no_speech_prob": 0.001454340061172843}, {"id": 218, "seek": 152400, "start": 1524.0, "end": 1528.0, "text": " So this is sort of the challenge we face against the end-to-end deep learning paradigm, right?", "tokens": [50364, 407, 341, 307, 1333, 295, 264, 3430, 321, 1851, 1970, 264, 917, 12, 1353, 12, 521, 2452, 2539, 24709, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.15586756015646047, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.0020814903546124697}, {"id": 219, "seek": 152400, "start": 1528.0, "end": 1540.0, "text": " Which is, it will, you know, as newer neural architectures and so on are designed, they might even outperform, say, the neurosymbolic approaches, okay?", "tokens": [50564, 3013, 307, 11, 309, 486, 11, 291, 458, 11, 382, 17628, 18161, 6331, 1303, 293, 370, 322, 366, 4761, 11, 436, 1062, 754, 484, 26765, 11, 584, 11, 264, 12087, 329, 88, 5612, 299, 11587, 11, 1392, 30, 51164], "temperature": 0.0, "avg_logprob": -0.15586756015646047, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.0020814903546124697}, {"id": 220, "seek": 152400, "start": 1540.0, "end": 1544.0, "text": " So, any questions before I proceed? Yes.", "tokens": [51164, 407, 11, 604, 1651, 949, 286, 8991, 30, 1079, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15586756015646047, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.0020814903546124697}, {"id": 221, "seek": 152400, "start": 1544.0, "end": 1547.0, "text": " Maybe also, right, so accuracy is one thing, right?", "tokens": [51364, 2704, 611, 11, 558, 11, 370, 14170, 307, 472, 551, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.15586756015646047, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.0020814903546124697}, {"id": 222, "seek": 152400, "start": 1547.0, "end": 1552.0, "text": " But I could also see that since you're encoding some of the main knowledge into your program, right?", "tokens": [51514, 583, 286, 727, 611, 536, 300, 1670, 291, 434, 43430, 512, 295, 264, 2135, 3601, 666, 428, 1461, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.15586756015646047, "compression_ratio": 1.5658362989323844, "no_speech_prob": 0.0020814903546124697}, {"id": 223, "seek": 155200, "start": 1552.0, "end": 1559.0, "text": " I could see that, for example, you might need less data to learn the model and things like that, and maybe it's more performant.", "tokens": [50364, 286, 727, 536, 300, 11, 337, 1365, 11, 291, 1062, 643, 1570, 1412, 281, 1466, 264, 2316, 293, 721, 411, 300, 11, 293, 1310, 309, 311, 544, 2042, 394, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1121630807524746, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0022862821351736784}, {"id": 224, "seek": 155200, "start": 1559.0, "end": 1561.0, "text": " Yes, so great question.", "tokens": [50714, 1079, 11, 370, 869, 1168, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1121630807524746, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0022862821351736784}, {"id": 225, "seek": 155200, "start": 1561.0, "end": 1569.0, "text": " And we have all of these results in our PLDI paper, PLDI 2023, where we talk about better interpretability.", "tokens": [50814, 400, 321, 362, 439, 295, 613, 3542, 294, 527, 6999, 3085, 3035, 11, 6999, 3085, 44377, 11, 689, 321, 751, 466, 1101, 7302, 2310, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1121630807524746, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0022862821351736784}, {"id": 226, "seek": 155200, "start": 1569.0, "end": 1579.0, "text": " So if you remember, the intermediate representation R on which no supervision was given, we can actually look back what did it actually learn the right representation, right?", "tokens": [51214, 407, 498, 291, 1604, 11, 264, 19376, 10290, 497, 322, 597, 572, 32675, 390, 2212, 11, 321, 393, 767, 574, 646, 437, 630, 309, 767, 1466, 264, 558, 10290, 11, 558, 30, 51714], "temperature": 0.0, "avg_logprob": -0.1121630807524746, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.0022862821351736784}, {"id": 227, "seek": 157900, "start": 1579.0, "end": 1587.0, "text": " And the answer is yes, so it is more interpretable, it is more robust and better generalizable.", "tokens": [50364, 400, 264, 1867, 307, 2086, 11, 370, 309, 307, 544, 7302, 712, 11, 309, 307, 544, 13956, 293, 1101, 2674, 22395, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13299754781460543, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004752056673169136}, {"id": 228, "seek": 157900, "start": 1587.0, "end": 1593.0, "text": " So these better neural networks, are they kind of trying to do what you do with your structured constraints?", "tokens": [50764, 407, 613, 1101, 18161, 9590, 11, 366, 436, 733, 295, 1382, 281, 360, 437, 291, 360, 365, 428, 18519, 18491, 30, 51064], "temperature": 0.0, "avg_logprob": -0.13299754781460543, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004752056673169136}, {"id": 229, "seek": 157900, "start": 1593.0, "end": 1596.0, "text": " Are they trying to do that through network structure?", "tokens": [51064, 2014, 436, 1382, 281, 360, 300, 807, 3209, 3877, 30, 51214], "temperature": 0.0, "avg_logprob": -0.13299754781460543, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004752056673169136}, {"id": 230, "seek": 157900, "start": 1596.0, "end": 1600.0, "text": " Are they trying to simulate, basically, what you can do in a more elegant way?", "tokens": [51214, 2014, 436, 1382, 281, 27817, 11, 1936, 11, 437, 291, 393, 360, 294, 257, 544, 21117, 636, 30, 51414], "temperature": 0.0, "avg_logprob": -0.13299754781460543, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004752056673169136}, {"id": 231, "seek": 157900, "start": 1600.0, "end": 1604.0, "text": " So, first of all, these are end-to-end deep neural models, right, like transformers and so on.", "tokens": [51414, 407, 11, 700, 295, 439, 11, 613, 366, 917, 12, 1353, 12, 521, 2452, 18161, 5245, 11, 558, 11, 411, 4088, 433, 293, 370, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13299754781460543, "compression_ratio": 1.7704918032786885, "no_speech_prob": 0.004752056673169136}, {"id": 232, "seek": 160400, "start": 1604.0, "end": 1609.0, "text": " We wouldn't necessarily know what they are trying to do, but they are solving this problem.", "tokens": [50364, 492, 2759, 380, 4725, 458, 437, 436, 366, 1382, 281, 360, 11, 457, 436, 366, 12606, 341, 1154, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09556683699289957, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.05996743589639664}, {"id": 233, "seek": 160400, "start": 1609.0, "end": 1611.0, "text": " Let me show you one, right?", "tokens": [50614, 961, 385, 855, 291, 472, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.09556683699289957, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.05996743589639664}, {"id": 234, "seek": 160400, "start": 1611.0, "end": 1616.0, "text": " So this pathfinder was a benchmark by, I think, Google Research a few years ago called PathX.", "tokens": [50714, 407, 341, 3100, 38977, 390, 257, 18927, 538, 11, 286, 519, 11, 3329, 10303, 257, 1326, 924, 2057, 1219, 21914, 55, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09556683699289957, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.05996743589639664}, {"id": 235, "seek": 160400, "start": 1616.0, "end": 1623.0, "text": " You see, they're two tiny dots, and you have to tell whether there's a dotted path from one to the other, okay?", "tokens": [50964, 509, 536, 11, 436, 434, 732, 5870, 15026, 11, 293, 291, 362, 281, 980, 1968, 456, 311, 257, 37459, 3100, 490, 472, 281, 264, 661, 11, 1392, 30, 51314], "temperature": 0.0, "avg_logprob": -0.09556683699289957, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.05996743589639664}, {"id": 236, "seek": 160400, "start": 1623.0, "end": 1632.0, "text": " And even for a human, it can take up to two minutes to tell for some of these difficult images whether there's actually a dotted path or not.", "tokens": [51314, 400, 754, 337, 257, 1952, 11, 309, 393, 747, 493, 281, 732, 2077, 281, 980, 337, 512, 295, 613, 2252, 5267, 1968, 456, 311, 767, 257, 37459, 3100, 420, 406, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09556683699289957, "compression_ratio": 1.6385964912280702, "no_speech_prob": 0.05996743589639664}, {"id": 237, "seek": 163200, "start": 1632.0, "end": 1635.0, "text": " So it's a binary classification problem, right?", "tokens": [50364, 407, 309, 311, 257, 17434, 21538, 1154, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.1189416821083326, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0019563506357371807}, {"id": 238, "seek": 163200, "start": 1635.0, "end": 1645.0, "text": " So, you know, so the state of the art now here is actually a transformer which, I'm sorry, which beats what we have.", "tokens": [50514, 407, 11, 291, 458, 11, 370, 264, 1785, 295, 264, 1523, 586, 510, 307, 767, 257, 31782, 597, 11, 286, 478, 2597, 11, 597, 16447, 437, 321, 362, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1189416821083326, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0019563506357371807}, {"id": 239, "seek": 163200, "start": 1645.0, "end": 1651.0, "text": " So you see for PathX, there's this transformer model which is doing even better than ours.", "tokens": [51014, 407, 291, 536, 337, 21914, 55, 11, 456, 311, 341, 31782, 2316, 597, 307, 884, 754, 1101, 813, 11896, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1189416821083326, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0019563506357371807}, {"id": 240, "seek": 163200, "start": 1651.0, "end": 1656.0, "text": " In our case, we simply, you know, we have the rule for computing transitive closure.", "tokens": [51314, 682, 527, 1389, 11, 321, 2935, 11, 291, 458, 11, 321, 362, 264, 4978, 337, 15866, 1145, 2187, 24653, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1189416821083326, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0019563506357371807}, {"id": 241, "seek": 165600, "start": 1656.0, "end": 1665.0, "text": " So once you know which, where are the two dots and where are the edges, you can use, you know, simple these two rules to tell whether they are reachable or not.", "tokens": [50364, 407, 1564, 291, 458, 597, 11, 689, 366, 264, 732, 15026, 293, 689, 366, 264, 8819, 11, 291, 393, 764, 11, 291, 458, 11, 2199, 613, 732, 4474, 281, 980, 1968, 436, 366, 2524, 712, 420, 406, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12339932250976562, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.003939853981137276}, {"id": 242, "seek": 165600, "start": 1665.0, "end": 1671.0, "text": " But you don't know if their model is trying to do something like that in the deep learning model directly?", "tokens": [50814, 583, 291, 500, 380, 458, 498, 641, 2316, 307, 1382, 281, 360, 746, 411, 300, 294, 264, 2452, 2539, 2316, 3838, 30, 51114], "temperature": 0.0, "avg_logprob": -0.12339932250976562, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.003939853981137276}, {"id": 243, "seek": 165600, "start": 1671.0, "end": 1675.0, "text": " Right, so we haven't actually seen, you know, like for explanations within them.", "tokens": [51114, 1779, 11, 370, 321, 2378, 380, 767, 1612, 11, 291, 458, 11, 411, 337, 28708, 1951, 552, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12339932250976562, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.003939853981137276}, {"id": 244, "seek": 165600, "start": 1675.0, "end": 1677.0, "text": " So it would be nice to see that.", "tokens": [51314, 407, 309, 576, 312, 1481, 281, 536, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12339932250976562, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.003939853981137276}, {"id": 245, "seek": 165600, "start": 1677.0, "end": 1680.0, "text": " There are also some neuro-symbolic baselines here.", "tokens": [51414, 821, 366, 611, 512, 16499, 12, 3187, 5612, 299, 987, 9173, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12339932250976562, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.003939853981137276}, {"id": 246, "seek": 165600, "start": 1680.0, "end": 1682.0, "text": " I mean, work that Guy and others have done.", "tokens": [51564, 286, 914, 11, 589, 300, 14690, 293, 2357, 362, 1096, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12339932250976562, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.003939853981137276}, {"id": 247, "seek": 168200, "start": 1682.0, "end": 1688.0, "text": " By the way, a lot of his work has gone into this with sentential decedent diagrams and so on in our weighted model accounting.", "tokens": [50364, 3146, 264, 636, 11, 257, 688, 295, 702, 589, 575, 2780, 666, 341, 365, 2279, 2549, 979, 292, 317, 36709, 293, 370, 322, 294, 527, 32807, 2316, 19163, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14701404127963755, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.0028425147756934166}, {"id": 248, "seek": 168200, "start": 1688.0, "end": 1691.0, "text": " You know, just ignore, you know, not mentioning here.", "tokens": [50664, 509, 458, 11, 445, 11200, 11, 291, 458, 11, 406, 18315, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14701404127963755, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.0028425147756934166}, {"id": 249, "seek": 168200, "start": 1691.0, "end": 1700.0, "text": " But there are other neuro-symbolic works as well based on, you know, abductive reasoning and so on that we were able to outperform.", "tokens": [50814, 583, 456, 366, 661, 16499, 12, 3187, 5612, 299, 1985, 382, 731, 2361, 322, 11, 291, 458, 11, 46465, 488, 21577, 293, 370, 322, 300, 321, 645, 1075, 281, 484, 26765, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14701404127963755, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.0028425147756934166}, {"id": 250, "seek": 168200, "start": 1700.0, "end": 1701.0, "text": " Yeah.", "tokens": [51264, 865, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14701404127963755, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.0028425147756934166}, {"id": 251, "seek": 170100, "start": 1701.0, "end": 1705.0, "text": " So I have a question about the gradient semi-ring, which was mentioned several times today.", "tokens": [50364, 407, 286, 362, 257, 1168, 466, 264, 16235, 12909, 12, 2937, 11, 597, 390, 2835, 2940, 1413, 965, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1692748885970932, "compression_ratio": 1.8133802816901408, "no_speech_prob": 0.15792910754680634}, {"id": 252, "seek": 170100, "start": 1705.0, "end": 1713.0, "text": " So I don't understand how it's useful in the context here because the gradient semi-ring really computes the forward derivatives,", "tokens": [50564, 407, 286, 500, 380, 1223, 577, 309, 311, 4420, 294, 264, 4319, 510, 570, 264, 16235, 12909, 12, 2937, 534, 715, 1819, 264, 2128, 33733, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1692748885970932, "compression_ratio": 1.8133802816901408, "no_speech_prob": 0.15792910754680634}, {"id": 253, "seek": 170100, "start": 1713.0, "end": 1718.0, "text": " which means that if you have a neural network for object detection with a million parameters,", "tokens": [50964, 597, 1355, 300, 498, 291, 362, 257, 18161, 3209, 337, 2657, 17784, 365, 257, 2459, 9834, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1692748885970932, "compression_ratio": 1.8133802816901408, "no_speech_prob": 0.15792910754680634}, {"id": 254, "seek": 170100, "start": 1718.0, "end": 1723.0, "text": " you have to push forward a million dimensional vector through your whole computation path.", "tokens": [51214, 291, 362, 281, 2944, 2128, 257, 2459, 18795, 8062, 807, 428, 1379, 24903, 3100, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1692748885970932, "compression_ratio": 1.8133802816901408, "no_speech_prob": 0.15792910754680634}, {"id": 255, "seek": 170100, "start": 1723.0, "end": 1729.0, "text": " And what you really need for machine learning is the backward derivatives, which are, you know, linear time.", "tokens": [51464, 400, 437, 291, 534, 643, 337, 3479, 2539, 307, 264, 23897, 33733, 11, 597, 366, 11, 291, 458, 11, 8213, 565, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1692748885970932, "compression_ratio": 1.8133802816901408, "no_speech_prob": 0.15792910754680634}, {"id": 256, "seek": 172900, "start": 1730.0, "end": 1734.0, "text": " So even though mathematically the gradient semi-ring is a beautiful thing,", "tokens": [50414, 407, 754, 1673, 44003, 264, 16235, 12909, 12, 2937, 307, 257, 2238, 551, 11, 50614], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 257, "seek": 172900, "start": 1734.0, "end": 1736.0, "text": " it's actually the wrong tool for machine learning.", "tokens": [50614, 309, 311, 767, 264, 2085, 2290, 337, 3479, 2539, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 258, "seek": 172900, "start": 1736.0, "end": 1739.0, "text": " You want the backward derivative not to forward.", "tokens": [50714, 509, 528, 264, 23897, 13760, 406, 281, 2128, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 259, "seek": 172900, "start": 1743.0, "end": 1746.0, "text": " Yeah, so I think we'll have to talk about this more offline.", "tokens": [51064, 865, 11, 370, 286, 519, 321, 603, 362, 281, 751, 466, 341, 544, 21857, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 260, "seek": 172900, "start": 1746.0, "end": 1747.0, "text": " Sorry about that.", "tokens": [51214, 4919, 466, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 261, "seek": 172900, "start": 1747.0, "end": 1751.0, "text": " I wasn't paying too close attention to the gradient semi-ring.", "tokens": [51264, 286, 2067, 380, 6229, 886, 1998, 3202, 281, 264, 16235, 12909, 12, 2937, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 262, "seek": 172900, "start": 1751.0, "end": 1753.0, "text": " Let's talk more.", "tokens": [51464, 961, 311, 751, 544, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15778801358979325, "compression_ratio": 1.616504854368932, "no_speech_prob": 0.0008826907724142075}, {"id": 263, "seek": 175300, "start": 1754.0, "end": 1757.0, "text": " Could you go back once?", "tokens": [50414, 7497, 291, 352, 646, 1564, 30, 50564], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 264, "seek": 175300, "start": 1757.0, "end": 1758.0, "text": " Yeah.", "tokens": [50564, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 265, "seek": 175300, "start": 1758.0, "end": 1763.0, "text": " So for the first two examples, we have the MNIST bit.", "tokens": [50614, 407, 337, 264, 700, 732, 5110, 11, 321, 362, 264, 376, 45, 19756, 857, 13, 50864], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 266, "seek": 175300, "start": 1763.0, "end": 1764.0, "text": " Yeah.", "tokens": [50864, 865, 13, 50914], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 267, "seek": 175300, "start": 1764.0, "end": 1774.0, "text": " And then this is, like after you recognize, so another approach is that you recognize bit and then you just write Python to something to get.", "tokens": [50914, 400, 550, 341, 307, 11, 411, 934, 291, 5521, 11, 370, 1071, 3109, 307, 300, 291, 5521, 857, 293, 550, 291, 445, 2464, 15329, 281, 746, 281, 483, 13, 51414], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 268, "seek": 175300, "start": 1774.0, "end": 1775.0, "text": " Right.", "tokens": [51414, 1779, 13, 51464], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 269, "seek": 175300, "start": 1775.0, "end": 1778.0, "text": " So why is this any better?", "tokens": [51464, 407, 983, 307, 341, 604, 1101, 30, 51614], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 270, "seek": 175300, "start": 1778.0, "end": 1780.0, "text": " Why are we doing anything better?", "tokens": [51614, 1545, 366, 321, 884, 1340, 1101, 30, 51714], "temperature": 0.0, "avg_logprob": -0.34015618671070447, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.016347860917448997}, {"id": 271, "seek": 178000, "start": 1780.0, "end": 1783.0, "text": " Your supervision is not given on the individual MNIST digits.", "tokens": [50364, 2260, 32675, 307, 406, 2212, 322, 264, 2609, 376, 45, 19756, 27011, 13, 50514], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 272, "seek": 178000, "start": 1783.0, "end": 1784.0, "text": " Okay.", "tokens": [50514, 1033, 13, 50564], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 273, "seek": 178000, "start": 1784.0, "end": 1786.0, "text": " It's only given on the final result.", "tokens": [50564, 467, 311, 787, 2212, 322, 264, 2572, 1874, 13, 50664], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 274, "seek": 178000, "start": 1786.0, "end": 1787.0, "text": " Yeah.", "tokens": [50664, 865, 13, 50714], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 275, "seek": 178000, "start": 1787.0, "end": 1791.0, "text": " So but this example is, it feels a little bit confined.", "tokens": [50714, 407, 457, 341, 1365, 307, 11, 309, 3417, 257, 707, 857, 31745, 13, 50914], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 276, "seek": 178000, "start": 1791.0, "end": 1792.0, "text": " Right.", "tokens": [50914, 1779, 13, 50964], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 277, "seek": 178000, "start": 1792.0, "end": 1794.0, "text": " So I could have done this by doing the two basic approach.", "tokens": [50964, 407, 286, 727, 362, 1096, 341, 538, 884, 264, 732, 3875, 3109, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 278, "seek": 178000, "start": 1794.0, "end": 1795.0, "text": " Yeah.", "tokens": [51064, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 279, "seek": 178000, "start": 1795.0, "end": 1796.0, "text": " Yeah.", "tokens": [51114, 865, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 280, "seek": 178000, "start": 1796.0, "end": 1801.0, "text": " For example, here we are this kind of more streamlined approach has a clear benefit.", "tokens": [51164, 1171, 1365, 11, 510, 321, 366, 341, 733, 295, 544, 48155, 3109, 575, 257, 1850, 5121, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 281, "seek": 178000, "start": 1801.0, "end": 1806.0, "text": " So if you had supervision on the intermediate results, you wouldn't need scallop at all.", "tokens": [51414, 407, 498, 291, 632, 32675, 322, 264, 19376, 3542, 11, 291, 2759, 380, 643, 30509, 404, 412, 439, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 282, "seek": 178000, "start": 1806.0, "end": 1807.0, "text": " Okay.", "tokens": [51664, 1033, 13, 51714], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 283, "seek": 178000, "start": 1807.0, "end": 1808.0, "text": " Right.", "tokens": [51714, 1779, 13, 51764], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 284, "seek": 178000, "start": 1808.0, "end": 1809.0, "text": " Right.", "tokens": [51764, 1779, 13, 51814], "temperature": 0.0, "avg_logprob": -0.24237235142634464, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0028864985797554255}, {"id": 285, "seek": 180900, "start": 1809.0, "end": 1816.0, "text": " So in, in none of these benchmarks, do we have intermediate supervision, even though many of them are synthetic and you actually know the intermediate labels.", "tokens": [50364, 407, 294, 11, 294, 6022, 295, 613, 43751, 11, 360, 321, 362, 19376, 32675, 11, 754, 1673, 867, 295, 552, 366, 23420, 293, 291, 767, 458, 264, 19376, 16949, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11166157342691337, "compression_ratio": 1.873134328358209, "no_speech_prob": 0.0025089294649660587}, {"id": 286, "seek": 180900, "start": 1816.0, "end": 1823.0, "text": " So that is how we actually, you know, measure whether you know, you know, the degree of interpretability, how much has it actually recovered the information.", "tokens": [50714, 407, 300, 307, 577, 321, 767, 11, 291, 458, 11, 3481, 1968, 291, 458, 11, 291, 458, 11, 264, 4314, 295, 7302, 2310, 11, 577, 709, 575, 309, 767, 19542, 264, 1589, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11166157342691337, "compression_ratio": 1.873134328358209, "no_speech_prob": 0.0025089294649660587}, {"id": 287, "seek": 180900, "start": 1823.0, "end": 1829.0, "text": " So I'm not showing you, you know, we have heat maps for all of this to show you actually what intermediate representation was learned.", "tokens": [51064, 407, 286, 478, 406, 4099, 291, 11, 291, 458, 11, 321, 362, 3738, 11317, 337, 439, 295, 341, 281, 855, 291, 767, 437, 19376, 10290, 390, 3264, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11166157342691337, "compression_ratio": 1.873134328358209, "no_speech_prob": 0.0025089294649660587}, {"id": 288, "seek": 180900, "start": 1829.0, "end": 1836.0, "text": " And it is, it matches the synthetic data's labels.", "tokens": [51364, 400, 309, 307, 11, 309, 10676, 264, 23420, 1412, 311, 16949, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11166157342691337, "compression_ratio": 1.873134328358209, "no_speech_prob": 0.0025089294649660587}, {"id": 289, "seek": 183600, "start": 1836.0, "end": 1837.0, "text": " Yes.", "tokens": [50364, 1079, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 290, "seek": 183600, "start": 1837.0, "end": 1838.0, "text": " Okay.", "tokens": [50414, 1033, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 291, "seek": 183600, "start": 1838.0, "end": 1841.0, "text": " So, you know, that is, you know, I'm just going to show you some fun things here.", "tokens": [50464, 407, 11, 291, 458, 11, 300, 307, 11, 291, 458, 11, 286, 478, 445, 516, 281, 855, 291, 512, 1019, 721, 510, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 292, "seek": 183600, "start": 1841.0, "end": 1851.0, "text": " There's not much more I can say here with, you know, so now what happened was in these two years that we did this work, LLMs and more generally foundation models came on the scene.", "tokens": [50614, 821, 311, 406, 709, 544, 286, 393, 584, 510, 365, 11, 291, 458, 11, 370, 586, 437, 2011, 390, 294, 613, 732, 924, 300, 321, 630, 341, 589, 11, 441, 43, 26386, 293, 544, 5101, 7030, 5245, 1361, 322, 264, 4145, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 293, "seek": 183600, "start": 1851.0, "end": 1853.0, "text": " And we wondered, you know, can we catch up?", "tokens": [51114, 400, 321, 17055, 11, 291, 458, 11, 393, 321, 3745, 493, 30, 51214], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 294, "seek": 183600, "start": 1853.0, "end": 1854.0, "text": " Right.", "tokens": [51214, 1779, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 295, "seek": 183600, "start": 1854.0, "end": 1856.0, "text": " Can we somehow integrate this into scallop?", "tokens": [51264, 1664, 321, 6063, 13365, 341, 666, 30509, 404, 30, 51364], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 296, "seek": 183600, "start": 1856.0, "end": 1858.0, "text": " And the answer surprisingly is yes.", "tokens": [51364, 400, 264, 1867, 17600, 307, 2086, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 297, "seek": 183600, "start": 1858.0, "end": 1859.0, "text": " Okay.", "tokens": [51464, 1033, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 298, "seek": 183600, "start": 1859.0, "end": 1860.0, "text": " And this is still open.", "tokens": [51514, 400, 341, 307, 920, 1269, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 299, "seek": 183600, "start": 1860.0, "end": 1864.0, "text": " I think Joe also brought this up, you know, if I understood correctly what you're saying.", "tokens": [51564, 286, 519, 6807, 611, 3038, 341, 493, 11, 291, 458, 11, 498, 286, 7320, 8944, 437, 291, 434, 1566, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09514862338438729, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.007687600329518318}, {"id": 300, "seek": 186400, "start": 1864.0, "end": 1872.0, "text": " So what is the, the programming, you know, abstraction for say, you know, these generative models, and surprisingly, the relational model still works.", "tokens": [50364, 407, 437, 307, 264, 11, 264, 9410, 11, 291, 458, 11, 37765, 337, 584, 11, 291, 458, 11, 613, 1337, 1166, 5245, 11, 293, 17600, 11, 264, 38444, 2316, 920, 1985, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 301, "seek": 186400, "start": 1872.0, "end": 1878.0, "text": " If you think of any foundation model, right, it is a binary relation which takes a prompt and gives a response.", "tokens": [50764, 759, 291, 519, 295, 604, 7030, 2316, 11, 558, 11, 309, 307, 257, 17434, 9721, 597, 2516, 257, 12391, 293, 2709, 257, 4134, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 302, "seek": 186400, "start": 1878.0, "end": 1879.0, "text": " Right.", "tokens": [51064, 1779, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 303, "seek": 186400, "start": 1879.0, "end": 1883.0, "text": " And these are data types where the strings or tensors and so on are all supported in scallop.", "tokens": [51114, 400, 613, 366, 1412, 3467, 689, 264, 13985, 420, 10688, 830, 293, 370, 322, 366, 439, 8104, 294, 30509, 404, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 304, "seek": 186400, "start": 1883.0, "end": 1884.0, "text": " Okay.", "tokens": [51314, 1033, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 305, "seek": 186400, "start": 1884.0, "end": 1891.0, "text": " And it's actually a relation, not a function because based on the temperature and so on that you use the same prompt could have different responses.", "tokens": [51364, 400, 309, 311, 767, 257, 9721, 11, 406, 257, 2445, 570, 2361, 322, 264, 4292, 293, 370, 322, 300, 291, 764, 264, 912, 12391, 727, 362, 819, 13019, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 306, "seek": 186400, "start": 1891.0, "end": 1892.0, "text": " Right.", "tokens": [51714, 1779, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09246771782636642, "compression_ratio": 1.7558528428093645, "no_speech_prob": 0.008313666097819805}, {"id": 307, "seek": 189200, "start": 1893.0, "end": 1895.0, "text": " You know, very well into scallop.", "tokens": [50414, 509, 458, 11, 588, 731, 666, 30509, 404, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09329362248265466, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.001596996677108109}, {"id": 308, "seek": 189200, "start": 1895.0, "end": 1898.0, "text": " And we built this library of plugins.", "tokens": [50514, 400, 321, 3094, 341, 6405, 295, 33759, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09329362248265466, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.001596996677108109}, {"id": 309, "seek": 189200, "start": 1898.0, "end": 1909.0, "text": " We now have 12 foundation models integrated into scallop and you can add new ones very easily using our foreign function and predicate interface.", "tokens": [50664, 492, 586, 362, 2272, 7030, 5245, 10919, 666, 30509, 404, 293, 291, 393, 909, 777, 2306, 588, 3612, 1228, 527, 5329, 2445, 293, 3852, 8700, 9226, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09329362248265466, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.001596996677108109}, {"id": 310, "seek": 189200, "start": 1909.0, "end": 1921.0, "text": " I'm not going to go too much into these, but I can, you can sort of see how we are, we have these decorators for relations.", "tokens": [51214, 286, 478, 406, 516, 281, 352, 886, 709, 666, 613, 11, 457, 286, 393, 11, 291, 393, 1333, 295, 536, 577, 321, 366, 11, 321, 362, 613, 7919, 3391, 337, 2299, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09329362248265466, "compression_ratio": 1.5429864253393666, "no_speech_prob": 0.001596996677108109}, {"id": 311, "seek": 192100, "start": 1921.0, "end": 1934.0, "text": " And you can use a few short examples or you can use chain of thought, you can use auto GPT, you can even fine tune, you know, layers of these models in scallop using again just end to end supervision.", "tokens": [50364, 400, 291, 393, 764, 257, 1326, 2099, 5110, 420, 291, 393, 764, 5021, 295, 1194, 11, 291, 393, 764, 8399, 26039, 51, 11, 291, 393, 754, 2489, 10864, 11, 291, 458, 11, 7914, 295, 613, 5245, 294, 30509, 404, 1228, 797, 445, 917, 281, 917, 32675, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0965246602108604, "compression_ratio": 1.7118644067796611, "no_speech_prob": 0.001836762996390462}, {"id": 312, "seek": 192100, "start": 1934.0, "end": 1948.0, "text": " In this case, you know, we break down this task into sort of this in context learning which extracts tuples, you know, which are the basic relationships between pairs of people mentioned in this passage.", "tokens": [51014, 682, 341, 1389, 11, 291, 458, 11, 321, 1821, 760, 341, 5633, 666, 1333, 295, 341, 294, 4319, 2539, 597, 8947, 82, 2604, 2622, 11, 291, 458, 11, 597, 366, 264, 3875, 6159, 1296, 15494, 295, 561, 2835, 294, 341, 11497, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0965246602108604, "compression_ratio": 1.7118644067796611, "no_speech_prob": 0.001836762996390462}, {"id": 313, "seek": 194800, "start": 1948.0, "end": 1958.0, "text": " And then we write a few rules in this case just three, which can compute the answer to a question which is how is a particular pair of people in this passage related.", "tokens": [50364, 400, 550, 321, 2464, 257, 1326, 4474, 294, 341, 1389, 445, 1045, 11, 597, 393, 14722, 264, 1867, 281, 257, 1168, 597, 307, 577, 307, 257, 1729, 6119, 295, 561, 294, 341, 11497, 4077, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0969916816069701, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.001048073056153953}, {"id": 314, "seek": 194800, "start": 1958.0, "end": 1959.0, "text": " Right.", "tokens": [50864, 1779, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0969916816069701, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.001048073056153953}, {"id": 315, "seek": 194800, "start": 1959.0, "end": 1962.0, "text": " So this is sort of showing you multi hop reasoning.", "tokens": [50914, 407, 341, 307, 1333, 295, 4099, 291, 4825, 3818, 21577, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0969916816069701, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.001048073056153953}, {"id": 316, "seek": 194800, "start": 1962.0, "end": 1971.0, "text": " By the way, we even have rule learning here so the parameters don't just have to be in the neural model but for example this relation composition is itself noisy.", "tokens": [51064, 3146, 264, 636, 11, 321, 754, 362, 4978, 2539, 510, 370, 264, 9834, 500, 380, 445, 362, 281, 312, 294, 264, 18161, 2316, 457, 337, 1365, 341, 9721, 12686, 307, 2564, 24518, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0969916816069701, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.001048073056153953}, {"id": 317, "seek": 194800, "start": 1971.0, "end": 1976.0, "text": " And you could learn the weights of individual tuples of this relation.", "tokens": [51514, 400, 291, 727, 1466, 264, 17443, 295, 2609, 2604, 2622, 295, 341, 9721, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0969916816069701, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.001048073056153953}, {"id": 318, "seek": 197600, "start": 1976.0, "end": 1988.0, "text": " You can extend it to vision models as well. So here's a simple one, which is actually a multi model model clip from open AI, which also provides probabilities.", "tokens": [50364, 509, 393, 10101, 309, 281, 5201, 5245, 382, 731, 13, 407, 510, 311, 257, 2199, 472, 11, 597, 307, 767, 257, 4825, 2316, 2316, 7353, 490, 1269, 7318, 11, 597, 611, 6417, 33783, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12925817489624022, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002510270569473505}, {"id": 319, "seek": 197600, "start": 1988.0, "end": 1991.0, "text": " So in this case, the input is an image.", "tokens": [50964, 407, 294, 341, 1389, 11, 264, 4846, 307, 364, 3256, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12925817489624022, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002510270569473505}, {"id": 320, "seek": 197600, "start": 1991.0, "end": 1994.0, "text": " It's a bound argument and the output is the label.", "tokens": [51114, 467, 311, 257, 5472, 6770, 293, 264, 5598, 307, 264, 7645, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12925817489624022, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002510270569473505}, {"id": 321, "seek": 197600, "start": 1994.0, "end": 2003.0, "text": " So in this case, if you give a set of labels such as cat and dog, it will tell you the probability of this image being cat or a dog.", "tokens": [51264, 407, 294, 341, 1389, 11, 498, 291, 976, 257, 992, 295, 16949, 1270, 382, 3857, 293, 3000, 11, 309, 486, 980, 291, 264, 8482, 295, 341, 3256, 885, 3857, 420, 257, 3000, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12925817489624022, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.002510270569473505}, {"id": 322, "seek": 200300, "start": 2004.0, "end": 2007.0, "text": " We have also integrated meta segment anything models.", "tokens": [50414, 492, 362, 611, 10919, 19616, 9469, 1340, 5245, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1435336430867513, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0014545079320669174}, {"id": 323, "seek": 200300, "start": 2007.0, "end": 2018.0, "text": " So this in this case, you are given an image as an input, and it produces a set of tuples with an ID of each segment and the tensor representation of the segment.", "tokens": [50564, 407, 341, 294, 341, 1389, 11, 291, 366, 2212, 364, 3256, 382, 364, 4846, 11, 293, 309, 14725, 257, 992, 295, 2604, 2622, 365, 364, 7348, 295, 1184, 9469, 293, 264, 40863, 10290, 295, 264, 9469, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1435336430867513, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0014545079320669174}, {"id": 324, "seek": 200300, "start": 2018.0, "end": 2019.0, "text": " Right.", "tokens": [51114, 1779, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1435336430867513, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0014545079320669174}, {"id": 325, "seek": 200300, "start": 2019.0, "end": 2024.0, "text": " You can put these all together and build very interesting multi model applications in scallops.", "tokens": [51164, 509, 393, 829, 613, 439, 1214, 293, 1322, 588, 1880, 4825, 2316, 5821, 294, 30509, 3370, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1435336430867513, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0014545079320669174}, {"id": 326, "seek": 202400, "start": 2024.0, "end": 2039.0, "text": " So in this case here, what you see is three different models put together to solve the problem from this clever benchmark, which asks in this case, some some question that involves elementary reasoning about a scene.", "tokens": [50364, 407, 294, 341, 1389, 510, 11, 437, 291, 536, 307, 1045, 819, 5245, 829, 1214, 281, 5039, 264, 1154, 490, 341, 13494, 18927, 11, 597, 8962, 294, 341, 1389, 11, 512, 512, 1168, 300, 11626, 16429, 21577, 466, 257, 4145, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12383185041711685, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0038822873029857874}, {"id": 327, "seek": 202400, "start": 2039.0, "end": 2040.0, "text": " Right.", "tokens": [51114, 1779, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12383185041711685, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0038822873029857874}, {"id": 328, "seek": 202400, "start": 2040.0, "end": 2042.0, "text": " How many green objects are there in the image.", "tokens": [51164, 1012, 867, 3092, 6565, 366, 456, 294, 264, 3256, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12383185041711685, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0038822873029857874}, {"id": 329, "seek": 202400, "start": 2042.0, "end": 2045.0, "text": " I'm not showing you all of the rules that we wrote in scallop.", "tokens": [51264, 286, 478, 406, 4099, 291, 439, 295, 264, 4474, 300, 321, 4114, 294, 30509, 404, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12383185041711685, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0038822873029857874}, {"id": 330, "seek": 202400, "start": 2045.0, "end": 2049.0, "text": " There are about 100 rules that we wrote for this particular task.", "tokens": [51414, 821, 366, 466, 2319, 4474, 300, 321, 4114, 337, 341, 1729, 5633, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12383185041711685, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0038822873029857874}, {"id": 331, "seek": 204900, "start": 2049.0, "end": 2053.0, "text": " But we use these three different models to extract basic information.", "tokens": [50364, 583, 321, 764, 613, 1045, 819, 5245, 281, 8947, 3875, 1589, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15546124981295678, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.0012059381697326899}, {"id": 332, "seek": 204900, "start": 2053.0, "end": 2063.0, "text": " In this case, doing the semantic parsing of this question, extracting segments from this image and finally labeling each segment with a piece of text.", "tokens": [50564, 682, 341, 1389, 11, 884, 264, 47982, 21156, 278, 295, 341, 1168, 11, 49844, 19904, 490, 341, 3256, 293, 2721, 40244, 1184, 9469, 365, 257, 2522, 295, 2487, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15546124981295678, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.0012059381697326899}, {"id": 333, "seek": 204900, "start": 2063.0, "end": 2068.0, "text": " Finally, we can get the answer that there are three main objects in this image.", "tokens": [51064, 6288, 11, 321, 393, 483, 264, 1867, 300, 456, 366, 1045, 2135, 6565, 294, 341, 3256, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15546124981295678, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.0012059381697326899}, {"id": 334, "seek": 204900, "start": 2068.0, "end": 2069.0, "text": " Okay.", "tokens": [51314, 1033, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15546124981295678, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.0012059381697326899}, {"id": 335, "seek": 204900, "start": 2069.0, "end": 2071.0, "text": " So I'm not going to show you the imperial results.", "tokens": [51364, 407, 286, 478, 406, 516, 281, 855, 291, 264, 21143, 3542, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15546124981295678, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.0012059381697326899}, {"id": 336, "seek": 204900, "start": 2071.0, "end": 2075.0, "text": " This work is still under review.", "tokens": [51464, 639, 589, 307, 920, 833, 3131, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15546124981295678, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.0012059381697326899}, {"id": 337, "seek": 207500, "start": 2075.0, "end": 2081.0, "text": " We have applied it to a wide range of benchmarks, including those involving vector databases.", "tokens": [50364, 492, 362, 6456, 309, 281, 257, 4874, 3613, 295, 43751, 11, 3009, 729, 17030, 8062, 22380, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12418283265212486, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.008422364480793476}, {"id": 338, "seek": 207500, "start": 2081.0, "end": 2088.0, "text": " So, you know, you're having retrieval and generation, but also image generation and so on.", "tokens": [50664, 407, 11, 291, 458, 11, 291, 434, 1419, 19817, 3337, 293, 5125, 11, 457, 611, 3256, 5125, 293, 370, 322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12418283265212486, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.008422364480793476}, {"id": 339, "seek": 207500, "start": 2088.0, "end": 2089.0, "text": " Right.", "tokens": [51014, 1779, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12418283265212486, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.008422364480793476}, {"id": 340, "seek": 207500, "start": 2089.0, "end": 2095.0, "text": " And you can actually run many of these applications at this URL.", "tokens": [51064, 400, 291, 393, 767, 1190, 867, 295, 613, 5821, 412, 341, 12905, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12418283265212486, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.008422364480793476}, {"id": 341, "seek": 207500, "start": 2095.0, "end": 2099.0, "text": " And there's a lot more resources at this particular URL.", "tokens": [51364, 400, 456, 311, 257, 688, 544, 3593, 412, 341, 1729, 12905, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12418283265212486, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.008422364480793476}, {"id": 342, "seek": 207500, "start": 2099.0, "end": 2101.0, "text": " Thank you very much for your attention.", "tokens": [51564, 1044, 291, 588, 709, 337, 428, 3202, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12418283265212486, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.008422364480793476}, {"id": 343, "seek": 210100, "start": 2101.0, "end": 2106.0, "text": " Any questions?", "tokens": [50364, 2639, 1651, 30, 50614], "temperature": 0.8, "avg_logprob": -0.8129231589181083, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.35791826248168945}, {"id": 344, "seek": 210600, "start": 2106.0, "end": 2107.04, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50416], "temperature": 1.0, "avg_logprob": -2.2021060943603517, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.19674092531204224}, {"id": 345, "seek": 210600, "start": 2107.04, "end": 2109.32, "text": " And once", "tokens": [50416, 400, 1564, 50530], "temperature": 1.0, "avg_logprob": -2.2021060943603517, "compression_ratio": 0.7037037037037037, "no_speech_prob": 0.19674092531204224}], "language": "en"}