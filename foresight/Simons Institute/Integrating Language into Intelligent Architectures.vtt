WEBVTT

00:00.000 --> 00:01.920
I mentioned Gabe Grand, another student collaborator

00:01.920 --> 00:04.560
who is one of Jacob and Dreyas's students,

00:04.560 --> 00:09.120
and Tanja Shren, a student also co-advised

00:09.120 --> 00:13.040
by Josh and Vikash, as well as Noah Goodman,

00:13.040 --> 00:15.280
my advisor Vikash and Jacob and Dreyas.

00:17.640 --> 00:19.480
So our broader goal today in this talk

00:19.480 --> 00:21.480
is actually going to be to reflect

00:21.480 --> 00:23.080
based on many of the recent advances

00:23.080 --> 00:25.360
that we all know of modeling natural language,

00:25.360 --> 00:26.920
but I think also drawing on evidence

00:26.920 --> 00:29.720
from toolkits, other toolkits in AI.

00:30.720 --> 00:33.040
Evidence from cognitive science and from neuroscience

00:33.040 --> 00:35.680
on kind of the broader spectrum of different ways

00:35.680 --> 00:38.360
that we might think about building intelligent architectures

00:38.360 --> 00:41.320
that use and produce and learn from language,

00:41.320 --> 00:43.960
as well as more generally, I think what role language

00:43.960 --> 00:46.880
might play or could play in a computational system

00:46.880 --> 00:48.320
that we say thinks.

00:48.320 --> 00:50.360
And obviously this is a question that many people

00:50.360 --> 00:52.160
in this room, but many other people who probably

00:52.160 --> 00:53.680
aren't in this room have thought about

00:53.680 --> 00:57.280
from philosophers of language to linguists and neuroscientists.

00:57.280 --> 00:59.640
And we thought it actually might be a useful exercise

00:59.640 --> 01:02.760
to kind of start by reflecting on the underlying answers

01:02.760 --> 01:05.320
to this question that are or aren't suggested

01:05.320 --> 01:06.560
by some of the most prominent directions

01:06.560 --> 01:09.520
that we're taking in AI research right now.

01:09.520 --> 01:10.800
And of course, one of the reasons

01:10.800 --> 01:12.920
why we're even asking this question at this scale,

01:12.920 --> 01:15.000
what is the role of language in intelligence

01:15.000 --> 01:17.960
is in large part driven by this remarkable observation

01:17.960 --> 01:20.480
that we all know about from just a few years ago,

01:20.480 --> 01:23.080
which is if you train these large

01:23.080 --> 01:26.240
and specifically transformer based neural architectures

01:26.240 --> 01:27.960
as language models, just to do this

01:27.960 --> 01:32.200
next word prediction task, with enough language data,

01:32.200 --> 01:34.480
they start to show behaviors that really suggest

01:34.480 --> 01:36.280
that there's something more than language at play.

01:36.280 --> 01:37.880
They look like they're thinking.

01:38.800 --> 01:41.880
They can induce patterns from just a few examples in data,

01:41.880 --> 01:43.560
or they can even read the definitions

01:43.560 --> 01:46.480
of totally novel words like Zaka Tota in context

01:46.480 --> 01:48.720
and produce realistic sentences that appear

01:48.720 --> 01:52.080
as if they understand how to use those words immediately.

01:52.080 --> 01:54.360
And so a lot of the excitement, I think it's fair to say

01:54.360 --> 01:57.080
around language models is that maybe for the first time,

01:57.080 --> 01:59.880
we're seeing something that is offering a scalable route

01:59.880 --> 02:03.840
towards implementing more generally intelligent architectures

02:03.840 --> 02:07.200
just by directly scaling, or largely by directly scaling

02:07.200 --> 02:08.160
the amount of language data

02:08.160 --> 02:09.960
that they're being trained to predict.

02:11.640 --> 02:13.280
All right, so what is the underlying idea here?

02:13.280 --> 02:15.200
What does this have to say about language?

02:15.200 --> 02:16.240
Well, I think it's fair to say

02:16.240 --> 02:18.880
that one of the dominant hypotheses that's underlying

02:18.880 --> 02:21.960
why we were even starting to see some of this behavior

02:21.960 --> 02:23.840
rests on kind of a two-part idea.

02:23.840 --> 02:27.120
One is something about the nature of language itself, right?

02:27.120 --> 02:31.280
It's the suggestion that language is sufficiently diverse

02:31.280 --> 02:34.360
and so broad, maybe because we suspect that humans

02:34.360 --> 02:35.760
express so much of their thoughts

02:35.760 --> 02:38.360
in such a diverse range of their thoughts and language

02:38.360 --> 02:41.520
that being able to perfectly solve this task,

02:41.520 --> 02:44.680
to be a perfect language model, or at least a really good one,

02:44.680 --> 02:48.440
is essentially an AGI complete task,

02:48.440 --> 02:50.400
and maybe also one that conveniently,

02:50.400 --> 02:51.560
unlike other kinds of tasks,

02:51.560 --> 02:53.320
like predicting all of the videos in the world,

02:54.000 --> 02:55.800
we have maybe efficient architectures to do

02:55.800 --> 02:57.200
and enough data to do.

02:57.200 --> 03:00.360
And I think it's worth noting that this doesn't actually

03:00.360 --> 03:03.200
really have to be a particularly strong hypothesis

03:03.200 --> 03:06.880
about what it is computationally that thinking looks like,

03:06.880 --> 03:10.520
or even how language necessarily is implicated internally

03:10.520 --> 03:13.920
inside the computational processes that we call thinking.

03:13.920 --> 03:15.360
Rather, it's really just a hypothesis

03:15.360 --> 03:18.360
about the nature of the language modeling task itself.

03:20.280 --> 03:22.200
And I think what a lot of people

03:22.400 --> 03:25.640
in this room would agree now is that,

03:25.640 --> 03:28.240
well, that might be true about the language modeling task

03:28.240 --> 03:31.280
in principle, much of the most exciting research

03:31.280 --> 03:32.640
that we're seeing using LMS right now

03:32.640 --> 03:35.920
actually isn't predicated on really the hope or the idea

03:35.920 --> 03:37.960
that just by scaling to more and more data,

03:37.960 --> 03:40.760
we're gonna expect transformers trained and used in this way

03:40.760 --> 03:42.200
to actually solve that task

03:42.200 --> 03:44.560
or become perfect next-world prediction models.

03:44.560 --> 03:46.520
And the intuition behind that,

03:46.520 --> 03:48.400
which I think many people have pointed out,

03:48.400 --> 03:51.600
comes both from the way in which we're looking

03:51.960 --> 03:53.800
in which transformers work, right?

03:53.800 --> 03:55.320
There are autoregressive language models

03:55.320 --> 03:57.520
that are doing a fixed, finite amount

03:57.520 --> 03:59.720
of internal computation that only scales

03:59.720 --> 04:01.520
based on the previous linguistic context.

04:01.520 --> 04:03.360
And it doesn't really make sense that,

04:03.360 --> 04:05.960
of course, you can pose questions in language

04:05.960 --> 04:08.240
like arbitrarily difficult math problems

04:08.240 --> 04:11.440
or planning problems that intuitively require

04:11.440 --> 04:12.360
an amount of computation,

04:12.360 --> 04:13.840
whose complexity doesn't actually depend

04:13.840 --> 04:16.720
linearly on the previous token context.

04:16.720 --> 04:18.680
And I think that's been matched empirically

04:18.680 --> 04:20.800
by lots of different observations

04:20.800 --> 04:23.480
about which kinds of sentences are hard to complete

04:23.480 --> 04:25.160
if you treat them as next-world prediction tasks

04:25.160 --> 04:26.160
in this way, right?

04:26.160 --> 04:27.360
Hard arbitrary math problems

04:27.360 --> 04:28.960
or planning problems like these.

04:29.960 --> 04:33.640
Right, so some of, you know,

04:33.640 --> 04:36.400
if you're thinking about the role of language,

04:36.400 --> 04:39.720
why is language, why are language models so big right now

04:39.720 --> 04:42.640
as, oh, well, language just has all the evidence necessary

04:42.640 --> 04:46.280
to train something to think.

04:48.080 --> 04:50.000
Like Leo said, that's not actually committing

04:50.000 --> 04:52.320
to any hypothesis about sort of how language

04:52.320 --> 04:53.520
is used internally in thinking.

04:53.520 --> 04:56.080
The transformers' computations are not necessarily

04:56.080 --> 04:59.040
doing anything linguistic as it's computing

04:59.040 --> 05:00.680
the distribution of the next word.

05:00.680 --> 05:04.440
But more recently, we've seen people use language models

05:04.440 --> 05:07.640
to solve these harder tasks by letting them think more.

05:07.640 --> 05:09.400
And what letting a transformer think more means

05:09.400 --> 05:11.800
is letting it generate more tokens.

05:11.800 --> 05:15.880
So in this view, thinking doesn't necessarily emerge

05:15.880 --> 05:17.880
just as predicting the next token.

05:18.880 --> 05:22.920
Rather, thinking happens in language, right?

05:22.920 --> 05:27.840
It happens by sort of producing a chain of thoughts

05:27.840 --> 05:30.960
or using a scratch pad or deciding

05:30.960 --> 05:32.640
that you're going to invoke a calculator

05:32.640 --> 05:34.640
or write some code and execute the code.

05:35.640 --> 05:40.280
And the sort of hypothesis embodied,

05:40.280 --> 05:42.000
I think by this line of work,

05:42.000 --> 05:43.160
is that the role of language

05:43.160 --> 05:45.880
and intelligence architecture is bigger.

05:45.880 --> 05:48.760
That language or some kind of like running monologue

05:48.760 --> 05:51.600
of language is the central controller of thought.

05:51.600 --> 05:54.760
And thinking is taking place primarily in language.

05:57.760 --> 06:01.840
And what's maybe striking about these proposals

06:01.840 --> 06:03.840
is that if you ask most cognitive scientists

06:03.840 --> 06:05.680
or neuroscientists, they'd probably say,

06:05.680 --> 06:07.520
this isn't the role that language plays

06:07.520 --> 06:09.320
in relation to general intelligence in humans,

06:09.320 --> 06:11.680
or at least it's not the dominant hypothesis.

06:12.760 --> 06:13.840
Until just a few years ago,

06:13.840 --> 06:14.920
this probably wouldn't have been the role

06:14.920 --> 06:16.000
that many AI researchers

06:16.000 --> 06:17.880
would have necessarily posited for language

06:17.880 --> 06:22.160
as the main controller or the main substrate

06:22.160 --> 06:25.240
of a running monologue that controls all thinking.

06:25.240 --> 06:27.320
And so we thought we'd review some of the background

06:27.320 --> 06:29.520
for what language seems to look like in humans

06:29.520 --> 06:32.000
as a basis for informing how we might build architectures

06:32.000 --> 06:34.360
that better capture those more human-like roles

06:34.360 --> 06:35.200
for language.

06:36.840 --> 06:38.080
Yeah, and so I just want to be clear

06:38.080 --> 06:39.760
that we're going to do kind of a quick background

06:39.760 --> 06:42.000
on some of the neuroscientific and cognitive evidence

06:42.000 --> 06:44.120
about what language might look like in people.

06:44.120 --> 06:47.080
And our goal really isn't to push back in any way

06:47.080 --> 06:48.840
against these other kinds of paradigms

06:48.840 --> 06:50.320
for where language might fit in.

06:50.320 --> 06:52.240
It's rather to present at a mode

06:52.240 --> 06:53.800
where we're really thinking a lot

06:53.800 --> 06:55.440
about scaling up language models

06:55.440 --> 06:57.960
as one really dominant role,

06:57.960 --> 07:00.200
what our other roots might be

07:00.200 --> 07:03.480
because it doesn't seem like that that really matches

07:03.480 --> 07:05.760
a different prominent intelligent architecture

07:05.760 --> 07:07.400
that's sitting here in the room today.

07:07.400 --> 07:09.280
So one source of evidence for the role

07:09.280 --> 07:12.080
that language appears to play in human cognition

07:12.080 --> 07:14.760
comes from neuroimaging data, fMRI data,

07:14.760 --> 07:18.160
suggesting how language is processed in the human brain.

07:18.160 --> 07:22.280
And at this point, convergent imaging data from many people,

07:22.280 --> 07:24.120
including Ed Fedorenko at MIT,

07:24.120 --> 07:26.120
Standa Hain and recent graduate students

07:26.120 --> 07:29.640
in our department, Kyle Malewald and Anya Ivanova,

07:29.640 --> 07:30.920
suggests that human brains

07:30.920 --> 07:33.440
have this language-specific network

07:33.440 --> 07:35.600
that handles many of the tasks that we associate with language.

07:35.600 --> 07:37.000
It's activated when people do tasks

07:37.000 --> 07:39.560
like listening to sentences or reading them

07:39.560 --> 07:41.440
or speaking or writing words.

07:42.280 --> 07:44.560
And this is not just an English-specific network.

07:44.560 --> 07:46.240
The same general region is activated

07:46.240 --> 07:48.720
no matter what language someone is speaking.

07:48.720 --> 07:51.640
It even fires when they're producing constructed languages

07:51.640 --> 07:53.360
for people who come to learn and become fluent

07:53.360 --> 07:55.320
in languages like Dothraki or Klingon.

07:56.960 --> 07:59.480
And right, what is it that this language network does?

07:59.480 --> 08:01.800
Well, increasingly, one dominant hypothesis

08:01.800 --> 08:03.480
is that it actually does do something

08:03.480 --> 08:05.120
like next-word prediction.

08:05.120 --> 08:08.600
And in fact, among many other kinds of alternative models,

08:08.600 --> 08:10.880
like people were earlier,

08:10.880 --> 08:12.480
for a long time I've been trying to do things

08:12.480 --> 08:14.560
like align words back to the brain,

08:14.560 --> 08:16.760
transformer architectures really do seem to be

08:16.760 --> 08:18.760
among the best models that we have right now

08:18.760 --> 08:21.280
of the neural activity of this language network,

08:21.280 --> 08:22.640
specifically when they're trained

08:22.640 --> 08:23.960
on next-word prediction tasks

08:23.960 --> 08:26.600
and not other linguistic tasks like NOI.

08:28.000 --> 08:30.600
But I think what we see in this neuroimaging evidence

08:30.600 --> 08:32.400
also highlights the ways in which the role

08:32.400 --> 08:34.360
of this human language network

08:34.360 --> 08:36.440
probably is not as the controller

08:36.440 --> 08:38.360
or the central seat of cognition.

08:38.360 --> 08:39.920
It's selective for language

08:39.920 --> 08:41.720
and it isn't involved in many other kinds

08:41.720 --> 08:43.440
of thinking activities,

08:43.440 --> 08:45.840
even ones that might seem to involve symbols

08:45.840 --> 08:47.320
like solving math problems

08:47.320 --> 08:49.680
or reasoning about logic and physics and social reasoning.

08:49.680 --> 08:53.160
Those invoke other regions of the brain

08:53.160 --> 08:55.360
and language interfaces modulary with them.

08:55.360 --> 08:57.320
It can interface very generally with them,

08:57.320 --> 08:58.160
but it doesn't seem to be

08:58.160 --> 09:00.360
where the bulk of thinking is taking place.

09:01.600 --> 09:02.440
Oh, yes.

09:02.440 --> 09:03.440
Oh yeah, I just had a question

09:03.440 --> 09:05.040
about the next-word prediction findings.

09:05.040 --> 09:08.840
So did you also try to say like math word prediction

09:08.960 --> 09:11.440
and this is actually worse than next-word?

09:11.440 --> 09:13.440
Yeah, so I think they do compare

09:13.440 --> 09:15.360
or like closed tasks are one of the alternates

09:15.360 --> 09:16.200
that they're looking for.

09:16.200 --> 09:18.440
And also, yeah, people should feel free to just shout out

09:18.440 --> 09:20.720
as we've been doing all along.

09:20.720 --> 09:24.880
So also, right, actually you can see adult patients

09:24.880 --> 09:28.120
that suffer strokes that only damage this area of their brain

09:28.120 --> 09:31.720
and we find that they can still think about,

09:31.720 --> 09:33.800
well, they can't comprehend spoken language,

09:33.800 --> 09:35.800
but they can still think about all these different kinds

09:35.800 --> 09:37.320
of tasks fronted in different mediums,

09:37.320 --> 09:38.920
like they can draw physical inferences

09:38.920 --> 09:40.760
from videos that they're watching.

09:40.760 --> 09:43.400
And conversely, maybe most tantalizingly,

09:43.400 --> 09:46.720
it's actually also possible to sustain localized damage

09:46.720 --> 09:49.360
that leaves you still able to produce these long,

09:49.360 --> 09:50.600
very relatively fluent

09:50.600 --> 09:53.080
and quite syntactically coherent sentences,

09:53.080 --> 09:55.400
almost as if they're maybe just like a very rudimentary

09:55.400 --> 09:58.080
and local word-based language model,

09:58.080 --> 09:59.880
while not really conditioning meaningfully

09:59.880 --> 10:01.920
on what someone else is saying.

10:01.920 --> 10:03.880
So they say, do you like it here in Kansas City?

10:03.880 --> 10:06.080
And this person says, yes, I am.

10:07.800 --> 10:09.120
Or really producing sentences

10:09.120 --> 10:10.720
that are obviously more globally meaningful

10:10.720 --> 10:12.080
or goal directed towards the question.

10:12.080 --> 10:13.880
And I don't want us to overindex on those results,

10:13.880 --> 10:17.000
but they kind of fit in with this more coherent picture.

10:17.000 --> 10:20.120
And in many ways, I think this kind of recent evidence

10:20.120 --> 10:22.800
from neuroscience actually supports the broader picture

10:22.800 --> 10:25.240
of human cognition and the place of language in it

10:25.240 --> 10:26.720
that most cognitive scientists

10:26.720 --> 10:28.800
and many linguists have already believed

10:28.800 --> 10:30.880
based on what we see and have come to learn

10:30.880 --> 10:32.800
through developmental science,

10:32.800 --> 10:34.200
studying how kids think

10:34.200 --> 10:36.320
and where language seems to fit into that picture, right?

10:36.320 --> 10:38.560
So a broad body of work at this point

10:38.560 --> 10:41.560
suggests that infants, well before they learn language

10:41.560 --> 10:43.320
of any kind or are speaking,

10:43.320 --> 10:45.880
already independently perform many of the tasks

10:45.880 --> 10:48.000
I think we associate with coherent thinking,

10:48.000 --> 10:50.120
from reasoning about physics to planning

10:50.120 --> 10:53.120
to drawing causal and probabilistic inferences.

10:53.120 --> 10:55.920
And language seems to be something that humans learn

10:55.920 --> 10:59.080
and scaffold on top of this structured basis for thinking.

10:59.080 --> 11:00.960
We take much less input data

11:00.960 --> 11:02.320
to learn to produce fluent language

11:02.320 --> 11:03.680
than a large language model.

11:03.680 --> 11:06.160
And humans that actually aren't exposed

11:06.160 --> 11:08.040
to any language input at all.

11:08.040 --> 11:11.000
So famously there are these deaf children in Nicaragua

11:11.000 --> 11:13.440
who grow up in isolated hearing families

11:13.440 --> 11:16.840
actually spontaneously come to produce early sign languages

11:16.840 --> 11:18.760
as a product of trying to communicate events

11:18.760 --> 11:20.640
that bear many of the hallmarks of the syntax

11:20.640 --> 11:23.480
that we associate of our own natural languages,

11:23.480 --> 11:26.200
like distinguishing between the subject who's punching

11:26.200 --> 11:28.440
and the person who's getting punched.

11:28.440 --> 11:30.000
Suggesting that in many ways

11:30.000 --> 11:32.640
the language we produce somehow externalizes

11:32.640 --> 11:36.040
the underlying structure of the thought that we already have.

11:36.080 --> 11:38.280
And of course, there are many other animals

11:38.280 --> 11:40.680
that we associate as being intelligent in some way

11:40.680 --> 11:42.960
and that have been modeled using the models

11:42.960 --> 11:46.400
that we associate with probabilistic reasoning and planning

11:46.400 --> 11:47.720
that don't use language at all.

11:47.720 --> 11:51.520
So it doesn't feel like language is by any means necessary

11:51.520 --> 11:53.040
for what we think of as thought.

11:54.560 --> 11:58.160
So formalizing this picture of an intelligent system,

11:58.160 --> 12:01.240
one that's shared across animals and non-linguistic infants

12:01.240 --> 12:03.880
and maybe captures some of the computations involved

12:03.880 --> 12:05.520
in many of those thinking tasks

12:05.520 --> 12:10.440
that don't involve the language network in our brains

12:10.440 --> 12:12.840
is both the central goal of a lot of cognitive science

12:12.840 --> 12:16.120
and has been a historic motivation

12:16.120 --> 12:18.360
for diverse fields within AI

12:18.360 --> 12:21.480
before the current sort of LLM centric moment.

12:21.480 --> 12:25.280
So if you open up Russell and Norvig's textbook,

12:25.280 --> 12:28.080
AI a modern approach, you'll see an equation like this

12:28.080 --> 12:30.960
that's supposed to sort of capture what we believe

12:30.960 --> 12:32.640
about how intelligence works.

12:34.440 --> 12:37.200
Computationally or a computational model of intelligence.

12:37.200 --> 12:39.200
Where the idea is that an intelligent agent

12:39.200 --> 12:42.720
is one that has sort of structured internal world models

12:42.720 --> 12:46.280
that can be updated based on observations of the world

12:46.280 --> 12:49.480
and in which we can sort of predict the results

12:49.480 --> 12:53.520
of our actions that the agent has some sort of values

12:53.520 --> 12:57.400
or desires that are captured in some kind of utility function

12:57.400 --> 12:59.480
that the agent can do probabilistic reasoning

12:59.480 --> 13:03.160
over its observations and the possible actions it might take

13:03.480 --> 13:05.120
and what their expected utilities are

13:05.120 --> 13:06.680
and that it can do some kind of planning

13:06.680 --> 13:08.760
to optimize the value of the back end.

13:08.760 --> 13:09.600
Yeah.

13:09.600 --> 13:12.320
Is there a normative statement or descriptive statement?

13:12.320 --> 13:15.320
Is it the case that we are defining intelligent agent

13:15.320 --> 13:17.200
to be having this kind of property

13:17.200 --> 13:20.000
or it's like if we want to do intelligent architecture,

13:20.000 --> 13:21.360
we should have these?

13:21.360 --> 13:23.040
Yeah, so I think some, it's a great question.

13:23.040 --> 13:24.960
I think some of the work definitely is coming at it

13:24.960 --> 13:26.160
from a normative perspective.

13:26.160 --> 13:28.920
This is like a definition of what rational actions

13:28.920 --> 13:30.080
might look like, right?

13:30.080 --> 13:31.240
But I think there is a lot of work

13:31.560 --> 13:32.960
computational cognitive science

13:32.960 --> 13:34.880
that sort of in various domains

13:34.880 --> 13:36.000
has collected a lot of evidence

13:36.000 --> 13:39.400
that people either like recognize this as normative

13:39.400 --> 13:42.920
or behave like this in computation,

13:42.920 --> 13:45.720
according to the limits of what they can do computationally.

13:48.240 --> 13:53.240
So in this architecture, the controller for thinking

13:53.320 --> 13:54.600
would be some kind of system

13:54.600 --> 13:56.240
that's capable of general world modeling

13:56.240 --> 13:57.800
and probabilistic reasoning

13:57.800 --> 13:59.800
or planning and utility maximization

13:59.840 --> 14:04.120
rather than sort of a primarily linguistic system necessarily.

14:05.840 --> 14:07.720
Now largely attempts in artificial intelligence

14:07.720 --> 14:09.360
to directly implement this equation

14:09.360 --> 14:11.920
by building out components that sort of map on

14:11.920 --> 14:15.520
to the different elements of this equation

14:15.520 --> 14:18.200
have struggled to match the computational efficiency

14:18.200 --> 14:20.640
and the generality of natural intelligence.

14:20.640 --> 14:23.200
But we have seen over the past couple of decades or so

14:23.200 --> 14:24.680
the emergence of this new class of tools

14:24.680 --> 14:26.480
called probabilistic programming languages

14:26.480 --> 14:27.320
for building software

14:27.320 --> 14:29.480
that can solve probabilistic reasoning tasks

14:29.480 --> 14:31.760
at least in limited domains.

14:31.760 --> 14:35.280
And sorry, and these languages have been applied

14:35.280 --> 14:37.280
to create systems that do everything

14:37.280 --> 14:39.720
from perceiving cluttered 3D scenes

14:39.720 --> 14:41.080
more accurately sometimes

14:41.080 --> 14:43.520
than object detector neural nets

14:43.520 --> 14:46.400
to interpreting and predicting economic trends

14:46.400 --> 14:48.360
more accurately than leading industry solutions

14:48.360 --> 14:49.760
like Facebook neural profit.

14:50.680 --> 14:52.480
And these probabilistic programming systems

14:52.480 --> 14:53.680
are enabling these applications

14:53.680 --> 14:55.200
with two key technical features.

14:55.200 --> 14:57.000
They feature modeling languages

14:57.000 --> 14:59.960
that let users express complicated probabilistic models

14:59.960 --> 15:01.440
of the world as programs

15:01.440 --> 15:04.240
making it easy to write down rich probabilistic models

15:04.240 --> 15:06.040
that are defined in terms of expressive

15:06.040 --> 15:07.760
program-like components.

15:07.760 --> 15:10.160
So for example, the probabilistic models behind

15:10.160 --> 15:11.560
these example applications

15:11.560 --> 15:14.120
are defined in terms of 3D renderers, symbolic planners,

15:14.120 --> 15:16.440
scientific simulators and so on.

15:16.440 --> 15:18.440
And the second thing that probabilistic programming systems do

15:18.440 --> 15:21.000
is that they automate various mathematical operations

15:21.000 --> 15:22.120
on these models

15:22.120 --> 15:23.960
and that automation makes it possible for users

15:23.960 --> 15:25.880
to concisely implement sophisticated algorithms

15:25.880 --> 15:27.200
for probabilistic reasoning,

15:27.200 --> 15:28.880
such as the variety of Monte Carlo

15:28.880 --> 15:30.160
and variational inference algorithms

15:30.160 --> 15:32.000
that power these example applications.

15:32.000 --> 15:33.360
One way of thinking about these tools

15:33.360 --> 15:38.160
is as kind of like PyTorch or TensorFlow,

15:38.160 --> 15:40.760
but instead of writing differentiable models

15:40.760 --> 15:41.720
and doing optimization,

15:41.720 --> 15:42.960
you're writing probabilistic models

15:42.960 --> 15:45.200
and doing various probabilistic reasoning tasks.

15:46.760 --> 15:48.840
So far, these AI engineering efforts

15:48.840 --> 15:50.000
haven't really made contact

15:50.000 --> 15:54.080
with the sort of language model part of AI.

15:55.000 --> 15:56.760
Much like there hasn't yet been a concerted effort

15:56.760 --> 15:59.080
to take this classical picture of intelligence architecture

15:59.080 --> 16:00.400
and figure out how language might be

16:00.400 --> 16:01.520
richly integrated into it.

16:01.520 --> 16:03.280
So for the rest of the talk today,

16:03.280 --> 16:04.920
we're gonna explore two different approaches

16:04.920 --> 16:06.360
for thinking about where language

16:06.360 --> 16:09.000
might fit into this picture and approach.

16:09.000 --> 16:11.160
So probably Leo's gonna begin

16:11.160 --> 16:12.800
by talking about how an intelligent agent

16:12.800 --> 16:16.120
might incorporate lots of externally produced languages,

16:16.120 --> 16:19.200
explanations, observations, questions,

16:19.200 --> 16:22.160
how an agent can incorporate all of those forms of language

16:22.200 --> 16:24.560
into the way that it updates its beliefs

16:24.560 --> 16:26.240
and decides how to act in the world.

16:26.240 --> 16:27.720
Then I'll talk a bit about systems

16:27.720 --> 16:30.120
that leverage language as a tool for thinking

16:30.120 --> 16:31.960
within its models of the world

16:31.960 --> 16:33.840
or its probabilistic reasoning algorithms.

16:33.840 --> 16:35.000
And each of these corresponds

16:35.000 --> 16:37.680
to very recent preprints of work.

16:37.680 --> 16:39.400
So I also wanna give a disclaimer.

16:39.400 --> 16:42.440
We should really emphasize that both are preliminary proposals

16:42.440 --> 16:44.800
and we're giving a very speculative talk,

16:44.800 --> 16:46.800
not scaled architectural solutions here.

16:48.320 --> 16:49.160
Cool, right.

16:49.160 --> 16:50.800
So the first person that's portion of this talk

16:50.800 --> 16:51.640
is summarizing work.

16:51.640 --> 16:52.640
If you wanna read more,

16:52.640 --> 16:54.360
you can find this very long paper

16:54.360 --> 16:56.320
from word models to world models.

16:56.320 --> 16:57.160
That's up on archive

16:57.160 --> 16:59.120
and this is primarily done with another student

16:59.120 --> 17:01.320
who's not here today, Gabe Grant.

17:01.320 --> 17:03.280
And as I just mentioned, the context for this work

17:03.280 --> 17:05.200
is thinking about how we can build systems

17:05.200 --> 17:07.620
that capture the breadth with which all the external language

17:07.620 --> 17:10.920
we hear seems to inform at least our human thinking.

17:10.920 --> 17:11.920
And what's clear, right,

17:11.920 --> 17:13.680
is that this role seems to be very broad.

17:13.680 --> 17:15.880
If we take the basic model of an agent

17:15.880 --> 17:17.200
with beliefs and goals,

17:17.200 --> 17:19.480
well, it seems like there's an incredibly diverse range

17:19.480 --> 17:21.960
of situations in which we can update our beliefs

17:21.960 --> 17:23.560
about a situation from observations

17:23.560 --> 17:25.160
that we express in language,

17:25.160 --> 17:26.640
or in which the goals of our thought

17:26.640 --> 17:29.480
are to answer questions that we specify linguistically.

17:29.480 --> 17:32.280
And these might implicate our knowledge of other agents

17:32.280 --> 17:33.760
drawing on our intuitive psychology

17:33.760 --> 17:36.240
to reason about what they think and what they'll do.

17:36.240 --> 17:38.640
Or we can talk about the physical world around us,

17:38.640 --> 17:40.280
what we perceive and ask questions

17:40.280 --> 17:41.760
that require us to reason about

17:41.760 --> 17:44.080
what we see and draw on our physical intuitions.

17:45.160 --> 17:46.520
And of course, one of the remarkable things

17:46.520 --> 17:48.280
where we're also excited about language

17:48.280 --> 17:50.400
is that it doesn't just draw on what we already know,

17:50.400 --> 17:52.360
it's this means by which humans seem to learn

17:52.360 --> 17:54.600
and pass on profoundly new knowledge,

17:54.600 --> 17:58.640
whether that's new concepts that we define in words

17:58.640 --> 18:01.360
or learn from words to really profound new theories

18:01.360 --> 18:02.960
and conceptual systems, right?

18:02.960 --> 18:04.400
Much of what we know about the world,

18:04.400 --> 18:06.960
the fact that there are wars, what wars are,

18:06.960 --> 18:08.720
legal systems, sciences,

18:08.720 --> 18:11.240
comes from information that it feels that we acquire

18:11.240 --> 18:12.880
from language in some way.

18:13.840 --> 18:15.320
But how do we do that?

18:15.320 --> 18:17.720
Well, I think one longstanding lens

18:17.720 --> 18:19.720
for thinking about language

18:19.720 --> 18:23.520
that's kind of persisted before the LLM-based moment

18:23.520 --> 18:25.400
is that what language is,

18:25.400 --> 18:27.280
is this external symbolic medium

18:27.280 --> 18:29.360
for communicating human thoughts.

18:29.360 --> 18:30.600
And the way that it does that

18:30.600 --> 18:33.360
is because there's some kind of general mapping function

18:33.360 --> 18:35.680
from our internal representations of thought

18:35.680 --> 18:38.720
into this external symbol system, that's language.

18:38.720 --> 18:41.360
And so in this kind of older framework,

18:41.360 --> 18:44.400
what it means to understand language or make meaning

18:44.400 --> 18:47.520
means mapping back from external sentences that we hear

18:47.520 --> 18:50.440
into structured internal representations.

18:50.440 --> 18:52.640
And what we explore in this paper

18:52.640 --> 18:56.880
is a general proposal that casts the meanings of language

18:56.880 --> 18:59.880
as these mappings or probabilistic distributions

18:59.880 --> 19:02.960
over expressions in a probabilistic programming language.

19:02.960 --> 19:05.200
And I'm gonna come back after some concrete examples

19:05.200 --> 19:07.720
to how I think this relates to the conceptual rule semantics

19:07.720 --> 19:09.480
that Steve articulated in the first talk,

19:09.480 --> 19:10.320
because I think there are actually

19:10.320 --> 19:11.600
some really deep connections here,

19:11.600 --> 19:14.560
the ways in which this might be one way to formalize

19:14.560 --> 19:16.160
or enrich some of those ideas.

19:17.120 --> 19:19.680
And this architecture, this proposal here

19:19.680 --> 19:21.720
also suggests how language can be integrated

19:21.720 --> 19:23.480
into a more general architecture,

19:23.480 --> 19:26.080
because it's one that already starts out with,

19:26.080 --> 19:27.280
as Alex mentioned,

19:27.280 --> 19:29.480
some kind of existing internal modeling language,

19:29.480 --> 19:32.440
whose goal is to represent the world probabilistically,

19:32.440 --> 19:34.560
query those models and specify what it means

19:34.560 --> 19:36.880
to draw coherent inferences over them.

19:36.880 --> 19:38.880
And it also suggests, I think, a framework

19:38.880 --> 19:40.920
for formally modeling the content

19:40.920 --> 19:43.040
of different kinds of sentences in language

19:43.040 --> 19:44.960
as the kinds of probabilistic programming expressions

19:45.040 --> 19:46.320
that they might map into.

19:48.320 --> 19:52.600
So, and so in this paper,

19:52.600 --> 19:54.640
we begin by exploring how this proposal

19:54.640 --> 19:56.920
might be instantiated with respect to

19:56.920 --> 19:59.200
a bunch of different domains of reasoning,

20:00.560 --> 20:03.520
sorry, including general probabilistic reasoning,

20:03.520 --> 20:05.160
but also reasoning about relations

20:05.160 --> 20:07.360
or physics and social situations.

20:07.360 --> 20:10.120
And in all of these, we're gonna propose

20:10.120 --> 20:11.280
that we might think about the language

20:11.280 --> 20:13.360
that communicates general conceptual knowledge

20:13.360 --> 20:16.520
about the world definitions or causal knowledge

20:16.520 --> 20:18.880
as constructing these probabilistic expressions

20:18.880 --> 20:22.160
that are those that build up probabilistic generative models.

20:22.160 --> 20:25.960
And then in this framework, observations in the language,

20:25.960 --> 20:29.160
like there is at least one red mug in the scene

20:29.160 --> 20:31.680
or Charlie is Dana's grandfather,

20:31.680 --> 20:34.120
construct formal conditioning statements,

20:34.120 --> 20:36.920
which update the state of this probabilistic model.

20:36.920 --> 20:39.480
And then questions map into query expressions

20:39.480 --> 20:42.320
that specify the formal target of probabilistic inference

20:42.320 --> 20:43.720
with respect to a model.

20:44.880 --> 20:46.120
And in this framework,

20:46.120 --> 20:49.760
we're thinking essentially cast as probabilistic reasoning.

20:49.760 --> 20:52.120
We suggest that another way that we can think about

20:52.120 --> 20:53.720
the role of a language model,

20:53.720 --> 20:55.160
one that's much smaller,

20:55.160 --> 20:56.800
like the language network in the brain

20:56.800 --> 20:58.800
is actually as a means of instantiating

20:58.800 --> 21:00.480
this meaning function in a way that

21:00.480 --> 21:02.680
we've really never had before

21:02.680 --> 21:05.240
to protect these kinds of context specific

21:05.240 --> 21:07.440
and previous discourse conditioned mappings

21:07.440 --> 21:09.640
from sentences in natural language

21:09.640 --> 21:11.520
into distributions over expressions

21:11.520 --> 21:14.120
that convey meaning in a probabilistic programming language.

21:16.040 --> 21:19.320
And so as a part of this long running disclaimer,

21:19.320 --> 21:20.880
what I'm gonna be showing is a really minimal

21:20.880 --> 21:22.880
implementation of this framework,

21:22.880 --> 21:25.240
but really intended as a pointer to different directions

21:25.240 --> 21:26.800
with which we might scale this approach

21:26.800 --> 21:28.880
to implement a more general interface between language

21:28.880 --> 21:31.000
and arrange a different core cognitive domains.

21:31.000 --> 21:32.120
So just to be clear,

21:32.120 --> 21:34.760
concretely in the examples that you see next,

21:34.760 --> 21:37.240
our meaning function is gonna be implemented using codex,

21:37.240 --> 21:40.240
right, an open AM model much smaller

21:40.240 --> 21:41.520
than the state of the art right now

21:41.520 --> 21:42.960
that's trained to learn joint distributions

21:42.960 --> 21:44.440
over language and code.

21:44.440 --> 21:46.080
And the probabilistic programming language

21:46.080 --> 21:47.480
we're gonna show is church,

21:47.480 --> 21:50.920
which is this very simple probabilistic programming language

21:50.920 --> 21:52.440
that supports kind of very general

21:52.440 --> 21:54.240
sample based inference procedures.

21:54.240 --> 21:56.360
And our goal is to demonstrate how this framework

21:56.360 --> 21:58.200
might broadly interface between language

21:58.200 --> 22:00.520
and a bunch of different core cognitive domains.

22:01.640 --> 22:03.840
So first to illustrate the basic sense

22:03.840 --> 22:06.280
in which a proposal like this might allow language

22:06.280 --> 22:09.440
to update an agent's beliefs and query a world model,

22:09.440 --> 22:11.480
I'm gonna begin with a really simple toy example

22:11.480 --> 22:13.160
that actually draws on a bunch of prior

22:13.160 --> 22:15.160
cognitive science experiments

22:15.160 --> 22:18.680
in which real people were asked to draw various inferences

22:18.680 --> 22:21.160
about which teams of players might win different games

22:21.160 --> 22:22.840
of tug of war based on the games

22:22.840 --> 22:25.360
that you'd previously seen players win or lose.

22:25.360 --> 22:27.920
And so this is older work from Josh's group

22:27.920 --> 22:30.560
that demonstrated I think the sense in which

22:30.560 --> 22:32.640
this normative model, this Norvig model

22:32.640 --> 22:34.960
of probabilistic inference actually in many ways

22:34.960 --> 22:37.720
predicts the actual behaviors and predictions made by humans

22:38.560 --> 22:40.560
using a very general probabilistic model

22:40.560 --> 22:43.120
of the mechanics of this tug of war game.

22:43.120 --> 22:45.840
And our goal here is to show how our framework

22:45.840 --> 22:48.280
we can implement an interface between natural language

22:48.280 --> 22:51.880
and all of the core examples of this older experiment.

22:51.880 --> 22:54.400
So just to go through here, I think what you're seeing

22:54.400 --> 22:56.600
in this little toy example, the world model

22:56.600 --> 22:58.640
that's being defined on the screen

22:58.640 --> 23:00.560
is capturing the basic causal relationship

23:00.560 --> 23:04.720
by which properties of different human players

23:04.720 --> 23:06.600
might influence the outcomes of different tournaments

23:06.640 --> 23:08.400
that they play in tug of war.

23:08.400 --> 23:12.440
So for instance, here we're modeling players

23:12.440 --> 23:16.360
as having some internal inherent strength value

23:16.360 --> 23:18.360
where strength varies approximately normally

23:18.360 --> 23:20.200
but as this unobserved latent variable

23:20.200 --> 23:21.880
over different kinds of players.

23:21.880 --> 23:24.200
And we also think of players as having

23:24.200 --> 23:26.600
some kind of internal laziness value

23:26.600 --> 23:28.400
which represents the percentage of the time

23:28.400 --> 23:29.440
that they actually don't act

23:29.440 --> 23:32.000
according to their underlying strength.

23:32.000 --> 23:35.000
And how did these variables determine the outcomes

23:35.000 --> 23:37.920
that we observe of given games of tug of war?

23:37.920 --> 23:40.280
Well, the strength of a whole team of players

23:40.280 --> 23:44.080
depends on the cumulative sum of its player strengths.

23:44.080 --> 23:46.280
But if a player is deciding to be lazy in this game,

23:46.280 --> 23:48.040
they might not pull as hard as they could.

23:48.040 --> 23:49.720
And whichever team pulls with the most strength

23:49.720 --> 23:52.560
in a given match is going to win that match, yeah.

23:52.560 --> 23:55.080
Did you design the primitives of strength and laziness

23:55.080 --> 23:57.400
or a codex come up with the primitives themselves?

23:57.400 --> 23:59.680
So in this one, we're looking at a model

23:59.680 --> 24:00.720
that's derived from the older work.

24:00.720 --> 24:03.200
So these are designed, but yes, that's later in this work.

24:03.200 --> 24:04.160
We're gonna show some examples

24:04.160 --> 24:05.720
of how you can learn this kind of model

24:05.720 --> 24:07.240
from someone just talking about it in language

24:07.240 --> 24:09.720
like the definition that I just gave.

24:09.720 --> 24:10.920
Right, and so again, you know,

24:10.920 --> 24:12.200
this is a really simple example,

24:12.200 --> 24:14.200
but I think also one that actually captures

24:14.200 --> 24:16.520
a surprising amount of the basic causal knowledge

24:16.520 --> 24:18.280
that people have if you tell them

24:18.280 --> 24:20.240
that you're gonna be listening to tug of war games,

24:20.240 --> 24:21.760
but sometimes people can be lazy

24:21.760 --> 24:23.440
and not pull as hard as they could.

24:23.440 --> 24:26.480
So how do we go about relating language in this domain?

24:27.680 --> 24:31.200
Right, well, one means by which we can induce

24:31.200 --> 24:33.120
a simple notion of a meaning function

24:33.120 --> 24:35.440
that actually fits the definition we just gave

24:35.440 --> 24:37.360
is by conditioning a language model

24:37.360 --> 24:40.840
both on this context-specific generative world model

24:40.840 --> 24:44.000
and on a few examples showing how language is mapped

24:44.000 --> 24:46.560
into sampled probabilistic programming expressions

24:46.560 --> 24:47.460
in this domain.

24:48.800 --> 24:50.520
And what we've done now, right,

24:50.520 --> 24:54.560
is effectively induce this kind of situation-specific

24:54.560 --> 24:57.680
contextual mapping from arbitrary new sentences

24:57.680 --> 24:59.000
to expressions that conditions

24:59.000 --> 25:02.680
both on the general prior distribution that codex is

25:02.680 --> 25:04.480
over language and code,

25:04.480 --> 25:07.320
and this kind of specific discourse thinking context

25:07.320 --> 25:10.720
of how language is being used in this situation.

25:10.720 --> 25:12.680
And there are clearly other ways to do this,

25:12.680 --> 25:14.080
some of which we'll talk about later,

25:14.080 --> 25:16.640
but we're using this example to illustrate

25:16.640 --> 25:17.660
just how much you might be able to do

25:17.660 --> 25:19.540
with this kind of minimal implementation,

25:19.540 --> 25:20.960
a notion of a model that translates

25:20.960 --> 25:22.800
between language to code.

25:22.800 --> 25:26.260
Right, so what kinds of language might we say here

25:26.260 --> 25:27.520
and how might we think about them

25:27.520 --> 25:29.640
in relation to probabilistic programming expressions?

25:29.640 --> 25:34.080
Well, a general proposition, like Josh won against Leo,

25:34.080 --> 25:37.000
gets translated into or might map,

25:37.000 --> 25:40.440
we might think of mapping or meaning a conditioned statement,

25:40.440 --> 25:42.880
an observation that Josh won against Leo.

25:43.740 --> 25:45.520
If we make subsequent observations,

25:45.520 --> 25:49.200
like then Josh went on to claim victory against Alex,

25:49.200 --> 25:51.840
we can continue to kind of generally use this

25:51.840 --> 25:54.640
meaning function that we've induced to turn that

25:54.640 --> 25:56.400
into a probabilistic programming language

25:56.400 --> 25:58.540
that captures the fact that Josh won against Alex.

25:58.580 --> 26:01.660
If we then say that even working together as a team,

26:01.660 --> 26:03.740
Leo and Alex still couldn't beat Josh

26:03.740 --> 26:05.260
in this game of tech of war.

26:06.820 --> 26:09.140
At this point, if we want to answer a query,

26:09.140 --> 26:12.020
like, okay, wait, how strong is Josh?

26:12.940 --> 26:17.020
What we think of as thinking in this situation

26:17.020 --> 26:20.860
is actually sampling from the posterior

26:20.860 --> 26:22.900
over possible worlds from the generative model

26:22.900 --> 26:25.220
that we just defined, subject to the observations

26:25.220 --> 26:26.340
that we've just made.

26:26.340 --> 26:29.540
And indeed, that means that the meaning of a sentence

26:29.540 --> 26:31.380
like how strong is Josh is really

26:31.380 --> 26:34.420
a structured publicistic inference query.

26:34.420 --> 26:37.780
What is the latent variable that is Josh's strength?

26:37.780 --> 26:40.460
And what we see here is that given his track record,

26:40.460 --> 26:42.660
all these people that he's beating, even playing together,

26:42.660 --> 26:44.580
our inference is that Josh is likely

26:44.580 --> 26:46.380
a good bit stronger than average.

26:46.380 --> 26:48.140
And that also means coherently,

26:48.140 --> 26:50.180
we might expect that a priori,

26:50.180 --> 26:52.820
a new player we've never seen, like Gabe,

26:52.820 --> 26:54.580
is going to be unlikely to beat him.

26:55.580 --> 26:59.260
So if we ask what are the odds of Gabe at beating Josh,

26:59.260 --> 27:01.300
we see that we think it's somewhat unlikely.

27:04.420 --> 27:05.260
Question?

27:05.260 --> 27:06.100
Oh, yes.

27:06.100 --> 27:08.420
So on the how strong is Josh,

27:08.420 --> 27:10.220
it seems like there's an interesting thing here

27:10.220 --> 27:11.860
where there's also an implicit question

27:11.860 --> 27:15.020
of what the word strong means in this context.

27:16.380 --> 27:20.300
And that, like, right, it's like not a number,

27:20.300 --> 27:22.580
it's kind of some like, comparative adjective.

27:22.580 --> 27:24.580
It's like, probably a non-intersective.

27:25.580 --> 27:27.780
So I guess, is that something you think about?

27:27.780 --> 27:29.740
This framework, or should I just be kind of, like,

27:29.740 --> 27:31.860
ignoring this sort of issue?

27:31.860 --> 27:33.340
Yeah, well, okay.

27:33.340 --> 27:34.740
So I think there's a number of ways

27:34.740 --> 27:35.860
that we can think about that.

27:35.860 --> 27:40.860
I mean, so, right, so the one sense we could say is like,

27:41.180 --> 27:42.380
right, how strong is Josh?

27:42.380 --> 27:44.980
Isn't, the answer to how strong of Josh isn't a number.

27:44.980 --> 27:47.380
Rather, it's kind of this distribution

27:47.380 --> 27:49.100
over this posterior distribution

27:49.100 --> 27:51.700
of various underlying strength values

27:51.700 --> 27:54.860
that we currently might infer that Josh has

27:54.860 --> 27:56.940
with respect to the general value that we have.

27:56.940 --> 27:59.660
I think another kind of popular definition

27:59.660 --> 28:02.020
of various uncertain adjectives,

28:02.020 --> 28:03.580
like a word like strong, right,

28:03.580 --> 28:06.160
is that you have some internal threshold value,

28:06.160 --> 28:08.020
or the person speaking has some kind of internal

28:08.020 --> 28:12.340
threshold value that you must kind of jointly infer

28:12.340 --> 28:14.860
with respect to the context in what you've seen.

28:14.860 --> 28:17.540
And some of the examples that I'll actually give later,

28:17.540 --> 28:19.860
so, right, there's kind of a long line of work

28:19.860 --> 28:21.540
in linguistics, including some work

28:21.540 --> 28:23.700
that treats that as like a pragmatic inference.

28:23.700 --> 28:25.340
I think some of the interesting work

28:25.340 --> 28:27.900
that we'll show a little bit later is that,

28:27.900 --> 28:29.340
there are some ways in which you might think

28:29.340 --> 28:32.820
of this mapping function as actually being a general one

28:32.820 --> 28:34.860
that includes that notion of pragmatic inference.

28:34.860 --> 28:36.940
And also, I think captures the sense

28:36.940 --> 28:38.740
in which if you continually,

28:38.740 --> 28:41.180
are you really doing this kind of pragmatic inference

28:41.180 --> 28:43.020
all the time, or do you actually,

28:43.020 --> 28:44.420
in many general settings,

28:44.420 --> 28:46.460
like talking about the strengths of people,

28:46.460 --> 28:49.340
actually have some kind of cashed older notion of strength

28:49.340 --> 28:50.180
that you can draw in.

28:50.180 --> 28:52.940
And I think actually, this notion of large language models

28:52.940 --> 28:55.460
as just being this learned mapping function

28:55.460 --> 28:57.580
from language into expressions include,

28:57.580 --> 28:59.220
can also capture the sense in which

28:59.220 --> 29:01.140
that knowledge is amortized away.

29:01.140 --> 29:02.860
And you might not be having that inference.

29:02.860 --> 29:03.700
Yeah, Chris.

29:08.500 --> 29:11.700
She's sort of just denying or ignoring

29:11.700 --> 29:16.420
what makes people so excited about large language models

29:16.460 --> 29:21.460
in their meaning representation and ability to do inference.

29:24.540 --> 29:27.100
I mean, because, okay, you've got sort of

29:27.100 --> 29:29.100
cooler probabilistic programming language

29:29.100 --> 29:30.860
on the right hand side,

29:30.860 --> 29:34.580
but in some sense, the picture is still,

29:34.580 --> 29:39.580
this is semantic parsing, like it was 2010 to 2015.

29:39.580 --> 29:44.580
And yes, you're using a large language model,

29:44.780 --> 29:48.300
but you're not actually using the excitement

29:48.300 --> 29:52.020
of a large language model as a representation system.

29:52.020 --> 29:54.540
Yeah, and I think, so probably each of us

29:54.540 --> 29:55.700
would have different answers to this,

29:55.700 --> 29:57.380
but part of what we're hoping to paint out

29:57.380 --> 29:59.220
over the course of this talk is I think

29:59.220 --> 30:02.100
some of the ways in which actually, right,

30:02.100 --> 30:04.900
of course, no one wants to say we're gonna go back

30:04.900 --> 30:07.020
to kind of the brittleness of semantic parsing,

30:07.020 --> 30:08.780
but I think one thing that large language models

30:08.780 --> 30:11.460
actually give us, or one proposal in this talk,

30:11.500 --> 30:15.580
is that there are some aspects of the theory,

30:15.580 --> 30:17.260
kind of the classic notion of linguistics,

30:17.260 --> 30:19.420
and certainly the classic notions of semantic parsing

30:19.420 --> 30:20.860
that actually normatively capture

30:20.860 --> 30:23.580
a lot of what we really might want when we think about,

30:23.580 --> 30:28.580
so one answer for an AI system is, well, yes,

30:28.580 --> 30:29.700
in some way, we don't wanna throw,

30:29.700 --> 30:31.100
certainly we don't wanna throw away

30:31.100 --> 30:32.980
everything that we're learning from large language models,

30:32.980 --> 30:34.340
and I think one answer to that

30:34.340 --> 30:37.260
is kind of the answer that I gave to Jacob, right?

30:37.260 --> 30:39.860
If we think about not always, you know,

30:39.860 --> 30:42.980
in these examples, we're showing this very direct system

30:42.980 --> 30:44.940
in which we always start with language

30:44.940 --> 30:46.700
and we always map into some sort of

30:46.700 --> 30:47.620
probabilistic programming expression,

30:47.620 --> 30:49.540
and that's where all of the thinking happens,

30:49.540 --> 30:51.980
and we might think, well, that doesn't totally make sense

30:51.980 --> 30:55.100
because there are lots of cases where, as you're saying,

30:55.100 --> 30:56.380
we have every reason to believe

30:56.380 --> 30:58.380
that large language models have learned

30:58.380 --> 31:00.020
a lot of latent information.

31:00.020 --> 31:01.940
They can do a lot of, they certainly have

31:01.940 --> 31:03.780
a lot of latent conceptual information,

31:03.780 --> 31:05.580
and maybe to some degree, they can even perform

31:05.580 --> 31:08.260
certain kinds of limited, amortized inferences,

31:08.260 --> 31:10.780
or reuse old inferences that they've learned

31:10.780 --> 31:11.740
from other people I've had,

31:11.740 --> 31:13.420
and so in the second part of this talk,

31:13.420 --> 31:15.220
we're going to show different ways in which,

31:15.220 --> 31:17.500
well, this probabilistic programming language itself

31:17.500 --> 31:20.340
doesn't necessarily need to be something that's isolated

31:20.340 --> 31:22.140
from what large language models have learned.

31:22.140 --> 31:25.740
It also can embed calls to large language models

31:25.740 --> 31:28.100
within it to kind of draw in that sort of knowledge.

31:28.100 --> 31:30.860
Haven't you gone back to the visualness of semantic housing

31:30.860 --> 31:33.340
because you're doing this translation

31:34.340 --> 31:39.260
into symbolic semantic representation,

31:39.260 --> 31:42.540
which really ends with your actual result,

31:42.540 --> 31:44.860
and it's riddled in the same way?

31:44.860 --> 31:48.820
Well, right, and also, so no, I think I would say,

31:48.820 --> 31:53.340
I don't totally think that the way in which we're using,

31:53.340 --> 31:55.620
or the sense in which, or I think there are some ways

31:55.620 --> 31:58.980
in which this kind of broader definition

31:58.980 --> 32:01.540
in which you are saying, well, the meaning of a distribution,

32:01.540 --> 32:04.140
or the meaning of a sentence in language

32:04.140 --> 32:06.900
isn't just one probabilistic programming expression, right?

32:06.900 --> 32:10.260
That's what we're showing here for pedagogical purposes,

32:10.260 --> 32:12.180
but you might say, well, okay, right,

32:12.180 --> 32:16.460
how are you going to obey kind of the ambiguity of language?

32:16.460 --> 32:19.620
There are kinds of sentences that are definitely ambiguous.

32:19.620 --> 32:23.100
So one example that we've looked at are sentences

32:23.100 --> 32:24.580
in which you say something like,

32:24.580 --> 32:26.780
Josh beat Alex and Leo, right?

32:26.780 --> 32:28.260
And you might ask, well, you know,

32:28.260 --> 32:30.540
that's kind of a classic syntactic construction.

32:30.580 --> 32:33.420
Does that mean that Josh beat Alex and Leo,

32:33.420 --> 32:34.940
and they were playing on the same team,

32:34.940 --> 32:38.260
or Josh beat Alex, and then Josh went on to beat Leo?

32:38.260 --> 32:42.380
And what we see, or generally, what you might say is,

32:42.380 --> 32:44.060
well, the meaning of that sentence

32:44.060 --> 32:47.820
actually shouldn't be picking one expression or the other.

32:47.820 --> 32:49.780
It should be kind of the distribution

32:49.780 --> 32:52.820
over those possible parses,

32:52.820 --> 32:55.460
and that distribution also shouldn't just be something

32:55.460 --> 32:58.060
that we can determine in this totally context

32:58.060 --> 33:00.180
and sensitive way, it should actually depend on

33:00.180 --> 33:02.300
all the previous patterns in the discourse.

33:02.300 --> 33:06.180
So if someone's continually been using this conjunctive

33:06.180 --> 33:10.660
and to refer to teams of players playing together,

33:10.660 --> 33:13.300
we should take that kind of discourse bias into account.

33:13.300 --> 33:16.780
And I think actually, right, this provides,

33:16.780 --> 33:19.580
or thinking of large language models

33:19.580 --> 33:21.260
as kind of generally having learned

33:21.260 --> 33:22.860
this broad joint distribution,

33:22.860 --> 33:26.500
but one that can be kind of conditioned quite richly

33:26.500 --> 33:30.620
both on the content of this generative model.

33:30.620 --> 33:32.220
So it's not trying to come up

33:32.220 --> 33:34.460
with a universal definition of strength.

33:34.460 --> 33:36.140
It's not even necessarily trying to come up

33:36.140 --> 33:38.500
with a universal definition of any of these words.

33:38.500 --> 33:40.500
It's thinking about how they might map contextually

33:40.500 --> 33:43.620
into the best possible expression in the context

33:43.620 --> 33:48.500
of a particular local model built for a particular situation.

33:48.500 --> 33:52.180
I think is obviously related to,

33:52.180 --> 33:57.180
but attempting to address some of the brittleness challenges

33:57.220 --> 33:59.420
of semantic parsing in the past.

33:59.420 --> 34:00.780
I think another answer to this, right,

34:00.780 --> 34:04.540
is that part of the problem of semantic parsing previously

34:04.540 --> 34:06.100
has been actually that the mapping functions

34:06.100 --> 34:09.020
have historically been difficult to get right.

34:09.020 --> 34:10.820
Whether you were thinking about those

34:10.820 --> 34:13.380
as kind of old hard-coded grammars

34:13.380 --> 34:16.060
or many of the attempts to kind of learn these things,

34:16.060 --> 34:17.940
we are very domain-specific supervision.

34:17.940 --> 34:19.740
So you wanna have a semantic parser

34:19.740 --> 34:21.740
for a particular robotics domain.

34:21.740 --> 34:25.660
You need a thousand examples of sentences

34:25.660 --> 34:27.660
about that particular robotics domain

34:27.660 --> 34:30.340
and a thousand paired with a thousand examples

34:30.340 --> 34:35.340
of programs that are operating on that particular domain.

34:35.660 --> 34:38.420
What we're seeing here is I think something that says no.

34:38.420 --> 34:40.460
What it means to learn language generally

34:40.460 --> 34:42.780
is to learn kind of this general mapping

34:42.780 --> 34:45.980
between language and some kind of underlying representation.

34:47.340 --> 34:51.420
And also, one reason why we might want a system like this

34:51.420 --> 34:53.980
is because we want to be able to condition coherently

34:53.980 --> 34:57.940
on information that's not just coming from language.

34:57.940 --> 35:01.860
And we want to think about how a general substrate

35:01.860 --> 35:05.660
in which the only, yes, we might be told

35:05.660 --> 35:06.900
that Josh went against Leo,

35:06.900 --> 35:09.700
but we might also be watching videos

35:09.700 --> 35:12.460
that give us information about Josh's strength,

35:12.460 --> 35:13.460
that convey our observations.

35:13.460 --> 35:16.340
We might also have seen pictures

35:16.340 --> 35:18.860
like the ones in the stimuli that we saw before

35:18.900 --> 35:22.620
demonstrating the results of previous outcomes of matches.

35:22.620 --> 35:25.860
And I think one thing that suggests

35:25.860 --> 35:29.740
is we want this kind of general substrate

35:30.740 --> 35:32.940
in which we can think about how those observations,

35:32.940 --> 35:35.340
including the observations from language,

35:35.340 --> 35:37.900
but without prioritizing language in any way,

35:37.900 --> 35:40.380
I think are coherently considered.

35:42.900 --> 35:47.260
So I think it depends on what part of semantic parsing

35:47.260 --> 35:50.940
you, or yeah, I think the answer to that depends

35:50.940 --> 35:53.380
on what part of semantic parsing we think of

35:53.380 --> 35:55.940
as being the source of the brittleness

35:55.940 --> 35:59.500
that caused us to throw that paradigm into question.

36:01.340 --> 36:04.980
Yeah, and maybe I'll just offer one more perspective.

36:04.980 --> 36:07.700
So one part of it is what Leo is saying.

36:07.700 --> 36:09.740
Traditional semantic parsing is brittle in two ways.

36:09.740 --> 36:11.540
One is, do you have broad coverage of language

36:11.540 --> 36:13.580
that you can parse into your system?

36:13.580 --> 36:15.940
And two is like how broad coverage

36:15.940 --> 36:18.060
are the set of query, like the set of semantic queries

36:18.060 --> 36:19.420
that you can actually answer.

36:19.420 --> 36:21.020
And I think what you're pointing out is

36:21.020 --> 36:23.100
this doesn't seem to address the second source

36:23.100 --> 36:24.580
of brittleness, which is that your system

36:24.580 --> 36:25.580
can only answer certain things,

36:25.580 --> 36:27.460
it can only reason about certain things.

36:28.460 --> 36:32.180
Brittleness of the formal representation language

36:32.180 --> 36:33.020
that you're using.

36:33.020 --> 36:38.020
Right, that's true of large language model representations.

36:38.860 --> 36:43.860
Right, so I think my sort of take on that

36:44.380 --> 36:48.100
is from a kind of AI engineering perspective

36:48.100 --> 36:52.860
is sort of a branching in two directions.

36:52.860 --> 36:55.740
One is, I think we have made some progress

36:55.740 --> 36:59.340
that this is not really evoking, probably,

36:59.340 --> 37:01.980
toward systems that within restricted domains

37:03.100 --> 37:06.140
can reason coherently and probabilistically

37:06.140 --> 37:07.900
about a wide range of queries.

37:07.900 --> 37:12.900
So we have systems like this inference QL system

37:13.180 --> 37:16.180
that uses nonparametric bays to analyze huge data tables

37:16.180 --> 37:19.020
and come up with a model of that system

37:19.020 --> 37:22.660
that or of your data that can answer all sorts of questions

37:22.660 --> 37:27.660
like, oh, you know, show me like which people

37:29.380 --> 37:32.420
in this data set are like probably overpaid

37:32.420 --> 37:34.340
given their experience or something like that.

37:34.340 --> 37:36.820
So in the same way that people are kind of excited

37:36.820 --> 37:39.860
about using natural language or using language models

37:39.860 --> 37:42.500
to parse into SQL, right?

37:42.500 --> 37:44.980
Because so much data is in SQL

37:44.980 --> 37:48.220
and it's a very SQL is a very expressive language

37:48.220 --> 37:50.220
for asking questions about that data.

37:51.300 --> 37:53.220
When we have a probabilistic system,

37:53.220 --> 37:55.740
like a good probabilistic model of that data under the hood,

37:55.740 --> 37:58.580
it enables conversational patterns that are not enabled

37:58.580 --> 38:01.020
when you have like SQL as the database

38:01.020 --> 38:03.020
because we expect our conversational partners

38:03.020 --> 38:05.100
to have coherent beliefs about the world,

38:05.100 --> 38:07.620
to update those beliefs in response to new evidence

38:07.660 --> 38:11.100
that we give it to be able to report uncertainty

38:12.540 --> 38:14.300
and make sort of modal judgments.

38:14.300 --> 38:18.860
And so one engineering path is to take those kinds of systems

38:18.860 --> 38:22.700
and sort of build conversational interfaces to them

38:22.700 --> 38:25.860
that behave more like an intelligent person would behave

38:25.860 --> 38:28.700
and can draw inferences that you might not draw

38:28.700 --> 38:30.820
if you're just talking to a SQL backup.

38:30.820 --> 38:33.020
The other path that sort of we'll talk about

38:33.020 --> 38:35.540
in the next part of the talk

38:35.580 --> 38:37.780
is how can we use those representations

38:37.780 --> 38:39.220
that language models have learned

38:39.220 --> 38:43.180
to make the probabilistic inferences more interesting

38:43.180 --> 38:45.100
and more robust, less brittle,

38:45.100 --> 38:48.580
without sort of totally embracing the other kind of brittleness

38:48.580 --> 38:50.180
which is the kind of brittleness that language models

38:50.180 --> 38:51.020
seem to have right now,

38:51.020 --> 38:52.540
which is that they draw,

38:52.540 --> 38:53.940
that they don't really necessarily reason

38:53.940 --> 38:56.340
with coherent probabilistic beliefs.

38:56.340 --> 38:59.340
So maybe, yeah, let's go into that next part.

38:59.340 --> 39:03.340
Yes, yes, okay, right.

39:04.300 --> 39:06.540
Well, so right, so in the interest of time,

39:06.540 --> 39:07.940
I'm actually gonna like skip through

39:07.940 --> 39:09.220
some of the rest of this example,

39:09.220 --> 39:11.660
which I think is just more of what you've seen,

39:11.660 --> 39:13.860
but one sense in which I think,

39:13.860 --> 39:16.540
yeah, maybe a third part of the answer to Chris,

39:16.540 --> 39:17.740
I would say, is that,

39:20.460 --> 39:22.300
right, you know, yeah.

39:22.300 --> 39:23.900
I think part of what this is trying to do

39:23.900 --> 39:26.700
is explore some of the ways in which we might answer ways,

39:26.700 --> 39:29.820
right, without giving an answer in all the ways in which

39:29.820 --> 39:32.780
we might answer the ways in which language models themselves

39:32.780 --> 39:35.260
are brittle with respect to what we also want

39:35.260 --> 39:36.860
from a model of intelligence, right?

39:36.860 --> 39:38.980
We might suspect that when we answer,

39:38.980 --> 39:40.540
ask questions like this,

39:40.540 --> 39:42.980
really what we are trying to do is specify

39:42.980 --> 39:46.980
some kind of normative query that captures formally

39:46.980 --> 39:50.260
a sense of, well, we want something like the posterior

39:50.260 --> 39:52.980
with respect to some kind of internal model of the world.

39:52.980 --> 39:55.780
And, you know, this is kind of the simplest means,

39:55.780 --> 39:57.620
or this is a very simple example

39:57.620 --> 40:00.380
of how we might formally impose that kind of structure,

40:01.380 --> 40:03.620
but one that I think can be elaborated on,

40:05.300 --> 40:07.660
depending on the kinds of primitives

40:07.660 --> 40:09.140
and the ways in which you're thinking about

40:09.140 --> 40:12.220
what it is that the probabilistic programs can express, right?

40:12.220 --> 40:15.460
So one way in which we might think about doing that

40:16.580 --> 40:19.820
is by thinking about probabilistic programs

40:19.820 --> 40:23.660
that themselves have access to other kinds of means

40:23.660 --> 40:26.820
of calling other different mechanisms and cognition, right?

40:26.820 --> 40:29.180
So I think I would draw a contrast here

40:29.300 --> 40:31.900
between the notion of the large language model

40:31.900 --> 40:34.620
as a controller, the one that's making the decisions

40:34.620 --> 40:37.340
about when to write little snippets of code

40:37.340 --> 40:41.460
and to execute them, when to call out to little planners

40:41.460 --> 40:44.340
and incorporate them, or stuff like the Minds Eye work,

40:44.340 --> 40:46.140
right, where there's a language model,

40:46.140 --> 40:48.540
it decides when to call out to a physics simulator,

40:48.540 --> 40:51.460
but the way it interprets the outputs

40:51.460 --> 40:53.620
of that physics simulator is to paste those back

40:53.620 --> 40:55.700
into the language model context

40:55.700 --> 40:58.540
and try to draw inferences on them in turn.

40:58.540 --> 41:00.580
Rather, in this kind of framework, right,

41:00.580 --> 41:03.460
what you can kind of see a, or yeah,

41:03.460 --> 41:05.700
the direction that this framework would be pointing towards

41:05.700 --> 41:09.300
is to say, well, on the other hand,

41:09.300 --> 41:12.700
we already have languages that allow us to do things

41:12.700 --> 41:14.980
like build expressive generative models

41:14.980 --> 41:17.900
over three-dimensional scenes that also capture things

41:17.900 --> 41:20.020
that we might want only from perception,

41:20.020 --> 41:22.540
like knowledge about how the shapes of objects

41:22.540 --> 41:24.380
tend to accude each other,

41:24.380 --> 41:27.460
or incorporate rich models of physics,

41:27.460 --> 41:31.340
or that model theory of mind as taking place recursively

41:31.340 --> 41:33.860
and thinking about agents who themselves have beliefs

41:33.860 --> 41:35.780
about their own internal world models

41:35.780 --> 41:39.420
and are actually choosing their actions as planners, right?

41:39.420 --> 41:42.060
And in this kind of framework,

41:42.060 --> 41:44.900
you can point the way towards a kind of model that says,

41:44.900 --> 41:47.780
well, how is it that I might incorporate language

41:47.780 --> 41:50.340
into these kinds of models sitting alongside

41:50.340 --> 41:53.140
these other kind of observations that I might make, right?

41:53.140 --> 41:56.140
So how might I think about the meaning

41:56.380 --> 41:58.940
of images that I want to generate

41:58.940 --> 42:01.260
that specify specific constraints,

42:01.260 --> 42:05.420
or imagination, or, right, go ahead, Jacob.

42:05.420 --> 42:07.060
Yeah, I just had a clarification question.

42:07.060 --> 42:11.060
So you were talking earlier about having this meeting function,

42:11.060 --> 42:13.340
and then I think also we're mentioning something

42:13.340 --> 42:18.340
about like code x, in terms of the questions.

42:18.420 --> 42:21.540
I'm just trying to understand which of these is that.

42:21.540 --> 42:23.460
Is that here, or is the meeting function

42:23.460 --> 42:24.700
when you come later?

42:25.660 --> 42:26.780
So that's maybe the first question

42:26.780 --> 42:27.860
and then the other clarification is,

42:27.860 --> 42:30.420
so are these statements actually just been programmatically

42:30.420 --> 42:34.620
created from code x by prompting the text that was on?

42:34.620 --> 42:35.460
Yes, that's right.

42:35.460 --> 42:40.460
So by meeting function in this framework, we say,

42:40.500 --> 42:42.100
well, there's kind of two generalizations

42:42.100 --> 42:42.940
of a meeting function.

42:42.940 --> 42:45.140
There's a general joint prior, right?

42:45.140 --> 42:46.660
That code x is already,

42:46.660 --> 42:47.940
that it's learned between language and code,

42:47.940 --> 42:50.740
and then there's this kind of context specific

42:50.740 --> 42:53.300
meeting function in the sense that it's conditioned

42:53.300 --> 42:55.860
on whatever's in the prompt, the generative model,

42:55.860 --> 42:59.940
and some examples of how language relates to expressions

42:59.940 --> 43:01.660
that it's doing, so that's a meeting function.

43:01.660 --> 43:03.500
And yes, all these examples that you're seeing

43:03.500 --> 43:05.980
are one sample from that distribution.

43:09.500 --> 43:13.060
Right, and one of the things that I wanna point to here,

43:13.060 --> 43:15.740
right, is it does, I think it suggests a framework

43:15.740 --> 43:18.220
or another means of thinking about what it means

43:18.220 --> 43:21.660
for language to construct new concepts from definitions,

43:21.700 --> 43:23.460
or even come to construct new world models

43:23.460 --> 43:25.820
from thinking like somebody in the beginning asked, right?

43:25.820 --> 43:29.500
So how, for instance, might we think about enriching

43:29.500 --> 43:31.940
an existing structured relational model

43:31.940 --> 43:33.900
with concepts that we learn from language?

43:33.900 --> 43:38.060
So for example, if we consider kind of a formal model

43:38.060 --> 43:43.060
of kinship relations, we might say that, well,

43:43.900 --> 43:46.020
the generative model of this domain

43:46.020 --> 43:49.340
is itself represented as a probabilistic program.

43:49.340 --> 43:52.940
It captures both the causal means by which

43:54.100 --> 43:56.900
people give rise to their children,

43:56.900 --> 44:00.540
and also the definitions or one notion of the definitions

44:00.540 --> 44:02.620
of what it means to be something like a sister or a father

44:02.620 --> 44:04.780
with respect to this core notion

44:04.780 --> 44:07.300
of how family trees come to be.

44:07.300 --> 44:10.500
And so if you take this kind of general notion

44:10.500 --> 44:13.900
of the meaning of language as being the distribution

44:13.900 --> 44:14.900
over expressions that it creates

44:14.900 --> 44:16.340
in a probabilistic programming language,

44:16.340 --> 44:20.380
you might start to think how we can formally think about

44:20.380 --> 44:24.260
relating definitions for various kinds of relational terms.

44:25.260 --> 44:26.860
An uncle is the brother of one's parent

44:26.860 --> 44:28.420
or the husband of one's aunt.

44:28.420 --> 44:31.420
A pibling is a gender neutral term for an aunt or uncle,

44:31.420 --> 44:33.220
that's the sibling of one's parent,

44:34.260 --> 44:39.260
or this relational notion of a sister of one's father

44:39.980 --> 44:41.420
from a language that's actually not found

44:41.420 --> 44:42.780
anywhere on the internet.

44:42.780 --> 44:45.620
And I think the core thing that we wanna suggest here, right,

44:46.020 --> 44:48.140
is why do we even have definitions at all?

44:49.500 --> 44:53.260
Well, one notion of what it even means

44:53.260 --> 44:55.660
to have learned the definition of this term

44:55.660 --> 44:58.340
is that it should drive coherently

44:58.340 --> 45:01.260
all of the downstream inferences that you make with that term,

45:01.260 --> 45:04.340
and it should graft onto the conceptual knowledge

45:04.340 --> 45:05.580
that you already have.

45:05.580 --> 45:10.020
And so you can think about forming new sentences directly

45:10.020 --> 45:13.380
that refer to someone's paani or one's pibling

45:13.380 --> 45:15.460
in this situation and expecting them

45:15.460 --> 45:18.620
to draw both on your existing conceptual knowledge

45:18.620 --> 45:22.180
of what it even means to have a family tree

45:22.180 --> 45:24.540
as well as all the other conceptual terms for a friendship

45:24.540 --> 45:25.780
that you may already have.

45:28.060 --> 45:32.100
And the same framework also suggests one mechanism

45:32.100 --> 45:34.340
by which we might formalize what it means

45:34.340 --> 45:36.660
to learn world models from language.

45:36.660 --> 45:39.380
So as I mentioned, if we return to the situation

45:39.380 --> 45:42.580
that opens this talk, tug of war games,

45:43.420 --> 45:45.820
we might think about how the definition that I gave

45:45.820 --> 45:48.140
when I sat up here at the podium, right,

45:48.140 --> 45:50.380
saying there are people whose strength levels vary

45:50.380 --> 45:52.420
from person to person.

45:52.420 --> 45:55.940
People have a percentage of time in which they're lazy.

45:55.940 --> 45:58.900
Strengths of the teams depend on the underlying strengths

45:58.900 --> 46:01.620
of the members of that team,

46:01.620 --> 46:03.420
and whether one team beats another

46:03.420 --> 46:05.980
just depends on which team pulls stronger that match.

46:05.980 --> 46:09.060
And this kind of setting is actually language, right,

46:09.060 --> 46:12.380
is building up the actual generative model itself.

46:12.380 --> 46:14.580
And you might think of a system like this

46:14.580 --> 46:17.300
that both learns these kinds of theories from language

46:17.300 --> 46:21.380
and then is appending to this kind of local problem-based

46:21.380 --> 46:23.700
context to answer arbitrary questions

46:23.700 --> 46:25.220
like the kinds that we gave or conditioned

46:25.220 --> 46:28.220
on various observations like Josh being stronger than Leo

46:28.220 --> 46:30.740
with respect to this kind of local notion

46:31.980 --> 46:36.300
of what strength means in this particular problem context

46:36.300 --> 46:38.300
that we're thinking about.

46:38.300 --> 46:39.700
In interest of time, why don't we just jump on

46:39.700 --> 46:40.540
to your section.

46:40.620 --> 46:41.460
Yeah, thanks.

46:48.540 --> 46:51.300
So we've just been talking about how natural language

46:51.300 --> 46:55.620
can sort of be interpreted or semantically parsed

46:55.620 --> 46:57.980
to a probabilistic language of thought,

46:57.980 --> 47:00.500
but we haven't talked about how cognition itself,

47:01.780 --> 47:03.620
which is sort of, we've been talking about

47:03.620 --> 47:05.180
as the product of general purpose

47:05.180 --> 47:06.900
probabilistic inference machinery,

47:06.900 --> 47:09.500
might interact with language cognitively

47:09.540 --> 47:12.420
or how our tools for, you know,

47:12.420 --> 47:14.980
our algorithms for inference, our model representations

47:14.980 --> 47:18.580
might benefit from recent advances in language models.

47:18.580 --> 47:21.260
So in the rest of the talk,

47:21.260 --> 47:23.740
I'll sort of talk about this also very preliminary work

47:23.740 --> 47:26.180
that we just presented at a workshop at ICML

47:27.660 --> 47:29.420
that is more about a role for natural language

47:29.420 --> 47:31.660
and language models in this part of the picture.

47:32.900 --> 47:34.700
And one reason to think that natural language

47:34.700 --> 47:37.260
must play some role in this part of the picture

47:37.340 --> 47:41.140
is that sometimes we set ourselves reasoning tasks

47:41.140 --> 47:43.340
whose specifications, what it would mean

47:43.340 --> 47:44.980
to solve the reasoning task correctly

47:44.980 --> 47:46.180
must involve natural language.

47:46.180 --> 47:47.820
So for example, if you have an iPhone,

47:47.820 --> 47:50.300
you might have used the visual voicemail feature,

47:50.300 --> 47:51.700
which automatically, but somewhat

47:51.700 --> 47:53.780
incompletely transcribes your voicemails.

47:53.780 --> 47:56.060
And these transcripts have gaps marked

47:56.060 --> 47:58.140
by underscored sequences of varying lengths,

47:58.140 --> 48:00.740
indicating Apple couldn't quite work out what was said.

48:01.740 --> 48:03.500
And an inference task that I sometimes face

48:03.500 --> 48:04.820
is squinting at these transcripts

48:04.820 --> 48:06.820
and trying to think what could the person have said

48:06.820 --> 48:09.740
during those bits that it didn't transcribe correctly.

48:09.740 --> 48:12.980
And is it worth my time to listen to this voicemail

48:12.980 --> 48:16.660
or am I pretty certain that I got all the relevant information

48:16.660 --> 48:18.980
from the part of the transcript that I've seen?

48:20.100 --> 48:23.100
So even if I'm representing that kind of inference problem

48:23.100 --> 48:24.940
in some kind of probabilistic language of thought

48:24.940 --> 48:27.540
and not in natural language, it must reference natural language

48:27.540 --> 48:29.300
because a key part of the reasoning that I'm doing

48:29.300 --> 48:30.660
is about how long those gaps are,

48:30.660 --> 48:32.220
about what words could go in those gaps,

48:32.220 --> 48:34.580
how they could semantically and syntactically

48:34.580 --> 48:35.860
with the words around them.

48:37.820 --> 48:39.980
And there are a lot of other tasks like this

48:39.980 --> 48:42.620
where the specification of some reasoning problem

48:42.620 --> 48:44.020
must in some way involve language.

48:44.020 --> 48:45.340
Maybe we're writing something

48:45.340 --> 48:47.100
that has to obey certain structural constraints

48:47.100 --> 48:49.340
like a poem or code.

48:50.260 --> 48:52.380
Maybe we're puzzling over a message from our advisor,

48:52.380 --> 48:53.700
trying to infer all the different meanings

48:53.700 --> 48:55.780
consistent with what they said.

48:55.780 --> 48:56.980
Maybe we're trying to figure out

48:56.980 --> 48:59.340
how to put together some words that we predict

48:59.340 --> 49:01.780
could achieve some desired effect in a listener.

49:03.140 --> 49:05.740
And beyond the fact that some inference problems

49:05.740 --> 49:07.780
implicate language and their specification,

49:07.780 --> 49:08.980
it seems like at least sometimes

49:08.980 --> 49:12.580
we sort of use language for thinking, right?

49:12.580 --> 49:15.980
Rubber duck debugging is when we successfully debug

49:15.980 --> 49:17.260
something that's been something us

49:17.260 --> 49:20.500
by just talking about it to ourselves or to a rubber duck.

49:22.180 --> 49:25.020
And I think this is the intuition also behind

49:25.020 --> 49:27.500
sort of chain of thought, scrap pad,

49:27.500 --> 49:31.380
those kinds of innovations in language model land.

49:32.540 --> 49:34.100
But one reason I'm drawing this distinction

49:34.100 --> 49:36.700
between task specification and algorithm

49:36.700 --> 49:39.820
is that this has long been a really important distinction

49:39.820 --> 49:41.140
in probabilistic modeling and inference.

49:41.140 --> 49:42.980
And it's something that I think we lose

49:42.980 --> 49:45.940
when we move just to asking a language model a question

49:45.940 --> 49:48.180
and hoping that it gives us the right answer.

49:48.180 --> 49:52.380
So in the kind of work that our lab does

49:52.380 --> 49:54.060
in modeling and inference,

49:54.060 --> 49:58.560
we sort of separately create a model probabilistic program

49:58.560 --> 50:00.180
that includes a task specification

50:00.180 --> 50:02.620
as a posterior distribution we wanna sample from

50:02.620 --> 50:04.020
and separately an inference program

50:04.020 --> 50:06.260
that compositionally encodes some kind of algorithm

50:06.260 --> 50:08.260
or strategy for solving that inference task.

50:08.260 --> 50:10.100
And when you use a probabilistic programming language

50:10.100 --> 50:11.740
to do this, you get some benefits

50:11.740 --> 50:14.700
from taking this approach of separating model inference.

50:14.700 --> 50:16.380
We know that we have soundness theorems

50:16.380 --> 50:18.500
guaranteeing that as computation increases,

50:18.500 --> 50:20.820
the inference is going to approach the posterior.

50:20.820 --> 50:22.340
We have automated tools and tests

50:22.340 --> 50:24.260
for measuring how accurate our inferences are

50:24.260 --> 50:27.060
relative to the model with finite computation.

50:27.060 --> 50:28.540
And we also have gradient estimators

50:28.540 --> 50:31.020
that help us tune any parameters of our inference programs

50:31.020 --> 50:33.860
to be better inference algorithms.

50:33.860 --> 50:35.820
And beyond being useful properties for engineering,

50:35.820 --> 50:37.300
these guarantees also reflect

50:37.300 --> 50:38.940
some key aspects of human cognition.

50:38.940 --> 50:41.900
We can often think more to reach more accurate conclusions.

50:41.900 --> 50:43.300
We can critically evaluate the extent

50:43.300 --> 50:45.260
to which our current hypotheses actually make sense

50:45.260 --> 50:46.900
given our model of the world.

50:46.900 --> 50:48.980
And if we repeatedly face the same kind of inference task,

50:48.980 --> 50:52.260
we can train ourselves to get better at solving it.

50:52.260 --> 50:53.620
So something we've begun to explore

50:53.620 --> 50:55.660
is whether adding LLMs to this picture

50:55.660 --> 50:58.460
might let us both specify various linguistic tasks

50:58.460 --> 50:59.900
as formal probabilistic models

50:59.940 --> 51:01.340
and enhance our inference algorithms

51:01.340 --> 51:03.060
by letting them do some of their thinking

51:03.060 --> 51:04.580
using languages as a tool.

51:04.580 --> 51:07.660
So I'll first talk about the modeling side of things.

51:07.660 --> 51:10.340
So we all know that an autoregressive language model

51:10.340 --> 51:12.180
defines a probability distribution

51:12.180 --> 51:13.500
over sequences of tokens.

51:14.420 --> 51:15.700
But we rarely just want to sample

51:15.700 --> 51:18.980
that unconditional distribution.

51:18.980 --> 51:21.380
You know, in the same way that in order to use a SAT solver,

51:21.380 --> 51:23.900
we need to reduce the problem we care about to a SAT formula,

51:23.900 --> 51:26.740
use a language model, we need to reduction

51:26.740 --> 51:28.900
from the task instance that we care about

51:28.900 --> 51:32.540
to a prompt.

51:32.540 --> 51:33.940
And the idea is that we're saying

51:33.940 --> 51:35.220
that the conditional distribution

51:35.220 --> 51:37.020
of the language model conditioned on that prompt

51:37.020 --> 51:39.260
is somehow a good specification of the task

51:39.260 --> 51:40.100
that we want to solve,

51:40.100 --> 51:42.020
or a good approximation of the task that we want to solve.

51:42.020 --> 51:43.580
But unlike the reductions to SAT,

51:43.580 --> 51:46.700
of course, this reduction is lossy.

51:46.700 --> 51:49.220
One problem is that sort of hard constraints,

51:49.220 --> 51:51.940
sort of instructions that we give the language model

51:51.940 --> 51:54.700
might be, you know, it might fail to follow them.

51:54.700 --> 51:57.060
So this conditional distribution, p-task,

51:59.140 --> 52:01.460
is not really the specification that we have in mind,

52:01.460 --> 52:03.620
it's just some close thing that we can get.

52:07.380 --> 52:09.820
Another problem is that the entropy of this distribution

52:09.820 --> 52:12.140
may not meaningfully reflect uncertainties.

52:12.140 --> 52:15.300
So you may have seen in the GPT-4 paper

52:15.300 --> 52:20.540
that on multiple choice tasks,

52:20.540 --> 52:22.900
where there's some multiple choice question

52:22.900 --> 52:26.060
and then the language model is asked to output A, B, C, or D.

52:26.060 --> 52:29.300
Before they did any RLHF and instruction tuning,

52:29.300 --> 52:31.460
if they create a calibration plot,

52:31.460 --> 52:34.980
where they plot sort of, you know,

52:34.980 --> 52:37.740
of all the answers in which GPT was, you know,

52:37.740 --> 52:41.580
0.4% confident, how often was that the correct answer?

52:41.580 --> 52:44.220
GPT-4 is strikingly well calibrated.

52:44.220 --> 52:46.140
And that's what you might expect from a model

52:46.140 --> 52:48.980
that's doing a very good job of next token prediction,

52:48.980 --> 52:50.980
of matching the distribution of language.

52:50.980 --> 52:53.260
When it's uncertain, the loss function is telling it,

52:53.260 --> 52:55.820
it should allocate its mass, its probability mass,

52:55.820 --> 52:57.380
according to that distribution.

52:57.380 --> 52:59.820
Whereas after RLHF, the calibration is shot.

52:59.820 --> 53:02.700
And this is also what you might expect,

53:02.700 --> 53:06.820
even if the humans who were sort of

53:06.820 --> 53:10.700
providing the human feedback in RLHF

53:10.700 --> 53:13.060
preferred the right answer, okay?

53:13.060 --> 53:17.980
The distribution that you get after performing RLHF

53:17.980 --> 53:20.780
with the objective that's commonly used for RLHF

53:20.780 --> 53:23.220
sort of creates a reduction in temperature.

53:23.220 --> 53:26.060
It's equivalent to reducing the temperature

53:26.060 --> 53:27.900
of the parts where the human feedback

53:27.900 --> 53:31.820
is exactly aligned with sort of the correct answer.

53:31.820 --> 53:33.540
And so it becomes overconfident.

53:34.500 --> 53:37.140
And, you know, this is very prompt dependent.

53:37.140 --> 53:39.060
I don't mean to say that this is like always gonna happen

53:39.060 --> 53:42.140
if you go and use GPT, but I went and used GPT-3.5

53:42.140 --> 53:44.300
to do this like infilling task,

53:44.300 --> 53:46.460
and it did it correctly, but also every time

53:46.460 --> 53:48.140
that I generated at temperature one,

53:48.140 --> 53:49.980
it gave me basically the same answer.

53:50.860 --> 53:53.660
So if I want to think of this distribution

53:53.660 --> 53:55.420
as sort of representing uncertainties

53:55.420 --> 53:57.100
that I can make decisions about whether to listen

53:57.100 --> 53:59.060
to my voicemail because it might contain things

53:59.060 --> 54:04.060
I don't know, this P-task is not up to that task.

54:04.260 --> 54:08.460
So our idea is to instead of reducing to a prompt,

54:08.460 --> 54:10.220
reduce to a probabilistic program

54:10.220 --> 54:12.860
that may call a language model,

54:12.860 --> 54:14.460
which is sort of a more flexible way

54:14.460 --> 54:17.020
of specifying what P-task distributions

54:17.020 --> 54:18.220
we want to sample from.

54:18.300 --> 54:20.380
And I know I'm running low on time,

54:20.380 --> 54:23.060
but the idea is that these models

54:23.060 --> 54:25.300
can mix calls to the language model

54:25.300 --> 54:28.220
with conditioning statements and other logic.

54:28.220 --> 54:31.580
So in this probabilistic program for this infilling task,

54:33.060 --> 54:36.540
it is in a loop going through each sort of blank

54:36.540 --> 54:38.660
in the that we need to infill,

54:38.660 --> 54:40.140
sampling a random number of tokens

54:40.140 --> 54:44.020
that should fill that spot, sampling those tokens

54:44.020 --> 54:47.700
and then observing the next sort of fixed fragment

54:48.660 --> 54:51.220
or conditioning on the next part being a fixed fragment.

54:51.220 --> 54:53.660
And this just lets us specify a model

54:53.660 --> 54:55.780
that doesn't just have a prefix prompt

54:55.780 --> 54:57.980
that sort of has a prompt with blanks in it.

54:59.100 --> 55:00.780
I haven't said yet how we're going to sample this,

55:00.780 --> 55:02.980
but the idea is that this defines a specification

55:02.980 --> 55:04.380
for the task that we want.

55:04.380 --> 55:08.140
And similarly, we have programs that sort of specify

55:08.140 --> 55:11.020
a variety of tasks that involve sort of thinking

55:11.020 --> 55:12.460
with language, right?

55:12.460 --> 55:13.940
We can condition on hard constraints

55:13.940 --> 55:17.020
if we want to parse into a formal grammar or write a high two.

55:17.020 --> 55:19.540
We can do kind of a product of experts model

55:19.540 --> 55:20.500
using multiple prompts.

55:20.500 --> 55:22.860
So maybe I want to think about a fun fact

55:22.860 --> 55:24.460
that's about both London and Paris.

55:24.460 --> 55:26.300
Well, you could just ask the prompt,

55:26.300 --> 55:28.700
hey, please give me a fun fact about both London and Paris,

55:28.700 --> 55:30.700
but you could also create a product of experts model

55:30.700 --> 55:32.300
where it has to come up with a completion

55:32.300 --> 55:34.580
that is both a completion to the sentence,

55:34.580 --> 55:36.060
a fun fact about London is,

55:36.060 --> 55:37.220
and a completion to the sentence,

55:37.220 --> 55:38.620
a fun fact about Paris is.

55:38.620 --> 55:41.420
And that's kind of a hybrid where the idea of and

55:41.420 --> 55:43.460
and both is symbolically encoded,

55:43.460 --> 55:46.100
but we're still using the language models representation

55:46.100 --> 55:49.860
of knowledge about fun facts and these things.

55:49.860 --> 55:52.860
Similarly, we can sort of represent reward steering

55:52.860 --> 55:55.420
or a classifier guidance by conditioning,

55:55.420 --> 55:58.020
by sort of soft conditioning on a reward function.

55:59.500 --> 56:01.020
And we can also include things like,

56:01.020 --> 56:04.220
hey, please generate a gloss of this code

56:04.220 --> 56:09.220
that when I try to semantic parse it back into code,

56:09.580 --> 56:12.700
gives me the same code I started with, things like that.

56:12.740 --> 56:16.700
So those are model programs for specifying various tasks.

56:16.700 --> 56:17.980
We need inference algorithms

56:17.980 --> 56:20.260
for actually sampling from these distributions.

56:20.260 --> 56:22.420
And so far we've been focusing

56:22.420 --> 56:24.620
on sequential Monte Carlo inference algorithms.

56:25.660 --> 56:27.780
And we kind of have a default version of this method

56:27.780 --> 56:29.260
and then fancier versions of this method

56:29.260 --> 56:31.100
that are necessary for harder tasks.

56:31.100 --> 56:33.840
In many ways, sequential Monte Carlo looks like beam search.

56:33.840 --> 56:36.300
You kind of keep multiple hypotheses around,

56:36.300 --> 56:38.900
you extend them, you reweight them

56:38.900 --> 56:40.780
according to model specific way,

56:40.780 --> 56:41.900
and then you resample,

56:41.900 --> 56:43.700
which is kind of like the part of beam search

56:43.700 --> 56:45.620
where you sort of down sample

56:45.620 --> 56:48.740
from your big expanded beam back to your beam size.

56:48.740 --> 56:50.780
But unlike beam search,

56:50.780 --> 56:52.260
sequential Monte Carlo,

56:52.260 --> 56:55.460
as you scale up the number of hypotheses that you're using,

56:55.460 --> 56:57.540
instead of converging to an arg max

56:57.540 --> 56:59.260
of your objective function,

56:59.260 --> 57:01.620
converges to the posterior distribution,

57:01.620 --> 57:03.820
sampling from the posterior distribution.

57:03.820 --> 57:06.220
And this sort of default version of SMC

57:06.220 --> 57:09.820
has worked for a few simple tasks that we've tried it on.

57:10.820 --> 57:13.900
For example, if I want a completion that follows

57:13.900 --> 57:15.380
my favorite physicists is probably,

57:15.380 --> 57:18.100
and my favorite writer is probably equally well,

57:18.100 --> 57:20.020
SMC can give me Richard Feynman.

57:20.020 --> 57:23.340
I really admire how he communicates complex ideas so clearly.

57:24.700 --> 57:27.220
Or if I want to finish the fed says,

57:27.220 --> 57:29.100
but only using words less than five letters,

57:29.100 --> 57:30.500
I get the fed says it will taper,

57:30.500 --> 57:33.340
but the rate hikes are still years away

57:33.340 --> 57:34.180
to something like that.

57:34.180 --> 57:35.860
And it's worth noting that if you do something

57:35.860 --> 57:38.420
like token masking to enforce this constraint,

57:38.460 --> 57:40.780
you just forbid the language model

57:40.780 --> 57:43.580
from generating anything that's longer than five letters.

57:43.580 --> 57:45.980
You get all sorts of weird completions.

57:45.980 --> 57:47.860
It's different from the posterior here.

57:47.860 --> 57:50.260
You get completions that set up an idiom

57:50.260 --> 57:51.260
that it could only complete

57:51.260 --> 57:52.860
if it used a word longer than five letters

57:52.860 --> 57:53.860
or something like that.

57:53.860 --> 57:55.540
And then it just gets very confused

57:55.540 --> 57:57.460
and right stop, that, that, read more or something.

57:57.460 --> 57:59.180
It tries to come up with some context

57:59.180 --> 58:01.300
in which the sentence would be cut off early.

58:02.540 --> 58:03.820
And for infilling tasks,

58:03.820 --> 58:07.660
we get a variety of samples that sort of fit semantically

58:07.700 --> 58:11.540
and syntactically with the text,

58:11.540 --> 58:14.500
but infilling tasks can be made much harder than this one.

58:14.500 --> 58:17.140
So I don't want to claim that this method yet solves

58:17.140 --> 58:18.220
all these infilling tasks.

58:18.220 --> 58:19.420
So for harder tasks,

58:19.420 --> 58:21.340
we think we're going to need to use fancier

58:21.340 --> 58:22.500
sequential Monte Carlo algorithms.

58:22.500 --> 58:24.540
And both of these sort of steps

58:24.540 --> 58:26.140
can actually be extended in various ways.

58:26.140 --> 58:28.900
We can use better proposal distributions

58:28.900 --> 58:32.300
and sort of better reweighting strategies

58:32.300 --> 58:34.340
that are trying to guess, okay,

58:34.340 --> 58:36.900
are we on the path to getting a good sample here?

58:36.900 --> 58:38.620
And we think that techniques

58:38.620 --> 58:40.140
that have already been developed in the literature

58:40.140 --> 58:43.460
for proposing good things in line with constraints

58:43.460 --> 58:46.340
or sort of discriminating whether we're likely

58:46.340 --> 58:51.140
to land in a constraint could be good ingredients to put here.

58:51.140 --> 58:54.660
But the important thing is, and this is the very end,

58:54.660 --> 58:56.540
the important thing is all of those things

58:56.540 --> 58:58.820
become part of the inference program.

58:58.820 --> 58:59.940
And there are still guarantees

58:59.940 --> 59:01.860
that as we scale up the number of particles,

59:01.860 --> 59:05.220
we're still targeting the original specification of the model.

59:05.220 --> 59:08.260
So all of those heuristics or biases don't sort of,

59:08.260 --> 59:09.740
we don't just trust them blindly.

59:09.740 --> 59:12.740
We don't hand the keys to those techniques.

59:12.740 --> 59:14.940
We still have a specification that we can understand.

59:14.940 --> 59:16.660
Okay, I'll stop there.

59:16.660 --> 59:17.500
Thanks.

59:17.500 --> 59:23.740
All right, we're a little bit behind time.

59:23.740 --> 59:25.780
So unless there is a burning question,

59:25.780 --> 59:27.740
burning question, no burning questions.

59:27.740 --> 59:31.980
Let's break for tea and maybe Zachertorte

59:31.980 --> 59:35.820
and you guys can talk to the speakers.

59:35.820 --> 59:39.620
And at 11.30, we are going to...

