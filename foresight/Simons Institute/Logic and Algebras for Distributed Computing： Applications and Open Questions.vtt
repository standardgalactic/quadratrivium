WEBVTT

00:00.000 --> 00:08.000
Second talk today has a title that has evolved since last time I looked at it.

00:08.000 --> 00:17.280
Okay, but it has logic and algebras and we are very excited to hear what this has to do with cloud computing,

00:17.280 --> 00:22.080
which we are all subjected to in our daily lives.

00:22.080 --> 00:31.440
Thank you. So this will be a bit unusual, both Joe and Connor will talk and I let them take

00:31.440 --> 00:38.640
care of the logistics of that. Again, remember that for questions that might be more appropriate

00:38.640 --> 00:43.360
for a longer discussion, we will have that discussion right after their talk.

00:45.840 --> 00:51.680
Okay, thanks. So this is work that obviously we're doing here at Berkeley and I'm also funded by

00:51.680 --> 00:55.840
Sutter Hill Ventures. So it's kind of cool. We've got venture capitalists funding basic research.

00:55.840 --> 01:00.240
I have promised them that there's no applicable. I'm not really sure what this could turn into

01:00.240 --> 01:04.400
as a company, but they're cool and they've given us developers to work on the project,

01:04.400 --> 01:08.000
which has just been great and they're funding me as well. So thanks to them and to Berkeley.

01:08.800 --> 01:12.480
There's this story people like to tell in computing. This is my standard opening slides.

01:13.920 --> 01:16.960
Operating systems people really like this story because it's sort of the Thompson and

01:16.960 --> 01:21.200
Richie Turing Award story. For every platform that comes out, there's a programming environment

01:21.440 --> 01:25.920
that's somehow suited to that platform that emerges and as a result, people write things you

01:25.920 --> 01:30.720
never would have expected on that platform and it succeeds. So the PDP 11 with Unix and C

01:30.720 --> 01:34.800
is the canonical example of this, but one can argue that in every generation of new kind of

01:34.800 --> 01:40.560
computing platforms, programming environments have arisen to allow people to build an app for that.

01:41.840 --> 01:45.760
And so nobody expected all the apps we have on our phones. It's wonderful. Developers were

01:46.400 --> 01:50.560
freed to write all sorts of things. Strangely, there's a platform that's as old as the iPhone

01:50.560 --> 01:56.000
called the cloud. So AWS is approximately the same age as the iPhone, but it doesn't have a

01:56.000 --> 02:01.680
canonical programming model. And there's many reasons why that might be, partly because it's

02:01.680 --> 02:05.680
a really hard programming environment, yes. So it has to deal with all the problems of parallel

02:05.680 --> 02:11.600
computing, as well as things like distributed consistency, what happens when you have partial

02:11.600 --> 02:15.600
failures in your system, but it keeps running. So componentry is down, but the system's still

02:15.600 --> 02:20.400
running. And then in the modern cloud, we want things to auto scale. So you allocate more machines

02:20.400 --> 02:24.160
and then you free up some machines, but the program's still running. So the platform is

02:24.160 --> 02:29.360
changing underneath you as you're executing. So this is all hard and programmers right now are

02:29.360 --> 02:34.640
trying to do this essentially in Java. That's sort of the state of the art. And the annoying thing

02:34.640 --> 02:40.000
is these compilers for these languages don't answer any of these questions that are hard. So I think,

02:40.000 --> 02:44.640
honestly, this is like this hole in computer science that nobody's filled and it seems like

02:44.640 --> 02:48.720
one of our grand challenges from my perspective. So I've been working on it for a long time,

02:49.360 --> 02:53.120
and I think there's still a lot of work to do. I take inspiration, of course, from this gentleman

02:53.120 --> 02:58.160
as maybe we all do. What was cool about Ted Codd was he said, look, you should write things in a

02:58.160 --> 03:02.000
formal language. You should have formal specifications. And then there should be machinery that

03:02.000 --> 03:06.960
automates the implementation. And if we do that, then the implementation can change

03:06.960 --> 03:11.520
while the specification remains the same. This is very nice for things like databases, right?

03:11.520 --> 03:17.520
So the thing is that Codd was trapped in this database prison for all these years. And I think

03:17.520 --> 03:22.560
there's a much broader applicability of the design principle. So we worked on things in our

03:22.560 --> 03:26.800
community like declarative networking. So we brought Codd out of the database and into the network.

03:27.760 --> 03:31.840
So I've done some work on that. Many of us I think in this room have done something around

03:31.840 --> 03:36.560
declarative data science and machine learning. This is a growing area, right? In the program

03:36.560 --> 03:41.120
analysis community, the use of declarative languages has been pretty powerful. So that's

03:41.120 --> 03:45.360
really cool. And then, of course, the hot thing, which is why we're all here, is that we're going

03:45.360 --> 03:49.840
to start to try to look at this stuff through the lenses of algebras instead of logic or in addition

03:49.840 --> 03:54.240
to logic, which is pretty neat. And we're going to, you know, we've heard or are hearing about a

03:54.240 --> 04:00.000
variety of different algebras that people are playing with in this domain. So what I'm interested in

04:00.000 --> 04:04.480
is taking Codd into the cloud, yeah? And here's sort of the analogy, the way to think about it.

04:04.480 --> 04:09.280
The relational database was invented to hide how data is laid out and how queries are executed,

04:09.280 --> 04:14.000
right? And all that should be decided sort of lazily based on the current environment.

04:14.000 --> 04:19.440
Well, the cloud is just a generalization. It was invented to hide all your computing resources

04:19.440 --> 04:23.200
and how they're laid out. So not just your blocks on your desk, but really everything.

04:23.760 --> 04:29.520
And it's for general purpose computations. So the cloud, in a lot of ways, is this abstraction.

04:29.520 --> 04:34.960
It's this physical layer abstraction. The physics of the deployment of your code is going to change,

04:34.960 --> 04:38.160
but you want your spec to remain the same. That's how you'd really like to program in an

04:38.160 --> 04:43.920
environment that is this heterogeneous and elastic. So I believe that it's extremely natural for

04:43.920 --> 04:48.560
techniques that we've been working on in our community to try to be applied to cloud computing.

04:48.560 --> 04:52.560
And we have a project in my group called Hydro, which you can read more about,

04:52.560 --> 04:58.480
which I will tell you a bit about today. Okay. So what are my goals? Well, I kind of want to

04:58.480 --> 05:04.080
build something like LLVM for the cloud. So LLVM, as you may know, is a very successful sort of

05:04.160 --> 05:10.320
language stack. It supports many languages, including C++ and Rust and Swift and others.

05:11.040 --> 05:14.720
It has an internal language called its internal representation, and then it compiles down to

05:14.720 --> 05:19.920
a variety of machine code for different platforms. So it's been extremely successful, but it doesn't

05:19.920 --> 05:24.480
answer any distributed questions. So if you're writing a distributed program, you might ask a

05:24.480 --> 05:29.040
question like, is my program consistent in some sense? Or if I talk to different machines in the

05:29.040 --> 05:33.440
network, will they give me different answers and be confused? That's a question that distributed

05:33.440 --> 05:38.080
programmers need to deal with. Here's another one. My state no longer fits on one computer. How do

05:38.080 --> 05:41.760
I partition it across multiple computers while getting the same answer out of my program?

05:42.320 --> 05:47.920
I want you, you compiler should figure that out for me. What failures can my system tolerate and

05:47.920 --> 05:51.360
how many of them before it stops working the way that the spec declares it should?

05:52.560 --> 05:57.680
What data is going where around the world and who can see it? These are all questions distributed

05:57.680 --> 06:02.800
systems always have to answer. And then I have different objective functions. So I'd like to

06:02.800 --> 06:08.240
optimize some days for maybe my dollar spend in the cloud, but I don't care about latency or maybe

06:08.240 --> 06:15.360
vice versa. Maybe I care about particular latency distribution. So I want the 99th percentile

06:15.360 --> 06:20.800
of my workload to achieve a certain latency versus the 95th or what have you. These will all

06:20.800 --> 06:26.640
lead to different decisions about resource allocation and program structure and so on, right?

06:26.640 --> 06:31.440
And if you ask these questions of LLVM, that's the answer you get. It doesn't deal with any of

06:31.440 --> 06:36.960
these issues. And that's kind of where we'd like to come in. We've written a vision paper a couple

06:36.960 --> 06:42.560
years ago that I can point you to and I won't go through all of it today. But the idea is to use

06:42.560 --> 06:48.640
database techniques and ideas to optimize in concert with LLVM. So LLVM is responsible for the

06:48.640 --> 06:52.960
single node, but database techniques perhaps responsible for the messaging, the data movement

06:52.960 --> 06:57.920
that happens in a distributed program. So here's how we envision the hydro stack. Many programming

06:57.920 --> 07:04.160
languages up top. Some techniques to translate them into an internal representation. We've

07:04.160 --> 07:08.640
got a little bit of initial work here. And then the internal representation should be some

07:09.920 --> 07:15.440
formal spec that is global in some sense. So it doesn't worry yet about how many machines I have

07:15.440 --> 07:21.360
or what the machines can do. It's just a formalized specification of what you wrote in a perhaps

07:21.360 --> 07:25.360
imperative language. Okay, so it's kind of machine oblivious. Maybe it's a logic. Maybe it's an

07:25.360 --> 07:29.840
algebra. Things that are in red are work in progress. Things that are in green kind of work

07:29.840 --> 07:36.080
at this point. And then from there we want to build a compiler and we're working with Max

07:36.080 --> 07:42.080
on using eGraphs for that compiler to translate down into a per node physical algebra. So every

07:42.080 --> 07:46.720
machine would run its own little program. Those programs communicate with each other very much

07:46.720 --> 07:51.280
like a parallel query plan is a bunch of individual query plans running on individual machines talking

07:51.280 --> 07:56.640
to each other over a network. Okay, so this is a sort of per node physical algebra because it's

07:56.640 --> 08:01.840
actually doing stuff. We've implemented this in Rust. It's very fast and I'll show you some of

08:01.840 --> 08:06.640
this today. So that's kind of what we envision as how this is all going to work. At the bottom

08:06.640 --> 08:10.560
there's something that's deploying this on machines and deciding how many machines and how few over

08:12.400 --> 08:16.000
time. Okay, so some of the topics I want to talk about today we're going to focus on this piece of

08:16.000 --> 08:22.640
the stack. Things in red are work in progress. Very much more questions than answers for sure.

08:22.640 --> 08:27.280
So how do we take code and automatically replicate it along with its associated state or data

08:27.920 --> 08:31.840
while ensuring that the program continues to produce the same outcomes as it would have on a

08:31.840 --> 08:36.560
single machine? So I'm particularly interested in doing this in cases where the replication comes

08:36.560 --> 08:41.840
for free and the individual machines don't have to coordinate with each other in a technical sense

08:41.840 --> 08:46.640
I'll talk about. But we'd like to avoid replication when we can. We'll call that free replication and

08:46.640 --> 08:51.840
this is the domain of the calm theorem which you may have heard of and I will review. Unfortunately,

08:51.840 --> 08:59.680
the calm theorem was done in a very particular framework for the proofs. It's not at all clear

08:59.680 --> 09:04.000
how it applies outside that framework and so what we'd really like is a more algebraic notion of the

09:04.000 --> 09:08.400
calm theorem which is something that Connor's working on and after the talk if you're interested

09:08.400 --> 09:13.440
come find Connor and or me to talk about that. Another topic that Connor's going to talk about

09:13.440 --> 09:18.320
today is termination detection and again ideally termination detection where I can decide it locally

09:18.320 --> 09:23.200
for free without asking anyone else. So how do I know in a streaming program that it's done

09:23.760 --> 09:29.200
when there's other agents in the world? So we're going to talk about how to do that with threshold

09:29.200 --> 09:33.920
morphisms but Connor's got ideas about more general notions of equivalence classes that may

09:33.920 --> 09:38.000
allow termination in more settings. So he'll give you a flavor of this work in progress.

09:39.120 --> 09:42.880
The third piece we may or may not have time for today but my student David Chu has been working

09:42.880 --> 09:47.600
on this how do you take a program and partition the state of the program across machines if the

09:47.600 --> 09:52.240
state doesn't fit on one machine. So you really need to partition the code and the data. This is

09:52.240 --> 09:57.440
very much like traditional shared nothing parallel databases if you like but we want to do this to

09:57.440 --> 10:02.880
full and rather complex data log programs and so we've got an implementation of Paxos which is

10:03.360 --> 10:07.520
number of lines of data log where David's able to do some of this automatic partitioning.

10:08.320 --> 10:12.400
Functional dependencies have a role to play here and it would be nice to integrate those into an

10:12.400 --> 10:18.480
algebraic frame as well. And of course all this has to go fast and ideally as fast as

10:18.480 --> 10:23.520
handwritten C++. I'm really previous iterations of my work we settled for interpreters and just

10:23.520 --> 10:28.320
showing things were possible. Now we'd like to convince the systems people that things will be

10:28.320 --> 10:34.560
as fast as they want them to be. So is this business just pie in the sky?

10:36.080 --> 10:42.720
Let's see. All right so we're building on an obsession of some 10 to 12 years that I've had

10:42.720 --> 10:47.600
now in my third group of grad students working in this area. So the initial batch of grad students

10:47.600 --> 10:53.040
was doing formalisms. So we have this very nice logic called Daedalus which is a subset of data

10:53.040 --> 10:58.000
log neg actually that allows you to talk about time and space in a particular way and it's got

10:58.000 --> 11:03.680
a nice model theory. And so that works all there and we can use that as a basis for you know

11:03.680 --> 11:10.000
our semantics to start which is nice. And then there was this lovely work that Tom Amalut did at

11:10.000 --> 11:15.280
Hasselt the column theorem which was a conjecture I had and he and colleagues went ahead and proved

11:15.920 --> 11:20.720
that talks about how things are monotone in some certain sense. Then you can do this free

11:21.840 --> 11:26.640
replication. So you don't need to do coordination to get replica consistency. So I will talk about

11:26.640 --> 11:31.200
the column theorem today to give a review. And then we actually built out a language. It was

11:31.200 --> 11:36.240
slow. It was interpreted. It was written in Ruby but it integrated lattices into a data log like

11:36.240 --> 11:40.560
language and we were able to show that you can get stratified negation and aggregation. You can

11:40.560 --> 11:45.520
get morphisms on your lattices to allow you to do semi-naive evaluation even with the lattices.

11:45.520 --> 11:49.920
So it's really actually rather a nice mixture of algebra and logic. None of this was formally

11:49.920 --> 11:54.000
proved but it was I think one of the earlier systems to observe that you could build this.

11:54.080 --> 11:58.480
That was pretty cool. And that's Neil Conway's thesis work. So we have this as a basis. This kind

11:58.480 --> 12:03.600
of ground zero for my team. And then what happened is I had a batch of students that didn't want to

12:03.600 --> 12:08.640
do languages or theory. So they just built stuff in the spirit of these ideas. And I'm not going

12:08.640 --> 12:13.920
to go through all this but it's things like functions as a service and protocols and testing

12:13.920 --> 12:19.440
and provenance. It's cool stuff. What I do want to focus on is one of those projects which was a

12:19.520 --> 12:24.800
key value store. Key value store is just like a hash table. It's a database where you look

12:24.800 --> 12:29.040
things up by key and you get a value. So you can think of it as a distributed hash table.

12:29.680 --> 12:33.520
These are like the Petri dishes of distributed systems. You're basically saying I have a memory.

12:33.520 --> 12:37.840
It's distributed across many computers. It may be replicated. It may be partitioned. But that's

12:37.840 --> 12:43.840
what I have. It's just registers with values, keys with values. So there's no algorithms per se.

12:43.840 --> 12:48.080
It's all just kind of like focusing on these semantic issues about replication and partitioning.

12:49.040 --> 12:53.280
But the idea that was behind the Anna key value store that we built was everything's a

12:53.280 --> 12:58.720
semi-ladys. And because everything's a semi-ladys and therefore associative, commutative, and item

12:58.720 --> 13:03.280
potent, messages in the network can be replicated. They can be interleaved. They can be reordered and

13:03.280 --> 13:08.080
everything will be fine. So if you design with semi-ladys from the bottom up, you can build a

13:08.080 --> 13:12.720
system that does no coordination. So it's fully monotonic, which means everything can be replicated

13:12.720 --> 13:17.600
as much or as little as you like. Across the globe, between disks and memory, you can replicate

13:17.600 --> 13:22.720
lots of ways. You can do updates anywhere. So multiple updates to the same key can be happening

13:22.720 --> 13:28.800
concurrently in different places at the same time. And they are merged by the lattice lazily via gossip.

13:28.800 --> 13:32.880
And so you build this thing with no concurrency control whatsoever. So there's no locks in the

13:32.880 --> 13:37.600
system. There's no calls to atomic instructions on processors. There's certainly no Paxos protocols

13:37.600 --> 13:42.560
or anything like that. It's just put, get, and gossip. It's really a simple piece of software.

13:43.520 --> 13:47.440
And so Chenggang Wu, the student who led this, won the dissertation award. I think it was very

13:47.440 --> 13:52.320
well deserved because this system was really elegant and really fast. So to give you a sense

13:52.320 --> 13:56.720
of kind of the lattices that he uses, he took from the literature a variety of these kind of

13:56.720 --> 14:00.800
consistency models that people talk about in distributed systems. And he showed how you can

14:00.800 --> 14:05.840
get them by building little composite lattices that wrap up the data. So this is what's called

14:05.840 --> 14:10.800
last-rater wins in the distributed systems, which is just whenever you see a value that's

14:10.800 --> 14:15.440
from later than your value, you update, otherwise you don't. And this you just, you know, take your

14:15.440 --> 14:21.600
map, which is from keys to things, and you wrap the things in a lexical pair of a clock or a version

14:22.160 --> 14:28.160
and the data, right? And you can only have one value per version. So this, this works out as a

14:28.160 --> 14:34.240
lattice. Here's a fancier one, though. This is actually one of the strongest forms of consistency

14:34.240 --> 14:38.560
you can get without coordination. It's called causal consistency. And here what you have is for

14:38.560 --> 14:44.240
every key, you have a vector clock and the data. And the vector clock itself is a map lattice with

14:45.120 --> 14:47.440
node IDs and counters. Yes.

14:47.440 --> 14:50.480
Just want to say a little bit sort of operational transform.

14:50.480 --> 14:52.080
A little bit. Yes.

14:52.080 --> 14:56.880
Can you ever get stuck? Like, do repairs always exist or do you set it up such that they do?

14:56.880 --> 15:03.200
So these, these particular well trodden forms of consistency work fine. And these lattices

15:03.200 --> 15:06.800
are capturing that. They're saying, look, it's just merge. All right. And it's always going to work

15:06.800 --> 15:11.600
because you've defined an associative commutative item potent merge function. OTs are like really

15:11.600 --> 15:16.640
weird and full of all sorts of reasoning I don't understand. And they would never be able to have

15:16.640 --> 15:20.640
such a simple assertion of correctness. All I'm saying here is it's associative commutative

15:20.640 --> 15:24.880
of an item potent. I got nothing more to say. It takes a little bit of convincing to say that

15:24.880 --> 15:29.040
gives you causal consistency, but it's not much convincing because this helps you make causal

15:29.040 --> 15:33.840
consistency with vector clocks. So the observation that clocks and vector clocks are lattices is

15:33.840 --> 15:37.360
just a nice thing about distributed programming. Yeah.

15:38.080 --> 15:40.960
So what do you mean by everything is a lattice? So you mentioned that

15:41.920 --> 15:43.840
Kira's story is a hash map. Right.

15:44.720 --> 15:51.760
Every key has to be a lattice thing. So the nice thing about the map lattice is the keys

15:51.760 --> 15:57.280
are not lattice values. The keys are just keys. The whole table is a lattice. But the object is

15:57.280 --> 16:01.680
a lattice because what happens is the merge function is for a particular key. If there's

16:01.680 --> 16:05.920
nothing, it gets the value you gave. Or for that key, if there's something there, you apply the merge

16:05.920 --> 16:12.000
function of this lattice. So that is itself a lattice. And these lattice constructors are very

16:12.000 --> 16:16.400
nice. We use map lattice. You see lexical pair over there. And these allow you to take simple

16:16.400 --> 16:20.560
lattices like sets and counters and build up richer lattices out of them, which is a trick

16:20.560 --> 16:25.600
our group likes to play a lot. Other groups sort of are doing research on inventing custom lattices

16:25.600 --> 16:29.520
for custom problems. We've been very much in this kind of know let's just build it up from very

16:29.520 --> 16:39.440
simple building blocks. So the quick version is it's monotone. And if it's monotone, the column

16:39.440 --> 16:43.360
theorem says it's going to be free replication. So you don't have to do coordination to get

16:43.360 --> 16:47.920
replicated consistency. Connor's going to give you a longer talk about this when we get to

16:48.960 --> 16:52.880
conversation about semi-lattice. It's a semi-lattice. I should be clear. It's a semi-lattice.

16:53.760 --> 17:00.240
Do you know whether the people working on coordination free replicated data structures

17:00.240 --> 17:05.280
are well-pure? Yes, they are aware. And Connor will talk about it soon. Yeah, that will come up for

17:05.280 --> 17:11.840
sure. Good. So just to kind of close out this anecdote with Anna, the system is ridiculously

17:11.840 --> 17:16.720
fast. And it's especially ridiculously fast under contention relative to other systems.

17:16.720 --> 17:20.560
So we compared against things like mass tree, which is from Harvard. It's 80 colors,

17:20.560 --> 17:24.320
very fast key value store. We also compared against the multi-threaded

17:24.320 --> 17:28.400
hash table that comes from Intel in their thread building blocks library. That's TBB.

17:28.400 --> 17:33.360
And under contention, those systems, if you look down here, spend most of their time trying and

17:33.360 --> 17:38.160
failing to get atomic instructions. So they'll say test and set on a particular memory address,

17:38.160 --> 17:42.800
and they'll be told no, you have to try again. And they'll spend 95% of their time under contention

17:42.800 --> 17:47.680
doing that, not doing useful work. So they're at 5% good put, if you're familiar with that term.

17:48.400 --> 17:52.000
Whereas Anna, because it does no concurrency control, is just doing puts and gets and puts

17:52.000 --> 17:56.800
and gets and puts and gets and spending most of its time doing good put. And that's why Anna can be

17:57.360 --> 18:01.200
700x better throughput under high contention than these other systems.

18:02.160 --> 18:06.880
But also because it does no coordination scales almost perfectly linearly across threads,

18:06.880 --> 18:12.400
and then across machines, and eventually across the globe. There's really nothing to keep it from

18:12.400 --> 18:16.800
scaling linearly, because the only extra work it has to do is some gossip. And that can be done

18:16.800 --> 18:20.800
in the background, and can be done as lazily as you like without breaking the semantics.

18:21.440 --> 18:24.720
So there's maybe a little fudging here on how stale your data is, but it's correct.

18:26.640 --> 18:32.160
So this was a crazy fast system. And the thing about this, oh, and if you try to run it in the

18:32.160 --> 18:35.840
cloud, it's also incredibly cheap to run relative to systems that are wasting all their time doing

18:35.840 --> 18:40.560
this business. They're charging you for this. They're trying to get locks, they're waiting on

18:40.560 --> 18:45.920
locks, and they're charging you money. So you'd like to avoid that if you can. Okay, that's all

18:45.920 --> 18:51.040
very nice. But it was written in C++ by Chenggang, who's an excellent coder. His implementation is

18:51.040 --> 18:56.320
correct by assertion. It would be really nice to be able to kind of do what CAD wants us to do,

18:56.320 --> 19:00.000
formalize a spec that is correct, and then synthesize an implementation from it through

19:00.000 --> 19:04.320
rules that are correct transformations. So we'd really like to do that, and we'd like to maintain

19:04.320 --> 19:10.320
the speed. What kind of formalisms? Well, we're using lattices mostly. So maybe we could have a

19:10.320 --> 19:15.520
type system that starts with some basic semilattices, like sets and counters, some composition

19:15.520 --> 19:20.560
lattices, like key value pairs, products, lexical products, which aren't always lattices. So you

19:20.560 --> 19:25.040
have to, there's some constraints on whether a lexical product is a lattice. And then we want

19:25.040 --> 19:30.640
like a data flow, like a query plan algebra. So you can imagine a semi-ring kind of algebra, but

19:30.640 --> 19:34.480
you know, there's going to be maps and folds, and then there's going to be physical stuff,

19:34.480 --> 19:38.800
like scan a collection, or get stuff over a network. You know, networks do weird things,

19:38.800 --> 19:43.840
like they permit things, and they form batches of things. They parenthesize streams, if you will.

19:44.800 --> 19:48.720
They multiplex and de-multiplex messages. So there's some physical stuff that we want here too,

19:48.720 --> 19:52.960
and I'd like to really be able to prove all that stuff is correct in a meaningful way.

19:54.320 --> 19:58.960
So just for fun, I don't expect you to read this. This is the ANA implementation. You just saw

19:58.960 --> 20:03.520
written in our low-level hydroflow language. This is the whole thing. It's a very simple program.

20:04.720 --> 20:09.040
And you can see this is kind of a data flow language. It's basically specifying graphs of

20:09.040 --> 20:13.920
data flow. The edges are directed edges in a graph. The words are nodes in the graph,

20:13.920 --> 20:21.360
and you'll see familiar operators like map and join, and cross-join, and so on.

20:21.360 --> 20:26.160
All right, and you can give views names. So this is a little name of a subgraph,

20:26.800 --> 20:31.040
and we use it, and so on. So it's just a little language for specifying graphs.

20:31.040 --> 20:37.680
This is a picture of that program that's output by the system. Okay, so it's a one piece of paper

20:38.240 --> 20:46.080
program. So in this particular program, their union is actually joined, semi-latest join,

20:47.440 --> 20:55.360
and that might be the only one. Yeah. Okay, and so just to convince ourselves this is fast,

20:55.360 --> 21:01.760
that's Chengang's original numbers. We have it now running through hydro, that implementation you saw

21:01.760 --> 21:07.120
on very similar machines, and we get very similar performance to the handwritten code. So we're

21:07.120 --> 21:14.000
feeling pretty good that we're hitting our goals for performance. And because this graph is green,

21:14.000 --> 21:17.920
it's telling us that this thing is all monotone, and therefore consistently replicable.

21:18.720 --> 21:23.280
And at a glance, we can see this is a safe program. And I'm sort of cheating at this point,

21:23.280 --> 21:27.920
and I'm going to confess to that. There's, I think, more work we need to do to make this robust. I

21:27.920 --> 21:33.120
think these green edges are kind of a, they're slightly bi-assertion at this point. So I would

21:33.120 --> 21:37.600
like to make them more fundamentally correct, and hopefully we'll have time to talk about that later.

21:38.720 --> 21:42.880
Okay. With that, I'm going to hand off to Connor. He's going to take us through the next chapter.

21:46.880 --> 21:54.160
Hello. People hear me? Yeah. I'm Connor. I'm a PhD student here working on hydro. I like systems

21:54.160 --> 21:57.440
and theories. So I thought I'd show you guys some of the theory stuff we've been thinking about,

21:57.440 --> 21:59.840
see if anyone has any thoughts, wants to collaborate on anything.

22:04.960 --> 22:10.240
Okay. So in the classical database lens, we have, you know, these three layers,

22:10.240 --> 22:15.680
the relational calculus at the top, relational algebra in the middle, and then a physical

22:15.680 --> 22:21.200
algebra at the bottom, concerned with things like hashing and sorting and so on. And we can think

22:21.200 --> 22:26.000
about how this changes when we move to the cloud setting. And there's good news and bad news on

22:26.000 --> 22:30.080
the current state of affairs when we move to the cloud setting. The good news is that,

22:30.080 --> 22:33.760
like Joe said at the top, we have this Daedalus language from the Bloom project

22:33.760 --> 22:40.160
that is a data log like dialect for distributed systems. The bad news is that developers are

22:40.160 --> 22:45.120
not asking for a data log dialect to build distributed systems in. The developers we've

22:45.120 --> 22:48.640
talked to are a lot more interested in a functional algebraic looking interface and

22:48.640 --> 22:55.520
especially something Pythonic looking like pandas. On the algebra side, the good news is that there

22:55.520 --> 22:59.840
is an algebraic model for distributed systems today. It's the semi-ladis model that Joe has

22:59.840 --> 23:04.560
mentioned that is referred to as CRDTs in a lot of places, especially in the programming languages

23:04.560 --> 23:10.000
community. The bad news is that this is a model for coordination-free updates of state and it

23:10.000 --> 23:14.880
doesn't actually have a query language or give guarantees about coordination-free-ness of queries

23:14.880 --> 23:21.680
today. And then at the physical layer, when we add a network to the situation, asynchronous

23:21.680 --> 23:25.680
computer network, a lot of non-determinism emerges that we need to be able to handle,

23:25.680 --> 23:29.600
in particular reordering, batching, and duplication of messages.

23:33.040 --> 23:39.200
So what we'd like to get to is unifying formalism across logic and algebra and this

23:39.200 --> 23:45.920
physical algebra and have correctness at the physical layer that we can prove for

23:45.920 --> 23:52.560
safe against this non-determinism from the network. And we're able to capture things like

23:52.560 --> 23:57.920
replication, partitioning, batching, incrementalization, and termination analysis. We'll talk about

23:57.920 --> 24:08.080
more later. All right, so let's talk about semi-ladis' CRDTs. So this is a model for

24:08.080 --> 24:12.000
distributed systems that in databases we usually call the semi-ladis model. It's what is called an

24:12.000 --> 24:17.040
Anna in Blum L. It came out of the programming languages community and there it's usually

24:17.040 --> 24:23.680
referred to as CRDTs. It stands for conflict-free replicated data types. It's introduced in this

24:23.680 --> 24:31.520
paper here and there's over 179 papers about CRDTs out there. It's also started to get popular

24:31.520 --> 24:36.400
amongst software engineers and you see people talk about this CRDT model on places like Hacker News,

24:36.400 --> 24:42.880
people starting startups with it. So what does it do? It tries to handle the sources of

24:42.880 --> 24:48.640
non-determinism that come from an asynchronous computer network. These are the arbitrary batching

24:48.640 --> 24:56.320
of messages, arbitrary reordering of messages, and arbitrary duplication of messages. And so it

24:56.320 --> 25:00.240
turns out if you want to be robust to these three things, these actually correspond to algebraic

25:00.240 --> 25:05.120
properties that you need to give you that robustness. So associativity gives you robustness to batching.

25:05.120 --> 25:09.680
You're indifferent to the parenthesization of messages. Communicativity gives you robustness

25:09.680 --> 25:15.600
to reordering and idempotence gives you robustness to duplication. And if you have a set with an

25:15.600 --> 25:19.520
operator that satisfies these three properties, that gives you a semi-ladis. So that's why we're

25:19.520 --> 25:27.280
talking about semi-ladises for distributed systems. So the conflict-free replicated data type is one

25:27.280 --> 25:32.160
specific model of a semi-ladis interface, but since it's the most popular one today,

25:32.240 --> 25:38.560
I'm going to talk about it. So the idea is that it's an object-oriented view of a distributed

25:38.560 --> 25:42.080
system where you define some object, and you're going to define three methods on that object.

25:42.720 --> 25:45.760
And then you can replicate this object across the distributed system,

25:45.760 --> 25:51.840
and the replicas will converge, regardless of network non-determinism. So you have your merge

25:51.840 --> 25:58.560
operator, which is your associative, commutative, and idempotent ACI semi-ladis operator, which

25:58.560 --> 26:04.960
combines the state of two replicas. You have a way to update state, which the requirement is just

26:04.960 --> 26:09.760
that that's monotone with respect to the partial order induced by this merge operation. And then

26:09.760 --> 26:14.960
you have a query, which is just a method on this object, but today there's not a specific query

26:14.960 --> 26:19.440
language. You don't have any sort of guarantees on what that query does. It just reads this semi-ladis

26:19.440 --> 26:27.200
state. So we're looking at an example of a CRDT. This comes from the Amazon Dynamo paper for how

26:27.200 --> 26:32.720
they're implementing shopping carts. And the idea is that you have updates that are going to add or

26:32.720 --> 26:36.560
remove elements to your shopping cart. In this case, you add a Ferrari to your shopping cart, add a

26:36.560 --> 26:41.920
potato, and you can also remove the Ferrari. And the state is going to be represented as two sets,

26:41.920 --> 26:48.080
a set of items that you've inserted and a set of items that you've removed. And then to merge,

26:48.080 --> 26:52.720
we do a pairwise union of these two sets. And the query, what's actually in my shopping cart I want

26:52.720 --> 26:59.840
to check out, is set difference. Subtract the removes from the inserts. And there's a few

26:59.840 --> 27:05.200
interesting things going on with this example. One is we're guaranteeing the coordination-free

27:07.200 --> 27:13.520
rights that these two states are going to converge. But our query is a non-monotone query,

27:13.520 --> 27:18.160
which the column theorem tells us is not a coordination-free query. So CRDTs are not giving

27:18.160 --> 27:23.120
us the invariant that the column theorem requires on queries, which is that if we output a tuple

27:23.120 --> 27:28.160
at a certain point in time, we're never going to retract that tuple in the future. Here, over time,

27:28.160 --> 27:36.080
we will retract tuples as the remove set grows. We had a vision paper in BLDB this year about

27:36.080 --> 27:41.280
this gap between what CRDTs guarantee and what the column theorem guarantees and ideas for how to

27:41.280 --> 27:47.760
resolve it. Another thing you might have noticed that's kind of odd about that representation

27:47.760 --> 27:54.320
of data is that if you think about how we might represent updates like this to a shopping cart

27:54.320 --> 27:59.200
in a database, you might have imagined that we would have a count on each item and we would

27:59.200 --> 28:02.720
increment that count when we add an item and we decrement that count when we remove an item.

28:02.720 --> 28:06.720
This is what you'd see, something like incremental view maintenance where your update

28:06.720 --> 28:12.400
operation forms an abelian group, not a semi-ladis. So why not do something like that?

28:13.280 --> 28:19.520
Well, for one, it doesn't form a semi-ladis and it's not immediately obvious how to convert it

28:19.520 --> 28:25.760
into one. So this representation is two sets, what you call a two-phase set. It's more obviously

28:25.760 --> 28:32.560
monotonously growing update operation, but it turns out it actually is possible to convert

28:32.560 --> 28:38.400
this abelian group representation into a valid semi-ladis in terms of being robust to network

28:38.400 --> 28:43.760
non-determinism. I won't go into all the details on that, but it's based on what Joe is saying with

28:43.760 --> 28:52.000
these vector clocks where you wrap the states basically in a vector clock which forms a semi-ladis.

28:53.200 --> 28:57.680
The downside of doing this is that vector clocks require linear memory and the number of replicas

28:57.680 --> 29:02.720
in the system, so that's the reason why people wouldn't use this representation today. But we

29:02.720 --> 29:10.480
have some work on a protocol for enforcing this kind of conversion into a semi-ladis in

29:10.480 --> 29:14.640
constant rather than linear space. So if anyone's interested in that idea, definitely come find me

29:14.640 --> 29:25.600
and talk about it. Okay, so like I said, the semi-ladis model today does not have a query

29:25.600 --> 29:30.800
language on top of it. So what might we want out of a query language for the semi-ladis model?

29:30.800 --> 29:36.640
Well, we want expressivity. Like we saw in the shopping cart example, we need set difference,

29:36.640 --> 29:44.080
so we need negation. Also recursion, something like datalog. We also want obviously classical

29:44.080 --> 29:48.560
query optimization options. We want identities that we can use to transform our query,

29:48.560 --> 29:53.520
get better performance. And we want to be able to do monotonicity analysis as well as functional

29:53.520 --> 30:02.080
and dependency analysis for partitioning. And so something like datalog for semi-ladises

30:02.960 --> 30:06.960
might be a good fit here, but there's a lot to explore. And so now Joe is going to talk about

30:07.520 --> 30:09.840
this monotonicity analysis and functional dependency analysis.

30:16.560 --> 30:21.280
I should say from the previous slide that some of this is things we took a crack at with

30:21.280 --> 30:25.840
BlueMal, so it's not that we've done nothing here. There's some answers to these questions,

30:25.840 --> 30:31.520
but there's also work to be done. All right, so I wanted to step back and review for you folks,

30:31.520 --> 30:37.360
the COM Theorem, which I know is sort of in a sub-corner of the pods community and not everyone's

30:37.360 --> 30:41.360
going to be familiar with it, but I think it's useful to go over. This will be high level,

30:41.360 --> 30:47.200
but hopefully helpful enough for you to get into the game. So the challenge is that we're going to

30:47.200 --> 30:52.800
have computers spread across the globe and we want our replicas to be consistent. So we have

30:52.800 --> 30:57.920
this nice couple here, they're in different places, and the classic example of replica consistency

30:57.920 --> 31:01.760
is data replication. So forget about programs, we're just going to have data, kind of like the

31:01.760 --> 31:07.120
CRDT model. And I want everybody to have common beliefs about the data, at least eventually.

31:07.840 --> 31:12.160
So these two folks currently both believe that X is love, which is lovely, but if it's a beautiful

31:12.160 --> 31:17.600
variable, things could change, right? And that's very sad. And once they disagree on the value,

31:17.600 --> 31:21.600
they might make decisions based on their disagreement that will lead to further

31:21.600 --> 31:25.920
divergence. This is sometimes called the split brain problem, because you can't put it back

31:25.920 --> 31:32.880
together later on, it's too messy. And so we want to generalize the idea of consistency of data

31:32.880 --> 31:38.160
replication to consistency of program outcomes. So I'm not just interested in the data, I'm interested

31:38.160 --> 31:43.520
in the queries, if you will, right? Much more powerful, and it will allow us to cheat sometimes,

31:43.520 --> 31:48.320
the data could be inconsistent if the query outcomes are not, right? So it'll give us more

31:48.320 --> 31:54.320
ability to relax our coordination. So we'd like to generalize this to program outcomes independent

31:54.320 --> 32:00.400
of data races when we can. The classical solution to this stuff is coordination. This is what things

32:00.400 --> 32:05.440
like Paxos and Two-Phase commit were invented to solve. And the way they solve it is by saying,

32:05.520 --> 32:09.840
what if we were just on one computer with one processor? Maybe we could implement that in a

32:09.840 --> 32:14.640
distributed fashion, which is a very heavy handed solution, right? You say that our solution to

32:14.640 --> 32:18.640
parallelism is to remove it. And how can we remove it in the distributed context? Well,

32:18.640 --> 32:24.320
it's expensive. But here's how it goes, right? On a single node, you use atomic instructions,

32:24.320 --> 32:28.320
right? So if you have shared memory, you can use atomic instructions, or maybe you use a locking

32:28.320 --> 32:32.240
system. In the distributed environment, you use something like Paxos or Two-Phase commit. And

32:32.320 --> 32:37.920
at every scale, as you saw in that ANA work, you'd like to not do these things. So even on a single

32:37.920 --> 32:41.520
machine, you really don't want to be doing coordination. And certainly in the distributed

32:41.520 --> 32:45.920
setting, this is very heavy weight. And there's people who will tell you at great length why

32:45.920 --> 32:49.680
they don't let the developers in Amazon call these libraries unless they have, you know,

32:49.680 --> 32:54.480
16 gold stars. Because it will slow down the whole environment and create queue backups and all

32:54.480 --> 32:59.440
kinds of horrible things. So when can we avoid coordination? This was a question that I asked

32:59.440 --> 33:03.600
as a lazy professor, because I was thinking maybe I should learn and teach Paxos, and I kind of

33:03.600 --> 33:07.680
didn't want to. So I was like, maybe, you know, maybe we don't need this stuff. Maybe Lamport's

33:07.680 --> 33:12.720
just a bunch of bunk. So that's kind of where this started, sheer laziness, intellectual laziness,

33:12.720 --> 33:17.600
which I will cop to. But what it led to, sometimes when you ask a question about how can I be lazy,

33:17.600 --> 33:21.600
you end up asking a question that turns out to be quite interesting. I think that's what arose here.

33:21.600 --> 33:26.240
And I'm seeing this not only in my work, but in other places. Back in the 20th century, if you

33:26.240 --> 33:31.200
will, the Lamport Gray era, we were trying to emulate sequential computation. We were doing

33:31.200 --> 33:35.440
everything we could to give the programmer the illusion of a sequential computer. And it was all

33:35.440 --> 33:40.240
about, you know, very low level stuff, reads and writes, accesses and stores, right? And then

33:41.120 --> 33:45.920
guarantees of order, total order, linearizability and serializability. And this was all based on

33:45.920 --> 33:50.480
the idea that programmers are hopeless. They'll write all kinds of crazy code. And the only thing

33:50.480 --> 33:54.080
they understand is sequential computers. So we'll make the worst case assumption that their stuff

33:54.080 --> 33:59.440
wouldn't work in parallel, right? And we'll give them mechanisms for avoiding parallelism.

33:59.440 --> 34:04.400
Seems like a terrible thing to do in a parallel environment. Yeah. So what's happening in the

34:04.400 --> 34:09.600
21st century is if we lift our, so this is all great. And sometimes you need it. I don't mean

34:09.600 --> 34:13.120
to denigrate the work. This is obviously foundational, touring awards. I use this stuff. I teach this

34:13.120 --> 34:17.440
stuff. It's all good. But when we don't need to use it, even better, right? So people have tried

34:17.440 --> 34:20.720
doing things like, what if all our states are mutable? That's a very functional programming game.

34:21.440 --> 34:25.600
It was sort of in my world, it's more about, well, you can mutate things as long as it's

34:25.600 --> 34:29.920
monotone. So if they're mutable, but they're monotone, maybe that'll work. And then using

34:29.920 --> 34:35.680
things like dependencies and provenance, all our ways of using application knowledge to avoid using

34:35.680 --> 34:41.120
the expensive stuff on the left. But the really big query is when do I need coordination and why

34:41.120 --> 34:47.360
do I need coordination? So if you ask, you know, a typical undergraduate or frankly, most people in

34:47.360 --> 34:52.560
computer science, including professors, when do you need coordination? What's a lock for, right?

34:52.560 --> 34:57.920
They'll say, well, it's to avoid conflicts on shared resources, right? This intersection needs

34:57.920 --> 35:02.880
coordination. If you would just put up some damn stop lights, right, then, you know, north, south

35:02.880 --> 35:07.040
could go for a while and west, east, west would wait. And then east, west would go for a while,

35:07.040 --> 35:12.960
north, south would wait, problem solved, right? But like, do I really need coordination? That's

35:13.040 --> 35:17.760
a solution. Is it the only solution? No, it's not the only solution, right? Here's a coordination

35:17.760 --> 35:23.120
free solution to that intersection problem, right? So I'd like to be able to think out of the box,

35:23.120 --> 35:28.400
right, and say, really, what is coordination for? Why am I required to use it?

35:30.720 --> 35:37.760
Okay. So that's a theory problem. So, you know, which programs have a coordination free implementation?

35:37.760 --> 35:42.000
We call those the green programs. These are specifications for which a clever programmer

35:42.000 --> 35:46.080
can find a coordination free solution. And then, of course, there's the rest of the programs,

35:47.440 --> 35:51.440
right? And I want to know this green line. Will someone please tell me, you know,

35:51.440 --> 35:55.760
Mr. Lamport, I think I only need you out here. So will you please tell me when I need you? And

35:55.760 --> 35:59.360
there's no answer from Mr. Lamport. At least he didn't, you know, pick up the phone when I call.

36:00.400 --> 36:05.600
But I'm happy to say that people at Hussalt did. And this is what led to the column theorem. So

36:05.600 --> 36:09.680
this is really a computability question. What's the expressive power of languages without coordination?

36:10.240 --> 36:16.720
Yeah. That's the green circle. Okay. So give you some intuition. Easy and hard questions.

36:16.720 --> 36:18.880
Here's an easy question. Is anyone in the room over 18?

36:21.360 --> 36:27.040
Excellent. Not only were you all happy to answer that coordination free, but you engaged in a

36:27.040 --> 36:30.560
little protocol, right? You made up a protocol where you raise a hand if you think it's true. So

36:30.560 --> 36:34.480
that was cool. So that was the monotone hand raising protocol or something. Great. All right.

36:34.480 --> 36:40.480
Who's the youngest person in the room? Oh, we have some brave assertions. But clearly,

36:40.480 --> 36:45.440
you don't know that. You could look at everyone, but that's cheating and also not necessarily

36:45.440 --> 36:52.880
right. Maybe, maybe. I don't know. I don't know. But the point here is, right, that somehow this

36:52.880 --> 36:57.360
requires you to communicate with people. And the first one maybe doesn't. Okay. More to the point.

36:57.360 --> 37:01.520
Let's look at the logic here, right? This is an existential question. And this is a question with

37:01.520 --> 37:07.680
the universal quantifier in it. Or for people like me who just want to do total pattern matching

37:07.680 --> 37:12.720
and look for not symbols, that one appears to be positive. So I'll say that it's monotone.

37:12.720 --> 37:16.640
And that one appears to be negative. So I'll say it's not monotone. So it gives you some intuition

37:16.640 --> 37:22.400
that universal quantification or negation requires coordination. It is coordination. That's what

37:22.400 --> 37:28.080
coordination is. It's universal quantification. So what is Lamport for? It's for universal quantifiers.

37:28.960 --> 37:33.040
So let's just prove this, right? I was like, well, somebody prove it. I'm not going to prove it.

37:33.040 --> 37:38.160
So nice guy named Tom Omelette wrote a thesis on this stuff. My conjecture was called the

37:38.160 --> 37:44.400
calm conjecture consistency is logical monotonicity. It was in a Paz keynote that I was gave some years

37:44.400 --> 37:48.720
ago. And then just a year later, there was a conference paper from the good folks at Haselt,

37:48.720 --> 37:54.400
which then they extended and then was further extended with weaker definitions for the monotonicity

37:54.400 --> 37:59.920
to really expand the results. If you want a quick, you know, kind of a version of what I'm saying now,

37:59.920 --> 38:03.600
you can read this CACOM overview, but it's really for systems people. I think you guys should just

38:03.600 --> 38:09.440
read Tom's papers. All right. To give you a flavor of what Tom did, definitions are half the battle.

38:09.440 --> 38:12.800
It seems, you know, when I read Paz papers, that's all the hard parts are the definitions, right?

38:13.920 --> 38:18.400
So, you know what monotonicity is in logic? That's fine. What is consistency here? Well,

38:18.400 --> 38:22.800
we want the program to produce the same output regardless of where the initial data is placed.

38:22.800 --> 38:26.720
So, I should be able to start the program with the data replicated pops possibly and

38:26.720 --> 38:32.080
partitioned in any way and get the same answer. And if that's true, then it would be the same

38:32.080 --> 38:35.360
answer across replicas. It would be the same answer across different runs. It would be the

38:35.360 --> 38:39.600
same answer if we start gossiping the data between each other. And this is what we want,

38:39.600 --> 38:44.000
right? So, that's our definition of consistency, where I think what's really clever and was the

38:44.000 --> 38:48.960
most beautiful part of the work is defining what coordination really means. So, we're sending

38:48.960 --> 38:53.840
messages around, right? That's data. But which data is really data and which data is kind of

38:53.840 --> 39:00.240
control messages? And how do you differentiate those in a formal way? And so, what they define

39:00.240 --> 39:05.760
in this paper is program is coordination free if there's some partitioning of the data when

39:05.760 --> 39:09.840
you first start. So, there's some way out of the data where you first start, such that the query

39:09.840 --> 39:14.560
can be answered without communication. So, for a particular query, for a particular data set,

39:14.640 --> 39:18.640
there is some world of where you lay it out where no communication is required.

39:19.920 --> 39:25.440
That's the definition of coordination freeness. And a program that you can't do that on is doing

39:25.440 --> 39:30.960
coordination messages. So, it's not really saying which messages are coordination and which messages

39:30.960 --> 39:39.360
are data, but it's telling you which programs can be run coordination. Yes. So, the trivial example

39:39.360 --> 39:43.840
of this is you put all the data in one node. And again, you know, this question of is there anybody

39:43.840 --> 39:50.400
who is older than me? What you don't know is whether anyone else has data. So, I may have all

39:50.400 --> 39:54.080
the data, but I don't know that. So, I still have to ask everybody, anybody got any data?

39:54.080 --> 39:58.560
And I have to wait for everybody to respond, right? So, it's a very nice intuition to just think

39:58.560 --> 40:06.560
about having all the data. All right. There's another thing in the paper that I hadn't even

40:06.560 --> 40:11.040
anticipated, which is really beautiful and speaks to stuff that the distributed systems

40:11.040 --> 40:15.280
community kind of knows, which is there's a third equivalent, which is that you can

40:15.280 --> 40:21.200
distributively compute this program on an oblivious transducer. And I haven't even talked

40:21.200 --> 40:25.440
about transducers yet, but just a minute. But what does it mean by oblivious? It means that

40:25.440 --> 40:30.240
the agent in the system doesn't know its own identity. It cannot distinguish messages from

40:30.240 --> 40:36.880
itself from messages from anyone else. So, it doesn't actually know who itself is. And it

40:36.880 --> 40:42.480
doesn't know the set of participants. We call this an oblivious agent, right? Oblivious programs

40:42.480 --> 40:46.320
that can be computed by oblivious agents are exactly the monotone programs and exactly the

40:46.320 --> 40:50.240
coordination free programs. So, that was very cool. And it speaks to questions of like

40:50.960 --> 40:55.120
membership protocols in distributed systems, which is about establishing common knowledge

40:55.120 --> 40:58.800
of the all relation. That's like one of the things that Paxos does is it has a membership

40:58.800 --> 41:03.680
protocol built in. So, it's one of the reasons it's coordination full is to establish all.

41:03.680 --> 41:08.960
So, this was really, really nice. So, this is all in this JACM paper. It's actually in the

41:08.960 --> 41:14.640
conference, the Paz paper as well. That's just a flavor of the calm stuff. And I'm going to stop

41:14.640 --> 41:18.480
with that. But happy to answer questions as best they can afterwards. And with that,

41:18.480 --> 41:19.600
I'm going to give it back to Connor.

41:20.160 --> 41:34.800
You guys can still hear me? All right. So, we have this calm theorem view of the world,

41:34.800 --> 41:41.040
relational transducers, logic, operating over sets. And then we have this semi lattice

41:41.040 --> 41:46.960
algebra view of the world. And they both are dealing with coordination and freeness in different

41:46.960 --> 41:52.240
lenses. But currently, they guarantee different things. The algebra view, like we said, is

41:52.240 --> 41:55.680
concerned with the coordination of freeness of rights and does not guarantee coordination

41:55.680 --> 41:59.760
of freeness of queries. Whereas the calm theorem view is only concerned with the

41:59.760 --> 42:02.400
coordination of freeness of queries. It actually doesn't have to worry about the

42:02.400 --> 42:06.000
coordination of freeness of rights because it assumes operating over sets and gets

42:06.000 --> 42:11.920
coordination of rights for free that way. And so, we're interested in the question of

42:11.920 --> 42:17.360
how can we combine these two lenses? Can we do an algebraic view of the calm theorem?

42:18.560 --> 42:25.280
And some intuition for how that might work is, you know, the semi lattice operator induces a

42:25.280 --> 42:31.120
partial order. And so, instead of having monotone logical queries without negation, you have

42:31.120 --> 42:37.600
monotone functions between lattices, between partial orders. So, that's something we've

42:37.600 --> 42:41.840
been exploring. We'd love to chat more about it with folks. I'm actually going to talk about

42:42.640 --> 42:48.640
a problem specific to the question Remy asked. It comes up in this setting.

42:49.360 --> 42:54.640
So, the calm theorem is all about basically not knowing when you have the entire input. What can

42:54.640 --> 43:01.360
I output and tell downstream consumers with certainty, even though I might have more updates

43:01.360 --> 43:06.000
in the future, there might be more messages arriving. And so, we call the ability to do this free

43:06.000 --> 43:15.680
termination without coordination. What can we be sure that we can output? And we're exploring

43:15.680 --> 43:21.280
this in a very generic setting of just we have two functions, an operation that's going to change

43:21.280 --> 43:29.520
our state over time in a query that's going to return some output. So, looking at an example

43:29.520 --> 43:34.480
of when we might be able to do this, we can look at this lattice. This is the set over these four

43:34.480 --> 43:42.400
socks and say that this is our state of a local node and we're at top in this lattice. And our

43:42.400 --> 43:51.120
update operation in the CRDT sense is union. So, we're going to union in more socks. We know that

43:51.120 --> 43:57.040
if we're at top, as updated as monotone, we'll never change our state. We're stuck at top.

43:57.920 --> 44:01.840
And so, whatever query we might be asking when we're in this state, we'd be able to locally

44:01.840 --> 44:06.000
detect that our query result is not going to change in the future and we'd be able to return

44:06.000 --> 44:12.000
our result with certainty. This might sound like it would not happen particularly often,

44:12.000 --> 44:16.240
but let's try and look at more examples where we would be able to figure out that with certainty,

44:16.240 --> 44:22.960
we can return an answer right now. So, what if we consider also the query that we're asking

44:22.960 --> 44:29.520
and say that this query is going to map us from this set socks lattice to the boolean lattice,

44:29.520 --> 44:37.600
true false, where top is true. Now, if this query is monotone, meaning as we go up in the partial

44:37.600 --> 44:44.640
order of socks, we also go up in the partial order of false and true, then we don't need to be at

44:44.640 --> 44:52.080
top on the left. We can actually be at top in the true false lattice and guarantee that our result

44:52.080 --> 44:57.040
won't change as future updates arrive. Any update that arrives is going to cause us to increase

44:57.040 --> 45:02.000
monotonically in the domain, which has to increase monotonically in the range and therefore

45:02.000 --> 45:09.600
our result will always stay true. For example, a query, is there a pair of socks?

45:11.120 --> 45:15.520
So, we call a threshold query. It effectively draws a cut through this partial order and says

45:15.520 --> 45:19.760
everything above this line in the partial order is true, everything below this line is false.

45:21.360 --> 45:25.120
So, these boolean threshold queries are a class of queries that we can freely terminate

45:25.120 --> 45:33.200
on if we know that our update is monotone. What about a totally different setting?

45:33.200 --> 45:38.240
What if we throw away monotonicity? So, now imagine that we have a deterministic finite

45:38.240 --> 45:45.040
automata and our query is mapping to true and false from accept and reject states in the automata

45:45.760 --> 45:49.200
and our update is appending a character. So, we think of a streaming

45:50.000 --> 45:54.960
character as appending. So, each update is going to transition us one step in this automata.

45:58.480 --> 46:05.040
And in this automata, any state that we're in, we can't conclude what the final result will be

46:05.040 --> 46:10.080
because from every state, there's a state that's accepting, that's reachable, and there's a state

46:10.080 --> 46:13.520
that's rejecting, that's reachable. So, some sequence of future updates might take us to

46:13.520 --> 46:15.680
false, some sequence of superstructures might take us to true.

46:18.400 --> 46:25.040
In contrast, if state three were also true, now from state one, we actually don't know if we're

46:25.040 --> 46:29.200
going to end up accepting or rejecting if we don't have the whole input yet. But if we're in states

46:29.200 --> 46:33.760
two or three, we know that every state that's reachable via future updates is going to keep us

46:33.760 --> 46:40.160
in our current result, which is true. And so, we can be certain that we can terminate here and return

46:40.160 --> 46:46.080
true. This is kind of like a reachability sort of visual view of how we're thinking about whether

46:46.080 --> 46:50.640
or not you can freely terminate given some arbitrary update operation on a domain and query operation

46:50.640 --> 46:56.160
that maps you to a range. This is a question raised in exploring a lot of different domains. If

46:56.160 --> 47:01.760
anyone has any ideas for what might connect to this, definitely come find us. Now, Joe is going to

47:01.760 --> 47:08.560
talk about partitioning. All right, this is a bit of a survey time. Boris, did you want to ask a

47:08.560 --> 47:12.960
question, Connor? I have a question to the last slide, right? Because in a sense, this now still

47:12.960 --> 47:18.640
has some monotone ordering, right? This kind of like, in a sense, it's kind of like if I can reach a

47:18.640 --> 47:26.240
node and it's smaller and now basically the nodes, if the terminal nodes are the top nodes and they

47:26.240 --> 47:31.520
don't have larger nodes, then so it's still monotone in the sense. Yeah, you can find some sort of

47:31.600 --> 47:37.760
it that, yeah. Yeah, I don't know if that's true for every freely terminating function.

47:38.480 --> 47:52.240
Psycho, do you think? Yeah, maybe. There's something about quotient lattice is, too, going at it in

47:52.240 --> 47:59.040
partial orders. Yeah, the graph looks like one. So this is love to have this conversation afterwards.

47:59.040 --> 48:04.240
That's why we're touching on a few things so we can have many conversations. So I think given time,

48:04.240 --> 48:08.400
I'm not going to go through this in any detail. I'm going to basically skip this chapter of the

48:08.400 --> 48:13.600
talk, except to quickly give some assertions. So first of all, we don't have HydroFlow, so we

48:13.600 --> 48:18.960
use Daedalus and we do have a full Daedalus to HydroFlow compiler. So we're able to write global

48:18.960 --> 48:24.160
programs in Daedalus and then auto partition and auto replicate them. And that's work being led by

48:24.160 --> 48:29.200
David Chu, who's in the room over here. David, three years ago, promised to do this and they

48:29.200 --> 48:34.560
gave him a certificate saying that's cool. So he won the SSP student research award. And three years

48:34.560 --> 48:39.200
later, he's got his first results. So it took a while. This was not an easy problem, but he's able

48:39.200 --> 48:47.440
to take arbitrary Daedalus programs, which are Daedalug, and partition them to run on multiple

48:47.440 --> 48:51.760
machines. And I'm really not going to spend a lot of time on this. What I'll say is that

48:52.720 --> 48:58.320
earlier student Michael Whitaker, who again did this by assertion, he found all sorts of opportunities

48:58.320 --> 49:03.520
to optimize Paxos because inside of Paxos, it was bottlenecking on things that were trivially

49:03.520 --> 49:08.720
parallelizable, like network message handling. So he's like, if I can just bust apart some of the

49:08.720 --> 49:14.160
roles in Paxos into sub roles, some of those sub roles can be replicated. And he got state-of-the-art

49:14.160 --> 49:19.280
performance in terms of throughput on Paxos by doing this. And what I observed after he did it

49:19.280 --> 49:23.920
was, oh my gosh, most of the things that you've split out are monotone subcomponents. And I should

49:23.920 --> 49:28.480
have known that we could pull those out and replicate those. In fact, I wish Bloom could do

49:28.480 --> 49:32.720
that automatically, but it couldn't. So three years later, David can now automatically pull out

49:32.720 --> 49:39.360
these things that Michael was observing and transform the program to do it. And the ideas are

49:39.360 --> 49:47.520
basically just two tricks. One trick is to take a pipeline on a single machine and split it across

49:47.520 --> 49:52.720
two machines. He calls that decoupling. Now in a general purpose program, this is taking,

49:52.720 --> 49:56.400
I don't know, 10,000 lines of C++ and figuring out which ones to run on this machine and which

49:56.400 --> 50:00.400
ones to run on that machine. That would be horrible, right? But in a data flow language or

50:00.400 --> 50:04.320
logic language, it's quite nice. And so he has conditions for when this is safe and when it's

50:04.320 --> 50:08.480
not. So that's decoupling. So you can think of this as refactoring a program into components

50:08.480 --> 50:13.040
that can be run on different machines with asynchronous communication. The other thing

50:13.040 --> 50:18.720
he does is what's sometimes called sharding in the practitioner community, but it's partitioning,

50:18.720 --> 50:24.240
shared nothing partitioning of subplants, right? So instead of having BC take all of the data from A,

50:24.240 --> 50:29.040
you have a hash partitioning here and certain values go to each machine. And how do you know

50:29.040 --> 50:32.560
that each one of these computations can be done independently? That's done through things like

50:34.160 --> 50:39.120
functional dependency analysis so that you can show that certain values have no dependency on

50:39.120 --> 50:43.840
other values because they're partitioned by, say, NFD. So I'm not going to go into any of this,

50:43.840 --> 50:47.680
but basically what David was able to do was take many of the optimizations here that were

50:47.680 --> 50:53.280
handwritten in Scala and automate them and formalize their correctness. And without getting

50:53.280 --> 50:58.160
into too much detail, although it is kind of fun, oh, and we borrowed some work from Paris. So

50:58.160 --> 51:08.960
shout out to Paris for parallel disjoint correctness and colleagues. It is really fast. So he was

51:09.200 --> 51:14.080
able automatically. This is Michael's results that we re-ran. This is Scala. This is throughput

51:14.080 --> 51:18.080
against latency. So what you want to do is you get as much throughput as you can until it starts

51:18.080 --> 51:22.560
to hurt you at latency and it curls back. So this is kind of where things start to

51:25.040 --> 51:30.160
top out, if you will. So that's Whitaker's implementation. This is the same logic as

51:30.160 --> 51:34.720
Whitaker's implementation in Hydro. So this is just Hydro is faster than Scala written by hand.

51:34.800 --> 51:40.080
So this is just a testament to the folks who wrote the Hydro flow engine. But the blue line is

51:40.080 --> 51:46.000
what David achieved through systematic correctly proven rewrites. So he was able to get performance

51:46.000 --> 51:51.280
that actually, because Hydro is fast, is better than Whitaker's Paxos implementation. And this gap

51:51.280 --> 51:54.880
is kind of what he's given up. These are the tricks that Michael had that we didn't cover in our

51:54.880 --> 52:02.960
rewrites. But we're doing 90% with the easy tricks. So it gives me confidence that simple

52:03.040 --> 52:07.920
query optimizations known in parallel databases can have impacts on places that we're really very

52:07.920 --> 52:12.480
fine tuned performance issues that people write PhDs to get this kind of performance and we're

52:12.480 --> 52:18.240
getting it through systematic rewrites. Very promising. David's only halfway there though,

52:18.240 --> 52:22.080
because he has to have a proper cost-based optimizer. Right now what he has is correct

52:22.080 --> 52:27.040
transformation rules. He needs a cost model with an objective function. And then he needs a search

52:27.040 --> 52:33.680
strategy. And we're hopefully going to be using egg log or some implementation of egg log in Hydro

52:33.680 --> 52:39.040
flow to achieve this. So we're one of the things with Maxis stuff that overlaps is if there's a

52:39.040 --> 52:46.080
lovely semi lattice based data flow implementation of Maxis stuff, maybe we can clean up some of the

52:46.080 --> 52:54.080
things where he's doing updates in place. This work? This work. Well, so this was written in

52:54.080 --> 52:59.680
Datalog and translated down into that Hydro flow data flow language you saw at the top. This stuff

52:59.680 --> 53:06.240
is also written in Datalog currently in a runtime that plays some ad-hoc tricks. That's not traditional

53:06.240 --> 53:13.040
Datalog execution here as Maxis. But I think, you know, Union Find is a nice target for an

53:13.040 --> 53:18.960
algebraic treatment and I think we have opportunity. Okay, what I'd like to do in the last few minutes

53:18.960 --> 53:23.440
is berate you with questions because these are the things that I don't know how to answer yet and I

53:23.440 --> 53:27.840
would love to get help with. So the first is, and this is an outline, so this section goes on for

53:27.840 --> 53:31.760
many slides, but there's the four questions. Can we please have one theory for all this nonsense

53:31.760 --> 53:37.280
instead of the list I'm about to show you? What would be a good type system for the physical

53:37.280 --> 53:43.040
layer where we could prove correctness of transformations? I have a follow on to Sudipa

53:43.040 --> 53:49.440
about the role of our kinds of work in the era of generative AI. And then I have this ongoing

53:49.440 --> 53:55.280
question of what time is for, which I probably don't have time to explain. But quickly, you know,

53:55.280 --> 54:00.400
the unifying theory thing. So CRD teaser semi-ladyses, Datalog, Daedalus was all done with model

54:00.400 --> 54:06.640
theory and it's fancy actually. It uses like stable models and stuff. It's actually ended up

54:06.640 --> 54:10.960
being kind of fancy. The column theorem, Amalut, proves have these relational transducers, which

54:10.960 --> 54:16.240
are this halfway world between operational and declarative semantics. You have these state

54:16.240 --> 54:22.000
machines on each node. They run declarative languages on each step, but then they output stuff and

54:26.720 --> 54:28.960
I think you can write non-terminating programs if you want to.

54:30.560 --> 54:36.880
So you can write Toggle, for example, and Daedalus and the transducers.

54:39.440 --> 54:43.920
Now they don't have to be terminated. In any sense, I don't think. But the point is,

54:43.920 --> 54:47.600
I really wish he'd have done this work with this, because he also was on this work, but he didn't.

54:47.600 --> 54:52.240
He did it with transducers, which is a bummer. If you talk to distributed systems people, they

54:52.240 --> 54:55.920
talk about essentially order theory. They talk about partial orders all the time, which is related

54:55.920 --> 55:01.600
to lattices, but you know, it's annoying. Programmers want to write these sort of algebraic functional

55:01.600 --> 55:06.880
expressions, which I think is a good thing for all of us. And then yeah, I give all these talks

55:06.880 --> 55:11.680
and then some joker raises their hand and says, well, what about transactions? And in fact, Peter

55:11.680 --> 55:15.680
Bayless, when he was a student, basically did an end run around my entire group and just wrote

55:15.680 --> 55:20.880
papers about transactions and coordination, and they don't align with the rest of this stuff.

55:20.880 --> 55:25.600
So it's an open challenge to reintegrate that work. And then, you know, I didn't actually say the S

55:25.600 --> 55:31.760
word yet, because I apparently didn't do joins as of yet. But we do do joins, so we probably need.

55:32.640 --> 55:38.080
So it would be really great to get all of it here. I would like to bring all of this work

55:38.080 --> 55:44.640
into this domain. That would be really nice. Okay. Here's a flavor of what I'm dealing with,

55:44.640 --> 55:51.200
though. So just finished reading the DBSP paper, which was very nice and related to our work, but

55:51.200 --> 55:56.640
we have some other things we need to keep track of that are relating to the network messing with

55:56.640 --> 56:02.480
stuff. So when we look at a stream, it's got some values. It's got some happenstance ordering,

56:02.480 --> 56:07.600
that's a mapping of the naturals to those values. It's got some happenstance batching. It came in

56:07.600 --> 56:12.560
over the network in chunks. So there's like singly nested parentheses in this stream that are

56:12.560 --> 56:18.160
randomly placed. Randomly, you know, arbitrarily ordered, arbitrarily placed. Maybe this is a

56:18.160 --> 56:22.960
stream of lattice points, but maybe it's not. I don't know. But if it is, you could say things

56:22.960 --> 56:27.280
like there's a monotonicity relationship between the type's natural order and the total order of

56:27.280 --> 56:34.560
arrival or not, right? And then what sort does is it enforces something like this, right? It's

56:34.560 --> 56:39.120
nice when these are atomistic, like data flow is basically a set lattice that you flow individual

56:39.120 --> 56:43.680
atoms. That's the traditional kind of database iterator thing, one tuple at a time, right?

56:43.680 --> 56:48.720
One tuple at a time is an atomistic lattice of the sets of tuples. And it's nice when you know

56:48.720 --> 56:52.880
you're dealing with atoms, because you can say things like item potents, right? I gave you

56:52.880 --> 56:56.640
Bob's tuple once, I gave you Bob's tuple twice, sorry, but you should only have it once. So delete

56:56.640 --> 57:01.360
one. But if I give you subsets, now you have to find how they overlap and you have to make sure

57:01.360 --> 57:05.600
that when you union them, you remove the overlapping bits. And so when you have non-atomistic things,

57:05.600 --> 57:11.680
it's just a little uglier. And you end up talking about like, does their meat, is there meat bot?

57:11.680 --> 57:15.040
kinds of things. Do they have no intersection, right? So these are the kinds of properties

57:15.040 --> 57:20.160
that I think I need to track in my rewrite rules. And then, you know, the operators are the invariants

57:20.160 --> 57:25.040
to these properties, like lattice, lattice operations are invariants of order and parenthesization.

57:25.600 --> 57:29.200
Do they preserve the properties? Do they enforce different values for the properties?

57:29.200 --> 57:35.280
The network non-deterministically enforces orders and parenthesizations. So like this is the stuff

57:35.280 --> 57:39.600
that I worry about in my operators. And this is kind of the soup I'm swimming in with this

57:40.400 --> 57:45.200
physical algebra. So I would like help with this. All right, I'm going to do one more quick slide.

57:45.920 --> 57:49.360
This is very much in the realm of what Sudipa was talking about. You know, we're in the era where

57:49.360 --> 57:54.880
people will be programming with green goo, right? It's just this is large language models, they're

57:54.880 --> 57:59.600
magical, they produce stuff. But what we really want is building blocks that we can count on,

57:59.600 --> 58:03.360
right? We're a database people, our industry is all about foundational building blocks.

58:03.360 --> 58:07.760
And I really do think declarative specification is a lovely narrow waste here between these two,

58:07.760 --> 58:13.520
where we can take a formal spec as Cod asked us to, we can render it in some sense so that it's

58:13.520 --> 58:18.480
readable, right? And this relates to things like Wolfgang's work on visualizing queries,

58:18.480 --> 58:22.320
and what Sudipa was talking about in terms of giving natural language things, but helping

58:22.320 --> 58:26.320
people look at this and say, is this what you meant? Not is it correct, but is this what you

58:26.320 --> 58:31.760
meant since a spec after all, right? Did you mean this query? And then of course, if it's in a nice

58:31.760 --> 58:37.920
formal language, we can check it for other things, right? And so that would be, I think, a role that

58:37.920 --> 58:43.040
we can really play in the world. And I suspect things like this will happen. These programs are

58:43.040 --> 58:46.720
going to be a selection of programs. You're constantly going to be given a menu of which of

58:46.720 --> 58:51.680
these things did you mean. And the answer to which is either some invariant checks or something,

58:51.680 --> 58:58.160
or some human judgment. So I think we're in a good spot in terms of intermediate languages.

58:58.160 --> 59:03.040
And I'll just close with one more slide, maybe just a handful of slides.

59:04.080 --> 59:07.600
What are clocks in time for in distributed systems? So there's this famous saying, which

59:07.600 --> 59:12.080
is correctly attributed to a sci-fi short story. Time is what keeps everything from happening

59:12.080 --> 59:17.360
all at once. So when should we use time and computing? What are clocks for? Well,

59:18.320 --> 59:23.520
they're not for monotone queries. I can run this embarrassingly parallel. It can all happen at the

59:23.520 --> 59:31.200
same time, and it's fine. And Buntalu was doing this long before this discussion. But I can't run

59:31.200 --> 59:36.240
this at the same time. You can't have P and not P at the same time. So what's the deadliest answer

59:36.240 --> 59:43.280
to that is, well, that's what time is for. This is the toggle program, right? And time is there to

59:43.280 --> 59:48.160
separate two things that can't coexist. That should be, I think, the only reason for time.

59:49.760 --> 59:55.120
Except it's not. Distributed systems, people use time for maintaining partial orders and knowing

59:55.120 --> 01:00:00.320
when things were concurrent. Sometimes you don't need that. Sometimes you do. But this is my question,

01:00:00.320 --> 01:00:04.960
especially because Val's in the room and worked on this DSP stuff. Daedalus has one clock per

01:00:04.960 --> 01:00:10.960
node and we update it only when there's a network event or we have to cycle through negation.

01:00:11.920 --> 01:00:16.560
Timely and differential data flow have clocks all over the damn place. And I'm not sure when you

01:00:16.560 --> 01:00:22.960
use them and when you don't. So, for example, tracking iterations of a monotonic recursive

01:00:22.960 --> 01:00:26.480
program. Why do I need a clock for that? I don't think I need a clock for that. Maybe if it's a

01:00:26.480 --> 01:00:32.160
while and we use the index in our computation, I need to know what index I'm at. So the general

01:00:32.160 --> 01:00:38.720
question is, when do we use this structure called a clock? And when don't we need? And can a compiler

01:00:38.720 --> 01:00:44.960
decide? All right. We are a little over time. I hope we have given you lots of things to ask us

01:00:44.960 --> 01:00:51.680
later. We need lots of help. So we'd love it. And we are big fans of working with folks like you.

01:00:51.680 --> 01:00:57.680
Can you leave these last four slides there so we can come up with some more folk questions?

