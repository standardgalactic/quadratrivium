{"text": " And we are going to see connections of language models that maybe you did not quite expect to anticipate, but they are very real. And to start with, we are very lucky to have Steven Pantadosi, he is a professor of psychology and neuroscience at UC Berkeley. And a friend of AI, we actually have a grant together. And a lot of us in AI are excited and interested in learning from the psychologists and seeing how they can inspire our work. Well, actually, this is a two-way street and because the psychologists are also getting excited about the language models to understand something about humans. And Steven will tell us about some of the work he has been doing, which I think is very exciting. So, Steven. So thank you for the invitation to speak here. I'm going to be talking about the meaning in the age of large language models and maybe finding meaning in the age of large language models. And this talk isn't a kind of technical talk about language models or evaluation or anything. It's almost closer to philosophy or closer to kind of high level theories in cognitive science and psychology, which are about meaning. And some of the kind of history of work of people's ideas about where meaning comes from in language or in semantics and how we can think about those in the context of large language models and in particular in the context of debates about large language models and whether they're just stochastic parrots that don't have any form of understanding or whether there's some sense in which they have understanding and if they do have some form of understanding, if that form of understanding is at all related to the types of understanding that people have. So my plan for the talk is first to talk about meaning and reference kind of generally. I'll talk about some claims in large language models and why people often think that there's no sense of meaning or kind of real semantics in them. Then I'll talk about some psychological theories about meaning and how meaning arises from what are called conceptual roles. I'll come back to large language models and talk a little bit about learning conceptual roles in large language models or in kind of general machine learning systems. And then some very kind of brief overview of a learning conceptual role experiment in people. So let me start with kind of how I got interested in this which was this paper by Bender and Kohler in 2020. Emily Bender is a computer scientist trained as a linguist also at the University of Washington who people may know as a very vocal critic of many aspects of large language models. The one that initially I think interested me was claims about the meaningfulness of large language models and essentially arguments that there's nothing meaningful at all in what statistical models that are trained on text can do. And Bender and Kohler came up with a very nice way of making this point, what they call the octopus test. The octopus test goes as follows. So kind of starting point is for them, meaning is an association between a word, say, and something external to language. Okay, so the meaning of the book is some physical object, a book that's out in the world. You can see that that's right because that's how we label physical objects and when we're learning words, right? We hear the word book when there's books around and we pick up on that association. And so that's kind of fundamentally what the, that kind of external reference, right? To something in the real world is fundamentally what the word means. And Bender and Kohler say, okay, let's take that as our definition of meaning and let's imagine an octopus. So an octopus who lives under the ocean and has tapped into a communication channel, say a telephone line between two islands, okay? So the octopus is there, it's eavesdropping on all of the communication that happens between those two islands. You can get a huge amount of linguistic input and you could imagine very smart octopus might be able to learn all of the statistical properties of what's happening across that communication channel, right? Might be able to learn, become very good at predicting text. Maybe it could predict it optimally, whatever. Their argument is that the octopus could never actually learn the meanings, right? Because it would never have access to the physical reference, right? So yeah. Is it really different from the Chinese room? Yeah, so it's interestingly a little different from the Chinese room and maybe can I defer that question to the end if that's okay? Because I think what's going on with the Chinese room might make more sense with what I say later, okay? But very similar in spirit, I would say. So this octopus has no access to the physical reference and therefore couldn't solve tasks involving the physical objects, right? If you ask them to visually recognize what a coconut was even if they knew all of the statistical properties of where the word coconut would be used, right? They wouldn't be able to solve the physical stuff, okay? And of course this is the situation that a large language model is in, at least one that's only trained on text, right? It doesn't get access to stuff from the world, it only has kind of text, okay? Therefore this octopus doesn't have meanings for the words. Such an octopus is like a large language model because they only have text. And predictive ability therefore can't give you the meanings, right? Meaning is something just fundamentally different than predictive ability. Let me give you one other example of this. Well, I'll just say, I think that on the surface this is a somewhat convincing argument, right? It's kind of compelling to think of meaning in this way. And certainly if you do, it seems pretty convincing, yeah. It may not be right in terms of your mathematical history but it's given me a couple of axioms. And you know, you're not supposed to have, because this is really referring to this particular interpretation of that one. This would make that up in your mind. Yeah, great, so hold on to that thought too because a mathematician thinking about axioms is very much related to another version of meaning that I'll talk about in a few minutes, okay? Even as Hilbert wanted it to be, right? Hilbert's desire was to convert Euclidean geometry to a set of axioms such that every symbol could be replaced by some arbitrary squiggle and the system should still work. That is what Hilbert did in the axiomatization of geometry. Yeah, so I think most mathematicians would have a slightly different sense of meaning but one which matches what I'll say in a few minutes, so yeah. Let me give you just another kind of gloss on meaning and language models. Here's Gary Marcus talking about lambda. In truth, literally everything the system says is bullshit. The sooner we realize that lambda's utterances are bullshit, just games with predictive word tools and no real meaning, the better off we'll be. Software like lambda doesn't even try to connect to the world at large, right? That's what he thinks makes it bullshit. Just tries to be the best version of autocomplete that it can be. They don't understand language in the sense of relating sentences to the world but just sequences of words to one another, okay? So I got interested in this in part because I find this view kind of compelling. On the other hand, I also think it's kind of deeply wrong and the way in which it's deeply wrong is really interesting for what it has to say about conceptual representations and meanings in human minds as well as in machine learning models. So a few years ago, I teamed up with Felix Hill, who's a researcher at DeepMind and wrote an article basically going through arguments that meaning is not this form of reference, right? So in fact, this idea that meaning should be equated with some mapping to things in the world has often been rejected by people in linguistics and philosophy and cognitive science. And I think for good reason, so just to give you a kind of flavor of why people often reject this, there's many concepts, many words, for example, that have no reference to the external world, right? Function words are a good example of this. Words like to or is or many, right? There's no to, to, to out in the world that that word refers to. There's also no is out in the world or no many. Those are function words in language and what they do is actually much more like kind of what an operator in mathematics or something does, right? These words have an internal meaning. They control the kind of compositional meaning of a sentence, meaning that they have to be composed internally in linguistic representations in order to express their meaning, right? They're not pointing to something out in the world. Even if you don't go to function words like this, there's other words which are very hard to make sense of in a kind of view that meaning is stuff in the world. So you can think of very abstract words like justice, right? There's probably not a justice out there. That's some kind of construct that we have or wit, or you can think of things that don't exist like dragons, right? There's no external thing in the world, which is a dragon. There's even words and concepts we have that have no possible reference to the world, right? So if I think about an imaginary bicycle, that's something which is by definition imaginary, right? It's not out there, or a perpetual motion machine, right? We can have a concept of a perpetual motion machine and think about it and reason about it, but there's certainly not one that exists out in the world. Even for ordinary concepts, we likely haven't even considered all of the possible things which could be reference of those concepts, right? So I could walk in wearing a shoe made out of eggplants and you could look at them and everybody might agree that they're shoes, right? But you would agree that they're shoes without ever having seen shoes made of eggplants before, right? So there's some object in the world which everybody would agree is a shoe. Even though you've never encountered that thing before, that means that it couldn't have been the stuff in the world which determined whether it was a shoe, right? Had to be some kind of more abstract conception of what makes something a shoe, right? You can think about things like the function or the origin, how they're used. These other kinds of properties of objects seem to be much more important for the categorization of the concept, yeah. Well, here this is a little bit of a survivorship bias because eggplants shoes might still be considered shoes, but then ice cream shoes are probably, nobody will recognize them as shoes, right? But then you don't think about ice cream shoes. So it's like the things that you can think of as shoes are in your little bowl and then you don't think about things that are already too far. So it seems like there is still kind of some distance to the closest real object. Yeah, yeah. So all of this is not to say that the real objects are irrelevant, right? Like I agree that eggplants are much more plausible issues than ice cream and that has to do with the kind of real physical properties of those substances. My point is just that the physical thing is not the defining thing, right? It's not something in the object you look to to decide whether it's a shoe or not, right? It's something more abstract about how it's used or made or something like that. I'll actually talk a bit in this paper about the concept of a postage stamp, right? Which is just an example of one that people probably have some intuitions about where you could easily think of postage stamps which are fundamentally different than anyone's you've seen before. You could think of one made of glass, for example, or you could think of one that was an RFID tag, which is probably physical incarnations of postage stamps like that that everybody would agree should be called a postage stamp. And if you try to get people to define it, right? You might say something like, well, a postage stamp is something you pay for and you put on a letter so that the letter will be delivered by the government or something like that, right? So what the term means is intrinsically connected to a bunch of these other terms like payment and letters and being delivered and those things. And in fact, if those terms change meaning, right? So if, for example, people develop a new way of paying for things, paying on the blockchain or something, right? Then you kind of know automatically that a postage stamp can be paid for in that way, at least in principle, right? So it's not just that the word is associated with those other things, but that its meaning is inherently connected to those other things. So that's one kind of take on why reference to stuff out in the world, right? Is not a good way of thinking about meaning. Let me tell you what one alternative is. Actually, before I do that, let me just show a couple of other alternatives, which I think are also not plausible, but might be familiar to people, okay? So what I just talked about is this kind of world mapping view, right? That there's some word and its meaning is some physical object or some thing. You can think about other kinds of views of concepts and meaning might have a kind of feature spacey kind of views, port vector machines or something, right? There's some abstract feature space and a concept is some dividing line or some region or something in this space. That I think is maybe fine in some narrow applications, but what I'll talk about next are cases of, say, human cognition, which really don't fit well into that picture, in the sense that things are much more complicated for how people think about concepts and their relationships. People might also have this sort of hierarchy or network view, right? So sometimes people think, oh, sorry, you can't see this. There's supposed to be lines connecting one concept book in the middle to a bunch of other concepts, right? And you might, you know, there's old theories of, say, semantic organization or very old, old AI kind of approaches, right? That think about building hierarchies of concepts or sometimes networks of concepts and trying to define meaning in terms of those relationships. I actually think that both this and the feature and the world mapping view have some of the, some kind of useful properties or useful insights about concepts, but just aren't quite the whole thing for reasons that I'll talk about next. So let me just start with, start trying to introduce this kind of other view of concepts by trying to get people's intuitions on a recent news story, okay? So here's a little recent news from the US versus Trump. I think this is not the most recent indictment, but one or two indictments ago. If you look through it, you can read all about Pence and Trump and efforts to manipulate the election and things. Here's a little paraphrase of one of the paragraphs, 90 C. So Pence, the vice president, right, opposed a Trump team lawsuit arguing that the vice president could reject electoral votes. So Pence didn't want them to argue that he could reject electoral votes. He said to Trump that he didn't have a constitutional authority and that the action would be improper. So it's according to Pence's notes at the time. And Trump responded, you're too honest, okay? According to Pence's notes. So think about that situation and everything you know about this context, right? And think about an answer to a question like, why did Trump say this? Right. You think about that, probably what's going on as you think about it, right? As you're thinking about lots of other things and how they're related to this situation, like what Trump was trying to achieve, maybe what kind of personality Trump had, what Trump was trying to do to Pence. Is he trying to manipulate him into taking some kinds of actions? What exactly that action would have, right? In terms of the election. Everybody is perfectly capable of reasoning through these and coming up with kind of plausible causal story about what's happening, right? It feels like we can come up with our own kind of internal explanations about events like these. And in fact, that process of coming up with internal explanations, interrelated kind of concepts and meanings is one that people in developmental psychology have been very interested in and excited about as a theory of kind of human cognition. So basic observation is that people form these very richly interconnected systems of concepts, right? All of the kind of interconnected stuff you would need to draw in order to answer a question like that, which feels totally, totally normal. People sometimes call these intuitive theories, right? You have some intuitive theory of how Trump is acting or how the political system would work or some intuitive theory of what Pence might be doing or might be trying to achieve. And these things are often compared to theories in science, right? So you can think of your theory of why Trump might do this as kind of analogous to a little scientific theory, right? It has some pieces, it has some relationships between the pieces, it has some dynamics. And maybe you can look at all of that and kind of reason about it causally as you might reason about any other kind of system that you've encountered. So the idea that people, and maybe most notably kids do this is one which has really been very popular in cognitive development, championed maybe most prominently by Alison Gopnik, who's a developmental psychologist here at Berkeley. Let me just give you a quick example of how kids, how experiments with kids like kids sometimes go in this domain. So here's an experiment from LaZot and Gelman. So kids are shown these two foxes, right? Which you might notice are identical pictures, okay? And then they're told things about these foxes and asked what they could do in order to answer a question, right? So this is like a simple version of why did Trump say that? You might be told that one is an animal and one is a toy, okay? So what could you do in order to determine which one is an animal and which one is a toy, right? In this experiment, kids will say that you should do things like check the insides, right? Like check their guts or whatever, right? Open them up and see. Or look at their behavior, right? If it acts like an animal then you could use that to figure out which one is the animal, which one's the toy. Or look at their parents, right? Like, you know, the animal will have animal parents and the toy won't, right? And importantly, they don't just say, yeah, you can check everything about these. They know, for example, that age is not relevant, right? So they won't tell you that age would tell you which one is an animal and which one is a toy. It's worth pausing and just thinking about this and what this means in terms of conceptual representations, right? Because you can think about your concept of what makes something an animal or what makes something a toy and kind of like the postage stamp example, right? It's intrinsically connected to these other things, like what parents are or what's going on with your guts inside, right? Or what your behavior is, right? That concept is just intrinsically linked there and kids, I think these are preschoolers know that from a pretty young age. Gals asked them a question like one is a dog, one is a wolf, what would you do, okay? Kids basically say the same things there. You could check the insides, you could look at behavior, you could look at their parents, see if they had a dog parent or a wolf parent. Some of these are actually kind of interesting, right? Because I don't think anybody knows, at least I don't, what you would look for on the insides to distinguish a dog versus a wolf, right? Like maybe you could go down to DNA or I'm sure you could go down to DNA to tell that. But people have the intuition that like, okay, there's something about being in this category which depends on these other aspects of being in the concept. To me, I read this much simpler. It's like basically what they're saying is look, like all of this are visual things. It's just that you're trying to project it into language but actually what the kids are probably meaning is, you will know it when you see it than when you play with it, right? It's vision and interaction. Yeah, yeah. And age is neither. Yeah, yeah. So I think it's true that, yes, all of these are visual cues. I don't know of experiments that look at non-visual cues but I agree, yeah, that's interesting. I'll give you one other example where they know that there's no cue, right? So if you tell them that one is named Amanda and one is named Melissa, then they'll reject all of these as tests, right? They'll say, okay, the insides are not gonna tell you which one is Amanda, the behavior and the parents and the age and these things are not going to, okay? So all of this is just to say that people have a, even kids, right, have pretty sophisticated theories of how concepts relate to other concepts, right? And in fact, in a situation like this, right, there's nothing visual apparently that could tell you, right? So it's not a visual discrimination task. It's really a kind of conceptual one that's asking you to look at other kinds of conceptual features and things. So people have these intuitive theories then one kind of proposal, quite a few people have argued for is that meaning arises from essentially the role that a word or a symbol or a concept plays in this theory, right? Like the meaning of animals really just intrinsically related to these ways of testing it and these kinds of features and all of the other things that are not kind of simple semantic associates with animal but are kind of deeply connected in the sense of an intuitive theory. I was trying to come up with examples where this, you know, we'll give people this intuition, right, that, you know, if you try to define these words, if you try to define what an indictment is, for example, it's very hard to do it in a way that doesn't reference other legal terms and other kind of social constructs and concepts that you already have, right? It's kind of intrinsically related to the system of other concepts and terms, right? Chord change is kind of like this too in music, right? You have to talk about chords and notes and circle of fifths or whatever, right? Like these things are just intrinsically related. I think force in physics is like this. It's very hard to talk about it in isolation independent of, you know, experiments or other concepts or things. Or if I said, like, what does a bobbin do in a sewing machine, okay, right? Like you have to talk about thread and you have to talk about the processes of sewing. Just the meaning of these things are just all intrinsically linked together. So this idea that meaning, not about reference, it's about the role that something plays is called conceptual role theory. Meaning of a word or concept is determined by the role it plays. And this has been argued for, I think maybe most prominently by Ned Block who's a philosopher of mind, who wrote one of my favorite paper titles, Advertisement for a Semantics of Psychology, which is basically all about, you know, how psychology needs a theory of meaning and a theory of semantics. And this idea of conceptual role is something that could do that. So it can explain kind of where meaning comes from. It can address questions of how meaning depends on things like your representations or categories that you know, can play nicely with compositionality or other aspects of language. And I think maybe most compellingly can explain how you could find meaning in brains, right? So if you open up a brain and you start recording from neurons, you know, it's really unclear what it means for there to be reference in there, reference to the external world in there. But maybe you could kind of make sense of patterns of activity in a way that lets you kind of interpret systems of signals and representations. Let me give you just one other example of this that maybe might make things more clear. This idea of conceptual role semantics, I think is also how meaning works in, say, a computer, okay? Also, I think in mathematics, which is why I was deferring the questions about mathematics, but you could look at something like this, right? This is a floating point representation and ask what makes the bits in this representation mean what they do, right? In particular, what makes the first bit mean the sign bit, right? It's nothing about being the first one because there've been dozens of different conventions for floating point numbers which put the sign bit in all kinds of different places, right? What makes it mean the sign bit is how it interacts with all of the other operations that you can do with floating point numbers, right? So meaning, in some sense, comes from the interaction between symbols, or in other words, their conceptual role. So in particular, like what does negation do, right? If I have a negation operator, okay, it flips the sign bit. Great, okay, that's where it gets its meaning from, right? Or what does addition do? The right thing with respect to the sign bit or multiplication or rounding or whatever, right? So what makes this the floating point representation or what makes that first bit represent sign is nothing intrinsic in the representation itself. It's how it interacts with all of the other components of the system, okay, yeah? You explain the difference between this way of thinking about the sense of here's a hierarchical approach where there's concepts and there's sort of numbers. Yeah, so I think that there's certainly concepts people have that are hierarchical, right? So we know that dogs are kind of animal and animals are a kind of living thing. I think what that kind of picture is missing is that our representations are actually, like computational objects, like they do something, right? They interact with each other and they allow us to solve certain kinds of inference problems and all of the stuff you could do with your concepts like the Trump example, right? Yeah, so the claim is not that they're not hierarchical, right? It's that the interesting important things they do come from interactions kind of internally between concepts much like the way that the sign bit is interesting or important here comes from its interactions with things like negation and multiplication, yeah, yeah. What you're saying here is perfectly good, but what I have trouble with is buying this as an exclusive theory of semantics. Just like you gave good arguments against that meaning is just reference, okay? I think you demolished that theory, but now you put up another theory which also I find that it has some good aspects, but to make that an exclusive theory is problematic. So if we look at children growing up, there are these studies on sort of concreteness judgments. So the vocabulary of a child at two, there are a lot of words in there like milk and bottle and jump and sit and so forth, which are very concrete, concrete in a visual sense, concrete in a motor program sense. At the age of 10, they have words like justice and fairness and so forth, which are very much, which fit much better into this conceptual road story where is the vocabulary of a child at two, maybe one where this kind of groundedness to sensory motor experience is a much better account. And this is not problematic for me. Why do we need to have one exclusive theory for meaning? Both of these are aspects of meaning. Yeah, I agree. So I think that you can think of the physical reference, as in some sense one of the conceptual roles that something can have. It is important. I'm not sure we know kind of how abstract kids early meanings are for those kinds of words, because it has to be a little bit abstract because you'll call a new bottle that you see a bottle still. So you have some abstraction away from the examples of bottles that you've seen, but I agree it feels early on very concrete and much less abstract than things we come later. Just trying to make sure I understand what this theory is saying. So is there a character to think of this that you're saying that meaning is basically like some homomorphism onto some either intuitive or formal theory? Ah, sure. So then maybe a follow up question is like, how do we know which homomorphisms are valid? Because I could always, if I can have some arbitrary correspondence mapping, I could make anything correspond to anything else. You know what's so loud here? Yeah, so I don't think anybody has been that formal. People like Putnam have made this kind of argument about understanding computation in physical systems, basically saying like physical systems, like a brain or in his example, a wall, right? Are so complicated that I could come up with kind of any mapping back and forth between the states of it and the states of the kind of arbitrary computational system. And that's probably a much longer thing to get into. I'll just say that I don't think I have a very easy answer about that, right? I think of this as not kind of, certainly not formalized in that sense, but in sort of a higher level in terms of like what kinds of theories we should be looking for, right? And then there's lots of work to do in terms of making that precise. So yeah, yeah. Yeah, Quine actually uses that example to motivate this kind of theory in like 1950s philosophy. Sorry, which example? Quine uses this example. He uses an example of Gavagai. You see something popping out and you're like, how do you know Gavagai means rabbit, not running, and not hole, and not something else because the real world doesn't determine what a meaning is. Yeah, yeah. Yeah. The hierarchical concepts and semantics have been extensively terminated, scripting orders and so on. Has any of this been operationalized at all? Can you comment on that? I don't think so, yeah. So I mean, I can talk, I have a couple of examples of kind of learning intuitive theories, which essentially have this kind of character, so of taking data and then trying to come up with some structures that obey the right relations, right? And yeah, I'll talk a little bit about that, but there hasn't been a ton of work on that, so yeah. Could you talk about how this theory deals with when the same symbols or words are in different kind of theories or settings? Is it kind of mean that the symbols themselves don't have meaning or how are the kind of the meanings shared across different contexts? Yeah, so that's an interesting question that I think people have not resolved very well. So your symbol for your father might play a bunch of different roles, right? Because you know what job your father has and you know what family relations and you know what hobbies and I don't think that there's good kind of formalized accounts of how to make sense of all of that, so there's not great theories of kind of formalizing conceptual roles. I'll give some arguments why I think it's possible that language models are doing this at least in a tiny version, but in terms of like rich and kind of human like conceptual roles, I think that's one of the key problems that's hard to solve, so, is there another one? Yeah. I think that the oncology of the kind of physical world people think that we're using right now, for example, is a subset of the oncology of a human's mind and probably also a subset of all possible future invented concepts and so on. Sorry, what was the, I missed the very first part, what was the question part? So the question is whether you think that the existing ontology that you are using now is a subset of the ontology of human's mind that we haven't fully explored and probably that is also a subset of what kind of can be inventive or creative produced concept that you mentioned in the beginning. By ontology, do you mean these theories? I mean terms, for example, yeah, concepts. Yeah, concepts. I mean, I don't think any of these is quite the right answer, right? Like these things are actually very difficult to figure out, but I think they're kind of pointing in some useful directions or something. So I don't know if that answers your question, but yeah. And if you know the symbols that you're using, why does it matter? Because that's not to define certain meaning, whether you use words to represent or you find to represent, it doesn't matter, right? In terms of which symbols, like mental representations or? Yeah, without a concept, whether you use the words to represent that or you think that's fine, you think that's all that comes out. Or like, I'm sure that this kind of concept has a lot to do with the form, but I think it's just a little bit of a question. Yeah, so yeah. Just to make sure that you, how far do you still have to, like do you have any? I have a little ways to go. Okay, so maybe we should push this the word after, after at the end, because it seems like a deeper discussion. Yeah, yeah, okay, great. Okay, so I talked about these kinds of accounts of meaning, particular meaning as conceptual role. And let me talk a little bit about learning conceptual roles and why we might think that's plausible or useful. Seems to me at least that large language models almost certainly need to learn some of these pieces of conceptual role, that these kinds of things seem really necessary for the stuff large language models are good at, right? Writing coherent texts or doing translations or providing definitions or providing elaborations or explanations, all of those things require you to put symbols in the right relationships with other symbols, right? And that means that to do those things well, you essentially have to have some little components of conceptual roles, right? One way to think about this is that human meanings or human conceptual roles generated the text, right? So maybe a smart inferential model could invert that and figure out what were the likely conceptual roles that generated the thing that I saw. I like this, the Stringer quote, right? The structure of sentences serves as an image of the structure of thoughts, right? Some projection of our thoughts or our meanings, our conceptual roles that gets realized into sentences. Yeah. Yeah, so are you gonna follow up on something that was a great, a recontextuality kind of example? Is that a recontextuality kind of thing? Which examples are you talking about? Are you gonna recontextualize, let's say a gentleman's two boxes example, or is that a hyper-projection? No, I wasn't gonna go back to that. Yeah, but I'll talk about a study in Large-Range Models in a minute, okay. Okay, a lot of people have the intuition this is not possible. I think this is kind of the Bender and Marcus intuition that our thoughts really get projected into this kind of impoverished sequence of sounds, right? How could you discover something like rich conceptual roles there, right? If you just have this projection of language, how could that ever support rich and interesting kinds of conceptual roles? One kind of way that I think is a helpful analogy, although not kind of a mathematically precise implementation or something, people may know these embedding theorems from dynamical systems, which I think are very cool. There's this paper called Geometry from a Time Series, which essentially shows that in some cases you can take projections of dynamical systems and recover things which capture the structure of the dynamics from that projection. So in particular, in this paper, they go through this, which is the Rossler Attractor. This is a three-dimensional system of differential equations. And what you can do is take a one-dimensional projection of those dynamics. So you can look at just the X location of what's happening there. And through a clever trick essentially translating the one-dimensions into three-dimensions using, by going backwards in time, some number of steps, you can actually recover the structure of this from the one-dimensional projection. And there's other kind of general theorems about when this is possible, Parkinson's embedding theorem and things like that. The point here is that we shouldn't really have strong intuitions about what's possible from some projection of thoughts, because oftentimes there might be possible for people to, or for learning models to reconstruct kind of interesting parts of the structure of some system just from simple kind of measurements of that system. Actually Shaw here, the senior author wrote an entire book on recovering the kind of dynamical properties of a dripping water faucet, where you can measure the time between drips and figure out things about the kind of latent variables and latent structures they're using techniques that are a lot like these. In psychology, actually people have also been interested in kind of closely related types of models. So there's this work by Roger Shepard in the 80s, which essentially would take behavioral judgments and try to infer the underlying structures behind them. So for example, this matrix here is different colors or different wavelengths of light and then confusability between them on judgment tasks. So just how similar are these things or how confusable is one color with another. And Shepard was using multi-dimensional scaling to go from data like this up to representation like this, which you might recognize as a color wheel. Basically you can arrange points so that their distances correspond to the distances in the confusion matrix and therefore recover something about the kind of underlying, in this case, psychological structure that generated that data. People have also done similar kinds of things in learning kind of real formalized versions of theories or of intuitive theories. I really like this paper by Tomer Ullmann and Noah Goodman and Josh who's speaking next on learning a theory of magnetism. So basically you take observations of which objects interact with other objects and do some learning to acquire a kind of high level theory of the fact that there are two different kinds of magnetic objects and those two different kinds of things will interact with each other, but they won't interact with things that are non-magnetic. So this is like a little tiny mini intuitive theory that you can acquire just from very simple, you might think kind of impoverished data about interactions. So when people talk about LOMs just being based on text, I think that isn't really enough to conclude anything about what theories they might induce, or what kinds of internal structures and conceptual roles they might induce from that text. And in fact, there's some evidence I think that what they are inducing looks pretty plausible at least in kind of simple domains. So there's this paper by Grandin and colleagues which essentially looked at word embedding vectors and projected them onto say intuitive dimensions. So here you have a bunch of words, you project them onto this line, which is the line connecting small and large. Okay, so all of our high dimensional word vectors get projected onto the small versus large line. And we take that as a way of measuring how large versus small different objects are. And then the question is, is in a model trained only on text prediction, is that, does that projection recover anything human like about the underlying conceptual spaces? And they show yes it does. So here's six plots where the x-axis is the semantic projection, right? So how far on that small to large line something is. And then the y-axis is human ratings of how small versus large an object is, right? You can see that the correlations here are not perfect but they're also not garbage, right? They're actually quite strong I think for a model like this that things which the model calls wet versus dry or big versus small or dangerous versus safe, people also agree with, right? So just in predicting text, this thing has recovered these kinds of aspects of semantic structure latent in the word vector representations, yeah. But this is probably sitting there in n-grams and in bi-grams in fact, that information, right? It doesn't have to do anything with the real world. Well, it does have something to do with the real world. It's like a small and large could just be linguistic constructs and you're testing it on language. Oh, I see, you think it's that you say small tiger versus large tiger or something. Small puppy, right? Puppy is always small, tiger is always large, yeah. So it might be true in n-grams, I'm not sure. I don't think that they looked at that. I don't think that defeats the argument though, right? Because I think it is the case that even n-gram statistics are statistics about word relations, right? So it might be that you don't need fancy language models or something to do this. But you're not actually like, you don't need real world for this to work. Well, the real world generated how often you hear small puppy versus large puppy, right? So the real world is mirrored in those statistics and then the configuration that system comes up with is also one that mirrors those properties of the real world. Yeah. Can I put back on that as well? I do kind of feel like what others are saying is right, but this is much closer to n-grams than it is to large language models. And the properties we're seeing are just so much wilder than any of these embedding tricks in practice. Oh, you mean that large language model is much smarter than this? I don't even see how this is comparable in a way, right? Like this pops out of PCA, whereas we're seeing these wild emergent behaviors come out of large language models. Yeah, yeah, so I mean, I don't think this explains wild emergent behaviors. I think that this was just trying to say that when you train on text prediction, you configure yourself to align with some of the true properties of the world, which are reflected in the text analysis. That's all, yeah. Okay, so I'm short on time. I'll skip this. I'll just say that there's other papers looking at transformers and kind of how they relate to classic studies on concepts in cognitive science, classic kinds of effects, but I'll skip that. Maybe I'll go very briefly just through this kind of fun experiment. This is Mark Gorenstein in my lab has been interested in learning concepts just from linguistic experience, maybe linguistic prediction. He's been doing these kind of cool experiments where we give people passages of natural language where there's some blanks. So here's a passage. The myth of blank is so powerful that the very words conjure up blank, of strudel and blank in a cozy Vietnese cafe, blah, blah, blah. And the job of participants in this is to learn where to put the word DAX. Okay, so DAX is a novel word they've never encountered before. You have to read this and understand the context and stuff in order to figure that out and see where DAX should go. Secretly behind the scenes, this example has been chosen as just from a big corpus of text of a really rare word that people probably don't know. So the rare word here is soccer tort. And that means that this language, like where soccer tort actually occurred here, was generated from real people and presumably reflects the underlying meaning and things of soccer tort. Maybe I don't know if I'm saying that correctly. But with enough examples of these people will learn where DAX is, they get feedback on whether or not they were correct, according to whether they chose the place where soccer tort actually appeared. Okay, so we're having them do kind of a version of a prediction task, trying to figure out where this word goes, but they don't actually see the word, they see it as DAX. People get pretty decent at this, so up to 80% or so, depending on the word. These are just, sorry, these are the examples of the words which generated the unseen context. And after that, we asked them a bunch of questions. So we asked them some reading comprehension, we asked them feature questions about DAX, is DAX a man-made object? Do biologists typically study DAX? Do people use DAX in painting? Just a whole collection of basic feature, kind of concept-y questions. We give them an image recognition task, right, picking out soccer tort, the real thing versus alternatives. And we also asked them for explicit definitions, right? These contexts are not definitions, they're not saying here's what a soccer tort is, it's naturalistic usages of the object. And what we find actually is within, within about 20 trials or so, sorry, everything is after 20 trials, people are actually very, very good at judging conceptual features for these concepts, almost at ceiling in most of the kinds of feature questions we asked them. Here's each word on a row, and then features here on the x-axis, almost everything is read, meaning they're good at this. They're also good at picking which picture is the object, so they've never encountered any pictures at all, but they're 80% or so good at picking these things out. And they're even good at giving definitions for these terms, okay? So here's a dictionary or dictionary or something definition of soccer tort, it's a chocolate cake or tort of Austrian origin invented by Franz soccer supposedly in 1832 for some prince in Vienna. And people just from these contexts, 20 of them will learn things like it's a chocolate dessert similar to a cake that was originally and most commonly made in Vienna. I think it's a type of chocolate cake that can be ordered for dessert in Austria, kind of rich chocolate cake from Vienna and so on, okay? So people are pretty good at taking kind of in-context language use and figuring out underlying aspects of conceptual representation from that. But they will never be able to taste it. I mean, it tastes so good. Have you had it? I've never heard of it, so okay. It's great. Okay, so let me just wrap up here. So I think of these kind of conceptual roles or theories as really both a strength and a weakness of these current large language models. One is that large language models, I think, seem very good at learning kind of shallow but broad theories, right? So things like could shoes be made out of eggplants or what would happen if shoes were made out of eggplants, right? They would know what some bad downsides of that kind of thing might be, right? Or answer basic kinds of questions that might rely on kind of reasoning through one or two kind of links about the relationships between the objects involved in a situation like that. I think it's been very surprising to people that this works so well, right? And part of, I think, what makes it surprising is that these models are able to be trained on a huge number of words, right? And so sort of superficially knowing a little bit about conceptual roles of a huge number of words seems to get you pretty far in terms of seeming convincing and in terms of language production. But it's also these conceptual roles and theories are also weakness and they don't seem very good at robust and precise theories, right? So if you think about conceptual roles like in mathematics, right? How you define say a natural number or how you define an integral or something, right? Like all of those are symbolic kinds of theories which are precise and which support chains of reasoning of arbitrary length, right? And that's what these systems really seem not to be very good at. Likely that's because there's some important things which are missing, right? Things like grounding, things like reasoning or even richer kinds of theories. I think Josh will talk about this some next. I think that there's a kind of broader view of concepts and meanings, which is really, I think the most exciting for people that work on concepts and concept representations in cognitive psychology, which is that large language models have really shown how vectors can do things that were long thought to be impossible for non-symbolic models, right? In particular, these kinds of arguments from people like Fodor and Polition about compositionality and systematicity and productivity, right? All of these kinds of things that people have pointed to as characteristic features of thinking have argued were characteristic features of symbolic thinking just turn out not to be right, right? It turns out you can get vectors to do those things. And I would argue that the solution to why vectors could do those things is probably that what these models are doing is training vectors that encode conceptual roles, right? Like what they're learning is representations of meaning which capture the important parts of conceptual roles. This is actually something which has been long sought after in say computational neuroscience. There's things like vector symbolic architectures that are very exciting ways of encoding say arbitrary symbolic systems or arbitrary mathematical systems into vectors. And I think that some marriage of those two things is going to be very exciting. So large language models point to a theory of meaning that's based on essentially vector based conceptual roles, and perhaps can capture a lot of the different features of meaning that people in say cognitive science or cognitive development have tried to kind of bring out in human conceptual systems, right? Like that our meanings are gradient or that they have hierarchies, that we know things like definitions and we can make inferences about relationships and similarities and all of those things seem like things that you can encode at least in principle in vectors which is great. So let's get that, let me just end there. I'll thank you again for the invitation and thanks also to all of my co-authors on the work here. All right. Yes. Maybe I'm reading too much into it but it seemed to me that you hinted at what these vector representations large language models tell us about both human cognition as well as about language, the nature of language. Could I ask you to do a projective measurement and come out and say something about that? I think that they tell us that vectors are really plausible. They kind of show us how vectors are plausible for meanings, and the way in which I think that they're plausible for meanings is that they encode conceptual roles. That's what I would say. And I think until them, until kind of recent deep learning, I think it was really unclear. So people had argued for decades about whether the foundation of concepts was definitions or is it like somehow similarities or is it that you just know a word and you know a bunch of associated features or whatever. And I think one of the main insights, for example, is that you can extract a definition from a large language model. We've even given it some of these kind of human experiments we've done and they're pretty good at coming up with the chocolate torch kind of definitions from those. And that tells you that the definitions can be encoded into vectors, right? And that's great, right? That means that you don't need to think about definitions as the defining part of concepts, right? There's some other kind of more abstract, you know, high dimensional space or whatever that defines the meanings. And the sense in which it defines the meanings is in terms of the relationships between vectors on the tasks that you use the concepts for. Is it just natural since the brain encodes information with lots of neurons firing through your brain? This should not be surprising. So it's not surprising, it was always unclear how that was even possible. Yeah, exactly, yeah, yeah, yeah. It's like everybody always kind of knew that there had to be a continuous system which could support these things. But when you look at, you know, the discreteness in language or this discreteness in mathematics, it was always kind of unclear where that could come from. So that's why I think things like vector symbolic architectures are very exciting too. So when a human fills in the meaning, they're using a lot of context that they've gotten from the real world. When an octopus tries to fill in the meaning, they have much less context to work with. And when a LLM fills in the meaning, they have no kind of context to work with. So I was wondering if you had thoughts about the differences and that implies. It's really interesting to think of what's exactly happening in that human experiment because I agree it's transfer of stuff you know, like you've encountered cakes and you've encountered fancy pastries or whatever. And part of what you know about those meanings are the grounded parts, right? You know what a cake looks like, which is why you can recognize the pictures and things. I always have a little bit of trouble thinking about it because it's never quite clear to me exactly what it means to transfer something grounded. Like it feels a little bit like in order for it to transfer at all, it has to be a little bit abstract. But I agree that that's the right question to ask and we don't have any theories or certainly no evidence about how exactly people solve that problem or the way in which it relies on conceptual roles versus grounded experience or something. Okay, so we'll have one more question. Meanwhile, maybe we can have the next speakers start setting up. Yeah, thanks Steve for the great talk. I wanted to understand better what the argument was in this kind of conceptual embedding experiment because it seems like you could just ask the LLM whether it's tall or not. Like you didn't really need to do this projection to know that it can do this task. So is it somehow, is there something special about the fact that you're looking at embeddings rather than the outputs or what's kind of going on there? That's a good question. So I think you probably could do that. I don't know how the results would compare if you just asked versus not. Yeah, I'm not sure. I mean, I think it's like if it doesn't succeed on just asking, then it's interesting to know whether it's kind of latent representation still has that information or not. So, but I don't know the whole space of kind of how you, how you can interrogate these models for those questions, so. All right, let's thank Steve again. We'll have plenty of time to talk to him more at the refreshments after this talk. And who knows, maybe there will be Zafar Turkish in there. Which is, by the way, amazing. If you're in Vienna, you should absolutely try it. So unfortunately, the next speaker could not be here. Josh Tenenbaum is a latent variable in this session because both Stephen was a student of Josh's and...", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.0, "text": " And we are going to see connections of language models", "tokens": [50364, 400, 321, 366, 516, 281, 536, 9271, 295, 2856, 5245, 50614], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 1, "seek": 0, "start": 5.18, "end": 8.08, "text": " that maybe you did not quite expect to anticipate,", "tokens": [50623, 300, 1310, 291, 630, 406, 1596, 2066, 281, 21685, 11, 50768], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 2, "seek": 0, "start": 8.08, "end": 9.08, "text": " but they are very real.", "tokens": [50768, 457, 436, 366, 588, 957, 13, 50818], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 3, "seek": 0, "start": 9.08, "end": 11.92, "text": " And to start with, we are very lucky to have", "tokens": [50818, 400, 281, 722, 365, 11, 321, 366, 588, 6356, 281, 362, 50960], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 4, "seek": 0, "start": 11.92, "end": 15.88, "text": " Steven Pantadosi, he is a professor of psychology", "tokens": [50960, 12754, 430, 394, 4181, 72, 11, 415, 307, 257, 8304, 295, 15105, 51158], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 5, "seek": 0, "start": 15.88, "end": 17.52, "text": " and neuroscience at UC Berkeley.", "tokens": [51158, 293, 42762, 412, 14079, 23684, 13, 51240], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 6, "seek": 0, "start": 17.52, "end": 22.52, "text": " And a friend of AI, we actually have a grant together.", "tokens": [51240, 400, 257, 1277, 295, 7318, 11, 321, 767, 362, 257, 6386, 1214, 13, 51490], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 7, "seek": 0, "start": 23.48, "end": 28.48, "text": " And a lot of us in AI are excited and interested", "tokens": [51538, 400, 257, 688, 295, 505, 294, 7318, 366, 2919, 293, 3102, 51788], "temperature": 0.0, "avg_logprob": -0.22714988472535438, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.026133382692933083}, {"id": 8, "seek": 2848, "start": 29.48, "end": 32.08, "text": " in learning from the psychologists", "tokens": [50414, 294, 2539, 490, 264, 41562, 50544], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 9, "seek": 2848, "start": 32.08, "end": 35.2, "text": " and seeing how they can inspire our work.", "tokens": [50544, 293, 2577, 577, 436, 393, 15638, 527, 589, 13, 50700], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 10, "seek": 2848, "start": 35.2, "end": 37.32, "text": " Well, actually, this is a two-way street", "tokens": [50700, 1042, 11, 767, 11, 341, 307, 257, 732, 12, 676, 4838, 50806], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 11, "seek": 2848, "start": 37.32, "end": 40.32, "text": " and because the psychologists are also getting excited", "tokens": [50806, 293, 570, 264, 41562, 366, 611, 1242, 2919, 50956], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 12, "seek": 2848, "start": 40.32, "end": 43.28, "text": " about the language models to understand", "tokens": [50956, 466, 264, 2856, 5245, 281, 1223, 51104], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 13, "seek": 2848, "start": 43.28, "end": 44.28, "text": " something about humans.", "tokens": [51104, 746, 466, 6255, 13, 51154], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 14, "seek": 2848, "start": 44.28, "end": 47.84, "text": " And Steven will tell us about some of the work", "tokens": [51154, 400, 12754, 486, 980, 505, 466, 512, 295, 264, 589, 51332], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 15, "seek": 2848, "start": 47.84, "end": 49.8, "text": " he has been doing, which I think is very exciting.", "tokens": [51332, 415, 575, 668, 884, 11, 597, 286, 519, 307, 588, 4670, 13, 51430], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 16, "seek": 2848, "start": 49.8, "end": 50.64, "text": " So, Steven.", "tokens": [51430, 407, 11, 12754, 13, 51472], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 17, "seek": 2848, "start": 52.72, "end": 55.84, "text": " So thank you for the invitation to speak here.", "tokens": [51576, 407, 1309, 291, 337, 264, 17890, 281, 1710, 510, 13, 51732], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 18, "seek": 2848, "start": 55.84, "end": 57.56, "text": " I'm going to be talking about the meaning", "tokens": [51732, 286, 478, 516, 281, 312, 1417, 466, 264, 3620, 51818], "temperature": 0.0, "avg_logprob": -0.17621074404035295, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0025054351426661015}, {"id": 19, "seek": 5756, "start": 57.56, "end": 59.04, "text": " in the age of large language models", "tokens": [50364, 294, 264, 3205, 295, 2416, 2856, 5245, 50438], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 20, "seek": 5756, "start": 59.04, "end": 60.64, "text": " and maybe finding meaning in the age", "tokens": [50438, 293, 1310, 5006, 3620, 294, 264, 3205, 50518], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 21, "seek": 5756, "start": 60.64, "end": 61.480000000000004, "text": " of large language models.", "tokens": [50518, 295, 2416, 2856, 5245, 13, 50560], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 22, "seek": 5756, "start": 61.480000000000004, "end": 64.88, "text": " And this talk isn't a kind of technical talk", "tokens": [50560, 400, 341, 751, 1943, 380, 257, 733, 295, 6191, 751, 50730], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 23, "seek": 5756, "start": 64.88, "end": 68.64, "text": " about language models or evaluation or anything.", "tokens": [50730, 466, 2856, 5245, 420, 13344, 420, 1340, 13, 50918], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 24, "seek": 5756, "start": 68.64, "end": 72.04, "text": " It's almost closer to philosophy", "tokens": [50918, 467, 311, 1920, 4966, 281, 10675, 51088], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 25, "seek": 5756, "start": 72.04, "end": 74.36, "text": " or closer to kind of high level theories", "tokens": [51088, 420, 4966, 281, 733, 295, 1090, 1496, 13667, 51204], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 26, "seek": 5756, "start": 74.36, "end": 77.2, "text": " in cognitive science and psychology,", "tokens": [51204, 294, 15605, 3497, 293, 15105, 11, 51346], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 27, "seek": 5756, "start": 77.2, "end": 78.88, "text": " which are about meaning.", "tokens": [51346, 597, 366, 466, 3620, 13, 51430], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 28, "seek": 5756, "start": 78.88, "end": 80.68, "text": " And some of the kind of history of work", "tokens": [51430, 400, 512, 295, 264, 733, 295, 2503, 295, 589, 51520], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 29, "seek": 5756, "start": 80.68, "end": 84.28, "text": " of people's ideas about where meaning comes from", "tokens": [51520, 295, 561, 311, 3487, 466, 689, 3620, 1487, 490, 51700], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 30, "seek": 5756, "start": 84.28, "end": 86.72, "text": " in language or in semantics", "tokens": [51700, 294, 2856, 420, 294, 4361, 45298, 51822], "temperature": 0.0, "avg_logprob": -0.15227442128317698, "compression_ratio": 1.8541666666666667, "no_speech_prob": 0.0009391974890604615}, {"id": 31, "seek": 8672, "start": 86.72, "end": 87.8, "text": " and how we can think about those", "tokens": [50364, 293, 577, 321, 393, 519, 466, 729, 50418], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 32, "seek": 8672, "start": 87.8, "end": 89.48, "text": " in the context of large language models", "tokens": [50418, 294, 264, 4319, 295, 2416, 2856, 5245, 50502], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 33, "seek": 8672, "start": 89.48, "end": 91.64, "text": " and in particular in the context of debates", "tokens": [50502, 293, 294, 1729, 294, 264, 4319, 295, 24203, 50610], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 34, "seek": 8672, "start": 91.64, "end": 93.0, "text": " about large language models", "tokens": [50610, 466, 2416, 2856, 5245, 50678], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 35, "seek": 8672, "start": 93.0, "end": 96.24, "text": " and whether they're just stochastic parrots", "tokens": [50678, 293, 1968, 436, 434, 445, 342, 8997, 2750, 971, 81, 1971, 50840], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 36, "seek": 8672, "start": 96.24, "end": 98.92, "text": " that don't have any form of understanding", "tokens": [50840, 300, 500, 380, 362, 604, 1254, 295, 3701, 50974], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 37, "seek": 8672, "start": 98.92, "end": 101.36, "text": " or whether there's some sense in which they have understanding", "tokens": [50974, 420, 1968, 456, 311, 512, 2020, 294, 597, 436, 362, 3701, 51096], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 38, "seek": 8672, "start": 101.36, "end": 104.2, "text": " and if they do have some form of understanding,", "tokens": [51096, 293, 498, 436, 360, 362, 512, 1254, 295, 3701, 11, 51238], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 39, "seek": 8672, "start": 104.2, "end": 106.8, "text": " if that form of understanding is at all related", "tokens": [51238, 498, 300, 1254, 295, 3701, 307, 412, 439, 4077, 51368], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 40, "seek": 8672, "start": 106.8, "end": 110.8, "text": " to the types of understanding that people have.", "tokens": [51368, 281, 264, 3467, 295, 3701, 300, 561, 362, 13, 51568], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 41, "seek": 8672, "start": 110.8, "end": 114.68, "text": " So my plan for the talk is first to talk about meaning", "tokens": [51568, 407, 452, 1393, 337, 264, 751, 307, 700, 281, 751, 466, 3620, 51762], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 42, "seek": 8672, "start": 114.68, "end": 116.12, "text": " and reference kind of generally.", "tokens": [51762, 293, 6408, 733, 295, 5101, 13, 51834], "temperature": 0.0, "avg_logprob": -0.10121902078390121, "compression_ratio": 2.0669291338582676, "no_speech_prob": 0.0002164616307709366}, {"id": 43, "seek": 11612, "start": 116.12, "end": 120.08, "text": " I'll talk about some claims in large language models", "tokens": [50364, 286, 603, 751, 466, 512, 9441, 294, 2416, 2856, 5245, 50562], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 44, "seek": 11612, "start": 120.08, "end": 122.84, "text": " and why people often think that there's no sense", "tokens": [50562, 293, 983, 561, 2049, 519, 300, 456, 311, 572, 2020, 50700], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 45, "seek": 11612, "start": 122.84, "end": 126.64, "text": " of meaning or kind of real semantics in them.", "tokens": [50700, 295, 3620, 420, 733, 295, 957, 4361, 45298, 294, 552, 13, 50890], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 46, "seek": 11612, "start": 126.64, "end": 130.84, "text": " Then I'll talk about some psychological theories", "tokens": [50890, 1396, 286, 603, 751, 466, 512, 14346, 13667, 51100], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 47, "seek": 11612, "start": 130.84, "end": 132.76, "text": " about meaning and how meaning arises", "tokens": [51100, 466, 3620, 293, 577, 3620, 27388, 51196], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 48, "seek": 11612, "start": 132.76, "end": 135.12, "text": " from what are called conceptual roles.", "tokens": [51196, 490, 437, 366, 1219, 24106, 9604, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 49, "seek": 11612, "start": 136.0, "end": 137.4, "text": " I'll come back to large language models", "tokens": [51358, 286, 603, 808, 646, 281, 2416, 2856, 5245, 51428], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 50, "seek": 11612, "start": 137.4, "end": 141.20000000000002, "text": " and talk a little bit about learning conceptual roles", "tokens": [51428, 293, 751, 257, 707, 857, 466, 2539, 24106, 9604, 51618], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 51, "seek": 11612, "start": 141.20000000000002, "end": 142.36, "text": " in large language models", "tokens": [51618, 294, 2416, 2856, 5245, 51676], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 52, "seek": 11612, "start": 142.36, "end": 144.84, "text": " or in kind of general machine learning systems.", "tokens": [51676, 420, 294, 733, 295, 2674, 3479, 2539, 3652, 13, 51800], "temperature": 0.0, "avg_logprob": -0.13143519254831168, "compression_ratio": 1.9598214285714286, "no_speech_prob": 0.00016861782933119684}, {"id": 53, "seek": 14484, "start": 145.08, "end": 147.56, "text": " And then some very kind of brief overview", "tokens": [50376, 400, 550, 512, 588, 733, 295, 5353, 12492, 50500], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 54, "seek": 14484, "start": 147.56, "end": 151.92000000000002, "text": " of a learning conceptual role experiment in people.", "tokens": [50500, 295, 257, 2539, 24106, 3090, 5120, 294, 561, 13, 50718], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 55, "seek": 14484, "start": 151.92000000000002, "end": 156.8, "text": " So let me start with kind of how I got interested in this", "tokens": [50718, 407, 718, 385, 722, 365, 733, 295, 577, 286, 658, 3102, 294, 341, 50962], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 56, "seek": 14484, "start": 156.8, "end": 160.32, "text": " which was this paper by Bender and Kohler in 2020.", "tokens": [50962, 597, 390, 341, 3035, 538, 363, 3216, 293, 30861, 1918, 294, 4808, 13, 51138], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 57, "seek": 14484, "start": 160.32, "end": 163.68, "text": " Emily Bender is a computer scientist", "tokens": [51138, 15034, 363, 3216, 307, 257, 3820, 12662, 51306], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 58, "seek": 14484, "start": 163.68, "end": 167.56, "text": " trained as a linguist also at the University of Washington", "tokens": [51306, 8895, 382, 257, 21766, 468, 611, 412, 264, 3535, 295, 6149, 51500], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 59, "seek": 14484, "start": 167.56, "end": 169.76, "text": " who people may know as a very vocal critic", "tokens": [51500, 567, 561, 815, 458, 382, 257, 588, 11657, 7850, 51610], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 60, "seek": 14484, "start": 169.76, "end": 173.48000000000002, "text": " of many aspects of large language models.", "tokens": [51610, 295, 867, 7270, 295, 2416, 2856, 5245, 13, 51796], "temperature": 0.0, "avg_logprob": -0.11062331994374593, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.0001233610528288409}, {"id": 61, "seek": 17348, "start": 173.51999999999998, "end": 176.11999999999998, "text": " The one that initially I think interested me", "tokens": [50366, 440, 472, 300, 9105, 286, 519, 3102, 385, 50496], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 62, "seek": 17348, "start": 176.11999999999998, "end": 179.72, "text": " was claims about the meaningfulness of large language models", "tokens": [50496, 390, 9441, 466, 264, 10995, 1287, 295, 2416, 2856, 5245, 50676], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 63, "seek": 17348, "start": 179.72, "end": 183.6, "text": " and essentially arguments that there's nothing meaningful", "tokens": [50676, 293, 4476, 12869, 300, 456, 311, 1825, 10995, 50870], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 64, "seek": 17348, "start": 183.6, "end": 186.76, "text": " at all in what statistical models", "tokens": [50870, 412, 439, 294, 437, 22820, 5245, 51028], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 65, "seek": 17348, "start": 186.76, "end": 189.0, "text": " that are trained on text can do.", "tokens": [51028, 300, 366, 8895, 322, 2487, 393, 360, 13, 51140], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 66, "seek": 17348, "start": 190.07999999999998, "end": 192.6, "text": " And Bender and Kohler came up with a very nice way", "tokens": [51194, 400, 363, 3216, 293, 30861, 1918, 1361, 493, 365, 257, 588, 1481, 636, 51320], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 67, "seek": 17348, "start": 192.6, "end": 197.6, "text": " of making this point, what they call the octopus test.", "tokens": [51320, 295, 1455, 341, 935, 11, 437, 436, 818, 264, 27962, 1500, 13, 51570], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 68, "seek": 17348, "start": 197.6, "end": 199.56, "text": " The octopus test goes as follows.", "tokens": [51570, 440, 27962, 1500, 1709, 382, 10002, 13, 51668], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 69, "seek": 17348, "start": 199.56, "end": 202.44, "text": " So kind of starting point is for them,", "tokens": [51668, 407, 733, 295, 2891, 935, 307, 337, 552, 11, 51812], "temperature": 0.0, "avg_logprob": -0.10199185173110206, "compression_ratio": 1.642570281124498, "no_speech_prob": 2.976725227199495e-05}, {"id": 70, "seek": 20244, "start": 202.44, "end": 206.0, "text": " meaning is an association between a word, say,", "tokens": [50364, 3620, 307, 364, 14598, 1296, 257, 1349, 11, 584, 11, 50542], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 71, "seek": 20244, "start": 206.0, "end": 208.0, "text": " and something external to language.", "tokens": [50542, 293, 746, 8320, 281, 2856, 13, 50642], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 72, "seek": 20244, "start": 208.0, "end": 210.12, "text": " Okay, so the meaning of the book", "tokens": [50642, 1033, 11, 370, 264, 3620, 295, 264, 1446, 50748], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 73, "seek": 20244, "start": 210.12, "end": 214.4, "text": " is some physical object, a book that's out in the world.", "tokens": [50748, 307, 512, 4001, 2657, 11, 257, 1446, 300, 311, 484, 294, 264, 1002, 13, 50962], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 74, "seek": 20244, "start": 214.4, "end": 215.4, "text": " You can see that that's right", "tokens": [50962, 509, 393, 536, 300, 300, 311, 558, 51012], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 75, "seek": 20244, "start": 215.4, "end": 217.92, "text": " because that's how we label physical objects", "tokens": [51012, 570, 300, 311, 577, 321, 7645, 4001, 6565, 51138], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 76, "seek": 20244, "start": 217.92, "end": 219.32, "text": " and when we're learning words, right?", "tokens": [51138, 293, 562, 321, 434, 2539, 2283, 11, 558, 30, 51208], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 77, "seek": 20244, "start": 219.32, "end": 221.4, "text": " We hear the word book when there's books around", "tokens": [51208, 492, 1568, 264, 1349, 1446, 562, 456, 311, 3642, 926, 51312], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 78, "seek": 20244, "start": 221.4, "end": 222.96, "text": " and we pick up on that association.", "tokens": [51312, 293, 321, 1888, 493, 322, 300, 14598, 13, 51390], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 79, "seek": 20244, "start": 222.96, "end": 224.56, "text": " And so that's kind of fundamentally", "tokens": [51390, 400, 370, 300, 311, 733, 295, 17879, 51470], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 80, "seek": 20244, "start": 224.56, "end": 227.92, "text": " what the, that kind of external reference, right?", "tokens": [51470, 437, 264, 11, 300, 733, 295, 8320, 6408, 11, 558, 30, 51638], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 81, "seek": 20244, "start": 227.92, "end": 230.68, "text": " To something in the real world is fundamentally", "tokens": [51638, 1407, 746, 294, 264, 957, 1002, 307, 17879, 51776], "temperature": 0.0, "avg_logprob": -0.13836983478430545, "compression_ratio": 1.876865671641791, "no_speech_prob": 0.0006459828000515699}, {"id": 82, "seek": 23068, "start": 231.56, "end": 232.8, "text": " what the word means.", "tokens": [50408, 437, 264, 1349, 1355, 13, 50470], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 83, "seek": 23068, "start": 234.12, "end": 235.64000000000001, "text": " And Bender and Kohler say, okay,", "tokens": [50536, 400, 363, 3216, 293, 30861, 1918, 584, 11, 1392, 11, 50612], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 84, "seek": 23068, "start": 235.64000000000001, "end": 238.52, "text": " let's take that as our definition of meaning", "tokens": [50612, 718, 311, 747, 300, 382, 527, 7123, 295, 3620, 50756], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 85, "seek": 23068, "start": 238.52, "end": 240.16, "text": " and let's imagine an octopus.", "tokens": [50756, 293, 718, 311, 3811, 364, 27962, 13, 50838], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 86, "seek": 23068, "start": 240.16, "end": 243.08, "text": " So an octopus who lives under the ocean", "tokens": [50838, 407, 364, 27962, 567, 2909, 833, 264, 7810, 50984], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 87, "seek": 23068, "start": 243.08, "end": 246.24, "text": " and has tapped into a communication channel,", "tokens": [50984, 293, 575, 38693, 666, 257, 6101, 2269, 11, 51142], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 88, "seek": 23068, "start": 246.24, "end": 250.52, "text": " say a telephone line between two islands, okay?", "tokens": [51142, 584, 257, 19800, 1622, 1296, 732, 17402, 11, 1392, 30, 51356], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 89, "seek": 23068, "start": 250.52, "end": 251.36, "text": " So the octopus is there,", "tokens": [51356, 407, 264, 27962, 307, 456, 11, 51398], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 90, "seek": 23068, "start": 251.36, "end": 253.24, "text": " it's eavesdropping on all of the communication", "tokens": [51398, 309, 311, 308, 5423, 45869, 3759, 322, 439, 295, 264, 6101, 51492], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 91, "seek": 23068, "start": 253.24, "end": 255.16, "text": " that happens between those two islands.", "tokens": [51492, 300, 2314, 1296, 729, 732, 17402, 13, 51588], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 92, "seek": 23068, "start": 255.16, "end": 257.96000000000004, "text": " You can get a huge amount of linguistic input", "tokens": [51588, 509, 393, 483, 257, 2603, 2372, 295, 43002, 4846, 51728], "temperature": 0.0, "avg_logprob": -0.13045307627895422, "compression_ratio": 1.7032520325203253, "no_speech_prob": 0.00015841265849303454}, {"id": 93, "seek": 25796, "start": 258.96, "end": 262.15999999999997, "text": " and you could imagine very smart octopus", "tokens": [50414, 293, 291, 727, 3811, 588, 4069, 27962, 50574], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 94, "seek": 25796, "start": 262.15999999999997, "end": 265.15999999999997, "text": " might be able to learn all of the statistical properties", "tokens": [50574, 1062, 312, 1075, 281, 1466, 439, 295, 264, 22820, 7221, 50724], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 95, "seek": 25796, "start": 265.15999999999997, "end": 268.44, "text": " of what's happening across that communication channel, right?", "tokens": [50724, 295, 437, 311, 2737, 2108, 300, 6101, 2269, 11, 558, 30, 50888], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 96, "seek": 25796, "start": 268.44, "end": 272.03999999999996, "text": " Might be able to learn, become very good at predicting text.", "tokens": [50888, 23964, 312, 1075, 281, 1466, 11, 1813, 588, 665, 412, 32884, 2487, 13, 51068], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 97, "seek": 25796, "start": 272.03999999999996, "end": 274.71999999999997, "text": " Maybe it could predict it optimally, whatever.", "tokens": [51068, 2704, 309, 727, 6069, 309, 5028, 379, 11, 2035, 13, 51202], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 98, "seek": 25796, "start": 275.71999999999997, "end": 278.08, "text": " Their argument is that the octopus", "tokens": [51252, 6710, 6770, 307, 300, 264, 27962, 51370], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 99, "seek": 25796, "start": 278.08, "end": 280.0, "text": " could never actually learn the meanings, right?", "tokens": [51370, 727, 1128, 767, 1466, 264, 28138, 11, 558, 30, 51466], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 100, "seek": 25796, "start": 280.0, "end": 281.32, "text": " Because it would never have access", "tokens": [51466, 1436, 309, 576, 1128, 362, 2105, 51532], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 101, "seek": 25796, "start": 281.32, "end": 283.52, "text": " to the physical reference, right?", "tokens": [51532, 281, 264, 4001, 6408, 11, 558, 30, 51642], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 102, "seek": 25796, "start": 283.52, "end": 284.35999999999996, "text": " So yeah.", "tokens": [51642, 407, 1338, 13, 51684], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 103, "seek": 25796, "start": 284.35999999999996, "end": 286.52, "text": " Is it really different from the Chinese room?", "tokens": [51684, 1119, 309, 534, 819, 490, 264, 4649, 1808, 30, 51792], "temperature": 0.0, "avg_logprob": -0.14084847491720448, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.00024151506659109145}, {"id": 104, "seek": 28652, "start": 286.68, "end": 290.03999999999996, "text": " Yeah, so it's interestingly a little different", "tokens": [50372, 865, 11, 370, 309, 311, 25873, 257, 707, 819, 50540], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 105, "seek": 28652, "start": 290.03999999999996, "end": 294.15999999999997, "text": " from the Chinese room and maybe can I defer that question", "tokens": [50540, 490, 264, 4649, 1808, 293, 1310, 393, 286, 25704, 300, 1168, 50746], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 106, "seek": 28652, "start": 294.15999999999997, "end": 295.59999999999997, "text": " to the end if that's okay?", "tokens": [50746, 281, 264, 917, 498, 300, 311, 1392, 30, 50818], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 107, "seek": 28652, "start": 296.56, "end": 298.2, "text": " Because I think what's going on with the Chinese room", "tokens": [50866, 1436, 286, 519, 437, 311, 516, 322, 365, 264, 4649, 1808, 50948], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 108, "seek": 28652, "start": 298.2, "end": 301.15999999999997, "text": " might make more sense with what I say later, okay?", "tokens": [50948, 1062, 652, 544, 2020, 365, 437, 286, 584, 1780, 11, 1392, 30, 51096], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 109, "seek": 28652, "start": 301.15999999999997, "end": 303.47999999999996, "text": " But very similar in spirit, I would say.", "tokens": [51096, 583, 588, 2531, 294, 3797, 11, 286, 576, 584, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 110, "seek": 28652, "start": 304.91999999999996, "end": 308.71999999999997, "text": " So this octopus has no access to the physical reference", "tokens": [51284, 407, 341, 27962, 575, 572, 2105, 281, 264, 4001, 6408, 51474], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 111, "seek": 28652, "start": 308.71999999999997, "end": 310.08, "text": " and therefore couldn't solve tasks", "tokens": [51474, 293, 4412, 2809, 380, 5039, 9608, 51542], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 112, "seek": 28652, "start": 310.08, "end": 311.79999999999995, "text": " involving the physical objects, right?", "tokens": [51542, 17030, 264, 4001, 6565, 11, 558, 30, 51628], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 113, "seek": 28652, "start": 311.79999999999995, "end": 314.88, "text": " If you ask them to visually recognize what a coconut was", "tokens": [51628, 759, 291, 1029, 552, 281, 19622, 5521, 437, 257, 13551, 390, 51782], "temperature": 0.0, "avg_logprob": -0.1338536841237647, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.00021641264902427793}, {"id": 114, "seek": 31488, "start": 314.88, "end": 317.44, "text": " even if they knew all of the statistical properties", "tokens": [50364, 754, 498, 436, 2586, 439, 295, 264, 22820, 7221, 50492], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 115, "seek": 31488, "start": 317.44, "end": 319.36, "text": " of where the word coconut would be used, right?", "tokens": [50492, 295, 689, 264, 1349, 13551, 576, 312, 1143, 11, 558, 30, 50588], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 116, "seek": 31488, "start": 319.36, "end": 322.96, "text": " They wouldn't be able to solve the physical stuff, okay?", "tokens": [50588, 814, 2759, 380, 312, 1075, 281, 5039, 264, 4001, 1507, 11, 1392, 30, 50768], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 117, "seek": 31488, "start": 322.96, "end": 324.32, "text": " And of course this is the situation", "tokens": [50768, 400, 295, 1164, 341, 307, 264, 2590, 50836], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 118, "seek": 31488, "start": 324.32, "end": 326.4, "text": " that a large language model is in,", "tokens": [50836, 300, 257, 2416, 2856, 2316, 307, 294, 11, 50940], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 119, "seek": 31488, "start": 326.4, "end": 329.08, "text": " at least one that's only trained on text, right?", "tokens": [50940, 412, 1935, 472, 300, 311, 787, 8895, 322, 2487, 11, 558, 30, 51074], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 120, "seek": 31488, "start": 329.08, "end": 330.92, "text": " It doesn't get access to stuff from the world,", "tokens": [51074, 467, 1177, 380, 483, 2105, 281, 1507, 490, 264, 1002, 11, 51166], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 121, "seek": 31488, "start": 330.92, "end": 335.12, "text": " it only has kind of text, okay?", "tokens": [51166, 309, 787, 575, 733, 295, 2487, 11, 1392, 30, 51376], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 122, "seek": 31488, "start": 336.2, "end": 339.6, "text": " Therefore this octopus doesn't have meanings for the words.", "tokens": [51430, 7504, 341, 27962, 1177, 380, 362, 28138, 337, 264, 2283, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 123, "seek": 31488, "start": 340.44, "end": 342.12, "text": " Such an octopus is like a large language model", "tokens": [51642, 9653, 364, 27962, 307, 411, 257, 2416, 2856, 2316, 51726], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 124, "seek": 31488, "start": 342.12, "end": 343.96, "text": " because they only have text.", "tokens": [51726, 570, 436, 787, 362, 2487, 13, 51818], "temperature": 0.0, "avg_logprob": -0.12980248377873346, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.0001559442316647619}, {"id": 125, "seek": 34396, "start": 343.96, "end": 345.44, "text": " And predictive ability therefore", "tokens": [50364, 400, 35521, 3485, 4412, 50438], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 126, "seek": 34396, "start": 345.44, "end": 347.32, "text": " can't give you the meanings, right?", "tokens": [50438, 393, 380, 976, 291, 264, 28138, 11, 558, 30, 50532], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 127, "seek": 34396, "start": 347.32, "end": 349.4, "text": " Meaning is something just fundamentally different", "tokens": [50532, 19948, 307, 746, 445, 17879, 819, 50636], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 128, "seek": 34396, "start": 349.4, "end": 351.76, "text": " than predictive ability.", "tokens": [50636, 813, 35521, 3485, 13, 50754], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 129, "seek": 34396, "start": 353.44, "end": 355.47999999999996, "text": " Let me give you one other example of this.", "tokens": [50838, 961, 385, 976, 291, 472, 661, 1365, 295, 341, 13, 50940], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 130, "seek": 34396, "start": 355.47999999999996, "end": 357.88, "text": " Well, I'll just say, I think that on the surface", "tokens": [50940, 1042, 11, 286, 603, 445, 584, 11, 286, 519, 300, 322, 264, 3753, 51060], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 131, "seek": 34396, "start": 357.88, "end": 360.56, "text": " this is a somewhat convincing argument, right?", "tokens": [51060, 341, 307, 257, 8344, 24823, 6770, 11, 558, 30, 51194], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 132, "seek": 34396, "start": 360.56, "end": 365.56, "text": " It's kind of compelling to think of meaning in this way.", "tokens": [51194, 467, 311, 733, 295, 20050, 281, 519, 295, 3620, 294, 341, 636, 13, 51444], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 133, "seek": 34396, "start": 365.84, "end": 368.96, "text": " And certainly if you do, it seems pretty convincing, yeah.", "tokens": [51458, 400, 3297, 498, 291, 360, 11, 309, 2544, 1238, 24823, 11, 1338, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16214789966545481, "compression_ratio": 1.6793248945147679, "no_speech_prob": 4.132107278564945e-05}, {"id": 134, "seek": 36896, "start": 369.79999999999995, "end": 372.03999999999996, "text": " It may not be right in terms of your mathematical history", "tokens": [50406, 467, 815, 406, 312, 558, 294, 2115, 295, 428, 18894, 2503, 50518], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 135, "seek": 36896, "start": 372.03999999999996, "end": 374.52, "text": " but it's given me a couple of axioms.", "tokens": [50518, 457, 309, 311, 2212, 385, 257, 1916, 295, 6360, 72, 4785, 13, 50642], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 136, "seek": 36896, "start": 374.52, "end": 377.64, "text": " And you know, you're not supposed to have,", "tokens": [50642, 400, 291, 458, 11, 291, 434, 406, 3442, 281, 362, 11, 50798], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 137, "seek": 36896, "start": 377.64, "end": 380.79999999999995, "text": " because this is really referring to", "tokens": [50798, 570, 341, 307, 534, 13761, 281, 50956], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 138, "seek": 36896, "start": 380.79999999999995, "end": 383.15999999999997, "text": " this particular interpretation of that one.", "tokens": [50956, 341, 1729, 14174, 295, 300, 472, 13, 51074], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 139, "seek": 36896, "start": 383.15999999999997, "end": 385.24, "text": " This would make that up in your mind.", "tokens": [51074, 639, 576, 652, 300, 493, 294, 428, 1575, 13, 51178], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 140, "seek": 36896, "start": 385.24, "end": 387.84, "text": " Yeah, great, so hold on to that thought too", "tokens": [51178, 865, 11, 869, 11, 370, 1797, 322, 281, 300, 1194, 886, 51308], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 141, "seek": 36896, "start": 387.84, "end": 389.64, "text": " because a mathematician thinking about axioms", "tokens": [51308, 570, 257, 48281, 1953, 466, 6360, 72, 4785, 51398], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 142, "seek": 36896, "start": 389.64, "end": 392.67999999999995, "text": " is very much related to another version of meaning", "tokens": [51398, 307, 588, 709, 4077, 281, 1071, 3037, 295, 3620, 51550], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 143, "seek": 36896, "start": 392.67999999999995, "end": 396.03999999999996, "text": " that I'll talk about in a few minutes, okay?", "tokens": [51550, 300, 286, 603, 751, 466, 294, 257, 1326, 2077, 11, 1392, 30, 51718], "temperature": 0.0, "avg_logprob": -0.3735054145425053, "compression_ratio": 1.637037037037037, "no_speech_prob": 0.003169568255543709}, {"id": 144, "seek": 39604, "start": 396.04, "end": 399.20000000000005, "text": " Even as Hilbert wanted it to be, right?", "tokens": [50364, 2754, 382, 19914, 4290, 1415, 309, 281, 312, 11, 558, 30, 50522], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 145, "seek": 39604, "start": 399.20000000000005, "end": 402.52000000000004, "text": " Hilbert's desire was to convert Euclidean geometry", "tokens": [50522, 19914, 4290, 311, 7516, 390, 281, 7620, 462, 1311, 31264, 282, 18426, 50688], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 146, "seek": 39604, "start": 402.52000000000004, "end": 405.36, "text": " to a set of axioms such that every symbol", "tokens": [50688, 281, 257, 992, 295, 6360, 72, 4785, 1270, 300, 633, 5986, 50830], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 147, "seek": 39604, "start": 405.36, "end": 408.12, "text": " could be replaced by some arbitrary squiggle", "tokens": [50830, 727, 312, 10772, 538, 512, 23211, 2339, 19694, 50968], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 148, "seek": 39604, "start": 408.12, "end": 410.32000000000005, "text": " and the system should still work.", "tokens": [50968, 293, 264, 1185, 820, 920, 589, 13, 51078], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 149, "seek": 39604, "start": 410.32000000000005, "end": 414.40000000000003, "text": " That is what Hilbert did in the axiomatization of geometry.", "tokens": [51078, 663, 307, 437, 19914, 4290, 630, 294, 264, 6360, 72, 298, 267, 2144, 295, 18426, 13, 51282], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 150, "seek": 39604, "start": 414.40000000000003, "end": 419.40000000000003, "text": " Yeah, so I think most mathematicians", "tokens": [51282, 865, 11, 370, 286, 519, 881, 32811, 2567, 51532], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 151, "seek": 39604, "start": 419.48, "end": 421.68, "text": " would have a slightly different sense of meaning", "tokens": [51536, 576, 362, 257, 4748, 819, 2020, 295, 3620, 51646], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 152, "seek": 39604, "start": 421.68, "end": 423.76, "text": " but one which matches what I'll say", "tokens": [51646, 457, 472, 597, 10676, 437, 286, 603, 584, 51750], "temperature": 0.0, "avg_logprob": -0.18819000524118407, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.0004440212214831263}, {"id": 153, "seek": 42376, "start": 423.96, "end": 425.76, "text": " in a few minutes, so yeah.", "tokens": [50374, 294, 257, 1326, 2077, 11, 370, 1338, 13, 50464], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 154, "seek": 42376, "start": 426.76, "end": 429.71999999999997, "text": " Let me give you just another kind of gloss", "tokens": [50514, 961, 385, 976, 291, 445, 1071, 733, 295, 19574, 50662], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 155, "seek": 42376, "start": 429.71999999999997, "end": 432.06, "text": " on meaning and language models.", "tokens": [50662, 322, 3620, 293, 2856, 5245, 13, 50779], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 156, "seek": 42376, "start": 432.06, "end": 435.2, "text": " Here's Gary Marcus talking about lambda.", "tokens": [50779, 1692, 311, 13788, 26574, 1417, 466, 13607, 13, 50936], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 157, "seek": 42376, "start": 435.2, "end": 437.76, "text": " In truth, literally everything the system says is bullshit.", "tokens": [50936, 682, 3494, 11, 3736, 1203, 264, 1185, 1619, 307, 22676, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 158, "seek": 42376, "start": 437.76, "end": 440.2, "text": " The sooner we realize that lambda's utterances are bullshit,", "tokens": [51064, 440, 15324, 321, 4325, 300, 13607, 311, 17567, 2676, 366, 22676, 11, 51186], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 159, "seek": 42376, "start": 440.2, "end": 442.0, "text": " just games with predictive word tools", "tokens": [51186, 445, 2813, 365, 35521, 1349, 3873, 51276], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 160, "seek": 42376, "start": 442.0, "end": 444.36, "text": " and no real meaning, the better off we'll be.", "tokens": [51276, 293, 572, 957, 3620, 11, 264, 1101, 766, 321, 603, 312, 13, 51394], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 161, "seek": 42376, "start": 445.48, "end": 447.32, "text": " Software like lambda doesn't even try to connect", "tokens": [51450, 27428, 411, 13607, 1177, 380, 754, 853, 281, 1745, 51542], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 162, "seek": 42376, "start": 447.32, "end": 448.56, "text": " to the world at large, right?", "tokens": [51542, 281, 264, 1002, 412, 2416, 11, 558, 30, 51604], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 163, "seek": 42376, "start": 448.56, "end": 450.88, "text": " That's what he thinks makes it bullshit.", "tokens": [51604, 663, 311, 437, 415, 7309, 1669, 309, 22676, 13, 51720], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 164, "seek": 42376, "start": 450.88, "end": 452.84, "text": " Just tries to be the best version of autocomplete", "tokens": [51720, 1449, 9898, 281, 312, 264, 1151, 3037, 295, 45833, 298, 17220, 51818], "temperature": 0.0, "avg_logprob": -0.16451729668511283, "compression_ratio": 1.6309148264984228, "no_speech_prob": 0.0002531145291868597}, {"id": 165, "seek": 45284, "start": 452.91999999999996, "end": 453.76, "text": " that it can be.", "tokens": [50368, 300, 309, 393, 312, 13, 50410], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 166, "seek": 45284, "start": 453.76, "end": 454.76, "text": " They don't understand language", "tokens": [50410, 814, 500, 380, 1223, 2856, 50460], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 167, "seek": 45284, "start": 454.76, "end": 457.71999999999997, "text": " in the sense of relating sentences to the world", "tokens": [50460, 294, 264, 2020, 295, 23968, 16579, 281, 264, 1002, 50608], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 168, "seek": 45284, "start": 457.71999999999997, "end": 461.09999999999997, "text": " but just sequences of words to one another, okay?", "tokens": [50608, 457, 445, 22978, 295, 2283, 281, 472, 1071, 11, 1392, 30, 50777], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 169, "seek": 45284, "start": 462.59999999999997, "end": 466.59999999999997, "text": " So I got interested in this in part", "tokens": [50852, 407, 286, 658, 3102, 294, 341, 294, 644, 51052], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 170, "seek": 45284, "start": 466.59999999999997, "end": 469.44, "text": " because I find this view kind of compelling.", "tokens": [51052, 570, 286, 915, 341, 1910, 733, 295, 20050, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 171, "seek": 45284, "start": 469.44, "end": 471.84, "text": " On the other hand, I also think it's kind of deeply wrong", "tokens": [51194, 1282, 264, 661, 1011, 11, 286, 611, 519, 309, 311, 733, 295, 8760, 2085, 51314], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 172, "seek": 45284, "start": 471.84, "end": 473.64, "text": " and the way in which it's deeply wrong", "tokens": [51314, 293, 264, 636, 294, 597, 309, 311, 8760, 2085, 51404], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 173, "seek": 45284, "start": 473.64, "end": 476.32, "text": " is really interesting for what it has to say", "tokens": [51404, 307, 534, 1880, 337, 437, 309, 575, 281, 584, 51538], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 174, "seek": 45284, "start": 476.32, "end": 480.03999999999996, "text": " about conceptual representations and meanings", "tokens": [51538, 466, 24106, 33358, 293, 28138, 51724], "temperature": 0.0, "avg_logprob": -0.13535409603478774, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.0001333912805421278}, {"id": 175, "seek": 48004, "start": 480.04, "end": 484.16, "text": " in human minds as well as in machine learning models.", "tokens": [50364, 294, 1952, 9634, 382, 731, 382, 294, 3479, 2539, 5245, 13, 50570], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 176, "seek": 48004, "start": 484.16, "end": 489.16, "text": " So a few years ago, I teamed up with Felix Hill,", "tokens": [50570, 407, 257, 1326, 924, 2057, 11, 286, 47426, 493, 365, 30169, 9109, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 177, "seek": 48004, "start": 490.0, "end": 492.84000000000003, "text": " who's a researcher at DeepMind", "tokens": [50862, 567, 311, 257, 21751, 412, 14895, 44, 471, 51004], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 178, "seek": 48004, "start": 492.84000000000003, "end": 497.84000000000003, "text": " and wrote an article basically going through arguments", "tokens": [51004, 293, 4114, 364, 7222, 1936, 516, 807, 12869, 51254], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 179, "seek": 48004, "start": 498.20000000000005, "end": 500.64000000000004, "text": " that meaning is not this form of reference, right?", "tokens": [51272, 300, 3620, 307, 406, 341, 1254, 295, 6408, 11, 558, 30, 51394], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 180, "seek": 48004, "start": 500.64000000000004, "end": 503.56, "text": " So in fact, this idea that meaning should be equated", "tokens": [51394, 407, 294, 1186, 11, 341, 1558, 300, 3620, 820, 312, 1267, 770, 51540], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 181, "seek": 48004, "start": 503.56, "end": 506.0, "text": " with some mapping to things in the world", "tokens": [51540, 365, 512, 18350, 281, 721, 294, 264, 1002, 51662], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 182, "seek": 48004, "start": 506.0, "end": 508.28000000000003, "text": " has often been rejected by people in linguistics", "tokens": [51662, 575, 2049, 668, 15749, 538, 561, 294, 21766, 6006, 51776], "temperature": 0.0, "avg_logprob": -0.1229426130956533, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.0003149902040604502}, {"id": 183, "seek": 50828, "start": 508.28, "end": 511.03999999999996, "text": " and philosophy and cognitive science.", "tokens": [50364, 293, 10675, 293, 15605, 3497, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 184, "seek": 50828, "start": 511.03999999999996, "end": 513.24, "text": " And I think for good reason,", "tokens": [50502, 400, 286, 519, 337, 665, 1778, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 185, "seek": 50828, "start": 513.24, "end": 514.8399999999999, "text": " so just to give you a kind of flavor", "tokens": [50612, 370, 445, 281, 976, 291, 257, 733, 295, 6813, 50692], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 186, "seek": 50828, "start": 514.8399999999999, "end": 518.72, "text": " of why people often reject this,", "tokens": [50692, 295, 983, 561, 2049, 8248, 341, 11, 50886], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 187, "seek": 50828, "start": 518.72, "end": 521.12, "text": " there's many concepts, many words, for example,", "tokens": [50886, 456, 311, 867, 10392, 11, 867, 2283, 11, 337, 1365, 11, 51006], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 188, "seek": 50828, "start": 521.12, "end": 524.3199999999999, "text": " that have no reference to the external world, right?", "tokens": [51006, 300, 362, 572, 6408, 281, 264, 8320, 1002, 11, 558, 30, 51166], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 189, "seek": 50828, "start": 524.3199999999999, "end": 526.12, "text": " Function words are a good example of this.", "tokens": [51166, 11166, 882, 2283, 366, 257, 665, 1365, 295, 341, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 190, "seek": 50828, "start": 526.12, "end": 529.04, "text": " Words like to or is or many, right?", "tokens": [51256, 32857, 411, 281, 420, 307, 420, 867, 11, 558, 30, 51402], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 191, "seek": 50828, "start": 529.04, "end": 531.68, "text": " There's no to, to, to out in the world", "tokens": [51402, 821, 311, 572, 281, 11, 281, 11, 281, 484, 294, 264, 1002, 51534], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 192, "seek": 50828, "start": 531.68, "end": 533.9599999999999, "text": " that that word refers to.", "tokens": [51534, 300, 300, 1349, 14942, 281, 13, 51648], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 193, "seek": 50828, "start": 533.9599999999999, "end": 537.36, "text": " There's also no is out in the world or no many.", "tokens": [51648, 821, 311, 611, 572, 307, 484, 294, 264, 1002, 420, 572, 867, 13, 51818], "temperature": 0.0, "avg_logprob": -0.1269981142074343, "compression_ratio": 1.7298387096774193, "no_speech_prob": 0.00033528084168210626}, {"id": 194, "seek": 53736, "start": 537.4, "end": 539.24, "text": " Those are function words in language", "tokens": [50366, 3950, 366, 2445, 2283, 294, 2856, 50458], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 195, "seek": 53736, "start": 539.24, "end": 542.6, "text": " and what they do is actually much more like", "tokens": [50458, 293, 437, 436, 360, 307, 767, 709, 544, 411, 50626], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 196, "seek": 53736, "start": 542.6, "end": 546.72, "text": " kind of what an operator in mathematics or something does,", "tokens": [50626, 733, 295, 437, 364, 12973, 294, 18666, 420, 746, 775, 11, 50832], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 197, "seek": 53736, "start": 546.72, "end": 547.5600000000001, "text": " right?", "tokens": [50832, 558, 30, 50874], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 198, "seek": 53736, "start": 547.5600000000001, "end": 549.84, "text": " These words have an internal meaning.", "tokens": [50874, 1981, 2283, 362, 364, 6920, 3620, 13, 50988], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 199, "seek": 53736, "start": 549.84, "end": 553.64, "text": " They control the kind of compositional meaning of a sentence,", "tokens": [50988, 814, 1969, 264, 733, 295, 10199, 2628, 3620, 295, 257, 8174, 11, 51178], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 200, "seek": 53736, "start": 553.64, "end": 555.88, "text": " meaning that they have to be composed", "tokens": [51178, 3620, 300, 436, 362, 281, 312, 18204, 51290], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 201, "seek": 53736, "start": 555.88, "end": 558.32, "text": " internally in linguistic representations", "tokens": [51290, 19501, 294, 43002, 33358, 51412], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 202, "seek": 53736, "start": 558.32, "end": 559.96, "text": " in order to express their meaning, right?", "tokens": [51412, 294, 1668, 281, 5109, 641, 3620, 11, 558, 30, 51494], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 203, "seek": 53736, "start": 559.96, "end": 563.24, "text": " They're not pointing to something out in the world.", "tokens": [51494, 814, 434, 406, 12166, 281, 746, 484, 294, 264, 1002, 13, 51658], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 204, "seek": 53736, "start": 563.24, "end": 565.24, "text": " Even if you don't go to function words like this,", "tokens": [51658, 2754, 498, 291, 500, 380, 352, 281, 2445, 2283, 411, 341, 11, 51758], "temperature": 0.0, "avg_logprob": -0.11517308051126045, "compression_ratio": 1.7969348659003832, "no_speech_prob": 5.828329813084565e-05}, {"id": 205, "seek": 56524, "start": 565.28, "end": 568.96, "text": " there's other words which are very hard to make sense of", "tokens": [50366, 456, 311, 661, 2283, 597, 366, 588, 1152, 281, 652, 2020, 295, 50550], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 206, "seek": 56524, "start": 568.96, "end": 571.6800000000001, "text": " in a kind of view that meaning is stuff in the world.", "tokens": [50550, 294, 257, 733, 295, 1910, 300, 3620, 307, 1507, 294, 264, 1002, 13, 50686], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 207, "seek": 56524, "start": 571.6800000000001, "end": 575.08, "text": " So you can think of very abstract words like justice, right?", "tokens": [50686, 407, 291, 393, 519, 295, 588, 12649, 2283, 411, 6118, 11, 558, 30, 50856], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 208, "seek": 56524, "start": 575.08, "end": 576.8, "text": " There's probably not a justice out there.", "tokens": [50856, 821, 311, 1391, 406, 257, 6118, 484, 456, 13, 50942], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 209, "seek": 56524, "start": 576.8, "end": 580.12, "text": " That's some kind of construct that we have or wit,", "tokens": [50942, 663, 311, 512, 733, 295, 7690, 300, 321, 362, 420, 32161, 11, 51108], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 210, "seek": 56524, "start": 580.12, "end": 582.5600000000001, "text": " or you can think of things that don't exist like dragons,", "tokens": [51108, 420, 291, 393, 519, 295, 721, 300, 500, 380, 2514, 411, 27240, 11, 51230], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 211, "seek": 56524, "start": 582.5600000000001, "end": 583.4, "text": " right?", "tokens": [51230, 558, 30, 51272], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 212, "seek": 56524, "start": 583.4, "end": 587.24, "text": " There's no external thing in the world, which is a dragon.", "tokens": [51272, 821, 311, 572, 8320, 551, 294, 264, 1002, 11, 597, 307, 257, 12165, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 213, "seek": 56524, "start": 587.24, "end": 589.36, "text": " There's even words and concepts we have", "tokens": [51464, 821, 311, 754, 2283, 293, 10392, 321, 362, 51570], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 214, "seek": 56524, "start": 589.36, "end": 592.08, "text": " that have no possible reference to the world, right?", "tokens": [51570, 300, 362, 572, 1944, 6408, 281, 264, 1002, 11, 558, 30, 51706], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 215, "seek": 56524, "start": 592.08, "end": 594.28, "text": " So if I think about an imaginary bicycle,", "tokens": [51706, 407, 498, 286, 519, 466, 364, 26164, 20888, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10465037482125418, "compression_ratio": 1.874551971326165, "no_speech_prob": 7.253158400999382e-05}, {"id": 216, "seek": 59428, "start": 594.3199999999999, "end": 596.8, "text": " that's something which is by definition imaginary, right?", "tokens": [50366, 300, 311, 746, 597, 307, 538, 7123, 26164, 11, 558, 30, 50490], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 217, "seek": 59428, "start": 596.8, "end": 600.4399999999999, "text": " It's not out there, or a perpetual motion machine, right?", "tokens": [50490, 467, 311, 406, 484, 456, 11, 420, 257, 48216, 5394, 3479, 11, 558, 30, 50672], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 218, "seek": 59428, "start": 600.4399999999999, "end": 603.04, "text": " We can have a concept of a perpetual motion machine", "tokens": [50672, 492, 393, 362, 257, 3410, 295, 257, 48216, 5394, 3479, 50802], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 219, "seek": 59428, "start": 603.04, "end": 604.92, "text": " and think about it and reason about it,", "tokens": [50802, 293, 519, 466, 309, 293, 1778, 466, 309, 11, 50896], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 220, "seek": 59428, "start": 604.92, "end": 607.92, "text": " but there's certainly not one that exists out in the world.", "tokens": [50896, 457, 456, 311, 3297, 406, 472, 300, 8198, 484, 294, 264, 1002, 13, 51046], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 221, "seek": 59428, "start": 609.76, "end": 611.8399999999999, "text": " Even for ordinary concepts,", "tokens": [51138, 2754, 337, 10547, 10392, 11, 51242], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 222, "seek": 59428, "start": 611.8399999999999, "end": 614.88, "text": " we likely haven't even considered all of the possible things", "tokens": [51242, 321, 3700, 2378, 380, 754, 4888, 439, 295, 264, 1944, 721, 51394], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 223, "seek": 59428, "start": 614.88, "end": 617.36, "text": " which could be reference of those concepts, right?", "tokens": [51394, 597, 727, 312, 6408, 295, 729, 10392, 11, 558, 30, 51518], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 224, "seek": 59428, "start": 617.36, "end": 621.68, "text": " So I could walk in wearing a shoe made out of eggplants", "tokens": [51518, 407, 286, 727, 1792, 294, 4769, 257, 12796, 1027, 484, 295, 3777, 42433, 51734], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 225, "seek": 59428, "start": 621.68, "end": 624.0, "text": " and you could look at them and everybody might agree", "tokens": [51734, 293, 291, 727, 574, 412, 552, 293, 2201, 1062, 3986, 51850], "temperature": 0.0, "avg_logprob": -0.10291341692209244, "compression_ratio": 1.7916666666666667, "no_speech_prob": 3.119801476714201e-05}, {"id": 226, "seek": 62400, "start": 624.04, "end": 626.52, "text": " that they're shoes, right?", "tokens": [50366, 300, 436, 434, 6654, 11, 558, 30, 50490], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 227, "seek": 62400, "start": 626.52, "end": 627.72, "text": " But you would agree that they're shoes", "tokens": [50490, 583, 291, 576, 3986, 300, 436, 434, 6654, 50550], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 228, "seek": 62400, "start": 627.72, "end": 630.96, "text": " without ever having seen shoes made of eggplants before, right?", "tokens": [50550, 1553, 1562, 1419, 1612, 6654, 1027, 295, 3777, 42433, 949, 11, 558, 30, 50712], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 229, "seek": 62400, "start": 630.96, "end": 632.72, "text": " So there's some object in the world", "tokens": [50712, 407, 456, 311, 512, 2657, 294, 264, 1002, 50800], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 230, "seek": 62400, "start": 632.72, "end": 635.16, "text": " which everybody would agree is a shoe.", "tokens": [50800, 597, 2201, 576, 3986, 307, 257, 12796, 13, 50922], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 231, "seek": 62400, "start": 635.16, "end": 637.14, "text": " Even though you've never encountered that thing before,", "tokens": [50922, 2754, 1673, 291, 600, 1128, 20381, 300, 551, 949, 11, 51021], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 232, "seek": 62400, "start": 637.14, "end": 638.56, "text": " that means that it couldn't have been the stuff", "tokens": [51021, 300, 1355, 300, 309, 2809, 380, 362, 668, 264, 1507, 51092], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 233, "seek": 62400, "start": 638.56, "end": 641.56, "text": " in the world which determined whether it was a shoe, right?", "tokens": [51092, 294, 264, 1002, 597, 9540, 1968, 309, 390, 257, 12796, 11, 558, 30, 51242], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 234, "seek": 62400, "start": 641.56, "end": 643.72, "text": " Had to be some kind of more abstract conception", "tokens": [51242, 12298, 281, 312, 512, 733, 295, 544, 12649, 30698, 51350], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 235, "seek": 62400, "start": 643.72, "end": 646.28, "text": " of what makes something a shoe, right?", "tokens": [51350, 295, 437, 1669, 746, 257, 12796, 11, 558, 30, 51478], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 236, "seek": 62400, "start": 646.28, "end": 648.04, "text": " You can think about things like the function", "tokens": [51478, 509, 393, 519, 466, 721, 411, 264, 2445, 51566], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 237, "seek": 62400, "start": 648.04, "end": 651.0, "text": " or the origin, how they're used.", "tokens": [51566, 420, 264, 4957, 11, 577, 436, 434, 1143, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 238, "seek": 62400, "start": 651.0, "end": 653.2, "text": " These other kinds of properties of objects", "tokens": [51714, 1981, 661, 3685, 295, 7221, 295, 6565, 51824], "temperature": 0.0, "avg_logprob": -0.09723532929712413, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0002694343274924904}, {"id": 239, "seek": 65320, "start": 653.2, "end": 655.76, "text": " seem to be much more important for the categorization", "tokens": [50364, 1643, 281, 312, 709, 544, 1021, 337, 264, 19250, 2144, 50492], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 240, "seek": 65320, "start": 655.76, "end": 658.0, "text": " of the concept, yeah.", "tokens": [50492, 295, 264, 3410, 11, 1338, 13, 50604], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 241, "seek": 65320, "start": 658.0, "end": 661.32, "text": " Well, here this is a little bit of a survivorship bias", "tokens": [50604, 1042, 11, 510, 341, 307, 257, 707, 857, 295, 257, 12324, 14752, 12577, 50770], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 242, "seek": 65320, "start": 661.32, "end": 665.4000000000001, "text": " because eggplants shoes might still be considered shoes,", "tokens": [50770, 570, 3777, 42433, 6654, 1062, 920, 312, 4888, 6654, 11, 50974], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 243, "seek": 65320, "start": 665.4000000000001, "end": 668.12, "text": " but then ice cream shoes are probably,", "tokens": [50974, 457, 550, 4435, 4689, 6654, 366, 1391, 11, 51110], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 244, "seek": 65320, "start": 668.12, "end": 670.36, "text": " nobody will recognize them as shoes, right?", "tokens": [51110, 5079, 486, 5521, 552, 382, 6654, 11, 558, 30, 51222], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 245, "seek": 65320, "start": 670.36, "end": 671.88, "text": " But then you don't think about ice cream shoes.", "tokens": [51222, 583, 550, 291, 500, 380, 519, 466, 4435, 4689, 6654, 13, 51298], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 246, "seek": 65320, "start": 671.88, "end": 675.44, "text": " So it's like the things that you can think of as shoes", "tokens": [51298, 407, 309, 311, 411, 264, 721, 300, 291, 393, 519, 295, 382, 6654, 51476], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 247, "seek": 65320, "start": 675.44, "end": 677.44, "text": " are in your little bowl", "tokens": [51476, 366, 294, 428, 707, 6571, 51576], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 248, "seek": 65320, "start": 677.44, "end": 678.8000000000001, "text": " and then you don't think about things", "tokens": [51576, 293, 550, 291, 500, 380, 519, 466, 721, 51644], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 249, "seek": 65320, "start": 678.8000000000001, "end": 679.84, "text": " that are already too far.", "tokens": [51644, 300, 366, 1217, 886, 1400, 13, 51696], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 250, "seek": 65320, "start": 679.84, "end": 681.9200000000001, "text": " So it seems like there is still kind of some distance", "tokens": [51696, 407, 309, 2544, 411, 456, 307, 920, 733, 295, 512, 4560, 51800], "temperature": 0.0, "avg_logprob": -0.15407591707566204, "compression_ratio": 1.826241134751773, "no_speech_prob": 0.0014533018693327904}, {"id": 251, "seek": 68192, "start": 681.92, "end": 684.52, "text": " to the closest real object.", "tokens": [50364, 281, 264, 13699, 957, 2657, 13, 50494], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 252, "seek": 68192, "start": 684.52, "end": 685.36, "text": " Yeah, yeah.", "tokens": [50494, 865, 11, 1338, 13, 50536], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 253, "seek": 68192, "start": 685.36, "end": 690.36, "text": " So all of this is not to say that the real objects", "tokens": [50536, 407, 439, 295, 341, 307, 406, 281, 584, 300, 264, 957, 6565, 50786], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 254, "seek": 68192, "start": 691.24, "end": 692.64, "text": " are irrelevant, right?", "tokens": [50830, 366, 28682, 11, 558, 30, 50900], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 255, "seek": 68192, "start": 692.64, "end": 695.28, "text": " Like I agree that eggplants are much more plausible issues", "tokens": [50900, 1743, 286, 3986, 300, 3777, 42433, 366, 709, 544, 39925, 2663, 51032], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 256, "seek": 68192, "start": 695.28, "end": 698.8399999999999, "text": " than ice cream and that has to do with the kind of real", "tokens": [51032, 813, 4435, 4689, 293, 300, 575, 281, 360, 365, 264, 733, 295, 957, 51210], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 257, "seek": 68192, "start": 698.8399999999999, "end": 702.0799999999999, "text": " physical properties of those substances.", "tokens": [51210, 4001, 7221, 295, 729, 25455, 13, 51372], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 258, "seek": 68192, "start": 702.0799999999999, "end": 703.88, "text": " My point is just that the physical thing", "tokens": [51372, 1222, 935, 307, 445, 300, 264, 4001, 551, 51462], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 259, "seek": 68192, "start": 703.88, "end": 705.5999999999999, "text": " is not the defining thing, right?", "tokens": [51462, 307, 406, 264, 17827, 551, 11, 558, 30, 51548], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 260, "seek": 68192, "start": 705.5999999999999, "end": 707.4, "text": " It's not something in the object you look to", "tokens": [51548, 467, 311, 406, 746, 294, 264, 2657, 291, 574, 281, 51638], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 261, "seek": 68192, "start": 707.4, "end": 709.0799999999999, "text": " to decide whether it's a shoe or not, right?", "tokens": [51638, 281, 4536, 1968, 309, 311, 257, 12796, 420, 406, 11, 558, 30, 51722], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 262, "seek": 68192, "start": 709.0799999999999, "end": 711.9, "text": " It's something more abstract about how it's used", "tokens": [51722, 467, 311, 746, 544, 12649, 466, 577, 309, 311, 1143, 51863], "temperature": 0.0, "avg_logprob": -0.13693895412765386, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000404420803533867}, {"id": 263, "seek": 71190, "start": 711.9, "end": 713.3, "text": " or made or something like that.", "tokens": [50364, 420, 1027, 420, 746, 411, 300, 13, 50434], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 264, "seek": 71190, "start": 715.38, "end": 717.38, "text": " I'll actually talk a bit in this paper", "tokens": [50538, 286, 603, 767, 751, 257, 857, 294, 341, 3035, 50638], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 265, "seek": 71190, "start": 717.38, "end": 721.78, "text": " about the concept of a postage stamp, right?", "tokens": [50638, 466, 264, 3410, 295, 257, 2183, 609, 9921, 11, 558, 30, 50858], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 266, "seek": 71190, "start": 721.78, "end": 725.06, "text": " Which is just an example of one that people probably", "tokens": [50858, 3013, 307, 445, 364, 1365, 295, 472, 300, 561, 1391, 51022], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 267, "seek": 71190, "start": 725.06, "end": 728.98, "text": " have some intuitions about where you could easily think", "tokens": [51022, 362, 512, 16224, 626, 466, 689, 291, 727, 3612, 519, 51218], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 268, "seek": 71190, "start": 728.98, "end": 731.5799999999999, "text": " of postage stamps which are fundamentally different", "tokens": [51218, 295, 2183, 609, 30800, 597, 366, 17879, 819, 51348], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 269, "seek": 71190, "start": 731.5799999999999, "end": 732.9399999999999, "text": " than anyone's you've seen before.", "tokens": [51348, 813, 2878, 311, 291, 600, 1612, 949, 13, 51416], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 270, "seek": 71190, "start": 732.9399999999999, "end": 735.18, "text": " You could think of one made of glass, for example,", "tokens": [51416, 509, 727, 519, 295, 472, 1027, 295, 4276, 11, 337, 1365, 11, 51528], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 271, "seek": 71190, "start": 735.18, "end": 737.9, "text": " or you could think of one that was an RFID tag,", "tokens": [51528, 420, 291, 727, 519, 295, 472, 300, 390, 364, 26204, 2777, 6162, 11, 51664], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 272, "seek": 71190, "start": 737.9, "end": 740.5, "text": " which is probably physical incarnations", "tokens": [51664, 597, 307, 1391, 4001, 30938, 763, 51794], "temperature": 0.0, "avg_logprob": -0.2112579345703125, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.00018519618606660515}, {"id": 273, "seek": 74050, "start": 740.5, "end": 742.94, "text": " of postage stamps like that that everybody would agree", "tokens": [50364, 295, 2183, 609, 30800, 411, 300, 300, 2201, 576, 3986, 50486], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 274, "seek": 74050, "start": 742.94, "end": 745.42, "text": " should be called a postage stamp.", "tokens": [50486, 820, 312, 1219, 257, 2183, 609, 9921, 13, 50610], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 275, "seek": 74050, "start": 745.42, "end": 747.94, "text": " And if you try to get people to define it, right?", "tokens": [50610, 400, 498, 291, 853, 281, 483, 561, 281, 6964, 309, 11, 558, 30, 50736], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 276, "seek": 74050, "start": 747.94, "end": 749.18, "text": " You might say something like, well,", "tokens": [50736, 509, 1062, 584, 746, 411, 11, 731, 11, 50798], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 277, "seek": 74050, "start": 749.18, "end": 750.62, "text": " a postage stamp is something you pay for", "tokens": [50798, 257, 2183, 609, 9921, 307, 746, 291, 1689, 337, 50870], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 278, "seek": 74050, "start": 750.62, "end": 752.26, "text": " and you put on a letter so that the letter", "tokens": [50870, 293, 291, 829, 322, 257, 5063, 370, 300, 264, 5063, 50952], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 279, "seek": 74050, "start": 752.26, "end": 753.54, "text": " will be delivered by the government", "tokens": [50952, 486, 312, 10144, 538, 264, 2463, 51016], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 280, "seek": 74050, "start": 753.54, "end": 754.62, "text": " or something like that, right?", "tokens": [51016, 420, 746, 411, 300, 11, 558, 30, 51070], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 281, "seek": 74050, "start": 754.62, "end": 759.62, "text": " So what the term means is intrinsically connected", "tokens": [51070, 407, 437, 264, 1433, 1355, 307, 28621, 984, 4582, 51320], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 282, "seek": 74050, "start": 759.62, "end": 762.9, "text": " to a bunch of these other terms like payment and letters", "tokens": [51320, 281, 257, 3840, 295, 613, 661, 2115, 411, 10224, 293, 7825, 51484], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 283, "seek": 74050, "start": 762.9, "end": 765.06, "text": " and being delivered and those things.", "tokens": [51484, 293, 885, 10144, 293, 729, 721, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 284, "seek": 74050, "start": 765.06, "end": 767.94, "text": " And in fact, if those terms change meaning, right?", "tokens": [51592, 400, 294, 1186, 11, 498, 729, 2115, 1319, 3620, 11, 558, 30, 51736], "temperature": 0.0, "avg_logprob": -0.1216891513151281, "compression_ratio": 1.9084249084249085, "no_speech_prob": 0.00027797420625574887}, {"id": 285, "seek": 76794, "start": 767.94, "end": 770.98, "text": " So if, for example, people develop a new way", "tokens": [50364, 407, 498, 11, 337, 1365, 11, 561, 1499, 257, 777, 636, 50516], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 286, "seek": 76794, "start": 770.98, "end": 773.5400000000001, "text": " of paying for things, paying on the blockchain", "tokens": [50516, 295, 6229, 337, 721, 11, 6229, 322, 264, 17176, 50644], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 287, "seek": 76794, "start": 773.5400000000001, "end": 774.3800000000001, "text": " or something, right?", "tokens": [50644, 420, 746, 11, 558, 30, 50686], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 288, "seek": 76794, "start": 774.3800000000001, "end": 776.2800000000001, "text": " Then you kind of know automatically", "tokens": [50686, 1396, 291, 733, 295, 458, 6772, 50781], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 289, "seek": 76794, "start": 776.2800000000001, "end": 779.0600000000001, "text": " that a postage stamp can be paid for in that way,", "tokens": [50781, 300, 257, 2183, 609, 9921, 393, 312, 4835, 337, 294, 300, 636, 11, 50920], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 290, "seek": 76794, "start": 779.0600000000001, "end": 780.5, "text": " at least in principle, right?", "tokens": [50920, 412, 1935, 294, 8665, 11, 558, 30, 50992], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 291, "seek": 76794, "start": 780.5, "end": 783.3800000000001, "text": " So it's not just that the word is associated", "tokens": [50992, 407, 309, 311, 406, 445, 300, 264, 1349, 307, 6615, 51136], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 292, "seek": 76794, "start": 783.3800000000001, "end": 785.1400000000001, "text": " with those other things, but that its meaning", "tokens": [51136, 365, 729, 661, 721, 11, 457, 300, 1080, 3620, 51224], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 293, "seek": 76794, "start": 785.1400000000001, "end": 787.94, "text": " is inherently connected to those other things.", "tokens": [51224, 307, 27993, 4582, 281, 729, 661, 721, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 294, "seek": 76794, "start": 789.94, "end": 794.94, "text": " So that's one kind of take on why reference to stuff", "tokens": [51464, 407, 300, 311, 472, 733, 295, 747, 322, 983, 6408, 281, 1507, 51714], "temperature": 0.0, "avg_logprob": -0.12961137616956556, "compression_ratio": 1.6963562753036436, "no_speech_prob": 4.399367026053369e-05}, {"id": 295, "seek": 79794, "start": 798.0200000000001, "end": 799.0600000000001, "text": " out in the world, right?", "tokens": [50368, 484, 294, 264, 1002, 11, 558, 30, 50420], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 296, "seek": 79794, "start": 799.0600000000001, "end": 802.86, "text": " Is not a good way of thinking about meaning.", "tokens": [50420, 1119, 406, 257, 665, 636, 295, 1953, 466, 3620, 13, 50610], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 297, "seek": 79794, "start": 802.86, "end": 805.5, "text": " Let me tell you what one alternative is.", "tokens": [50610, 961, 385, 980, 291, 437, 472, 8535, 307, 13, 50742], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 298, "seek": 79794, "start": 805.5, "end": 808.46, "text": " Actually, before I do that, let me just show", "tokens": [50742, 5135, 11, 949, 286, 360, 300, 11, 718, 385, 445, 855, 50890], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 299, "seek": 79794, "start": 808.46, "end": 809.62, "text": " a couple of other alternatives,", "tokens": [50890, 257, 1916, 295, 661, 20478, 11, 50948], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 300, "seek": 79794, "start": 809.62, "end": 811.62, "text": " which I think are also not plausible,", "tokens": [50948, 597, 286, 519, 366, 611, 406, 39925, 11, 51048], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 301, "seek": 79794, "start": 811.62, "end": 814.0200000000001, "text": " but might be familiar to people, okay?", "tokens": [51048, 457, 1062, 312, 4963, 281, 561, 11, 1392, 30, 51168], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 302, "seek": 79794, "start": 814.0200000000001, "end": 816.46, "text": " So what I just talked about is this kind of", "tokens": [51168, 407, 437, 286, 445, 2825, 466, 307, 341, 733, 295, 51290], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 303, "seek": 79794, "start": 816.46, "end": 817.62, "text": " world mapping view, right?", "tokens": [51290, 1002, 18350, 1910, 11, 558, 30, 51348], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 304, "seek": 79794, "start": 817.62, "end": 819.5, "text": " That there's some word and its meaning", "tokens": [51348, 663, 456, 311, 512, 1349, 293, 1080, 3620, 51442], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 305, "seek": 79794, "start": 819.5, "end": 822.4000000000001, "text": " is some physical object or some thing.", "tokens": [51442, 307, 512, 4001, 2657, 420, 512, 551, 13, 51587], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 306, "seek": 79794, "start": 824.1, "end": 825.5, "text": " You can think about other kinds of views", "tokens": [51672, 509, 393, 519, 466, 661, 3685, 295, 6809, 51742], "temperature": 0.0, "avg_logprob": -0.13443865094866073, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.0005192054668441415}, {"id": 307, "seek": 82550, "start": 825.74, "end": 829.14, "text": " of concepts and meaning might have a kind of", "tokens": [50376, 295, 10392, 293, 3620, 1062, 362, 257, 733, 295, 50546], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 308, "seek": 82550, "start": 829.14, "end": 830.5, "text": " feature spacey kind of views,", "tokens": [50546, 4111, 1901, 88, 733, 295, 6809, 11, 50614], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 309, "seek": 82550, "start": 830.5, "end": 832.02, "text": " port vector machines or something, right?", "tokens": [50614, 2436, 8062, 8379, 420, 746, 11, 558, 30, 50690], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 310, "seek": 82550, "start": 832.02, "end": 834.06, "text": " There's some abstract feature space", "tokens": [50690, 821, 311, 512, 12649, 4111, 1901, 50792], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 311, "seek": 82550, "start": 834.06, "end": 836.64, "text": " and a concept is some dividing line", "tokens": [50792, 293, 257, 3410, 307, 512, 26764, 1622, 50921], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 312, "seek": 82550, "start": 836.64, "end": 839.1, "text": " or some region or something in this space.", "tokens": [50921, 420, 512, 4458, 420, 746, 294, 341, 1901, 13, 51044], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 313, "seek": 82550, "start": 841.34, "end": 846.18, "text": " That I think is maybe fine in some narrow applications,", "tokens": [51156, 663, 286, 519, 307, 1310, 2489, 294, 512, 9432, 5821, 11, 51398], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 314, "seek": 82550, "start": 846.18, "end": 849.98, "text": " but what I'll talk about next are cases of, say,", "tokens": [51398, 457, 437, 286, 603, 751, 466, 958, 366, 3331, 295, 11, 584, 11, 51588], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 315, "seek": 82550, "start": 849.98, "end": 852.86, "text": " human cognition, which really don't fit well", "tokens": [51588, 1952, 46905, 11, 597, 534, 500, 380, 3318, 731, 51732], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 316, "seek": 82550, "start": 852.86, "end": 854.82, "text": " into that picture, in the sense that things", "tokens": [51732, 666, 300, 3036, 11, 294, 264, 2020, 300, 721, 51830], "temperature": 0.0, "avg_logprob": -0.20516156283291903, "compression_ratio": 1.7137096774193548, "no_speech_prob": 0.0002736771712079644}, {"id": 317, "seek": 85482, "start": 854.82, "end": 856.6600000000001, "text": " are much more complicated for how people think", "tokens": [50364, 366, 709, 544, 6179, 337, 577, 561, 519, 50456], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 318, "seek": 85482, "start": 856.6600000000001, "end": 859.74, "text": " about concepts and their relationships.", "tokens": [50456, 466, 10392, 293, 641, 6159, 13, 50610], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 319, "seek": 85482, "start": 859.74, "end": 862.0600000000001, "text": " People might also have this sort of hierarchy", "tokens": [50610, 3432, 1062, 611, 362, 341, 1333, 295, 22333, 50726], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 320, "seek": 85482, "start": 862.0600000000001, "end": 863.38, "text": " or network view, right?", "tokens": [50726, 420, 3209, 1910, 11, 558, 30, 50792], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 321, "seek": 85482, "start": 863.38, "end": 865.58, "text": " So sometimes people think, oh, sorry, you can't see this.", "tokens": [50792, 407, 2171, 561, 519, 11, 1954, 11, 2597, 11, 291, 393, 380, 536, 341, 13, 50902], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 322, "seek": 85482, "start": 865.58, "end": 867.88, "text": " There's supposed to be lines connecting", "tokens": [50902, 821, 311, 3442, 281, 312, 3876, 11015, 51017], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 323, "seek": 85482, "start": 867.88, "end": 870.1, "text": " one concept book in the middle to a bunch", "tokens": [51017, 472, 3410, 1446, 294, 264, 2808, 281, 257, 3840, 51128], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 324, "seek": 85482, "start": 870.1, "end": 871.46, "text": " of other concepts, right?", "tokens": [51128, 295, 661, 10392, 11, 558, 30, 51196], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 325, "seek": 85482, "start": 871.46, "end": 874.38, "text": " And you might, you know, there's old theories", "tokens": [51196, 400, 291, 1062, 11, 291, 458, 11, 456, 311, 1331, 13667, 51342], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 326, "seek": 85482, "start": 874.38, "end": 876.98, "text": " of, say, semantic organization or very old,", "tokens": [51342, 295, 11, 584, 11, 47982, 4475, 420, 588, 1331, 11, 51472], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 327, "seek": 85482, "start": 876.98, "end": 879.1, "text": " old AI kind of approaches, right?", "tokens": [51472, 1331, 7318, 733, 295, 11587, 11, 558, 30, 51578], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 328, "seek": 85482, "start": 879.1, "end": 883.0200000000001, "text": " That think about building hierarchies of concepts", "tokens": [51578, 663, 519, 466, 2390, 35250, 530, 295, 10392, 51774], "temperature": 0.0, "avg_logprob": -0.15718667209148407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003249458677601069}, {"id": 329, "seek": 88302, "start": 883.9399999999999, "end": 886.62, "text": " or sometimes networks of concepts", "tokens": [50410, 420, 2171, 9590, 295, 10392, 50544], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 330, "seek": 88302, "start": 886.62, "end": 889.5, "text": " and trying to define meaning in terms of those relationships.", "tokens": [50544, 293, 1382, 281, 6964, 3620, 294, 2115, 295, 729, 6159, 13, 50688], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 331, "seek": 88302, "start": 889.5, "end": 892.06, "text": " I actually think that both this and the feature", "tokens": [50688, 286, 767, 519, 300, 1293, 341, 293, 264, 4111, 50816], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 332, "seek": 88302, "start": 892.06, "end": 895.38, "text": " and the world mapping view have some of the,", "tokens": [50816, 293, 264, 1002, 18350, 1910, 362, 512, 295, 264, 11, 50982], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 333, "seek": 88302, "start": 895.38, "end": 898.62, "text": " some kind of useful properties or useful insights", "tokens": [50982, 512, 733, 295, 4420, 7221, 420, 4420, 14310, 51144], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 334, "seek": 88302, "start": 898.62, "end": 901.34, "text": " about concepts, but just aren't quite the whole thing", "tokens": [51144, 466, 10392, 11, 457, 445, 3212, 380, 1596, 264, 1379, 551, 51280], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 335, "seek": 88302, "start": 901.34, "end": 904.02, "text": " for reasons that I'll talk about next.", "tokens": [51280, 337, 4112, 300, 286, 603, 751, 466, 958, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 336, "seek": 88302, "start": 904.02, "end": 908.5, "text": " So let me just start with, start trying to introduce", "tokens": [51414, 407, 718, 385, 445, 722, 365, 11, 722, 1382, 281, 5366, 51638], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 337, "seek": 88302, "start": 908.5, "end": 911.06, "text": " this kind of other view of concepts", "tokens": [51638, 341, 733, 295, 661, 1910, 295, 10392, 51766], "temperature": 0.0, "avg_logprob": -0.13902798265513808, "compression_ratio": 1.7427385892116183, "no_speech_prob": 0.0002453218912705779}, {"id": 338, "seek": 91106, "start": 911.06, "end": 913.5799999999999, "text": " by trying to get people's intuitions", "tokens": [50364, 538, 1382, 281, 483, 561, 311, 16224, 626, 50490], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 339, "seek": 91106, "start": 913.5799999999999, "end": 916.14, "text": " on a recent news story, okay?", "tokens": [50490, 322, 257, 5162, 2583, 1657, 11, 1392, 30, 50618], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 340, "seek": 91106, "start": 916.14, "end": 920.4599999999999, "text": " So here's a little recent news from the US versus Trump.", "tokens": [50618, 407, 510, 311, 257, 707, 5162, 2583, 490, 264, 2546, 5717, 3899, 13, 50834], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 341, "seek": 91106, "start": 920.4599999999999, "end": 922.14, "text": " I think this is not the most recent indictment,", "tokens": [50834, 286, 519, 341, 307, 406, 264, 881, 5162, 49981, 518, 11, 50918], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 342, "seek": 91106, "start": 922.14, "end": 923.9, "text": " but one or two indictments ago.", "tokens": [50918, 457, 472, 420, 732, 49981, 1117, 2057, 13, 51006], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 343, "seek": 91106, "start": 925.18, "end": 928.14, "text": " If you look through it, you can read all about", "tokens": [51070, 759, 291, 574, 807, 309, 11, 291, 393, 1401, 439, 466, 51218], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 344, "seek": 91106, "start": 929.02, "end": 931.6199999999999, "text": " Pence and Trump and efforts to manipulate", "tokens": [51262, 48402, 293, 3899, 293, 6484, 281, 20459, 51392], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 345, "seek": 91106, "start": 931.6199999999999, "end": 933.3, "text": " the election and things.", "tokens": [51392, 264, 6618, 293, 721, 13, 51476], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 346, "seek": 91106, "start": 933.3, "end": 936.5, "text": " Here's a little paraphrase of one of the paragraphs, 90 C.", "tokens": [51476, 1692, 311, 257, 707, 36992, 1703, 651, 295, 472, 295, 264, 48910, 11, 4289, 383, 13, 51636], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 347, "seek": 91106, "start": 936.5, "end": 938.3399999999999, "text": " So Pence, the vice president, right,", "tokens": [51636, 407, 48402, 11, 264, 11964, 3868, 11, 558, 11, 51728], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 348, "seek": 91106, "start": 938.3399999999999, "end": 940.7399999999999, "text": " opposed a Trump team lawsuit arguing", "tokens": [51728, 8851, 257, 3899, 1469, 22504, 19697, 51848], "temperature": 0.0, "avg_logprob": -0.16501355549645802, "compression_ratio": 1.6483516483516483, "no_speech_prob": 0.001064431038685143}, {"id": 349, "seek": 94074, "start": 940.74, "end": 943.5, "text": " that the vice president could reject electoral votes.", "tokens": [50364, 300, 264, 11964, 3868, 727, 8248, 28633, 12068, 13, 50502], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 350, "seek": 94074, "start": 943.5, "end": 945.66, "text": " So Pence didn't want them to argue", "tokens": [50502, 407, 48402, 994, 380, 528, 552, 281, 9695, 50610], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 351, "seek": 94074, "start": 945.66, "end": 948.22, "text": " that he could reject electoral votes.", "tokens": [50610, 300, 415, 727, 8248, 28633, 12068, 13, 50738], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 352, "seek": 94074, "start": 948.22, "end": 950.1, "text": " He said to Trump that he didn't have a constitutional", "tokens": [50738, 634, 848, 281, 3899, 300, 415, 994, 380, 362, 257, 20176, 50832], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 353, "seek": 94074, "start": 950.1, "end": 952.66, "text": " authority and that the action would be improper.", "tokens": [50832, 8281, 293, 300, 264, 3069, 576, 312, 40651, 13, 50960], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 354, "seek": 94074, "start": 952.66, "end": 955.7, "text": " So it's according to Pence's notes at the time.", "tokens": [50960, 407, 309, 311, 4650, 281, 48402, 311, 5570, 412, 264, 565, 13, 51112], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 355, "seek": 94074, "start": 955.7, "end": 958.94, "text": " And Trump responded, you're too honest, okay?", "tokens": [51112, 400, 3899, 15806, 11, 291, 434, 886, 3245, 11, 1392, 30, 51274], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 356, "seek": 94074, "start": 958.94, "end": 960.66, "text": " According to Pence's notes.", "tokens": [51274, 7328, 281, 48402, 311, 5570, 13, 51360], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 357, "seek": 94074, "start": 960.66, "end": 962.46, "text": " So think about that situation and everything", "tokens": [51360, 407, 519, 466, 300, 2590, 293, 1203, 51450], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 358, "seek": 94074, "start": 962.46, "end": 964.78, "text": " you know about this context, right?", "tokens": [51450, 291, 458, 466, 341, 4319, 11, 558, 30, 51566], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 359, "seek": 94074, "start": 964.78, "end": 967.34, "text": " And think about an answer to a question like,", "tokens": [51566, 400, 519, 466, 364, 1867, 281, 257, 1168, 411, 11, 51694], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 360, "seek": 94074, "start": 967.34, "end": 969.5, "text": " why did Trump say this?", "tokens": [51694, 983, 630, 3899, 584, 341, 30, 51802], "temperature": 0.0, "avg_logprob": -0.15999915783221905, "compression_ratio": 1.8388278388278387, "no_speech_prob": 0.00018519589502830058}, {"id": 361, "seek": 96950, "start": 969.5, "end": 970.34, "text": " Right.", "tokens": [50364, 1779, 13, 50406], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 362, "seek": 96950, "start": 971.78, "end": 975.94, "text": " You think about that, probably what's going on", "tokens": [50478, 509, 519, 466, 300, 11, 1391, 437, 311, 516, 322, 50686], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 363, "seek": 96950, "start": 975.94, "end": 977.26, "text": " as you think about it, right?", "tokens": [50686, 382, 291, 519, 466, 309, 11, 558, 30, 50752], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 364, "seek": 96950, "start": 977.26, "end": 980.06, "text": " As you're thinking about lots of other things", "tokens": [50752, 1018, 291, 434, 1953, 466, 3195, 295, 661, 721, 50892], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 365, "seek": 96950, "start": 980.06, "end": 981.98, "text": " and how they're related to this situation,", "tokens": [50892, 293, 577, 436, 434, 4077, 281, 341, 2590, 11, 50988], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 366, "seek": 96950, "start": 981.98, "end": 983.98, "text": " like what Trump was trying to achieve,", "tokens": [50988, 411, 437, 3899, 390, 1382, 281, 4584, 11, 51088], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 367, "seek": 96950, "start": 983.98, "end": 986.9, "text": " maybe what kind of personality Trump had,", "tokens": [51088, 1310, 437, 733, 295, 9033, 3899, 632, 11, 51234], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 368, "seek": 96950, "start": 986.9, "end": 989.06, "text": " what Trump was trying to do to Pence.", "tokens": [51234, 437, 3899, 390, 1382, 281, 360, 281, 48402, 13, 51342], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 369, "seek": 96950, "start": 989.06, "end": 990.74, "text": " Is he trying to manipulate him", "tokens": [51342, 1119, 415, 1382, 281, 20459, 796, 51426], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 370, "seek": 96950, "start": 990.74, "end": 993.02, "text": " into taking some kinds of actions?", "tokens": [51426, 666, 1940, 512, 3685, 295, 5909, 30, 51540], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 371, "seek": 96950, "start": 993.02, "end": 995.9, "text": " What exactly that action would have, right?", "tokens": [51540, 708, 2293, 300, 3069, 576, 362, 11, 558, 30, 51684], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 372, "seek": 96950, "start": 995.9, "end": 998.02, "text": " In terms of the election.", "tokens": [51684, 682, 2115, 295, 264, 6618, 13, 51790], "temperature": 0.0, "avg_logprob": -0.16131490723699585, "compression_ratio": 1.75, "no_speech_prob": 0.0002304726222064346}, {"id": 373, "seek": 99802, "start": 998.02, "end": 1000.8199999999999, "text": " Everybody is perfectly capable of reasoning through these", "tokens": [50364, 7646, 307, 6239, 8189, 295, 21577, 807, 613, 50504], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 374, "seek": 99802, "start": 1000.8199999999999, "end": 1005.54, "text": " and coming up with kind of plausible causal story", "tokens": [50504, 293, 1348, 493, 365, 733, 295, 39925, 38755, 1657, 50740], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 375, "seek": 99802, "start": 1005.54, "end": 1006.5799999999999, "text": " about what's happening, right?", "tokens": [50740, 466, 437, 311, 2737, 11, 558, 30, 50792], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 376, "seek": 99802, "start": 1006.5799999999999, "end": 1008.46, "text": " It feels like we can come up with our own", "tokens": [50792, 467, 3417, 411, 321, 393, 808, 493, 365, 527, 1065, 50886], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 377, "seek": 99802, "start": 1008.46, "end": 1012.74, "text": " kind of internal explanations about events like these.", "tokens": [50886, 733, 295, 6920, 28708, 466, 3931, 411, 613, 13, 51100], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 378, "seek": 99802, "start": 1012.74, "end": 1015.1, "text": " And in fact, that process of coming up", "tokens": [51100, 400, 294, 1186, 11, 300, 1399, 295, 1348, 493, 51218], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 379, "seek": 99802, "start": 1015.1, "end": 1018.98, "text": " with internal explanations, interrelated kind of concepts", "tokens": [51218, 365, 6920, 28708, 11, 728, 12004, 733, 295, 10392, 51412], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 380, "seek": 99802, "start": 1018.98, "end": 1023.02, "text": " and meanings is one that people in developmental psychology", "tokens": [51412, 293, 28138, 307, 472, 300, 561, 294, 30160, 15105, 51614], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 381, "seek": 99802, "start": 1023.02, "end": 1025.42, "text": " have been very interested in and excited about", "tokens": [51614, 362, 668, 588, 3102, 294, 293, 2919, 466, 51734], "temperature": 0.0, "avg_logprob": -0.1399887532603984, "compression_ratio": 1.7351778656126482, "no_speech_prob": 0.0002959146804641932}, {"id": 382, "seek": 102542, "start": 1025.7, "end": 1028.8200000000002, "text": " as a theory of kind of human cognition.", "tokens": [50378, 382, 257, 5261, 295, 733, 295, 1952, 46905, 13, 50534], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 383, "seek": 102542, "start": 1028.8200000000002, "end": 1033.18, "text": " So basic observation is that people form these", "tokens": [50534, 407, 3875, 14816, 307, 300, 561, 1254, 613, 50752], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 384, "seek": 102542, "start": 1033.18, "end": 1036.66, "text": " very richly interconnected systems of concepts, right?", "tokens": [50752, 588, 4593, 356, 36611, 3652, 295, 10392, 11, 558, 30, 50926], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 385, "seek": 102542, "start": 1036.66, "end": 1040.0600000000002, "text": " All of the kind of interconnected stuff you would need to draw", "tokens": [50926, 1057, 295, 264, 733, 295, 36611, 1507, 291, 576, 643, 281, 2642, 51096], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 386, "seek": 102542, "start": 1040.0600000000002, "end": 1042.6200000000001, "text": " in order to answer a question like that,", "tokens": [51096, 294, 1668, 281, 1867, 257, 1168, 411, 300, 11, 51224], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 387, "seek": 102542, "start": 1042.6200000000001, "end": 1045.14, "text": " which feels totally, totally normal.", "tokens": [51224, 597, 3417, 3879, 11, 3879, 2710, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 388, "seek": 102542, "start": 1045.14, "end": 1047.3000000000002, "text": " People sometimes call these intuitive theories, right?", "tokens": [51350, 3432, 2171, 818, 613, 21769, 13667, 11, 558, 30, 51458], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 389, "seek": 102542, "start": 1047.3000000000002, "end": 1049.8200000000002, "text": " You have some intuitive theory of how Trump is acting", "tokens": [51458, 509, 362, 512, 21769, 5261, 295, 577, 3899, 307, 6577, 51584], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 390, "seek": 102542, "start": 1049.8200000000002, "end": 1051.54, "text": " or how the political system would work", "tokens": [51584, 420, 577, 264, 3905, 1185, 576, 589, 51670], "temperature": 0.0, "avg_logprob": -0.1280104665472956, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.0004582383844535798}, {"id": 391, "seek": 105154, "start": 1051.54, "end": 1056.1399999999999, "text": " or some intuitive theory of what Pence might be doing", "tokens": [50364, 420, 512, 21769, 5261, 295, 437, 48402, 1062, 312, 884, 50594], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 392, "seek": 105154, "start": 1056.1399999999999, "end": 1058.1399999999999, "text": " or might be trying to achieve.", "tokens": [50594, 420, 1062, 312, 1382, 281, 4584, 13, 50694], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 393, "seek": 105154, "start": 1058.1399999999999, "end": 1059.86, "text": " And these things are often compared", "tokens": [50694, 400, 613, 721, 366, 2049, 5347, 50780], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 394, "seek": 105154, "start": 1059.86, "end": 1061.82, "text": " to theories in science, right?", "tokens": [50780, 281, 13667, 294, 3497, 11, 558, 30, 50878], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 395, "seek": 105154, "start": 1061.82, "end": 1066.82, "text": " So you can think of your theory of why Trump might do this", "tokens": [50878, 407, 291, 393, 519, 295, 428, 5261, 295, 983, 3899, 1062, 360, 341, 51128], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 396, "seek": 105154, "start": 1067.78, "end": 1070.78, "text": " as kind of analogous to a little scientific theory, right?", "tokens": [51176, 382, 733, 295, 16660, 563, 281, 257, 707, 8134, 5261, 11, 558, 30, 51326], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 397, "seek": 105154, "start": 1070.78, "end": 1073.3, "text": " It has some pieces, it has some relationships", "tokens": [51326, 467, 575, 512, 3755, 11, 309, 575, 512, 6159, 51452], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 398, "seek": 105154, "start": 1073.3, "end": 1076.1, "text": " between the pieces, it has some dynamics.", "tokens": [51452, 1296, 264, 3755, 11, 309, 575, 512, 15679, 13, 51592], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 399, "seek": 105154, "start": 1076.1, "end": 1078.54, "text": " And maybe you can look at all of that", "tokens": [51592, 400, 1310, 291, 393, 574, 412, 439, 295, 300, 51714], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 400, "seek": 105154, "start": 1078.54, "end": 1080.34, "text": " and kind of reason about it causally", "tokens": [51714, 293, 733, 295, 1778, 466, 309, 3302, 379, 51804], "temperature": 0.0, "avg_logprob": -0.10547204144233097, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0002268976386403665}, {"id": 401, "seek": 108034, "start": 1080.3799999999999, "end": 1082.6599999999999, "text": " as you might reason about any other kind of system", "tokens": [50366, 382, 291, 1062, 1778, 466, 604, 661, 733, 295, 1185, 50480], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 402, "seek": 108034, "start": 1082.6599999999999, "end": 1084.3799999999999, "text": " that you've encountered.", "tokens": [50480, 300, 291, 600, 20381, 13, 50566], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 403, "seek": 108034, "start": 1085.22, "end": 1087.58, "text": " So the idea that people,", "tokens": [50608, 407, 264, 1558, 300, 561, 11, 50726], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 404, "seek": 108034, "start": 1087.58, "end": 1090.62, "text": " and maybe most notably kids do this is one", "tokens": [50726, 293, 1310, 881, 31357, 2301, 360, 341, 307, 472, 50878], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 405, "seek": 108034, "start": 1090.62, "end": 1094.1399999999999, "text": " which has really been very popular", "tokens": [50878, 597, 575, 534, 668, 588, 3743, 51054], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 406, "seek": 108034, "start": 1094.1399999999999, "end": 1098.6999999999998, "text": " in cognitive development, championed maybe most prominently", "tokens": [51054, 294, 15605, 3250, 11, 10971, 292, 1310, 881, 39225, 2276, 51282], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 407, "seek": 108034, "start": 1098.6999999999998, "end": 1101.4599999999998, "text": " by Alison Gopnik, who's a developmental psychologist", "tokens": [51282, 538, 41001, 460, 404, 13123, 11, 567, 311, 257, 30160, 29514, 51420], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 408, "seek": 108034, "start": 1101.4599999999998, "end": 1103.06, "text": " here at Berkeley.", "tokens": [51420, 510, 412, 23684, 13, 51500], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 409, "seek": 108034, "start": 1103.06, "end": 1106.1799999999998, "text": " Let me just give you a quick example of how kids,", "tokens": [51500, 961, 385, 445, 976, 291, 257, 1702, 1365, 295, 577, 2301, 11, 51656], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 410, "seek": 108034, "start": 1106.1799999999998, "end": 1110.22, "text": " how experiments with kids like kids sometimes go", "tokens": [51656, 577, 12050, 365, 2301, 411, 2301, 2171, 352, 51858], "temperature": 0.0, "avg_logprob": -0.21049899499393204, "compression_ratio": 1.5875486381322956, "no_speech_prob": 0.0002001934772124514}, {"id": 411, "seek": 111022, "start": 1110.42, "end": 1111.26, "text": " in this domain.", "tokens": [50374, 294, 341, 9274, 13, 50416], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 412, "seek": 111022, "start": 1111.26, "end": 1116.26, "text": " So here's an experiment from LaZot and Gelman.", "tokens": [50416, 407, 510, 311, 364, 5120, 490, 2369, 57, 310, 293, 16142, 1601, 13, 50666], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 413, "seek": 111022, "start": 1117.06, "end": 1119.38, "text": " So kids are shown these two foxes, right?", "tokens": [50706, 407, 2301, 366, 4898, 613, 732, 21026, 279, 11, 558, 30, 50822], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 414, "seek": 111022, "start": 1119.38, "end": 1123.22, "text": " Which you might notice are identical pictures, okay?", "tokens": [50822, 3013, 291, 1062, 3449, 366, 14800, 5242, 11, 1392, 30, 51014], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 415, "seek": 111022, "start": 1124.06, "end": 1126.74, "text": " And then they're told things about these foxes", "tokens": [51056, 400, 550, 436, 434, 1907, 721, 466, 613, 21026, 279, 51190], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 416, "seek": 111022, "start": 1126.74, "end": 1130.74, "text": " and asked what they could do in order to answer a question,", "tokens": [51190, 293, 2351, 437, 436, 727, 360, 294, 1668, 281, 1867, 257, 1168, 11, 51390], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 417, "seek": 111022, "start": 1130.74, "end": 1131.58, "text": " right?", "tokens": [51390, 558, 30, 51432], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 418, "seek": 111022, "start": 1131.58, "end": 1133.94, "text": " So this is like a simple version of why did Trump say that?", "tokens": [51432, 407, 341, 307, 411, 257, 2199, 3037, 295, 983, 630, 3899, 584, 300, 30, 51550], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 419, "seek": 111022, "start": 1133.94, "end": 1137.22, "text": " You might be told that one is an animal and one is a toy,", "tokens": [51550, 509, 1062, 312, 1907, 300, 472, 307, 364, 5496, 293, 472, 307, 257, 12058, 11, 51714], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 420, "seek": 111022, "start": 1137.22, "end": 1138.06, "text": " okay?", "tokens": [51714, 1392, 30, 51756], "temperature": 0.0, "avg_logprob": -0.19554667637265963, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.00039193822885863483}, {"id": 421, "seek": 113806, "start": 1138.06, "end": 1139.82, "text": " So what could you do in order to determine", "tokens": [50364, 407, 437, 727, 291, 360, 294, 1668, 281, 6997, 50452], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 422, "seek": 113806, "start": 1139.82, "end": 1143.34, "text": " which one is an animal and which one is a toy, right?", "tokens": [50452, 597, 472, 307, 364, 5496, 293, 597, 472, 307, 257, 12058, 11, 558, 30, 50628], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 423, "seek": 113806, "start": 1146.82, "end": 1150.34, "text": " In this experiment, kids will say that you should do things", "tokens": [50802, 682, 341, 5120, 11, 2301, 486, 584, 300, 291, 820, 360, 721, 50978], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 424, "seek": 113806, "start": 1150.34, "end": 1152.02, "text": " like check the insides, right?", "tokens": [50978, 411, 1520, 264, 1028, 1875, 11, 558, 30, 51062], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 425, "seek": 113806, "start": 1152.02, "end": 1153.86, "text": " Like check their guts or whatever, right?", "tokens": [51062, 1743, 1520, 641, 28560, 420, 2035, 11, 558, 30, 51154], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 426, "seek": 113806, "start": 1153.86, "end": 1156.1399999999999, "text": " Open them up and see.", "tokens": [51154, 7238, 552, 493, 293, 536, 13, 51268], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 427, "seek": 113806, "start": 1156.1399999999999, "end": 1159.22, "text": " Or look at their behavior, right?", "tokens": [51268, 1610, 574, 412, 641, 5223, 11, 558, 30, 51422], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 428, "seek": 113806, "start": 1159.22, "end": 1161.82, "text": " If it acts like an animal then you could use that", "tokens": [51422, 759, 309, 10672, 411, 364, 5496, 550, 291, 727, 764, 300, 51552], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 429, "seek": 113806, "start": 1161.82, "end": 1164.6599999999999, "text": " to figure out which one is the animal, which one's the toy.", "tokens": [51552, 281, 2573, 484, 597, 472, 307, 264, 5496, 11, 597, 472, 311, 264, 12058, 13, 51694], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 430, "seek": 113806, "start": 1164.6599999999999, "end": 1166.1799999999998, "text": " Or look at their parents, right?", "tokens": [51694, 1610, 574, 412, 641, 3152, 11, 558, 30, 51770], "temperature": 0.0, "avg_logprob": -0.13272951470046748, "compression_ratio": 1.852813852813853, "no_speech_prob": 0.00011590859503485262}, {"id": 431, "seek": 116618, "start": 1166.22, "end": 1170.26, "text": " Like, you know, the animal will have animal parents", "tokens": [50366, 1743, 11, 291, 458, 11, 264, 5496, 486, 362, 5496, 3152, 50568], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 432, "seek": 116618, "start": 1170.26, "end": 1171.8200000000002, "text": " and the toy won't, right?", "tokens": [50568, 293, 264, 12058, 1582, 380, 11, 558, 30, 50646], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 433, "seek": 116618, "start": 1171.8200000000002, "end": 1173.74, "text": " And importantly, they don't just say, yeah,", "tokens": [50646, 400, 8906, 11, 436, 500, 380, 445, 584, 11, 1338, 11, 50742], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 434, "seek": 116618, "start": 1173.74, "end": 1175.6200000000001, "text": " you can check everything about these.", "tokens": [50742, 291, 393, 1520, 1203, 466, 613, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 435, "seek": 116618, "start": 1175.6200000000001, "end": 1178.18, "text": " They know, for example, that age is not relevant, right?", "tokens": [50836, 814, 458, 11, 337, 1365, 11, 300, 3205, 307, 406, 7340, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 436, "seek": 116618, "start": 1178.18, "end": 1181.54, "text": " So they won't tell you that age would tell you", "tokens": [50964, 407, 436, 1582, 380, 980, 291, 300, 3205, 576, 980, 291, 51132], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 437, "seek": 116618, "start": 1181.54, "end": 1184.14, "text": " which one is an animal and which one is a toy.", "tokens": [51132, 597, 472, 307, 364, 5496, 293, 597, 472, 307, 257, 12058, 13, 51262], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 438, "seek": 116618, "start": 1184.14, "end": 1185.9, "text": " It's worth pausing and just thinking about this", "tokens": [51262, 467, 311, 3163, 2502, 7981, 293, 445, 1953, 466, 341, 51350], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 439, "seek": 116618, "start": 1185.9, "end": 1189.46, "text": " and what this means in terms of conceptual representations,", "tokens": [51350, 293, 437, 341, 1355, 294, 2115, 295, 24106, 33358, 11, 51528], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 440, "seek": 116618, "start": 1189.46, "end": 1190.66, "text": " right?", "tokens": [51528, 558, 30, 51588], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 441, "seek": 116618, "start": 1190.66, "end": 1192.8200000000002, "text": " Because you can think about your concept of what makes", "tokens": [51588, 1436, 291, 393, 519, 466, 428, 3410, 295, 437, 1669, 51696], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 442, "seek": 116618, "start": 1192.8200000000002, "end": 1195.46, "text": " something an animal or what makes something a toy", "tokens": [51696, 746, 364, 5496, 420, 437, 1669, 746, 257, 12058, 51828], "temperature": 0.0, "avg_logprob": -0.1109714169874259, "compression_ratio": 1.9133574007220218, "no_speech_prob": 0.0001535506162326783}, {"id": 443, "seek": 119546, "start": 1195.46, "end": 1197.46, "text": " and kind of like the postage stamp example, right?", "tokens": [50364, 293, 733, 295, 411, 264, 2183, 609, 9921, 1365, 11, 558, 30, 50464], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 444, "seek": 119546, "start": 1197.46, "end": 1199.54, "text": " It's intrinsically connected to these other things,", "tokens": [50464, 467, 311, 28621, 984, 4582, 281, 613, 661, 721, 11, 50568], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 445, "seek": 119546, "start": 1199.54, "end": 1202.3, "text": " like what parents are or what's going on", "tokens": [50568, 411, 437, 3152, 366, 420, 437, 311, 516, 322, 50706], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 446, "seek": 119546, "start": 1202.3, "end": 1204.3400000000001, "text": " with your guts inside, right?", "tokens": [50706, 365, 428, 28560, 1854, 11, 558, 30, 50808], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 447, "seek": 119546, "start": 1204.3400000000001, "end": 1206.02, "text": " Or what your behavior is, right?", "tokens": [50808, 1610, 437, 428, 5223, 307, 11, 558, 30, 50892], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 448, "seek": 119546, "start": 1206.02, "end": 1207.9, "text": " That concept is just intrinsically linked there", "tokens": [50892, 663, 3410, 307, 445, 28621, 984, 9408, 456, 50986], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 449, "seek": 119546, "start": 1207.9, "end": 1211.5, "text": " and kids, I think these are preschoolers know that", "tokens": [50986, 293, 2301, 11, 286, 519, 613, 366, 39809, 433, 458, 300, 51166], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 450, "seek": 119546, "start": 1211.5, "end": 1213.26, "text": " from a pretty young age.", "tokens": [51166, 490, 257, 1238, 2037, 3205, 13, 51254], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 451, "seek": 119546, "start": 1214.66, "end": 1216.3400000000001, "text": " Gals asked them a question like one is a dog,", "tokens": [51324, 460, 1124, 2351, 552, 257, 1168, 411, 472, 307, 257, 3000, 11, 51408], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 452, "seek": 119546, "start": 1216.3400000000001, "end": 1219.66, "text": " one is a wolf, what would you do, okay?", "tokens": [51408, 472, 307, 257, 19216, 11, 437, 576, 291, 360, 11, 1392, 30, 51574], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 453, "seek": 119546, "start": 1219.66, "end": 1221.18, "text": " Kids basically say the same things there.", "tokens": [51574, 15694, 1936, 584, 264, 912, 721, 456, 13, 51650], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 454, "seek": 119546, "start": 1221.18, "end": 1223.46, "text": " You could check the insides, you could look at behavior,", "tokens": [51650, 509, 727, 1520, 264, 1028, 1875, 11, 291, 727, 574, 412, 5223, 11, 51764], "temperature": 0.0, "avg_logprob": -0.14063178317647584, "compression_ratio": 1.7636986301369864, "no_speech_prob": 3.5352892155060545e-05}, {"id": 455, "seek": 122346, "start": 1223.46, "end": 1224.7, "text": " you could look at their parents,", "tokens": [50364, 291, 727, 574, 412, 641, 3152, 11, 50426], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 456, "seek": 122346, "start": 1224.7, "end": 1228.38, "text": " see if they had a dog parent or a wolf parent.", "tokens": [50426, 536, 498, 436, 632, 257, 3000, 2596, 420, 257, 19216, 2596, 13, 50610], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 457, "seek": 122346, "start": 1228.38, "end": 1230.14, "text": " Some of these are actually kind of interesting, right?", "tokens": [50610, 2188, 295, 613, 366, 767, 733, 295, 1880, 11, 558, 30, 50698], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 458, "seek": 122346, "start": 1230.14, "end": 1232.6200000000001, "text": " Because I don't think anybody knows,", "tokens": [50698, 1436, 286, 500, 380, 519, 4472, 3255, 11, 50822], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 459, "seek": 122346, "start": 1232.6200000000001, "end": 1235.98, "text": " at least I don't, what you would look for on the insides", "tokens": [50822, 412, 1935, 286, 500, 380, 11, 437, 291, 576, 574, 337, 322, 264, 1028, 1875, 50990], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 460, "seek": 122346, "start": 1235.98, "end": 1238.02, "text": " to distinguish a dog versus a wolf, right?", "tokens": [50990, 281, 20206, 257, 3000, 5717, 257, 19216, 11, 558, 30, 51092], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 461, "seek": 122346, "start": 1238.02, "end": 1241.14, "text": " Like maybe you could go down to DNA", "tokens": [51092, 1743, 1310, 291, 727, 352, 760, 281, 8272, 51248], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 462, "seek": 122346, "start": 1241.14, "end": 1245.06, "text": " or I'm sure you could go down to DNA to tell that.", "tokens": [51248, 420, 286, 478, 988, 291, 727, 352, 760, 281, 8272, 281, 980, 300, 13, 51444], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 463, "seek": 122346, "start": 1245.06, "end": 1246.66, "text": " But people have the intuition that like, okay,", "tokens": [51444, 583, 561, 362, 264, 24002, 300, 411, 11, 1392, 11, 51524], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 464, "seek": 122346, "start": 1246.66, "end": 1248.38, "text": " there's something about being in this category", "tokens": [51524, 456, 311, 746, 466, 885, 294, 341, 7719, 51610], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 465, "seek": 122346, "start": 1248.38, "end": 1250.66, "text": " which depends on these other aspects", "tokens": [51610, 597, 5946, 322, 613, 661, 7270, 51724], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 466, "seek": 122346, "start": 1250.66, "end": 1252.42, "text": " of being in the concept.", "tokens": [51724, 295, 885, 294, 264, 3410, 13, 51812], "temperature": 0.0, "avg_logprob": -0.13193617475793717, "compression_ratio": 1.7423728813559323, "no_speech_prob": 0.0001088899516616948}, {"id": 467, "seek": 125242, "start": 1253.3000000000002, "end": 1254.74, "text": " To me, I read this much simpler.", "tokens": [50408, 1407, 385, 11, 286, 1401, 341, 709, 18587, 13, 50480], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 468, "seek": 125242, "start": 1254.74, "end": 1257.3400000000001, "text": " It's like basically what they're saying is look,", "tokens": [50480, 467, 311, 411, 1936, 437, 436, 434, 1566, 307, 574, 11, 50610], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 469, "seek": 125242, "start": 1257.3400000000001, "end": 1259.5, "text": " like all of this are visual things.", "tokens": [50610, 411, 439, 295, 341, 366, 5056, 721, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 470, "seek": 125242, "start": 1259.5, "end": 1261.42, "text": " It's just that you're trying to project it into language", "tokens": [50718, 467, 311, 445, 300, 291, 434, 1382, 281, 1716, 309, 666, 2856, 50814], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 471, "seek": 125242, "start": 1261.42, "end": 1264.5, "text": " but actually what the kids are probably meaning is,", "tokens": [50814, 457, 767, 437, 264, 2301, 366, 1391, 3620, 307, 11, 50968], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 472, "seek": 125242, "start": 1264.5, "end": 1266.0600000000002, "text": " you will know it when you see it", "tokens": [50968, 291, 486, 458, 309, 562, 291, 536, 309, 51046], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 473, "seek": 125242, "start": 1266.0600000000002, "end": 1267.98, "text": " than when you play with it, right?", "tokens": [51046, 813, 562, 291, 862, 365, 309, 11, 558, 30, 51142], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 474, "seek": 125242, "start": 1267.98, "end": 1270.3000000000002, "text": " It's vision and interaction.", "tokens": [51142, 467, 311, 5201, 293, 9285, 13, 51258], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 475, "seek": 125242, "start": 1271.18, "end": 1272.02, "text": " Yeah, yeah.", "tokens": [51302, 865, 11, 1338, 13, 51344], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 476, "seek": 125242, "start": 1272.02, "end": 1273.5800000000002, "text": " And age is neither.", "tokens": [51344, 400, 3205, 307, 9662, 13, 51422], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 477, "seek": 125242, "start": 1273.5800000000002, "end": 1274.42, "text": " Yeah, yeah.", "tokens": [51422, 865, 11, 1338, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 478, "seek": 125242, "start": 1274.42, "end": 1279.1000000000001, "text": " So I think it's true that, yes, all of these are visual cues.", "tokens": [51464, 407, 286, 519, 309, 311, 2074, 300, 11, 2086, 11, 439, 295, 613, 366, 5056, 32192, 13, 51698], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 479, "seek": 125242, "start": 1279.1000000000001, "end": 1282.14, "text": " I don't know of experiments that look at non-visual cues", "tokens": [51698, 286, 500, 380, 458, 295, 12050, 300, 574, 412, 2107, 12, 4938, 901, 32192, 51850], "temperature": 0.0, "avg_logprob": -0.2698219617207845, "compression_ratio": 1.7545126353790614, "no_speech_prob": 0.0005032362532801926}, {"id": 480, "seek": 128242, "start": 1283.38, "end": 1285.14, "text": " but I agree, yeah, that's interesting.", "tokens": [50412, 457, 286, 3986, 11, 1338, 11, 300, 311, 1880, 13, 50500], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 481, "seek": 128242, "start": 1285.14, "end": 1286.42, "text": " I'll give you one other example", "tokens": [50500, 286, 603, 976, 291, 472, 661, 1365, 50564], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 482, "seek": 128242, "start": 1286.42, "end": 1290.5800000000002, "text": " where they know that there's no cue, right?", "tokens": [50564, 689, 436, 458, 300, 456, 311, 572, 22656, 11, 558, 30, 50772], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 483, "seek": 128242, "start": 1290.5800000000002, "end": 1292.14, "text": " So if you tell them that one is named Amanda", "tokens": [50772, 407, 498, 291, 980, 552, 300, 472, 307, 4926, 20431, 50850], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 484, "seek": 128242, "start": 1292.14, "end": 1294.5, "text": " and one is named Melissa,", "tokens": [50850, 293, 472, 307, 4926, 22844, 11, 50968], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 485, "seek": 128242, "start": 1294.5, "end": 1297.22, "text": " then they'll reject all of these as tests, right?", "tokens": [50968, 550, 436, 603, 8248, 439, 295, 613, 382, 6921, 11, 558, 30, 51104], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 486, "seek": 128242, "start": 1297.22, "end": 1299.02, "text": " They'll say, okay, the insides are not gonna tell you", "tokens": [51104, 814, 603, 584, 11, 1392, 11, 264, 1028, 1875, 366, 406, 799, 980, 291, 51194], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 487, "seek": 128242, "start": 1299.02, "end": 1302.02, "text": " which one is Amanda, the behavior and the parents", "tokens": [51194, 597, 472, 307, 20431, 11, 264, 5223, 293, 264, 3152, 51344], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 488, "seek": 128242, "start": 1302.02, "end": 1304.8200000000002, "text": " and the age and these things are not going to, okay?", "tokens": [51344, 293, 264, 3205, 293, 613, 721, 366, 406, 516, 281, 11, 1392, 30, 51484], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 489, "seek": 128242, "start": 1304.8200000000002, "end": 1306.3400000000001, "text": " So all of this is just to say", "tokens": [51484, 407, 439, 295, 341, 307, 445, 281, 584, 51560], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 490, "seek": 128242, "start": 1306.3400000000001, "end": 1309.78, "text": " that people have a, even kids, right,", "tokens": [51560, 300, 561, 362, 257, 11, 754, 2301, 11, 558, 11, 51732], "temperature": 0.0, "avg_logprob": -0.14147186279296875, "compression_ratio": 1.7692307692307692, "no_speech_prob": 9.02694882825017e-05}, {"id": 491, "seek": 130978, "start": 1310.06, "end": 1312.06, "text": " have pretty sophisticated theories", "tokens": [50378, 362, 1238, 16950, 13667, 50478], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 492, "seek": 130978, "start": 1312.06, "end": 1315.26, "text": " of how concepts relate to other concepts, right?", "tokens": [50478, 295, 577, 10392, 10961, 281, 661, 10392, 11, 558, 30, 50638], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 493, "seek": 130978, "start": 1315.26, "end": 1317.3799999999999, "text": " And in fact, in a situation like this, right,", "tokens": [50638, 400, 294, 1186, 11, 294, 257, 2590, 411, 341, 11, 558, 11, 50744], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 494, "seek": 130978, "start": 1317.3799999999999, "end": 1320.98, "text": " there's nothing visual apparently that could tell you, right?", "tokens": [50744, 456, 311, 1825, 5056, 7970, 300, 727, 980, 291, 11, 558, 30, 50924], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 495, "seek": 130978, "start": 1320.98, "end": 1322.62, "text": " So it's not a visual discrimination task.", "tokens": [50924, 407, 309, 311, 406, 257, 5056, 15973, 5633, 13, 51006], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 496, "seek": 130978, "start": 1322.62, "end": 1325.34, "text": " It's really a kind of conceptual one", "tokens": [51006, 467, 311, 534, 257, 733, 295, 24106, 472, 51142], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 497, "seek": 130978, "start": 1325.34, "end": 1327.66, "text": " that's asking you to look at other kinds", "tokens": [51142, 300, 311, 3365, 291, 281, 574, 412, 661, 3685, 51258], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 498, "seek": 130978, "start": 1327.66, "end": 1329.3, "text": " of conceptual features and things.", "tokens": [51258, 295, 24106, 4122, 293, 721, 13, 51340], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 499, "seek": 130978, "start": 1331.06, "end": 1334.58, "text": " So people have these intuitive theories", "tokens": [51428, 407, 561, 362, 613, 21769, 13667, 51604], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 500, "seek": 130978, "start": 1334.58, "end": 1336.8999999999999, "text": " then one kind of proposal,", "tokens": [51604, 550, 472, 733, 295, 11494, 11, 51720], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 501, "seek": 130978, "start": 1336.8999999999999, "end": 1339.3799999999999, "text": " quite a few people have argued for is that meaning arises", "tokens": [51720, 1596, 257, 1326, 561, 362, 20219, 337, 307, 300, 3620, 27388, 51844], "temperature": 0.0, "avg_logprob": -0.15397383208967683, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.0006461037555709481}, {"id": 502, "seek": 133938, "start": 1339.38, "end": 1341.94, "text": " from essentially the role that a word or a symbol", "tokens": [50364, 490, 4476, 264, 3090, 300, 257, 1349, 420, 257, 5986, 50492], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 503, "seek": 133938, "start": 1341.94, "end": 1344.66, "text": " or a concept plays in this theory, right?", "tokens": [50492, 420, 257, 3410, 5749, 294, 341, 5261, 11, 558, 30, 50628], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 504, "seek": 133938, "start": 1344.66, "end": 1348.5800000000002, "text": " Like the meaning of animals really just intrinsically related", "tokens": [50628, 1743, 264, 3620, 295, 4882, 534, 445, 28621, 984, 4077, 50824], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 505, "seek": 133938, "start": 1348.5800000000002, "end": 1351.5, "text": " to these ways of testing it and these kinds of features", "tokens": [50824, 281, 613, 2098, 295, 4997, 309, 293, 613, 3685, 295, 4122, 50970], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 506, "seek": 133938, "start": 1351.5, "end": 1353.5400000000002, "text": " and all of the other things", "tokens": [50970, 293, 439, 295, 264, 661, 721, 51072], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 507, "seek": 133938, "start": 1353.5400000000002, "end": 1356.46, "text": " that are not kind of simple semantic associates with animal", "tokens": [51072, 300, 366, 406, 733, 295, 2199, 47982, 36914, 365, 5496, 51218], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 508, "seek": 133938, "start": 1356.46, "end": 1359.6200000000001, "text": " but are kind of deeply connected", "tokens": [51218, 457, 366, 733, 295, 8760, 4582, 51376], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 509, "seek": 133938, "start": 1359.6200000000001, "end": 1362.6200000000001, "text": " in the sense of an intuitive theory.", "tokens": [51376, 294, 264, 2020, 295, 364, 21769, 5261, 13, 51526], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 510, "seek": 133938, "start": 1363.46, "end": 1364.8600000000001, "text": " I was trying to come up with examples", "tokens": [51568, 286, 390, 1382, 281, 808, 493, 365, 5110, 51638], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 511, "seek": 133938, "start": 1364.8600000000001, "end": 1368.46, "text": " where this, you know,", "tokens": [51638, 689, 341, 11, 291, 458, 11, 51818], "temperature": 0.0, "avg_logprob": -0.12889563480270244, "compression_ratio": 1.708, "no_speech_prob": 9.31175090954639e-05}, {"id": 512, "seek": 136846, "start": 1368.54, "end": 1370.5, "text": " we'll give people this intuition, right,", "tokens": [50368, 321, 603, 976, 561, 341, 24002, 11, 558, 11, 50466], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 513, "seek": 136846, "start": 1370.5, "end": 1374.02, "text": " that, you know, if you try to define these words,", "tokens": [50466, 300, 11, 291, 458, 11, 498, 291, 853, 281, 6964, 613, 2283, 11, 50642], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 514, "seek": 136846, "start": 1374.02, "end": 1377.06, "text": " if you try to define what an indictment is, for example,", "tokens": [50642, 498, 291, 853, 281, 6964, 437, 364, 49981, 518, 307, 11, 337, 1365, 11, 50794], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 515, "seek": 136846, "start": 1377.06, "end": 1378.3400000000001, "text": " it's very hard to do it in a way", "tokens": [50794, 309, 311, 588, 1152, 281, 360, 309, 294, 257, 636, 50858], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 516, "seek": 136846, "start": 1378.3400000000001, "end": 1380.66, "text": " that doesn't reference other legal terms", "tokens": [50858, 300, 1177, 380, 6408, 661, 5089, 2115, 50974], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 517, "seek": 136846, "start": 1380.66, "end": 1382.42, "text": " and other kind of social constructs", "tokens": [50974, 293, 661, 733, 295, 2093, 7690, 82, 51062], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 518, "seek": 136846, "start": 1382.42, "end": 1387.1000000000001, "text": " and concepts that you already have, right?", "tokens": [51062, 293, 10392, 300, 291, 1217, 362, 11, 558, 30, 51296], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 519, "seek": 136846, "start": 1387.1000000000001, "end": 1388.54, "text": " It's kind of intrinsically related", "tokens": [51296, 467, 311, 733, 295, 28621, 984, 4077, 51368], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 520, "seek": 136846, "start": 1388.54, "end": 1391.46, "text": " to the system of other concepts and terms, right?", "tokens": [51368, 281, 264, 1185, 295, 661, 10392, 293, 2115, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 521, "seek": 136846, "start": 1391.46, "end": 1393.58, "text": " Chord change is kind of like this too in music, right?", "tokens": [51514, 761, 765, 1319, 307, 733, 295, 411, 341, 886, 294, 1318, 11, 558, 30, 51620], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 522, "seek": 136846, "start": 1393.58, "end": 1395.38, "text": " You have to talk about chords and notes", "tokens": [51620, 509, 362, 281, 751, 466, 21733, 293, 5570, 51710], "temperature": 0.0, "avg_logprob": -0.12392502978332061, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005031412001699209}, {"id": 523, "seek": 139538, "start": 1395.8600000000001, "end": 1398.94, "text": " and circle of fifths or whatever, right?", "tokens": [50388, 293, 6329, 295, 9266, 82, 420, 2035, 11, 558, 30, 50542], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 524, "seek": 139538, "start": 1398.94, "end": 1400.8600000000001, "text": " Like these things are just intrinsically related.", "tokens": [50542, 1743, 613, 721, 366, 445, 28621, 984, 4077, 13, 50638], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 525, "seek": 139538, "start": 1400.8600000000001, "end": 1403.5800000000002, "text": " I think force in physics is like this.", "tokens": [50638, 286, 519, 3464, 294, 10649, 307, 411, 341, 13, 50774], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 526, "seek": 139538, "start": 1403.5800000000002, "end": 1405.8200000000002, "text": " It's very hard to talk about it in isolation", "tokens": [50774, 467, 311, 588, 1152, 281, 751, 466, 309, 294, 16001, 50886], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 527, "seek": 139538, "start": 1405.8200000000002, "end": 1408.3000000000002, "text": " independent of, you know, experiments", "tokens": [50886, 6695, 295, 11, 291, 458, 11, 12050, 51010], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 528, "seek": 139538, "start": 1408.3000000000002, "end": 1409.74, "text": " or other concepts or things.", "tokens": [51010, 420, 661, 10392, 420, 721, 13, 51082], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 529, "seek": 139538, "start": 1409.74, "end": 1411.0600000000002, "text": " Or if I said, like, what does a bobbin do", "tokens": [51082, 1610, 498, 286, 848, 11, 411, 11, 437, 775, 257, 748, 6692, 259, 360, 51148], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 530, "seek": 139538, "start": 1411.0600000000002, "end": 1412.7, "text": " in a sewing machine, okay, right?", "tokens": [51148, 294, 257, 19311, 3479, 11, 1392, 11, 558, 30, 51230], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 531, "seek": 139538, "start": 1412.7, "end": 1414.3400000000001, "text": " Like you have to talk about thread", "tokens": [51230, 1743, 291, 362, 281, 751, 466, 7207, 51312], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 532, "seek": 139538, "start": 1414.3400000000001, "end": 1416.0600000000002, "text": " and you have to talk about the processes of sewing.", "tokens": [51312, 293, 291, 362, 281, 751, 466, 264, 7555, 295, 19311, 13, 51398], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 533, "seek": 139538, "start": 1416.0600000000002, "end": 1417.22, "text": " Just the meaning of these things", "tokens": [51398, 1449, 264, 3620, 295, 613, 721, 51456], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 534, "seek": 139538, "start": 1417.22, "end": 1419.98, "text": " are just all intrinsically linked together.", "tokens": [51456, 366, 445, 439, 28621, 984, 9408, 1214, 13, 51594], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 535, "seek": 139538, "start": 1421.2600000000002, "end": 1424.5, "text": " So this idea that meaning,", "tokens": [51658, 407, 341, 1558, 300, 3620, 11, 51820], "temperature": 0.0, "avg_logprob": -0.14312701906476702, "compression_ratio": 1.82078853046595, "no_speech_prob": 0.000404434947995469}, {"id": 536, "seek": 142450, "start": 1424.54, "end": 1426.02, "text": " not about reference, it's about the role", "tokens": [50366, 406, 466, 6408, 11, 309, 311, 466, 264, 3090, 50440], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 537, "seek": 142450, "start": 1426.02, "end": 1428.74, "text": " that something plays is called conceptual role theory.", "tokens": [50440, 300, 746, 5749, 307, 1219, 24106, 3090, 5261, 13, 50576], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 538, "seek": 142450, "start": 1429.66, "end": 1431.22, "text": " Meaning of a word or concept is determined", "tokens": [50622, 19948, 295, 257, 1349, 420, 3410, 307, 9540, 50700], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 539, "seek": 142450, "start": 1431.22, "end": 1433.02, "text": " by the role it plays.", "tokens": [50700, 538, 264, 3090, 309, 5749, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 540, "seek": 142450, "start": 1434.38, "end": 1439.38, "text": " And this has been argued for,", "tokens": [50858, 400, 341, 575, 668, 20219, 337, 11, 51108], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 541, "seek": 142450, "start": 1439.82, "end": 1441.94, "text": " I think maybe most prominently by Ned Block", "tokens": [51130, 286, 519, 1310, 881, 39225, 2276, 538, 31355, 17500, 51236], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 542, "seek": 142450, "start": 1441.94, "end": 1444.22, "text": " who's a philosopher of mind,", "tokens": [51236, 567, 311, 257, 29805, 295, 1575, 11, 51350], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 543, "seek": 142450, "start": 1444.22, "end": 1447.1, "text": " who wrote one of my favorite paper titles,", "tokens": [51350, 567, 4114, 472, 295, 452, 2954, 3035, 12992, 11, 51494], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 544, "seek": 142450, "start": 1447.1, "end": 1450.26, "text": " Advertisement for a Semantics of Psychology,", "tokens": [51494, 1999, 3281, 271, 1712, 337, 257, 14421, 45298, 295, 42827, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 545, "seek": 142450, "start": 1450.26, "end": 1451.9, "text": " which is basically all about, you know,", "tokens": [51652, 597, 307, 1936, 439, 466, 11, 291, 458, 11, 51734], "temperature": 0.0, "avg_logprob": -0.1900874803651054, "compression_ratio": 1.570281124497992, "no_speech_prob": 9.608594700694084e-05}, {"id": 546, "seek": 145190, "start": 1451.9, "end": 1454.74, "text": " how psychology needs a theory of meaning", "tokens": [50364, 577, 15105, 2203, 257, 5261, 295, 3620, 50506], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 547, "seek": 145190, "start": 1454.74, "end": 1456.6200000000001, "text": " and a theory of semantics.", "tokens": [50506, 293, 257, 5261, 295, 4361, 45298, 13, 50600], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 548, "seek": 145190, "start": 1456.6200000000001, "end": 1458.18, "text": " And this idea of conceptual role", "tokens": [50600, 400, 341, 1558, 295, 24106, 3090, 50678], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 549, "seek": 145190, "start": 1458.18, "end": 1460.02, "text": " is something that could do that.", "tokens": [50678, 307, 746, 300, 727, 360, 300, 13, 50770], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 550, "seek": 145190, "start": 1460.02, "end": 1462.38, "text": " So it can explain kind of where meaning comes from.", "tokens": [50770, 407, 309, 393, 2903, 733, 295, 689, 3620, 1487, 490, 13, 50888], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 551, "seek": 145190, "start": 1462.38, "end": 1465.66, "text": " It can address questions of how meaning depends", "tokens": [50888, 467, 393, 2985, 1651, 295, 577, 3620, 5946, 51052], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 552, "seek": 145190, "start": 1465.66, "end": 1467.3400000000001, "text": " on things like your representations", "tokens": [51052, 322, 721, 411, 428, 33358, 51136], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 553, "seek": 145190, "start": 1467.3400000000001, "end": 1469.3000000000002, "text": " or categories that you know,", "tokens": [51136, 420, 10479, 300, 291, 458, 11, 51234], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 554, "seek": 145190, "start": 1469.3000000000002, "end": 1471.38, "text": " can play nicely with compositionality", "tokens": [51234, 393, 862, 9594, 365, 12686, 1860, 51338], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 555, "seek": 145190, "start": 1471.38, "end": 1474.6200000000001, "text": " or other aspects of language.", "tokens": [51338, 420, 661, 7270, 295, 2856, 13, 51500], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 556, "seek": 145190, "start": 1474.6200000000001, "end": 1476.5400000000002, "text": " And I think maybe most compellingly", "tokens": [51500, 400, 286, 519, 1310, 881, 20050, 356, 51596], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 557, "seek": 145190, "start": 1476.5400000000002, "end": 1478.98, "text": " can explain how you could find meaning in brains, right?", "tokens": [51596, 393, 2903, 577, 291, 727, 915, 3620, 294, 15442, 11, 558, 30, 51718], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 558, "seek": 145190, "start": 1478.98, "end": 1480.26, "text": " So if you open up a brain", "tokens": [51718, 407, 498, 291, 1269, 493, 257, 3567, 51782], "temperature": 0.0, "avg_logprob": -0.09682428158395659, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0002531165082473308}, {"id": 559, "seek": 148026, "start": 1480.26, "end": 1482.46, "text": " and you start recording from neurons,", "tokens": [50364, 293, 291, 722, 6613, 490, 22027, 11, 50474], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 560, "seek": 148026, "start": 1483.74, "end": 1485.1, "text": " you know, it's really unclear what it means", "tokens": [50538, 291, 458, 11, 309, 311, 534, 25636, 437, 309, 1355, 50606], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 561, "seek": 148026, "start": 1485.1, "end": 1486.78, "text": " for there to be reference in there,", "tokens": [50606, 337, 456, 281, 312, 6408, 294, 456, 11, 50690], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 562, "seek": 148026, "start": 1486.78, "end": 1489.26, "text": " reference to the external world in there.", "tokens": [50690, 6408, 281, 264, 8320, 1002, 294, 456, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 563, "seek": 148026, "start": 1489.26, "end": 1490.74, "text": " But maybe you could kind of make sense", "tokens": [50814, 583, 1310, 291, 727, 733, 295, 652, 2020, 50888], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 564, "seek": 148026, "start": 1490.74, "end": 1493.34, "text": " of patterns of activity in a way", "tokens": [50888, 295, 8294, 295, 5191, 294, 257, 636, 51018], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 565, "seek": 148026, "start": 1493.34, "end": 1497.9, "text": " that lets you kind of interpret systems", "tokens": [51018, 300, 6653, 291, 733, 295, 7302, 3652, 51246], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 566, "seek": 148026, "start": 1497.9, "end": 1501.1, "text": " of signals and representations.", "tokens": [51246, 295, 12354, 293, 33358, 13, 51406], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 567, "seek": 148026, "start": 1502.42, "end": 1505.02, "text": " Let me give you just one other example of this", "tokens": [51472, 961, 385, 976, 291, 445, 472, 661, 1365, 295, 341, 51602], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 568, "seek": 148026, "start": 1505.02, "end": 1508.82, "text": " that maybe might make things more clear.", "tokens": [51602, 300, 1310, 1062, 652, 721, 544, 1850, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1265937614440918, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.0002131367364199832}, {"id": 569, "seek": 150882, "start": 1508.82, "end": 1510.9399999999998, "text": " This idea of conceptual role semantics,", "tokens": [50364, 639, 1558, 295, 24106, 3090, 4361, 45298, 11, 50470], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 570, "seek": 150882, "start": 1510.9399999999998, "end": 1515.82, "text": " I think is also how meaning works in, say, a computer, okay?", "tokens": [50470, 286, 519, 307, 611, 577, 3620, 1985, 294, 11, 584, 11, 257, 3820, 11, 1392, 30, 50714], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 571, "seek": 150882, "start": 1515.82, "end": 1517.3, "text": " Also, I think in mathematics,", "tokens": [50714, 2743, 11, 286, 519, 294, 18666, 11, 50788], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 572, "seek": 150882, "start": 1517.3, "end": 1519.5, "text": " which is why I was deferring the questions", "tokens": [50788, 597, 307, 983, 286, 390, 25704, 2937, 264, 1651, 50898], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 573, "seek": 150882, "start": 1519.5, "end": 1521.1799999999998, "text": " about mathematics,", "tokens": [50898, 466, 18666, 11, 50982], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 574, "seek": 150882, "start": 1521.1799999999998, "end": 1522.54, "text": " but you could look at something like this, right?", "tokens": [50982, 457, 291, 727, 574, 412, 746, 411, 341, 11, 558, 30, 51050], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 575, "seek": 150882, "start": 1522.54, "end": 1525.6599999999999, "text": " This is a floating point representation", "tokens": [51050, 639, 307, 257, 12607, 935, 10290, 51206], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 576, "seek": 150882, "start": 1525.6599999999999, "end": 1528.26, "text": " and ask what makes the bits in this representation", "tokens": [51206, 293, 1029, 437, 1669, 264, 9239, 294, 341, 10290, 51336], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 577, "seek": 150882, "start": 1528.26, "end": 1529.54, "text": " mean what they do, right?", "tokens": [51336, 914, 437, 436, 360, 11, 558, 30, 51400], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 578, "seek": 150882, "start": 1529.54, "end": 1532.54, "text": " In particular, what makes the first bit mean the sign bit,", "tokens": [51400, 682, 1729, 11, 437, 1669, 264, 700, 857, 914, 264, 1465, 857, 11, 51550], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 579, "seek": 150882, "start": 1532.54, "end": 1533.3799999999999, "text": " right?", "tokens": [51550, 558, 30, 51592], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 580, "seek": 150882, "start": 1533.3799999999999, "end": 1534.78, "text": " It's nothing about being the first one", "tokens": [51592, 467, 311, 1825, 466, 885, 264, 700, 472, 51662], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 581, "seek": 150882, "start": 1534.78, "end": 1536.8999999999999, "text": " because there've been dozens of different conventions", "tokens": [51662, 570, 456, 600, 668, 18431, 295, 819, 33520, 51768], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 582, "seek": 150882, "start": 1536.8999999999999, "end": 1537.9399999999998, "text": " for floating point numbers", "tokens": [51768, 337, 12607, 935, 3547, 51820], "temperature": 0.0, "avg_logprob": -0.15665415500072724, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.00043049739906564355}, {"id": 583, "seek": 153794, "start": 1538.06, "end": 1541.66, "text": " which put the sign bit in all kinds of different places, right?", "tokens": [50370, 597, 829, 264, 1465, 857, 294, 439, 3685, 295, 819, 3190, 11, 558, 30, 50550], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 584, "seek": 153794, "start": 1542.74, "end": 1545.18, "text": " What makes it mean the sign bit", "tokens": [50604, 708, 1669, 309, 914, 264, 1465, 857, 50726], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 585, "seek": 153794, "start": 1545.18, "end": 1547.78, "text": " is how it interacts with all of the other operations", "tokens": [50726, 307, 577, 309, 43582, 365, 439, 295, 264, 661, 7705, 50856], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 586, "seek": 153794, "start": 1547.78, "end": 1551.02, "text": " that you can do with floating point numbers, right?", "tokens": [50856, 300, 291, 393, 360, 365, 12607, 935, 3547, 11, 558, 30, 51018], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 587, "seek": 153794, "start": 1551.02, "end": 1551.98, "text": " So meaning, in some sense,", "tokens": [51018, 407, 3620, 11, 294, 512, 2020, 11, 51066], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 588, "seek": 153794, "start": 1551.98, "end": 1554.26, "text": " comes from the interaction between symbols,", "tokens": [51066, 1487, 490, 264, 9285, 1296, 16944, 11, 51180], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 589, "seek": 153794, "start": 1554.26, "end": 1555.9, "text": " or in other words, their conceptual role.", "tokens": [51180, 420, 294, 661, 2283, 11, 641, 24106, 3090, 13, 51262], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 590, "seek": 153794, "start": 1555.9, "end": 1558.66, "text": " So in particular, like what does negation do, right?", "tokens": [51262, 407, 294, 1729, 11, 411, 437, 775, 2485, 399, 360, 11, 558, 30, 51400], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 591, "seek": 153794, "start": 1558.66, "end": 1561.66, "text": " If I have a negation operator, okay, it flips the sign bit.", "tokens": [51400, 759, 286, 362, 257, 2485, 399, 12973, 11, 1392, 11, 309, 40249, 264, 1465, 857, 13, 51550], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 592, "seek": 153794, "start": 1561.66, "end": 1566.42, "text": " Great, okay, that's where it gets its meaning from, right?", "tokens": [51550, 3769, 11, 1392, 11, 300, 311, 689, 309, 2170, 1080, 3620, 490, 11, 558, 30, 51788], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 593, "seek": 153794, "start": 1566.42, "end": 1567.3400000000001, "text": " Or what does addition do?", "tokens": [51788, 1610, 437, 775, 4500, 360, 30, 51834], "temperature": 0.0, "avg_logprob": -0.12352128337613112, "compression_ratio": 1.7743055555555556, "no_speech_prob": 9.026920452015474e-05}, {"id": 594, "seek": 156734, "start": 1567.8999999999999, "end": 1570.58, "text": " The right thing with respect to the sign bit", "tokens": [50392, 440, 558, 551, 365, 3104, 281, 264, 1465, 857, 50526], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 595, "seek": 156734, "start": 1570.58, "end": 1574.1799999999998, "text": " or multiplication or rounding or whatever, right?", "tokens": [50526, 420, 27290, 420, 48237, 420, 2035, 11, 558, 30, 50706], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 596, "seek": 156734, "start": 1574.1799999999998, "end": 1577.1399999999999, "text": " So what makes this the floating point representation", "tokens": [50706, 407, 437, 1669, 341, 264, 12607, 935, 10290, 50854], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 597, "seek": 156734, "start": 1577.1399999999999, "end": 1580.3799999999999, "text": " or what makes that first bit represent sign", "tokens": [50854, 420, 437, 1669, 300, 700, 857, 2906, 1465, 51016], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 598, "seek": 156734, "start": 1580.3799999999999, "end": 1582.82, "text": " is nothing intrinsic in the representation itself.", "tokens": [51016, 307, 1825, 35698, 294, 264, 10290, 2564, 13, 51138], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 599, "seek": 156734, "start": 1582.82, "end": 1585.1, "text": " It's how it interacts with all of the other components", "tokens": [51138, 467, 311, 577, 309, 43582, 365, 439, 295, 264, 661, 6677, 51252], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 600, "seek": 156734, "start": 1585.1, "end": 1587.8999999999999, "text": " of the system, okay, yeah?", "tokens": [51252, 295, 264, 1185, 11, 1392, 11, 1338, 30, 51392], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 601, "seek": 156734, "start": 1587.8999999999999, "end": 1590.4199999999998, "text": " You explain the difference between this way of thinking", "tokens": [51392, 509, 2903, 264, 2649, 1296, 341, 636, 295, 1953, 51518], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 602, "seek": 156734, "start": 1590.4199999999998, "end": 1593.3, "text": " about the sense of here's a hierarchical approach", "tokens": [51518, 466, 264, 2020, 295, 510, 311, 257, 35250, 804, 3109, 51662], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 603, "seek": 156734, "start": 1593.3, "end": 1595.98, "text": " where there's concepts and there's sort of numbers.", "tokens": [51662, 689, 456, 311, 10392, 293, 456, 311, 1333, 295, 3547, 13, 51796], "temperature": 0.0, "avg_logprob": -0.28458637473857507, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0001123377078329213}, {"id": 604, "seek": 159598, "start": 1596.98, "end": 1601.98, "text": " Yeah, so I think that there's certainly concepts people have", "tokens": [50414, 865, 11, 370, 286, 519, 300, 456, 311, 3297, 10392, 561, 362, 50664], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 605, "seek": 159598, "start": 1603.3, "end": 1604.7, "text": " that are hierarchical, right?", "tokens": [50730, 300, 366, 35250, 804, 11, 558, 30, 50800], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 606, "seek": 159598, "start": 1604.7, "end": 1606.8600000000001, "text": " So we know that dogs are kind of animal", "tokens": [50800, 407, 321, 458, 300, 7197, 366, 733, 295, 5496, 50908], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 607, "seek": 159598, "start": 1606.8600000000001, "end": 1609.34, "text": " and animals are a kind of living thing.", "tokens": [50908, 293, 4882, 366, 257, 733, 295, 2647, 551, 13, 51032], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 608, "seek": 159598, "start": 1609.34, "end": 1612.02, "text": " I think what that kind of picture is missing", "tokens": [51032, 286, 519, 437, 300, 733, 295, 3036, 307, 5361, 51166], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 609, "seek": 159598, "start": 1612.02, "end": 1614.54, "text": " is that our representations are actually,", "tokens": [51166, 307, 300, 527, 33358, 366, 767, 11, 51292], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 610, "seek": 159598, "start": 1614.54, "end": 1617.14, "text": " like computational objects, like they do something, right?", "tokens": [51292, 411, 28270, 6565, 11, 411, 436, 360, 746, 11, 558, 30, 51422], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 611, "seek": 159598, "start": 1617.14, "end": 1621.14, "text": " They interact with each other and they allow us to solve", "tokens": [51422, 814, 4648, 365, 1184, 661, 293, 436, 2089, 505, 281, 5039, 51622], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 612, "seek": 159598, "start": 1621.14, "end": 1622.7, "text": " certain kinds of inference problems", "tokens": [51622, 1629, 3685, 295, 38253, 2740, 51700], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 613, "seek": 159598, "start": 1622.7, "end": 1625.58, "text": " and all of the stuff you could do with your concepts", "tokens": [51700, 293, 439, 295, 264, 1507, 291, 727, 360, 365, 428, 10392, 51844], "temperature": 0.0, "avg_logprob": -0.15204127488938052, "compression_ratio": 1.7566539923954372, "no_speech_prob": 8.749480184633285e-05}, {"id": 614, "seek": 162558, "start": 1625.6599999999999, "end": 1627.82, "text": " like the Trump example, right?", "tokens": [50368, 411, 264, 3899, 1365, 11, 558, 30, 50476], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 615, "seek": 162558, "start": 1629.26, "end": 1632.22, "text": " Yeah, so the claim is not that they're not hierarchical, right?", "tokens": [50548, 865, 11, 370, 264, 3932, 307, 406, 300, 436, 434, 406, 35250, 804, 11, 558, 30, 50696], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 616, "seek": 162558, "start": 1632.22, "end": 1635.62, "text": " It's that the interesting important things they do", "tokens": [50696, 467, 311, 300, 264, 1880, 1021, 721, 436, 360, 50866], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 617, "seek": 162558, "start": 1635.62, "end": 1638.34, "text": " come from interactions kind of internally between concepts", "tokens": [50866, 808, 490, 13280, 733, 295, 19501, 1296, 10392, 51002], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 618, "seek": 162558, "start": 1638.34, "end": 1641.02, "text": " much like the way that the sign bit is interesting", "tokens": [51002, 709, 411, 264, 636, 300, 264, 1465, 857, 307, 1880, 51136], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 619, "seek": 162558, "start": 1641.02, "end": 1643.62, "text": " or important here comes from its interactions", "tokens": [51136, 420, 1021, 510, 1487, 490, 1080, 13280, 51266], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 620, "seek": 162558, "start": 1643.62, "end": 1646.62, "text": " with things like negation and multiplication, yeah, yeah.", "tokens": [51266, 365, 721, 411, 2485, 399, 293, 27290, 11, 1338, 11, 1338, 13, 51416], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 621, "seek": 162558, "start": 1648.22, "end": 1650.4199999999998, "text": " What you're saying here is perfectly good,", "tokens": [51496, 708, 291, 434, 1566, 510, 307, 6239, 665, 11, 51606], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 622, "seek": 162558, "start": 1650.4199999999998, "end": 1653.4199999999998, "text": " but what I have trouble with is buying this", "tokens": [51606, 457, 437, 286, 362, 5253, 365, 307, 6382, 341, 51756], "temperature": 0.0, "avg_logprob": -0.14324736372332705, "compression_ratio": 1.7628458498023716, "no_speech_prob": 0.0001559192023705691}, {"id": 623, "seek": 165342, "start": 1653.46, "end": 1656.42, "text": " as an exclusive theory of semantics.", "tokens": [50366, 382, 364, 13005, 5261, 295, 4361, 45298, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 624, "seek": 165342, "start": 1656.42, "end": 1658.98, "text": " Just like you gave good arguments", "tokens": [50514, 1449, 411, 291, 2729, 665, 12869, 50642], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 625, "seek": 165342, "start": 1658.98, "end": 1663.26, "text": " against that meaning is just reference, okay?", "tokens": [50642, 1970, 300, 3620, 307, 445, 6408, 11, 1392, 30, 50856], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 626, "seek": 165342, "start": 1663.26, "end": 1665.54, "text": " I think you demolished that theory,", "tokens": [50856, 286, 519, 291, 26933, 4729, 300, 5261, 11, 50970], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 627, "seek": 165342, "start": 1665.54, "end": 1669.74, "text": " but now you put up another theory which also I find", "tokens": [50970, 457, 586, 291, 829, 493, 1071, 5261, 597, 611, 286, 915, 51180], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 628, "seek": 165342, "start": 1669.74, "end": 1671.3400000000001, "text": " that it has some good aspects,", "tokens": [51180, 300, 309, 575, 512, 665, 7270, 11, 51260], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 629, "seek": 165342, "start": 1671.3400000000001, "end": 1675.14, "text": " but to make that an exclusive theory is problematic.", "tokens": [51260, 457, 281, 652, 300, 364, 13005, 5261, 307, 19011, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 630, "seek": 165342, "start": 1675.14, "end": 1677.3400000000001, "text": " So if we look at children growing up,", "tokens": [51450, 407, 498, 321, 574, 412, 2227, 4194, 493, 11, 51560], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 631, "seek": 165342, "start": 1677.3400000000001, "end": 1680.54, "text": " there are these studies on sort of concreteness judgments.", "tokens": [51560, 456, 366, 613, 5313, 322, 1333, 295, 1588, 35383, 442, 40337, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 632, "seek": 165342, "start": 1680.54, "end": 1683.14, "text": " So the vocabulary of a child at two,", "tokens": [51720, 407, 264, 19864, 295, 257, 1440, 412, 732, 11, 51850], "temperature": 0.0, "avg_logprob": -0.1456368076908696, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.005807214416563511}, {"id": 633, "seek": 168314, "start": 1683.14, "end": 1686.66, "text": " there are a lot of words in there like milk and bottle", "tokens": [50364, 456, 366, 257, 688, 295, 2283, 294, 456, 411, 5392, 293, 7817, 50540], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 634, "seek": 168314, "start": 1686.66, "end": 1690.1000000000001, "text": " and jump and sit and so forth, which are very concrete,", "tokens": [50540, 293, 3012, 293, 1394, 293, 370, 5220, 11, 597, 366, 588, 9859, 11, 50712], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 635, "seek": 168314, "start": 1690.1000000000001, "end": 1692.0200000000002, "text": " concrete in a visual sense,", "tokens": [50712, 9859, 294, 257, 5056, 2020, 11, 50808], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 636, "seek": 168314, "start": 1692.0200000000002, "end": 1694.18, "text": " concrete in a motor program sense.", "tokens": [50808, 9859, 294, 257, 5932, 1461, 2020, 13, 50916], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 637, "seek": 168314, "start": 1695.26, "end": 1699.0200000000002, "text": " At the age of 10, they have words like justice and fairness", "tokens": [50970, 1711, 264, 3205, 295, 1266, 11, 436, 362, 2283, 411, 6118, 293, 29765, 51158], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 638, "seek": 168314, "start": 1699.0200000000002, "end": 1702.38, "text": " and so forth, which are very much,", "tokens": [51158, 293, 370, 5220, 11, 597, 366, 588, 709, 11, 51326], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 639, "seek": 168314, "start": 1702.38, "end": 1705.3000000000002, "text": " which fit much better into this conceptual road story", "tokens": [51326, 597, 3318, 709, 1101, 666, 341, 24106, 3060, 1657, 51472], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 640, "seek": 168314, "start": 1705.3000000000002, "end": 1708.38, "text": " where is the vocabulary of a child at two,", "tokens": [51472, 689, 307, 264, 19864, 295, 257, 1440, 412, 732, 11, 51626], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 641, "seek": 168314, "start": 1708.38, "end": 1711.46, "text": " maybe one where this kind of groundedness", "tokens": [51626, 1310, 472, 689, 341, 733, 295, 23535, 1287, 51780], "temperature": 0.0, "avg_logprob": -0.19160113824862185, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.001240997458808124}, {"id": 642, "seek": 171146, "start": 1711.46, "end": 1714.94, "text": " to sensory motor experience is a much better account.", "tokens": [50364, 281, 27233, 5932, 1752, 307, 257, 709, 1101, 2696, 13, 50538], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 643, "seek": 171146, "start": 1714.94, "end": 1717.46, "text": " And this is not problematic for me.", "tokens": [50538, 400, 341, 307, 406, 19011, 337, 385, 13, 50664], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 644, "seek": 171146, "start": 1717.46, "end": 1722.06, "text": " Why do we need to have one exclusive theory for meaning?", "tokens": [50664, 1545, 360, 321, 643, 281, 362, 472, 13005, 5261, 337, 3620, 30, 50894], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 645, "seek": 171146, "start": 1722.06, "end": 1725.8600000000001, "text": " Both of these are aspects of meaning.", "tokens": [50894, 6767, 295, 613, 366, 7270, 295, 3620, 13, 51084], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 646, "seek": 171146, "start": 1725.8600000000001, "end": 1727.3, "text": " Yeah, I agree.", "tokens": [51084, 865, 11, 286, 3986, 13, 51156], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 647, "seek": 171146, "start": 1727.3, "end": 1732.3, "text": " So I think that you can think of the physical reference,", "tokens": [51156, 407, 286, 519, 300, 291, 393, 519, 295, 264, 4001, 6408, 11, 51406], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 648, "seek": 171146, "start": 1732.82, "end": 1734.7, "text": " as in some sense one of the conceptual roles", "tokens": [51432, 382, 294, 512, 2020, 472, 295, 264, 24106, 9604, 51526], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 649, "seek": 171146, "start": 1734.7, "end": 1735.94, "text": " that something can have.", "tokens": [51526, 300, 746, 393, 362, 13, 51588], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 650, "seek": 171146, "start": 1737.3, "end": 1738.22, "text": " It is important.", "tokens": [51656, 467, 307, 1021, 13, 51702], "temperature": 0.0, "avg_logprob": -0.25834019978841144, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.0003510566020850092}, {"id": 651, "seek": 173822, "start": 1738.22, "end": 1741.74, "text": " I'm not sure we know kind of how abstract kids early meanings", "tokens": [50364, 286, 478, 406, 988, 321, 458, 733, 295, 577, 12649, 2301, 2440, 28138, 50540], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 652, "seek": 173822, "start": 1741.74, "end": 1743.98, "text": " are for those kinds of words,", "tokens": [50540, 366, 337, 729, 3685, 295, 2283, 11, 50652], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 653, "seek": 173822, "start": 1743.98, "end": 1746.34, "text": " because it has to be a little bit abstract", "tokens": [50652, 570, 309, 575, 281, 312, 257, 707, 857, 12649, 50770], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 654, "seek": 173822, "start": 1746.34, "end": 1748.94, "text": " because you'll call a new bottle that you see a bottle still.", "tokens": [50770, 570, 291, 603, 818, 257, 777, 7817, 300, 291, 536, 257, 7817, 920, 13, 50900], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 655, "seek": 173822, "start": 1748.94, "end": 1751.94, "text": " So you have some abstraction away from the examples", "tokens": [50900, 407, 291, 362, 512, 37765, 1314, 490, 264, 5110, 51050], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 656, "seek": 173822, "start": 1751.94, "end": 1753.38, "text": " of bottles that you've seen,", "tokens": [51050, 295, 15923, 300, 291, 600, 1612, 11, 51122], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 657, "seek": 173822, "start": 1753.38, "end": 1757.1000000000001, "text": " but I agree it feels early on very concrete", "tokens": [51122, 457, 286, 3986, 309, 3417, 2440, 322, 588, 9859, 51308], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 658, "seek": 173822, "start": 1757.1000000000001, "end": 1760.38, "text": " and much less abstract than things we come later.", "tokens": [51308, 293, 709, 1570, 12649, 813, 721, 321, 808, 1780, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 659, "seek": 173822, "start": 1763.26, "end": 1765.42, "text": " Just trying to make sure I understand", "tokens": [51616, 1449, 1382, 281, 652, 988, 286, 1223, 51724], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 660, "seek": 173822, "start": 1765.42, "end": 1766.7, "text": " what this theory is saying.", "tokens": [51724, 437, 341, 5261, 307, 1566, 13, 51788], "temperature": 0.0, "avg_logprob": -0.14846232929060946, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00020337975001893938}, {"id": 661, "seek": 176670, "start": 1766.7, "end": 1769.7, "text": " So is there a character to think of this", "tokens": [50364, 407, 307, 456, 257, 2517, 281, 519, 295, 341, 50514], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 662, "seek": 176670, "start": 1769.7, "end": 1772.3400000000001, "text": " that you're saying that meaning is basically", "tokens": [50514, 300, 291, 434, 1566, 300, 3620, 307, 1936, 50646], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 663, "seek": 176670, "start": 1772.3400000000001, "end": 1776.54, "text": " like some homomorphism onto some either intuitive", "tokens": [50646, 411, 512, 3655, 32702, 1434, 3911, 512, 2139, 21769, 50856], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 664, "seek": 176670, "start": 1776.54, "end": 1778.0800000000002, "text": " or formal theory?", "tokens": [50856, 420, 9860, 5261, 30, 50933], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 665, "seek": 176670, "start": 1779.3400000000001, "end": 1780.26, "text": " Ah, sure.", "tokens": [50996, 2438, 11, 988, 13, 51042], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 666, "seek": 176670, "start": 1781.18, "end": 1784.46, "text": " So then maybe a follow up question is like,", "tokens": [51088, 407, 550, 1310, 257, 1524, 493, 1168, 307, 411, 11, 51252], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 667, "seek": 176670, "start": 1784.46, "end": 1786.78, "text": " how do we know which homomorphisms are valid?", "tokens": [51252, 577, 360, 321, 458, 597, 3655, 32702, 13539, 366, 7363, 30, 51368], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 668, "seek": 176670, "start": 1786.78, "end": 1788.22, "text": " Because I could always,", "tokens": [51368, 1436, 286, 727, 1009, 11, 51440], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 669, "seek": 176670, "start": 1788.22, "end": 1791.3, "text": " if I can have some arbitrary correspondence mapping,", "tokens": [51440, 498, 286, 393, 362, 512, 23211, 38135, 18350, 11, 51594], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 670, "seek": 176670, "start": 1791.3, "end": 1793.78, "text": " I could make anything correspond to anything else.", "tokens": [51594, 286, 727, 652, 1340, 6805, 281, 1340, 1646, 13, 51718], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 671, "seek": 176670, "start": 1794.78, "end": 1796.54, "text": " You know what's so loud here?", "tokens": [51768, 509, 458, 437, 311, 370, 6588, 510, 30, 51856], "temperature": 0.0, "avg_logprob": -0.2832985444502397, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.00023775314912199974}, {"id": 672, "seek": 179654, "start": 1796.54, "end": 1799.42, "text": " Yeah, so I don't think anybody has been that formal.", "tokens": [50364, 865, 11, 370, 286, 500, 380, 519, 4472, 575, 668, 300, 9860, 13, 50508], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 673, "seek": 179654, "start": 1799.42, "end": 1802.02, "text": " People like Putnam have made this kind of argument", "tokens": [50508, 3432, 411, 4935, 5378, 362, 1027, 341, 733, 295, 6770, 50638], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 674, "seek": 179654, "start": 1802.02, "end": 1804.42, "text": " about understanding computation in physical systems,", "tokens": [50638, 466, 3701, 24903, 294, 4001, 3652, 11, 50758], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 675, "seek": 179654, "start": 1804.42, "end": 1806.7, "text": " basically saying like physical systems,", "tokens": [50758, 1936, 1566, 411, 4001, 3652, 11, 50872], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 676, "seek": 179654, "start": 1806.7, "end": 1809.3, "text": " like a brain or in his example, a wall, right?", "tokens": [50872, 411, 257, 3567, 420, 294, 702, 1365, 11, 257, 2929, 11, 558, 30, 51002], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 677, "seek": 179654, "start": 1809.3, "end": 1812.98, "text": " Are so complicated that I could come up with", "tokens": [51002, 2014, 370, 6179, 300, 286, 727, 808, 493, 365, 51186], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 678, "seek": 179654, "start": 1812.98, "end": 1815.26, "text": " kind of any mapping back and forth between the states of it", "tokens": [51186, 733, 295, 604, 18350, 646, 293, 5220, 1296, 264, 4368, 295, 309, 51300], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 679, "seek": 179654, "start": 1815.26, "end": 1819.1, "text": " and the states of the kind of arbitrary computational system.", "tokens": [51300, 293, 264, 4368, 295, 264, 733, 295, 23211, 28270, 1185, 13, 51492], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 680, "seek": 179654, "start": 1819.1, "end": 1822.3, "text": " And that's probably a much longer thing to get into.", "tokens": [51492, 400, 300, 311, 1391, 257, 709, 2854, 551, 281, 483, 666, 13, 51652], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 681, "seek": 179654, "start": 1822.3, "end": 1824.54, "text": " I'll just say that I don't think I have a very easy answer", "tokens": [51652, 286, 603, 445, 584, 300, 286, 500, 380, 519, 286, 362, 257, 588, 1858, 1867, 51764], "temperature": 0.0, "avg_logprob": -0.13439894456129808, "compression_ratio": 1.769491525423729, "no_speech_prob": 7.252662180690095e-05}, {"id": 682, "seek": 182454, "start": 1824.54, "end": 1825.74, "text": " about that, right?", "tokens": [50364, 466, 300, 11, 558, 30, 50424], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 683, "seek": 182454, "start": 1826.74, "end": 1828.98, "text": " I think of this as not kind of,", "tokens": [50474, 286, 519, 295, 341, 382, 406, 733, 295, 11, 50586], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 684, "seek": 182454, "start": 1830.18, "end": 1832.34, "text": " certainly not formalized in that sense,", "tokens": [50646, 3297, 406, 9860, 1602, 294, 300, 2020, 11, 50754], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 685, "seek": 182454, "start": 1832.34, "end": 1835.1599999999999, "text": " but in sort of a higher level in terms of like", "tokens": [50754, 457, 294, 1333, 295, 257, 2946, 1496, 294, 2115, 295, 411, 50895], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 686, "seek": 182454, "start": 1835.1599999999999, "end": 1837.74, "text": " what kinds of theories we should be looking for, right?", "tokens": [50895, 437, 3685, 295, 13667, 321, 820, 312, 1237, 337, 11, 558, 30, 51024], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 687, "seek": 182454, "start": 1837.74, "end": 1838.94, "text": " And then there's lots of work to do", "tokens": [51024, 400, 550, 456, 311, 3195, 295, 589, 281, 360, 51084], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 688, "seek": 182454, "start": 1838.94, "end": 1841.78, "text": " in terms of making that precise.", "tokens": [51084, 294, 2115, 295, 1455, 300, 13600, 13, 51226], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 689, "seek": 182454, "start": 1841.78, "end": 1842.82, "text": " So yeah, yeah.", "tokens": [51226, 407, 1338, 11, 1338, 13, 51278], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 690, "seek": 182454, "start": 1842.82, "end": 1844.6599999999999, "text": " Yeah, Quine actually uses that example", "tokens": [51278, 865, 11, 2326, 533, 767, 4960, 300, 1365, 51370], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 691, "seek": 182454, "start": 1844.6599999999999, "end": 1849.22, "text": " to motivate this kind of theory in like 1950s philosophy.", "tokens": [51370, 281, 28497, 341, 733, 295, 5261, 294, 411, 18141, 82, 10675, 13, 51598], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 692, "seek": 182454, "start": 1849.22, "end": 1850.3799999999999, "text": " Sorry, which example?", "tokens": [51598, 4919, 11, 597, 1365, 30, 51656], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 693, "seek": 182454, "start": 1850.3799999999999, "end": 1852.1, "text": " Quine uses this example.", "tokens": [51656, 2326, 533, 4960, 341, 1365, 13, 51742], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 694, "seek": 182454, "start": 1852.1, "end": 1853.58, "text": " He uses an example of Gavagai.", "tokens": [51742, 634, 4960, 364, 1365, 295, 460, 706, 559, 1301, 13, 51816], "temperature": 0.0, "avg_logprob": -0.23061695805302374, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.000666641048155725}, {"id": 695, "seek": 185358, "start": 1853.58, "end": 1856.1, "text": " You see something popping out and you're like,", "tokens": [50364, 509, 536, 746, 18374, 484, 293, 291, 434, 411, 11, 50490], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 696, "seek": 185358, "start": 1856.1, "end": 1858.62, "text": " how do you know Gavagai means rabbit, not running,", "tokens": [50490, 577, 360, 291, 458, 460, 706, 559, 1301, 1355, 19509, 11, 406, 2614, 11, 50616], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 697, "seek": 185358, "start": 1858.62, "end": 1860.6599999999999, "text": " and not hole, and not something else", "tokens": [50616, 293, 406, 5458, 11, 293, 406, 746, 1646, 50718], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 698, "seek": 185358, "start": 1860.6599999999999, "end": 1865.1, "text": " because the real world doesn't determine what a meaning is.", "tokens": [50718, 570, 264, 957, 1002, 1177, 380, 6997, 437, 257, 3620, 307, 13, 50940], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 699, "seek": 185358, "start": 1865.1, "end": 1866.26, "text": " Yeah, yeah.", "tokens": [50940, 865, 11, 1338, 13, 50998], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 700, "seek": 185358, "start": 1866.26, "end": 1867.1, "text": " Yeah.", "tokens": [50998, 865, 13, 51040], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 701, "seek": 185358, "start": 1867.1, "end": 1869.6999999999998, "text": " The hierarchical concepts and semantics", "tokens": [51040, 440, 35250, 804, 10392, 293, 4361, 45298, 51170], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 702, "seek": 185358, "start": 1869.6999999999998, "end": 1871.86, "text": " have been extensively terminated,", "tokens": [51170, 362, 668, 32636, 1433, 5410, 11, 51278], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 703, "seek": 185358, "start": 1871.86, "end": 1873.1799999999998, "text": " scripting orders and so on.", "tokens": [51278, 5755, 278, 9470, 293, 370, 322, 13, 51344], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 704, "seek": 185358, "start": 1873.1799999999998, "end": 1875.6599999999999, "text": " Has any of this been operationalized at all?", "tokens": [51344, 8646, 604, 295, 341, 668, 16607, 1602, 412, 439, 30, 51468], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 705, "seek": 185358, "start": 1875.6599999999999, "end": 1877.58, "text": " Can you comment on that?", "tokens": [51468, 1664, 291, 2871, 322, 300, 30, 51564], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 706, "seek": 185358, "start": 1877.58, "end": 1878.78, "text": " I don't think so, yeah.", "tokens": [51564, 286, 500, 380, 519, 370, 11, 1338, 13, 51624], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 707, "seek": 185358, "start": 1878.78, "end": 1880.62, "text": " So I mean, I can talk,", "tokens": [51624, 407, 286, 914, 11, 286, 393, 751, 11, 51716], "temperature": 0.0, "avg_logprob": -0.36074705857496997, "compression_ratio": 1.6142322097378277, "no_speech_prob": 0.0015471380902454257}, {"id": 708, "seek": 188062, "start": 1881.62, "end": 1884.58, "text": " I have a couple of examples of kind of learning", "tokens": [50414, 286, 362, 257, 1916, 295, 5110, 295, 733, 295, 2539, 50562], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 709, "seek": 188062, "start": 1884.58, "end": 1889.3799999999999, "text": " intuitive theories, which essentially have this kind", "tokens": [50562, 21769, 13667, 11, 597, 4476, 362, 341, 733, 50802], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 710, "seek": 188062, "start": 1889.3799999999999, "end": 1893.7399999999998, "text": " of character, so of taking data and then trying to come up", "tokens": [50802, 295, 2517, 11, 370, 295, 1940, 1412, 293, 550, 1382, 281, 808, 493, 51020], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 711, "seek": 188062, "start": 1893.7399999999998, "end": 1897.34, "text": " with some structures that obey the right relations, right?", "tokens": [51020, 365, 512, 9227, 300, 19297, 264, 558, 2299, 11, 558, 30, 51200], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 712, "seek": 188062, "start": 1897.34, "end": 1901.1799999999998, "text": " And yeah, I'll talk a little bit about that,", "tokens": [51200, 400, 1338, 11, 286, 603, 751, 257, 707, 857, 466, 300, 11, 51392], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 713, "seek": 188062, "start": 1901.1799999999998, "end": 1905.86, "text": " but there hasn't been a ton of work on that, so yeah.", "tokens": [51392, 457, 456, 6132, 380, 668, 257, 2952, 295, 589, 322, 300, 11, 370, 1338, 13, 51626], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 714, "seek": 188062, "start": 1905.86, "end": 1908.6599999999999, "text": " Could you talk about how this theory deals with", "tokens": [51626, 7497, 291, 751, 466, 577, 341, 5261, 11215, 365, 51766], "temperature": 0.0, "avg_logprob": -0.2212452584124626, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0001910507562570274}, {"id": 715, "seek": 190866, "start": 1908.66, "end": 1911.9, "text": " when the same symbols or words are in different kind", "tokens": [50364, 562, 264, 912, 16944, 420, 2283, 366, 294, 819, 733, 50526], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 716, "seek": 190866, "start": 1911.9, "end": 1913.9, "text": " of theories or settings?", "tokens": [50526, 295, 13667, 420, 6257, 30, 50626], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 717, "seek": 190866, "start": 1913.9, "end": 1916.9, "text": " Is it kind of mean that the symbols themselves", "tokens": [50626, 1119, 309, 733, 295, 914, 300, 264, 16944, 2969, 50776], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 718, "seek": 190866, "start": 1916.9, "end": 1919.26, "text": " don't have meaning or how are the kind of the meanings", "tokens": [50776, 500, 380, 362, 3620, 420, 577, 366, 264, 733, 295, 264, 28138, 50894], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 719, "seek": 190866, "start": 1919.26, "end": 1921.14, "text": " shared across different contexts?", "tokens": [50894, 5507, 2108, 819, 30628, 30, 50988], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 720, "seek": 190866, "start": 1921.14, "end": 1925.6200000000001, "text": " Yeah, so that's an interesting question", "tokens": [50988, 865, 11, 370, 300, 311, 364, 1880, 1168, 51212], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 721, "seek": 190866, "start": 1925.6200000000001, "end": 1928.14, "text": " that I think people have not resolved very well.", "tokens": [51212, 300, 286, 519, 561, 362, 406, 20772, 588, 731, 13, 51338], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 722, "seek": 190866, "start": 1928.14, "end": 1933.14, "text": " So your symbol for your father might play", "tokens": [51338, 407, 428, 5986, 337, 428, 3086, 1062, 862, 51588], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 723, "seek": 190866, "start": 1934.42, "end": 1936.14, "text": " a bunch of different roles, right?", "tokens": [51652, 257, 3840, 295, 819, 9604, 11, 558, 30, 51738], "temperature": 0.0, "avg_logprob": -0.18337799875359787, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0021477430127561092}, {"id": 724, "seek": 193614, "start": 1936.14, "end": 1938.7, "text": " Because you know what job your father has", "tokens": [50364, 1436, 291, 458, 437, 1691, 428, 3086, 575, 50492], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 725, "seek": 193614, "start": 1938.7, "end": 1941.3000000000002, "text": " and you know what family relations and you know", "tokens": [50492, 293, 291, 458, 437, 1605, 2299, 293, 291, 458, 50622], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 726, "seek": 193614, "start": 1941.3000000000002, "end": 1945.94, "text": " what hobbies and I don't think that there's good", "tokens": [50622, 437, 35750, 293, 286, 500, 380, 519, 300, 456, 311, 665, 50854], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 727, "seek": 193614, "start": 1945.94, "end": 1948.94, "text": " kind of formalized accounts of how to make sense", "tokens": [50854, 733, 295, 9860, 1602, 9402, 295, 577, 281, 652, 2020, 51004], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 728, "seek": 193614, "start": 1948.94, "end": 1951.5400000000002, "text": " of all of that, so there's not great theories", "tokens": [51004, 295, 439, 295, 300, 11, 370, 456, 311, 406, 869, 13667, 51134], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 729, "seek": 193614, "start": 1951.5400000000002, "end": 1953.5800000000002, "text": " of kind of formalizing conceptual roles.", "tokens": [51134, 295, 733, 295, 9860, 3319, 24106, 9604, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 730, "seek": 193614, "start": 1954.8200000000002, "end": 1957.42, "text": " I'll give some arguments why I think it's possible", "tokens": [51298, 286, 603, 976, 512, 12869, 983, 286, 519, 309, 311, 1944, 51428], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 731, "seek": 193614, "start": 1957.42, "end": 1959.26, "text": " that language models are doing this at least", "tokens": [51428, 300, 2856, 5245, 366, 884, 341, 412, 1935, 51520], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 732, "seek": 193614, "start": 1959.26, "end": 1962.5800000000002, "text": " in a tiny version, but in terms of like rich", "tokens": [51520, 294, 257, 5870, 3037, 11, 457, 294, 2115, 295, 411, 4593, 51686], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 733, "seek": 193614, "start": 1962.5800000000002, "end": 1964.74, "text": " and kind of human like conceptual roles,", "tokens": [51686, 293, 733, 295, 1952, 411, 24106, 9604, 11, 51794], "temperature": 0.0, "avg_logprob": -0.12851287972213876, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.00013132586900610477}, {"id": 734, "seek": 196474, "start": 1964.74, "end": 1967.06, "text": " I think that's one of the key problems that's hard to solve,", "tokens": [50364, 286, 519, 300, 311, 472, 295, 264, 2141, 2740, 300, 311, 1152, 281, 5039, 11, 50480], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 735, "seek": 196474, "start": 1967.06, "end": 1969.74, "text": " so, is there another one?", "tokens": [50480, 370, 11, 307, 456, 1071, 472, 30, 50614], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 736, "seek": 196474, "start": 1969.74, "end": 1970.58, "text": " Yeah.", "tokens": [50614, 865, 13, 50656], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 737, "seek": 196474, "start": 1970.58, "end": 1975.26, "text": " I think that the oncology of the kind of physical world", "tokens": [50656, 286, 519, 300, 264, 40592, 1793, 295, 264, 733, 295, 4001, 1002, 50890], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 738, "seek": 196474, "start": 1975.26, "end": 1978.14, "text": " people think that we're using right now, for example,", "tokens": [50890, 561, 519, 300, 321, 434, 1228, 558, 586, 11, 337, 1365, 11, 51034], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 739, "seek": 196474, "start": 1978.14, "end": 1983.14, "text": " is a subset of the oncology of a human's mind", "tokens": [51034, 307, 257, 25993, 295, 264, 40592, 1793, 295, 257, 1952, 311, 1575, 51284], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 740, "seek": 196474, "start": 1985.3, "end": 1988.38, "text": " and probably also a subset of all possible", "tokens": [51392, 293, 1391, 611, 257, 25993, 295, 439, 1944, 51546], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 741, "seek": 196474, "start": 1988.38, "end": 1992.5, "text": " future invented concepts and so on.", "tokens": [51546, 2027, 14479, 10392, 293, 370, 322, 13, 51752], "temperature": 0.0, "avg_logprob": -0.3653315882528982, "compression_ratio": 1.6598984771573604, "no_speech_prob": 0.001344475895166397}, {"id": 742, "seek": 199250, "start": 1993.34, "end": 1995.66, "text": " Sorry, what was the, I missed the very first part,", "tokens": [50406, 4919, 11, 437, 390, 264, 11, 286, 6721, 264, 588, 700, 644, 11, 50522], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 743, "seek": 199250, "start": 1995.66, "end": 1996.66, "text": " what was the question part?", "tokens": [50522, 437, 390, 264, 1168, 644, 30, 50572], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 744, "seek": 199250, "start": 1996.66, "end": 1999.02, "text": " So the question is whether you think", "tokens": [50572, 407, 264, 1168, 307, 1968, 291, 519, 50690], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 745, "seek": 199250, "start": 2000.3, "end": 2003.94, "text": " that the existing ontology that you are using now", "tokens": [50754, 300, 264, 6741, 6592, 1793, 300, 291, 366, 1228, 586, 50936], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 746, "seek": 199250, "start": 2003.94, "end": 2007.42, "text": " is a subset of the ontology of human's mind", "tokens": [50936, 307, 257, 25993, 295, 264, 6592, 1793, 295, 1952, 311, 1575, 51110], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 747, "seek": 199250, "start": 2007.42, "end": 2010.54, "text": " that we haven't fully explored", "tokens": [51110, 300, 321, 2378, 380, 4498, 24016, 51266], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 748, "seek": 199250, "start": 2010.54, "end": 2013.54, "text": " and probably that is also a subset of what kind of", "tokens": [51266, 293, 1391, 300, 307, 611, 257, 25993, 295, 437, 733, 295, 51416], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 749, "seek": 199250, "start": 2013.54, "end": 2018.34, "text": " can be inventive or creative produced concept", "tokens": [51416, 393, 312, 7962, 488, 420, 5880, 7126, 3410, 51656], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 750, "seek": 199250, "start": 2018.34, "end": 2019.62, "text": " that you mentioned in the beginning.", "tokens": [51656, 300, 291, 2835, 294, 264, 2863, 13, 51720], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 751, "seek": 199250, "start": 2019.62, "end": 2022.3, "text": " By ontology, do you mean these theories?", "tokens": [51720, 3146, 6592, 1793, 11, 360, 291, 914, 613, 13667, 30, 51854], "temperature": 0.0, "avg_logprob": -0.1660443555127393, "compression_ratio": 1.7659574468085106, "no_speech_prob": 0.0005106859025545418}, {"id": 752, "seek": 202230, "start": 2022.3, "end": 2025.1399999999999, "text": " I mean terms, for example, yeah, concepts.", "tokens": [50364, 286, 914, 2115, 11, 337, 1365, 11, 1338, 11, 10392, 13, 50506], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 753, "seek": 202230, "start": 2025.1399999999999, "end": 2026.1399999999999, "text": " Yeah, concepts.", "tokens": [50506, 865, 11, 10392, 13, 50556], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 754, "seek": 202230, "start": 2026.1399999999999, "end": 2029.1399999999999, "text": " I mean, I don't think any of these", "tokens": [50556, 286, 914, 11, 286, 500, 380, 519, 604, 295, 613, 50706], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 755, "seek": 202230, "start": 2029.1399999999999, "end": 2030.94, "text": " is quite the right answer, right?", "tokens": [50706, 307, 1596, 264, 558, 1867, 11, 558, 30, 50796], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 756, "seek": 202230, "start": 2030.94, "end": 2034.26, "text": " Like these things are actually very difficult to figure out,", "tokens": [50796, 1743, 613, 721, 366, 767, 588, 2252, 281, 2573, 484, 11, 50962], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 757, "seek": 202230, "start": 2035.7, "end": 2037.18, "text": " but I think they're kind of pointing", "tokens": [51034, 457, 286, 519, 436, 434, 733, 295, 12166, 51108], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 758, "seek": 202230, "start": 2037.18, "end": 2039.4199999999998, "text": " in some useful directions or something.", "tokens": [51108, 294, 512, 4420, 11095, 420, 746, 13, 51220], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 759, "seek": 202230, "start": 2039.4199999999998, "end": 2043.22, "text": " So I don't know if that answers your question, but yeah.", "tokens": [51220, 407, 286, 500, 380, 458, 498, 300, 6338, 428, 1168, 11, 457, 1338, 13, 51410], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 760, "seek": 202230, "start": 2043.22, "end": 2045.74, "text": " And if you know the symbols that you're using,", "tokens": [51410, 400, 498, 291, 458, 264, 16944, 300, 291, 434, 1228, 11, 51536], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 761, "seek": 202230, "start": 2045.74, "end": 2046.58, "text": " why does it matter?", "tokens": [51536, 983, 775, 309, 1871, 30, 51578], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 762, "seek": 202230, "start": 2046.58, "end": 2049.7, "text": " Because that's not to define certain meaning,", "tokens": [51578, 1436, 300, 311, 406, 281, 6964, 1629, 3620, 11, 51734], "temperature": 0.0, "avg_logprob": -0.2880522931208376, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.00046517865848727524}, {"id": 763, "seek": 204970, "start": 2049.7, "end": 2054.7, "text": " whether you use words to represent or you find to represent,", "tokens": [50364, 1968, 291, 764, 2283, 281, 2906, 420, 291, 915, 281, 2906, 11, 50614], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 764, "seek": 204970, "start": 2055.66, "end": 2056.98, "text": " it doesn't matter, right?", "tokens": [50662, 309, 1177, 380, 1871, 11, 558, 30, 50728], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 765, "seek": 204970, "start": 2058.1, "end": 2062.3799999999997, "text": " In terms of which symbols, like mental representations or?", "tokens": [50784, 682, 2115, 295, 597, 16944, 11, 411, 4973, 33358, 420, 30, 50998], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 766, "seek": 204970, "start": 2062.3799999999997, "end": 2065.8599999999997, "text": " Yeah, without a concept, whether you use the words", "tokens": [50998, 865, 11, 1553, 257, 3410, 11, 1968, 291, 764, 264, 2283, 51172], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 767, "seek": 204970, "start": 2065.8599999999997, "end": 2068.8599999999997, "text": " to represent that or you think that's fine,", "tokens": [51172, 281, 2906, 300, 420, 291, 519, 300, 311, 2489, 11, 51322], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 768, "seek": 204970, "start": 2068.8599999999997, "end": 2071.2999999999997, "text": " you think that's all that comes out.", "tokens": [51322, 291, 519, 300, 311, 439, 300, 1487, 484, 13, 51444], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 769, "seek": 204970, "start": 2071.2999999999997, "end": 2073.74, "text": " Or like, I'm sure that this kind of concept", "tokens": [51444, 1610, 411, 11, 286, 478, 988, 300, 341, 733, 295, 3410, 51566], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 770, "seek": 204970, "start": 2073.74, "end": 2076.3799999999997, "text": " has a lot to do with the form,", "tokens": [51566, 575, 257, 688, 281, 360, 365, 264, 1254, 11, 51698], "temperature": 0.0, "avg_logprob": -0.6929404589594627, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003762939479202032}, {"id": 771, "seek": 207638, "start": 2077.34, "end": 2079.7000000000003, "text": " but I think it's just a little bit of a question.", "tokens": [50412, 457, 286, 519, 309, 311, 445, 257, 707, 857, 295, 257, 1168, 13, 50530], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 772, "seek": 207638, "start": 2079.7000000000003, "end": 2081.2200000000003, "text": " Yeah, so yeah.", "tokens": [50530, 865, 11, 370, 1338, 13, 50606], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 773, "seek": 207638, "start": 2081.2200000000003, "end": 2084.62, "text": " Just to make sure that you, how far do you still have to,", "tokens": [50606, 1449, 281, 652, 988, 300, 291, 11, 577, 1400, 360, 291, 920, 362, 281, 11, 50776], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 774, "seek": 207638, "start": 2084.62, "end": 2085.94, "text": " like do you have any?", "tokens": [50776, 411, 360, 291, 362, 604, 30, 50842], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 775, "seek": 207638, "start": 2085.94, "end": 2087.3, "text": " I have a little ways to go.", "tokens": [50842, 286, 362, 257, 707, 2098, 281, 352, 13, 50910], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 776, "seek": 207638, "start": 2087.3, "end": 2090.6600000000003, "text": " Okay, so maybe we should push this the word", "tokens": [50910, 1033, 11, 370, 1310, 321, 820, 2944, 341, 264, 1349, 51078], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 777, "seek": 207638, "start": 2090.6600000000003, "end": 2091.9, "text": " after, after at the end,", "tokens": [51078, 934, 11, 934, 412, 264, 917, 11, 51140], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 778, "seek": 207638, "start": 2091.9, "end": 2094.78, "text": " because it seems like a deeper discussion.", "tokens": [51140, 570, 309, 2544, 411, 257, 7731, 5017, 13, 51284], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 779, "seek": 207638, "start": 2094.78, "end": 2096.42, "text": " Yeah, yeah, okay, great.", "tokens": [51284, 865, 11, 1338, 11, 1392, 11, 869, 13, 51366], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 780, "seek": 207638, "start": 2098.62, "end": 2103.62, "text": " Okay, so I talked about these kinds of accounts of meaning,", "tokens": [51476, 1033, 11, 370, 286, 2825, 466, 613, 3685, 295, 9402, 295, 3620, 11, 51726], "temperature": 0.0, "avg_logprob": -0.48174498184867526, "compression_ratio": 1.6255506607929515, "no_speech_prob": 0.0019539445638656616}, {"id": 781, "seek": 210362, "start": 2104.3399999999997, "end": 2108.7799999999997, "text": " particular meaning as conceptual role.", "tokens": [50400, 1729, 3620, 382, 24106, 3090, 13, 50622], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 782, "seek": 210362, "start": 2108.7799999999997, "end": 2111.46, "text": " And let me talk a little bit about learning conceptual roles", "tokens": [50622, 400, 718, 385, 751, 257, 707, 857, 466, 2539, 24106, 9604, 50756], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 783, "seek": 210362, "start": 2111.46, "end": 2115.8199999999997, "text": " and why we might think that's plausible or useful.", "tokens": [50756, 293, 983, 321, 1062, 519, 300, 311, 39925, 420, 4420, 13, 50974], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 784, "seek": 210362, "start": 2117.74, "end": 2120.54, "text": " Seems to me at least that large language models", "tokens": [51070, 22524, 281, 385, 412, 1935, 300, 2416, 2856, 5245, 51210], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 785, "seek": 210362, "start": 2120.54, "end": 2122.62, "text": " almost certainly need to learn some of these pieces", "tokens": [51210, 1920, 3297, 643, 281, 1466, 512, 295, 613, 3755, 51314], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 786, "seek": 210362, "start": 2122.62, "end": 2124.3399999999997, "text": " of conceptual role,", "tokens": [51314, 295, 24106, 3090, 11, 51400], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 787, "seek": 210362, "start": 2124.3399999999997, "end": 2126.14, "text": " that these kinds of things seem really necessary", "tokens": [51400, 300, 613, 3685, 295, 721, 1643, 534, 4818, 51490], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 788, "seek": 210362, "start": 2126.14, "end": 2129.02, "text": " for the stuff large language models are good at, right?", "tokens": [51490, 337, 264, 1507, 2416, 2856, 5245, 366, 665, 412, 11, 558, 30, 51634], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 789, "seek": 210362, "start": 2129.02, "end": 2131.8599999999997, "text": " Writing coherent texts or doing translations", "tokens": [51634, 32774, 36239, 15765, 420, 884, 37578, 51776], "temperature": 0.0, "avg_logprob": -0.18046442667643228, "compression_ratio": 1.75, "no_speech_prob": 0.0003568552783690393}, {"id": 790, "seek": 213186, "start": 2132.2200000000003, "end": 2134.58, "text": " or providing definitions or providing elaborations", "tokens": [50382, 420, 6530, 21988, 420, 6530, 16298, 763, 50500], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 791, "seek": 213186, "start": 2134.58, "end": 2136.7000000000003, "text": " or explanations, all of those things require you", "tokens": [50500, 420, 28708, 11, 439, 295, 729, 721, 3651, 291, 50606], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 792, "seek": 213186, "start": 2136.7000000000003, "end": 2138.7400000000002, "text": " to put symbols in the right relationships", "tokens": [50606, 281, 829, 16944, 294, 264, 558, 6159, 50708], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 793, "seek": 213186, "start": 2138.7400000000002, "end": 2140.7400000000002, "text": " with other symbols, right?", "tokens": [50708, 365, 661, 16944, 11, 558, 30, 50808], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 794, "seek": 213186, "start": 2140.7400000000002, "end": 2144.6600000000003, "text": " And that means that to do those things well,", "tokens": [50808, 400, 300, 1355, 300, 281, 360, 729, 721, 731, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 795, "seek": 213186, "start": 2144.6600000000003, "end": 2146.7000000000003, "text": " you essentially have to have some little components", "tokens": [51004, 291, 4476, 362, 281, 362, 512, 707, 6677, 51106], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 796, "seek": 213186, "start": 2146.7000000000003, "end": 2148.54, "text": " of conceptual roles, right?", "tokens": [51106, 295, 24106, 9604, 11, 558, 30, 51198], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 797, "seek": 213186, "start": 2151.6200000000003, "end": 2153.78, "text": " One way to think about this is that human meanings", "tokens": [51352, 1485, 636, 281, 519, 466, 341, 307, 300, 1952, 28138, 51460], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 798, "seek": 213186, "start": 2153.78, "end": 2156.6600000000003, "text": " or human conceptual roles generated the text, right?", "tokens": [51460, 420, 1952, 24106, 9604, 10833, 264, 2487, 11, 558, 30, 51604], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 799, "seek": 213186, "start": 2156.6600000000003, "end": 2159.9, "text": " So maybe a smart inferential model could invert that", "tokens": [51604, 407, 1310, 257, 4069, 13596, 2549, 2316, 727, 33966, 300, 51766], "temperature": 0.0, "avg_logprob": -0.13230636233375187, "compression_ratio": 1.8, "no_speech_prob": 0.0001686244213487953}, {"id": 800, "seek": 215990, "start": 2159.9, "end": 2162.6600000000003, "text": " and figure out what were the likely conceptual roles", "tokens": [50364, 293, 2573, 484, 437, 645, 264, 3700, 24106, 9604, 50502], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 801, "seek": 215990, "start": 2162.6600000000003, "end": 2165.98, "text": " that generated the thing that I saw.", "tokens": [50502, 300, 10833, 264, 551, 300, 286, 1866, 13, 50668], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 802, "seek": 215990, "start": 2165.98, "end": 2167.5, "text": " I like this, the Stringer quote, right?", "tokens": [50668, 286, 411, 341, 11, 264, 745, 2937, 260, 6513, 11, 558, 30, 50744], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 803, "seek": 215990, "start": 2167.5, "end": 2169.2200000000003, "text": " The structure of sentences serves as an image", "tokens": [50744, 440, 3877, 295, 16579, 13451, 382, 364, 3256, 50830], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 804, "seek": 215990, "start": 2169.2200000000003, "end": 2170.54, "text": " of the structure of thoughts, right?", "tokens": [50830, 295, 264, 3877, 295, 4598, 11, 558, 30, 50896], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 805, "seek": 215990, "start": 2170.54, "end": 2173.82, "text": " Some projection of our thoughts or our meanings,", "tokens": [50896, 2188, 22743, 295, 527, 4598, 420, 527, 28138, 11, 51060], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 806, "seek": 215990, "start": 2173.82, "end": 2176.7400000000002, "text": " our conceptual roles that gets realized into sentences.", "tokens": [51060, 527, 24106, 9604, 300, 2170, 5334, 666, 16579, 13, 51206], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 807, "seek": 215990, "start": 2176.7400000000002, "end": 2177.58, "text": " Yeah.", "tokens": [51206, 865, 13, 51248], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 808, "seek": 215990, "start": 2177.58, "end": 2178.98, "text": " Yeah, so are you gonna follow up on something", "tokens": [51248, 865, 11, 370, 366, 291, 799, 1524, 493, 322, 746, 51318], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 809, "seek": 215990, "start": 2178.98, "end": 2181.46, "text": " that was a great, a recontextuality kind of example?", "tokens": [51318, 300, 390, 257, 869, 11, 257, 850, 896, 3828, 901, 507, 733, 295, 1365, 30, 51442], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 810, "seek": 215990, "start": 2181.46, "end": 2184.7000000000003, "text": " Is that a recontextuality kind of thing?", "tokens": [51442, 1119, 300, 257, 850, 896, 3828, 901, 507, 733, 295, 551, 30, 51604], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 811, "seek": 215990, "start": 2184.7000000000003, "end": 2185.54, "text": " Which examples are you talking about?", "tokens": [51604, 3013, 5110, 366, 291, 1417, 466, 30, 51646], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 812, "seek": 215990, "start": 2185.54, "end": 2186.6600000000003, "text": " Are you gonna recontextualize,", "tokens": [51646, 2014, 291, 799, 850, 896, 3828, 901, 1125, 11, 51702], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 813, "seek": 215990, "start": 2186.6600000000003, "end": 2188.5, "text": " let's say a gentleman's two boxes example,", "tokens": [51702, 718, 311, 584, 257, 15761, 311, 732, 9002, 1365, 11, 51794], "temperature": 0.0, "avg_logprob": -0.4461465183692642, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.0017535133520141244}, {"id": 814, "seek": 218850, "start": 2188.5, "end": 2190.42, "text": " or is that a hyper-projection?", "tokens": [50364, 420, 307, 300, 257, 9848, 12, 4318, 1020, 313, 30, 50460], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 815, "seek": 218850, "start": 2191.86, "end": 2193.94, "text": " No, I wasn't gonna go back to that.", "tokens": [50532, 883, 11, 286, 2067, 380, 799, 352, 646, 281, 300, 13, 50636], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 816, "seek": 218850, "start": 2193.94, "end": 2196.38, "text": " Yeah, but I'll talk about a study", "tokens": [50636, 865, 11, 457, 286, 603, 751, 466, 257, 2979, 50758], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 817, "seek": 218850, "start": 2196.38, "end": 2200.02, "text": " in Large-Range Models in a minute, okay.", "tokens": [50758, 294, 33092, 12, 49, 933, 6583, 1625, 294, 257, 3456, 11, 1392, 13, 50940], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 818, "seek": 218850, "start": 2201.26, "end": 2202.98, "text": " Okay, a lot of people have the intuition", "tokens": [51002, 1033, 11, 257, 688, 295, 561, 362, 264, 24002, 51088], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 819, "seek": 218850, "start": 2202.98, "end": 2203.82, "text": " this is not possible.", "tokens": [51088, 341, 307, 406, 1944, 13, 51130], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 820, "seek": 218850, "start": 2203.82, "end": 2206.74, "text": " I think this is kind of the Bender and Marcus intuition", "tokens": [51130, 286, 519, 341, 307, 733, 295, 264, 363, 3216, 293, 26574, 24002, 51276], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 821, "seek": 218850, "start": 2206.74, "end": 2208.38, "text": " that our thoughts really get projected", "tokens": [51276, 300, 527, 4598, 534, 483, 26231, 51358], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 822, "seek": 218850, "start": 2208.38, "end": 2210.78, "text": " into this kind of impoverished sequence of sounds, right?", "tokens": [51358, 666, 341, 733, 295, 704, 3570, 4729, 8310, 295, 3263, 11, 558, 30, 51478], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 823, "seek": 218850, "start": 2210.78, "end": 2212.18, "text": " How could you discover something", "tokens": [51478, 1012, 727, 291, 4411, 746, 51548], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 824, "seek": 218850, "start": 2212.18, "end": 2215.74, "text": " like rich conceptual roles there, right?", "tokens": [51548, 411, 4593, 24106, 9604, 456, 11, 558, 30, 51726], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 825, "seek": 218850, "start": 2215.74, "end": 2218.06, "text": " If you just have this projection of language,", "tokens": [51726, 759, 291, 445, 362, 341, 22743, 295, 2856, 11, 51842], "temperature": 0.0, "avg_logprob": -0.30346104677985697, "compression_ratio": 1.6114864864864864, "no_speech_prob": 0.00012928906653542072}, {"id": 826, "seek": 221806, "start": 2218.06, "end": 2220.34, "text": " how could that ever support rich and interesting", "tokens": [50364, 577, 727, 300, 1562, 1406, 4593, 293, 1880, 50478], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 827, "seek": 221806, "start": 2220.34, "end": 2222.22, "text": " kinds of conceptual roles?", "tokens": [50478, 3685, 295, 24106, 9604, 30, 50572], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 828, "seek": 221806, "start": 2224.58, "end": 2227.2599999999998, "text": " One kind of way that I think is a helpful analogy,", "tokens": [50690, 1485, 733, 295, 636, 300, 286, 519, 307, 257, 4961, 21663, 11, 50824], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 829, "seek": 221806, "start": 2227.2599999999998, "end": 2230.18, "text": " although not kind of a mathematically precise", "tokens": [50824, 4878, 406, 733, 295, 257, 44003, 13600, 50970], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 830, "seek": 221806, "start": 2230.18, "end": 2231.86, "text": " implementation or something,", "tokens": [50970, 11420, 420, 746, 11, 51054], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 831, "seek": 221806, "start": 2231.86, "end": 2233.66, "text": " people may know these embedding theorems", "tokens": [51054, 561, 815, 458, 613, 12240, 3584, 10299, 2592, 51144], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 832, "seek": 221806, "start": 2233.66, "end": 2237.2599999999998, "text": " from dynamical systems, which I think are very cool.", "tokens": [51144, 490, 5999, 804, 3652, 11, 597, 286, 519, 366, 588, 1627, 13, 51324], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 833, "seek": 221806, "start": 2237.2599999999998, "end": 2240.14, "text": " There's this paper called Geometry from a Time Series,", "tokens": [51324, 821, 311, 341, 3035, 1219, 2876, 34730, 490, 257, 6161, 13934, 11, 51468], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 834, "seek": 221806, "start": 2241.14, "end": 2243.14, "text": " which essentially shows that in some cases", "tokens": [51518, 597, 4476, 3110, 300, 294, 512, 3331, 51618], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 835, "seek": 221806, "start": 2243.14, "end": 2245.9, "text": " you can take projections of dynamical systems", "tokens": [51618, 291, 393, 747, 32371, 295, 5999, 804, 3652, 51756], "temperature": 0.0, "avg_logprob": -0.18565735589890253, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.00020339286129456013}, {"id": 836, "seek": 224590, "start": 2245.94, "end": 2248.7000000000003, "text": " and recover things which capture the structure", "tokens": [50366, 293, 8114, 721, 597, 7983, 264, 3877, 50504], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 837, "seek": 224590, "start": 2248.7000000000003, "end": 2251.42, "text": " of the dynamics from that projection.", "tokens": [50504, 295, 264, 15679, 490, 300, 22743, 13, 50640], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 838, "seek": 224590, "start": 2251.42, "end": 2253.3, "text": " So in particular, in this paper, they go through this,", "tokens": [50640, 407, 294, 1729, 11, 294, 341, 3035, 11, 436, 352, 807, 341, 11, 50734], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 839, "seek": 224590, "start": 2253.3, "end": 2255.6600000000003, "text": " which is the Rossler Attractor.", "tokens": [50734, 597, 307, 264, 16140, 1918, 7298, 1897, 284, 13, 50852], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 840, "seek": 224590, "start": 2255.6600000000003, "end": 2258.94, "text": " This is a three-dimensional system", "tokens": [50852, 639, 307, 257, 1045, 12, 18759, 1185, 51016], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 841, "seek": 224590, "start": 2258.94, "end": 2261.34, "text": " of differential equations.", "tokens": [51016, 295, 15756, 11787, 13, 51136], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 842, "seek": 224590, "start": 2261.34, "end": 2264.14, "text": " And what you can do is take a one-dimensional projection", "tokens": [51136, 400, 437, 291, 393, 360, 307, 747, 257, 472, 12, 18759, 22743, 51276], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 843, "seek": 224590, "start": 2264.14, "end": 2265.2200000000003, "text": " of those dynamics.", "tokens": [51276, 295, 729, 15679, 13, 51330], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 844, "seek": 224590, "start": 2265.2200000000003, "end": 2267.06, "text": " So you can look at just the X location", "tokens": [51330, 407, 291, 393, 574, 412, 445, 264, 1783, 4914, 51422], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 845, "seek": 224590, "start": 2267.06, "end": 2269.58, "text": " of what's happening there.", "tokens": [51422, 295, 437, 311, 2737, 456, 13, 51548], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 846, "seek": 224590, "start": 2269.58, "end": 2273.1800000000003, "text": " And through a clever trick essentially translating", "tokens": [51548, 400, 807, 257, 13494, 4282, 4476, 35030, 51728], "temperature": 0.0, "avg_logprob": -0.18402488078546086, "compression_ratio": 1.704, "no_speech_prob": 8.480138058075681e-05}, {"id": 847, "seek": 227318, "start": 2273.18, "end": 2276.06, "text": " the one-dimensions into three-dimensions", "tokens": [50364, 264, 472, 12, 13595, 8302, 666, 1045, 12, 13595, 8302, 50508], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 848, "seek": 227318, "start": 2276.06, "end": 2281.06, "text": " using, by going backwards in time, some number of steps,", "tokens": [50508, 1228, 11, 538, 516, 12204, 294, 565, 11, 512, 1230, 295, 4439, 11, 50758], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 849, "seek": 227318, "start": 2281.2599999999998, "end": 2283.06, "text": " you can actually recover the structure of this", "tokens": [50768, 291, 393, 767, 8114, 264, 3877, 295, 341, 50858], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 850, "seek": 227318, "start": 2283.06, "end": 2284.8999999999996, "text": " from the one-dimensional projection.", "tokens": [50858, 490, 264, 472, 12, 18759, 22743, 13, 50950], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 851, "seek": 227318, "start": 2286.7799999999997, "end": 2289.4199999999996, "text": " And there's other kind of general theorems", "tokens": [51044, 400, 456, 311, 661, 733, 295, 2674, 10299, 2592, 51176], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 852, "seek": 227318, "start": 2289.4199999999996, "end": 2290.8199999999997, "text": " about when this is possible,", "tokens": [51176, 466, 562, 341, 307, 1944, 11, 51246], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 853, "seek": 227318, "start": 2290.8199999999997, "end": 2293.3399999999997, "text": " Parkinson's embedding theorem and things like that.", "tokens": [51246, 35823, 311, 12240, 3584, 20904, 293, 721, 411, 300, 13, 51372], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 854, "seek": 227318, "start": 2293.3399999999997, "end": 2297.58, "text": " The point here is that we shouldn't really have", "tokens": [51372, 440, 935, 510, 307, 300, 321, 4659, 380, 534, 362, 51584], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 855, "seek": 227318, "start": 2297.58, "end": 2299.2999999999997, "text": " strong intuitions about what's possible", "tokens": [51584, 2068, 16224, 626, 466, 437, 311, 1944, 51670], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 856, "seek": 227318, "start": 2299.2999999999997, "end": 2301.62, "text": " from some projection of thoughts,", "tokens": [51670, 490, 512, 22743, 295, 4598, 11, 51786], "temperature": 0.0, "avg_logprob": -0.18505559144196687, "compression_ratio": 1.714859437751004, "no_speech_prob": 0.00022337470727507025}, {"id": 857, "seek": 230162, "start": 2301.62, "end": 2304.98, "text": " because oftentimes there might be possible", "tokens": [50364, 570, 18349, 456, 1062, 312, 1944, 50532], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 858, "seek": 230162, "start": 2304.98, "end": 2308.1, "text": " for people to, or for learning models", "tokens": [50532, 337, 561, 281, 11, 420, 337, 2539, 5245, 50688], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 859, "seek": 230162, "start": 2308.1, "end": 2311.42, "text": " to reconstruct kind of interesting parts", "tokens": [50688, 281, 31499, 733, 295, 1880, 3166, 50854], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 860, "seek": 230162, "start": 2311.42, "end": 2313.8199999999997, "text": " of the structure of some system", "tokens": [50854, 295, 264, 3877, 295, 512, 1185, 50974], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 861, "seek": 230162, "start": 2313.8199999999997, "end": 2317.22, "text": " just from simple kind of measurements of that system.", "tokens": [50974, 445, 490, 2199, 733, 295, 15383, 295, 300, 1185, 13, 51144], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 862, "seek": 230162, "start": 2317.22, "end": 2321.1, "text": " Actually Shaw here, the senior author wrote an entire book", "tokens": [51144, 5135, 27132, 510, 11, 264, 7965, 3793, 4114, 364, 2302, 1446, 51338], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 863, "seek": 230162, "start": 2321.1, "end": 2324.2999999999997, "text": " on recovering the kind of dynamical properties", "tokens": [51338, 322, 29180, 264, 733, 295, 5999, 804, 7221, 51498], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 864, "seek": 230162, "start": 2324.2999999999997, "end": 2325.62, "text": " of a dripping water faucet,", "tokens": [51498, 295, 257, 37460, 1281, 49567, 302, 11, 51564], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 865, "seek": 230162, "start": 2326.8399999999997, "end": 2328.94, "text": " where you can measure the time between drips", "tokens": [51625, 689, 291, 393, 3481, 264, 565, 1296, 1630, 1878, 51730], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 866, "seek": 230162, "start": 2328.94, "end": 2331.3399999999997, "text": " and figure out things about the kind of latent variables", "tokens": [51730, 293, 2573, 484, 721, 466, 264, 733, 295, 48994, 9102, 51850], "temperature": 0.0, "avg_logprob": -0.15936438551226867, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.0001354874111711979}, {"id": 867, "seek": 233134, "start": 2331.34, "end": 2332.94, "text": " and latent structures they're using techniques", "tokens": [50364, 293, 48994, 9227, 436, 434, 1228, 7512, 50444], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 868, "seek": 233134, "start": 2332.94, "end": 2334.46, "text": " that are a lot like these.", "tokens": [50444, 300, 366, 257, 688, 411, 613, 13, 50520], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 869, "seek": 233134, "start": 2335.82, "end": 2338.2200000000003, "text": " In psychology, actually people have also been interested", "tokens": [50588, 682, 15105, 11, 767, 561, 362, 611, 668, 3102, 50708], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 870, "seek": 233134, "start": 2338.2200000000003, "end": 2341.34, "text": " in kind of closely related types of models.", "tokens": [50708, 294, 733, 295, 8185, 4077, 3467, 295, 5245, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 871, "seek": 233134, "start": 2341.34, "end": 2346.0, "text": " So there's this work by Roger Shepard in the 80s,", "tokens": [50864, 407, 456, 311, 341, 589, 538, 17666, 1160, 595, 515, 294, 264, 4688, 82, 11, 51097], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 872, "seek": 233134, "start": 2346.0, "end": 2349.02, "text": " which essentially would take behavioral judgments", "tokens": [51097, 597, 4476, 576, 747, 19124, 40337, 51248], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 873, "seek": 233134, "start": 2349.02, "end": 2352.78, "text": " and try to infer the underlying structures behind them.", "tokens": [51248, 293, 853, 281, 13596, 264, 14217, 9227, 2261, 552, 13, 51436], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 874, "seek": 233134, "start": 2352.78, "end": 2357.26, "text": " So for example, this matrix here is different colors", "tokens": [51436, 407, 337, 1365, 11, 341, 8141, 510, 307, 819, 4577, 51660], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 875, "seek": 233134, "start": 2357.26, "end": 2358.7000000000003, "text": " or different wavelengths of light", "tokens": [51660, 420, 819, 47424, 295, 1442, 51732], "temperature": 0.0, "avg_logprob": -0.15756742321715064, "compression_ratio": 1.585551330798479, "no_speech_prob": 7.030159758869559e-05}, {"id": 876, "seek": 235870, "start": 2358.7, "end": 2362.2599999999998, "text": " and then confusability between them on judgment tasks.", "tokens": [50364, 293, 550, 1497, 301, 2310, 1296, 552, 322, 12216, 9608, 13, 50542], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 877, "seek": 235870, "start": 2362.2599999999998, "end": 2364.1, "text": " So just how similar are these things", "tokens": [50542, 407, 445, 577, 2531, 366, 613, 721, 50634], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 878, "seek": 235870, "start": 2364.1, "end": 2366.2999999999997, "text": " or how confusable is one color with another.", "tokens": [50634, 420, 577, 1497, 301, 712, 307, 472, 2017, 365, 1071, 13, 50744], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 879, "seek": 235870, "start": 2367.3799999999997, "end": 2369.96, "text": " And Shepard was using multi-dimensional scaling", "tokens": [50798, 400, 1160, 595, 515, 390, 1228, 4825, 12, 18759, 21589, 50927], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 880, "seek": 235870, "start": 2369.96, "end": 2375.02, "text": " to go from data like this up to representation like this,", "tokens": [50927, 281, 352, 490, 1412, 411, 341, 493, 281, 10290, 411, 341, 11, 51180], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 881, "seek": 235870, "start": 2375.02, "end": 2377.18, "text": " which you might recognize as a color wheel.", "tokens": [51180, 597, 291, 1062, 5521, 382, 257, 2017, 5589, 13, 51288], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 882, "seek": 235870, "start": 2377.18, "end": 2380.66, "text": " Basically you can arrange points", "tokens": [51288, 8537, 291, 393, 9424, 2793, 51462], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 883, "seek": 235870, "start": 2380.66, "end": 2382.3799999999997, "text": " so that their distances correspond", "tokens": [51462, 370, 300, 641, 22182, 6805, 51548], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 884, "seek": 235870, "start": 2382.3799999999997, "end": 2385.7, "text": " to the distances in the confusion matrix", "tokens": [51548, 281, 264, 22182, 294, 264, 15075, 8141, 51714], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 885, "seek": 235870, "start": 2385.7, "end": 2388.22, "text": " and therefore recover something", "tokens": [51714, 293, 4412, 8114, 746, 51840], "temperature": 0.0, "avg_logprob": -0.15006857936822096, "compression_ratio": 1.66147859922179, "no_speech_prob": 4.683406223193742e-05}, {"id": 886, "seek": 238822, "start": 2388.22, "end": 2389.5, "text": " about the kind of underlying,", "tokens": [50364, 466, 264, 733, 295, 14217, 11, 50428], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 887, "seek": 238822, "start": 2389.5, "end": 2391.54, "text": " in this case, psychological structure", "tokens": [50428, 294, 341, 1389, 11, 14346, 3877, 50530], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 888, "seek": 238822, "start": 2391.54, "end": 2393.18, "text": " that generated that data.", "tokens": [50530, 300, 10833, 300, 1412, 13, 50612], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 889, "seek": 238822, "start": 2395.8599999999997, "end": 2399.4599999999996, "text": " People have also done similar kinds of things", "tokens": [50746, 3432, 362, 611, 1096, 2531, 3685, 295, 721, 50926], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 890, "seek": 238822, "start": 2399.4599999999996, "end": 2402.98, "text": " in learning kind of real formalized versions", "tokens": [50926, 294, 2539, 733, 295, 957, 9860, 1602, 9606, 51102], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 891, "seek": 238822, "start": 2402.98, "end": 2406.62, "text": " of theories or of intuitive theories.", "tokens": [51102, 295, 13667, 420, 295, 21769, 13667, 13, 51284], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 892, "seek": 238822, "start": 2406.62, "end": 2409.2999999999997, "text": " I really like this paper by Tomer Ullmann", "tokens": [51284, 286, 534, 411, 341, 3035, 538, 5041, 260, 624, 285, 14912, 51418], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 893, "seek": 238822, "start": 2409.2999999999997, "end": 2411.9399999999996, "text": " and Noah Goodman and Josh who's speaking next", "tokens": [51418, 293, 20895, 2205, 1601, 293, 9785, 567, 311, 4124, 958, 51550], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 894, "seek": 238822, "start": 2411.9399999999996, "end": 2414.1, "text": " on learning a theory of magnetism.", "tokens": [51550, 322, 2539, 257, 5261, 295, 15211, 1434, 13, 51658], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 895, "seek": 238822, "start": 2414.1, "end": 2415.7799999999997, "text": " So basically you take observations", "tokens": [51658, 407, 1936, 291, 747, 18163, 51742], "temperature": 0.0, "avg_logprob": -0.19224204619725546, "compression_ratio": 1.5833333333333333, "no_speech_prob": 6.813845538999885e-05}, {"id": 896, "seek": 241578, "start": 2415.78, "end": 2420.78, "text": " of which objects interact with other objects", "tokens": [50364, 295, 597, 6565, 4648, 365, 661, 6565, 50614], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 897, "seek": 241578, "start": 2420.86, "end": 2425.86, "text": " and do some learning to acquire a kind of high level theory", "tokens": [50618, 293, 360, 512, 2539, 281, 20001, 257, 733, 295, 1090, 1496, 5261, 50868], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 898, "seek": 241578, "start": 2426.34, "end": 2430.1200000000003, "text": " of the fact that there are two different kinds", "tokens": [50892, 295, 264, 1186, 300, 456, 366, 732, 819, 3685, 51081], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 899, "seek": 241578, "start": 2430.1200000000003, "end": 2433.36, "text": " of magnetic objects and those two different kinds", "tokens": [51081, 295, 12688, 6565, 293, 729, 732, 819, 3685, 51243], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 900, "seek": 241578, "start": 2433.36, "end": 2434.6000000000004, "text": " of things will interact with each other,", "tokens": [51243, 295, 721, 486, 4648, 365, 1184, 661, 11, 51305], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 901, "seek": 241578, "start": 2434.6000000000004, "end": 2435.7400000000002, "text": " but they won't interact with things", "tokens": [51305, 457, 436, 1582, 380, 4648, 365, 721, 51362], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 902, "seek": 241578, "start": 2435.7400000000002, "end": 2437.38, "text": " that are non-magnetic.", "tokens": [51362, 300, 366, 2107, 12, 76, 4535, 3532, 13, 51444], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 903, "seek": 241578, "start": 2437.38, "end": 2440.02, "text": " So this is like a little tiny mini intuitive theory", "tokens": [51444, 407, 341, 307, 411, 257, 707, 5870, 8382, 21769, 5261, 51576], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 904, "seek": 241578, "start": 2440.02, "end": 2443.0600000000004, "text": " that you can acquire just from very simple,", "tokens": [51576, 300, 291, 393, 20001, 445, 490, 588, 2199, 11, 51728], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 905, "seek": 241578, "start": 2443.0600000000004, "end": 2444.5800000000004, "text": " you might think kind of impoverished data", "tokens": [51728, 291, 1062, 519, 733, 295, 704, 3570, 4729, 1412, 51804], "temperature": 0.0, "avg_logprob": -0.15665740266852424, "compression_ratio": 1.8841201716738198, "no_speech_prob": 0.00018520490266382694}, {"id": 906, "seek": 244458, "start": 2444.58, "end": 2446.24, "text": " about interactions.", "tokens": [50364, 466, 13280, 13, 50447], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 907, "seek": 244458, "start": 2447.74, "end": 2451.64, "text": " So when people talk about LOMs just being based on text,", "tokens": [50522, 407, 562, 561, 751, 466, 441, 5251, 82, 445, 885, 2361, 322, 2487, 11, 50717], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 908, "seek": 244458, "start": 2451.64, "end": 2452.98, "text": " I think that isn't really enough", "tokens": [50717, 286, 519, 300, 1943, 380, 534, 1547, 50784], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 909, "seek": 244458, "start": 2452.98, "end": 2455.62, "text": " to conclude anything about what theories they might induce,", "tokens": [50784, 281, 16886, 1340, 466, 437, 13667, 436, 1062, 41263, 11, 50916], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 910, "seek": 244458, "start": 2455.62, "end": 2457.14, "text": " or what kinds of internal structures", "tokens": [50916, 420, 437, 3685, 295, 6920, 9227, 50992], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 911, "seek": 244458, "start": 2457.14, "end": 2461.02, "text": " and conceptual roles they might induce from that text.", "tokens": [50992, 293, 24106, 9604, 436, 1062, 41263, 490, 300, 2487, 13, 51186], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 912, "seek": 244458, "start": 2461.02, "end": 2462.46, "text": " And in fact, there's some evidence I think", "tokens": [51186, 400, 294, 1186, 11, 456, 311, 512, 4467, 286, 519, 51258], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 913, "seek": 244458, "start": 2462.46, "end": 2467.46, "text": " that what they are inducing looks pretty plausible", "tokens": [51258, 300, 437, 436, 366, 13716, 2175, 1542, 1238, 39925, 51508], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 914, "seek": 244458, "start": 2467.7799999999997, "end": 2469.86, "text": " at least in kind of simple domains.", "tokens": [51524, 412, 1935, 294, 733, 295, 2199, 25514, 13, 51628], "temperature": 0.0, "avg_logprob": -0.19605752610668695, "compression_ratio": 1.6359832635983265, "no_speech_prob": 7.253372314153239e-05}, {"id": 915, "seek": 246986, "start": 2469.86, "end": 2474.7000000000003, "text": " So there's this paper by Grandin and colleagues", "tokens": [50364, 407, 456, 311, 341, 3035, 538, 6757, 259, 293, 7734, 50606], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 916, "seek": 246986, "start": 2474.7000000000003, "end": 2478.02, "text": " which essentially looked at word embedding vectors", "tokens": [50606, 597, 4476, 2956, 412, 1349, 12240, 3584, 18875, 50772], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 917, "seek": 246986, "start": 2478.02, "end": 2481.98, "text": " and projected them onto say intuitive dimensions.", "tokens": [50772, 293, 26231, 552, 3911, 584, 21769, 12819, 13, 50970], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 918, "seek": 246986, "start": 2481.98, "end": 2484.58, "text": " So here you have a bunch of words,", "tokens": [50970, 407, 510, 291, 362, 257, 3840, 295, 2283, 11, 51100], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 919, "seek": 246986, "start": 2484.58, "end": 2486.54, "text": " you project them onto this line,", "tokens": [51100, 291, 1716, 552, 3911, 341, 1622, 11, 51198], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 920, "seek": 246986, "start": 2486.54, "end": 2489.02, "text": " which is the line connecting small and large.", "tokens": [51198, 597, 307, 264, 1622, 11015, 1359, 293, 2416, 13, 51322], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 921, "seek": 246986, "start": 2489.02, "end": 2491.9, "text": " Okay, so all of our high dimensional word vectors", "tokens": [51322, 1033, 11, 370, 439, 295, 527, 1090, 18795, 1349, 18875, 51466], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 922, "seek": 246986, "start": 2491.9, "end": 2494.78, "text": " get projected onto the small versus large line.", "tokens": [51466, 483, 26231, 3911, 264, 1359, 5717, 2416, 1622, 13, 51610], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 923, "seek": 246986, "start": 2494.78, "end": 2496.78, "text": " And we take that as a way of measuring", "tokens": [51610, 400, 321, 747, 300, 382, 257, 636, 295, 13389, 51710], "temperature": 0.0, "avg_logprob": -0.15567130271834556, "compression_ratio": 1.6835443037974684, "no_speech_prob": 4.331419404479675e-05}, {"id": 924, "seek": 249678, "start": 2496.78, "end": 2499.6600000000003, "text": " how large versus small different objects are.", "tokens": [50364, 577, 2416, 5717, 1359, 819, 6565, 366, 13, 50508], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 925, "seek": 249678, "start": 2500.5800000000004, "end": 2502.02, "text": " And then the question is,", "tokens": [50554, 400, 550, 264, 1168, 307, 11, 50626], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 926, "seek": 249678, "start": 2502.02, "end": 2506.02, "text": " is in a model trained only on text prediction,", "tokens": [50626, 307, 294, 257, 2316, 8895, 787, 322, 2487, 17630, 11, 50826], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 927, "seek": 249678, "start": 2506.02, "end": 2511.02, "text": " is that, does that projection recover anything human like", "tokens": [50826, 307, 300, 11, 775, 300, 22743, 8114, 1340, 1952, 411, 51076], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 928, "seek": 249678, "start": 2511.1200000000003, "end": 2514.1800000000003, "text": " about the underlying conceptual spaces?", "tokens": [51081, 466, 264, 14217, 24106, 7673, 30, 51234], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 929, "seek": 249678, "start": 2514.1800000000003, "end": 2515.3, "text": " And they show yes it does.", "tokens": [51234, 400, 436, 855, 2086, 309, 775, 13, 51290], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 930, "seek": 249678, "start": 2515.3, "end": 2518.78, "text": " So here's six plots where the x-axis", "tokens": [51290, 407, 510, 311, 2309, 28609, 689, 264, 2031, 12, 24633, 51464], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 931, "seek": 249678, "start": 2518.78, "end": 2521.1800000000003, "text": " is the semantic projection, right?", "tokens": [51464, 307, 264, 47982, 22743, 11, 558, 30, 51584], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 932, "seek": 249678, "start": 2521.1800000000003, "end": 2523.86, "text": " So how far on that small to large line something is.", "tokens": [51584, 407, 577, 1400, 322, 300, 1359, 281, 2416, 1622, 746, 307, 13, 51718], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 933, "seek": 249678, "start": 2523.86, "end": 2525.8, "text": " And then the y-axis is human ratings", "tokens": [51718, 400, 550, 264, 288, 12, 24633, 307, 1952, 24603, 51815], "temperature": 0.0, "avg_logprob": -0.2104254392819984, "compression_ratio": 1.6735537190082646, "no_speech_prob": 0.00015354919014498591}, {"id": 934, "seek": 252580, "start": 2525.8, "end": 2529.28, "text": " of how small versus large an object is, right?", "tokens": [50364, 295, 577, 1359, 5717, 2416, 364, 2657, 307, 11, 558, 30, 50538], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 935, "seek": 252580, "start": 2529.28, "end": 2531.0800000000004, "text": " You can see that the correlations here are not perfect", "tokens": [50538, 509, 393, 536, 300, 264, 13983, 763, 510, 366, 406, 2176, 50628], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 936, "seek": 252580, "start": 2531.0800000000004, "end": 2533.0800000000004, "text": " but they're also not garbage, right?", "tokens": [50628, 457, 436, 434, 611, 406, 14150, 11, 558, 30, 50728], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 937, "seek": 252580, "start": 2533.0800000000004, "end": 2537.96, "text": " They're actually quite strong I think for a model like this", "tokens": [50728, 814, 434, 767, 1596, 2068, 286, 519, 337, 257, 2316, 411, 341, 50972], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 938, "seek": 252580, "start": 2537.96, "end": 2541.02, "text": " that things which the model calls wet versus dry", "tokens": [50972, 300, 721, 597, 264, 2316, 5498, 6630, 5717, 4016, 51125], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 939, "seek": 252580, "start": 2541.02, "end": 2544.4, "text": " or big versus small or dangerous versus safe,", "tokens": [51125, 420, 955, 5717, 1359, 420, 5795, 5717, 3273, 11, 51294], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 940, "seek": 252580, "start": 2545.44, "end": 2547.9, "text": " people also agree with, right?", "tokens": [51346, 561, 611, 3986, 365, 11, 558, 30, 51469], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 941, "seek": 252580, "start": 2547.9, "end": 2549.7400000000002, "text": " So just in predicting text,", "tokens": [51469, 407, 445, 294, 32884, 2487, 11, 51561], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 942, "seek": 252580, "start": 2549.7400000000002, "end": 2552.0800000000004, "text": " this thing has recovered these kinds of aspects", "tokens": [51561, 341, 551, 575, 19542, 613, 3685, 295, 7270, 51678], "temperature": 0.0, "avg_logprob": -0.13691427193435968, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.00017397500050719827}, {"id": 943, "seek": 255208, "start": 2552.08, "end": 2554.72, "text": " of semantic structure latent", "tokens": [50364, 295, 47982, 3877, 48994, 50496], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 944, "seek": 255208, "start": 2554.72, "end": 2557.68, "text": " in the word vector representations, yeah.", "tokens": [50496, 294, 264, 1349, 8062, 33358, 11, 1338, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 945, "seek": 255208, "start": 2557.68, "end": 2560.36, "text": " But this is probably sitting there in n-grams", "tokens": [50644, 583, 341, 307, 1391, 3798, 456, 294, 297, 12, 1342, 82, 50778], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 946, "seek": 255208, "start": 2560.36, "end": 2562.7599999999998, "text": " and in bi-grams in fact, that information, right?", "tokens": [50778, 293, 294, 3228, 12, 1342, 82, 294, 1186, 11, 300, 1589, 11, 558, 30, 50898], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 947, "seek": 255208, "start": 2562.7599999999998, "end": 2565.36, "text": " It doesn't have to do anything with the real world.", "tokens": [50898, 467, 1177, 380, 362, 281, 360, 1340, 365, 264, 957, 1002, 13, 51028], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 948, "seek": 255208, "start": 2566.36, "end": 2568.44, "text": " Well, it does have something to do with the real world.", "tokens": [51078, 1042, 11, 309, 775, 362, 746, 281, 360, 365, 264, 957, 1002, 13, 51182], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 949, "seek": 255208, "start": 2568.44, "end": 2572.7599999999998, "text": " It's like a small and large", "tokens": [51182, 467, 311, 411, 257, 1359, 293, 2416, 51398], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 950, "seek": 255208, "start": 2572.7599999999998, "end": 2575.08, "text": " could just be linguistic constructs", "tokens": [51398, 727, 445, 312, 43002, 7690, 82, 51514], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 951, "seek": 255208, "start": 2575.08, "end": 2577.3199999999997, "text": " and you're testing it on language.", "tokens": [51514, 293, 291, 434, 4997, 309, 322, 2856, 13, 51626], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 952, "seek": 255208, "start": 2577.3199999999997, "end": 2580.2, "text": " Oh, I see, you think it's that you say small tiger", "tokens": [51626, 876, 11, 286, 536, 11, 291, 519, 309, 311, 300, 291, 584, 1359, 21432, 51770], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 953, "seek": 255208, "start": 2580.2, "end": 2581.88, "text": " versus large tiger or something.", "tokens": [51770, 5717, 2416, 21432, 420, 746, 13, 51854], "temperature": 0.0, "avg_logprob": -0.2648782204455278, "compression_ratio": 1.6863468634686347, "no_speech_prob": 0.001366336364299059}, {"id": 954, "seek": 258188, "start": 2582.32, "end": 2584.1600000000003, "text": " Small puppy, right?", "tokens": [50386, 15287, 18196, 11, 558, 30, 50478], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 955, "seek": 258188, "start": 2584.1600000000003, "end": 2586.6800000000003, "text": " Puppy is always small, tiger is always large, yeah.", "tokens": [50478, 13605, 7966, 307, 1009, 1359, 11, 21432, 307, 1009, 2416, 11, 1338, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 956, "seek": 258188, "start": 2586.6800000000003, "end": 2589.96, "text": " So it might be true in n-grams, I'm not sure.", "tokens": [50604, 407, 309, 1062, 312, 2074, 294, 297, 12, 1342, 82, 11, 286, 478, 406, 988, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 957, "seek": 258188, "start": 2589.96, "end": 2592.1600000000003, "text": " I don't think that they looked at that.", "tokens": [50768, 286, 500, 380, 519, 300, 436, 2956, 412, 300, 13, 50878], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 958, "seek": 258188, "start": 2593.8, "end": 2598.6400000000003, "text": " I don't think that defeats the argument though, right?", "tokens": [50960, 286, 500, 380, 519, 300, 7486, 1720, 264, 6770, 1673, 11, 558, 30, 51202], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 959, "seek": 258188, "start": 2598.6400000000003, "end": 2600.96, "text": " Because I think it is the case that", "tokens": [51202, 1436, 286, 519, 309, 307, 264, 1389, 300, 51318], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 960, "seek": 258188, "start": 2602.08, "end": 2604.2000000000003, "text": " even n-gram statistics are statistics", "tokens": [51374, 754, 297, 12, 1342, 12523, 366, 12523, 51480], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 961, "seek": 258188, "start": 2604.2000000000003, "end": 2606.4, "text": " about word relations, right?", "tokens": [51480, 466, 1349, 2299, 11, 558, 30, 51590], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 962, "seek": 258188, "start": 2606.4, "end": 2608.2400000000002, "text": " So it might be that you don't need fancy language models", "tokens": [51590, 407, 309, 1062, 312, 300, 291, 500, 380, 643, 10247, 2856, 5245, 51682], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 963, "seek": 258188, "start": 2608.2400000000002, "end": 2609.7200000000003, "text": " or something to do this.", "tokens": [51682, 420, 746, 281, 360, 341, 13, 51756], "temperature": 0.0, "avg_logprob": -0.2273021148423017, "compression_ratio": 1.7566371681415929, "no_speech_prob": 0.00023046007845550776}, {"id": 964, "seek": 260972, "start": 2610.2, "end": 2611.3599999999997, "text": " But you're not actually like,", "tokens": [50388, 583, 291, 434, 406, 767, 411, 11, 50446], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 965, "seek": 260972, "start": 2611.3599999999997, "end": 2613.6, "text": " you don't need real world for this to work.", "tokens": [50446, 291, 500, 380, 643, 957, 1002, 337, 341, 281, 589, 13, 50558], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 966, "seek": 260972, "start": 2615.12, "end": 2617.9199999999996, "text": " Well, the real world generated", "tokens": [50634, 1042, 11, 264, 957, 1002, 10833, 50774], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 967, "seek": 260972, "start": 2617.9199999999996, "end": 2620.8399999999997, "text": " how often you hear small puppy versus large puppy, right?", "tokens": [50774, 577, 2049, 291, 1568, 1359, 18196, 5717, 2416, 18196, 11, 558, 30, 50920], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 968, "seek": 260972, "start": 2620.8399999999997, "end": 2624.9199999999996, "text": " So the real world is mirrored in those statistics", "tokens": [50920, 407, 264, 957, 1002, 307, 3149, 340, 986, 294, 729, 12523, 51124], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 969, "seek": 260972, "start": 2624.9199999999996, "end": 2627.3999999999996, "text": " and then the configuration that system comes up with", "tokens": [51124, 293, 550, 264, 11694, 300, 1185, 1487, 493, 365, 51248], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 970, "seek": 260972, "start": 2627.3999999999996, "end": 2630.52, "text": " is also one that mirrors those properties of the real world.", "tokens": [51248, 307, 611, 472, 300, 24238, 729, 7221, 295, 264, 957, 1002, 13, 51404], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 971, "seek": 260972, "start": 2630.52, "end": 2631.3599999999997, "text": " Yeah.", "tokens": [51404, 865, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 972, "seek": 260972, "start": 2632.64, "end": 2633.9599999999996, "text": " Can I put back on that as well?", "tokens": [51510, 1664, 286, 829, 646, 322, 300, 382, 731, 30, 51576], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 973, "seek": 260972, "start": 2633.9599999999996, "end": 2636.3199999999997, "text": " I do kind of feel like what others are saying is right,", "tokens": [51576, 286, 360, 733, 295, 841, 411, 437, 2357, 366, 1566, 307, 558, 11, 51694], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 974, "seek": 260972, "start": 2636.3199999999997, "end": 2638.72, "text": " but this is much closer to n-grams", "tokens": [51694, 457, 341, 307, 709, 4966, 281, 297, 12, 1342, 82, 51814], "temperature": 0.0, "avg_logprob": -0.2495407225593688, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00049539131578058}, {"id": 975, "seek": 263872, "start": 2638.72, "end": 2640.68, "text": " than it is to large language models.", "tokens": [50364, 813, 309, 307, 281, 2416, 2856, 5245, 13, 50462], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 976, "seek": 263872, "start": 2640.68, "end": 2643.4399999999996, "text": " And the properties we're seeing are just so much wilder", "tokens": [50462, 400, 264, 7221, 321, 434, 2577, 366, 445, 370, 709, 4868, 260, 50600], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 977, "seek": 263872, "start": 2643.4399999999996, "end": 2647.24, "text": " than any of these embedding tricks in practice.", "tokens": [50600, 813, 604, 295, 613, 12240, 3584, 11733, 294, 3124, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 978, "seek": 263872, "start": 2647.24, "end": 2648.52, "text": " Oh, you mean that large language model", "tokens": [50790, 876, 11, 291, 914, 300, 2416, 2856, 2316, 50854], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 979, "seek": 263872, "start": 2648.52, "end": 2650.16, "text": " is much smarter than this?", "tokens": [50854, 307, 709, 20294, 813, 341, 30, 50936], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 980, "seek": 263872, "start": 2650.16, "end": 2652.8799999999997, "text": " I don't even see how this is comparable in a way, right?", "tokens": [50936, 286, 500, 380, 754, 536, 577, 341, 307, 25323, 294, 257, 636, 11, 558, 30, 51072], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 981, "seek": 263872, "start": 2652.8799999999997, "end": 2655.3199999999997, "text": " Like this pops out of PCA,", "tokens": [51072, 1743, 341, 16795, 484, 295, 6465, 32, 11, 51194], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 982, "seek": 263872, "start": 2655.3199999999997, "end": 2658.04, "text": " whereas we're seeing these wild emergent behaviors", "tokens": [51194, 9735, 321, 434, 2577, 613, 4868, 4345, 6930, 15501, 51330], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 983, "seek": 263872, "start": 2658.04, "end": 2658.9199999999996, "text": " come out of large language models.", "tokens": [51330, 808, 484, 295, 2416, 2856, 5245, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 984, "seek": 263872, "start": 2658.9199999999996, "end": 2660.3999999999996, "text": " Yeah, yeah, so I mean,", "tokens": [51374, 865, 11, 1338, 11, 370, 286, 914, 11, 51448], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 985, "seek": 263872, "start": 2660.3999999999996, "end": 2662.68, "text": " I don't think this explains wild emergent behaviors.", "tokens": [51448, 286, 500, 380, 519, 341, 13948, 4868, 4345, 6930, 15501, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 986, "seek": 263872, "start": 2662.68, "end": 2665.12, "text": " I think that this was just trying to say", "tokens": [51562, 286, 519, 300, 341, 390, 445, 1382, 281, 584, 51684], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 987, "seek": 263872, "start": 2665.12, "end": 2667.8399999999997, "text": " that when you train on text prediction,", "tokens": [51684, 300, 562, 291, 3847, 322, 2487, 17630, 11, 51820], "temperature": 0.0, "avg_logprob": -0.1934426755321269, "compression_ratio": 1.8442906574394464, "no_speech_prob": 0.000356931472197175}, {"id": 988, "seek": 266784, "start": 2667.84, "end": 2669.48, "text": " you configure yourself to align", "tokens": [50364, 291, 22162, 1803, 281, 7975, 50446], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 989, "seek": 266784, "start": 2669.48, "end": 2671.84, "text": " with some of the true properties of the world,", "tokens": [50446, 365, 512, 295, 264, 2074, 7221, 295, 264, 1002, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 990, "seek": 266784, "start": 2671.84, "end": 2673.96, "text": " which are reflected in the text analysis.", "tokens": [50564, 597, 366, 15502, 294, 264, 2487, 5215, 13, 50670], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 991, "seek": 266784, "start": 2673.96, "end": 2674.96, "text": " That's all, yeah.", "tokens": [50670, 663, 311, 439, 11, 1338, 13, 50720], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 992, "seek": 266784, "start": 2677.32, "end": 2678.32, "text": " Okay, so I'm short on time.", "tokens": [50838, 1033, 11, 370, 286, 478, 2099, 322, 565, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 993, "seek": 266784, "start": 2678.32, "end": 2679.1600000000003, "text": " I'll skip this.", "tokens": [50888, 286, 603, 10023, 341, 13, 50930], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 994, "seek": 266784, "start": 2679.1600000000003, "end": 2680.84, "text": " I'll just say that there's other papers", "tokens": [50930, 286, 603, 445, 584, 300, 456, 311, 661, 10577, 51014], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 995, "seek": 266784, "start": 2680.84, "end": 2685.6000000000004, "text": " looking at transformers and kind of how they relate", "tokens": [51014, 1237, 412, 4088, 433, 293, 733, 295, 577, 436, 10961, 51252], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 996, "seek": 266784, "start": 2685.6000000000004, "end": 2688.8, "text": " to classic studies on concepts in cognitive science,", "tokens": [51252, 281, 7230, 5313, 322, 10392, 294, 15605, 3497, 11, 51412], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 997, "seek": 266784, "start": 2688.8, "end": 2691.36, "text": " classic kinds of effects, but I'll skip that.", "tokens": [51412, 7230, 3685, 295, 5065, 11, 457, 286, 603, 10023, 300, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 998, "seek": 266784, "start": 2692.88, "end": 2694.08, "text": " Maybe I'll go very briefly", "tokens": [51616, 2704, 286, 603, 352, 588, 10515, 51676], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 999, "seek": 266784, "start": 2694.08, "end": 2696.2000000000003, "text": " just through this kind of fun experiment.", "tokens": [51676, 445, 807, 341, 733, 295, 1019, 5120, 13, 51782], "temperature": 0.0, "avg_logprob": -0.1640563558359615, "compression_ratio": 1.6492537313432836, "no_speech_prob": 0.00020985322771593928}, {"id": 1000, "seek": 269620, "start": 2696.2, "end": 2699.08, "text": " This is Mark Gorenstein in my lab", "tokens": [50364, 639, 307, 3934, 460, 10948, 9089, 294, 452, 2715, 50508], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1001, "seek": 269620, "start": 2699.08, "end": 2702.72, "text": " has been interested in learning concepts", "tokens": [50508, 575, 668, 3102, 294, 2539, 10392, 50690], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1002, "seek": 269620, "start": 2702.72, "end": 2705.8799999999997, "text": " just from linguistic experience,", "tokens": [50690, 445, 490, 43002, 1752, 11, 50848], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1003, "seek": 269620, "start": 2705.8799999999997, "end": 2707.7599999999998, "text": " maybe linguistic prediction.", "tokens": [50848, 1310, 43002, 17630, 13, 50942], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1004, "seek": 269620, "start": 2707.7599999999998, "end": 2710.3599999999997, "text": " He's been doing these kind of cool experiments", "tokens": [50942, 634, 311, 668, 884, 613, 733, 295, 1627, 12050, 51072], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1005, "seek": 269620, "start": 2710.3599999999997, "end": 2714.3999999999996, "text": " where we give people passages of natural language", "tokens": [51072, 689, 321, 976, 561, 31589, 295, 3303, 2856, 51274], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1006, "seek": 269620, "start": 2714.3999999999996, "end": 2716.3999999999996, "text": " where there's some blanks.", "tokens": [51274, 689, 456, 311, 512, 8247, 82, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1007, "seek": 269620, "start": 2716.3999999999996, "end": 2718.12, "text": " So here's a passage.", "tokens": [51374, 407, 510, 311, 257, 11497, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1008, "seek": 269620, "start": 2718.12, "end": 2719.64, "text": " The myth of blank is so powerful", "tokens": [51460, 440, 9474, 295, 8247, 307, 370, 4005, 51536], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1009, "seek": 269620, "start": 2719.64, "end": 2721.16, "text": " that the very words conjure up blank,", "tokens": [51536, 300, 264, 588, 2283, 20295, 540, 493, 8247, 11, 51612], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1010, "seek": 269620, "start": 2721.16, "end": 2725.04, "text": " of strudel and blank in a cozy Vietnese cafe, blah, blah, blah.", "tokens": [51612, 295, 1056, 532, 338, 293, 8247, 294, 257, 29414, 691, 1684, 77, 1130, 17773, 11, 12288, 11, 12288, 11, 12288, 13, 51806], "temperature": 0.0, "avg_logprob": -0.1768856546153193, "compression_ratio": 1.6507936507936507, "no_speech_prob": 1.4283652490121312e-05}, {"id": 1011, "seek": 272504, "start": 2725.04, "end": 2727.72, "text": " And the job of participants in this", "tokens": [50364, 400, 264, 1691, 295, 10503, 294, 341, 50498], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1012, "seek": 272504, "start": 2727.72, "end": 2731.0, "text": " is to learn where to put the word DAX.", "tokens": [50498, 307, 281, 1466, 689, 281, 829, 264, 1349, 9578, 55, 13, 50662], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1013, "seek": 272504, "start": 2731.0, "end": 2732.24, "text": " Okay, so DAX is a novel word", "tokens": [50662, 1033, 11, 370, 9578, 55, 307, 257, 7613, 1349, 50724], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1014, "seek": 272504, "start": 2732.24, "end": 2734.16, "text": " they've never encountered before.", "tokens": [50724, 436, 600, 1128, 20381, 949, 13, 50820], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1015, "seek": 272504, "start": 2735.36, "end": 2738.44, "text": " You have to read this and understand the context and stuff", "tokens": [50880, 509, 362, 281, 1401, 341, 293, 1223, 264, 4319, 293, 1507, 51034], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1016, "seek": 272504, "start": 2738.44, "end": 2742.7599999999998, "text": " in order to figure that out and see where DAX should go.", "tokens": [51034, 294, 1668, 281, 2573, 300, 484, 293, 536, 689, 9578, 55, 820, 352, 13, 51250], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1017, "seek": 272504, "start": 2742.7599999999998, "end": 2747.44, "text": " Secretly behind the scenes, this example has been chosen", "tokens": [51250, 7400, 356, 2261, 264, 8026, 11, 341, 1365, 575, 668, 8614, 51484], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1018, "seek": 272504, "start": 2747.44, "end": 2751.6, "text": " as just from a big corpus of text of a really rare word", "tokens": [51484, 382, 445, 490, 257, 955, 1181, 31624, 295, 2487, 295, 257, 534, 5892, 1349, 51692], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1019, "seek": 272504, "start": 2751.6, "end": 2753.4, "text": " that people probably don't know.", "tokens": [51692, 300, 561, 1391, 500, 380, 458, 13, 51782], "temperature": 0.0, "avg_logprob": -0.12362098693847656, "compression_ratio": 1.6024096385542168, "no_speech_prob": 7.48319725971669e-05}, {"id": 1020, "seek": 275340, "start": 2753.8, "end": 2756.6, "text": " So the rare word here is soccer tort.", "tokens": [50384, 407, 264, 5892, 1349, 510, 307, 15469, 10806, 13, 50524], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1021, "seek": 275340, "start": 2757.52, "end": 2760.2400000000002, "text": " And that means that this language,", "tokens": [50570, 400, 300, 1355, 300, 341, 2856, 11, 50706], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1022, "seek": 275340, "start": 2760.2400000000002, "end": 2762.84, "text": " like where soccer tort actually occurred here,", "tokens": [50706, 411, 689, 15469, 10806, 767, 11068, 510, 11, 50836], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1023, "seek": 275340, "start": 2763.84, "end": 2765.96, "text": " was generated from real people", "tokens": [50886, 390, 10833, 490, 957, 561, 50992], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1024, "seek": 275340, "start": 2765.96, "end": 2767.96, "text": " and presumably reflects the underlying meaning", "tokens": [50992, 293, 26742, 18926, 264, 14217, 3620, 51092], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1025, "seek": 275340, "start": 2767.96, "end": 2769.6800000000003, "text": " and things of soccer tort.", "tokens": [51092, 293, 721, 295, 15469, 10806, 13, 51178], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1026, "seek": 275340, "start": 2770.6800000000003, "end": 2773.2000000000003, "text": " Maybe I don't know if I'm saying that correctly.", "tokens": [51228, 2704, 286, 500, 380, 458, 498, 286, 478, 1566, 300, 8944, 13, 51354], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1027, "seek": 275340, "start": 2773.2000000000003, "end": 2775.56, "text": " But with enough examples of these people", "tokens": [51354, 583, 365, 1547, 5110, 295, 613, 561, 51472], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1028, "seek": 275340, "start": 2775.56, "end": 2776.88, "text": " will learn where DAX is,", "tokens": [51472, 486, 1466, 689, 9578, 55, 307, 11, 51538], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1029, "seek": 275340, "start": 2776.88, "end": 2779.08, "text": " they get feedback on whether or not they were correct,", "tokens": [51538, 436, 483, 5824, 322, 1968, 420, 406, 436, 645, 3006, 11, 51648], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1030, "seek": 275340, "start": 2779.08, "end": 2780.96, "text": " according to whether they chose the place", "tokens": [51648, 4650, 281, 1968, 436, 5111, 264, 1081, 51742], "temperature": 0.0, "avg_logprob": -0.18267530038816118, "compression_ratio": 1.689922480620155, "no_speech_prob": 0.00016862648772075772}, {"id": 1031, "seek": 278096, "start": 2781.0, "end": 2783.64, "text": " where soccer tort actually appeared.", "tokens": [50366, 689, 15469, 10806, 767, 8516, 13, 50498], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1032, "seek": 278096, "start": 2783.64, "end": 2787.08, "text": " Okay, so we're having them do kind of a version", "tokens": [50498, 1033, 11, 370, 321, 434, 1419, 552, 360, 733, 295, 257, 3037, 50670], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1033, "seek": 278096, "start": 2787.08, "end": 2788.88, "text": " of a prediction task,", "tokens": [50670, 295, 257, 17630, 5633, 11, 50760], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1034, "seek": 278096, "start": 2790.0, "end": 2793.56, "text": " trying to figure out where this word goes,", "tokens": [50816, 1382, 281, 2573, 484, 689, 341, 1349, 1709, 11, 50994], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1035, "seek": 278096, "start": 2793.56, "end": 2794.92, "text": " but they don't actually see the word,", "tokens": [50994, 457, 436, 500, 380, 767, 536, 264, 1349, 11, 51062], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1036, "seek": 278096, "start": 2794.92, "end": 2796.76, "text": " they see it as DAX.", "tokens": [51062, 436, 536, 309, 382, 9578, 55, 13, 51154], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1037, "seek": 278096, "start": 2798.52, "end": 2800.44, "text": " People get pretty decent at this,", "tokens": [51242, 3432, 483, 1238, 8681, 412, 341, 11, 51338], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1038, "seek": 278096, "start": 2800.44, "end": 2804.76, "text": " so up to 80% or so, depending on the word.", "tokens": [51338, 370, 493, 281, 4688, 4, 420, 370, 11, 5413, 322, 264, 1349, 13, 51554], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1039, "seek": 278096, "start": 2806.08, "end": 2808.96, "text": " These are just, sorry, these are the examples of the words", "tokens": [51620, 1981, 366, 445, 11, 2597, 11, 613, 366, 264, 5110, 295, 264, 2283, 51764], "temperature": 0.0, "avg_logprob": -0.19749832153320312, "compression_ratio": 1.5662100456621004, "no_speech_prob": 0.00014198968710843474}, {"id": 1040, "seek": 280896, "start": 2809.0, "end": 2811.6, "text": " which generated the unseen context.", "tokens": [50366, 597, 10833, 264, 40608, 4319, 13, 50496], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1041, "seek": 280896, "start": 2812.76, "end": 2814.4, "text": " And after that, we asked them a bunch of questions.", "tokens": [50554, 400, 934, 300, 11, 321, 2351, 552, 257, 3840, 295, 1651, 13, 50636], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1042, "seek": 280896, "start": 2814.4, "end": 2816.4, "text": " So we asked them some reading comprehension,", "tokens": [50636, 407, 321, 2351, 552, 512, 3760, 44991, 11, 50736], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1043, "seek": 280896, "start": 2816.4, "end": 2818.84, "text": " we asked them feature questions about DAX,", "tokens": [50736, 321, 2351, 552, 4111, 1651, 466, 9578, 55, 11, 50858], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1044, "seek": 280896, "start": 2818.84, "end": 2820.32, "text": " is DAX a man-made object?", "tokens": [50858, 307, 9578, 55, 257, 587, 12, 10341, 2657, 30, 50932], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1045, "seek": 280896, "start": 2820.32, "end": 2822.2400000000002, "text": " Do biologists typically study DAX?", "tokens": [50932, 1144, 3228, 12256, 5850, 2979, 9578, 55, 30, 51028], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1046, "seek": 280896, "start": 2822.2400000000002, "end": 2824.2, "text": " Do people use DAX in painting?", "tokens": [51028, 1144, 561, 764, 9578, 55, 294, 5370, 30, 51126], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1047, "seek": 280896, "start": 2824.2, "end": 2826.96, "text": " Just a whole collection of basic feature,", "tokens": [51126, 1449, 257, 1379, 5765, 295, 3875, 4111, 11, 51264], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1048, "seek": 280896, "start": 2826.96, "end": 2828.8, "text": " kind of concept-y questions.", "tokens": [51264, 733, 295, 3410, 12, 88, 1651, 13, 51356], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1049, "seek": 280896, "start": 2828.8, "end": 2830.68, "text": " We give them an image recognition task,", "tokens": [51356, 492, 976, 552, 364, 3256, 11150, 5633, 11, 51450], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1050, "seek": 280896, "start": 2830.68, "end": 2833.16, "text": " right, picking out soccer tort,", "tokens": [51450, 558, 11, 8867, 484, 15469, 10806, 11, 51574], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1051, "seek": 280896, "start": 2833.16, "end": 2837.04, "text": " the real thing versus alternatives.", "tokens": [51574, 264, 957, 551, 5717, 20478, 13, 51768], "temperature": 0.0, "avg_logprob": -0.18603087253257877, "compression_ratio": 1.6336996336996337, "no_speech_prob": 0.00012337103544268757}, {"id": 1052, "seek": 283704, "start": 2837.2799999999997, "end": 2839.72, "text": " And we also asked them for explicit definitions, right?", "tokens": [50376, 400, 321, 611, 2351, 552, 337, 13691, 21988, 11, 558, 30, 50498], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1053, "seek": 283704, "start": 2839.72, "end": 2841.44, "text": " These contexts are not definitions,", "tokens": [50498, 1981, 30628, 366, 406, 21988, 11, 50584], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1054, "seek": 283704, "start": 2841.44, "end": 2843.64, "text": " they're not saying here's what a soccer tort is,", "tokens": [50584, 436, 434, 406, 1566, 510, 311, 437, 257, 15469, 10806, 307, 11, 50694], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1055, "seek": 283704, "start": 2843.64, "end": 2847.56, "text": " it's naturalistic usages of the object.", "tokens": [50694, 309, 311, 3303, 3142, 505, 1660, 295, 264, 2657, 13, 50890], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1056, "seek": 283704, "start": 2847.56, "end": 2850.08, "text": " And what we find actually is within,", "tokens": [50890, 400, 437, 321, 915, 767, 307, 1951, 11, 51016], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1057, "seek": 283704, "start": 2853.36, "end": 2856.6, "text": " within about 20 trials or so,", "tokens": [51180, 1951, 466, 945, 12450, 420, 370, 11, 51342], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1058, "seek": 283704, "start": 2856.6, "end": 2859.24, "text": " sorry, everything is after 20 trials,", "tokens": [51342, 2597, 11, 1203, 307, 934, 945, 12450, 11, 51474], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1059, "seek": 283704, "start": 2859.24, "end": 2860.7599999999998, "text": " people are actually very, very good", "tokens": [51474, 561, 366, 767, 588, 11, 588, 665, 51550], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1060, "seek": 283704, "start": 2860.7599999999998, "end": 2865.24, "text": " at judging conceptual features for these concepts,", "tokens": [51550, 412, 23587, 24106, 4122, 337, 613, 10392, 11, 51774], "temperature": 0.0, "avg_logprob": -0.18104899298284471, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.00030527656781487167}, {"id": 1061, "seek": 286524, "start": 2865.2799999999997, "end": 2868.04, "text": " almost at ceiling in most of the kinds", "tokens": [50366, 1920, 412, 13655, 294, 881, 295, 264, 3685, 50504], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1062, "seek": 286524, "start": 2868.04, "end": 2870.56, "text": " of feature questions we asked them.", "tokens": [50504, 295, 4111, 1651, 321, 2351, 552, 13, 50630], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1063, "seek": 286524, "start": 2870.56, "end": 2874.2, "text": " Here's each word on a row,", "tokens": [50630, 1692, 311, 1184, 1349, 322, 257, 5386, 11, 50812], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1064, "seek": 286524, "start": 2874.2, "end": 2877.64, "text": " and then features here on the x-axis,", "tokens": [50812, 293, 550, 4122, 510, 322, 264, 2031, 12, 24633, 11, 50984], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1065, "seek": 286524, "start": 2877.64, "end": 2880.08, "text": " almost everything is read, meaning they're good at this.", "tokens": [50984, 1920, 1203, 307, 1401, 11, 3620, 436, 434, 665, 412, 341, 13, 51106], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1066, "seek": 286524, "start": 2880.08, "end": 2885.08, "text": " They're also good at picking which picture is the object,", "tokens": [51106, 814, 434, 611, 665, 412, 8867, 597, 3036, 307, 264, 2657, 11, 51356], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1067, "seek": 286524, "start": 2885.08, "end": 2886.7599999999998, "text": " so they've never encountered any pictures at all,", "tokens": [51356, 370, 436, 600, 1128, 20381, 604, 5242, 412, 439, 11, 51440], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1068, "seek": 286524, "start": 2886.7599999999998, "end": 2890.9199999999996, "text": " but they're 80% or so good at picking these things out.", "tokens": [51440, 457, 436, 434, 4688, 4, 420, 370, 665, 412, 8867, 613, 721, 484, 13, 51648], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1069, "seek": 286524, "start": 2890.9199999999996, "end": 2892.7599999999998, "text": " And they're even good at giving definitions", "tokens": [51648, 400, 436, 434, 754, 665, 412, 2902, 21988, 51740], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1070, "seek": 286524, "start": 2892.7599999999998, "end": 2894.24, "text": " for these terms, okay?", "tokens": [51740, 337, 613, 2115, 11, 1392, 30, 51814], "temperature": 0.0, "avg_logprob": -0.14152603475456563, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.00036818988155573606}, {"id": 1071, "seek": 289424, "start": 2894.24, "end": 2896.68, "text": " So here's a dictionary or dictionary", "tokens": [50364, 407, 510, 311, 257, 25890, 420, 25890, 50486], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1072, "seek": 289424, "start": 2896.68, "end": 2898.4399999999996, "text": " or something definition of soccer tort,", "tokens": [50486, 420, 746, 7123, 295, 15469, 10806, 11, 50574], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1073, "seek": 289424, "start": 2898.4399999999996, "end": 2900.7999999999997, "text": " it's a chocolate cake or tort of Austrian origin", "tokens": [50574, 309, 311, 257, 6215, 5908, 420, 10806, 295, 41507, 4957, 50692], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1074, "seek": 289424, "start": 2900.7999999999997, "end": 2904.04, "text": " invented by Franz soccer supposedly in 1832", "tokens": [50692, 14479, 538, 33084, 15469, 20581, 294, 2443, 11440, 50854], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1075, "seek": 289424, "start": 2904.04, "end": 2906.6, "text": " for some prince in Vienna.", "tokens": [50854, 337, 512, 16467, 294, 31024, 13, 50982], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1076, "seek": 289424, "start": 2906.6, "end": 2908.12, "text": " And people just from these contexts,", "tokens": [50982, 400, 561, 445, 490, 613, 30628, 11, 51058], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1077, "seek": 289424, "start": 2908.12, "end": 2910.3199999999997, "text": " 20 of them will learn things like it's a chocolate dessert", "tokens": [51058, 945, 295, 552, 486, 1466, 721, 411, 309, 311, 257, 6215, 14593, 51168], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1078, "seek": 289424, "start": 2910.3199999999997, "end": 2911.7599999999998, "text": " similar to a cake that was originally", "tokens": [51168, 2531, 281, 257, 5908, 300, 390, 7993, 51240], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1079, "seek": 289424, "start": 2911.7599999999998, "end": 2913.6, "text": " and most commonly made in Vienna.", "tokens": [51240, 293, 881, 12719, 1027, 294, 31024, 13, 51332], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1080, "seek": 289424, "start": 2913.6, "end": 2914.7999999999997, "text": " I think it's a type of chocolate cake", "tokens": [51332, 286, 519, 309, 311, 257, 2010, 295, 6215, 5908, 51392], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1081, "seek": 289424, "start": 2914.7999999999997, "end": 2917.2799999999997, "text": " that can be ordered for dessert in Austria,", "tokens": [51392, 300, 393, 312, 8866, 337, 14593, 294, 26501, 11, 51516], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1082, "seek": 289424, "start": 2917.2799999999997, "end": 2919.7599999999998, "text": " kind of rich chocolate cake from Vienna and so on, okay?", "tokens": [51516, 733, 295, 4593, 6215, 5908, 490, 31024, 293, 370, 322, 11, 1392, 30, 51640], "temperature": 0.0, "avg_logprob": -0.1492205560207367, "compression_ratio": 1.790035587188612, "no_speech_prob": 7.842515333322808e-05}, {"id": 1083, "seek": 291976, "start": 2919.76, "end": 2924.76, "text": " So people are pretty good at taking", "tokens": [50364, 407, 561, 366, 1238, 665, 412, 1940, 50614], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1084, "seek": 291976, "start": 2926.5200000000004, "end": 2928.2000000000003, "text": " kind of in-context language use", "tokens": [50702, 733, 295, 294, 12, 9000, 3828, 2856, 764, 50786], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1085, "seek": 291976, "start": 2928.2000000000003, "end": 2930.32, "text": " and figuring out underlying aspects", "tokens": [50786, 293, 15213, 484, 14217, 7270, 50892], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1086, "seek": 291976, "start": 2930.32, "end": 2932.6800000000003, "text": " of conceptual representation from that.", "tokens": [50892, 295, 24106, 10290, 490, 300, 13, 51010], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1087, "seek": 291976, "start": 2936.1200000000003, "end": 2937.76, "text": " But they will never be able to taste it.", "tokens": [51182, 583, 436, 486, 1128, 312, 1075, 281, 3939, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1088, "seek": 291976, "start": 2937.76, "end": 2938.92, "text": " I mean, it tastes so good.", "tokens": [51264, 286, 914, 11, 309, 8666, 370, 665, 13, 51322], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1089, "seek": 291976, "start": 2938.92, "end": 2940.92, "text": " Have you had it?", "tokens": [51322, 3560, 291, 632, 309, 30, 51422], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1090, "seek": 291976, "start": 2940.92, "end": 2943.8, "text": " I've never heard of it, so okay.", "tokens": [51422, 286, 600, 1128, 2198, 295, 309, 11, 370, 1392, 13, 51566], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1091, "seek": 291976, "start": 2943.8, "end": 2945.0800000000004, "text": " It's great.", "tokens": [51566, 467, 311, 869, 13, 51630], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1092, "seek": 291976, "start": 2945.0800000000004, "end": 2946.8, "text": " Okay, so let me just wrap up here.", "tokens": [51630, 1033, 11, 370, 718, 385, 445, 7019, 493, 510, 13, 51716], "temperature": 0.0, "avg_logprob": -0.39523628636410363, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.00043041040771640837}, {"id": 1093, "seek": 294680, "start": 2946.8, "end": 2951.8, "text": " So I think of these kind of conceptual roles or theories", "tokens": [50364, 407, 286, 519, 295, 613, 733, 295, 24106, 9604, 420, 13667, 50614], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1094, "seek": 294680, "start": 2952.28, "end": 2954.0800000000004, "text": " as really both a strength and a weakness", "tokens": [50638, 382, 534, 1293, 257, 3800, 293, 257, 12772, 50728], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1095, "seek": 294680, "start": 2954.0800000000004, "end": 2957.2000000000003, "text": " of these current large language models.", "tokens": [50728, 295, 613, 2190, 2416, 2856, 5245, 13, 50884], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1096, "seek": 294680, "start": 2958.04, "end": 2959.52, "text": " One is that large language models,", "tokens": [50926, 1485, 307, 300, 2416, 2856, 5245, 11, 51000], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1097, "seek": 294680, "start": 2959.52, "end": 2961.2000000000003, "text": " I think, seem very good at learning", "tokens": [51000, 286, 519, 11, 1643, 588, 665, 412, 2539, 51084], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1098, "seek": 294680, "start": 2961.2000000000003, "end": 2963.6400000000003, "text": " kind of shallow but broad theories, right?", "tokens": [51084, 733, 295, 20488, 457, 4152, 13667, 11, 558, 30, 51206], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1099, "seek": 294680, "start": 2963.6400000000003, "end": 2966.1200000000003, "text": " So things like could shoes be made out of eggplants", "tokens": [51206, 407, 721, 411, 727, 6654, 312, 1027, 484, 295, 3777, 42433, 51330], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1100, "seek": 294680, "start": 2966.1200000000003, "end": 2968.76, "text": " or what would happen if shoes were made out of eggplants, right?", "tokens": [51330, 420, 437, 576, 1051, 498, 6654, 645, 1027, 484, 295, 3777, 42433, 11, 558, 30, 51462], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1101, "seek": 294680, "start": 2968.76, "end": 2973.76, "text": " They would know what some bad downsides", "tokens": [51462, 814, 576, 458, 437, 512, 1578, 21554, 1875, 51712], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1102, "seek": 294680, "start": 2973.76, "end": 2976.0, "text": " of that kind of thing might be, right?", "tokens": [51712, 295, 300, 733, 295, 551, 1062, 312, 11, 558, 30, 51824], "temperature": 0.0, "avg_logprob": -0.15401381459729424, "compression_ratio": 1.8547717842323652, "no_speech_prob": 4.985224950360134e-05}, {"id": 1103, "seek": 297600, "start": 2976.0, "end": 2980.08, "text": " Or answer basic kinds of questions", "tokens": [50364, 1610, 1867, 3875, 3685, 295, 1651, 50568], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1104, "seek": 297600, "start": 2980.08, "end": 2983.88, "text": " that might rely on kind of reasoning", "tokens": [50568, 300, 1062, 10687, 322, 733, 295, 21577, 50758], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1105, "seek": 297600, "start": 2983.88, "end": 2987.44, "text": " through one or two kind of links about the relationships", "tokens": [50758, 807, 472, 420, 732, 733, 295, 6123, 466, 264, 6159, 50936], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1106, "seek": 297600, "start": 2987.44, "end": 2990.42, "text": " between the objects involved in a situation like that.", "tokens": [50936, 1296, 264, 6565, 3288, 294, 257, 2590, 411, 300, 13, 51085], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1107, "seek": 297600, "start": 2991.44, "end": 2992.96, "text": " I think it's been very surprising to people", "tokens": [51136, 286, 519, 309, 311, 668, 588, 8830, 281, 561, 51212], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1108, "seek": 297600, "start": 2992.96, "end": 2995.52, "text": " that this works so well, right?", "tokens": [51212, 300, 341, 1985, 370, 731, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1109, "seek": 297600, "start": 2995.52, "end": 2998.48, "text": " And part of, I think, what makes it surprising", "tokens": [51340, 400, 644, 295, 11, 286, 519, 11, 437, 1669, 309, 8830, 51488], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1110, "seek": 297600, "start": 2998.48, "end": 3003.48, "text": " is that these models are able to be trained", "tokens": [51488, 307, 300, 613, 5245, 366, 1075, 281, 312, 8895, 51738], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1111, "seek": 297600, "start": 3003.68, "end": 3005.88, "text": " on a huge number of words, right?", "tokens": [51748, 322, 257, 2603, 1230, 295, 2283, 11, 558, 30, 51858], "temperature": 0.0, "avg_logprob": -0.1308357258035679, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.0002164753241231665}, {"id": 1112, "seek": 300588, "start": 3005.88, "end": 3007.96, "text": " And so sort of superficially knowing a little bit", "tokens": [50364, 400, 370, 1333, 295, 23881, 2270, 5276, 257, 707, 857, 50468], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1113, "seek": 300588, "start": 3007.96, "end": 3010.12, "text": " about conceptual roles of a huge number of words", "tokens": [50468, 466, 24106, 9604, 295, 257, 2603, 1230, 295, 2283, 50576], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1114, "seek": 300588, "start": 3010.12, "end": 3011.92, "text": " seems to get you pretty far", "tokens": [50576, 2544, 281, 483, 291, 1238, 1400, 50666], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1115, "seek": 300588, "start": 3012.92, "end": 3014.44, "text": " in terms of seeming convincing", "tokens": [50716, 294, 2115, 295, 1643, 278, 24823, 50792], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1116, "seek": 300588, "start": 3014.44, "end": 3016.6600000000003, "text": " and in terms of language production.", "tokens": [50792, 293, 294, 2115, 295, 2856, 4265, 13, 50903], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1117, "seek": 300588, "start": 3017.6, "end": 3019.6800000000003, "text": " But it's also these conceptual roles and theories", "tokens": [50950, 583, 309, 311, 611, 613, 24106, 9604, 293, 13667, 51054], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1118, "seek": 300588, "start": 3019.6800000000003, "end": 3021.7200000000003, "text": " are also weakness and they don't seem very good", "tokens": [51054, 366, 611, 12772, 293, 436, 500, 380, 1643, 588, 665, 51156], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1119, "seek": 300588, "start": 3021.7200000000003, "end": 3023.52, "text": " at robust and precise theories, right?", "tokens": [51156, 412, 13956, 293, 13600, 13667, 11, 558, 30, 51246], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1120, "seek": 300588, "start": 3023.52, "end": 3025.48, "text": " So if you think about conceptual roles", "tokens": [51246, 407, 498, 291, 519, 466, 24106, 9604, 51344], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1121, "seek": 300588, "start": 3025.48, "end": 3026.78, "text": " like in mathematics, right?", "tokens": [51344, 411, 294, 18666, 11, 558, 30, 51409], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1122, "seek": 300588, "start": 3026.78, "end": 3028.96, "text": " How you define say a natural number", "tokens": [51409, 1012, 291, 6964, 584, 257, 3303, 1230, 51518], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1123, "seek": 300588, "start": 3028.96, "end": 3030.8, "text": " or how you define an integral or something, right?", "tokens": [51518, 420, 577, 291, 6964, 364, 11573, 420, 746, 11, 558, 30, 51610], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1124, "seek": 300588, "start": 3030.8, "end": 3034.2000000000003, "text": " Like all of those are symbolic kinds of theories", "tokens": [51610, 1743, 439, 295, 729, 366, 25755, 3685, 295, 13667, 51780], "temperature": 0.0, "avg_logprob": -0.12705405672690026, "compression_ratio": 1.8225255972696246, "no_speech_prob": 2.7964624678133987e-05}, {"id": 1125, "seek": 303420, "start": 3034.2, "end": 3037.08, "text": " which are precise and which support", "tokens": [50364, 597, 366, 13600, 293, 597, 1406, 50508], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1126, "seek": 303420, "start": 3037.08, "end": 3039.72, "text": " chains of reasoning of arbitrary length, right?", "tokens": [50508, 12626, 295, 21577, 295, 23211, 4641, 11, 558, 30, 50640], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1127, "seek": 303420, "start": 3039.72, "end": 3041.4399999999996, "text": " And that's what these systems really seem", "tokens": [50640, 400, 300, 311, 437, 613, 3652, 534, 1643, 50726], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1128, "seek": 303420, "start": 3041.4399999999996, "end": 3043.96, "text": " not to be very good at.", "tokens": [50726, 406, 281, 312, 588, 665, 412, 13, 50852], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1129, "seek": 303420, "start": 3043.96, "end": 3046.0, "text": " Likely that's because there's some important things", "tokens": [50852, 1743, 356, 300, 311, 570, 456, 311, 512, 1021, 721, 50954], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1130, "seek": 303420, "start": 3046.0, "end": 3047.04, "text": " which are missing, right?", "tokens": [50954, 597, 366, 5361, 11, 558, 30, 51006], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1131, "seek": 303420, "start": 3047.04, "end": 3050.8799999999997, "text": " Things like grounding, things like reasoning", "tokens": [51006, 9514, 411, 46727, 11, 721, 411, 21577, 51198], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1132, "seek": 303420, "start": 3050.8799999999997, "end": 3053.4399999999996, "text": " or even richer kinds of theories.", "tokens": [51198, 420, 754, 29021, 3685, 295, 13667, 13, 51326], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1133, "seek": 303420, "start": 3053.4399999999996, "end": 3057.1, "text": " I think Josh will talk about this some next.", "tokens": [51326, 286, 519, 9785, 486, 751, 466, 341, 512, 958, 13, 51509], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1134, "seek": 303420, "start": 3058.1, "end": 3061.08, "text": " I think that there's a kind of broader view", "tokens": [51559, 286, 519, 300, 456, 311, 257, 733, 295, 13227, 1910, 51708], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1135, "seek": 303420, "start": 3061.08, "end": 3063.8999999999996, "text": " of concepts and meanings, which is really,", "tokens": [51708, 295, 10392, 293, 28138, 11, 597, 307, 534, 11, 51849], "temperature": 0.0, "avg_logprob": -0.13457721102554188, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.0002304632362211123}, {"id": 1136, "seek": 306390, "start": 3063.9, "end": 3065.26, "text": " I think the most exciting for people", "tokens": [50364, 286, 519, 264, 881, 4670, 337, 561, 50432], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1137, "seek": 306390, "start": 3065.26, "end": 3069.42, "text": " that work on concepts and concept representations", "tokens": [50432, 300, 589, 322, 10392, 293, 3410, 33358, 50640], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1138, "seek": 306390, "start": 3069.42, "end": 3072.94, "text": " in cognitive psychology, which is that large language models", "tokens": [50640, 294, 15605, 15105, 11, 597, 307, 300, 2416, 2856, 5245, 50816], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1139, "seek": 306390, "start": 3072.94, "end": 3074.54, "text": " have really shown how vectors can do things", "tokens": [50816, 362, 534, 4898, 577, 18875, 393, 360, 721, 50896], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1140, "seek": 306390, "start": 3074.54, "end": 3076.46, "text": " that were long thought to be impossible", "tokens": [50896, 300, 645, 938, 1194, 281, 312, 6243, 50992], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1141, "seek": 306390, "start": 3076.46, "end": 3078.1800000000003, "text": " for non-symbolic models, right?", "tokens": [50992, 337, 2107, 12, 3187, 5612, 299, 5245, 11, 558, 30, 51078], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1142, "seek": 306390, "start": 3078.1800000000003, "end": 3079.76, "text": " In particular, these kinds of arguments", "tokens": [51078, 682, 1729, 11, 613, 3685, 295, 12869, 51157], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1143, "seek": 306390, "start": 3079.76, "end": 3082.06, "text": " from people like Fodor and Polition", "tokens": [51157, 490, 561, 411, 479, 34024, 293, 3635, 849, 51272], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1144, "seek": 306390, "start": 3082.06, "end": 3085.54, "text": " about compositionality and systematicity and productivity,", "tokens": [51272, 466, 12686, 1860, 293, 27249, 507, 293, 15604, 11, 51446], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1145, "seek": 306390, "start": 3085.54, "end": 3086.38, "text": " right?", "tokens": [51446, 558, 30, 51488], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1146, "seek": 306390, "start": 3086.38, "end": 3089.02, "text": " All of these kinds of things that people have pointed to", "tokens": [51488, 1057, 295, 613, 3685, 295, 721, 300, 561, 362, 10932, 281, 51620], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1147, "seek": 306390, "start": 3089.02, "end": 3092.14, "text": " as characteristic features of thinking", "tokens": [51620, 382, 16282, 4122, 295, 1953, 51776], "temperature": 0.0, "avg_logprob": -0.14636724423139524, "compression_ratio": 1.7157534246575343, "no_speech_prob": 0.00013548914284911007}, {"id": 1148, "seek": 309214, "start": 3092.66, "end": 3094.7, "text": " have argued were characteristic features", "tokens": [50390, 362, 20219, 645, 16282, 4122, 50492], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1149, "seek": 309214, "start": 3094.7, "end": 3097.8199999999997, "text": " of symbolic thinking just turn out not to be right, right?", "tokens": [50492, 295, 25755, 1953, 445, 1261, 484, 406, 281, 312, 558, 11, 558, 30, 50648], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1150, "seek": 309214, "start": 3097.8199999999997, "end": 3101.54, "text": " It turns out you can get vectors to do those things.", "tokens": [50648, 467, 4523, 484, 291, 393, 483, 18875, 281, 360, 729, 721, 13, 50834], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1151, "seek": 309214, "start": 3101.54, "end": 3103.12, "text": " And I would argue that the solution", "tokens": [50834, 400, 286, 576, 9695, 300, 264, 3827, 50913], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1152, "seek": 309214, "start": 3103.12, "end": 3104.98, "text": " to why vectors could do those things", "tokens": [50913, 281, 983, 18875, 727, 360, 729, 721, 51006], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1153, "seek": 309214, "start": 3104.98, "end": 3107.9, "text": " is probably that what these models are doing", "tokens": [51006, 307, 1391, 300, 437, 613, 5245, 366, 884, 51152], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1154, "seek": 309214, "start": 3107.9, "end": 3111.5, "text": " is training vectors that encode conceptual roles, right?", "tokens": [51152, 307, 3097, 18875, 300, 2058, 1429, 24106, 9604, 11, 558, 30, 51332], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1155, "seek": 309214, "start": 3111.5, "end": 3114.58, "text": " Like what they're learning is representations of meaning", "tokens": [51332, 1743, 437, 436, 434, 2539, 307, 33358, 295, 3620, 51486], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1156, "seek": 309214, "start": 3114.58, "end": 3118.14, "text": " which capture the important parts of conceptual roles.", "tokens": [51486, 597, 7983, 264, 1021, 3166, 295, 24106, 9604, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1157, "seek": 309214, "start": 3118.14, "end": 3121.98, "text": " This is actually something which has been long sought after", "tokens": [51664, 639, 307, 767, 746, 597, 575, 668, 938, 17532, 934, 51856], "temperature": 0.0, "avg_logprob": -0.1241766836790912, "compression_ratio": 1.7885304659498207, "no_speech_prob": 0.0004582350084092468}, {"id": 1158, "seek": 312198, "start": 3121.98, "end": 3124.14, "text": " in say computational neuroscience.", "tokens": [50364, 294, 584, 28270, 42762, 13, 50472], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1159, "seek": 312198, "start": 3124.14, "end": 3126.26, "text": " There's things like vector symbolic architectures", "tokens": [50472, 821, 311, 721, 411, 8062, 25755, 6331, 1303, 50578], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1160, "seek": 312198, "start": 3126.26, "end": 3129.94, "text": " that are very exciting ways of encoding", "tokens": [50578, 300, 366, 588, 4670, 2098, 295, 43430, 50762], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1161, "seek": 312198, "start": 3129.94, "end": 3132.22, "text": " say arbitrary symbolic systems", "tokens": [50762, 584, 23211, 25755, 3652, 50876], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1162, "seek": 312198, "start": 3132.22, "end": 3136.7, "text": " or arbitrary mathematical systems into vectors.", "tokens": [50876, 420, 23211, 18894, 3652, 666, 18875, 13, 51100], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1163, "seek": 312198, "start": 3136.7, "end": 3139.7, "text": " And I think that some marriage of those two things", "tokens": [51100, 400, 286, 519, 300, 512, 7194, 295, 729, 732, 721, 51250], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1164, "seek": 312198, "start": 3139.7, "end": 3142.9, "text": " is going to be very exciting.", "tokens": [51250, 307, 516, 281, 312, 588, 4670, 13, 51410], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1165, "seek": 312198, "start": 3144.5, "end": 3146.38, "text": " So large language models point to a theory of meaning", "tokens": [51490, 407, 2416, 2856, 5245, 935, 281, 257, 5261, 295, 3620, 51584], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1166, "seek": 312198, "start": 3146.38, "end": 3150.66, "text": " that's based on essentially vector based conceptual roles,", "tokens": [51584, 300, 311, 2361, 322, 4476, 8062, 2361, 24106, 9604, 11, 51798], "temperature": 0.0, "avg_logprob": -0.14004859057339755, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.00031496078008785844}, {"id": 1167, "seek": 315066, "start": 3151.02, "end": 3154.06, "text": " and perhaps can capture a lot of the different features", "tokens": [50382, 293, 4317, 393, 7983, 257, 688, 295, 264, 819, 4122, 50534], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1168, "seek": 315066, "start": 3154.06, "end": 3158.8199999999997, "text": " of meaning that people in say cognitive science", "tokens": [50534, 295, 3620, 300, 561, 294, 584, 15605, 3497, 50772], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1169, "seek": 315066, "start": 3158.8199999999997, "end": 3162.8999999999996, "text": " or cognitive development have tried to kind of bring out", "tokens": [50772, 420, 15605, 3250, 362, 3031, 281, 733, 295, 1565, 484, 50976], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1170, "seek": 315066, "start": 3162.8999999999996, "end": 3164.54, "text": " in human conceptual systems, right?", "tokens": [50976, 294, 1952, 24106, 3652, 11, 558, 30, 51058], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1171, "seek": 315066, "start": 3164.54, "end": 3165.98, "text": " Like that our meanings are gradient", "tokens": [51058, 1743, 300, 527, 28138, 366, 16235, 51130], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1172, "seek": 315066, "start": 3165.98, "end": 3167.98, "text": " or that they have hierarchies,", "tokens": [51130, 420, 300, 436, 362, 35250, 530, 11, 51230], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1173, "seek": 315066, "start": 3167.98, "end": 3169.62, "text": " that we know things like definitions", "tokens": [51230, 300, 321, 458, 721, 411, 21988, 51312], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1174, "seek": 315066, "start": 3169.62, "end": 3171.7, "text": " and we can make inferences about relationships", "tokens": [51312, 293, 321, 393, 652, 13596, 2667, 466, 6159, 51416], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1175, "seek": 315066, "start": 3171.7, "end": 3175.5, "text": " and similarities and all of those things seem like things", "tokens": [51416, 293, 24197, 293, 439, 295, 729, 721, 1643, 411, 721, 51606], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1176, "seek": 315066, "start": 3175.5, "end": 3178.54, "text": " that you can encode at least in principle in vectors", "tokens": [51606, 300, 291, 393, 2058, 1429, 412, 1935, 294, 8665, 294, 18875, 51758], "temperature": 0.0, "avg_logprob": -0.11810502551850818, "compression_ratio": 1.748091603053435, "no_speech_prob": 0.0002131139044649899}, {"id": 1177, "seek": 317854, "start": 3179.54, "end": 3180.98, "text": " which is great.", "tokens": [50414, 597, 307, 869, 13, 50486], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1178, "seek": 317854, "start": 3181.94, "end": 3186.02, "text": " So let's get that, let me just end there.", "tokens": [50534, 407, 718, 311, 483, 300, 11, 718, 385, 445, 917, 456, 13, 50738], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1179, "seek": 317854, "start": 3186.02, "end": 3187.94, "text": " I'll thank you again for the invitation", "tokens": [50738, 286, 603, 1309, 291, 797, 337, 264, 17890, 50834], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1180, "seek": 317854, "start": 3187.94, "end": 3191.54, "text": " and thanks also to all of my co-authors on the work here.", "tokens": [50834, 293, 3231, 611, 281, 439, 295, 452, 598, 12, 40198, 830, 322, 264, 589, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1181, "seek": 317854, "start": 3191.54, "end": 3196.54, "text": " All right.", "tokens": [51014, 1057, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1182, "seek": 317854, "start": 3196.7, "end": 3197.54, "text": " Yes.", "tokens": [51272, 1079, 13, 51314], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1183, "seek": 317854, "start": 3199.86, "end": 3201.74, "text": " Maybe I'm reading too much into it", "tokens": [51430, 2704, 286, 478, 3760, 886, 709, 666, 309, 51524], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1184, "seek": 317854, "start": 3201.74, "end": 3204.38, "text": " but it seemed to me that you hinted at", "tokens": [51524, 457, 309, 6576, 281, 385, 300, 291, 12075, 292, 412, 51656], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1185, "seek": 317854, "start": 3206.02, "end": 3207.94, "text": " what these vector representations", "tokens": [51738, 437, 613, 8062, 33358, 51834], "temperature": 0.0, "avg_logprob": -0.27515496878788387, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0003049215883947909}, {"id": 1186, "seek": 320794, "start": 3208.7000000000003, "end": 3211.26, "text": " large language models tell us about", "tokens": [50402, 2416, 2856, 5245, 980, 505, 466, 50530], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1187, "seek": 320794, "start": 3211.26, "end": 3215.7000000000003, "text": " both human cognition as well as about language,", "tokens": [50530, 1293, 1952, 46905, 382, 731, 382, 466, 2856, 11, 50752], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1188, "seek": 320794, "start": 3215.7000000000003, "end": 3217.02, "text": " the nature of language.", "tokens": [50752, 264, 3687, 295, 2856, 13, 50818], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1189, "seek": 320794, "start": 3218.02, "end": 3220.9, "text": " Could I ask you to do a projective measurement", "tokens": [50868, 7497, 286, 1029, 291, 281, 360, 257, 1716, 488, 13160, 51012], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1190, "seek": 320794, "start": 3220.9, "end": 3223.7000000000003, "text": " and come out and say something about that?", "tokens": [51012, 293, 808, 484, 293, 584, 746, 466, 300, 30, 51152], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1191, "seek": 320794, "start": 3225.94, "end": 3230.94, "text": " I think that they tell us that vectors are really plausible.", "tokens": [51264, 286, 519, 300, 436, 980, 505, 300, 18875, 366, 534, 39925, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1192, "seek": 320794, "start": 3231.94, "end": 3235.3, "text": " They kind of show us how vectors are plausible for meanings,", "tokens": [51564, 814, 733, 295, 855, 505, 577, 18875, 366, 39925, 337, 28138, 11, 51732], "temperature": 0.0, "avg_logprob": -0.16740797743012634, "compression_ratio": 1.6443298969072164, "no_speech_prob": 0.0003677356871776283}, {"id": 1193, "seek": 323530, "start": 3235.82, "end": 3238.94, "text": " and the way in which I think that they're plausible", "tokens": [50390, 293, 264, 636, 294, 597, 286, 519, 300, 436, 434, 39925, 50546], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1194, "seek": 323530, "start": 3238.94, "end": 3241.6600000000003, "text": " for meanings is that they encode conceptual roles.", "tokens": [50546, 337, 28138, 307, 300, 436, 2058, 1429, 24106, 9604, 13, 50682], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1195, "seek": 323530, "start": 3242.82, "end": 3243.6600000000003, "text": " That's what I would say.", "tokens": [50740, 663, 311, 437, 286, 576, 584, 13, 50782], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1196, "seek": 323530, "start": 3243.6600000000003, "end": 3245.86, "text": " And I think until them,", "tokens": [50782, 400, 286, 519, 1826, 552, 11, 50892], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1197, "seek": 323530, "start": 3245.86, "end": 3247.38, "text": " until kind of recent deep learning,", "tokens": [50892, 1826, 733, 295, 5162, 2452, 2539, 11, 50968], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1198, "seek": 323530, "start": 3247.38, "end": 3248.98, "text": " I think it was really unclear.", "tokens": [50968, 286, 519, 309, 390, 534, 25636, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1199, "seek": 323530, "start": 3248.98, "end": 3252.38, "text": " So people had argued for decades about whether", "tokens": [51048, 407, 561, 632, 20219, 337, 7878, 466, 1968, 51218], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1200, "seek": 323530, "start": 3252.38, "end": 3254.5, "text": " the foundation of concepts was definitions", "tokens": [51218, 264, 7030, 295, 10392, 390, 21988, 51324], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1201, "seek": 323530, "start": 3254.5, "end": 3256.3, "text": " or is it like somehow similarities", "tokens": [51324, 420, 307, 309, 411, 6063, 24197, 51414], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1202, "seek": 323530, "start": 3256.3, "end": 3257.6600000000003, "text": " or is it that you just know a word", "tokens": [51414, 420, 307, 309, 300, 291, 445, 458, 257, 1349, 51482], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1203, "seek": 323530, "start": 3257.6600000000003, "end": 3261.1400000000003, "text": " and you know a bunch of associated features or whatever.", "tokens": [51482, 293, 291, 458, 257, 3840, 295, 6615, 4122, 420, 2035, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1180877685546875, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002530680794734508}, {"id": 1204, "seek": 326114, "start": 3262.1, "end": 3266.06, "text": " And I think one of the main insights, for example,", "tokens": [50412, 400, 286, 519, 472, 295, 264, 2135, 14310, 11, 337, 1365, 11, 50610], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1205, "seek": 326114, "start": 3266.06, "end": 3269.8599999999997, "text": " is that you can extract a definition", "tokens": [50610, 307, 300, 291, 393, 8947, 257, 7123, 50800], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1206, "seek": 326114, "start": 3269.8599999999997, "end": 3271.18, "text": " from a large language model.", "tokens": [50800, 490, 257, 2416, 2856, 2316, 13, 50866], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1207, "seek": 326114, "start": 3271.18, "end": 3272.3399999999997, "text": " We've even given it some of these", "tokens": [50866, 492, 600, 754, 2212, 309, 512, 295, 613, 50924], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1208, "seek": 326114, "start": 3272.3399999999997, "end": 3273.8199999999997, "text": " kind of human experiments we've done", "tokens": [50924, 733, 295, 1952, 12050, 321, 600, 1096, 50998], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1209, "seek": 326114, "start": 3273.8199999999997, "end": 3275.74, "text": " and they're pretty good at coming up", "tokens": [50998, 293, 436, 434, 1238, 665, 412, 1348, 493, 51094], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1210, "seek": 326114, "start": 3275.74, "end": 3279.22, "text": " with the chocolate torch kind of definitions from those.", "tokens": [51094, 365, 264, 6215, 27822, 733, 295, 21988, 490, 729, 13, 51268], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1211, "seek": 326114, "start": 3279.22, "end": 3280.7799999999997, "text": " And that tells you that the definitions", "tokens": [51268, 400, 300, 5112, 291, 300, 264, 21988, 51346], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1212, "seek": 326114, "start": 3280.7799999999997, "end": 3283.74, "text": " can be encoded into vectors, right?", "tokens": [51346, 393, 312, 2058, 12340, 666, 18875, 11, 558, 30, 51494], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1213, "seek": 326114, "start": 3283.74, "end": 3284.74, "text": " And that's great, right?", "tokens": [51494, 400, 300, 311, 869, 11, 558, 30, 51544], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1214, "seek": 326114, "start": 3284.74, "end": 3289.1, "text": " That means that you don't need to think about definitions", "tokens": [51544, 663, 1355, 300, 291, 500, 380, 643, 281, 519, 466, 21988, 51762], "temperature": 0.0, "avg_logprob": -0.10312363197063577, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.0002304511290276423}, {"id": 1215, "seek": 328910, "start": 3289.1, "end": 3291.86, "text": " as the defining part of concepts, right?", "tokens": [50364, 382, 264, 17827, 644, 295, 10392, 11, 558, 30, 50502], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1216, "seek": 328910, "start": 3291.86, "end": 3294.54, "text": " There's some other kind of more abstract,", "tokens": [50502, 821, 311, 512, 661, 733, 295, 544, 12649, 11, 50636], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1217, "seek": 328910, "start": 3294.54, "end": 3297.1, "text": " you know, high dimensional space or whatever", "tokens": [50636, 291, 458, 11, 1090, 18795, 1901, 420, 2035, 50764], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1218, "seek": 328910, "start": 3297.1, "end": 3298.46, "text": " that defines the meanings.", "tokens": [50764, 300, 23122, 264, 28138, 13, 50832], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1219, "seek": 328910, "start": 3298.46, "end": 3300.18, "text": " And the sense in which it defines the meanings", "tokens": [50832, 400, 264, 2020, 294, 597, 309, 23122, 264, 28138, 50918], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1220, "seek": 328910, "start": 3300.18, "end": 3302.14, "text": " is in terms of the relationships between vectors", "tokens": [50918, 307, 294, 2115, 295, 264, 6159, 1296, 18875, 51016], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1221, "seek": 328910, "start": 3302.14, "end": 3304.3399999999997, "text": " on the tasks that you use the concepts for.", "tokens": [51016, 322, 264, 9608, 300, 291, 764, 264, 10392, 337, 13, 51126], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1222, "seek": 328910, "start": 3305.3399999999997, "end": 3308.5, "text": " Is it just natural since the brain encodes information", "tokens": [51176, 1119, 309, 445, 3303, 1670, 264, 3567, 2058, 4789, 1589, 51334], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1223, "seek": 328910, "start": 3308.5, "end": 3311.42, "text": " with lots of neurons firing through your brain?", "tokens": [51334, 365, 3195, 295, 22027, 16045, 807, 428, 3567, 30, 51480], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1224, "seek": 328910, "start": 3311.42, "end": 3313.1, "text": " This should not be surprising.", "tokens": [51480, 639, 820, 406, 312, 8830, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1225, "seek": 328910, "start": 3313.1, "end": 3317.06, "text": " So it's not surprising, it was always unclear", "tokens": [51564, 407, 309, 311, 406, 8830, 11, 309, 390, 1009, 25636, 51762], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1226, "seek": 328910, "start": 3317.06, "end": 3318.54, "text": " how that was even possible.", "tokens": [51762, 577, 300, 390, 754, 1944, 13, 51836], "temperature": 0.0, "avg_logprob": -0.22222405095254222, "compression_ratio": 1.713310580204778, "no_speech_prob": 0.0002531013742554933}, {"id": 1227, "seek": 331910, "start": 3319.1, "end": 3323.9, "text": " Yeah, exactly, yeah, yeah, yeah.", "tokens": [50364, 865, 11, 2293, 11, 1338, 11, 1338, 11, 1338, 13, 50604], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1228, "seek": 331910, "start": 3323.9, "end": 3325.54, "text": " It's like everybody always kind of knew", "tokens": [50604, 467, 311, 411, 2201, 1009, 733, 295, 2586, 50686], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1229, "seek": 331910, "start": 3325.54, "end": 3327.58, "text": " that there had to be a continuous system", "tokens": [50686, 300, 456, 632, 281, 312, 257, 10957, 1185, 50788], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1230, "seek": 331910, "start": 3327.58, "end": 3328.8199999999997, "text": " which could support these things.", "tokens": [50788, 597, 727, 1406, 613, 721, 13, 50850], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1231, "seek": 331910, "start": 3328.8199999999997, "end": 3330.9, "text": " But when you look at, you know, the discreteness", "tokens": [50850, 583, 562, 291, 574, 412, 11, 291, 458, 11, 264, 2983, 35383, 442, 50954], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1232, "seek": 331910, "start": 3330.9, "end": 3332.86, "text": " in language or this discreteness in mathematics,", "tokens": [50954, 294, 2856, 420, 341, 2983, 35383, 442, 294, 18666, 11, 51052], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1233, "seek": 331910, "start": 3332.86, "end": 3335.3399999999997, "text": " it was always kind of unclear where that could come from.", "tokens": [51052, 309, 390, 1009, 733, 295, 25636, 689, 300, 727, 808, 490, 13, 51176], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1234, "seek": 331910, "start": 3335.3399999999997, "end": 3336.9, "text": " So that's why I think things like vector symbolic", "tokens": [51176, 407, 300, 311, 983, 286, 519, 721, 411, 8062, 25755, 51254], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1235, "seek": 331910, "start": 3336.9, "end": 3339.66, "text": " architectures are very exciting too.", "tokens": [51254, 6331, 1303, 366, 588, 4670, 886, 13, 51392], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1236, "seek": 331910, "start": 3339.66, "end": 3342.9, "text": " So when a human fills in the meaning,", "tokens": [51392, 407, 562, 257, 1952, 22498, 294, 264, 3620, 11, 51554], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1237, "seek": 331910, "start": 3342.9, "end": 3344.2999999999997, "text": " they're using a lot of context", "tokens": [51554, 436, 434, 1228, 257, 688, 295, 4319, 51624], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1238, "seek": 331910, "start": 3344.2999999999997, "end": 3346.14, "text": " that they've gotten from the real world.", "tokens": [51624, 300, 436, 600, 5768, 490, 264, 957, 1002, 13, 51716], "temperature": 0.0, "avg_logprob": -0.18316020259150753, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0003300397365819663}, {"id": 1239, "seek": 334614, "start": 3346.14, "end": 3349.18, "text": " When an octopus tries to fill in the meaning,", "tokens": [50364, 1133, 364, 27962, 9898, 281, 2836, 294, 264, 3620, 11, 50516], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1240, "seek": 334614, "start": 3349.18, "end": 3352.18, "text": " they have much less context to work with.", "tokens": [50516, 436, 362, 709, 1570, 4319, 281, 589, 365, 13, 50666], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1241, "seek": 334614, "start": 3352.18, "end": 3354.2999999999997, "text": " And when a LLM fills in the meaning,", "tokens": [50666, 400, 562, 257, 441, 43, 44, 22498, 294, 264, 3620, 11, 50772], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1242, "seek": 334614, "start": 3354.2999999999997, "end": 3357.5, "text": " they have no kind of context to work with.", "tokens": [50772, 436, 362, 572, 733, 295, 4319, 281, 589, 365, 13, 50932], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1243, "seek": 334614, "start": 3357.5, "end": 3358.62, "text": " So I was wondering if you had thoughts", "tokens": [50932, 407, 286, 390, 6359, 498, 291, 632, 4598, 50988], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1244, "seek": 334614, "start": 3358.62, "end": 3361.98, "text": " about the differences and that implies.", "tokens": [50988, 466, 264, 7300, 293, 300, 18779, 13, 51156], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1245, "seek": 334614, "start": 3361.98, "end": 3364.62, "text": " It's really interesting to think of what's exactly", "tokens": [51156, 467, 311, 534, 1880, 281, 519, 295, 437, 311, 2293, 51288], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1246, "seek": 334614, "start": 3364.62, "end": 3366.06, "text": " happening in that human experiment", "tokens": [51288, 2737, 294, 300, 1952, 5120, 51360], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1247, "seek": 334614, "start": 3366.06, "end": 3368.2599999999998, "text": " because I agree it's transfer of stuff you know,", "tokens": [51360, 570, 286, 3986, 309, 311, 5003, 295, 1507, 291, 458, 11, 51470], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1248, "seek": 334614, "start": 3368.2599999999998, "end": 3369.42, "text": " like you've encountered cakes", "tokens": [51470, 411, 291, 600, 20381, 19932, 51528], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1249, "seek": 334614, "start": 3369.42, "end": 3372.66, "text": " and you've encountered fancy pastries or whatever.", "tokens": [51528, 293, 291, 600, 20381, 10247, 1791, 2244, 420, 2035, 13, 51690], "temperature": 0.0, "avg_logprob": -0.16627405484517416, "compression_ratio": 1.75, "no_speech_prob": 0.00012925517512485385}, {"id": 1250, "seek": 337266, "start": 3372.7, "end": 3376.14, "text": " And part of what you know about those meanings", "tokens": [50366, 400, 644, 295, 437, 291, 458, 466, 729, 28138, 50538], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1251, "seek": 337266, "start": 3376.14, "end": 3377.98, "text": " are the grounded parts, right?", "tokens": [50538, 366, 264, 23535, 3166, 11, 558, 30, 50630], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1252, "seek": 337266, "start": 3377.98, "end": 3378.98, "text": " You know what a cake looks like,", "tokens": [50630, 509, 458, 437, 257, 5908, 1542, 411, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1253, "seek": 337266, "start": 3378.98, "end": 3383.62, "text": " which is why you can recognize the pictures and things.", "tokens": [50680, 597, 307, 983, 291, 393, 5521, 264, 5242, 293, 721, 13, 50912], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1254, "seek": 337266, "start": 3383.62, "end": 3385.3799999999997, "text": " I always have a little bit of trouble thinking about it", "tokens": [50912, 286, 1009, 362, 257, 707, 857, 295, 5253, 1953, 466, 309, 51000], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1255, "seek": 337266, "start": 3385.3799999999997, "end": 3387.7, "text": " because it's never quite clear to me exactly", "tokens": [51000, 570, 309, 311, 1128, 1596, 1850, 281, 385, 2293, 51116], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1256, "seek": 337266, "start": 3387.7, "end": 3389.7799999999997, "text": " what it means to transfer something grounded.", "tokens": [51116, 437, 309, 1355, 281, 5003, 746, 23535, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1257, "seek": 337266, "start": 3389.7799999999997, "end": 3391.7799999999997, "text": " Like it feels a little bit like in order for it", "tokens": [51220, 1743, 309, 3417, 257, 707, 857, 411, 294, 1668, 337, 309, 51320], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1258, "seek": 337266, "start": 3391.7799999999997, "end": 3394.5, "text": " to transfer at all, it has to be a little bit abstract.", "tokens": [51320, 281, 5003, 412, 439, 11, 309, 575, 281, 312, 257, 707, 857, 12649, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1259, "seek": 337266, "start": 3396.14, "end": 3398.06, "text": " But I agree that that's the right question to ask", "tokens": [51538, 583, 286, 3986, 300, 300, 311, 264, 558, 1168, 281, 1029, 51634], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1260, "seek": 337266, "start": 3398.06, "end": 3400.7, "text": " and we don't have any theories or certainly no evidence", "tokens": [51634, 293, 321, 500, 380, 362, 604, 13667, 420, 3297, 572, 4467, 51766], "temperature": 0.0, "avg_logprob": -0.1213648781847598, "compression_ratio": 1.7433333333333334, "no_speech_prob": 6.401112477760762e-05}, {"id": 1261, "seek": 340070, "start": 3400.7, "end": 3403.7799999999997, "text": " about how exactly people solve that problem", "tokens": [50364, 466, 577, 2293, 561, 5039, 300, 1154, 50518], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1262, "seek": 340070, "start": 3403.7799999999997, "end": 3406.7, "text": " or the way in which it relies on conceptual roles", "tokens": [50518, 420, 264, 636, 294, 597, 309, 30910, 322, 24106, 9604, 50664], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1263, "seek": 340070, "start": 3406.7, "end": 3409.46, "text": " versus grounded experience or something.", "tokens": [50664, 5717, 23535, 1752, 420, 746, 13, 50802], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1264, "seek": 340070, "start": 3409.46, "end": 3411.06, "text": " Okay, so we'll have one more question.", "tokens": [50802, 1033, 11, 370, 321, 603, 362, 472, 544, 1168, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1265, "seek": 340070, "start": 3411.06, "end": 3413.58, "text": " Meanwhile, maybe we can have the next speakers", "tokens": [50882, 13879, 11, 1310, 321, 393, 362, 264, 958, 9518, 51008], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1266, "seek": 340070, "start": 3413.58, "end": 3414.66, "text": " start setting up.", "tokens": [51008, 722, 3287, 493, 13, 51062], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1267, "seek": 340070, "start": 3418.3399999999997, "end": 3420.98, "text": " Yeah, thanks Steve for the great talk.", "tokens": [51246, 865, 11, 3231, 7466, 337, 264, 869, 751, 13, 51378], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1268, "seek": 340070, "start": 3420.98, "end": 3423.3399999999997, "text": " I wanted to understand better what the argument was", "tokens": [51378, 286, 1415, 281, 1223, 1101, 437, 264, 6770, 390, 51496], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1269, "seek": 340070, "start": 3423.3399999999997, "end": 3427.66, "text": " in this kind of conceptual embedding experiment", "tokens": [51496, 294, 341, 733, 295, 24106, 12240, 3584, 5120, 51712], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1270, "seek": 340070, "start": 3427.66, "end": 3429.98, "text": " because it seems like you could just ask the LLM", "tokens": [51712, 570, 309, 2544, 411, 291, 727, 445, 1029, 264, 441, 43, 44, 51828], "temperature": 0.0, "avg_logprob": -0.1249568057510088, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0004724730097223073}, {"id": 1271, "seek": 342998, "start": 3430.46, "end": 3431.62, "text": " whether it's tall or not.", "tokens": [50388, 1968, 309, 311, 6764, 420, 406, 13, 50446], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1272, "seek": 342998, "start": 3431.62, "end": 3434.26, "text": " Like you didn't really need to do this projection", "tokens": [50446, 1743, 291, 994, 380, 534, 643, 281, 360, 341, 22743, 50578], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1273, "seek": 342998, "start": 3434.26, "end": 3436.42, "text": " to know that it can do this task.", "tokens": [50578, 281, 458, 300, 309, 393, 360, 341, 5633, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1274, "seek": 342998, "start": 3436.42, "end": 3439.02, "text": " So is it somehow, is there something special", "tokens": [50686, 407, 307, 309, 6063, 11, 307, 456, 746, 2121, 50816], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1275, "seek": 342998, "start": 3439.02, "end": 3440.78, "text": " about the fact that you're looking at embeddings", "tokens": [50816, 466, 264, 1186, 300, 291, 434, 1237, 412, 12240, 29432, 50904], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1276, "seek": 342998, "start": 3440.78, "end": 3443.86, "text": " rather than the outputs or what's kind of going on there?", "tokens": [50904, 2831, 813, 264, 23930, 420, 437, 311, 733, 295, 516, 322, 456, 30, 51058], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1277, "seek": 342998, "start": 3444.9, "end": 3446.18, "text": " That's a good question.", "tokens": [51110, 663, 311, 257, 665, 1168, 13, 51174], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1278, "seek": 342998, "start": 3446.18, "end": 3448.34, "text": " So I think you probably could do that.", "tokens": [51174, 407, 286, 519, 291, 1391, 727, 360, 300, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1279, "seek": 342998, "start": 3448.34, "end": 3452.1, "text": " I don't know how the results would compare", "tokens": [51282, 286, 500, 380, 458, 577, 264, 3542, 576, 6794, 51470], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1280, "seek": 342998, "start": 3452.1, "end": 3454.02, "text": " if you just asked versus not.", "tokens": [51470, 498, 291, 445, 2351, 5717, 406, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1281, "seek": 342998, "start": 3456.26, "end": 3457.7400000000002, "text": " Yeah, I'm not sure.", "tokens": [51678, 865, 11, 286, 478, 406, 988, 13, 51752], "temperature": 0.0, "avg_logprob": -0.1127129077911377, "compression_ratio": 1.62890625, "no_speech_prob": 0.0007095106993801892}, {"id": 1282, "seek": 345774, "start": 3457.74, "end": 3461.4199999999996, "text": " I mean, I think it's like if it doesn't succeed", "tokens": [50364, 286, 914, 11, 286, 519, 309, 311, 411, 498, 309, 1177, 380, 7754, 50548], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1283, "seek": 345774, "start": 3461.4199999999996, "end": 3463.2999999999997, "text": " on just asking, then it's interesting to know", "tokens": [50548, 322, 445, 3365, 11, 550, 309, 311, 1880, 281, 458, 50642], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1284, "seek": 345774, "start": 3463.2999999999997, "end": 3465.8999999999996, "text": " whether it's kind of latent representation", "tokens": [50642, 1968, 309, 311, 733, 295, 48994, 10290, 50772], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1285, "seek": 345774, "start": 3465.8999999999996, "end": 3467.58, "text": " still has that information or not.", "tokens": [50772, 920, 575, 300, 1589, 420, 406, 13, 50856], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1286, "seek": 345774, "start": 3467.58, "end": 3472.58, "text": " So, but I don't know the whole space of kind of how you,", "tokens": [50856, 407, 11, 457, 286, 500, 380, 458, 264, 1379, 1901, 295, 733, 295, 577, 291, 11, 51106], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1287, "seek": 345774, "start": 3472.8999999999996, "end": 3474.58, "text": " how you can interrogate these models", "tokens": [51122, 577, 291, 393, 24871, 473, 613, 5245, 51206], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1288, "seek": 345774, "start": 3474.58, "end": 3475.8999999999996, "text": " for those questions, so.", "tokens": [51206, 337, 729, 1651, 11, 370, 13, 51272], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1289, "seek": 345774, "start": 3477.4199999999996, "end": 3480.8999999999996, "text": " All right, let's thank Steve again.", "tokens": [51348, 1057, 558, 11, 718, 311, 1309, 7466, 797, 13, 51522], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1290, "seek": 345774, "start": 3483.7799999999997, "end": 3486.9799999999996, "text": " We'll have plenty of time to talk to him more", "tokens": [51666, 492, 603, 362, 7140, 295, 565, 281, 751, 281, 796, 544, 51826], "temperature": 0.0, "avg_logprob": -0.21608102546547944, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.00024119432782754302}, {"id": 1291, "seek": 348698, "start": 3487.94, "end": 3491.14, "text": " at the refreshments after this talk.", "tokens": [50412, 412, 264, 15134, 1117, 934, 341, 751, 13, 50572], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1292, "seek": 348698, "start": 3491.14, "end": 3494.22, "text": " And who knows, maybe there will be Zafar Turkish in there.", "tokens": [50572, 400, 567, 3255, 11, 1310, 456, 486, 312, 1176, 47030, 18565, 294, 456, 13, 50726], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1293, "seek": 348698, "start": 3494.22, "end": 3495.78, "text": " Which is, by the way, amazing.", "tokens": [50726, 3013, 307, 11, 538, 264, 636, 11, 2243, 13, 50804], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1294, "seek": 348698, "start": 3495.78, "end": 3498.66, "text": " If you're in Vienna, you should absolutely try it.", "tokens": [50804, 759, 291, 434, 294, 31024, 11, 291, 820, 3122, 853, 309, 13, 50948], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1295, "seek": 348698, "start": 3498.66, "end": 3503.66, "text": " So unfortunately, the next speaker could not be here.", "tokens": [50948, 407, 7015, 11, 264, 958, 8145, 727, 406, 312, 510, 13, 51198], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1296, "seek": 348698, "start": 3503.66, "end": 3508.66, "text": " Josh Tenenbaum is a latent variable in this session", "tokens": [51198, 9785, 9380, 268, 46641, 307, 257, 48994, 7006, 294, 341, 5481, 51448], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1297, "seek": 348698, "start": 3509.98, "end": 3514.66, "text": " because both Stephen was a student of Josh's", "tokens": [51514, 570, 1293, 13391, 390, 257, 3107, 295, 9785, 311, 51748], "temperature": 0.0, "avg_logprob": -0.23133956061469185, "compression_ratio": 1.4977168949771689, "no_speech_prob": 0.0012436298420652747}, {"id": 1298, "seek": 351466, "start": 3514.66, "end": 3515.62, "text": " and...", "tokens": [50370, 293, 485, 50412], "temperature": 0.0, "avg_logprob": -0.886524772644043, "compression_ratio": 0.42857142857142855, "no_speech_prob": 0.3603125214576721}], "language": "en"}