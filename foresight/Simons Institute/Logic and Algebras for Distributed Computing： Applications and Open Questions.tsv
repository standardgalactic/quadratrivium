start	end	text
0	8000	Second talk today has a title that has evolved since last time I looked at it.
8000	17280	Okay, but it has logic and algebras and we are very excited to hear what this has to do with cloud computing,
17280	22080	which we are all subjected to in our daily lives.
22080	31440	Thank you. So this will be a bit unusual, both Joe and Connor will talk and I let them take
31440	38640	care of the logistics of that. Again, remember that for questions that might be more appropriate
38640	43360	for a longer discussion, we will have that discussion right after their talk.
45840	51680	Okay, thanks. So this is work that obviously we're doing here at Berkeley and I'm also funded by
51680	55840	Sutter Hill Ventures. So it's kind of cool. We've got venture capitalists funding basic research.
55840	60240	I have promised them that there's no applicable. I'm not really sure what this could turn into
60240	64400	as a company, but they're cool and they've given us developers to work on the project,
64400	68000	which has just been great and they're funding me as well. So thanks to them and to Berkeley.
68800	72480	There's this story people like to tell in computing. This is my standard opening slides.
73920	76960	Operating systems people really like this story because it's sort of the Thompson and
76960	81200	Richie Turing Award story. For every platform that comes out, there's a programming environment
81440	85920	that's somehow suited to that platform that emerges and as a result, people write things you
85920	90720	never would have expected on that platform and it succeeds. So the PDP 11 with Unix and C
90720	94800	is the canonical example of this, but one can argue that in every generation of new kind of
94800	100560	computing platforms, programming environments have arisen to allow people to build an app for that.
101840	105760	And so nobody expected all the apps we have on our phones. It's wonderful. Developers were
106400	110560	freed to write all sorts of things. Strangely, there's a platform that's as old as the iPhone
110560	116000	called the cloud. So AWS is approximately the same age as the iPhone, but it doesn't have a
116000	121680	canonical programming model. And there's many reasons why that might be, partly because it's
121680	125680	a really hard programming environment, yes. So it has to deal with all the problems of parallel
125680	131600	computing, as well as things like distributed consistency, what happens when you have partial
131600	135600	failures in your system, but it keeps running. So componentry is down, but the system's still
135600	140400	running. And then in the modern cloud, we want things to auto scale. So you allocate more machines
140400	144160	and then you free up some machines, but the program's still running. So the platform is
144160	149360	changing underneath you as you're executing. So this is all hard and programmers right now are
149360	154640	trying to do this essentially in Java. That's sort of the state of the art. And the annoying thing
154640	160000	is these compilers for these languages don't answer any of these questions that are hard. So I think,
160000	164640	honestly, this is like this hole in computer science that nobody's filled and it seems like
164640	168720	one of our grand challenges from my perspective. So I've been working on it for a long time,
169360	173120	and I think there's still a lot of work to do. I take inspiration, of course, from this gentleman
173120	178160	as maybe we all do. What was cool about Ted Codd was he said, look, you should write things in a
178160	182000	formal language. You should have formal specifications. And then there should be machinery that
182000	186960	automates the implementation. And if we do that, then the implementation can change
186960	191520	while the specification remains the same. This is very nice for things like databases, right?
191520	197520	So the thing is that Codd was trapped in this database prison for all these years. And I think
197520	202560	there's a much broader applicability of the design principle. So we worked on things in our
202560	206800	community like declarative networking. So we brought Codd out of the database and into the network.
207760	211840	So I've done some work on that. Many of us I think in this room have done something around
211840	216560	declarative data science and machine learning. This is a growing area, right? In the program
216560	221120	analysis community, the use of declarative languages has been pretty powerful. So that's
221120	225360	really cool. And then, of course, the hot thing, which is why we're all here, is that we're going
225360	229840	to start to try to look at this stuff through the lenses of algebras instead of logic or in addition
229840	234240	to logic, which is pretty neat. And we're going to, you know, we've heard or are hearing about a
234240	240000	variety of different algebras that people are playing with in this domain. So what I'm interested in
240000	244480	is taking Codd into the cloud, yeah? And here's sort of the analogy, the way to think about it.
244480	249280	The relational database was invented to hide how data is laid out and how queries are executed,
249280	254000	right? And all that should be decided sort of lazily based on the current environment.
254000	259440	Well, the cloud is just a generalization. It was invented to hide all your computing resources
259440	263200	and how they're laid out. So not just your blocks on your desk, but really everything.
263760	269520	And it's for general purpose computations. So the cloud, in a lot of ways, is this abstraction.
269520	274960	It's this physical layer abstraction. The physics of the deployment of your code is going to change,
274960	278160	but you want your spec to remain the same. That's how you'd really like to program in an
278160	283920	environment that is this heterogeneous and elastic. So I believe that it's extremely natural for
283920	288560	techniques that we've been working on in our community to try to be applied to cloud computing.
288560	292560	And we have a project in my group called Hydro, which you can read more about,
292560	298480	which I will tell you a bit about today. Okay. So what are my goals? Well, I kind of want to
298480	304080	build something like LLVM for the cloud. So LLVM, as you may know, is a very successful sort of
304160	310320	language stack. It supports many languages, including C++ and Rust and Swift and others.
311040	314720	It has an internal language called its internal representation, and then it compiles down to
314720	319920	a variety of machine code for different platforms. So it's been extremely successful, but it doesn't
319920	324480	answer any distributed questions. So if you're writing a distributed program, you might ask a
324480	329040	question like, is my program consistent in some sense? Or if I talk to different machines in the
329040	333440	network, will they give me different answers and be confused? That's a question that distributed
333440	338080	programmers need to deal with. Here's another one. My state no longer fits on one computer. How do
338080	341760	I partition it across multiple computers while getting the same answer out of my program?
342320	347920	I want you, you compiler should figure that out for me. What failures can my system tolerate and
347920	351360	how many of them before it stops working the way that the spec declares it should?
352560	357680	What data is going where around the world and who can see it? These are all questions distributed
357680	362800	systems always have to answer. And then I have different objective functions. So I'd like to
362800	368240	optimize some days for maybe my dollar spend in the cloud, but I don't care about latency or maybe
368240	375360	vice versa. Maybe I care about particular latency distribution. So I want the 99th percentile
375360	380800	of my workload to achieve a certain latency versus the 95th or what have you. These will all
380800	386640	lead to different decisions about resource allocation and program structure and so on, right?
386640	391440	And if you ask these questions of LLVM, that's the answer you get. It doesn't deal with any of
391440	396960	these issues. And that's kind of where we'd like to come in. We've written a vision paper a couple
396960	402560	years ago that I can point you to and I won't go through all of it today. But the idea is to use
402560	408640	database techniques and ideas to optimize in concert with LLVM. So LLVM is responsible for the
408640	412960	single node, but database techniques perhaps responsible for the messaging, the data movement
412960	417920	that happens in a distributed program. So here's how we envision the hydro stack. Many programming
417920	424160	languages up top. Some techniques to translate them into an internal representation. We've
424160	428640	got a little bit of initial work here. And then the internal representation should be some
429920	435440	formal spec that is global in some sense. So it doesn't worry yet about how many machines I have
435440	441360	or what the machines can do. It's just a formalized specification of what you wrote in a perhaps
441360	445360	imperative language. Okay, so it's kind of machine oblivious. Maybe it's a logic. Maybe it's an
445360	449840	algebra. Things that are in red are work in progress. Things that are in green kind of work
449840	456080	at this point. And then from there we want to build a compiler and we're working with Max
456080	462080	on using eGraphs for that compiler to translate down into a per node physical algebra. So every
462080	466720	machine would run its own little program. Those programs communicate with each other very much
466720	471280	like a parallel query plan is a bunch of individual query plans running on individual machines talking
471280	476640	to each other over a network. Okay, so this is a sort of per node physical algebra because it's
476640	481840	actually doing stuff. We've implemented this in Rust. It's very fast and I'll show you some of
481840	486640	this today. So that's kind of what we envision as how this is all going to work. At the bottom
486640	490560	there's something that's deploying this on machines and deciding how many machines and how few over
492400	496000	time. Okay, so some of the topics I want to talk about today we're going to focus on this piece of
496000	502640	the stack. Things in red are work in progress. Very much more questions than answers for sure.
502640	507280	So how do we take code and automatically replicate it along with its associated state or data
507920	511840	while ensuring that the program continues to produce the same outcomes as it would have on a
511840	516560	single machine? So I'm particularly interested in doing this in cases where the replication comes
516560	521840	for free and the individual machines don't have to coordinate with each other in a technical sense
521840	526640	I'll talk about. But we'd like to avoid replication when we can. We'll call that free replication and
526640	531840	this is the domain of the calm theorem which you may have heard of and I will review. Unfortunately,
531840	539680	the calm theorem was done in a very particular framework for the proofs. It's not at all clear
539680	544000	how it applies outside that framework and so what we'd really like is a more algebraic notion of the
544000	548400	calm theorem which is something that Connor's working on and after the talk if you're interested
548400	553440	come find Connor and or me to talk about that. Another topic that Connor's going to talk about
553440	558320	today is termination detection and again ideally termination detection where I can decide it locally
558320	563200	for free without asking anyone else. So how do I know in a streaming program that it's done
563760	569200	when there's other agents in the world? So we're going to talk about how to do that with threshold
569200	573920	morphisms but Connor's got ideas about more general notions of equivalence classes that may
573920	578000	allow termination in more settings. So he'll give you a flavor of this work in progress.
579120	582880	The third piece we may or may not have time for today but my student David Chu has been working
582880	587600	on this how do you take a program and partition the state of the program across machines if the
587600	592240	state doesn't fit on one machine. So you really need to partition the code and the data. This is
592240	597440	very much like traditional shared nothing parallel databases if you like but we want to do this to
597440	602880	full and rather complex data log programs and so we've got an implementation of Paxos which is
603360	607520	number of lines of data log where David's able to do some of this automatic partitioning.
608320	612400	Functional dependencies have a role to play here and it would be nice to integrate those into an
612400	618480	algebraic frame as well. And of course all this has to go fast and ideally as fast as
618480	623520	handwritten C++. I'm really previous iterations of my work we settled for interpreters and just
623520	628320	showing things were possible. Now we'd like to convince the systems people that things will be
628320	634560	as fast as they want them to be. So is this business just pie in the sky?
636080	642720	Let's see. All right so we're building on an obsession of some 10 to 12 years that I've had
642720	647600	now in my third group of grad students working in this area. So the initial batch of grad students
647600	653040	was doing formalisms. So we have this very nice logic called Daedalus which is a subset of data
653040	658000	log neg actually that allows you to talk about time and space in a particular way and it's got
658000	663680	a nice model theory. And so that works all there and we can use that as a basis for you know
663680	670000	our semantics to start which is nice. And then there was this lovely work that Tom Amalut did at
670000	675280	Hasselt the column theorem which was a conjecture I had and he and colleagues went ahead and proved
675920	680720	that talks about how things are monotone in some certain sense. Then you can do this free
681840	686640	replication. So you don't need to do coordination to get replica consistency. So I will talk about
686640	691200	the column theorem today to give a review. And then we actually built out a language. It was
691200	696240	slow. It was interpreted. It was written in Ruby but it integrated lattices into a data log like
696240	700560	language and we were able to show that you can get stratified negation and aggregation. You can
700560	705520	get morphisms on your lattices to allow you to do semi-naive evaluation even with the lattices.
705520	709920	So it's really actually rather a nice mixture of algebra and logic. None of this was formally
709920	714000	proved but it was I think one of the earlier systems to observe that you could build this.
714080	718480	That was pretty cool. And that's Neil Conway's thesis work. So we have this as a basis. This kind
718480	723600	of ground zero for my team. And then what happened is I had a batch of students that didn't want to
723600	728640	do languages or theory. So they just built stuff in the spirit of these ideas. And I'm not going
728640	733920	to go through all this but it's things like functions as a service and protocols and testing
733920	739440	and provenance. It's cool stuff. What I do want to focus on is one of those projects which was a
739520	744800	key value store. Key value store is just like a hash table. It's a database where you look
744800	749040	things up by key and you get a value. So you can think of it as a distributed hash table.
749680	753520	These are like the Petri dishes of distributed systems. You're basically saying I have a memory.
753520	757840	It's distributed across many computers. It may be replicated. It may be partitioned. But that's
757840	763840	what I have. It's just registers with values, keys with values. So there's no algorithms per se.
763840	768080	It's all just kind of like focusing on these semantic issues about replication and partitioning.
769040	773280	But the idea that was behind the Anna key value store that we built was everything's a
773280	778720	semi-ladys. And because everything's a semi-ladys and therefore associative, commutative, and item
778720	783280	potent, messages in the network can be replicated. They can be interleaved. They can be reordered and
783280	788080	everything will be fine. So if you design with semi-ladys from the bottom up, you can build a
788080	792720	system that does no coordination. So it's fully monotonic, which means everything can be replicated
792720	797600	as much or as little as you like. Across the globe, between disks and memory, you can replicate
797600	802720	lots of ways. You can do updates anywhere. So multiple updates to the same key can be happening
802720	808800	concurrently in different places at the same time. And they are merged by the lattice lazily via gossip.
808800	812880	And so you build this thing with no concurrency control whatsoever. So there's no locks in the
812880	817600	system. There's no calls to atomic instructions on processors. There's certainly no Paxos protocols
817600	822560	or anything like that. It's just put, get, and gossip. It's really a simple piece of software.
823520	827440	And so Chenggang Wu, the student who led this, won the dissertation award. I think it was very
827440	832320	well deserved because this system was really elegant and really fast. So to give you a sense
832320	836720	of kind of the lattices that he uses, he took from the literature a variety of these kind of
836720	840800	consistency models that people talk about in distributed systems. And he showed how you can
840800	845840	get them by building little composite lattices that wrap up the data. So this is what's called
845840	850800	last-rater wins in the distributed systems, which is just whenever you see a value that's
850800	855440	from later than your value, you update, otherwise you don't. And this you just, you know, take your
855440	861600	map, which is from keys to things, and you wrap the things in a lexical pair of a clock or a version
862160	868160	and the data, right? And you can only have one value per version. So this, this works out as a
868160	874240	lattice. Here's a fancier one, though. This is actually one of the strongest forms of consistency
874240	878560	you can get without coordination. It's called causal consistency. And here what you have is for
878560	884240	every key, you have a vector clock and the data. And the vector clock itself is a map lattice with
885120	887440	node IDs and counters. Yes.
887440	890480	Just want to say a little bit sort of operational transform.
890480	892080	A little bit. Yes.
892080	896880	Can you ever get stuck? Like, do repairs always exist or do you set it up such that they do?
896880	903200	So these, these particular well trodden forms of consistency work fine. And these lattices
903200	906800	are capturing that. They're saying, look, it's just merge. All right. And it's always going to work
906800	911600	because you've defined an associative commutative item potent merge function. OTs are like really
911600	916640	weird and full of all sorts of reasoning I don't understand. And they would never be able to have
916640	920640	such a simple assertion of correctness. All I'm saying here is it's associative commutative
920640	924880	of an item potent. I got nothing more to say. It takes a little bit of convincing to say that
924880	929040	gives you causal consistency, but it's not much convincing because this helps you make causal
929040	933840	consistency with vector clocks. So the observation that clocks and vector clocks are lattices is
933840	937360	just a nice thing about distributed programming. Yeah.
938080	940960	So what do you mean by everything is a lattice? So you mentioned that
941920	943840	Kira's story is a hash map. Right.
944720	951760	Every key has to be a lattice thing. So the nice thing about the map lattice is the keys
951760	957280	are not lattice values. The keys are just keys. The whole table is a lattice. But the object is
957280	961680	a lattice because what happens is the merge function is for a particular key. If there's
961680	965920	nothing, it gets the value you gave. Or for that key, if there's something there, you apply the merge
965920	972000	function of this lattice. So that is itself a lattice. And these lattice constructors are very
972000	976400	nice. We use map lattice. You see lexical pair over there. And these allow you to take simple
976400	980560	lattices like sets and counters and build up richer lattices out of them, which is a trick
980560	985600	our group likes to play a lot. Other groups sort of are doing research on inventing custom lattices
985600	989520	for custom problems. We've been very much in this kind of know let's just build it up from very
989520	999440	simple building blocks. So the quick version is it's monotone. And if it's monotone, the column
999440	1003360	theorem says it's going to be free replication. So you don't have to do coordination to get
1003360	1007920	replicated consistency. Connor's going to give you a longer talk about this when we get to
1008960	1012880	conversation about semi-lattice. It's a semi-lattice. I should be clear. It's a semi-lattice.
1013760	1020240	Do you know whether the people working on coordination free replicated data structures
1020240	1025280	are well-pure? Yes, they are aware. And Connor will talk about it soon. Yeah, that will come up for
1025280	1031840	sure. Good. So just to kind of close out this anecdote with Anna, the system is ridiculously
1031840	1036720	fast. And it's especially ridiculously fast under contention relative to other systems.
1036720	1040560	So we compared against things like mass tree, which is from Harvard. It's 80 colors,
1040560	1044320	very fast key value store. We also compared against the multi-threaded
1044320	1048400	hash table that comes from Intel in their thread building blocks library. That's TBB.
1048400	1053360	And under contention, those systems, if you look down here, spend most of their time trying and
1053360	1058160	failing to get atomic instructions. So they'll say test and set on a particular memory address,
1058160	1062800	and they'll be told no, you have to try again. And they'll spend 95% of their time under contention
1062800	1067680	doing that, not doing useful work. So they're at 5% good put, if you're familiar with that term.
1068400	1072000	Whereas Anna, because it does no concurrency control, is just doing puts and gets and puts
1072000	1076800	and gets and puts and gets and spending most of its time doing good put. And that's why Anna can be
1077360	1081200	700x better throughput under high contention than these other systems.
1082160	1086880	But also because it does no coordination scales almost perfectly linearly across threads,
1086880	1092400	and then across machines, and eventually across the globe. There's really nothing to keep it from
1092400	1096800	scaling linearly, because the only extra work it has to do is some gossip. And that can be done
1096800	1100800	in the background, and can be done as lazily as you like without breaking the semantics.
1101440	1104720	So there's maybe a little fudging here on how stale your data is, but it's correct.
1106640	1112160	So this was a crazy fast system. And the thing about this, oh, and if you try to run it in the
1112160	1115840	cloud, it's also incredibly cheap to run relative to systems that are wasting all their time doing
1115840	1120560	this business. They're charging you for this. They're trying to get locks, they're waiting on
1120560	1125920	locks, and they're charging you money. So you'd like to avoid that if you can. Okay, that's all
1125920	1131040	very nice. But it was written in C++ by Chenggang, who's an excellent coder. His implementation is
1131040	1136320	correct by assertion. It would be really nice to be able to kind of do what CAD wants us to do,
1136320	1140000	formalize a spec that is correct, and then synthesize an implementation from it through
1140000	1144320	rules that are correct transformations. So we'd really like to do that, and we'd like to maintain
1144320	1150320	the speed. What kind of formalisms? Well, we're using lattices mostly. So maybe we could have a
1150320	1155520	type system that starts with some basic semilattices, like sets and counters, some composition
1155520	1160560	lattices, like key value pairs, products, lexical products, which aren't always lattices. So you
1160560	1165040	have to, there's some constraints on whether a lexical product is a lattice. And then we want
1165040	1170640	like a data flow, like a query plan algebra. So you can imagine a semi-ring kind of algebra, but
1170640	1174480	you know, there's going to be maps and folds, and then there's going to be physical stuff,
1174480	1178800	like scan a collection, or get stuff over a network. You know, networks do weird things,
1178800	1183840	like they permit things, and they form batches of things. They parenthesize streams, if you will.
1184800	1188720	They multiplex and de-multiplex messages. So there's some physical stuff that we want here too,
1188720	1192960	and I'd like to really be able to prove all that stuff is correct in a meaningful way.
1194320	1198960	So just for fun, I don't expect you to read this. This is the ANA implementation. You just saw
1198960	1203520	written in our low-level hydroflow language. This is the whole thing. It's a very simple program.
1204720	1209040	And you can see this is kind of a data flow language. It's basically specifying graphs of
1209040	1213920	data flow. The edges are directed edges in a graph. The words are nodes in the graph,
1213920	1221360	and you'll see familiar operators like map and join, and cross-join, and so on.
1221360	1226160	All right, and you can give views names. So this is a little name of a subgraph,
1226800	1231040	and we use it, and so on. So it's just a little language for specifying graphs.
1231040	1237680	This is a picture of that program that's output by the system. Okay, so it's a one piece of paper
1238240	1246080	program. So in this particular program, their union is actually joined, semi-latest join,
1247440	1255360	and that might be the only one. Yeah. Okay, and so just to convince ourselves this is fast,
1255360	1261760	that's Chengang's original numbers. We have it now running through hydro, that implementation you saw
1261760	1267120	on very similar machines, and we get very similar performance to the handwritten code. So we're
1267120	1274000	feeling pretty good that we're hitting our goals for performance. And because this graph is green,
1274000	1277920	it's telling us that this thing is all monotone, and therefore consistently replicable.
1278720	1283280	And at a glance, we can see this is a safe program. And I'm sort of cheating at this point,
1283280	1287920	and I'm going to confess to that. There's, I think, more work we need to do to make this robust. I
1287920	1293120	think these green edges are kind of a, they're slightly bi-assertion at this point. So I would
1293120	1297600	like to make them more fundamentally correct, and hopefully we'll have time to talk about that later.
1298720	1302880	Okay. With that, I'm going to hand off to Connor. He's going to take us through the next chapter.
1306880	1314160	Hello. People hear me? Yeah. I'm Connor. I'm a PhD student here working on hydro. I like systems
1314160	1317440	and theories. So I thought I'd show you guys some of the theory stuff we've been thinking about,
1317440	1319840	see if anyone has any thoughts, wants to collaborate on anything.
1324960	1330240	Okay. So in the classical database lens, we have, you know, these three layers,
1330240	1335680	the relational calculus at the top, relational algebra in the middle, and then a physical
1335680	1341200	algebra at the bottom, concerned with things like hashing and sorting and so on. And we can think
1341200	1346000	about how this changes when we move to the cloud setting. And there's good news and bad news on
1346000	1350080	the current state of affairs when we move to the cloud setting. The good news is that,
1350080	1353760	like Joe said at the top, we have this Daedalus language from the Bloom project
1353760	1360160	that is a data log like dialect for distributed systems. The bad news is that developers are
1360160	1365120	not asking for a data log dialect to build distributed systems in. The developers we've
1365120	1368640	talked to are a lot more interested in a functional algebraic looking interface and
1368640	1375520	especially something Pythonic looking like pandas. On the algebra side, the good news is that there
1375520	1379840	is an algebraic model for distributed systems today. It's the semi-ladis model that Joe has
1379840	1384560	mentioned that is referred to as CRDTs in a lot of places, especially in the programming languages
1384560	1390000	community. The bad news is that this is a model for coordination-free updates of state and it
1390000	1394880	doesn't actually have a query language or give guarantees about coordination-free-ness of queries
1394880	1401680	today. And then at the physical layer, when we add a network to the situation, asynchronous
1401680	1405680	computer network, a lot of non-determinism emerges that we need to be able to handle,
1405680	1409600	in particular reordering, batching, and duplication of messages.
1413040	1419200	So what we'd like to get to is unifying formalism across logic and algebra and this
1419200	1425920	physical algebra and have correctness at the physical layer that we can prove for
1425920	1432560	safe against this non-determinism from the network. And we're able to capture things like
1432560	1437920	replication, partitioning, batching, incrementalization, and termination analysis. We'll talk about
1437920	1448080	more later. All right, so let's talk about semi-ladis' CRDTs. So this is a model for
1448080	1452000	distributed systems that in databases we usually call the semi-ladis model. It's what is called an
1452000	1457040	Anna in Blum L. It came out of the programming languages community and there it's usually
1457040	1463680	referred to as CRDTs. It stands for conflict-free replicated data types. It's introduced in this
1463680	1471520	paper here and there's over 179 papers about CRDTs out there. It's also started to get popular
1471520	1476400	amongst software engineers and you see people talk about this CRDT model on places like Hacker News,
1476400	1482880	people starting startups with it. So what does it do? It tries to handle the sources of
1482880	1488640	non-determinism that come from an asynchronous computer network. These are the arbitrary batching
1488640	1496320	of messages, arbitrary reordering of messages, and arbitrary duplication of messages. And so it
1496320	1500240	turns out if you want to be robust to these three things, these actually correspond to algebraic
1500240	1505120	properties that you need to give you that robustness. So associativity gives you robustness to batching.
1505120	1509680	You're indifferent to the parenthesization of messages. Communicativity gives you robustness
1509680	1515600	to reordering and idempotence gives you robustness to duplication. And if you have a set with an
1515600	1519520	operator that satisfies these three properties, that gives you a semi-ladis. So that's why we're
1519520	1527280	talking about semi-ladises for distributed systems. So the conflict-free replicated data type is one
1527280	1532160	specific model of a semi-ladis interface, but since it's the most popular one today,
1532240	1538560	I'm going to talk about it. So the idea is that it's an object-oriented view of a distributed
1538560	1542080	system where you define some object, and you're going to define three methods on that object.
1542720	1545760	And then you can replicate this object across the distributed system,
1545760	1551840	and the replicas will converge, regardless of network non-determinism. So you have your merge
1551840	1558560	operator, which is your associative, commutative, and idempotent ACI semi-ladis operator, which
1558560	1564960	combines the state of two replicas. You have a way to update state, which the requirement is just
1564960	1569760	that that's monotone with respect to the partial order induced by this merge operation. And then
1569760	1574960	you have a query, which is just a method on this object, but today there's not a specific query
1574960	1579440	language. You don't have any sort of guarantees on what that query does. It just reads this semi-ladis
1579440	1587200	state. So we're looking at an example of a CRDT. This comes from the Amazon Dynamo paper for how
1587200	1592720	they're implementing shopping carts. And the idea is that you have updates that are going to add or
1592720	1596560	remove elements to your shopping cart. In this case, you add a Ferrari to your shopping cart, add a
1596560	1601920	potato, and you can also remove the Ferrari. And the state is going to be represented as two sets,
1601920	1608080	a set of items that you've inserted and a set of items that you've removed. And then to merge,
1608080	1612720	we do a pairwise union of these two sets. And the query, what's actually in my shopping cart I want
1612720	1619840	to check out, is set difference. Subtract the removes from the inserts. And there's a few
1619840	1625200	interesting things going on with this example. One is we're guaranteeing the coordination-free
1627200	1633520	rights that these two states are going to converge. But our query is a non-monotone query,
1633520	1638160	which the column theorem tells us is not a coordination-free query. So CRDTs are not giving
1638160	1643120	us the invariant that the column theorem requires on queries, which is that if we output a tuple
1643120	1648160	at a certain point in time, we're never going to retract that tuple in the future. Here, over time,
1648160	1656080	we will retract tuples as the remove set grows. We had a vision paper in BLDB this year about
1656080	1661280	this gap between what CRDTs guarantee and what the column theorem guarantees and ideas for how to
1661280	1667760	resolve it. Another thing you might have noticed that's kind of odd about that representation
1667760	1674320	of data is that if you think about how we might represent updates like this to a shopping cart
1674320	1679200	in a database, you might have imagined that we would have a count on each item and we would
1679200	1682720	increment that count when we add an item and we decrement that count when we remove an item.
1682720	1686720	This is what you'd see, something like incremental view maintenance where your update
1686720	1692400	operation forms an abelian group, not a semi-ladis. So why not do something like that?
1693280	1699520	Well, for one, it doesn't form a semi-ladis and it's not immediately obvious how to convert it
1699520	1705760	into one. So this representation is two sets, what you call a two-phase set. It's more obviously
1705760	1712560	monotonously growing update operation, but it turns out it actually is possible to convert
1712560	1718400	this abelian group representation into a valid semi-ladis in terms of being robust to network
1718400	1723760	non-determinism. I won't go into all the details on that, but it's based on what Joe is saying with
1723760	1732000	these vector clocks where you wrap the states basically in a vector clock which forms a semi-ladis.
1733200	1737680	The downside of doing this is that vector clocks require linear memory and the number of replicas
1737680	1742720	in the system, so that's the reason why people wouldn't use this representation today. But we
1742720	1750480	have some work on a protocol for enforcing this kind of conversion into a semi-ladis in
1750480	1754640	constant rather than linear space. So if anyone's interested in that idea, definitely come find me
1754640	1765600	and talk about it. Okay, so like I said, the semi-ladis model today does not have a query
1765600	1770800	language on top of it. So what might we want out of a query language for the semi-ladis model?
1770800	1776640	Well, we want expressivity. Like we saw in the shopping cart example, we need set difference,
1776640	1784080	so we need negation. Also recursion, something like datalog. We also want obviously classical
1784080	1788560	query optimization options. We want identities that we can use to transform our query,
1788560	1793520	get better performance. And we want to be able to do monotonicity analysis as well as functional
1793520	1802080	and dependency analysis for partitioning. And so something like datalog for semi-ladises
1802960	1806960	might be a good fit here, but there's a lot to explore. And so now Joe is going to talk about
1807520	1809840	this monotonicity analysis and functional dependency analysis.
1816560	1821280	I should say from the previous slide that some of this is things we took a crack at with
1821280	1825840	BlueMal, so it's not that we've done nothing here. There's some answers to these questions,
1825840	1831520	but there's also work to be done. All right, so I wanted to step back and review for you folks,
1831520	1837360	the COM Theorem, which I know is sort of in a sub-corner of the pods community and not everyone's
1837360	1841360	going to be familiar with it, but I think it's useful to go over. This will be high level,
1841360	1847200	but hopefully helpful enough for you to get into the game. So the challenge is that we're going to
1847200	1852800	have computers spread across the globe and we want our replicas to be consistent. So we have
1852800	1857920	this nice couple here, they're in different places, and the classic example of replica consistency
1857920	1861760	is data replication. So forget about programs, we're just going to have data, kind of like the
1861760	1867120	CRDT model. And I want everybody to have common beliefs about the data, at least eventually.
1867840	1872160	So these two folks currently both believe that X is love, which is lovely, but if it's a beautiful
1872160	1877600	variable, things could change, right? And that's very sad. And once they disagree on the value,
1877600	1881600	they might make decisions based on their disagreement that will lead to further
1881600	1885920	divergence. This is sometimes called the split brain problem, because you can't put it back
1885920	1892880	together later on, it's too messy. And so we want to generalize the idea of consistency of data
1892880	1898160	replication to consistency of program outcomes. So I'm not just interested in the data, I'm interested
1898160	1903520	in the queries, if you will, right? Much more powerful, and it will allow us to cheat sometimes,
1903520	1908320	the data could be inconsistent if the query outcomes are not, right? So it'll give us more
1908320	1914320	ability to relax our coordination. So we'd like to generalize this to program outcomes independent
1914320	1920400	of data races when we can. The classical solution to this stuff is coordination. This is what things
1920400	1925440	like Paxos and Two-Phase commit were invented to solve. And the way they solve it is by saying,
1925520	1929840	what if we were just on one computer with one processor? Maybe we could implement that in a
1929840	1934640	distributed fashion, which is a very heavy handed solution, right? You say that our solution to
1934640	1938640	parallelism is to remove it. And how can we remove it in the distributed context? Well,
1938640	1944320	it's expensive. But here's how it goes, right? On a single node, you use atomic instructions,
1944320	1948320	right? So if you have shared memory, you can use atomic instructions, or maybe you use a locking
1948320	1952240	system. In the distributed environment, you use something like Paxos or Two-Phase commit. And
1952320	1957920	at every scale, as you saw in that ANA work, you'd like to not do these things. So even on a single
1957920	1961520	machine, you really don't want to be doing coordination. And certainly in the distributed
1961520	1965920	setting, this is very heavy weight. And there's people who will tell you at great length why
1965920	1969680	they don't let the developers in Amazon call these libraries unless they have, you know,
1969680	1974480	16 gold stars. Because it will slow down the whole environment and create queue backups and all
1974480	1979440	kinds of horrible things. So when can we avoid coordination? This was a question that I asked
1979440	1983600	as a lazy professor, because I was thinking maybe I should learn and teach Paxos, and I kind of
1983600	1987680	didn't want to. So I was like, maybe, you know, maybe we don't need this stuff. Maybe Lamport's
1987680	1992720	just a bunch of bunk. So that's kind of where this started, sheer laziness, intellectual laziness,
1992720	1997600	which I will cop to. But what it led to, sometimes when you ask a question about how can I be lazy,
1997600	2001600	you end up asking a question that turns out to be quite interesting. I think that's what arose here.
2001600	2006240	And I'm seeing this not only in my work, but in other places. Back in the 20th century, if you
2006240	2011200	will, the Lamport Gray era, we were trying to emulate sequential computation. We were doing
2011200	2015440	everything we could to give the programmer the illusion of a sequential computer. And it was all
2015440	2020240	about, you know, very low level stuff, reads and writes, accesses and stores, right? And then
2021120	2025920	guarantees of order, total order, linearizability and serializability. And this was all based on
2025920	2030480	the idea that programmers are hopeless. They'll write all kinds of crazy code. And the only thing
2030480	2034080	they understand is sequential computers. So we'll make the worst case assumption that their stuff
2034080	2039440	wouldn't work in parallel, right? And we'll give them mechanisms for avoiding parallelism.
2039440	2044400	Seems like a terrible thing to do in a parallel environment. Yeah. So what's happening in the
2044400	2049600	21st century is if we lift our, so this is all great. And sometimes you need it. I don't mean
2049600	2053120	to denigrate the work. This is obviously foundational, touring awards. I use this stuff. I teach this
2053120	2057440	stuff. It's all good. But when we don't need to use it, even better, right? So people have tried
2057440	2060720	doing things like, what if all our states are mutable? That's a very functional programming game.
2061440	2065600	It was sort of in my world, it's more about, well, you can mutate things as long as it's
2065600	2069920	monotone. So if they're mutable, but they're monotone, maybe that'll work. And then using
2069920	2075680	things like dependencies and provenance, all our ways of using application knowledge to avoid using
2075680	2081120	the expensive stuff on the left. But the really big query is when do I need coordination and why
2081120	2087360	do I need coordination? So if you ask, you know, a typical undergraduate or frankly, most people in
2087360	2092560	computer science, including professors, when do you need coordination? What's a lock for, right?
2092560	2097920	They'll say, well, it's to avoid conflicts on shared resources, right? This intersection needs
2097920	2102880	coordination. If you would just put up some damn stop lights, right, then, you know, north, south
2102880	2107040	could go for a while and west, east, west would wait. And then east, west would go for a while,
2107040	2112960	north, south would wait, problem solved, right? But like, do I really need coordination? That's
2113040	2117760	a solution. Is it the only solution? No, it's not the only solution, right? Here's a coordination
2117760	2123120	free solution to that intersection problem, right? So I'd like to be able to think out of the box,
2123120	2128400	right, and say, really, what is coordination for? Why am I required to use it?
2130720	2137760	Okay. So that's a theory problem. So, you know, which programs have a coordination free implementation?
2137760	2142000	We call those the green programs. These are specifications for which a clever programmer
2142000	2146080	can find a coordination free solution. And then, of course, there's the rest of the programs,
2147440	2151440	right? And I want to know this green line. Will someone please tell me, you know,
2151440	2155760	Mr. Lamport, I think I only need you out here. So will you please tell me when I need you? And
2155760	2159360	there's no answer from Mr. Lamport. At least he didn't, you know, pick up the phone when I call.
2160400	2165600	But I'm happy to say that people at Hussalt did. And this is what led to the column theorem. So
2165600	2169680	this is really a computability question. What's the expressive power of languages without coordination?
2170240	2176720	Yeah. That's the green circle. Okay. So give you some intuition. Easy and hard questions.
2176720	2178880	Here's an easy question. Is anyone in the room over 18?
2181360	2187040	Excellent. Not only were you all happy to answer that coordination free, but you engaged in a
2187040	2190560	little protocol, right? You made up a protocol where you raise a hand if you think it's true. So
2190560	2194480	that was cool. So that was the monotone hand raising protocol or something. Great. All right.
2194480	2200480	Who's the youngest person in the room? Oh, we have some brave assertions. But clearly,
2200480	2205440	you don't know that. You could look at everyone, but that's cheating and also not necessarily
2205440	2212880	right. Maybe, maybe. I don't know. I don't know. But the point here is, right, that somehow this
2212880	2217360	requires you to communicate with people. And the first one maybe doesn't. Okay. More to the point.
2217360	2221520	Let's look at the logic here, right? This is an existential question. And this is a question with
2221520	2227680	the universal quantifier in it. Or for people like me who just want to do total pattern matching
2227680	2232720	and look for not symbols, that one appears to be positive. So I'll say that it's monotone.
2232720	2236640	And that one appears to be negative. So I'll say it's not monotone. So it gives you some intuition
2236640	2242400	that universal quantification or negation requires coordination. It is coordination. That's what
2242400	2248080	coordination is. It's universal quantification. So what is Lamport for? It's for universal quantifiers.
2248960	2253040	So let's just prove this, right? I was like, well, somebody prove it. I'm not going to prove it.
2253040	2258160	So nice guy named Tom Omelette wrote a thesis on this stuff. My conjecture was called the
2258160	2264400	calm conjecture consistency is logical monotonicity. It was in a Paz keynote that I was gave some years
2264400	2268720	ago. And then just a year later, there was a conference paper from the good folks at Haselt,
2268720	2274400	which then they extended and then was further extended with weaker definitions for the monotonicity
2274400	2279920	to really expand the results. If you want a quick, you know, kind of a version of what I'm saying now,
2279920	2283600	you can read this CACOM overview, but it's really for systems people. I think you guys should just
2283600	2289440	read Tom's papers. All right. To give you a flavor of what Tom did, definitions are half the battle.
2289440	2292800	It seems, you know, when I read Paz papers, that's all the hard parts are the definitions, right?
2293920	2298400	So, you know what monotonicity is in logic? That's fine. What is consistency here? Well,
2298400	2302800	we want the program to produce the same output regardless of where the initial data is placed.
2302800	2306720	So, I should be able to start the program with the data replicated pops possibly and
2306720	2312080	partitioned in any way and get the same answer. And if that's true, then it would be the same
2312080	2315360	answer across replicas. It would be the same answer across different runs. It would be the
2315360	2319600	same answer if we start gossiping the data between each other. And this is what we want,
2319600	2324000	right? So, that's our definition of consistency, where I think what's really clever and was the
2324000	2328960	most beautiful part of the work is defining what coordination really means. So, we're sending
2328960	2333840	messages around, right? That's data. But which data is really data and which data is kind of
2333840	2340240	control messages? And how do you differentiate those in a formal way? And so, what they define
2340240	2345760	in this paper is program is coordination free if there's some partitioning of the data when
2345760	2349840	you first start. So, there's some way out of the data where you first start, such that the query
2349840	2354560	can be answered without communication. So, for a particular query, for a particular data set,
2354640	2358640	there is some world of where you lay it out where no communication is required.
2359920	2365440	That's the definition of coordination freeness. And a program that you can't do that on is doing
2365440	2370960	coordination messages. So, it's not really saying which messages are coordination and which messages
2370960	2379360	are data, but it's telling you which programs can be run coordination. Yes. So, the trivial example
2379360	2383840	of this is you put all the data in one node. And again, you know, this question of is there anybody
2383840	2390400	who is older than me? What you don't know is whether anyone else has data. So, I may have all
2390400	2394080	the data, but I don't know that. So, I still have to ask everybody, anybody got any data?
2394080	2398560	And I have to wait for everybody to respond, right? So, it's a very nice intuition to just think
2398560	2406560	about having all the data. All right. There's another thing in the paper that I hadn't even
2406560	2411040	anticipated, which is really beautiful and speaks to stuff that the distributed systems
2411040	2415280	community kind of knows, which is there's a third equivalent, which is that you can
2415280	2421200	distributively compute this program on an oblivious transducer. And I haven't even talked
2421200	2425440	about transducers yet, but just a minute. But what does it mean by oblivious? It means that
2425440	2430240	the agent in the system doesn't know its own identity. It cannot distinguish messages from
2430240	2436880	itself from messages from anyone else. So, it doesn't actually know who itself is. And it
2436880	2442480	doesn't know the set of participants. We call this an oblivious agent, right? Oblivious programs
2442480	2446320	that can be computed by oblivious agents are exactly the monotone programs and exactly the
2446320	2450240	coordination free programs. So, that was very cool. And it speaks to questions of like
2450960	2455120	membership protocols in distributed systems, which is about establishing common knowledge
2455120	2458800	of the all relation. That's like one of the things that Paxos does is it has a membership
2458800	2463680	protocol built in. So, it's one of the reasons it's coordination full is to establish all.
2463680	2468960	So, this was really, really nice. So, this is all in this JACM paper. It's actually in the
2468960	2474640	conference, the Paz paper as well. That's just a flavor of the calm stuff. And I'm going to stop
2474640	2478480	with that. But happy to answer questions as best they can afterwards. And with that,
2478480	2479600	I'm going to give it back to Connor.
2480160	2494800	You guys can still hear me? All right. So, we have this calm theorem view of the world,
2494800	2501040	relational transducers, logic, operating over sets. And then we have this semi lattice
2501040	2506960	algebra view of the world. And they both are dealing with coordination and freeness in different
2506960	2512240	lenses. But currently, they guarantee different things. The algebra view, like we said, is
2512240	2515680	concerned with the coordination of freeness of rights and does not guarantee coordination
2515680	2519760	of freeness of queries. Whereas the calm theorem view is only concerned with the
2519760	2522400	coordination of freeness of queries. It actually doesn't have to worry about the
2522400	2526000	coordination of freeness of rights because it assumes operating over sets and gets
2526000	2531920	coordination of rights for free that way. And so, we're interested in the question of
2531920	2537360	how can we combine these two lenses? Can we do an algebraic view of the calm theorem?
2538560	2545280	And some intuition for how that might work is, you know, the semi lattice operator induces a
2545280	2551120	partial order. And so, instead of having monotone logical queries without negation, you have
2551120	2557600	monotone functions between lattices, between partial orders. So, that's something we've
2557600	2561840	been exploring. We'd love to chat more about it with folks. I'm actually going to talk about
2562640	2568640	a problem specific to the question Remy asked. It comes up in this setting.
2569360	2574640	So, the calm theorem is all about basically not knowing when you have the entire input. What can
2574640	2581360	I output and tell downstream consumers with certainty, even though I might have more updates
2581360	2586000	in the future, there might be more messages arriving. And so, we call the ability to do this free
2586000	2595680	termination without coordination. What can we be sure that we can output? And we're exploring
2595680	2601280	this in a very generic setting of just we have two functions, an operation that's going to change
2601280	2609520	our state over time in a query that's going to return some output. So, looking at an example
2609520	2614480	of when we might be able to do this, we can look at this lattice. This is the set over these four
2614480	2622400	socks and say that this is our state of a local node and we're at top in this lattice. And our
2622400	2631120	update operation in the CRDT sense is union. So, we're going to union in more socks. We know that
2631120	2637040	if we're at top, as updated as monotone, we'll never change our state. We're stuck at top.
2637920	2641840	And so, whatever query we might be asking when we're in this state, we'd be able to locally
2641840	2646000	detect that our query result is not going to change in the future and we'd be able to return
2646000	2652000	our result with certainty. This might sound like it would not happen particularly often,
2652000	2656240	but let's try and look at more examples where we would be able to figure out that with certainty,
2656240	2662960	we can return an answer right now. So, what if we consider also the query that we're asking
2662960	2669520	and say that this query is going to map us from this set socks lattice to the boolean lattice,
2669520	2677600	true false, where top is true. Now, if this query is monotone, meaning as we go up in the partial
2677600	2684640	order of socks, we also go up in the partial order of false and true, then we don't need to be at
2684640	2692080	top on the left. We can actually be at top in the true false lattice and guarantee that our result
2692080	2697040	won't change as future updates arrive. Any update that arrives is going to cause us to increase
2697040	2702000	monotonically in the domain, which has to increase monotonically in the range and therefore
2702000	2709600	our result will always stay true. For example, a query, is there a pair of socks?
2711120	2715520	So, we call a threshold query. It effectively draws a cut through this partial order and says
2715520	2719760	everything above this line in the partial order is true, everything below this line is false.
2721360	2725120	So, these boolean threshold queries are a class of queries that we can freely terminate
2725120	2733200	on if we know that our update is monotone. What about a totally different setting?
2733200	2738240	What if we throw away monotonicity? So, now imagine that we have a deterministic finite
2738240	2745040	automata and our query is mapping to true and false from accept and reject states in the automata
2745760	2749200	and our update is appending a character. So, we think of a streaming
2750000	2754960	character as appending. So, each update is going to transition us one step in this automata.
2758480	2765040	And in this automata, any state that we're in, we can't conclude what the final result will be
2765040	2770080	because from every state, there's a state that's accepting, that's reachable, and there's a state
2770080	2773520	that's rejecting, that's reachable. So, some sequence of future updates might take us to
2773520	2775680	false, some sequence of superstructures might take us to true.
2778400	2785040	In contrast, if state three were also true, now from state one, we actually don't know if we're
2785040	2789200	going to end up accepting or rejecting if we don't have the whole input yet. But if we're in states
2789200	2793760	two or three, we know that every state that's reachable via future updates is going to keep us
2793760	2800160	in our current result, which is true. And so, we can be certain that we can terminate here and return
2800160	2806080	true. This is kind of like a reachability sort of visual view of how we're thinking about whether
2806080	2810640	or not you can freely terminate given some arbitrary update operation on a domain and query operation
2810640	2816160	that maps you to a range. This is a question raised in exploring a lot of different domains. If
2816160	2821760	anyone has any ideas for what might connect to this, definitely come find us. Now, Joe is going to
2821760	2828560	talk about partitioning. All right, this is a bit of a survey time. Boris, did you want to ask a
2828560	2832960	question, Connor? I have a question to the last slide, right? Because in a sense, this now still
2832960	2838640	has some monotone ordering, right? This kind of like, in a sense, it's kind of like if I can reach a
2838640	2846240	node and it's smaller and now basically the nodes, if the terminal nodes are the top nodes and they
2846240	2851520	don't have larger nodes, then so it's still monotone in the sense. Yeah, you can find some sort of
2851600	2857760	it that, yeah. Yeah, I don't know if that's true for every freely terminating function.
2858480	2872240	Psycho, do you think? Yeah, maybe. There's something about quotient lattice is, too, going at it in
2872240	2879040	partial orders. Yeah, the graph looks like one. So this is love to have this conversation afterwards.
2879040	2884240	That's why we're touching on a few things so we can have many conversations. So I think given time,
2884240	2888400	I'm not going to go through this in any detail. I'm going to basically skip this chapter of the
2888400	2893600	talk, except to quickly give some assertions. So first of all, we don't have HydroFlow, so we
2893600	2898960	use Daedalus and we do have a full Daedalus to HydroFlow compiler. So we're able to write global
2898960	2904160	programs in Daedalus and then auto partition and auto replicate them. And that's work being led by
2904160	2909200	David Chu, who's in the room over here. David, three years ago, promised to do this and they
2909200	2914560	gave him a certificate saying that's cool. So he won the SSP student research award. And three years
2914560	2919200	later, he's got his first results. So it took a while. This was not an easy problem, but he's able
2919200	2927440	to take arbitrary Daedalus programs, which are Daedalug, and partition them to run on multiple
2927440	2931760	machines. And I'm really not going to spend a lot of time on this. What I'll say is that
2932720	2938320	earlier student Michael Whitaker, who again did this by assertion, he found all sorts of opportunities
2938320	2943520	to optimize Paxos because inside of Paxos, it was bottlenecking on things that were trivially
2943520	2948720	parallelizable, like network message handling. So he's like, if I can just bust apart some of the
2948720	2954160	roles in Paxos into sub roles, some of those sub roles can be replicated. And he got state-of-the-art
2954160	2959280	performance in terms of throughput on Paxos by doing this. And what I observed after he did it
2959280	2963920	was, oh my gosh, most of the things that you've split out are monotone subcomponents. And I should
2963920	2968480	have known that we could pull those out and replicate those. In fact, I wish Bloom could do
2968480	2972720	that automatically, but it couldn't. So three years later, David can now automatically pull out
2972720	2979360	these things that Michael was observing and transform the program to do it. And the ideas are
2979360	2987520	basically just two tricks. One trick is to take a pipeline on a single machine and split it across
2987520	2992720	two machines. He calls that decoupling. Now in a general purpose program, this is taking,
2992720	2996400	I don't know, 10,000 lines of C++ and figuring out which ones to run on this machine and which
2996400	3000400	ones to run on that machine. That would be horrible, right? But in a data flow language or
3000400	3004320	logic language, it's quite nice. And so he has conditions for when this is safe and when it's
3004320	3008480	not. So that's decoupling. So you can think of this as refactoring a program into components
3008480	3013040	that can be run on different machines with asynchronous communication. The other thing
3013040	3018720	he does is what's sometimes called sharding in the practitioner community, but it's partitioning,
3018720	3024240	shared nothing partitioning of subplants, right? So instead of having BC take all of the data from A,
3024240	3029040	you have a hash partitioning here and certain values go to each machine. And how do you know
3029040	3032560	that each one of these computations can be done independently? That's done through things like
3034160	3039120	functional dependency analysis so that you can show that certain values have no dependency on
3039120	3043840	other values because they're partitioned by, say, NFD. So I'm not going to go into any of this,
3043840	3047680	but basically what David was able to do was take many of the optimizations here that were
3047680	3053280	handwritten in Scala and automate them and formalize their correctness. And without getting
3053280	3058160	into too much detail, although it is kind of fun, oh, and we borrowed some work from Paris. So
3058160	3068960	shout out to Paris for parallel disjoint correctness and colleagues. It is really fast. So he was
3069200	3074080	able automatically. This is Michael's results that we re-ran. This is Scala. This is throughput
3074080	3078080	against latency. So what you want to do is you get as much throughput as you can until it starts
3078080	3082560	to hurt you at latency and it curls back. So this is kind of where things start to
3085040	3090160	top out, if you will. So that's Whitaker's implementation. This is the same logic as
3090160	3094720	Whitaker's implementation in Hydro. So this is just Hydro is faster than Scala written by hand.
3094800	3100080	So this is just a testament to the folks who wrote the Hydro flow engine. But the blue line is
3100080	3106000	what David achieved through systematic correctly proven rewrites. So he was able to get performance
3106000	3111280	that actually, because Hydro is fast, is better than Whitaker's Paxos implementation. And this gap
3111280	3114880	is kind of what he's given up. These are the tricks that Michael had that we didn't cover in our
3114880	3122960	rewrites. But we're doing 90% with the easy tricks. So it gives me confidence that simple
3123040	3127920	query optimizations known in parallel databases can have impacts on places that we're really very
3127920	3132480	fine tuned performance issues that people write PhDs to get this kind of performance and we're
3132480	3138240	getting it through systematic rewrites. Very promising. David's only halfway there though,
3138240	3142080	because he has to have a proper cost-based optimizer. Right now what he has is correct
3142080	3147040	transformation rules. He needs a cost model with an objective function. And then he needs a search
3147040	3153680	strategy. And we're hopefully going to be using egg log or some implementation of egg log in Hydro
3153680	3159040	flow to achieve this. So we're one of the things with Maxis stuff that overlaps is if there's a
3159040	3166080	lovely semi lattice based data flow implementation of Maxis stuff, maybe we can clean up some of the
3166080	3174080	things where he's doing updates in place. This work? This work. Well, so this was written in
3174080	3179680	Datalog and translated down into that Hydro flow data flow language you saw at the top. This stuff
3179680	3186240	is also written in Datalog currently in a runtime that plays some ad-hoc tricks. That's not traditional
3186240	3193040	Datalog execution here as Maxis. But I think, you know, Union Find is a nice target for an
3193040	3198960	algebraic treatment and I think we have opportunity. Okay, what I'd like to do in the last few minutes
3198960	3203440	is berate you with questions because these are the things that I don't know how to answer yet and I
3203440	3207840	would love to get help with. So the first is, and this is an outline, so this section goes on for
3207840	3211760	many slides, but there's the four questions. Can we please have one theory for all this nonsense
3211760	3217280	instead of the list I'm about to show you? What would be a good type system for the physical
3217280	3223040	layer where we could prove correctness of transformations? I have a follow on to Sudipa
3223040	3229440	about the role of our kinds of work in the era of generative AI. And then I have this ongoing
3229440	3235280	question of what time is for, which I probably don't have time to explain. But quickly, you know,
3235280	3240400	the unifying theory thing. So CRD teaser semi-ladyses, Datalog, Daedalus was all done with model
3240400	3246640	theory and it's fancy actually. It uses like stable models and stuff. It's actually ended up
3246640	3250960	being kind of fancy. The column theorem, Amalut, proves have these relational transducers, which
3250960	3256240	are this halfway world between operational and declarative semantics. You have these state
3256240	3262000	machines on each node. They run declarative languages on each step, but then they output stuff and
3266720	3268960	I think you can write non-terminating programs if you want to.
3270560	3276880	So you can write Toggle, for example, and Daedalus and the transducers.
3279440	3283920	Now they don't have to be terminated. In any sense, I don't think. But the point is,
3283920	3287600	I really wish he'd have done this work with this, because he also was on this work, but he didn't.
3287600	3292240	He did it with transducers, which is a bummer. If you talk to distributed systems people, they
3292240	3295920	talk about essentially order theory. They talk about partial orders all the time, which is related
3295920	3301600	to lattices, but you know, it's annoying. Programmers want to write these sort of algebraic functional
3301600	3306880	expressions, which I think is a good thing for all of us. And then yeah, I give all these talks
3306880	3311680	and then some joker raises their hand and says, well, what about transactions? And in fact, Peter
3311680	3315680	Bayless, when he was a student, basically did an end run around my entire group and just wrote
3315680	3320880	papers about transactions and coordination, and they don't align with the rest of this stuff.
3320880	3325600	So it's an open challenge to reintegrate that work. And then, you know, I didn't actually say the S
3325600	3331760	word yet, because I apparently didn't do joins as of yet. But we do do joins, so we probably need.
3332640	3338080	So it would be really great to get all of it here. I would like to bring all of this work
3338080	3344640	into this domain. That would be really nice. Okay. Here's a flavor of what I'm dealing with,
3344640	3351200	though. So just finished reading the DBSP paper, which was very nice and related to our work, but
3351200	3356640	we have some other things we need to keep track of that are relating to the network messing with
3356640	3362480	stuff. So when we look at a stream, it's got some values. It's got some happenstance ordering,
3362480	3367600	that's a mapping of the naturals to those values. It's got some happenstance batching. It came in
3367600	3372560	over the network in chunks. So there's like singly nested parentheses in this stream that are
3372560	3378160	randomly placed. Randomly, you know, arbitrarily ordered, arbitrarily placed. Maybe this is a
3378160	3382960	stream of lattice points, but maybe it's not. I don't know. But if it is, you could say things
3382960	3387280	like there's a monotonicity relationship between the type's natural order and the total order of
3387280	3394560	arrival or not, right? And then what sort does is it enforces something like this, right? It's
3394560	3399120	nice when these are atomistic, like data flow is basically a set lattice that you flow individual
3399120	3403680	atoms. That's the traditional kind of database iterator thing, one tuple at a time, right?
3403680	3408720	One tuple at a time is an atomistic lattice of the sets of tuples. And it's nice when you know
3408720	3412880	you're dealing with atoms, because you can say things like item potents, right? I gave you
3412880	3416640	Bob's tuple once, I gave you Bob's tuple twice, sorry, but you should only have it once. So delete
3416640	3421360	one. But if I give you subsets, now you have to find how they overlap and you have to make sure
3421360	3425600	that when you union them, you remove the overlapping bits. And so when you have non-atomistic things,
3425600	3431680	it's just a little uglier. And you end up talking about like, does their meat, is there meat bot?
3431680	3435040	kinds of things. Do they have no intersection, right? So these are the kinds of properties
3435040	3440160	that I think I need to track in my rewrite rules. And then, you know, the operators are the invariants
3440160	3445040	to these properties, like lattice, lattice operations are invariants of order and parenthesization.
3445600	3449200	Do they preserve the properties? Do they enforce different values for the properties?
3449200	3455280	The network non-deterministically enforces orders and parenthesizations. So like this is the stuff
3455280	3459600	that I worry about in my operators. And this is kind of the soup I'm swimming in with this
3460400	3465200	physical algebra. So I would like help with this. All right, I'm going to do one more quick slide.
3465920	3469360	This is very much in the realm of what Sudipa was talking about. You know, we're in the era where
3469360	3474880	people will be programming with green goo, right? It's just this is large language models, they're
3474880	3479600	magical, they produce stuff. But what we really want is building blocks that we can count on,
3479600	3483360	right? We're a database people, our industry is all about foundational building blocks.
3483360	3487760	And I really do think declarative specification is a lovely narrow waste here between these two,
3487760	3493520	where we can take a formal spec as Cod asked us to, we can render it in some sense so that it's
3493520	3498480	readable, right? And this relates to things like Wolfgang's work on visualizing queries,
3498480	3502320	and what Sudipa was talking about in terms of giving natural language things, but helping
3502320	3506320	people look at this and say, is this what you meant? Not is it correct, but is this what you
3506320	3511760	meant since a spec after all, right? Did you mean this query? And then of course, if it's in a nice
3511760	3517920	formal language, we can check it for other things, right? And so that would be, I think, a role that
3517920	3523040	we can really play in the world. And I suspect things like this will happen. These programs are
3523040	3526720	going to be a selection of programs. You're constantly going to be given a menu of which of
3526720	3531680	these things did you mean. And the answer to which is either some invariant checks or something,
3531680	3538160	or some human judgment. So I think we're in a good spot in terms of intermediate languages.
3538160	3543040	And I'll just close with one more slide, maybe just a handful of slides.
3544080	3547600	What are clocks in time for in distributed systems? So there's this famous saying, which
3547600	3552080	is correctly attributed to a sci-fi short story. Time is what keeps everything from happening
3552080	3557360	all at once. So when should we use time and computing? What are clocks for? Well,
3558320	3563520	they're not for monotone queries. I can run this embarrassingly parallel. It can all happen at the
3563520	3571200	same time, and it's fine. And Buntalu was doing this long before this discussion. But I can't run
3571200	3576240	this at the same time. You can't have P and not P at the same time. So what's the deadliest answer
3576240	3583280	to that is, well, that's what time is for. This is the toggle program, right? And time is there to
3583280	3588160	separate two things that can't coexist. That should be, I think, the only reason for time.
3589760	3595120	Except it's not. Distributed systems, people use time for maintaining partial orders and knowing
3595120	3600320	when things were concurrent. Sometimes you don't need that. Sometimes you do. But this is my question,
3600320	3604960	especially because Val's in the room and worked on this DSP stuff. Daedalus has one clock per
3604960	3610960	node and we update it only when there's a network event or we have to cycle through negation.
3611920	3616560	Timely and differential data flow have clocks all over the damn place. And I'm not sure when you
3616560	3622960	use them and when you don't. So, for example, tracking iterations of a monotonic recursive
3622960	3626480	program. Why do I need a clock for that? I don't think I need a clock for that. Maybe if it's a
3626480	3632160	while and we use the index in our computation, I need to know what index I'm at. So the general
3632160	3638720	question is, when do we use this structure called a clock? And when don't we need? And can a compiler
3638720	3644960	decide? All right. We are a little over time. I hope we have given you lots of things to ask us
3644960	3651680	later. We need lots of help. So we'd love it. And we are big fans of working with folks like you.
3651680	3657680	Can you leave these last four slides there so we can come up with some more folk questions?
