{"text": " pro six circuits on tiny binary assets. So what we'll do today is actually train PCs on language, language models, and Juan-Wes going to tell us how that works and what that's useful for. So today I'll present our paper, Attractable Control for Autoregressive Language Generation. So unlike probably the interesting ones, today I'll focus more on the application side of probabilistic circuits. Like more machine learning oriented. Okay, so let's get started with the basic concept of large language models. They have been like really popular recently. I'm not sure if people here really care or not. Like I see like this interested face you do. Okay, that's a good sign. That's a good sign. So basically the idea is very simple. In the sense that you collect large amount of text consisting of trillions of words, and you train neural networks with billions of parameters on these data, then you have these so-called large language models. These chat GPT or GPT-4 thing has become like really popular. People talking to them on the internet, asking them to polish their papers, using them to do their homeworks, and even asking them to play D&D together. So yeah, that is actually true. So the idea is, so people have now having this feeling that we probably have solved AI, we have solved artificial intelligence, and chat GPT has passed Turing test, but is that really the case, okay? So I tried this a few months ago. It was an actual conversation between me and chat GPT. So I asked the model to generate a sentence using Frisbee, Cod, and Dog following the given order. Well, I mean, this should be quite simple for a super intelligent AI model, right? So it does give me a sentence. The sentence look quite decent, and all the key words are in there, but Dog and Cod are in the wrong order, right? Well, I mean, humans make mistakes, so super intelligent AI does. So I was being patient, I tried again, okay? Again, it gives me a sentence, but it's even worse now. All of the words are in the wrong order. So you see that chat GPT fails to follow even this simple logical constraint, okay? So people have been trying to fix this. Some of the methods, including like, okay, we can prompt them in a different way, like chain of thought. I'm not comfortable saying this phrase, but that's one of the methods. Or people try to use search algorithms trying to find the correct sentence in this huge search space, so on and so forth. But none of them actually guarantees that chat GPT or whatever other language model give us what we want. So what we actually want is like 100% guarantee. When we have like a huge powerful model, when we instructed to do something, we wanted to follow our instructions exactly. So in today's talk, I'll show you how we can do this with probabilistic circuits. Before some of the detail, so let's get started with some basics of language modeling. So language modeling is really not like a fancy idea. It's just a joined distribution over some text, probably less than or equal to n words. Each random variable here is a word taking value in a fixed, in a vocabulary of finite size. The size usually in practice ranges from 10,000 to 50,000 or something, okay? And here's some examples. You might notice there's some special word called EOS here, which means end of sentence, which is only like a special token or word we use to pad sentences of different length to the maximum length. So we have this like joined distribution well defined, okay? So we collect a lot of text of this form, sentence fragments, paragraph fragments, and we train this joined distribution by maximizing a lot of likelihood, a very, very simple idea. Okay, so what about the architecture of this distribution of our model? Well, like the most popular ones, like GPT, they are autoregressive models. Well, so by the chain rule of probability, we can decompose our joined distribution this way. Represented as a graphical model is basically for each random variable, we have arrows going from all the previous random variables. From a generative point of view, it's basically we start with, I can't see my cursor. So we start generating the first word from this prior distribution. Once we have the first word, we move on to the second one, conditioning on the first one, and then we generate the third one, conditioning on the first two words, very simple, okay? By some classic results, marginose for Bayesian networks with these structures is intractable. Okay, so before when we asked GPT to generate a sentence for us, we are like talking to it as if it's human, right? So the assumption is if our distribution over language is a perfect distribution, then we should be able to interact with it as a human. When we're doing these kind of like prompting, when we're talking to it, most of the time we are actually trying to do conditioning. So let's consider an even simpler example. Suppose in our autoregressive generation, we have generated the first three words, the weather is, and our constraint is that we want the sentence to contain the keyword winter. Well, I mean, we wanna write something with the topic of winter, so that's our constraint. What we really want is, okay, so what language model gives us is the next token, next word probability. So basically given the weather is, it might prefer whether it's warm or cold, while its prior distribution might want the weather to be warm. But what we actually want, but what we'll actually need is this conditional next word probability, right? We want our sentence to satisfy the constraint, once it contain the winter, and well, intuitively, if we want the winter to be in the sentence, weather is more likely to be cold, okay? But this is intractable for those autoregressive models. So why is it intractable? Let's go more into the details. So by Bayes' rule, this conditional probability can be decomposed into these two terms. The next term here is very simple, it's just a language model next word probability. But the first term here is actually a marginal probability. So it's basically the marginal probability over all possible suffixes that we could generate that contains winter, okay? So this is intractable for autoregressive models, but we have PCs, right? PCs are tractable, at least, we know how to compute marginal probabilities with them. So why don't we just approximate this term here with probabilistic circuits? And we refer to our pipeline as gelato, in generating language with tractable constraints. Okay, so how do we do this? So the first step is that we pick our favorite PC for sequential modeling, which is a hidden markup model, the simplest we can have. We have, on the other hand, we have our pre-trained language model we want to control or guide. We sample a lot of data from our language model unconditionally, and then we train our hidden markup model on these data with maximum likelihood estimation. So effectively minimizing the KL divergence between the two joint distributions, okay? And our assumption is that if the joint distributions are close enough, then all of the marginal distributions and conditional distributions are gonna be similar, okay? Any questions? So the intuition is that we have a black box autoregressive model, and we train some sort of white box PC, so we can use as a representative of the black box, okay? So we don't want to, since this is the workshop for circuits and logic, let's represent HMMs as PCs. So here is the graphical model version of PC. The Zs here are the hidden variables, the latent variables. And the X here are the observed variables, or the words. And let me use the whiteboard to show you how to do this. Okay, so one assumption we have is that our hidden states, our latent variables, are discrete variables taking values from one to H, and H is our hidden state. Any questions? Okay, so we start from the very beginning. In an HMM, we start with the initial state, Z1, and it has H choices. So we want these eight product nodes to be representing the probability, can people see it actually? So representing the probability of X1 to N conditioning on Z1 equals to I. So I would be the hidden state, so I will, this will be one, two, and H, okay? And the edge will have weights of the prior distribution on Z1. Okay, I see nodding, I see confusing faces, but I'll move on whatever. Okay, so it'll be clear later. So given, so by the Markov property, given Z1, X1, and so basically given Z1, this part and the remaining part will be independent. So let's deal with X1 first. We have some input distributions, and this input distribution will be representing the probability, the emission probability, basically X1 given Z1 equals to I. Okay, so basically here, we're in the states of Z1 equals to I, and this will represent the probability of X1 given at Z1 equals to I. I see more nodding faces now. Okay, and we want this part to describe the remaining distribution, so we have some nodes here, and they will represent the distribution of P of X2 to N given Z1 equals to I, okay? Corresponding to the inputs. And so we proceed to the next layer. And this part will be the transition probabilities, will be the transition probability of Z2 equals to J given Z1 equals to I, and these will recursively, we do this construction recursively, will be representing the probability from X2 to N, conditioning on Z2 equals to I, okay? Does that make sense? So we just proceed, so it's like Antonio and Robert was trying to make the point, this is a tensorized layer representation of a PC. Okay, let's move on, okay, so now we have our, oh, sorry. So let's move on, so we have our circuit representing the distribution over text, then we need to answer the query, we need to encode the logical constraint as the logical circuit. Let's go back to the constraint we have in the very beginning, we want Frisbee, dog and dog. Frisbee, caught and dog to appear in the given order. This is like a naive way to represent this constraint, basically the IJK here are enumerating all possible positions of these three words, and we take a conjunction whenever we know the positions. Any questions? Okay, okay, but this is not really ideal, we can directly convert it into a PC that represents the constraint, but what are the problems, do people see that? I'll give a hint, complexity, what's the complexity of this, what's the size of this DNF? Yes, cubic, cubic, and to be more precise, it's n choose number of keywords, right? Well, I mean, we can do it like cubic, but suppose we have five or 10 keywords, we can no longer take that complexity, okay? And the other problem is more subtle, which is that this DNF is not deterministic, okay? So we know that we can multiply circuits when they are compatible, or when they are structured decomposable with respect to the same vitri, but here we want exact conditioning, so we actually need to make sure that our circuit represents a uniform distribution over the support specified by this DNF. DNF, and in general, it is sharply hard to do model counting to normalize everything. Okay, I'll go into the details on the board. Yes, yes, I'll talk about it on the board. Okay, I'll move to the other side. Okay, so for the first question, why does it need to be deterministic? It is because so, okay, suppose we have a very, okay, so suppose we have a very simple distribution, X1, X2. So, we have some weights, so this is a distribution over two random variables, okay, and this is its probability mass function. So when we are conditioning, we are actually selecting all the terms that satisfy our constraint and zeroing out everything else, right? So if we want to do that with circuit multiplication, suppose our constraint, our support, would be something like X1, X2, and X1 bar, X2, okay? So the constraint circuit should be a uniform distribution over its support, otherwise it messes up with the original weights, right? Does that make sense? Okay, so however, okay, let me, so this will be 0.5, X1, X2 plus 0.5, X1 bar. Well, I mean, suppose, and we have a bunch of zeros, and we multiply these two circuits point-wise, and we kind, we keep these two terms and erase these two terms, right? But we want W and W3 to be proportional to each other, like the ratio should stay the same. So this circuit must be a uniform distribution over the support of constraint, okay? So given a logical circuit, how do we convert it into a PC that represents a uniform distribution over the support? To do that, we need to do model counting, but model counting in general is hard if determinism is missing. So this one won't work. Does that make sense? Yes, no? Robert? Isn't that full of a new mistake? Oh yeah, so it's a very subtle, subtle thing. So, well, we only require Frisbee, Caught, and Dog to appear in some positions, but we do not limit that they have to appear exactly once. So you can totally have Frisbee, Frisbee, Caught, Caught, Dog, Dog, something like this. So we have two sub-sequences. We have a lot of sub-sequences, right? So this is position one, two, three, four, five, six. So when IJK are one, three, five, it satisfies the clauses, right? One of them, right? Let me choose a different way. Okay, so basically for this huge disjunction, for each instantiation to the variable, we only satisfy one of the clauses, right? We want, that's determinism. Yes. And suppose IJK are one, three, five. Does this instance satisfy that? Yes, it does, right? But when IJK are one, four, five, it also satisfies that. So it's not. Does that make sense? Okay. Okay, so now let's construct a deterministic circuit representing this constraint. The idea is similar to a deterministic finite automata. Basically, you track how many words have you included, how many words have you used, which state are you in satisfying the constraint? So we want to construct these sub formulas, V, T, say cod and dog. The sub formula representing the constraint that cod and dog appears in XT, XT plus one. So how do we construct this sub circuit or this sub formula? Oh, it's pretty simple. It's a sum node. We consider two different cases. One of the cases that XT is cod. And the other case is that XT is not cod. Does this make sense? Okay. So if XT is cod, then we have like step one. We have made one step towards satisfying the constraint. So we can reduce it to V, T plus one. Dog. And suppose XT is not cod. We're in the same state. We reduce it to T plus one. Dog. Is that clear? So suppose we have constructed FI for all time step greater than T then we can construct all the fees for T. So it's a recursive algorithm. Oh, what's the base case? Yes, that's very interesting. Okay, I'm gonna erase this. So just cod and dog in the part. So yeah, I mean, we can make it even simpler. I'm lazy, so I wanna make things simpler. So suppose in the nth position, we only have one word, right? So fee and, what is this formula? It means that from Xn to Xn, we need to have cod and dog. So this is false, right? Zero. What if we only have dog? So this is true only if Xn equals dog. So yeah, those are the base cases. Okay, so what's the size of the circuit? So at each time step, we need fee T, maybe we have already satisfied the constraint. So something empty, right? We need fee T, we have one word left. Fee T, we have cod and dog, we have two word left. And we have generated none of them yet. Okay, so at each time step, we have four sum notes or four more notes representing all these four cases. And the size, the number of notes in the circuit would be four times. Okay, does that make sense? And you can notice that when we are constructing this circuit, we are always conditioning on XT, current variable, which is very similar to HMM. It generates one variable at a time, one word at a time. So it is compatible with HMM. And also it's deterministic. And we can do the apply operation, the product operation layer-wise. So the size of the eventual circuit would be four, so originally, so originally, here for each layer, we have H nodes. Now we, here for each layer, we have four nodes. So eventually we have four H nodes just for each layer. Acceptable, okay, does that make sense? So we can still generate this. We can generate this even further. Well, some time, well, one simple generalization is, you might think it doesn't have to be caught, right? It can be caught or either catch, right? Or catches, they all kind of represent the same concept. So in the construction, we can modify our original circuit a little bit. Before it was XT equals caught, we can replace it with another OR node, enumerating caught, catch, and catches. Okay, and the circuit size stays roughly the same. And there are many other things we can do. We can also say, oh, we don't need them to be in the same order, in a fixed order. We can, we want them to be like in arbitrary order. We don't care about the order. How do we do that? So in this case, we have four states, right? Basically enumerating all suffixes of these three keywords. In that case, we would have all subsets of these three keywords. So in that way, the complexity would be going from four, we have three keywords, and this is four, two, two to the three. That's there, eight subsets, two to the three subsets, okay? So still acceptable, two to the n is not really, say we have 10 keywords, two to the n, it's just 1024. It's still doable in practice, yes. So it seems that you can do any kind of regular expression constrained, but you're focusing here on language as a fixed length, right? Because you have a fixed number of random variables. So is that the kind of equivalent of what you're saying? Yes, yes, that's a very, very good point. So fixed length here is a very essential assumption. So suppose we have a distribution over language of arbitrary length, then applying the constraint could be hard to define. So basically these three words, they can appear in arbitrarily far positions, and we need probably you take like an infinite sum to define this conditional probability. But this is not terrible in practice, because in practice, even for models like GPT, they have a finite sequence length, yeah, that's a very good point, okay, I'm trying to be fast. Oh, I mean, there are variations of constraints we have talked about, and okay, so now we have our, sorry, now we have our probabilistic circuit representing the distribution, and we have our constraint circuit, and we'll close them to take the product. So we have our constraint circuit now, we can compute the probabilities we need. So the step two is very simple. So this is the original like base decomposition of the conditional probability we have. So this term here is intractable, so we secretly replace the subscript of LM with HMM, the one we notice, and we define this conditional distribution, the gelato distribution, and we can compute this with the linear pass of the circuit. Okay, so what are the advantages? So number one, by definition, the constraint alpha is guaranteed to be satisfied, so finally, other than compared to all the other methods, we have 100% reliable thing we can trust, and number two is that the training of this HMM does not depend on the constraint. So basically once we have this HMM trained, we can use it to enforce whatever constraint we want. So maybe today I want to write something using some keywords and key concepts, and tomorrow I feel like this language model is like using a lot of inappropriate languages, and we can detoxify it by specifying a list of bad words that it cannot use, and all the same model, and all the constraints are only enforced at inference time. Yes? What kind of constraints can be represented as the composable pieces? Yes, that's a very, very good question. That's actually one of the main reason in presenting this work here, because my feeling is that though people have been studying like probabilistic queries like marginal map, marginal, and all these kinds of stuff extensively, but in practice, we really care about some more complicated, less generic ones, and whether is there a language to define or to describe their tractability is kind of missing from the literature. But I could relate this to say there's some work on compiling DFAs to circuits. I think that could be something as a starting point to look at. I'm not sure if that answers your question. I don't have an answer. My answer is, okay. Okay, so experiments and benchmarks. So we evaluate our method on this common sense generation, common gem benchmark. Well, it's very similar to the example I gave you. It gave you a bunch of keywords, and each example comes with a bunch of gold sentences, and you want to generate something that looks similar to the gold sentences using keywords. And here, it's like the most general case. So basically these keywords, they can appear in any order, and they can appear in any form of their inflections. Okay, so this is the, yes, then. And it's a gigantic table. The numbers themselves are not that important. So one thing to note that is compared to all the other baselines, our method, gelato, achieves a state-of-the-art performance with respect to basically all metrics. So these metrics here, Rouge L, Blue Four Ciders, Spice State, there are just some standard NLP metrics that people use to evaluate the quality of your text. So basically you have a gold sentence, and you have your generated sentence. They compute somehow the NBREM overlap to measure the quality. But the other thing is that all of the previous method cannot achieve 100% constraint satisfiability, but ours does in practice as well. Well, there is this one baseline, they achieve 100% accuracy as well, but they kind of did it by starting from the keywords. So they're always gonna be there. And you can see their generation quality is really poor, so. We also conducted some sort of human evaluation. Okay, I guess, yes? Which language model does that look like? Oh, the language model, we use GPT-2, GPT-2 large. Yeah, and all of the baseline state, they use GPT-2 large, yeah. And in case people don't really trust these automatic evaluation metrics, we also conducted human evaluation. And you can see that our model performs much better than previous state-of-the-art. Okay, well, are they very significant? I mean, they're pretty close. Yeah, so, yeah, so basically the, I'm not sure if you can see it clearly, but the bold-faced ones are statistically significantly, what equivalent? So these ones, these two are statistically equivalent. This one is statistically significant. And we're looking at, when defibrating, what's the number here? Oh, yeah, so basically we provide the annotator some sentence and we provide description of one of each of the aspects. So basically, concept means that does it use all the concepts naturally? And plausibility means that is the sentence like a plausible sentence describing a realistic scene. And quality is basically fluency, grammar, and stuff. Overall is the like another, how do you feel about the sentence? And the numbers are from one, two, three. One, two, three, go, yeah. Okay. Okay, so let's get back to the very first motivating example and you can see that gelato, we use our model to add, and it is actually able to get everything correct and generate a fluent sentence. And another, I don't, okay, so we also found this one in one of the generated candidates. A pair of Frisbee players are caught in a dog fight, which is not like the thing that most people would like to think of. So it also shows some sort of creativity here. Here, okay, that's my talk. Please ask questions if you have, and otherwise we can go to lunch. Thank you. Thank you. One more question. Thanks for your talk, if I understand right, you said that to generate every word, you have to do a linear pass over the circuit. So how much is this overhead compared to the kind of latency from the language model? Okay, I like that question. So we don't really need to take a pass over the whole circuit. With some caching, we can do like constant time. So basically to generate each word, the cost is like constant time. It's like a pass through one layer. Is it in any practice? How fast is it? Oh, how fast is it? We actually, so. I don't have a table here, but we have a table in the paper. So if say generating a sentence with five keywords, a GPT-2 large would take around 20 seconds, and our method is like 100-ish seconds. So it's not terrible. And one of the baselines here, well, which was actually like the best paper award at one of the top NLP conferences, they use search-based method. And for them to generate a sentence, they take like 700 seconds, 800 seconds. So because they're search-based, so when a search-based gets large, their method shows like a bottleneck. That's pretty well. Thanks. Can you also give an idea about the time required to derive the hidden Markov model? Oh, yeah, so I do have the time for that. So our hidden Markov model has like 4,000 states, and the emission is 50,000. We trained them with the juice framework that we developed in our lab. The training takes like 20 hours. So it's, we sample about 200, we sampled eight million sentences from GPT, and we trained them for 40 epochs, 20 hours. What's the number of parameters? Number of parameters? Of HMM. Yeah, so HMM is 4,000 hidden states and 50,000 emissions. So the main, mainly that the parameters are centered on the emission table. So it's like 40,000, 4,000 times 50,000. I didn't do that in my head. So those are unique parameters, right? Yes, yeah. Yes, so in the PC, yes, you kind of like, you've rolled out all the parameters. All the positions. Controlable generation is huge, right? So this is great. What, I mean, and I probably understand you, you gave a logical perspective for this audience, but I mean, a reg acts as a much more natural sort of control structure. So presumably you can handle any of our complication given the DFA interpretation, right? Yes, yes. I think that's a very important follow up. We are kind of looking at considering like, compiling DFA's to circuits and kind of automate all these process. Yeah, so ACLs in January, you submit there. I mean, I think they'll love it. Your initial example, you wanted a winter tree, you wanted the presence of winter to select for warm, right? Which is more of a natural language entailment. And I don't think you handle that, right? You handle variations and factual variations because you actually code them. I think we can totally do that winter example. So basically, let me go back. It seems like you encoded the specific variations that you were allowing, right? Oh, you mean like, so we can have winter, winters, but like, maybe if the winter, the word is not explicitly mentioned, you cannot do that. That's what I understand from your formalism, right? If it's in your or, you'll generate it. If it's not, you won't, you won't capture the entailment. No, I mean, so basically, so that's the other thing. So sometimes people, or most of the time, people want to have like kind of soft control. They can't really write their constraint in logical form. For example, toxic, like how can you tell a sentence is toxic? But there are many ways to approximate it. One of the ways is we have like a long list of phrases or words that you're now allowed to use in a toxic sentence and we can basically just write down a logical formula approximating that toxicity constraint. Yeah, I mean, there's also plug-and-play generation tricks that are actually quite interesting, I think that... So basically, there is a, of course, there's a naive way to do it. You can say whenever I encounter one of the words in the list, I just remote, like, prevent it from generating, prevent it from being sampled. But that's not exactly what we're trying to do probabilistically, right? So... Yeah, but I just, maybe look into plug-and-play, control generation is actually a paper. Yes, yeah. So where I think they're doing much more than that, right? Well, so, yeah, so it's like... They're also modifying the posterior selection. So if I'm not wrong, if I remember this correctly, they're basically trying to train some sort of... They use a classifier, right? Yeah, so basically, kind of, they're trying to train a newer model to approximate this. Yeah. So, but they're... So, yeah, so their methods has like two disadvantages compared to our advantages. So one of them is that they cannot guarantee that this is... Right. 100% logically satisfiable. And the other is that they have to retrain their model for all different constraints. Their model for all... Yes, okay, but... Okay, so suppose in their training data, they're only trained to kind of satisfy... They only have seen using, say, less than 10 keywords to generate a sentence, right? What if today I want to use 20 keywords? They would not be able to generalize well to that one. Right, so they're examples of toxicity, right? So they pretrain for toxicity and they can use that anywhere. So there's some tasks that you're just going to use repeatedly. I guess I see a combination of what you're doing and what they're doing together, which gives you this sort of entailment during the... Like this broader sense of satisfaction, right? Which I think was an interesting motivation. You didn't quite deliver on, but you could deliver on. That's actually the current project we're working on. So we're trying to combine the models that can handle soft constraints with our model, too. Okay, beautiful. And finally, the reason large types of models work so well is just because of attention, right? And so when you're using a hidden marker model, right, you're losing the power of attention. But I guess I'm looking at this and trying to convince myself that, well, you get all the attention in the right-hand part and then you just need a little bit of bias of selection in the left-hand part here, and that's what's happening. Yeah, so basically, intuitively, you can think of this part as only providing, like, guide and suggestions, leading the model, leading GPT to satisfy the constraint. But on this part, it's kind of responsible for fluency, grammar, and everything. Beautiful work. Thank you. All right, thanks. Maybe we should wrap up, and thanks again. And we can have lunch with the database folks and talk about generating SQL query.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.36, "text": " pro six circuits on tiny binary assets.", "tokens": [50364, 447, 2309, 26354, 322, 5870, 17434, 9769, 13, 50532], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 1, "seek": 0, "start": 3.36, "end": 6.88, "text": " So what we'll do today is actually train PCs", "tokens": [50532, 407, 437, 321, 603, 360, 965, 307, 767, 3847, 46913, 50708], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 2, "seek": 0, "start": 6.88, "end": 9.120000000000001, "text": " on language, language models,", "tokens": [50708, 322, 2856, 11, 2856, 5245, 11, 50820], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 3, "seek": 0, "start": 9.120000000000001, "end": 11.4, "text": " and Juan-Wes going to tell us how that works", "tokens": [50820, 293, 17064, 12, 54, 279, 516, 281, 980, 505, 577, 300, 1985, 50934], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 4, "seek": 0, "start": 11.4, "end": 12.8, "text": " and what that's useful for.", "tokens": [50934, 293, 437, 300, 311, 4420, 337, 13, 51004], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 5, "seek": 0, "start": 15.16, "end": 18.8, "text": " So today I'll present our paper,", "tokens": [51122, 407, 965, 286, 603, 1974, 527, 3035, 11, 51304], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 6, "seek": 0, "start": 18.8, "end": 22.2, "text": " Attractable Control for Autoregressive Language Generation.", "tokens": [51304, 7298, 1897, 712, 12912, 337, 6049, 418, 3091, 488, 24445, 23898, 13, 51474], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 7, "seek": 0, "start": 23.48, "end": 28.48, "text": " So unlike probably the interesting ones,", "tokens": [51538, 407, 8343, 1391, 264, 1880, 2306, 11, 51788], "temperature": 0.0, "avg_logprob": -0.29661614244634454, "compression_ratio": 1.5, "no_speech_prob": 0.03889339417219162}, {"id": 8, "seek": 2848, "start": 28.64, "end": 32.04, "text": " today I'll focus more on the application side", "tokens": [50372, 965, 286, 603, 1879, 544, 322, 264, 3861, 1252, 50542], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 9, "seek": 2848, "start": 32.04, "end": 34.08, "text": " of probabilistic circuits.", "tokens": [50542, 295, 31959, 3142, 26354, 13, 50644], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 10, "seek": 2848, "start": 34.08, "end": 36.68, "text": " Like more machine learning oriented.", "tokens": [50644, 1743, 544, 3479, 2539, 21841, 13, 50774], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 11, "seek": 2848, "start": 38.480000000000004, "end": 43.480000000000004, "text": " Okay, so let's get started with the basic concept", "tokens": [50864, 1033, 11, 370, 718, 311, 483, 1409, 365, 264, 3875, 3410, 51114], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 12, "seek": 2848, "start": 43.84, "end": 45.56, "text": " of large language models.", "tokens": [51132, 295, 2416, 2856, 5245, 13, 51218], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 13, "seek": 2848, "start": 46.96, "end": 49.519999999999996, "text": " They have been like really popular recently.", "tokens": [51288, 814, 362, 668, 411, 534, 3743, 3938, 13, 51416], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 14, "seek": 2848, "start": 49.519999999999996, "end": 52.760000000000005, "text": " I'm not sure if people here really care or not.", "tokens": [51416, 286, 478, 406, 988, 498, 561, 510, 534, 1127, 420, 406, 13, 51578], "temperature": 0.0, "avg_logprob": -0.24383550220065647, "compression_ratio": 1.4479166666666667, "no_speech_prob": 0.00031492483685724437}, {"id": 15, "seek": 5276, "start": 53.76, "end": 58.76, "text": " Like I see like this interested face you do.", "tokens": [50414, 1743, 286, 536, 411, 341, 3102, 1851, 291, 360, 13, 50664], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 16, "seek": 5276, "start": 60.28, "end": 62.0, "text": " Okay, that's a good sign.", "tokens": [50740, 1033, 11, 300, 311, 257, 665, 1465, 13, 50826], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 17, "seek": 5276, "start": 62.0, "end": 63.68, "text": " That's a good sign.", "tokens": [50826, 663, 311, 257, 665, 1465, 13, 50910], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 18, "seek": 5276, "start": 63.68, "end": 67.8, "text": " So basically the idea is very simple.", "tokens": [50910, 407, 1936, 264, 1558, 307, 588, 2199, 13, 51116], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 19, "seek": 5276, "start": 67.8, "end": 71.32, "text": " In the sense that you collect large amount of text", "tokens": [51116, 682, 264, 2020, 300, 291, 2500, 2416, 2372, 295, 2487, 51292], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 20, "seek": 5276, "start": 71.32, "end": 73.75999999999999, "text": " consisting of trillions of words,", "tokens": [51292, 33921, 295, 504, 46279, 295, 2283, 11, 51414], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 21, "seek": 5276, "start": 73.75999999999999, "end": 77.4, "text": " and you train neural networks with billions of parameters", "tokens": [51414, 293, 291, 3847, 18161, 9590, 365, 17375, 295, 9834, 51596], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 22, "seek": 5276, "start": 77.4, "end": 81.4, "text": " on these data, then you have these so-called large", "tokens": [51596, 322, 613, 1412, 11, 550, 291, 362, 613, 370, 12, 11880, 2416, 51796], "temperature": 0.0, "avg_logprob": -0.23243128169666638, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.0006665967521257699}, {"id": 23, "seek": 8140, "start": 81.4, "end": 82.52000000000001, "text": " language models.", "tokens": [50364, 2856, 5245, 13, 50420], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 24, "seek": 8140, "start": 84.88000000000001, "end": 89.88000000000001, "text": " These chat GPT or GPT-4 thing has become like really popular.", "tokens": [50538, 1981, 5081, 26039, 51, 420, 26039, 51, 12, 19, 551, 575, 1813, 411, 534, 3743, 13, 50788], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 25, "seek": 8140, "start": 92.60000000000001, "end": 94.44000000000001, "text": " People talking to them on the internet,", "tokens": [50924, 3432, 1417, 281, 552, 322, 264, 4705, 11, 51016], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 26, "seek": 8140, "start": 94.44000000000001, "end": 98.2, "text": " asking them to polish their papers,", "tokens": [51016, 3365, 552, 281, 20452, 641, 10577, 11, 51204], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 27, "seek": 8140, "start": 98.2, "end": 100.72, "text": " using them to do their homeworks,", "tokens": [51204, 1228, 552, 281, 360, 641, 14578, 82, 11, 51330], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 28, "seek": 8140, "start": 100.72, "end": 105.72, "text": " and even asking them to play D&D together.", "tokens": [51330, 293, 754, 3365, 552, 281, 862, 413, 5, 35, 1214, 13, 51580], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 29, "seek": 8140, "start": 107.52000000000001, "end": 110.32000000000001, "text": " So yeah, that is actually true.", "tokens": [51670, 407, 1338, 11, 300, 307, 767, 2074, 13, 51810], "temperature": 0.0, "avg_logprob": -0.2107307632248123, "compression_ratio": 1.4858757062146892, "no_speech_prob": 0.0006360478000715375}, {"id": 30, "seek": 11032, "start": 111.32, "end": 116.32, "text": " So the idea is, so people have now having this feeling", "tokens": [50414, 407, 264, 1558, 307, 11, 370, 561, 362, 586, 1419, 341, 2633, 50664], "temperature": 0.0, "avg_logprob": -0.23097476674549616, "compression_ratio": 1.4294478527607362, "no_speech_prob": 7.030434062471613e-05}, {"id": 31, "seek": 11032, "start": 117.27999999999999, "end": 119.83999999999999, "text": " that we probably have solved AI,", "tokens": [50712, 300, 321, 1391, 362, 13041, 7318, 11, 50840], "temperature": 0.0, "avg_logprob": -0.23097476674549616, "compression_ratio": 1.4294478527607362, "no_speech_prob": 7.030434062471613e-05}, {"id": 32, "seek": 11032, "start": 119.83999999999999, "end": 123.55999999999999, "text": " we have solved artificial intelligence,", "tokens": [50840, 321, 362, 13041, 11677, 7599, 11, 51026], "temperature": 0.0, "avg_logprob": -0.23097476674549616, "compression_ratio": 1.4294478527607362, "no_speech_prob": 7.030434062471613e-05}, {"id": 33, "seek": 11032, "start": 125.67999999999999, "end": 127.63999999999999, "text": " and chat GPT has passed Turing test,", "tokens": [51132, 293, 5081, 26039, 51, 575, 4678, 314, 1345, 1500, 11, 51230], "temperature": 0.0, "avg_logprob": -0.23097476674549616, "compression_ratio": 1.4294478527607362, "no_speech_prob": 7.030434062471613e-05}, {"id": 34, "seek": 11032, "start": 127.63999999999999, "end": 130.84, "text": " but is that really the case, okay?", "tokens": [51230, 457, 307, 300, 534, 264, 1389, 11, 1392, 30, 51390], "temperature": 0.0, "avg_logprob": -0.23097476674549616, "compression_ratio": 1.4294478527607362, "no_speech_prob": 7.030434062471613e-05}, {"id": 35, "seek": 11032, "start": 130.84, "end": 135.84, "text": " So I tried this a few months ago.", "tokens": [51390, 407, 286, 3031, 341, 257, 1326, 2493, 2057, 13, 51640], "temperature": 0.0, "avg_logprob": -0.23097476674549616, "compression_ratio": 1.4294478527607362, "no_speech_prob": 7.030434062471613e-05}, {"id": 36, "seek": 13584, "start": 136.84, "end": 141.84, "text": " It was an actual conversation between me and chat GPT.", "tokens": [50414, 467, 390, 364, 3539, 3761, 1296, 385, 293, 5081, 26039, 51, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 37, "seek": 13584, "start": 141.92000000000002, "end": 145.6, "text": " So I asked the model to generate a sentence", "tokens": [50668, 407, 286, 2351, 264, 2316, 281, 8460, 257, 8174, 50852], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 38, "seek": 13584, "start": 145.6, "end": 149.56, "text": " using Frisbee, Cod, and Dog following the given order.", "tokens": [50852, 1228, 1526, 271, 24872, 11, 383, 378, 11, 293, 13472, 3480, 264, 2212, 1668, 13, 51050], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 39, "seek": 13584, "start": 149.56, "end": 152.28, "text": " Well, I mean, this should be quite simple", "tokens": [51050, 1042, 11, 286, 914, 11, 341, 820, 312, 1596, 2199, 51186], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 40, "seek": 13584, "start": 152.28, "end": 156.4, "text": " for a super intelligent AI model, right?", "tokens": [51186, 337, 257, 1687, 13232, 7318, 2316, 11, 558, 30, 51392], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 41, "seek": 13584, "start": 156.4, "end": 159.76, "text": " So it does give me a sentence.", "tokens": [51392, 407, 309, 775, 976, 385, 257, 8174, 13, 51560], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 42, "seek": 13584, "start": 159.76, "end": 162.84, "text": " The sentence look quite decent,", "tokens": [51560, 440, 8174, 574, 1596, 8681, 11, 51714], "temperature": 0.0, "avg_logprob": -0.16283376357134652, "compression_ratio": 1.4801980198019802, "no_speech_prob": 0.0007792196702212095}, {"id": 43, "seek": 16284, "start": 163.84, "end": 166.08, "text": " and all the key words are in there,", "tokens": [50414, 293, 439, 264, 2141, 2283, 366, 294, 456, 11, 50526], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 44, "seek": 16284, "start": 166.08, "end": 170.96, "text": " but Dog and Cod are in the wrong order, right?", "tokens": [50526, 457, 13472, 293, 383, 378, 366, 294, 264, 2085, 1668, 11, 558, 30, 50770], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 45, "seek": 16284, "start": 170.96, "end": 173.24, "text": " Well, I mean, humans make mistakes,", "tokens": [50770, 1042, 11, 286, 914, 11, 6255, 652, 8038, 11, 50884], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 46, "seek": 16284, "start": 173.24, "end": 175.12, "text": " so super intelligent AI does.", "tokens": [50884, 370, 1687, 13232, 7318, 775, 13, 50978], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 47, "seek": 16284, "start": 176.12, "end": 180.12, "text": " So I was being patient, I tried again, okay?", "tokens": [51028, 407, 286, 390, 885, 4537, 11, 286, 3031, 797, 11, 1392, 30, 51228], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 48, "seek": 16284, "start": 180.12, "end": 181.92000000000002, "text": " Again, it gives me a sentence,", "tokens": [51228, 3764, 11, 309, 2709, 385, 257, 8174, 11, 51318], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 49, "seek": 16284, "start": 181.92000000000002, "end": 184.44, "text": " but it's even worse now.", "tokens": [51318, 457, 309, 311, 754, 5324, 586, 13, 51444], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 50, "seek": 16284, "start": 184.44, "end": 186.48000000000002, "text": " All of the words are in the wrong order.", "tokens": [51444, 1057, 295, 264, 2283, 366, 294, 264, 2085, 1668, 13, 51546], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 51, "seek": 16284, "start": 186.48000000000002, "end": 191.48000000000002, "text": " So you see that chat GPT fails to follow", "tokens": [51546, 407, 291, 536, 300, 5081, 26039, 51, 18199, 281, 1524, 51796], "temperature": 0.0, "avg_logprob": -0.12808908866002008, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.00017128430772572756}, {"id": 52, "seek": 19148, "start": 191.76, "end": 195.04, "text": " even this simple logical constraint, okay?", "tokens": [50378, 754, 341, 2199, 14978, 25534, 11, 1392, 30, 50542], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 53, "seek": 19148, "start": 195.04, "end": 197.67999999999998, "text": " So people have been trying to fix this.", "tokens": [50542, 407, 561, 362, 668, 1382, 281, 3191, 341, 13, 50674], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 54, "seek": 19148, "start": 197.67999999999998, "end": 200.23999999999998, "text": " Some of the methods, including like,", "tokens": [50674, 2188, 295, 264, 7150, 11, 3009, 411, 11, 50802], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 55, "seek": 19148, "start": 200.23999999999998, "end": 202.32, "text": " okay, we can prompt them in a different way,", "tokens": [50802, 1392, 11, 321, 393, 12391, 552, 294, 257, 819, 636, 11, 50906], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 56, "seek": 19148, "start": 202.32, "end": 205.1, "text": " like chain of thought.", "tokens": [50906, 411, 5021, 295, 1194, 13, 51045], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 57, "seek": 19148, "start": 206.16, "end": 208.2, "text": " I'm not comfortable saying this phrase,", "tokens": [51098, 286, 478, 406, 4619, 1566, 341, 9535, 11, 51200], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 58, "seek": 19148, "start": 208.2, "end": 210.44, "text": " but that's one of the methods.", "tokens": [51200, 457, 300, 311, 472, 295, 264, 7150, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 59, "seek": 19148, "start": 211.67999999999998, "end": 214.39999999999998, "text": " Or people try to use search algorithms", "tokens": [51374, 1610, 561, 853, 281, 764, 3164, 14642, 51510], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 60, "seek": 19148, "start": 214.39999999999998, "end": 216.39999999999998, "text": " trying to find the correct sentence", "tokens": [51510, 1382, 281, 915, 264, 3006, 8174, 51610], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 61, "seek": 19148, "start": 216.39999999999998, "end": 219.88, "text": " in this huge search space, so on and so forth.", "tokens": [51610, 294, 341, 2603, 3164, 1901, 11, 370, 322, 293, 370, 5220, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1454001206618089, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0004802341281902045}, {"id": 62, "seek": 21988, "start": 219.88, "end": 223.68, "text": " But none of them actually guarantees that chat GPT", "tokens": [50364, 583, 6022, 295, 552, 767, 32567, 300, 5081, 26039, 51, 50554], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 63, "seek": 21988, "start": 223.68, "end": 227.72, "text": " or whatever other language model give us what we want.", "tokens": [50554, 420, 2035, 661, 2856, 2316, 976, 505, 437, 321, 528, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 64, "seek": 21988, "start": 227.72, "end": 232.44, "text": " So what we actually want is like 100% guarantee.", "tokens": [50756, 407, 437, 321, 767, 528, 307, 411, 2319, 4, 10815, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 65, "seek": 21988, "start": 232.44, "end": 235.4, "text": " When we have like a huge powerful model,", "tokens": [50992, 1133, 321, 362, 411, 257, 2603, 4005, 2316, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 66, "seek": 21988, "start": 235.4, "end": 237.35999999999999, "text": " when we instructed to do something,", "tokens": [51140, 562, 321, 36384, 281, 360, 746, 11, 51238], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 67, "seek": 21988, "start": 237.35999999999999, "end": 241.48, "text": " we wanted to follow our instructions exactly.", "tokens": [51238, 321, 1415, 281, 1524, 527, 9415, 2293, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 68, "seek": 21988, "start": 241.48, "end": 245.16, "text": " So in today's talk, I'll show you how we can do this", "tokens": [51444, 407, 294, 965, 311, 751, 11, 286, 603, 855, 291, 577, 321, 393, 360, 341, 51628], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 69, "seek": 21988, "start": 245.16, "end": 247.24, "text": " with probabilistic circuits.", "tokens": [51628, 365, 31959, 3142, 26354, 13, 51732], "temperature": 0.0, "avg_logprob": -0.1235939695480022, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00011410716251702979}, {"id": 70, "seek": 24724, "start": 248.24, "end": 251.32000000000002, "text": " Before some of the detail,", "tokens": [50414, 4546, 512, 295, 264, 2607, 11, 50568], "temperature": 0.0, "avg_logprob": -0.1914201790178326, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.00011773897131206468}, {"id": 71, "seek": 24724, "start": 251.32000000000002, "end": 254.84, "text": " so let's get started with some basics of language modeling.", "tokens": [50568, 370, 718, 311, 483, 1409, 365, 512, 14688, 295, 2856, 15983, 13, 50744], "temperature": 0.0, "avg_logprob": -0.1914201790178326, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.00011773897131206468}, {"id": 72, "seek": 24724, "start": 254.84, "end": 257.72, "text": " So language modeling is really not like a fancy idea.", "tokens": [50744, 407, 2856, 15983, 307, 534, 406, 411, 257, 10247, 1558, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1914201790178326, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.00011773897131206468}, {"id": 73, "seek": 24724, "start": 257.72, "end": 261.28000000000003, "text": " It's just a joined distribution over some text,", "tokens": [50888, 467, 311, 445, 257, 6869, 7316, 670, 512, 2487, 11, 51066], "temperature": 0.0, "avg_logprob": -0.1914201790178326, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.00011773897131206468}, {"id": 74, "seek": 24724, "start": 262.40000000000003, "end": 265.48, "text": " probably less than or equal to n words.", "tokens": [51122, 1391, 1570, 813, 420, 2681, 281, 297, 2283, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1914201790178326, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.00011773897131206468}, {"id": 75, "seek": 24724, "start": 267.40000000000003, "end": 272.40000000000003, "text": " Each random variable here is a word taking value", "tokens": [51372, 6947, 4974, 7006, 510, 307, 257, 1349, 1940, 2158, 51622], "temperature": 0.0, "avg_logprob": -0.1914201790178326, "compression_ratio": 1.489247311827957, "no_speech_prob": 0.00011773897131206468}, {"id": 76, "seek": 27240, "start": 273.32, "end": 278.32, "text": " in a fixed, in a vocabulary of finite size.", "tokens": [50410, 294, 257, 6806, 11, 294, 257, 19864, 295, 19362, 2744, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 77, "seek": 27240, "start": 280.12, "end": 284.52, "text": " The size usually in practice ranges from 10,000 to 50,000", "tokens": [50750, 440, 2744, 2673, 294, 3124, 22526, 490, 1266, 11, 1360, 281, 2625, 11, 1360, 50970], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 78, "seek": 27240, "start": 284.52, "end": 286.2, "text": " or something, okay?", "tokens": [50970, 420, 746, 11, 1392, 30, 51054], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 79, "seek": 27240, "start": 287.28, "end": 290.0, "text": " And here's some examples.", "tokens": [51108, 400, 510, 311, 512, 5110, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 80, "seek": 27240, "start": 290.0, "end": 293.91999999999996, "text": " You might notice there's some special word called EOS here,", "tokens": [51244, 509, 1062, 3449, 456, 311, 512, 2121, 1349, 1219, 462, 4367, 510, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 81, "seek": 27240, "start": 293.91999999999996, "end": 295.12, "text": " which means end of sentence,", "tokens": [51440, 597, 1355, 917, 295, 8174, 11, 51500], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 82, "seek": 27240, "start": 295.12, "end": 298.0, "text": " which is only like a special token or word", "tokens": [51500, 597, 307, 787, 411, 257, 2121, 14862, 420, 1349, 51644], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 83, "seek": 27240, "start": 298.0, "end": 300.84, "text": " we use to pad sentences of different length", "tokens": [51644, 321, 764, 281, 6887, 16579, 295, 819, 4641, 51786], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 84, "seek": 27240, "start": 300.84, "end": 301.91999999999996, "text": " to the maximum length.", "tokens": [51786, 281, 264, 6674, 4641, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1835592210907297, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0004441553319338709}, {"id": 85, "seek": 30192, "start": 301.92, "end": 306.12, "text": " So we have this like joined distribution well defined, okay?", "tokens": [50364, 407, 321, 362, 341, 411, 6869, 7316, 731, 7642, 11, 1392, 30, 50574], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 86, "seek": 30192, "start": 306.12, "end": 310.0, "text": " So we collect a lot of text of this form,", "tokens": [50574, 407, 321, 2500, 257, 688, 295, 2487, 295, 341, 1254, 11, 50768], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 87, "seek": 30192, "start": 310.0, "end": 313.2, "text": " sentence fragments, paragraph fragments,", "tokens": [50768, 8174, 29197, 11, 18865, 29197, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 88, "seek": 30192, "start": 313.2, "end": 316.0, "text": " and we train this joined distribution", "tokens": [50928, 293, 321, 3847, 341, 6869, 7316, 51068], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 89, "seek": 30192, "start": 316.0, "end": 319.8, "text": " by maximizing a lot of likelihood, a very, very simple idea.", "tokens": [51068, 538, 5138, 3319, 257, 688, 295, 22119, 11, 257, 588, 11, 588, 2199, 1558, 13, 51258], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 90, "seek": 30192, "start": 320.64, "end": 322.20000000000005, "text": " Okay, so what about the architecture", "tokens": [51300, 1033, 11, 370, 437, 466, 264, 9482, 51378], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 91, "seek": 30192, "start": 322.20000000000005, "end": 324.6, "text": " of this distribution of our model?", "tokens": [51378, 295, 341, 7316, 295, 527, 2316, 30, 51498], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 92, "seek": 30192, "start": 324.6, "end": 329.6, "text": " Well, like the most popular ones, like GPT,", "tokens": [51498, 1042, 11, 411, 264, 881, 3743, 2306, 11, 411, 26039, 51, 11, 51748], "temperature": 0.0, "avg_logprob": -0.1948852944881358, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.822635335382074e-05}, {"id": 93, "seek": 32960, "start": 330.32000000000005, "end": 332.44, "text": " they are autoregressive models.", "tokens": [50400, 436, 366, 1476, 418, 3091, 488, 5245, 13, 50506], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 94, "seek": 32960, "start": 332.44, "end": 336.8, "text": " Well, so by the chain rule of probability,", "tokens": [50506, 1042, 11, 370, 538, 264, 5021, 4978, 295, 8482, 11, 50724], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 95, "seek": 32960, "start": 336.8, "end": 340.68, "text": " we can decompose our joined distribution this way.", "tokens": [50724, 321, 393, 22867, 541, 527, 6869, 7316, 341, 636, 13, 50918], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 96, "seek": 32960, "start": 342.36, "end": 345.76000000000005, "text": " Represented as a graphical model is basically", "tokens": [51002, 3696, 495, 6003, 382, 257, 35942, 2316, 307, 1936, 51172], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 97, "seek": 32960, "start": 345.76000000000005, "end": 347.76000000000005, "text": " for each random variable,", "tokens": [51172, 337, 1184, 4974, 7006, 11, 51272], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 98, "seek": 32960, "start": 347.76000000000005, "end": 351.52000000000004, "text": " we have arrows going from all the previous random variables.", "tokens": [51272, 321, 362, 19669, 516, 490, 439, 264, 3894, 4974, 9102, 13, 51460], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 99, "seek": 32960, "start": 352.6, "end": 355.0, "text": " From a generative point of view,", "tokens": [51514, 3358, 257, 1337, 1166, 935, 295, 1910, 11, 51634], "temperature": 0.0, "avg_logprob": -0.17705360016265473, "compression_ratio": 1.515625, "no_speech_prob": 0.00026526389410719275}, {"id": 100, "seek": 35500, "start": 356.0, "end": 358.64, "text": " it's basically we start with,", "tokens": [50414, 309, 311, 1936, 321, 722, 365, 11, 50546], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 101, "seek": 35500, "start": 359.8, "end": 361.04, "text": " I can't see my cursor.", "tokens": [50604, 286, 393, 380, 536, 452, 28169, 13, 50666], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 102, "seek": 35500, "start": 361.04, "end": 365.36, "text": " So we start generating the first word", "tokens": [50666, 407, 321, 722, 17746, 264, 700, 1349, 50882], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 103, "seek": 35500, "start": 365.36, "end": 367.48, "text": " from this prior distribution.", "tokens": [50882, 490, 341, 4059, 7316, 13, 50988], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 104, "seek": 35500, "start": 367.48, "end": 368.92, "text": " Once we have the first word,", "tokens": [50988, 3443, 321, 362, 264, 700, 1349, 11, 51060], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 105, "seek": 35500, "start": 368.92, "end": 370.92, "text": " we move on to the second one,", "tokens": [51060, 321, 1286, 322, 281, 264, 1150, 472, 11, 51160], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 106, "seek": 35500, "start": 370.92, "end": 372.32, "text": " conditioning on the first one,", "tokens": [51160, 21901, 322, 264, 700, 472, 11, 51230], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 107, "seek": 35500, "start": 372.32, "end": 373.96, "text": " and then we generate the third one,", "tokens": [51230, 293, 550, 321, 8460, 264, 2636, 472, 11, 51312], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 108, "seek": 35500, "start": 373.96, "end": 377.16, "text": " conditioning on the first two words, very simple, okay?", "tokens": [51312, 21901, 322, 264, 700, 732, 2283, 11, 588, 2199, 11, 1392, 30, 51472], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 109, "seek": 35500, "start": 379.56, "end": 382.84, "text": " By some classic results,", "tokens": [51592, 3146, 512, 7230, 3542, 11, 51756], "temperature": 0.0, "avg_logprob": -0.18741585345978432, "compression_ratio": 1.703125, "no_speech_prob": 0.00024153717095032334}, {"id": 110, "seek": 38284, "start": 382.91999999999996, "end": 386.67999999999995, "text": " marginose for Bayesian networks", "tokens": [50368, 10270, 541, 337, 7840, 42434, 9590, 50556], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 111, "seek": 38284, "start": 386.67999999999995, "end": 389.4, "text": " with these structures is intractable.", "tokens": [50556, 365, 613, 9227, 307, 560, 1897, 712, 13, 50692], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 112, "seek": 38284, "start": 391.35999999999996, "end": 396.35999999999996, "text": " Okay, so before when we asked GPT", "tokens": [50790, 1033, 11, 370, 949, 562, 321, 2351, 26039, 51, 51040], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 113, "seek": 38284, "start": 399.2, "end": 400.59999999999997, "text": " to generate a sentence for us,", "tokens": [51182, 281, 8460, 257, 8174, 337, 505, 11, 51252], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 114, "seek": 38284, "start": 400.59999999999997, "end": 404.44, "text": " we are like talking to it as if it's human, right?", "tokens": [51252, 321, 366, 411, 1417, 281, 309, 382, 498, 309, 311, 1952, 11, 558, 30, 51444], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 115, "seek": 38284, "start": 404.44, "end": 408.08, "text": " So the assumption is if our distribution over language", "tokens": [51444, 407, 264, 15302, 307, 498, 527, 7316, 670, 2856, 51626], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 116, "seek": 38284, "start": 408.08, "end": 411.12, "text": " is a perfect distribution,", "tokens": [51626, 307, 257, 2176, 7316, 11, 51778], "temperature": 0.0, "avg_logprob": -0.19892657769692912, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.0005192168755456805}, {"id": 117, "seek": 41112, "start": 411.12, "end": 414.96, "text": " then we should be able to interact with it as a human.", "tokens": [50364, 550, 321, 820, 312, 1075, 281, 4648, 365, 309, 382, 257, 1952, 13, 50556], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 118, "seek": 41112, "start": 416.4, "end": 418.64, "text": " When we're doing these kind of like prompting,", "tokens": [50628, 1133, 321, 434, 884, 613, 733, 295, 411, 12391, 278, 11, 50740], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 119, "seek": 41112, "start": 418.64, "end": 419.84000000000003, "text": " when we're talking to it,", "tokens": [50740, 562, 321, 434, 1417, 281, 309, 11, 50800], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 120, "seek": 41112, "start": 419.84000000000003, "end": 423.56, "text": " most of the time we are actually trying to do conditioning.", "tokens": [50800, 881, 295, 264, 565, 321, 366, 767, 1382, 281, 360, 21901, 13, 50986], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 121, "seek": 41112, "start": 423.56, "end": 426.56, "text": " So let's consider an even simpler example.", "tokens": [50986, 407, 718, 311, 1949, 364, 754, 18587, 1365, 13, 51136], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 122, "seek": 41112, "start": 427.68, "end": 430.56, "text": " Suppose in our autoregressive generation,", "tokens": [51192, 21360, 294, 527, 1476, 418, 3091, 488, 5125, 11, 51336], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 123, "seek": 41112, "start": 430.56, "end": 434.4, "text": " we have generated the first three words, the weather is,", "tokens": [51336, 321, 362, 10833, 264, 700, 1045, 2283, 11, 264, 5503, 307, 11, 51528], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 124, "seek": 41112, "start": 434.4, "end": 437.6, "text": " and our constraint is that we want the sentence", "tokens": [51528, 293, 527, 25534, 307, 300, 321, 528, 264, 8174, 51688], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 125, "seek": 41112, "start": 437.6, "end": 439.28000000000003, "text": " to contain the keyword winter.", "tokens": [51688, 281, 5304, 264, 20428, 6355, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12378145147252965, "compression_ratio": 1.6653061224489796, "no_speech_prob": 4.264447488822043e-05}, {"id": 126, "seek": 43928, "start": 439.28, "end": 441.2, "text": " Well, I mean, we wanna write something", "tokens": [50364, 1042, 11, 286, 914, 11, 321, 1948, 2464, 746, 50460], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 127, "seek": 43928, "start": 443.0, "end": 445.84, "text": " with the topic of winter, so that's our constraint.", "tokens": [50550, 365, 264, 4829, 295, 6355, 11, 370, 300, 311, 527, 25534, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 128, "seek": 43928, "start": 446.91999999999996, "end": 450.15999999999997, "text": " What we really want is, okay,", "tokens": [50746, 708, 321, 534, 528, 307, 11, 1392, 11, 50908], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 129, "seek": 43928, "start": 450.15999999999997, "end": 454.64, "text": " so what language model gives us is the next token,", "tokens": [50908, 370, 437, 2856, 2316, 2709, 505, 307, 264, 958, 14862, 11, 51132], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 130, "seek": 43928, "start": 454.64, "end": 456.44, "text": " next word probability.", "tokens": [51132, 958, 1349, 8482, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 131, "seek": 43928, "start": 456.44, "end": 459.84, "text": " So basically given the weather is,", "tokens": [51222, 407, 1936, 2212, 264, 5503, 307, 11, 51392], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 132, "seek": 43928, "start": 459.84, "end": 464.03999999999996, "text": " it might prefer whether it's warm or cold,", "tokens": [51392, 309, 1062, 4382, 1968, 309, 311, 4561, 420, 3554, 11, 51602], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 133, "seek": 43928, "start": 465.11999999999995, "end": 467.44, "text": " while its prior distribution might want the weather", "tokens": [51656, 1339, 1080, 4059, 7316, 1062, 528, 264, 5503, 51772], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 134, "seek": 43928, "start": 467.44, "end": 468.91999999999996, "text": " to be warm.", "tokens": [51772, 281, 312, 4561, 13, 51846], "temperature": 0.0, "avg_logprob": -0.2024033454156691, "compression_ratio": 1.6, "no_speech_prob": 0.00011060218821512535}, {"id": 135, "seek": 46892, "start": 468.92, "end": 471.44, "text": " But what we actually want,", "tokens": [50364, 583, 437, 321, 767, 528, 11, 50490], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 136, "seek": 46892, "start": 472.48, "end": 473.92, "text": " but what we'll actually need", "tokens": [50542, 457, 437, 321, 603, 767, 643, 50614], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 137, "seek": 46892, "start": 473.92, "end": 477.12, "text": " is this conditional next word probability, right?", "tokens": [50614, 307, 341, 27708, 958, 1349, 8482, 11, 558, 30, 50774], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 138, "seek": 46892, "start": 477.12, "end": 480.6, "text": " We want our sentence to satisfy the constraint,", "tokens": [50774, 492, 528, 527, 8174, 281, 19319, 264, 25534, 11, 50948], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 139, "seek": 46892, "start": 480.6, "end": 481.96000000000004, "text": " once it contain the winter,", "tokens": [50948, 1564, 309, 5304, 264, 6355, 11, 51016], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 140, "seek": 46892, "start": 483.44, "end": 485.44, "text": " and well, intuitively,", "tokens": [51090, 293, 731, 11, 46506, 11, 51190], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 141, "seek": 46892, "start": 485.44, "end": 488.44, "text": " if we want the winter to be in the sentence,", "tokens": [51190, 498, 321, 528, 264, 6355, 281, 312, 294, 264, 8174, 11, 51340], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 142, "seek": 46892, "start": 488.44, "end": 491.44, "text": " weather is more likely to be cold, okay?", "tokens": [51340, 5503, 307, 544, 3700, 281, 312, 3554, 11, 1392, 30, 51490], "temperature": 0.0, "avg_logprob": -0.24010605812072755, "compression_ratio": 1.6292134831460674, "no_speech_prob": 3.882814780808985e-05}, {"id": 143, "seek": 49144, "start": 491.44, "end": 496.44, "text": " But this is intractable for those autoregressive models.", "tokens": [50364, 583, 341, 307, 560, 1897, 712, 337, 729, 1476, 418, 3091, 488, 5245, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 144, "seek": 49144, "start": 501.24, "end": 503.12, "text": " So why is it intractable?", "tokens": [50854, 407, 983, 307, 309, 560, 1897, 712, 30, 50948], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 145, "seek": 49144, "start": 503.12, "end": 505.24, "text": " Let's go more into the details.", "tokens": [50948, 961, 311, 352, 544, 666, 264, 4365, 13, 51054], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 146, "seek": 49144, "start": 505.24, "end": 509.44, "text": " So by Bayes' rule, this conditional probability", "tokens": [51054, 407, 538, 7840, 279, 6, 4978, 11, 341, 27708, 8482, 51264], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 147, "seek": 49144, "start": 509.44, "end": 512.16, "text": " can be decomposed into these two terms.", "tokens": [51264, 393, 312, 22867, 1744, 666, 613, 732, 2115, 13, 51400], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 148, "seek": 49144, "start": 513.64, "end": 515.44, "text": " The next term here is very simple,", "tokens": [51474, 440, 958, 1433, 510, 307, 588, 2199, 11, 51564], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 149, "seek": 49144, "start": 515.44, "end": 518.4, "text": " it's just a language model next word probability.", "tokens": [51564, 309, 311, 445, 257, 2856, 2316, 958, 1349, 8482, 13, 51712], "temperature": 0.0, "avg_logprob": -0.20597168336431665, "compression_ratio": 1.5513513513513513, "no_speech_prob": 3.647763514891267e-05}, {"id": 150, "seek": 51840, "start": 518.4, "end": 523.4, "text": " But the first term here is actually a marginal probability.", "tokens": [50364, 583, 264, 700, 1433, 510, 307, 767, 257, 16885, 8482, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 151, "seek": 51840, "start": 525.0, "end": 527.8, "text": " So it's basically the marginal probability", "tokens": [50694, 407, 309, 311, 1936, 264, 16885, 8482, 50834], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 152, "seek": 51840, "start": 527.8, "end": 531.36, "text": " over all possible suffixes that we could generate", "tokens": [50834, 670, 439, 1944, 3889, 36005, 300, 321, 727, 8460, 51012], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 153, "seek": 51840, "start": 531.36, "end": 533.8, "text": " that contains winter, okay?", "tokens": [51012, 300, 8306, 6355, 11, 1392, 30, 51134], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 154, "seek": 51840, "start": 535.48, "end": 538.1999999999999, "text": " So this is intractable for autoregressive models,", "tokens": [51218, 407, 341, 307, 560, 1897, 712, 337, 1476, 418, 3091, 488, 5245, 11, 51354], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 155, "seek": 51840, "start": 538.1999999999999, "end": 540.0799999999999, "text": " but we have PCs, right?", "tokens": [51354, 457, 321, 362, 46913, 11, 558, 30, 51448], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 156, "seek": 51840, "start": 540.0799999999999, "end": 543.1999999999999, "text": " PCs are tractable, at least,", "tokens": [51448, 46913, 366, 24207, 712, 11, 412, 1935, 11, 51604], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 157, "seek": 51840, "start": 544.96, "end": 547.88, "text": " we know how to compute marginal probabilities with them.", "tokens": [51692, 321, 458, 577, 281, 14722, 16885, 33783, 365, 552, 13, 51838], "temperature": 0.0, "avg_logprob": -0.13244248508067613, "compression_ratio": 1.6113744075829384, "no_speech_prob": 1.363099454465555e-05}, {"id": 158, "seek": 54788, "start": 547.88, "end": 552.88, "text": " So why don't we just approximate this term here", "tokens": [50364, 407, 983, 500, 380, 321, 445, 30874, 341, 1433, 510, 50614], "temperature": 0.0, "avg_logprob": -0.15991220901261516, "compression_ratio": 1.3687150837988826, "no_speech_prob": 2.5463181373197585e-05}, {"id": 159, "seek": 54788, "start": 553.28, "end": 554.84, "text": " with probabilistic circuits?", "tokens": [50634, 365, 31959, 3142, 26354, 30, 50712], "temperature": 0.0, "avg_logprob": -0.15991220901261516, "compression_ratio": 1.3687150837988826, "no_speech_prob": 2.5463181373197585e-05}, {"id": 160, "seek": 54788, "start": 558.12, "end": 562.72, "text": " And we refer to our pipeline as gelato,", "tokens": [50876, 400, 321, 2864, 281, 527, 15517, 382, 4087, 2513, 11, 51106], "temperature": 0.0, "avg_logprob": -0.15991220901261516, "compression_ratio": 1.3687150837988826, "no_speech_prob": 2.5463181373197585e-05}, {"id": 161, "seek": 54788, "start": 563.76, "end": 566.12, "text": " in generating language with tractable constraints.", "tokens": [51158, 294, 17746, 2856, 365, 24207, 712, 18491, 13, 51276], "temperature": 0.0, "avg_logprob": -0.15991220901261516, "compression_ratio": 1.3687150837988826, "no_speech_prob": 2.5463181373197585e-05}, {"id": 162, "seek": 54788, "start": 568.84, "end": 571.4, "text": " Okay, so how do we do this?", "tokens": [51412, 1033, 11, 370, 577, 360, 321, 360, 341, 30, 51540], "temperature": 0.0, "avg_logprob": -0.15991220901261516, "compression_ratio": 1.3687150837988826, "no_speech_prob": 2.5463181373197585e-05}, {"id": 163, "seek": 54788, "start": 571.4, "end": 576.4, "text": " So the first step is that we pick our favorite PC", "tokens": [51540, 407, 264, 700, 1823, 307, 300, 321, 1888, 527, 2954, 6465, 51790], "temperature": 0.0, "avg_logprob": -0.15991220901261516, "compression_ratio": 1.3687150837988826, "no_speech_prob": 2.5463181373197585e-05}, {"id": 164, "seek": 57640, "start": 576.4, "end": 580.36, "text": " for sequential modeling, which is a hidden markup model,", "tokens": [50364, 337, 42881, 15983, 11, 597, 307, 257, 7633, 1491, 1010, 2316, 11, 50562], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 165, "seek": 57640, "start": 580.36, "end": 581.88, "text": " the simplest we can have.", "tokens": [50562, 264, 22811, 321, 393, 362, 13, 50638], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 166, "seek": 57640, "start": 583.16, "end": 586.48, "text": " We have, on the other hand,", "tokens": [50702, 492, 362, 11, 322, 264, 661, 1011, 11, 50868], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 167, "seek": 57640, "start": 586.48, "end": 588.12, "text": " we have our pre-trained language model", "tokens": [50868, 321, 362, 527, 659, 12, 17227, 2001, 2856, 2316, 50950], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 168, "seek": 57640, "start": 588.12, "end": 590.36, "text": " we want to control or guide.", "tokens": [50950, 321, 528, 281, 1969, 420, 5934, 13, 51062], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 169, "seek": 57640, "start": 590.36, "end": 593.36, "text": " We sample a lot of data from our language model", "tokens": [51062, 492, 6889, 257, 688, 295, 1412, 490, 527, 2856, 2316, 51212], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 170, "seek": 57640, "start": 593.36, "end": 597.68, "text": " unconditionally, and then we train our hidden markup model", "tokens": [51212, 34959, 15899, 11, 293, 550, 321, 3847, 527, 7633, 1491, 1010, 2316, 51428], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 171, "seek": 57640, "start": 597.68, "end": 601.78, "text": " on these data with maximum likelihood estimation.", "tokens": [51428, 322, 613, 1412, 365, 6674, 22119, 35701, 13, 51633], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 172, "seek": 57640, "start": 601.78, "end": 604.3199999999999, "text": " So effectively minimizing the KL divergence", "tokens": [51633, 407, 8659, 46608, 264, 47991, 47387, 51760], "temperature": 0.0, "avg_logprob": -0.16120223609768614, "compression_ratio": 1.662280701754386, "no_speech_prob": 0.0005032573244534433}, {"id": 173, "seek": 60432, "start": 604.32, "end": 608.0, "text": " between the two joint distributions, okay?", "tokens": [50364, 1296, 264, 732, 7225, 37870, 11, 1392, 30, 50548], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 174, "seek": 60432, "start": 608.0, "end": 613.0, "text": " And our assumption is that if the joint distributions", "tokens": [50548, 400, 527, 15302, 307, 300, 498, 264, 7225, 37870, 50798], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 175, "seek": 60432, "start": 613.2800000000001, "end": 617.9200000000001, "text": " are close enough, then all of the marginal distributions", "tokens": [50812, 366, 1998, 1547, 11, 550, 439, 295, 264, 16885, 37870, 51044], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 176, "seek": 60432, "start": 617.9200000000001, "end": 622.5600000000001, "text": " and conditional distributions are gonna be similar, okay?", "tokens": [51044, 293, 27708, 37870, 366, 799, 312, 2531, 11, 1392, 30, 51276], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 177, "seek": 60432, "start": 622.5600000000001, "end": 623.4000000000001, "text": " Any questions?", "tokens": [51276, 2639, 1651, 30, 51318], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 178, "seek": 60432, "start": 625.0400000000001, "end": 627.9200000000001, "text": " So the intuition is that we have a black box", "tokens": [51400, 407, 264, 24002, 307, 300, 321, 362, 257, 2211, 2424, 51544], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 179, "seek": 60432, "start": 627.9200000000001, "end": 630.2800000000001, "text": " autoregressive model, and we train", "tokens": [51544, 1476, 418, 3091, 488, 2316, 11, 293, 321, 3847, 51662], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 180, "seek": 60432, "start": 630.2800000000001, "end": 632.0, "text": " some sort of white box PC,", "tokens": [51662, 512, 1333, 295, 2418, 2424, 6465, 11, 51748], "temperature": 0.0, "avg_logprob": -0.19127546037946427, "compression_ratio": 1.734375, "no_speech_prob": 0.00017951631161849946}, {"id": 181, "seek": 63200, "start": 632.04, "end": 637.04, "text": " so we can use as a representative of the black box, okay?", "tokens": [50366, 370, 321, 393, 764, 382, 257, 12424, 295, 264, 2211, 2424, 11, 1392, 30, 50616], "temperature": 0.0, "avg_logprob": -0.22743906656901042, "compression_ratio": 1.4426229508196722, "no_speech_prob": 1.9524874005583115e-05}, {"id": 182, "seek": 63200, "start": 641.4, "end": 645.8, "text": " So we don't want to, since this is the workshop", "tokens": [50834, 407, 321, 500, 380, 528, 281, 11, 1670, 341, 307, 264, 13541, 51054], "temperature": 0.0, "avg_logprob": -0.22743906656901042, "compression_ratio": 1.4426229508196722, "no_speech_prob": 1.9524874005583115e-05}, {"id": 183, "seek": 63200, "start": 645.8, "end": 650.8, "text": " for circuits and logic, let's represent HMMs as PCs.", "tokens": [51054, 337, 26354, 293, 9952, 11, 718, 311, 2906, 389, 17365, 82, 382, 46913, 13, 51304], "temperature": 0.0, "avg_logprob": -0.22743906656901042, "compression_ratio": 1.4426229508196722, "no_speech_prob": 1.9524874005583115e-05}, {"id": 184, "seek": 63200, "start": 651.28, "end": 654.76, "text": " So here is the graphical model version of PC.", "tokens": [51328, 407, 510, 307, 264, 35942, 2316, 3037, 295, 6465, 13, 51502], "temperature": 0.0, "avg_logprob": -0.22743906656901042, "compression_ratio": 1.4426229508196722, "no_speech_prob": 1.9524874005583115e-05}, {"id": 185, "seek": 63200, "start": 655.6, "end": 660.36, "text": " The Zs here are the hidden variables, the latent variables.", "tokens": [51544, 440, 1176, 82, 510, 366, 264, 7633, 9102, 11, 264, 48994, 9102, 13, 51782], "temperature": 0.0, "avg_logprob": -0.22743906656901042, "compression_ratio": 1.4426229508196722, "no_speech_prob": 1.9524874005583115e-05}, {"id": 186, "seek": 66036, "start": 661.36, "end": 665.48, "text": " And the X here are the observed variables, or the words.", "tokens": [50414, 400, 264, 1783, 510, 366, 264, 13095, 9102, 11, 420, 264, 2283, 13, 50620], "temperature": 0.0, "avg_logprob": -0.2242517729063292, "compression_ratio": 1.583815028901734, "no_speech_prob": 7.843281491659582e-05}, {"id": 187, "seek": 66036, "start": 667.04, "end": 671.36, "text": " And let me use the whiteboard to show you how to do this.", "tokens": [50698, 400, 718, 385, 764, 264, 2418, 3787, 281, 855, 291, 577, 281, 360, 341, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2242517729063292, "compression_ratio": 1.583815028901734, "no_speech_prob": 7.843281491659582e-05}, {"id": 188, "seek": 66036, "start": 676.5600000000001, "end": 681.5600000000001, "text": " Okay, so one assumption we have is that our hidden states,", "tokens": [51174, 1033, 11, 370, 472, 15302, 321, 362, 307, 300, 527, 7633, 4368, 11, 51424], "temperature": 0.0, "avg_logprob": -0.2242517729063292, "compression_ratio": 1.583815028901734, "no_speech_prob": 7.843281491659582e-05}, {"id": 189, "seek": 66036, "start": 681.72, "end": 685.44, "text": " our latent variables, are discrete variables", "tokens": [51432, 527, 48994, 9102, 11, 366, 27706, 9102, 51618], "temperature": 0.0, "avg_logprob": -0.2242517729063292, "compression_ratio": 1.583815028901734, "no_speech_prob": 7.843281491659582e-05}, {"id": 190, "seek": 66036, "start": 685.44, "end": 690.2, "text": " taking values from one to H, and H is our hidden state.", "tokens": [51618, 1940, 4190, 490, 472, 281, 389, 11, 293, 389, 307, 527, 7633, 1785, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2242517729063292, "compression_ratio": 1.583815028901734, "no_speech_prob": 7.843281491659582e-05}, {"id": 191, "seek": 69020, "start": 690.2, "end": 691.0400000000001, "text": " Any questions?", "tokens": [50364, 2639, 1651, 30, 50406], "temperature": 0.0, "avg_logprob": -0.21611312457493373, "compression_ratio": 1.297872340425532, "no_speech_prob": 6.013589518261142e-05}, {"id": 192, "seek": 69020, "start": 692.5600000000001, "end": 694.84, "text": " Okay, so we start from the very beginning.", "tokens": [50482, 1033, 11, 370, 321, 722, 490, 264, 588, 2863, 13, 50596], "temperature": 0.0, "avg_logprob": -0.21611312457493373, "compression_ratio": 1.297872340425532, "no_speech_prob": 6.013589518261142e-05}, {"id": 193, "seek": 69020, "start": 694.84, "end": 699.2, "text": " In an HMM, we start with the initial state, Z1,", "tokens": [50596, 682, 364, 389, 17365, 11, 321, 722, 365, 264, 5883, 1785, 11, 1176, 16, 11, 50814], "temperature": 0.0, "avg_logprob": -0.21611312457493373, "compression_ratio": 1.297872340425532, "no_speech_prob": 6.013589518261142e-05}, {"id": 194, "seek": 69020, "start": 699.2, "end": 702.12, "text": " and it has H choices.", "tokens": [50814, 293, 309, 575, 389, 7994, 13, 50960], "temperature": 0.0, "avg_logprob": -0.21611312457493373, "compression_ratio": 1.297872340425532, "no_speech_prob": 6.013589518261142e-05}, {"id": 195, "seek": 69020, "start": 712.0, "end": 717.0, "text": " So we want these eight product nodes to be representing", "tokens": [51454, 407, 321, 528, 613, 3180, 1674, 13891, 281, 312, 13460, 51704], "temperature": 0.0, "avg_logprob": -0.21611312457493373, "compression_ratio": 1.297872340425532, "no_speech_prob": 6.013589518261142e-05}, {"id": 196, "seek": 71700, "start": 717.92, "end": 721.44, "text": " the probability, can people see it actually?", "tokens": [50410, 264, 8482, 11, 393, 561, 536, 309, 767, 30, 50586], "temperature": 0.0, "avg_logprob": -0.23366001674107142, "compression_ratio": 1.4223602484472049, "no_speech_prob": 0.00018812724738381803}, {"id": 197, "seek": 71700, "start": 721.44, "end": 726.44, "text": " So representing the probability of X1 to N", "tokens": [50586, 407, 13460, 264, 8482, 295, 1783, 16, 281, 426, 50836], "temperature": 0.0, "avg_logprob": -0.23366001674107142, "compression_ratio": 1.4223602484472049, "no_speech_prob": 0.00018812724738381803}, {"id": 198, "seek": 71700, "start": 726.96, "end": 729.64, "text": " conditioning on Z1 equals to I.", "tokens": [50862, 21901, 322, 1176, 16, 6915, 281, 286, 13, 50996], "temperature": 0.0, "avg_logprob": -0.23366001674107142, "compression_ratio": 1.4223602484472049, "no_speech_prob": 0.00018812724738381803}, {"id": 199, "seek": 71700, "start": 731.96, "end": 735.08, "text": " So I would be the hidden state, so I will,", "tokens": [51112, 407, 286, 576, 312, 264, 7633, 1785, 11, 370, 286, 486, 11, 51268], "temperature": 0.0, "avg_logprob": -0.23366001674107142, "compression_ratio": 1.4223602484472049, "no_speech_prob": 0.00018812724738381803}, {"id": 200, "seek": 71700, "start": 735.08, "end": 739.4, "text": " this will be one, two, and H, okay?", "tokens": [51268, 341, 486, 312, 472, 11, 732, 11, 293, 389, 11, 1392, 30, 51484], "temperature": 0.0, "avg_logprob": -0.23366001674107142, "compression_ratio": 1.4223602484472049, "no_speech_prob": 0.00018812724738381803}, {"id": 201, "seek": 71700, "start": 739.4, "end": 744.4, "text": " And the edge will have weights", "tokens": [51484, 400, 264, 4691, 486, 362, 17443, 51734], "temperature": 0.0, "avg_logprob": -0.23366001674107142, "compression_ratio": 1.4223602484472049, "no_speech_prob": 0.00018812724738381803}, {"id": 202, "seek": 74440, "start": 744.4, "end": 747.3199999999999, "text": " of the prior distribution on Z1.", "tokens": [50364, 295, 264, 4059, 7316, 322, 1176, 16, 13, 50510], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 203, "seek": 74440, "start": 753.3199999999999, "end": 756.12, "text": " Okay, I see nodding, I see confusing faces,", "tokens": [50810, 1033, 11, 286, 536, 15224, 3584, 11, 286, 536, 13181, 8475, 11, 50950], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 204, "seek": 74440, "start": 756.12, "end": 757.48, "text": " but I'll move on whatever.", "tokens": [50950, 457, 286, 603, 1286, 322, 2035, 13, 51018], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 205, "seek": 74440, "start": 759.28, "end": 761.9599999999999, "text": " Okay, so it'll be clear later.", "tokens": [51108, 1033, 11, 370, 309, 603, 312, 1850, 1780, 13, 51242], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 206, "seek": 74440, "start": 761.9599999999999, "end": 766.9599999999999, "text": " So given, so by the Markov property, given Z1, X1,", "tokens": [51242, 407, 2212, 11, 370, 538, 264, 3934, 5179, 4707, 11, 2212, 1176, 16, 11, 1783, 16, 11, 51492], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 207, "seek": 74440, "start": 767.84, "end": 771.6, "text": " and so basically given Z1, this part", "tokens": [51536, 293, 370, 1936, 2212, 1176, 16, 11, 341, 644, 51724], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 208, "seek": 74440, "start": 771.6, "end": 773.72, "text": " and the remaining part will be independent.", "tokens": [51724, 293, 264, 8877, 644, 486, 312, 6695, 13, 51830], "temperature": 0.0, "avg_logprob": -0.22236560111822085, "compression_ratio": 1.4777777777777779, "no_speech_prob": 0.00020021591626573354}, {"id": 209, "seek": 77372, "start": 773.72, "end": 776.1600000000001, "text": " So let's deal with X1 first.", "tokens": [50364, 407, 718, 311, 2028, 365, 1783, 16, 700, 13, 50486], "temperature": 0.0, "avg_logprob": -0.24075603485107422, "compression_ratio": 1.4104477611940298, "no_speech_prob": 7.253287185449153e-05}, {"id": 210, "seek": 77372, "start": 783.88, "end": 786.08, "text": " We have some input distributions,", "tokens": [50872, 492, 362, 512, 4846, 37870, 11, 50982], "temperature": 0.0, "avg_logprob": -0.24075603485107422, "compression_ratio": 1.4104477611940298, "no_speech_prob": 7.253287185449153e-05}, {"id": 211, "seek": 77372, "start": 789.72, "end": 794.08, "text": " and this input distribution will be representing", "tokens": [51164, 293, 341, 4846, 7316, 486, 312, 13460, 51382], "temperature": 0.0, "avg_logprob": -0.24075603485107422, "compression_ratio": 1.4104477611940298, "no_speech_prob": 7.253287185449153e-05}, {"id": 212, "seek": 77372, "start": 794.08, "end": 797.2, "text": " the probability, the emission probability,", "tokens": [51382, 264, 8482, 11, 264, 29513, 8482, 11, 51538], "temperature": 0.0, "avg_logprob": -0.24075603485107422, "compression_ratio": 1.4104477611940298, "no_speech_prob": 7.253287185449153e-05}, {"id": 213, "seek": 77372, "start": 797.2, "end": 800.76, "text": " basically X1 given Z1 equals to I.", "tokens": [51538, 1936, 1783, 16, 2212, 1176, 16, 6915, 281, 286, 13, 51716], "temperature": 0.0, "avg_logprob": -0.24075603485107422, "compression_ratio": 1.4104477611940298, "no_speech_prob": 7.253287185449153e-05}, {"id": 214, "seek": 80076, "start": 801.4399999999999, "end": 805.48, "text": " Okay, so basically here, we're in the states", "tokens": [50398, 1033, 11, 370, 1936, 510, 11, 321, 434, 294, 264, 4368, 50600], "temperature": 0.0, "avg_logprob": -0.24733478992016283, "compression_ratio": 1.4942528735632183, "no_speech_prob": 7.484261732315645e-05}, {"id": 215, "seek": 80076, "start": 805.48, "end": 808.28, "text": " of Z1 equals to I, and this will represent", "tokens": [50600, 295, 1176, 16, 6915, 281, 286, 11, 293, 341, 486, 2906, 50740], "temperature": 0.0, "avg_logprob": -0.24733478992016283, "compression_ratio": 1.4942528735632183, "no_speech_prob": 7.484261732315645e-05}, {"id": 216, "seek": 80076, "start": 808.28, "end": 811.28, "text": " the probability of X1 given at Z1 equals to I.", "tokens": [50740, 264, 8482, 295, 1783, 16, 2212, 412, 1176, 16, 6915, 281, 286, 13, 50890], "temperature": 0.0, "avg_logprob": -0.24733478992016283, "compression_ratio": 1.4942528735632183, "no_speech_prob": 7.484261732315645e-05}, {"id": 217, "seek": 80076, "start": 814.92, "end": 816.56, "text": " I see more nodding faces now.", "tokens": [51072, 286, 536, 544, 15224, 3584, 8475, 586, 13, 51154], "temperature": 0.0, "avg_logprob": -0.24733478992016283, "compression_ratio": 1.4942528735632183, "no_speech_prob": 7.484261732315645e-05}, {"id": 218, "seek": 80076, "start": 820.48, "end": 823.12, "text": " Okay, and we want this part to describe", "tokens": [51350, 1033, 11, 293, 321, 528, 341, 644, 281, 6786, 51482], "temperature": 0.0, "avg_logprob": -0.24733478992016283, "compression_ratio": 1.4942528735632183, "no_speech_prob": 7.484261732315645e-05}, {"id": 219, "seek": 80076, "start": 823.12, "end": 827.68, "text": " the remaining distribution, so we have some nodes here,", "tokens": [51482, 264, 8877, 7316, 11, 370, 321, 362, 512, 13891, 510, 11, 51710], "temperature": 0.0, "avg_logprob": -0.24733478992016283, "compression_ratio": 1.4942528735632183, "no_speech_prob": 7.484261732315645e-05}, {"id": 220, "seek": 82768, "start": 827.8399999999999, "end": 832.8399999999999, "text": " and they will represent the distribution of P of X2", "tokens": [50372, 293, 436, 486, 2906, 264, 7316, 295, 430, 295, 1783, 17, 50622], "temperature": 0.0, "avg_logprob": -0.3638712700377119, "compression_ratio": 1.1904761904761905, "no_speech_prob": 0.00013980947551317513}, {"id": 221, "seek": 82768, "start": 841.12, "end": 846.12, "text": " to N given Z1 equals to I, okay?", "tokens": [51036, 281, 426, 2212, 1176, 16, 6915, 281, 286, 11, 1392, 30, 51286], "temperature": 0.0, "avg_logprob": -0.3638712700377119, "compression_ratio": 1.1904761904761905, "no_speech_prob": 0.00013980947551317513}, {"id": 222, "seek": 82768, "start": 848.4799999999999, "end": 850.68, "text": " Corresponding to the inputs.", "tokens": [51404, 3925, 6663, 278, 281, 264, 15743, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3638712700377119, "compression_ratio": 1.1904761904761905, "no_speech_prob": 0.00013980947551317513}, {"id": 223, "seek": 82768, "start": 850.68, "end": 854.64, "text": " And so we proceed to the next layer.", "tokens": [51514, 400, 370, 321, 8991, 281, 264, 958, 4583, 13, 51712], "temperature": 0.0, "avg_logprob": -0.3638712700377119, "compression_ratio": 1.1904761904761905, "no_speech_prob": 0.00013980947551317513}, {"id": 224, "seek": 85768, "start": 857.68, "end": 862.68, "text": " And this part will be the transition probabilities,", "tokens": [50364, 400, 341, 644, 486, 312, 264, 6034, 33783, 11, 50614], "temperature": 0.0, "avg_logprob": -0.2492940635011907, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00010554272739682347}, {"id": 225, "seek": 85768, "start": 869.0799999999999, "end": 872.92, "text": " will be the transition probability of Z2 equals to J", "tokens": [50934, 486, 312, 264, 6034, 8482, 295, 1176, 17, 6915, 281, 508, 51126], "temperature": 0.0, "avg_logprob": -0.2492940635011907, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00010554272739682347}, {"id": 226, "seek": 85768, "start": 872.92, "end": 877.92, "text": " given Z1 equals to I, and these will recursively,", "tokens": [51126, 2212, 1176, 16, 6915, 281, 286, 11, 293, 613, 486, 20560, 3413, 11, 51376], "temperature": 0.0, "avg_logprob": -0.2492940635011907, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00010554272739682347}, {"id": 227, "seek": 85768, "start": 879.7199999999999, "end": 882.3199999999999, "text": " we do this construction recursively,", "tokens": [51466, 321, 360, 341, 6435, 20560, 3413, 11, 51596], "temperature": 0.0, "avg_logprob": -0.2492940635011907, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00010554272739682347}, {"id": 228, "seek": 85768, "start": 882.3199999999999, "end": 884.1999999999999, "text": " will be representing the probability", "tokens": [51596, 486, 312, 13460, 264, 8482, 51690], "temperature": 0.0, "avg_logprob": -0.2492940635011907, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00010554272739682347}, {"id": 229, "seek": 88420, "start": 884.24, "end": 888.24, "text": " from X2 to N, conditioning on Z2 equals to I, okay?", "tokens": [50366, 490, 1783, 17, 281, 426, 11, 21901, 322, 1176, 17, 6915, 281, 286, 11, 1392, 30, 50566], "temperature": 0.0, "avg_logprob": -0.2857175658020792, "compression_ratio": 1.3457446808510638, "no_speech_prob": 0.00018521255697123706}, {"id": 230, "seek": 88420, "start": 890.2800000000001, "end": 891.12, "text": " Does that make sense?", "tokens": [50668, 4402, 300, 652, 2020, 30, 50710], "temperature": 0.0, "avg_logprob": -0.2857175658020792, "compression_ratio": 1.3457446808510638, "no_speech_prob": 0.00018521255697123706}, {"id": 231, "seek": 88420, "start": 891.12, "end": 896.12, "text": " So we just proceed, so it's like Antonio and Robert", "tokens": [50710, 407, 321, 445, 8991, 11, 370, 309, 311, 411, 22527, 293, 7977, 50960], "temperature": 0.0, "avg_logprob": -0.2857175658020792, "compression_ratio": 1.3457446808510638, "no_speech_prob": 0.00018521255697123706}, {"id": 232, "seek": 88420, "start": 897.4000000000001, "end": 899.72, "text": " was trying to make the point,", "tokens": [51024, 390, 1382, 281, 652, 264, 935, 11, 51140], "temperature": 0.0, "avg_logprob": -0.2857175658020792, "compression_ratio": 1.3457446808510638, "no_speech_prob": 0.00018521255697123706}, {"id": 233, "seek": 88420, "start": 899.72, "end": 903.1600000000001, "text": " this is a tensorized layer representation of a PC.", "tokens": [51140, 341, 307, 257, 40863, 1602, 4583, 10290, 295, 257, 6465, 13, 51312], "temperature": 0.0, "avg_logprob": -0.2857175658020792, "compression_ratio": 1.3457446808510638, "no_speech_prob": 0.00018521255697123706}, {"id": 234, "seek": 88420, "start": 906.6, "end": 911.6, "text": " Okay, let's move on, okay, so now we have our,", "tokens": [51484, 1033, 11, 718, 311, 1286, 322, 11, 1392, 11, 370, 586, 321, 362, 527, 11, 51734], "temperature": 0.0, "avg_logprob": -0.2857175658020792, "compression_ratio": 1.3457446808510638, "no_speech_prob": 0.00018521255697123706}, {"id": 235, "seek": 91420, "start": 914.32, "end": 915.1400000000001, "text": " oh, sorry.", "tokens": [50370, 1954, 11, 2597, 13, 50411], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 236, "seek": 91420, "start": 918.0, "end": 921.72, "text": " So let's move on, so we have our circuit", "tokens": [50554, 407, 718, 311, 1286, 322, 11, 370, 321, 362, 527, 9048, 50740], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 237, "seek": 91420, "start": 921.72, "end": 926.44, "text": " representing the distribution over text,", "tokens": [50740, 13460, 264, 7316, 670, 2487, 11, 50976], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 238, "seek": 91420, "start": 926.44, "end": 928.96, "text": " then we need to answer the query,", "tokens": [50976, 550, 321, 643, 281, 1867, 264, 14581, 11, 51102], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 239, "seek": 91420, "start": 928.96, "end": 931.0400000000001, "text": " we need to encode the logical constraint", "tokens": [51102, 321, 643, 281, 2058, 1429, 264, 14978, 25534, 51206], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 240, "seek": 91420, "start": 931.0400000000001, "end": 932.26, "text": " as the logical circuit.", "tokens": [51206, 382, 264, 14978, 9048, 13, 51267], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 241, "seek": 91420, "start": 935.6, "end": 940.6, "text": " Let's go back to the constraint we have in the very beginning,", "tokens": [51434, 961, 311, 352, 646, 281, 264, 25534, 321, 362, 294, 264, 588, 2863, 11, 51684], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 242, "seek": 91420, "start": 941.12, "end": 943.1800000000001, "text": " we want Frisbee, dog and dog.", "tokens": [51710, 321, 528, 1526, 271, 24872, 11, 3000, 293, 3000, 13, 51813], "temperature": 0.0, "avg_logprob": -0.1897993547370635, "compression_ratio": 1.6416184971098267, "no_speech_prob": 2.9771601475658827e-05}, {"id": 243, "seek": 94420, "start": 944.48, "end": 947.72, "text": " Frisbee, caught and dog to appear in the given order.", "tokens": [50378, 1526, 271, 24872, 11, 5415, 293, 3000, 281, 4204, 294, 264, 2212, 1668, 13, 50540], "temperature": 0.0, "avg_logprob": -0.2584294727870396, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.00016343253082595766}, {"id": 244, "seek": 94420, "start": 950.2800000000001, "end": 953.76, "text": " This is like a naive way to represent this constraint,", "tokens": [50668, 639, 307, 411, 257, 29052, 636, 281, 2906, 341, 25534, 11, 50842], "temperature": 0.0, "avg_logprob": -0.2584294727870396, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.00016343253082595766}, {"id": 245, "seek": 94420, "start": 954.6400000000001, "end": 959.6400000000001, "text": " basically the IJK here are enumerating all possible", "tokens": [50886, 1936, 264, 286, 41, 42, 510, 366, 465, 15583, 990, 439, 1944, 51136], "temperature": 0.0, "avg_logprob": -0.2584294727870396, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.00016343253082595766}, {"id": 246, "seek": 94420, "start": 962.12, "end": 966.0400000000001, "text": " positions of these three words,", "tokens": [51260, 8432, 295, 613, 1045, 2283, 11, 51456], "temperature": 0.0, "avg_logprob": -0.2584294727870396, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.00016343253082595766}, {"id": 247, "seek": 94420, "start": 966.0400000000001, "end": 971.0400000000001, "text": " and we take a conjunction whenever we know the positions.", "tokens": [51456, 293, 321, 747, 257, 27482, 5699, 321, 458, 264, 8432, 13, 51706], "temperature": 0.0, "avg_logprob": -0.2584294727870396, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.00016343253082595766}, {"id": 248, "seek": 94420, "start": 971.9200000000001, "end": 973.0200000000001, "text": " Any questions?", "tokens": [51750, 2639, 1651, 30, 51805], "temperature": 0.0, "avg_logprob": -0.2584294727870396, "compression_ratio": 1.4640883977900552, "no_speech_prob": 0.00016343253082595766}, {"id": 249, "seek": 97420, "start": 975.2, "end": 978.8000000000001, "text": " Okay, okay, but this is not really ideal,", "tokens": [50414, 1033, 11, 1392, 11, 457, 341, 307, 406, 534, 7157, 11, 50594], "temperature": 0.0, "avg_logprob": -0.23400500416755676, "compression_ratio": 1.4533333333333334, "no_speech_prob": 6.204691453604028e-05}, {"id": 250, "seek": 97420, "start": 978.8000000000001, "end": 982.8000000000001, "text": " we can directly convert it into a PC", "tokens": [50594, 321, 393, 3838, 7620, 309, 666, 257, 6465, 50794], "temperature": 0.0, "avg_logprob": -0.23400500416755676, "compression_ratio": 1.4533333333333334, "no_speech_prob": 6.204691453604028e-05}, {"id": 251, "seek": 97420, "start": 982.8000000000001, "end": 984.2800000000001, "text": " that represents the constraint,", "tokens": [50794, 300, 8855, 264, 25534, 11, 50868], "temperature": 0.0, "avg_logprob": -0.23400500416755676, "compression_ratio": 1.4533333333333334, "no_speech_prob": 6.204691453604028e-05}, {"id": 252, "seek": 97420, "start": 984.2800000000001, "end": 989.2800000000001, "text": " but what are the problems, do people see that?", "tokens": [50868, 457, 437, 366, 264, 2740, 11, 360, 561, 536, 300, 30, 51118], "temperature": 0.0, "avg_logprob": -0.23400500416755676, "compression_ratio": 1.4533333333333334, "no_speech_prob": 6.204691453604028e-05}, {"id": 253, "seek": 97420, "start": 997.72, "end": 1001.0400000000001, "text": " I'll give a hint, complexity,", "tokens": [51540, 286, 603, 976, 257, 12075, 11, 14024, 11, 51706], "temperature": 0.0, "avg_logprob": -0.23400500416755676, "compression_ratio": 1.4533333333333334, "no_speech_prob": 6.204691453604028e-05}, {"id": 254, "seek": 97420, "start": 1001.0400000000001, "end": 1002.8000000000001, "text": " what's the complexity of this,", "tokens": [51706, 437, 311, 264, 14024, 295, 341, 11, 51794], "temperature": 0.0, "avg_logprob": -0.23400500416755676, "compression_ratio": 1.4533333333333334, "no_speech_prob": 6.204691453604028e-05}, {"id": 255, "seek": 100280, "start": 1002.8399999999999, "end": 1004.4799999999999, "text": " what's the size of this DNF?", "tokens": [50366, 437, 311, 264, 2744, 295, 341, 21500, 37, 30, 50448], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 256, "seek": 100280, "start": 1007.88, "end": 1010.68, "text": " Yes, cubic, cubic, and to be more precise,", "tokens": [50618, 1079, 11, 28733, 11, 28733, 11, 293, 281, 312, 544, 13600, 11, 50758], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 257, "seek": 100280, "start": 1010.68, "end": 1013.4, "text": " it's n choose number of keywords, right?", "tokens": [50758, 309, 311, 297, 2826, 1230, 295, 21009, 11, 558, 30, 50894], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 258, "seek": 100280, "start": 1013.4, "end": 1015.4799999999999, "text": " Well, I mean, we can do it like cubic,", "tokens": [50894, 1042, 11, 286, 914, 11, 321, 393, 360, 309, 411, 28733, 11, 50998], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 259, "seek": 100280, "start": 1015.4799999999999, "end": 1017.8399999999999, "text": " but suppose we have five or 10 keywords,", "tokens": [50998, 457, 7297, 321, 362, 1732, 420, 1266, 21009, 11, 51116], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 260, "seek": 100280, "start": 1017.8399999999999, "end": 1021.16, "text": " we can no longer take that complexity, okay?", "tokens": [51116, 321, 393, 572, 2854, 747, 300, 14024, 11, 1392, 30, 51282], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 261, "seek": 100280, "start": 1021.16, "end": 1023.88, "text": " And the other problem is more subtle,", "tokens": [51282, 400, 264, 661, 1154, 307, 544, 13743, 11, 51418], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 262, "seek": 100280, "start": 1023.88, "end": 1028.8799999999999, "text": " which is that this DNF is not deterministic, okay?", "tokens": [51418, 597, 307, 300, 341, 21500, 37, 307, 406, 15957, 3142, 11, 1392, 30, 51668], "temperature": 0.0, "avg_logprob": -0.17862569223536123, "compression_ratio": 1.5092592592592593, "no_speech_prob": 0.0002694619761314243}, {"id": 263, "seek": 102888, "start": 1029.7600000000002, "end": 1033.6000000000001, "text": " So we know that we can multiply circuits", "tokens": [50408, 407, 321, 458, 300, 321, 393, 12972, 26354, 50600], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 264, "seek": 102888, "start": 1033.6000000000001, "end": 1035.7600000000002, "text": " when they are compatible,", "tokens": [50600, 562, 436, 366, 18218, 11, 50708], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 265, "seek": 102888, "start": 1035.7600000000002, "end": 1038.2800000000002, "text": " or when they are structured decomposable", "tokens": [50708, 420, 562, 436, 366, 18519, 22867, 329, 712, 50834], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 266, "seek": 102888, "start": 1038.2800000000002, "end": 1040.3200000000002, "text": " with respect to the same vitri,", "tokens": [50834, 365, 3104, 281, 264, 912, 9467, 470, 11, 50936], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 267, "seek": 102888, "start": 1040.3200000000002, "end": 1042.92, "text": " but here we want exact conditioning,", "tokens": [50936, 457, 510, 321, 528, 1900, 21901, 11, 51066], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 268, "seek": 102888, "start": 1042.92, "end": 1047.92, "text": " so we actually need to make sure that our circuit", "tokens": [51066, 370, 321, 767, 643, 281, 652, 988, 300, 527, 9048, 51316], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 269, "seek": 102888, "start": 1049.2800000000002, "end": 1051.3600000000001, "text": " represents a uniform distribution", "tokens": [51384, 8855, 257, 9452, 7316, 51488], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 270, "seek": 102888, "start": 1051.3600000000001, "end": 1055.1200000000001, "text": " over the support specified by this DNF.", "tokens": [51488, 670, 264, 1406, 22206, 538, 341, 21500, 37, 13, 51676], "temperature": 0.0, "avg_logprob": -0.18742135283234831, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.0006562764756381512}, {"id": 271, "seek": 105512, "start": 1055.56, "end": 1060.56, "text": " DNF, and in general, it is sharply hard", "tokens": [50386, 21500, 37, 11, 293, 294, 2674, 11, 309, 307, 8199, 356, 1152, 50636], "temperature": 0.0, "avg_logprob": -0.25296518695888237, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0007913694134913385}, {"id": 272, "seek": 105512, "start": 1062.3999999999999, "end": 1065.2399999999998, "text": " to do model counting to normalize everything.", "tokens": [50728, 281, 360, 2316, 13251, 281, 2710, 1125, 1203, 13, 50870], "temperature": 0.0, "avg_logprob": -0.25296518695888237, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0007913694134913385}, {"id": 273, "seek": 105512, "start": 1065.2399999999998, "end": 1068.32, "text": " Okay, I'll go into the details on the board.", "tokens": [50870, 1033, 11, 286, 603, 352, 666, 264, 4365, 322, 264, 3150, 13, 51024], "temperature": 0.0, "avg_logprob": -0.25296518695888237, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0007913694134913385}, {"id": 274, "seek": 105512, "start": 1069.28, "end": 1073.7199999999998, "text": " Yes, yes, I'll talk about it on the board.", "tokens": [51072, 1079, 11, 2086, 11, 286, 603, 751, 466, 309, 322, 264, 3150, 13, 51294], "temperature": 0.0, "avg_logprob": -0.25296518695888237, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0007913694134913385}, {"id": 275, "seek": 105512, "start": 1079.6, "end": 1081.4399999999998, "text": " Okay, I'll move to the other side.", "tokens": [51588, 1033, 11, 286, 603, 1286, 281, 264, 661, 1252, 13, 51680], "temperature": 0.0, "avg_logprob": -0.25296518695888237, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0007913694134913385}, {"id": 276, "seek": 108512, "start": 1085.12, "end": 1090.12, "text": " Okay, so for the first question,", "tokens": [50364, 1033, 11, 370, 337, 264, 700, 1168, 11, 50614], "temperature": 0.4, "avg_logprob": -0.2561428135839002, "compression_ratio": 1.3984375, "no_speech_prob": 0.00037990682176314294}, {"id": 277, "seek": 108512, "start": 1090.56, "end": 1093.2399999999998, "text": " why does it need to be deterministic?", "tokens": [50636, 983, 775, 309, 643, 281, 312, 15957, 3142, 30, 50770], "temperature": 0.4, "avg_logprob": -0.2561428135839002, "compression_ratio": 1.3984375, "no_speech_prob": 0.00037990682176314294}, {"id": 278, "seek": 108512, "start": 1093.2399999999998, "end": 1096.34, "text": " It is because so, okay, suppose we have a very,", "tokens": [50770, 467, 307, 570, 370, 11, 1392, 11, 7297, 321, 362, 257, 588, 11, 50925], "temperature": 0.4, "avg_logprob": -0.2561428135839002, "compression_ratio": 1.3984375, "no_speech_prob": 0.00037990682176314294}, {"id": 279, "seek": 108512, "start": 1098.1999999999998, "end": 1101.28, "text": " okay, so suppose we have a very simple distribution,", "tokens": [51018, 1392, 11, 370, 7297, 321, 362, 257, 588, 2199, 7316, 11, 51172], "temperature": 0.4, "avg_logprob": -0.2561428135839002, "compression_ratio": 1.3984375, "no_speech_prob": 0.00037990682176314294}, {"id": 280, "seek": 108512, "start": 1101.28, "end": 1104.32, "text": " X1, X2.", "tokens": [51172, 1783, 16, 11, 1783, 17, 13, 51324], "temperature": 0.4, "avg_logprob": -0.2561428135839002, "compression_ratio": 1.3984375, "no_speech_prob": 0.00037990682176314294}, {"id": 281, "seek": 111512, "start": 1115.56, "end": 1120.56, "text": " So, we have some weights,", "tokens": [50386, 407, 11, 321, 362, 512, 17443, 11, 50636], "temperature": 0.0, "avg_logprob": -0.3954162230858436, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.0003799643600359559}, {"id": 282, "seek": 111512, "start": 1128.36, "end": 1132.1599999999999, "text": " so this is a distribution over two random variables,", "tokens": [51026, 370, 341, 307, 257, 7316, 670, 732, 4974, 9102, 11, 51216], "temperature": 0.0, "avg_logprob": -0.3954162230858436, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.0003799643600359559}, {"id": 283, "seek": 111512, "start": 1132.1599999999999, "end": 1135.36, "text": " okay, and this is its probability mass function.", "tokens": [51216, 1392, 11, 293, 341, 307, 1080, 8482, 2758, 2445, 13, 51376], "temperature": 0.0, "avg_logprob": -0.3954162230858436, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.0003799643600359559}, {"id": 284, "seek": 111512, "start": 1140.52, "end": 1142.0, "text": " So when we are conditioning,", "tokens": [51634, 407, 562, 321, 366, 21901, 11, 51708], "temperature": 0.0, "avg_logprob": -0.3954162230858436, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.0003799643600359559}, {"id": 285, "seek": 111512, "start": 1142.0, "end": 1144.84, "text": " we are actually selecting all the terms", "tokens": [51708, 321, 366, 767, 18182, 439, 264, 2115, 51850], "temperature": 0.0, "avg_logprob": -0.3954162230858436, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.0003799643600359559}, {"id": 286, "seek": 114484, "start": 1144.8799999999999, "end": 1146.8799999999999, "text": " that satisfy our constraint", "tokens": [50366, 300, 19319, 527, 25534, 50466], "temperature": 0.0, "avg_logprob": -0.212690740391828, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0001398145832354203}, {"id": 287, "seek": 114484, "start": 1146.8799999999999, "end": 1150.3999999999999, "text": " and zeroing out everything else, right?", "tokens": [50466, 293, 4018, 278, 484, 1203, 1646, 11, 558, 30, 50642], "temperature": 0.0, "avg_logprob": -0.212690740391828, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0001398145832354203}, {"id": 288, "seek": 114484, "start": 1150.3999999999999, "end": 1154.1999999999998, "text": " So if we want to do that with circuit multiplication,", "tokens": [50642, 407, 498, 321, 528, 281, 360, 300, 365, 9048, 27290, 11, 50832], "temperature": 0.0, "avg_logprob": -0.212690740391828, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0001398145832354203}, {"id": 289, "seek": 114484, "start": 1155.04, "end": 1157.72, "text": " suppose our constraint, our support,", "tokens": [50874, 7297, 527, 25534, 11, 527, 1406, 11, 51008], "temperature": 0.0, "avg_logprob": -0.212690740391828, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0001398145832354203}, {"id": 290, "seek": 114484, "start": 1157.72, "end": 1162.72, "text": " would be something like X1, X2, and X1 bar, X2, okay?", "tokens": [51008, 576, 312, 746, 411, 1783, 16, 11, 1783, 17, 11, 293, 1783, 16, 2159, 11, 1783, 17, 11, 1392, 30, 51258], "temperature": 0.0, "avg_logprob": -0.212690740391828, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0001398145832354203}, {"id": 291, "seek": 114484, "start": 1165.84, "end": 1168.08, "text": " So the constraint circuit", "tokens": [51414, 407, 264, 25534, 9048, 51526], "temperature": 0.0, "avg_logprob": -0.212690740391828, "compression_ratio": 1.515923566878981, "no_speech_prob": 0.0001398145832354203}, {"id": 292, "seek": 116808, "start": 1168.52, "end": 1173.52, "text": " should be a uniform distribution over its support,", "tokens": [50386, 820, 312, 257, 9452, 7316, 670, 1080, 1406, 11, 50636], "temperature": 0.0, "avg_logprob": -0.2731012814286826, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.0010986172128468752}, {"id": 293, "seek": 116808, "start": 1177.4399999999998, "end": 1182.4399999999998, "text": " otherwise it messes up with the original weights, right?", "tokens": [50832, 5911, 309, 2082, 279, 493, 365, 264, 3380, 17443, 11, 558, 30, 51082], "temperature": 0.0, "avg_logprob": -0.2731012814286826, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.0010986172128468752}, {"id": 294, "seek": 116808, "start": 1183.36, "end": 1184.4399999999998, "text": " Does that make sense?", "tokens": [51128, 4402, 300, 652, 2020, 30, 51182], "temperature": 0.0, "avg_logprob": -0.2731012814286826, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.0010986172128468752}, {"id": 295, "seek": 116808, "start": 1185.6, "end": 1188.8799999999999, "text": " Okay, so however, okay, let me,", "tokens": [51240, 1033, 11, 370, 4461, 11, 1392, 11, 718, 385, 11, 51404], "temperature": 0.0, "avg_logprob": -0.2731012814286826, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.0010986172128468752}, {"id": 296, "seek": 116808, "start": 1188.8799999999999, "end": 1193.8799999999999, "text": " so this will be 0.5, X1, X2 plus 0.5, X1 bar.", "tokens": [51404, 370, 341, 486, 312, 1958, 13, 20, 11, 1783, 16, 11, 1783, 17, 1804, 1958, 13, 20, 11, 1783, 16, 2159, 13, 51654], "temperature": 0.0, "avg_logprob": -0.2731012814286826, "compression_ratio": 1.344155844155844, "no_speech_prob": 0.0010986172128468752}, {"id": 297, "seek": 119808, "start": 1199.04, "end": 1201.28, "text": " Well, I mean, suppose,", "tokens": [50412, 1042, 11, 286, 914, 11, 7297, 11, 50524], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 298, "seek": 119808, "start": 1203.48, "end": 1204.76, "text": " and we have a bunch of zeros,", "tokens": [50634, 293, 321, 362, 257, 3840, 295, 35193, 11, 50698], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 299, "seek": 119808, "start": 1204.76, "end": 1208.24, "text": " and we multiply these two circuits point-wise,", "tokens": [50698, 293, 321, 12972, 613, 732, 26354, 935, 12, 3711, 11, 50872], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 300, "seek": 119808, "start": 1209.24, "end": 1213.28, "text": " and we kind, we keep these two terms", "tokens": [50922, 293, 321, 733, 11, 321, 1066, 613, 732, 2115, 51124], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 301, "seek": 119808, "start": 1213.28, "end": 1215.4399999999998, "text": " and erase these two terms, right?", "tokens": [51124, 293, 23525, 613, 732, 2115, 11, 558, 30, 51232], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 302, "seek": 119808, "start": 1215.4399999999998, "end": 1219.0, "text": " But we want W and W3 to be proportional to each other,", "tokens": [51232, 583, 321, 528, 343, 293, 343, 18, 281, 312, 24969, 281, 1184, 661, 11, 51410], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 303, "seek": 119808, "start": 1219.0, "end": 1221.8, "text": " like the ratio should stay the same.", "tokens": [51410, 411, 264, 8509, 820, 1754, 264, 912, 13, 51550], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 304, "seek": 119808, "start": 1221.8, "end": 1224.72, "text": " So this circuit must be a uniform distribution", "tokens": [51550, 407, 341, 9048, 1633, 312, 257, 9452, 7316, 51696], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 305, "seek": 119808, "start": 1224.72, "end": 1227.6399999999999, "text": " over the support of constraint, okay?", "tokens": [51696, 670, 264, 1406, 295, 25534, 11, 1392, 30, 51842], "temperature": 0.0, "avg_logprob": -0.17784400939941405, "compression_ratio": 1.6214953271028036, "no_speech_prob": 8.349108247784898e-05}, {"id": 306, "seek": 122764, "start": 1227.68, "end": 1230.24, "text": " So given a logical circuit,", "tokens": [50366, 407, 2212, 257, 14978, 9048, 11, 50494], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 307, "seek": 122764, "start": 1230.24, "end": 1232.44, "text": " how do we convert it into a PC", "tokens": [50494, 577, 360, 321, 7620, 309, 666, 257, 6465, 50604], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 308, "seek": 122764, "start": 1232.44, "end": 1236.2800000000002, "text": " that represents a uniform distribution over the support?", "tokens": [50604, 300, 8855, 257, 9452, 7316, 670, 264, 1406, 30, 50796], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 309, "seek": 122764, "start": 1237.48, "end": 1240.44, "text": " To do that, we need to do model counting,", "tokens": [50856, 1407, 360, 300, 11, 321, 643, 281, 360, 2316, 13251, 11, 51004], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 310, "seek": 122764, "start": 1240.44, "end": 1243.0400000000002, "text": " but model counting in general is hard", "tokens": [51004, 457, 2316, 13251, 294, 2674, 307, 1152, 51134], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 311, "seek": 122764, "start": 1243.0400000000002, "end": 1244.72, "text": " if determinism is missing.", "tokens": [51134, 498, 15957, 1434, 307, 5361, 13, 51218], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 312, "seek": 122764, "start": 1246.24, "end": 1249.76, "text": " So this one won't work.", "tokens": [51294, 407, 341, 472, 1582, 380, 589, 13, 51470], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 313, "seek": 122764, "start": 1252.2, "end": 1253.5200000000002, "text": " Does that make sense?", "tokens": [51592, 4402, 300, 652, 2020, 30, 51658], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 314, "seek": 122764, "start": 1255.64, "end": 1257.2, "text": " Yes, no?", "tokens": [51764, 1079, 11, 572, 30, 51842], "temperature": 0.0, "avg_logprob": -0.20093717919774803, "compression_ratio": 1.4656084656084656, "no_speech_prob": 2.668683009687811e-05}, {"id": 315, "seek": 125720, "start": 1257.2, "end": 1258.04, "text": " Robert?", "tokens": [50364, 7977, 30, 50406], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 316, "seek": 125720, "start": 1259.64, "end": 1262.04, "text": " Isn't that full of a new mistake?", "tokens": [50486, 6998, 380, 300, 1577, 295, 257, 777, 6146, 30, 50606], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 317, "seek": 125720, "start": 1262.04, "end": 1264.64, "text": " Oh yeah, so it's a very subtle, subtle thing.", "tokens": [50606, 876, 1338, 11, 370, 309, 311, 257, 588, 13743, 11, 13743, 551, 13, 50736], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 318, "seek": 125720, "start": 1264.64, "end": 1269.64, "text": " So, well, we only require Frisbee, Caught, and Dog", "tokens": [50736, 407, 11, 731, 11, 321, 787, 3651, 1526, 271, 24872, 11, 7544, 1599, 11, 293, 13472, 50986], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 319, "seek": 125720, "start": 1270.24, "end": 1271.92, "text": " to appear in some positions,", "tokens": [51016, 281, 4204, 294, 512, 8432, 11, 51100], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 320, "seek": 125720, "start": 1271.92, "end": 1276.76, "text": " but we do not limit that they have to appear exactly once.", "tokens": [51100, 457, 321, 360, 406, 4948, 300, 436, 362, 281, 4204, 2293, 1564, 13, 51342], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 321, "seek": 125720, "start": 1276.76, "end": 1281.76, "text": " So you can totally have Frisbee, Frisbee, Caught,", "tokens": [51342, 407, 291, 393, 3879, 362, 1526, 271, 24872, 11, 1526, 271, 24872, 11, 7544, 1599, 11, 51592], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 322, "seek": 125720, "start": 1283.1200000000001, "end": 1286.32, "text": " Caught, Dog, Dog, something like this.", "tokens": [51660, 7544, 1599, 11, 13472, 11, 13472, 11, 746, 411, 341, 13, 51820], "temperature": 0.0, "avg_logprob": -0.26936501438177907, "compression_ratio": 1.5671641791044777, "no_speech_prob": 0.00026525993598625064}, {"id": 323, "seek": 128720, "start": 1287.56, "end": 1289.6000000000001, "text": " So we have two sub-sequences.", "tokens": [50382, 407, 321, 362, 732, 1422, 12, 11834, 2667, 13, 50484], "temperature": 0.0, "avg_logprob": -0.22655576070149738, "compression_ratio": 1.5035971223021583, "no_speech_prob": 0.00011959394760197029}, {"id": 324, "seek": 128720, "start": 1290.4, "end": 1292.68, "text": " We have a lot of sub-sequences, right?", "tokens": [50524, 492, 362, 257, 688, 295, 1422, 12, 11834, 2667, 11, 558, 30, 50638], "temperature": 0.0, "avg_logprob": -0.22655576070149738, "compression_ratio": 1.5035971223021583, "no_speech_prob": 0.00011959394760197029}, {"id": 325, "seek": 128720, "start": 1294.32, "end": 1299.32, "text": " So this is position one, two, three, four, five, six.", "tokens": [50720, 407, 341, 307, 2535, 472, 11, 732, 11, 1045, 11, 1451, 11, 1732, 11, 2309, 13, 50970], "temperature": 0.0, "avg_logprob": -0.22655576070149738, "compression_ratio": 1.5035971223021583, "no_speech_prob": 0.00011959394760197029}, {"id": 326, "seek": 128720, "start": 1301.16, "end": 1305.32, "text": " So when IJK are one, three, five,", "tokens": [51062, 407, 562, 286, 41, 42, 366, 472, 11, 1045, 11, 1732, 11, 51270], "temperature": 0.0, "avg_logprob": -0.22655576070149738, "compression_ratio": 1.5035971223021583, "no_speech_prob": 0.00011959394760197029}, {"id": 327, "seek": 128720, "start": 1306.24, "end": 1309.24, "text": " it satisfies the clauses, right?", "tokens": [51316, 309, 44271, 264, 49072, 11, 558, 30, 51466], "temperature": 0.0, "avg_logprob": -0.22655576070149738, "compression_ratio": 1.5035971223021583, "no_speech_prob": 0.00011959394760197029}, {"id": 328, "seek": 128720, "start": 1309.24, "end": 1310.76, "text": " One of them, right?", "tokens": [51466, 1485, 295, 552, 11, 558, 30, 51542], "temperature": 0.0, "avg_logprob": -0.22655576070149738, "compression_ratio": 1.5035971223021583, "no_speech_prob": 0.00011959394760197029}, {"id": 329, "seek": 131720, "start": 1317.2, "end": 1319.92, "text": " Let me choose a different way.", "tokens": [50364, 961, 385, 2826, 257, 819, 636, 13, 50500], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 330, "seek": 131720, "start": 1319.92, "end": 1324.8, "text": " Okay, so basically for this huge disjunction,", "tokens": [50500, 1033, 11, 370, 1936, 337, 341, 2603, 717, 10010, 882, 11, 50744], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 331, "seek": 131720, "start": 1324.8, "end": 1327.44, "text": " for each instantiation to the variable,", "tokens": [50744, 337, 1184, 9836, 6642, 281, 264, 7006, 11, 50876], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 332, "seek": 131720, "start": 1327.44, "end": 1329.72, "text": " we only satisfy one of the clauses, right?", "tokens": [50876, 321, 787, 19319, 472, 295, 264, 49072, 11, 558, 30, 50990], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 333, "seek": 131720, "start": 1329.72, "end": 1331.32, "text": " We want, that's determinism.", "tokens": [50990, 492, 528, 11, 300, 311, 15957, 1434, 13, 51070], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 334, "seek": 131720, "start": 1333.28, "end": 1334.48, "text": " Yes.", "tokens": [51168, 1079, 13, 51228], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 335, "seek": 131720, "start": 1334.48, "end": 1339.48, "text": " And suppose IJK are one, three, five.", "tokens": [51228, 400, 7297, 286, 41, 42, 366, 472, 11, 1045, 11, 1732, 13, 51478], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 336, "seek": 131720, "start": 1340.24, "end": 1342.28, "text": " Does this instance satisfy that?", "tokens": [51516, 4402, 341, 5197, 19319, 300, 30, 51618], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 337, "seek": 131720, "start": 1342.28, "end": 1343.4, "text": " Yes, it does, right?", "tokens": [51618, 1079, 11, 309, 775, 11, 558, 30, 51674], "temperature": 0.0, "avg_logprob": -0.15727303054306532, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.0005702642956748605}, {"id": 338, "seek": 134340, "start": 1344.4, "end": 1348.0400000000002, "text": " But when IJK are one, four, five,", "tokens": [50414, 583, 562, 286, 41, 42, 366, 472, 11, 1451, 11, 1732, 11, 50596], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 339, "seek": 134340, "start": 1348.0400000000002, "end": 1349.5600000000002, "text": " it also satisfies that.", "tokens": [50596, 309, 611, 44271, 300, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 340, "seek": 134340, "start": 1349.5600000000002, "end": 1350.4, "text": " So it's not.", "tokens": [50672, 407, 309, 311, 406, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 341, "seek": 134340, "start": 1351.3600000000001, "end": 1353.1200000000001, "text": " Does that make sense?", "tokens": [50762, 4402, 300, 652, 2020, 30, 50850], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 342, "seek": 134340, "start": 1353.1200000000001, "end": 1353.96, "text": " Okay.", "tokens": [50850, 1033, 13, 50892], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 343, "seek": 134340, "start": 1353.96, "end": 1355.72, "text": " Okay, so now let's construct", "tokens": [50892, 1033, 11, 370, 586, 718, 311, 7690, 50980], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 344, "seek": 134340, "start": 1355.72, "end": 1358.3200000000002, "text": " a deterministic circuit representing this constraint.", "tokens": [50980, 257, 15957, 3142, 9048, 13460, 341, 25534, 13, 51110], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 345, "seek": 134340, "start": 1360.52, "end": 1365.52, "text": " The idea is similar to a deterministic finite automata.", "tokens": [51220, 440, 1558, 307, 2531, 281, 257, 15957, 3142, 19362, 3553, 3274, 13, 51470], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 346, "seek": 134340, "start": 1366.64, "end": 1371.44, "text": " Basically, you track how many words have you included,", "tokens": [51526, 8537, 11, 291, 2837, 577, 867, 2283, 362, 291, 5556, 11, 51766], "temperature": 0.0, "avg_logprob": -0.2646687825520833, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.0001488269481342286}, {"id": 347, "seek": 137144, "start": 1371.44, "end": 1372.6000000000001, "text": " how many words have you used,", "tokens": [50364, 577, 867, 2283, 362, 291, 1143, 11, 50422], "temperature": 0.0, "avg_logprob": -0.3605468879311772, "compression_ratio": 1.5467625899280575, "no_speech_prob": 0.0003149466647300869}, {"id": 348, "seek": 137144, "start": 1372.6000000000001, "end": 1375.48, "text": " which state are you in satisfying the constraint?", "tokens": [50422, 597, 1785, 366, 291, 294, 18348, 264, 25534, 30, 50566], "temperature": 0.0, "avg_logprob": -0.3605468879311772, "compression_ratio": 1.5467625899280575, "no_speech_prob": 0.0003149466647300869}, {"id": 349, "seek": 137144, "start": 1376.72, "end": 1379.52, "text": " So we want to construct these sub formulas,", "tokens": [50628, 407, 321, 528, 281, 7690, 613, 1422, 30546, 11, 50768], "temperature": 0.0, "avg_logprob": -0.3605468879311772, "compression_ratio": 1.5467625899280575, "no_speech_prob": 0.0003149466647300869}, {"id": 350, "seek": 137144, "start": 1379.52, "end": 1383.52, "text": " V, T, say cod and dog.", "tokens": [50768, 691, 11, 314, 11, 584, 17656, 293, 3000, 13, 50968], "temperature": 0.0, "avg_logprob": -0.3605468879311772, "compression_ratio": 1.5467625899280575, "no_speech_prob": 0.0003149466647300869}, {"id": 351, "seek": 137144, "start": 1387.72, "end": 1390.56, "text": " The sub formula representing the constraint", "tokens": [51178, 440, 1422, 8513, 13460, 264, 25534, 51320], "temperature": 0.0, "avg_logprob": -0.3605468879311772, "compression_ratio": 1.5467625899280575, "no_speech_prob": 0.0003149466647300869}, {"id": 352, "seek": 137144, "start": 1390.56, "end": 1395.56, "text": " that cod and dog appears", "tokens": [51320, 300, 17656, 293, 3000, 7038, 51570], "temperature": 0.0, "avg_logprob": -0.3605468879311772, "compression_ratio": 1.5467625899280575, "no_speech_prob": 0.0003149466647300869}, {"id": 353, "seek": 139556, "start": 1396.56, "end": 1401.56, "text": " in XT, XT plus one.", "tokens": [50414, 294, 1783, 51, 11, 1783, 51, 1804, 472, 13, 50664], "temperature": 0.0, "avg_logprob": -0.25177486126239484, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0007205662550404668}, {"id": 354, "seek": 139556, "start": 1407.3999999999999, "end": 1411.6399999999999, "text": " So how do we construct this sub circuit or this sub formula?", "tokens": [50956, 407, 577, 360, 321, 7690, 341, 1422, 9048, 420, 341, 1422, 8513, 30, 51168], "temperature": 0.0, "avg_logprob": -0.25177486126239484, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0007205662550404668}, {"id": 355, "seek": 139556, "start": 1411.6399999999999, "end": 1412.6799999999998, "text": " Oh, it's pretty simple.", "tokens": [51168, 876, 11, 309, 311, 1238, 2199, 13, 51220], "temperature": 0.0, "avg_logprob": -0.25177486126239484, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0007205662550404668}, {"id": 356, "seek": 139556, "start": 1412.6799999999998, "end": 1414.6799999999998, "text": " It's a sum node.", "tokens": [51220, 467, 311, 257, 2408, 9984, 13, 51320], "temperature": 0.0, "avg_logprob": -0.25177486126239484, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0007205662550404668}, {"id": 357, "seek": 139556, "start": 1414.6799999999998, "end": 1416.6399999999999, "text": " We consider two different cases.", "tokens": [51320, 492, 1949, 732, 819, 3331, 13, 51418], "temperature": 0.0, "avg_logprob": -0.25177486126239484, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0007205662550404668}, {"id": 358, "seek": 139556, "start": 1417.8, "end": 1421.48, "text": " One of the cases that XT is cod.", "tokens": [51476, 1485, 295, 264, 3331, 300, 1783, 51, 307, 17656, 13, 51660], "temperature": 0.0, "avg_logprob": -0.25177486126239484, "compression_ratio": 1.326241134751773, "no_speech_prob": 0.0007205662550404668}, {"id": 359, "seek": 142148, "start": 1422.32, "end": 1427.32, "text": " And the other case is that XT is not cod.", "tokens": [50406, 400, 264, 661, 1389, 307, 300, 1783, 51, 307, 406, 17656, 13, 50656], "temperature": 0.0, "avg_logprob": -0.3380082915810978, "compression_ratio": 1.4026845637583893, "no_speech_prob": 9.609821427147835e-05}, {"id": 360, "seek": 142148, "start": 1432.84, "end": 1434.64, "text": " Does this make sense?", "tokens": [50932, 4402, 341, 652, 2020, 30, 51022], "temperature": 0.0, "avg_logprob": -0.3380082915810978, "compression_ratio": 1.4026845637583893, "no_speech_prob": 9.609821427147835e-05}, {"id": 361, "seek": 142148, "start": 1434.64, "end": 1435.48, "text": " Okay.", "tokens": [51022, 1033, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3380082915810978, "compression_ratio": 1.4026845637583893, "no_speech_prob": 9.609821427147835e-05}, {"id": 362, "seek": 142148, "start": 1435.48, "end": 1440.28, "text": " So if XT is cod, then we have like step one.", "tokens": [51064, 407, 498, 1783, 51, 307, 17656, 11, 550, 321, 362, 411, 1823, 472, 13, 51304], "temperature": 0.0, "avg_logprob": -0.3380082915810978, "compression_ratio": 1.4026845637583893, "no_speech_prob": 9.609821427147835e-05}, {"id": 363, "seek": 142148, "start": 1442.68, "end": 1445.4, "text": " We have made one step towards satisfying the constraint.", "tokens": [51424, 492, 362, 1027, 472, 1823, 3030, 18348, 264, 25534, 13, 51560], "temperature": 0.0, "avg_logprob": -0.3380082915810978, "compression_ratio": 1.4026845637583893, "no_speech_prob": 9.609821427147835e-05}, {"id": 364, "seek": 142148, "start": 1447.48, "end": 1451.1200000000001, "text": " So we can reduce it to V, T plus one.", "tokens": [51664, 407, 321, 393, 5407, 309, 281, 691, 11, 314, 1804, 472, 13, 51846], "temperature": 0.0, "avg_logprob": -0.3380082915810978, "compression_ratio": 1.4026845637583893, "no_speech_prob": 9.609821427147835e-05}, {"id": 365, "seek": 145148, "start": 1452.08, "end": 1452.92, "text": " Dog.", "tokens": [50394, 13472, 13, 50436], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 366, "seek": 145148, "start": 1454.4, "end": 1456.4, "text": " And suppose XT is not cod.", "tokens": [50510, 400, 7297, 1783, 51, 307, 406, 17656, 13, 50610], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 367, "seek": 145148, "start": 1456.4, "end": 1457.72, "text": " We're in the same state.", "tokens": [50610, 492, 434, 294, 264, 912, 1785, 13, 50676], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 368, "seek": 145148, "start": 1459.0, "end": 1461.04, "text": " We reduce it to T plus one.", "tokens": [50740, 492, 5407, 309, 281, 314, 1804, 472, 13, 50842], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 369, "seek": 145148, "start": 1464.76, "end": 1465.6, "text": " Dog.", "tokens": [51028, 13472, 13, 51070], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 370, "seek": 145148, "start": 1468.56, "end": 1469.4, "text": " Is that clear?", "tokens": [51218, 1119, 300, 1850, 30, 51260], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 371, "seek": 145148, "start": 1470.28, "end": 1473.72, "text": " So suppose we have constructed FI", "tokens": [51304, 407, 7297, 321, 362, 17083, 479, 40, 51476], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 372, "seek": 145148, "start": 1473.72, "end": 1478.72, "text": " for all time step greater than T", "tokens": [51476, 337, 439, 565, 1823, 5044, 813, 314, 51726], "temperature": 0.0, "avg_logprob": -0.3889408414326017, "compression_ratio": 1.3053435114503817, "no_speech_prob": 9.46061045397073e-05}, {"id": 373, "seek": 147872, "start": 1479.56, "end": 1482.8, "text": " then we can construct all the fees for T.", "tokens": [50406, 550, 321, 393, 7690, 439, 264, 13370, 337, 314, 13, 50568], "temperature": 0.0, "avg_logprob": -0.34753233194351196, "compression_ratio": 1.3216783216783217, "no_speech_prob": 0.000511141202878207}, {"id": 374, "seek": 147872, "start": 1485.8, "end": 1487.68, "text": " So it's a recursive algorithm.", "tokens": [50718, 407, 309, 311, 257, 20560, 488, 9284, 13, 50812], "temperature": 0.0, "avg_logprob": -0.34753233194351196, "compression_ratio": 1.3216783216783217, "no_speech_prob": 0.000511141202878207}, {"id": 375, "seek": 147872, "start": 1491.4, "end": 1492.52, "text": " Oh, what's the base case?", "tokens": [50998, 876, 11, 437, 311, 264, 3096, 1389, 30, 51054], "temperature": 0.0, "avg_logprob": -0.34753233194351196, "compression_ratio": 1.3216783216783217, "no_speech_prob": 0.000511141202878207}, {"id": 376, "seek": 147872, "start": 1492.52, "end": 1494.1200000000001, "text": " Yes, that's very interesting.", "tokens": [51054, 1079, 11, 300, 311, 588, 1880, 13, 51134], "temperature": 0.0, "avg_logprob": -0.34753233194351196, "compression_ratio": 1.3216783216783217, "no_speech_prob": 0.000511141202878207}, {"id": 377, "seek": 147872, "start": 1498.24, "end": 1499.68, "text": " Okay, I'm gonna erase this.", "tokens": [51340, 1033, 11, 286, 478, 799, 23525, 341, 13, 51412], "temperature": 0.0, "avg_logprob": -0.34753233194351196, "compression_ratio": 1.3216783216783217, "no_speech_prob": 0.000511141202878207}, {"id": 378, "seek": 147872, "start": 1503.1200000000001, "end": 1506.52, "text": " So just cod and dog in the part.", "tokens": [51584, 407, 445, 17656, 293, 3000, 294, 264, 644, 13, 51754], "temperature": 0.0, "avg_logprob": -0.34753233194351196, "compression_ratio": 1.3216783216783217, "no_speech_prob": 0.000511141202878207}, {"id": 379, "seek": 150652, "start": 1506.52, "end": 1510.32, "text": " So yeah, I mean, we can make it even simpler.", "tokens": [50364, 407, 1338, 11, 286, 914, 11, 321, 393, 652, 309, 754, 18587, 13, 50554], "temperature": 0.0, "avg_logprob": -0.3152597215440538, "compression_ratio": 1.3759398496240602, "no_speech_prob": 4.683416773332283e-05}, {"id": 380, "seek": 150652, "start": 1510.32, "end": 1512.32, "text": " I'm lazy, so I wanna make things simpler.", "tokens": [50554, 286, 478, 14847, 11, 370, 286, 1948, 652, 721, 18587, 13, 50654], "temperature": 0.0, "avg_logprob": -0.3152597215440538, "compression_ratio": 1.3759398496240602, "no_speech_prob": 4.683416773332283e-05}, {"id": 381, "seek": 150652, "start": 1514.16, "end": 1516.44, "text": " So suppose in the nth position,", "tokens": [50746, 407, 7297, 294, 264, 297, 392, 2535, 11, 50860], "temperature": 0.0, "avg_logprob": -0.3152597215440538, "compression_ratio": 1.3759398496240602, "no_speech_prob": 4.683416773332283e-05}, {"id": 382, "seek": 150652, "start": 1518.16, "end": 1520.52, "text": " we only have one word, right?", "tokens": [50946, 321, 787, 362, 472, 1349, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.3152597215440538, "compression_ratio": 1.3759398496240602, "no_speech_prob": 4.683416773332283e-05}, {"id": 383, "seek": 150652, "start": 1521.76, "end": 1523.76, "text": " So fee and,", "tokens": [51126, 407, 12054, 293, 11, 51226], "temperature": 0.0, "avg_logprob": -0.3152597215440538, "compression_ratio": 1.3759398496240602, "no_speech_prob": 4.683416773332283e-05}, {"id": 384, "seek": 150652, "start": 1530.24, "end": 1532.12, "text": " what is this formula?", "tokens": [51550, 437, 307, 341, 8513, 30, 51644], "temperature": 0.0, "avg_logprob": -0.3152597215440538, "compression_ratio": 1.3759398496240602, "no_speech_prob": 4.683416773332283e-05}, {"id": 385, "seek": 153212, "start": 1532.12, "end": 1537.12, "text": " It means that from Xn to Xn,", "tokens": [50364, 467, 1355, 300, 490, 1783, 77, 281, 1783, 77, 11, 50614], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 386, "seek": 153212, "start": 1538.2399999999998, "end": 1540.9599999999998, "text": " we need to have cod and dog.", "tokens": [50670, 321, 643, 281, 362, 17656, 293, 3000, 13, 50806], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 387, "seek": 153212, "start": 1540.9599999999998, "end": 1543.52, "text": " So this is false, right?", "tokens": [50806, 407, 341, 307, 7908, 11, 558, 30, 50934], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 388, "seek": 153212, "start": 1543.52, "end": 1544.36, "text": " Zero.", "tokens": [50934, 17182, 13, 50976], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 389, "seek": 153212, "start": 1547.36, "end": 1549.28, "text": " What if we only have dog?", "tokens": [51126, 708, 498, 321, 787, 362, 3000, 30, 51222], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 390, "seek": 153212, "start": 1549.28, "end": 1554.28, "text": " So this is true only if Xn equals dog.", "tokens": [51222, 407, 341, 307, 2074, 787, 498, 1783, 77, 6915, 3000, 13, 51472], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 391, "seek": 153212, "start": 1557.52, "end": 1559.6399999999999, "text": " So yeah, those are the base cases.", "tokens": [51634, 407, 1338, 11, 729, 366, 264, 3096, 3331, 13, 51740], "temperature": 0.0, "avg_logprob": -0.22427279608590261, "compression_ratio": 1.4029850746268657, "no_speech_prob": 0.00045107415644451976}, {"id": 392, "seek": 155964, "start": 1560.48, "end": 1565.48, "text": " Okay, so what's the size of the circuit?", "tokens": [50406, 1033, 11, 370, 437, 311, 264, 2744, 295, 264, 9048, 30, 50656], "temperature": 0.0, "avg_logprob": -0.2805843999830343, "compression_ratio": 1.3475177304964538, "no_speech_prob": 6.501188909169286e-05}, {"id": 393, "seek": 155964, "start": 1567.0400000000002, "end": 1568.76, "text": " So at each time step,", "tokens": [50734, 407, 412, 1184, 565, 1823, 11, 50820], "temperature": 0.0, "avg_logprob": -0.2805843999830343, "compression_ratio": 1.3475177304964538, "no_speech_prob": 6.501188909169286e-05}, {"id": 394, "seek": 155964, "start": 1570.4, "end": 1575.4, "text": " we need fee T, maybe we have already satisfied the constraint.", "tokens": [50902, 321, 643, 12054, 314, 11, 1310, 321, 362, 1217, 11239, 264, 25534, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2805843999830343, "compression_ratio": 1.3475177304964538, "no_speech_prob": 6.501188909169286e-05}, {"id": 395, "seek": 155964, "start": 1575.4, "end": 1578.88, "text": " So something empty, right?", "tokens": [51152, 407, 746, 6707, 11, 558, 30, 51326], "temperature": 0.0, "avg_logprob": -0.2805843999830343, "compression_ratio": 1.3475177304964538, "no_speech_prob": 6.501188909169286e-05}, {"id": 396, "seek": 155964, "start": 1578.88, "end": 1583.88, "text": " We need fee T, we have one word left.", "tokens": [51326, 492, 643, 12054, 314, 11, 321, 362, 472, 1349, 1411, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2805843999830343, "compression_ratio": 1.3475177304964538, "no_speech_prob": 6.501188909169286e-05}, {"id": 397, "seek": 158388, "start": 1584.88, "end": 1589.24, "text": " Fee T, we have cod and dog, we have two word left.", "tokens": [50414, 479, 1653, 314, 11, 321, 362, 17656, 293, 3000, 11, 321, 362, 732, 1349, 1411, 13, 50632], "temperature": 0.0, "avg_logprob": -0.2921691576639811, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0009108675876632333}, {"id": 398, "seek": 158388, "start": 1590.8400000000001, "end": 1594.2, "text": " And we have generated none of them yet.", "tokens": [50712, 400, 321, 362, 10833, 6022, 295, 552, 1939, 13, 50880], "temperature": 0.0, "avg_logprob": -0.2921691576639811, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0009108675876632333}, {"id": 399, "seek": 158388, "start": 1598.5200000000002, "end": 1601.8000000000002, "text": " Okay, so at each time step,", "tokens": [51096, 1033, 11, 370, 412, 1184, 565, 1823, 11, 51260], "temperature": 0.0, "avg_logprob": -0.2921691576639811, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0009108675876632333}, {"id": 400, "seek": 158388, "start": 1601.8000000000002, "end": 1606.8000000000002, "text": " we have four sum notes or four more notes", "tokens": [51260, 321, 362, 1451, 2408, 5570, 420, 1451, 544, 5570, 51510], "temperature": 0.0, "avg_logprob": -0.2921691576639811, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0009108675876632333}, {"id": 401, "seek": 158388, "start": 1607.3200000000002, "end": 1609.44, "text": " representing all these four cases.", "tokens": [51536, 13460, 439, 613, 1451, 3331, 13, 51642], "temperature": 0.0, "avg_logprob": -0.2921691576639811, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0009108675876632333}, {"id": 402, "seek": 160944, "start": 1610.4, "end": 1614.68, "text": " And the size, the number of notes in the circuit", "tokens": [50412, 400, 264, 2744, 11, 264, 1230, 295, 5570, 294, 264, 9048, 50626], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 403, "seek": 160944, "start": 1614.68, "end": 1615.92, "text": " would be four times.", "tokens": [50626, 576, 312, 1451, 1413, 13, 50688], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 404, "seek": 160944, "start": 1617.76, "end": 1619.24, "text": " Okay, does that make sense?", "tokens": [50780, 1033, 11, 775, 300, 652, 2020, 30, 50854], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 405, "seek": 160944, "start": 1622.24, "end": 1625.44, "text": " And you can notice that when we are constructing this circuit,", "tokens": [51004, 400, 291, 393, 3449, 300, 562, 321, 366, 39969, 341, 9048, 11, 51164], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 406, "seek": 160944, "start": 1625.44, "end": 1628.28, "text": " we are always conditioning on XT,", "tokens": [51164, 321, 366, 1009, 21901, 322, 1783, 51, 11, 51306], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 407, "seek": 160944, "start": 1628.28, "end": 1630.68, "text": " current variable, which is very similar to HMM.", "tokens": [51306, 2190, 7006, 11, 597, 307, 588, 2531, 281, 389, 17365, 13, 51426], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 408, "seek": 160944, "start": 1630.68, "end": 1634.24, "text": " It generates one variable at a time, one word at a time.", "tokens": [51426, 467, 23815, 472, 7006, 412, 257, 565, 11, 472, 1349, 412, 257, 565, 13, 51604], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 409, "seek": 160944, "start": 1634.24, "end": 1636.6000000000001, "text": " So it is compatible with HMM.", "tokens": [51604, 407, 309, 307, 18218, 365, 389, 17365, 13, 51722], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 410, "seek": 160944, "start": 1636.6000000000001, "end": 1638.28, "text": " And also it's deterministic.", "tokens": [51722, 400, 611, 309, 311, 15957, 3142, 13, 51806], "temperature": 0.0, "avg_logprob": -0.23166140388039982, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.0002340922219445929}, {"id": 411, "seek": 163944, "start": 1640.3200000000002, "end": 1643.16, "text": " And we can do the apply operation,", "tokens": [50408, 400, 321, 393, 360, 264, 3079, 6916, 11, 50550], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 412, "seek": 163944, "start": 1643.16, "end": 1645.4, "text": " the product operation layer-wise.", "tokens": [50550, 264, 1674, 6916, 4583, 12, 3711, 13, 50662], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 413, "seek": 163944, "start": 1645.4, "end": 1648.28, "text": " So the size of the eventual circuit", "tokens": [50662, 407, 264, 2744, 295, 264, 33160, 9048, 50806], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 414, "seek": 163944, "start": 1649.76, "end": 1653.56, "text": " would be four, so originally,", "tokens": [50880, 576, 312, 1451, 11, 370, 7993, 11, 51070], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 415, "seek": 163944, "start": 1653.56, "end": 1658.56, "text": " so originally, here for each layer, we have H nodes.", "tokens": [51070, 370, 7993, 11, 510, 337, 1184, 4583, 11, 321, 362, 389, 13891, 13, 51320], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 416, "seek": 163944, "start": 1659.6000000000001, "end": 1663.04, "text": " Now we, here for each layer, we have four nodes.", "tokens": [51372, 823, 321, 11, 510, 337, 1184, 4583, 11, 321, 362, 1451, 13891, 13, 51544], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 417, "seek": 163944, "start": 1663.04, "end": 1666.6000000000001, "text": " So eventually we have four H nodes just for each layer.", "tokens": [51544, 407, 4728, 321, 362, 1451, 389, 13891, 445, 337, 1184, 4583, 13, 51722], "temperature": 0.0, "avg_logprob": -0.24190447703901544, "compression_ratio": 1.8598726114649682, "no_speech_prob": 0.0003568426473066211}, {"id": 418, "seek": 166660, "start": 1667.6, "end": 1671.6, "text": " Acceptable, okay, does that make sense?", "tokens": [50414, 39957, 712, 11, 1392, 11, 775, 300, 652, 2020, 30, 50614], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 419, "seek": 166660, "start": 1673.28, "end": 1675.36, "text": " So we can still generate this.", "tokens": [50698, 407, 321, 393, 920, 8460, 341, 13, 50802], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 420, "seek": 166660, "start": 1676.36, "end": 1678.48, "text": " We can generate this even further.", "tokens": [50852, 492, 393, 8460, 341, 754, 3052, 13, 50958], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 421, "seek": 166660, "start": 1678.48, "end": 1682.0, "text": " Well, some time, well, one simple generalization is,", "tokens": [50958, 1042, 11, 512, 565, 11, 731, 11, 472, 2199, 2674, 2144, 307, 11, 51134], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 422, "seek": 166660, "start": 1682.8799999999999, "end": 1686.56, "text": " you might think it doesn't have to be caught, right?", "tokens": [51178, 291, 1062, 519, 309, 1177, 380, 362, 281, 312, 5415, 11, 558, 30, 51362], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 423, "seek": 166660, "start": 1686.56, "end": 1690.52, "text": " It can be caught or either catch, right?", "tokens": [51362, 467, 393, 312, 5415, 420, 2139, 3745, 11, 558, 30, 51560], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 424, "seek": 166660, "start": 1690.52, "end": 1695.52, "text": " Or catches, they all kind of represent the same concept.", "tokens": [51560, 1610, 25496, 11, 436, 439, 733, 295, 2906, 264, 912, 3410, 13, 51810], "temperature": 0.0, "avg_logprob": -0.29357530878878185, "compression_ratio": 1.5606060606060606, "no_speech_prob": 0.00026940679526887834}, {"id": 425, "seek": 169660, "start": 1697.12, "end": 1700.56, "text": " So in the construction, we can", "tokens": [50390, 407, 294, 264, 6435, 11, 321, 393, 50562], "temperature": 0.0, "avg_logprob": -0.29096799426608616, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.0002165252954000607}, {"id": 426, "seek": 169660, "start": 1703.84, "end": 1706.56, "text": " modify our original circuit a little bit.", "tokens": [50726, 16927, 527, 3380, 9048, 257, 707, 857, 13, 50862], "temperature": 0.0, "avg_logprob": -0.29096799426608616, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.0002165252954000607}, {"id": 427, "seek": 169660, "start": 1707.52, "end": 1711.56, "text": " Before it was XT equals caught,", "tokens": [50910, 4546, 309, 390, 1783, 51, 6915, 5415, 11, 51112], "temperature": 0.0, "avg_logprob": -0.29096799426608616, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.0002165252954000607}, {"id": 428, "seek": 169660, "start": 1713.1599999999999, "end": 1716.12, "text": " we can replace it with another OR node,", "tokens": [51192, 321, 393, 7406, 309, 365, 1071, 19654, 9984, 11, 51340], "temperature": 0.0, "avg_logprob": -0.29096799426608616, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.0002165252954000607}, {"id": 429, "seek": 169660, "start": 1718.56, "end": 1721.7199999999998, "text": " enumerating caught, catch, and catches.", "tokens": [51462, 465, 15583, 990, 5415, 11, 3745, 11, 293, 25496, 13, 51620], "temperature": 0.0, "avg_logprob": -0.29096799426608616, "compression_ratio": 1.362962962962963, "no_speech_prob": 0.0002165252954000607}, {"id": 430, "seek": 172172, "start": 1722.32, "end": 1726.52, "text": " Okay, and the circuit size stays roughly the same.", "tokens": [50394, 1033, 11, 293, 264, 9048, 2744, 10834, 9810, 264, 912, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 431, "seek": 172172, "start": 1728.08, "end": 1729.92, "text": " And there are many other things we can do.", "tokens": [50682, 400, 456, 366, 867, 661, 721, 321, 393, 360, 13, 50774], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 432, "seek": 172172, "start": 1729.92, "end": 1732.6000000000001, "text": " We can also say, oh, we don't need them", "tokens": [50774, 492, 393, 611, 584, 11, 1954, 11, 321, 500, 380, 643, 552, 50908], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 433, "seek": 172172, "start": 1732.6000000000001, "end": 1735.16, "text": " to be in the same order, in a fixed order.", "tokens": [50908, 281, 312, 294, 264, 912, 1668, 11, 294, 257, 6806, 1668, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 434, "seek": 172172, "start": 1735.16, "end": 1738.44, "text": " We can, we want them to be like in arbitrary order.", "tokens": [51036, 492, 393, 11, 321, 528, 552, 281, 312, 411, 294, 23211, 1668, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 435, "seek": 172172, "start": 1738.44, "end": 1739.64, "text": " We don't care about the order.", "tokens": [51200, 492, 500, 380, 1127, 466, 264, 1668, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 436, "seek": 172172, "start": 1739.64, "end": 1741.4, "text": " How do we do that?", "tokens": [51260, 1012, 360, 321, 360, 300, 30, 51348], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 437, "seek": 172172, "start": 1741.4, "end": 1744.64, "text": " So in this case, we have four states, right?", "tokens": [51348, 407, 294, 341, 1389, 11, 321, 362, 1451, 4368, 11, 558, 30, 51510], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 438, "seek": 172172, "start": 1744.64, "end": 1749.64, "text": " Basically enumerating all suffixes of these three keywords.", "tokens": [51510, 8537, 465, 15583, 990, 439, 3889, 36005, 295, 613, 1045, 21009, 13, 51760], "temperature": 0.0, "avg_logprob": -0.1430560802591258, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.914730617310852e-05}, {"id": 439, "seek": 174964, "start": 1750.64, "end": 1755.0400000000002, "text": " In that case, we would have all subsets of these three keywords.", "tokens": [50414, 682, 300, 1389, 11, 321, 576, 362, 439, 2090, 1385, 295, 613, 1045, 21009, 13, 50634], "temperature": 0.0, "avg_logprob": -0.3119633629208519, "compression_ratio": 1.755, "no_speech_prob": 0.0010320391738787293}, {"id": 440, "seek": 174964, "start": 1755.0400000000002, "end": 1760.0400000000002, "text": " So in that way, the complexity would be going from four,", "tokens": [50634, 407, 294, 300, 636, 11, 264, 14024, 576, 312, 516, 490, 1451, 11, 50884], "temperature": 0.0, "avg_logprob": -0.3119633629208519, "compression_ratio": 1.755, "no_speech_prob": 0.0010320391738787293}, {"id": 441, "seek": 174964, "start": 1761.0400000000002, "end": 1765.4, "text": " we have three keywords, and this is four, two, two to the three.", "tokens": [50934, 321, 362, 1045, 21009, 11, 293, 341, 307, 1451, 11, 732, 11, 732, 281, 264, 1045, 13, 51152], "temperature": 0.0, "avg_logprob": -0.3119633629208519, "compression_ratio": 1.755, "no_speech_prob": 0.0010320391738787293}, {"id": 442, "seek": 174964, "start": 1765.4, "end": 1769.4, "text": " That's there, eight subsets, two to the three subsets, okay?", "tokens": [51152, 663, 311, 456, 11, 3180, 2090, 1385, 11, 732, 281, 264, 1045, 2090, 1385, 11, 1392, 30, 51352], "temperature": 0.0, "avg_logprob": -0.3119633629208519, "compression_ratio": 1.755, "no_speech_prob": 0.0010320391738787293}, {"id": 443, "seek": 174964, "start": 1769.4, "end": 1774.4, "text": " So still acceptable, two to the n is not really,", "tokens": [51352, 407, 920, 15513, 11, 732, 281, 264, 297, 307, 406, 534, 11, 51602], "temperature": 0.0, "avg_logprob": -0.3119633629208519, "compression_ratio": 1.755, "no_speech_prob": 0.0010320391738787293}, {"id": 444, "seek": 174964, "start": 1776.0400000000002, "end": 1778.88, "text": " say we have 10 keywords, two to the n, it's just 1024.", "tokens": [51684, 584, 321, 362, 1266, 21009, 11, 732, 281, 264, 297, 11, 309, 311, 445, 1266, 7911, 13, 51826], "temperature": 0.0, "avg_logprob": -0.3119633629208519, "compression_ratio": 1.755, "no_speech_prob": 0.0010320391738787293}, {"id": 445, "seek": 177888, "start": 1779.16, "end": 1781.72, "text": " It's still doable in practice, yes.", "tokens": [50378, 467, 311, 920, 41183, 294, 3124, 11, 2086, 13, 50506], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 446, "seek": 177888, "start": 1781.72, "end": 1785.72, "text": " So it seems that you can do any kind of regular expression", "tokens": [50506, 407, 309, 2544, 300, 291, 393, 360, 604, 733, 295, 3890, 6114, 50706], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 447, "seek": 177888, "start": 1785.72, "end": 1790.2, "text": " constrained, but you're focusing here on language", "tokens": [50706, 38901, 11, 457, 291, 434, 8416, 510, 322, 2856, 50930], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 448, "seek": 177888, "start": 1790.2, "end": 1792.0800000000002, "text": " as a fixed length, right?", "tokens": [50930, 382, 257, 6806, 4641, 11, 558, 30, 51024], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 449, "seek": 177888, "start": 1792.0800000000002, "end": 1797.0800000000002, "text": " Because you have a fixed number of random variables.", "tokens": [51024, 1436, 291, 362, 257, 6806, 1230, 295, 4974, 9102, 13, 51274], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 450, "seek": 177888, "start": 1799.0, "end": 1803.4, "text": " So is that the kind of equivalent of what you're saying?", "tokens": [51370, 407, 307, 300, 264, 733, 295, 10344, 295, 437, 291, 434, 1566, 30, 51590], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 451, "seek": 177888, "start": 1803.4, "end": 1805.3600000000001, "text": " Yes, yes, that's a very, very good point.", "tokens": [51590, 1079, 11, 2086, 11, 300, 311, 257, 588, 11, 588, 665, 935, 13, 51688], "temperature": 0.0, "avg_logprob": -0.24792399031392645, "compression_ratio": 1.5406698564593302, "no_speech_prob": 0.0034791138023138046}, {"id": 452, "seek": 180536, "start": 1805.36, "end": 1809.6799999999998, "text": " So fixed length here is a very essential assumption.", "tokens": [50364, 407, 6806, 4641, 510, 307, 257, 588, 7115, 15302, 13, 50580], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 453, "seek": 180536, "start": 1809.6799999999998, "end": 1814.6799999999998, "text": " So suppose we have a distribution over language of arbitrary length,", "tokens": [50580, 407, 7297, 321, 362, 257, 7316, 670, 2856, 295, 23211, 4641, 11, 50830], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 454, "seek": 180536, "start": 1816.56, "end": 1821.04, "text": " then applying the constraint could be hard to define.", "tokens": [50924, 550, 9275, 264, 25534, 727, 312, 1152, 281, 6964, 13, 51148], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 455, "seek": 180536, "start": 1822.1599999999999, "end": 1824.7199999999998, "text": " So basically these three words,", "tokens": [51204, 407, 1936, 613, 1045, 2283, 11, 51332], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 456, "seek": 180536, "start": 1824.7199999999998, "end": 1829.76, "text": " they can appear in arbitrarily far positions,", "tokens": [51332, 436, 393, 4204, 294, 19071, 3289, 1400, 8432, 11, 51584], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 457, "seek": 180536, "start": 1829.76, "end": 1832.56, "text": " and we need probably you take like an infinite sum", "tokens": [51584, 293, 321, 643, 1391, 291, 747, 411, 364, 13785, 2408, 51724], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 458, "seek": 180536, "start": 1832.56, "end": 1834.56, "text": " to define this conditional probability.", "tokens": [51724, 281, 6964, 341, 27708, 8482, 13, 51824], "temperature": 0.0, "avg_logprob": -0.2109535657442533, "compression_ratio": 1.6, "no_speech_prob": 0.00025310731143690646}, {"id": 459, "seek": 183536, "start": 1836.08, "end": 1839.8799999999999, "text": " But this is not terrible in practice,", "tokens": [50400, 583, 341, 307, 406, 6237, 294, 3124, 11, 50590], "temperature": 0.0, "avg_logprob": -0.26166494269120066, "compression_ratio": 1.434065934065934, "no_speech_prob": 0.00021990496315993369}, {"id": 460, "seek": 183536, "start": 1839.8799999999999, "end": 1843.12, "text": " because in practice, even for models like GPT,", "tokens": [50590, 570, 294, 3124, 11, 754, 337, 5245, 411, 26039, 51, 11, 50752], "temperature": 0.0, "avg_logprob": -0.26166494269120066, "compression_ratio": 1.434065934065934, "no_speech_prob": 0.00021990496315993369}, {"id": 461, "seek": 183536, "start": 1843.12, "end": 1846.3999999999999, "text": " they have a finite sequence length, yeah,", "tokens": [50752, 436, 362, 257, 19362, 8310, 4641, 11, 1338, 11, 50916], "temperature": 0.0, "avg_logprob": -0.26166494269120066, "compression_ratio": 1.434065934065934, "no_speech_prob": 0.00021990496315993369}, {"id": 462, "seek": 183536, "start": 1846.3999999999999, "end": 1851.3999999999999, "text": " that's a very good point, okay, I'm trying to be fast.", "tokens": [50916, 300, 311, 257, 588, 665, 935, 11, 1392, 11, 286, 478, 1382, 281, 312, 2370, 13, 51166], "temperature": 0.0, "avg_logprob": -0.26166494269120066, "compression_ratio": 1.434065934065934, "no_speech_prob": 0.00021990496315993369}, {"id": 463, "seek": 183536, "start": 1855.08, "end": 1857.56, "text": " Oh, I mean, there are variations of constraints", "tokens": [51350, 876, 11, 286, 914, 11, 456, 366, 17840, 295, 18491, 51474], "temperature": 0.0, "avg_logprob": -0.26166494269120066, "compression_ratio": 1.434065934065934, "no_speech_prob": 0.00021990496315993369}, {"id": 464, "seek": 183536, "start": 1857.56, "end": 1862.4399999999998, "text": " we have talked about, and okay,", "tokens": [51474, 321, 362, 2825, 466, 11, 293, 1392, 11, 51718], "temperature": 0.0, "avg_logprob": -0.26166494269120066, "compression_ratio": 1.434065934065934, "no_speech_prob": 0.00021990496315993369}, {"id": 465, "seek": 186244, "start": 1862.44, "end": 1864.8, "text": " so now we have our,", "tokens": [50364, 370, 586, 321, 362, 527, 11, 50482], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 466, "seek": 186244, "start": 1867.96, "end": 1872.24, "text": " sorry, now we have our probabilistic circuit", "tokens": [50640, 2597, 11, 586, 321, 362, 527, 31959, 3142, 9048, 50854], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 467, "seek": 186244, "start": 1872.24, "end": 1874.2, "text": " representing the distribution,", "tokens": [50854, 13460, 264, 7316, 11, 50952], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 468, "seek": 186244, "start": 1874.2, "end": 1876.48, "text": " and we have our constraint circuit,", "tokens": [50952, 293, 321, 362, 527, 25534, 9048, 11, 51066], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 469, "seek": 186244, "start": 1876.48, "end": 1878.52, "text": " and we'll close them to take the product.", "tokens": [51066, 293, 321, 603, 1998, 552, 281, 747, 264, 1674, 13, 51168], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 470, "seek": 186244, "start": 1880.1200000000001, "end": 1881.72, "text": " So we have our constraint circuit now,", "tokens": [51248, 407, 321, 362, 527, 25534, 9048, 586, 11, 51328], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 471, "seek": 186244, "start": 1881.72, "end": 1884.0800000000002, "text": " we can compute the probabilities we need.", "tokens": [51328, 321, 393, 14722, 264, 33783, 321, 643, 13, 51446], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 472, "seek": 186244, "start": 1884.0800000000002, "end": 1886.4, "text": " So the step two is very simple.", "tokens": [51446, 407, 264, 1823, 732, 307, 588, 2199, 13, 51562], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 473, "seek": 186244, "start": 1886.4, "end": 1890.68, "text": " So this is the original like base decomposition", "tokens": [51562, 407, 341, 307, 264, 3380, 411, 3096, 48356, 51776], "temperature": 0.0, "avg_logprob": -0.19888669794256036, "compression_ratio": 1.825136612021858, "no_speech_prob": 0.00016600600793026388}, {"id": 474, "seek": 189068, "start": 1890.68, "end": 1893.28, "text": " of the conditional probability we have.", "tokens": [50364, 295, 264, 27708, 8482, 321, 362, 13, 50494], "temperature": 0.0, "avg_logprob": -0.2505967649694991, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0006461106240749359}, {"id": 475, "seek": 189068, "start": 1893.28, "end": 1896.0800000000002, "text": " So this term here is intractable,", "tokens": [50494, 407, 341, 1433, 510, 307, 560, 1897, 712, 11, 50634], "temperature": 0.0, "avg_logprob": -0.2505967649694991, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0006461106240749359}, {"id": 476, "seek": 189068, "start": 1896.92, "end": 1900.5600000000002, "text": " so we secretly replace the subscript of LM with HMM,", "tokens": [50676, 370, 321, 22611, 7406, 264, 2325, 662, 295, 46529, 365, 389, 17365, 11, 50858], "temperature": 0.0, "avg_logprob": -0.2505967649694991, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0006461106240749359}, {"id": 477, "seek": 189068, "start": 1900.5600000000002, "end": 1905.5600000000002, "text": " the one we notice, and we define this conditional distribution,", "tokens": [50858, 264, 472, 321, 3449, 11, 293, 321, 6964, 341, 27708, 7316, 11, 51108], "temperature": 0.0, "avg_logprob": -0.2505967649694991, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0006461106240749359}, {"id": 478, "seek": 189068, "start": 1907.3600000000001, "end": 1911.96, "text": " the gelato distribution, and we can compute this", "tokens": [51198, 264, 4087, 2513, 7316, 11, 293, 321, 393, 14722, 341, 51428], "temperature": 0.0, "avg_logprob": -0.2505967649694991, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0006461106240749359}, {"id": 479, "seek": 189068, "start": 1911.96, "end": 1915.48, "text": " with the linear pass of the circuit.", "tokens": [51428, 365, 264, 8213, 1320, 295, 264, 9048, 13, 51604], "temperature": 0.0, "avg_logprob": -0.2505967649694991, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.0006461106240749359}, {"id": 480, "seek": 191548, "start": 1915.84, "end": 1921.48, "text": " Okay, so what are the advantages?", "tokens": [50382, 1033, 11, 370, 437, 366, 264, 14906, 30, 50664], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 481, "seek": 191548, "start": 1922.48, "end": 1924.8, "text": " So number one, by definition,", "tokens": [50714, 407, 1230, 472, 11, 538, 7123, 11, 50830], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 482, "seek": 191548, "start": 1924.8, "end": 1927.24, "text": " the constraint alpha is guaranteed to be satisfied,", "tokens": [50830, 264, 25534, 8961, 307, 18031, 281, 312, 11239, 11, 50952], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 483, "seek": 191548, "start": 1927.24, "end": 1931.16, "text": " so finally, other than compared to all the other methods,", "tokens": [50952, 370, 2721, 11, 661, 813, 5347, 281, 439, 264, 661, 7150, 11, 51148], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 484, "seek": 191548, "start": 1931.16, "end": 1935.28, "text": " we have 100% reliable thing we can trust,", "tokens": [51148, 321, 362, 2319, 4, 12924, 551, 321, 393, 3361, 11, 51354], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 485, "seek": 191548, "start": 1935.28, "end": 1938.28, "text": " and number two is that the training of this HMM", "tokens": [51354, 293, 1230, 732, 307, 300, 264, 3097, 295, 341, 389, 17365, 51504], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 486, "seek": 191548, "start": 1940.28, "end": 1942.1200000000001, "text": " does not depend on the constraint.", "tokens": [51604, 775, 406, 5672, 322, 264, 25534, 13, 51696], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 487, "seek": 191548, "start": 1942.1200000000001, "end": 1945.2, "text": " So basically once we have this HMM trained,", "tokens": [51696, 407, 1936, 1564, 321, 362, 341, 389, 17365, 8895, 11, 51850], "temperature": 0.0, "avg_logprob": -0.20333762790845789, "compression_ratio": 1.6056338028169015, "no_speech_prob": 0.00011234913108637556}, {"id": 488, "seek": 194520, "start": 1945.2, "end": 1949.24, "text": " we can use it to enforce whatever constraint we want.", "tokens": [50364, 321, 393, 764, 309, 281, 24825, 2035, 25534, 321, 528, 13, 50566], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 489, "seek": 194520, "start": 1949.24, "end": 1952.16, "text": " So maybe today I want to write something", "tokens": [50566, 407, 1310, 965, 286, 528, 281, 2464, 746, 50712], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 490, "seek": 194520, "start": 1952.16, "end": 1954.92, "text": " using some keywords and key concepts,", "tokens": [50712, 1228, 512, 21009, 293, 2141, 10392, 11, 50850], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 491, "seek": 194520, "start": 1954.92, "end": 1957.0800000000002, "text": " and tomorrow I feel like this language model", "tokens": [50850, 293, 4153, 286, 841, 411, 341, 2856, 2316, 50958], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 492, "seek": 194520, "start": 1957.0800000000002, "end": 1961.72, "text": " is like using a lot of inappropriate languages,", "tokens": [50958, 307, 411, 1228, 257, 688, 295, 26723, 8650, 11, 51190], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 493, "seek": 194520, "start": 1961.72, "end": 1965.44, "text": " and we can detoxify it by specifying a list of bad words", "tokens": [51190, 293, 321, 393, 34904, 2505, 309, 538, 1608, 5489, 257, 1329, 295, 1578, 2283, 51376], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 494, "seek": 194520, "start": 1965.44, "end": 1970.1200000000001, "text": " that it cannot use, and all the same model,", "tokens": [51376, 300, 309, 2644, 764, 11, 293, 439, 264, 912, 2316, 11, 51610], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 495, "seek": 194520, "start": 1970.1200000000001, "end": 1973.6000000000001, "text": " and all the constraints are only enforced at inference time.", "tokens": [51610, 293, 439, 264, 18491, 366, 787, 40953, 412, 38253, 565, 13, 51784], "temperature": 0.0, "avg_logprob": -0.14712975688816346, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.00041724604670889676}, {"id": 496, "seek": 197360, "start": 1974.6, "end": 1975.6, "text": " Yes?", "tokens": [50414, 1079, 30, 50464], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 497, "seek": 197360, "start": 1975.6, "end": 1977.52, "text": " What kind of constraints can be represented", "tokens": [50464, 708, 733, 295, 18491, 393, 312, 10379, 50560], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 498, "seek": 197360, "start": 1977.52, "end": 1981.36, "text": " as the composable pieces?", "tokens": [50560, 382, 264, 10199, 712, 3755, 30, 50752], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 499, "seek": 197360, "start": 1981.36, "end": 1983.84, "text": " Yes, that's a very, very good question.", "tokens": [50752, 1079, 11, 300, 311, 257, 588, 11, 588, 665, 1168, 13, 50876], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 500, "seek": 197360, "start": 1983.84, "end": 1986.0, "text": " That's actually one of the main reason", "tokens": [50876, 663, 311, 767, 472, 295, 264, 2135, 1778, 50984], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 501, "seek": 197360, "start": 1986.0, "end": 1988.1999999999998, "text": " in presenting this work here,", "tokens": [50984, 294, 15578, 341, 589, 510, 11, 51094], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 502, "seek": 197360, "start": 1988.1999999999998, "end": 1991.6399999999999, "text": " because my feeling is that though people have been studying", "tokens": [51094, 570, 452, 2633, 307, 300, 1673, 561, 362, 668, 7601, 51266], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 503, "seek": 197360, "start": 1991.6399999999999, "end": 1996.0, "text": " like probabilistic queries like marginal map, marginal,", "tokens": [51266, 411, 31959, 3142, 24109, 411, 16885, 4471, 11, 16885, 11, 51484], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 504, "seek": 197360, "start": 1998.24, "end": 2001.7199999999998, "text": " and all these kinds of stuff extensively,", "tokens": [51596, 293, 439, 613, 3685, 295, 1507, 32636, 11, 51770], "temperature": 0.0, "avg_logprob": -0.2622437260367654, "compression_ratio": 1.5360360360360361, "no_speech_prob": 0.0009245099499821663}, {"id": 505, "seek": 200172, "start": 2001.72, "end": 2005.56, "text": " but in practice, we really care", "tokens": [50364, 457, 294, 3124, 11, 321, 534, 1127, 50556], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 506, "seek": 200172, "start": 2005.56, "end": 2009.28, "text": " about some more complicated, less generic ones,", "tokens": [50556, 466, 512, 544, 6179, 11, 1570, 19577, 2306, 11, 50742], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 507, "seek": 200172, "start": 2009.28, "end": 2012.3600000000001, "text": " and whether is there a language to define", "tokens": [50742, 293, 1968, 307, 456, 257, 2856, 281, 6964, 50896], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 508, "seek": 200172, "start": 2012.3600000000001, "end": 2014.16, "text": " or to describe their tractability", "tokens": [50896, 420, 281, 6786, 641, 24207, 2310, 50986], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 509, "seek": 200172, "start": 2014.16, "end": 2017.52, "text": " is kind of missing from the literature.", "tokens": [50986, 307, 733, 295, 5361, 490, 264, 10394, 13, 51154], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 510, "seek": 200172, "start": 2017.52, "end": 2020.72, "text": " But I could relate this to say", "tokens": [51154, 583, 286, 727, 10961, 341, 281, 584, 51314], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 511, "seek": 200172, "start": 2020.72, "end": 2025.72, "text": " there's some work on compiling DFAs to circuits.", "tokens": [51314, 456, 311, 512, 589, 322, 715, 4883, 48336, 10884, 281, 26354, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 512, "seek": 200172, "start": 2026.72, "end": 2030.4, "text": " I think that could be something as a starting point", "tokens": [51614, 286, 519, 300, 727, 312, 746, 382, 257, 2891, 935, 51798], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 513, "seek": 200172, "start": 2030.4, "end": 2031.3600000000001, "text": " to look at.", "tokens": [51798, 281, 574, 412, 13, 51846], "temperature": 0.0, "avg_logprob": -0.1596539780333802, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.0001123393012676388}, {"id": 514, "seek": 203136, "start": 2031.3999999999999, "end": 2032.9199999999998, "text": " I'm not sure if that answers your question.", "tokens": [50366, 286, 478, 406, 988, 498, 300, 6338, 428, 1168, 13, 50442], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 515, "seek": 203136, "start": 2032.9199999999998, "end": 2033.9199999999998, "text": " I don't have an answer.", "tokens": [50442, 286, 500, 380, 362, 364, 1867, 13, 50492], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 516, "seek": 203136, "start": 2033.9199999999998, "end": 2036.24, "text": " My answer is, okay.", "tokens": [50492, 1222, 1867, 307, 11, 1392, 13, 50608], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 517, "seek": 203136, "start": 2037.84, "end": 2040.12, "text": " Okay, so experiments and benchmarks.", "tokens": [50688, 1033, 11, 370, 12050, 293, 43751, 13, 50802], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 518, "seek": 203136, "start": 2041.1599999999999, "end": 2046.04, "text": " So we evaluate our method on this common sense generation,", "tokens": [50854, 407, 321, 13059, 527, 3170, 322, 341, 2689, 2020, 5125, 11, 51098], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 519, "seek": 203136, "start": 2046.04, "end": 2047.36, "text": " common gem benchmark.", "tokens": [51098, 2689, 7173, 18927, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 520, "seek": 203136, "start": 2047.36, "end": 2050.08, "text": " Well, it's very similar to the example I gave you.", "tokens": [51164, 1042, 11, 309, 311, 588, 2531, 281, 264, 1365, 286, 2729, 291, 13, 51300], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 521, "seek": 203136, "start": 2050.08, "end": 2052.2799999999997, "text": " It gave you a bunch of keywords,", "tokens": [51300, 467, 2729, 291, 257, 3840, 295, 21009, 11, 51410], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 522, "seek": 203136, "start": 2052.2799999999997, "end": 2057.2799999999997, "text": " and each example comes with a bunch of gold sentences,", "tokens": [51410, 293, 1184, 1365, 1487, 365, 257, 3840, 295, 3821, 16579, 11, 51660], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 523, "seek": 203136, "start": 2057.6, "end": 2060.3599999999997, "text": " and you want to generate something", "tokens": [51676, 293, 291, 528, 281, 8460, 746, 51814], "temperature": 0.0, "avg_logprob": -0.18462996709914434, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.000241507645114325}, {"id": 524, "seek": 206036, "start": 2060.36, "end": 2065.04, "text": " that looks similar to the gold sentences using keywords.", "tokens": [50364, 300, 1542, 2531, 281, 264, 3821, 16579, 1228, 21009, 13, 50598], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 525, "seek": 206036, "start": 2066.0, "end": 2069.6, "text": " And here, it's like the most general case.", "tokens": [50646, 400, 510, 11, 309, 311, 411, 264, 881, 2674, 1389, 13, 50826], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 526, "seek": 206036, "start": 2069.6, "end": 2071.44, "text": " So basically these keywords,", "tokens": [50826, 407, 1936, 613, 21009, 11, 50918], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 527, "seek": 206036, "start": 2071.44, "end": 2072.92, "text": " they can appear in any order,", "tokens": [50918, 436, 393, 4204, 294, 604, 1668, 11, 50992], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 528, "seek": 206036, "start": 2072.92, "end": 2077.1600000000003, "text": " and they can appear in any form of their inflections.", "tokens": [50992, 293, 436, 393, 4204, 294, 604, 1254, 295, 641, 1536, 1809, 626, 13, 51204], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 529, "seek": 206036, "start": 2078.7200000000003, "end": 2082.48, "text": " Okay, so this is the, yes, then.", "tokens": [51282, 1033, 11, 370, 341, 307, 264, 11, 2086, 11, 550, 13, 51470], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 530, "seek": 206036, "start": 2083.52, "end": 2086.52, "text": " And it's a gigantic table.", "tokens": [51522, 400, 309, 311, 257, 26800, 3199, 13, 51672], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 531, "seek": 206036, "start": 2086.52, "end": 2088.92, "text": " The numbers themselves are not that important.", "tokens": [51672, 440, 3547, 2969, 366, 406, 300, 1021, 13, 51792], "temperature": 0.0, "avg_logprob": -0.275103644038854, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.00011772699508583173}, {"id": 532, "seek": 209036, "start": 2091.08, "end": 2093.08, "text": " So one thing to note that is compared", "tokens": [50400, 407, 472, 551, 281, 3637, 300, 307, 5347, 50500], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 533, "seek": 209036, "start": 2093.08, "end": 2094.88, "text": " to all the other baselines,", "tokens": [50500, 281, 439, 264, 661, 987, 9173, 11, 50590], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 534, "seek": 209036, "start": 2096.04, "end": 2100.76, "text": " our method, gelato, achieves a state-of-the-art performance", "tokens": [50648, 527, 3170, 11, 4087, 2513, 11, 3538, 977, 257, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 50884], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 535, "seek": 209036, "start": 2100.76, "end": 2104.36, "text": " with respect to basically all metrics.", "tokens": [50884, 365, 3104, 281, 1936, 439, 16367, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 536, "seek": 209036, "start": 2104.36, "end": 2108.2000000000003, "text": " So these metrics here, Rouge L, Blue Four Ciders,", "tokens": [51064, 407, 613, 16367, 510, 11, 47607, 441, 11, 8510, 7451, 383, 6936, 11, 51256], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 537, "seek": 209036, "start": 2108.2000000000003, "end": 2112.32, "text": " Spice State, there are just some standard NLP metrics", "tokens": [51256, 1738, 573, 4533, 11, 456, 366, 445, 512, 3832, 426, 45196, 16367, 51462], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 538, "seek": 209036, "start": 2112.32, "end": 2117.32, "text": " that people use to evaluate the quality of your text.", "tokens": [51462, 300, 561, 764, 281, 13059, 264, 3125, 295, 428, 2487, 13, 51712], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 539, "seek": 209036, "start": 2117.6, "end": 2119.92, "text": " So basically you have a gold sentence,", "tokens": [51726, 407, 1936, 291, 362, 257, 3821, 8174, 11, 51842], "temperature": 0.0, "avg_logprob": -0.20735929508020381, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.00010553477477515116}, {"id": 540, "seek": 211992, "start": 2119.92, "end": 2122.36, "text": " and you have your generated sentence.", "tokens": [50364, 293, 291, 362, 428, 10833, 8174, 13, 50486], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 541, "seek": 211992, "start": 2122.36, "end": 2125.7200000000003, "text": " They compute somehow the NBREM overlap", "tokens": [50486, 814, 14722, 6063, 264, 426, 33, 3850, 44, 19959, 50654], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 542, "seek": 211992, "start": 2125.7200000000003, "end": 2127.8, "text": " to measure the quality.", "tokens": [50654, 281, 3481, 264, 3125, 13, 50758], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 543, "seek": 211992, "start": 2131.64, "end": 2135.44, "text": " But the other thing is that all of the previous method", "tokens": [50950, 583, 264, 661, 551, 307, 300, 439, 295, 264, 3894, 3170, 51140], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 544, "seek": 211992, "start": 2137.44, "end": 2142.16, "text": " cannot achieve 100% constraint satisfiability,", "tokens": [51240, 2644, 4584, 2319, 4, 25534, 5519, 72, 2310, 11, 51476], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 545, "seek": 211992, "start": 2142.16, "end": 2145.2000000000003, "text": " but ours does in practice as well.", "tokens": [51476, 457, 11896, 775, 294, 3124, 382, 731, 13, 51628], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 546, "seek": 211992, "start": 2145.2000000000003, "end": 2146.7200000000003, "text": " Well, there is this one baseline,", "tokens": [51628, 1042, 11, 456, 307, 341, 472, 20518, 11, 51704], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 547, "seek": 211992, "start": 2146.7200000000003, "end": 2149.6, "text": " they achieve 100% accuracy as well,", "tokens": [51704, 436, 4584, 2319, 4, 14170, 382, 731, 11, 51848], "temperature": 0.0, "avg_logprob": -0.2005934657820736, "compression_ratio": 1.4830917874396135, "no_speech_prob": 6.9212939706631e-05}, {"id": 548, "seek": 214960, "start": 2149.6, "end": 2154.6, "text": " but they kind of did it by starting from the keywords.", "tokens": [50364, 457, 436, 733, 295, 630, 309, 538, 2891, 490, 264, 21009, 13, 50614], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 549, "seek": 214960, "start": 2154.8399999999997, "end": 2157.4, "text": " So they're always gonna be there.", "tokens": [50626, 407, 436, 434, 1009, 799, 312, 456, 13, 50754], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 550, "seek": 214960, "start": 2158.36, "end": 2161.24, "text": " And you can see their generation quality", "tokens": [50802, 400, 291, 393, 536, 641, 5125, 3125, 50946], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 551, "seek": 214960, "start": 2161.24, "end": 2162.6, "text": " is really poor, so.", "tokens": [50946, 307, 534, 4716, 11, 370, 13, 51014], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 552, "seek": 214960, "start": 2167.8399999999997, "end": 2171.44, "text": " We also conducted some sort of human evaluation.", "tokens": [51276, 492, 611, 13809, 512, 1333, 295, 1952, 13344, 13, 51456], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 553, "seek": 214960, "start": 2174.04, "end": 2175.2799999999997, "text": " Okay, I guess, yes?", "tokens": [51586, 1033, 11, 286, 2041, 11, 2086, 30, 51648], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 554, "seek": 214960, "start": 2175.2799999999997, "end": 2177.2799999999997, "text": " Which language model does that look like?", "tokens": [51648, 3013, 2856, 2316, 775, 300, 574, 411, 30, 51748], "temperature": 0.0, "avg_logprob": -0.28938178493552014, "compression_ratio": 1.3903743315508021, "no_speech_prob": 0.00011585860920604318}, {"id": 555, "seek": 217728, "start": 2177.28, "end": 2182.28, "text": " Oh, the language model, we use GPT-2, GPT-2 large.", "tokens": [50364, 876, 11, 264, 2856, 2316, 11, 321, 764, 26039, 51, 12, 17, 11, 26039, 51, 12, 17, 2416, 13, 50614], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 556, "seek": 217728, "start": 2183.1600000000003, "end": 2185.0, "text": " Yeah, and all of the baseline state,", "tokens": [50658, 865, 11, 293, 439, 295, 264, 20518, 1785, 11, 50750], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 557, "seek": 217728, "start": 2185.0, "end": 2186.4, "text": " they use GPT-2 large, yeah.", "tokens": [50750, 436, 764, 26039, 51, 12, 17, 2416, 11, 1338, 13, 50820], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 558, "seek": 217728, "start": 2188.96, "end": 2191.44, "text": " And in case people don't really trust", "tokens": [50948, 400, 294, 1389, 561, 500, 380, 534, 3361, 51072], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 559, "seek": 217728, "start": 2191.44, "end": 2193.52, "text": " these automatic evaluation metrics,", "tokens": [51072, 613, 12509, 13344, 16367, 11, 51176], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 560, "seek": 217728, "start": 2193.52, "end": 2195.7200000000003, "text": " we also conducted human evaluation.", "tokens": [51176, 321, 611, 13809, 1952, 13344, 13, 51286], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 561, "seek": 217728, "start": 2198.6000000000004, "end": 2201.2000000000003, "text": " And you can see that our model performs", "tokens": [51430, 400, 291, 393, 536, 300, 527, 2316, 26213, 51560], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 562, "seek": 217728, "start": 2202.2000000000003, "end": 2204.4, "text": " much better than previous state-of-the-art.", "tokens": [51610, 709, 1101, 813, 3894, 1785, 12, 2670, 12, 3322, 12, 446, 13, 51720], "temperature": 0.0, "avg_logprob": -0.30033824318333674, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.00011958240065723658}, {"id": 563, "seek": 220440, "start": 2205.4, "end": 2207.52, "text": " Okay, well, are they very significant?", "tokens": [50414, 1033, 11, 731, 11, 366, 436, 588, 4776, 30, 50520], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 564, "seek": 220440, "start": 2207.52, "end": 2209.52, "text": " I mean, they're pretty close.", "tokens": [50520, 286, 914, 11, 436, 434, 1238, 1998, 13, 50620], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 565, "seek": 220440, "start": 2209.52, "end": 2212.64, "text": " Yeah, so, yeah, so basically the,", "tokens": [50620, 865, 11, 370, 11, 1338, 11, 370, 1936, 264, 11, 50776], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 566, "seek": 220440, "start": 2214.32, "end": 2216.08, "text": " I'm not sure if you can see it clearly,", "tokens": [50860, 286, 478, 406, 988, 498, 291, 393, 536, 309, 4448, 11, 50948], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 567, "seek": 220440, "start": 2216.08, "end": 2219.2400000000002, "text": " but the bold-faced ones are statistically", "tokens": [50948, 457, 264, 11928, 12, 48691, 2306, 366, 36478, 51106], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 568, "seek": 220440, "start": 2220.56, "end": 2224.52, "text": " significantly, what equivalent?", "tokens": [51172, 10591, 11, 437, 10344, 30, 51370], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 569, "seek": 220440, "start": 2224.52, "end": 2229.52, "text": " So these ones, these two are statistically equivalent.", "tokens": [51370, 407, 613, 2306, 11, 613, 732, 366, 36478, 10344, 13, 51620], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 570, "seek": 220440, "start": 2229.84, "end": 2232.58, "text": " This one is statistically significant.", "tokens": [51636, 639, 472, 307, 36478, 4776, 13, 51773], "temperature": 0.0, "avg_logprob": -0.340748685948989, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0007095631444826722}, {"id": 571, "seek": 223258, "start": 2233.58, "end": 2235.42, "text": " And we're looking at, when defibrating,", "tokens": [50414, 400, 321, 434, 1237, 412, 11, 562, 1060, 6414, 990, 11, 50506], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 572, "seek": 223258, "start": 2235.42, "end": 2237.62, "text": " what's the number here?", "tokens": [50506, 437, 311, 264, 1230, 510, 30, 50616], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 573, "seek": 223258, "start": 2237.62, "end": 2240.66, "text": " Oh, yeah, so basically we provide the annotator", "tokens": [50616, 876, 11, 1338, 11, 370, 1936, 321, 2893, 264, 25339, 1639, 50768], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 574, "seek": 223258, "start": 2240.66, "end": 2243.62, "text": " some sentence and we provide description", "tokens": [50768, 512, 8174, 293, 321, 2893, 3855, 50916], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 575, "seek": 223258, "start": 2243.62, "end": 2246.94, "text": " of one of each of the aspects.", "tokens": [50916, 295, 472, 295, 1184, 295, 264, 7270, 13, 51082], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 576, "seek": 223258, "start": 2246.94, "end": 2249.42, "text": " So basically, concept means that does it use", "tokens": [51082, 407, 1936, 11, 3410, 1355, 300, 775, 309, 764, 51206], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 577, "seek": 223258, "start": 2249.42, "end": 2251.54, "text": " all the concepts naturally?", "tokens": [51206, 439, 264, 10392, 8195, 30, 51312], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 578, "seek": 223258, "start": 2251.54, "end": 2254.2999999999997, "text": " And plausibility means that is the sentence", "tokens": [51312, 400, 34946, 2841, 1355, 300, 307, 264, 8174, 51450], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 579, "seek": 223258, "start": 2254.2999999999997, "end": 2258.18, "text": " like a plausible sentence describing a realistic scene.", "tokens": [51450, 411, 257, 39925, 8174, 16141, 257, 12465, 4145, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 580, "seek": 223258, "start": 2258.18, "end": 2261.92, "text": " And quality is basically fluency, grammar, and stuff.", "tokens": [51644, 400, 3125, 307, 1936, 5029, 3020, 11, 22317, 11, 293, 1507, 13, 51831], "temperature": 0.0, "avg_logprob": -0.2042329021703417, "compression_ratio": 1.7226890756302522, "no_speech_prob": 0.00047275645192712545}, {"id": 581, "seek": 226192, "start": 2261.96, "end": 2264.04, "text": " Overall is the like another,", "tokens": [50366, 18420, 307, 264, 411, 1071, 11, 50470], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 582, "seek": 226192, "start": 2264.04, "end": 2265.8, "text": " how do you feel about the sentence?", "tokens": [50470, 577, 360, 291, 841, 466, 264, 8174, 30, 50558], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 583, "seek": 226192, "start": 2265.8, "end": 2268.4, "text": " And the numbers are from one, two, three.", "tokens": [50558, 400, 264, 3547, 366, 490, 472, 11, 732, 11, 1045, 13, 50688], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 584, "seek": 226192, "start": 2268.4, "end": 2270.52, "text": " One, two, three, go, yeah.", "tokens": [50688, 1485, 11, 732, 11, 1045, 11, 352, 11, 1338, 13, 50794], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 585, "seek": 226192, "start": 2270.52, "end": 2271.36, "text": " Okay.", "tokens": [50794, 1033, 13, 50836], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 586, "seek": 226192, "start": 2273.7200000000003, "end": 2277.8, "text": " Okay, so let's get back to the very first", "tokens": [50954, 1033, 11, 370, 718, 311, 483, 646, 281, 264, 588, 700, 51158], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 587, "seek": 226192, "start": 2277.8, "end": 2281.2400000000002, "text": " motivating example and you can see that gelato,", "tokens": [51158, 41066, 1365, 293, 291, 393, 536, 300, 4087, 2513, 11, 51330], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 588, "seek": 226192, "start": 2281.2400000000002, "end": 2283.86, "text": " we use our model to add, and it is actually able", "tokens": [51330, 321, 764, 527, 2316, 281, 909, 11, 293, 309, 307, 767, 1075, 51461], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 589, "seek": 226192, "start": 2283.86, "end": 2287.02, "text": " to get everything correct and generate a fluent sentence.", "tokens": [51461, 281, 483, 1203, 3006, 293, 8460, 257, 40799, 8174, 13, 51619], "temperature": 0.0, "avg_logprob": -0.3057341003417969, "compression_ratio": 1.5700934579439252, "no_speech_prob": 0.0002779837523121387}, {"id": 590, "seek": 228702, "start": 2287.86, "end": 2292.86, "text": " And another, I don't, okay, so we also found this one", "tokens": [50406, 400, 1071, 11, 286, 500, 380, 11, 1392, 11, 370, 321, 611, 1352, 341, 472, 50656], "temperature": 0.0, "avg_logprob": -0.2679296875, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.0007552375318482518}, {"id": 591, "seek": 228702, "start": 2296.54, "end": 2298.54, "text": " in one of the generated candidates.", "tokens": [50840, 294, 472, 295, 264, 10833, 11255, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2679296875, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.0007552375318482518}, {"id": 592, "seek": 228702, "start": 2298.54, "end": 2301.58, "text": " A pair of Frisbee players are caught in a dog fight,", "tokens": [50940, 316, 6119, 295, 1526, 271, 24872, 4150, 366, 5415, 294, 257, 3000, 2092, 11, 51092], "temperature": 0.0, "avg_logprob": -0.2679296875, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.0007552375318482518}, {"id": 593, "seek": 228702, "start": 2301.58, "end": 2306.58, "text": " which is not like the thing that most people", "tokens": [51092, 597, 307, 406, 411, 264, 551, 300, 881, 561, 51342], "temperature": 0.0, "avg_logprob": -0.2679296875, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.0007552375318482518}, {"id": 594, "seek": 228702, "start": 2306.82, "end": 2307.74, "text": " would like to think of.", "tokens": [51354, 576, 411, 281, 519, 295, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2679296875, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.0007552375318482518}, {"id": 595, "seek": 228702, "start": 2307.74, "end": 2312.74, "text": " So it also shows some sort of creativity here.", "tokens": [51400, 407, 309, 611, 3110, 512, 1333, 295, 12915, 510, 13, 51650], "temperature": 0.0, "avg_logprob": -0.2679296875, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.0007552375318482518}, {"id": 596, "seek": 231274, "start": 2312.8599999999997, "end": 2316.7799999999997, "text": " Here, okay, that's my talk.", "tokens": [50370, 1692, 11, 1392, 11, 300, 311, 452, 751, 13, 50566], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 597, "seek": 231274, "start": 2318.06, "end": 2320.4599999999996, "text": " Please ask questions if you have,", "tokens": [50630, 2555, 1029, 1651, 498, 291, 362, 11, 50750], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 598, "seek": 231274, "start": 2320.4599999999996, "end": 2322.62, "text": " and otherwise we can go to lunch.", "tokens": [50750, 293, 5911, 321, 393, 352, 281, 6349, 13, 50858], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 599, "seek": 231274, "start": 2322.62, "end": 2323.4599999999996, "text": " Thank you.", "tokens": [50858, 1044, 291, 13, 50900], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 600, "seek": 231274, "start": 2323.4599999999996, "end": 2324.2999999999997, "text": " Thank you.", "tokens": [50900, 1044, 291, 13, 50942], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 601, "seek": 231274, "start": 2327.2999999999997, "end": 2328.2999999999997, "text": " One more question.", "tokens": [51092, 1485, 544, 1168, 13, 51142], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 602, "seek": 231274, "start": 2330.8599999999997, "end": 2332.7, "text": " Thanks for your talk, if I understand right,", "tokens": [51270, 2561, 337, 428, 751, 11, 498, 286, 1223, 558, 11, 51362], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 603, "seek": 231274, "start": 2332.7, "end": 2335.54, "text": " you said that to generate every word,", "tokens": [51362, 291, 848, 300, 281, 8460, 633, 1349, 11, 51504], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 604, "seek": 231274, "start": 2335.54, "end": 2338.74, "text": " you have to do a linear pass over the circuit.", "tokens": [51504, 291, 362, 281, 360, 257, 8213, 1320, 670, 264, 9048, 13, 51664], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 605, "seek": 231274, "start": 2338.74, "end": 2342.1, "text": " So how much is this overhead compared to the kind", "tokens": [51664, 407, 577, 709, 307, 341, 19922, 5347, 281, 264, 733, 51832], "temperature": 0.0, "avg_logprob": -0.40513418097245063, "compression_ratio": 1.572139303482587, "no_speech_prob": 0.0008036255603656173}, {"id": 606, "seek": 234210, "start": 2342.2599999999998, "end": 2343.7799999999997, "text": " of latency from the language model?", "tokens": [50372, 295, 27043, 490, 264, 2856, 2316, 30, 50448], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 607, "seek": 234210, "start": 2343.7799999999997, "end": 2346.18, "text": " Okay, I like that question.", "tokens": [50448, 1033, 11, 286, 411, 300, 1168, 13, 50568], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 608, "seek": 234210, "start": 2346.18, "end": 2350.7799999999997, "text": " So we don't really need to take a pass over the whole circuit.", "tokens": [50568, 407, 321, 500, 380, 534, 643, 281, 747, 257, 1320, 670, 264, 1379, 9048, 13, 50798], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 609, "seek": 234210, "start": 2350.7799999999997, "end": 2354.2799999999997, "text": " With some caching, we can do like constant time.", "tokens": [50798, 2022, 512, 269, 2834, 11, 321, 393, 360, 411, 5754, 565, 13, 50973], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 610, "seek": 234210, "start": 2355.5, "end": 2357.62, "text": " So basically to generate each word,", "tokens": [51034, 407, 1936, 281, 8460, 1184, 1349, 11, 51140], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 611, "seek": 234210, "start": 2357.62, "end": 2359.3399999999997, "text": " the cost is like constant time.", "tokens": [51140, 264, 2063, 307, 411, 5754, 565, 13, 51226], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 612, "seek": 234210, "start": 2359.3399999999997, "end": 2361.14, "text": " It's like a pass through one layer.", "tokens": [51226, 467, 311, 411, 257, 1320, 807, 472, 4583, 13, 51316], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 613, "seek": 234210, "start": 2362.2999999999997, "end": 2363.2999999999997, "text": " Is it in any practice?", "tokens": [51374, 1119, 309, 294, 604, 3124, 30, 51424], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 614, "seek": 234210, "start": 2363.2999999999997, "end": 2364.14, "text": " How fast is it?", "tokens": [51424, 1012, 2370, 307, 309, 30, 51466], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 615, "seek": 234210, "start": 2364.14, "end": 2365.5, "text": " Oh, how fast is it?", "tokens": [51466, 876, 11, 577, 2370, 307, 309, 30, 51534], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 616, "seek": 234210, "start": 2365.5, "end": 2366.9, "text": " We actually, so.", "tokens": [51534, 492, 767, 11, 370, 13, 51604], "temperature": 0.0, "avg_logprob": -0.28671757091175426, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0008687646477483213}, {"id": 617, "seek": 237210, "start": 2373.1, "end": 2375.86, "text": " I don't have a table here,", "tokens": [50414, 286, 500, 380, 362, 257, 3199, 510, 11, 50552], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 618, "seek": 237210, "start": 2375.86, "end": 2377.46, "text": " but we have a table in the paper.", "tokens": [50552, 457, 321, 362, 257, 3199, 294, 264, 3035, 13, 50632], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 619, "seek": 237210, "start": 2377.46, "end": 2382.46, "text": " So if say generating a sentence with five keywords,", "tokens": [50632, 407, 498, 584, 17746, 257, 8174, 365, 1732, 21009, 11, 50882], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 620, "seek": 237210, "start": 2383.46, "end": 2388.46, "text": " a GPT-2 large would take around 20 seconds,", "tokens": [50932, 257, 26039, 51, 12, 17, 2416, 576, 747, 926, 945, 3949, 11, 51182], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 621, "seek": 237210, "start": 2390.02, "end": 2394.5, "text": " and our method is like 100-ish seconds.", "tokens": [51260, 293, 527, 3170, 307, 411, 2319, 12, 742, 3949, 13, 51484], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 622, "seek": 237210, "start": 2394.5, "end": 2397.14, "text": " So it's not terrible.", "tokens": [51484, 407, 309, 311, 406, 6237, 13, 51616], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 623, "seek": 237210, "start": 2397.14, "end": 2399.66, "text": " And one of the baselines here,", "tokens": [51616, 400, 472, 295, 264, 987, 9173, 510, 11, 51742], "temperature": 0.0, "avg_logprob": -0.21871716463113133, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0003568849351722747}, {"id": 624, "seek": 239966, "start": 2399.66, "end": 2403.02, "text": " well, which was actually like the best paper award", "tokens": [50364, 731, 11, 597, 390, 767, 411, 264, 1151, 3035, 7130, 50532], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 625, "seek": 239966, "start": 2403.02, "end": 2405.14, "text": " at one of the top NLP conferences,", "tokens": [50532, 412, 472, 295, 264, 1192, 426, 45196, 22032, 11, 50638], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 626, "seek": 239966, "start": 2405.14, "end": 2407.06, "text": " they use search-based method.", "tokens": [50638, 436, 764, 3164, 12, 6032, 3170, 13, 50734], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 627, "seek": 239966, "start": 2407.06, "end": 2409.2599999999998, "text": " And for them to generate a sentence,", "tokens": [50734, 400, 337, 552, 281, 8460, 257, 8174, 11, 50844], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 628, "seek": 239966, "start": 2409.2599999999998, "end": 2413.1, "text": " they take like 700 seconds, 800 seconds.", "tokens": [50844, 436, 747, 411, 15204, 3949, 11, 13083, 3949, 13, 51036], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 629, "seek": 239966, "start": 2413.1, "end": 2415.74, "text": " So because they're search-based,", "tokens": [51036, 407, 570, 436, 434, 3164, 12, 6032, 11, 51168], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 630, "seek": 239966, "start": 2415.74, "end": 2418.3399999999997, "text": " so when a search-based gets large,", "tokens": [51168, 370, 562, 257, 3164, 12, 6032, 2170, 2416, 11, 51298], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 631, "seek": 239966, "start": 2421.74, "end": 2423.54, "text": " their method shows like a bottleneck.", "tokens": [51468, 641, 3170, 3110, 411, 257, 44641, 547, 13, 51558], "temperature": 0.0, "avg_logprob": -0.29447216211363325, "compression_ratio": 1.5572916666666667, "no_speech_prob": 0.0030732043087482452}, {"id": 632, "seek": 242354, "start": 2423.62, "end": 2425.46, "text": " That's pretty well.", "tokens": [50368, 663, 311, 1238, 731, 13, 50460], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 633, "seek": 242354, "start": 2425.46, "end": 2426.3, "text": " Thanks.", "tokens": [50460, 2561, 13, 50502], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 634, "seek": 242354, "start": 2428.1, "end": 2431.3, "text": " Can you also give an idea about the time required", "tokens": [50592, 1664, 291, 611, 976, 364, 1558, 466, 264, 565, 4739, 50752], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 635, "seek": 242354, "start": 2431.3, "end": 2433.62, "text": " to derive the hidden Markov model?", "tokens": [50752, 281, 28446, 264, 7633, 3934, 5179, 2316, 30, 50868], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 636, "seek": 242354, "start": 2433.62, "end": 2436.3, "text": " Oh, yeah, so I do have the time for that.", "tokens": [50868, 876, 11, 1338, 11, 370, 286, 360, 362, 264, 565, 337, 300, 13, 51002], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 637, "seek": 242354, "start": 2436.3, "end": 2440.82, "text": " So our hidden Markov model has like 4,000 states,", "tokens": [51002, 407, 527, 7633, 3934, 5179, 2316, 575, 411, 1017, 11, 1360, 4368, 11, 51228], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 638, "seek": 242354, "start": 2440.82, "end": 2443.34, "text": " and the emission is 50,000.", "tokens": [51228, 293, 264, 29513, 307, 2625, 11, 1360, 13, 51354], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 639, "seek": 242354, "start": 2443.34, "end": 2448.02, "text": " We trained them with the juice framework", "tokens": [51354, 492, 8895, 552, 365, 264, 8544, 8388, 51588], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 640, "seek": 242354, "start": 2448.02, "end": 2449.5, "text": " that we developed in our lab.", "tokens": [51588, 300, 321, 4743, 294, 527, 2715, 13, 51662], "temperature": 0.0, "avg_logprob": -0.31570850905551706, "compression_ratio": 1.4780487804878049, "no_speech_prob": 0.005949041806161404}, {"id": 641, "seek": 244950, "start": 2450.34, "end": 2455.34, "text": " The training takes like 20 hours.", "tokens": [50406, 440, 3097, 2516, 411, 945, 2496, 13, 50656], "temperature": 0.0, "avg_logprob": -0.39634110850672566, "compression_ratio": 1.4344827586206896, "no_speech_prob": 0.00041727707139216363}, {"id": 642, "seek": 244950, "start": 2455.94, "end": 2459.86, "text": " So it's, we sample about 200,", "tokens": [50686, 407, 309, 311, 11, 321, 6889, 466, 2331, 11, 50882], "temperature": 0.0, "avg_logprob": -0.39634110850672566, "compression_ratio": 1.4344827586206896, "no_speech_prob": 0.00041727707139216363}, {"id": 643, "seek": 244950, "start": 2462.94, "end": 2466.66, "text": " we sampled eight million sentences from GPT,", "tokens": [51036, 321, 3247, 15551, 3180, 2459, 16579, 490, 26039, 51, 11, 51222], "temperature": 0.0, "avg_logprob": -0.39634110850672566, "compression_ratio": 1.4344827586206896, "no_speech_prob": 0.00041727707139216363}, {"id": 644, "seek": 244950, "start": 2466.66, "end": 2471.06, "text": " and we trained them for 40 epochs, 20 hours.", "tokens": [51222, 293, 321, 8895, 552, 337, 3356, 30992, 28346, 11, 945, 2496, 13, 51442], "temperature": 0.0, "avg_logprob": -0.39634110850672566, "compression_ratio": 1.4344827586206896, "no_speech_prob": 0.00041727707139216363}, {"id": 645, "seek": 244950, "start": 2476.14, "end": 2478.02, "text": " What's the number of parameters?", "tokens": [51696, 708, 311, 264, 1230, 295, 9834, 30, 51790], "temperature": 0.0, "avg_logprob": -0.39634110850672566, "compression_ratio": 1.4344827586206896, "no_speech_prob": 0.00041727707139216363}, {"id": 646, "seek": 244950, "start": 2478.02, "end": 2479.06, "text": " Number of parameters?", "tokens": [51790, 5118, 295, 9834, 30, 51842], "temperature": 0.0, "avg_logprob": -0.39634110850672566, "compression_ratio": 1.4344827586206896, "no_speech_prob": 0.00041727707139216363}, {"id": 647, "seek": 247950, "start": 2480.5, "end": 2481.82, "text": " Of HMM.", "tokens": [50414, 2720, 389, 17365, 13, 50480], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 648, "seek": 247950, "start": 2481.82, "end": 2486.82, "text": " Yeah, so HMM is 4,000 hidden states and 50,000 emissions.", "tokens": [50480, 865, 11, 370, 389, 17365, 307, 1017, 11, 1360, 7633, 4368, 293, 2625, 11, 1360, 14607, 13, 50730], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 649, "seek": 247950, "start": 2488.18, "end": 2491.82, "text": " So the main, mainly that the parameters are centered", "tokens": [50798, 407, 264, 2135, 11, 8704, 300, 264, 9834, 366, 18988, 50980], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 650, "seek": 247950, "start": 2491.82, "end": 2493.42, "text": " on the emission table.", "tokens": [50980, 322, 264, 29513, 3199, 13, 51060], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 651, "seek": 247950, "start": 2493.42, "end": 2497.5, "text": " So it's like 40,000, 4,000 times 50,000.", "tokens": [51060, 407, 309, 311, 411, 3356, 11, 1360, 11, 1017, 11, 1360, 1413, 2625, 11, 1360, 13, 51264], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 652, "seek": 247950, "start": 2497.5, "end": 2499.7, "text": " I didn't do that in my head.", "tokens": [51264, 286, 994, 380, 360, 300, 294, 452, 1378, 13, 51374], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 653, "seek": 247950, "start": 2499.7, "end": 2501.38, "text": " So those are unique parameters, right?", "tokens": [51374, 407, 729, 366, 3845, 9834, 11, 558, 30, 51458], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 654, "seek": 247950, "start": 2501.38, "end": 2502.62, "text": " Yes, yeah.", "tokens": [51458, 1079, 11, 1338, 13, 51520], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 655, "seek": 247950, "start": 2502.62, "end": 2506.02, "text": " Yes, so in the PC, yes, you kind of like,", "tokens": [51520, 1079, 11, 370, 294, 264, 6465, 11, 2086, 11, 291, 733, 295, 411, 11, 51690], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 656, "seek": 247950, "start": 2506.02, "end": 2508.66, "text": " you've rolled out all the parameters.", "tokens": [51690, 291, 600, 14306, 484, 439, 264, 9834, 13, 51822], "temperature": 0.0, "avg_logprob": -0.31415205988390693, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0003568702668417245}, {"id": 657, "seek": 250950, "start": 2509.62, "end": 2510.62, "text": " All the positions.", "tokens": [50370, 1057, 264, 8432, 13, 50420], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 658, "seek": 250950, "start": 2513.18, "end": 2516.14, "text": " Controlable generation is huge, right?", "tokens": [50548, 12912, 712, 5125, 307, 2603, 11, 558, 30, 50696], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 659, "seek": 250950, "start": 2516.14, "end": 2517.58, "text": " So this is great.", "tokens": [50696, 407, 341, 307, 869, 13, 50768], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 660, "seek": 250950, "start": 2517.58, "end": 2519.78, "text": " What, I mean, and I probably understand you,", "tokens": [50768, 708, 11, 286, 914, 11, 293, 286, 1391, 1223, 291, 11, 50878], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 661, "seek": 250950, "start": 2519.78, "end": 2521.54, "text": " you gave a logical perspective for this audience,", "tokens": [50878, 291, 2729, 257, 14978, 4585, 337, 341, 4034, 11, 50966], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 662, "seek": 250950, "start": 2521.54, "end": 2524.1, "text": " but I mean, a reg acts as a much more natural", "tokens": [50966, 457, 286, 914, 11, 257, 1121, 10672, 382, 257, 709, 544, 3303, 51094], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 663, "seek": 250950, "start": 2524.1, "end": 2526.54, "text": " sort of control structure.", "tokens": [51094, 1333, 295, 1969, 3877, 13, 51216], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 664, "seek": 250950, "start": 2526.54, "end": 2529.46, "text": " So presumably you can handle any of our complication", "tokens": [51216, 407, 26742, 291, 393, 4813, 604, 295, 527, 1209, 8758, 51362], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 665, "seek": 250950, "start": 2529.46, "end": 2531.34, "text": " given the DFA interpretation, right?", "tokens": [51362, 2212, 264, 413, 19684, 14174, 11, 558, 30, 51456], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 666, "seek": 250950, "start": 2531.34, "end": 2532.42, "text": " Yes, yes.", "tokens": [51456, 1079, 11, 2086, 13, 51510], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 667, "seek": 250950, "start": 2532.42, "end": 2535.06, "text": " I think that's a very important follow up.", "tokens": [51510, 286, 519, 300, 311, 257, 588, 1021, 1524, 493, 13, 51642], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 668, "seek": 250950, "start": 2535.06, "end": 2537.78, "text": " We are kind of looking at considering like,", "tokens": [51642, 492, 366, 733, 295, 1237, 412, 8079, 411, 11, 51778], "temperature": 0.0, "avg_logprob": -0.24392031835130423, "compression_ratio": 1.5693430656934306, "no_speech_prob": 0.00015595417062286288}, {"id": 669, "seek": 253778, "start": 2537.82, "end": 2540.5800000000004, "text": " compiling DFA's to circuits", "tokens": [50366, 715, 4883, 413, 19684, 311, 281, 26354, 50504], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 670, "seek": 253778, "start": 2540.5800000000004, "end": 2544.5800000000004, "text": " and kind of automate all these process.", "tokens": [50504, 293, 733, 295, 31605, 439, 613, 1399, 13, 50704], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 671, "seek": 253778, "start": 2544.5800000000004, "end": 2546.7000000000003, "text": " Yeah, so ACLs in January, you submit there.", "tokens": [50704, 865, 11, 370, 43873, 82, 294, 7061, 11, 291, 10315, 456, 13, 50810], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 672, "seek": 253778, "start": 2546.7000000000003, "end": 2548.26, "text": " I mean, I think they'll love it.", "tokens": [50810, 286, 914, 11, 286, 519, 436, 603, 959, 309, 13, 50888], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 673, "seek": 253778, "start": 2549.1800000000003, "end": 2551.98, "text": " Your initial example, you wanted a winter tree,", "tokens": [50934, 2260, 5883, 1365, 11, 291, 1415, 257, 6355, 4230, 11, 51074], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 674, "seek": 253778, "start": 2551.98, "end": 2555.78, "text": " you wanted the presence of winter to select for warm, right?", "tokens": [51074, 291, 1415, 264, 6814, 295, 6355, 281, 3048, 337, 4561, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 675, "seek": 253778, "start": 2555.78, "end": 2557.98, "text": " Which is more of a natural language entailment.", "tokens": [51264, 3013, 307, 544, 295, 257, 3303, 2856, 948, 864, 518, 13, 51374], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 676, "seek": 253778, "start": 2557.98, "end": 2560.0600000000004, "text": " And I don't think you handle that, right?", "tokens": [51374, 400, 286, 500, 380, 519, 291, 4813, 300, 11, 558, 30, 51478], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 677, "seek": 253778, "start": 2560.0600000000004, "end": 2562.5400000000004, "text": " You handle variations and factual variations", "tokens": [51478, 509, 4813, 17840, 293, 48029, 17840, 51602], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 678, "seek": 253778, "start": 2562.5400000000004, "end": 2563.38, "text": " because you actually code them.", "tokens": [51602, 570, 291, 767, 3089, 552, 13, 51644], "temperature": 0.0, "avg_logprob": -0.21602992353768186, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048772519221529365}, {"id": 679, "seek": 256338, "start": 2563.38, "end": 2568.3, "text": " I think we can totally do that winter example.", "tokens": [50364, 286, 519, 321, 393, 3879, 360, 300, 6355, 1365, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 680, "seek": 256338, "start": 2568.3, "end": 2572.58, "text": " So basically, let me go back.", "tokens": [50610, 407, 1936, 11, 718, 385, 352, 646, 13, 50824], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 681, "seek": 256338, "start": 2573.58, "end": 2576.2200000000003, "text": " It seems like you encoded the specific variations", "tokens": [50874, 467, 2544, 411, 291, 2058, 12340, 264, 2685, 17840, 51006], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 682, "seek": 256338, "start": 2576.2200000000003, "end": 2577.58, "text": " that you were allowing, right?", "tokens": [51006, 300, 291, 645, 8293, 11, 558, 30, 51074], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 683, "seek": 256338, "start": 2577.58, "end": 2581.78, "text": " Oh, you mean like, so we can have winter, winters,", "tokens": [51074, 876, 11, 291, 914, 411, 11, 370, 321, 393, 362, 6355, 11, 1942, 1559, 11, 51284], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 684, "seek": 256338, "start": 2581.78, "end": 2584.62, "text": " but like, maybe if the winter,", "tokens": [51284, 457, 411, 11, 1310, 498, 264, 6355, 11, 51426], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 685, "seek": 256338, "start": 2584.62, "end": 2586.46, "text": " the word is not explicitly mentioned,", "tokens": [51426, 264, 1349, 307, 406, 20803, 2835, 11, 51518], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 686, "seek": 256338, "start": 2586.46, "end": 2587.78, "text": " you cannot do that.", "tokens": [51518, 291, 2644, 360, 300, 13, 51584], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 687, "seek": 256338, "start": 2587.78, "end": 2589.6600000000003, "text": " That's what I understand from your formalism, right?", "tokens": [51584, 663, 311, 437, 286, 1223, 490, 428, 9860, 1434, 11, 558, 30, 51678], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 688, "seek": 256338, "start": 2589.6600000000003, "end": 2591.9, "text": " If it's in your or, you'll generate it.", "tokens": [51678, 759, 309, 311, 294, 428, 420, 11, 291, 603, 8460, 309, 13, 51790], "temperature": 0.0, "avg_logprob": -0.2084619770879331, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.00045826099812984467}, {"id": 689, "seek": 259190, "start": 2591.9, "end": 2595.58, "text": " If it's not, you won't, you won't capture the entailment.", "tokens": [50364, 759, 309, 311, 406, 11, 291, 1582, 380, 11, 291, 1582, 380, 7983, 264, 948, 864, 518, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 690, "seek": 259190, "start": 2595.58, "end": 2599.3, "text": " No, I mean, so basically, so that's the other thing.", "tokens": [50548, 883, 11, 286, 914, 11, 370, 1936, 11, 370, 300, 311, 264, 661, 551, 13, 50734], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 691, "seek": 259190, "start": 2599.3, "end": 2603.2200000000003, "text": " So sometimes people, or most of the time,", "tokens": [50734, 407, 2171, 561, 11, 420, 881, 295, 264, 565, 11, 50930], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 692, "seek": 259190, "start": 2603.2200000000003, "end": 2606.38, "text": " people want to have like kind of soft control.", "tokens": [50930, 561, 528, 281, 362, 411, 733, 295, 2787, 1969, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 693, "seek": 259190, "start": 2606.38, "end": 2611.46, "text": " They can't really write their constraint in logical form.", "tokens": [51088, 814, 393, 380, 534, 2464, 641, 25534, 294, 14978, 1254, 13, 51342], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 694, "seek": 259190, "start": 2611.46, "end": 2616.38, "text": " For example, toxic, like how can you tell a sentence is toxic?", "tokens": [51342, 1171, 1365, 11, 12786, 11, 411, 577, 393, 291, 980, 257, 8174, 307, 12786, 30, 51588], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 695, "seek": 259190, "start": 2616.38, "end": 2618.78, "text": " But there are many ways to approximate it.", "tokens": [51588, 583, 456, 366, 867, 2098, 281, 30874, 309, 13, 51708], "temperature": 0.0, "avg_logprob": -0.1678630021902231, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0006068096845410764}, {"id": 696, "seek": 261878, "start": 2618.78, "end": 2622.86, "text": " One of the ways is we have like a long list of phrases or words", "tokens": [50364, 1485, 295, 264, 2098, 307, 321, 362, 411, 257, 938, 1329, 295, 20312, 420, 2283, 50568], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 697, "seek": 261878, "start": 2622.86, "end": 2626.46, "text": " that you're now allowed to use in a toxic sentence", "tokens": [50568, 300, 291, 434, 586, 4350, 281, 764, 294, 257, 12786, 8174, 50748], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 698, "seek": 261878, "start": 2626.46, "end": 2629.7400000000002, "text": " and we can basically just write down a logical formula", "tokens": [50748, 293, 321, 393, 1936, 445, 2464, 760, 257, 14978, 8513, 50912], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 699, "seek": 261878, "start": 2629.7400000000002, "end": 2633.6200000000003, "text": " approximating that toxicity constraint.", "tokens": [50912, 8542, 990, 300, 45866, 25534, 13, 51106], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 700, "seek": 261878, "start": 2633.6200000000003, "end": 2635.86, "text": " Yeah, I mean, there's also plug-and-play generation tricks", "tokens": [51106, 865, 11, 286, 914, 11, 456, 311, 611, 5452, 12, 474, 12, 2858, 5125, 11733, 51218], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 701, "seek": 261878, "start": 2635.86, "end": 2638.46, "text": " that are actually quite interesting, I think that...", "tokens": [51218, 300, 366, 767, 1596, 1880, 11, 286, 519, 300, 485, 51348], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 702, "seek": 261878, "start": 2638.46, "end": 2644.5800000000004, "text": " So basically, there is a, of course, there's a naive way to do it.", "tokens": [51348, 407, 1936, 11, 456, 307, 257, 11, 295, 1164, 11, 456, 311, 257, 29052, 636, 281, 360, 309, 13, 51654], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 703, "seek": 261878, "start": 2644.5800000000004, "end": 2647.86, "text": " You can say whenever I encounter one of the words in the list,", "tokens": [51654, 509, 393, 584, 5699, 286, 8593, 472, 295, 264, 2283, 294, 264, 1329, 11, 51818], "temperature": 0.0, "avg_logprob": -0.1931492110430184, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0003100865287706256}, {"id": 704, "seek": 264786, "start": 2647.9, "end": 2651.6600000000003, "text": " I just remote, like, prevent it from generating,", "tokens": [50366, 286, 445, 8607, 11, 411, 11, 4871, 309, 490, 17746, 11, 50554], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 705, "seek": 264786, "start": 2651.6600000000003, "end": 2654.3, "text": " prevent it from being sampled.", "tokens": [50554, 4871, 309, 490, 885, 3247, 15551, 13, 50686], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 706, "seek": 264786, "start": 2654.3, "end": 2658.58, "text": " But that's not exactly what we're trying to do probabilistically, right?", "tokens": [50686, 583, 300, 311, 406, 2293, 437, 321, 434, 1382, 281, 360, 31959, 20458, 11, 558, 30, 50900], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 707, "seek": 264786, "start": 2658.58, "end": 2659.3, "text": " So...", "tokens": [50900, 407, 485, 50936], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 708, "seek": 264786, "start": 2659.3, "end": 2662.3, "text": " Yeah, but I just, maybe look into plug-and-play,", "tokens": [50936, 865, 11, 457, 286, 445, 11, 1310, 574, 666, 5452, 12, 474, 12, 2858, 11, 51086], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 709, "seek": 264786, "start": 2662.3, "end": 2664.42, "text": " control generation is actually a paper.", "tokens": [51086, 1969, 5125, 307, 767, 257, 3035, 13, 51192], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 710, "seek": 264786, "start": 2664.42, "end": 2665.46, "text": " Yes, yeah.", "tokens": [51192, 1079, 11, 1338, 13, 51244], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 711, "seek": 264786, "start": 2665.46, "end": 2668.42, "text": " So where I think they're doing much more than that, right?", "tokens": [51244, 407, 689, 286, 519, 436, 434, 884, 709, 544, 813, 300, 11, 558, 30, 51392], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 712, "seek": 264786, "start": 2668.42, "end": 2671.1, "text": " Well, so, yeah, so it's like...", "tokens": [51392, 1042, 11, 370, 11, 1338, 11, 370, 309, 311, 411, 485, 51526], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 713, "seek": 264786, "start": 2671.1, "end": 2674.58, "text": " They're also modifying the posterior selection.", "tokens": [51526, 814, 434, 611, 42626, 264, 33529, 9450, 13, 51700], "temperature": 0.0, "avg_logprob": -0.33649522830278444, "compression_ratio": 1.588, "no_speech_prob": 0.0006985734798945487}, {"id": 714, "seek": 267458, "start": 2674.58, "end": 2679.2999999999997, "text": " So if I'm not wrong, if I remember this correctly,", "tokens": [50364, 407, 498, 286, 478, 406, 2085, 11, 498, 286, 1604, 341, 8944, 11, 50600], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 715, "seek": 267458, "start": 2679.2999999999997, "end": 2682.94, "text": " they're basically trying to train some sort of...", "tokens": [50600, 436, 434, 1936, 1382, 281, 3847, 512, 1333, 295, 485, 50782], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 716, "seek": 267458, "start": 2687.02, "end": 2688.86, "text": " They use a classifier, right?", "tokens": [50986, 814, 764, 257, 1508, 9902, 11, 558, 30, 51078], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 717, "seek": 267458, "start": 2688.86, "end": 2691.22, "text": " Yeah, so basically, kind of, they're trying to train", "tokens": [51078, 865, 11, 370, 1936, 11, 733, 295, 11, 436, 434, 1382, 281, 3847, 51196], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 718, "seek": 267458, "start": 2691.22, "end": 2693.5, "text": " a newer model to approximate this.", "tokens": [51196, 257, 17628, 2316, 281, 30874, 341, 13, 51310], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 719, "seek": 267458, "start": 2693.5, "end": 2694.74, "text": " Yeah.", "tokens": [51310, 865, 13, 51372], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 720, "seek": 267458, "start": 2694.74, "end": 2696.2599999999998, "text": " So, but they're...", "tokens": [51372, 407, 11, 457, 436, 434, 485, 51448], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 721, "seek": 267458, "start": 2696.2599999999998, "end": 2700.1, "text": " So, yeah, so their methods has like two disadvantages", "tokens": [51448, 407, 11, 1338, 11, 370, 641, 7150, 575, 411, 732, 37431, 51640], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 722, "seek": 267458, "start": 2700.1, "end": 2702.22, "text": " compared to our advantages.", "tokens": [51640, 5347, 281, 527, 14906, 13, 51746], "temperature": 0.0, "avg_logprob": -0.26116670307360196, "compression_ratio": 1.6009852216748768, "no_speech_prob": 0.00017397796909790486}, {"id": 723, "seek": 270222, "start": 2702.2599999999998, "end": 2706.02, "text": " So one of them is that they cannot guarantee that this is...", "tokens": [50366, 407, 472, 295, 552, 307, 300, 436, 2644, 10815, 300, 341, 307, 485, 50554], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 724, "seek": 270222, "start": 2706.02, "end": 2706.66, "text": " Right.", "tokens": [50554, 1779, 13, 50586], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 725, "seek": 270222, "start": 2706.66, "end": 2710.4599999999996, "text": " 100% logically satisfiable.", "tokens": [50586, 2319, 4, 38887, 5519, 9364, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 726, "seek": 270222, "start": 2710.4599999999996, "end": 2713.74, "text": " And the other is that they have to retrain their model", "tokens": [50776, 400, 264, 661, 307, 300, 436, 362, 281, 1533, 7146, 641, 2316, 50940], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 727, "seek": 270222, "start": 2713.74, "end": 2715.66, "text": " for all different constraints.", "tokens": [50940, 337, 439, 819, 18491, 13, 51036], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 728, "seek": 270222, "start": 2715.66, "end": 2717.2999999999997, "text": " Their model for all...", "tokens": [51036, 6710, 2316, 337, 439, 485, 51118], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 729, "seek": 270222, "start": 2717.2999999999997, "end": 2718.54, "text": " Yes, okay, but...", "tokens": [51118, 1079, 11, 1392, 11, 457, 485, 51180], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 730, "seek": 270222, "start": 2718.54, "end": 2721.2999999999997, "text": " Okay, so suppose in their training data,", "tokens": [51180, 1033, 11, 370, 7297, 294, 641, 3097, 1412, 11, 51318], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 731, "seek": 270222, "start": 2721.2999999999997, "end": 2723.8599999999997, "text": " they're only trained to kind of satisfy...", "tokens": [51318, 436, 434, 787, 8895, 281, 733, 295, 19319, 485, 51446], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 732, "seek": 270222, "start": 2723.8599999999997, "end": 2728.3799999999997, "text": " They only have seen using, say, less than 10 keywords", "tokens": [51446, 814, 787, 362, 1612, 1228, 11, 584, 11, 1570, 813, 1266, 21009, 51672], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 733, "seek": 270222, "start": 2728.3799999999997, "end": 2730.66, "text": " to generate a sentence, right?", "tokens": [51672, 281, 8460, 257, 8174, 11, 558, 30, 51786], "temperature": 0.0, "avg_logprob": -0.2614423925226385, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004511074803303927}, {"id": 734, "seek": 273066, "start": 2730.8199999999997, "end": 2732.98, "text": " What if today I want to use 20 keywords?", "tokens": [50372, 708, 498, 965, 286, 528, 281, 764, 945, 21009, 30, 50480], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 735, "seek": 273066, "start": 2732.98, "end": 2735.2999999999997, "text": " They would not be able to generalize well to that one.", "tokens": [50480, 814, 576, 406, 312, 1075, 281, 2674, 1125, 731, 281, 300, 472, 13, 50596], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 736, "seek": 273066, "start": 2735.2999999999997, "end": 2737.58, "text": " Right, so they're examples of toxicity, right?", "tokens": [50596, 1779, 11, 370, 436, 434, 5110, 295, 45866, 11, 558, 30, 50710], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 737, "seek": 273066, "start": 2737.58, "end": 2740.14, "text": " So they pretrain for toxicity and they can use that anywhere.", "tokens": [50710, 407, 436, 1162, 7146, 337, 45866, 293, 436, 393, 764, 300, 4992, 13, 50838], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 738, "seek": 273066, "start": 2740.14, "end": 2743.18, "text": " So there's some tasks that you're just going to use repeatedly.", "tokens": [50838, 407, 456, 311, 512, 9608, 300, 291, 434, 445, 516, 281, 764, 18227, 13, 50990], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 739, "seek": 273066, "start": 2743.18, "end": 2745.3399999999997, "text": " I guess I see a combination of what you're doing", "tokens": [50990, 286, 2041, 286, 536, 257, 6562, 295, 437, 291, 434, 884, 51098], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 740, "seek": 273066, "start": 2745.3399999999997, "end": 2746.62, "text": " and what they're doing together,", "tokens": [51098, 293, 437, 436, 434, 884, 1214, 11, 51162], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 741, "seek": 273066, "start": 2746.62, "end": 2749.1, "text": " which gives you this sort of entailment during the...", "tokens": [51162, 597, 2709, 291, 341, 1333, 295, 948, 864, 518, 1830, 264, 485, 51286], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 742, "seek": 273066, "start": 2749.1, "end": 2751.58, "text": " Like this broader sense of satisfaction, right?", "tokens": [51286, 1743, 341, 13227, 2020, 295, 18715, 11, 558, 30, 51410], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 743, "seek": 273066, "start": 2751.58, "end": 2754.3399999999997, "text": " Which I think was an interesting motivation.", "tokens": [51410, 3013, 286, 519, 390, 364, 1880, 12335, 13, 51548], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 744, "seek": 273066, "start": 2754.3399999999997, "end": 2757.2599999999998, "text": " You didn't quite deliver on, but you could deliver on.", "tokens": [51548, 509, 994, 380, 1596, 4239, 322, 11, 457, 291, 727, 4239, 322, 13, 51694], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 745, "seek": 273066, "start": 2757.2599999999998, "end": 2759.5, "text": " That's actually the current project we're working on.", "tokens": [51694, 663, 311, 767, 264, 2190, 1716, 321, 434, 1364, 322, 13, 51806], "temperature": 0.0, "avg_logprob": -0.18227398546436166, "compression_ratio": 1.702247191011236, "no_speech_prob": 0.0007792479009367526}, {"id": 746, "seek": 275950, "start": 2759.5, "end": 2761.46, "text": " So we're trying to combine the models", "tokens": [50364, 407, 321, 434, 1382, 281, 10432, 264, 5245, 50462], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 747, "seek": 275950, "start": 2761.46, "end": 2764.78, "text": " that can handle soft constraints with our model, too.", "tokens": [50462, 300, 393, 4813, 2787, 18491, 365, 527, 2316, 11, 886, 13, 50628], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 748, "seek": 275950, "start": 2764.78, "end": 2766.26, "text": " Okay, beautiful.", "tokens": [50628, 1033, 11, 2238, 13, 50702], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 749, "seek": 275950, "start": 2766.26, "end": 2768.94, "text": " And finally, the reason large types of models work so well", "tokens": [50702, 400, 2721, 11, 264, 1778, 2416, 3467, 295, 5245, 589, 370, 731, 50836], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 750, "seek": 275950, "start": 2768.94, "end": 2770.94, "text": " is just because of attention, right?", "tokens": [50836, 307, 445, 570, 295, 3202, 11, 558, 30, 50936], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 751, "seek": 275950, "start": 2770.94, "end": 2773.62, "text": " And so when you're using a hidden marker model, right,", "tokens": [50936, 400, 370, 562, 291, 434, 1228, 257, 7633, 15247, 2316, 11, 558, 11, 51070], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 752, "seek": 275950, "start": 2773.62, "end": 2775.86, "text": " you're losing the power of attention.", "tokens": [51070, 291, 434, 7027, 264, 1347, 295, 3202, 13, 51182], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 753, "seek": 275950, "start": 2775.86, "end": 2779.46, "text": " But I guess I'm looking at this and trying to convince myself", "tokens": [51182, 583, 286, 2041, 286, 478, 1237, 412, 341, 293, 1382, 281, 13447, 2059, 51362], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 754, "seek": 275950, "start": 2779.46, "end": 2781.94, "text": " that, well, you get all the attention in the right-hand part", "tokens": [51362, 300, 11, 731, 11, 291, 483, 439, 264, 3202, 294, 264, 558, 12, 5543, 644, 51486], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 755, "seek": 275950, "start": 2781.94, "end": 2784.5, "text": " and then you just need a little bit of bias of selection", "tokens": [51486, 293, 550, 291, 445, 643, 257, 707, 857, 295, 12577, 295, 9450, 51614], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 756, "seek": 275950, "start": 2784.5, "end": 2788.02, "text": " in the left-hand part here, and that's what's happening.", "tokens": [51614, 294, 264, 1411, 12, 5543, 644, 510, 11, 293, 300, 311, 437, 311, 2737, 13, 51790], "temperature": 0.0, "avg_logprob": -0.18390864925784664, "compression_ratio": 1.785953177257525, "no_speech_prob": 0.00017397524788975716}, {"id": 757, "seek": 278802, "start": 2788.02, "end": 2790.7, "text": " Yeah, so basically, intuitively, you can think of this part", "tokens": [50364, 865, 11, 370, 1936, 11, 46506, 11, 291, 393, 519, 295, 341, 644, 50498], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 758, "seek": 278802, "start": 2790.7, "end": 2793.54, "text": " as only providing, like, guide and suggestions,", "tokens": [50498, 382, 787, 6530, 11, 411, 11, 5934, 293, 13396, 11, 50640], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 759, "seek": 278802, "start": 2793.54, "end": 2798.54, "text": " leading the model, leading GPT to satisfy the constraint.", "tokens": [50640, 5775, 264, 2316, 11, 5775, 26039, 51, 281, 19319, 264, 25534, 13, 50890], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 760, "seek": 278802, "start": 2798.62, "end": 2800.62, "text": " But on this part, it's kind of responsible", "tokens": [50894, 583, 322, 341, 644, 11, 309, 311, 733, 295, 6250, 50994], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 761, "seek": 278802, "start": 2800.62, "end": 2802.98, "text": " for fluency, grammar, and everything.", "tokens": [50994, 337, 5029, 3020, 11, 22317, 11, 293, 1203, 13, 51112], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 762, "seek": 278802, "start": 2806.42, "end": 2807.54, "text": " Beautiful work.", "tokens": [51284, 14724, 589, 13, 51340], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 763, "seek": 278802, "start": 2807.54, "end": 2808.38, "text": " Thank you.", "tokens": [51340, 1044, 291, 13, 51382], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 764, "seek": 278802, "start": 2810.54, "end": 2811.38, "text": " All right, thanks.", "tokens": [51490, 1057, 558, 11, 3231, 13, 51532], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 765, "seek": 278802, "start": 2811.38, "end": 2814.66, "text": " Maybe we should wrap up, and thanks again.", "tokens": [51532, 2704, 321, 820, 7019, 493, 11, 293, 3231, 797, 13, 51696], "temperature": 0.0, "avg_logprob": -0.24325304533305922, "compression_ratio": 1.4955357142857142, "no_speech_prob": 0.00046506189391948283}, {"id": 766, "seek": 281466, "start": 2815.66, "end": 2821.2999999999997, "text": " And we can have lunch with the database folks", "tokens": [50414, 400, 321, 393, 362, 6349, 365, 264, 8149, 4024, 50696], "temperature": 0.0, "avg_logprob": -0.41347172146751765, "compression_ratio": 1.0123456790123457, "no_speech_prob": 0.0037868621293455362}, {"id": 767, "seek": 281466, "start": 2821.2999999999997, "end": 2823.2999999999997, "text": " and talk about generating SQL query.", "tokens": [50696, 293, 751, 466, 17746, 19200, 14581, 13, 50796], "temperature": 0.0, "avg_logprob": -0.41347172146751765, "compression_ratio": 1.0123456790123457, "no_speech_prob": 0.0037868621293455362}], "language": "en"}