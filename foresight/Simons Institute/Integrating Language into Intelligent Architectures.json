{"text": " I mentioned Gabe Grand, another student collaborator who is one of Jacob and Dreyas's students, and Tanja Shren, a student also co-advised by Josh and Vikash, as well as Noah Goodman, my advisor Vikash and Jacob and Dreyas. So our broader goal today in this talk is actually going to be to reflect based on many of the recent advances that we all know of modeling natural language, but I think also drawing on evidence from toolkits, other toolkits in AI. Evidence from cognitive science and from neuroscience on kind of the broader spectrum of different ways that we might think about building intelligent architectures that use and produce and learn from language, as well as more generally, I think what role language might play or could play in a computational system that we say thinks. And obviously this is a question that many people in this room, but many other people who probably aren't in this room have thought about from philosophers of language to linguists and neuroscientists. And we thought it actually might be a useful exercise to kind of start by reflecting on the underlying answers to this question that are or aren't suggested by some of the most prominent directions that we're taking in AI research right now. And of course, one of the reasons why we're even asking this question at this scale, what is the role of language in intelligence is in large part driven by this remarkable observation that we all know about from just a few years ago, which is if you train these large and specifically transformer based neural architectures as language models, just to do this next word prediction task, with enough language data, they start to show behaviors that really suggest that there's something more than language at play. They look like they're thinking. They can induce patterns from just a few examples in data, or they can even read the definitions of totally novel words like Zaka Tota in context and produce realistic sentences that appear as if they understand how to use those words immediately. And so a lot of the excitement, I think it's fair to say around language models is that maybe for the first time, we're seeing something that is offering a scalable route towards implementing more generally intelligent architectures just by directly scaling, or largely by directly scaling the amount of language data that they're being trained to predict. All right, so what is the underlying idea here? What does this have to say about language? Well, I think it's fair to say that one of the dominant hypotheses that's underlying why we were even starting to see some of this behavior rests on kind of a two-part idea. One is something about the nature of language itself, right? It's the suggestion that language is sufficiently diverse and so broad, maybe because we suspect that humans express so much of their thoughts in such a diverse range of their thoughts and language that being able to perfectly solve this task, to be a perfect language model, or at least a really good one, is essentially an AGI complete task, and maybe also one that conveniently, unlike other kinds of tasks, like predicting all of the videos in the world, we have maybe efficient architectures to do and enough data to do. And I think it's worth noting that this doesn't actually really have to be a particularly strong hypothesis about what it is computationally that thinking looks like, or even how language necessarily is implicated internally inside the computational processes that we call thinking. Rather, it's really just a hypothesis about the nature of the language modeling task itself. And I think what a lot of people in this room would agree now is that, well, that might be true about the language modeling task in principle, much of the most exciting research that we're seeing using LMS right now actually isn't predicated on really the hope or the idea that just by scaling to more and more data, we're gonna expect transformers trained and used in this way to actually solve that task or become perfect next-world prediction models. And the intuition behind that, which I think many people have pointed out, comes both from the way in which we're looking in which transformers work, right? There are autoregressive language models that are doing a fixed, finite amount of internal computation that only scales based on the previous linguistic context. And it doesn't really make sense that, of course, you can pose questions in language like arbitrarily difficult math problems or planning problems that intuitively require an amount of computation, whose complexity doesn't actually depend linearly on the previous token context. And I think that's been matched empirically by lots of different observations about which kinds of sentences are hard to complete if you treat them as next-world prediction tasks in this way, right? Hard arbitrary math problems or planning problems like these. Right, so some of, you know, if you're thinking about the role of language, why is language, why are language models so big right now as, oh, well, language just has all the evidence necessary to train something to think. Like Leo said, that's not actually committing to any hypothesis about sort of how language is used internally in thinking. The transformers' computations are not necessarily doing anything linguistic as it's computing the distribution of the next word. But more recently, we've seen people use language models to solve these harder tasks by letting them think more. And what letting a transformer think more means is letting it generate more tokens. So in this view, thinking doesn't necessarily emerge just as predicting the next token. Rather, thinking happens in language, right? It happens by sort of producing a chain of thoughts or using a scratch pad or deciding that you're going to invoke a calculator or write some code and execute the code. And the sort of hypothesis embodied, I think by this line of work, is that the role of language and intelligence architecture is bigger. That language or some kind of like running monologue of language is the central controller of thought. And thinking is taking place primarily in language. And what's maybe striking about these proposals is that if you ask most cognitive scientists or neuroscientists, they'd probably say, this isn't the role that language plays in relation to general intelligence in humans, or at least it's not the dominant hypothesis. Until just a few years ago, this probably wouldn't have been the role that many AI researchers would have necessarily posited for language as the main controller or the main substrate of a running monologue that controls all thinking. And so we thought we'd review some of the background for what language seems to look like in humans as a basis for informing how we might build architectures that better capture those more human-like roles for language. Yeah, and so I just want to be clear that we're going to do kind of a quick background on some of the neuroscientific and cognitive evidence about what language might look like in people. And our goal really isn't to push back in any way against these other kinds of paradigms for where language might fit in. It's rather to present at a mode where we're really thinking a lot about scaling up language models as one really dominant role, what our other roots might be because it doesn't seem like that that really matches a different prominent intelligent architecture that's sitting here in the room today. So one source of evidence for the role that language appears to play in human cognition comes from neuroimaging data, fMRI data, suggesting how language is processed in the human brain. And at this point, convergent imaging data from many people, including Ed Fedorenko at MIT, Standa Hain and recent graduate students in our department, Kyle Malewald and Anya Ivanova, suggests that human brains have this language-specific network that handles many of the tasks that we associate with language. It's activated when people do tasks like listening to sentences or reading them or speaking or writing words. And this is not just an English-specific network. The same general region is activated no matter what language someone is speaking. It even fires when they're producing constructed languages for people who come to learn and become fluent in languages like Dothraki or Klingon. And right, what is it that this language network does? Well, increasingly, one dominant hypothesis is that it actually does do something like next-word prediction. And in fact, among many other kinds of alternative models, like people were earlier, for a long time I've been trying to do things like align words back to the brain, transformer architectures really do seem to be among the best models that we have right now of the neural activity of this language network, specifically when they're trained on next-word prediction tasks and not other linguistic tasks like NOI. But I think what we see in this neuroimaging evidence also highlights the ways in which the role of this human language network probably is not as the controller or the central seat of cognition. It's selective for language and it isn't involved in many other kinds of thinking activities, even ones that might seem to involve symbols like solving math problems or reasoning about logic and physics and social reasoning. Those invoke other regions of the brain and language interfaces modulary with them. It can interface very generally with them, but it doesn't seem to be where the bulk of thinking is taking place. Oh, yes. Oh yeah, I just had a question about the next-word prediction findings. So did you also try to say like math word prediction and this is actually worse than next-word? Yeah, so I think they do compare or like closed tasks are one of the alternates that they're looking for. And also, yeah, people should feel free to just shout out as we've been doing all along. So also, right, actually you can see adult patients that suffer strokes that only damage this area of their brain and we find that they can still think about, well, they can't comprehend spoken language, but they can still think about all these different kinds of tasks fronted in different mediums, like they can draw physical inferences from videos that they're watching. And conversely, maybe most tantalizingly, it's actually also possible to sustain localized damage that leaves you still able to produce these long, very relatively fluent and quite syntactically coherent sentences, almost as if they're maybe just like a very rudimentary and local word-based language model, while not really conditioning meaningfully on what someone else is saying. So they say, do you like it here in Kansas City? And this person says, yes, I am. Or really producing sentences that are obviously more globally meaningful or goal directed towards the question. And I don't want us to overindex on those results, but they kind of fit in with this more coherent picture. And in many ways, I think this kind of recent evidence from neuroscience actually supports the broader picture of human cognition and the place of language in it that most cognitive scientists and many linguists have already believed based on what we see and have come to learn through developmental science, studying how kids think and where language seems to fit into that picture, right? So a broad body of work at this point suggests that infants, well before they learn language of any kind or are speaking, already independently perform many of the tasks I think we associate with coherent thinking, from reasoning about physics to planning to drawing causal and probabilistic inferences. And language seems to be something that humans learn and scaffold on top of this structured basis for thinking. We take much less input data to learn to produce fluent language than a large language model. And humans that actually aren't exposed to any language input at all. So famously there are these deaf children in Nicaragua who grow up in isolated hearing families actually spontaneously come to produce early sign languages as a product of trying to communicate events that bear many of the hallmarks of the syntax that we associate of our own natural languages, like distinguishing between the subject who's punching and the person who's getting punched. Suggesting that in many ways the language we produce somehow externalizes the underlying structure of the thought that we already have. And of course, there are many other animals that we associate as being intelligent in some way and that have been modeled using the models that we associate with probabilistic reasoning and planning that don't use language at all. So it doesn't feel like language is by any means necessary for what we think of as thought. So formalizing this picture of an intelligent system, one that's shared across animals and non-linguistic infants and maybe captures some of the computations involved in many of those thinking tasks that don't involve the language network in our brains is both the central goal of a lot of cognitive science and has been a historic motivation for diverse fields within AI before the current sort of LLM centric moment. So if you open up Russell and Norvig's textbook, AI a modern approach, you'll see an equation like this that's supposed to sort of capture what we believe about how intelligence works. Computationally or a computational model of intelligence. Where the idea is that an intelligent agent is one that has sort of structured internal world models that can be updated based on observations of the world and in which we can sort of predict the results of our actions that the agent has some sort of values or desires that are captured in some kind of utility function that the agent can do probabilistic reasoning over its observations and the possible actions it might take and what their expected utilities are and that it can do some kind of planning to optimize the value of the back end. Yeah. Is there a normative statement or descriptive statement? Is it the case that we are defining intelligent agent to be having this kind of property or it's like if we want to do intelligent architecture, we should have these? Yeah, so I think some, it's a great question. I think some of the work definitely is coming at it from a normative perspective. This is like a definition of what rational actions might look like, right? But I think there is a lot of work computational cognitive science that sort of in various domains has collected a lot of evidence that people either like recognize this as normative or behave like this in computation, according to the limits of what they can do computationally. So in this architecture, the controller for thinking would be some kind of system that's capable of general world modeling and probabilistic reasoning or planning and utility maximization rather than sort of a primarily linguistic system necessarily. Now largely attempts in artificial intelligence to directly implement this equation by building out components that sort of map on to the different elements of this equation have struggled to match the computational efficiency and the generality of natural intelligence. But we have seen over the past couple of decades or so the emergence of this new class of tools called probabilistic programming languages for building software that can solve probabilistic reasoning tasks at least in limited domains. And sorry, and these languages have been applied to create systems that do everything from perceiving cluttered 3D scenes more accurately sometimes than object detector neural nets to interpreting and predicting economic trends more accurately than leading industry solutions like Facebook neural profit. And these probabilistic programming systems are enabling these applications with two key technical features. They feature modeling languages that let users express complicated probabilistic models of the world as programs making it easy to write down rich probabilistic models that are defined in terms of expressive program-like components. So for example, the probabilistic models behind these example applications are defined in terms of 3D renderers, symbolic planners, scientific simulators and so on. And the second thing that probabilistic programming systems do is that they automate various mathematical operations on these models and that automation makes it possible for users to concisely implement sophisticated algorithms for probabilistic reasoning, such as the variety of Monte Carlo and variational inference algorithms that power these example applications. One way of thinking about these tools is as kind of like PyTorch or TensorFlow, but instead of writing differentiable models and doing optimization, you're writing probabilistic models and doing various probabilistic reasoning tasks. So far, these AI engineering efforts haven't really made contact with the sort of language model part of AI. Much like there hasn't yet been a concerted effort to take this classical picture of intelligence architecture and figure out how language might be richly integrated into it. So for the rest of the talk today, we're gonna explore two different approaches for thinking about where language might fit into this picture and approach. So probably Leo's gonna begin by talking about how an intelligent agent might incorporate lots of externally produced languages, explanations, observations, questions, how an agent can incorporate all of those forms of language into the way that it updates its beliefs and decides how to act in the world. Then I'll talk a bit about systems that leverage language as a tool for thinking within its models of the world or its probabilistic reasoning algorithms. And each of these corresponds to very recent preprints of work. So I also wanna give a disclaimer. We should really emphasize that both are preliminary proposals and we're giving a very speculative talk, not scaled architectural solutions here. Cool, right. So the first person that's portion of this talk is summarizing work. If you wanna read more, you can find this very long paper from word models to world models. That's up on archive and this is primarily done with another student who's not here today, Gabe Grant. And as I just mentioned, the context for this work is thinking about how we can build systems that capture the breadth with which all the external language we hear seems to inform at least our human thinking. And what's clear, right, is that this role seems to be very broad. If we take the basic model of an agent with beliefs and goals, well, it seems like there's an incredibly diverse range of situations in which we can update our beliefs about a situation from observations that we express in language, or in which the goals of our thought are to answer questions that we specify linguistically. And these might implicate our knowledge of other agents drawing on our intuitive psychology to reason about what they think and what they'll do. Or we can talk about the physical world around us, what we perceive and ask questions that require us to reason about what we see and draw on our physical intuitions. And of course, one of the remarkable things where we're also excited about language is that it doesn't just draw on what we already know, it's this means by which humans seem to learn and pass on profoundly new knowledge, whether that's new concepts that we define in words or learn from words to really profound new theories and conceptual systems, right? Much of what we know about the world, the fact that there are wars, what wars are, legal systems, sciences, comes from information that it feels that we acquire from language in some way. But how do we do that? Well, I think one longstanding lens for thinking about language that's kind of persisted before the LLM-based moment is that what language is, is this external symbolic medium for communicating human thoughts. And the way that it does that is because there's some kind of general mapping function from our internal representations of thought into this external symbol system, that's language. And so in this kind of older framework, what it means to understand language or make meaning means mapping back from external sentences that we hear into structured internal representations. And what we explore in this paper is a general proposal that casts the meanings of language as these mappings or probabilistic distributions over expressions in a probabilistic programming language. And I'm gonna come back after some concrete examples to how I think this relates to the conceptual rule semantics that Steve articulated in the first talk, because I think there are actually some really deep connections here, the ways in which this might be one way to formalize or enrich some of those ideas. And this architecture, this proposal here also suggests how language can be integrated into a more general architecture, because it's one that already starts out with, as Alex mentioned, some kind of existing internal modeling language, whose goal is to represent the world probabilistically, query those models and specify what it means to draw coherent inferences over them. And it also suggests, I think, a framework for formally modeling the content of different kinds of sentences in language as the kinds of probabilistic programming expressions that they might map into. So, and so in this paper, we begin by exploring how this proposal might be instantiated with respect to a bunch of different domains of reasoning, sorry, including general probabilistic reasoning, but also reasoning about relations or physics and social situations. And in all of these, we're gonna propose that we might think about the language that communicates general conceptual knowledge about the world definitions or causal knowledge as constructing these probabilistic expressions that are those that build up probabilistic generative models. And then in this framework, observations in the language, like there is at least one red mug in the scene or Charlie is Dana's grandfather, construct formal conditioning statements, which update the state of this probabilistic model. And then questions map into query expressions that specify the formal target of probabilistic inference with respect to a model. And in this framework, we're thinking essentially cast as probabilistic reasoning. We suggest that another way that we can think about the role of a language model, one that's much smaller, like the language network in the brain is actually as a means of instantiating this meaning function in a way that we've really never had before to protect these kinds of context specific and previous discourse conditioned mappings from sentences in natural language into distributions over expressions that convey meaning in a probabilistic programming language. And so as a part of this long running disclaimer, what I'm gonna be showing is a really minimal implementation of this framework, but really intended as a pointer to different directions with which we might scale this approach to implement a more general interface between language and arrange a different core cognitive domains. So just to be clear, concretely in the examples that you see next, our meaning function is gonna be implemented using codex, right, an open AM model much smaller than the state of the art right now that's trained to learn joint distributions over language and code. And the probabilistic programming language we're gonna show is church, which is this very simple probabilistic programming language that supports kind of very general sample based inference procedures. And our goal is to demonstrate how this framework might broadly interface between language and a bunch of different core cognitive domains. So first to illustrate the basic sense in which a proposal like this might allow language to update an agent's beliefs and query a world model, I'm gonna begin with a really simple toy example that actually draws on a bunch of prior cognitive science experiments in which real people were asked to draw various inferences about which teams of players might win different games of tug of war based on the games that you'd previously seen players win or lose. And so this is older work from Josh's group that demonstrated I think the sense in which this normative model, this Norvig model of probabilistic inference actually in many ways predicts the actual behaviors and predictions made by humans using a very general probabilistic model of the mechanics of this tug of war game. And our goal here is to show how our framework we can implement an interface between natural language and all of the core examples of this older experiment. So just to go through here, I think what you're seeing in this little toy example, the world model that's being defined on the screen is capturing the basic causal relationship by which properties of different human players might influence the outcomes of different tournaments that they play in tug of war. So for instance, here we're modeling players as having some internal inherent strength value where strength varies approximately normally but as this unobserved latent variable over different kinds of players. And we also think of players as having some kind of internal laziness value which represents the percentage of the time that they actually don't act according to their underlying strength. And how did these variables determine the outcomes that we observe of given games of tug of war? Well, the strength of a whole team of players depends on the cumulative sum of its player strengths. But if a player is deciding to be lazy in this game, they might not pull as hard as they could. And whichever team pulls with the most strength in a given match is going to win that match, yeah. Did you design the primitives of strength and laziness or a codex come up with the primitives themselves? So in this one, we're looking at a model that's derived from the older work. So these are designed, but yes, that's later in this work. We're gonna show some examples of how you can learn this kind of model from someone just talking about it in language like the definition that I just gave. Right, and so again, you know, this is a really simple example, but I think also one that actually captures a surprising amount of the basic causal knowledge that people have if you tell them that you're gonna be listening to tug of war games, but sometimes people can be lazy and not pull as hard as they could. So how do we go about relating language in this domain? Right, well, one means by which we can induce a simple notion of a meaning function that actually fits the definition we just gave is by conditioning a language model both on this context-specific generative world model and on a few examples showing how language is mapped into sampled probabilistic programming expressions in this domain. And what we've done now, right, is effectively induce this kind of situation-specific contextual mapping from arbitrary new sentences to expressions that conditions both on the general prior distribution that codex is over language and code, and this kind of specific discourse thinking context of how language is being used in this situation. And there are clearly other ways to do this, some of which we'll talk about later, but we're using this example to illustrate just how much you might be able to do with this kind of minimal implementation, a notion of a model that translates between language to code. Right, so what kinds of language might we say here and how might we think about them in relation to probabilistic programming expressions? Well, a general proposition, like Josh won against Leo, gets translated into or might map, we might think of mapping or meaning a conditioned statement, an observation that Josh won against Leo. If we make subsequent observations, like then Josh went on to claim victory against Alex, we can continue to kind of generally use this meaning function that we've induced to turn that into a probabilistic programming language that captures the fact that Josh won against Alex. If we then say that even working together as a team, Leo and Alex still couldn't beat Josh in this game of tech of war. At this point, if we want to answer a query, like, okay, wait, how strong is Josh? What we think of as thinking in this situation is actually sampling from the posterior over possible worlds from the generative model that we just defined, subject to the observations that we've just made. And indeed, that means that the meaning of a sentence like how strong is Josh is really a structured publicistic inference query. What is the latent variable that is Josh's strength? And what we see here is that given his track record, all these people that he's beating, even playing together, our inference is that Josh is likely a good bit stronger than average. And that also means coherently, we might expect that a priori, a new player we've never seen, like Gabe, is going to be unlikely to beat him. So if we ask what are the odds of Gabe at beating Josh, we see that we think it's somewhat unlikely. Question? Oh, yes. So on the how strong is Josh, it seems like there's an interesting thing here where there's also an implicit question of what the word strong means in this context. And that, like, right, it's like not a number, it's kind of some like, comparative adjective. It's like, probably a non-intersective. So I guess, is that something you think about? This framework, or should I just be kind of, like, ignoring this sort of issue? Yeah, well, okay. So I think there's a number of ways that we can think about that. I mean, so, right, so the one sense we could say is like, right, how strong is Josh? Isn't, the answer to how strong of Josh isn't a number. Rather, it's kind of this distribution over this posterior distribution of various underlying strength values that we currently might infer that Josh has with respect to the general value that we have. I think another kind of popular definition of various uncertain adjectives, like a word like strong, right, is that you have some internal threshold value, or the person speaking has some kind of internal threshold value that you must kind of jointly infer with respect to the context in what you've seen. And some of the examples that I'll actually give later, so, right, there's kind of a long line of work in linguistics, including some work that treats that as like a pragmatic inference. I think some of the interesting work that we'll show a little bit later is that, there are some ways in which you might think of this mapping function as actually being a general one that includes that notion of pragmatic inference. And also, I think captures the sense in which if you continually, are you really doing this kind of pragmatic inference all the time, or do you actually, in many general settings, like talking about the strengths of people, actually have some kind of cashed older notion of strength that you can draw in. And I think actually, this notion of large language models as just being this learned mapping function from language into expressions include, can also capture the sense in which that knowledge is amortized away. And you might not be having that inference. Yeah, Chris. She's sort of just denying or ignoring what makes people so excited about large language models in their meaning representation and ability to do inference. I mean, because, okay, you've got sort of cooler probabilistic programming language on the right hand side, but in some sense, the picture is still, this is semantic parsing, like it was 2010 to 2015. And yes, you're using a large language model, but you're not actually using the excitement of a large language model as a representation system. Yeah, and I think, so probably each of us would have different answers to this, but part of what we're hoping to paint out over the course of this talk is I think some of the ways in which actually, right, of course, no one wants to say we're gonna go back to kind of the brittleness of semantic parsing, but I think one thing that large language models actually give us, or one proposal in this talk, is that there are some aspects of the theory, kind of the classic notion of linguistics, and certainly the classic notions of semantic parsing that actually normatively capture a lot of what we really might want when we think about, so one answer for an AI system is, well, yes, in some way, we don't wanna throw, certainly we don't wanna throw away everything that we're learning from large language models, and I think one answer to that is kind of the answer that I gave to Jacob, right? If we think about not always, you know, in these examples, we're showing this very direct system in which we always start with language and we always map into some sort of probabilistic programming expression, and that's where all of the thinking happens, and we might think, well, that doesn't totally make sense because there are lots of cases where, as you're saying, we have every reason to believe that large language models have learned a lot of latent information. They can do a lot of, they certainly have a lot of latent conceptual information, and maybe to some degree, they can even perform certain kinds of limited, amortized inferences, or reuse old inferences that they've learned from other people I've had, and so in the second part of this talk, we're going to show different ways in which, well, this probabilistic programming language itself doesn't necessarily need to be something that's isolated from what large language models have learned. It also can embed calls to large language models within it to kind of draw in that sort of knowledge. Haven't you gone back to the visualness of semantic housing because you're doing this translation into symbolic semantic representation, which really ends with your actual result, and it's riddled in the same way? Well, right, and also, so no, I think I would say, I don't totally think that the way in which we're using, or the sense in which, or I think there are some ways in which this kind of broader definition in which you are saying, well, the meaning of a distribution, or the meaning of a sentence in language isn't just one probabilistic programming expression, right? That's what we're showing here for pedagogical purposes, but you might say, well, okay, right, how are you going to obey kind of the ambiguity of language? There are kinds of sentences that are definitely ambiguous. So one example that we've looked at are sentences in which you say something like, Josh beat Alex and Leo, right? And you might ask, well, you know, that's kind of a classic syntactic construction. Does that mean that Josh beat Alex and Leo, and they were playing on the same team, or Josh beat Alex, and then Josh went on to beat Leo? And what we see, or generally, what you might say is, well, the meaning of that sentence actually shouldn't be picking one expression or the other. It should be kind of the distribution over those possible parses, and that distribution also shouldn't just be something that we can determine in this totally context and sensitive way, it should actually depend on all the previous patterns in the discourse. So if someone's continually been using this conjunctive and to refer to teams of players playing together, we should take that kind of discourse bias into account. And I think actually, right, this provides, or thinking of large language models as kind of generally having learned this broad joint distribution, but one that can be kind of conditioned quite richly both on the content of this generative model. So it's not trying to come up with a universal definition of strength. It's not even necessarily trying to come up with a universal definition of any of these words. It's thinking about how they might map contextually into the best possible expression in the context of a particular local model built for a particular situation. I think is obviously related to, but attempting to address some of the brittleness challenges of semantic parsing in the past. I think another answer to this, right, is that part of the problem of semantic parsing previously has been actually that the mapping functions have historically been difficult to get right. Whether you were thinking about those as kind of old hard-coded grammars or many of the attempts to kind of learn these things, we are very domain-specific supervision. So you wanna have a semantic parser for a particular robotics domain. You need a thousand examples of sentences about that particular robotics domain and a thousand paired with a thousand examples of programs that are operating on that particular domain. What we're seeing here is I think something that says no. What it means to learn language generally is to learn kind of this general mapping between language and some kind of underlying representation. And also, one reason why we might want a system like this is because we want to be able to condition coherently on information that's not just coming from language. And we want to think about how a general substrate in which the only, yes, we might be told that Josh went against Leo, but we might also be watching videos that give us information about Josh's strength, that convey our observations. We might also have seen pictures like the ones in the stimuli that we saw before demonstrating the results of previous outcomes of matches. And I think one thing that suggests is we want this kind of general substrate in which we can think about how those observations, including the observations from language, but without prioritizing language in any way, I think are coherently considered. So I think it depends on what part of semantic parsing you, or yeah, I think the answer to that depends on what part of semantic parsing we think of as being the source of the brittleness that caused us to throw that paradigm into question. Yeah, and maybe I'll just offer one more perspective. So one part of it is what Leo is saying. Traditional semantic parsing is brittle in two ways. One is, do you have broad coverage of language that you can parse into your system? And two is like how broad coverage are the set of query, like the set of semantic queries that you can actually answer. And I think what you're pointing out is this doesn't seem to address the second source of brittleness, which is that your system can only answer certain things, it can only reason about certain things. Brittleness of the formal representation language that you're using. Right, that's true of large language model representations. Right, so I think my sort of take on that is from a kind of AI engineering perspective is sort of a branching in two directions. One is, I think we have made some progress that this is not really evoking, probably, toward systems that within restricted domains can reason coherently and probabilistically about a wide range of queries. So we have systems like this inference QL system that uses nonparametric bays to analyze huge data tables and come up with a model of that system that or of your data that can answer all sorts of questions like, oh, you know, show me like which people in this data set are like probably overpaid given their experience or something like that. So in the same way that people are kind of excited about using natural language or using language models to parse into SQL, right? Because so much data is in SQL and it's a very SQL is a very expressive language for asking questions about that data. When we have a probabilistic system, like a good probabilistic model of that data under the hood, it enables conversational patterns that are not enabled when you have like SQL as the database because we expect our conversational partners to have coherent beliefs about the world, to update those beliefs in response to new evidence that we give it to be able to report uncertainty and make sort of modal judgments. And so one engineering path is to take those kinds of systems and sort of build conversational interfaces to them that behave more like an intelligent person would behave and can draw inferences that you might not draw if you're just talking to a SQL backup. The other path that sort of we'll talk about in the next part of the talk is how can we use those representations that language models have learned to make the probabilistic inferences more interesting and more robust, less brittle, without sort of totally embracing the other kind of brittleness which is the kind of brittleness that language models seem to have right now, which is that they draw, that they don't really necessarily reason with coherent probabilistic beliefs. So maybe, yeah, let's go into that next part. Yes, yes, okay, right. Well, so right, so in the interest of time, I'm actually gonna like skip through some of the rest of this example, which I think is just more of what you've seen, but one sense in which I think, yeah, maybe a third part of the answer to Chris, I would say, is that, right, you know, yeah. I think part of what this is trying to do is explore some of the ways in which we might answer ways, right, without giving an answer in all the ways in which we might answer the ways in which language models themselves are brittle with respect to what we also want from a model of intelligence, right? We might suspect that when we answer, ask questions like this, really what we are trying to do is specify some kind of normative query that captures formally a sense of, well, we want something like the posterior with respect to some kind of internal model of the world. And, you know, this is kind of the simplest means, or this is a very simple example of how we might formally impose that kind of structure, but one that I think can be elaborated on, depending on the kinds of primitives and the ways in which you're thinking about what it is that the probabilistic programs can express, right? So one way in which we might think about doing that is by thinking about probabilistic programs that themselves have access to other kinds of means of calling other different mechanisms and cognition, right? So I think I would draw a contrast here between the notion of the large language model as a controller, the one that's making the decisions about when to write little snippets of code and to execute them, when to call out to little planners and incorporate them, or stuff like the Minds Eye work, right, where there's a language model, it decides when to call out to a physics simulator, but the way it interprets the outputs of that physics simulator is to paste those back into the language model context and try to draw inferences on them in turn. Rather, in this kind of framework, right, what you can kind of see a, or yeah, the direction that this framework would be pointing towards is to say, well, on the other hand, we already have languages that allow us to do things like build expressive generative models over three-dimensional scenes that also capture things that we might want only from perception, like knowledge about how the shapes of objects tend to accude each other, or incorporate rich models of physics, or that model theory of mind as taking place recursively and thinking about agents who themselves have beliefs about their own internal world models and are actually choosing their actions as planners, right? And in this kind of framework, you can point the way towards a kind of model that says, well, how is it that I might incorporate language into these kinds of models sitting alongside these other kind of observations that I might make, right? So how might I think about the meaning of images that I want to generate that specify specific constraints, or imagination, or, right, go ahead, Jacob. Yeah, I just had a clarification question. So you were talking earlier about having this meeting function, and then I think also we're mentioning something about like code x, in terms of the questions. I'm just trying to understand which of these is that. Is that here, or is the meeting function when you come later? So that's maybe the first question and then the other clarification is, so are these statements actually just been programmatically created from code x by prompting the text that was on? Yes, that's right. So by meeting function in this framework, we say, well, there's kind of two generalizations of a meeting function. There's a general joint prior, right? That code x is already, that it's learned between language and code, and then there's this kind of context specific meeting function in the sense that it's conditioned on whatever's in the prompt, the generative model, and some examples of how language relates to expressions that it's doing, so that's a meeting function. And yes, all these examples that you're seeing are one sample from that distribution. Right, and one of the things that I wanna point to here, right, is it does, I think it suggests a framework or another means of thinking about what it means for language to construct new concepts from definitions, or even come to construct new world models from thinking like somebody in the beginning asked, right? So how, for instance, might we think about enriching an existing structured relational model with concepts that we learn from language? So for example, if we consider kind of a formal model of kinship relations, we might say that, well, the generative model of this domain is itself represented as a probabilistic program. It captures both the causal means by which people give rise to their children, and also the definitions or one notion of the definitions of what it means to be something like a sister or a father with respect to this core notion of how family trees come to be. And so if you take this kind of general notion of the meaning of language as being the distribution over expressions that it creates in a probabilistic programming language, you might start to think how we can formally think about relating definitions for various kinds of relational terms. An uncle is the brother of one's parent or the husband of one's aunt. A pibling is a gender neutral term for an aunt or uncle, that's the sibling of one's parent, or this relational notion of a sister of one's father from a language that's actually not found anywhere on the internet. And I think the core thing that we wanna suggest here, right, is why do we even have definitions at all? Well, one notion of what it even means to have learned the definition of this term is that it should drive coherently all of the downstream inferences that you make with that term, and it should graft onto the conceptual knowledge that you already have. And so you can think about forming new sentences directly that refer to someone's paani or one's pibling in this situation and expecting them to draw both on your existing conceptual knowledge of what it even means to have a family tree as well as all the other conceptual terms for a friendship that you may already have. And the same framework also suggests one mechanism by which we might formalize what it means to learn world models from language. So as I mentioned, if we return to the situation that opens this talk, tug of war games, we might think about how the definition that I gave when I sat up here at the podium, right, saying there are people whose strength levels vary from person to person. People have a percentage of time in which they're lazy. Strengths of the teams depend on the underlying strengths of the members of that team, and whether one team beats another just depends on which team pulls stronger that match. And this kind of setting is actually language, right, is building up the actual generative model itself. And you might think of a system like this that both learns these kinds of theories from language and then is appending to this kind of local problem-based context to answer arbitrary questions like the kinds that we gave or conditioned on various observations like Josh being stronger than Leo with respect to this kind of local notion of what strength means in this particular problem context that we're thinking about. In interest of time, why don't we just jump on to your section. Yeah, thanks. So we've just been talking about how natural language can sort of be interpreted or semantically parsed to a probabilistic language of thought, but we haven't talked about how cognition itself, which is sort of, we've been talking about as the product of general purpose probabilistic inference machinery, might interact with language cognitively or how our tools for, you know, our algorithms for inference, our model representations might benefit from recent advances in language models. So in the rest of the talk, I'll sort of talk about this also very preliminary work that we just presented at a workshop at ICML that is more about a role for natural language and language models in this part of the picture. And one reason to think that natural language must play some role in this part of the picture is that sometimes we set ourselves reasoning tasks whose specifications, what it would mean to solve the reasoning task correctly must involve natural language. So for example, if you have an iPhone, you might have used the visual voicemail feature, which automatically, but somewhat incompletely transcribes your voicemails. And these transcripts have gaps marked by underscored sequences of varying lengths, indicating Apple couldn't quite work out what was said. And an inference task that I sometimes face is squinting at these transcripts and trying to think what could the person have said during those bits that it didn't transcribe correctly. And is it worth my time to listen to this voicemail or am I pretty certain that I got all the relevant information from the part of the transcript that I've seen? So even if I'm representing that kind of inference problem in some kind of probabilistic language of thought and not in natural language, it must reference natural language because a key part of the reasoning that I'm doing is about how long those gaps are, about what words could go in those gaps, how they could semantically and syntactically with the words around them. And there are a lot of other tasks like this where the specification of some reasoning problem must in some way involve language. Maybe we're writing something that has to obey certain structural constraints like a poem or code. Maybe we're puzzling over a message from our advisor, trying to infer all the different meanings consistent with what they said. Maybe we're trying to figure out how to put together some words that we predict could achieve some desired effect in a listener. And beyond the fact that some inference problems implicate language and their specification, it seems like at least sometimes we sort of use language for thinking, right? Rubber duck debugging is when we successfully debug something that's been something us by just talking about it to ourselves or to a rubber duck. And I think this is the intuition also behind sort of chain of thought, scrap pad, those kinds of innovations in language model land. But one reason I'm drawing this distinction between task specification and algorithm is that this has long been a really important distinction in probabilistic modeling and inference. And it's something that I think we lose when we move just to asking a language model a question and hoping that it gives us the right answer. So in the kind of work that our lab does in modeling and inference, we sort of separately create a model probabilistic program that includes a task specification as a posterior distribution we wanna sample from and separately an inference program that compositionally encodes some kind of algorithm or strategy for solving that inference task. And when you use a probabilistic programming language to do this, you get some benefits from taking this approach of separating model inference. We know that we have soundness theorems guaranteeing that as computation increases, the inference is going to approach the posterior. We have automated tools and tests for measuring how accurate our inferences are relative to the model with finite computation. And we also have gradient estimators that help us tune any parameters of our inference programs to be better inference algorithms. And beyond being useful properties for engineering, these guarantees also reflect some key aspects of human cognition. We can often think more to reach more accurate conclusions. We can critically evaluate the extent to which our current hypotheses actually make sense given our model of the world. And if we repeatedly face the same kind of inference task, we can train ourselves to get better at solving it. So something we've begun to explore is whether adding LLMs to this picture might let us both specify various linguistic tasks as formal probabilistic models and enhance our inference algorithms by letting them do some of their thinking using languages as a tool. So I'll first talk about the modeling side of things. So we all know that an autoregressive language model defines a probability distribution over sequences of tokens. But we rarely just want to sample that unconditional distribution. You know, in the same way that in order to use a SAT solver, we need to reduce the problem we care about to a SAT formula, use a language model, we need to reduction from the task instance that we care about to a prompt. And the idea is that we're saying that the conditional distribution of the language model conditioned on that prompt is somehow a good specification of the task that we want to solve, or a good approximation of the task that we want to solve. But unlike the reductions to SAT, of course, this reduction is lossy. One problem is that sort of hard constraints, sort of instructions that we give the language model might be, you know, it might fail to follow them. So this conditional distribution, p-task, is not really the specification that we have in mind, it's just some close thing that we can get. Another problem is that the entropy of this distribution may not meaningfully reflect uncertainties. So you may have seen in the GPT-4 paper that on multiple choice tasks, where there's some multiple choice question and then the language model is asked to output A, B, C, or D. Before they did any RLHF and instruction tuning, if they create a calibration plot, where they plot sort of, you know, of all the answers in which GPT was, you know, 0.4% confident, how often was that the correct answer? GPT-4 is strikingly well calibrated. And that's what you might expect from a model that's doing a very good job of next token prediction, of matching the distribution of language. When it's uncertain, the loss function is telling it, it should allocate its mass, its probability mass, according to that distribution. Whereas after RLHF, the calibration is shot. And this is also what you might expect, even if the humans who were sort of providing the human feedback in RLHF preferred the right answer, okay? The distribution that you get after performing RLHF with the objective that's commonly used for RLHF sort of creates a reduction in temperature. It's equivalent to reducing the temperature of the parts where the human feedback is exactly aligned with sort of the correct answer. And so it becomes overconfident. And, you know, this is very prompt dependent. I don't mean to say that this is like always gonna happen if you go and use GPT, but I went and used GPT-3.5 to do this like infilling task, and it did it correctly, but also every time that I generated at temperature one, it gave me basically the same answer. So if I want to think of this distribution as sort of representing uncertainties that I can make decisions about whether to listen to my voicemail because it might contain things I don't know, this P-task is not up to that task. So our idea is to instead of reducing to a prompt, reduce to a probabilistic program that may call a language model, which is sort of a more flexible way of specifying what P-task distributions we want to sample from. And I know I'm running low on time, but the idea is that these models can mix calls to the language model with conditioning statements and other logic. So in this probabilistic program for this infilling task, it is in a loop going through each sort of blank in the that we need to infill, sampling a random number of tokens that should fill that spot, sampling those tokens and then observing the next sort of fixed fragment or conditioning on the next part being a fixed fragment. And this just lets us specify a model that doesn't just have a prefix prompt that sort of has a prompt with blanks in it. I haven't said yet how we're going to sample this, but the idea is that this defines a specification for the task that we want. And similarly, we have programs that sort of specify a variety of tasks that involve sort of thinking with language, right? We can condition on hard constraints if we want to parse into a formal grammar or write a high two. We can do kind of a product of experts model using multiple prompts. So maybe I want to think about a fun fact that's about both London and Paris. Well, you could just ask the prompt, hey, please give me a fun fact about both London and Paris, but you could also create a product of experts model where it has to come up with a completion that is both a completion to the sentence, a fun fact about London is, and a completion to the sentence, a fun fact about Paris is. And that's kind of a hybrid where the idea of and and both is symbolically encoded, but we're still using the language models representation of knowledge about fun facts and these things. Similarly, we can sort of represent reward steering or a classifier guidance by conditioning, by sort of soft conditioning on a reward function. And we can also include things like, hey, please generate a gloss of this code that when I try to semantic parse it back into code, gives me the same code I started with, things like that. So those are model programs for specifying various tasks. We need inference algorithms for actually sampling from these distributions. And so far we've been focusing on sequential Monte Carlo inference algorithms. And we kind of have a default version of this method and then fancier versions of this method that are necessary for harder tasks. In many ways, sequential Monte Carlo looks like beam search. You kind of keep multiple hypotheses around, you extend them, you reweight them according to model specific way, and then you resample, which is kind of like the part of beam search where you sort of down sample from your big expanded beam back to your beam size. But unlike beam search, sequential Monte Carlo, as you scale up the number of hypotheses that you're using, instead of converging to an arg max of your objective function, converges to the posterior distribution, sampling from the posterior distribution. And this sort of default version of SMC has worked for a few simple tasks that we've tried it on. For example, if I want a completion that follows my favorite physicists is probably, and my favorite writer is probably equally well, SMC can give me Richard Feynman. I really admire how he communicates complex ideas so clearly. Or if I want to finish the fed says, but only using words less than five letters, I get the fed says it will taper, but the rate hikes are still years away to something like that. And it's worth noting that if you do something like token masking to enforce this constraint, you just forbid the language model from generating anything that's longer than five letters. You get all sorts of weird completions. It's different from the posterior here. You get completions that set up an idiom that it could only complete if it used a word longer than five letters or something like that. And then it just gets very confused and right stop, that, that, read more or something. It tries to come up with some context in which the sentence would be cut off early. And for infilling tasks, we get a variety of samples that sort of fit semantically and syntactically with the text, but infilling tasks can be made much harder than this one. So I don't want to claim that this method yet solves all these infilling tasks. So for harder tasks, we think we're going to need to use fancier sequential Monte Carlo algorithms. And both of these sort of steps can actually be extended in various ways. We can use better proposal distributions and sort of better reweighting strategies that are trying to guess, okay, are we on the path to getting a good sample here? And we think that techniques that have already been developed in the literature for proposing good things in line with constraints or sort of discriminating whether we're likely to land in a constraint could be good ingredients to put here. But the important thing is, and this is the very end, the important thing is all of those things become part of the inference program. And there are still guarantees that as we scale up the number of particles, we're still targeting the original specification of the model. So all of those heuristics or biases don't sort of, we don't just trust them blindly. We don't hand the keys to those techniques. We still have a specification that we can understand. Okay, I'll stop there. Thanks. All right, we're a little bit behind time. So unless there is a burning question, burning question, no burning questions. Let's break for tea and maybe Zachertorte and you guys can talk to the speakers. And at 11.30, we are going to...", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.92, "text": " I mentioned Gabe Grand, another student collaborator", "tokens": [50364, 286, 2835, 39524, 6757, 11, 1071, 3107, 5091, 1639, 50460], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 1, "seek": 0, "start": 1.92, "end": 4.5600000000000005, "text": " who is one of Jacob and Dreyas's students,", "tokens": [50460, 567, 307, 472, 295, 14117, 293, 413, 7950, 296, 311, 1731, 11, 50592], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 2, "seek": 0, "start": 4.5600000000000005, "end": 9.120000000000001, "text": " and Tanja Shren, a student also co-advised", "tokens": [50592, 293, 17046, 2938, 1160, 1095, 11, 257, 3107, 611, 598, 12, 345, 24420, 50820], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 3, "seek": 0, "start": 9.120000000000001, "end": 13.040000000000001, "text": " by Josh and Vikash, as well as Noah Goodman,", "tokens": [50820, 538, 9785, 293, 29465, 1299, 11, 382, 731, 382, 20895, 2205, 1601, 11, 51016], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 4, "seek": 0, "start": 13.040000000000001, "end": 15.280000000000001, "text": " my advisor Vikash and Jacob and Dreyas.", "tokens": [51016, 452, 19161, 29465, 1299, 293, 14117, 293, 413, 7950, 296, 13, 51128], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 5, "seek": 0, "start": 17.64, "end": 19.48, "text": " So our broader goal today in this talk", "tokens": [51246, 407, 527, 13227, 3387, 965, 294, 341, 751, 51338], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 6, "seek": 0, "start": 19.48, "end": 21.48, "text": " is actually going to be to reflect", "tokens": [51338, 307, 767, 516, 281, 312, 281, 5031, 51438], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 7, "seek": 0, "start": 21.48, "end": 23.080000000000002, "text": " based on many of the recent advances", "tokens": [51438, 2361, 322, 867, 295, 264, 5162, 25297, 51518], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 8, "seek": 0, "start": 23.080000000000002, "end": 25.36, "text": " that we all know of modeling natural language,", "tokens": [51518, 300, 321, 439, 458, 295, 15983, 3303, 2856, 11, 51632], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 9, "seek": 0, "start": 25.36, "end": 26.92, "text": " but I think also drawing on evidence", "tokens": [51632, 457, 286, 519, 611, 6316, 322, 4467, 51710], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 10, "seek": 0, "start": 26.92, "end": 29.72, "text": " from toolkits, other toolkits in AI.", "tokens": [51710, 490, 2290, 74, 1208, 11, 661, 2290, 74, 1208, 294, 7318, 13, 51850], "temperature": 0.0, "avg_logprob": -0.21780651265924628, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0124254385009408}, {"id": 11, "seek": 2972, "start": 30.72, "end": 33.04, "text": " Evidence from cognitive science and from neuroscience", "tokens": [50414, 5689, 2778, 490, 15605, 3497, 293, 490, 42762, 50530], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 12, "seek": 2972, "start": 33.04, "end": 35.68, "text": " on kind of the broader spectrum of different ways", "tokens": [50530, 322, 733, 295, 264, 13227, 11143, 295, 819, 2098, 50662], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 13, "seek": 2972, "start": 35.68, "end": 38.36, "text": " that we might think about building intelligent architectures", "tokens": [50662, 300, 321, 1062, 519, 466, 2390, 13232, 6331, 1303, 50796], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 14, "seek": 2972, "start": 38.36, "end": 41.32, "text": " that use and produce and learn from language,", "tokens": [50796, 300, 764, 293, 5258, 293, 1466, 490, 2856, 11, 50944], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 15, "seek": 2972, "start": 41.32, "end": 43.96, "text": " as well as more generally, I think what role language", "tokens": [50944, 382, 731, 382, 544, 5101, 11, 286, 519, 437, 3090, 2856, 51076], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 16, "seek": 2972, "start": 43.96, "end": 46.879999999999995, "text": " might play or could play in a computational system", "tokens": [51076, 1062, 862, 420, 727, 862, 294, 257, 28270, 1185, 51222], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 17, "seek": 2972, "start": 46.879999999999995, "end": 48.32, "text": " that we say thinks.", "tokens": [51222, 300, 321, 584, 7309, 13, 51294], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 18, "seek": 2972, "start": 48.32, "end": 50.36, "text": " And obviously this is a question that many people", "tokens": [51294, 400, 2745, 341, 307, 257, 1168, 300, 867, 561, 51396], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 19, "seek": 2972, "start": 50.36, "end": 52.16, "text": " in this room, but many other people who probably", "tokens": [51396, 294, 341, 1808, 11, 457, 867, 661, 561, 567, 1391, 51486], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 20, "seek": 2972, "start": 52.16, "end": 53.68, "text": " aren't in this room have thought about", "tokens": [51486, 3212, 380, 294, 341, 1808, 362, 1194, 466, 51562], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 21, "seek": 2972, "start": 53.68, "end": 57.28, "text": " from philosophers of language to linguists and neuroscientists.", "tokens": [51562, 490, 36839, 295, 2856, 281, 21766, 1751, 293, 28813, 5412, 1751, 13, 51742], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 22, "seek": 2972, "start": 57.28, "end": 59.64, "text": " And we thought it actually might be a useful exercise", "tokens": [51742, 400, 321, 1194, 309, 767, 1062, 312, 257, 4420, 5380, 51860], "temperature": 0.0, "avg_logprob": -0.12071288779929833, "compression_ratio": 1.841121495327103, "no_speech_prob": 0.0002232950209872797}, {"id": 23, "seek": 5964, "start": 59.64, "end": 62.76, "text": " to kind of start by reflecting on the underlying answers", "tokens": [50364, 281, 733, 295, 722, 538, 23543, 322, 264, 14217, 6338, 50520], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 24, "seek": 5964, "start": 62.76, "end": 65.32, "text": " to this question that are or aren't suggested", "tokens": [50520, 281, 341, 1168, 300, 366, 420, 3212, 380, 10945, 50648], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 25, "seek": 5964, "start": 65.32, "end": 66.56, "text": " by some of the most prominent directions", "tokens": [50648, 538, 512, 295, 264, 881, 17034, 11095, 50710], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 26, "seek": 5964, "start": 66.56, "end": 69.52, "text": " that we're taking in AI research right now.", "tokens": [50710, 300, 321, 434, 1940, 294, 7318, 2132, 558, 586, 13, 50858], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 27, "seek": 5964, "start": 69.52, "end": 70.8, "text": " And of course, one of the reasons", "tokens": [50858, 400, 295, 1164, 11, 472, 295, 264, 4112, 50922], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 28, "seek": 5964, "start": 70.8, "end": 72.92, "text": " why we're even asking this question at this scale,", "tokens": [50922, 983, 321, 434, 754, 3365, 341, 1168, 412, 341, 4373, 11, 51028], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 29, "seek": 5964, "start": 72.92, "end": 75.0, "text": " what is the role of language in intelligence", "tokens": [51028, 437, 307, 264, 3090, 295, 2856, 294, 7599, 51132], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 30, "seek": 5964, "start": 75.0, "end": 77.96000000000001, "text": " is in large part driven by this remarkable observation", "tokens": [51132, 307, 294, 2416, 644, 9555, 538, 341, 12802, 14816, 51280], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 31, "seek": 5964, "start": 77.96000000000001, "end": 80.48, "text": " that we all know about from just a few years ago,", "tokens": [51280, 300, 321, 439, 458, 466, 490, 445, 257, 1326, 924, 2057, 11, 51406], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 32, "seek": 5964, "start": 80.48, "end": 83.08, "text": " which is if you train these large", "tokens": [51406, 597, 307, 498, 291, 3847, 613, 2416, 51536], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 33, "seek": 5964, "start": 83.08, "end": 86.24000000000001, "text": " and specifically transformer based neural architectures", "tokens": [51536, 293, 4682, 31782, 2361, 18161, 6331, 1303, 51694], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 34, "seek": 5964, "start": 86.24000000000001, "end": 87.96000000000001, "text": " as language models, just to do this", "tokens": [51694, 382, 2856, 5245, 11, 445, 281, 360, 341, 51780], "temperature": 0.0, "avg_logprob": -0.0873971346652869, "compression_ratio": 1.7620578778135048, "no_speech_prob": 4.398731834953651e-05}, {"id": 35, "seek": 8796, "start": 87.96, "end": 92.19999999999999, "text": " next word prediction task, with enough language data,", "tokens": [50364, 958, 1349, 17630, 5633, 11, 365, 1547, 2856, 1412, 11, 50576], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 36, "seek": 8796, "start": 92.19999999999999, "end": 94.47999999999999, "text": " they start to show behaviors that really suggest", "tokens": [50576, 436, 722, 281, 855, 15501, 300, 534, 3402, 50690], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 37, "seek": 8796, "start": 94.47999999999999, "end": 96.28, "text": " that there's something more than language at play.", "tokens": [50690, 300, 456, 311, 746, 544, 813, 2856, 412, 862, 13, 50780], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 38, "seek": 8796, "start": 96.28, "end": 97.88, "text": " They look like they're thinking.", "tokens": [50780, 814, 574, 411, 436, 434, 1953, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 39, "seek": 8796, "start": 98.8, "end": 101.88, "text": " They can induce patterns from just a few examples in data,", "tokens": [50906, 814, 393, 41263, 8294, 490, 445, 257, 1326, 5110, 294, 1412, 11, 51060], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 40, "seek": 8796, "start": 101.88, "end": 103.55999999999999, "text": " or they can even read the definitions", "tokens": [51060, 420, 436, 393, 754, 1401, 264, 21988, 51144], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 41, "seek": 8796, "start": 103.55999999999999, "end": 106.47999999999999, "text": " of totally novel words like Zaka Tota in context", "tokens": [51144, 295, 3879, 7613, 2283, 411, 1176, 7849, 314, 5377, 294, 4319, 51290], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 42, "seek": 8796, "start": 106.47999999999999, "end": 108.72, "text": " and produce realistic sentences that appear", "tokens": [51290, 293, 5258, 12465, 16579, 300, 4204, 51402], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 43, "seek": 8796, "start": 108.72, "end": 112.08, "text": " as if they understand how to use those words immediately.", "tokens": [51402, 382, 498, 436, 1223, 577, 281, 764, 729, 2283, 4258, 13, 51570], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 44, "seek": 8796, "start": 112.08, "end": 114.36, "text": " And so a lot of the excitement, I think it's fair to say", "tokens": [51570, 400, 370, 257, 688, 295, 264, 14755, 11, 286, 519, 309, 311, 3143, 281, 584, 51684], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 45, "seek": 8796, "start": 114.36, "end": 117.08, "text": " around language models is that maybe for the first time,", "tokens": [51684, 926, 2856, 5245, 307, 300, 1310, 337, 264, 700, 565, 11, 51820], "temperature": 0.0, "avg_logprob": -0.15065122373176343, "compression_ratio": 1.7178683385579938, "no_speech_prob": 0.0006875945255160332}, {"id": 46, "seek": 11708, "start": 117.08, "end": 119.88, "text": " we're seeing something that is offering a scalable route", "tokens": [50364, 321, 434, 2577, 746, 300, 307, 8745, 257, 38481, 7955, 50504], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 47, "seek": 11708, "start": 119.88, "end": 123.84, "text": " towards implementing more generally intelligent architectures", "tokens": [50504, 3030, 18114, 544, 5101, 13232, 6331, 1303, 50702], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 48, "seek": 11708, "start": 123.84, "end": 127.2, "text": " just by directly scaling, or largely by directly scaling", "tokens": [50702, 445, 538, 3838, 21589, 11, 420, 11611, 538, 3838, 21589, 50870], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 49, "seek": 11708, "start": 127.2, "end": 128.16, "text": " the amount of language data", "tokens": [50870, 264, 2372, 295, 2856, 1412, 50918], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 50, "seek": 11708, "start": 128.16, "end": 129.96, "text": " that they're being trained to predict.", "tokens": [50918, 300, 436, 434, 885, 8895, 281, 6069, 13, 51008], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 51, "seek": 11708, "start": 131.64, "end": 133.28, "text": " All right, so what is the underlying idea here?", "tokens": [51092, 1057, 558, 11, 370, 437, 307, 264, 14217, 1558, 510, 30, 51174], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 52, "seek": 11708, "start": 133.28, "end": 135.2, "text": " What does this have to say about language?", "tokens": [51174, 708, 775, 341, 362, 281, 584, 466, 2856, 30, 51270], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 53, "seek": 11708, "start": 135.2, "end": 136.24, "text": " Well, I think it's fair to say", "tokens": [51270, 1042, 11, 286, 519, 309, 311, 3143, 281, 584, 51322], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 54, "seek": 11708, "start": 136.24, "end": 138.88, "text": " that one of the dominant hypotheses that's underlying", "tokens": [51322, 300, 472, 295, 264, 15657, 49969, 300, 311, 14217, 51454], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 55, "seek": 11708, "start": 138.88, "end": 141.96, "text": " why we were even starting to see some of this behavior", "tokens": [51454, 983, 321, 645, 754, 2891, 281, 536, 512, 295, 341, 5223, 51608], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 56, "seek": 11708, "start": 141.96, "end": 143.84, "text": " rests on kind of a two-part idea.", "tokens": [51608, 39755, 322, 733, 295, 257, 732, 12, 6971, 1558, 13, 51702], "temperature": 0.0, "avg_logprob": -0.12225282870657075, "compression_ratio": 1.707070707070707, "no_speech_prob": 3.1196523195831105e-05}, {"id": 57, "seek": 14384, "start": 143.84, "end": 147.12, "text": " One is something about the nature of language itself, right?", "tokens": [50364, 1485, 307, 746, 466, 264, 3687, 295, 2856, 2564, 11, 558, 30, 50528], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 58, "seek": 14384, "start": 147.12, "end": 151.28, "text": " It's the suggestion that language is sufficiently diverse", "tokens": [50528, 467, 311, 264, 16541, 300, 2856, 307, 31868, 9521, 50736], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 59, "seek": 14384, "start": 151.28, "end": 154.36, "text": " and so broad, maybe because we suspect that humans", "tokens": [50736, 293, 370, 4152, 11, 1310, 570, 321, 9091, 300, 6255, 50890], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 60, "seek": 14384, "start": 154.36, "end": 155.76, "text": " express so much of their thoughts", "tokens": [50890, 5109, 370, 709, 295, 641, 4598, 50960], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 61, "seek": 14384, "start": 155.76, "end": 158.36, "text": " in such a diverse range of their thoughts and language", "tokens": [50960, 294, 1270, 257, 9521, 3613, 295, 641, 4598, 293, 2856, 51090], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 62, "seek": 14384, "start": 158.36, "end": 161.52, "text": " that being able to perfectly solve this task,", "tokens": [51090, 300, 885, 1075, 281, 6239, 5039, 341, 5633, 11, 51248], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 63, "seek": 14384, "start": 161.52, "end": 164.68, "text": " to be a perfect language model, or at least a really good one,", "tokens": [51248, 281, 312, 257, 2176, 2856, 2316, 11, 420, 412, 1935, 257, 534, 665, 472, 11, 51406], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 64, "seek": 14384, "start": 164.68, "end": 168.44, "text": " is essentially an AGI complete task,", "tokens": [51406, 307, 4476, 364, 316, 26252, 3566, 5633, 11, 51594], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 65, "seek": 14384, "start": 168.44, "end": 170.4, "text": " and maybe also one that conveniently,", "tokens": [51594, 293, 1310, 611, 472, 300, 44375, 11, 51692], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 66, "seek": 14384, "start": 170.4, "end": 171.56, "text": " unlike other kinds of tasks,", "tokens": [51692, 8343, 661, 3685, 295, 9608, 11, 51750], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 67, "seek": 14384, "start": 171.56, "end": 173.32, "text": " like predicting all of the videos in the world,", "tokens": [51750, 411, 32884, 439, 295, 264, 2145, 294, 264, 1002, 11, 51838], "temperature": 0.0, "avg_logprob": -0.1034942278786311, "compression_ratio": 1.7773972602739727, "no_speech_prob": 0.00017947556625586003}, {"id": 68, "seek": 17332, "start": 174.0, "end": 175.79999999999998, "text": " we have maybe efficient architectures to do", "tokens": [50398, 321, 362, 1310, 7148, 6331, 1303, 281, 360, 50488], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 69, "seek": 17332, "start": 175.79999999999998, "end": 177.2, "text": " and enough data to do.", "tokens": [50488, 293, 1547, 1412, 281, 360, 13, 50558], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 70, "seek": 17332, "start": 177.2, "end": 180.35999999999999, "text": " And I think it's worth noting that this doesn't actually", "tokens": [50558, 400, 286, 519, 309, 311, 3163, 26801, 300, 341, 1177, 380, 767, 50716], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 71, "seek": 17332, "start": 180.35999999999999, "end": 183.2, "text": " really have to be a particularly strong hypothesis", "tokens": [50716, 534, 362, 281, 312, 257, 4098, 2068, 17291, 50858], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 72, "seek": 17332, "start": 183.2, "end": 186.88, "text": " about what it is computationally that thinking looks like,", "tokens": [50858, 466, 437, 309, 307, 24903, 379, 300, 1953, 1542, 411, 11, 51042], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 73, "seek": 17332, "start": 186.88, "end": 190.51999999999998, "text": " or even how language necessarily is implicated internally", "tokens": [51042, 420, 754, 577, 2856, 4725, 307, 8484, 3587, 19501, 51224], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 74, "seek": 17332, "start": 190.51999999999998, "end": 193.92, "text": " inside the computational processes that we call thinking.", "tokens": [51224, 1854, 264, 28270, 7555, 300, 321, 818, 1953, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 75, "seek": 17332, "start": 193.92, "end": 195.35999999999999, "text": " Rather, it's really just a hypothesis", "tokens": [51394, 16571, 11, 309, 311, 534, 445, 257, 17291, 51466], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 76, "seek": 17332, "start": 195.35999999999999, "end": 198.35999999999999, "text": " about the nature of the language modeling task itself.", "tokens": [51466, 466, 264, 3687, 295, 264, 2856, 15983, 5633, 2564, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 77, "seek": 17332, "start": 200.28, "end": 202.2, "text": " And I think what a lot of people", "tokens": [51712, 400, 286, 519, 437, 257, 688, 295, 561, 51808], "temperature": 0.0, "avg_logprob": -0.2887191428794517, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00017395394388586283}, {"id": 78, "seek": 20220, "start": 202.39999999999998, "end": 205.64, "text": " in this room would agree now is that,", "tokens": [50374, 294, 341, 1808, 576, 3986, 586, 307, 300, 11, 50536], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 79, "seek": 20220, "start": 205.64, "end": 208.23999999999998, "text": " well, that might be true about the language modeling task", "tokens": [50536, 731, 11, 300, 1062, 312, 2074, 466, 264, 2856, 15983, 5633, 50666], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 80, "seek": 20220, "start": 208.23999999999998, "end": 211.28, "text": " in principle, much of the most exciting research", "tokens": [50666, 294, 8665, 11, 709, 295, 264, 881, 4670, 2132, 50818], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 81, "seek": 20220, "start": 211.28, "end": 212.64, "text": " that we're seeing using LMS right now", "tokens": [50818, 300, 321, 434, 2577, 1228, 441, 10288, 558, 586, 50886], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 82, "seek": 20220, "start": 212.64, "end": 215.92, "text": " actually isn't predicated on really the hope or the idea", "tokens": [50886, 767, 1943, 380, 3852, 3587, 322, 534, 264, 1454, 420, 264, 1558, 51050], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 83, "seek": 20220, "start": 215.92, "end": 217.95999999999998, "text": " that just by scaling to more and more data,", "tokens": [51050, 300, 445, 538, 21589, 281, 544, 293, 544, 1412, 11, 51152], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 84, "seek": 20220, "start": 217.95999999999998, "end": 220.76, "text": " we're gonna expect transformers trained and used in this way", "tokens": [51152, 321, 434, 799, 2066, 4088, 433, 8895, 293, 1143, 294, 341, 636, 51292], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 85, "seek": 20220, "start": 220.76, "end": 222.2, "text": " to actually solve that task", "tokens": [51292, 281, 767, 5039, 300, 5633, 51364], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 86, "seek": 20220, "start": 222.2, "end": 224.56, "text": " or become perfect next-world prediction models.", "tokens": [51364, 420, 1813, 2176, 958, 12, 13217, 17630, 5245, 13, 51482], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 87, "seek": 20220, "start": 224.56, "end": 226.51999999999998, "text": " And the intuition behind that,", "tokens": [51482, 400, 264, 24002, 2261, 300, 11, 51580], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 88, "seek": 20220, "start": 226.51999999999998, "end": 228.39999999999998, "text": " which I think many people have pointed out,", "tokens": [51580, 597, 286, 519, 867, 561, 362, 10932, 484, 11, 51674], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 89, "seek": 20220, "start": 228.39999999999998, "end": 231.6, "text": " comes both from the way in which we're looking", "tokens": [51674, 1487, 1293, 490, 264, 636, 294, 597, 321, 434, 1237, 51834], "temperature": 0.0, "avg_logprob": -0.336587502138458, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.0004580079694278538}, {"id": 90, "seek": 23160, "start": 231.96, "end": 233.79999999999998, "text": " in which transformers work, right?", "tokens": [50382, 294, 597, 4088, 433, 589, 11, 558, 30, 50474], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 91, "seek": 23160, "start": 233.79999999999998, "end": 235.32, "text": " There are autoregressive language models", "tokens": [50474, 821, 366, 1476, 418, 3091, 488, 2856, 5245, 50550], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 92, "seek": 23160, "start": 235.32, "end": 237.51999999999998, "text": " that are doing a fixed, finite amount", "tokens": [50550, 300, 366, 884, 257, 6806, 11, 19362, 2372, 50660], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 93, "seek": 23160, "start": 237.51999999999998, "end": 239.72, "text": " of internal computation that only scales", "tokens": [50660, 295, 6920, 24903, 300, 787, 17408, 50770], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 94, "seek": 23160, "start": 239.72, "end": 241.51999999999998, "text": " based on the previous linguistic context.", "tokens": [50770, 2361, 322, 264, 3894, 43002, 4319, 13, 50860], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 95, "seek": 23160, "start": 241.51999999999998, "end": 243.35999999999999, "text": " And it doesn't really make sense that,", "tokens": [50860, 400, 309, 1177, 380, 534, 652, 2020, 300, 11, 50952], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 96, "seek": 23160, "start": 243.35999999999999, "end": 245.95999999999998, "text": " of course, you can pose questions in language", "tokens": [50952, 295, 1164, 11, 291, 393, 10774, 1651, 294, 2856, 51082], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 97, "seek": 23160, "start": 245.95999999999998, "end": 248.24, "text": " like arbitrarily difficult math problems", "tokens": [51082, 411, 19071, 3289, 2252, 5221, 2740, 51196], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 98, "seek": 23160, "start": 248.24, "end": 251.44, "text": " or planning problems that intuitively require", "tokens": [51196, 420, 5038, 2740, 300, 46506, 3651, 51356], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 99, "seek": 23160, "start": 251.44, "end": 252.35999999999999, "text": " an amount of computation,", "tokens": [51356, 364, 2372, 295, 24903, 11, 51402], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 100, "seek": 23160, "start": 252.35999999999999, "end": 253.84, "text": " whose complexity doesn't actually depend", "tokens": [51402, 6104, 14024, 1177, 380, 767, 5672, 51476], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 101, "seek": 23160, "start": 253.84, "end": 256.71999999999997, "text": " linearly on the previous token context.", "tokens": [51476, 43586, 322, 264, 3894, 14862, 4319, 13, 51620], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 102, "seek": 23160, "start": 256.71999999999997, "end": 258.68, "text": " And I think that's been matched empirically", "tokens": [51620, 400, 286, 519, 300, 311, 668, 21447, 25790, 984, 51718], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 103, "seek": 23160, "start": 258.68, "end": 260.8, "text": " by lots of different observations", "tokens": [51718, 538, 3195, 295, 819, 18163, 51824], "temperature": 0.0, "avg_logprob": -0.12048693960027178, "compression_ratio": 1.7335423197492164, "no_speech_prob": 0.0005191063974052668}, {"id": 104, "seek": 26080, "start": 260.8, "end": 263.48, "text": " about which kinds of sentences are hard to complete", "tokens": [50364, 466, 597, 3685, 295, 16579, 366, 1152, 281, 3566, 50498], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 105, "seek": 26080, "start": 263.48, "end": 265.16, "text": " if you treat them as next-world prediction tasks", "tokens": [50498, 498, 291, 2387, 552, 382, 958, 12, 13217, 17630, 9608, 50582], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 106, "seek": 26080, "start": 265.16, "end": 266.16, "text": " in this way, right?", "tokens": [50582, 294, 341, 636, 11, 558, 30, 50632], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 107, "seek": 26080, "start": 266.16, "end": 267.36, "text": " Hard arbitrary math problems", "tokens": [50632, 11817, 23211, 5221, 2740, 50692], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 108, "seek": 26080, "start": 267.36, "end": 268.96000000000004, "text": " or planning problems like these.", "tokens": [50692, 420, 5038, 2740, 411, 613, 13, 50772], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 109, "seek": 26080, "start": 269.96000000000004, "end": 273.64, "text": " Right, so some of, you know,", "tokens": [50822, 1779, 11, 370, 512, 295, 11, 291, 458, 11, 51006], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 110, "seek": 26080, "start": 273.64, "end": 276.40000000000003, "text": " if you're thinking about the role of language,", "tokens": [51006, 498, 291, 434, 1953, 466, 264, 3090, 295, 2856, 11, 51144], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 111, "seek": 26080, "start": 276.40000000000003, "end": 279.72, "text": " why is language, why are language models so big right now", "tokens": [51144, 983, 307, 2856, 11, 983, 366, 2856, 5245, 370, 955, 558, 586, 51310], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 112, "seek": 26080, "start": 279.72, "end": 282.64, "text": " as, oh, well, language just has all the evidence necessary", "tokens": [51310, 382, 11, 1954, 11, 731, 11, 2856, 445, 575, 439, 264, 4467, 4818, 51456], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 113, "seek": 26080, "start": 282.64, "end": 286.28000000000003, "text": " to train something to think.", "tokens": [51456, 281, 3847, 746, 281, 519, 13, 51638], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 114, "seek": 26080, "start": 288.08000000000004, "end": 290.0, "text": " Like Leo said, that's not actually committing", "tokens": [51728, 1743, 19344, 848, 11, 300, 311, 406, 767, 26659, 51824], "temperature": 0.0, "avg_logprob": -0.16436067678160587, "compression_ratio": 1.6423357664233578, "no_speech_prob": 6.204152305144817e-05}, {"id": 115, "seek": 29000, "start": 290.0, "end": 292.32, "text": " to any hypothesis about sort of how language", "tokens": [50364, 281, 604, 17291, 466, 1333, 295, 577, 2856, 50480], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 116, "seek": 29000, "start": 292.32, "end": 293.52, "text": " is used internally in thinking.", "tokens": [50480, 307, 1143, 19501, 294, 1953, 13, 50540], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 117, "seek": 29000, "start": 293.52, "end": 296.08, "text": " The transformers' computations are not necessarily", "tokens": [50540, 440, 4088, 433, 6, 2807, 763, 366, 406, 4725, 50668], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 118, "seek": 29000, "start": 296.08, "end": 299.04, "text": " doing anything linguistic as it's computing", "tokens": [50668, 884, 1340, 43002, 382, 309, 311, 15866, 50816], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 119, "seek": 29000, "start": 299.04, "end": 300.68, "text": " the distribution of the next word.", "tokens": [50816, 264, 7316, 295, 264, 958, 1349, 13, 50898], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 120, "seek": 29000, "start": 300.68, "end": 304.44, "text": " But more recently, we've seen people use language models", "tokens": [50898, 583, 544, 3938, 11, 321, 600, 1612, 561, 764, 2856, 5245, 51086], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 121, "seek": 29000, "start": 304.44, "end": 307.64, "text": " to solve these harder tasks by letting them think more.", "tokens": [51086, 281, 5039, 613, 6081, 9608, 538, 8295, 552, 519, 544, 13, 51246], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 122, "seek": 29000, "start": 307.64, "end": 309.4, "text": " And what letting a transformer think more means", "tokens": [51246, 400, 437, 8295, 257, 31782, 519, 544, 1355, 51334], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 123, "seek": 29000, "start": 309.4, "end": 311.8, "text": " is letting it generate more tokens.", "tokens": [51334, 307, 8295, 309, 8460, 544, 22667, 13, 51454], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 124, "seek": 29000, "start": 311.8, "end": 315.88, "text": " So in this view, thinking doesn't necessarily emerge", "tokens": [51454, 407, 294, 341, 1910, 11, 1953, 1177, 380, 4725, 21511, 51658], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 125, "seek": 29000, "start": 315.88, "end": 317.88, "text": " just as predicting the next token.", "tokens": [51658, 445, 382, 32884, 264, 958, 14862, 13, 51758], "temperature": 0.0, "avg_logprob": -0.11086225924284562, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.00036820516106672585}, {"id": 126, "seek": 31788, "start": 318.88, "end": 322.92, "text": " Rather, thinking happens in language, right?", "tokens": [50414, 16571, 11, 1953, 2314, 294, 2856, 11, 558, 30, 50616], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 127, "seek": 31788, "start": 322.92, "end": 327.84, "text": " It happens by sort of producing a chain of thoughts", "tokens": [50616, 467, 2314, 538, 1333, 295, 10501, 257, 5021, 295, 4598, 50862], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 128, "seek": 31788, "start": 327.84, "end": 330.96, "text": " or using a scratch pad or deciding", "tokens": [50862, 420, 1228, 257, 8459, 6887, 420, 17990, 51018], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 129, "seek": 31788, "start": 330.96, "end": 332.64, "text": " that you're going to invoke a calculator", "tokens": [51018, 300, 291, 434, 516, 281, 41117, 257, 24993, 51102], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 130, "seek": 31788, "start": 332.64, "end": 334.64, "text": " or write some code and execute the code.", "tokens": [51102, 420, 2464, 512, 3089, 293, 14483, 264, 3089, 13, 51202], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 131, "seek": 31788, "start": 335.64, "end": 340.28, "text": " And the sort of hypothesis embodied,", "tokens": [51252, 400, 264, 1333, 295, 17291, 42046, 11, 51484], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 132, "seek": 31788, "start": 340.28, "end": 342.0, "text": " I think by this line of work,", "tokens": [51484, 286, 519, 538, 341, 1622, 295, 589, 11, 51570], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 133, "seek": 31788, "start": 342.0, "end": 343.15999999999997, "text": " is that the role of language", "tokens": [51570, 307, 300, 264, 3090, 295, 2856, 51628], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 134, "seek": 31788, "start": 343.15999999999997, "end": 345.88, "text": " and intelligence architecture is bigger.", "tokens": [51628, 293, 7599, 9482, 307, 3801, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16586901346842448, "compression_ratio": 1.583710407239819, "no_speech_prob": 9.313233749708161e-05}, {"id": 135, "seek": 34588, "start": 345.88, "end": 348.76, "text": " That language or some kind of like running monologue", "tokens": [50364, 663, 2856, 420, 512, 733, 295, 411, 2614, 1108, 42298, 50508], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 136, "seek": 34588, "start": 348.76, "end": 351.6, "text": " of language is the central controller of thought.", "tokens": [50508, 295, 2856, 307, 264, 5777, 10561, 295, 1194, 13, 50650], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 137, "seek": 34588, "start": 351.6, "end": 354.76, "text": " And thinking is taking place primarily in language.", "tokens": [50650, 400, 1953, 307, 1940, 1081, 10029, 294, 2856, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 138, "seek": 34588, "start": 357.76, "end": 361.84, "text": " And what's maybe striking about these proposals", "tokens": [50958, 400, 437, 311, 1310, 18559, 466, 613, 20198, 51162], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 139, "seek": 34588, "start": 361.84, "end": 363.84, "text": " is that if you ask most cognitive scientists", "tokens": [51162, 307, 300, 498, 291, 1029, 881, 15605, 7708, 51262], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 140, "seek": 34588, "start": 363.84, "end": 365.68, "text": " or neuroscientists, they'd probably say,", "tokens": [51262, 420, 28813, 5412, 1751, 11, 436, 1116, 1391, 584, 11, 51354], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 141, "seek": 34588, "start": 365.68, "end": 367.52, "text": " this isn't the role that language plays", "tokens": [51354, 341, 1943, 380, 264, 3090, 300, 2856, 5749, 51446], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 142, "seek": 34588, "start": 367.52, "end": 369.32, "text": " in relation to general intelligence in humans,", "tokens": [51446, 294, 9721, 281, 2674, 7599, 294, 6255, 11, 51536], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 143, "seek": 34588, "start": 369.32, "end": 371.68, "text": " or at least it's not the dominant hypothesis.", "tokens": [51536, 420, 412, 1935, 309, 311, 406, 264, 15657, 17291, 13, 51654], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 144, "seek": 34588, "start": 372.76, "end": 373.84, "text": " Until just a few years ago,", "tokens": [51708, 9088, 445, 257, 1326, 924, 2057, 11, 51762], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 145, "seek": 34588, "start": 373.84, "end": 374.92, "text": " this probably wouldn't have been the role", "tokens": [51762, 341, 1391, 2759, 380, 362, 668, 264, 3090, 51816], "temperature": 0.0, "avg_logprob": -0.1296935682537175, "compression_ratio": 1.7535714285714286, "no_speech_prob": 9.60885954555124e-05}, {"id": 146, "seek": 37492, "start": 374.92, "end": 376.0, "text": " that many AI researchers", "tokens": [50364, 300, 867, 7318, 10309, 50418], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 147, "seek": 37492, "start": 376.0, "end": 377.88, "text": " would have necessarily posited for language", "tokens": [50418, 576, 362, 4725, 1366, 1226, 337, 2856, 50512], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 148, "seek": 37492, "start": 377.88, "end": 382.16, "text": " as the main controller or the main substrate", "tokens": [50512, 382, 264, 2135, 10561, 420, 264, 2135, 27585, 50726], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 149, "seek": 37492, "start": 382.16, "end": 385.24, "text": " of a running monologue that controls all thinking.", "tokens": [50726, 295, 257, 2614, 1108, 42298, 300, 9003, 439, 1953, 13, 50880], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 150, "seek": 37492, "start": 385.24, "end": 387.32, "text": " And so we thought we'd review some of the background", "tokens": [50880, 400, 370, 321, 1194, 321, 1116, 3131, 512, 295, 264, 3678, 50984], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 151, "seek": 37492, "start": 387.32, "end": 389.52000000000004, "text": " for what language seems to look like in humans", "tokens": [50984, 337, 437, 2856, 2544, 281, 574, 411, 294, 6255, 51094], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 152, "seek": 37492, "start": 389.52000000000004, "end": 392.0, "text": " as a basis for informing how we might build architectures", "tokens": [51094, 382, 257, 5143, 337, 43969, 577, 321, 1062, 1322, 6331, 1303, 51218], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 153, "seek": 37492, "start": 392.0, "end": 394.36, "text": " that better capture those more human-like roles", "tokens": [51218, 300, 1101, 7983, 729, 544, 1952, 12, 4092, 9604, 51336], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 154, "seek": 37492, "start": 394.36, "end": 395.20000000000005, "text": " for language.", "tokens": [51336, 337, 2856, 13, 51378], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 155, "seek": 37492, "start": 396.84000000000003, "end": 398.08000000000004, "text": " Yeah, and so I just want to be clear", "tokens": [51460, 865, 11, 293, 370, 286, 445, 528, 281, 312, 1850, 51522], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 156, "seek": 37492, "start": 398.08000000000004, "end": 399.76, "text": " that we're going to do kind of a quick background", "tokens": [51522, 300, 321, 434, 516, 281, 360, 733, 295, 257, 1702, 3678, 51606], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 157, "seek": 37492, "start": 399.76, "end": 402.0, "text": " on some of the neuroscientific and cognitive evidence", "tokens": [51606, 322, 512, 295, 264, 28813, 5412, 1089, 293, 15605, 4467, 51718], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 158, "seek": 37492, "start": 402.0, "end": 404.12, "text": " about what language might look like in people.", "tokens": [51718, 466, 437, 2856, 1062, 574, 411, 294, 561, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09769679478236608, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.00016340860747732222}, {"id": 159, "seek": 40412, "start": 404.12, "end": 407.08, "text": " And our goal really isn't to push back in any way", "tokens": [50364, 400, 527, 3387, 534, 1943, 380, 281, 2944, 646, 294, 604, 636, 50512], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 160, "seek": 40412, "start": 407.08, "end": 408.84000000000003, "text": " against these other kinds of paradigms", "tokens": [50512, 1970, 613, 661, 3685, 295, 13480, 328, 2592, 50600], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 161, "seek": 40412, "start": 408.84000000000003, "end": 410.32, "text": " for where language might fit in.", "tokens": [50600, 337, 689, 2856, 1062, 3318, 294, 13, 50674], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 162, "seek": 40412, "start": 410.32, "end": 412.24, "text": " It's rather to present at a mode", "tokens": [50674, 467, 311, 2831, 281, 1974, 412, 257, 4391, 50770], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 163, "seek": 40412, "start": 412.24, "end": 413.8, "text": " where we're really thinking a lot", "tokens": [50770, 689, 321, 434, 534, 1953, 257, 688, 50848], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 164, "seek": 40412, "start": 413.8, "end": 415.44, "text": " about scaling up language models", "tokens": [50848, 466, 21589, 493, 2856, 5245, 50930], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 165, "seek": 40412, "start": 415.44, "end": 417.96, "text": " as one really dominant role,", "tokens": [50930, 382, 472, 534, 15657, 3090, 11, 51056], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 166, "seek": 40412, "start": 417.96, "end": 420.2, "text": " what our other roots might be", "tokens": [51056, 437, 527, 661, 10669, 1062, 312, 51168], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 167, "seek": 40412, "start": 420.2, "end": 423.48, "text": " because it doesn't seem like that that really matches", "tokens": [51168, 570, 309, 1177, 380, 1643, 411, 300, 300, 534, 10676, 51332], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 168, "seek": 40412, "start": 423.48, "end": 425.76, "text": " a different prominent intelligent architecture", "tokens": [51332, 257, 819, 17034, 13232, 9482, 51446], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 169, "seek": 40412, "start": 425.76, "end": 427.4, "text": " that's sitting here in the room today.", "tokens": [51446, 300, 311, 3798, 510, 294, 264, 1808, 965, 13, 51528], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 170, "seek": 40412, "start": 427.4, "end": 429.28000000000003, "text": " So one source of evidence for the role", "tokens": [51528, 407, 472, 4009, 295, 4467, 337, 264, 3090, 51622], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 171, "seek": 40412, "start": 429.28000000000003, "end": 432.08, "text": " that language appears to play in human cognition", "tokens": [51622, 300, 2856, 7038, 281, 862, 294, 1952, 46905, 51762], "temperature": 0.0, "avg_logprob": -0.10726213830662525, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.24920246284455e-05}, {"id": 172, "seek": 43208, "start": 432.08, "end": 434.76, "text": " comes from neuroimaging data, fMRI data,", "tokens": [50364, 1487, 490, 16499, 332, 3568, 1412, 11, 283, 44, 5577, 1412, 11, 50498], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 173, "seek": 43208, "start": 434.76, "end": 438.15999999999997, "text": " suggesting how language is processed in the human brain.", "tokens": [50498, 18094, 577, 2856, 307, 18846, 294, 264, 1952, 3567, 13, 50668], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 174, "seek": 43208, "start": 438.15999999999997, "end": 442.28, "text": " And at this point, convergent imaging data from many people,", "tokens": [50668, 400, 412, 341, 935, 11, 9652, 6930, 25036, 1412, 490, 867, 561, 11, 50874], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 175, "seek": 43208, "start": 442.28, "end": 444.12, "text": " including Ed Fedorenko at MIT,", "tokens": [50874, 3009, 3977, 7772, 10948, 4093, 412, 13100, 11, 50966], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 176, "seek": 43208, "start": 444.12, "end": 446.12, "text": " Standa Hain and recent graduate students", "tokens": [50966, 745, 5575, 389, 491, 293, 5162, 8080, 1731, 51066], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 177, "seek": 43208, "start": 446.12, "end": 449.64, "text": " in our department, Kyle Malewald and Anya Ivanova,", "tokens": [51066, 294, 527, 5882, 11, 18023, 5746, 1023, 3976, 293, 2639, 64, 26546, 3730, 2757, 11, 51242], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 178, "seek": 43208, "start": 449.64, "end": 450.91999999999996, "text": " suggests that human brains", "tokens": [51242, 13409, 300, 1952, 15442, 51306], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 179, "seek": 43208, "start": 450.91999999999996, "end": 453.44, "text": " have this language-specific network", "tokens": [51306, 362, 341, 2856, 12, 29258, 3209, 51432], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 180, "seek": 43208, "start": 453.44, "end": 455.59999999999997, "text": " that handles many of the tasks that we associate with language.", "tokens": [51432, 300, 18722, 867, 295, 264, 9608, 300, 321, 14644, 365, 2856, 13, 51540], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 181, "seek": 43208, "start": 455.59999999999997, "end": 457.0, "text": " It's activated when people do tasks", "tokens": [51540, 467, 311, 18157, 562, 561, 360, 9608, 51610], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 182, "seek": 43208, "start": 457.0, "end": 459.56, "text": " like listening to sentences or reading them", "tokens": [51610, 411, 4764, 281, 16579, 420, 3760, 552, 51738], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 183, "seek": 43208, "start": 459.56, "end": 461.44, "text": " or speaking or writing words.", "tokens": [51738, 420, 4124, 420, 3579, 2283, 13, 51832], "temperature": 0.0, "avg_logprob": -0.18517204514123442, "compression_ratio": 1.6709677419354838, "no_speech_prob": 0.0007790020317770541}, {"id": 184, "seek": 46208, "start": 462.28, "end": 464.56, "text": " And this is not just an English-specific network.", "tokens": [50374, 400, 341, 307, 406, 445, 364, 3669, 12, 29258, 3209, 13, 50488], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 185, "seek": 46208, "start": 464.56, "end": 466.24, "text": " The same general region is activated", "tokens": [50488, 440, 912, 2674, 4458, 307, 18157, 50572], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 186, "seek": 46208, "start": 466.24, "end": 468.71999999999997, "text": " no matter what language someone is speaking.", "tokens": [50572, 572, 1871, 437, 2856, 1580, 307, 4124, 13, 50696], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 187, "seek": 46208, "start": 468.71999999999997, "end": 471.64, "text": " It even fires when they're producing constructed languages", "tokens": [50696, 467, 754, 15044, 562, 436, 434, 10501, 17083, 8650, 50842], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 188, "seek": 46208, "start": 471.64, "end": 473.35999999999996, "text": " for people who come to learn and become fluent", "tokens": [50842, 337, 561, 567, 808, 281, 1466, 293, 1813, 40799, 50928], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 189, "seek": 46208, "start": 473.35999999999996, "end": 475.32, "text": " in languages like Dothraki or Klingon.", "tokens": [50928, 294, 8650, 411, 413, 900, 424, 2984, 420, 591, 1688, 266, 13, 51026], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 190, "seek": 46208, "start": 476.96, "end": 479.47999999999996, "text": " And right, what is it that this language network does?", "tokens": [51108, 400, 558, 11, 437, 307, 309, 300, 341, 2856, 3209, 775, 30, 51234], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 191, "seek": 46208, "start": 479.47999999999996, "end": 481.79999999999995, "text": " Well, increasingly, one dominant hypothesis", "tokens": [51234, 1042, 11, 12980, 11, 472, 15657, 17291, 51350], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 192, "seek": 46208, "start": 481.79999999999995, "end": 483.47999999999996, "text": " is that it actually does do something", "tokens": [51350, 307, 300, 309, 767, 775, 360, 746, 51434], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 193, "seek": 46208, "start": 483.47999999999996, "end": 485.12, "text": " like next-word prediction.", "tokens": [51434, 411, 958, 12, 7462, 17630, 13, 51516], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 194, "seek": 46208, "start": 485.12, "end": 488.59999999999997, "text": " And in fact, among many other kinds of alternative models,", "tokens": [51516, 400, 294, 1186, 11, 3654, 867, 661, 3685, 295, 8535, 5245, 11, 51690], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 195, "seek": 46208, "start": 488.59999999999997, "end": 490.88, "text": " like people were earlier,", "tokens": [51690, 411, 561, 645, 3071, 11, 51804], "temperature": 0.0, "avg_logprob": -0.12666807908278246, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.00035127426963299513}, {"id": 196, "seek": 49088, "start": 490.88, "end": 492.48, "text": " for a long time I've been trying to do things", "tokens": [50364, 337, 257, 938, 565, 286, 600, 668, 1382, 281, 360, 721, 50444], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 197, "seek": 49088, "start": 492.48, "end": 494.56, "text": " like align words back to the brain,", "tokens": [50444, 411, 7975, 2283, 646, 281, 264, 3567, 11, 50548], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 198, "seek": 49088, "start": 494.56, "end": 496.76, "text": " transformer architectures really do seem to be", "tokens": [50548, 31782, 6331, 1303, 534, 360, 1643, 281, 312, 50658], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 199, "seek": 49088, "start": 496.76, "end": 498.76, "text": " among the best models that we have right now", "tokens": [50658, 3654, 264, 1151, 5245, 300, 321, 362, 558, 586, 50758], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 200, "seek": 49088, "start": 498.76, "end": 501.28, "text": " of the neural activity of this language network,", "tokens": [50758, 295, 264, 18161, 5191, 295, 341, 2856, 3209, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 201, "seek": 49088, "start": 501.28, "end": 502.64, "text": " specifically when they're trained", "tokens": [50884, 4682, 562, 436, 434, 8895, 50952], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 202, "seek": 49088, "start": 502.64, "end": 503.96, "text": " on next-word prediction tasks", "tokens": [50952, 322, 958, 12, 7462, 17630, 9608, 51018], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 203, "seek": 49088, "start": 503.96, "end": 506.6, "text": " and not other linguistic tasks like NOI.", "tokens": [51018, 293, 406, 661, 43002, 9608, 411, 9146, 40, 13, 51150], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 204, "seek": 49088, "start": 508.0, "end": 510.6, "text": " But I think what we see in this neuroimaging evidence", "tokens": [51220, 583, 286, 519, 437, 321, 536, 294, 341, 16499, 332, 3568, 4467, 51350], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 205, "seek": 49088, "start": 510.6, "end": 512.4, "text": " also highlights the ways in which the role", "tokens": [51350, 611, 14254, 264, 2098, 294, 597, 264, 3090, 51440], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 206, "seek": 49088, "start": 512.4, "end": 514.36, "text": " of this human language network", "tokens": [51440, 295, 341, 1952, 2856, 3209, 51538], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 207, "seek": 49088, "start": 514.36, "end": 516.44, "text": " probably is not as the controller", "tokens": [51538, 1391, 307, 406, 382, 264, 10561, 51642], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 208, "seek": 49088, "start": 516.44, "end": 518.36, "text": " or the central seat of cognition.", "tokens": [51642, 420, 264, 5777, 6121, 295, 46905, 13, 51738], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 209, "seek": 49088, "start": 518.36, "end": 519.92, "text": " It's selective for language", "tokens": [51738, 467, 311, 33930, 337, 2856, 51816], "temperature": 0.0, "avg_logprob": -0.09059477543485338, "compression_ratio": 1.7327044025157232, "no_speech_prob": 6.814018706791103e-05}, {"id": 210, "seek": 51992, "start": 519.92, "end": 521.7199999999999, "text": " and it isn't involved in many other kinds", "tokens": [50364, 293, 309, 1943, 380, 3288, 294, 867, 661, 3685, 50454], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 211, "seek": 51992, "start": 521.7199999999999, "end": 523.4399999999999, "text": " of thinking activities,", "tokens": [50454, 295, 1953, 5354, 11, 50540], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 212, "seek": 51992, "start": 523.4399999999999, "end": 525.8399999999999, "text": " even ones that might seem to involve symbols", "tokens": [50540, 754, 2306, 300, 1062, 1643, 281, 9494, 16944, 50660], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 213, "seek": 51992, "start": 525.8399999999999, "end": 527.3199999999999, "text": " like solving math problems", "tokens": [50660, 411, 12606, 5221, 2740, 50734], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 214, "seek": 51992, "start": 527.3199999999999, "end": 529.68, "text": " or reasoning about logic and physics and social reasoning.", "tokens": [50734, 420, 21577, 466, 9952, 293, 10649, 293, 2093, 21577, 13, 50852], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 215, "seek": 51992, "start": 529.68, "end": 533.16, "text": " Those invoke other regions of the brain", "tokens": [50852, 3950, 41117, 661, 10682, 295, 264, 3567, 51026], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 216, "seek": 51992, "start": 533.16, "end": 535.36, "text": " and language interfaces modulary with them.", "tokens": [51026, 293, 2856, 28416, 1072, 425, 822, 365, 552, 13, 51136], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 217, "seek": 51992, "start": 535.36, "end": 537.3199999999999, "text": " It can interface very generally with them,", "tokens": [51136, 467, 393, 9226, 588, 5101, 365, 552, 11, 51234], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 218, "seek": 51992, "start": 537.3199999999999, "end": 538.16, "text": " but it doesn't seem to be", "tokens": [51234, 457, 309, 1177, 380, 1643, 281, 312, 51276], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 219, "seek": 51992, "start": 538.16, "end": 540.36, "text": " where the bulk of thinking is taking place.", "tokens": [51276, 689, 264, 16139, 295, 1953, 307, 1940, 1081, 13, 51386], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 220, "seek": 51992, "start": 541.5999999999999, "end": 542.4399999999999, "text": " Oh, yes.", "tokens": [51448, 876, 11, 2086, 13, 51490], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 221, "seek": 51992, "start": 542.4399999999999, "end": 543.4399999999999, "text": " Oh yeah, I just had a question", "tokens": [51490, 876, 1338, 11, 286, 445, 632, 257, 1168, 51540], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 222, "seek": 51992, "start": 543.4399999999999, "end": 545.04, "text": " about the next-word prediction findings.", "tokens": [51540, 466, 264, 958, 12, 7462, 17630, 16483, 13, 51620], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 223, "seek": 51992, "start": 545.04, "end": 548.8399999999999, "text": " So did you also try to say like math word prediction", "tokens": [51620, 407, 630, 291, 611, 853, 281, 584, 411, 5221, 1349, 17630, 51810], "temperature": 0.0, "avg_logprob": -0.17166252697215362, "compression_ratio": 1.7508305647840532, "no_speech_prob": 0.00015353920753113925}, {"id": 224, "seek": 54884, "start": 548.96, "end": 551.44, "text": " and this is actually worse than next-word?", "tokens": [50370, 293, 341, 307, 767, 5324, 813, 958, 12, 7462, 30, 50494], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 225, "seek": 54884, "start": 551.44, "end": 553.44, "text": " Yeah, so I think they do compare", "tokens": [50494, 865, 11, 370, 286, 519, 436, 360, 6794, 50594], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 226, "seek": 54884, "start": 553.44, "end": 555.36, "text": " or like closed tasks are one of the alternates", "tokens": [50594, 420, 411, 5395, 9608, 366, 472, 295, 264, 5400, 1024, 50690], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 227, "seek": 54884, "start": 555.36, "end": 556.2, "text": " that they're looking for.", "tokens": [50690, 300, 436, 434, 1237, 337, 13, 50732], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 228, "seek": 54884, "start": 556.2, "end": 558.44, "text": " And also, yeah, people should feel free to just shout out", "tokens": [50732, 400, 611, 11, 1338, 11, 561, 820, 841, 1737, 281, 445, 8043, 484, 50844], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 229, "seek": 54884, "start": 558.44, "end": 560.72, "text": " as we've been doing all along.", "tokens": [50844, 382, 321, 600, 668, 884, 439, 2051, 13, 50958], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 230, "seek": 54884, "start": 560.72, "end": 564.88, "text": " So also, right, actually you can see adult patients", "tokens": [50958, 407, 611, 11, 558, 11, 767, 291, 393, 536, 5075, 4209, 51166], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 231, "seek": 54884, "start": 564.88, "end": 568.12, "text": " that suffer strokes that only damage this area of their brain", "tokens": [51166, 300, 9753, 24493, 300, 787, 4344, 341, 1859, 295, 641, 3567, 51328], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 232, "seek": 54884, "start": 568.12, "end": 571.72, "text": " and we find that they can still think about,", "tokens": [51328, 293, 321, 915, 300, 436, 393, 920, 519, 466, 11, 51508], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 233, "seek": 54884, "start": 571.72, "end": 573.8000000000001, "text": " well, they can't comprehend spoken language,", "tokens": [51508, 731, 11, 436, 393, 380, 38183, 10759, 2856, 11, 51612], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 234, "seek": 54884, "start": 573.8000000000001, "end": 575.8000000000001, "text": " but they can still think about all these different kinds", "tokens": [51612, 457, 436, 393, 920, 519, 466, 439, 613, 819, 3685, 51712], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 235, "seek": 54884, "start": 575.8000000000001, "end": 577.32, "text": " of tasks fronted in different mediums,", "tokens": [51712, 295, 9608, 1868, 292, 294, 819, 6399, 82, 11, 51788], "temperature": 0.0, "avg_logprob": -0.17205515651838155, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.0001910772843984887}, {"id": 236, "seek": 57732, "start": 577.32, "end": 578.9200000000001, "text": " like they can draw physical inferences", "tokens": [50364, 411, 436, 393, 2642, 4001, 13596, 2667, 50444], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 237, "seek": 57732, "start": 578.9200000000001, "end": 580.7600000000001, "text": " from videos that they're watching.", "tokens": [50444, 490, 2145, 300, 436, 434, 1976, 13, 50536], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 238, "seek": 57732, "start": 580.7600000000001, "end": 583.4000000000001, "text": " And conversely, maybe most tantalizingly,", "tokens": [50536, 400, 2615, 736, 11, 1310, 881, 12095, 304, 3319, 356, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 239, "seek": 57732, "start": 583.4000000000001, "end": 586.72, "text": " it's actually also possible to sustain localized damage", "tokens": [50668, 309, 311, 767, 611, 1944, 281, 6769, 44574, 4344, 50834], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 240, "seek": 57732, "start": 586.72, "end": 589.36, "text": " that leaves you still able to produce these long,", "tokens": [50834, 300, 5510, 291, 920, 1075, 281, 5258, 613, 938, 11, 50966], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 241, "seek": 57732, "start": 589.36, "end": 590.6, "text": " very relatively fluent", "tokens": [50966, 588, 7226, 40799, 51028], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 242, "seek": 57732, "start": 590.6, "end": 593.08, "text": " and quite syntactically coherent sentences,", "tokens": [51028, 293, 1596, 23980, 578, 984, 36239, 16579, 11, 51152], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 243, "seek": 57732, "start": 593.08, "end": 595.4000000000001, "text": " almost as if they're maybe just like a very rudimentary", "tokens": [51152, 1920, 382, 498, 436, 434, 1310, 445, 411, 257, 588, 32109, 2328, 822, 51268], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 244, "seek": 57732, "start": 595.4000000000001, "end": 598.08, "text": " and local word-based language model,", "tokens": [51268, 293, 2654, 1349, 12, 6032, 2856, 2316, 11, 51402], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 245, "seek": 57732, "start": 598.08, "end": 599.88, "text": " while not really conditioning meaningfully", "tokens": [51402, 1339, 406, 534, 21901, 3620, 2277, 51492], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 246, "seek": 57732, "start": 599.88, "end": 601.9200000000001, "text": " on what someone else is saying.", "tokens": [51492, 322, 437, 1580, 1646, 307, 1566, 13, 51594], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 247, "seek": 57732, "start": 601.9200000000001, "end": 603.8800000000001, "text": " So they say, do you like it here in Kansas City?", "tokens": [51594, 407, 436, 584, 11, 360, 291, 411, 309, 510, 294, 19422, 4392, 30, 51692], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 248, "seek": 57732, "start": 603.8800000000001, "end": 606.08, "text": " And this person says, yes, I am.", "tokens": [51692, 400, 341, 954, 1619, 11, 2086, 11, 286, 669, 13, 51802], "temperature": 0.0, "avg_logprob": -0.11244481587581498, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0005191320087760687}, {"id": 249, "seek": 60732, "start": 607.8000000000001, "end": 609.12, "text": " Or really producing sentences", "tokens": [50388, 1610, 534, 10501, 16579, 50454], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 250, "seek": 60732, "start": 609.12, "end": 610.72, "text": " that are obviously more globally meaningful", "tokens": [50454, 300, 366, 2745, 544, 18958, 10995, 50534], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 251, "seek": 60732, "start": 610.72, "end": 612.08, "text": " or goal directed towards the question.", "tokens": [50534, 420, 3387, 12898, 3030, 264, 1168, 13, 50602], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 252, "seek": 60732, "start": 612.08, "end": 613.88, "text": " And I don't want us to overindex on those results,", "tokens": [50602, 400, 286, 500, 380, 528, 505, 281, 670, 471, 3121, 322, 729, 3542, 11, 50692], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 253, "seek": 60732, "start": 613.88, "end": 617.0, "text": " but they kind of fit in with this more coherent picture.", "tokens": [50692, 457, 436, 733, 295, 3318, 294, 365, 341, 544, 36239, 3036, 13, 50848], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 254, "seek": 60732, "start": 617.0, "end": 620.12, "text": " And in many ways, I think this kind of recent evidence", "tokens": [50848, 400, 294, 867, 2098, 11, 286, 519, 341, 733, 295, 5162, 4467, 51004], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 255, "seek": 60732, "start": 620.12, "end": 622.8000000000001, "text": " from neuroscience actually supports the broader picture", "tokens": [51004, 490, 42762, 767, 9346, 264, 13227, 3036, 51138], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 256, "seek": 60732, "start": 622.8000000000001, "end": 625.24, "text": " of human cognition and the place of language in it", "tokens": [51138, 295, 1952, 46905, 293, 264, 1081, 295, 2856, 294, 309, 51260], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 257, "seek": 60732, "start": 625.24, "end": 626.72, "text": " that most cognitive scientists", "tokens": [51260, 300, 881, 15605, 7708, 51334], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 258, "seek": 60732, "start": 626.72, "end": 628.8000000000001, "text": " and many linguists have already believed", "tokens": [51334, 293, 867, 21766, 1751, 362, 1217, 7847, 51438], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 259, "seek": 60732, "start": 628.8000000000001, "end": 630.88, "text": " based on what we see and have come to learn", "tokens": [51438, 2361, 322, 437, 321, 536, 293, 362, 808, 281, 1466, 51542], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 260, "seek": 60732, "start": 630.88, "end": 632.8000000000001, "text": " through developmental science,", "tokens": [51542, 807, 30160, 3497, 11, 51638], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 261, "seek": 60732, "start": 632.8000000000001, "end": 634.2, "text": " studying how kids think", "tokens": [51638, 7601, 577, 2301, 519, 51708], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 262, "seek": 60732, "start": 634.2, "end": 636.32, "text": " and where language seems to fit into that picture, right?", "tokens": [51708, 293, 689, 2856, 2544, 281, 3318, 666, 300, 3036, 11, 558, 30, 51814], "temperature": 0.0, "avg_logprob": -0.08456231497384452, "compression_ratio": 1.7557471264367817, "no_speech_prob": 0.0004582265974022448}, {"id": 263, "seek": 63632, "start": 636.32, "end": 638.5600000000001, "text": " So a broad body of work at this point", "tokens": [50364, 407, 257, 4152, 1772, 295, 589, 412, 341, 935, 50476], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 264, "seek": 63632, "start": 638.5600000000001, "end": 641.5600000000001, "text": " suggests that infants, well before they learn language", "tokens": [50476, 13409, 300, 38829, 11, 731, 949, 436, 1466, 2856, 50626], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 265, "seek": 63632, "start": 641.5600000000001, "end": 643.32, "text": " of any kind or are speaking,", "tokens": [50626, 295, 604, 733, 420, 366, 4124, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 266, "seek": 63632, "start": 643.32, "end": 645.88, "text": " already independently perform many of the tasks", "tokens": [50714, 1217, 21761, 2042, 867, 295, 264, 9608, 50842], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 267, "seek": 63632, "start": 645.88, "end": 648.0, "text": " I think we associate with coherent thinking,", "tokens": [50842, 286, 519, 321, 14644, 365, 36239, 1953, 11, 50948], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 268, "seek": 63632, "start": 648.0, "end": 650.12, "text": " from reasoning about physics to planning", "tokens": [50948, 490, 21577, 466, 10649, 281, 5038, 51054], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 269, "seek": 63632, "start": 650.12, "end": 653.12, "text": " to drawing causal and probabilistic inferences.", "tokens": [51054, 281, 6316, 38755, 293, 31959, 3142, 13596, 2667, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 270, "seek": 63632, "start": 653.12, "end": 655.9200000000001, "text": " And language seems to be something that humans learn", "tokens": [51204, 400, 2856, 2544, 281, 312, 746, 300, 6255, 1466, 51344], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 271, "seek": 63632, "start": 655.9200000000001, "end": 659.08, "text": " and scaffold on top of this structured basis for thinking.", "tokens": [51344, 293, 44094, 322, 1192, 295, 341, 18519, 5143, 337, 1953, 13, 51502], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 272, "seek": 63632, "start": 659.08, "end": 660.96, "text": " We take much less input data", "tokens": [51502, 492, 747, 709, 1570, 4846, 1412, 51596], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 273, "seek": 63632, "start": 660.96, "end": 662.32, "text": " to learn to produce fluent language", "tokens": [51596, 281, 1466, 281, 5258, 40799, 2856, 51664], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 274, "seek": 63632, "start": 662.32, "end": 663.6800000000001, "text": " than a large language model.", "tokens": [51664, 813, 257, 2416, 2856, 2316, 13, 51732], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 275, "seek": 63632, "start": 663.6800000000001, "end": 666.1600000000001, "text": " And humans that actually aren't exposed", "tokens": [51732, 400, 6255, 300, 767, 3212, 380, 9495, 51856], "temperature": 0.0, "avg_logprob": -0.08933962881565094, "compression_ratio": 1.7318611987381702, "no_speech_prob": 4.198330134386197e-05}, {"id": 276, "seek": 66616, "start": 666.16, "end": 668.04, "text": " to any language input at all.", "tokens": [50364, 281, 604, 2856, 4846, 412, 439, 13, 50458], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 277, "seek": 66616, "start": 668.04, "end": 671.0, "text": " So famously there are these deaf children in Nicaragua", "tokens": [50458, 407, 34360, 456, 366, 613, 15559, 2227, 294, 426, 7953, 48692, 50606], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 278, "seek": 66616, "start": 671.0, "end": 673.4399999999999, "text": " who grow up in isolated hearing families", "tokens": [50606, 567, 1852, 493, 294, 14621, 4763, 4466, 50728], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 279, "seek": 66616, "start": 673.4399999999999, "end": 676.8399999999999, "text": " actually spontaneously come to produce early sign languages", "tokens": [50728, 767, 47632, 808, 281, 5258, 2440, 1465, 8650, 50898], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 280, "seek": 66616, "start": 676.8399999999999, "end": 678.76, "text": " as a product of trying to communicate events", "tokens": [50898, 382, 257, 1674, 295, 1382, 281, 7890, 3931, 50994], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 281, "seek": 66616, "start": 678.76, "end": 680.64, "text": " that bear many of the hallmarks of the syntax", "tokens": [50994, 300, 6155, 867, 295, 264, 6500, 37307, 295, 264, 28431, 51088], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 282, "seek": 66616, "start": 680.64, "end": 683.48, "text": " that we associate of our own natural languages,", "tokens": [51088, 300, 321, 14644, 295, 527, 1065, 3303, 8650, 11, 51230], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 283, "seek": 66616, "start": 683.48, "end": 686.1999999999999, "text": " like distinguishing between the subject who's punching", "tokens": [51230, 411, 11365, 3807, 1296, 264, 3983, 567, 311, 34866, 51366], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 284, "seek": 66616, "start": 686.1999999999999, "end": 688.4399999999999, "text": " and the person who's getting punched.", "tokens": [51366, 293, 264, 954, 567, 311, 1242, 37842, 13, 51478], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 285, "seek": 66616, "start": 688.4399999999999, "end": 690.0, "text": " Suggesting that in many ways", "tokens": [51478, 39131, 2629, 278, 300, 294, 867, 2098, 51556], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 286, "seek": 66616, "start": 690.0, "end": 692.64, "text": " the language we produce somehow externalizes", "tokens": [51556, 264, 2856, 321, 5258, 6063, 8320, 5660, 51688], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 287, "seek": 66616, "start": 692.64, "end": 696.04, "text": " the underlying structure of the thought that we already have.", "tokens": [51688, 264, 14217, 3877, 295, 264, 1194, 300, 321, 1217, 362, 13, 51858], "temperature": 0.0, "avg_logprob": -0.08407998085021973, "compression_ratio": 1.766773162939297, "no_speech_prob": 0.00035673423553816974}, {"id": 288, "seek": 69604, "start": 696.0799999999999, "end": 698.28, "text": " And of course, there are many other animals", "tokens": [50366, 400, 295, 1164, 11, 456, 366, 867, 661, 4882, 50476], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 289, "seek": 69604, "start": 698.28, "end": 700.68, "text": " that we associate as being intelligent in some way", "tokens": [50476, 300, 321, 14644, 382, 885, 13232, 294, 512, 636, 50596], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 290, "seek": 69604, "start": 700.68, "end": 702.9599999999999, "text": " and that have been modeled using the models", "tokens": [50596, 293, 300, 362, 668, 37140, 1228, 264, 5245, 50710], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 291, "seek": 69604, "start": 702.9599999999999, "end": 706.4, "text": " that we associate with probabilistic reasoning and planning", "tokens": [50710, 300, 321, 14644, 365, 31959, 3142, 21577, 293, 5038, 50882], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 292, "seek": 69604, "start": 706.4, "end": 707.7199999999999, "text": " that don't use language at all.", "tokens": [50882, 300, 500, 380, 764, 2856, 412, 439, 13, 50948], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 293, "seek": 69604, "start": 707.7199999999999, "end": 711.52, "text": " So it doesn't feel like language is by any means necessary", "tokens": [50948, 407, 309, 1177, 380, 841, 411, 2856, 307, 538, 604, 1355, 4818, 51138], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 294, "seek": 69604, "start": 711.52, "end": 713.04, "text": " for what we think of as thought.", "tokens": [51138, 337, 437, 321, 519, 295, 382, 1194, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 295, "seek": 69604, "start": 714.56, "end": 718.16, "text": " So formalizing this picture of an intelligent system,", "tokens": [51290, 407, 9860, 3319, 341, 3036, 295, 364, 13232, 1185, 11, 51470], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 296, "seek": 69604, "start": 718.16, "end": 721.24, "text": " one that's shared across animals and non-linguistic infants", "tokens": [51470, 472, 300, 311, 5507, 2108, 4882, 293, 2107, 12, 1688, 84, 3142, 38829, 51624], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 297, "seek": 69604, "start": 721.24, "end": 723.88, "text": " and maybe captures some of the computations involved", "tokens": [51624, 293, 1310, 27986, 512, 295, 264, 2807, 763, 3288, 51756], "temperature": 0.0, "avg_logprob": -0.14989275810046074, "compression_ratio": 1.7653429602888087, "no_speech_prob": 3.218467463739216e-05}, {"id": 298, "seek": 72388, "start": 723.88, "end": 725.52, "text": " in many of those thinking tasks", "tokens": [50364, 294, 867, 295, 729, 1953, 9608, 50446], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 299, "seek": 72388, "start": 725.52, "end": 730.4399999999999, "text": " that don't involve the language network in our brains", "tokens": [50446, 300, 500, 380, 9494, 264, 2856, 3209, 294, 527, 15442, 50692], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 300, "seek": 72388, "start": 730.4399999999999, "end": 732.84, "text": " is both the central goal of a lot of cognitive science", "tokens": [50692, 307, 1293, 264, 5777, 3387, 295, 257, 688, 295, 15605, 3497, 50812], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 301, "seek": 72388, "start": 732.84, "end": 736.12, "text": " and has been a historic motivation", "tokens": [50812, 293, 575, 668, 257, 13236, 12335, 50976], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 302, "seek": 72388, "start": 736.12, "end": 738.36, "text": " for diverse fields within AI", "tokens": [50976, 337, 9521, 7909, 1951, 7318, 51088], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 303, "seek": 72388, "start": 738.36, "end": 741.48, "text": " before the current sort of LLM centric moment.", "tokens": [51088, 949, 264, 2190, 1333, 295, 441, 43, 44, 1489, 1341, 1623, 13, 51244], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 304, "seek": 72388, "start": 741.48, "end": 745.28, "text": " So if you open up Russell and Norvig's textbook,", "tokens": [51244, 407, 498, 291, 1269, 493, 20937, 293, 6966, 85, 328, 311, 25591, 11, 51434], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 305, "seek": 72388, "start": 745.28, "end": 748.08, "text": " AI a modern approach, you'll see an equation like this", "tokens": [51434, 7318, 257, 4363, 3109, 11, 291, 603, 536, 364, 5367, 411, 341, 51574], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 306, "seek": 72388, "start": 748.08, "end": 750.96, "text": " that's supposed to sort of capture what we believe", "tokens": [51574, 300, 311, 3442, 281, 1333, 295, 7983, 437, 321, 1697, 51718], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 307, "seek": 72388, "start": 750.96, "end": 752.64, "text": " about how intelligence works.", "tokens": [51718, 466, 577, 7599, 1985, 13, 51802], "temperature": 0.0, "avg_logprob": -0.17151198516020905, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.001409356016665697}, {"id": 308, "seek": 75388, "start": 754.4399999999999, "end": 757.2, "text": " Computationally or a computational model of intelligence.", "tokens": [50392, 37804, 399, 379, 420, 257, 28270, 2316, 295, 7599, 13, 50530], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 309, "seek": 75388, "start": 757.2, "end": 759.2, "text": " Where the idea is that an intelligent agent", "tokens": [50530, 2305, 264, 1558, 307, 300, 364, 13232, 9461, 50630], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 310, "seek": 75388, "start": 759.2, "end": 762.72, "text": " is one that has sort of structured internal world models", "tokens": [50630, 307, 472, 300, 575, 1333, 295, 18519, 6920, 1002, 5245, 50806], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 311, "seek": 75388, "start": 762.72, "end": 766.28, "text": " that can be updated based on observations of the world", "tokens": [50806, 300, 393, 312, 10588, 2361, 322, 18163, 295, 264, 1002, 50984], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 312, "seek": 75388, "start": 766.28, "end": 769.48, "text": " and in which we can sort of predict the results", "tokens": [50984, 293, 294, 597, 321, 393, 1333, 295, 6069, 264, 3542, 51144], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 313, "seek": 75388, "start": 769.48, "end": 773.52, "text": " of our actions that the agent has some sort of values", "tokens": [51144, 295, 527, 5909, 300, 264, 9461, 575, 512, 1333, 295, 4190, 51346], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 314, "seek": 75388, "start": 773.52, "end": 777.4, "text": " or desires that are captured in some kind of utility function", "tokens": [51346, 420, 18005, 300, 366, 11828, 294, 512, 733, 295, 14877, 2445, 51540], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 315, "seek": 75388, "start": 777.4, "end": 779.48, "text": " that the agent can do probabilistic reasoning", "tokens": [51540, 300, 264, 9461, 393, 360, 31959, 3142, 21577, 51644], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 316, "seek": 75388, "start": 779.48, "end": 783.16, "text": " over its observations and the possible actions it might take", "tokens": [51644, 670, 1080, 18163, 293, 264, 1944, 5909, 309, 1062, 747, 51828], "temperature": 0.0, "avg_logprob": -0.10646753840976292, "compression_ratio": 1.9282868525896415, "no_speech_prob": 0.0003352975763846189}, {"id": 317, "seek": 78316, "start": 783.48, "end": 785.12, "text": " and what their expected utilities are", "tokens": [50380, 293, 437, 641, 5176, 30482, 366, 50462], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 318, "seek": 78316, "start": 785.12, "end": 786.68, "text": " and that it can do some kind of planning", "tokens": [50462, 293, 300, 309, 393, 360, 512, 733, 295, 5038, 50540], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 319, "seek": 78316, "start": 786.68, "end": 788.76, "text": " to optimize the value of the back end.", "tokens": [50540, 281, 19719, 264, 2158, 295, 264, 646, 917, 13, 50644], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 320, "seek": 78316, "start": 788.76, "end": 789.6, "text": " Yeah.", "tokens": [50644, 865, 13, 50686], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 321, "seek": 78316, "start": 789.6, "end": 792.3199999999999, "text": " Is there a normative statement or descriptive statement?", "tokens": [50686, 1119, 456, 257, 2026, 1166, 5629, 420, 42585, 5629, 30, 50822], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 322, "seek": 78316, "start": 792.3199999999999, "end": 795.3199999999999, "text": " Is it the case that we are defining intelligent agent", "tokens": [50822, 1119, 309, 264, 1389, 300, 321, 366, 17827, 13232, 9461, 50972], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 323, "seek": 78316, "start": 795.3199999999999, "end": 797.1999999999999, "text": " to be having this kind of property", "tokens": [50972, 281, 312, 1419, 341, 733, 295, 4707, 51066], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 324, "seek": 78316, "start": 797.1999999999999, "end": 800.0, "text": " or it's like if we want to do intelligent architecture,", "tokens": [51066, 420, 309, 311, 411, 498, 321, 528, 281, 360, 13232, 9482, 11, 51206], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 325, "seek": 78316, "start": 800.0, "end": 801.36, "text": " we should have these?", "tokens": [51206, 321, 820, 362, 613, 30, 51274], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 326, "seek": 78316, "start": 801.36, "end": 803.04, "text": " Yeah, so I think some, it's a great question.", "tokens": [51274, 865, 11, 370, 286, 519, 512, 11, 309, 311, 257, 869, 1168, 13, 51358], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 327, "seek": 78316, "start": 803.04, "end": 804.9599999999999, "text": " I think some of the work definitely is coming at it", "tokens": [51358, 286, 519, 512, 295, 264, 589, 2138, 307, 1348, 412, 309, 51454], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 328, "seek": 78316, "start": 804.9599999999999, "end": 806.16, "text": " from a normative perspective.", "tokens": [51454, 490, 257, 2026, 1166, 4585, 13, 51514], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 329, "seek": 78316, "start": 806.16, "end": 808.92, "text": " This is like a definition of what rational actions", "tokens": [51514, 639, 307, 411, 257, 7123, 295, 437, 15090, 5909, 51652], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 330, "seek": 78316, "start": 808.92, "end": 810.0799999999999, "text": " might look like, right?", "tokens": [51652, 1062, 574, 411, 11, 558, 30, 51710], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 331, "seek": 78316, "start": 810.0799999999999, "end": 811.24, "text": " But I think there is a lot of work", "tokens": [51710, 583, 286, 519, 456, 307, 257, 688, 295, 589, 51768], "temperature": 0.0, "avg_logprob": -0.22625830234625402, "compression_ratio": 1.8512658227848102, "no_speech_prob": 0.0019872107077389956}, {"id": 332, "seek": 81124, "start": 811.5600000000001, "end": 812.96, "text": " computational cognitive science", "tokens": [50380, 28270, 15605, 3497, 50450], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 333, "seek": 81124, "start": 812.96, "end": 814.88, "text": " that sort of in various domains", "tokens": [50450, 300, 1333, 295, 294, 3683, 25514, 50546], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 334, "seek": 81124, "start": 814.88, "end": 816.0, "text": " has collected a lot of evidence", "tokens": [50546, 575, 11087, 257, 688, 295, 4467, 50602], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 335, "seek": 81124, "start": 816.0, "end": 819.4, "text": " that people either like recognize this as normative", "tokens": [50602, 300, 561, 2139, 411, 5521, 341, 382, 2026, 1166, 50772], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 336, "seek": 81124, "start": 819.4, "end": 822.92, "text": " or behave like this in computation,", "tokens": [50772, 420, 15158, 411, 341, 294, 24903, 11, 50948], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 337, "seek": 81124, "start": 822.92, "end": 825.72, "text": " according to the limits of what they can do computationally.", "tokens": [50948, 4650, 281, 264, 10406, 295, 437, 436, 393, 360, 24903, 379, 13, 51088], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 338, "seek": 81124, "start": 828.24, "end": 833.24, "text": " So in this architecture, the controller for thinking", "tokens": [51214, 407, 294, 341, 9482, 11, 264, 10561, 337, 1953, 51464], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 339, "seek": 81124, "start": 833.32, "end": 834.6, "text": " would be some kind of system", "tokens": [51468, 576, 312, 512, 733, 295, 1185, 51532], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 340, "seek": 81124, "start": 834.6, "end": 836.24, "text": " that's capable of general world modeling", "tokens": [51532, 300, 311, 8189, 295, 2674, 1002, 15983, 51614], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 341, "seek": 81124, "start": 836.24, "end": 837.8, "text": " and probabilistic reasoning", "tokens": [51614, 293, 31959, 3142, 21577, 51692], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 342, "seek": 81124, "start": 837.8, "end": 839.8, "text": " or planning and utility maximization", "tokens": [51692, 420, 5038, 293, 14877, 5138, 2144, 51792], "temperature": 0.0, "avg_logprob": -0.14167837663130325, "compression_ratio": 1.7007874015748032, "no_speech_prob": 0.0001795018615666777}, {"id": 343, "seek": 83980, "start": 839.8399999999999, "end": 844.12, "text": " rather than sort of a primarily linguistic system necessarily.", "tokens": [50366, 2831, 813, 1333, 295, 257, 10029, 43002, 1185, 4725, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 344, "seek": 83980, "start": 845.8399999999999, "end": 847.7199999999999, "text": " Now largely attempts in artificial intelligence", "tokens": [50666, 823, 11611, 15257, 294, 11677, 7599, 50760], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 345, "seek": 83980, "start": 847.7199999999999, "end": 849.3599999999999, "text": " to directly implement this equation", "tokens": [50760, 281, 3838, 4445, 341, 5367, 50842], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 346, "seek": 83980, "start": 849.3599999999999, "end": 851.92, "text": " by building out components that sort of map on", "tokens": [50842, 538, 2390, 484, 6677, 300, 1333, 295, 4471, 322, 50970], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 347, "seek": 83980, "start": 851.92, "end": 855.52, "text": " to the different elements of this equation", "tokens": [50970, 281, 264, 819, 4959, 295, 341, 5367, 51150], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 348, "seek": 83980, "start": 855.52, "end": 858.1999999999999, "text": " have struggled to match the computational efficiency", "tokens": [51150, 362, 19023, 281, 2995, 264, 28270, 10493, 51284], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 349, "seek": 83980, "start": 858.1999999999999, "end": 860.64, "text": " and the generality of natural intelligence.", "tokens": [51284, 293, 264, 1337, 1860, 295, 3303, 7599, 13, 51406], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 350, "seek": 83980, "start": 860.64, "end": 863.1999999999999, "text": " But we have seen over the past couple of decades or so", "tokens": [51406, 583, 321, 362, 1612, 670, 264, 1791, 1916, 295, 7878, 420, 370, 51534], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 351, "seek": 83980, "start": 863.1999999999999, "end": 864.68, "text": " the emergence of this new class of tools", "tokens": [51534, 264, 36211, 295, 341, 777, 1508, 295, 3873, 51608], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 352, "seek": 83980, "start": 864.68, "end": 866.4799999999999, "text": " called probabilistic programming languages", "tokens": [51608, 1219, 31959, 3142, 9410, 8650, 51698], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 353, "seek": 83980, "start": 866.4799999999999, "end": 867.3199999999999, "text": " for building software", "tokens": [51698, 337, 2390, 4722, 51740], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 354, "seek": 83980, "start": 867.3199999999999, "end": 869.4799999999999, "text": " that can solve probabilistic reasoning tasks", "tokens": [51740, 300, 393, 5039, 31959, 3142, 21577, 9608, 51848], "temperature": 0.0, "avg_logprob": -0.09999599287995195, "compression_ratio": 1.8271186440677967, "no_speech_prob": 0.0001088783101295121}, {"id": 355, "seek": 86948, "start": 869.48, "end": 871.76, "text": " at least in limited domains.", "tokens": [50364, 412, 1935, 294, 5567, 25514, 13, 50478], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 356, "seek": 86948, "start": 871.76, "end": 875.28, "text": " And sorry, and these languages have been applied", "tokens": [50478, 400, 2597, 11, 293, 613, 8650, 362, 668, 6456, 50654], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 357, "seek": 86948, "start": 875.28, "end": 877.28, "text": " to create systems that do everything", "tokens": [50654, 281, 1884, 3652, 300, 360, 1203, 50754], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 358, "seek": 86948, "start": 877.28, "end": 879.72, "text": " from perceiving cluttered 3D scenes", "tokens": [50754, 490, 9016, 2123, 40614, 292, 805, 35, 8026, 50876], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 359, "seek": 86948, "start": 879.72, "end": 881.08, "text": " more accurately sometimes", "tokens": [50876, 544, 20095, 2171, 50944], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 360, "seek": 86948, "start": 881.08, "end": 883.52, "text": " than object detector neural nets", "tokens": [50944, 813, 2657, 25712, 18161, 36170, 51066], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 361, "seek": 86948, "start": 883.52, "end": 886.4, "text": " to interpreting and predicting economic trends", "tokens": [51066, 281, 37395, 293, 32884, 4836, 13892, 51210], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 362, "seek": 86948, "start": 886.4, "end": 888.36, "text": " more accurately than leading industry solutions", "tokens": [51210, 544, 20095, 813, 5775, 3518, 6547, 51308], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 363, "seek": 86948, "start": 888.36, "end": 889.76, "text": " like Facebook neural profit.", "tokens": [51308, 411, 4384, 18161, 7475, 13, 51378], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 364, "seek": 86948, "start": 890.6800000000001, "end": 892.48, "text": " And these probabilistic programming systems", "tokens": [51424, 400, 613, 31959, 3142, 9410, 3652, 51514], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 365, "seek": 86948, "start": 892.48, "end": 893.6800000000001, "text": " are enabling these applications", "tokens": [51514, 366, 23148, 613, 5821, 51574], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 366, "seek": 86948, "start": 893.6800000000001, "end": 895.2, "text": " with two key technical features.", "tokens": [51574, 365, 732, 2141, 6191, 4122, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 367, "seek": 86948, "start": 895.2, "end": 897.0, "text": " They feature modeling languages", "tokens": [51650, 814, 4111, 15983, 8650, 51740], "temperature": 0.0, "avg_logprob": -0.1591936186248181, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.0001159021703642793}, {"id": 368, "seek": 89700, "start": 897.0, "end": 899.96, "text": " that let users express complicated probabilistic models", "tokens": [50364, 300, 718, 5022, 5109, 6179, 31959, 3142, 5245, 50512], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 369, "seek": 89700, "start": 899.96, "end": 901.44, "text": " of the world as programs", "tokens": [50512, 295, 264, 1002, 382, 4268, 50586], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 370, "seek": 89700, "start": 901.44, "end": 904.24, "text": " making it easy to write down rich probabilistic models", "tokens": [50586, 1455, 309, 1858, 281, 2464, 760, 4593, 31959, 3142, 5245, 50726], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 371, "seek": 89700, "start": 904.24, "end": 906.04, "text": " that are defined in terms of expressive", "tokens": [50726, 300, 366, 7642, 294, 2115, 295, 40189, 50816], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 372, "seek": 89700, "start": 906.04, "end": 907.76, "text": " program-like components.", "tokens": [50816, 1461, 12, 4092, 6677, 13, 50902], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 373, "seek": 89700, "start": 907.76, "end": 910.16, "text": " So for example, the probabilistic models behind", "tokens": [50902, 407, 337, 1365, 11, 264, 31959, 3142, 5245, 2261, 51022], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 374, "seek": 89700, "start": 910.16, "end": 911.56, "text": " these example applications", "tokens": [51022, 613, 1365, 5821, 51092], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 375, "seek": 89700, "start": 911.56, "end": 914.12, "text": " are defined in terms of 3D renderers, symbolic planners,", "tokens": [51092, 366, 7642, 294, 2115, 295, 805, 35, 15529, 433, 11, 25755, 49674, 11, 51220], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 376, "seek": 89700, "start": 914.12, "end": 916.44, "text": " scientific simulators and so on.", "tokens": [51220, 8134, 1034, 39265, 293, 370, 322, 13, 51336], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 377, "seek": 89700, "start": 916.44, "end": 918.44, "text": " And the second thing that probabilistic programming systems do", "tokens": [51336, 400, 264, 1150, 551, 300, 31959, 3142, 9410, 3652, 360, 51436], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 378, "seek": 89700, "start": 918.44, "end": 921.0, "text": " is that they automate various mathematical operations", "tokens": [51436, 307, 300, 436, 31605, 3683, 18894, 7705, 51564], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 379, "seek": 89700, "start": 921.0, "end": 922.12, "text": " on these models", "tokens": [51564, 322, 613, 5245, 51620], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 380, "seek": 89700, "start": 922.12, "end": 923.96, "text": " and that automation makes it possible for users", "tokens": [51620, 293, 300, 17769, 1669, 309, 1944, 337, 5022, 51712], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 381, "seek": 89700, "start": 923.96, "end": 925.88, "text": " to concisely implement sophisticated algorithms", "tokens": [51712, 281, 1588, 271, 736, 4445, 16950, 14642, 51808], "temperature": 0.0, "avg_logprob": -0.0902686191327644, "compression_ratio": 1.9038461538461537, "no_speech_prob": 0.00036821371759288013}, {"id": 382, "seek": 92588, "start": 925.88, "end": 927.2, "text": " for probabilistic reasoning,", "tokens": [50364, 337, 31959, 3142, 21577, 11, 50430], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 383, "seek": 92588, "start": 927.2, "end": 928.88, "text": " such as the variety of Monte Carlo", "tokens": [50430, 1270, 382, 264, 5673, 295, 38105, 45112, 50514], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 384, "seek": 92588, "start": 928.88, "end": 930.16, "text": " and variational inference algorithms", "tokens": [50514, 293, 3034, 1478, 38253, 14642, 50578], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 385, "seek": 92588, "start": 930.16, "end": 932.0, "text": " that power these example applications.", "tokens": [50578, 300, 1347, 613, 1365, 5821, 13, 50670], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 386, "seek": 92588, "start": 932.0, "end": 933.36, "text": " One way of thinking about these tools", "tokens": [50670, 1485, 636, 295, 1953, 466, 613, 3873, 50738], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 387, "seek": 92588, "start": 933.36, "end": 938.16, "text": " is as kind of like PyTorch or TensorFlow,", "tokens": [50738, 307, 382, 733, 295, 411, 9953, 51, 284, 339, 420, 37624, 11, 50978], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 388, "seek": 92588, "start": 938.16, "end": 940.76, "text": " but instead of writing differentiable models", "tokens": [50978, 457, 2602, 295, 3579, 819, 9364, 5245, 51108], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 389, "seek": 92588, "start": 940.76, "end": 941.72, "text": " and doing optimization,", "tokens": [51108, 293, 884, 19618, 11, 51156], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 390, "seek": 92588, "start": 941.72, "end": 942.96, "text": " you're writing probabilistic models", "tokens": [51156, 291, 434, 3579, 31959, 3142, 5245, 51218], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 391, "seek": 92588, "start": 942.96, "end": 945.2, "text": " and doing various probabilistic reasoning tasks.", "tokens": [51218, 293, 884, 3683, 31959, 3142, 21577, 9608, 13, 51330], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 392, "seek": 92588, "start": 946.76, "end": 948.84, "text": " So far, these AI engineering efforts", "tokens": [51408, 407, 1400, 11, 613, 7318, 7043, 6484, 51512], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 393, "seek": 92588, "start": 948.84, "end": 950.0, "text": " haven't really made contact", "tokens": [51512, 2378, 380, 534, 1027, 3385, 51570], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 394, "seek": 92588, "start": 950.0, "end": 954.08, "text": " with the sort of language model part of AI.", "tokens": [51570, 365, 264, 1333, 295, 2856, 2316, 644, 295, 7318, 13, 51774], "temperature": 0.0, "avg_logprob": -0.14205826653374565, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0004877715837210417}, {"id": 395, "seek": 95408, "start": 955.0, "end": 956.76, "text": " Much like there hasn't yet been a concerted effort", "tokens": [50410, 12313, 411, 456, 6132, 380, 1939, 668, 257, 8543, 292, 4630, 50498], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 396, "seek": 95408, "start": 956.76, "end": 959.08, "text": " to take this classical picture of intelligence architecture", "tokens": [50498, 281, 747, 341, 13735, 3036, 295, 7599, 9482, 50614], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 397, "seek": 95408, "start": 959.08, "end": 960.4000000000001, "text": " and figure out how language might be", "tokens": [50614, 293, 2573, 484, 577, 2856, 1062, 312, 50680], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 398, "seek": 95408, "start": 960.4000000000001, "end": 961.5200000000001, "text": " richly integrated into it.", "tokens": [50680, 4593, 356, 10919, 666, 309, 13, 50736], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 399, "seek": 95408, "start": 961.5200000000001, "end": 963.2800000000001, "text": " So for the rest of the talk today,", "tokens": [50736, 407, 337, 264, 1472, 295, 264, 751, 965, 11, 50824], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 400, "seek": 95408, "start": 963.2800000000001, "end": 964.9200000000001, "text": " we're gonna explore two different approaches", "tokens": [50824, 321, 434, 799, 6839, 732, 819, 11587, 50906], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 401, "seek": 95408, "start": 964.9200000000001, "end": 966.36, "text": " for thinking about where language", "tokens": [50906, 337, 1953, 466, 689, 2856, 50978], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 402, "seek": 95408, "start": 966.36, "end": 969.0, "text": " might fit into this picture and approach.", "tokens": [50978, 1062, 3318, 666, 341, 3036, 293, 3109, 13, 51110], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 403, "seek": 95408, "start": 969.0, "end": 971.1600000000001, "text": " So probably Leo's gonna begin", "tokens": [51110, 407, 1391, 19344, 311, 799, 1841, 51218], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 404, "seek": 95408, "start": 971.1600000000001, "end": 972.8000000000001, "text": " by talking about how an intelligent agent", "tokens": [51218, 538, 1417, 466, 577, 364, 13232, 9461, 51300], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 405, "seek": 95408, "start": 972.8000000000001, "end": 976.12, "text": " might incorporate lots of externally produced languages,", "tokens": [51300, 1062, 16091, 3195, 295, 40899, 7126, 8650, 11, 51466], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 406, "seek": 95408, "start": 976.12, "end": 979.2, "text": " explanations, observations, questions,", "tokens": [51466, 28708, 11, 18163, 11, 1651, 11, 51620], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 407, "seek": 95408, "start": 979.2, "end": 982.1600000000001, "text": " how an agent can incorporate all of those forms of language", "tokens": [51620, 577, 364, 9461, 393, 16091, 439, 295, 729, 6422, 295, 2856, 51768], "temperature": 0.0, "avg_logprob": -0.09537554913618433, "compression_ratio": 1.8724832214765101, "no_speech_prob": 0.00021647692483384162}, {"id": 408, "seek": 98216, "start": 982.1999999999999, "end": 984.56, "text": " into the way that it updates its beliefs", "tokens": [50366, 666, 264, 636, 300, 309, 9205, 1080, 13585, 50484], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 409, "seek": 98216, "start": 984.56, "end": 986.24, "text": " and decides how to act in the world.", "tokens": [50484, 293, 14898, 577, 281, 605, 294, 264, 1002, 13, 50568], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 410, "seek": 98216, "start": 986.24, "end": 987.7199999999999, "text": " Then I'll talk a bit about systems", "tokens": [50568, 1396, 286, 603, 751, 257, 857, 466, 3652, 50642], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 411, "seek": 98216, "start": 987.7199999999999, "end": 990.12, "text": " that leverage language as a tool for thinking", "tokens": [50642, 300, 13982, 2856, 382, 257, 2290, 337, 1953, 50762], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 412, "seek": 98216, "start": 990.12, "end": 991.9599999999999, "text": " within its models of the world", "tokens": [50762, 1951, 1080, 5245, 295, 264, 1002, 50854], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 413, "seek": 98216, "start": 991.9599999999999, "end": 993.8399999999999, "text": " or its probabilistic reasoning algorithms.", "tokens": [50854, 420, 1080, 31959, 3142, 21577, 14642, 13, 50948], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 414, "seek": 98216, "start": 993.8399999999999, "end": 995.0, "text": " And each of these corresponds", "tokens": [50948, 400, 1184, 295, 613, 23249, 51006], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 415, "seek": 98216, "start": 995.0, "end": 997.68, "text": " to very recent preprints of work.", "tokens": [51006, 281, 588, 5162, 659, 25788, 295, 589, 13, 51140], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 416, "seek": 98216, "start": 997.68, "end": 999.4, "text": " So I also wanna give a disclaimer.", "tokens": [51140, 407, 286, 611, 1948, 976, 257, 40896, 13, 51226], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 417, "seek": 98216, "start": 999.4, "end": 1002.4399999999999, "text": " We should really emphasize that both are preliminary proposals", "tokens": [51226, 492, 820, 534, 16078, 300, 1293, 366, 28817, 20198, 51378], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 418, "seek": 98216, "start": 1002.4399999999999, "end": 1004.8, "text": " and we're giving a very speculative talk,", "tokens": [51378, 293, 321, 434, 2902, 257, 588, 49415, 751, 11, 51496], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 419, "seek": 98216, "start": 1004.8, "end": 1006.8, "text": " not scaled architectural solutions here.", "tokens": [51496, 406, 36039, 26621, 6547, 510, 13, 51596], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 420, "seek": 98216, "start": 1008.3199999999999, "end": 1009.16, "text": " Cool, right.", "tokens": [51672, 8561, 11, 558, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 421, "seek": 98216, "start": 1009.16, "end": 1010.8, "text": " So the first person that's portion of this talk", "tokens": [51714, 407, 264, 700, 954, 300, 311, 8044, 295, 341, 751, 51796], "temperature": 0.0, "avg_logprob": -0.17017569365324797, "compression_ratio": 1.6452599388379205, "no_speech_prob": 0.0002531051868572831}, {"id": 422, "seek": 101080, "start": 1010.8, "end": 1011.64, "text": " is summarizing work.", "tokens": [50364, 307, 14611, 3319, 589, 13, 50406], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 423, "seek": 101080, "start": 1011.64, "end": 1012.64, "text": " If you wanna read more,", "tokens": [50406, 759, 291, 1948, 1401, 544, 11, 50456], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 424, "seek": 101080, "start": 1012.64, "end": 1014.3599999999999, "text": " you can find this very long paper", "tokens": [50456, 291, 393, 915, 341, 588, 938, 3035, 50542], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 425, "seek": 101080, "start": 1014.3599999999999, "end": 1016.3199999999999, "text": " from word models to world models.", "tokens": [50542, 490, 1349, 5245, 281, 1002, 5245, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 426, "seek": 101080, "start": 1016.3199999999999, "end": 1017.16, "text": " That's up on archive", "tokens": [50640, 663, 311, 493, 322, 23507, 50682], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 427, "seek": 101080, "start": 1017.16, "end": 1019.12, "text": " and this is primarily done with another student", "tokens": [50682, 293, 341, 307, 10029, 1096, 365, 1071, 3107, 50780], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 428, "seek": 101080, "start": 1019.12, "end": 1021.3199999999999, "text": " who's not here today, Gabe Grant.", "tokens": [50780, 567, 311, 406, 510, 965, 11, 39524, 17529, 13, 50890], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 429, "seek": 101080, "start": 1021.3199999999999, "end": 1023.28, "text": " And as I just mentioned, the context for this work", "tokens": [50890, 400, 382, 286, 445, 2835, 11, 264, 4319, 337, 341, 589, 50988], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 430, "seek": 101080, "start": 1023.28, "end": 1025.2, "text": " is thinking about how we can build systems", "tokens": [50988, 307, 1953, 466, 577, 321, 393, 1322, 3652, 51084], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 431, "seek": 101080, "start": 1025.2, "end": 1027.62, "text": " that capture the breadth with which all the external language", "tokens": [51084, 300, 7983, 264, 35862, 365, 597, 439, 264, 8320, 2856, 51205], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 432, "seek": 101080, "start": 1027.62, "end": 1030.9199999999998, "text": " we hear seems to inform at least our human thinking.", "tokens": [51205, 321, 1568, 2544, 281, 1356, 412, 1935, 527, 1952, 1953, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 433, "seek": 101080, "start": 1030.9199999999998, "end": 1031.9199999999998, "text": " And what's clear, right,", "tokens": [51370, 400, 437, 311, 1850, 11, 558, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 434, "seek": 101080, "start": 1031.9199999999998, "end": 1033.68, "text": " is that this role seems to be very broad.", "tokens": [51420, 307, 300, 341, 3090, 2544, 281, 312, 588, 4152, 13, 51508], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 435, "seek": 101080, "start": 1033.68, "end": 1035.8799999999999, "text": " If we take the basic model of an agent", "tokens": [51508, 759, 321, 747, 264, 3875, 2316, 295, 364, 9461, 51618], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 436, "seek": 101080, "start": 1035.8799999999999, "end": 1037.2, "text": " with beliefs and goals,", "tokens": [51618, 365, 13585, 293, 5493, 11, 51684], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 437, "seek": 101080, "start": 1037.2, "end": 1039.48, "text": " well, it seems like there's an incredibly diverse range", "tokens": [51684, 731, 11, 309, 2544, 411, 456, 311, 364, 6252, 9521, 3613, 51798], "temperature": 0.0, "avg_logprob": -0.1248501736693587, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.0004581770917866379}, {"id": 438, "seek": 103948, "start": 1039.48, "end": 1041.96, "text": " of situations in which we can update our beliefs", "tokens": [50364, 295, 6851, 294, 597, 321, 393, 5623, 527, 13585, 50488], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 439, "seek": 103948, "start": 1041.96, "end": 1043.56, "text": " about a situation from observations", "tokens": [50488, 466, 257, 2590, 490, 18163, 50568], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 440, "seek": 103948, "start": 1043.56, "end": 1045.16, "text": " that we express in language,", "tokens": [50568, 300, 321, 5109, 294, 2856, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 441, "seek": 103948, "start": 1045.16, "end": 1046.64, "text": " or in which the goals of our thought", "tokens": [50648, 420, 294, 597, 264, 5493, 295, 527, 1194, 50722], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 442, "seek": 103948, "start": 1046.64, "end": 1049.48, "text": " are to answer questions that we specify linguistically.", "tokens": [50722, 366, 281, 1867, 1651, 300, 321, 16500, 21766, 20458, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 443, "seek": 103948, "start": 1049.48, "end": 1052.28, "text": " And these might implicate our knowledge of other agents", "tokens": [50864, 400, 613, 1062, 10629, 473, 527, 3601, 295, 661, 12554, 51004], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 444, "seek": 103948, "start": 1052.28, "end": 1053.76, "text": " drawing on our intuitive psychology", "tokens": [51004, 6316, 322, 527, 21769, 15105, 51078], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 445, "seek": 103948, "start": 1053.76, "end": 1056.24, "text": " to reason about what they think and what they'll do.", "tokens": [51078, 281, 1778, 466, 437, 436, 519, 293, 437, 436, 603, 360, 13, 51202], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 446, "seek": 103948, "start": 1056.24, "end": 1058.64, "text": " Or we can talk about the physical world around us,", "tokens": [51202, 1610, 321, 393, 751, 466, 264, 4001, 1002, 926, 505, 11, 51322], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 447, "seek": 103948, "start": 1058.64, "end": 1060.28, "text": " what we perceive and ask questions", "tokens": [51322, 437, 321, 20281, 293, 1029, 1651, 51404], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 448, "seek": 103948, "start": 1060.28, "end": 1061.76, "text": " that require us to reason about", "tokens": [51404, 300, 3651, 505, 281, 1778, 466, 51478], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 449, "seek": 103948, "start": 1061.76, "end": 1064.08, "text": " what we see and draw on our physical intuitions.", "tokens": [51478, 437, 321, 536, 293, 2642, 322, 527, 4001, 16224, 626, 13, 51594], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 450, "seek": 103948, "start": 1065.16, "end": 1066.52, "text": " And of course, one of the remarkable things", "tokens": [51648, 400, 295, 1164, 11, 472, 295, 264, 12802, 721, 51716], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 451, "seek": 103948, "start": 1066.52, "end": 1068.28, "text": " where we're also excited about language", "tokens": [51716, 689, 321, 434, 611, 2919, 466, 2856, 51804], "temperature": 0.0, "avg_logprob": -0.11593343800511853, "compression_ratio": 1.8930817610062893, "no_speech_prob": 0.00031496273004449904}, {"id": 452, "seek": 106828, "start": 1068.28, "end": 1070.3999999999999, "text": " is that it doesn't just draw on what we already know,", "tokens": [50364, 307, 300, 309, 1177, 380, 445, 2642, 322, 437, 321, 1217, 458, 11, 50470], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 453, "seek": 106828, "start": 1070.3999999999999, "end": 1072.36, "text": " it's this means by which humans seem to learn", "tokens": [50470, 309, 311, 341, 1355, 538, 597, 6255, 1643, 281, 1466, 50568], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 454, "seek": 106828, "start": 1072.36, "end": 1074.6, "text": " and pass on profoundly new knowledge,", "tokens": [50568, 293, 1320, 322, 39954, 777, 3601, 11, 50680], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 455, "seek": 106828, "start": 1074.6, "end": 1078.6399999999999, "text": " whether that's new concepts that we define in words", "tokens": [50680, 1968, 300, 311, 777, 10392, 300, 321, 6964, 294, 2283, 50882], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 456, "seek": 106828, "start": 1078.6399999999999, "end": 1081.36, "text": " or learn from words to really profound new theories", "tokens": [50882, 420, 1466, 490, 2283, 281, 534, 14382, 777, 13667, 51018], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 457, "seek": 106828, "start": 1081.36, "end": 1082.96, "text": " and conceptual systems, right?", "tokens": [51018, 293, 24106, 3652, 11, 558, 30, 51098], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 458, "seek": 106828, "start": 1082.96, "end": 1084.3999999999999, "text": " Much of what we know about the world,", "tokens": [51098, 12313, 295, 437, 321, 458, 466, 264, 1002, 11, 51170], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 459, "seek": 106828, "start": 1084.3999999999999, "end": 1086.96, "text": " the fact that there are wars, what wars are,", "tokens": [51170, 264, 1186, 300, 456, 366, 13718, 11, 437, 13718, 366, 11, 51298], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 460, "seek": 106828, "start": 1086.96, "end": 1088.72, "text": " legal systems, sciences,", "tokens": [51298, 5089, 3652, 11, 17677, 11, 51386], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 461, "seek": 106828, "start": 1088.72, "end": 1091.24, "text": " comes from information that it feels that we acquire", "tokens": [51386, 1487, 490, 1589, 300, 309, 3417, 300, 321, 20001, 51512], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 462, "seek": 106828, "start": 1091.24, "end": 1092.8799999999999, "text": " from language in some way.", "tokens": [51512, 490, 2856, 294, 512, 636, 13, 51594], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 463, "seek": 106828, "start": 1093.84, "end": 1095.32, "text": " But how do we do that?", "tokens": [51642, 583, 577, 360, 321, 360, 300, 30, 51716], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 464, "seek": 106828, "start": 1095.32, "end": 1097.72, "text": " Well, I think one longstanding lens", "tokens": [51716, 1042, 11, 286, 519, 472, 938, 8618, 6765, 51836], "temperature": 0.0, "avg_logprob": -0.12007031924482706, "compression_ratio": 1.7357859531772575, "no_speech_prob": 0.0001794847776181996}, {"id": 465, "seek": 109772, "start": 1097.72, "end": 1099.72, "text": " for thinking about language", "tokens": [50364, 337, 1953, 466, 2856, 50464], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 466, "seek": 109772, "start": 1099.72, "end": 1103.52, "text": " that's kind of persisted before the LLM-based moment", "tokens": [50464, 300, 311, 733, 295, 13233, 292, 949, 264, 441, 43, 44, 12, 6032, 1623, 50654], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 467, "seek": 109772, "start": 1103.52, "end": 1105.4, "text": " is that what language is,", "tokens": [50654, 307, 300, 437, 2856, 307, 11, 50748], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 468, "seek": 109772, "start": 1105.4, "end": 1107.28, "text": " is this external symbolic medium", "tokens": [50748, 307, 341, 8320, 25755, 6399, 50842], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 469, "seek": 109772, "start": 1107.28, "end": 1109.3600000000001, "text": " for communicating human thoughts.", "tokens": [50842, 337, 17559, 1952, 4598, 13, 50946], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 470, "seek": 109772, "start": 1109.3600000000001, "end": 1110.6000000000001, "text": " And the way that it does that", "tokens": [50946, 400, 264, 636, 300, 309, 775, 300, 51008], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 471, "seek": 109772, "start": 1110.6000000000001, "end": 1113.3600000000001, "text": " is because there's some kind of general mapping function", "tokens": [51008, 307, 570, 456, 311, 512, 733, 295, 2674, 18350, 2445, 51146], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 472, "seek": 109772, "start": 1113.3600000000001, "end": 1115.68, "text": " from our internal representations of thought", "tokens": [51146, 490, 527, 6920, 33358, 295, 1194, 51262], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 473, "seek": 109772, "start": 1115.68, "end": 1118.72, "text": " into this external symbol system, that's language.", "tokens": [51262, 666, 341, 8320, 5986, 1185, 11, 300, 311, 2856, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 474, "seek": 109772, "start": 1118.72, "end": 1121.3600000000001, "text": " And so in this kind of older framework,", "tokens": [51414, 400, 370, 294, 341, 733, 295, 4906, 8388, 11, 51546], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 475, "seek": 109772, "start": 1121.3600000000001, "end": 1124.4, "text": " what it means to understand language or make meaning", "tokens": [51546, 437, 309, 1355, 281, 1223, 2856, 420, 652, 3620, 51698], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 476, "seek": 109772, "start": 1124.4, "end": 1127.52, "text": " means mapping back from external sentences that we hear", "tokens": [51698, 1355, 18350, 646, 490, 8320, 16579, 300, 321, 1568, 51854], "temperature": 0.0, "avg_logprob": -0.08814558188120523, "compression_ratio": 1.8703703703703705, "no_speech_prob": 0.00024528359062969685}, {"id": 477, "seek": 112752, "start": 1127.52, "end": 1130.44, "text": " into structured internal representations.", "tokens": [50364, 666, 18519, 6920, 33358, 13, 50510], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 478, "seek": 112752, "start": 1130.44, "end": 1132.6399999999999, "text": " And what we explore in this paper", "tokens": [50510, 400, 437, 321, 6839, 294, 341, 3035, 50620], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 479, "seek": 112752, "start": 1132.6399999999999, "end": 1136.8799999999999, "text": " is a general proposal that casts the meanings of language", "tokens": [50620, 307, 257, 2674, 11494, 300, 41921, 264, 28138, 295, 2856, 50832], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 480, "seek": 112752, "start": 1136.8799999999999, "end": 1139.8799999999999, "text": " as these mappings or probabilistic distributions", "tokens": [50832, 382, 613, 463, 28968, 420, 31959, 3142, 37870, 50982], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 481, "seek": 112752, "start": 1139.8799999999999, "end": 1142.96, "text": " over expressions in a probabilistic programming language.", "tokens": [50982, 670, 15277, 294, 257, 31959, 3142, 9410, 2856, 13, 51136], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 482, "seek": 112752, "start": 1142.96, "end": 1145.2, "text": " And I'm gonna come back after some concrete examples", "tokens": [51136, 400, 286, 478, 799, 808, 646, 934, 512, 9859, 5110, 51248], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 483, "seek": 112752, "start": 1145.2, "end": 1147.72, "text": " to how I think this relates to the conceptual rule semantics", "tokens": [51248, 281, 577, 286, 519, 341, 16155, 281, 264, 24106, 4978, 4361, 45298, 51374], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 484, "seek": 112752, "start": 1147.72, "end": 1149.48, "text": " that Steve articulated in the first talk,", "tokens": [51374, 300, 7466, 43322, 294, 264, 700, 751, 11, 51462], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 485, "seek": 112752, "start": 1149.48, "end": 1150.32, "text": " because I think there are actually", "tokens": [51462, 570, 286, 519, 456, 366, 767, 51504], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 486, "seek": 112752, "start": 1150.32, "end": 1151.6, "text": " some really deep connections here,", "tokens": [51504, 512, 534, 2452, 9271, 510, 11, 51568], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 487, "seek": 112752, "start": 1151.6, "end": 1154.56, "text": " the ways in which this might be one way to formalize", "tokens": [51568, 264, 2098, 294, 597, 341, 1062, 312, 472, 636, 281, 9860, 1125, 51716], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 488, "seek": 112752, "start": 1154.56, "end": 1156.16, "text": " or enrich some of those ideas.", "tokens": [51716, 420, 18849, 512, 295, 729, 3487, 13, 51796], "temperature": 0.0, "avg_logprob": -0.0941760502164326, "compression_ratio": 1.7628205128205128, "no_speech_prob": 9.311218309449032e-05}, {"id": 489, "seek": 115616, "start": 1157.1200000000001, "end": 1159.68, "text": " And this architecture, this proposal here", "tokens": [50412, 400, 341, 9482, 11, 341, 11494, 510, 50540], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 490, "seek": 115616, "start": 1159.68, "end": 1161.72, "text": " also suggests how language can be integrated", "tokens": [50540, 611, 13409, 577, 2856, 393, 312, 10919, 50642], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 491, "seek": 115616, "start": 1161.72, "end": 1163.48, "text": " into a more general architecture,", "tokens": [50642, 666, 257, 544, 2674, 9482, 11, 50730], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 492, "seek": 115616, "start": 1163.48, "end": 1166.0800000000002, "text": " because it's one that already starts out with,", "tokens": [50730, 570, 309, 311, 472, 300, 1217, 3719, 484, 365, 11, 50860], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 493, "seek": 115616, "start": 1166.0800000000002, "end": 1167.28, "text": " as Alex mentioned,", "tokens": [50860, 382, 5202, 2835, 11, 50920], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 494, "seek": 115616, "start": 1167.28, "end": 1169.48, "text": " some kind of existing internal modeling language,", "tokens": [50920, 512, 733, 295, 6741, 6920, 15983, 2856, 11, 51030], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 495, "seek": 115616, "start": 1169.48, "end": 1172.44, "text": " whose goal is to represent the world probabilistically,", "tokens": [51030, 6104, 3387, 307, 281, 2906, 264, 1002, 31959, 20458, 11, 51178], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 496, "seek": 115616, "start": 1172.44, "end": 1174.5600000000002, "text": " query those models and specify what it means", "tokens": [51178, 14581, 729, 5245, 293, 16500, 437, 309, 1355, 51284], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 497, "seek": 115616, "start": 1174.5600000000002, "end": 1176.88, "text": " to draw coherent inferences over them.", "tokens": [51284, 281, 2642, 36239, 13596, 2667, 670, 552, 13, 51400], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 498, "seek": 115616, "start": 1176.88, "end": 1178.88, "text": " And it also suggests, I think, a framework", "tokens": [51400, 400, 309, 611, 13409, 11, 286, 519, 11, 257, 8388, 51500], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 499, "seek": 115616, "start": 1178.88, "end": 1180.92, "text": " for formally modeling the content", "tokens": [51500, 337, 25983, 15983, 264, 2701, 51602], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 500, "seek": 115616, "start": 1180.92, "end": 1183.0400000000002, "text": " of different kinds of sentences in language", "tokens": [51602, 295, 819, 3685, 295, 16579, 294, 2856, 51708], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 501, "seek": 115616, "start": 1183.0400000000002, "end": 1184.96, "text": " as the kinds of probabilistic programming expressions", "tokens": [51708, 382, 264, 3685, 295, 31959, 3142, 9410, 15277, 51804], "temperature": 0.0, "avg_logprob": -0.10261448602827769, "compression_ratio": 1.7717041800643087, "no_speech_prob": 7.720063149463385e-05}, {"id": 502, "seek": 118496, "start": 1185.04, "end": 1186.32, "text": " that they might map into.", "tokens": [50368, 300, 436, 1062, 4471, 666, 13, 50432], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 503, "seek": 118496, "start": 1188.32, "end": 1192.6000000000001, "text": " So, and so in this paper,", "tokens": [50532, 407, 11, 293, 370, 294, 341, 3035, 11, 50746], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 504, "seek": 118496, "start": 1192.6000000000001, "end": 1194.64, "text": " we begin by exploring how this proposal", "tokens": [50746, 321, 1841, 538, 12736, 577, 341, 11494, 50848], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 505, "seek": 118496, "start": 1194.64, "end": 1196.92, "text": " might be instantiated with respect to", "tokens": [50848, 1062, 312, 9836, 72, 770, 365, 3104, 281, 50962], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 506, "seek": 118496, "start": 1196.92, "end": 1199.2, "text": " a bunch of different domains of reasoning,", "tokens": [50962, 257, 3840, 295, 819, 25514, 295, 21577, 11, 51076], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 507, "seek": 118496, "start": 1200.56, "end": 1203.52, "text": " sorry, including general probabilistic reasoning,", "tokens": [51144, 2597, 11, 3009, 2674, 31959, 3142, 21577, 11, 51292], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 508, "seek": 118496, "start": 1203.52, "end": 1205.16, "text": " but also reasoning about relations", "tokens": [51292, 457, 611, 21577, 466, 2299, 51374], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 509, "seek": 118496, "start": 1205.16, "end": 1207.3600000000001, "text": " or physics and social situations.", "tokens": [51374, 420, 10649, 293, 2093, 6851, 13, 51484], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 510, "seek": 118496, "start": 1207.3600000000001, "end": 1210.1200000000001, "text": " And in all of these, we're gonna propose", "tokens": [51484, 400, 294, 439, 295, 613, 11, 321, 434, 799, 17421, 51622], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 511, "seek": 118496, "start": 1210.1200000000001, "end": 1211.28, "text": " that we might think about the language", "tokens": [51622, 300, 321, 1062, 519, 466, 264, 2856, 51680], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 512, "seek": 118496, "start": 1211.28, "end": 1213.3600000000001, "text": " that communicates general conceptual knowledge", "tokens": [51680, 300, 3363, 1024, 2674, 24106, 3601, 51784], "temperature": 0.0, "avg_logprob": -0.13495083225583568, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.00035683915484696627}, {"id": 513, "seek": 121336, "start": 1213.36, "end": 1216.52, "text": " about the world definitions or causal knowledge", "tokens": [50364, 466, 264, 1002, 21988, 420, 38755, 3601, 50522], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 514, "seek": 121336, "start": 1216.52, "end": 1218.8799999999999, "text": " as constructing these probabilistic expressions", "tokens": [50522, 382, 39969, 613, 31959, 3142, 15277, 50640], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 515, "seek": 121336, "start": 1218.8799999999999, "end": 1222.1599999999999, "text": " that are those that build up probabilistic generative models.", "tokens": [50640, 300, 366, 729, 300, 1322, 493, 31959, 3142, 1337, 1166, 5245, 13, 50804], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 516, "seek": 121336, "start": 1222.1599999999999, "end": 1225.9599999999998, "text": " And then in this framework, observations in the language,", "tokens": [50804, 400, 550, 294, 341, 8388, 11, 18163, 294, 264, 2856, 11, 50994], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 517, "seek": 121336, "start": 1225.9599999999998, "end": 1229.1599999999999, "text": " like there is at least one red mug in the scene", "tokens": [50994, 411, 456, 307, 412, 1935, 472, 2182, 23610, 294, 264, 4145, 51154], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 518, "seek": 121336, "start": 1229.1599999999999, "end": 1231.6799999999998, "text": " or Charlie is Dana's grandfather,", "tokens": [51154, 420, 13754, 307, 23759, 311, 14754, 11, 51280], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 519, "seek": 121336, "start": 1231.6799999999998, "end": 1234.12, "text": " construct formal conditioning statements,", "tokens": [51280, 7690, 9860, 21901, 12363, 11, 51402], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 520, "seek": 121336, "start": 1234.12, "end": 1236.9199999999998, "text": " which update the state of this probabilistic model.", "tokens": [51402, 597, 5623, 264, 1785, 295, 341, 31959, 3142, 2316, 13, 51542], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 521, "seek": 121336, "start": 1236.9199999999998, "end": 1239.4799999999998, "text": " And then questions map into query expressions", "tokens": [51542, 400, 550, 1651, 4471, 666, 14581, 15277, 51670], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 522, "seek": 121336, "start": 1239.4799999999998, "end": 1242.32, "text": " that specify the formal target of probabilistic inference", "tokens": [51670, 300, 16500, 264, 9860, 3779, 295, 31959, 3142, 38253, 51812], "temperature": 0.0, "avg_logprob": -0.09883667598260898, "compression_ratio": 1.8401486988847584, "no_speech_prob": 0.0005882587283849716}, {"id": 523, "seek": 124232, "start": 1242.32, "end": 1243.72, "text": " with respect to a model.", "tokens": [50364, 365, 3104, 281, 257, 2316, 13, 50434], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 524, "seek": 124232, "start": 1244.8799999999999, "end": 1246.12, "text": " And in this framework,", "tokens": [50492, 400, 294, 341, 8388, 11, 50554], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 525, "seek": 124232, "start": 1246.12, "end": 1249.76, "text": " we're thinking essentially cast as probabilistic reasoning.", "tokens": [50554, 321, 434, 1953, 4476, 4193, 382, 31959, 3142, 21577, 13, 50736], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 526, "seek": 124232, "start": 1249.76, "end": 1252.12, "text": " We suggest that another way that we can think about", "tokens": [50736, 492, 3402, 300, 1071, 636, 300, 321, 393, 519, 466, 50854], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 527, "seek": 124232, "start": 1252.12, "end": 1253.72, "text": " the role of a language model,", "tokens": [50854, 264, 3090, 295, 257, 2856, 2316, 11, 50934], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 528, "seek": 124232, "start": 1253.72, "end": 1255.1599999999999, "text": " one that's much smaller,", "tokens": [50934, 472, 300, 311, 709, 4356, 11, 51006], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 529, "seek": 124232, "start": 1255.1599999999999, "end": 1256.8, "text": " like the language network in the brain", "tokens": [51006, 411, 264, 2856, 3209, 294, 264, 3567, 51088], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 530, "seek": 124232, "start": 1256.8, "end": 1258.8, "text": " is actually as a means of instantiating", "tokens": [51088, 307, 767, 382, 257, 1355, 295, 9836, 72, 990, 51188], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 531, "seek": 124232, "start": 1258.8, "end": 1260.48, "text": " this meaning function in a way that", "tokens": [51188, 341, 3620, 2445, 294, 257, 636, 300, 51272], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 532, "seek": 124232, "start": 1260.48, "end": 1262.6799999999998, "text": " we've really never had before", "tokens": [51272, 321, 600, 534, 1128, 632, 949, 51382], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 533, "seek": 124232, "start": 1262.6799999999998, "end": 1265.24, "text": " to protect these kinds of context specific", "tokens": [51382, 281, 2371, 613, 3685, 295, 4319, 2685, 51510], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 534, "seek": 124232, "start": 1265.24, "end": 1267.4399999999998, "text": " and previous discourse conditioned mappings", "tokens": [51510, 293, 3894, 23938, 35833, 463, 28968, 51620], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 535, "seek": 124232, "start": 1267.4399999999998, "end": 1269.6399999999999, "text": " from sentences in natural language", "tokens": [51620, 490, 16579, 294, 3303, 2856, 51730], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 536, "seek": 124232, "start": 1269.6399999999999, "end": 1271.52, "text": " into distributions over expressions", "tokens": [51730, 666, 37870, 670, 15277, 51824], "temperature": 0.0, "avg_logprob": -0.09952069854736328, "compression_ratio": 1.7233333333333334, "no_speech_prob": 6.813773507019505e-05}, {"id": 537, "seek": 127152, "start": 1271.52, "end": 1274.12, "text": " that convey meaning in a probabilistic programming language.", "tokens": [50364, 300, 16965, 3620, 294, 257, 31959, 3142, 9410, 2856, 13, 50494], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 538, "seek": 127152, "start": 1276.04, "end": 1279.32, "text": " And so as a part of this long running disclaimer,", "tokens": [50590, 400, 370, 382, 257, 644, 295, 341, 938, 2614, 40896, 11, 50754], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 539, "seek": 127152, "start": 1279.32, "end": 1280.8799999999999, "text": " what I'm gonna be showing is a really minimal", "tokens": [50754, 437, 286, 478, 799, 312, 4099, 307, 257, 534, 13206, 50832], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 540, "seek": 127152, "start": 1280.8799999999999, "end": 1282.8799999999999, "text": " implementation of this framework,", "tokens": [50832, 11420, 295, 341, 8388, 11, 50932], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 541, "seek": 127152, "start": 1282.8799999999999, "end": 1285.24, "text": " but really intended as a pointer to different directions", "tokens": [50932, 457, 534, 10226, 382, 257, 23918, 281, 819, 11095, 51050], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 542, "seek": 127152, "start": 1285.24, "end": 1286.8, "text": " with which we might scale this approach", "tokens": [51050, 365, 597, 321, 1062, 4373, 341, 3109, 51128], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 543, "seek": 127152, "start": 1286.8, "end": 1288.8799999999999, "text": " to implement a more general interface between language", "tokens": [51128, 281, 4445, 257, 544, 2674, 9226, 1296, 2856, 51232], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 544, "seek": 127152, "start": 1288.8799999999999, "end": 1291.0, "text": " and arrange a different core cognitive domains.", "tokens": [51232, 293, 9424, 257, 819, 4965, 15605, 25514, 13, 51338], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 545, "seek": 127152, "start": 1291.0, "end": 1292.12, "text": " So just to be clear,", "tokens": [51338, 407, 445, 281, 312, 1850, 11, 51394], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 546, "seek": 127152, "start": 1292.12, "end": 1294.76, "text": " concretely in the examples that you see next,", "tokens": [51394, 39481, 736, 294, 264, 5110, 300, 291, 536, 958, 11, 51526], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 547, "seek": 127152, "start": 1294.76, "end": 1297.24, "text": " our meaning function is gonna be implemented using codex,", "tokens": [51526, 527, 3620, 2445, 307, 799, 312, 12270, 1228, 3089, 87, 11, 51650], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 548, "seek": 127152, "start": 1297.24, "end": 1300.24, "text": " right, an open AM model much smaller", "tokens": [51650, 558, 11, 364, 1269, 6475, 2316, 709, 4356, 51800], "temperature": 0.0, "avg_logprob": -0.16775126789891442, "compression_ratio": 1.7304075235109717, "no_speech_prob": 7.720934081589803e-05}, {"id": 549, "seek": 130024, "start": 1300.24, "end": 1301.52, "text": " than the state of the art right now", "tokens": [50364, 813, 264, 1785, 295, 264, 1523, 558, 586, 50428], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 550, "seek": 130024, "start": 1301.52, "end": 1302.96, "text": " that's trained to learn joint distributions", "tokens": [50428, 300, 311, 8895, 281, 1466, 7225, 37870, 50500], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 551, "seek": 130024, "start": 1302.96, "end": 1304.44, "text": " over language and code.", "tokens": [50500, 670, 2856, 293, 3089, 13, 50574], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 552, "seek": 130024, "start": 1304.44, "end": 1306.08, "text": " And the probabilistic programming language", "tokens": [50574, 400, 264, 31959, 3142, 9410, 2856, 50656], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 553, "seek": 130024, "start": 1306.08, "end": 1307.48, "text": " we're gonna show is church,", "tokens": [50656, 321, 434, 799, 855, 307, 4128, 11, 50726], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 554, "seek": 130024, "start": 1307.48, "end": 1310.92, "text": " which is this very simple probabilistic programming language", "tokens": [50726, 597, 307, 341, 588, 2199, 31959, 3142, 9410, 2856, 50898], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 555, "seek": 130024, "start": 1310.92, "end": 1312.44, "text": " that supports kind of very general", "tokens": [50898, 300, 9346, 733, 295, 588, 2674, 50974], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 556, "seek": 130024, "start": 1312.44, "end": 1314.24, "text": " sample based inference procedures.", "tokens": [50974, 6889, 2361, 38253, 13846, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 557, "seek": 130024, "start": 1314.24, "end": 1316.36, "text": " And our goal is to demonstrate how this framework", "tokens": [51064, 400, 527, 3387, 307, 281, 11698, 577, 341, 8388, 51170], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 558, "seek": 130024, "start": 1316.36, "end": 1318.2, "text": " might broadly interface between language", "tokens": [51170, 1062, 19511, 9226, 1296, 2856, 51262], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 559, "seek": 130024, "start": 1318.2, "end": 1320.52, "text": " and a bunch of different core cognitive domains.", "tokens": [51262, 293, 257, 3840, 295, 819, 4965, 15605, 25514, 13, 51378], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 560, "seek": 130024, "start": 1321.64, "end": 1323.84, "text": " So first to illustrate the basic sense", "tokens": [51434, 407, 700, 281, 23221, 264, 3875, 2020, 51544], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 561, "seek": 130024, "start": 1323.84, "end": 1326.28, "text": " in which a proposal like this might allow language", "tokens": [51544, 294, 597, 257, 11494, 411, 341, 1062, 2089, 2856, 51666], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 562, "seek": 130024, "start": 1326.28, "end": 1329.44, "text": " to update an agent's beliefs and query a world model,", "tokens": [51666, 281, 5623, 364, 9461, 311, 13585, 293, 14581, 257, 1002, 2316, 11, 51824], "temperature": 0.0, "avg_logprob": -0.10712087688161366, "compression_ratio": 1.784848484848485, "no_speech_prob": 4.8317353503080085e-05}, {"id": 563, "seek": 132944, "start": 1329.44, "end": 1331.48, "text": " I'm gonna begin with a really simple toy example", "tokens": [50364, 286, 478, 799, 1841, 365, 257, 534, 2199, 12058, 1365, 50466], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 564, "seek": 132944, "start": 1331.48, "end": 1333.16, "text": " that actually draws on a bunch of prior", "tokens": [50466, 300, 767, 20045, 322, 257, 3840, 295, 4059, 50550], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 565, "seek": 132944, "start": 1333.16, "end": 1335.16, "text": " cognitive science experiments", "tokens": [50550, 15605, 3497, 12050, 50650], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 566, "seek": 132944, "start": 1335.16, "end": 1338.68, "text": " in which real people were asked to draw various inferences", "tokens": [50650, 294, 597, 957, 561, 645, 2351, 281, 2642, 3683, 13596, 2667, 50826], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 567, "seek": 132944, "start": 1338.68, "end": 1341.16, "text": " about which teams of players might win different games", "tokens": [50826, 466, 597, 5491, 295, 4150, 1062, 1942, 819, 2813, 50950], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 568, "seek": 132944, "start": 1341.16, "end": 1342.8400000000001, "text": " of tug of war based on the games", "tokens": [50950, 295, 33543, 295, 1516, 2361, 322, 264, 2813, 51034], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 569, "seek": 132944, "start": 1342.8400000000001, "end": 1345.3600000000001, "text": " that you'd previously seen players win or lose.", "tokens": [51034, 300, 291, 1116, 8046, 1612, 4150, 1942, 420, 3624, 13, 51160], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 570, "seek": 132944, "start": 1345.3600000000001, "end": 1347.92, "text": " And so this is older work from Josh's group", "tokens": [51160, 400, 370, 341, 307, 4906, 589, 490, 9785, 311, 1594, 51288], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 571, "seek": 132944, "start": 1347.92, "end": 1350.56, "text": " that demonstrated I think the sense in which", "tokens": [51288, 300, 18772, 286, 519, 264, 2020, 294, 597, 51420], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 572, "seek": 132944, "start": 1350.56, "end": 1352.64, "text": " this normative model, this Norvig model", "tokens": [51420, 341, 2026, 1166, 2316, 11, 341, 6966, 85, 328, 2316, 51524], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 573, "seek": 132944, "start": 1352.64, "end": 1354.96, "text": " of probabilistic inference actually in many ways", "tokens": [51524, 295, 31959, 3142, 38253, 767, 294, 867, 2098, 51640], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 574, "seek": 132944, "start": 1354.96, "end": 1357.72, "text": " predicts the actual behaviors and predictions made by humans", "tokens": [51640, 6069, 82, 264, 3539, 15501, 293, 21264, 1027, 538, 6255, 51778], "temperature": 0.0, "avg_logprob": -0.12676019158982138, "compression_ratio": 1.7579617834394905, "no_speech_prob": 8.747958054300398e-05}, {"id": 575, "seek": 135772, "start": 1358.56, "end": 1360.56, "text": " using a very general probabilistic model", "tokens": [50406, 1228, 257, 588, 2674, 31959, 3142, 2316, 50506], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 576, "seek": 135772, "start": 1360.56, "end": 1363.1200000000001, "text": " of the mechanics of this tug of war game.", "tokens": [50506, 295, 264, 12939, 295, 341, 33543, 295, 1516, 1216, 13, 50634], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 577, "seek": 135772, "start": 1363.1200000000001, "end": 1365.84, "text": " And our goal here is to show how our framework", "tokens": [50634, 400, 527, 3387, 510, 307, 281, 855, 577, 527, 8388, 50770], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 578, "seek": 135772, "start": 1365.84, "end": 1368.28, "text": " we can implement an interface between natural language", "tokens": [50770, 321, 393, 4445, 364, 9226, 1296, 3303, 2856, 50892], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 579, "seek": 135772, "start": 1368.28, "end": 1371.88, "text": " and all of the core examples of this older experiment.", "tokens": [50892, 293, 439, 295, 264, 4965, 5110, 295, 341, 4906, 5120, 13, 51072], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 580, "seek": 135772, "start": 1371.88, "end": 1374.4, "text": " So just to go through here, I think what you're seeing", "tokens": [51072, 407, 445, 281, 352, 807, 510, 11, 286, 519, 437, 291, 434, 2577, 51198], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 581, "seek": 135772, "start": 1374.4, "end": 1376.6000000000001, "text": " in this little toy example, the world model", "tokens": [51198, 294, 341, 707, 12058, 1365, 11, 264, 1002, 2316, 51308], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 582, "seek": 135772, "start": 1376.6000000000001, "end": 1378.64, "text": " that's being defined on the screen", "tokens": [51308, 300, 311, 885, 7642, 322, 264, 2568, 51410], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 583, "seek": 135772, "start": 1378.64, "end": 1380.56, "text": " is capturing the basic causal relationship", "tokens": [51410, 307, 23384, 264, 3875, 38755, 2480, 51506], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 584, "seek": 135772, "start": 1380.56, "end": 1384.72, "text": " by which properties of different human players", "tokens": [51506, 538, 597, 7221, 295, 819, 1952, 4150, 51714], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 585, "seek": 135772, "start": 1384.72, "end": 1386.6000000000001, "text": " might influence the outcomes of different tournaments", "tokens": [51714, 1062, 6503, 264, 10070, 295, 819, 32004, 51808], "temperature": 0.0, "avg_logprob": -0.07890945723076828, "compression_ratio": 1.6895424836601307, "no_speech_prob": 0.00016861947369761765}, {"id": 586, "seek": 138660, "start": 1386.6399999999999, "end": 1388.3999999999999, "text": " that they play in tug of war.", "tokens": [50366, 300, 436, 862, 294, 33543, 295, 1516, 13, 50454], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 587, "seek": 138660, "start": 1388.3999999999999, "end": 1392.4399999999998, "text": " So for instance, here we're modeling players", "tokens": [50454, 407, 337, 5197, 11, 510, 321, 434, 15983, 4150, 50656], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 588, "seek": 138660, "start": 1392.4399999999998, "end": 1396.36, "text": " as having some internal inherent strength value", "tokens": [50656, 382, 1419, 512, 6920, 26387, 3800, 2158, 50852], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 589, "seek": 138660, "start": 1396.36, "end": 1398.36, "text": " where strength varies approximately normally", "tokens": [50852, 689, 3800, 21716, 10447, 5646, 50952], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 590, "seek": 138660, "start": 1398.36, "end": 1400.1999999999998, "text": " but as this unobserved latent variable", "tokens": [50952, 457, 382, 341, 8526, 929, 6913, 48994, 7006, 51044], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 591, "seek": 138660, "start": 1400.1999999999998, "end": 1401.8799999999999, "text": " over different kinds of players.", "tokens": [51044, 670, 819, 3685, 295, 4150, 13, 51128], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 592, "seek": 138660, "start": 1401.8799999999999, "end": 1404.1999999999998, "text": " And we also think of players as having", "tokens": [51128, 400, 321, 611, 519, 295, 4150, 382, 1419, 51244], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 593, "seek": 138660, "start": 1404.1999999999998, "end": 1406.6, "text": " some kind of internal laziness value", "tokens": [51244, 512, 733, 295, 6920, 19320, 1324, 2158, 51364], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 594, "seek": 138660, "start": 1406.6, "end": 1408.3999999999999, "text": " which represents the percentage of the time", "tokens": [51364, 597, 8855, 264, 9668, 295, 264, 565, 51454], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 595, "seek": 138660, "start": 1408.3999999999999, "end": 1409.4399999999998, "text": " that they actually don't act", "tokens": [51454, 300, 436, 767, 500, 380, 605, 51506], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 596, "seek": 138660, "start": 1409.4399999999998, "end": 1412.0, "text": " according to their underlying strength.", "tokens": [51506, 4650, 281, 641, 14217, 3800, 13, 51634], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 597, "seek": 138660, "start": 1412.0, "end": 1415.0, "text": " And how did these variables determine the outcomes", "tokens": [51634, 400, 577, 630, 613, 9102, 6997, 264, 10070, 51784], "temperature": 0.0, "avg_logprob": -0.11293015179333386, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.0006260625086724758}, {"id": 598, "seek": 141500, "start": 1415.0, "end": 1417.92, "text": " that we observe of given games of tug of war?", "tokens": [50364, 300, 321, 11441, 295, 2212, 2813, 295, 33543, 295, 1516, 30, 50510], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 599, "seek": 141500, "start": 1417.92, "end": 1420.28, "text": " Well, the strength of a whole team of players", "tokens": [50510, 1042, 11, 264, 3800, 295, 257, 1379, 1469, 295, 4150, 50628], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 600, "seek": 141500, "start": 1420.28, "end": 1424.08, "text": " depends on the cumulative sum of its player strengths.", "tokens": [50628, 5946, 322, 264, 38379, 2408, 295, 1080, 4256, 16986, 13, 50818], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 601, "seek": 141500, "start": 1424.08, "end": 1426.28, "text": " But if a player is deciding to be lazy in this game,", "tokens": [50818, 583, 498, 257, 4256, 307, 17990, 281, 312, 14847, 294, 341, 1216, 11, 50928], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 602, "seek": 141500, "start": 1426.28, "end": 1428.04, "text": " they might not pull as hard as they could.", "tokens": [50928, 436, 1062, 406, 2235, 382, 1152, 382, 436, 727, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 603, "seek": 141500, "start": 1428.04, "end": 1429.72, "text": " And whichever team pulls with the most strength", "tokens": [51016, 400, 24123, 1469, 16982, 365, 264, 881, 3800, 51100], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 604, "seek": 141500, "start": 1429.72, "end": 1432.56, "text": " in a given match is going to win that match, yeah.", "tokens": [51100, 294, 257, 2212, 2995, 307, 516, 281, 1942, 300, 2995, 11, 1338, 13, 51242], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 605, "seek": 141500, "start": 1432.56, "end": 1435.08, "text": " Did you design the primitives of strength and laziness", "tokens": [51242, 2589, 291, 1715, 264, 2886, 38970, 295, 3800, 293, 19320, 1324, 51368], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 606, "seek": 141500, "start": 1435.08, "end": 1437.4, "text": " or a codex come up with the primitives themselves?", "tokens": [51368, 420, 257, 3089, 87, 808, 493, 365, 264, 2886, 38970, 2969, 30, 51484], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 607, "seek": 141500, "start": 1437.4, "end": 1439.68, "text": " So in this one, we're looking at a model", "tokens": [51484, 407, 294, 341, 472, 11, 321, 434, 1237, 412, 257, 2316, 51598], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 608, "seek": 141500, "start": 1439.68, "end": 1440.72, "text": " that's derived from the older work.", "tokens": [51598, 300, 311, 18949, 490, 264, 4906, 589, 13, 51650], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 609, "seek": 141500, "start": 1440.72, "end": 1443.2, "text": " So these are designed, but yes, that's later in this work.", "tokens": [51650, 407, 613, 366, 4761, 11, 457, 2086, 11, 300, 311, 1780, 294, 341, 589, 13, 51774], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 610, "seek": 141500, "start": 1443.2, "end": 1444.16, "text": " We're gonna show some examples", "tokens": [51774, 492, 434, 799, 855, 512, 5110, 51822], "temperature": 0.0, "avg_logprob": -0.16656008111425194, "compression_ratio": 1.7900874635568513, "no_speech_prob": 0.0012443703599274158}, {"id": 611, "seek": 144416, "start": 1444.16, "end": 1445.72, "text": " of how you can learn this kind of model", "tokens": [50364, 295, 577, 291, 393, 1466, 341, 733, 295, 2316, 50442], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 612, "seek": 144416, "start": 1445.72, "end": 1447.24, "text": " from someone just talking about it in language", "tokens": [50442, 490, 1580, 445, 1417, 466, 309, 294, 2856, 50518], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 613, "seek": 144416, "start": 1447.24, "end": 1449.72, "text": " like the definition that I just gave.", "tokens": [50518, 411, 264, 7123, 300, 286, 445, 2729, 13, 50642], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 614, "seek": 144416, "start": 1449.72, "end": 1450.92, "text": " Right, and so again, you know,", "tokens": [50642, 1779, 11, 293, 370, 797, 11, 291, 458, 11, 50702], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 615, "seek": 144416, "start": 1450.92, "end": 1452.2, "text": " this is a really simple example,", "tokens": [50702, 341, 307, 257, 534, 2199, 1365, 11, 50766], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 616, "seek": 144416, "start": 1452.2, "end": 1454.2, "text": " but I think also one that actually captures", "tokens": [50766, 457, 286, 519, 611, 472, 300, 767, 27986, 50866], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 617, "seek": 144416, "start": 1454.2, "end": 1456.52, "text": " a surprising amount of the basic causal knowledge", "tokens": [50866, 257, 8830, 2372, 295, 264, 3875, 38755, 3601, 50982], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 618, "seek": 144416, "start": 1456.52, "end": 1458.28, "text": " that people have if you tell them", "tokens": [50982, 300, 561, 362, 498, 291, 980, 552, 51070], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 619, "seek": 144416, "start": 1458.28, "end": 1460.24, "text": " that you're gonna be listening to tug of war games,", "tokens": [51070, 300, 291, 434, 799, 312, 4764, 281, 33543, 295, 1516, 2813, 11, 51168], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 620, "seek": 144416, "start": 1460.24, "end": 1461.76, "text": " but sometimes people can be lazy", "tokens": [51168, 457, 2171, 561, 393, 312, 14847, 51244], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 621, "seek": 144416, "start": 1461.76, "end": 1463.44, "text": " and not pull as hard as they could.", "tokens": [51244, 293, 406, 2235, 382, 1152, 382, 436, 727, 13, 51328], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 622, "seek": 144416, "start": 1463.44, "end": 1466.48, "text": " So how do we go about relating language in this domain?", "tokens": [51328, 407, 577, 360, 321, 352, 466, 23968, 2856, 294, 341, 9274, 30, 51480], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 623, "seek": 144416, "start": 1467.68, "end": 1471.2, "text": " Right, well, one means by which we can induce", "tokens": [51540, 1779, 11, 731, 11, 472, 1355, 538, 597, 321, 393, 41263, 51716], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 624, "seek": 144416, "start": 1471.2, "end": 1473.1200000000001, "text": " a simple notion of a meaning function", "tokens": [51716, 257, 2199, 10710, 295, 257, 3620, 2445, 51812], "temperature": 0.0, "avg_logprob": -0.10834782328826702, "compression_ratio": 1.7484848484848485, "no_speech_prob": 0.00032496414496563375}, {"id": 625, "seek": 147312, "start": 1473.12, "end": 1475.4399999999998, "text": " that actually fits the definition we just gave", "tokens": [50364, 300, 767, 9001, 264, 7123, 321, 445, 2729, 50480], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 626, "seek": 147312, "start": 1475.4399999999998, "end": 1477.36, "text": " is by conditioning a language model", "tokens": [50480, 307, 538, 21901, 257, 2856, 2316, 50576], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 627, "seek": 147312, "start": 1477.36, "end": 1480.84, "text": " both on this context-specific generative world model", "tokens": [50576, 1293, 322, 341, 4319, 12, 29258, 1337, 1166, 1002, 2316, 50750], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 628, "seek": 147312, "start": 1480.84, "end": 1484.0, "text": " and on a few examples showing how language is mapped", "tokens": [50750, 293, 322, 257, 1326, 5110, 4099, 577, 2856, 307, 33318, 50908], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 629, "seek": 147312, "start": 1484.0, "end": 1486.56, "text": " into sampled probabilistic programming expressions", "tokens": [50908, 666, 3247, 15551, 31959, 3142, 9410, 15277, 51036], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 630, "seek": 147312, "start": 1486.56, "end": 1487.4599999999998, "text": " in this domain.", "tokens": [51036, 294, 341, 9274, 13, 51081], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 631, "seek": 147312, "start": 1488.8, "end": 1490.52, "text": " And what we've done now, right,", "tokens": [51148, 400, 437, 321, 600, 1096, 586, 11, 558, 11, 51234], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 632, "seek": 147312, "start": 1490.52, "end": 1494.56, "text": " is effectively induce this kind of situation-specific", "tokens": [51234, 307, 8659, 41263, 341, 733, 295, 2590, 12, 29258, 51436], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 633, "seek": 147312, "start": 1494.56, "end": 1497.6799999999998, "text": " contextual mapping from arbitrary new sentences", "tokens": [51436, 35526, 18350, 490, 23211, 777, 16579, 51592], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 634, "seek": 147312, "start": 1497.6799999999998, "end": 1499.0, "text": " to expressions that conditions", "tokens": [51592, 281, 15277, 300, 4487, 51658], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 635, "seek": 147312, "start": 1499.0, "end": 1502.6799999999998, "text": " both on the general prior distribution that codex is", "tokens": [51658, 1293, 322, 264, 2674, 4059, 7316, 300, 3089, 87, 307, 51842], "temperature": 0.0, "avg_logprob": -0.14502346181423864, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.00011590091162361205}, {"id": 636, "seek": 150268, "start": 1502.68, "end": 1504.48, "text": " over language and code,", "tokens": [50364, 670, 2856, 293, 3089, 11, 50454], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 637, "seek": 150268, "start": 1504.48, "end": 1507.3200000000002, "text": " and this kind of specific discourse thinking context", "tokens": [50454, 293, 341, 733, 295, 2685, 23938, 1953, 4319, 50596], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 638, "seek": 150268, "start": 1507.3200000000002, "end": 1510.72, "text": " of how language is being used in this situation.", "tokens": [50596, 295, 577, 2856, 307, 885, 1143, 294, 341, 2590, 13, 50766], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 639, "seek": 150268, "start": 1510.72, "end": 1512.68, "text": " And there are clearly other ways to do this,", "tokens": [50766, 400, 456, 366, 4448, 661, 2098, 281, 360, 341, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 640, "seek": 150268, "start": 1512.68, "end": 1514.0800000000002, "text": " some of which we'll talk about later,", "tokens": [50864, 512, 295, 597, 321, 603, 751, 466, 1780, 11, 50934], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 641, "seek": 150268, "start": 1514.0800000000002, "end": 1516.64, "text": " but we're using this example to illustrate", "tokens": [50934, 457, 321, 434, 1228, 341, 1365, 281, 23221, 51062], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 642, "seek": 150268, "start": 1516.64, "end": 1517.66, "text": " just how much you might be able to do", "tokens": [51062, 445, 577, 709, 291, 1062, 312, 1075, 281, 360, 51113], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 643, "seek": 150268, "start": 1517.66, "end": 1519.54, "text": " with this kind of minimal implementation,", "tokens": [51113, 365, 341, 733, 295, 13206, 11420, 11, 51207], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 644, "seek": 150268, "start": 1519.54, "end": 1520.96, "text": " a notion of a model that translates", "tokens": [51207, 257, 10710, 295, 257, 2316, 300, 28468, 51278], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 645, "seek": 150268, "start": 1520.96, "end": 1522.8, "text": " between language to code.", "tokens": [51278, 1296, 2856, 281, 3089, 13, 51370], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 646, "seek": 150268, "start": 1522.8, "end": 1526.26, "text": " Right, so what kinds of language might we say here", "tokens": [51370, 1779, 11, 370, 437, 3685, 295, 2856, 1062, 321, 584, 510, 51543], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 647, "seek": 150268, "start": 1526.26, "end": 1527.52, "text": " and how might we think about them", "tokens": [51543, 293, 577, 1062, 321, 519, 466, 552, 51606], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 648, "seek": 150268, "start": 1527.52, "end": 1529.64, "text": " in relation to probabilistic programming expressions?", "tokens": [51606, 294, 9721, 281, 31959, 3142, 9410, 15277, 30, 51712], "temperature": 0.0, "avg_logprob": -0.10453021165096399, "compression_ratio": 1.7792642140468227, "no_speech_prob": 0.00010228415339952335}, {"id": 649, "seek": 152964, "start": 1529.64, "end": 1534.0800000000002, "text": " Well, a general proposition, like Josh won against Leo,", "tokens": [50364, 1042, 11, 257, 2674, 24830, 11, 411, 9785, 1582, 1970, 19344, 11, 50586], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 650, "seek": 152964, "start": 1534.0800000000002, "end": 1537.0, "text": " gets translated into or might map,", "tokens": [50586, 2170, 16805, 666, 420, 1062, 4471, 11, 50732], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 651, "seek": 152964, "start": 1537.0, "end": 1540.44, "text": " we might think of mapping or meaning a conditioned statement,", "tokens": [50732, 321, 1062, 519, 295, 18350, 420, 3620, 257, 35833, 5629, 11, 50904], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 652, "seek": 152964, "start": 1540.44, "end": 1542.88, "text": " an observation that Josh won against Leo.", "tokens": [50904, 364, 14816, 300, 9785, 1582, 1970, 19344, 13, 51026], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 653, "seek": 152964, "start": 1543.74, "end": 1545.5200000000002, "text": " If we make subsequent observations,", "tokens": [51069, 759, 321, 652, 19962, 18163, 11, 51158], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 654, "seek": 152964, "start": 1545.5200000000002, "end": 1549.2, "text": " like then Josh went on to claim victory against Alex,", "tokens": [51158, 411, 550, 9785, 1437, 322, 281, 3932, 9812, 1970, 5202, 11, 51342], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 655, "seek": 152964, "start": 1549.2, "end": 1551.8400000000001, "text": " we can continue to kind of generally use this", "tokens": [51342, 321, 393, 2354, 281, 733, 295, 5101, 764, 341, 51474], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 656, "seek": 152964, "start": 1551.8400000000001, "end": 1554.64, "text": " meaning function that we've induced to turn that", "tokens": [51474, 3620, 2445, 300, 321, 600, 33991, 281, 1261, 300, 51614], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 657, "seek": 152964, "start": 1554.64, "end": 1556.4, "text": " into a probabilistic programming language", "tokens": [51614, 666, 257, 31959, 3142, 9410, 2856, 51702], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 658, "seek": 152964, "start": 1556.4, "end": 1558.5400000000002, "text": " that captures the fact that Josh won against Alex.", "tokens": [51702, 300, 27986, 264, 1186, 300, 9785, 1582, 1970, 5202, 13, 51809], "temperature": 0.0, "avg_logprob": -0.15194021688925252, "compression_ratio": 1.8294573643410852, "no_speech_prob": 6.814038351876661e-05}, {"id": 659, "seek": 155854, "start": 1558.58, "end": 1561.6599999999999, "text": " If we then say that even working together as a team,", "tokens": [50366, 759, 321, 550, 584, 300, 754, 1364, 1214, 382, 257, 1469, 11, 50520], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 660, "seek": 155854, "start": 1561.6599999999999, "end": 1563.74, "text": " Leo and Alex still couldn't beat Josh", "tokens": [50520, 19344, 293, 5202, 920, 2809, 380, 4224, 9785, 50624], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 661, "seek": 155854, "start": 1563.74, "end": 1565.26, "text": " in this game of tech of war.", "tokens": [50624, 294, 341, 1216, 295, 7553, 295, 1516, 13, 50700], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 662, "seek": 155854, "start": 1566.82, "end": 1569.1399999999999, "text": " At this point, if we want to answer a query,", "tokens": [50778, 1711, 341, 935, 11, 498, 321, 528, 281, 1867, 257, 14581, 11, 50894], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 663, "seek": 155854, "start": 1569.1399999999999, "end": 1572.02, "text": " like, okay, wait, how strong is Josh?", "tokens": [50894, 411, 11, 1392, 11, 1699, 11, 577, 2068, 307, 9785, 30, 51038], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 664, "seek": 155854, "start": 1572.94, "end": 1577.02, "text": " What we think of as thinking in this situation", "tokens": [51084, 708, 321, 519, 295, 382, 1953, 294, 341, 2590, 51288], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 665, "seek": 155854, "start": 1577.02, "end": 1580.86, "text": " is actually sampling from the posterior", "tokens": [51288, 307, 767, 21179, 490, 264, 33529, 51480], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 666, "seek": 155854, "start": 1580.86, "end": 1582.8999999999999, "text": " over possible worlds from the generative model", "tokens": [51480, 670, 1944, 13401, 490, 264, 1337, 1166, 2316, 51582], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 667, "seek": 155854, "start": 1582.8999999999999, "end": 1585.22, "text": " that we just defined, subject to the observations", "tokens": [51582, 300, 321, 445, 7642, 11, 3983, 281, 264, 18163, 51698], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 668, "seek": 155854, "start": 1585.22, "end": 1586.34, "text": " that we've just made.", "tokens": [51698, 300, 321, 600, 445, 1027, 13, 51754], "temperature": 0.0, "avg_logprob": -0.15791817398758623, "compression_ratio": 1.6, "no_speech_prob": 5.6486824178136885e-05}, {"id": 669, "seek": 158634, "start": 1586.34, "end": 1589.54, "text": " And indeed, that means that the meaning of a sentence", "tokens": [50364, 400, 6451, 11, 300, 1355, 300, 264, 3620, 295, 257, 8174, 50524], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 670, "seek": 158634, "start": 1589.54, "end": 1591.3799999999999, "text": " like how strong is Josh is really", "tokens": [50524, 411, 577, 2068, 307, 9785, 307, 534, 50616], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 671, "seek": 158634, "start": 1591.3799999999999, "end": 1594.4199999999998, "text": " a structured publicistic inference query.", "tokens": [50616, 257, 18519, 1908, 3142, 38253, 14581, 13, 50768], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 672, "seek": 158634, "start": 1594.4199999999998, "end": 1597.78, "text": " What is the latent variable that is Josh's strength?", "tokens": [50768, 708, 307, 264, 48994, 7006, 300, 307, 9785, 311, 3800, 30, 50936], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 673, "seek": 158634, "start": 1597.78, "end": 1600.4599999999998, "text": " And what we see here is that given his track record,", "tokens": [50936, 400, 437, 321, 536, 510, 307, 300, 2212, 702, 2837, 2136, 11, 51070], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 674, "seek": 158634, "start": 1600.4599999999998, "end": 1602.6599999999999, "text": " all these people that he's beating, even playing together,", "tokens": [51070, 439, 613, 561, 300, 415, 311, 13497, 11, 754, 2433, 1214, 11, 51180], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 675, "seek": 158634, "start": 1602.6599999999999, "end": 1604.58, "text": " our inference is that Josh is likely", "tokens": [51180, 527, 38253, 307, 300, 9785, 307, 3700, 51276], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 676, "seek": 158634, "start": 1604.58, "end": 1606.3799999999999, "text": " a good bit stronger than average.", "tokens": [51276, 257, 665, 857, 7249, 813, 4274, 13, 51366], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 677, "seek": 158634, "start": 1606.3799999999999, "end": 1608.1399999999999, "text": " And that also means coherently,", "tokens": [51366, 400, 300, 611, 1355, 26528, 2276, 11, 51454], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 678, "seek": 158634, "start": 1608.1399999999999, "end": 1610.1799999999998, "text": " we might expect that a priori,", "tokens": [51454, 321, 1062, 2066, 300, 257, 4059, 72, 11, 51556], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 679, "seek": 158634, "start": 1610.1799999999998, "end": 1612.82, "text": " a new player we've never seen, like Gabe,", "tokens": [51556, 257, 777, 4256, 321, 600, 1128, 1612, 11, 411, 39524, 11, 51688], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 680, "seek": 158634, "start": 1612.82, "end": 1614.58, "text": " is going to be unlikely to beat him.", "tokens": [51688, 307, 516, 281, 312, 17518, 281, 4224, 796, 13, 51776], "temperature": 0.0, "avg_logprob": -0.12380720067907262, "compression_ratio": 1.7422680412371134, "no_speech_prob": 8.749603148316965e-05}, {"id": 681, "seek": 161458, "start": 1615.58, "end": 1619.26, "text": " So if we ask what are the odds of Gabe at beating Josh,", "tokens": [50414, 407, 498, 321, 1029, 437, 366, 264, 17439, 295, 39524, 412, 13497, 9785, 11, 50598], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 682, "seek": 161458, "start": 1619.26, "end": 1621.3, "text": " we see that we think it's somewhat unlikely.", "tokens": [50598, 321, 536, 300, 321, 519, 309, 311, 8344, 17518, 13, 50700], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 683, "seek": 161458, "start": 1624.4199999999998, "end": 1625.26, "text": " Question?", "tokens": [50856, 14464, 30, 50898], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 684, "seek": 161458, "start": 1625.26, "end": 1626.1, "text": " Oh, yes.", "tokens": [50898, 876, 11, 2086, 13, 50940], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 685, "seek": 161458, "start": 1626.1, "end": 1628.4199999999998, "text": " So on the how strong is Josh,", "tokens": [50940, 407, 322, 264, 577, 2068, 307, 9785, 11, 51056], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 686, "seek": 161458, "start": 1628.4199999999998, "end": 1630.22, "text": " it seems like there's an interesting thing here", "tokens": [51056, 309, 2544, 411, 456, 311, 364, 1880, 551, 510, 51146], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 687, "seek": 161458, "start": 1630.22, "end": 1631.86, "text": " where there's also an implicit question", "tokens": [51146, 689, 456, 311, 611, 364, 26947, 1168, 51228], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 688, "seek": 161458, "start": 1631.86, "end": 1635.02, "text": " of what the word strong means in this context.", "tokens": [51228, 295, 437, 264, 1349, 2068, 1355, 294, 341, 4319, 13, 51386], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 689, "seek": 161458, "start": 1636.3799999999999, "end": 1640.3, "text": " And that, like, right, it's like not a number,", "tokens": [51454, 400, 300, 11, 411, 11, 558, 11, 309, 311, 411, 406, 257, 1230, 11, 51650], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 690, "seek": 161458, "start": 1640.3, "end": 1642.58, "text": " it's kind of some like, comparative adjective.", "tokens": [51650, 309, 311, 733, 295, 512, 411, 11, 39292, 44129, 13, 51764], "temperature": 0.0, "avg_logprob": -0.31609445051713425, "compression_ratio": 1.6293103448275863, "no_speech_prob": 6.708349246764556e-05}, {"id": 691, "seek": 164258, "start": 1642.58, "end": 1644.58, "text": " It's like, probably a non-intersective.", "tokens": [50364, 467, 311, 411, 11, 1391, 257, 2107, 12, 5106, 9632, 488, 13, 50464], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 692, "seek": 164258, "start": 1645.58, "end": 1647.78, "text": " So I guess, is that something you think about?", "tokens": [50514, 407, 286, 2041, 11, 307, 300, 746, 291, 519, 466, 30, 50624], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 693, "seek": 164258, "start": 1647.78, "end": 1649.74, "text": " This framework, or should I just be kind of, like,", "tokens": [50624, 639, 8388, 11, 420, 820, 286, 445, 312, 733, 295, 11, 411, 11, 50722], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 694, "seek": 164258, "start": 1649.74, "end": 1651.86, "text": " ignoring this sort of issue?", "tokens": [50722, 26258, 341, 1333, 295, 2734, 30, 50828], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 695, "seek": 164258, "start": 1651.86, "end": 1653.34, "text": " Yeah, well, okay.", "tokens": [50828, 865, 11, 731, 11, 1392, 13, 50902], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 696, "seek": 164258, "start": 1653.34, "end": 1654.74, "text": " So I think there's a number of ways", "tokens": [50902, 407, 286, 519, 456, 311, 257, 1230, 295, 2098, 50972], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 697, "seek": 164258, "start": 1654.74, "end": 1655.86, "text": " that we can think about that.", "tokens": [50972, 300, 321, 393, 519, 466, 300, 13, 51028], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 698, "seek": 164258, "start": 1655.86, "end": 1660.86, "text": " I mean, so, right, so the one sense we could say is like,", "tokens": [51028, 286, 914, 11, 370, 11, 558, 11, 370, 264, 472, 2020, 321, 727, 584, 307, 411, 11, 51278], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 699, "seek": 164258, "start": 1661.1799999999998, "end": 1662.3799999999999, "text": " right, how strong is Josh?", "tokens": [51294, 558, 11, 577, 2068, 307, 9785, 30, 51354], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 700, "seek": 164258, "start": 1662.3799999999999, "end": 1664.98, "text": " Isn't, the answer to how strong of Josh isn't a number.", "tokens": [51354, 6998, 380, 11, 264, 1867, 281, 577, 2068, 295, 9785, 1943, 380, 257, 1230, 13, 51484], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 701, "seek": 164258, "start": 1664.98, "end": 1667.3799999999999, "text": " Rather, it's kind of this distribution", "tokens": [51484, 16571, 11, 309, 311, 733, 295, 341, 7316, 51604], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 702, "seek": 164258, "start": 1667.3799999999999, "end": 1669.1, "text": " over this posterior distribution", "tokens": [51604, 670, 341, 33529, 7316, 51690], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 703, "seek": 164258, "start": 1669.1, "end": 1671.6999999999998, "text": " of various underlying strength values", "tokens": [51690, 295, 3683, 14217, 3800, 4190, 51820], "temperature": 0.0, "avg_logprob": -0.27172372147843643, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.00041725748451426625}, {"id": 704, "seek": 167170, "start": 1671.7, "end": 1674.8600000000001, "text": " that we currently might infer that Josh has", "tokens": [50364, 300, 321, 4362, 1062, 13596, 300, 9785, 575, 50522], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 705, "seek": 167170, "start": 1674.8600000000001, "end": 1676.94, "text": " with respect to the general value that we have.", "tokens": [50522, 365, 3104, 281, 264, 2674, 2158, 300, 321, 362, 13, 50626], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 706, "seek": 167170, "start": 1676.94, "end": 1679.66, "text": " I think another kind of popular definition", "tokens": [50626, 286, 519, 1071, 733, 295, 3743, 7123, 50762], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 707, "seek": 167170, "start": 1679.66, "end": 1682.02, "text": " of various uncertain adjectives,", "tokens": [50762, 295, 3683, 11308, 29378, 1539, 11, 50880], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 708, "seek": 167170, "start": 1682.02, "end": 1683.5800000000002, "text": " like a word like strong, right,", "tokens": [50880, 411, 257, 1349, 411, 2068, 11, 558, 11, 50958], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 709, "seek": 167170, "start": 1683.5800000000002, "end": 1686.16, "text": " is that you have some internal threshold value,", "tokens": [50958, 307, 300, 291, 362, 512, 6920, 14678, 2158, 11, 51087], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 710, "seek": 167170, "start": 1686.16, "end": 1688.02, "text": " or the person speaking has some kind of internal", "tokens": [51087, 420, 264, 954, 4124, 575, 512, 733, 295, 6920, 51180], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 711, "seek": 167170, "start": 1688.02, "end": 1692.3400000000001, "text": " threshold value that you must kind of jointly infer", "tokens": [51180, 14678, 2158, 300, 291, 1633, 733, 295, 46557, 13596, 51396], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 712, "seek": 167170, "start": 1692.3400000000001, "end": 1694.8600000000001, "text": " with respect to the context in what you've seen.", "tokens": [51396, 365, 3104, 281, 264, 4319, 294, 437, 291, 600, 1612, 13, 51522], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 713, "seek": 167170, "start": 1694.8600000000001, "end": 1697.54, "text": " And some of the examples that I'll actually give later,", "tokens": [51522, 400, 512, 295, 264, 5110, 300, 286, 603, 767, 976, 1780, 11, 51656], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 714, "seek": 167170, "start": 1697.54, "end": 1699.8600000000001, "text": " so, right, there's kind of a long line of work", "tokens": [51656, 370, 11, 558, 11, 456, 311, 733, 295, 257, 938, 1622, 295, 589, 51772], "temperature": 0.0, "avg_logprob": -0.12615635281517393, "compression_ratio": 1.838235294117647, "no_speech_prob": 0.0001158898085122928}, {"id": 715, "seek": 169986, "start": 1699.86, "end": 1701.54, "text": " in linguistics, including some work", "tokens": [50364, 294, 21766, 6006, 11, 3009, 512, 589, 50448], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 716, "seek": 169986, "start": 1701.54, "end": 1703.6999999999998, "text": " that treats that as like a pragmatic inference.", "tokens": [50448, 300, 19566, 300, 382, 411, 257, 46904, 38253, 13, 50556], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 717, "seek": 169986, "start": 1703.6999999999998, "end": 1705.34, "text": " I think some of the interesting work", "tokens": [50556, 286, 519, 512, 295, 264, 1880, 589, 50638], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 718, "seek": 169986, "start": 1705.34, "end": 1707.8999999999999, "text": " that we'll show a little bit later is that,", "tokens": [50638, 300, 321, 603, 855, 257, 707, 857, 1780, 307, 300, 11, 50766], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 719, "seek": 169986, "start": 1707.8999999999999, "end": 1709.34, "text": " there are some ways in which you might think", "tokens": [50766, 456, 366, 512, 2098, 294, 597, 291, 1062, 519, 50838], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 720, "seek": 169986, "start": 1709.34, "end": 1712.82, "text": " of this mapping function as actually being a general one", "tokens": [50838, 295, 341, 18350, 2445, 382, 767, 885, 257, 2674, 472, 51012], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 721, "seek": 169986, "start": 1712.82, "end": 1714.86, "text": " that includes that notion of pragmatic inference.", "tokens": [51012, 300, 5974, 300, 10710, 295, 46904, 38253, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 722, "seek": 169986, "start": 1714.86, "end": 1716.9399999999998, "text": " And also, I think captures the sense", "tokens": [51114, 400, 611, 11, 286, 519, 27986, 264, 2020, 51218], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 723, "seek": 169986, "start": 1716.9399999999998, "end": 1718.74, "text": " in which if you continually,", "tokens": [51218, 294, 597, 498, 291, 22277, 11, 51308], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 724, "seek": 169986, "start": 1718.74, "end": 1721.1799999999998, "text": " are you really doing this kind of pragmatic inference", "tokens": [51308, 366, 291, 534, 884, 341, 733, 295, 46904, 38253, 51430], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 725, "seek": 169986, "start": 1721.1799999999998, "end": 1723.02, "text": " all the time, or do you actually,", "tokens": [51430, 439, 264, 565, 11, 420, 360, 291, 767, 11, 51522], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 726, "seek": 169986, "start": 1723.02, "end": 1724.4199999999998, "text": " in many general settings,", "tokens": [51522, 294, 867, 2674, 6257, 11, 51592], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 727, "seek": 169986, "start": 1724.4199999999998, "end": 1726.4599999999998, "text": " like talking about the strengths of people,", "tokens": [51592, 411, 1417, 466, 264, 16986, 295, 561, 11, 51694], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 728, "seek": 169986, "start": 1726.4599999999998, "end": 1729.34, "text": " actually have some kind of cashed older notion of strength", "tokens": [51694, 767, 362, 512, 733, 295, 6388, 292, 4906, 10710, 295, 3800, 51838], "temperature": 0.0, "avg_logprob": -0.13163399209781568, "compression_ratio": 1.9966666666666666, "no_speech_prob": 0.0005356417968869209}, {"id": 729, "seek": 172934, "start": 1729.34, "end": 1730.1799999999998, "text": " that you can draw in.", "tokens": [50364, 300, 291, 393, 2642, 294, 13, 50406], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 730, "seek": 172934, "start": 1730.1799999999998, "end": 1732.9399999999998, "text": " And I think actually, this notion of large language models", "tokens": [50406, 400, 286, 519, 767, 11, 341, 10710, 295, 2416, 2856, 5245, 50544], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 731, "seek": 172934, "start": 1732.9399999999998, "end": 1735.4599999999998, "text": " as just being this learned mapping function", "tokens": [50544, 382, 445, 885, 341, 3264, 18350, 2445, 50670], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 732, "seek": 172934, "start": 1735.4599999999998, "end": 1737.58, "text": " from language into expressions include,", "tokens": [50670, 490, 2856, 666, 15277, 4090, 11, 50776], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 733, "seek": 172934, "start": 1737.58, "end": 1739.22, "text": " can also capture the sense in which", "tokens": [50776, 393, 611, 7983, 264, 2020, 294, 597, 50858], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 734, "seek": 172934, "start": 1739.22, "end": 1741.1399999999999, "text": " that knowledge is amortized away.", "tokens": [50858, 300, 3601, 307, 669, 477, 1602, 1314, 13, 50954], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 735, "seek": 172934, "start": 1741.1399999999999, "end": 1742.86, "text": " And you might not be having that inference.", "tokens": [50954, 400, 291, 1062, 406, 312, 1419, 300, 38253, 13, 51040], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 736, "seek": 172934, "start": 1742.86, "end": 1743.6999999999998, "text": " Yeah, Chris.", "tokens": [51040, 865, 11, 6688, 13, 51082], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 737, "seek": 172934, "start": 1748.5, "end": 1751.6999999999998, "text": " She's sort of just denying or ignoring", "tokens": [51322, 1240, 311, 1333, 295, 445, 30363, 420, 26258, 51482], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 738, "seek": 172934, "start": 1751.6999999999998, "end": 1756.4199999999998, "text": " what makes people so excited about large language models", "tokens": [51482, 437, 1669, 561, 370, 2919, 466, 2416, 2856, 5245, 51718], "temperature": 0.0, "avg_logprob": -0.24556140309756563, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0003567771054804325}, {"id": 739, "seek": 175642, "start": 1756.46, "end": 1761.46, "text": " in their meaning representation and ability to do inference.", "tokens": [50366, 294, 641, 3620, 10290, 293, 3485, 281, 360, 38253, 13, 50616], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 740, "seek": 175642, "start": 1764.54, "end": 1767.1000000000001, "text": " I mean, because, okay, you've got sort of", "tokens": [50770, 286, 914, 11, 570, 11, 1392, 11, 291, 600, 658, 1333, 295, 50898], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 741, "seek": 175642, "start": 1767.1000000000001, "end": 1769.1000000000001, "text": " cooler probabilistic programming language", "tokens": [50898, 15566, 31959, 3142, 9410, 2856, 50998], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 742, "seek": 175642, "start": 1769.1000000000001, "end": 1770.8600000000001, "text": " on the right hand side,", "tokens": [50998, 322, 264, 558, 1011, 1252, 11, 51086], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 743, "seek": 175642, "start": 1770.8600000000001, "end": 1774.5800000000002, "text": " but in some sense, the picture is still,", "tokens": [51086, 457, 294, 512, 2020, 11, 264, 3036, 307, 920, 11, 51272], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 744, "seek": 175642, "start": 1774.5800000000002, "end": 1779.5800000000002, "text": " this is semantic parsing, like it was 2010 to 2015.", "tokens": [51272, 341, 307, 47982, 21156, 278, 11, 411, 309, 390, 9657, 281, 7546, 13, 51522], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 745, "seek": 175642, "start": 1779.5800000000002, "end": 1784.5800000000002, "text": " And yes, you're using a large language model,", "tokens": [51522, 400, 2086, 11, 291, 434, 1228, 257, 2416, 2856, 2316, 11, 51772], "temperature": 0.0, "avg_logprob": -0.20464603010430393, "compression_ratio": 1.5049019607843137, "no_speech_prob": 0.0017440851079300046}, {"id": 746, "seek": 178458, "start": 1784.78, "end": 1788.3, "text": " but you're not actually using the excitement", "tokens": [50374, 457, 291, 434, 406, 767, 1228, 264, 14755, 50550], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 747, "seek": 178458, "start": 1788.3, "end": 1792.02, "text": " of a large language model as a representation system.", "tokens": [50550, 295, 257, 2416, 2856, 2316, 382, 257, 10290, 1185, 13, 50736], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 748, "seek": 178458, "start": 1792.02, "end": 1794.54, "text": " Yeah, and I think, so probably each of us", "tokens": [50736, 865, 11, 293, 286, 519, 11, 370, 1391, 1184, 295, 505, 50862], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 749, "seek": 178458, "start": 1794.54, "end": 1795.6999999999998, "text": " would have different answers to this,", "tokens": [50862, 576, 362, 819, 6338, 281, 341, 11, 50920], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 750, "seek": 178458, "start": 1795.6999999999998, "end": 1797.3799999999999, "text": " but part of what we're hoping to paint out", "tokens": [50920, 457, 644, 295, 437, 321, 434, 7159, 281, 4225, 484, 51004], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 751, "seek": 178458, "start": 1797.3799999999999, "end": 1799.22, "text": " over the course of this talk is I think", "tokens": [51004, 670, 264, 1164, 295, 341, 751, 307, 286, 519, 51096], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 752, "seek": 178458, "start": 1799.22, "end": 1802.1, "text": " some of the ways in which actually, right,", "tokens": [51096, 512, 295, 264, 2098, 294, 597, 767, 11, 558, 11, 51240], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 753, "seek": 178458, "start": 1802.1, "end": 1804.8999999999999, "text": " of course, no one wants to say we're gonna go back", "tokens": [51240, 295, 1164, 11, 572, 472, 2738, 281, 584, 321, 434, 799, 352, 646, 51380], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 754, "seek": 178458, "start": 1804.8999999999999, "end": 1807.02, "text": " to kind of the brittleness of semantic parsing,", "tokens": [51380, 281, 733, 295, 264, 738, 593, 45887, 295, 47982, 21156, 278, 11, 51486], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 755, "seek": 178458, "start": 1807.02, "end": 1808.78, "text": " but I think one thing that large language models", "tokens": [51486, 457, 286, 519, 472, 551, 300, 2416, 2856, 5245, 51574], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 756, "seek": 178458, "start": 1808.78, "end": 1811.46, "text": " actually give us, or one proposal in this talk,", "tokens": [51574, 767, 976, 505, 11, 420, 472, 11494, 294, 341, 751, 11, 51708], "temperature": 0.0, "avg_logprob": -0.12730423372183272, "compression_ratio": 1.7730496453900708, "no_speech_prob": 0.0002091839851345867}, {"id": 757, "seek": 181146, "start": 1811.5, "end": 1815.58, "text": " is that there are some aspects of the theory,", "tokens": [50366, 307, 300, 456, 366, 512, 7270, 295, 264, 5261, 11, 50570], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 758, "seek": 181146, "start": 1815.58, "end": 1817.26, "text": " kind of the classic notion of linguistics,", "tokens": [50570, 733, 295, 264, 7230, 10710, 295, 21766, 6006, 11, 50654], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 759, "seek": 181146, "start": 1817.26, "end": 1819.42, "text": " and certainly the classic notions of semantic parsing", "tokens": [50654, 293, 3297, 264, 7230, 35799, 295, 47982, 21156, 278, 50762], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 760, "seek": 181146, "start": 1819.42, "end": 1820.8600000000001, "text": " that actually normatively capture", "tokens": [50762, 300, 767, 2026, 19020, 7983, 50834], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 761, "seek": 181146, "start": 1820.8600000000001, "end": 1823.58, "text": " a lot of what we really might want when we think about,", "tokens": [50834, 257, 688, 295, 437, 321, 534, 1062, 528, 562, 321, 519, 466, 11, 50970], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 762, "seek": 181146, "start": 1823.58, "end": 1828.58, "text": " so one answer for an AI system is, well, yes,", "tokens": [50970, 370, 472, 1867, 337, 364, 7318, 1185, 307, 11, 731, 11, 2086, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 763, "seek": 181146, "start": 1828.58, "end": 1829.7, "text": " in some way, we don't wanna throw,", "tokens": [51220, 294, 512, 636, 11, 321, 500, 380, 1948, 3507, 11, 51276], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 764, "seek": 181146, "start": 1829.7, "end": 1831.1000000000001, "text": " certainly we don't wanna throw away", "tokens": [51276, 3297, 321, 500, 380, 1948, 3507, 1314, 51346], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 765, "seek": 181146, "start": 1831.1000000000001, "end": 1832.98, "text": " everything that we're learning from large language models,", "tokens": [51346, 1203, 300, 321, 434, 2539, 490, 2416, 2856, 5245, 11, 51440], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 766, "seek": 181146, "start": 1832.98, "end": 1834.3400000000001, "text": " and I think one answer to that", "tokens": [51440, 293, 286, 519, 472, 1867, 281, 300, 51508], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 767, "seek": 181146, "start": 1834.3400000000001, "end": 1837.26, "text": " is kind of the answer that I gave to Jacob, right?", "tokens": [51508, 307, 733, 295, 264, 1867, 300, 286, 2729, 281, 14117, 11, 558, 30, 51654], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 768, "seek": 181146, "start": 1837.26, "end": 1839.8600000000001, "text": " If we think about not always, you know,", "tokens": [51654, 759, 321, 519, 466, 406, 1009, 11, 291, 458, 11, 51784], "temperature": 0.0, "avg_logprob": -0.11634869642660652, "compression_ratio": 1.8928571428571428, "no_speech_prob": 7.965730765135959e-05}, {"id": 769, "seek": 183986, "start": 1839.86, "end": 1842.9799999999998, "text": " in these examples, we're showing this very direct system", "tokens": [50364, 294, 613, 5110, 11, 321, 434, 4099, 341, 588, 2047, 1185, 50520], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 770, "seek": 183986, "start": 1842.9799999999998, "end": 1844.9399999999998, "text": " in which we always start with language", "tokens": [50520, 294, 597, 321, 1009, 722, 365, 2856, 50618], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 771, "seek": 183986, "start": 1844.9399999999998, "end": 1846.6999999999998, "text": " and we always map into some sort of", "tokens": [50618, 293, 321, 1009, 4471, 666, 512, 1333, 295, 50706], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 772, "seek": 183986, "start": 1846.6999999999998, "end": 1847.62, "text": " probabilistic programming expression,", "tokens": [50706, 31959, 3142, 9410, 6114, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 773, "seek": 183986, "start": 1847.62, "end": 1849.54, "text": " and that's where all of the thinking happens,", "tokens": [50752, 293, 300, 311, 689, 439, 295, 264, 1953, 2314, 11, 50848], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 774, "seek": 183986, "start": 1849.54, "end": 1851.9799999999998, "text": " and we might think, well, that doesn't totally make sense", "tokens": [50848, 293, 321, 1062, 519, 11, 731, 11, 300, 1177, 380, 3879, 652, 2020, 50970], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 775, "seek": 183986, "start": 1851.9799999999998, "end": 1855.1, "text": " because there are lots of cases where, as you're saying,", "tokens": [50970, 570, 456, 366, 3195, 295, 3331, 689, 11, 382, 291, 434, 1566, 11, 51126], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 776, "seek": 183986, "start": 1855.1, "end": 1856.3799999999999, "text": " we have every reason to believe", "tokens": [51126, 321, 362, 633, 1778, 281, 1697, 51190], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 777, "seek": 183986, "start": 1856.3799999999999, "end": 1858.3799999999999, "text": " that large language models have learned", "tokens": [51190, 300, 2416, 2856, 5245, 362, 3264, 51290], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 778, "seek": 183986, "start": 1858.3799999999999, "end": 1860.02, "text": " a lot of latent information.", "tokens": [51290, 257, 688, 295, 48994, 1589, 13, 51372], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 779, "seek": 183986, "start": 1860.02, "end": 1861.9399999999998, "text": " They can do a lot of, they certainly have", "tokens": [51372, 814, 393, 360, 257, 688, 295, 11, 436, 3297, 362, 51468], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 780, "seek": 183986, "start": 1861.9399999999998, "end": 1863.78, "text": " a lot of latent conceptual information,", "tokens": [51468, 257, 688, 295, 48994, 24106, 1589, 11, 51560], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 781, "seek": 183986, "start": 1863.78, "end": 1865.58, "text": " and maybe to some degree, they can even perform", "tokens": [51560, 293, 1310, 281, 512, 4314, 11, 436, 393, 754, 2042, 51650], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 782, "seek": 183986, "start": 1865.58, "end": 1868.26, "text": " certain kinds of limited, amortized inferences,", "tokens": [51650, 1629, 3685, 295, 5567, 11, 669, 477, 1602, 13596, 2667, 11, 51784], "temperature": 0.0, "avg_logprob": -0.1331926358291526, "compression_ratio": 1.8288288288288288, "no_speech_prob": 0.0001634398940950632}, {"id": 783, "seek": 186826, "start": 1868.26, "end": 1870.78, "text": " or reuse old inferences that they've learned", "tokens": [50364, 420, 26225, 1331, 13596, 2667, 300, 436, 600, 3264, 50490], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 784, "seek": 186826, "start": 1870.78, "end": 1871.74, "text": " from other people I've had,", "tokens": [50490, 490, 661, 561, 286, 600, 632, 11, 50538], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 785, "seek": 186826, "start": 1871.74, "end": 1873.42, "text": " and so in the second part of this talk,", "tokens": [50538, 293, 370, 294, 264, 1150, 644, 295, 341, 751, 11, 50622], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 786, "seek": 186826, "start": 1873.42, "end": 1875.22, "text": " we're going to show different ways in which,", "tokens": [50622, 321, 434, 516, 281, 855, 819, 2098, 294, 597, 11, 50712], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 787, "seek": 186826, "start": 1875.22, "end": 1877.5, "text": " well, this probabilistic programming language itself", "tokens": [50712, 731, 11, 341, 31959, 3142, 9410, 2856, 2564, 50826], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 788, "seek": 186826, "start": 1877.5, "end": 1880.34, "text": " doesn't necessarily need to be something that's isolated", "tokens": [50826, 1177, 380, 4725, 643, 281, 312, 746, 300, 311, 14621, 50968], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 789, "seek": 186826, "start": 1880.34, "end": 1882.14, "text": " from what large language models have learned.", "tokens": [50968, 490, 437, 2416, 2856, 5245, 362, 3264, 13, 51058], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 790, "seek": 186826, "start": 1882.14, "end": 1885.74, "text": " It also can embed calls to large language models", "tokens": [51058, 467, 611, 393, 12240, 5498, 281, 2416, 2856, 5245, 51238], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 791, "seek": 186826, "start": 1885.74, "end": 1888.1, "text": " within it to kind of draw in that sort of knowledge.", "tokens": [51238, 1951, 309, 281, 733, 295, 2642, 294, 300, 1333, 295, 3601, 13, 51356], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 792, "seek": 186826, "start": 1888.1, "end": 1890.86, "text": " Haven't you gone back to the visualness of semantic housing", "tokens": [51356, 23770, 380, 291, 2780, 646, 281, 264, 5056, 1287, 295, 47982, 6849, 51494], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 793, "seek": 186826, "start": 1890.86, "end": 1893.34, "text": " because you're doing this translation", "tokens": [51494, 570, 291, 434, 884, 341, 12853, 51618], "temperature": 0.0, "avg_logprob": -0.19244215393066405, "compression_ratio": 1.721476510067114, "no_speech_prob": 0.0014753044815734029}, {"id": 794, "seek": 189334, "start": 1894.34, "end": 1899.26, "text": " into symbolic semantic representation,", "tokens": [50414, 666, 25755, 47982, 10290, 11, 50660], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 795, "seek": 189334, "start": 1899.26, "end": 1902.54, "text": " which really ends with your actual result,", "tokens": [50660, 597, 534, 5314, 365, 428, 3539, 1874, 11, 50824], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 796, "seek": 189334, "start": 1902.54, "end": 1904.86, "text": " and it's riddled in the same way?", "tokens": [50824, 293, 309, 311, 3973, 43130, 294, 264, 912, 636, 30, 50940], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 797, "seek": 189334, "start": 1904.86, "end": 1908.82, "text": " Well, right, and also, so no, I think I would say,", "tokens": [50940, 1042, 11, 558, 11, 293, 611, 11, 370, 572, 11, 286, 519, 286, 576, 584, 11, 51138], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 798, "seek": 189334, "start": 1908.82, "end": 1913.34, "text": " I don't totally think that the way in which we're using,", "tokens": [51138, 286, 500, 380, 3879, 519, 300, 264, 636, 294, 597, 321, 434, 1228, 11, 51364], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 799, "seek": 189334, "start": 1913.34, "end": 1915.62, "text": " or the sense in which, or I think there are some ways", "tokens": [51364, 420, 264, 2020, 294, 597, 11, 420, 286, 519, 456, 366, 512, 2098, 51478], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 800, "seek": 189334, "start": 1915.62, "end": 1918.98, "text": " in which this kind of broader definition", "tokens": [51478, 294, 597, 341, 733, 295, 13227, 7123, 51646], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 801, "seek": 189334, "start": 1918.98, "end": 1921.54, "text": " in which you are saying, well, the meaning of a distribution,", "tokens": [51646, 294, 597, 291, 366, 1566, 11, 731, 11, 264, 3620, 295, 257, 7316, 11, 51774], "temperature": 0.0, "avg_logprob": -0.2607220785958426, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0019507475662976503}, {"id": 802, "seek": 192154, "start": 1921.54, "end": 1924.1399999999999, "text": " or the meaning of a sentence in language", "tokens": [50364, 420, 264, 3620, 295, 257, 8174, 294, 2856, 50494], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 803, "seek": 192154, "start": 1924.1399999999999, "end": 1926.8999999999999, "text": " isn't just one probabilistic programming expression, right?", "tokens": [50494, 1943, 380, 445, 472, 31959, 3142, 9410, 6114, 11, 558, 30, 50632], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 804, "seek": 192154, "start": 1926.8999999999999, "end": 1930.26, "text": " That's what we're showing here for pedagogical purposes,", "tokens": [50632, 663, 311, 437, 321, 434, 4099, 510, 337, 5670, 31599, 804, 9932, 11, 50800], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 805, "seek": 192154, "start": 1930.26, "end": 1932.18, "text": " but you might say, well, okay, right,", "tokens": [50800, 457, 291, 1062, 584, 11, 731, 11, 1392, 11, 558, 11, 50896], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 806, "seek": 192154, "start": 1932.18, "end": 1936.46, "text": " how are you going to obey kind of the ambiguity of language?", "tokens": [50896, 577, 366, 291, 516, 281, 19297, 733, 295, 264, 46519, 295, 2856, 30, 51110], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 807, "seek": 192154, "start": 1936.46, "end": 1939.62, "text": " There are kinds of sentences that are definitely ambiguous.", "tokens": [51110, 821, 366, 3685, 295, 16579, 300, 366, 2138, 39465, 13, 51268], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 808, "seek": 192154, "start": 1939.62, "end": 1943.1, "text": " So one example that we've looked at are sentences", "tokens": [51268, 407, 472, 1365, 300, 321, 600, 2956, 412, 366, 16579, 51442], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 809, "seek": 192154, "start": 1943.1, "end": 1944.58, "text": " in which you say something like,", "tokens": [51442, 294, 597, 291, 584, 746, 411, 11, 51516], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 810, "seek": 192154, "start": 1944.58, "end": 1946.78, "text": " Josh beat Alex and Leo, right?", "tokens": [51516, 9785, 4224, 5202, 293, 19344, 11, 558, 30, 51626], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 811, "seek": 192154, "start": 1946.78, "end": 1948.26, "text": " And you might ask, well, you know,", "tokens": [51626, 400, 291, 1062, 1029, 11, 731, 11, 291, 458, 11, 51700], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 812, "seek": 192154, "start": 1948.26, "end": 1950.54, "text": " that's kind of a classic syntactic construction.", "tokens": [51700, 300, 311, 733, 295, 257, 7230, 23980, 19892, 6435, 13, 51814], "temperature": 0.0, "avg_logprob": -0.117779216059932, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.00026108400197699666}, {"id": 813, "seek": 195054, "start": 1950.58, "end": 1953.42, "text": " Does that mean that Josh beat Alex and Leo,", "tokens": [50366, 4402, 300, 914, 300, 9785, 4224, 5202, 293, 19344, 11, 50508], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 814, "seek": 195054, "start": 1953.42, "end": 1954.94, "text": " and they were playing on the same team,", "tokens": [50508, 293, 436, 645, 2433, 322, 264, 912, 1469, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 815, "seek": 195054, "start": 1954.94, "end": 1958.26, "text": " or Josh beat Alex, and then Josh went on to beat Leo?", "tokens": [50584, 420, 9785, 4224, 5202, 11, 293, 550, 9785, 1437, 322, 281, 4224, 19344, 30, 50750], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 816, "seek": 195054, "start": 1958.26, "end": 1962.3799999999999, "text": " And what we see, or generally, what you might say is,", "tokens": [50750, 400, 437, 321, 536, 11, 420, 5101, 11, 437, 291, 1062, 584, 307, 11, 50956], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 817, "seek": 195054, "start": 1962.3799999999999, "end": 1964.06, "text": " well, the meaning of that sentence", "tokens": [50956, 731, 11, 264, 3620, 295, 300, 8174, 51040], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 818, "seek": 195054, "start": 1964.06, "end": 1967.82, "text": " actually shouldn't be picking one expression or the other.", "tokens": [51040, 767, 4659, 380, 312, 8867, 472, 6114, 420, 264, 661, 13, 51228], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 819, "seek": 195054, "start": 1967.82, "end": 1969.78, "text": " It should be kind of the distribution", "tokens": [51228, 467, 820, 312, 733, 295, 264, 7316, 51326], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 820, "seek": 195054, "start": 1969.78, "end": 1972.82, "text": " over those possible parses,", "tokens": [51326, 670, 729, 1944, 21156, 279, 11, 51478], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 821, "seek": 195054, "start": 1972.82, "end": 1975.46, "text": " and that distribution also shouldn't just be something", "tokens": [51478, 293, 300, 7316, 611, 4659, 380, 445, 312, 746, 51610], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 822, "seek": 195054, "start": 1975.46, "end": 1978.06, "text": " that we can determine in this totally context", "tokens": [51610, 300, 321, 393, 6997, 294, 341, 3879, 4319, 51740], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 823, "seek": 195054, "start": 1978.06, "end": 1980.18, "text": " and sensitive way, it should actually depend on", "tokens": [51740, 293, 9477, 636, 11, 309, 820, 767, 5672, 322, 51846], "temperature": 0.0, "avg_logprob": -0.10886731743812561, "compression_ratio": 1.8248175182481752, "no_speech_prob": 3.88239132007584e-05}, {"id": 824, "seek": 198018, "start": 1980.18, "end": 1982.3, "text": " all the previous patterns in the discourse.", "tokens": [50364, 439, 264, 3894, 8294, 294, 264, 23938, 13, 50470], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 825, "seek": 198018, "start": 1982.3, "end": 1986.18, "text": " So if someone's continually been using this conjunctive", "tokens": [50470, 407, 498, 1580, 311, 22277, 668, 1228, 341, 18244, 20221, 50664], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 826, "seek": 198018, "start": 1986.18, "end": 1990.66, "text": " and to refer to teams of players playing together,", "tokens": [50664, 293, 281, 2864, 281, 5491, 295, 4150, 2433, 1214, 11, 50888], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 827, "seek": 198018, "start": 1990.66, "end": 1993.3, "text": " we should take that kind of discourse bias into account.", "tokens": [50888, 321, 820, 747, 300, 733, 295, 23938, 12577, 666, 2696, 13, 51020], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 828, "seek": 198018, "start": 1993.3, "end": 1996.78, "text": " And I think actually, right, this provides,", "tokens": [51020, 400, 286, 519, 767, 11, 558, 11, 341, 6417, 11, 51194], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 829, "seek": 198018, "start": 1996.78, "end": 1999.5800000000002, "text": " or thinking of large language models", "tokens": [51194, 420, 1953, 295, 2416, 2856, 5245, 51334], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 830, "seek": 198018, "start": 1999.5800000000002, "end": 2001.26, "text": " as kind of generally having learned", "tokens": [51334, 382, 733, 295, 5101, 1419, 3264, 51418], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 831, "seek": 198018, "start": 2001.26, "end": 2002.8600000000001, "text": " this broad joint distribution,", "tokens": [51418, 341, 4152, 7225, 7316, 11, 51498], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 832, "seek": 198018, "start": 2002.8600000000001, "end": 2006.5, "text": " but one that can be kind of conditioned quite richly", "tokens": [51498, 457, 472, 300, 393, 312, 733, 295, 35833, 1596, 4593, 356, 51680], "temperature": 0.0, "avg_logprob": -0.11446540871846307, "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.00047268258640542626}, {"id": 833, "seek": 200650, "start": 2006.5, "end": 2010.62, "text": " both on the content of this generative model.", "tokens": [50364, 1293, 322, 264, 2701, 295, 341, 1337, 1166, 2316, 13, 50570], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 834, "seek": 200650, "start": 2010.62, "end": 2012.22, "text": " So it's not trying to come up", "tokens": [50570, 407, 309, 311, 406, 1382, 281, 808, 493, 50650], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 835, "seek": 200650, "start": 2012.22, "end": 2014.46, "text": " with a universal definition of strength.", "tokens": [50650, 365, 257, 11455, 7123, 295, 3800, 13, 50762], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 836, "seek": 200650, "start": 2014.46, "end": 2016.14, "text": " It's not even necessarily trying to come up", "tokens": [50762, 467, 311, 406, 754, 4725, 1382, 281, 808, 493, 50846], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 837, "seek": 200650, "start": 2016.14, "end": 2018.5, "text": " with a universal definition of any of these words.", "tokens": [50846, 365, 257, 11455, 7123, 295, 604, 295, 613, 2283, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 838, "seek": 200650, "start": 2018.5, "end": 2020.5, "text": " It's thinking about how they might map contextually", "tokens": [50964, 467, 311, 1953, 466, 577, 436, 1062, 4471, 4319, 671, 51064], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 839, "seek": 200650, "start": 2020.5, "end": 2023.62, "text": " into the best possible expression in the context", "tokens": [51064, 666, 264, 1151, 1944, 6114, 294, 264, 4319, 51220], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 840, "seek": 200650, "start": 2023.62, "end": 2028.5, "text": " of a particular local model built for a particular situation.", "tokens": [51220, 295, 257, 1729, 2654, 2316, 3094, 337, 257, 1729, 2590, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 841, "seek": 200650, "start": 2028.5, "end": 2032.18, "text": " I think is obviously related to,", "tokens": [51464, 286, 519, 307, 2745, 4077, 281, 11, 51648], "temperature": 0.0, "avg_logprob": -0.13038009643554688, "compression_ratio": 1.8169642857142858, "no_speech_prob": 0.003169994568452239}, {"id": 842, "seek": 203218, "start": 2032.18, "end": 2037.18, "text": " but attempting to address some of the brittleness challenges", "tokens": [50364, 457, 22001, 281, 2985, 512, 295, 264, 738, 593, 45887, 4759, 50614], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 843, "seek": 203218, "start": 2037.22, "end": 2039.42, "text": " of semantic parsing in the past.", "tokens": [50616, 295, 47982, 21156, 278, 294, 264, 1791, 13, 50726], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 844, "seek": 203218, "start": 2039.42, "end": 2040.78, "text": " I think another answer to this, right,", "tokens": [50726, 286, 519, 1071, 1867, 281, 341, 11, 558, 11, 50794], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 845, "seek": 203218, "start": 2040.78, "end": 2044.54, "text": " is that part of the problem of semantic parsing previously", "tokens": [50794, 307, 300, 644, 295, 264, 1154, 295, 47982, 21156, 278, 8046, 50982], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 846, "seek": 203218, "start": 2044.54, "end": 2046.1000000000001, "text": " has been actually that the mapping functions", "tokens": [50982, 575, 668, 767, 300, 264, 18350, 6828, 51060], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 847, "seek": 203218, "start": 2046.1000000000001, "end": 2049.02, "text": " have historically been difficult to get right.", "tokens": [51060, 362, 16180, 668, 2252, 281, 483, 558, 13, 51206], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 848, "seek": 203218, "start": 2049.02, "end": 2050.82, "text": " Whether you were thinking about those", "tokens": [51206, 8503, 291, 645, 1953, 466, 729, 51296], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 849, "seek": 203218, "start": 2050.82, "end": 2053.38, "text": " as kind of old hard-coded grammars", "tokens": [51296, 382, 733, 295, 1331, 1152, 12, 66, 12340, 17570, 685, 51424], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 850, "seek": 203218, "start": 2053.38, "end": 2056.06, "text": " or many of the attempts to kind of learn these things,", "tokens": [51424, 420, 867, 295, 264, 15257, 281, 733, 295, 1466, 613, 721, 11, 51558], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 851, "seek": 203218, "start": 2056.06, "end": 2057.94, "text": " we are very domain-specific supervision.", "tokens": [51558, 321, 366, 588, 9274, 12, 29258, 32675, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 852, "seek": 203218, "start": 2057.94, "end": 2059.7400000000002, "text": " So you wanna have a semantic parser", "tokens": [51652, 407, 291, 1948, 362, 257, 47982, 21156, 260, 51742], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 853, "seek": 203218, "start": 2059.7400000000002, "end": 2061.7400000000002, "text": " for a particular robotics domain.", "tokens": [51742, 337, 257, 1729, 34145, 9274, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12771914555476263, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.0007551541784778237}, {"id": 854, "seek": 206174, "start": 2061.74, "end": 2065.66, "text": " You need a thousand examples of sentences", "tokens": [50364, 509, 643, 257, 4714, 5110, 295, 16579, 50560], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 855, "seek": 206174, "start": 2065.66, "end": 2067.66, "text": " about that particular robotics domain", "tokens": [50560, 466, 300, 1729, 34145, 9274, 50660], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 856, "seek": 206174, "start": 2067.66, "end": 2070.3399999999997, "text": " and a thousand paired with a thousand examples", "tokens": [50660, 293, 257, 4714, 25699, 365, 257, 4714, 5110, 50794], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 857, "seek": 206174, "start": 2070.3399999999997, "end": 2075.3399999999997, "text": " of programs that are operating on that particular domain.", "tokens": [50794, 295, 4268, 300, 366, 7447, 322, 300, 1729, 9274, 13, 51044], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 858, "seek": 206174, "start": 2075.66, "end": 2078.4199999999996, "text": " What we're seeing here is I think something that says no.", "tokens": [51060, 708, 321, 434, 2577, 510, 307, 286, 519, 746, 300, 1619, 572, 13, 51198], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 859, "seek": 206174, "start": 2078.4199999999996, "end": 2080.4599999999996, "text": " What it means to learn language generally", "tokens": [51198, 708, 309, 1355, 281, 1466, 2856, 5101, 51300], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 860, "seek": 206174, "start": 2080.4599999999996, "end": 2082.7799999999997, "text": " is to learn kind of this general mapping", "tokens": [51300, 307, 281, 1466, 733, 295, 341, 2674, 18350, 51416], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 861, "seek": 206174, "start": 2082.7799999999997, "end": 2085.9799999999996, "text": " between language and some kind of underlying representation.", "tokens": [51416, 1296, 2856, 293, 512, 733, 295, 14217, 10290, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 862, "seek": 206174, "start": 2087.3399999999997, "end": 2091.4199999999996, "text": " And also, one reason why we might want a system like this", "tokens": [51644, 400, 611, 11, 472, 1778, 983, 321, 1062, 528, 257, 1185, 411, 341, 51848], "temperature": 0.0, "avg_logprob": -0.12152125358581543, "compression_ratio": 1.8271604938271604, "no_speech_prob": 6.706825661240146e-05}, {"id": 863, "seek": 209142, "start": 2091.42, "end": 2093.98, "text": " is because we want to be able to condition coherently", "tokens": [50364, 307, 570, 321, 528, 281, 312, 1075, 281, 4188, 26528, 2276, 50492], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 864, "seek": 209142, "start": 2093.98, "end": 2097.94, "text": " on information that's not just coming from language.", "tokens": [50492, 322, 1589, 300, 311, 406, 445, 1348, 490, 2856, 13, 50690], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 865, "seek": 209142, "start": 2097.94, "end": 2101.86, "text": " And we want to think about how a general substrate", "tokens": [50690, 400, 321, 528, 281, 519, 466, 577, 257, 2674, 27585, 50886], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 866, "seek": 209142, "start": 2101.86, "end": 2105.66, "text": " in which the only, yes, we might be told", "tokens": [50886, 294, 597, 264, 787, 11, 2086, 11, 321, 1062, 312, 1907, 51076], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 867, "seek": 209142, "start": 2105.66, "end": 2106.9, "text": " that Josh went against Leo,", "tokens": [51076, 300, 9785, 1437, 1970, 19344, 11, 51138], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 868, "seek": 209142, "start": 2106.9, "end": 2109.7000000000003, "text": " but we might also be watching videos", "tokens": [51138, 457, 321, 1062, 611, 312, 1976, 2145, 51278], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 869, "seek": 209142, "start": 2109.7000000000003, "end": 2112.46, "text": " that give us information about Josh's strength,", "tokens": [51278, 300, 976, 505, 1589, 466, 9785, 311, 3800, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 870, "seek": 209142, "start": 2112.46, "end": 2113.46, "text": " that convey our observations.", "tokens": [51416, 300, 16965, 527, 18163, 13, 51466], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 871, "seek": 209142, "start": 2113.46, "end": 2116.34, "text": " We might also have seen pictures", "tokens": [51466, 492, 1062, 611, 362, 1612, 5242, 51610], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 872, "seek": 209142, "start": 2116.34, "end": 2118.86, "text": " like the ones in the stimuli that we saw before", "tokens": [51610, 411, 264, 2306, 294, 264, 47752, 300, 321, 1866, 949, 51736], "temperature": 0.0, "avg_logprob": -0.1074481144129673, "compression_ratio": 1.688, "no_speech_prob": 0.0003681434609461576}, {"id": 873, "seek": 211886, "start": 2118.9, "end": 2122.6200000000003, "text": " demonstrating the results of previous outcomes of matches.", "tokens": [50366, 29889, 264, 3542, 295, 3894, 10070, 295, 10676, 13, 50552], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 874, "seek": 211886, "start": 2122.6200000000003, "end": 2125.86, "text": " And I think one thing that suggests", "tokens": [50552, 400, 286, 519, 472, 551, 300, 13409, 50714], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 875, "seek": 211886, "start": 2125.86, "end": 2129.7400000000002, "text": " is we want this kind of general substrate", "tokens": [50714, 307, 321, 528, 341, 733, 295, 2674, 27585, 50908], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 876, "seek": 211886, "start": 2130.7400000000002, "end": 2132.94, "text": " in which we can think about how those observations,", "tokens": [50958, 294, 597, 321, 393, 519, 466, 577, 729, 18163, 11, 51068], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 877, "seek": 211886, "start": 2132.94, "end": 2135.34, "text": " including the observations from language,", "tokens": [51068, 3009, 264, 18163, 490, 2856, 11, 51188], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 878, "seek": 211886, "start": 2135.34, "end": 2137.9, "text": " but without prioritizing language in any way,", "tokens": [51188, 457, 1553, 14846, 3319, 2856, 294, 604, 636, 11, 51316], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 879, "seek": 211886, "start": 2137.9, "end": 2140.38, "text": " I think are coherently considered.", "tokens": [51316, 286, 519, 366, 26528, 2276, 4888, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 880, "seek": 211886, "start": 2142.9, "end": 2147.26, "text": " So I think it depends on what part of semantic parsing", "tokens": [51566, 407, 286, 519, 309, 5946, 322, 437, 644, 295, 47982, 21156, 278, 51784], "temperature": 0.0, "avg_logprob": -0.12213511799657067, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.0002693943097256124}, {"id": 881, "seek": 214726, "start": 2147.26, "end": 2150.94, "text": " you, or yeah, I think the answer to that depends", "tokens": [50364, 291, 11, 420, 1338, 11, 286, 519, 264, 1867, 281, 300, 5946, 50548], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 882, "seek": 214726, "start": 2150.94, "end": 2153.38, "text": " on what part of semantic parsing we think of", "tokens": [50548, 322, 437, 644, 295, 47982, 21156, 278, 321, 519, 295, 50670], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 883, "seek": 214726, "start": 2153.38, "end": 2155.94, "text": " as being the source of the brittleness", "tokens": [50670, 382, 885, 264, 4009, 295, 264, 738, 593, 45887, 50798], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 884, "seek": 214726, "start": 2155.94, "end": 2159.5, "text": " that caused us to throw that paradigm into question.", "tokens": [50798, 300, 7008, 505, 281, 3507, 300, 24709, 666, 1168, 13, 50976], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 885, "seek": 214726, "start": 2161.34, "end": 2164.98, "text": " Yeah, and maybe I'll just offer one more perspective.", "tokens": [51068, 865, 11, 293, 1310, 286, 603, 445, 2626, 472, 544, 4585, 13, 51250], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 886, "seek": 214726, "start": 2164.98, "end": 2167.7000000000003, "text": " So one part of it is what Leo is saying.", "tokens": [51250, 407, 472, 644, 295, 309, 307, 437, 19344, 307, 1566, 13, 51386], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 887, "seek": 214726, "start": 2167.7000000000003, "end": 2169.7400000000002, "text": " Traditional semantic parsing is brittle in two ways.", "tokens": [51386, 46738, 47982, 21156, 278, 307, 49325, 294, 732, 2098, 13, 51488], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 888, "seek": 214726, "start": 2169.7400000000002, "end": 2171.5400000000004, "text": " One is, do you have broad coverage of language", "tokens": [51488, 1485, 307, 11, 360, 291, 362, 4152, 9645, 295, 2856, 51578], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 889, "seek": 214726, "start": 2171.5400000000004, "end": 2173.5800000000004, "text": " that you can parse into your system?", "tokens": [51578, 300, 291, 393, 48377, 666, 428, 1185, 30, 51680], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 890, "seek": 214726, "start": 2173.5800000000004, "end": 2175.94, "text": " And two is like how broad coverage", "tokens": [51680, 400, 732, 307, 411, 577, 4152, 9645, 51798], "temperature": 0.0, "avg_logprob": -0.13203059740302978, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0004043509834446013}, {"id": 891, "seek": 217594, "start": 2175.94, "end": 2178.06, "text": " are the set of query, like the set of semantic queries", "tokens": [50364, 366, 264, 992, 295, 14581, 11, 411, 264, 992, 295, 47982, 24109, 50470], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 892, "seek": 217594, "start": 2178.06, "end": 2179.42, "text": " that you can actually answer.", "tokens": [50470, 300, 291, 393, 767, 1867, 13, 50538], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 893, "seek": 217594, "start": 2179.42, "end": 2181.02, "text": " And I think what you're pointing out is", "tokens": [50538, 400, 286, 519, 437, 291, 434, 12166, 484, 307, 50618], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 894, "seek": 217594, "start": 2181.02, "end": 2183.1, "text": " this doesn't seem to address the second source", "tokens": [50618, 341, 1177, 380, 1643, 281, 2985, 264, 1150, 4009, 50722], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 895, "seek": 217594, "start": 2183.1, "end": 2184.58, "text": " of brittleness, which is that your system", "tokens": [50722, 295, 738, 593, 45887, 11, 597, 307, 300, 428, 1185, 50796], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 896, "seek": 217594, "start": 2184.58, "end": 2185.58, "text": " can only answer certain things,", "tokens": [50796, 393, 787, 1867, 1629, 721, 11, 50846], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 897, "seek": 217594, "start": 2185.58, "end": 2187.46, "text": " it can only reason about certain things.", "tokens": [50846, 309, 393, 787, 1778, 466, 1629, 721, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 898, "seek": 217594, "start": 2188.46, "end": 2192.18, "text": " Brittleness of the formal representation language", "tokens": [50990, 1603, 593, 45887, 295, 264, 9860, 10290, 2856, 51176], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 899, "seek": 217594, "start": 2192.18, "end": 2193.02, "text": " that you're using.", "tokens": [51176, 300, 291, 434, 1228, 13, 51218], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 900, "seek": 217594, "start": 2193.02, "end": 2198.02, "text": " Right, that's true of large language model representations.", "tokens": [51218, 1779, 11, 300, 311, 2074, 295, 2416, 2856, 2316, 33358, 13, 51468], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 901, "seek": 217594, "start": 2198.86, "end": 2203.86, "text": " Right, so I think my sort of take on that", "tokens": [51510, 1779, 11, 370, 286, 519, 452, 1333, 295, 747, 322, 300, 51760], "temperature": 0.0, "avg_logprob": -0.2283380973238905, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0007201380794867873}, {"id": 902, "seek": 220386, "start": 2204.38, "end": 2208.1, "text": " is from a kind of AI engineering perspective", "tokens": [50390, 307, 490, 257, 733, 295, 7318, 7043, 4585, 50576], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 903, "seek": 220386, "start": 2208.1, "end": 2212.86, "text": " is sort of a branching in two directions.", "tokens": [50576, 307, 1333, 295, 257, 9819, 278, 294, 732, 11095, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 904, "seek": 220386, "start": 2212.86, "end": 2215.7400000000002, "text": " One is, I think we have made some progress", "tokens": [50814, 1485, 307, 11, 286, 519, 321, 362, 1027, 512, 4205, 50958], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 905, "seek": 220386, "start": 2215.7400000000002, "end": 2219.34, "text": " that this is not really evoking, probably,", "tokens": [50958, 300, 341, 307, 406, 534, 1073, 5953, 11, 1391, 11, 51138], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 906, "seek": 220386, "start": 2219.34, "end": 2221.98, "text": " toward systems that within restricted domains", "tokens": [51138, 7361, 3652, 300, 1951, 20608, 25514, 51270], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 907, "seek": 220386, "start": 2223.1, "end": 2226.1400000000003, "text": " can reason coherently and probabilistically", "tokens": [51326, 393, 1778, 26528, 2276, 293, 31959, 20458, 51478], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 908, "seek": 220386, "start": 2226.1400000000003, "end": 2227.9, "text": " about a wide range of queries.", "tokens": [51478, 466, 257, 4874, 3613, 295, 24109, 13, 51566], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 909, "seek": 220386, "start": 2227.9, "end": 2232.9, "text": " So we have systems like this inference QL system", "tokens": [51566, 407, 321, 362, 3652, 411, 341, 38253, 1249, 43, 1185, 51816], "temperature": 0.0, "avg_logprob": -0.11682386176530705, "compression_ratio": 1.576036866359447, "no_speech_prob": 0.0003798770485445857}, {"id": 910, "seek": 223290, "start": 2233.1800000000003, "end": 2236.1800000000003, "text": " that uses nonparametric bays to analyze huge data tables", "tokens": [50378, 300, 4960, 2107, 2181, 335, 17475, 272, 3772, 281, 12477, 2603, 1412, 8020, 50528], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 911, "seek": 223290, "start": 2236.1800000000003, "end": 2239.02, "text": " and come up with a model of that system", "tokens": [50528, 293, 808, 493, 365, 257, 2316, 295, 300, 1185, 50670], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 912, "seek": 223290, "start": 2239.02, "end": 2242.6600000000003, "text": " that or of your data that can answer all sorts of questions", "tokens": [50670, 300, 420, 295, 428, 1412, 300, 393, 1867, 439, 7527, 295, 1651, 50852], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 913, "seek": 223290, "start": 2242.6600000000003, "end": 2247.6600000000003, "text": " like, oh, you know, show me like which people", "tokens": [50852, 411, 11, 1954, 11, 291, 458, 11, 855, 385, 411, 597, 561, 51102], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 914, "seek": 223290, "start": 2249.38, "end": 2252.42, "text": " in this data set are like probably overpaid", "tokens": [51188, 294, 341, 1412, 992, 366, 411, 1391, 670, 35035, 51340], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 915, "seek": 223290, "start": 2252.42, "end": 2254.34, "text": " given their experience or something like that.", "tokens": [51340, 2212, 641, 1752, 420, 746, 411, 300, 13, 51436], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 916, "seek": 223290, "start": 2254.34, "end": 2256.82, "text": " So in the same way that people are kind of excited", "tokens": [51436, 407, 294, 264, 912, 636, 300, 561, 366, 733, 295, 2919, 51560], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 917, "seek": 223290, "start": 2256.82, "end": 2259.86, "text": " about using natural language or using language models", "tokens": [51560, 466, 1228, 3303, 2856, 420, 1228, 2856, 5245, 51712], "temperature": 0.0, "avg_logprob": -0.17052352905273438, "compression_ratio": 1.6793248945147679, "no_speech_prob": 0.0004581950488500297}, {"id": 918, "seek": 225986, "start": 2259.86, "end": 2262.5, "text": " to parse into SQL, right?", "tokens": [50364, 281, 48377, 666, 19200, 11, 558, 30, 50496], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 919, "seek": 225986, "start": 2262.5, "end": 2264.98, "text": " Because so much data is in SQL", "tokens": [50496, 1436, 370, 709, 1412, 307, 294, 19200, 50620], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 920, "seek": 225986, "start": 2264.98, "end": 2268.2200000000003, "text": " and it's a very SQL is a very expressive language", "tokens": [50620, 293, 309, 311, 257, 588, 19200, 307, 257, 588, 40189, 2856, 50782], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 921, "seek": 225986, "start": 2268.2200000000003, "end": 2270.2200000000003, "text": " for asking questions about that data.", "tokens": [50782, 337, 3365, 1651, 466, 300, 1412, 13, 50882], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 922, "seek": 225986, "start": 2271.3, "end": 2273.2200000000003, "text": " When we have a probabilistic system,", "tokens": [50936, 1133, 321, 362, 257, 31959, 3142, 1185, 11, 51032], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 923, "seek": 225986, "start": 2273.2200000000003, "end": 2275.7400000000002, "text": " like a good probabilistic model of that data under the hood,", "tokens": [51032, 411, 257, 665, 31959, 3142, 2316, 295, 300, 1412, 833, 264, 13376, 11, 51158], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 924, "seek": 225986, "start": 2275.7400000000002, "end": 2278.58, "text": " it enables conversational patterns that are not enabled", "tokens": [51158, 309, 17077, 2615, 1478, 8294, 300, 366, 406, 15172, 51300], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 925, "seek": 225986, "start": 2278.58, "end": 2281.02, "text": " when you have like SQL as the database", "tokens": [51300, 562, 291, 362, 411, 19200, 382, 264, 8149, 51422], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 926, "seek": 225986, "start": 2281.02, "end": 2283.02, "text": " because we expect our conversational partners", "tokens": [51422, 570, 321, 2066, 527, 2615, 1478, 4462, 51522], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 927, "seek": 225986, "start": 2283.02, "end": 2285.1, "text": " to have coherent beliefs about the world,", "tokens": [51522, 281, 362, 36239, 13585, 466, 264, 1002, 11, 51626], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 928, "seek": 225986, "start": 2285.1, "end": 2287.6200000000003, "text": " to update those beliefs in response to new evidence", "tokens": [51626, 281, 5623, 729, 13585, 294, 4134, 281, 777, 4467, 51752], "temperature": 0.0, "avg_logprob": -0.130868944071107, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.0008294006693176925}, {"id": 929, "seek": 228762, "start": 2287.66, "end": 2291.1, "text": " that we give it to be able to report uncertainty", "tokens": [50366, 300, 321, 976, 309, 281, 312, 1075, 281, 2275, 15697, 50538], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 930, "seek": 228762, "start": 2292.54, "end": 2294.2999999999997, "text": " and make sort of modal judgments.", "tokens": [50610, 293, 652, 1333, 295, 39745, 40337, 13, 50698], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 931, "seek": 228762, "start": 2294.2999999999997, "end": 2298.8599999999997, "text": " And so one engineering path is to take those kinds of systems", "tokens": [50698, 400, 370, 472, 7043, 3100, 307, 281, 747, 729, 3685, 295, 3652, 50926], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 932, "seek": 228762, "start": 2298.8599999999997, "end": 2302.7, "text": " and sort of build conversational interfaces to them", "tokens": [50926, 293, 1333, 295, 1322, 2615, 1478, 28416, 281, 552, 51118], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 933, "seek": 228762, "start": 2302.7, "end": 2305.8599999999997, "text": " that behave more like an intelligent person would behave", "tokens": [51118, 300, 15158, 544, 411, 364, 13232, 954, 576, 15158, 51276], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 934, "seek": 228762, "start": 2305.8599999999997, "end": 2308.7, "text": " and can draw inferences that you might not draw", "tokens": [51276, 293, 393, 2642, 13596, 2667, 300, 291, 1062, 406, 2642, 51418], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 935, "seek": 228762, "start": 2308.7, "end": 2310.8199999999997, "text": " if you're just talking to a SQL backup.", "tokens": [51418, 498, 291, 434, 445, 1417, 281, 257, 19200, 14807, 13, 51524], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 936, "seek": 228762, "start": 2310.8199999999997, "end": 2313.02, "text": " The other path that sort of we'll talk about", "tokens": [51524, 440, 661, 3100, 300, 1333, 295, 321, 603, 751, 466, 51634], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 937, "seek": 228762, "start": 2313.02, "end": 2315.54, "text": " in the next part of the talk", "tokens": [51634, 294, 264, 958, 644, 295, 264, 751, 51760], "temperature": 0.0, "avg_logprob": -0.125076202245859, "compression_ratio": 1.6338582677165354, "no_speech_prob": 0.0007552713504992425}, {"id": 938, "seek": 231554, "start": 2315.58, "end": 2317.7799999999997, "text": " is how can we use those representations", "tokens": [50366, 307, 577, 393, 321, 764, 729, 33358, 50476], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 939, "seek": 231554, "start": 2317.7799999999997, "end": 2319.22, "text": " that language models have learned", "tokens": [50476, 300, 2856, 5245, 362, 3264, 50548], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 940, "seek": 231554, "start": 2319.22, "end": 2323.18, "text": " to make the probabilistic inferences more interesting", "tokens": [50548, 281, 652, 264, 31959, 3142, 13596, 2667, 544, 1880, 50746], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 941, "seek": 231554, "start": 2323.18, "end": 2325.1, "text": " and more robust, less brittle,", "tokens": [50746, 293, 544, 13956, 11, 1570, 49325, 11, 50842], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 942, "seek": 231554, "start": 2325.1, "end": 2328.58, "text": " without sort of totally embracing the other kind of brittleness", "tokens": [50842, 1553, 1333, 295, 3879, 31596, 264, 661, 733, 295, 738, 593, 45887, 51016], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 943, "seek": 231554, "start": 2328.58, "end": 2330.18, "text": " which is the kind of brittleness that language models", "tokens": [51016, 597, 307, 264, 733, 295, 738, 593, 45887, 300, 2856, 5245, 51096], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 944, "seek": 231554, "start": 2330.18, "end": 2331.02, "text": " seem to have right now,", "tokens": [51096, 1643, 281, 362, 558, 586, 11, 51138], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 945, "seek": 231554, "start": 2331.02, "end": 2332.54, "text": " which is that they draw,", "tokens": [51138, 597, 307, 300, 436, 2642, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 946, "seek": 231554, "start": 2332.54, "end": 2333.94, "text": " that they don't really necessarily reason", "tokens": [51214, 300, 436, 500, 380, 534, 4725, 1778, 51284], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 947, "seek": 231554, "start": 2333.94, "end": 2336.34, "text": " with coherent probabilistic beliefs.", "tokens": [51284, 365, 36239, 31959, 3142, 13585, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 948, "seek": 231554, "start": 2336.34, "end": 2339.34, "text": " So maybe, yeah, let's go into that next part.", "tokens": [51404, 407, 1310, 11, 1338, 11, 718, 311, 352, 666, 300, 958, 644, 13, 51554], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 949, "seek": 231554, "start": 2339.34, "end": 2343.34, "text": " Yes, yes, okay, right.", "tokens": [51554, 1079, 11, 2086, 11, 1392, 11, 558, 13, 51754], "temperature": 0.0, "avg_logprob": -0.1873057605774422, "compression_ratio": 1.8404669260700388, "no_speech_prob": 0.00037391085061244667}, {"id": 950, "seek": 234334, "start": 2344.3, "end": 2346.54, "text": " Well, so right, so in the interest of time,", "tokens": [50412, 1042, 11, 370, 558, 11, 370, 294, 264, 1179, 295, 565, 11, 50524], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 951, "seek": 234334, "start": 2346.54, "end": 2347.94, "text": " I'm actually gonna like skip through", "tokens": [50524, 286, 478, 767, 799, 411, 10023, 807, 50594], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 952, "seek": 234334, "start": 2347.94, "end": 2349.2200000000003, "text": " some of the rest of this example,", "tokens": [50594, 512, 295, 264, 1472, 295, 341, 1365, 11, 50658], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 953, "seek": 234334, "start": 2349.2200000000003, "end": 2351.6600000000003, "text": " which I think is just more of what you've seen,", "tokens": [50658, 597, 286, 519, 307, 445, 544, 295, 437, 291, 600, 1612, 11, 50780], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 954, "seek": 234334, "start": 2351.6600000000003, "end": 2353.86, "text": " but one sense in which I think,", "tokens": [50780, 457, 472, 2020, 294, 597, 286, 519, 11, 50890], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 955, "seek": 234334, "start": 2353.86, "end": 2356.54, "text": " yeah, maybe a third part of the answer to Chris,", "tokens": [50890, 1338, 11, 1310, 257, 2636, 644, 295, 264, 1867, 281, 6688, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 956, "seek": 234334, "start": 2356.54, "end": 2357.7400000000002, "text": " I would say, is that,", "tokens": [51024, 286, 576, 584, 11, 307, 300, 11, 51084], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 957, "seek": 234334, "start": 2360.46, "end": 2362.3, "text": " right, you know, yeah.", "tokens": [51220, 558, 11, 291, 458, 11, 1338, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 958, "seek": 234334, "start": 2362.3, "end": 2363.9, "text": " I think part of what this is trying to do", "tokens": [51312, 286, 519, 644, 295, 437, 341, 307, 1382, 281, 360, 51392], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 959, "seek": 234334, "start": 2363.9, "end": 2366.7000000000003, "text": " is explore some of the ways in which we might answer ways,", "tokens": [51392, 307, 6839, 512, 295, 264, 2098, 294, 597, 321, 1062, 1867, 2098, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 960, "seek": 234334, "start": 2366.7000000000003, "end": 2369.82, "text": " right, without giving an answer in all the ways in which", "tokens": [51532, 558, 11, 1553, 2902, 364, 1867, 294, 439, 264, 2098, 294, 597, 51688], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 961, "seek": 234334, "start": 2369.82, "end": 2372.78, "text": " we might answer the ways in which language models themselves", "tokens": [51688, 321, 1062, 1867, 264, 2098, 294, 597, 2856, 5245, 2969, 51836], "temperature": 0.0, "avg_logprob": -0.1757689184612698, "compression_ratio": 1.9425287356321839, "no_speech_prob": 0.0005613260436803102}, {"id": 962, "seek": 237278, "start": 2372.78, "end": 2375.26, "text": " are brittle with respect to what we also want", "tokens": [50364, 366, 49325, 365, 3104, 281, 437, 321, 611, 528, 50488], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 963, "seek": 237278, "start": 2375.26, "end": 2376.86, "text": " from a model of intelligence, right?", "tokens": [50488, 490, 257, 2316, 295, 7599, 11, 558, 30, 50568], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 964, "seek": 237278, "start": 2376.86, "end": 2378.98, "text": " We might suspect that when we answer,", "tokens": [50568, 492, 1062, 9091, 300, 562, 321, 1867, 11, 50674], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 965, "seek": 237278, "start": 2378.98, "end": 2380.5400000000004, "text": " ask questions like this,", "tokens": [50674, 1029, 1651, 411, 341, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 966, "seek": 237278, "start": 2380.5400000000004, "end": 2382.98, "text": " really what we are trying to do is specify", "tokens": [50752, 534, 437, 321, 366, 1382, 281, 360, 307, 16500, 50874], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 967, "seek": 237278, "start": 2382.98, "end": 2386.98, "text": " some kind of normative query that captures formally", "tokens": [50874, 512, 733, 295, 2026, 1166, 14581, 300, 27986, 25983, 51074], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 968, "seek": 237278, "start": 2386.98, "end": 2390.26, "text": " a sense of, well, we want something like the posterior", "tokens": [51074, 257, 2020, 295, 11, 731, 11, 321, 528, 746, 411, 264, 33529, 51238], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 969, "seek": 237278, "start": 2390.26, "end": 2392.98, "text": " with respect to some kind of internal model of the world.", "tokens": [51238, 365, 3104, 281, 512, 733, 295, 6920, 2316, 295, 264, 1002, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 970, "seek": 237278, "start": 2392.98, "end": 2395.78, "text": " And, you know, this is kind of the simplest means,", "tokens": [51374, 400, 11, 291, 458, 11, 341, 307, 733, 295, 264, 22811, 1355, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 971, "seek": 237278, "start": 2395.78, "end": 2397.6200000000003, "text": " or this is a very simple example", "tokens": [51514, 420, 341, 307, 257, 588, 2199, 1365, 51606], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 972, "seek": 237278, "start": 2397.6200000000003, "end": 2400.38, "text": " of how we might formally impose that kind of structure,", "tokens": [51606, 295, 577, 321, 1062, 25983, 26952, 300, 733, 295, 3877, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1103922851442352, "compression_ratio": 1.8533834586466165, "no_speech_prob": 0.000666349078528583}, {"id": 973, "seek": 240038, "start": 2401.38, "end": 2403.62, "text": " but one that I think can be elaborated on,", "tokens": [50414, 457, 472, 300, 286, 519, 393, 312, 16298, 770, 322, 11, 50526], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 974, "seek": 240038, "start": 2405.3, "end": 2407.6600000000003, "text": " depending on the kinds of primitives", "tokens": [50610, 5413, 322, 264, 3685, 295, 2886, 38970, 50728], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 975, "seek": 240038, "start": 2407.6600000000003, "end": 2409.1400000000003, "text": " and the ways in which you're thinking about", "tokens": [50728, 293, 264, 2098, 294, 597, 291, 434, 1953, 466, 50802], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 976, "seek": 240038, "start": 2409.1400000000003, "end": 2412.2200000000003, "text": " what it is that the probabilistic programs can express, right?", "tokens": [50802, 437, 309, 307, 300, 264, 31959, 3142, 4268, 393, 5109, 11, 558, 30, 50956], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 977, "seek": 240038, "start": 2412.2200000000003, "end": 2415.46, "text": " So one way in which we might think about doing that", "tokens": [50956, 407, 472, 636, 294, 597, 321, 1062, 519, 466, 884, 300, 51118], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 978, "seek": 240038, "start": 2416.58, "end": 2419.82, "text": " is by thinking about probabilistic programs", "tokens": [51174, 307, 538, 1953, 466, 31959, 3142, 4268, 51336], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 979, "seek": 240038, "start": 2419.82, "end": 2423.6600000000003, "text": " that themselves have access to other kinds of means", "tokens": [51336, 300, 2969, 362, 2105, 281, 661, 3685, 295, 1355, 51528], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 980, "seek": 240038, "start": 2423.6600000000003, "end": 2426.82, "text": " of calling other different mechanisms and cognition, right?", "tokens": [51528, 295, 5141, 661, 819, 15902, 293, 46905, 11, 558, 30, 51686], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 981, "seek": 240038, "start": 2426.82, "end": 2429.1800000000003, "text": " So I think I would draw a contrast here", "tokens": [51686, 407, 286, 519, 286, 576, 2642, 257, 8712, 510, 51804], "temperature": 0.0, "avg_logprob": -0.12651560441502985, "compression_ratio": 1.8083333333333333, "no_speech_prob": 0.00013975439651403576}, {"id": 982, "seek": 242918, "start": 2429.2999999999997, "end": 2431.8999999999996, "text": " between the notion of the large language model", "tokens": [50370, 1296, 264, 10710, 295, 264, 2416, 2856, 2316, 50500], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 983, "seek": 242918, "start": 2431.8999999999996, "end": 2434.62, "text": " as a controller, the one that's making the decisions", "tokens": [50500, 382, 257, 10561, 11, 264, 472, 300, 311, 1455, 264, 5327, 50636], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 984, "seek": 242918, "start": 2434.62, "end": 2437.3399999999997, "text": " about when to write little snippets of code", "tokens": [50636, 466, 562, 281, 2464, 707, 35623, 1385, 295, 3089, 50772], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 985, "seek": 242918, "start": 2437.3399999999997, "end": 2441.46, "text": " and to execute them, when to call out to little planners", "tokens": [50772, 293, 281, 14483, 552, 11, 562, 281, 818, 484, 281, 707, 49674, 50978], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 986, "seek": 242918, "start": 2441.46, "end": 2444.3399999999997, "text": " and incorporate them, or stuff like the Minds Eye work,", "tokens": [50978, 293, 16091, 552, 11, 420, 1507, 411, 264, 13719, 82, 21603, 589, 11, 51122], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 987, "seek": 242918, "start": 2444.3399999999997, "end": 2446.14, "text": " right, where there's a language model,", "tokens": [51122, 558, 11, 689, 456, 311, 257, 2856, 2316, 11, 51212], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 988, "seek": 242918, "start": 2446.14, "end": 2448.54, "text": " it decides when to call out to a physics simulator,", "tokens": [51212, 309, 14898, 562, 281, 818, 484, 281, 257, 10649, 32974, 11, 51332], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 989, "seek": 242918, "start": 2448.54, "end": 2451.46, "text": " but the way it interprets the outputs", "tokens": [51332, 457, 264, 636, 309, 17489, 1373, 264, 23930, 51478], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 990, "seek": 242918, "start": 2451.46, "end": 2453.62, "text": " of that physics simulator is to paste those back", "tokens": [51478, 295, 300, 10649, 32974, 307, 281, 9163, 729, 646, 51586], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 991, "seek": 242918, "start": 2453.62, "end": 2455.7, "text": " into the language model context", "tokens": [51586, 666, 264, 2856, 2316, 4319, 51690], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 992, "seek": 242918, "start": 2455.7, "end": 2458.54, "text": " and try to draw inferences on them in turn.", "tokens": [51690, 293, 853, 281, 2642, 13596, 2667, 322, 552, 294, 1261, 13, 51832], "temperature": 0.0, "avg_logprob": -0.09962585155780498, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.00024532570387236774}, {"id": 993, "seek": 245854, "start": 2458.54, "end": 2460.58, "text": " Rather, in this kind of framework, right,", "tokens": [50364, 16571, 11, 294, 341, 733, 295, 8388, 11, 558, 11, 50466], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 994, "seek": 245854, "start": 2460.58, "end": 2463.46, "text": " what you can kind of see a, or yeah,", "tokens": [50466, 437, 291, 393, 733, 295, 536, 257, 11, 420, 1338, 11, 50610], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 995, "seek": 245854, "start": 2463.46, "end": 2465.7, "text": " the direction that this framework would be pointing towards", "tokens": [50610, 264, 3513, 300, 341, 8388, 576, 312, 12166, 3030, 50722], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 996, "seek": 245854, "start": 2465.7, "end": 2469.3, "text": " is to say, well, on the other hand,", "tokens": [50722, 307, 281, 584, 11, 731, 11, 322, 264, 661, 1011, 11, 50902], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 997, "seek": 245854, "start": 2469.3, "end": 2472.7, "text": " we already have languages that allow us to do things", "tokens": [50902, 321, 1217, 362, 8650, 300, 2089, 505, 281, 360, 721, 51072], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 998, "seek": 245854, "start": 2472.7, "end": 2474.98, "text": " like build expressive generative models", "tokens": [51072, 411, 1322, 40189, 1337, 1166, 5245, 51186], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 999, "seek": 245854, "start": 2474.98, "end": 2477.9, "text": " over three-dimensional scenes that also capture things", "tokens": [51186, 670, 1045, 12, 18759, 8026, 300, 611, 7983, 721, 51332], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 1000, "seek": 245854, "start": 2477.9, "end": 2480.02, "text": " that we might want only from perception,", "tokens": [51332, 300, 321, 1062, 528, 787, 490, 12860, 11, 51438], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 1001, "seek": 245854, "start": 2480.02, "end": 2482.54, "text": " like knowledge about how the shapes of objects", "tokens": [51438, 411, 3601, 466, 577, 264, 10854, 295, 6565, 51564], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 1002, "seek": 245854, "start": 2482.54, "end": 2484.38, "text": " tend to accude each other,", "tokens": [51564, 3928, 281, 1317, 2303, 1184, 661, 11, 51656], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 1003, "seek": 245854, "start": 2484.38, "end": 2487.46, "text": " or incorporate rich models of physics,", "tokens": [51656, 420, 16091, 4593, 5245, 295, 10649, 11, 51810], "temperature": 0.0, "avg_logprob": -0.15322184562683105, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0002531027712393552}, {"id": 1004, "seek": 248746, "start": 2487.46, "end": 2491.34, "text": " or that model theory of mind as taking place recursively", "tokens": [50364, 420, 300, 2316, 5261, 295, 1575, 382, 1940, 1081, 20560, 3413, 50558], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1005, "seek": 248746, "start": 2491.34, "end": 2493.86, "text": " and thinking about agents who themselves have beliefs", "tokens": [50558, 293, 1953, 466, 12554, 567, 2969, 362, 13585, 50684], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1006, "seek": 248746, "start": 2493.86, "end": 2495.78, "text": " about their own internal world models", "tokens": [50684, 466, 641, 1065, 6920, 1002, 5245, 50780], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1007, "seek": 248746, "start": 2495.78, "end": 2499.42, "text": " and are actually choosing their actions as planners, right?", "tokens": [50780, 293, 366, 767, 10875, 641, 5909, 382, 49674, 11, 558, 30, 50962], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1008, "seek": 248746, "start": 2499.42, "end": 2502.06, "text": " And in this kind of framework,", "tokens": [50962, 400, 294, 341, 733, 295, 8388, 11, 51094], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1009, "seek": 248746, "start": 2502.06, "end": 2504.9, "text": " you can point the way towards a kind of model that says,", "tokens": [51094, 291, 393, 935, 264, 636, 3030, 257, 733, 295, 2316, 300, 1619, 11, 51236], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1010, "seek": 248746, "start": 2504.9, "end": 2507.78, "text": " well, how is it that I might incorporate language", "tokens": [51236, 731, 11, 577, 307, 309, 300, 286, 1062, 16091, 2856, 51380], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1011, "seek": 248746, "start": 2507.78, "end": 2510.34, "text": " into these kinds of models sitting alongside", "tokens": [51380, 666, 613, 3685, 295, 5245, 3798, 12385, 51508], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1012, "seek": 248746, "start": 2510.34, "end": 2513.14, "text": " these other kind of observations that I might make, right?", "tokens": [51508, 613, 661, 733, 295, 18163, 300, 286, 1062, 652, 11, 558, 30, 51648], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1013, "seek": 248746, "start": 2513.14, "end": 2516.14, "text": " So how might I think about the meaning", "tokens": [51648, 407, 577, 1062, 286, 519, 466, 264, 3620, 51798], "temperature": 0.0, "avg_logprob": -0.10627720874288808, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.00015838778926990926}, {"id": 1014, "seek": 251614, "start": 2516.3799999999997, "end": 2518.94, "text": " of images that I want to generate", "tokens": [50376, 295, 5267, 300, 286, 528, 281, 8460, 50504], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1015, "seek": 251614, "start": 2518.94, "end": 2521.2599999999998, "text": " that specify specific constraints,", "tokens": [50504, 300, 16500, 2685, 18491, 11, 50620], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1016, "seek": 251614, "start": 2521.2599999999998, "end": 2525.42, "text": " or imagination, or, right, go ahead, Jacob.", "tokens": [50620, 420, 12938, 11, 420, 11, 558, 11, 352, 2286, 11, 14117, 13, 50828], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1017, "seek": 251614, "start": 2525.42, "end": 2527.06, "text": " Yeah, I just had a clarification question.", "tokens": [50828, 865, 11, 286, 445, 632, 257, 34449, 1168, 13, 50910], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1018, "seek": 251614, "start": 2527.06, "end": 2531.06, "text": " So you were talking earlier about having this meeting function,", "tokens": [50910, 407, 291, 645, 1417, 3071, 466, 1419, 341, 3440, 2445, 11, 51110], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1019, "seek": 251614, "start": 2531.06, "end": 2533.3399999999997, "text": " and then I think also we're mentioning something", "tokens": [51110, 293, 550, 286, 519, 611, 321, 434, 18315, 746, 51224], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1020, "seek": 251614, "start": 2533.3399999999997, "end": 2538.3399999999997, "text": " about like code x, in terms of the questions.", "tokens": [51224, 466, 411, 3089, 2031, 11, 294, 2115, 295, 264, 1651, 13, 51474], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1021, "seek": 251614, "start": 2538.42, "end": 2541.54, "text": " I'm just trying to understand which of these is that.", "tokens": [51478, 286, 478, 445, 1382, 281, 1223, 597, 295, 613, 307, 300, 13, 51634], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1022, "seek": 251614, "start": 2541.54, "end": 2543.46, "text": " Is that here, or is the meeting function", "tokens": [51634, 1119, 300, 510, 11, 420, 307, 264, 3440, 2445, 51730], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1023, "seek": 251614, "start": 2543.46, "end": 2544.7, "text": " when you come later?", "tokens": [51730, 562, 291, 808, 1780, 30, 51792], "temperature": 0.0, "avg_logprob": -0.30778023174830843, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0018095733830705285}, {"id": 1024, "seek": 254470, "start": 2545.66, "end": 2546.7799999999997, "text": " So that's maybe the first question", "tokens": [50412, 407, 300, 311, 1310, 264, 700, 1168, 50468], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1025, "seek": 254470, "start": 2546.7799999999997, "end": 2547.8599999999997, "text": " and then the other clarification is,", "tokens": [50468, 293, 550, 264, 661, 34449, 307, 11, 50522], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1026, "seek": 254470, "start": 2547.8599999999997, "end": 2550.4199999999996, "text": " so are these statements actually just been programmatically", "tokens": [50522, 370, 366, 613, 12363, 767, 445, 668, 37648, 5030, 50650], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1027, "seek": 254470, "start": 2550.4199999999996, "end": 2554.62, "text": " created from code x by prompting the text that was on?", "tokens": [50650, 2942, 490, 3089, 2031, 538, 12391, 278, 264, 2487, 300, 390, 322, 30, 50860], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1028, "seek": 254470, "start": 2554.62, "end": 2555.46, "text": " Yes, that's right.", "tokens": [50860, 1079, 11, 300, 311, 558, 13, 50902], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1029, "seek": 254470, "start": 2555.46, "end": 2560.46, "text": " So by meeting function in this framework, we say,", "tokens": [50902, 407, 538, 3440, 2445, 294, 341, 8388, 11, 321, 584, 11, 51152], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1030, "seek": 254470, "start": 2560.5, "end": 2562.1, "text": " well, there's kind of two generalizations", "tokens": [51154, 731, 11, 456, 311, 733, 295, 732, 2674, 14455, 51234], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1031, "seek": 254470, "start": 2562.1, "end": 2562.9399999999996, "text": " of a meeting function.", "tokens": [51234, 295, 257, 3440, 2445, 13, 51276], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1032, "seek": 254470, "start": 2562.9399999999996, "end": 2565.14, "text": " There's a general joint prior, right?", "tokens": [51276, 821, 311, 257, 2674, 7225, 4059, 11, 558, 30, 51386], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1033, "seek": 254470, "start": 2565.14, "end": 2566.66, "text": " That code x is already,", "tokens": [51386, 663, 3089, 2031, 307, 1217, 11, 51462], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1034, "seek": 254470, "start": 2566.66, "end": 2567.9399999999996, "text": " that it's learned between language and code,", "tokens": [51462, 300, 309, 311, 3264, 1296, 2856, 293, 3089, 11, 51526], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1035, "seek": 254470, "start": 2567.9399999999996, "end": 2570.74, "text": " and then there's this kind of context specific", "tokens": [51526, 293, 550, 456, 311, 341, 733, 295, 4319, 2685, 51666], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1036, "seek": 254470, "start": 2570.74, "end": 2573.2999999999997, "text": " meeting function in the sense that it's conditioned", "tokens": [51666, 3440, 2445, 294, 264, 2020, 300, 309, 311, 35833, 51794], "temperature": 0.0, "avg_logprob": -0.177211719707851, "compression_ratio": 1.8263888888888888, "no_speech_prob": 0.00015840568812564015}, {"id": 1037, "seek": 257330, "start": 2573.3, "end": 2575.86, "text": " on whatever's in the prompt, the generative model,", "tokens": [50364, 322, 2035, 311, 294, 264, 12391, 11, 264, 1337, 1166, 2316, 11, 50492], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1038, "seek": 257330, "start": 2575.86, "end": 2579.94, "text": " and some examples of how language relates to expressions", "tokens": [50492, 293, 512, 5110, 295, 577, 2856, 16155, 281, 15277, 50696], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1039, "seek": 257330, "start": 2579.94, "end": 2581.6600000000003, "text": " that it's doing, so that's a meeting function.", "tokens": [50696, 300, 309, 311, 884, 11, 370, 300, 311, 257, 3440, 2445, 13, 50782], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1040, "seek": 257330, "start": 2581.6600000000003, "end": 2583.5, "text": " And yes, all these examples that you're seeing", "tokens": [50782, 400, 2086, 11, 439, 613, 5110, 300, 291, 434, 2577, 50874], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1041, "seek": 257330, "start": 2583.5, "end": 2585.98, "text": " are one sample from that distribution.", "tokens": [50874, 366, 472, 6889, 490, 300, 7316, 13, 50998], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1042, "seek": 257330, "start": 2589.5, "end": 2593.0600000000004, "text": " Right, and one of the things that I wanna point to here,", "tokens": [51174, 1779, 11, 293, 472, 295, 264, 721, 300, 286, 1948, 935, 281, 510, 11, 51352], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1043, "seek": 257330, "start": 2593.0600000000004, "end": 2595.7400000000002, "text": " right, is it does, I think it suggests a framework", "tokens": [51352, 558, 11, 307, 309, 775, 11, 286, 519, 309, 13409, 257, 8388, 51486], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1044, "seek": 257330, "start": 2595.7400000000002, "end": 2598.2200000000003, "text": " or another means of thinking about what it means", "tokens": [51486, 420, 1071, 1355, 295, 1953, 466, 437, 309, 1355, 51610], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1045, "seek": 257330, "start": 2598.2200000000003, "end": 2601.6600000000003, "text": " for language to construct new concepts from definitions,", "tokens": [51610, 337, 2856, 281, 7690, 777, 10392, 490, 21988, 11, 51782], "temperature": 0.0, "avg_logprob": -0.18040054722836144, "compression_ratio": 1.7196969696969697, "no_speech_prob": 8.480459655402228e-05}, {"id": 1046, "seek": 260166, "start": 2601.7, "end": 2603.46, "text": " or even come to construct new world models", "tokens": [50366, 420, 754, 808, 281, 7690, 777, 1002, 5245, 50454], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1047, "seek": 260166, "start": 2603.46, "end": 2605.8199999999997, "text": " from thinking like somebody in the beginning asked, right?", "tokens": [50454, 490, 1953, 411, 2618, 294, 264, 2863, 2351, 11, 558, 30, 50572], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1048, "seek": 260166, "start": 2605.8199999999997, "end": 2609.5, "text": " So how, for instance, might we think about enriching", "tokens": [50572, 407, 577, 11, 337, 5197, 11, 1062, 321, 519, 466, 18849, 278, 50756], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1049, "seek": 260166, "start": 2609.5, "end": 2611.94, "text": " an existing structured relational model", "tokens": [50756, 364, 6741, 18519, 38444, 2316, 50878], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1050, "seek": 260166, "start": 2611.94, "end": 2613.8999999999996, "text": " with concepts that we learn from language?", "tokens": [50878, 365, 10392, 300, 321, 1466, 490, 2856, 30, 50976], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1051, "seek": 260166, "start": 2613.8999999999996, "end": 2618.06, "text": " So for example, if we consider kind of a formal model", "tokens": [50976, 407, 337, 1365, 11, 498, 321, 1949, 733, 295, 257, 9860, 2316, 51184], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1052, "seek": 260166, "start": 2618.06, "end": 2623.06, "text": " of kinship relations, we might say that, well,", "tokens": [51184, 295, 350, 1292, 1210, 2299, 11, 321, 1062, 584, 300, 11, 731, 11, 51434], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1053, "seek": 260166, "start": 2623.8999999999996, "end": 2626.02, "text": " the generative model of this domain", "tokens": [51476, 264, 1337, 1166, 2316, 295, 341, 9274, 51582], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1054, "seek": 260166, "start": 2626.02, "end": 2629.3399999999997, "text": " is itself represented as a probabilistic program.", "tokens": [51582, 307, 2564, 10379, 382, 257, 31959, 3142, 1461, 13, 51748], "temperature": 0.0, "avg_logprob": -0.10945935930524553, "compression_ratio": 1.6892430278884463, "no_speech_prob": 7.719927089056e-05}, {"id": 1055, "seek": 262934, "start": 2629.34, "end": 2632.94, "text": " It captures both the causal means by which", "tokens": [50364, 467, 27986, 1293, 264, 38755, 1355, 538, 597, 50544], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1056, "seek": 262934, "start": 2634.1000000000004, "end": 2636.9, "text": " people give rise to their children,", "tokens": [50602, 561, 976, 6272, 281, 641, 2227, 11, 50742], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1057, "seek": 262934, "start": 2636.9, "end": 2640.54, "text": " and also the definitions or one notion of the definitions", "tokens": [50742, 293, 611, 264, 21988, 420, 472, 10710, 295, 264, 21988, 50924], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1058, "seek": 262934, "start": 2640.54, "end": 2642.6200000000003, "text": " of what it means to be something like a sister or a father", "tokens": [50924, 295, 437, 309, 1355, 281, 312, 746, 411, 257, 4892, 420, 257, 3086, 51028], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1059, "seek": 262934, "start": 2642.6200000000003, "end": 2644.78, "text": " with respect to this core notion", "tokens": [51028, 365, 3104, 281, 341, 4965, 10710, 51136], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1060, "seek": 262934, "start": 2644.78, "end": 2647.3, "text": " of how family trees come to be.", "tokens": [51136, 295, 577, 1605, 5852, 808, 281, 312, 13, 51262], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1061, "seek": 262934, "start": 2647.3, "end": 2650.5, "text": " And so if you take this kind of general notion", "tokens": [51262, 400, 370, 498, 291, 747, 341, 733, 295, 2674, 10710, 51422], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1062, "seek": 262934, "start": 2650.5, "end": 2653.9, "text": " of the meaning of language as being the distribution", "tokens": [51422, 295, 264, 3620, 295, 2856, 382, 885, 264, 7316, 51592], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1063, "seek": 262934, "start": 2653.9, "end": 2654.9, "text": " over expressions that it creates", "tokens": [51592, 670, 15277, 300, 309, 7829, 51642], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1064, "seek": 262934, "start": 2654.9, "end": 2656.34, "text": " in a probabilistic programming language,", "tokens": [51642, 294, 257, 31959, 3142, 9410, 2856, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1170415787469773, "compression_ratio": 1.7642276422764227, "no_speech_prob": 0.0006458365241996944}, {"id": 1065, "seek": 265634, "start": 2656.34, "end": 2660.38, "text": " you might start to think how we can formally think about", "tokens": [50364, 291, 1062, 722, 281, 519, 577, 321, 393, 25983, 519, 466, 50566], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1066, "seek": 265634, "start": 2660.38, "end": 2664.26, "text": " relating definitions for various kinds of relational terms.", "tokens": [50566, 23968, 21988, 337, 3683, 3685, 295, 38444, 2115, 13, 50760], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1067, "seek": 265634, "start": 2665.26, "end": 2666.86, "text": " An uncle is the brother of one's parent", "tokens": [50810, 1107, 9153, 307, 264, 3708, 295, 472, 311, 2596, 50890], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1068, "seek": 265634, "start": 2666.86, "end": 2668.42, "text": " or the husband of one's aunt.", "tokens": [50890, 420, 264, 5213, 295, 472, 311, 15654, 13, 50968], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1069, "seek": 265634, "start": 2668.42, "end": 2671.42, "text": " A pibling is a gender neutral term for an aunt or uncle,", "tokens": [50968, 316, 280, 897, 1688, 307, 257, 7898, 10598, 1433, 337, 364, 15654, 420, 9153, 11, 51118], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1070, "seek": 265634, "start": 2671.42, "end": 2673.2200000000003, "text": " that's the sibling of one's parent,", "tokens": [51118, 300, 311, 264, 39409, 295, 472, 311, 2596, 11, 51208], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1071, "seek": 265634, "start": 2674.26, "end": 2679.26, "text": " or this relational notion of a sister of one's father", "tokens": [51260, 420, 341, 38444, 10710, 295, 257, 4892, 295, 472, 311, 3086, 51510], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1072, "seek": 265634, "start": 2679.98, "end": 2681.42, "text": " from a language that's actually not found", "tokens": [51546, 490, 257, 2856, 300, 311, 767, 406, 1352, 51618], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1073, "seek": 265634, "start": 2681.42, "end": 2682.78, "text": " anywhere on the internet.", "tokens": [51618, 4992, 322, 264, 4705, 13, 51686], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1074, "seek": 265634, "start": 2682.78, "end": 2685.6200000000003, "text": " And I think the core thing that we wanna suggest here, right,", "tokens": [51686, 400, 286, 519, 264, 4965, 551, 300, 321, 1948, 3402, 510, 11, 558, 11, 51828], "temperature": 0.0, "avg_logprob": -0.13772045482288708, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.00014878874935675412}, {"id": 1075, "seek": 268562, "start": 2686.02, "end": 2688.14, "text": " is why do we even have definitions at all?", "tokens": [50384, 307, 983, 360, 321, 754, 362, 21988, 412, 439, 30, 50490], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1076, "seek": 268562, "start": 2689.5, "end": 2693.2599999999998, "text": " Well, one notion of what it even means", "tokens": [50558, 1042, 11, 472, 10710, 295, 437, 309, 754, 1355, 50746], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1077, "seek": 268562, "start": 2693.2599999999998, "end": 2695.66, "text": " to have learned the definition of this term", "tokens": [50746, 281, 362, 3264, 264, 7123, 295, 341, 1433, 50866], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1078, "seek": 268562, "start": 2695.66, "end": 2698.3399999999997, "text": " is that it should drive coherently", "tokens": [50866, 307, 300, 309, 820, 3332, 26528, 2276, 51000], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1079, "seek": 268562, "start": 2698.3399999999997, "end": 2701.2599999999998, "text": " all of the downstream inferences that you make with that term,", "tokens": [51000, 439, 295, 264, 30621, 13596, 2667, 300, 291, 652, 365, 300, 1433, 11, 51146], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1080, "seek": 268562, "start": 2701.2599999999998, "end": 2704.3399999999997, "text": " and it should graft onto the conceptual knowledge", "tokens": [51146, 293, 309, 820, 44767, 3911, 264, 24106, 3601, 51300], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1081, "seek": 268562, "start": 2704.3399999999997, "end": 2705.58, "text": " that you already have.", "tokens": [51300, 300, 291, 1217, 362, 13, 51362], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1082, "seek": 268562, "start": 2705.58, "end": 2710.02, "text": " And so you can think about forming new sentences directly", "tokens": [51362, 400, 370, 291, 393, 519, 466, 15745, 777, 16579, 3838, 51584], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1083, "seek": 268562, "start": 2710.02, "end": 2713.38, "text": " that refer to someone's paani or one's pibling", "tokens": [51584, 300, 2864, 281, 1580, 311, 2502, 3782, 420, 472, 311, 280, 897, 1688, 51752], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1084, "seek": 268562, "start": 2713.38, "end": 2715.46, "text": " in this situation and expecting them", "tokens": [51752, 294, 341, 2590, 293, 9650, 552, 51856], "temperature": 0.0, "avg_logprob": -0.14728858879020623, "compression_ratio": 1.745019920318725, "no_speech_prob": 0.0002693129063118249}, {"id": 1085, "seek": 271546, "start": 2715.46, "end": 2718.62, "text": " to draw both on your existing conceptual knowledge", "tokens": [50364, 281, 2642, 1293, 322, 428, 6741, 24106, 3601, 50522], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1086, "seek": 271546, "start": 2718.62, "end": 2722.18, "text": " of what it even means to have a family tree", "tokens": [50522, 295, 437, 309, 754, 1355, 281, 362, 257, 1605, 4230, 50700], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1087, "seek": 271546, "start": 2722.18, "end": 2724.54, "text": " as well as all the other conceptual terms for a friendship", "tokens": [50700, 382, 731, 382, 439, 264, 661, 24106, 2115, 337, 257, 13216, 50818], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1088, "seek": 271546, "start": 2724.54, "end": 2725.78, "text": " that you may already have.", "tokens": [50818, 300, 291, 815, 1217, 362, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1089, "seek": 271546, "start": 2728.06, "end": 2732.1, "text": " And the same framework also suggests one mechanism", "tokens": [50994, 400, 264, 912, 8388, 611, 13409, 472, 7513, 51196], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1090, "seek": 271546, "start": 2732.1, "end": 2734.34, "text": " by which we might formalize what it means", "tokens": [51196, 538, 597, 321, 1062, 9860, 1125, 437, 309, 1355, 51308], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1091, "seek": 271546, "start": 2734.34, "end": 2736.66, "text": " to learn world models from language.", "tokens": [51308, 281, 1466, 1002, 5245, 490, 2856, 13, 51424], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1092, "seek": 271546, "start": 2736.66, "end": 2739.38, "text": " So as I mentioned, if we return to the situation", "tokens": [51424, 407, 382, 286, 2835, 11, 498, 321, 2736, 281, 264, 2590, 51560], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1093, "seek": 271546, "start": 2739.38, "end": 2742.58, "text": " that opens this talk, tug of war games,", "tokens": [51560, 300, 9870, 341, 751, 11, 33543, 295, 1516, 2813, 11, 51720], "temperature": 0.0, "avg_logprob": -0.11215694427490235, "compression_ratio": 1.6153846153846154, "no_speech_prob": 3.9421058318112046e-05}, {"id": 1094, "seek": 274258, "start": 2743.42, "end": 2745.8199999999997, "text": " we might think about how the definition that I gave", "tokens": [50406, 321, 1062, 519, 466, 577, 264, 7123, 300, 286, 2729, 50526], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1095, "seek": 274258, "start": 2745.8199999999997, "end": 2748.14, "text": " when I sat up here at the podium, right,", "tokens": [50526, 562, 286, 3227, 493, 510, 412, 264, 26827, 11, 558, 11, 50642], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1096, "seek": 274258, "start": 2748.14, "end": 2750.38, "text": " saying there are people whose strength levels vary", "tokens": [50642, 1566, 456, 366, 561, 6104, 3800, 4358, 10559, 50754], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1097, "seek": 274258, "start": 2750.38, "end": 2752.42, "text": " from person to person.", "tokens": [50754, 490, 954, 281, 954, 13, 50856], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1098, "seek": 274258, "start": 2752.42, "end": 2755.94, "text": " People have a percentage of time in which they're lazy.", "tokens": [50856, 3432, 362, 257, 9668, 295, 565, 294, 597, 436, 434, 14847, 13, 51032], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1099, "seek": 274258, "start": 2755.94, "end": 2758.9, "text": " Strengths of the teams depend on the underlying strengths", "tokens": [51032, 39251, 82, 295, 264, 5491, 5672, 322, 264, 14217, 16986, 51180], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1100, "seek": 274258, "start": 2758.9, "end": 2761.62, "text": " of the members of that team,", "tokens": [51180, 295, 264, 2679, 295, 300, 1469, 11, 51316], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1101, "seek": 274258, "start": 2761.62, "end": 2763.42, "text": " and whether one team beats another", "tokens": [51316, 293, 1968, 472, 1469, 16447, 1071, 51406], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1102, "seek": 274258, "start": 2763.42, "end": 2765.98, "text": " just depends on which team pulls stronger that match.", "tokens": [51406, 445, 5946, 322, 597, 1469, 16982, 7249, 300, 2995, 13, 51534], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1103, "seek": 274258, "start": 2765.98, "end": 2769.06, "text": " And this kind of setting is actually language, right,", "tokens": [51534, 400, 341, 733, 295, 3287, 307, 767, 2856, 11, 558, 11, 51688], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1104, "seek": 274258, "start": 2769.06, "end": 2772.38, "text": " is building up the actual generative model itself.", "tokens": [51688, 307, 2390, 493, 264, 3539, 1337, 1166, 2316, 2564, 13, 51854], "temperature": 0.0, "avg_logprob": -0.10789857372160881, "compression_ratio": 1.7226027397260273, "no_speech_prob": 0.0003149367985315621}, {"id": 1105, "seek": 277238, "start": 2772.38, "end": 2774.58, "text": " And you might think of a system like this", "tokens": [50364, 400, 291, 1062, 519, 295, 257, 1185, 411, 341, 50474], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1106, "seek": 277238, "start": 2774.58, "end": 2777.3, "text": " that both learns these kinds of theories from language", "tokens": [50474, 300, 1293, 27152, 613, 3685, 295, 13667, 490, 2856, 50610], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1107, "seek": 277238, "start": 2777.3, "end": 2781.38, "text": " and then is appending to this kind of local problem-based", "tokens": [50610, 293, 550, 307, 724, 2029, 281, 341, 733, 295, 2654, 1154, 12, 6032, 50814], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1108, "seek": 277238, "start": 2781.38, "end": 2783.7000000000003, "text": " context to answer arbitrary questions", "tokens": [50814, 4319, 281, 1867, 23211, 1651, 50930], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1109, "seek": 277238, "start": 2783.7000000000003, "end": 2785.2200000000003, "text": " like the kinds that we gave or conditioned", "tokens": [50930, 411, 264, 3685, 300, 321, 2729, 420, 35833, 51006], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1110, "seek": 277238, "start": 2785.2200000000003, "end": 2788.2200000000003, "text": " on various observations like Josh being stronger than Leo", "tokens": [51006, 322, 3683, 18163, 411, 9785, 885, 7249, 813, 19344, 51156], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1111, "seek": 277238, "start": 2788.2200000000003, "end": 2790.7400000000002, "text": " with respect to this kind of local notion", "tokens": [51156, 365, 3104, 281, 341, 733, 295, 2654, 10710, 51282], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1112, "seek": 277238, "start": 2791.98, "end": 2796.3, "text": " of what strength means in this particular problem context", "tokens": [51344, 295, 437, 3800, 1355, 294, 341, 1729, 1154, 4319, 51560], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1113, "seek": 277238, "start": 2796.3, "end": 2798.3, "text": " that we're thinking about.", "tokens": [51560, 300, 321, 434, 1953, 466, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1114, "seek": 277238, "start": 2798.3, "end": 2799.7000000000003, "text": " In interest of time, why don't we just jump on", "tokens": [51660, 682, 1179, 295, 565, 11, 983, 500, 380, 321, 445, 3012, 322, 51730], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1115, "seek": 277238, "start": 2799.7000000000003, "end": 2800.54, "text": " to your section.", "tokens": [51730, 281, 428, 3541, 13, 51772], "temperature": 0.0, "avg_logprob": -0.1430577902958311, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.00016075540042947978}, {"id": 1116, "seek": 280054, "start": 2800.62, "end": 2801.46, "text": " Yeah, thanks.", "tokens": [50368, 865, 11, 3231, 13, 50410], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1117, "seek": 280054, "start": 2808.54, "end": 2811.3, "text": " So we've just been talking about how natural language", "tokens": [50764, 407, 321, 600, 445, 668, 1417, 466, 577, 3303, 2856, 50902], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1118, "seek": 280054, "start": 2811.3, "end": 2815.62, "text": " can sort of be interpreted or semantically parsed", "tokens": [50902, 393, 1333, 295, 312, 26749, 420, 4361, 49505, 21156, 292, 51118], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1119, "seek": 280054, "start": 2815.62, "end": 2817.98, "text": " to a probabilistic language of thought,", "tokens": [51118, 281, 257, 31959, 3142, 2856, 295, 1194, 11, 51236], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1120, "seek": 280054, "start": 2817.98, "end": 2820.5, "text": " but we haven't talked about how cognition itself,", "tokens": [51236, 457, 321, 2378, 380, 2825, 466, 577, 46905, 2564, 11, 51362], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1121, "seek": 280054, "start": 2821.7799999999997, "end": 2823.62, "text": " which is sort of, we've been talking about", "tokens": [51426, 597, 307, 1333, 295, 11, 321, 600, 668, 1417, 466, 51518], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1122, "seek": 280054, "start": 2823.62, "end": 2825.18, "text": " as the product of general purpose", "tokens": [51518, 382, 264, 1674, 295, 2674, 4334, 51596], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1123, "seek": 280054, "start": 2825.18, "end": 2826.9, "text": " probabilistic inference machinery,", "tokens": [51596, 31959, 3142, 38253, 27302, 11, 51682], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1124, "seek": 280054, "start": 2826.9, "end": 2829.5, "text": " might interact with language cognitively", "tokens": [51682, 1062, 4648, 365, 2856, 15605, 356, 51812], "temperature": 0.0, "avg_logprob": -0.18058982591950493, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0005526119493879378}, {"id": 1125, "seek": 282950, "start": 2829.54, "end": 2832.42, "text": " or how our tools for, you know,", "tokens": [50366, 420, 577, 527, 3873, 337, 11, 291, 458, 11, 50510], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1126, "seek": 282950, "start": 2832.42, "end": 2834.98, "text": " our algorithms for inference, our model representations", "tokens": [50510, 527, 14642, 337, 38253, 11, 527, 2316, 33358, 50638], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1127, "seek": 282950, "start": 2834.98, "end": 2838.58, "text": " might benefit from recent advances in language models.", "tokens": [50638, 1062, 5121, 490, 5162, 25297, 294, 2856, 5245, 13, 50818], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1128, "seek": 282950, "start": 2838.58, "end": 2841.26, "text": " So in the rest of the talk,", "tokens": [50818, 407, 294, 264, 1472, 295, 264, 751, 11, 50952], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1129, "seek": 282950, "start": 2841.26, "end": 2843.74, "text": " I'll sort of talk about this also very preliminary work", "tokens": [50952, 286, 603, 1333, 295, 751, 466, 341, 611, 588, 28817, 589, 51076], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1130, "seek": 282950, "start": 2843.74, "end": 2846.18, "text": " that we just presented at a workshop at ICML", "tokens": [51076, 300, 321, 445, 8212, 412, 257, 13541, 412, 14360, 12683, 51198], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1131, "seek": 282950, "start": 2847.66, "end": 2849.42, "text": " that is more about a role for natural language", "tokens": [51272, 300, 307, 544, 466, 257, 3090, 337, 3303, 2856, 51360], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1132, "seek": 282950, "start": 2849.42, "end": 2851.66, "text": " and language models in this part of the picture.", "tokens": [51360, 293, 2856, 5245, 294, 341, 644, 295, 264, 3036, 13, 51472], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1133, "seek": 282950, "start": 2852.9, "end": 2854.7, "text": " And one reason to think that natural language", "tokens": [51534, 400, 472, 1778, 281, 519, 300, 3303, 2856, 51624], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1134, "seek": 282950, "start": 2854.7, "end": 2857.26, "text": " must play some role in this part of the picture", "tokens": [51624, 1633, 862, 512, 3090, 294, 341, 644, 295, 264, 3036, 51752], "temperature": 0.0, "avg_logprob": -0.13010596392447488, "compression_ratio": 1.844, "no_speech_prob": 0.00034592891461215913}, {"id": 1135, "seek": 285726, "start": 2857.34, "end": 2861.1400000000003, "text": " is that sometimes we set ourselves reasoning tasks", "tokens": [50368, 307, 300, 2171, 321, 992, 4175, 21577, 9608, 50558], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1136, "seek": 285726, "start": 2861.1400000000003, "end": 2863.34, "text": " whose specifications, what it would mean", "tokens": [50558, 6104, 29448, 11, 437, 309, 576, 914, 50668], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1137, "seek": 285726, "start": 2863.34, "end": 2864.98, "text": " to solve the reasoning task correctly", "tokens": [50668, 281, 5039, 264, 21577, 5633, 8944, 50750], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1138, "seek": 285726, "start": 2864.98, "end": 2866.1800000000003, "text": " must involve natural language.", "tokens": [50750, 1633, 9494, 3303, 2856, 13, 50810], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1139, "seek": 285726, "start": 2866.1800000000003, "end": 2867.82, "text": " So for example, if you have an iPhone,", "tokens": [50810, 407, 337, 1365, 11, 498, 291, 362, 364, 7252, 11, 50892], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1140, "seek": 285726, "start": 2867.82, "end": 2870.3, "text": " you might have used the visual voicemail feature,", "tokens": [50892, 291, 1062, 362, 1143, 264, 5056, 1650, 46343, 864, 4111, 11, 51016], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1141, "seek": 285726, "start": 2870.3, "end": 2871.7000000000003, "text": " which automatically, but somewhat", "tokens": [51016, 597, 6772, 11, 457, 8344, 51086], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1142, "seek": 285726, "start": 2871.7000000000003, "end": 2873.78, "text": " incompletely transcribes your voicemails.", "tokens": [51086, 14036, 14657, 736, 1145, 1142, 6446, 428, 1650, 46343, 6227, 13, 51190], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1143, "seek": 285726, "start": 2873.78, "end": 2876.0600000000004, "text": " And these transcripts have gaps marked", "tokens": [51190, 400, 613, 24444, 82, 362, 15031, 12658, 51304], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1144, "seek": 285726, "start": 2876.0600000000004, "end": 2878.1400000000003, "text": " by underscored sequences of varying lengths,", "tokens": [51304, 538, 16692, 66, 2769, 22978, 295, 22984, 26329, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1145, "seek": 285726, "start": 2878.1400000000003, "end": 2880.7400000000002, "text": " indicating Apple couldn't quite work out what was said.", "tokens": [51408, 25604, 6373, 2809, 380, 1596, 589, 484, 437, 390, 848, 13, 51538], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1146, "seek": 285726, "start": 2881.7400000000002, "end": 2883.5, "text": " And an inference task that I sometimes face", "tokens": [51588, 400, 364, 38253, 5633, 300, 286, 2171, 1851, 51676], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1147, "seek": 285726, "start": 2883.5, "end": 2884.82, "text": " is squinting at these transcripts", "tokens": [51676, 307, 2339, 686, 278, 412, 613, 24444, 82, 51742], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1148, "seek": 285726, "start": 2884.82, "end": 2886.82, "text": " and trying to think what could the person have said", "tokens": [51742, 293, 1382, 281, 519, 437, 727, 264, 954, 362, 848, 51842], "temperature": 0.0, "avg_logprob": -0.1609810607074058, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.002114941133186221}, {"id": 1149, "seek": 288682, "start": 2886.82, "end": 2889.7400000000002, "text": " during those bits that it didn't transcribe correctly.", "tokens": [50364, 1830, 729, 9239, 300, 309, 994, 380, 1145, 8056, 8944, 13, 50510], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1150, "seek": 288682, "start": 2889.7400000000002, "end": 2892.98, "text": " And is it worth my time to listen to this voicemail", "tokens": [50510, 400, 307, 309, 3163, 452, 565, 281, 2140, 281, 341, 1650, 46343, 864, 50672], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1151, "seek": 288682, "start": 2892.98, "end": 2896.6600000000003, "text": " or am I pretty certain that I got all the relevant information", "tokens": [50672, 420, 669, 286, 1238, 1629, 300, 286, 658, 439, 264, 7340, 1589, 50856], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1152, "seek": 288682, "start": 2896.6600000000003, "end": 2898.98, "text": " from the part of the transcript that I've seen?", "tokens": [50856, 490, 264, 644, 295, 264, 24444, 300, 286, 600, 1612, 30, 50972], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1153, "seek": 288682, "start": 2900.1000000000004, "end": 2903.1000000000004, "text": " So even if I'm representing that kind of inference problem", "tokens": [51028, 407, 754, 498, 286, 478, 13460, 300, 733, 295, 38253, 1154, 51178], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1154, "seek": 288682, "start": 2903.1000000000004, "end": 2904.94, "text": " in some kind of probabilistic language of thought", "tokens": [51178, 294, 512, 733, 295, 31959, 3142, 2856, 295, 1194, 51270], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1155, "seek": 288682, "start": 2904.94, "end": 2907.54, "text": " and not in natural language, it must reference natural language", "tokens": [51270, 293, 406, 294, 3303, 2856, 11, 309, 1633, 6408, 3303, 2856, 51400], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1156, "seek": 288682, "start": 2907.54, "end": 2909.3, "text": " because a key part of the reasoning that I'm doing", "tokens": [51400, 570, 257, 2141, 644, 295, 264, 21577, 300, 286, 478, 884, 51488], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1157, "seek": 288682, "start": 2909.3, "end": 2910.6600000000003, "text": " is about how long those gaps are,", "tokens": [51488, 307, 466, 577, 938, 729, 15031, 366, 11, 51556], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1158, "seek": 288682, "start": 2910.6600000000003, "end": 2912.2200000000003, "text": " about what words could go in those gaps,", "tokens": [51556, 466, 437, 2283, 727, 352, 294, 729, 15031, 11, 51634], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1159, "seek": 288682, "start": 2912.2200000000003, "end": 2914.5800000000004, "text": " how they could semantically and syntactically", "tokens": [51634, 577, 436, 727, 4361, 49505, 293, 23980, 578, 984, 51752], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1160, "seek": 288682, "start": 2914.5800000000004, "end": 2915.86, "text": " with the words around them.", "tokens": [51752, 365, 264, 2283, 926, 552, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1338301227517324, "compression_ratio": 1.826625386996904, "no_speech_prob": 0.0001253055816050619}, {"id": 1161, "seek": 291682, "start": 2917.82, "end": 2919.98, "text": " And there are a lot of other tasks like this", "tokens": [50414, 400, 456, 366, 257, 688, 295, 661, 9608, 411, 341, 50522], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1162, "seek": 291682, "start": 2919.98, "end": 2922.6200000000003, "text": " where the specification of some reasoning problem", "tokens": [50522, 689, 264, 31256, 295, 512, 21577, 1154, 50654], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1163, "seek": 291682, "start": 2922.6200000000003, "end": 2924.02, "text": " must in some way involve language.", "tokens": [50654, 1633, 294, 512, 636, 9494, 2856, 13, 50724], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1164, "seek": 291682, "start": 2924.02, "end": 2925.34, "text": " Maybe we're writing something", "tokens": [50724, 2704, 321, 434, 3579, 746, 50790], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1165, "seek": 291682, "start": 2925.34, "end": 2927.1000000000004, "text": " that has to obey certain structural constraints", "tokens": [50790, 300, 575, 281, 19297, 1629, 15067, 18491, 50878], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1166, "seek": 291682, "start": 2927.1000000000004, "end": 2929.34, "text": " like a poem or code.", "tokens": [50878, 411, 257, 13065, 420, 3089, 13, 50990], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1167, "seek": 291682, "start": 2930.26, "end": 2932.38, "text": " Maybe we're puzzling over a message from our advisor,", "tokens": [51036, 2704, 321, 434, 18741, 1688, 670, 257, 3636, 490, 527, 19161, 11, 51142], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1168, "seek": 291682, "start": 2932.38, "end": 2933.7000000000003, "text": " trying to infer all the different meanings", "tokens": [51142, 1382, 281, 13596, 439, 264, 819, 28138, 51208], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1169, "seek": 291682, "start": 2933.7000000000003, "end": 2935.78, "text": " consistent with what they said.", "tokens": [51208, 8398, 365, 437, 436, 848, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1170, "seek": 291682, "start": 2935.78, "end": 2936.98, "text": " Maybe we're trying to figure out", "tokens": [51312, 2704, 321, 434, 1382, 281, 2573, 484, 51372], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1171, "seek": 291682, "start": 2936.98, "end": 2939.34, "text": " how to put together some words that we predict", "tokens": [51372, 577, 281, 829, 1214, 512, 2283, 300, 321, 6069, 51490], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1172, "seek": 291682, "start": 2939.34, "end": 2941.78, "text": " could achieve some desired effect in a listener.", "tokens": [51490, 727, 4584, 512, 14721, 1802, 294, 257, 31569, 13, 51612], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1173, "seek": 291682, "start": 2943.1400000000003, "end": 2945.7400000000002, "text": " And beyond the fact that some inference problems", "tokens": [51680, 400, 4399, 264, 1186, 300, 512, 38253, 2740, 51810], "temperature": 0.0, "avg_logprob": -0.10563970357179642, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.000185184515430592}, {"id": 1174, "seek": 294574, "start": 2945.74, "end": 2947.7799999999997, "text": " implicate language and their specification,", "tokens": [50364, 10629, 473, 2856, 293, 641, 31256, 11, 50466], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1175, "seek": 294574, "start": 2947.7799999999997, "end": 2948.9799999999996, "text": " it seems like at least sometimes", "tokens": [50466, 309, 2544, 411, 412, 1935, 2171, 50526], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1176, "seek": 294574, "start": 2948.9799999999996, "end": 2952.58, "text": " we sort of use language for thinking, right?", "tokens": [50526, 321, 1333, 295, 764, 2856, 337, 1953, 11, 558, 30, 50706], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1177, "seek": 294574, "start": 2952.58, "end": 2955.9799999999996, "text": " Rubber duck debugging is when we successfully debug", "tokens": [50706, 10518, 607, 12482, 45592, 307, 562, 321, 10727, 24083, 50876], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1178, "seek": 294574, "start": 2955.9799999999996, "end": 2957.2599999999998, "text": " something that's been something us", "tokens": [50876, 746, 300, 311, 668, 746, 505, 50940], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1179, "seek": 294574, "start": 2957.2599999999998, "end": 2960.5, "text": " by just talking about it to ourselves or to a rubber duck.", "tokens": [50940, 538, 445, 1417, 466, 309, 281, 4175, 420, 281, 257, 11593, 12482, 13, 51102], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1180, "seek": 294574, "start": 2962.18, "end": 2965.02, "text": " And I think this is the intuition also behind", "tokens": [51186, 400, 286, 519, 341, 307, 264, 24002, 611, 2261, 51328], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1181, "seek": 294574, "start": 2965.02, "end": 2967.5, "text": " sort of chain of thought, scrap pad,", "tokens": [51328, 1333, 295, 5021, 295, 1194, 11, 23138, 6887, 11, 51452], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1182, "seek": 294574, "start": 2967.5, "end": 2971.3799999999997, "text": " those kinds of innovations in language model land.", "tokens": [51452, 729, 3685, 295, 24283, 294, 2856, 2316, 2117, 13, 51646], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1183, "seek": 294574, "start": 2972.54, "end": 2974.1, "text": " But one reason I'm drawing this distinction", "tokens": [51704, 583, 472, 1778, 286, 478, 6316, 341, 16844, 51782], "temperature": 0.0, "avg_logprob": -0.18896964744285302, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0009107509977184236}, {"id": 1184, "seek": 297410, "start": 2974.1, "end": 2976.7, "text": " between task specification and algorithm", "tokens": [50364, 1296, 5633, 31256, 293, 9284, 50494], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1185, "seek": 297410, "start": 2976.7, "end": 2979.8199999999997, "text": " is that this has long been a really important distinction", "tokens": [50494, 307, 300, 341, 575, 938, 668, 257, 534, 1021, 16844, 50650], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1186, "seek": 297410, "start": 2979.8199999999997, "end": 2981.14, "text": " in probabilistic modeling and inference.", "tokens": [50650, 294, 31959, 3142, 15983, 293, 38253, 13, 50716], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1187, "seek": 297410, "start": 2981.14, "end": 2982.98, "text": " And it's something that I think we lose", "tokens": [50716, 400, 309, 311, 746, 300, 286, 519, 321, 3624, 50808], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1188, "seek": 297410, "start": 2982.98, "end": 2985.94, "text": " when we move just to asking a language model a question", "tokens": [50808, 562, 321, 1286, 445, 281, 3365, 257, 2856, 2316, 257, 1168, 50956], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1189, "seek": 297410, "start": 2985.94, "end": 2988.18, "text": " and hoping that it gives us the right answer.", "tokens": [50956, 293, 7159, 300, 309, 2709, 505, 264, 558, 1867, 13, 51068], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1190, "seek": 297410, "start": 2988.18, "end": 2992.38, "text": " So in the kind of work that our lab does", "tokens": [51068, 407, 294, 264, 733, 295, 589, 300, 527, 2715, 775, 51278], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1191, "seek": 297410, "start": 2992.38, "end": 2994.06, "text": " in modeling and inference,", "tokens": [51278, 294, 15983, 293, 38253, 11, 51362], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1192, "seek": 297410, "start": 2994.06, "end": 2998.56, "text": " we sort of separately create a model probabilistic program", "tokens": [51362, 321, 1333, 295, 14759, 1884, 257, 2316, 31959, 3142, 1461, 51587], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1193, "seek": 297410, "start": 2998.56, "end": 3000.18, "text": " that includes a task specification", "tokens": [51587, 300, 5974, 257, 5633, 31256, 51668], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1194, "seek": 297410, "start": 3000.18, "end": 3002.62, "text": " as a posterior distribution we wanna sample from", "tokens": [51668, 382, 257, 33529, 7316, 321, 1948, 6889, 490, 51790], "temperature": 0.0, "avg_logprob": -0.09593442448398523, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0003052750835195184}, {"id": 1195, "seek": 300262, "start": 3002.62, "end": 3004.02, "text": " and separately an inference program", "tokens": [50364, 293, 14759, 364, 38253, 1461, 50434], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1196, "seek": 300262, "start": 3004.02, "end": 3006.2599999999998, "text": " that compositionally encodes some kind of algorithm", "tokens": [50434, 300, 12686, 379, 2058, 4789, 512, 733, 295, 9284, 50546], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1197, "seek": 300262, "start": 3006.2599999999998, "end": 3008.2599999999998, "text": " or strategy for solving that inference task.", "tokens": [50546, 420, 5206, 337, 12606, 300, 38253, 5633, 13, 50646], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1198, "seek": 300262, "start": 3008.2599999999998, "end": 3010.1, "text": " And when you use a probabilistic programming language", "tokens": [50646, 400, 562, 291, 764, 257, 31959, 3142, 9410, 2856, 50738], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1199, "seek": 300262, "start": 3010.1, "end": 3011.74, "text": " to do this, you get some benefits", "tokens": [50738, 281, 360, 341, 11, 291, 483, 512, 5311, 50820], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1200, "seek": 300262, "start": 3011.74, "end": 3014.7, "text": " from taking this approach of separating model inference.", "tokens": [50820, 490, 1940, 341, 3109, 295, 29279, 2316, 38253, 13, 50968], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1201, "seek": 300262, "start": 3014.7, "end": 3016.38, "text": " We know that we have soundness theorems", "tokens": [50968, 492, 458, 300, 321, 362, 1626, 1287, 10299, 2592, 51052], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1202, "seek": 300262, "start": 3016.38, "end": 3018.5, "text": " guaranteeing that as computation increases,", "tokens": [51052, 10815, 278, 300, 382, 24903, 8637, 11, 51158], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1203, "seek": 300262, "start": 3018.5, "end": 3020.8199999999997, "text": " the inference is going to approach the posterior.", "tokens": [51158, 264, 38253, 307, 516, 281, 3109, 264, 33529, 13, 51274], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1204, "seek": 300262, "start": 3020.8199999999997, "end": 3022.3399999999997, "text": " We have automated tools and tests", "tokens": [51274, 492, 362, 18473, 3873, 293, 6921, 51350], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1205, "seek": 300262, "start": 3022.3399999999997, "end": 3024.2599999999998, "text": " for measuring how accurate our inferences are", "tokens": [51350, 337, 13389, 577, 8559, 527, 13596, 2667, 366, 51446], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1206, "seek": 300262, "start": 3024.2599999999998, "end": 3027.06, "text": " relative to the model with finite computation.", "tokens": [51446, 4972, 281, 264, 2316, 365, 19362, 24903, 13, 51586], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1207, "seek": 300262, "start": 3027.06, "end": 3028.54, "text": " And we also have gradient estimators", "tokens": [51586, 400, 321, 611, 362, 16235, 8017, 3391, 51660], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1208, "seek": 300262, "start": 3028.54, "end": 3031.02, "text": " that help us tune any parameters of our inference programs", "tokens": [51660, 300, 854, 505, 10864, 604, 9834, 295, 527, 38253, 4268, 51784], "temperature": 0.0, "avg_logprob": -0.11903816545513314, "compression_ratio": 1.8925373134328358, "no_speech_prob": 0.00023780023911967874}, {"id": 1209, "seek": 303102, "start": 3031.02, "end": 3033.86, "text": " to be better inference algorithms.", "tokens": [50364, 281, 312, 1101, 38253, 14642, 13, 50506], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1210, "seek": 303102, "start": 3033.86, "end": 3035.82, "text": " And beyond being useful properties for engineering,", "tokens": [50506, 400, 4399, 885, 4420, 7221, 337, 7043, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1211, "seek": 303102, "start": 3035.82, "end": 3037.3, "text": " these guarantees also reflect", "tokens": [50604, 613, 32567, 611, 5031, 50678], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1212, "seek": 303102, "start": 3037.3, "end": 3038.94, "text": " some key aspects of human cognition.", "tokens": [50678, 512, 2141, 7270, 295, 1952, 46905, 13, 50760], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1213, "seek": 303102, "start": 3038.94, "end": 3041.9, "text": " We can often think more to reach more accurate conclusions.", "tokens": [50760, 492, 393, 2049, 519, 544, 281, 2524, 544, 8559, 22865, 13, 50908], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1214, "seek": 303102, "start": 3041.9, "end": 3043.3, "text": " We can critically evaluate the extent", "tokens": [50908, 492, 393, 22797, 13059, 264, 8396, 50978], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1215, "seek": 303102, "start": 3043.3, "end": 3045.2599999999998, "text": " to which our current hypotheses actually make sense", "tokens": [50978, 281, 597, 527, 2190, 49969, 767, 652, 2020, 51076], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1216, "seek": 303102, "start": 3045.2599999999998, "end": 3046.9, "text": " given our model of the world.", "tokens": [51076, 2212, 527, 2316, 295, 264, 1002, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1217, "seek": 303102, "start": 3046.9, "end": 3048.98, "text": " And if we repeatedly face the same kind of inference task,", "tokens": [51158, 400, 498, 321, 18227, 1851, 264, 912, 733, 295, 38253, 5633, 11, 51262], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1218, "seek": 303102, "start": 3048.98, "end": 3052.2599999999998, "text": " we can train ourselves to get better at solving it.", "tokens": [51262, 321, 393, 3847, 4175, 281, 483, 1101, 412, 12606, 309, 13, 51426], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1219, "seek": 303102, "start": 3052.2599999999998, "end": 3053.62, "text": " So something we've begun to explore", "tokens": [51426, 407, 746, 321, 600, 16009, 281, 6839, 51494], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1220, "seek": 303102, "start": 3053.62, "end": 3055.66, "text": " is whether adding LLMs to this picture", "tokens": [51494, 307, 1968, 5127, 441, 43, 26386, 281, 341, 3036, 51596], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1221, "seek": 303102, "start": 3055.66, "end": 3058.46, "text": " might let us both specify various linguistic tasks", "tokens": [51596, 1062, 718, 505, 1293, 16500, 3683, 43002, 9608, 51736], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1222, "seek": 303102, "start": 3058.46, "end": 3059.9, "text": " as formal probabilistic models", "tokens": [51736, 382, 9860, 31959, 3142, 5245, 51808], "temperature": 0.0, "avg_logprob": -0.10643060258824191, "compression_ratio": 1.7025495750708215, "no_speech_prob": 0.00034594847238622606}, {"id": 1223, "seek": 305990, "start": 3059.94, "end": 3061.34, "text": " and enhance our inference algorithms", "tokens": [50366, 293, 11985, 527, 38253, 14642, 50436], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1224, "seek": 305990, "start": 3061.34, "end": 3063.06, "text": " by letting them do some of their thinking", "tokens": [50436, 538, 8295, 552, 360, 512, 295, 641, 1953, 50522], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1225, "seek": 305990, "start": 3063.06, "end": 3064.58, "text": " using languages as a tool.", "tokens": [50522, 1228, 8650, 382, 257, 2290, 13, 50598], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1226, "seek": 305990, "start": 3064.58, "end": 3067.6600000000003, "text": " So I'll first talk about the modeling side of things.", "tokens": [50598, 407, 286, 603, 700, 751, 466, 264, 15983, 1252, 295, 721, 13, 50752], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1227, "seek": 305990, "start": 3067.6600000000003, "end": 3070.34, "text": " So we all know that an autoregressive language model", "tokens": [50752, 407, 321, 439, 458, 300, 364, 1476, 418, 3091, 488, 2856, 2316, 50886], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1228, "seek": 305990, "start": 3070.34, "end": 3072.1800000000003, "text": " defines a probability distribution", "tokens": [50886, 23122, 257, 8482, 7316, 50978], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1229, "seek": 305990, "start": 3072.1800000000003, "end": 3073.5, "text": " over sequences of tokens.", "tokens": [50978, 670, 22978, 295, 22667, 13, 51044], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1230, "seek": 305990, "start": 3074.42, "end": 3075.7000000000003, "text": " But we rarely just want to sample", "tokens": [51090, 583, 321, 13752, 445, 528, 281, 6889, 51154], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1231, "seek": 305990, "start": 3075.7000000000003, "end": 3078.98, "text": " that unconditional distribution.", "tokens": [51154, 300, 47916, 7316, 13, 51318], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1232, "seek": 305990, "start": 3078.98, "end": 3081.38, "text": " You know, in the same way that in order to use a SAT solver,", "tokens": [51318, 509, 458, 11, 294, 264, 912, 636, 300, 294, 1668, 281, 764, 257, 31536, 1404, 331, 11, 51438], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1233, "seek": 305990, "start": 3081.38, "end": 3083.9, "text": " we need to reduce the problem we care about to a SAT formula,", "tokens": [51438, 321, 643, 281, 5407, 264, 1154, 321, 1127, 466, 281, 257, 31536, 8513, 11, 51564], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1234, "seek": 305990, "start": 3083.9, "end": 3086.7400000000002, "text": " use a language model, we need to reduction", "tokens": [51564, 764, 257, 2856, 2316, 11, 321, 643, 281, 11004, 51706], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1235, "seek": 305990, "start": 3086.7400000000002, "end": 3088.9, "text": " from the task instance that we care about", "tokens": [51706, 490, 264, 5633, 5197, 300, 321, 1127, 466, 51814], "temperature": 0.0, "avg_logprob": -0.14694183678935757, "compression_ratio": 1.79672131147541, "no_speech_prob": 6.60446094116196e-05}, {"id": 1236, "seek": 308890, "start": 3088.9, "end": 3092.54, "text": " to a prompt.", "tokens": [50364, 281, 257, 12391, 13, 50546], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1237, "seek": 308890, "start": 3092.54, "end": 3093.94, "text": " And the idea is that we're saying", "tokens": [50546, 400, 264, 1558, 307, 300, 321, 434, 1566, 50616], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1238, "seek": 308890, "start": 3093.94, "end": 3095.2200000000003, "text": " that the conditional distribution", "tokens": [50616, 300, 264, 27708, 7316, 50680], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1239, "seek": 308890, "start": 3095.2200000000003, "end": 3097.02, "text": " of the language model conditioned on that prompt", "tokens": [50680, 295, 264, 2856, 2316, 35833, 322, 300, 12391, 50770], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1240, "seek": 308890, "start": 3097.02, "end": 3099.26, "text": " is somehow a good specification of the task", "tokens": [50770, 307, 6063, 257, 665, 31256, 295, 264, 5633, 50882], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1241, "seek": 308890, "start": 3099.26, "end": 3100.1, "text": " that we want to solve,", "tokens": [50882, 300, 321, 528, 281, 5039, 11, 50924], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1242, "seek": 308890, "start": 3100.1, "end": 3102.02, "text": " or a good approximation of the task that we want to solve.", "tokens": [50924, 420, 257, 665, 28023, 295, 264, 5633, 300, 321, 528, 281, 5039, 13, 51020], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1243, "seek": 308890, "start": 3102.02, "end": 3103.58, "text": " But unlike the reductions to SAT,", "tokens": [51020, 583, 8343, 264, 40296, 281, 31536, 11, 51098], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1244, "seek": 308890, "start": 3103.58, "end": 3106.7000000000003, "text": " of course, this reduction is lossy.", "tokens": [51098, 295, 1164, 11, 341, 11004, 307, 4470, 88, 13, 51254], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1245, "seek": 308890, "start": 3106.7000000000003, "end": 3109.2200000000003, "text": " One problem is that sort of hard constraints,", "tokens": [51254, 1485, 1154, 307, 300, 1333, 295, 1152, 18491, 11, 51380], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1246, "seek": 308890, "start": 3109.2200000000003, "end": 3111.94, "text": " sort of instructions that we give the language model", "tokens": [51380, 1333, 295, 9415, 300, 321, 976, 264, 2856, 2316, 51516], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1247, "seek": 308890, "start": 3111.94, "end": 3114.7000000000003, "text": " might be, you know, it might fail to follow them.", "tokens": [51516, 1062, 312, 11, 291, 458, 11, 309, 1062, 3061, 281, 1524, 552, 13, 51654], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1248, "seek": 308890, "start": 3114.7000000000003, "end": 3117.06, "text": " So this conditional distribution, p-task,", "tokens": [51654, 407, 341, 27708, 7316, 11, 280, 12, 83, 3863, 11, 51772], "temperature": 0.0, "avg_logprob": -0.17671361852575232, "compression_ratio": 1.961977186311787, "no_speech_prob": 0.0001442355860490352}, {"id": 1249, "seek": 311890, "start": 3119.14, "end": 3121.46, "text": " is not really the specification that we have in mind,", "tokens": [50376, 307, 406, 534, 264, 31256, 300, 321, 362, 294, 1575, 11, 50492], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1250, "seek": 311890, "start": 3121.46, "end": 3123.62, "text": " it's just some close thing that we can get.", "tokens": [50492, 309, 311, 445, 512, 1998, 551, 300, 321, 393, 483, 13, 50600], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1251, "seek": 311890, "start": 3127.38, "end": 3129.82, "text": " Another problem is that the entropy of this distribution", "tokens": [50788, 3996, 1154, 307, 300, 264, 30867, 295, 341, 7316, 50910], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1252, "seek": 311890, "start": 3129.82, "end": 3132.14, "text": " may not meaningfully reflect uncertainties.", "tokens": [50910, 815, 406, 3620, 2277, 5031, 11308, 6097, 13, 51026], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1253, "seek": 311890, "start": 3132.14, "end": 3135.3, "text": " So you may have seen in the GPT-4 paper", "tokens": [51026, 407, 291, 815, 362, 1612, 294, 264, 26039, 51, 12, 19, 3035, 51184], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1254, "seek": 311890, "start": 3135.3, "end": 3140.54, "text": " that on multiple choice tasks,", "tokens": [51184, 300, 322, 3866, 3922, 9608, 11, 51446], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1255, "seek": 311890, "start": 3140.54, "end": 3142.9, "text": " where there's some multiple choice question", "tokens": [51446, 689, 456, 311, 512, 3866, 3922, 1168, 51564], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1256, "seek": 311890, "start": 3142.9, "end": 3146.06, "text": " and then the language model is asked to output A, B, C, or D.", "tokens": [51564, 293, 550, 264, 2856, 2316, 307, 2351, 281, 5598, 316, 11, 363, 11, 383, 11, 420, 413, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15488676071166993, "compression_ratio": 1.5690376569037656, "no_speech_prob": 9.313401096733287e-05}, {"id": 1257, "seek": 314606, "start": 3146.06, "end": 3149.2999999999997, "text": " Before they did any RLHF and instruction tuning,", "tokens": [50364, 4546, 436, 630, 604, 497, 43, 39, 37, 293, 10951, 15164, 11, 50526], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1258, "seek": 314606, "start": 3149.2999999999997, "end": 3151.46, "text": " if they create a calibration plot,", "tokens": [50526, 498, 436, 1884, 257, 38732, 7542, 11, 50634], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1259, "seek": 314606, "start": 3151.46, "end": 3154.98, "text": " where they plot sort of, you know,", "tokens": [50634, 689, 436, 7542, 1333, 295, 11, 291, 458, 11, 50810], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1260, "seek": 314606, "start": 3154.98, "end": 3157.74, "text": " of all the answers in which GPT was, you know,", "tokens": [50810, 295, 439, 264, 6338, 294, 597, 26039, 51, 390, 11, 291, 458, 11, 50948], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1261, "seek": 314606, "start": 3157.74, "end": 3161.58, "text": " 0.4% confident, how often was that the correct answer?", "tokens": [50948, 1958, 13, 19, 4, 6679, 11, 577, 2049, 390, 300, 264, 3006, 1867, 30, 51140], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1262, "seek": 314606, "start": 3161.58, "end": 3164.22, "text": " GPT-4 is strikingly well calibrated.", "tokens": [51140, 26039, 51, 12, 19, 307, 18559, 356, 731, 21583, 5468, 13, 51272], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1263, "seek": 314606, "start": 3164.22, "end": 3166.14, "text": " And that's what you might expect from a model", "tokens": [51272, 400, 300, 311, 437, 291, 1062, 2066, 490, 257, 2316, 51368], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1264, "seek": 314606, "start": 3166.14, "end": 3168.98, "text": " that's doing a very good job of next token prediction,", "tokens": [51368, 300, 311, 884, 257, 588, 665, 1691, 295, 958, 14862, 17630, 11, 51510], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1265, "seek": 314606, "start": 3168.98, "end": 3170.98, "text": " of matching the distribution of language.", "tokens": [51510, 295, 14324, 264, 7316, 295, 2856, 13, 51610], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1266, "seek": 314606, "start": 3170.98, "end": 3173.2599999999998, "text": " When it's uncertain, the loss function is telling it,", "tokens": [51610, 1133, 309, 311, 11308, 11, 264, 4470, 2445, 307, 3585, 309, 11, 51724], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1267, "seek": 314606, "start": 3173.2599999999998, "end": 3175.82, "text": " it should allocate its mass, its probability mass,", "tokens": [51724, 309, 820, 35713, 1080, 2758, 11, 1080, 8482, 2758, 11, 51852], "temperature": 0.0, "avg_logprob": -0.10046985301565617, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.00024151839897967875}, {"id": 1268, "seek": 317582, "start": 3175.82, "end": 3177.38, "text": " according to that distribution.", "tokens": [50364, 4650, 281, 300, 7316, 13, 50442], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1269, "seek": 317582, "start": 3177.38, "end": 3179.82, "text": " Whereas after RLHF, the calibration is shot.", "tokens": [50442, 13813, 934, 497, 43, 39, 37, 11, 264, 38732, 307, 3347, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1270, "seek": 317582, "start": 3179.82, "end": 3182.7000000000003, "text": " And this is also what you might expect,", "tokens": [50564, 400, 341, 307, 611, 437, 291, 1062, 2066, 11, 50708], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1271, "seek": 317582, "start": 3182.7000000000003, "end": 3186.82, "text": " even if the humans who were sort of", "tokens": [50708, 754, 498, 264, 6255, 567, 645, 1333, 295, 50914], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1272, "seek": 317582, "start": 3186.82, "end": 3190.7000000000003, "text": " providing the human feedback in RLHF", "tokens": [50914, 6530, 264, 1952, 5824, 294, 497, 43, 39, 37, 51108], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1273, "seek": 317582, "start": 3190.7000000000003, "end": 3193.06, "text": " preferred the right answer, okay?", "tokens": [51108, 16494, 264, 558, 1867, 11, 1392, 30, 51226], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1274, "seek": 317582, "start": 3193.06, "end": 3197.98, "text": " The distribution that you get after performing RLHF", "tokens": [51226, 440, 7316, 300, 291, 483, 934, 10205, 497, 43, 39, 37, 51472], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1275, "seek": 317582, "start": 3197.98, "end": 3200.78, "text": " with the objective that's commonly used for RLHF", "tokens": [51472, 365, 264, 10024, 300, 311, 12719, 1143, 337, 497, 43, 39, 37, 51612], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1276, "seek": 317582, "start": 3200.78, "end": 3203.2200000000003, "text": " sort of creates a reduction in temperature.", "tokens": [51612, 1333, 295, 7829, 257, 11004, 294, 4292, 13, 51734], "temperature": 0.0, "avg_logprob": -0.11374319661961924, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00014423140964936465}, {"id": 1277, "seek": 320322, "start": 3203.22, "end": 3206.06, "text": " It's equivalent to reducing the temperature", "tokens": [50364, 467, 311, 10344, 281, 12245, 264, 4292, 50506], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1278, "seek": 320322, "start": 3206.06, "end": 3207.8999999999996, "text": " of the parts where the human feedback", "tokens": [50506, 295, 264, 3166, 689, 264, 1952, 5824, 50598], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1279, "seek": 320322, "start": 3207.8999999999996, "end": 3211.8199999999997, "text": " is exactly aligned with sort of the correct answer.", "tokens": [50598, 307, 2293, 17962, 365, 1333, 295, 264, 3006, 1867, 13, 50794], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1280, "seek": 320322, "start": 3211.8199999999997, "end": 3213.54, "text": " And so it becomes overconfident.", "tokens": [50794, 400, 370, 309, 3643, 670, 24697, 1078, 13, 50880], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1281, "seek": 320322, "start": 3214.5, "end": 3217.14, "text": " And, you know, this is very prompt dependent.", "tokens": [50928, 400, 11, 291, 458, 11, 341, 307, 588, 12391, 12334, 13, 51060], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1282, "seek": 320322, "start": 3217.14, "end": 3219.06, "text": " I don't mean to say that this is like always gonna happen", "tokens": [51060, 286, 500, 380, 914, 281, 584, 300, 341, 307, 411, 1009, 799, 1051, 51156], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1283, "seek": 320322, "start": 3219.06, "end": 3222.14, "text": " if you go and use GPT, but I went and used GPT-3.5", "tokens": [51156, 498, 291, 352, 293, 764, 26039, 51, 11, 457, 286, 1437, 293, 1143, 26039, 51, 12, 18, 13, 20, 51310], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1284, "seek": 320322, "start": 3222.14, "end": 3224.2999999999997, "text": " to do this like infilling task,", "tokens": [51310, 281, 360, 341, 411, 1536, 7345, 5633, 11, 51418], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1285, "seek": 320322, "start": 3224.2999999999997, "end": 3226.4599999999996, "text": " and it did it correctly, but also every time", "tokens": [51418, 293, 309, 630, 309, 8944, 11, 457, 611, 633, 565, 51526], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1286, "seek": 320322, "start": 3226.4599999999996, "end": 3228.14, "text": " that I generated at temperature one,", "tokens": [51526, 300, 286, 10833, 412, 4292, 472, 11, 51610], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1287, "seek": 320322, "start": 3228.14, "end": 3229.98, "text": " it gave me basically the same answer.", "tokens": [51610, 309, 2729, 385, 1936, 264, 912, 1867, 13, 51702], "temperature": 0.0, "avg_logprob": -0.13289135152643378, "compression_ratio": 1.6310344827586207, "no_speech_prob": 0.0001795096177374944}, {"id": 1288, "seek": 322998, "start": 3230.86, "end": 3233.66, "text": " So if I want to think of this distribution", "tokens": [50408, 407, 498, 286, 528, 281, 519, 295, 341, 7316, 50548], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1289, "seek": 322998, "start": 3233.66, "end": 3235.42, "text": " as sort of representing uncertainties", "tokens": [50548, 382, 1333, 295, 13460, 11308, 6097, 50636], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1290, "seek": 322998, "start": 3235.42, "end": 3237.1, "text": " that I can make decisions about whether to listen", "tokens": [50636, 300, 286, 393, 652, 5327, 466, 1968, 281, 2140, 50720], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1291, "seek": 322998, "start": 3237.1, "end": 3239.06, "text": " to my voicemail because it might contain things", "tokens": [50720, 281, 452, 1650, 46343, 864, 570, 309, 1062, 5304, 721, 50818], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1292, "seek": 322998, "start": 3239.06, "end": 3244.06, "text": " I don't know, this P-task is not up to that task.", "tokens": [50818, 286, 500, 380, 458, 11, 341, 430, 12, 83, 3863, 307, 406, 493, 281, 300, 5633, 13, 51068], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1293, "seek": 322998, "start": 3244.26, "end": 3248.46, "text": " So our idea is to instead of reducing to a prompt,", "tokens": [51078, 407, 527, 1558, 307, 281, 2602, 295, 12245, 281, 257, 12391, 11, 51288], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1294, "seek": 322998, "start": 3248.46, "end": 3250.22, "text": " reduce to a probabilistic program", "tokens": [51288, 5407, 281, 257, 31959, 3142, 1461, 51376], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1295, "seek": 322998, "start": 3250.22, "end": 3252.86, "text": " that may call a language model,", "tokens": [51376, 300, 815, 818, 257, 2856, 2316, 11, 51508], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1296, "seek": 322998, "start": 3252.86, "end": 3254.46, "text": " which is sort of a more flexible way", "tokens": [51508, 597, 307, 1333, 295, 257, 544, 11358, 636, 51588], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1297, "seek": 322998, "start": 3254.46, "end": 3257.02, "text": " of specifying what P-task distributions", "tokens": [51588, 295, 1608, 5489, 437, 430, 12, 83, 3863, 37870, 51716], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1298, "seek": 322998, "start": 3257.02, "end": 3258.22, "text": " we want to sample from.", "tokens": [51716, 321, 528, 281, 6889, 490, 13, 51776], "temperature": 0.0, "avg_logprob": -0.09508183331993537, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.0001159109961008653}, {"id": 1299, "seek": 325822, "start": 3258.2999999999997, "end": 3260.3799999999997, "text": " And I know I'm running low on time,", "tokens": [50368, 400, 286, 458, 286, 478, 2614, 2295, 322, 565, 11, 50472], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1300, "seek": 325822, "start": 3260.3799999999997, "end": 3263.06, "text": " but the idea is that these models", "tokens": [50472, 457, 264, 1558, 307, 300, 613, 5245, 50606], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1301, "seek": 325822, "start": 3263.06, "end": 3265.2999999999997, "text": " can mix calls to the language model", "tokens": [50606, 393, 2890, 5498, 281, 264, 2856, 2316, 50718], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1302, "seek": 325822, "start": 3265.2999999999997, "end": 3268.22, "text": " with conditioning statements and other logic.", "tokens": [50718, 365, 21901, 12363, 293, 661, 9952, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1303, "seek": 325822, "start": 3268.22, "end": 3271.58, "text": " So in this probabilistic program for this infilling task,", "tokens": [50864, 407, 294, 341, 31959, 3142, 1461, 337, 341, 1536, 7345, 5633, 11, 51032], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1304, "seek": 325822, "start": 3273.06, "end": 3276.54, "text": " it is in a loop going through each sort of blank", "tokens": [51106, 309, 307, 294, 257, 6367, 516, 807, 1184, 1333, 295, 8247, 51280], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1305, "seek": 325822, "start": 3276.54, "end": 3278.66, "text": " in the that we need to infill,", "tokens": [51280, 294, 264, 300, 321, 643, 281, 1536, 373, 11, 51386], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1306, "seek": 325822, "start": 3278.66, "end": 3280.14, "text": " sampling a random number of tokens", "tokens": [51386, 21179, 257, 4974, 1230, 295, 22667, 51460], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1307, "seek": 325822, "start": 3280.14, "end": 3284.02, "text": " that should fill that spot, sampling those tokens", "tokens": [51460, 300, 820, 2836, 300, 4008, 11, 21179, 729, 22667, 51654], "temperature": 0.0, "avg_logprob": -0.17045046358692403, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.00011590715439524502}, {"id": 1308, "seek": 328402, "start": 3284.02, "end": 3287.7, "text": " and then observing the next sort of fixed fragment", "tokens": [50364, 293, 550, 22107, 264, 958, 1333, 295, 6806, 26424, 50548], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1309, "seek": 328402, "start": 3288.66, "end": 3291.22, "text": " or conditioning on the next part being a fixed fragment.", "tokens": [50596, 420, 21901, 322, 264, 958, 644, 885, 257, 6806, 26424, 13, 50724], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1310, "seek": 328402, "start": 3291.22, "end": 3293.66, "text": " And this just lets us specify a model", "tokens": [50724, 400, 341, 445, 6653, 505, 16500, 257, 2316, 50846], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1311, "seek": 328402, "start": 3293.66, "end": 3295.78, "text": " that doesn't just have a prefix prompt", "tokens": [50846, 300, 1177, 380, 445, 362, 257, 46969, 12391, 50952], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1312, "seek": 328402, "start": 3295.78, "end": 3297.98, "text": " that sort of has a prompt with blanks in it.", "tokens": [50952, 300, 1333, 295, 575, 257, 12391, 365, 8247, 82, 294, 309, 13, 51062], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1313, "seek": 328402, "start": 3299.1, "end": 3300.78, "text": " I haven't said yet how we're going to sample this,", "tokens": [51118, 286, 2378, 380, 848, 1939, 577, 321, 434, 516, 281, 6889, 341, 11, 51202], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1314, "seek": 328402, "start": 3300.78, "end": 3302.98, "text": " but the idea is that this defines a specification", "tokens": [51202, 457, 264, 1558, 307, 300, 341, 23122, 257, 31256, 51312], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1315, "seek": 328402, "start": 3302.98, "end": 3304.38, "text": " for the task that we want.", "tokens": [51312, 337, 264, 5633, 300, 321, 528, 13, 51382], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1316, "seek": 328402, "start": 3304.38, "end": 3308.14, "text": " And similarly, we have programs that sort of specify", "tokens": [51382, 400, 14138, 11, 321, 362, 4268, 300, 1333, 295, 16500, 51570], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1317, "seek": 328402, "start": 3308.14, "end": 3311.02, "text": " a variety of tasks that involve sort of thinking", "tokens": [51570, 257, 5673, 295, 9608, 300, 9494, 1333, 295, 1953, 51714], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1318, "seek": 328402, "start": 3311.02, "end": 3312.46, "text": " with language, right?", "tokens": [51714, 365, 2856, 11, 558, 30, 51786], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1319, "seek": 328402, "start": 3312.46, "end": 3313.94, "text": " We can condition on hard constraints", "tokens": [51786, 492, 393, 4188, 322, 1152, 18491, 51860], "temperature": 0.0, "avg_logprob": -0.10730483119649098, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.0002531250938773155}, {"id": 1320, "seek": 331394, "start": 3313.94, "end": 3317.02, "text": " if we want to parse into a formal grammar or write a high two.", "tokens": [50364, 498, 321, 528, 281, 48377, 666, 257, 9860, 22317, 420, 2464, 257, 1090, 732, 13, 50518], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1321, "seek": 331394, "start": 3317.02, "end": 3319.54, "text": " We can do kind of a product of experts model", "tokens": [50518, 492, 393, 360, 733, 295, 257, 1674, 295, 8572, 2316, 50644], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1322, "seek": 331394, "start": 3319.54, "end": 3320.5, "text": " using multiple prompts.", "tokens": [50644, 1228, 3866, 41095, 13, 50692], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1323, "seek": 331394, "start": 3320.5, "end": 3322.86, "text": " So maybe I want to think about a fun fact", "tokens": [50692, 407, 1310, 286, 528, 281, 519, 466, 257, 1019, 1186, 50810], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1324, "seek": 331394, "start": 3322.86, "end": 3324.46, "text": " that's about both London and Paris.", "tokens": [50810, 300, 311, 466, 1293, 7042, 293, 8380, 13, 50890], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1325, "seek": 331394, "start": 3324.46, "end": 3326.3, "text": " Well, you could just ask the prompt,", "tokens": [50890, 1042, 11, 291, 727, 445, 1029, 264, 12391, 11, 50982], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1326, "seek": 331394, "start": 3326.3, "end": 3328.7000000000003, "text": " hey, please give me a fun fact about both London and Paris,", "tokens": [50982, 4177, 11, 1767, 976, 385, 257, 1019, 1186, 466, 1293, 7042, 293, 8380, 11, 51102], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1327, "seek": 331394, "start": 3328.7000000000003, "end": 3330.7000000000003, "text": " but you could also create a product of experts model", "tokens": [51102, 457, 291, 727, 611, 1884, 257, 1674, 295, 8572, 2316, 51202], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1328, "seek": 331394, "start": 3330.7000000000003, "end": 3332.3, "text": " where it has to come up with a completion", "tokens": [51202, 689, 309, 575, 281, 808, 493, 365, 257, 19372, 51282], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1329, "seek": 331394, "start": 3332.3, "end": 3334.58, "text": " that is both a completion to the sentence,", "tokens": [51282, 300, 307, 1293, 257, 19372, 281, 264, 8174, 11, 51396], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1330, "seek": 331394, "start": 3334.58, "end": 3336.06, "text": " a fun fact about London is,", "tokens": [51396, 257, 1019, 1186, 466, 7042, 307, 11, 51470], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1331, "seek": 331394, "start": 3336.06, "end": 3337.2200000000003, "text": " and a completion to the sentence,", "tokens": [51470, 293, 257, 19372, 281, 264, 8174, 11, 51528], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1332, "seek": 331394, "start": 3337.2200000000003, "end": 3338.62, "text": " a fun fact about Paris is.", "tokens": [51528, 257, 1019, 1186, 466, 8380, 307, 13, 51598], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1333, "seek": 331394, "start": 3338.62, "end": 3341.42, "text": " And that's kind of a hybrid where the idea of and", "tokens": [51598, 400, 300, 311, 733, 295, 257, 13051, 689, 264, 1558, 295, 293, 51738], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1334, "seek": 331394, "start": 3341.42, "end": 3343.46, "text": " and both is symbolically encoded,", "tokens": [51738, 293, 1293, 307, 5986, 984, 2058, 12340, 11, 51840], "temperature": 0.0, "avg_logprob": -0.11332971450181036, "compression_ratio": 2.0296052631578947, "no_speech_prob": 9.312284964835271e-05}, {"id": 1335, "seek": 334346, "start": 3343.46, "end": 3346.1, "text": " but we're still using the language models representation", "tokens": [50364, 457, 321, 434, 920, 1228, 264, 2856, 5245, 10290, 50496], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1336, "seek": 334346, "start": 3346.1, "end": 3349.86, "text": " of knowledge about fun facts and these things.", "tokens": [50496, 295, 3601, 466, 1019, 9130, 293, 613, 721, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1337, "seek": 334346, "start": 3349.86, "end": 3352.86, "text": " Similarly, we can sort of represent reward steering", "tokens": [50684, 13157, 11, 321, 393, 1333, 295, 2906, 7782, 14823, 50834], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1338, "seek": 334346, "start": 3352.86, "end": 3355.42, "text": " or a classifier guidance by conditioning,", "tokens": [50834, 420, 257, 1508, 9902, 10056, 538, 21901, 11, 50962], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1339, "seek": 334346, "start": 3355.42, "end": 3358.02, "text": " by sort of soft conditioning on a reward function.", "tokens": [50962, 538, 1333, 295, 2787, 21901, 322, 257, 7782, 2445, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1340, "seek": 334346, "start": 3359.5, "end": 3361.02, "text": " And we can also include things like,", "tokens": [51166, 400, 321, 393, 611, 4090, 721, 411, 11, 51242], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1341, "seek": 334346, "start": 3361.02, "end": 3364.2200000000003, "text": " hey, please generate a gloss of this code", "tokens": [51242, 4177, 11, 1767, 8460, 257, 19574, 295, 341, 3089, 51402], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1342, "seek": 334346, "start": 3364.2200000000003, "end": 3369.2200000000003, "text": " that when I try to semantic parse it back into code,", "tokens": [51402, 300, 562, 286, 853, 281, 47982, 48377, 309, 646, 666, 3089, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1343, "seek": 334346, "start": 3369.58, "end": 3372.7, "text": " gives me the same code I started with, things like that.", "tokens": [51670, 2709, 385, 264, 912, 3089, 286, 1409, 365, 11, 721, 411, 300, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1521899499625803, "compression_ratio": 1.6872586872586872, "no_speech_prob": 2.177986243623309e-05}, {"id": 1344, "seek": 337270, "start": 3372.74, "end": 3376.7, "text": " So those are model programs for specifying various tasks.", "tokens": [50366, 407, 729, 366, 2316, 4268, 337, 1608, 5489, 3683, 9608, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1345, "seek": 337270, "start": 3376.7, "end": 3377.98, "text": " We need inference algorithms", "tokens": [50564, 492, 643, 38253, 14642, 50628], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1346, "seek": 337270, "start": 3377.98, "end": 3380.2599999999998, "text": " for actually sampling from these distributions.", "tokens": [50628, 337, 767, 21179, 490, 613, 37870, 13, 50742], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1347, "seek": 337270, "start": 3380.2599999999998, "end": 3382.4199999999996, "text": " And so far we've been focusing", "tokens": [50742, 400, 370, 1400, 321, 600, 668, 8416, 50850], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1348, "seek": 337270, "start": 3382.4199999999996, "end": 3384.62, "text": " on sequential Monte Carlo inference algorithms.", "tokens": [50850, 322, 42881, 38105, 45112, 38253, 14642, 13, 50960], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1349, "seek": 337270, "start": 3385.66, "end": 3387.7799999999997, "text": " And we kind of have a default version of this method", "tokens": [51012, 400, 321, 733, 295, 362, 257, 7576, 3037, 295, 341, 3170, 51118], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1350, "seek": 337270, "start": 3387.7799999999997, "end": 3389.2599999999998, "text": " and then fancier versions of this method", "tokens": [51118, 293, 550, 3429, 27674, 9606, 295, 341, 3170, 51192], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1351, "seek": 337270, "start": 3389.2599999999998, "end": 3391.1, "text": " that are necessary for harder tasks.", "tokens": [51192, 300, 366, 4818, 337, 6081, 9608, 13, 51284], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1352, "seek": 337270, "start": 3391.1, "end": 3393.8399999999997, "text": " In many ways, sequential Monte Carlo looks like beam search.", "tokens": [51284, 682, 867, 2098, 11, 42881, 38105, 45112, 1542, 411, 14269, 3164, 13, 51421], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1353, "seek": 337270, "start": 3393.8399999999997, "end": 3396.2999999999997, "text": " You kind of keep multiple hypotheses around,", "tokens": [51421, 509, 733, 295, 1066, 3866, 49969, 926, 11, 51544], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1354, "seek": 337270, "start": 3396.2999999999997, "end": 3398.8999999999996, "text": " you extend them, you reweight them", "tokens": [51544, 291, 10101, 552, 11, 291, 319, 12329, 552, 51674], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1355, "seek": 337270, "start": 3398.8999999999996, "end": 3400.7799999999997, "text": " according to model specific way,", "tokens": [51674, 4650, 281, 2316, 2685, 636, 11, 51768], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1356, "seek": 337270, "start": 3400.7799999999997, "end": 3401.8999999999996, "text": " and then you resample,", "tokens": [51768, 293, 550, 291, 725, 335, 781, 11, 51824], "temperature": 0.0, "avg_logprob": -0.18604530450951962, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.00013132348249200732}, {"id": 1357, "seek": 340190, "start": 3401.9, "end": 3403.7000000000003, "text": " which is kind of like the part of beam search", "tokens": [50364, 597, 307, 733, 295, 411, 264, 644, 295, 14269, 3164, 50454], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1358, "seek": 340190, "start": 3403.7000000000003, "end": 3405.62, "text": " where you sort of down sample", "tokens": [50454, 689, 291, 1333, 295, 760, 6889, 50550], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1359, "seek": 340190, "start": 3405.62, "end": 3408.7400000000002, "text": " from your big expanded beam back to your beam size.", "tokens": [50550, 490, 428, 955, 14342, 14269, 646, 281, 428, 14269, 2744, 13, 50706], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1360, "seek": 340190, "start": 3408.7400000000002, "end": 3410.78, "text": " But unlike beam search,", "tokens": [50706, 583, 8343, 14269, 3164, 11, 50808], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1361, "seek": 340190, "start": 3410.78, "end": 3412.26, "text": " sequential Monte Carlo,", "tokens": [50808, 42881, 38105, 45112, 11, 50882], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1362, "seek": 340190, "start": 3412.26, "end": 3415.46, "text": " as you scale up the number of hypotheses that you're using,", "tokens": [50882, 382, 291, 4373, 493, 264, 1230, 295, 49969, 300, 291, 434, 1228, 11, 51042], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1363, "seek": 340190, "start": 3415.46, "end": 3417.54, "text": " instead of converging to an arg max", "tokens": [51042, 2602, 295, 9652, 3249, 281, 364, 3882, 11469, 51146], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1364, "seek": 340190, "start": 3417.54, "end": 3419.26, "text": " of your objective function,", "tokens": [51146, 295, 428, 10024, 2445, 11, 51232], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1365, "seek": 340190, "start": 3419.26, "end": 3421.62, "text": " converges to the posterior distribution,", "tokens": [51232, 9652, 2880, 281, 264, 33529, 7316, 11, 51350], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1366, "seek": 340190, "start": 3421.62, "end": 3423.82, "text": " sampling from the posterior distribution.", "tokens": [51350, 21179, 490, 264, 33529, 7316, 13, 51460], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1367, "seek": 340190, "start": 3423.82, "end": 3426.2200000000003, "text": " And this sort of default version of SMC", "tokens": [51460, 400, 341, 1333, 295, 7576, 3037, 295, 13115, 34, 51580], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1368, "seek": 340190, "start": 3426.2200000000003, "end": 3429.82, "text": " has worked for a few simple tasks that we've tried it on.", "tokens": [51580, 575, 2732, 337, 257, 1326, 2199, 9608, 300, 321, 600, 3031, 309, 322, 13, 51760], "temperature": 0.0, "avg_logprob": -0.12806886242282006, "compression_ratio": 1.7454545454545454, "no_speech_prob": 6.604315422009677e-05}, {"id": 1369, "seek": 342982, "start": 3430.82, "end": 3433.9, "text": " For example, if I want a completion that follows", "tokens": [50414, 1171, 1365, 11, 498, 286, 528, 257, 19372, 300, 10002, 50568], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1370, "seek": 342982, "start": 3433.9, "end": 3435.38, "text": " my favorite physicists is probably,", "tokens": [50568, 452, 2954, 48716, 307, 1391, 11, 50642], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1371, "seek": 342982, "start": 3435.38, "end": 3438.1000000000004, "text": " and my favorite writer is probably equally well,", "tokens": [50642, 293, 452, 2954, 9936, 307, 1391, 12309, 731, 11, 50778], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1372, "seek": 342982, "start": 3438.1000000000004, "end": 3440.02, "text": " SMC can give me Richard Feynman.", "tokens": [50778, 13115, 34, 393, 976, 385, 9809, 46530, 77, 1601, 13, 50874], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1373, "seek": 342982, "start": 3440.02, "end": 3443.34, "text": " I really admire how he communicates complex ideas so clearly.", "tokens": [50874, 286, 534, 21951, 577, 415, 3363, 1024, 3997, 3487, 370, 4448, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1374, "seek": 342982, "start": 3444.7000000000003, "end": 3447.2200000000003, "text": " Or if I want to finish the fed says,", "tokens": [51108, 1610, 498, 286, 528, 281, 2413, 264, 4636, 1619, 11, 51234], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1375, "seek": 342982, "start": 3447.2200000000003, "end": 3449.1000000000004, "text": " but only using words less than five letters,", "tokens": [51234, 457, 787, 1228, 2283, 1570, 813, 1732, 7825, 11, 51328], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1376, "seek": 342982, "start": 3449.1000000000004, "end": 3450.5, "text": " I get the fed says it will taper,", "tokens": [51328, 286, 483, 264, 4636, 1619, 309, 486, 36277, 11, 51398], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1377, "seek": 342982, "start": 3450.5, "end": 3453.34, "text": " but the rate hikes are still years away", "tokens": [51398, 457, 264, 3314, 276, 8916, 366, 920, 924, 1314, 51540], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1378, "seek": 342982, "start": 3453.34, "end": 3454.1800000000003, "text": " to something like that.", "tokens": [51540, 281, 746, 411, 300, 13, 51582], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1379, "seek": 342982, "start": 3454.1800000000003, "end": 3455.86, "text": " And it's worth noting that if you do something", "tokens": [51582, 400, 309, 311, 3163, 26801, 300, 498, 291, 360, 746, 51666], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1380, "seek": 342982, "start": 3455.86, "end": 3458.42, "text": " like token masking to enforce this constraint,", "tokens": [51666, 411, 14862, 31226, 281, 24825, 341, 25534, 11, 51794], "temperature": 0.0, "avg_logprob": -0.17137789905519413, "compression_ratio": 1.6845637583892616, "no_speech_prob": 0.0002165234909625724}, {"id": 1381, "seek": 345842, "start": 3458.46, "end": 3460.78, "text": " you just forbid the language model", "tokens": [50366, 291, 445, 34117, 264, 2856, 2316, 50482], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1382, "seek": 345842, "start": 3460.78, "end": 3463.58, "text": " from generating anything that's longer than five letters.", "tokens": [50482, 490, 17746, 1340, 300, 311, 2854, 813, 1732, 7825, 13, 50622], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1383, "seek": 345842, "start": 3463.58, "end": 3465.98, "text": " You get all sorts of weird completions.", "tokens": [50622, 509, 483, 439, 7527, 295, 3657, 1557, 626, 13, 50742], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1384, "seek": 345842, "start": 3465.98, "end": 3467.86, "text": " It's different from the posterior here.", "tokens": [50742, 467, 311, 819, 490, 264, 33529, 510, 13, 50836], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1385, "seek": 345842, "start": 3467.86, "end": 3470.26, "text": " You get completions that set up an idiom", "tokens": [50836, 509, 483, 1557, 626, 300, 992, 493, 364, 18014, 298, 50956], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1386, "seek": 345842, "start": 3470.26, "end": 3471.26, "text": " that it could only complete", "tokens": [50956, 300, 309, 727, 787, 3566, 51006], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1387, "seek": 345842, "start": 3471.26, "end": 3472.86, "text": " if it used a word longer than five letters", "tokens": [51006, 498, 309, 1143, 257, 1349, 2854, 813, 1732, 7825, 51086], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1388, "seek": 345842, "start": 3472.86, "end": 3473.86, "text": " or something like that.", "tokens": [51086, 420, 746, 411, 300, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1389, "seek": 345842, "start": 3473.86, "end": 3475.54, "text": " And then it just gets very confused", "tokens": [51136, 400, 550, 309, 445, 2170, 588, 9019, 51220], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1390, "seek": 345842, "start": 3475.54, "end": 3477.46, "text": " and right stop, that, that, read more or something.", "tokens": [51220, 293, 558, 1590, 11, 300, 11, 300, 11, 1401, 544, 420, 746, 13, 51316], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1391, "seek": 345842, "start": 3477.46, "end": 3479.1800000000003, "text": " It tries to come up with some context", "tokens": [51316, 467, 9898, 281, 808, 493, 365, 512, 4319, 51402], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1392, "seek": 345842, "start": 3479.1800000000003, "end": 3481.3, "text": " in which the sentence would be cut off early.", "tokens": [51402, 294, 597, 264, 8174, 576, 312, 1723, 766, 2440, 13, 51508], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1393, "seek": 345842, "start": 3482.54, "end": 3483.82, "text": " And for infilling tasks,", "tokens": [51570, 400, 337, 1536, 7345, 9608, 11, 51634], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1394, "seek": 345842, "start": 3483.82, "end": 3487.66, "text": " we get a variety of samples that sort of fit semantically", "tokens": [51634, 321, 483, 257, 5673, 295, 10938, 300, 1333, 295, 3318, 4361, 49505, 51826], "temperature": 0.0, "avg_logprob": -0.14958013070596232, "compression_ratio": 1.8161290322580645, "no_speech_prob": 0.0002531200007069856}, {"id": 1395, "seek": 348766, "start": 3487.7, "end": 3491.54, "text": " and syntactically with the text,", "tokens": [50366, 293, 23980, 578, 984, 365, 264, 2487, 11, 50558], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1396, "seek": 348766, "start": 3491.54, "end": 3494.5, "text": " but infilling tasks can be made much harder than this one.", "tokens": [50558, 457, 1536, 7345, 9608, 393, 312, 1027, 709, 6081, 813, 341, 472, 13, 50706], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1397, "seek": 348766, "start": 3494.5, "end": 3497.14, "text": " So I don't want to claim that this method yet solves", "tokens": [50706, 407, 286, 500, 380, 528, 281, 3932, 300, 341, 3170, 1939, 39890, 50838], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1398, "seek": 348766, "start": 3497.14, "end": 3498.22, "text": " all these infilling tasks.", "tokens": [50838, 439, 613, 1536, 7345, 9608, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1399, "seek": 348766, "start": 3498.22, "end": 3499.42, "text": " So for harder tasks,", "tokens": [50892, 407, 337, 6081, 9608, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1400, "seek": 348766, "start": 3499.42, "end": 3501.3399999999997, "text": " we think we're going to need to use fancier", "tokens": [50952, 321, 519, 321, 434, 516, 281, 643, 281, 764, 3429, 27674, 51048], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1401, "seek": 348766, "start": 3501.3399999999997, "end": 3502.5, "text": " sequential Monte Carlo algorithms.", "tokens": [51048, 42881, 38105, 45112, 14642, 13, 51106], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1402, "seek": 348766, "start": 3502.5, "end": 3504.54, "text": " And both of these sort of steps", "tokens": [51106, 400, 1293, 295, 613, 1333, 295, 4439, 51208], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1403, "seek": 348766, "start": 3504.54, "end": 3506.14, "text": " can actually be extended in various ways.", "tokens": [51208, 393, 767, 312, 10913, 294, 3683, 2098, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1404, "seek": 348766, "start": 3506.14, "end": 3508.8999999999996, "text": " We can use better proposal distributions", "tokens": [51288, 492, 393, 764, 1101, 11494, 37870, 51426], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1405, "seek": 348766, "start": 3508.8999999999996, "end": 3512.2999999999997, "text": " and sort of better reweighting strategies", "tokens": [51426, 293, 1333, 295, 1101, 319, 12329, 278, 9029, 51596], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1406, "seek": 348766, "start": 3512.2999999999997, "end": 3514.3399999999997, "text": " that are trying to guess, okay,", "tokens": [51596, 300, 366, 1382, 281, 2041, 11, 1392, 11, 51698], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1407, "seek": 348766, "start": 3514.3399999999997, "end": 3516.8999999999996, "text": " are we on the path to getting a good sample here?", "tokens": [51698, 366, 321, 322, 264, 3100, 281, 1242, 257, 665, 6889, 510, 30, 51826], "temperature": 0.0, "avg_logprob": -0.1495002551670492, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00031496764859184623}, {"id": 1408, "seek": 351690, "start": 3516.9, "end": 3518.62, "text": " And we think that techniques", "tokens": [50364, 400, 321, 519, 300, 7512, 50450], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1409, "seek": 351690, "start": 3518.62, "end": 3520.14, "text": " that have already been developed in the literature", "tokens": [50450, 300, 362, 1217, 668, 4743, 294, 264, 10394, 50526], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1410, "seek": 351690, "start": 3520.14, "end": 3523.46, "text": " for proposing good things in line with constraints", "tokens": [50526, 337, 29939, 665, 721, 294, 1622, 365, 18491, 50692], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1411, "seek": 351690, "start": 3523.46, "end": 3526.34, "text": " or sort of discriminating whether we're likely", "tokens": [50692, 420, 1333, 295, 20828, 990, 1968, 321, 434, 3700, 50836], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1412, "seek": 351690, "start": 3526.34, "end": 3531.14, "text": " to land in a constraint could be good ingredients to put here.", "tokens": [50836, 281, 2117, 294, 257, 25534, 727, 312, 665, 6952, 281, 829, 510, 13, 51076], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1413, "seek": 351690, "start": 3531.14, "end": 3534.6600000000003, "text": " But the important thing is, and this is the very end,", "tokens": [51076, 583, 264, 1021, 551, 307, 11, 293, 341, 307, 264, 588, 917, 11, 51252], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1414, "seek": 351690, "start": 3534.6600000000003, "end": 3536.54, "text": " the important thing is all of those things", "tokens": [51252, 264, 1021, 551, 307, 439, 295, 729, 721, 51346], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1415, "seek": 351690, "start": 3536.54, "end": 3538.82, "text": " become part of the inference program.", "tokens": [51346, 1813, 644, 295, 264, 38253, 1461, 13, 51460], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1416, "seek": 351690, "start": 3538.82, "end": 3539.94, "text": " And there are still guarantees", "tokens": [51460, 400, 456, 366, 920, 32567, 51516], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1417, "seek": 351690, "start": 3539.94, "end": 3541.86, "text": " that as we scale up the number of particles,", "tokens": [51516, 300, 382, 321, 4373, 493, 264, 1230, 295, 10007, 11, 51612], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1418, "seek": 351690, "start": 3541.86, "end": 3545.2200000000003, "text": " we're still targeting the original specification of the model.", "tokens": [51612, 321, 434, 920, 17918, 264, 3380, 31256, 295, 264, 2316, 13, 51780], "temperature": 0.0, "avg_logprob": -0.11569727747893531, "compression_ratio": 1.7724137931034483, "no_speech_prob": 3.119853499811143e-05}, {"id": 1419, "seek": 354522, "start": 3545.22, "end": 3548.2599999999998, "text": " So all of those heuristics or biases don't sort of,", "tokens": [50364, 407, 439, 295, 729, 415, 374, 6006, 420, 32152, 500, 380, 1333, 295, 11, 50516], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1420, "seek": 354522, "start": 3548.2599999999998, "end": 3549.74, "text": " we don't just trust them blindly.", "tokens": [50516, 321, 500, 380, 445, 3361, 552, 47744, 13, 50590], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1421, "seek": 354522, "start": 3549.74, "end": 3552.74, "text": " We don't hand the keys to those techniques.", "tokens": [50590, 492, 500, 380, 1011, 264, 9317, 281, 729, 7512, 13, 50740], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1422, "seek": 354522, "start": 3552.74, "end": 3554.9399999999996, "text": " We still have a specification that we can understand.", "tokens": [50740, 492, 920, 362, 257, 31256, 300, 321, 393, 1223, 13, 50850], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1423, "seek": 354522, "start": 3554.9399999999996, "end": 3556.66, "text": " Okay, I'll stop there.", "tokens": [50850, 1033, 11, 286, 603, 1590, 456, 13, 50936], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1424, "seek": 354522, "start": 3556.66, "end": 3557.5, "text": " Thanks.", "tokens": [50936, 2561, 13, 50978], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1425, "seek": 354522, "start": 3557.5, "end": 3563.74, "text": " All right, we're a little bit behind time.", "tokens": [50978, 1057, 558, 11, 321, 434, 257, 707, 857, 2261, 565, 13, 51290], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1426, "seek": 354522, "start": 3563.74, "end": 3565.7799999999997, "text": " So unless there is a burning question,", "tokens": [51290, 407, 5969, 456, 307, 257, 9488, 1168, 11, 51392], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1427, "seek": 354522, "start": 3565.7799999999997, "end": 3567.74, "text": " burning question, no burning questions.", "tokens": [51392, 9488, 1168, 11, 572, 9488, 1651, 13, 51490], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1428, "seek": 354522, "start": 3567.74, "end": 3571.98, "text": " Let's break for tea and maybe Zachertorte", "tokens": [51490, 961, 311, 1821, 337, 5817, 293, 1310, 1176, 4062, 83, 12752, 51702], "temperature": 0.0, "avg_logprob": -0.24532302509654652, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.00021639553597196937}, {"id": 1429, "seek": 357198, "start": 3571.98, "end": 3575.82, "text": " and you guys can talk to the speakers.", "tokens": [50364, 293, 291, 1074, 393, 751, 281, 264, 9518, 13, 50556], "temperature": 0.0, "avg_logprob": -0.3734831237792969, "compression_ratio": 0.9726027397260274, "no_speech_prob": 0.4209139943122864}, {"id": 1430, "seek": 357198, "start": 3575.82, "end": 3579.62, "text": " And at 11.30, we are going to...", "tokens": [50556, 400, 412, 2975, 13, 3446, 11, 321, 366, 516, 281, 485, 50746], "temperature": 0.0, "avg_logprob": -0.3734831237792969, "compression_ratio": 0.9726027397260274, "no_speech_prob": 0.4209139943122864}], "language": "en"}