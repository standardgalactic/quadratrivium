WEBVTT

00:00.000 --> 00:14.320
So our first speaker for the afternoon session is Jacob Steinhardt.

00:14.320 --> 00:21.400
Jacob is a remarkable researcher who kind of combines like crazy creativity and technical

00:21.400 --> 00:22.400
know-how.

00:22.400 --> 00:26.300
And he's gotten me to think very differently about a lot of problems both technically

00:26.300 --> 00:29.200
and I guess societally.

00:29.200 --> 00:36.200
So I predict it will be a very interesting talk.

00:36.200 --> 00:37.760
Thank you, Sasha.

00:37.760 --> 00:45.320
So I decided that I wanted to talk about something new kind of in the spirit of the Simon's

00:45.320 --> 00:46.320
workshop.

00:46.320 --> 00:49.120
So these slides are actually prepared totally from scratch.

00:49.120 --> 00:52.900
No one has seen them before except me on my laptop.

00:52.900 --> 00:55.240
So hopefully it will be interesting.

00:55.240 --> 01:01.720
So the kind of overall motivation here is, you know, at least I really want to understand

01:01.720 --> 01:07.480
what, you know, these ML systems that we're deploying all over the world are doing, how

01:07.480 --> 01:11.480
they behave, what's going on under the hood.

01:11.480 --> 01:15.960
But I find this very challenging because there's so many new ones, you know, maybe like every

01:15.960 --> 01:21.880
month some new model is released and they get bigger and more capable and more complex.

01:21.880 --> 01:26.960
And so how kind of understanding of what these models are doing keep up, especially given

01:26.960 --> 01:32.240
that we often get kind of new capabilities or qualitative behaviors just kind of emerging

01:32.240 --> 01:34.120
every time we scale up.

01:34.120 --> 01:42.200
And so I think, you know, there's maybe more than one answer to this, but something that

01:42.200 --> 01:48.320
will be the focus of this talk is the idea that, well, if we can somehow use LLMs to

01:48.320 --> 01:54.640
understand LLMs, then maybe we're in better shape because then every time a new better

01:54.640 --> 01:58.320
LLM is released, well, there's a new thing to understand, but we've also gotten better

01:58.320 --> 01:59.760
at understanding things.

01:59.760 --> 02:07.000
So, you know, that's kind of the idea, can we get this virtuous cycle?

02:07.000 --> 02:10.120
And then, you know, hopefully as models get better, understanding will as well.

02:10.120 --> 02:15.400
So this is going to be based on work with actually a lot of different collaborators,

02:15.400 --> 02:22.080
but three that I wanted to highlight are my students, Rachie, Eric, and Colin, in particular,

02:22.080 --> 02:29.960
a lot of the kind of LLM as statistician perspective in this talk was developed by Rachie.

02:29.960 --> 02:34.360
So what do I mean by LLMs as statisticians?

02:34.360 --> 02:40.000
So what I want to argue is that many forms of understanding that we could care about

02:40.000 --> 02:45.120
essentially reduce to some sort of statistical or data science problem, right?

02:45.120 --> 02:52.880
So maybe we're given a model, we just see what it outputs on some huge number of inputs,

02:52.880 --> 02:53.880
right?

02:53.880 --> 02:56.720
We could easily do that by just taking all of the data sets we have and seeing what

02:56.720 --> 02:57.840
the model does.

02:57.840 --> 02:59.920
And then maybe we want to kind of identify patterns, right?

02:59.920 --> 03:04.400
Are there some things that the model is good at, some things it's bad at, cases where there's

03:04.400 --> 03:09.800
something surprising, and then, you know, you could try to formalize that as some statistical

03:09.800 --> 03:11.520
hypothesis and test it.

03:11.520 --> 03:14.480
So that's kind of a statistical problem.

03:14.480 --> 03:19.480
Maybe we want to understand the training set and understand, you know, what are the important

03:19.480 --> 03:23.040
sources of variation?

03:23.040 --> 03:27.960
You know, if it turns out that large fractions of the data set are in some weird language

03:27.960 --> 03:32.840
called base 64, maybe we want to know about that.

03:32.840 --> 03:36.720
And then you could also have kind of more active learning or active sampling problems

03:36.720 --> 03:41.640
where we want to, say, generate new inputs that elicit problematic behaviors so that

03:41.640 --> 03:43.440
we can identify and fix it, right?

03:43.440 --> 03:49.280
So I think of these all as on some level kind of statistics or data problems.

03:49.280 --> 03:55.680
And so if we can, in some sort of general sense, get large language models to do statistics,

03:55.680 --> 04:00.720
then they can help us tackle all of these problems, and of course, many other useful

04:00.720 --> 04:02.440
problems as well.

04:02.440 --> 04:05.640
So to do that, what would we need to do?

04:05.640 --> 04:10.120
So I'm going to kind of take a very high level view of, you know, what is, like, the

04:10.120 --> 04:12.880
pipeline of doing statistics?

04:12.880 --> 04:16.160
And I'd say it kind of has four steps.

04:16.160 --> 04:19.280
The first is we look at some initial data.

04:19.280 --> 04:23.040
Maybe we want to think of this as training data, but, you know, just some sort of data

04:23.040 --> 04:25.560
that kind of helps us get our bearings.

04:25.560 --> 04:31.120
From this data, we maybe want to form some hypothesis, you know, and maybe, like, the

04:31.120 --> 04:38.160
hypothesis that models do worse on long inputs than short inputs, right?

04:38.160 --> 04:42.720
And then formalize that quantitatively and then test this on new data.

04:42.720 --> 04:46.120
This data could just be, you know, a held out set from the same distribution.

04:46.120 --> 04:52.280
We might care about kind of, you know, generalization to new domains, or maybe if, maybe we want

04:52.280 --> 04:56.880
to even, like, actively collect data to really stress test our hypothesis.

04:56.880 --> 05:02.560
And so I'm going to go over a couple of case studies where we'll have problems that kind

05:02.560 --> 05:06.800
of follow this structure and will automate each step with LLMs.

05:06.800 --> 05:12.600
The kind of key difference from maybe traditional ways of looking at statistics is going to

05:12.600 --> 05:19.440
be in this hypothesis H. So a key difference is, you know, often H is, like, maybe expressed

05:19.440 --> 05:25.960
as some mathematical function, like, you know, patients with this future in a data set are

05:25.960 --> 05:27.240
more likely to get this disease.

05:27.240 --> 05:31.360
I can write this as, like, expectation of some feature function.

05:31.360 --> 05:35.720
But here, because we're working with language models, H is going to be a natural language

05:35.720 --> 05:36.840
string.

05:36.840 --> 05:41.920
And so this third step, which maybe is often almost trivial, it's just, like, taking the

05:41.920 --> 05:47.520
average of your feature over the data set, now actually becomes kind of non-trivial because

05:47.520 --> 05:51.880
we have to formalize what it means for, say, a natural language string to be true about

05:51.880 --> 05:54.280
a data set or not.

05:54.280 --> 05:58.600
So if that's kind of abstract, don't worry because we're going to go into some case studies

05:58.600 --> 05:59.600
very soon.

05:59.600 --> 06:00.920
In fact, right now.

06:00.920 --> 06:01.920
Okay.

06:01.920 --> 06:08.960
So the first case study I want to talk about is finding failures in a model called Clip.

06:08.960 --> 06:14.420
So for those who aren't familiar with it, Clip is an encoder model.

06:14.420 --> 06:20.840
So it takes in inputs and kind of embeds them as some feature vector.

06:20.840 --> 06:24.760
And it's kind of used as the backbone of many later models, right?

06:24.760 --> 06:27.440
So it's often useful to have good embeddings.

06:27.440 --> 06:31.560
So many models kind of use these embeddings and then do something with them.

06:31.560 --> 06:38.720
And the thing that is kind of special about Clip is that it embeds both images and text

06:38.720 --> 06:41.880
into a kind of shared space.

06:41.880 --> 06:48.880
So it was one of the most early multimodal models that kind of did this effectively.

06:48.880 --> 06:52.080
And as sort of shown here, lots of models use it.

06:52.080 --> 06:55.320
So mid-journey uses it as its kind of first embedding step.

06:55.320 --> 07:01.880
Dolly does, stable diffusion does, and lots of like 3D and video models do as well.

07:01.880 --> 07:06.160
And you can kind of get pretty amazing results using these embeddings.

07:06.160 --> 07:10.080
Often they're used to kind of do text to image generation.

07:10.080 --> 07:14.720
So you have some text description of what you want, and then these models kind of try

07:14.720 --> 07:20.720
to generate an image that matches this description in some way.

07:20.720 --> 07:26.440
So you can see here, these are maybe a little bit small, but this is an empty glass.

07:26.440 --> 07:29.640
And then you get this glass, a family of five members.

07:29.640 --> 07:32.560
And then you get this, a man descending a mountain.

07:32.560 --> 07:33.560
You get this.

07:33.680 --> 07:37.840
So these are all amazing, except for the problem that they're also all completely wrong.

07:37.840 --> 07:39.840
This is a full glass.

07:39.840 --> 07:41.920
This is a family of six members.

07:41.920 --> 07:43.960
This is somewhat us sending a mountain.

07:43.960 --> 07:48.680
This says there's no star in the night sky, and it shows the Milky Way.

07:48.680 --> 07:53.520
And so you get these amazing images, but you often get these kind of like very semantically

07:53.520 --> 07:57.400
obvious failures.

07:57.400 --> 08:03.560
So the kind of plot twist here, aside from the fact that here's a bunch of failures that

08:03.560 --> 08:09.000
we can find, is actually that we didn't find these failures, and LLM found these failures.

08:09.000 --> 08:13.920
So these are all automatically generated by actually not a single LLM, but a kind of

08:13.920 --> 08:20.160
pipeline of several LLMs working together on kind of complimentary tasks that played

08:20.160 --> 08:22.600
to their individual strengths.

08:22.600 --> 08:28.200
So we'll talk about how could we actually build a system that, just given Clip, could

08:28.200 --> 08:33.120
kind of automatically generate all of these failures at a large scale.

08:33.120 --> 08:36.480
So are there any questions so far before I go into the details of this?

08:36.480 --> 08:37.480
Yes?

08:37.480 --> 08:38.480
One quick question.

08:38.480 --> 08:45.520
Why do you say Dolly, new being, what does that mean in this context?

08:45.520 --> 08:52.800
I think different versions of Dolly, and one of them, I'm not totally sure of the details

08:52.800 --> 08:59.280
here, but I believe that Dolly was developed by OpenAI, but then Microsoft served some

08:59.280 --> 09:02.120
version of Dolly as part of their being product.

09:02.120 --> 09:10.560
And so this is that, which we use because we can actually query it kind of publicly.

09:10.560 --> 09:11.560
Yes?

09:11.560 --> 09:18.640
Are there any new type of errors that pop up from this process?

09:18.640 --> 09:24.120
For example, like the counting error or the direction error, those are already kind of

09:24.120 --> 09:25.120
known, right?

09:25.120 --> 09:28.080
So are there any new hypothesized forms in this process?

09:28.080 --> 09:29.800
Yes, that's a good question.

09:29.800 --> 09:33.280
So I'll get to kind of numbers later.

09:33.280 --> 09:38.160
We generated 14 categories of errors.

09:38.160 --> 09:42.480
We looked for papers that kind of described these errors.

09:42.480 --> 09:47.680
I think there were a handful that had already been described in the literature.

09:47.680 --> 09:49.120
The others were not in the literature.

09:49.120 --> 09:54.640
I would probably guess that someone whose day job was to play around with these models

09:54.640 --> 09:57.240
would be familiar with these errors.

09:57.240 --> 10:04.320
But the nice thing is even without spending lots and lots of time, you can do this.

10:04.320 --> 10:12.240
In fact, I think there were cases where reviewers asked us to add a new system to this pipeline

10:12.240 --> 10:17.760
and in a half hour we just got all of the new errors that that system had.

10:17.760 --> 10:22.800
So I think right now that's the advantage is that it's much quicker.

10:22.800 --> 10:27.040
I think as models become better, I'm hoping that we'll also get things that even an expert

10:27.040 --> 10:30.040
who spent lots and lots of time might not necessarily find.

10:30.040 --> 10:32.200
Yeah, great question.

10:32.200 --> 10:33.200
Any other questions?

10:33.200 --> 10:38.760
Okay, so how do we do this?

10:38.760 --> 10:45.320
So first maybe let me give a kind of overview of the key ideas here.

10:45.320 --> 10:50.240
So again, remember, Clip is a feature encoder that is encoding both text and images.

10:50.240 --> 10:55.800
So the kind of main key idea in terms of like where we get off the ground is there's going

10:55.800 --> 10:59.600
to be a sort of notion of what I'll call a hash collision in the encoder.

10:59.600 --> 11:03.560
And we'll come up with a sort of automated way of identifying lots of hash collisions

11:03.560 --> 11:04.560
in the encoder.

11:04.560 --> 11:09.600
So this is going to give us a lot of kind of like individual examples where if you have

11:09.600 --> 11:12.920
a collision, you don't know which of those two is wrong, but you know at least one of

11:12.920 --> 11:16.760
them has to be wrong.

11:16.760 --> 11:21.680
Then kind of given those failures, we're going to use LLMs to kind of categorize them into

11:21.680 --> 11:27.280
coherent patterns and test that those patterns are actually correct by generating new examples

11:27.760 --> 11:33.520
from those patterns and making sure that they actually in fact induce failures consistently

11:33.520 --> 11:39.440
both in the encoder and on downstream tasks and even kind of also generalizing to new

11:39.440 --> 11:44.600
domains that are kind of different from what we found these patterns on.

11:44.600 --> 11:46.720
So that's kind of the high level.

11:46.720 --> 11:50.160
Let me kind of step through these all one by one.

11:50.160 --> 11:55.440
So again, remember we had this statistical pipeline of get some data, generate a hypothesis,

11:56.440 --> 11:57.880
test it on new data.

11:57.880 --> 11:59.920
So let's just go through these one by one.

11:59.920 --> 12:03.960
So first let's just talk about where are we getting this initial data?

12:03.960 --> 12:06.240
What is this kind of hash collision idea?

12:06.240 --> 12:11.160
So to talk about this, I need to give you a little bit more background on Clip.

12:11.160 --> 12:18.160
So as I said before, Clip embeds either an image I or text T, but what is it actually

12:18.160 --> 12:20.680
designed to do or what was it sort of trained on?

12:20.680 --> 12:24.160
So it's actually trained on a bunch of pairs of images and their captions.

12:24.880 --> 12:31.880
And in general, the idea is that if there's text that describes an image, then that text

12:31.880 --> 12:37.720
in that image should have similar embeddings, ideally more similar embeddings to each other

12:37.720 --> 12:39.120
than to anything else.

12:39.120 --> 12:44.320
So the training process was basically you got a bunch of images, you got a bunch of

12:44.320 --> 12:50.280
captions, and then you want to make sure that under this embedding, the cosine similarity

12:50.320 --> 12:56.200
between an image and its caption is higher than between that image and any other captions.

12:56.200 --> 12:59.760
So you kind of want, if we form this matrix of dot products, you want the diagonal to

12:59.760 --> 13:03.200
be really big and everything else to be really small.

13:03.200 --> 13:08.160
And so the kind of point here is that if T is the description of I, they should have

13:08.160 --> 13:11.520
similar embeddings.

13:11.520 --> 13:15.680
Now how can I use this to find problems?

13:15.800 --> 13:22.000
Well, if T and T prime describe different images, but have the same embedding or have

13:22.000 --> 13:28.000
very similar embeddings, then at least one of them has to be wrong in some sense, right?

13:28.000 --> 13:33.760
Because they, you know, like whatever images these corresponded to, they kind of can't

13:33.760 --> 13:37.160
both be, they can't both be right.

13:37.160 --> 13:38.160
Ah, yes, Aliosha.

13:38.160 --> 13:43.480
That's the subjection between images and text, and we know that, you know, pictures

13:43.480 --> 13:45.680
weren't a thousand words.

13:45.680 --> 13:52.200
Right, so the examples I have in mind would be something like an empty cup and a full

13:52.200 --> 13:53.200
cup.

13:53.200 --> 13:56.680
And if those, like if those sentences had the same embedding.

13:56.680 --> 14:00.480
But those are, that's, that's a very small subset of all the, like there is a lot of

14:00.480 --> 14:04.440
synonyms, right, visual synonyms.

14:04.440 --> 14:05.440
That's right.

14:05.440 --> 14:07.480
So you have to worry about visual synonyms.

14:07.480 --> 14:13.320
So we need, we need some way of measuring kind of semantic difference that hopefully

14:13.320 --> 14:16.200
implies that things are actually visually different.

14:16.200 --> 14:19.040
So I'll get to that on the next slide.

14:19.040 --> 14:20.800
Other questions though?

14:20.800 --> 14:28.720
So one thing here also is the nice thing is that I can do this only looking at text, right?

14:28.720 --> 14:32.240
I mean, I have to use the clip encoder, but I'm only encoding text.

14:32.240 --> 14:33.800
And why do I want to only look at text?

14:33.800 --> 14:37.680
Well, basically, because language models work really well and image models don't.

14:37.680 --> 14:42.920
Sorry, Aliyosha.

14:42.920 --> 14:47.280
But you know, according to Aliyosha, image models will work really well soon.

14:47.280 --> 14:48.280
Even better.

14:48.280 --> 14:50.960
But right now, right now, we want to stick with language.

14:50.960 --> 14:53.160
So, so what do we do?

14:53.160 --> 14:57.160
We're going to collect some initial corpus of text inputs, and we want these inputs to

14:57.160 --> 15:00.880
be inputs that have some visual significance.

15:00.880 --> 15:06.920
So we'll often take them from some, say, captioning data set or other kind of data

15:06.920 --> 15:10.120
set that has visual descriptions.

15:10.120 --> 15:12.760
And then we're going to embed them all under clip.

15:12.760 --> 15:16.800
And we're also going to embed them under another model called Distill Roberta, which

15:16.800 --> 15:22.320
is a very good text model, especially for embeddings.

15:22.320 --> 15:24.720
So it's a text-only model.

15:24.720 --> 15:28.560
And it also has a higher-dimensional embedding space than clip.

15:28.560 --> 15:31.920
And so for both of these reasons, it's kind of, you know, has a better understanding of

15:31.920 --> 15:37.000
text than clip does, because it gets to only focus on text and it has more parameters.

15:37.000 --> 15:45.080
And so the basic idea is, you know, so we have all these clip embeddings.

15:45.080 --> 15:52.640
And if there's two inputs that are very close in clip space, but actually have low Roberta

15:52.640 --> 15:54.560
similarity, right?

15:54.560 --> 15:58.760
So that means they're kind of different, like Roberta thinks that they're semantically

15:58.760 --> 16:03.200
different sentences, but clip says they're the same, then we're going to say, OK, that's

16:03.200 --> 16:06.720
a hash collision, probably something is wrong there.

16:06.720 --> 16:12.880
Now, I agree with you that we also want to check that these really are semantically different

16:12.880 --> 16:14.920
in a way that matters visually.

16:14.920 --> 16:19.960
Empirically, it turns out that that's the case about 90% of the time.

16:19.960 --> 16:24.800
So this is kind of good enough, and this is something we kind of verified with human

16:24.800 --> 16:25.800
subject studies.

16:25.800 --> 16:28.080
Jacob, how do you find these things?

16:28.080 --> 16:32.640
Because aren't the clip vectors like 2048 dimensional or something?

16:32.640 --> 16:39.880
Yeah, so we're just taking the cosine similarity.

16:39.880 --> 16:42.800
So this is like, this is an n squared algorithm.

16:42.800 --> 16:47.440
Oh, yeah, but OK, but I mean, if we imagine that you were just looking for collisions

16:47.440 --> 16:53.120
in this 2048 dimensional space, we would say a priori that could take astronomical time.

16:53.120 --> 16:57.840
You're saying like in practice, it takes much less time because, you know, there's something

16:57.840 --> 17:03.400
about these text inputs that makes the collisions likelier.

17:03.400 --> 17:04.760
So yeah, a couple of things.

17:04.760 --> 17:08.240
I guess first we don't need exact collisions.

17:08.240 --> 17:13.920
If the cosine similarity is large enough, I guess empirically, if it's larger than 0.88,

17:13.920 --> 17:19.720
it turns out that it's pretty likely to create a problem.

17:19.720 --> 17:21.440
So you don't need them to be exactly the same.

17:21.440 --> 17:23.400
That kind of helped you somewhat.

17:23.400 --> 17:25.400
And yeah, so somehow this...

17:25.400 --> 17:33.040
But I mean, two unit vectors having an inner product of 0.88 in a 2048 dimensional space,

17:33.040 --> 17:36.400
we might as well call that a collision, right, at that point.

17:36.400 --> 17:41.120
But he's assuming n squared time for u n with the exponential, right?

17:41.800 --> 17:42.800
So, okay, right.

17:42.800 --> 17:46.040
So Scott's claiming we would need an exponential large data set, but this is not...

17:46.040 --> 17:51.320
Yeah, yeah, yeah, but okay, but it happens that...

17:51.320 --> 17:54.840
So basically the reason why there are collisions is not...

17:54.840 --> 17:59.040
has nothing to do with like the pigeonhole principle with the space, right?

17:59.040 --> 18:04.320
It's just that, you know, the way that it's something special about this mapping that

18:04.320 --> 18:09.880
causes there to be collisions, even though a priori there could have been no collisions.

18:09.880 --> 18:10.880
Yeah, that's right.

18:10.880 --> 18:15.240
Okay, and if you care, couldn't you do it much faster by using the same kind of h and

18:15.240 --> 18:20.320
s, w in the cities with all of the embeddings?

18:20.320 --> 18:23.000
Yeah, you could do this much faster than n squared.

18:23.000 --> 18:28.280
It just turned out that this is not the bottleneck, like the bottleneck is running the forward

18:28.280 --> 18:35.080
passes of all the models and kind of looping over a bunch of pairs of, you know, thousand

18:35.080 --> 18:39.440
dimensional vectors is pretty cheap compared to running an LLM.

18:40.160 --> 18:44.240
Yeah, so we tried two different corpora.

18:44.240 --> 18:52.280
One is Cocoa and the other is, what's the other one, SNLI, yeah.

18:52.280 --> 18:58.000
These are both kind of text data sets that have visual significance.

18:58.000 --> 19:01.720
And yeah, I mean, basically the point is that there's enough, yeah, there's enough structure

19:01.720 --> 19:04.120
in text that you actually do get collisions.

19:04.120 --> 19:06.880
I don't care about the n squared.

19:06.880 --> 19:09.040
Okay, okay, got it, got it.

19:09.240 --> 19:10.920
Okay, so this is the first step, right?

19:10.920 --> 19:14.520
This is going to give us a bunch of pairs where we kind of know that like one of the

19:14.520 --> 19:18.480
two things in the pair is wrong.

19:18.480 --> 19:21.560
And so now we want to do something with that.

19:21.560 --> 19:26.520
So this is kind of the next stage is we want to generate some hypotheses based on these

19:26.520 --> 19:27.720
pairs, right?

19:27.720 --> 19:32.600
So this is where we're going to use the fact that we have text and not images, right?

19:32.600 --> 19:37.240
So the individual pair fillers are text inputs, they can feed them to GPT4.

19:37.240 --> 19:42.160
And so, you know, here's the magical prompt, it says I'll provide a series of data for

19:42.160 --> 19:43.160
you to remember.

19:43.160 --> 19:46.600
Subsequently, I'll ask you some questions to test your performance.

19:46.600 --> 19:51.400
Here's some pairs of prompts to memorize, and then you give it all of, well, you give

19:51.400 --> 19:55.040
it as many of these failures as you can fit in the context window.

19:55.040 --> 19:59.000
And then you tell it, hey, I'm trying to find failures as an embedding model.

19:59.000 --> 20:03.840
These are pairs of sentences that are encoded very similarly using the specific examples.

20:03.840 --> 20:07.160
Are there any general types of failures you notice the embedding is making?

20:07.160 --> 20:11.760
And then, you know, you kind of give it some more context and you'd say, okay, what does

20:11.760 --> 20:12.760
it generate, right?

20:12.760 --> 20:18.120
So you're basically saying, here's some data, please look at it, please tell me some patterns.

20:18.120 --> 20:23.080
And so then it, you know, it does a pretty good job of coming up with things.

20:23.080 --> 20:28.840
It says, okay, there's negation, temporal differences, quantifiers, and the nice thing

20:28.840 --> 20:33.080
is actually it doesn't just say negation, but it gives a bunch of elaborations.

20:33.080 --> 20:36.880
So it says embedding models may not correctly capture the negative context in a sentence

20:36.880 --> 20:41.920
leading to similarities between sentences with and without negation.

20:41.920 --> 20:45.960
This can result in incorrect visual representations if the presence or absence of an action is

20:45.960 --> 20:49.200
significant in video generation.

20:49.200 --> 20:53.120
And it kind of keeps going.

20:53.120 --> 20:59.240
And you get, I guess in this case, you get kind of 14 distinct failures in total on this

20:59.240 --> 21:02.400
list.

21:02.400 --> 21:05.880
And the other thing is empirically, GPD4 just kind of always uses this consistent list

21:05.880 --> 21:09.600
format so you can automatically just parse out the individual hypotheses.

21:09.600 --> 21:10.600
Yeah, Lisa.

21:10.600 --> 21:18.080
Have you tried just like asking GPT without these like inputs, like what are common failure

21:18.080 --> 21:21.120
cases of image embedding models or something?

21:21.120 --> 21:22.120
Yes.

21:22.120 --> 21:27.240
So that's a baseline that I will show results on later.

21:27.240 --> 21:32.560
And yeah, it works a lot less well.

21:33.200 --> 21:38.280
So this kind of gives us hypotheses.

21:38.280 --> 21:40.080
So this was step two.

21:40.080 --> 21:44.520
But now we're running into this problem of, OK, usually in statistics a hypothesis is

21:44.520 --> 21:49.840
like actually some mathematical function, but here these are just sentences.

21:49.840 --> 21:52.400
So now we need to formalize this hypothesis.

21:52.400 --> 21:57.920
So we have this list of hypotheses, h1 through hk, that are all natural language descriptions.

21:57.920 --> 22:03.440
So how can we test if one of these hypotheses is actually any good?

22:03.440 --> 22:07.440
So I think this is a pretty interesting conceptual question to think about.

22:07.440 --> 22:14.400
So maybe I'll pose it to the audience if anyone has ideas for how we could formalize this.

22:14.400 --> 22:16.400
Hi, Chris.

22:16.400 --> 22:18.400
Good afternoon.

22:18.400 --> 22:25.520
Two or more about pairs as to whether they match this hypothesis and then see if the

22:25.520 --> 22:26.520
images are wrong.

22:26.520 --> 22:27.520
OK.

22:27.520 --> 22:31.520
In this analysis of DPT4, if you have access.

22:31.520 --> 22:32.520
Good.

22:32.520 --> 22:33.520
Yes.

22:33.520 --> 22:34.520
So that is Yan Yi.

22:34.520 --> 22:41.680
I mean, if we think about research hypothesis, there are a few dimensions that you can use

22:41.680 --> 22:44.160
to categorize whether some say it's a hypothesis.

22:44.160 --> 22:47.160
So for example, it should be testable, right?

22:47.280 --> 22:49.280
There should be a clear scope.

22:49.280 --> 22:53.280
There are a few dimensions I think that you can come up with based on experts.

22:53.280 --> 22:54.280
Right.

22:54.280 --> 23:01.000
So you can kind of ask experts or DPT4 if it had those properties.

23:01.000 --> 23:05.960
So it turns out we're going to do something pretty similar to what Chris said, although

23:05.960 --> 23:09.120
we're going to look at generation rather than classification.

23:09.120 --> 23:17.880
So we're going to say H is a good hypothesis if when you hand that description to some

23:17.880 --> 23:24.400
intelligent agent, in this case not humans, because humans are expensive, but DPT4, they

23:24.400 --> 23:31.440
do a better job of generating new failures than they would otherwise.

23:31.440 --> 23:36.360
So this is the way we're going to quantify this.

23:36.360 --> 23:43.040
So we'll say H is a good hypothesis if it can be used to generate new failures better

23:43.040 --> 23:46.720
than some just baseline method of generating failures.

23:46.720 --> 23:50.080
And this is where we're going to get to your question, Lisa.

23:50.080 --> 23:57.480
So we can either hand DPT4 these hypotheses or we could just ask DPT4 to brainstorm without

23:57.480 --> 24:03.520
any data ways in which vision models might be bad and kind of test those against each

24:03.520 --> 24:06.200
other and see which one does better.

24:06.200 --> 24:09.920
So we're going to test this by prompting an LLM with H as a context.

24:09.920 --> 24:12.680
And so again, what is the magical prompt?

24:12.680 --> 24:18.000
The magical prompt is to say write down 41 pairs of prompts that an embedding model with

24:18.000 --> 24:22.240
the following failure mode might encode similarly, even though they would correspond to different

24:22.240 --> 24:24.280
images if used as captions.

24:24.280 --> 24:30.240
Use the following format so you give it kind of a format so that we can extract things programmatically.

24:30.240 --> 24:34.680
And then we say some other stuff to motivate it, saying that it will be evaluated based

24:34.680 --> 24:37.680
on how well it performs.

24:37.680 --> 24:44.840
And then you give the failure mode and as kind of the description that we extracted before.

24:44.840 --> 24:54.080
Y41, that's the length, basically the length of the output context window that can be fit.

24:54.080 --> 24:59.160
So if you want more than 41, you just have to ask it a couple of times.

24:59.200 --> 25:01.600
So this is what we did.

25:01.600 --> 25:04.600
Yeah, Nicholas.

25:04.600 --> 25:10.240
How much does this be creative and cautious and these kinds of things like actually help?

25:10.240 --> 25:15.440
Or is this just like black magic that you sort of sprinkle on top?

25:15.440 --> 25:20.240
We didn't do careful ablations on the prompt.

25:20.240 --> 25:26.200
I think, yeah, we added a bunch of stuff until it worked.

25:26.600 --> 25:29.960
I don't think we tried removing things to see what was actually necessary.

25:29.960 --> 25:36.480
So it seems totally like I would guess that if you tried to distill this to its bare essentials,

25:36.480 --> 25:39.080
you could get something simpler.

25:39.080 --> 25:41.320
But we didn't try to do that.

25:41.320 --> 25:44.120
But yeah, great question.

25:44.120 --> 25:48.240
OK, so then we want to quantify by measuring the success rate.

25:48.240 --> 25:54.600
So we get all these pairs of prompts that are supposedly supposed to be new failures.

25:54.600 --> 25:56.960
So we can do this in two ways.

25:56.960 --> 26:04.640
We can look at the fraction of things generated that are hash collisions in the same sense as before.

26:04.640 --> 26:07.680
So that's kind of an easy thing to do.

26:07.680 --> 26:11.120
At some point, we want to make sure that the system is actually doing something

26:11.120 --> 26:16.440
and that the something doesn't involve trusting that LLMs are good at their job.

26:16.440 --> 26:23.880
So we also do a human evaluation where we look at downstream systems that rely on clip

26:23.960 --> 26:29.440
and ask humans if there's a failure to make sure that these actually are failures

26:29.440 --> 26:33.560
and not just happen to have high cosine similarity.

26:33.560 --> 26:36.040
So those are the two things we do.

26:36.040 --> 26:39.000
So let's kind of go over the results.

26:39.000 --> 26:45.240
So first, just kind of looking at hash collisions,

26:45.240 --> 26:50.400
but testing on these new inputs that were generated.

26:50.400 --> 26:58.480
So we say an input has a success if these similarities are above some threshold.

26:58.480 --> 27:00.760
And what is this table saying?

27:00.760 --> 27:06.920
So these rows are kind of the different failures generated by the system.

27:06.920 --> 27:10.480
And actually, we considered six different systems.

27:10.480 --> 27:16.600
So you can ask different models to look at the data and propose hypotheses.

27:16.600 --> 27:21.800
So these are different kind of proposal models, GPT-4-Claude or GPT-3.5.

27:21.800 --> 27:28.680
And then you can also vary the data set that you used to actually get these failures out.

27:28.680 --> 27:34.160
So these are the two data sets that people asked about before, COCO and SNLI.

27:34.160 --> 27:36.840
So I guess a couple interesting things.

27:36.840 --> 27:46.320
One is that, oh, and a check mark means that the model generated the failure at all in its list.

27:46.360 --> 27:55.680
And then the color is kind of the success rate of generating new inputs conditional on that failure description.

27:55.680 --> 27:58.080
So a couple interesting things.

27:58.080 --> 28:01.360
First of all, the data set seems to actually matter.

28:01.360 --> 28:13.640
So kind of for both GPT-4 and Claude, action, well, OK, maybe let's pick a more intuitive one.

28:13.640 --> 28:26.160
OK, so for both GPT-4 and Claude and GPT-3.5, SNLI elicits granularity as a failure, whereas COCO never does.

28:26.160 --> 28:28.960
And sometimes it's kind of not quite so systematic.

28:28.960 --> 28:35.120
But in general, it sort of seems like these data sets actually do kind of like elicit different failures.

28:35.120 --> 28:41.400
So there is at least some dependence on the data, which is somewhat reassuring.

28:41.400 --> 28:48.280
The other thing is maybe as expected, GPT-4 and Claude in general find many more failures than GPT-3.5 does.

28:48.280 --> 28:54.040
So these better models actually generate more distinct hypotheses.

28:54.040 --> 29:07.560
And then a final thing that is interesting is actually even for the same failure, bigger models often are giving you higher success rates.

29:07.560 --> 29:12.480
So you can see this in a couple places like for granularity.

29:12.480 --> 29:22.200
The description of the granularity failure that GPT-4 generated was apparently better in terms of if you then hand that back to GPT-4,

29:22.200 --> 29:32.280
it more successfully generates novel failure instances compared to the kind of description that GPT-3.5 gave.

29:32.280 --> 29:37.360
In all of these cases, we're fixing GPT-4 as kind of the thing that's generating new failures.

29:37.360 --> 29:40.480
So there's no effect from that.

29:40.480 --> 29:47.000
So this kind of difference is just coming from the actual text description output by them all.

29:47.000 --> 29:56.720
So are there any questions about this data?

29:56.720 --> 30:11.640
What more is left about when GPT-4 does better than GPT-3.5, is it because it better understood the instruction versus maybe GPT-3.5 also understood the description,

30:11.640 --> 30:24.160
but somehow the examples were very just curious what sort of qualitative differences are there between different models.

30:25.080 --> 30:29.720
So I haven't thought about this a ton.

30:29.720 --> 30:41.280
I think my two main hypotheses here would be, I believe GPT-4 has a larger context window so it can see more examples, which might be useful.

30:41.280 --> 30:54.040
But I think probably the more important thing is actually just that the task of proposing hypotheses from a data set is actually a pretty challenging task.

30:54.040 --> 31:00.240
And so even kind of frontier models are not that good at it.

31:00.240 --> 31:11.520
So then once you drop down from GPT-4 to GPT-3.5, you're kind of losing, probably just losing too much capability for it to be super consistent.

31:11.520 --> 31:13.080
That would be my hypothesis.

31:13.080 --> 31:13.880
I don't know.

31:13.880 --> 31:14.560
Yeah, Richie.

31:14.560 --> 31:23.040
Yeah, so in practice, we found that GPT-3.5 doesn't really condition on the data very well, while GPT-4 actually does condition on the data.

31:23.040 --> 31:25.040
And then describe the same thing that they do.

31:28.040 --> 31:29.040
Yes?

31:29.040 --> 31:38.600
If I have the samples that the model generated actually fall into those categories, so maybe they overlap with some other categories.

31:38.600 --> 31:48.800
So I don't think we did a systematic evaluation of whether all of the examples fall into those categories.

31:49.560 --> 31:56.560
Yeah, I can give some orders on the next slide that get at least implicitly at that.

31:56.560 --> 32:00.560
It looked like maybe someone else had a question.

32:00.560 --> 32:08.560
OK, so maybe let's go to the human evaluation, which will at least partially answer your question.

32:08.560 --> 32:18.560
So we wanted to not just stick with saying that you actually get these hash collisions, but show that these actually lead to images that humans say are wrong.

32:19.320 --> 32:30.320
So, OK, darn, the text here is a little bit small, but this is kind of the human annotator interface that we gave.

32:30.320 --> 32:39.320
So we had prompt one, a city skyline with a bridge, prompt two, a city skyline without a bridge.

32:39.320 --> 32:42.320
So this is this kind of collision pair.

32:42.320 --> 32:52.320
This is an image that came from one of these two prompts chosen at random.

32:52.320 --> 33:04.320
And so the annotator has to say either that this corresponds to prompt one, it corresponds to prompt two, it corresponds to neither of them,

33:05.320 --> 33:10.320
or these prompts are described visually identical situations.

33:10.320 --> 33:17.320
So this kind of gets at your earlier question, Alyosha, on whether these are actually semantically different.

33:17.320 --> 33:29.320
And so we're kind of measuring what counts as a successful failure, either if the annotator says that it's wrong,

33:29.320 --> 33:35.320
or if they say it's prompt two, but it was actually generated by prompt one, or vice versa.

33:35.320 --> 33:49.320
And so then you can look at the kind of rate at which mistakes are made conditional on different levels of clip similarity in the two prompts.

33:49.320 --> 33:53.320
So this is kind of testing that high similarity actually leads to failures.

33:53.320 --> 33:59.320
And you can kind of see there's this like inflection point at like 0.88.

33:59.320 --> 34:07.320
So this is kind of one verifying that actually we do get pretty high rates of failure,

34:07.320 --> 34:13.320
and two that this magical threshold I told you about earlier is actually kind of reasonable threshold.

34:13.320 --> 34:20.320
And then finally, to get at this question of whether these descriptions are actually doing anything.

34:20.320 --> 34:24.320
So I guess this doesn't test whether the failures correspond to the descriptions,

34:24.320 --> 34:29.320
but it tests that the descriptions are actually needed to get high failure rates.

34:29.320 --> 34:35.320
If you have a baseline system where you just ask it to like brainstorm possible failures images might have,

34:35.320 --> 34:42.320
and then condition on those, you only get about 20% failure rate,

34:42.320 --> 34:47.320
whereas you get an 80% failure rate if you use this data conditioned system.

34:47.320 --> 34:55.320
So these are the human evaluated rather than model evaluated equals.

34:55.320 --> 35:02.320
So are there questions about this?

35:02.320 --> 35:05.320
This is a talk where high failure rate is better.

35:05.320 --> 35:12.320
Yes, yeah, you want high failure because we're trying to find failures so that we can fix them.

35:12.320 --> 35:21.320
Okay, and then I guess a final cool thing is, you know, a kind of really big bonus of having this come from language models

35:21.320 --> 35:24.320
is language models are kind of automatically steerable.

35:24.320 --> 35:26.320
Right, so I have this way of generating failures,

35:26.320 --> 35:33.320
but I can then just ask the model to give me failures that are relevant to some new domain.

35:33.320 --> 35:40.320
And so in this case, we kind of asked it to generate failures that are relevant to self-driving.

35:40.320 --> 35:46.320
The data sets are still cocoa and SNLI so we didn't give it data that would specialize to self-driving,

35:46.320 --> 35:54.320
but it can still kind of generate these failures in this novel domain and still have a good success rate.

35:54.320 --> 35:57.320
And so these are just kind of examples.

35:57.320 --> 36:01.320
Stable diffusion, you have the cars on the right side of the lane, but it's on the left side.

36:01.320 --> 36:04.320
This is not a green light, gives you a green light.

36:04.320 --> 36:13.320
A yield sign gives you something that is at least not shaped like a yield sign, probably a stop sign.

36:13.320 --> 36:15.320
And then a car stops for a red light.

36:15.320 --> 36:20.320
This is actually a text to video model and the light is green.

36:21.320 --> 36:28.320
What data from cocoa and SNLI are you passing in?

36:28.320 --> 36:31.320
No, no, so you're passing in the text from...

36:31.320 --> 36:39.320
So this is for the very first stage where you're giving it a bunch of just text inputs and embedding them to check for hash collisions.

36:39.320 --> 36:46.320
So those hash collisions were from embedding text sentences from cocoa and SNLI.

36:46.320 --> 36:54.320
So there's actually no images anywhere here except in the output of the systems.

36:54.320 --> 37:00.320
I'm kind of curious about the hypothesis part of this and whether that's kind of necessary.

37:00.320 --> 37:05.320
So we had a paper a couple years ago and just tried finding these collisions.

37:05.320 --> 37:11.320
And I kind of wonder if you could just give it a sentence and like search for a collision and just cut out the language model.

37:11.320 --> 37:15.320
What is it adding in the process?

37:15.320 --> 37:19.320
Well, finding the initial... Oh, I see.

37:19.320 --> 37:34.320
So I think one thing is this steerability I think would be challenging in some cases if you were doing that because you would need a large data set of text in whatever new domain you were looking at.

37:34.320 --> 37:42.320
We don't need a bunch of sentences about self-driving cars to do this, but if you were looking for collisions manually, then you would have to do that.

37:42.320 --> 37:45.320
I see.

37:45.320 --> 37:48.320
Yeah, cool.

37:48.320 --> 37:57.320
Okay, so to summarize this, right, we had these four stages of, you know, first we want to get initial data, which we did by scraping hash collisions from this text data set.

37:57.320 --> 38:02.320
This kind of invoked these two models clipped into still Roberta.

38:02.320 --> 38:06.320
Then we generate hypotheses by prompting GPT-4.

38:06.320 --> 38:11.320
Then we kind of formalize these hypotheses by looking at the success rate of generating new failures.

38:11.320 --> 38:16.320
So we use GPT-4 to generate the failures and clip to evaluate them.

38:16.320 --> 38:21.320
And then we can also do this active steering, again, prompting GPT-4.

38:21.320 --> 38:35.320
So I think one thing I want to highlight here is that, you know, often we think about just having this one language model and we just come up with our super clever prompt that solves everything and maybe do chain of thoughts.

38:35.320 --> 38:37.320
And this sort of thing.

38:37.320 --> 38:52.320
But I think you can actually get a lot further if you're willing to kind of use this kind of, you know, ecosystem of models together in creative ways.

38:52.320 --> 39:02.320
And I think statistics is a particularly kind of good use case for this because there are these different stages of the pipeline that require kind of different skills.

39:02.320 --> 39:04.320
And statistics also has some nice properties, right?

39:04.320 --> 39:10.320
Like many parts of it are kind of automatically measurable and verifiable.

39:10.320 --> 39:21.320
And so you get a lot of the same strengths as Adam was talking about yesterday with computer programming where, you know, you can...

39:21.320 --> 39:25.320
We haven't done much of this, but like you can, you know, maybe do this self-training.

39:25.320 --> 39:35.320
And maybe if you get models that were really, really, really good at statistics, like superhuman at statistics, because there's so much automatically-generatable data.

39:35.320 --> 39:36.320
Okay.

39:36.320 --> 39:40.320
So that was the first case study.

39:40.320 --> 39:42.320
Let me go over the second one.

39:42.320 --> 39:48.320
We'll go a bit more quickly now that we've kind of built up a lot of these conceptual ideas.

39:48.320 --> 40:00.320
So here I'm actually going to talk about a kind of meta task that is then going to be useful for lots of individual ways in which we would want to understand language models.

40:00.320 --> 40:04.320
So this meta task is classifying with natural language predicates.

40:04.320 --> 40:08.320
So the task here is we're going to be given two text data sets, D1 and D2.

40:08.320 --> 40:11.320
We want to find out what's different between them.

40:11.320 --> 40:15.320
And this difference, again, should be some natural language string H.

40:15.320 --> 40:20.320
And so we can kind of think about this as isomorphic to binary classification, right?

40:20.320 --> 40:26.320
We're kind of trying to classify between D1 and D2, but where the function is described in natural language.

40:26.320 --> 40:30.320
So let me just give you an example of what this task might look like, right?

40:30.320 --> 40:34.320
So maybe these are my two data sets, D1 and D2.

40:34.320 --> 40:39.320
We want to come up with a natural language description of how they're different.

40:39.320 --> 40:44.320
So can anyone figure this one out?

40:45.320 --> 40:47.320
Okay, yes.

40:47.320 --> 40:50.320
So the left is French and the right is English.

40:50.320 --> 40:56.320
So our H would be D1 contains more French sentences compared to D2.

40:56.320 --> 41:00.320
Okay, so here's a harder example.

41:00.320 --> 41:06.320
Maybe partly hard due to text size.

41:06.320 --> 41:14.320
So I claim that even if you looked at this for a while, it would be hard to tell what the difference was.

41:14.320 --> 41:23.320
And in fact, the difference is that sentences in D1 contain at least two female characters in them, whereas sentences in D2 do not.

41:23.320 --> 41:32.320
This is actually pretty challenging because there's things like she carried a total of eight torpedoes where she refers to a ship,

41:32.320 --> 41:35.320
which is not in fact a female character.

41:35.320 --> 41:40.320
And you also have to know that Professor McKeown is female.

41:40.320 --> 41:49.320
And so there's kind of a lot of world knowledge and kind of non-trivial stuff in solving a problem like this.

41:49.320 --> 41:52.320
So I guess that's the kind of meta task.

41:52.320 --> 41:54.320
Why should we care about this?

41:54.320 --> 42:00.320
So first, I'll give you three use cases that would help us better understand LLMs.

42:00.320 --> 42:05.320
So we could want to understand distribution shift.

42:05.320 --> 42:10.320
So especially if a model is doing poorly out of distribution, maybe we want to diagnose what's different.

42:10.320 --> 42:16.320
And so we might find out that the test distribution involves more formal writing than the training distribution.

42:16.320 --> 42:20.320
And that might help us diagnose failures or tell us what we should fine tune on.

42:20.320 --> 42:24.320
The positive class contains more URLs than the negative class.

42:24.320 --> 42:30.320
If this were a spam classification data set, this would tell us that there was this potential spurious queue.

42:30.320 --> 42:37.320
That maybe the model just looks at the presence or absence of URLs and we should make sure that that's not just what it's doing.

42:37.320 --> 42:39.320
We could do error analysis.

42:39.320 --> 42:47.320
I could give you two models and I could look at the difference between inputs where one of them versus the other one makes mistakes.

42:47.320 --> 42:51.320
And then we could also go beyond just trying to understand LLMs.

42:51.320 --> 42:57.320
You know, you could start trying to do, you know, for like say like social science, right?

42:57.320 --> 43:07.320
I look at a bunch of tweets from one year versus another year and it turns out and say, okay, public opinion from this year is more optimistic about the pandemic than last year, right?

43:07.320 --> 43:12.320
We can kind of generate at least descriptive hypotheses like this.

43:12.320 --> 43:25.320
Of course, you would then want to carefully do all of the causal inference and other stuff to validate these, but this at least can generate hypotheses for you.

43:25.320 --> 43:27.320
Okay, so how will we do this?

43:27.320 --> 43:30.320
Well, yes, D.

43:30.320 --> 43:38.320
So, if you go back to the examples you have, it feels like the hypothesized space is huge, right?

43:38.320 --> 43:45.320
So there seems to be a recall precision issue here, and it seems your ground truth only has some of them.

43:45.320 --> 43:51.320
Like for them, the very easy example, my hypothesis could be there is no relation between those two sentences.

43:51.320 --> 43:55.320
I mean, you have like, it has more French versus the second sentence.

43:55.320 --> 43:57.320
The space seems huge.

43:57.320 --> 44:00.320
How do you locate it to the final ones we want?

44:00.320 --> 44:04.320
So we're not going to have ground truth in most cases.

44:04.320 --> 44:13.320
There are a couple of cases where we did create a ground truth so that we could kind of test in the traditional setting with kind of gold labels,

44:13.320 --> 44:20.320
but we're going to kind of take a similar perspective to how you'd evaluate any other classifier, right?

44:20.320 --> 44:25.320
So we're going to come up with a way to quantify these in terms of some classification error rate,

44:25.320 --> 44:33.320
and then we'll say that one hypothesis is better than another if it has a lower error rate in distinguishing D1 and D2.

44:33.320 --> 44:38.320
And so rather than having a fixed ground truth, we can just talk about which systems are better or worse,

44:38.320 --> 44:44.320
and maybe we could also say compare to humans to get some like overall benchmark.

44:44.320 --> 44:50.320
Some of the hypothesis might be very true, and others may be more usable, right?

44:50.320 --> 44:51.320
Okay, right.

44:51.320 --> 44:58.320
So there's, right, so you might also want to evaluate if there's some like goal,

44:58.320 --> 45:03.320
you might want to evaluate like relevance to the goal and some notion of novelty.

45:03.320 --> 45:05.320
These become, start to become subjective.

45:05.320 --> 45:15.320
So we have done some of this where we get human ratings of novelty and relevance.

45:15.320 --> 45:24.320
Reviewers didn't like it actually because it's too hard to define novelty, but anyways, like you can do this and like I think,

45:24.320 --> 45:27.320
but I do agree it's kind of a tricky problem.

45:27.320 --> 45:37.320
So just building on these questions, are you constraining some complexity of your hypothesis?

45:37.320 --> 45:40.320
Are you looking for short hypotheses?

45:40.320 --> 45:51.320
So we'll implicitly have short hypotheses here, although that kind of just comes from the fact that these are going to be generated by an LLM,

45:51.320 --> 45:56.320
and LLMs will only output things that are so long.

45:56.320 --> 46:06.320
But yeah, we like, for usability reasons, you would want this to be kind of short and interpretable.

46:06.320 --> 46:08.320
So yeah, how are we going to do this?

46:08.320 --> 46:14.320
We're basically just going to use LLMs somewhat similar to before, right?

46:14.320 --> 46:22.320
So I won't give you the full magical prompt that Rachel came up with, but just kind of schematically,

46:22.320 --> 46:30.320
give it a bunch of examples from the first distribution and label them as A, a bunch from the second label them as B.

46:30.320 --> 46:36.320
And then we say, you know, compared to group B, each sentence from group A, and then we ask it to complete it.

46:36.320 --> 46:40.320
This was done at a point where we were using GBD3.

46:40.320 --> 46:43.320
So we needed it to be in this completion format.

46:43.320 --> 46:51.320
Once you have instruction tuned models, there's maybe nicer prompts you can have.

46:51.320 --> 46:53.320
But this is kind of the basic idea.

46:53.320 --> 46:59.320
And then you sample this a bunch of times with different, you know, sub-samples of the data, right?

46:59.320 --> 47:03.320
So keep in mind that we can only fit maybe like 30 or so examples into the context window.

47:03.320 --> 47:08.320
So we have a data set with thousands of examples is like a very tiny fraction.

47:08.320 --> 47:13.320
So we kind of keep sub-sampling to generate different hypotheses.

47:13.320 --> 47:20.320
And then, you know, you'll get things like is more positive, contains the word chapter, is longer.

47:20.320 --> 47:26.320
Some of them actually kind of have, yeah, some of them end up being kind of trivial.

47:26.320 --> 47:28.320
So you might want ways of filtering them out.

47:28.320 --> 47:32.320
But you kind of get this set of candidate hypotheses.

47:32.320 --> 47:38.320
So this is kind of telling us how we do the first two steps of this statistics pipeline, right?

47:38.320 --> 47:40.320
We look at this initial data.

47:40.320 --> 47:47.320
We prompt GPTN for some N with examples from D1 and D2.

47:47.320 --> 47:50.320
And then we ask how they're different in this form of the hypothesis.

47:50.320 --> 47:55.320
But now we need to somehow formalize each quantitatively and test it on new data.

47:55.320 --> 48:07.320
So I guess, again, maybe I'll pose the question, you know, how could we formalize this each quantitatively?

48:07.320 --> 48:14.320
How could we sort of say quantitatively how good is it?

48:14.320 --> 48:16.320
Sorry, what do you say first?

48:16.320 --> 48:18.320
Okay, good.

48:18.320 --> 48:26.320
So we can say, you know, good hypothesis is something that helps tell D1 and D2 apart.

48:26.320 --> 48:36.320
So we'll, you know, take a sample from D1, a sample from D2, mix them up randomly so you don't know which is which.

48:36.320 --> 48:45.320
Tell either a human or an LLM the hypothesis and ask them to say which is which.

48:45.320 --> 48:53.320
So, you know, as an example, say H is involves more formal writing, we can interpret this as basically a two argument predicate, right?

48:53.320 --> 49:05.320
So if I have sentences X1 and X2, H of X1, X2 is some binary predicate that is the truth value of, you know, the utterance X1 involves more formal writing than X2.

49:06.320 --> 49:10.320
And so this should be true or false.

49:10.320 --> 49:16.320
And so then we just ask a human or a language model if it's true or false.

49:16.320 --> 49:30.320
And so then we'll say H is a correct hypothesis about D1 versus D2 if in expectation over samples X1 from D1 and X2 from D2, this is much less than 0.5, right?

49:30.320 --> 49:44.320
So 0.5 would be chance if this is sort of the, like some measure of the classification error, if it's much less than 0.5, then we've, we figured out something non-trivial about D1 and D2.

49:44.320 --> 49:54.320
And so yeah, how to implement this, I guess you could ask humans or you could just query an LLM.

49:55.320 --> 50:11.320
And so what does this look like, right? So just to illustrate this, if H is samples from D1 or more positive than those from D2, we, you know, we give, in this case, Charlie Snell, who was an undergrad at Berkeley at the time and is now a PhD student at Berkeley,

50:12.320 --> 50:30.320
this paper proposes an impactful task or the approach of this paper is too trivial, and ask him which of these it's true about, and then he says something, and then, you know, maybe Charlie's time is pretty valuable, so you can hire crowd workers to do this instead.

50:30.320 --> 50:48.320
But the problem is, you know, even if we just wanted to average over, say, like 100 samples from this distribution to get some notion of accuracy, this would cost $10 per text description, and this is very expensive.

50:48.320 --> 50:50.320
You don't want to do this.

50:50.320 --> 51:02.320
And so the nice thing is LLMs kind of reduce the cost of this pipeline by about a factor of 1000. You can do this 100 samples for only seven cents with GP3.5 turbo.

51:02.320 --> 51:11.320
And so then this gives you a kind of automatic, quantifiable measure of how successful this hypothesis is.

51:11.320 --> 51:25.320
And the nice thing is also that it's somewhat more reproducible than humans, like you don't have to worry about getting back the same human label errors again, because, you know, the model, well, the model's not actually fixed, open AI keeps updating it, that's kind of annoying.

51:25.320 --> 51:33.320
But if they were to serve a stable version of the model, then this would be reproducible.

51:34.320 --> 51:51.320
Right, so now we've kind of gotten this whole pipeline, right, so the overall system as we have this proposal, which at the time we initially wrote this paper was a fine tune to GP3 that generates these candidate hypotheses.

51:51.320 --> 51:57.320
Then we have a verifier that kind of, you know, does this check on each of the hypotheses.

51:57.320 --> 52:04.320
And at the time it was fine tune unified QA. Now you can kind of replace with newer models.

52:04.320 --> 52:11.320
And then you kind of, you know, re rank the hypotheses based on their actual success rate at this classification task.

52:11.320 --> 52:18.320
And, you know, why is this decomposition useful. Well, from an engineering perspective, and I think this is actually very important.

52:18.320 --> 52:24.320
The proposer only sees 30 examples, because that's length of its context window.

52:24.320 --> 52:41.320
So it's in that sense fairly limited, even though it's this very smart, like GPT three or four system, whereas the verifier can see thousands of examples and so you can get much better tests of statistical significance.

52:41.320 --> 52:43.320
How am I doing on time.

52:43.320 --> 52:46.320
I think we have about five minutes left.

52:46.320 --> 52:48.320
Okay, cool.

52:48.320 --> 52:54.320
So maybe I'll just say, you know, you can use this for a bunch of things. So for describing distribution shifts.

52:54.320 --> 53:03.320
There's these two data sets MNLI and SNLI where SNLI is often used as like an OD version of MNLI.

53:03.320 --> 53:09.320
Here's four samples to which are from SNLI and to which are from MNLI.

53:09.320 --> 53:14.320
It's not immediately obvious what distinguishes them.

53:14.320 --> 53:28.320
But if I say that SNLI describes a picture, then it's very clear that the green ones are SNLI because it says the church choir sings to the masses and old man with the packages poses in front of an advertisement.

53:28.320 --> 53:31.320
And the other two are not about pictures.

53:31.320 --> 53:36.320
So you kind of immediately see what the distribution shift is here.

53:36.320 --> 53:44.320
There's two paraphrase data sets, Twitter, PPTV and QQP, which stands for core question pairs.

53:44.320 --> 53:48.320
It says Twitter talks about a news story and core contains a question.

53:48.320 --> 53:55.320
These are just kind of sanity checks like these would be kind of totally obvious to anyone who was familiar with these data sets.

53:55.320 --> 53:57.320
But you can do more interesting things.

53:57.320 --> 54:04.320
So this was one that I think to our knowledge was novel at the time we discovered it detecting spurious cues in a data set.

54:04.320 --> 54:10.320
So we handed it this data set called SUBJ, which is a data set for subjectivity analysis.

54:10.320 --> 54:13.320
And it said the objective class was a plot summary of a film.

54:13.320 --> 54:23.320
The subjective class is a quote from a film review, which seems like it should be wrong for a data set that's about subjectivity analysis.

54:23.320 --> 54:28.320
But if you actually go back and read the paper, it says,

54:28.320 --> 54:34.320
to gather subjective sentences, we collected 5000 movie reviews snippets from Rotten Tomatoes to obtain mostly objective data.

54:34.320 --> 54:38.320
We took 5000 sentences from plot summaries available from IMDB.

54:38.320 --> 54:47.320
So actually, if you did well in this data set, you were basically learning this rather than stuff about subjectivity.

54:47.320 --> 54:54.320
There's like other shortcuts we found somewhere new somewhere old, but you can sort of find all these various cues.

54:54.320 --> 54:57.320
You can use this for error analysis and so on.

54:57.320 --> 55:02.320
So maybe just to summarize, right, we have these four steps of this pipeline.

55:02.320 --> 55:08.320
Initial data was just the two text distributions, generate the hypothesis by prompting GPT-3,

55:08.320 --> 55:16.320
formalize the hypothesis by measuring the success rate, and then you kind of test on new held out samples.

55:16.320 --> 55:26.320
The final thing I'll just leave up is we have ongoing work that is kind of taking this far beyond classification to just sort of like generally using natural language

55:26.320 --> 55:31.320
predicates as features in statistical models.

55:31.320 --> 55:41.320
So this example here is trying to describe temporal drift in news headlines from the Australian Broadcasting Company,

55:41.320 --> 55:48.320
and it kind of identifies these five features that kind of vary.

55:48.320 --> 55:54.320
And you could think of as maybe the top five principal components explain the variation in this data set.

55:54.320 --> 55:58.320
Although the percent of variance explained is actually still pretty low here.

55:58.320 --> 56:05.320
So I think you should think of this as just like a initial result.

56:05.320 --> 56:07.320
So maybe I'll end there and take questions.

56:07.320 --> 56:26.320
So for the proposal, there can be just like do prompt optimization like overtime,

56:26.320 --> 56:30.320
you keep selecting like an example and then see where it fails and then show it again.

56:30.320 --> 56:36.320
And then after a while you have like this good hypothesis and that you can somehow guarantee that is correct

56:36.320 --> 56:38.320
in a set of checking things.

56:38.320 --> 56:40.320
So you could.

56:40.320 --> 56:44.320
So is the idea like basically like fine tune the proposal to get better and better?

56:44.320 --> 56:46.320
Yeah, fine tune the prompt.

56:46.320 --> 56:48.320
You do that prompt optimization.

56:48.320 --> 56:50.320
Oh, prompt optimization.

56:50.320 --> 56:59.320
So I think so my general sense is if you just use the proposal,

56:59.320 --> 57:02.320
I mean, we didn't try to do like the full prompt optimization,

57:02.320 --> 57:09.320
but we do do ablations of like just using the proposal and that generally does a lot worse.

57:09.320 --> 57:12.320
I think that there's a couple issues.

57:12.320 --> 57:16.320
What one is just that the proposal gets much less data than the verifier.

57:16.320 --> 57:20.320
So, you know, even like if you do prompt optimization.

57:20.320 --> 57:27.320
Oh, no, what's your I thought that your proposal like get like a few number of data.

57:27.320 --> 57:29.320
That's why you should do the checking.

57:29.320 --> 57:30.320
Yes.

57:30.320 --> 57:35.320
And then I was just saying that can you make the proposal more strong by just to prompt.

57:35.320 --> 57:41.320
I see everything and then you can guarantee that these hypothesis is correct.

57:41.320 --> 57:44.320
Yeah, so that's that's an interesting idea.

57:44.320 --> 57:51.320
So you're basically saying, okay, like do prompt optimization to find a prompt that gets this feel like we discussed this.

57:51.320 --> 57:55.320
What was your claim is that you don't get semantically.

57:55.320 --> 58:03.320
The point that you do gradient is gradient is that you find the prompts and you are you very easily get like a readable prompt that is not natural.

58:03.320 --> 58:06.320
I would stream because what you typically find with adversarial examples.

58:06.320 --> 58:08.320
They are relatively possible.

58:08.320 --> 58:09.320
Yeah.

58:09.320 --> 58:11.320
So I think the issue is at least right now.

58:11.320 --> 58:16.320
If you do this, it's hard to get kind of like natural language out right now.

58:16.320 --> 58:27.320
Right now you asked you for to give the gradients but

58:27.320 --> 58:30.320
Thank you for this amazing talk.

58:30.320 --> 58:38.320
The quick question maybe this is not so you're asking you're asking for hypotheses that separate the two data set.

58:38.320 --> 58:44.320
And when you were testing them, they were like, okay, let me pick two examples and say one is more positive than the other.

58:44.320 --> 58:56.320
But could you tweak this to possibly look for hypotheses that certainly answer yes in one case and no in the other case that you don't have to look at two comparative examples.

58:56.320 --> 58:59.320
Like, okay, this one talks of a female character.

58:59.320 --> 59:02.320
The other one talks of a male character.

59:02.320 --> 59:07.320
So what would be just so I can understand like what would be an example.

59:07.320 --> 59:16.320
So as an example, the second data, the second example you showed was that this statement talks of two female characters.

59:16.320 --> 59:22.320
That's a statement you can answer in yes or no without looking at two different samples from the two data sets.

59:22.320 --> 59:23.320
Yeah.

59:23.320 --> 59:32.320
So could you tweak this to possibly just look for hypotheses that could be answered on a single data point rather than a comparison between data points from different data sets.

59:32.320 --> 59:41.320
Yeah, so we actually, I think, worked with both versions of the system, one that is kind of unary predicates and one that's binary predicates.

59:41.320 --> 59:47.320
I think in practice, a lot of the interesting things you want out are kind of binary predicates.

59:47.320 --> 59:57.320
So you can get like you can get somewhere with this unary thing, but you're kind of losing something if you don't consider comparatives.

59:57.320 --> 59:59.320
Okay, let's thank the speaker.

