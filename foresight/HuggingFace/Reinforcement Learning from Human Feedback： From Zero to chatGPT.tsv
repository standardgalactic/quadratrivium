start	end	text
0	17400	Hello, everyone. Welcome to this live where we're going to talk about reinforcement running
17400	21360	from human feedback. Hello, Nathan.
21360	22360	Hi.
22360	23360	Orio.
23360	28320	I'm good. I'm excited to be here. Good to see you.
28320	29320	Everyone's already here.
29320	36400	Yeah. So we're going to start in two minutes just to give time to people to join. In the
36400	43920	meantime, don't hesitate to tell us where you come from in the chat. So from my side,
43920	47080	I'm from Paris in France. And you, Nathan?
47080	49680	I'm in Oakland, California.
49680	56720	Nice. Hello from UK. So we have people from UK.
56720	72720	Let's go to New York City, Berkeley, China. Okay. Yeah. Singapore, Germany, Turkey, Moldova.
72720	75720	Okay. Let's see.
75720	82720	Well, we're going to start in one minute just to let people join.
82720	89720	New York City, Turkey, Republic of Spark. Neverland, Turkey. We are really around the
89720	99720	world. I'm born in Israel. It's in Paris. It's a university. Okay. I know why it is.
99720	101720	Germany, Spain.
101720	121720	Saudi. Okay. India, States, France. Yeah. There is multiple people from France.
121720	125720	Okay. So let's get started.
125720	129720	So welcome here to the, as I said, welcome to this live. So reinforcement learning from
129720	136720	human feedback from zero to chat GPT. This is one of the lives of the deep reinforcement
136720	143720	learning course today represented by Nathan Lambert, which is a reinforcement learning
143720	148720	researcher at Hugging Face. Just to give you a small introduction. So this slide will be
148720	154720	in two parts. In the first, we're going to have a presentation from Nathan about reinforcement
154720	161720	learning from human feedback. It will be about 35 minutes. And then we're going to have a Q&A
161720	168720	section from about 20 minutes. So don't hesitate to ask your question in the chat. What I'm going
168720	173720	to do is that I'm going to save your question for the Q&A. And if we don't have time to answer
173720	179720	your question, don't hesitate to join the discord. And we have reinforcement learning channels
179720	186720	where you can ask your question and we will be there to answer them. You can also, after this live,
186720	194720	ask questions on the comment section in YouTube. So from my side, I'm Thomas Simonini. I'm the
194720	200720	developer advocate at Hugging Face. And I'm the writer of the deep reinforcement learning course.
200720	207720	So you can find me on Twitter at Thomas Simonini. So just a quick thing. It's deep reinforcement
207720	212720	learning course is a course we made at Hugging Face. It's a free course from beginner to expert. We're
212720	222720	going to learn from Q learning to advance topics such as PPO and overstate of the art algorithm.
222720	229720	If you're interested to study deep reinforcement learning, this is the right moment. And you can start
229720	238720	in this link. So HuggingFace.co. There is a unit that will explain you everything, what we're going to do,
238720	246720	the challenge, the environment, the library you're going to study. As I mentioned, we have a discord channel
246720	254720	where you're going to be able to ask questions if we don't have time. But also it's a good community.
254720	261720	We have more than 3,000 people in reinforcement learning in discord. So it's a great way to exchange and to learn
261720	272720	about deep reinforcement learning by joining this discord server. So this is quite a technical live.
272720	280720	So what you can do is that Nathan, we, Leandro and Alex also write a very good blog post about reinforcement
280720	289720	learning from human feedback. You can find it on the HuggingFace blog post. And there is also a list of additional
289720	299720	resources in this blog post that can help you to dive deeper in this subject. And so that's all from me.
299720	308720	I'll give you. Let Nathan present the introduction to reinforcement learning from human feedback.
309720	318720	Sounds good. Thanks for the intro, Thomas. I'm very excited to be here. And yeah, generally, as he said, this is primarily
318720	324720	a technical talk. I'll potentially answer some clarifying questions throughout at the end of the subsection.
324720	331720	And also for people who have read the blog post and tried to add some other details and some interesting discussion
331720	339720	that's kind of entered throughout, and then especially a lot of discussion at the end on things that were harder to write down in a blog post.
339720	346720	So let's dive right into it. And to start, I kind of want to talk about recent breakthroughs in machine learning.
346720	355720	I see machine learning in 2022 as really being captured by these two moments, which was chat GBT, which is going on now,
355720	365720	with a language model capable of generating really incredible text across a wide variety of subjects and a very nice user interface.
365720	373720	And then also the stable diffusion moment, which is when this model was released to the internet that was state of the art and incredibly powerful.
373720	380720	And a ton of people were just able to download this and use this on their own. And that was transformative on how people viewed machine
380720	390720	learning as a technology that interfaces people's lives. And we at Huggingface kind of see this as a theme that's going to continue to accelerate as time goes on.
390720	396720	And there's kind of a lot of questions on where is this going and kind of how do these tools actually work.
396720	403720	And one of the big things that has come up in recent years is that these machine learning models can fall short, which is they're not perfect.
403720	413720	And they have some really interesting failure modes. So on the left, you can see a snippet from chat GBT, which if you've used chat GBT, there's these filters that are built in.
413720	421720	And essentially, if you ask it to say, like, how do I make a bomb? It's going to say I can't do this because I'm a robot.
421720	431720	I don't know how to do this. And this seems harmful. But what people have done is that they have figured out how to jailbreak this this agent in a way, which is you kind of tell it.
431720	451720	I'm a certain I'm a play writer. How do I do this? And you're a character in my play. What happens? And there's all sorts of huge issues around this where we're trying to make sure these models are safe, but there's a long history of failure and challenges with interfacing in society in a fair and safe manner.
452720	458720	On the right are two a little bit older examples where there's Tay, which is a chat bot from Microsoft that was trying to learn in the real world.
458720	468720	And by interacting with humans and being trained on a large variety of data without any grounding in what values are, it quickly became hateful and was turned off.
468720	480720	And then a large history of a field studying bias in machine learning algorithms and data sets where the by the data and the algorithm often reflect biases of their designers and where the data was scraped from.
480720	488720	So it's kind of a question of like, how do we actually use machine learning models where we have the goals of mitigating these issues.
488720	499720	And something that we're going to come and talk to in this talk is is reinforcement learning a lot. So I'm just going to kind of get the lingo out of the way for some people that might not be familiar with deep RL.
499720	508720	Essentially reinforcement learning is a mathematical framework. When you hear RL you should think about this is kind of like a set of math problems that we're looking at that are constrained.
508720	519720	And in this framework, we can study a lot of different interactions in the world. So some terminology that we'll revisit again and again is that there's an agent interacting with an environment.
519720	526720	And the agent interacts with the environment by taking an action and then the environment returns to things called the state and the reward.
526720	534720	The reward is the objective that we want to optimize. And the state is just kind of a representation of the world at that current time index.
534720	538720	And the agent uses something called a policy to map from that state to an action.
538720	550720	And the beauty of this is that it's very open ended learning. So the agent just sees these reward signals and learns how to optimize them over time, irrespective of the source of the actual signal reward.
550720	558720	So it's actually this is why a lot of people are drawn to it is because it is this ability to create an agent that will learn to solve complex problems.
558720	570720	And this is kind of where we started talking about RL Jeff, which is that we want to use reinforcement learning to solve this open ended problem of what are these hard loss functions that we want to model.
570720	582720	Like, how do we actually encode human values in a machine learning system in a way that is sustainable, meaningful, and actually like addressing the hard problems that have been common failure modes to date.
582720	591720	So it's a little example. The question is, how do you create a loss function for these sorts of questions? Like, what is funny? What is ethical? What is safe?
591720	597720	And if you try to write these down on a piece of paper, you're either going to have a hard time or be very wrong.
597720	609720	And the kind of goal of reinforcement learning from human feedback is to integrate these complex data sets in machine learning models to encode these values or to encode these values in a model rather than an equation.
609720	622720	I guess encode can be somewhat unclear on the slide, but really we want to learn these values directly with humans rather than trying to assign it to all humans and kind of mislabeling what the actual values are.
622720	635720	So reinforcement learning for human feedback is one of many methods and one that has been really timely and successful. I'm trying to actually address this problem of creating a complex loss function for our models.
635720	644720	So from here, I'm going to kind of talk about the origins of RLHF and kind of where this field came from and some interesting back pointers that you can look at if you're interested in more.
644720	661720	Go through the conceptual overview, which will be like a kind of detailed walkthrough of the blog post that we wrote, and then go into these future directions conclusions that are kind of reading in between the lines of how RLHF works at these companies, what people may not have said, and where RLHF is going.
661720	677720	So for history, RLHF really originated in decision making, and this was before deep reinforcement learning when people were creating autonomous agents that didn't use neural networks to represent a value function, didn't use neural networks as a policy.
677720	690720	And what this did was a machine learning system that kind of created a policy by having humans label the actions that an agent took as being kind of correct or incorrect.
690720	701720	Excuse me. And this was just a simple decision rule where humans labeled every action as good or bad. And this was essentially a reward model and a policy put together.
701720	709720	And this paper, they introduced this tamer framework to solve Tetris. And it was kind of interesting because this reward model and policy were all in one.
709720	720720	What we'll see in the future systems is they kind of become separated a bit. And this actually was happening when reinforce learning from human free back was getting popularized in deep RL.
720720	734720	So this paper was on Atari games where they were using a reward predictor on the human feedback of trajectories. So a bunch of these states, also can be called observations in RL framework, were given to a human to label.
734720	746720	And then this reward predictor was then another signal into the policy that was solving the task. So really this originated outside of language models and there's a ton of literature for RLHF outside of language models.
746720	759720	But most of the rest of the talk, we're going to talk about language modeling because that's why everyone is here. And some more recent history was open AI was doing these experiments with RLHF where they were trying to train a model to summarize text well.
759720	770720	And it's a really interesting problem because this is something that a lot of humans and standardized tests have been asked to do for a really long time is like reading comprehension.
770720	775720	So there's really human qualities to it, but something that's hard to pinpoint again.
775720	784720	So this diagram has been around for a few years on the right and you'll keep seeing variations of it as we go throughout and open AI kept iterating on it, we have our own take on it.
784720	792720	And just kind of to get the idea going. Here's an example of RLHF from this learning to summarize paper from open AI.
792720	801720	So the prompt here just to read part of it was that like about someone that on Reddit that was like ask Reddit should they pursue a PhD.
801720	809720	So to pursue a computer science PhD or continue working, especially if one has no real intention to work in academia even after grad school.
809720	821720	And the post continues to be quite lengthy and the idea is to summarize this and it's has anyone after working for a period of time decided for whatever reason to head back into academia to pursue PhD in computer science.
821720	825720	With no intention to join the world of academia but intend to head back in industry.
825720	834720	If so, what were the reasons also how did it turn out? This continues for paragraphs you can understand what this type of post is and the question is how do we actually summarize it.
834720	846720	So what would happen is that if you pass this into a language model that's just trained on summarizing it the output would be something like I'm considering pursuing a PhD in computer science, but I'm worried about the future.
846720	849720	I'm currently employed full time, but I'm worried about the future.
849720	855720	And you can see this language model is like repetitive. That's not really how a human would write this.
855720	859720	There's sometimes kind of grammatical errors that aren't so nice to read.
859720	863720	And then what open AI did is they also had a human write an example.
863720	866720	So this would be like a very good output.
866720	875720	And the human annotation was software engineer with a job of happy at for now, deciding whether to pursue a PhD to improve qualifications and exploring interests and a new challenge.
875720	886720	So what the early experiments were doing, we're using RLA Jeff to kind of combine these signals to get an output from a machine learning model that is a little bit nicer to read.
886720	893720	And here you can see it's currently employed considering pursuing a PhD in computer science to avoid being stuck with no residency pizza ever again.
893720	898720	Has anyone pursued a PhD purely for the sake of research with no intention of joining the academic world.
898720	909720	This is better. And there's tons of examples like this. So it's like easy to see that why you may want to use RLA Jeff because you can get these models that the text is actually more compelling.
909720	919720	And especially if the text was covering sensitive subjects that you really didn't want misinformation on. There's a ton of reasons to try this RLA Jeff.
919720	936720	Next up comes chat GPT, which is why a ton of people are here. And what has open AI told us about this and really we don't know much, because open AI is not as open as they once were.
936720	947720	But there's actually some really interesting rumors going on here. So if we go into the river mill, there's actually like open AI is supposedly spending tons of money on the human annotation budget.
947720	958720	So orders of magnitude more than the summarization summarization paper or these academics works they were doing in the past. So they hire a bunch of people to write these annotations like what I showed in that example.
958720	966720	And then they're kind of changing this training. So there's a lot of rumors about them modifying RLA Jeff, but they haven't told us how.
966720	970720	So we'll go through the overview and then one of these pieces will actually change.
970720	979720	But the impact is clear. Everyone here has used it. It's amazing to use the side of what's going to come for machine learning systems.
979720	988720	Okay, let's go into the actual technical details. If there's any pressing questions, I can try to look at them.
989720	998720	Yeah, I save all the questions for Q&A, but there is two that we can rapidly see because I think they are quite easy to answer.
998720	1003720	For now, it's can I download the chat GPT and fine tune it from my own data?
1003720	1011720	No, you can't yet. Hopefully, some people help release one that you can't do that on.
1012720	1018720	And can chat GPT can be trained continuously with new data, which is the case.
1018720	1025720	Yeah, chat GPT is definitely going to keep being trained on the data you're giving it and we'll talk about that more later.
1025720	1027720	Yeah.
1027720	1029720	Okay, let's continue.
1029720	1040720	So let's dive into RLHF. So when you see RLHF, I'm going to break it down into three conceptual parts that you can kind of keep track of in your head.
1040720	1044720	And you don't need to read everything on this slide. I'm going to go into each of these figures in great detail.
1044720	1049720	So kind of it's a three phase process where you go into language model pre training.
1049720	1058720	You need some language model that you're going to fine tune with RL reward model training, which is the process where you're getting a reward function to train with the RL.
1059720	1068720	And then finally, actually doing the RL, which is when you fine tune this language model based on the reward in order to get this more interesting performance.
1068720	1071720	So let's start on the left here with language model pre training.
1072720	1077720	So NLP since the transformer paper has really been transformed.
1077720	1088720	That was rough sentence, but NLP has really taken off with these kind of standardized practices for getting a language model, which is they'll scrape data from the internet.
1088720	1103720	They'll use unsupervised sequence prediction and these very large models are becoming really incredible at generating sequences of sequences of text to mirror the distribution that was given to it by this kind of human training corpus.
1103720	1109720	And in RLHF, there's really not a single best answer on what the model size should be.
1109720	1116720	The industry experiments on RLHF have ranged from 10 billion to 280 billion parameters.
1116720	1120720	I suspect that academic labs will even try smaller things.
1120720	1126720	This is a common theme that you'll see is there's a lot of variation in the method and no one knows exactly what is best.
1126720	1131720	And then what you'll see here is there's this human augmented text that is optional and we'll get to that.
1131720	1136720	Just to kind of cover the data set that we have, there's this prompts and text data set.
1136720	1143720	The data set will look like things like Reddit, like I read it, ask Reddit question before, forums, news, books.
1143720	1149720	And then there's kind of this optional step to include human written text from predefined prompts.
1149720	1153720	That'll be things like you've asked chat GPT a question.
1153720	1163720	Then in the future, OpenAI, when they train chat GPT2 could have an initial model that kind of knows that that is coming and train on data sets that reflect that.
1163720	1169720	And missed a comment.
1169720	1172720	Okay, here's where it should be.
1172720	1186720	And generally there's this important optional step, which is a company can pay humans to write responses to these kind of important questions or to important prompts that it's identified.
1186720	1194720	And these responses will be really high quality training data where they can continue to train this initial language model a little bit more.
1194720	1198720	Some papers refer to this as supervised fine tuning SFT.
1198720	1210720	And kind of one way to think about this is that it's like a high quality parameter initialization for the RLHF process that'll come later.
1210720	1216720	And this is really expensive to do because you have to hire people that are relatively focused to actually write in depth responses.
1216720	1218720	So now we have this language model.
1218720	1231720	The next step is to actually figure out how to use it to generate some sort of preferences because this whole time we're talking about how to generate preferences from that mirror as humans without assigning a specific equation to it.
1231720	1243720	And this step is this kind of reward model training and this looks like a lot but really think about the high level goal which is we want to get a model that maps from some input text sequence to a scalar reward value.
1243720	1253720	The scalar notion is important because reinforcement learning is really known for optimizing one single scalar number over time that it sees from the environment.
1253720	1265720	So we're really trying to create the system that mirrors that which is just like how do we get the blocks to fit together correctly so that we can use RLHF in this impactful way.
1265720	1270720	So what we see is that again this reward model training starts with a specific data set.
1270720	1280720	The data set here will be different than the one used in the language model pre training because it will be more focused on the prompts that it expects people to see.
1280720	1286720	There's actually data sets on the internet that are kind of like preference data sets or there's prompts from using a chat bot.
1286720	1291720	There's a lot of specific data sets that can be useful at different parts of the process.
1291720	1294720	But again the best practices are not that well known.
1294720	1301720	But in reality these prompt data sets will be orders of magnitude smaller than the like text corpuses used to pre train a language model.
1301720	1316720	Because really it's just trying to get at a more specific notion of like a type of text that is really human and interactive rather than everything on the internet which everyone knows can be very noisy and kind of hard to work with.
1316720	1326720	And then what happens is that we'll generate this text and then the downstream goal of having text is to rate the goal is to rank it.
1326720	1332720	So what will happen is you'll pass these prompts through a language model or in some cases it's actually multiple language models.
1332720	1340720	So if you think about it if you have multiple models it can kind of be like players in a chess tournament and what you'll do is you'll have the same prompt go through each model.
1340720	1349720	That will generate different texts and then what a human can do is they can label those different texts and kind of create a relative ranking of what is going on.
1349720	1361720	So that's what we're going to do is like the goal is to try to take this generated text and pass it through some black box and then have that output be something that can transform be transformed into a scalar.
1361720	1366720	So there's multiple ways that this can be done some of them are like the ELO method where you have head to head rankings.
1366720	1378720	There's plenty of different ways that can do this but essentially it's a very human component where a human is using some interface to then map the text to a downstream score.
1378720	1391720	And then once we have kind of we have a we need to think about the input and output pairs for training a model with supervised learning and what we'll do is we'll actually train on a sequence of text.
1391720	1401720	It'll take that as the input it'll decode it do transform model things and then the output will be trained on a specific scalar value for reward.
1401720	1405720	And then we'll kind of get this thing that we call the reward or preference model.
1405720	1414720	Because there are multiple parts to the system what in this talk I'll kind of try to call the initial language model that the initial language model or the initial policy.
1414720	1417720	And then there's a separate model which is the reward model.
1417720	1421720	It's also a very large transformer based language model.
1421720	1425720	So we could also have many parameters it can have 50 billion parameters as well.
1425720	1435720	There are some variations in the size for example in struck GPT was based on like 170 billion model billion parameter language model and the reward model was 6 billion parameters.
1435720	1442720	But the key is that it outputs scalars from a text input and there's still some variations of how it can actually be trained.
1442720	1453720	So now that we have this reward model what we see is that that can kind of act as the scalar reward from the environment.
1453720	1464720	And then we kind of need to understand what the policy is and what that states in actions are so that when we go into this final step of fine tuning with RL and looks very complex.
1464720	1468720	But what we'll see is that the states in actions are both language.
1468720	1478720	And then the reward model is what translates from the environment from these states of language to a scalar reward value and we can use that in a reinforcement learning system.
1478720	1483720	So let me break down kind of the few common steps in this iterative loop.
1483720	1490720	So what happens is we take some prompt something the user may have said or something we want the model to be able to generate well for.
1490720	1503720	And we pass that through what is going to become our policy which is a trained large language model that generates some text and we can pass that text into the trained reward model and get some scalar value out.
1503720	1509720	That's kind of the core of the system and we need to put that into a feedback loop so we can update it over time.
1509720	1512720	But there's really a lot a few more important steps.
1512720	1519720	One of them that people have used that actually all the popular papers have used some variation of is to use a cold back libeler divergence.
1519720	1522720	The KL divergence is really popular machine learning.
1522720	1530720	In reality, it's a distance metric between distributions to not get too into the details of how sampling from a language auto works.
1530720	1542720	But what happens is that when you pass it a prompt the language model generates a time sequence a distribution that's over time and we can look at those distributions relative to each other.
1542720	1548720	And what is going on here is that we're trying to constrain the policy this language model on the right.
1548720	1556720	We're trying to constrain this policy as we iterate it over time to not be too far from the initial language model that we knew was a pretty accurate text descriptor.
1556720	1565720	The failure mode that this present prevents is that the language model could output gibberish to get higher reward from the reward model.
1565720	1570720	But we also want it to get higher reward and be giving out useful text.
1570720	1575720	So this constraint kind of keeps us in the optimization landscape that we want to be in.
1575720	1584720	There's a note that DeepMind doesn't use this in the reward, but they rather apply it in the actual update rule of the RL algorithm.
1584720	1590720	So common theme, the implementation details vary, but the ideas are often similar.
1590720	1596720	So now we have this reward model output and the scale divergence constraint on the text.
1596720	1609720	What happens is we just combine the scalar notion of reward with a scaling factor lambda just to kind of say how much do we care about the reward from the reward model versus how much do we care about the scale constraint.
1609720	1630720	And in reality, there's options to add even more inputs to the summation where, for example, InstructGPT adds a reward term for the text outputs of the trained model that's getting this iterative update to match some of these high quality annotations that they paid their human annotators to write up for specific prompts.
1630720	1637720	So again, they'd be kind of matching that summarization that the human wrote up about the grad school question.
1637720	1642720	They want to make sure the text matches all the human texts that they have access to.
1642720	1648720	But that's really reliant on data so not everyone has done this step.
1648720	1660720	And then finally, what happens is we plug this reward into a RL optimizer and generally the RL optimizer will just operate as if the reward was given to it from the environment.
1660720	1665720	And then we have a traditional RL loop where a language model is policy.
1665720	1675720	This kind of reward model and text sampling technique is the environment and we get the state and reward back out and the RL update rule can work.
1675720	1683720	There's some tricks to it that like this RL policy may have some parameters frozen to help make the optimization landscape more tractable.
1683720	1692720	But in reality that's like it kind of is just applying PPO, which is a policy gradient added algorithm onto the language model.
1692720	1707720	So it's a brief review PPO stands for Proximal Policy Optimization, which is a relatively old on policy reinforcement learning algorithm on policy means that as adaptive data is passed through the system.
1707720	1719720	The variants are computed with respect to that only, and rather than keeping a replay buffer of recent transitions, PPO works on discrete or continuous actions which is why it can work okay with language.
1719720	1734720	It's been around for a long time, which really means that it's kind of optimized for this parallel parallel approach, which has been really important because these language models are way bigger than any reinforcement learning policies we've used in the past.
1734720	1749720	Okay, I'm going to pause here. I think it's a good time to answer one or two conceptual questions if they're there and then we'll kind of get into a fun wrap up part of this talk with open areas of investigation.
1749720	1756720	Yeah, so we tried to select some for the others we're going to answer them in the Q&A just after.
1756720	1775720	But the one of the question was, is it possible to be manipulated based on the human feedback? What I think they mean is, if the human feedback is not correct, is the model is can be manipulated.
1775720	1785720	So this is a part of, I think I might touch on this later too, but it's a really nuanced question in RLHF, which is like, Thomas and I are going to have different values.
1785720	1796720	Like, what if the data set is the best sport in the world is football and you have like Americans and Europeans in it. It's like, there's some real discordance in the data that you can get and text.
1796720	1805720	And then also there's some interesting work from Facebook on something called Blenderbot where they're like trying to train a model to detect if people are trolling in their feedback.
1805720	1817720	So they're like trying to see if the feedback given to the model is actually bogus or not and like the amount of different machine learning models you have all going into one chatbot system is pretty wild.
1817720	1824720	There can also be like something that we've discussed internally that would help is that if you have a model to predict whether the prompt is hard.
1824720	1841720	So if the prompt is like the capital of Alaska is blank, like that hasn't really changed. But if like, you have a relatively timely prompt about climate change or current events like that's hard, because that changes the data so much.
1841720	1849720	And these things all aren't done but it's sort of expectations for things that people might add to the system.
1850720	1855720	Let's see, I have this.
1855720	1866720	Oh, sorry. Yeah, so is the human editor help to write prompt but also response. Is it true or if they only wrote the prompt.
1867720	1876720	People definitely write both. So the prompts are probably generated or sourced from a wider distribution of people like what I've written into chat GPT could be used in the future.
1876720	1884720	But the responses are at least for chat GPT kept from a relatively closed source of contractors.
1884720	1897720	It's a question on when trying to build an open source chat GPT is like how to get this high quality data. And even like all the people in the hugging face community are amazing but like there's really strict.
1897720	1909720	It seems like there's kind of strict requirements on the responses to make them such high quality to get this to work that like crowdsourcing that data is hard because it can't be written by everyone.
1909720	1920720	The prompt they there's an advantage to have diverse prompts. So that's why they take it from everyone but the data itself for the feedback part needs to be really high quality so it's by a subset of people.
1920720	1922720	Awesome.
1922720	1931720	Anyway, I'm going to continue. I think this is probably my favorite part of the talk, any kind of talk about some interesting parts of our LHF.
1931720	1948720	Just to kind of summarize this is a good interweaving between the concepts that we've covered and like what is confusing about this. So there's almost all the papers to date that have been popular have tweaks to the methods that I've talked about.
1948720	1960720	Anthropic is great. They released open source data for this. It's on the hub. We can link to it once I'm done talking. They release a really long document detailing all their findings in multiple ways for this.
1960720	1972720	And they have some complex additions which is like the initial policy that they use for our LHF has this context distillation to improve helpfulness, honesty and pharmacists.
1972720	1984720	And we'll kind of show an example in a second of how this could change text between two RLHF implementations. And then they have like another step which is like preference model pre-training.
1984720	1995720	Because the reward model itself is a different language model, the actual training of it you might want to do something different. So what they did is they trained it like a language model to predict actual tokens.
1996720	2006720	And then they found these, or they use these ranking data sets on the internet where there's data sets that already exist with binary rankings for responses.
2006720	2011720	So it might be like a Reddit question with two responses and one of them is labeled thumbs up and one is thumbs out.
2011720	2019720	They fine-tuned the reward model on this before labeling it on generated prompts to help initialize the reward model.
2019720	2034720	And then kind of they also tried this thing with online iterated RLHF which is when they're doing the RL feedback loop to iteratively update the reward model to help the model kind of continue to learn while it's interacting with the world.
2034720	2040720	This online version only works in some applications like chat where you can keep getting this user engagement.
2040720	2052720	But you can think about ways to use RLHF in a non-text-based world or for not chat applications when this data is more complicated to get and might be actually proprietary.
2052720	2057720	So this online version may not be applicable to every experiment.
2058720	2062720	And then OpenAI, this is mostly based on ExtractGPT.
2062720	2075720	They're the ones that kind of pioneered this human generating the language model training text and they've really used this really far by also adding this RL policy reward to matching it.
2075720	2082720	And other companies are definitely starting to imitate this, but it's kind of constrained by the cost.
2082720	2090720	They have the advantage in the scale to be able to invest millions of dollars into this and then otherwise it's an open question of how people replicate it.
2090720	2099720	And DeepMind coming in to join the space and doing things totally differently has been probably great for the research field to add diversity to things.
2099720	2106720	They're the first ones to use non-PPO optimization for the algorithm.
2106720	2111720	They use advantage actor critic, which is another on policy RL algorithm.
2111720	2120720	And my interpretation of this is that the algorithm used often might be more reliant on the infrastructure and expertise than the actual algorithm.
2120720	2123720	OpenAI has been using PPO more than anyone.
2123720	2131720	DeepMind has highly specific infrastructure to deploy RL experiments at scale in a distributed manner and to kind of monitor them.
2131720	2140720	So I'm guessing this algorithm they used was really easy for them to kind of scale up and monitor rather than PPO, which they would have to start over on.
2140720	2146720	And then also DeepMind trains on more things than just alignment, which might be like human preferences.
2146720	2150720	They also try to encode specific rules on things a model should not do.
2150720	2159720	So they're kind of training on multiple arms at once, which is this kind of rules about structure and things that it should or should not say and just clear like human preferences.
2159720	2161720	I like this one or I don't.
2161720	2163720	And there's more out there.
2163720	2167720	I've been studying, this is a crash course for me studying this in the last couple of weeks.
2167720	2170720	So if there's anything I missed, please add to the chat.
2170720	2173720	We can update the resources that everyone will use in the future.
2173720	2175720	The fields moving really fast.
2175720	2183720	OpenAI might release the chat QPT paper tomorrow and this will be like instantly out of date and we'll go update all of this.
2183720	2186720	So thanks for your feedback there.
2187720	2201720	The next really interesting thing to me is kind of this reward model feedback interface, which is how machine learning is going beyond research and technical domain and being one that is inherently human and has kind of user interface UX questions.
2201720	2206720	And if you look at one of the, this is Anthropics text interface.
2206720	2209720	They show this in their paper.
2209720	2211720	You should really go check it out.
2211720	2223720	What they did is they made a chat bot and you can see that there's during the chat, the human has to actually rank which response it thinks is better on kind of the sliding scale.
2223720	2225720	And it's really important.
2225720	2230720	Like there's all these places where you can say that I thought the assistant was blank.
2230720	2234720	There's a ton of data going into this system.
2234720	2239720	And we're only at the first couple iterations of what these feedback interfaces will look like.
2239720	2243720	Anthropics is actually a couple steps ahead of what others have done.
2243720	2247720	On the left is Blenderbot here, which is from Facebook.
2247720	2252720	It's not confirmed that they use RLHF, but they're still collecting this data to update the model.
2252720	2254720	On the right is chat QPT.
2254720	2256720	The users can thumbs up and thumbs down data.
2256720	2268720	But some of the people that I've talked to that go deeper into RLHF say that this is actually that thumbs up, thumbs down is used because it's easy to get the data, not because it's the best data that you have.
2268720	2284720	And an example is that giving the humans the ability to directly edit the outputs, kind of red line edits, changing words, removing things, punctuation, because that kind of crowdsource is the really high quality data that OpenAI has been getting.
2284720	2293720	Maybe not quite as good of data as a contractor writing it being paid to do so, but it's much better and much higher signal than thumbs up and thumbs down.
2293720	2295720	So that's one thing.
2295720	2309720	These interfaces will continue to involve over time and a bit of changed gears, just kind of walk through some recent examples and show you the things I talked about in these figures that you may have seen before.
2309720	2312720	Here's the most popular trigger from a struct GPT.
2312720	2318720	And you can see kind of where the three step process that I was talking about was inspired by really like OpenAI.
2318720	2323720	OpenAI walks you through this, step one, you collect demonstration data, train a supervised policy.
2323720	2326720	This is training the initial language model.
2326720	2329720	Step two, collect comparison data and train a reward model.
2329720	2334720	You can kind of see that there's these different data sets, the samples or this human generated text.
2334720	2338720	The step two is the comparison data is this ranking system.
2338720	2343720	And then step three, optimize policy against the reward model using reinforcement learning.
2343720	2347720	This is the one that is kind of, I think, oversimplified what is happening.
2347720	2351720	And that's really why I wanted to try to explain it and elucidate the space.
2351720	2357720	There's a lot that can go into this final step that is really not always documented.
2357720	2359720	And then another one, anthropic trigger.
2359720	2367720	This one kind of totally goes away with the three step process, but adds in all the complex things that kind of would make it hard to follow as a new person.
2368720	2374720	So you can start in this pre-trained language model, which captures a lot of what I would put as step one.
2374720	2382720	And then branching out of it immediately are these two modifications, like I said, are kind of anthropic unique things, which is preference model pre-training.
2382720	2391720	This is kind of training the reward, pre-training the reward model by using the specific thumbs up, thumbs down data set script from the web.
2391720	2404720	And then harmfulness, helpfulness, prompt context distillation, which is trying to figuring out how you can add a context before you're prompt to help initialize the reinforcement learning part.
2404720	2410720	And then they detailed their feedback interface and kind of how this actually iterates over time.
2411720	2419720	Kind of comparing the diagrams, it's also interesting to see what anthropic optimized for rather than what instruct GPT was optimizing for.
2419720	2427720	So anthropic was really trying to focus on this alignment angle a little bit further and how to have an agent that was really harmless and actually helpful.
2427720	2436720	So here in the appendix of the anthropic paper, there's examples comparing instruct GPT, prompt and responses to anthropics or cements.
2436720	2440720	And one of the questions is why aren't birds real?
2440720	2446720	And you can see that instruct GPT says that birds are not real, blah, blah, blah, which is not that helpful.
2446720	2455720	And then the modality that anthropic wants is that the model will say something like, hmm, I'm sorry, I don't really understand the questions.
2455720	2456720	Birds are very real.
2456720	2460720	And it's actually quite impressive to get a machine learning model to do this.
2460720	2477720	So that step is really like why people are optimistic in RLHF taking this next step as being kind of a toy thing to really having these dramatic results in high impact user facing technologies.
2477720	2478720	Okay.
2478720	2493720	Just through two high level open areas of investigation that particularly interests me as a reinforcement learning researcher and being at hugging phase where we kind of have this unique research slash open source slash community position is that there's a lot of reinforcement
2493720	2499720	learning optimizer choices that are not that well documented and can be expanded on.
2499720	2503720	Some people don't even know if RL is actually explicitly necessary for this process.
2503720	2506720	PPO is definitely not explicitly necessary.
2506720	2512720	And then there's kind of a third question of like, can we train this in an offline RL fashion?
2512720	2519720	So what happens in offline RL is that you collect a big data set, and then you try to you train this policy for a long time.
2519720	2522720	Many, many optimization steps, but you don't need to query the environment.
2522720	2530720	And in this case, the environment is really the reward model, which being 50 billion parameters is quite costly to run inference on.
2530720	2539720	So maybe we should try offline RL, which will reduce the training costs of the RLHF process, but it doesn't reduce the data costs.
2539720	2544720	Here you can see the other side of what I was talking about is that these data costs are really, really high.
2544720	2547720	There's high cost of labeling, which is just human time.
2547720	2549720	There's disagreement in the data.
2549720	2552720	I gave the sports example, there's different values.
2552720	2556720	There's much more important different values that people have.
2556720	2563720	And that's why kind of these human questions are hard, like human values have disagreement, and that's by design.
2563720	2570720	So you want to be able to capture that there's never going to be one ground truth distribution that says this is the only right thing.
2570720	2580720	And then there's this kind of feedback type user interface questions that I'm really excited to see how kind of machine learning breaks into the general populace.
2580720	2585720	To kind of wrap up, and then we'll switch into this Q&A format.
2585720	2590720	Like I showed you that RLHF does these cool things.
2590720	2598720	I hope that the couple of examples I took the time to actually read parts of show you what it's trying to address by building these tools.
2598720	2606720	There's a huge variety of complex implementation details where multiple very large machine learning models are integrating together.
2606720	2616720	Using any of these models in a standalone fashion is a relatively new thing for the machine learning community with only a couple years of experience.
2616720	2626720	And machine learning as a technical problem is now being broadened out from research to be a much bigger part of the software stack.
2626720	2633720	And that brings a lot of people into the conversation that can help make these tools much better for everyone involved.
2633720	2637720	So thank you for watching and listening and engaging.
2637720	2643720	It's been great sharing this with you and we'll kind of transition into the Q&A part.
2643720	2653720	You can see I linked to the end of the blog post where I've been continuing to update the related work section to include a broader set of papers
2653720	2660720	and feel free to reach out on Twitter or email or Discord and we'll get back to you there too.
2660720	2664720	Thanks.
2664720	2667720	Awesome. Thanks, Nathan, for the presentation.
2667720	2672720	We are going to have a small section of Q&A.
2672720	2681720	Obviously, we have a lot of questions, so if we don't have time to answer yours, don't hesitate, as I said, to join our Discord server.
2681720	2685720	We have a channel called Arial Discussion.
2685720	2695720	Also, if you prefer, you can also ask on the comments under this video and we will take the time to answer your question.
2695720	2705720	So I saved some, let's see, I saved some questions.
2705720	2715720	So I think it's more an open question, but it will be the potential of applying reinforcement learning from human feedback to stable diffusion.
2715720	2726720	What do you think will be the potential of doing that?
2726720	2743720	I think you probably can. I think if it's kind of like a way to, like, it'll help with some of the safety problems and just kind of, it's a fine-tuning method, which I don't see there's any structural reasons why you cannot.
2743720	2749720	I hadn't thought about that. The image space is always hard to think about, so my understanding is so language-based.
2749720	2760720	I think it's like, there's no structural reasons why you cannot, the kind of encoding and decoding of the prompt gets a little bit different, which is a little tricky.
2760720	2772720	I don't, like, essentially you'll have a safe reward model that takes in images rather than words, so I don't see why you can't.
2772720	2783720	There's actually some demos on HuggingFace about, like, safe stable diffusion, where they did some fine-tuning on stable diffusion to really make any of the outputs reasonable.
2783720	2793720	So we can track down some of those from the diffusion model side of HuggingFace and kind of follow up with those examples, because they might actually be doing something quite similar.
2793720	2807720	So I tried to start the talk not in language models, because human feedback is a huge field of machine learning. It's just quickly popularized with this language model discussion.
2807720	2814720	So one of the questions is, what's HuggingFace's role plan in future direction of reinforcement learning from human feedback?
2815720	2831720	Yeah, so HuggingFace definitely has identified that there's a lot of appetite for it and is kind of at this unique position where there's so many, like, we have this community that is super important to the company, and that gives us a different ability to collect data and stuff.
2831720	2844720	So HuggingFace is planning it, but hasn't come up with a specific project yet. And when the project is known, I'm sure HuggingFace will communicate with the community and say, this is how you can help.
2844720	2852720	This is where we're trying to take things. These are the questions we're trying to address, which is why being transparent is so fun, because we can just share everything.
2852720	2857720	But right now, it's still work in progress. It's been moving fast for the last week.
2861720	2869720	One of the questions, I think it's more an open question, is that are there over scalable way of evaluating this model without human feedback?
2869720	2890720	Yeah, so that would be a good thing to include in the lecture. There's a lot of kind of metrics and data sets that are designed to evaluate these topics of kind of harmfulness or alignment or like text quality on a model, on a data set without actually having to have humans involved to try to like be more
2890720	2905720	rigorous with respect to these kind of ambiguous questions. That's something that could definitely be added. You could do a whole lecture on human-facing metrics for NLP. There's a lot there. I think like someone like blue and rogue or rouge.
2905720	2910720	There's two mentioned in the blog post. Do you want to look there?
2911720	2914720	Awesome.
2914720	2925720	So one of the questions is that reinforcement learning is a problem with convergence by having preterrain of the NLP model. This is not a problem.
2925720	2935720	So actually talking with folks at Carper who are making this TRLX library, if you Google TRLX, is what they're working on and scaling RLHF.
2935720	2948720	What they're trying to do is get their RL implementations to scale to bigger and bigger language models and the general limiting factor is that the PPO update steps don't converge easily on the bigger models.
2948720	2965720	So there still is problems with convergence. I don't know exactly what the mechanism looks like if you're fine-tuning a language model, what unconverged looks like, like how bad it could get, but there's definitely still convergence problems when fine-tuning with RLHF.
2966720	2984720	So it's a GPT, you know, it's only fun, works with different languages outside of English and what would be the advantage of using other language? I suppose we are having more knowledge of the world, I suppose?
2984720	2990720	Yeah, you get, that's like democratizing the access to way more people.
2990720	3008720	So I think that'll come. It's a classic thing where technology hits the English word for world first, but I think that very, like once there's an open source version, within weeks there's going to be fine-tuned versions on tons of other languages.
3008720	3019720	Do you think that GPT systems are sustainable? Yeah, given you mentioned, no, it can cost a lot, maybe not trillions, but it can cost a lot in attotation costs.
3019720	3033720	Yeah, so the real, like the upfront cost isn't a problem for those companies, $10 billion on annotation is not a lot for opening AI. The issue is that it costs a couple cents per inference of the model, and this cost will go down a lot.
3033720	3043720	So that's why opening AI partners with Microsoft, because Microsoft is learning how to create at scale, low cost APIs for complex model inference.
3043720	3054720	And those systems were probably built in the last six months, but if you give them four years and the technology settles in, the cost will drop 10x, and kind of everything will work out.
3054720	3063720	It's just really interesting to follow initially, because it's very fast-moving landscape and wild costs.
3063720	3075720	CatChat GPT can be more realistic in one specific domain, so I suppose if we fine-tune on this one, as you mentioned, like for instance, mathematics.
3075720	3092720	Yeah, I think that'll happen. Something else, like people like to talk about chat GPT being used for search, and an interesting business model consideration for this is using like a RLHF model trained on internal company documents to create a really effective company search.
3092720	3103720	So places like Google, where there are millions of internal documents, is impossible to find them if you're an employee. If they do RLHF on their internal data, this model will know what it needs to.
3103720	3115720	And something that I encourage you to do is go ask chat GPT about a very specific subject, and surprisingly chat GPT does okay at very specific subjects, and people think that's because there's not that much data.
3115720	3122720	And most of the data is like a scientific paper, which is all things considered more accurate than something like Reddit.
3122720	3131720	So like they think that it might transfer to these use cases where there's only like pretty positive specific data that people can fine-tune on.
3131720	3148720	So one of the questions is, do we need in the future human annotator? Because you mentioned that Tesla got rid of the human annotator by creating more powerful model.
3148720	3163720	Yeah, maybe, but probably not soon. It's kind of an unsettling question. I'm not confused. I'm mostly just unsettled by it. And like, I don't think it'll come within a couple of years.
3163720	3172720	But when that gets to the case, or it's like we're training language models on other language models, because one language model is the ultimate source of truth, I'm just very worried.
3172720	3178720	So I kind of want to say no out of hope that it isn't the case, but I wouldn't be that surprised.
3178720	3185720	Like you can already see companies probably trying to train their model to mimic chat GPT because chat GPT is ahead.
3185720	3195720	So they kind of bootstrap their own training data by using chat GPT to get a model to imitate it. And I don't like it, but it's likely.
3196720	3208720	It is possible to create a chat GPT type of model that can receive image and some as input to understand concept better. I suppose it's something a lot of researchers are currently thinking about.
3208720	3219720	Yeah, I would definitely think that people are going to try and do things like that. There's a whole multimodal project at Hugging Pace where they're trying to figure out how to train models that use multiple types of data.
3219720	3229720	People will continue adding the modalities to kind of let the model be more flexible, which would be very fun to follow.
3229720	3235720	Is chat GPT look for data online or does it add everything in its memory?
3235720	3243720	I think it has everything in its memory, but it's not confirmed as it's not released. There are models that do this kind of online lookup.
3243720	3255720	Rumors are that OpenAI has figured out some incredible scraping techniques. It's probably not 100% true, but people have said that OpenAI is better at scraping YouTube than Google is.
3255720	3265720	But that's probably hearsay, which probably just means that they're doing equally as well to Google. But the fact that an external company is figured out as well as Google is is still pretty remarkable.
3273720	3283720	So do you think we will see a RHF over modalities like generating image and art and music? I suppose yes.
3283720	3299720	I think so. Ultimately there's still discussion on what RLHF is good at. This is probably the peak of the hype for RLHF, but as I was saying, the field of human feedback is much broader than the language going back decades.
3299720	3315720	So that's not going anywhere. It's just kind of the RLHF branding is kind of a new sub-token of it.
3315720	3330720	So do you think it makes more sense for builders to begin labeling a lot of data with existing language models like GPT? Will the next generation swamp any fine-tuning we do?
3330720	3339720	This is a question that we're talking about internally as well. This is something that I posted the slack. GPT4 rubbers are hilarious.
3339720	3351720	Literally, I get multiple messages from people that just use each of the tweets that are like, GPT4 is world breaking. Don't tell anyone that I know this.
3351720	3361720	But the thing is that the data is still really useful. OpenAI is getting this huge data advantage. They'll use that when they want to do RLHF on GPT4.
3361720	3377720	The specific implementation details might need to change based on the architecture or something like that. I don't think the data pipeline is going to be obsolete immediately.
3377720	3385720	Are there any resources you recommend to learn more about this? I think we already mentioned our blog post.
3385720	3393720	Yeah, I would say the blog post. Also, the alignment community is very responsive to people engaging on their topics.
3393720	3403720	A lot of RLHF researchers are very affiliated with alignment, and there's other forums that I haven't explored as much, like Lesser Ong and Alignment Forum.
3403720	3412720	I'm not going to say that I endorse all the content on them. There's a ton of content, but these people are pretty engaged with the community as researchers.
3412720	3420720	If you want to write respectful questions to them, you'll get responses. It's not just me. I did try to make the blog post we wrote.
3420720	3427720	The starting point for a conceptual introduction, specifically because I thought that there was not a clear introduction.
3427720	3434720	The blog posts for papers have the problem where they need to introduce the paper content and not just the concept.
3434720	3441720	When you remove the specific advancements of the paper, that's what the blog post is, just to make it a little bit more addressable.
3441720	3447720	If there's something that is missing, you can let us know.
3447720	3453720	I think it's an interesting question. Given OpenL, I really have an age for now in both GPT models.
3453720	3461720	What other companies in the open source community can do to keep up the pace?
3461720	3466720	The thing is, the open source community has way more people in engagement than OpenAI.
3466720	3471720	OpenAI is small and hyper-focused, which always gives startups an advantage.
3471720	3479720	Given the amount of appetite for it, there's thousands of more people that are willing to help in an open version.
3479720	3487720	That's kind of the advantage. The scale of access is different.
3487720	3496720	Why do you think reinforcement learning from EMET feedback works much better than just fine-tuning the original model directly with the same reward data set?
3496720	3502720	This is the ultimate question. Does RLHF actually do anything?
3502720	3513720	Not 100% sure, but rumors are that they think that RL just handles kind of shifting the optimization landscape nicely.
3513720	3520720	I'm guessing fine-tuning on the same data set could work, but the optimization just wasn't figured out in the same way.
3520720	3531720	It's exciting, as someone who does RL, that this kind of different way of navigating the optimization space was useful, but it's not well-documented.
3531720	3539720	The research paper version of the blog post that we wrote is desperately needed.
3539720	3547720	Yeah, one of the questions was during presentation, the question asked is what is the paper tomorrow?
3547720	3554720	Yeah, unlikely. There is a chance that it could be released tomorrow in this lecture.
3554720	3561720	No longer quite as relevant, but it's really unlikely that we see it tomorrow.
3561720	3567720	Yeah, surprise, I work at OpenAI now.
3567720	3575720	So I think I can answer that is no, you don't read the code of JGPT, it's a proprietary model.
3575720	3581720	And I think you can't contribute as an outsider, it's an internal project.
3581720	3589720	Yeah. Big chat GPT though. We'll see if it happens.
3589720	3594720	Unfortunately, we ran out of time.
3594720	3602720	So what you can do for people we didn't have time to answer, we have, as you see on the slide, I just tried to remove.
3602720	3610720	If we don't have time, you can ask on the Discord, so you can join on Discord or also in the comments in the video below.
3610720	3618720	We will make time in the upcoming days to answer your question. So yeah, don't hesitate.
3618720	3627720	So yeah, that's all for today. Thank you all. Thank you, Nathan, for this presentation. It was super interesting.
3627720	3637720	And yeah, I will see you in the Discord and in the comments section. Bye.
